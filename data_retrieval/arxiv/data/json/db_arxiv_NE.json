[{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/9809049v1", 
    "title": "Aspects of Evolutionary Design by Computers", 
    "arxiv-id": "cs/9809049v1", 
    "author": "Peter J Bentley", 
    "publish": "1998-09-23T11:01:55Z", 
    "summary": "This paper examines the four main types of Evolutionary Design by computers:\nEvolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial\nLife Forms and Creative Evolutionary Design. Definitions for all four areas are\nprovided. A review of current work in each of these areas is given, with\nexamples of the types of applications that have been tackled. The different\nproperties and requirements of each are examined. Descriptions of typical\nrepresentations and evolutionary algorithms are provided and examples of\ndesigns evolved using these techniques are shown. The paper then discusses how\nthe boundaries of these areas are beginning to merge, resulting in four new\n'overlapping' types of Evolutionary Design: Integral Evolutionary Design,\nArtificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and\nAesthetic Evolutionary Design. Finally, the last part of the paper discusses\nsome common problems faced by creators of Evolutionary Design systems,\nincluding: interdependent elements in designs, epistasis, and constraint\nhandling."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/9812002v1", 
    "title": "Training Reinforcement Neurocontrollers Using the Polytope Algorithm", 
    "arxiv-id": "cs/9812002v1", 
    "author": "I. E. Lagaris", 
    "publish": "1998-12-03T09:08:03Z", 
    "summary": "A new training algorithm is presented for delayed reinforcement learning\nproblems that does not assume the existence of a critic model and employs the\npolytope optimization algorithm to adjust the weights of the action network so\nthat a simple direct measure of the training performance is maximized.\nExperimental results from the application of the method to the pole balancing\nproblem indicate improved training performance compared with critic-based and\ngenetic reinforcement approaches."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/9902025v1", 
    "title": "An Efficient Mean Field Approach to the Set Covering Problem", 
    "arxiv-id": "cs/9902025v1", 
    "author": "Bo S\u00f6derberg", 
    "publish": "1999-02-12T09:18:53Z", 
    "summary": "A mean field feedback artificial neural network algorithm is developed and\nexplored for the set covering problem. A convenient encoding of the inequality\nconstraints is achieved by means of a multilinear penalty function. An\napproximate energy minimum is obtained by iterating a set of mean field\nequations, in combination with annealing. The approach is numerically tested\nagainst a set of publicly available test problems with sizes ranging up to\n5x10^3 rows and 10^6 columns. When comparing the performance with exact results\nfor sizes where these are available, the approach yields results within a few\npercent from the optimal solutions. Comparisons with other approximate methods\nalso come out well, in particular given the very low CPU consumption required\n-- typically a few seconds. Arbitrary problems can be processed using the\nalgorithm via a public domain server."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0110021v1", 
    "title": "Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior", 
    "arxiv-id": "cs/0110021v1", 
    "author": "Roman V. Gusarev", 
    "publish": "2001-10-08T11:34:34Z", 
    "summary": "The process of evolutionary emergence of purposeful adaptive behavior is\ninvestigated by means of computer simulations. The model proposed implies that\nthere is an evolving population of simple agents, which have two natural needs:\nenergy and reproduction. Any need is characterized quantitatively by a\ncorresponding motivation. Motivations determine goal-directed behavior of\nagents. The model demonstrates that purposeful behavior does emerge in the\nsimulated evolutionary processes. Emergence of purposefulness is accompanied by\norigin of a simple hierarchy in the control system of agents."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0201024v1", 
    "title": "Design of statistical quality control procedures using genetic   algorithms", 
    "arxiv-id": "cs/0201024v1", 
    "author": "Theophanes T. Hatjimihail", 
    "publish": "2002-01-27T21:01:45Z", 
    "summary": "In general, we can not use algebraic or enumerative methods to optimize a\nquality control (QC) procedure so as to detect the critical random and\nsystematic analytical errors with stated probabilities, while the probability\nfor false rejection is minimum. Genetic algorithms (GAs) offer an alternative,\nas they do not require knowledge of the objective function to be optimized and\nsearch through large parameter spaces quickly. To explore the application of\nGAs in statistical QC, we have developed an interactive GAs based computer\nprogram that designs a novel near optimal QC procedure, given an analytical\nprocess. The program uses the deterministic crowding algorithm. An illustrative\napplication of the program suggests that it has the potential to design QC\nprocedures that are significantly better than 45 alternative ones that are used\nin the clinical laboratories."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0210012v1", 
    "title": "Selection of future events from a time series in relation to estimations   of forecasting uncertainty", 
    "arxiv-id": "cs/0210012v1", 
    "author": "Igor B. Konovalov", 
    "publish": "2002-10-14T09:00:23Z", 
    "summary": "A new general procedure for a priori selection of more predictable events\nfrom a time series of observed variable is proposed. The procedure is\napplicable to time series which contains different types of events that feature\nsignificantly different predictability, or, in other words, to heteroskedastic\ntime series. A priori selection of future events in accordance to expected\nuncertainty of their forecasts may be helpful for making practical decisions.\nThe procedure first implies creation of two neural network based forecasting\nmodels, one of which is aimed at prediction of conditional mean and other -\nconditional dispersion, and then elaboration of the rule for future event\nselection into groups of more and less predictable events. The method is\ndemonstrated and tested by the example of the computer generated time series,\nand then applied to the real world time series, Dow Jones Industrial Average\nindex."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0212019v1", 
    "title": "Thinking, Learning, and Autonomous Problem Solving", 
    "arxiv-id": "cs/0212019v1", 
    "author": "Joerg D. Becker", 
    "publish": "2002-12-10T15:18:33Z", 
    "summary": "Ever increasing computational power will require methods for automatic\nprogramming. We present an alternative to genetic programming, based on a\ngeneral model of thinking and learning. The advantage is that evolution takes\nplace in the space of constructs and can thus exploit the mathematical\nstructures of this space. The model is formalized, and a macro language is\npresented which allows for a formal yet intuitive description of the problem\nunder consideration. A prototype has been developed to implement the scheme in\nPERL. This method will lead to a concentration on the analysis of problems, to\na more rapid prototyping, to the treatment of new problem classes, and to the\ninvestigation of philosophical problems. We see fields of application in\nnonlinear differential equations, pattern recognition, robotics, model\nbuilding, and animated pictures."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0302002v1", 
    "title": "Optimizing GoTools' Search Heuristics using Genetic Algorithms", 
    "arxiv-id": "cs/0302002v1", 
    "author": "Thomas Wolf", 
    "publish": "2003-02-02T03:30:52Z", 
    "summary": "GoTools is a program which solves life & death problems in the game of Go.\nThis paper describes experiments using a Genetic Algorithm to optimize\nheuristic weights used by GoTools' tree-search. The complete set of heuristic\nweights is composed of different subgroups, each of which can be optimized with\na suitable fitness function. As a useful side product, an MPI interface for\nFreePascal was implemented to allow the use of a parallelized fitness function\nrunning on a Beowulf cluster. The aim of this exercise is to optimize the\ncurrent version of GoTools, and to make tools available in preparation of an\nextension of GoTools for solving open boundary life & death problems, which\nwill introduce more heuristic parameters to be fine tuned."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijasuc.2010.1308", 
    "link": "http://arxiv.org/pdf/cs/0306125v1", 
    "title": "Predicting Response-Function Results of Electrical/Mechanical Systems   Through Artificial Neural Network", 
    "arxiv-id": "cs/0306125v1", 
    "author": "Sanjay Gupta", 
    "publish": "2003-06-24T06:28:12Z", 
    "summary": "In the present paper a newer application of Artificial Neural Network (ANN)\nhas been developed i.e., predicting response-function results of\nelectrical-mechanical system through ANN. This method is specially useful to\ncomplex systems for which it is not possible to find the response-function\nbecause of complexity of the system. The proposed approach suggests that how\neven without knowing the response-function, the response-function results can\nbe predicted with the use of ANN to the system. The steps used are: (i)\nDepending on the system, the ANN-architecture and the input & output parameters\nare decided, (ii) Training & test data are generated from simplified circuits\nand through tactic-superposition of it for complex circuits, (iii) Training the\nANN with training data through many cycles and (iv) Test-data are used for\npredicting the response-function results. It is found that the proposed novel\nmethod for response prediction works satisfactorily. Thus this method could be\nused specially for complex systems where other methods are unable to tackle it.\nIn this paper the application of ANN is particularly demonstrated to\nelectrical-circuit system but can be applied to other systems too."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10878-004-4835-9", 
    "link": "http://arxiv.org/pdf/cs/0309038v1", 
    "title": "A novel evolutionary formulation of the maximum independent set problem", 
    "arxiv-id": "cs/0309038v1", 
    "author": "L. C. D. Campos", 
    "publish": "2003-09-22T13:05:51Z", 
    "summary": "We introduce a novel evolutionary formulation of the problem of finding a\nmaximum independent set of a graph. The new formulation is based on the\nrelationship that exists between a graph's independence number and its acyclic\norientations. It views such orientations as individuals and evolves them with\nthe aid of evolutionary operators that are very heavily based on the structure\nof the graph and its acyclic orientations. The resulting heuristic has been\ntested on some of the Second DIMACS Implementation Challenge benchmark graphs,\nand has been found to be competitive when compared to several of the other\nheuristics that have also been tested on those graphs."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0309039v1", 
    "title": "Two novel evolutionary formulations of the graph coloring problem", 
    "arxiv-id": "cs/0309039v1", 
    "author": "J. O. do Nascimento", 
    "publish": "2003-09-23T00:53:20Z", 
    "summary": "We introduce two novel evolutionary formulations of the problem of coloring\nthe nodes of a graph. The first formulation is based on the relationship that\nexists between a graph's chromatic number and its acyclic orientations. It\nviews such orientations as individuals and evolves them with the aid of\nevolutionary operators that are very heavily based on the structure of the\ngraph and its acyclic orientations. The second formulation, unlike the first\none, does not tackle one graph at a time, but rather aims at evolving a\n`program' to color all graphs belonging to a class whose members all have the\nsame number of nodes and other common attributes. The heuristics that result\nfrom these formulations have been tested on some of the Second DIMACS\nImplementation Challenge benchmark graphs, and have been found to be\ncompetitive when compared to the several other heuristics that have also been\ntested on those graphs."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0310009v3", 
    "title": "On Interference of Signals and Generalization in Feedforward Neural   Networks", 
    "arxiv-id": "cs/0310009v3", 
    "author": "Artur Rataj", 
    "publish": "2003-10-06T15:40:44Z", 
    "summary": "This paper studies how the generalization ability of neurons can be affected\nby mutual processing of different signals. This study is done on the basis of a\nfeedforward artificial neural network. The mutual processing of signals can\npossibly be a good model of patterns in a set generalized by a neural network\nand in effect may improve generalization. In this paper it is discussed that\nthe interference may also cause a highly random generalization. Adaptive\nactivation functions are discussed as a way of reducing that type of\ngeneralization. A test of a feedforward neural network is performed that shows\nthe discussed random generalization."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0310050v4", 
    "title": "Feedforward Neural Networks with Diffused Nonlinear Weight Functions", 
    "arxiv-id": "cs/0310050v4", 
    "author": "Artur Rataj", 
    "publish": "2003-10-27T14:27:14Z", 
    "summary": "In this paper, feedforward neural networks are presented that have nonlinear\nweight functions based on look--up tables, that are specially smoothed in a\nregularization called the diffusion. The idea of such a type of networks is\nbased on the hypothesis that the greater number of adaptive parameters per a\nweight function might reduce the total number of the weight functions needed to\nsolve a given problem. Then, if the computational complexity of a propagation\nthrough a single such a weight function would be kept low, then the introduced\nneural networks might possibly be relatively fast.\n  A number of tests is performed, showing that the presented neural networks\nmay indeed perform better in some cases than the classic neural networks and a\nnumber of other learning machines."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0312047v1", 
    "title": "Mapping weblog communities", 
    "arxiv-id": "cs/0312047v1", 
    "author": "Fernando Tricas", 
    "publish": "2003-12-20T08:55:48Z", 
    "summary": "Websites of a particular class form increasingly complex networks, and new\ntools are needed to map and understand them. A way of visualizing this complex\nnetwork is by mapping it. A map highlights which members of the community have\nsimilar interests, and reveals the underlying social network. In this paper, we\nwill map a network of websites using Kohonen's self-organizing map (SOM), a\nneural-net like method generally used for clustering and visualization of\ncomplex data sets. The set of websites considered has been the Blogalia weblog\nhosting site (based at http://www.blogalia.com/), a thriving community of\naround 200 members, created in January 2002. In this paper we show how SOM\ndiscovers interesting community features, its relation with other\ncommunity-discovering algorithms, and the way it highlights the set of\ncommunities formed over the network."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0402047v1", 
    "title": "Parameter-less Optimization with the Extended Compact Genetic Algorithm   and Iterated Local Search", 
    "arxiv-id": "cs/0402047v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2004-02-19T19:37:32Z", 
    "summary": "This paper presents a parameter-less optimization framework that uses the\nextended compact genetic algorithm (ECGA) and iterated local search (ILS), but\nis not restricted to these algorithms. The presented optimization algorithm\n(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),\nwhere the parameters of a selecto-recombinative GA are eliminated. The approach\nthat we propose is tested on several well known problems. In the absence of\ndomain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use\noptimization method."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0402049v1", 
    "title": "An architecture for massive parallelization of the compact genetic   algorithm", 
    "arxiv-id": "cs/0402049v1", 
    "author": "Hugo Martires", 
    "publish": "2004-02-20T16:36:20Z", 
    "summary": "This paper presents an architecture which is suitable for a massive\nparallelization of the compact genetic algorithm. The resulting scheme has\nthree major advantages. First, it has low synchronization costs. Second, it is\nfault tolerant, and third, it is scalable.\n  The paper argues that the benefits that can be obtained with the proposed\napproach is potentially higher than those obtained with traditional parallel\ngenetic algorithms. In addition, the ideas suggested in the paper may also be\nrelevant towards parallelizing more complex probabilistic model building\ngenetic algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0402050v1", 
    "title": "A philosophical essay on life and its connections with genetic   algorithms", 
    "arxiv-id": "cs/0402050v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2004-02-20T16:52:11Z", 
    "summary": "This paper makes a number of connections between life and various facets of\ngenetic and evolutionary algorithms research. Specifically, it addresses the\ntopics of adaptation, multiobjective optimization, decision making, deception,\nand search operators, among others. It argues that human life, from birth to\ndeath, is an adaptive or dynamic optimization problem where people are\ncontinuously searching for happiness. More important, the paper speculates that\ngenetic algorithms can be used as a source of inspiration for helping people\nmake decisions in their everyday life."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0403003v1", 
    "title": "Genetic Algorithms and Quantum Computation", 
    "arxiv-id": "cs/0403003v1", 
    "author": "Ricardo N. Thess", 
    "publish": "2004-03-04T19:24:10Z", 
    "summary": "Recently, researchers have applied genetic algorithms (GAs) to address some\nproblems in quantum computation. Also, there has been some works in the\ndesigning of genetic algorithms based on quantum theoretical concepts and\ntechniques. The so called Quantum Evolutionary Programming has two major\nsub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic\nAlgorithms (QGAs). The former adopts qubit chromosomes as representations and\nemploys quantum gates for the search of the best solution. The later tries to\nsolve a key question in this field: what GAs will look like as an\nimplementation on quantum hardware? As we shall see, there is not a complete\nanswer for this question. An important point for QGAs is to build a quantum\nalgorithm that takes advantage of both the GA and quantum computing parallelism\nas well as true randomness provided by quantum computers. In the first part of\nthis paper we present a survey of the main works in GAs plus quantum computing\nincluding also our works in this area. Henceforth, we review some basic\nconcepts in quantum computation and GAs and emphasize their inherent\nparallelism. Next, we review the application of GAs for learning quantum\noperators and circuit design. Then, quantum evolutionary programming is\nconsidered. Finally, we present our current research in this field and some\nperspectives."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0405062v1", 
    "title": "Efficiency Enhancement of Probabilistic Model Building Genetic   Algorithms", 
    "arxiv-id": "cs/0405062v1", 
    "author": "Martin Pelikan", 
    "publish": "2004-05-18T16:19:59Z", 
    "summary": "This paper presents two different efficiency-enhancement techniques for\nprobabilistic model building genetic algorithms. The first technique proposes\nthe use of a mutation operator which performs local search in the sub-solution\nneighborhood identified through the probabilistic model. The second technique\nproposes building and using an internal probabilistic model of the fitness\nalong with the probabilistic model of variable interactions. The fitness values\nof some offspring are estimated using the probabilistic model, thereby avoiding\ncomputationally expensive function evaluations. The scalability of the\naforementioned techniques are analyzed using facetwise models for convergence\ntime and population sizing. The speed-up obtained by each of the methods is\npredicted and verified with empirical results. The results show that for\nadditively separable problems the competent mutation operator requires O(k 0.5\nlogm)--where k is the building-block size, and m is the number of building\nblocks--less function evaluations than its selectorecombinative counterpart.\nThe results also show that the use of an internal probabilistic fitness model\nreduces the required number of function evaluations to as low as 1-10% and\nyields a speed-up of 2--50."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0405063v1", 
    "title": "Let's Get Ready to Rumble: Crossover Versus Mutation Head to Head", 
    "arxiv-id": "cs/0405063v1", 
    "author": "David E. Goldberg", 
    "publish": "2004-05-18T16:31:56Z", 
    "summary": "This paper analyzes the relative advantages between crossover and mutation on\na class of deterministic and stochastic additively separable problems. This\nstudy assumes that the recombination and mutation operators have the knowledge\nof the building blocks (BBs) and effectively exchange or search among competing\nBBs. Facetwise models of convergence time and population sizing have been used\nto determine the scalability of each algorithm. The analysis shows that for\nadditively separable deterministic problems, the BB-wise mutation is more\nefficient than crossover, while the crossover outperforms the mutation on\nadditively separable problems perturbed with additive Gaussian noise. The\nresults show that the speed-up of using BB-wise mutation on deterministic\nproblems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs.\nLikewise, the speed-up of using crossover on stochastic problems with fixed\nnoise variance is O(mk^{0.5}log m)."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:JOCO.0000021937.26468.b2", 
    "link": "http://arxiv.org/pdf/cs/0405064v1", 
    "title": "Designing Competent Mutation Operators via Probabilistic Model Building   of Neighborhoods", 
    "arxiv-id": "cs/0405064v1", 
    "author": "David E. Goldberg", 
    "publish": "2004-05-18T16:41:34Z", 
    "summary": "This paper presents a competent selectomutative genetic algorithm (GA), that\nadapts linkage and solves hard problems quickly, reliably, and accurately. A\nprobabilistic model building process is used to automatically identify key\nbuilding blocks (BBs) of the search problem. The mutation operator uses the\nprobabilistic model of linkage groups to find the best among competing building\nblocks. The competent selectomutative GA successfully solves additively\nseparable problems of bounded difficulty, requiring only subquadratic number of\nfunction evaluations. The results show that for additively separable problems\nthe probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and\nrequires O(k^{0.5}logm) less function evaluations than its selectorecombinative\ncounterpart, confirming theoretical results reported elsewhere (Sastry &\nGoldberg, 2004)."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2004.1330930", 
    "link": "http://arxiv.org/pdf/cs/0405065v1", 
    "title": "Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise   Fitness Estimation", 
    "arxiv-id": "cs/0405065v1", 
    "author": "David E. Goldberg", 
    "publish": "2004-05-18T16:55:00Z", 
    "summary": "This paper studies fitness inheritance as an efficiency enhancement technique\nfor a class of competent genetic algorithms called estimation distribution\nalgorithms. Probabilistic models of important sub-solutions are developed to\nestimate the fitness of a proportion of individuals in the population, thereby\navoiding computationally expensive function evaluations. The effect of fitness\ninheritance on the convergence time and population sizing are modeled and the\nspeed-up obtained through inheritance is predicted. The results show that a\nfitness-inheritance mechanism which utilizes information on building-block\nfitnesses provides significant efficiency enhancement. For additively separable\nproblems, fitness inheritance reduces the number of function evaluations to\nabout half and yields a speed-up of about 1.75--2.25."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0501005v1", 
    "title": "Portfolio selection using neural networks", 
    "arxiv-id": "cs/0501005v1", 
    "author": "Sergio Gomez", 
    "publish": "2005-01-03T18:55:47Z", 
    "summary": "In this paper we apply a heuristic method based on artificial neural networks\nin order to trace out the efficient frontier associated to the portfolio\nselection problem. We consider a generalization of the standard Markowitz\nmean-variance model which includes cardinality and bounding constraints. These\nconstraints ensure the investment in a given number of different assets and\nlimit the amount of capital to be invested in each asset. We present some\nexperimental results obtained with the neural network heuristic and we compare\nthem to those obtained with three previous heuristic methods."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0503078v1", 
    "title": "Obtaining Membership Functions from a Neuron Fuzzy System extended by   Kohonen Network", 
    "arxiv-id": "cs/0503078v1", 
    "author": "Fernando D. Sasse", 
    "publish": "2005-03-29T19:40:14Z", 
    "summary": "This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network\n(NFN-MK), an hybrid computational model that combines fuzzy system technique\nand artificial neural networks. Its main task consists in the automatic\ngeneration of membership functions, in particular, triangle forms, aiming a\ndynamic modeling of a system. The model is tested by simulating real systems,\nhere represented by a nonlinear mathematical function. Comparison with the\nresults obtained by traditional neural networks, and correlated studies of\nneurofuzzy systems applied in system identification area, shows that the NFN-MK\nmodel has a similar performance, despite its greater simplicity."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0504053v1", 
    "title": "A Neural-Network Technique for Recognition of Filaments in Solar Images", 
    "arxiv-id": "cs/0504053v1", 
    "author": "V. Schetinin", 
    "publish": "2005-04-13T13:28:15Z", 
    "summary": "We describe a new neural-network technique developed for an automated\nrecognition of solar filaments visible in the hydrogen H-alpha line full disk\nspectroheliograms. This technique allows neural networks learn from a few image\nfragments labelled manually to recognize the single filaments depicted on a\nlocal background. The trained network is able to recognize filaments depicted\non the backgrounds with variations in brightness caused by atmospherics\ndistortions. Despite the difference in backgrounds in our experiments the\nneural network has properly recognized filaments in the testing image\nfragments. Using a parabolic activation function we extend this technique to\nrecognize multiple solar filaments which may appear in one fragment."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505003v1", 
    "title": "A New Kind of Hopfield Networks for Finding Global Optimum", 
    "arxiv-id": "cs/0505003v1", 
    "author": "Xiaofei Huang", 
    "publish": "2005-04-30T16:13:32Z", 
    "summary": "The Hopfield network has been applied to solve optimization problems over\ndecades. However, it still has many limitations in accomplishing this task.\nMost of them are inherited from the optimization algorithms it implements. The\ncomputation of a Hopfield network, defined by a set of difference equations,\ncan easily be trapped into one local optimum or another, sensitive to initial\nconditions, perturbations, and neuron update orders. It doesn't know how long\nit will take to converge, as well as if the final solution is a global optimum,\nor not. In this paper, we present a Hopfield network with a new set of\ndifference equations to fix those problems. The difference equations directly\nimplement a new powerful optimization algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505016v1", 
    "title": "Visual Character Recognition using Artificial Neural Networks", 
    "arxiv-id": "cs/0505016v1", 
    "author": "Shashank Araokar", 
    "publish": "2005-05-07T20:56:58Z", 
    "summary": "The recognition of optical characters is known to be one of the earliest\napplications of Artificial Neural Networks, which partially emulate human\nthinking in the domain of artificial intelligence. In this paper, a simplified\nneural approach to recognition of optical or visual characters is portrayed and\ndiscussed. The document is expected to serve as a resource for learners and\namateur investigators in pattern recognition, neural networking and related\ndisciplines."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505019v1", 
    "title": "Artificial Neural Networks and their Applications", 
    "arxiv-id": "cs/0505019v1", 
    "author": "Nitin Malik", 
    "publish": "2005-05-10T06:37:31Z", 
    "summary": "The Artificial Neural network is a functional imitation of simplified model\nof the biological neurons and their goal is to construct useful computers for\nreal world problems. The ANN applications have increased dramatically in the\nlast few years fired by both theoretical and practical applications in a wide\nvariety of applications. A brief theory of ANN is presented and potential areas\nare identified and future trends are discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505021v3", 
    "title": "Distant generalization by feedforward neural networks", 
    "arxiv-id": "cs/0505021v3", 
    "author": "Artur Rataj", 
    "publish": "2005-05-10T11:36:35Z", 
    "summary": "This paper discusses the notion of generalization of training samples over\nlong distances in the input space of a feedforward neural network. Such a\ngeneralization might occur in various ways, that differ in how great the\ncontribution of different training features should be.\n  The structure of a neuron in a feedforward neural network is analyzed and it\nis concluded, that the actual performance of the discussed generalization in\nsuch neural networks may be problematic -- while such neural networks might be\ncapable for such a distant generalization, a random and spurious generalization\nmay occur as well.\n  To illustrate the differences in generalizing of the same function by\ndifferent learning machines, results given by the support vector machines are\nalso presented."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505065v2", 
    "title": "A dissipative particle swarm optimization", 
    "arxiv-id": "cs/0505065v2", 
    "author": "Zhi-Lian Yang", 
    "publish": "2005-05-24T14:54:06Z", 
    "summary": "A dissipative particle swarm optimization is developed according to the\nself-organization of dissipative structure. The negative entropy is introduced\nto construct an opening dissipative system that is far-from-equilibrium so as\nto driving the irreversible evolution process with better fitness. The testing\nof two multimodal functions indicates it improves the performance effectively"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505067v1", 
    "title": "Optimizing semiconductor devices by self-organizing particle swarm", 
    "arxiv-id": "cs/0505067v1", 
    "author": "De-Chun Bi", 
    "publish": "2005-05-25T01:28:18Z", 
    "summary": "A self-organizing particle swarm is presented. It works in dissipative state\nby employing the small inertia weight, according to experimental analysis on a\nsimplified model, which with fast convergence. Then by recognizing and\nreplacing inactive particles according to the process deviation information of\ndevice parameters, the fluctuation is introduced so as to driving the\nirreversible evolution process with better fitness. The testing on benchmark\nfunctions and an application example for device optimization with designed\nfitness function indicates it improves the performance effectively."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505068v1", 
    "title": "Handling equality constraints by adaptive relaxing rule for swarm   algorithms", 
    "arxiv-id": "cs/0505068v1", 
    "author": "De-Chun Bi", 
    "publish": "2005-05-25T01:32:30Z", 
    "summary": "The adaptive constraints relaxing rule for swarm algorithms to handle with\nthe problems with equality constraints is presented. The feasible space of such\nproblems may be similiar to ridge function class, which is hard for applying\nswarm algorithms. To enter the solution space more easily, the relaxed quasi\nfeasible space is introduced and shrinked adaptively. The experimental results\non benchmark functions are compared with the performance of other algorithms,\nwhich show its efficiency."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505069v1", 
    "title": "Handling boundary constraints for numerical optimization by particle   swarm flying in periodic search space", 
    "arxiv-id": "cs/0505069v1", 
    "author": "De-Chun Bi", 
    "publish": "2005-05-25T01:36:07Z", 
    "summary": "The periodic mode is analyzed together with two conventional boundary\nhandling modes for particle swarm. By providing an infinite space that\ncomprises periodic copies of original search space, it avoids possible\ndisorganizing of particle swarm that is induced by the undesired mutations at\nthe boundary. The results on benchmark functions show that particle swarm with\nperiodic mode is capable of improving the search performance significantly, by\ncompared with that of conventional modes and other algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0505070v1", 
    "title": "SWAF: Swarm Algorithm Framework for Numerical Optimization", 
    "arxiv-id": "cs/0505070v1", 
    "author": "Wen-Jun Zhang", 
    "publish": "2005-05-25T01:39:55Z", 
    "summary": "A swarm algorithm framework (SWAF), realized by agent-based modeling, is\npresented to solve numerical optimization problems. Each agent is a bare bones\ncognitive architecture, which learns knowledge by appropriately deploying a set\nof simple rules in fast and frugal heuristics. Two essential categories of\nrules, the generate-and-test and the problem-formulation rules, are\nimplemented, and both of the macro rules by simple combination and subsymbolic\ndeploying of multiple rules among them are also studied. Experimental results\non benchmark problems are presented, and performance comparison between SWAF\nand other existing algorithms indicates that it is efficiently."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0506032v1", 
    "title": "Framework for Hopfield Network based Adaptive routing - A design level   approach for adaptive routing phenomena with Artificial Neural Network", 
    "arxiv-id": "cs/0506032v1", 
    "author": "R. Shankar", 
    "publish": "2005-06-10T05:30:41Z", 
    "summary": "Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse,\ndiscuss and arrive at an optimal solution for the technocrats over years.\nRouting is analysed based on many factors; few key constraints that decide the\nfactors are communication medium, time dependency, information source nature.\nParametric routing has become the requirement of the day, with some kind of\nadaptation to the underlying network environment. Satellite constellations,\nparticularly LEO satellite constellations have become a reality in operational\nto have a non-breaking voice/data communication around the world.Routing in\nthese constellations has to be treated in a non conventional way, taking their\nnetwork geometry into consideration. One of the efficient methods of\noptimization is putting Neural Networks to use. Few Artificial Neural Network\nmodels are very much suitable for the adaptive control mechanism, by their\nnature of network arrangement. One such efficient model is Hopfield Network\nmodel.\n  This paper is an attempt to design a framework for the Hopfield Network based\nadaptive routing phenomena in satellite constellations."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0511022v1", 
    "title": "Does a Plane Imitate a Bird? Does Computer Vision Have to Follow   Biological Paradigms?", 
    "arxiv-id": "cs/0511022v1", 
    "author": "Emanuel Diamant", 
    "publish": "2005-11-04T15:08:47Z", 
    "summary": "We posit a new paradigm for image information processing. For the last 25\nyears, this task was usually approached in the frame of Treisman's two-stage\nparadigm [1]. The latter supposes an unsupervised, bottom-up directed process\nof preliminary information pieces gathering at the lower processing stages and\na supervised, top-down directed process of information pieces binding and\ngrouping at the higher stages. It is acknowledged that these sub-processes\ninteract and intervene between them in a tricky and a complicated manner.\nNotwithstanding the prevalence of this paradigm in biological and computer\nvision, we nevertheless propose to replace it with a new one, which we would\nlike to designate as a two-part paradigm. In it, information contained in an\nimage is initially extracted in an independent top-down manner by one part of\nthe system, and then it is examined and interpreted by another, separate system\npart. We argue that the new paradigm seems to be more plausible than its\nforerunner. We provide evidence from human attention vision studies and\ninsights of Kolmogorov's complexity theory to support our arguments. We also\nprovide some reasons in favor of separate image interpretation issues."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0511027v1", 
    "title": "Discrete Network Dynamics. Part 1: Operator Theory", 
    "arxiv-id": "cs/0511027v1", 
    "author": "Stephen Luttrell", 
    "publish": "2005-11-07T19:09:01Z", 
    "summary": "An operator algebra implementation of Markov chain Monte Carlo algorithms for\nsimulating Markov random fields is proposed. It allows the dynamics of networks\nwhose nodes have discrete state spaces to be specified by the action of an\nupdate operator that is composed of creation and annihilation operators. This\nformulation of discrete network dynamics has properties that are similar to\nthose of a quantum field theory of bosons, which allows reuse of many\nconceptual and theoretical structures from QFT. The equilibrium behaviour of\none of these generalised MRFs and of the adaptive cluster expansion network\n(ACEnet) are shown to be equivalent, which provides a way of unifying these two\ntheories."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0512062v1", 
    "title": "Evolino for recurrent support vector machines", 
    "arxiv-id": "cs/0512062v1", 
    "author": "Faustino Gomez", 
    "publish": "2005-12-15T15:05:22Z", 
    "summary": "Traditional Support Vector Machines (SVMs) need pre-wired finite time windows\nto predict and classify time series. They do not have an internal state\nnecessary to deal with sequences involving arbitrary long-term dependencies.\nHere we introduce a new class of recurrent, truly sequential SVM-like devices\nwith internal adaptive states, trained by a novel method called EVOlution of\nsystems with KErnel-based outputs (Evoke), an instance of the recent Evolino\nclass of methods. Evoke evolves recurrent neural networks to detect and\nrepresent temporal dependencies while using quadratic programming/support\nvector regression to produce precise outputs. Evoke is the first SVM-based\nmechanism learning to classify a context-sensitive language. It also\noutperforms recent state-of-the-art gradient-based recurrent neural networks\n(RNNs) on various time series prediction tasks."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0602036v1", 
    "title": "R\u00e9seaux d'Automates de Caianiello Revisit\u00e9", 
    "arxiv-id": "cs/0602036v1", 
    "author": "Maurice Tchuente", 
    "publish": "2006-02-10T06:32:29Z", 
    "summary": "We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$\nwhich can be simulated by a neural networks of Caianiello of size $2n+2$ and\nmemory length $k$. This simulation allows us to find again one of the result of\nthe following article: [Cycles exponentiels des r\\'{e}seaux de Caianiello et\ncompteurs en arithm\\'{e}tique redondante, Technique et Science Informatiques\nVol. 19, pages 985-1008] on the existence of neural networks of Caianiello of\nsize $2n+2$ and memory length $k$ which describes a cycle of length $k \\times\n2^{nk}$."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0602051v1", 
    "title": "On the utility of the multimodal problem generator for assessing the   performance of Evolutionary Algorithms", 
    "arxiv-id": "cs/0602051v1", 
    "author": "Claudio F. Lima", 
    "publish": "2006-02-14T10:30:36Z", 
    "summary": "This paper looks in detail at how an evolutionary algorithm attempts to solve\ninstances from the multimodal problem generator. The paper shows that in order\nto consistently reach the global optimum, an evolutionary algorithm requires a\npopulation size that should grow at least linearly with the number of peaks. It\nis also shown a close relationship between the supply and decision making\nissues that have been identified previously in the context of population sizing\nmodels for additively decomposable problems.\n  The most important result of the paper, however, is that solving an instance\nof the multimodal problem generator is like solving a peak-in-a-haystack, and\nit is argued that evolutionary algorithms are not the best algorithms for such\na task. Finally, and as opposed to what several researchers have been doing, it\nis our strong belief that the multimodal problem generator is not adequate for\nassessing the performance of evolutionary algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0602055v1", 
    "title": "Revisiting Evolutionary Algorithms with On-the-Fly Population Size   Adjustment", 
    "arxiv-id": "cs/0602055v1", 
    "author": "Claudio F. Lima", 
    "publish": "2006-02-15T13:48:13Z", 
    "summary": "In an evolutionary algorithm, the population has a very important role as its\nsize has direct implications regarding solution quality, speed, and\nreliability. Theoretical studies have been done in the past to investigate the\nrole of population sizing in evolutionary algorithms. In addition to those\nstudies, several self-adjusting population sizing mechanisms have been proposed\nin the literature. This paper revisits the latter topic and pays special\nattention to the genetic algorithm with adaptive population size (APGA), for\nwhich several researchers have claimed to be very effective at autonomously\n(re)sizing the population.\n  As opposed to those previous claims, this paper suggests a complete opposite\nview. Specifically, it shows that APGA is not capable of adapting the\npopulation size at all. This claim is supported on theoretical grounds and\nconfirmed by computer simulations."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0603004v1", 
    "title": "Lamarckian Evolution and the Baldwin Effect in Evolutionary Neural   Networks", 
    "arxiv-id": "cs/0603004v1", 
    "author": "G. Romero", 
    "publish": "2006-03-01T12:26:09Z", 
    "summary": "Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or\nLamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect,\nthat is, the progressive incorporation of learned characteristics to the\ngenotypes, can be observed and leveraged to improve the search. The purpose of\nthis paper is to carry out an exper- imental study into how learning can\nimprove G-Prop genetic search. Two ways of combining learning and genetic\nsearch are explored: one exploits the Baldwin effect, while the other uses a\nLamarckian strategy. Our experiments show that using a Lamarckian op- erator\nmakes the algorithm find networks with a low error rate, and the smallest size,\nwhile using the Bald- win effect obtains MLPs with the smallest error rate, and\na larger size, taking longer to reach a solution. Both approaches obtain a\nlower average error than other BP-based algorithms like RPROP, other evolu-\ntionary methods and fuzzy logic based methods"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0603015v1", 
    "title": "The Basic Kak Neural Network with Complex Inputs", 
    "arxiv-id": "cs/0603015v1", 
    "author": "Pritam Rajagopal", 
    "publish": "2006-03-02T23:59:19Z", 
    "summary": "The Kak family of neural networks is able to learn patterns quickly, and this\nspeed of learning can be a decisive advantage over other competing models in\nmany applications. Amongst the implementations of these networks are those\nusing reconfigurable networks, FPGAs and optical networks. In some\napplications, it is useful to use complex data, and it is with that in mind\nthat this introduction to the basic Kak network with complex inputs is being\npresented. The training algorithm is prescriptive and the network weights are\nassigned simply upon examining the inputs. The input is mapped using quaternary\nencoding for purpose of efficienty. This network family is part of a larger\nhierarchy of learning schemes that include quantum models."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.cor.2005.06.017", 
    "link": "http://arxiv.org/pdf/cs/0603042v1", 
    "title": "The NoN Approach to Autonomic Face Recognition", 
    "arxiv-id": "cs/0603042v1", 
    "author": "Willie L. Scott II", 
    "publish": "2006-03-09T17:35:31Z", 
    "summary": "A method of autonomic face recognition based on the biologically plausible\nnetwork of networks (NoN) model of information processing is presented. The NoN\nmodel is based on locally parallel and globally coordinated transformations in\nwhich the neurons or computational units form distributed networks, which\nthemselves link to form larger networks. This models the structures in the\ncerebral cortex described by Mountcastle and the architecture based on that\nproposed for information processing by Sutton. In the proposed implementation,\nface images are processed by a nested family of locally operating networks\nalong with a hierarchically superior network that classifies the information\nfrom each of the local networks. The results of the experiments yielded a\nmaximum of 98.5% recognition accuracy and an average of 97.4% recognition\naccuracy on a benchmark database."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11063-005-3100-2", 
    "link": "http://arxiv.org/pdf/cs/0604001v1", 
    "title": "Theoretical Properties of Projection Based Multilayer Perceptrons with   Functional Inputs", 
    "arxiv-id": "cs/0604001v1", 
    "author": "Brieuc Conan-Guez", 
    "publish": "2006-04-01T20:16:39Z", 
    "summary": "Many real world data are sampled functions. As shown by Functional Data\nAnalysis (FDA) methods, spectra, time series, images, gesture recognition data,\netc. can be processed more efficiently if their functional nature is taken into\naccount during the data analysis process. This is done by extending standard\ndata analysis methods so that they can apply to functional inputs. A general\nway to achieve this goal is to compute projections of the functional data onto\na finite dimensional sub-space of the functional space. The coordinates of the\ndata on a basis of this sub-space provide standard vector representations of\nthe functions. The obtained vectors can be processed by any standard method. In\nour previous work, this general approach has been used to define projection\nbased Multilayer Perceptrons (MLPs) with functional inputs. We study in this\npaper important theoretical properties of the proposed model. We show in\nparticular that MLPs with functional inputs are universal approximators: they\ncan approximate to arbitrary accuracy any continuous mapping from a compact\nsub-space of a functional space to R. Moreover, we provide a consistency result\nthat shows that any mapping from a functional space to R can be learned thanks\nto examples by a projection based MLP: the generalization mean square error of\nthe MLP decreases to the smallest possible mean square error on the data when\nthe number of examples goes to infinity."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11063-005-3100-2", 
    "link": "http://arxiv.org/pdf/cs/0607019v1", 
    "title": "Modelling the Probability Density of Markov Sources", 
    "arxiv-id": "cs/0607019v1", 
    "author": "Stephen Luttrell", 
    "publish": "2006-07-06T18:49:20Z", 
    "summary": "This paper introduces an objective function that seeks to minimise the\naverage total number of bits required to encode the joint state of all of the\nlayers of a Markov source. This type of encoder may be applied to the problem\nof optimising the bottom-up (recognition model) and top-down (generative model)\nconnections in a multilayer neural network, and it unifies several previous\nresults on the optimisation of multilayer neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11063-005-3100-2", 
    "link": "http://arxiv.org/pdf/cs/0607090v1", 
    "title": "Neural Networks with Complex and Quaternion Inputs", 
    "arxiv-id": "cs/0607090v1", 
    "author": "Adityan Rishiyur", 
    "publish": "2006-07-18T21:01:43Z", 
    "summary": "This article investigates Kak neural networks, which can be instantaneously\ntrained, for complex and quaternion inputs. The performance of the basic\nalgorithm has been analyzed and shown how it provides a plausible model of\nhuman perception and understanding of images. The motivation for studying\nquaternion inputs is their use in representing spatial rotations that find\napplications in computer graphics, robotics, global navigation, computer vision\nand the spatial orientation of instruments. The problem of efficient mapping of\ndata in quaternion neural networks is examined. Some problems that need to be\naddressed before quaternion neural networks find applications are identified."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11063-005-3100-2", 
    "link": "http://arxiv.org/pdf/cs/0609125v1", 
    "title": "Problem Evolution: A new approach to problem solving systems", 
    "arxiv-id": "cs/0609125v1", 
    "author": "Uri Einziger-Lowicz", 
    "publish": "2006-09-22T12:40:07Z", 
    "summary": "In this paper we present a novel tool to evaluate problem solving systems.\nInstead of using a system to solve a problem, we suggest using the problem to\nevaluate the system. By finding a numerical representation of a problem's\ncomplexity, one can implement genetic algorithm to search for the most complex\nproblem the given system can solve. This allows a comparison between different\nsystems that solve the same set of problems. In this paper we implement this\napproach on pattern recognition neural networks to try and find the most\ncomplex pattern a given configuration can solve. The complexity of the pattern\nis calculated using linguistic complexity. The results demonstrate the power of\nthe problem evolution approach in ranking different neural network\nconfigurations according to their pattern recognition abilities. Future\nresearch and implementations of this technique are also discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11063-005-3100-2", 
    "link": "http://arxiv.org/pdf/cs/0610041v1", 
    "title": "A Computational Model of Spatial Memory Anticipation during Visual   Search", 
    "arxiv-id": "cs/0610041v1", 
    "author": "Nicolas Rougier", 
    "publish": "2006-10-09T11:42:27Z", 
    "summary": "Some visual search tasks require to memorize the location of stimuli that\nhave been previously scanned. Considerations about the eye movements raise the\nquestion of how we are able to maintain a coherent memory, despite the frequent\ndrastically changes in the perception. In this article, we present a\ncomputational model that is able to anticipate the consequences of the eye\nmovements on the visual perception in order to update a spatial memory"
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/cs/0611032v2", 
    "title": "V-like formations in flocks of artificial birds", 
    "arxiv-id": "cs/0611032v2", 
    "author": "Valmir C. Barbosa", 
    "publish": "2006-11-07T20:25:28Z", 
    "summary": "We consider flocks of artificial birds and study the emergence of V-like\nformations during flight. We introduce a small set of fully distributed\npositioning rules to guide the birds' movements and demonstrate, by means of\nsimulations, that they tend to lead to stabilization into several of the\nwell-known V-like formations that have been observed in nature. We also provide\nquantitative indicators that we believe are closely related to achieving V-like\nformations, and study their behavior over a large set of independent\nsimulations."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/cs/0702055v1", 
    "title": "On the possibility of making the complete computer model of a human   brain", 
    "arxiv-id": "cs/0702055v1", 
    "author": "A. V. Paraskevov", 
    "publish": "2007-02-09T13:16:14Z", 
    "summary": "The development of the algorithm of a neural network building by the\ncorresponding parts of a DNA code is discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0704.2725v2", 
    "title": "Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A   Case Study with the UCI Thyroid Disease Database", 
    "arxiv-id": "0704.2725v2", 
    "author": "Ivan Cantador", 
    "publish": "2007-04-20T15:58:04Z", 
    "summary": "The random initialization of weights of a multilayer perceptron makes it\npossible to model its training process as a Las Vegas algorithm, i.e. a\nrandomized algorithm which stops when some required training error is obtained,\nand whose execution time is a random variable. This modeling is used to perform\na case study on a well-known pattern recognition benchmark: the UCI Thyroid\nDisease Database. Empirical evidence is presented of the training time\nprobability distribution exhibiting a heavy tail behavior, meaning a big\nprobability mass of long executions. This fact is exploited to reduce the\ntraining time cost by applying two simple restart strategies. The first assumes\nfull knowledge of the distribution yielding a 40% cut down in expected time\nwith respect to the training without restarts. The second, assumes null\nknowledge, yielding a reduction ranging from 9% to 23%."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0704.3780v1", 
    "title": "Stochastic Optimization Algorithms", 
    "arxiv-id": "0704.3780v1", 
    "author": "Jean-Philippe Rennard", 
    "publish": "2007-04-28T06:52:19Z", 
    "summary": "When looking for a solution, deterministic methods have the enormous\nadvantage that they do find global optima. Unfortunately, they are very\nCPU-intensive, and are useless on untractable NP-hard problems that would\nrequire thousands of years for cutting-edge computers to explore. In order to\nget a result, one needs to revert to stochastic algorithms, that sample the\nsearch space without exploring it thoroughly. Such algorithms can find very\ngood results, without any guarantee that the global optimum has been reached;\nbut there is often no other choice than using them. This chapter is a short\nintroduction to the main methods used in stochastic optimization."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0705.0602v1", 
    "title": "Risk Assessment Algorithms Based On Recursive Neural Networks", 
    "arxiv-id": "0705.0602v1", 
    "author": "Michel Parent", 
    "publish": "2007-05-04T11:53:35Z", 
    "summary": "The assessment of highly-risky situations at road intersections have been\nrecently revealed as an important research topic within the context of the\nautomotive industry. In this paper we shall introduce a novel approach to\ncompute risk functions by using a combination of a highly non-linear processing\nmodel in conjunction with a powerful information encoding procedure.\nSpecifically, the elements of information either static or dynamic that appear\nin a road intersection scene are encoded by using directed positional acyclic\nlabeled graphs. The risk assessment problem is then reformulated in terms of an\ninductive learning task carried out by a recursive neural network. Recursive\nneural networks are connectionist models capable of solving supervised and\nnon-supervised learning problems represented by directed ordered acyclic\ngraphs. The potential of this novel approach is demonstrated through well\npredefined scenarios. The major difference of our approach compared to others\nis expressed by the fact of learning the structure of the risk. Furthermore,\nthe combination of a rich information encoding procedure with a generalized\nmodel of dynamical recurrent networks permit us, as we shall demonstrate, a\nsophisticated processing of information that we believe as being a first step\nfor building future advanced intersection safety systems"
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0705.1481v1", 
    "title": "Actin - Technical Report", 
    "arxiv-id": "0705.1481v1", 
    "author": "Raihan H. Kibria", 
    "publish": "2007-05-10T14:10:08Z", 
    "summary": "The Boolean satisfiability problem (SAT) can be solved efficiently with\nvariants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict\nanalysis dependent dynamic decision heuristics has proved to be particularly\nefficient, e.g. in Chaff. In this work, algorithms that initialize the variable\nactivity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved\nusing genetic programming (GP), with the goal to reduce the total number of\nconflicts of the search and the solving time. The effect of using initial\nactivities other than zero is examined by initializing with random numbers. The\npossibility of countering the detrimental effects of reordering the CNF with\nimproved initialization is investigated. The best result found (with validation\ntesting on further problems) was used in the solver Actin, which was submitted\nto SAT-Race 2006."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0706.1051v1", 
    "title": "Improved Neural Modeling of Real-World Systems Using Genetic Algorithm   Based Variable Selection", 
    "arxiv-id": "0706.1051v1", 
    "author": "David L. Elliott", 
    "publish": "2007-06-07T18:13:59Z", 
    "summary": "Neural network models of real-world systems, such as industrial processes,\nmade from sensor data must often rely on incomplete data. System states may not\nall be known, sensor data may be biased or noisy, and it is not often known\nwhich sensor data may be useful for predictive modelling. Genetic algorithms\nmay be used to help to address this problem by determining the near optimal\nsubset of sensor variables most appropriate to produce good models. This paper\ndescribes the use of genetic search to optimize variable selection to determine\ninputs into the neural network model. We discuss genetic algorithm\nimplementation issues including data representation types and genetic operators\nsuch as crossover and mutation. We present the use of this technique for neural\nnetwork modelling of a typical industrial application, a liquid fed ceramic\nmelter, and detail the results of the genetic search to optimize the neural\nnetwork model for this application."
},{
    "category": "cs.NE", 
    "doi": "10.1162/artl.2008.14.2.179", 
    "link": "http://arxiv.org/pdf/0707.0548v1", 
    "title": "From Royal Road to Epistatic Road for Variable Length Evolution   Algorithm", 
    "arxiv-id": "0707.0548v1", 
    "author": "Philippe Collard", 
    "publish": "2007-07-04T06:57:52Z", 
    "summary": "Although there are some real world applications where the use of variable\nlength representation (VLR) in Evolutionary Algorithm is natural and suitable,\nan academic framework is lacking for such representations. In this work we\npropose a family of tunable fitness landscapes based on VLR of genotypes. The\nfitness landscapes we propose possess a tunable degree of both neutrality and\nepistasis; they are inspired, on the one hand by the Royal Road fitness\nlandscapes, and the other hand by the NK fitness landscapes. So these\nlandscapes offer a scale of continuity from Royal Road functions, with\nneutrality and no epistasis, to landscapes with a large amount of epistasis and\nno redundancy. To gain insight into these fitness landscapes, we first use\nstandard tools such as adaptive walks and correlation length. Second, we\nevaluate the performances of evolutionary algorithms on these landscapes for\nvarious values of the neutral and the epistatic parameters; the results allow\nus to correlate the performances with the expected degrees of neutrality and\nepistasis."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299585", 
    "link": "http://arxiv.org/pdf/0707.0641v1", 
    "title": "Where are Bottlenecks in NK Fitness Landscapes?", 
    "arxiv-id": "0707.0641v1", 
    "author": "Manuel Clergue", 
    "publish": "2007-07-04T15:30:54Z", 
    "summary": "Usually the offspring-parent fitness correlation is used to visualize and\nanalyze some caracteristics of fitness landscapes such as evolvability. In this\npaper, we introduce a more general representation of this correlation, the\nFitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels\nin landscape that cause local search process to slow down. For a local search\nheuristic such as hill-climbing or simulated annealing, FC allows to visualize\nbottleneck and neutrality of landscapes. To confirm the relevance of the FC\nrepresentation we show where the bottlenecks are in the well-know NK fitness\nlandscape and also how to use neutrality information from the FC to combine\nsome neutral operator with local search heuristic."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2004.1330960", 
    "link": "http://arxiv.org/pdf/0707.0643v1", 
    "title": "Scuba Search : when selection meets innovation", 
    "arxiv-id": "0707.0643v1", 
    "author": "Manuel Clergue", 
    "publish": "2007-07-04T15:36:35Z", 
    "summary": "We proposed a new search heuristic using the scuba diving metaphor. This\napproach is based on the concept of evolvability and tends to exploit\nneutrality in fitness landscape. Despite the fact that natural evolution does\nnot directly select for evolvability, the basic idea behind the scuba search\nheuristic is to explicitly push the evolvability to increase. The search\nprocess switches between two phases: Conquest-of-the-Waters and\nInvasion-of-the-Land. A comparative study of the new algorithm and standard\nlocal search heuristics on the NKq-landscapes has shown advantage and limit of\nthe scuba search. To enlighten qualitative differences between neutral search\nprocesses, the space is changed into a connected graph to visualize the\npathways that the search is likely to follow."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2004.1330960", 
    "link": "http://arxiv.org/pdf/0707.0652v1", 
    "title": "How to use the Scuba Diving metaphor to solve problem with neutrality ?", 
    "arxiv-id": "0707.0652v1", 
    "author": "Manuel Clergue", 
    "publish": "2007-07-04T16:12:17Z", 
    "summary": "We proposed a new search heuristic using the scuba diving metaphor. This\napproach is based on the concept of evolvability and tends to exploit\nneutrality which exists in many real-world problems. Despite the fact that\nnatural evolution does not directly select for evolvability, the basic idea\nbehind the scuba search heuristic is to explicitly push evolvability to\nincrease. A comparative study of the scuba algorithm and standard local search\nheuristics has shown the advantage and the limitation of the scuba search. In\norder to tune neutrality, we use the NKq fitness landscapes and a family of\ntravelling salesman problems (TSP) where cities are randomly placed on a\nlattice and where travel distance between cities is computed with the Manhattan\nmetric. In this last problem the amount of neutrality varies with the city\nconcentration on the grid ; assuming the concentration below one, this TSP\nreasonably remains a NP-hard problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2004.1330960", 
    "link": "http://arxiv.org/pdf/0708.2686v1", 
    "title": "The universal evolutionary computer based on super-recursive algorithms   of evolvability", 
    "arxiv-id": "0708.2686v1", 
    "author": "D. Roglic", 
    "publish": "2007-07-24T18:50:22Z", 
    "summary": "This work exposes which mechanisms and procesess in the Nature of evolution\ncompute a function not computable by Turing machine. The computer with\nintelligence that is not higher than one bacteria population could have, but\nwith efficency to solve the problems that are non-computable by Turing machine\nis represented. This theoretical construction is called Universal Evolutinary\nComputer and it is based on the superecursive algorithms of evolvability."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2004.11.012", 
    "link": "http://arxiv.org/pdf/0709.3641v1", 
    "title": "Representation of Functional Data in Neural Networks", 
    "arxiv-id": "0709.3641v1", 
    "author": "Michel Verleysen", 
    "publish": "2007-09-23T14:10:08Z", 
    "summary": "Functional Data Analysis (FDA) is an extension of traditional data analysis\nto functional data, for example spectra, temporal series, spatio-temporal\nimages, gesture recognition data, etc. Functional data are rarely known in\npractice; usually a regular or irregular sampling is known. For this reason,\nsome processing is needed in order to benefit from the smooth character of\nfunctional data in the analysis methods. This paper shows how to extend the\nRadial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models\nto functional data inputs, in particular when the latter are known through\nlists of input-output pairs. Various possibilities for functional processing\nare discussed, including the projection on smooth bases, Functional Principal\nComponent Analysis, functional centering and reduction, and the use of\ndifferential operators. It is shown how to incorporate these functional\nprocessing into the RBFN and MLP models. The functional approach is illustrated\non a benchmark of spectrometric data analysis."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2004.07.001", 
    "link": "http://arxiv.org/pdf/0709.3642v1", 
    "title": "Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data   Analysis", 
    "arxiv-id": "0709.3642v1", 
    "author": "Brieuc Conan-Guez", 
    "publish": "2007-09-23T14:10:48Z", 
    "summary": "In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)\nto functional inputs. We show that fundamental results for classical MLP can be\nextended to functional MLP. We obtain universal approximation results that show\nthe expressive power of functional MLP is comparable to that of numerical MLP.\nWe obtain consistency results which imply that the estimation of optimal\nparameters for functional MLP is statistically well defined. We finally show on\nsimulated and real world data that the proposed model performs in a very\nsatisfactory way."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0710.0672v2", 
    "title": "Optimization of supply diversity for the self-assembly of simple objects   in two and three dimensions", 
    "arxiv-id": "0710.0672v2", 
    "author": "Valmir C. Barbosa", 
    "publish": "2007-10-03T18:29:12Z", 
    "summary": "The field of algorithmic self-assembly is concerned with the design and\nanalysis of self-assembly systems from a computational perspective, that is,\nfrom the perspective of mathematical problems whose study may give insight into\nthe natural processes through which elementary objects self-assemble into more\ncomplex ones. One of the main problems of algorithmic self-assembly is the\nminimum tile set problem (MTSP), which asks for a collection of types of\nelementary objects (called tiles) to be found for the self-assembly of an\nobject having a pre-established shape. Such a collection is to be as concise as\npossible, thus minimizing supply diversity, while satisfying a set of stringent\nconstraints having to do with the termination and other properties of the\nself-assembly process from its tile types. We present a study of what we think\nis the first practical approach to MTSP. Our study starts with the introduction\nof an evolutionary heuristic to tackle MTSP and includes results from extensive\nexperimentation with the heuristic on the self-assembly of simple objects in\ntwo and three dimensions. The heuristic we introduce combines classic elements\nfrom the field of evolutionary computation with a problem-specific variant of\nPareto dominance into a multi-objective approach to MTSP."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0710.4182v1", 
    "title": "Beyond Feedforward Models Trained by Backpropagation: a Practical   Training Tool for a More Efficient Universal Approximator", 
    "arxiv-id": "0710.4182v1", 
    "author": "Paul J. Werbos", 
    "publish": "2007-10-23T03:43:57Z", 
    "summary": "Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a\nfunction approximator more powerful than the MLP. This means that the\ncomplexity of MLP would be prohibitively large for some problems while SRN\ncould realize the desired mapping with acceptable computational constraints.\nThe speed of training of complex recurrent networks is crucial to their\nsuccessful application. Present work improves the previous results by training\nthe network with extended Kalman filter (EKF). We implemented a generic\nCellular SRN and applied it for solving two challenging problems: 2D maze\nnavigation and a subset of the connectedness problem. The speed of convergence\nhas been improved by several orders of magnitude in comparison with the earlier\nresults in the case of maze navigation, and superior generalization has been\ndemonstrated in the case of connectedness. The implications of this\nimprovements are discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0710.4725v1", 
    "title": "Fault-Trajectory Approach for Fault Diagnosis on Analog Circuits", 
    "arxiv-id": "0710.4725v1", 
    "author": "Antonio Carneiro De Mesquita Filho", 
    "publish": "2007-10-25T09:37:48Z", 
    "summary": "This issue discusses the fault-trajectory approach suitability for fault\ndiagnosis on analog networks. Recent works have shown promising results\nconcerning a method based on this concept for ATPG for diagnosing faults on\nanalog networks. Such method relies on evolutionary techniques, where a generic\nalgorithm (GA) is coded to generate a set of optimum frequencies capable to\ndisclose faults."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0711.2897v1", 
    "title": "Estimation of fuzzy anomalies in Water Distribution Systems", 
    "arxiv-id": "0711.2897v1", 
    "author": "F. J. Martinez", 
    "publish": "2007-11-19T11:24:47Z", 
    "summary": "State estimation is necessary in diagnosing anomalies in Water Demand Systems\n(WDS). In this paper we present a neural network performing such a task. State\nestimation is performed by using optimization, which tries to reconcile all the\navailable information. Quantification of the uncertainty of the input data\n(telemetry measures and demand predictions) can be achieved by means of robust\nestate estimation. Using a mathematical model of the network, fuzzy estimated\nstates for anomalous states of the network can be obtained. They are used to\ntrain a neural network capable of assessing WDS anomalies associated with\nparticular sets of measurements."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0712.3973v1", 
    "title": "GUIDE: Unifying Evolutionary Engines through a Graphical User Interface", 
    "arxiv-id": "0712.3973v1", 
    "author": "Marc Schoenauer", 
    "publish": "2007-12-24T07:31:58Z", 
    "summary": "Many kinds of Evolutionary Algorithms (EAs) have been described in the\nliterature since the last 30 years. However, though most of them share a common\nstructure, no existing software package allows the user to actually shift from\none model to another by simply changing a few parameters, e.g. in a single\nwindow of a Graphical User Interface. This paper presents GUIDE, a Graphical\nUser Interface for DREAM Experiments that, among other user-friendly features,\nunifies all kinds of EAs into a single panel, as far as evolution parameters\nare concerned. Such a window can be used either to ask for one of the well\nknown ready-to-use algorithms, or to very easily explore new combinations that\nhave not yet been studied. Another advantage of grouping all necessary elements\nto describe virtually all kinds of EAs is that it creates a fantastic pedagogic\ntool to teach EAs to students and newcomers to the field."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0712.4099v3", 
    "title": "Digital Ecosystems: Optimisation by a Distributed Intelligence", 
    "arxiv-id": "0712.4099v3", 
    "author": "P. De Wilde", 
    "publish": "2007-12-26T04:13:20Z", 
    "summary": "Can intelligence optimise Digital Ecosystems? How could a distributed\nintelligence interact with the ecosystem dynamics? Can the software components\nthat are part of genetic selection be intelligent in themselves, as in an\nadaptive technology? We consider the effect of a distributed intelligence\nmechanism on the evolutionary and ecological dynamics of our Digital Ecosystem,\nwhich is the digital counterpart of a biological ecosystem for evolving\nsoftware services in a distributed network. We investigate Neural Networks and\nSupport Vector Machine for the learning based pattern recognition functionality\nof our distributed intelligence. Simulation results imply that the Digital\nEcosystem performs better with the application of a distributed intelligence,\nmarginally more effectively when powered by Support Vector Machine than Neural\nNetworks, and suggest that it can contribute to optimising the operation of our\nDigital Ecosystem."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0712.4101v5", 
    "title": "Digital Ecosystems: Stability of Evolving Agent Populations", 
    "arxiv-id": "0712.4101v5", 
    "author": "G. Briscoe", 
    "publish": "2007-12-26T04:40:16Z", 
    "summary": "Stability is perhaps one of the most desirable features of any engineered\nsystem, given the importance of being able to predict its response to various\nenvironmental conditions prior to actual deployment. Engineered systems are\nbecoming ever more complex, approaching the same levels of biological\necosystems, and so their stability becomes ever more important, but taking on\nmore and more differential dynamics can make stability an ever more elusive\nproperty. The Chli-DeWilde definition of stability views a Multi-Agent System\nas a discrete time Markov chain with potentially unknown transition\nprobabilities. With a Multi-Agent System being considered stable when its\nstate, a stochastic process, has converged to an equilibrium distribution,\nbecause stability of a system can be understood intuitively as exhibiting\nbounded behaviour. We investigate an extension to include Multi-Agent Systems\nwith evolutionary dynamics, focusing on the evolving agent populations of our\nDigital Ecosystem. We then built upon this to construct an entropy-based\ndefinition for the degree of instability (entropy of the limit probabilities),\nwhich was later used to perform a stability analysis. The Digital Ecosystem is\nconsidered to investigate the stability of an evolving agent population through\nsimulations, for which the results were consistent with the original\nChli-DeWilde definition."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0712.4102v6", 
    "title": "Digital Ecosystems: Evolving Service-Oriented Architectures", 
    "arxiv-id": "0712.4102v6", 
    "author": "P. De Wilde", 
    "publish": "2007-12-26T05:44:31Z", 
    "summary": "We view Digital Ecosystems to be the digital counterparts of biological\necosystems, exploiting the self-organising properties of biological ecosystems,\nwhich are considered to be robust, self-organising and scalable architectures\nthat can automatically solve complex, dynamic problems. Digital Ecosystems are\na novel optimisation technique where the optimisation works at two levels: a\nfirst optimisation, migration of agents (representing services) which are\ndistributed in a decentralised peer-to-peer network, operating continuously in\ntime; this process feeds a second optimisation based on evolutionary computing\nthat operates locally on single peers and is aimed at finding solutions to\nsatisfy locally relevant constraints. We created an Ecosystem-Oriented\nArchitecture of Digital Ecosystems by extending Service-Oriented Architectures\nwith distributed evolutionary computing, allowing services to recombine and\nevolve over time, constantly seeking to improve their effectiveness for the\nuser base. Individuals within our Digital Ecosystem will be applications\n(groups of services), created in response to user requests by using\nevolutionary optimisation to aggregate the services. These individuals will\nmigrate through the Digital Ecosystem and adapt to find niches where they are\nuseful in fulfilling other user requests for applications. Simulation results\nimply that the Digital Ecosystem performs better at large scales than a\ncomparable Service-Oriented Architecture, suggesting that incorporating ideas\nfrom theoretical ecology can contribute to useful self-organising properties in\ndigital ecosystems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0712.4159v5", 
    "title": "Creating a Digital Ecosystem: Service-Oriented Architectures with   Distributed Evolutionary Computing", 
    "arxiv-id": "0712.4159v5", 
    "author": "G Briscoe", 
    "publish": "2007-12-26T23:32:10Z", 
    "summary": "We start with a discussion of the relevant literature, including Nature\nInspired Computing as a framework in which to understand this work, and the\nprocess of biomimicry to be used in mimicking the necessary biological\nprocesses to create Digital Ecosystems. We then consider the relevant\ntheoretical ecology in creating the digital counterpart of a biological\necosystem, including the topological structure of ecosystems, and evolutionary\nprocesses within distributed environments. This leads to a discussion of the\nrelevant fields from computer science for the creation of Digital Ecosystems,\nincluding evolutionary computing, Multi-Agent Systems, and Service-Oriented\nArchitectures. We then define Ecosystem-Oriented Architectures for the creation\nof Digital Ecosystems, imbibed with the properties of self-organisation and\nscalability from biological ecosystems, including a novel form of distributed\nevolutionary computing."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0802.0251v1", 
    "title": "Multi-Layer Perceptrons and Symbolic Data", 
    "arxiv-id": "0802.0251v1", 
    "author": "Brieuc Conan-Guez", 
    "publish": "2008-02-02T15:09:42Z", 
    "summary": "In some real world situations, linear models are not sufficient to represent\naccurately complex relations between input variables and output variables of a\nstudied system. Multilayer Perceptrons are one of the most successful\nnon-linear regression tool but they are unfortunately restricted to inputs and\noutputs that belong to a normed vector space. In this chapter, we propose a\ngeneral recoding method that allows to use symbolic data both as inputs and\noutputs to Multilayer Perceptrons. The recoding is quite simple to implement\nand yet provides a flexible framework that allows to deal with almost all\npractical cases. The proposed method is illustrated on a real world data set."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-010-9209-x", 
    "link": "http://arxiv.org/pdf/0802.0252v1", 
    "title": "Acc\u00e9l\u00e9ration des cartes auto-organisatrices sur tableau de   dissimilarit\u00e9s par s\u00e9paration et \u00e9valuation", 
    "arxiv-id": "0802.0252v1", 
    "author": "Fabrice Rossi", 
    "publish": "2008-02-02T15:10:35Z", 
    "summary": "In this paper, a new implementation of the adaptation of Kohonen\nself-organising maps (SOM) to dissimilarity matrices is proposed. This\nimplementation relies on the branch and bound principle to reduce the algorithm\nrunning time. An important property of this new approach is that the obtained\nalgorithm produces exactly the same results as the standard algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.chemolab.2007.09.004", 
    "link": "http://arxiv.org/pdf/0802.0287v1", 
    "title": "A data-driven functional projection approach for the selection of   feature ranges in spectra with ICA or cluster analysis", 
    "arxiv-id": "0802.0287v1", 
    "author": "Michel Verleysen", 
    "publish": "2008-02-03T19:02:49Z", 
    "summary": "Prediction problems from spectra are largely encountered in chemometry. In\naddition to accurate predictions, it is often needed to extract information\nabout which wavelengths in the spectra contribute in an effective way to the\nquality of the prediction. This implies to select wavelengths (or wavelength\nintervals), a problem associated to variable selection. In this paper, it is\nshown how this problem may be tackled in the specific case of smooth (for\nexample infrared) spectra. The functional character of the spectra (their\nsmoothness) is taken into account through a functional variable projection\nprocedure. Contrarily to standard approaches, the projection is performed on a\nbasis that is driven by the spectra themselves, in order to best fit their\ncharacteristics. The methodology is illustrated by two examples of functional\nprojection, using Independent Component Analysis and functional variable\nclustering, respectively. The performances on two standard infrared spectra\nbenchmarks are illustrated."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.chemolab.2007.09.004", 
    "link": "http://arxiv.org/pdf/0802.0861v1", 
    "title": "Using Bayesian Blocks to Partition Self-Organizing Maps", 
    "arxiv-id": "0802.0861v1", 
    "author": "Jeffrey D. Scargle", 
    "publish": "2008-02-06T18:50:16Z", 
    "summary": "Self organizing maps (SOMs) are widely-used for unsupervised classification.\nFor this application, they must be combined with some partitioning scheme that\ncan identify boundaries between distinct regions in the maps they produce. We\ndiscuss a novel partitioning scheme for SOMs based on the Bayesian Blocks\nsegmentation algorithm of Scargle [1998]. This algorithm minimizes a cost\nfunction to identify contiguous regions over which the values of the attributes\ncan be represented as approximately constant. Because this cost function is\nwell-defined and largely independent of assumptions regarding the number and\nstructure of clusters in the original sample space, this partitioning scheme\noffers significant advantages over many conventional methods. Sample code is\navailable."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0802.3875v1", 
    "title": "Are complex systems hard to evolve?", 
    "arxiv-id": "0802.3875v1", 
    "author": "Larry Bull", 
    "publish": "2008-02-26T19:07:53Z", 
    "summary": "Evolutionary complexity is here measured by the number of trials/evaluations\nneeded for evolving a logical gate in a non-linear medium. Behavioural\ncomplexity of the gates evolved is characterised in terms of cellular automata\nbehaviour. We speculate that hierarchies of behavioural and evolutionary\ncomplexities are isomorphic up to some degree, subject to substrate specificity\nof evolution and the spectrum of evolution parameters."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.1598v1", 
    "title": "A Multi-Agent Simulation of Retail Management Practices", 
    "arxiv-id": "0803.1598v1", 
    "author": "Christopher Clegg", 
    "publish": "2008-03-11T14:37:40Z", 
    "summary": "We apply Agent-Based Modeling and Simulation (ABMS) to investigate a set of\nproblems in a retail context. Specifically, we are working to understand the\nrelationship between human resource management practices and retail\nproductivity. Despite the fact we are working within a relatively novel and\ncomplex domain, it is clear that intelligent agents do offer potential for\ndeveloping organizational capabilities in the future. Our multi-disciplinary\nresearch team has worked with a UK department store to collect data and capture\nperceptions about operations from actors within departments. Based on this case\nstudy work, we have built a simulator that we present in this paper. We then\nuse the simulator to gather empirical evidence regarding two specific\nmanagement practices: empowerment and employee development."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.1600v1", 
    "title": "Understanding Retail Productivity by Simulating Management Practise", 
    "arxiv-id": "0803.1600v1", 
    "author": "Christopher Clegg", 
    "publish": "2008-03-11T14:46:10Z", 
    "summary": "Intelligent agents offer a new and exciting way of understanding the world of\nwork. In this paper we apply agent-based modeling and simulation to investigate\na set of problems in a retail context. Specifically, we are working to\nunderstand the relationship between human resource management practices and\nretail productivity. Despite the fact we are working within a relatively novel\nand complex domain, it is clear that intelligent agents could offer potential\nfor fostering sustainable organizational capabilities in the future. Our\nresearch so far has led us to conduct case study work with a top ten UK\nretailer, collecting data in four departments in two stores. Based on our case\nstudy data we have built and tested a first version of a department store\nsimulator. In this paper we will report on the current development of our\nsimulator which includes new features concerning more realistic data on the\npattern of footfall during the day and the week, a more differentiated view of\ncustomers, and the evolution of customers over time. This allows us to\ninvestigate more complex scenarios and to analyze the impact of various\nmanagement practices."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.2314v1", 
    "title": "Problem Solving and Complex Systems", 
    "arxiv-id": "0803.2314v1", 
    "author": "Yoann Pign\u00e9", 
    "publish": "2008-03-15T18:07:49Z", 
    "summary": "The observation and modeling of natural Complex Systems (CSs) like the human\nnervous system, the evolution or the weather, allows the definition of special\nabilities and models reusable to solve other problems. For instance, Genetic\nAlgorithms or Ant Colony Optimizations are inspired from natural CSs to solve\noptimization problems. This paper proposes the use of ant-based systems to\nsolve various problems with a non assessing approach. This means that solutions\nto some problem are not evaluated. They appear as resultant structures from the\nactivity of the system. Problems are modeled with graphs and such structures\nare observed directly on these graphs. Problems of Multiple Sequences Alignment\nand Natural Language Processing are addressed with this approach."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.2925v1", 
    "title": "Equivalence of Probabilistic Tournament and Polynomial Ranking Selection", 
    "arxiv-id": "0803.2925v1", 
    "author": "Marcus Hutter", 
    "publish": "2008-03-20T03:50:53Z", 
    "summary": "Crucial to an Evolutionary Algorithm's performance is its selection scheme.\nWe mathematically investigate the relation between polynomial rank and\nprobabilistic tournament methods which are (respectively) generalisations of\nthe popular linear ranking and tournament selection schemes. We show that every\nprobabilistic tournament is equivalent to a unique polynomial rank scheme. In\nfact, we derived explicit operators for translating between these two types of\nselection. Of particular importance is that most linear and most practical\nquadratic rank schemes are probabilistic tournaments."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.4240v1", 
    "title": "Neutral Fitness Landscape in the Cellular Automata Majority Problem", 
    "arxiv-id": "0803.4240v1", 
    "author": "Leonardo Vanneschi", 
    "publish": "2008-03-29T07:50:24Z", 
    "summary": "We study in detail the fitness landscape of a difficult cellular automata\ncomputational task: the majority problem. Our results show why this problem\nlandscape is so hard to search, and we quantify the large degree of neutrality\nfound in various ways. We show that a particular subspace of the solution\nspace, called the \"Olympus\", is where good solutions concentrate, and give\nmeasures to quantitatively characterize this subspace."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.4241v1", 
    "title": "Evolving Dynamic Change and Exchange of Genotype Encoding in Genetic   Algorithms for Difficult Optimization Problems", 
    "arxiv-id": "0803.4241v1", 
    "author": "S\u00e9bastien Verel", 
    "publish": "2008-03-29T07:51:18Z", 
    "summary": "The application of genetic algorithms (GAs) to many optimization problems in\norganizations often results in good performance and high quality solutions. For\nsuccessful and efficient use of GAs, it is not enough to simply apply simple\nGAs (SGAs). In addition, it is necessary to find a proper representation for\nthe problem and to develop appropriate search operators that fit well to the\nproperties of the genotype encoding. The representation must at least be able\nto encode all possible solutions of an optimization problem, and genetic\noperators such as crossover and mutation should be applicable to it. In this\npaper, serial alternation strategies between two codings are formulated in the\nframework of dynamic change of genotype encoding in GAs for function\noptimization. Likewise, a new variant of GAs for difficult optimization\nproblems denoted {\\it Split-and-Merge} GA (SM-GA) is developed using a parallel\nimplementation of an SGA and evolving a dynamic exchange of individual\nrepresentation in the context of Dual Coding concept. Numerical experiments\nshow that the evolved SM-GA significantly outperforms an SGA with static single\ncoding."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0803.4248v1", 
    "title": "From Cells to Islands: An unified Model of Cellular Parallel Genetic   Algorithms", 
    "arxiv-id": "0803.4248v1", 
    "author": "Manuel Clergue", 
    "publish": "2008-03-29T08:20:58Z", 
    "summary": "This paper presents the Anisotropic selection scheme for cellular Genetic\nAlgorithms (cGA). This new scheme allows to enhance diversity and to control\nthe selective pressure which are two important issues in Genetic Algorithms,\nespecially when trying to solve difficult optimization problems. Varying the\nanisotropic degree of selection allows swapping from a cellular to an island\nmodel of parallel genetic algorithm. Measures of performances and diversity\nhave been performed on one well-known problem: the Quadratic Assignment Problem\nwhich is known to be difficult to optimize. Experiences show that, tuning the\nanisotropic degree, we can find the accurate trade-off between cGA and island\nmodels to optimize performances of parallel evolutionary algorithms. This\ntrade-off can be interpreted as the suitable degree of migration among\nsubpopulations in a parallel Genetic Algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0804.1133v1", 
    "title": "Prospective Algorithms for Quantum Evolutionary Computation", 
    "arxiv-id": "0804.1133v1", 
    "author": "Donald A. Sofge", 
    "publish": "2008-04-07T20:11:24Z", 
    "summary": "This effort examines the intersection of the emerging field of quantum\ncomputing and the more established field of evolutionary computation. The goal\nis to understand what benefits quantum computing might offer to computational\nintelligence and how computational intelligence paradigms might be implemented\nas quantum programs to be run on a future quantum computer. We critically\nexamine proposed algorithms and methods for implementing computational\nintelligence paradigms, primarily focused on heuristic optimization methods\nincluding and related to evolutionary computation, with particular regard for\ntheir potential for eventual implementation on quantum computing hardware."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0804.4622v3", 
    "title": "Fast Density Codes for Image Data", 
    "arxiv-id": "0804.4622v3", 
    "author": "Pierre Courrieu", 
    "publish": "2008-04-29T14:25:30Z", 
    "summary": "Recently, a new method for encoding data sets in the form of \"Density Codes\"\nwas proposed in the literature (Courrieu, 2006). This method allows to compare\nsets of points belonging to every multidimensional space, and to build shape\nspaces invariant to a wide variety of affine and non-affine transformations.\nHowever, this general method does not take advantage of the special properties\nof image data, resulting in a quite slow encoding process that makes this tool\npractically unusable for processing large image databases with conventional\ncomputers. This paper proposes a very simple variant of the density code method\nthat directly works on the image function, which is thousands times faster than\nthe original Parzen window based method, without loss of its useful properties."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0804.4808v1", 
    "title": "Solving Time of Least Square Systems in Sigma-Pi Unit Networks", 
    "arxiv-id": "0804.4808v1", 
    "author": "Pierre Courrieu", 
    "publish": "2008-04-30T12:23:05Z", 
    "summary": "The solving of least square systems is a useful operation in\nneurocomputational modeling of learning, pattern matching, and pattern\nrecognition. In these last two cases, the solution must be obtained on-line,\nthus the time required to solve a system in a plausible neural architecture is\ncritical. This paper presents a recurrent network of Sigma-Pi neurons, whose\nsolving time increases at most like the logarithm of the system size, and of\nits condition number, which provides plausible computation times for biological\nsystems."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0804.4809v1", 
    "title": "Fast Computation of Moore-Penrose Inverse Matrices", 
    "arxiv-id": "0804.4809v1", 
    "author": "Pierre Courrieu", 
    "publish": "2008-04-30T12:24:54Z", 
    "summary": "Many neural learning algorithms require to solve large least square systems\nin order to obtain synaptic weights. Moore-Penrose inverse matrices allow for\nsolving such systems, even with rank deficiency, and they provide minimum-norm\nvectors of synaptic weights, which contribute to the regularization of the\ninput-output mapping. It is thus of interest to develop fast and accurate\nalgorithms for computing Moore-Penrose inverse matrices. In this paper, an\nalgorithm based on a full rank Cholesky factorization is proposed. The\nresulting pseudoinverse matrices are similar to those provided by other\nalgorithms. However the computation time is substantially shorter, particularly\nfor large systems."
},{
    "category": "cs.NE", 
    "doi": "10.1002/cplx.20269", 
    "link": "http://arxiv.org/pdf/0805.0231v4", 
    "title": "CMA-ES with Two-Point Step-Size Adaptation", 
    "arxiv-id": "0805.0231v4", 
    "author": "Nikolaus Hansen", 
    "publish": "2008-05-02T13:55:37Z", 
    "summary": "We combine a refined version of two-point step-size adaptation with the\ncovariance matrix adaptation evolution strategy (CMA-ES). Additionally, we\nsuggest polished formulae for the learning rate of the covariance matrix and\nthe recombination weights. In contrast to cumulative step-size adaptation or to\nthe 1/5-th success rule, the refined two-point adaptation (TPA) does not rely\non any internal model of optimality. In contrast to conventional\nself-adaptation, the TPA will achieve a better target step-size in particular\nwith large populations. The disadvantage of TPA is that it relies on two\nadditional objective function"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0805.0697v1", 
    "title": "Stochastic Optimization Approaches for Solving Sudoku", 
    "arxiv-id": "0805.0697v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2008-05-06T11:06:49Z", 
    "summary": "In this paper the Sudoku problem is solved using stochastic search techniques\nand these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm\nOptimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method\nthat combines Genetic Algorithm with Simulated Annealing (HGASA). The results\nobtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle\nwith CGA finding a solution in 28 seconds, while QSA finding a solution in 65\nseconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the\nparallel searching of GA with the flexibility of SA. The RPSO was found to be\nunable to solve the puzzle."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0805.4247v1", 
    "title": "Neural network learning of optimal Kalman prediction and control", 
    "arxiv-id": "0805.4247v1", 
    "author": "Ralph Linsker", 
    "publish": "2008-05-28T01:57:11Z", 
    "summary": "Although there are many neural network (NN) algorithms for prediction and for\ncontrol, and although methods for optimal estimation (including filtering and\nprediction) and for optimal control in linear systems were provided by Kalman\nin 1960 (with nonlinear extensions since then), there has been, to my\nknowledge, no NN algorithm that learns either Kalman prediction or Kalman\ncontrol (apart from the special case of stationary control). Here we show how\noptimal Kalman prediction and control (KPC), as well as system identification,\ncan be learned and executed by a recurrent neural network composed of\nlinear-response nodes, using as input only a stream of noisy measurement data.\n  The requirements of KPC appear to impose significant constraints on the\nallowed NN circuitry and signal flows. The NN architecture implied by these\nconstraints bears certain resemblances to the local-circuit architecture of\nmammalian cerebral cortex. We discuss these resemblances, as well as caveats\nthat limit our current ability to draw inferences for biological function. It\nhas been suggested that the local cortical circuit (LCC) architecture may\nperform core functions (as yet unknown) that underlie sensory, motor,and other\ncortical processing. It is reasonable to conjecture that such functions may\ninclude prediction, the estimation or inference of missing or noisy sensory\ndata, and the goal-driven generation of control signals. The resemblances found\nbetween the KPC NN architecture and that of the LCC are consistent with this\nconjecture."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0806.4650v1", 
    "title": "Structural Damage Detection Using Randomized Trained Neural Networks", 
    "arxiv-id": "0806.4650v1", 
    "author": "Agus Budiyono", 
    "publish": "2008-06-28T04:43:40Z", 
    "summary": "A computationally method on damage detection problems in structures was\nconducted using neural networks. The problem that is considered in this works\nconsists of estimating the existence, location and extent of stiffness\nreduction in structure which is indicated by the changes of the structural\nstatic parameters such as deflection and strain. The neural network was trained\nto recognize the behaviour of static parameter of the undamaged structure as\nwell as of the structure with various possible damage extent and location which\nwere modelled as random states. The proposed techniques were applied to detect\ndamage in a simply supported beam. The structure was analyzed using\nfinite-element-method (FEM) and the damage identification was conducted by a\nback-propagation neural network using the change of the structural strain and\ndisplacement. The results showed that using proposed method the strain is more\nefficient for identification of damage than the displacement."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0809.4622v1", 
    "title": "A computational approach to the covert and overt deployment of spatial   attention", 
    "arxiv-id": "0809.4622v1", 
    "author": "Fr\u00e9d\u00e9ric Alexandre", 
    "publish": "2008-09-26T13:12:36Z", 
    "summary": "Popular computational models of visual attention tend to neglect the\ninfluence of saccadic eye movements whereas it has been shown that the primates\nperform on average three of them per seconds and that the neural substrate for\nthe deployment of attention and the execution of an eye movement might\nconsiderably overlap. Here we propose a computational model in which the\ndeployment of attention with or without a subsequent eye movement emerges from\nlocal, distributed and numerical computations."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0809.5087v1", 
    "title": "Hybrid Neural Network Architecture for On-Line Learning", 
    "arxiv-id": "0809.5087v1", 
    "author": "Lei Wang", 
    "publish": "2008-09-29T23:00:22Z", 
    "summary": "Approaches to machine intelligence based on brain models have stressed the\nuse of neural networks for generalization. Here we propose the use of a hybrid\nneural network architecture that uses two kind of neural networks\nsimultaneously: (i) a surface learning agent that quickly adapt to new modes of\noperation; and, (ii) a deep learning agent that is very accurate within a\nspecific regime of operation. The two networks of the hybrid architecture\nperform complementary functions that improve the overall performance. The\nperformance of the hybrid architecture has been compared with that of\nback-propagation perceptrons and the CC and FC networks for chaotic time-series\nprediction, the CATS benchmark test, and smooth function approximation. It has\nbeen shown that the hybrid architecture provides a superior performance based\non the RMS error criterion."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0810.3356v1", 
    "title": "The Fundamental Problem with the Building Block Hypothesis", 
    "arxiv-id": "0810.3356v1", 
    "author": "Keki Burjorjee", 
    "publish": "2008-10-19T00:38:06Z", 
    "summary": "Skepticism of the building block hypothesis (BBH) has previously been\nexpressed on account of the weak theoretical foundations of this hypothesis and\nthe anomalies in the empirical record of the simple genetic algorithm. In this\npaper we hone in on a more fundamental cause for skepticism--the extraordinary\nstrength of some of the assumptions that undergird the BBH. Specifically, we\nfocus on assumptions made about the distribution of fitness over the genome\nset, and argue that these assumptions are unacceptably strong. As most of these\nassumptions have been embraced by the designers of so-called \"competent\"\ngenetic algorithms, our critique is relevant to an appraisal of such algorithms\nas well."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.eswa.2012.04.019", 
    "link": "http://arxiv.org/pdf/0810.3357v2", 
    "title": "Two Remarkable Computational Competencies of the Simple Genetic   Algorithm", 
    "arxiv-id": "0810.3357v2", 
    "author": "Keki M. Burjorjee", 
    "publish": "2008-10-19T01:08:00Z", 
    "summary": "Since the inception of genetic algorithmics the identification of\ncomputational efficiencies of the simple genetic algorithm (SGA) has been an\nimportant goal. In this paper we distinguish between a computational competency\nof the SGA--an efficient, but narrow computational ability--and a computational\nproficiency of the SGA--a computational ability that is both efficient and\nbroad. Till date, attempts to deduce a computational proficiency of the SGA\nhave been unsuccessful. It may, however, be possible to inductively infer a\ncomputational proficiency of the SGA from a set of related computational\ncompetencies that have been deduced. With this in mind we deduce two\ncomputational competencies of the SGA. These competencies, when considered\ntogether, point toward a remarkable computational proficiency of the SGA. This\nproficiency is pertinent to a general problem that is closely related to a\nwell-known statistical problem at the cutting edge of computational genetics."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0810.3484v1", 
    "title": "A Study of NK Landscapes' Basins and Local Optima Networks", 
    "arxiv-id": "0810.3484v1", 
    "author": "Christian Darabos", 
    "publish": "2008-10-20T08:06:40Z", 
    "summary": "We propose a network characterization of combinatorial fitness landscapes by\nadapting the notion of inherent networks proposed for energy surfaces (Doye,\n2002). We use the well-known family of $NK$ landscapes as an example. In our\ncase the inherent network is the graph where the vertices are all the local\nmaxima and edges mean basin adjacency between two maxima. We exhaustively\nextract such networks on representative small NK landscape instances, and show\nthat they are 'small-worlds'. However, the maxima graphs are not random, since\ntheir clustering coefficients are much larger than those of corresponding\nrandom graphs. Furthermore, the degree distributions are close to exponential\ninstead of Poissonian. We also describe the nature of the basins of attraction\nand their relationship with the local maxima network."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0810.3492v1", 
    "title": "The Connectivity of NK Landscapes' Basins: A Network Analysis", 
    "arxiv-id": "0810.3492v1", 
    "author": "Marco Tomassini", 
    "publish": "2008-10-20T08:36:22Z", 
    "summary": "We propose a network characterization of combinatorial fitness landscapes by\nadapting the notion of inherent networks proposed for energy surfaces. We use\nthe well-known family of NK landscapes as an example. In our case the inherent\nnetwork is the graph where the vertices represent the local maxima in the\nlandscape, and the edges account for the transition probabilities between their\ncorresponding basins of attraction. We exhaustively extracted such networks on\nrepresentative small NK landscape instances, and performed a statistical\ncharacterization of their properties. We found that most of these network\nproperties can be related to the search difficulty on the underlying NK\nlandscapes with varying values of K."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0812.0882v1", 
    "title": "Elagage d'un perceptron multicouches : utilisation de l'analyse de la   variance de la sensibilit\u00e9 des param\u00e8tres", 
    "arxiv-id": "0812.0882v1", 
    "author": "Andr\u00e9 Thomas", 
    "publish": "2008-12-04T09:12:14Z", 
    "summary": "The stucture determination of a neural network for the modelisation of a\nsystem remain the core of the problem. Within this framework, we propose a\npruning algorithm of the network based on the use of the analysis of the\nsensitivity of the variance of all the parameters of the network. This\nalgorithm will be tested on two examples of simulation and its performances\nwill be compared with three other algorithms of pruning of the literature"
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0812.1094v1", 
    "title": "S\u00e9lection de la structure d'un perceptron multicouches pour la   r\u00e9duction dun mod\u00e8le de simulation d'une scierie", 
    "arxiv-id": "0812.1094v1", 
    "author": "Andr\u00e9 Thomas", 
    "publish": "2008-12-05T08:49:53Z", 
    "summary": "Simulation is often used to evaluate the relevance of a Directing Program of\nProduction (PDP) or to evaluate its impact on detailed sc\\'enarii of\nscheduling. Within this framework, we propose to reduce the complexity of a\nmodel of simulation by exploiting a multilayer perceptron. A main phase of the\nmodeling of one system using a multilayer perceptron remains the determination\nof the structure of the network. We propose to compare and use various pruning\nalgorithms in order to determine the optimal structure of the network used to\nreduce the complexity of the model of simulation of our case of application: a\nsawmill."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0903.2516v1", 
    "title": "Effect of Degree Distribution on Evolutionary Search", 
    "arxiv-id": "0903.2516v1", 
    "author": "Susan Khor", 
    "publish": "2009-03-14T00:18:08Z", 
    "summary": "This paper introduces a method to generate hierarchically modular networks\nwith prescribed node degree list and proposes a metric to measure network\nmodularity based on the notion of edge distance. The generated networks are\nused as test problems to explore the effect of modularity and degree\ndistribution on evolutionary algorithm performance. Results from the\nexperiments (i) confirm a previous finding that modularity increases the\nperformance advantage of genetic algorithms over hill climbers, and (ii)\nsupport a new conjecture that test problems with modularized constraint\nnetworks having heavy-tailed right-skewed degree distributions are more easily\nsolved than test problems with modularized constraint networks having\nbell-shaped normal degree distributions."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0904.3063v1", 
    "title": "Using Dissortative Mating Genetic Algorithms to Track the Extrema of   Dynamic Deceptive Functions", 
    "arxiv-id": "0904.3063v1", 
    "author": "A. C. Rosa", 
    "publish": "2009-04-20T15:57:20Z", 
    "summary": "Traditional Genetic Algorithms (GAs) mating schemes select individuals for\ncrossover independently of their genotypic or phenotypic similarities. In\nNature, this behaviour is known as random mating. However, non-random schemes -\nin which individuals mate according to their kinship or likeness - are more\ncommon in natural systems. Previous studies indicate that, when applied to GAs,\nnegative assortative mating (a specific type of non-random mating, also known\nas dissortative mating) may improve their performance (on both speed and\nreliability) in a wide range of problems. Dissortative mating maintains the\ngenetic diversity at a higher level during the run, and that fact is frequently\nobserved as an explanation for dissortative GAs ability to escape local optima\ntraps. Dynamic problems, due to their specificities, demand special care when\ntuning a GA, because diversity plays an even more crucial role than it does\nwhen tackling static ones. This paper investigates the behaviour of\ndissortative mating GAs, namely the recently proposed Adaptive Dissortative\nMating GA (ADMGA), on dynamic trap functions. ADMGA selects parents according\nto their Hamming distance, via a self-adjustable threshold value. The method,\nby keeping population diversity during the run, provides an effective means to\ndeal with dynamic problems. Tests conducted with deceptive and nearly deceptive\ntrap functions indicate that ADMGA is able to outperform other GAs, some\nspecifically designed for tracking moving extrema, on a wide range of tests,\nbeing particularly effective when speed of change is not very fast. When\ncomparing the algorithm to a previously proposed dissortative GA, results show\nthat performance is equivalent on the majority of the experiments, but ADMGA\nperforms better when solving the hardest instances of the test set."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0904.3650v1", 
    "title": "The use of invariant moments in hand-written character recognition", 
    "arxiv-id": "0904.3650v1", 
    "author": "Ioan Snep", 
    "publish": "2009-04-23T10:44:21Z", 
    "summary": "The goal of this paper is to present the implementation of a Radial Basis\nFunction neural network with built-in knowledge to recognize hand-written\ncharacters. The neural network includes in its architecture gates controlled by\nan attraction/repulsion system of coefficients. These coefficients are derived\nfrom a preprocessing stage which groups the characters according to their\nascendant, central, or descendent components. The neural network is trained\nusing data from invariant moment functions. Results are compared with those\nobtained using a K nearest neighbor method on the same moment data."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0905.2649v1", 
    "title": "An Immune System Inspired Approach to Automated Program Verification", 
    "arxiv-id": "0905.2649v1", 
    "author": "Soumya Banerjee", 
    "publish": "2009-05-16T02:34:32Z", 
    "summary": "An immune system inspired Artificial Immune System (AIS) algorithm is\npresented, and is used for the purposes of automated program verification.\nRelevant immunological concepts are discussed and the field of AIS is briefly\nreviewed. It is proposed to use this AIS algorithm for a specific automated\nprogram verification task: that of predicting shape of program invariants. It\nis shown that the algorithm correctly predicts program invariant shape for a\nvariety of benchmarked programs."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0906.0798v1", 
    "title": "Single Neuron Memories and the Network's Proximity Matrix", 
    "arxiv-id": "0906.0798v1", 
    "author": "Subhash Kak", 
    "publish": "2009-06-03T23:10:25Z", 
    "summary": "This paper extends the treatment of single-neuron memories obtained by the\nB-matrix approach. The spreading of the activity within the network is\ndetermined by the network's proximity matrix which represents the separations\namongst the neurons through the neural pathways."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0906.1900v1", 
    "title": "How deals with discrete data for the reduction of simulation models   using neural network", 
    "arxiv-id": "0906.1900v1", 
    "author": "Andr\u00e9 Thomas", 
    "publish": "2009-06-10T09:56:29Z", 
    "summary": "Simulation is useful for the evaluation of a Master Production/distribution\nSchedule (MPS). Also, the goal of this paper is the study of the design of a\nsimulation model by reducing its complexity. According to theory of\nconstraints, we want to build reduced models composed exclusively by\nbottlenecks and a neural network. Particularly a multilayer perceptron, is\nused. The structure of the network is determined by using a pruning procedure.\nThis work focuses on the impact of discrete data on the results and compares\ndifferent approaches to deal with these data. This approach is applied to\nsawmill internal supply chain"
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0906.4846v1", 
    "title": "A genetic algorithm for structure-activity relationships: software   implementation", 
    "arxiv-id": "0906.4846v1", 
    "author": "Lorentz Jantschi", 
    "publish": "2009-06-26T06:25:07Z", 
    "summary": "The design and the implementation of a genetic algorithm are described. The\napplicability domain is on structure-activity relationships expressed as\nmultiple linear regressions and predictor variables are from families of\nstructure-based molecular descriptors. An experiment to compare different\nselection and survival strategies was designed and realized. The genetic\nalgorithm was run using the designed experiment on a set of 206 polychlorinated\nbiphenyls searching on structure-activity relationships having known the\nmeasured octanol-water partition coefficients and a family of molecular\ndescriptors. The experiment shows that different selection and survival\nstrategies create different partitions on the entire population of all possible\ngenotypes."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0907.0075v1", 
    "title": "XDANNG: XML based Distributed Artificial Neural Network with Globus   Toolkit", 
    "arxiv-id": "0907.0075v1", 
    "author": "Javad Ghofrani", 
    "publish": "2009-07-01T07:35:52Z", 
    "summary": "Artificial Neural Network is one of the most common AI application fields.\nThis field has direct and indirect usages most sciences. The main goal of ANN\nis to imitate biological neural networks for solving scientific problems. But\nthe level of parallelism is the main problem of ANN systems in comparison with\nbiological systems. To solve this problem, we have offered a XML-based\nframework for implementing ANN on the Globus Toolkit Platform. Globus Toolkit\nis well known management software for multipurpose Grids. Using the Grid for\nsimulating the neuron network will lead to a high degree of parallelism in the\nimplementation of ANN. We have used the XML for improving flexibility and\nscalability in our framework."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0907.0516v1", 
    "title": "Adaptation and Self-Organization in Evolutionary Algorithms", 
    "arxiv-id": "0907.0516v1", 
    "author": "James M Whitacre", 
    "publish": "2009-07-03T02:21:36Z", 
    "summary": "Abbreviated Abstract: The objective of Evolutionary Computation is to solve\npractical problems (e.g. optimization, data mining) by simulating the\nmechanisms of natural evolution. This thesis addresses several topics related\nto adaptation and self-organization in evolving systems with the overall aims\nof improving the performance of Evolutionary Algorithms (EA), understanding its\nrelation to natural evolution, and incorporating new mechanisms for mimicking\ncomplex biological systems."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0907.0611v1", 
    "title": "A process planning system with feature based neural network search   strategy for aluminum extrusion die manufacturing", 
    "arxiv-id": "0907.0611v1", 
    "author": "Serge Tichkiewitch", 
    "publish": "2009-07-03T12:08:13Z", 
    "summary": "Aluminum extrusion die manufacturing is a critical task for productive\nimprovement and increasing potential of competition in aluminum extrusion\nindustry. It causes to meet the efficiency not only consistent quality but also\ntime and production cost reduction. Die manufacturing consists first of die\ndesign and process planning in order to make a die for extruding the customer's\nrequirement products. The efficiency of die design and process planning are\nbased on the knowledge and experience of die design and die manufacturer\nexperts. This knowledge has been formulated into a computer system called the\nknowledge-based system. It can be reused to support a new die design and\nprocess planning. Such knowledge can be extracted directly from die geometry\nwhich is composed of die features. These features are stored in die feature\nlibrary to be prepared for producing a new die manufacturing. Die geometry is\ndefined according to the characteristics of the profile so we can reuse die\nfeatures from the previous similar profile design cases. This paper presents\nthe CaseXpert Process Planning System for die manufacturing based on feature\nbased neural network technique. Die manufacturing cases in the case library\nwould be retrieved with searching and learning method by neural network for\nreusing or revising it to build a die design and process planning when a new\ncase is similar with the previous die manufacturing cases. The results of the\nsystem are dies design and machining process. The system has been successfully\ntested, it has been proved that the system can reduce planning time and respond\nhigh consistent plans."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0907.4426v1", 
    "title": "Evolution of Digital Logic Functionality via a Genetic Algorithm", 
    "arxiv-id": "0907.4426v1", 
    "author": "Wilson Julien", 
    "publish": "2009-07-25T15:21:16Z", 
    "summary": "Digital logic forms the functional basics of most modern electronic equipment\nand as such the creation of novel digital logic circuits is an active area of\ncomputer engineering research. This study demonstrates that genetic algorithms\ncan be used to evolve functionally useful sets of logic gate interconnections\nto create useful digital logic circuits. The efficacy of this approach is\nillustrated via the evolution of AND, OR, XOR, NOR, and XNOR functionality from\nsets of NAND gates, thereby illustrating that evolutionary methods have the\npotential be applied to the design of digital electronics."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0908.0516v1", 
    "title": "Still doing evolutionary algorithms with Perl", 
    "arxiv-id": "0908.0516v1", 
    "author": "Juan J. Merelo Guervos", 
    "publish": "2009-08-04T19:16:14Z", 
    "summary": "Algorithm::Evolutionary (A::E from now on) was introduced in 2002, after a\ntalk in YAPC::EU in Munich. 7 years later, A::E is in its 0.67 version (past\nits \"number of the beast\" 0.666), and has been used extensively, to the point\nof being the foundation of much of the (computer) science being done by our\nresearch group (and, admittedly, not many others). All is not done, however;\nnow A::E is being integrated with POE so that evolutionary algorithms (EAs) can\nbe combined with all kinds of servers and used in client, servers, and anything\nin between. In this companion to the talk I will explain what evolutionary\nalgorithms are, what they are being used for, how to do them with Perl (using\nthese or other fine modules found in CPAN) and what evolutionary algorithms can\ndo for Perl at large."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0908.1453v1", 
    "title": "Training Process Reduction Based On Potential Weights Linear Analysis To   Accelarate Back Propagation Network", 
    "arxiv-id": "0908.1453v1", 
    "author": "Nasir Sulaiman", 
    "publish": "2009-08-11T05:30:01Z", 
    "summary": "Learning is the important property of Back Propagation Network (BPN) and\nfinding the suitable weights and thresholds during training in order to improve\ntraining time as well as achieve high accuracy. Currently, data pre-processing\nsuch as dimension reduction input values and pre-training are the contributing\nfactors in developing efficient techniques for reducing training time with high\naccuracy and initialization of the weights is the important issue which is\nrandom and creates paradox, and leads to low accuracy with high training time.\nOne good data preprocessing technique for accelerating BPN classification is\ndimension reduction technique but it has problem of missing data. In this\npaper, we study current pre-training techniques and new preprocessing technique\ncalled Potential Weight Linear Analysis (PWLA) which combines normalization,\ndimension reduction input values and pre-training. In PWLA, the first data\npreprocessing is performed for generating normalized input values and then\napplying them by pre-training technique in order to obtain the potential\nweights. After these phases, dimension of input values matrix will be reduced\nby using real potential weights. For experiment results XOR problem and three\ndatasets, which are SPECT Heart, SPECTF Heart and Liver disorders (BUPA) will\nbe evaluated. Our results, however, will show that the new technique of PWLA\nwill change BPN to new Supervised Multi Layer Feed Forward Neural Network\n(SMFFNN) model with high accuracy in one epoch without training cycle. Also\nPWLA will be able to have power of non linear supervised and unsupervised\ndimension reduction property for applying by other supervised multi layer feed\nforward neural network model in future work."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0908.1597v1", 
    "title": "A quantum diffusion network", 
    "arxiv-id": "0908.1597v1", 
    "author": "George Kesidis", 
    "publish": "2009-08-11T23:45:08Z", 
    "summary": "Wong's diffusion network is a stochastic, zero-input Hopfield network with a\nGibbs stationary distribution over a bounded, connected continuum. Previously,\nlogarithmic thermal annealing was demonstrated for the diffusion network and\ndigital versions of it were studied and applied to imaging. Recently, \"quantum\"\nannealed Markov chains have garnered significant attention because of their\nimproved performance over \"pure\" thermal annealing. In this note, a joint\nquantum and thermal version of Wong's diffusion network is described and its\nconvergence properties are studied. Different choices for \"auxiliary\" functions\nare discussed, including those of the kinetic type previously associated with\nquantum annealing."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0908.3025v1", 
    "title": "Techniques for Highly Multiobjective Optimisation: Some Nondominated   Points are Better than Others", 
    "arxiv-id": "0908.3025v1", 
    "author": "Joshua Knowles", 
    "publish": "2009-08-20T21:44:32Z", 
    "summary": "The research area of evolutionary multiobjective optimization (EMO) is\nreaching better understandings of the properties and capabilities of EMO\nalgorithms, and accumulating much evidence of their worth in practical\nscenarios. An urgent emerging issue is that the favoured EMO algorithms scale\npoorly when problems have many (e.g. five or more) objectives. One of the chief\nreasons for this is believed to be that, in many-objective EMO search,\npopulations are likely to be largely composed of nondominated solutions. In\nturn, this means that the commonly-used algorithms cannot distinguish between\nthese for selective purposes. However, there are methods that can be used\nvalidly to rank points in a nondominated set, and may therefore usefully\nunderpin selection in EMO search. Here we discuss and compare several such\nmethods. Our main finding is that simple variants of the often-overlooked\nAverage Ranking strategy usually outperform other methods tested, covering\nproblems with 5-20 objectives and differing amounts of inter-objective\ncorrelation."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0908.3184v2", 
    "title": "Location of Single Neuron Memories in a Hebbian Network", 
    "arxiv-id": "0908.3184v2", 
    "author": "Krishna Chaithanya Lingashetty", 
    "publish": "2009-08-21T19:53:54Z", 
    "summary": "This paper reports the results of an experiment on the use of Kak's B-Matrix\napproach to spreading activity in a Hebbian neural network. Specifically, it\nconcentrates on the memory retrieval from single neurons and compares the\nperformance of the B-Matrix approach to that of the traditional approach."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0909.3384v1", 
    "title": "Comparing Single and Multiobjective Evolutionary Approaches to the   Inventory and Transportation Problem", 
    "arxiv-id": "0909.3384v1", 
    "author": "Ken Sharman", 
    "publish": "2009-09-18T08:35:42Z", 
    "summary": "EVITA, standing for Evolutionary Inventory and Transportation Algorithm, is a\ntwo-level methodology designed to address the Inventory and Transportation\nProblem (ITP) in retail chains. The top level uses an evolutionary algorithm to\nobtain delivery patterns for each shop on a weekly basis so as to minimise the\ninventory costs, while the bottom level solves the Vehicle Routing Problem\n(VRP) for every day in order to obtain the minimum transport costs associated\nto a particular set of patterns. The aim of this paper is to investigate\nwhether a multiobjective approach to this problem can yield any advantage over\nthe previously used single objective approach. The analysis performed allows us\nto conclude that this is not the case and that the single objective approach is\nin gene- ral preferable for the ITP in the case studied. A further conclusion\nis that it is useful to employ a classical algorithm such as Clarke & Wright's\nas the seed for other metaheuristics like local search or tabu search in order\nto provide good results for the Vehicle Routing Problem."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0910.0646v1", 
    "title": "Digital Business Ecosystems: Natural Science Paradigms", 
    "arxiv-id": "0910.0646v1", 
    "author": "Suzanne Sadedin", 
    "publish": "2009-10-04T22:09:18Z", 
    "summary": "A primary motivation for research in Digital Ecosystems is the desire to\nexploit the self-organising properties of natural ecosystems. Ecosystems arc\nthought to be robust, scalable architectures that can automatically solve\ncomplex, dynamic problems. However, the biological processes that contribute to\nthese properties have not been made explicit in Digital Ecosystem research.\nHere, we introduce how biological properties contribute to the self-organising\nfeatures of natural ecosystems. These properties include populations of\nevolving agents, a complex dynamic environment, and spatial distributions which\ngenerate local interactions. The potential for exploiting these properties in\nartificial systems is then considered."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0912.0936v1", 
    "title": "Neural-estimator for the surface emission rate of atmospheric gases", 
    "arxiv-id": "0912.0936v1", 
    "author": "H. F. Campos Velho", 
    "publish": "2009-12-04T21:33:10Z", 
    "summary": "The emission rate of minority atmospheric gases is inferred by a new approach\nbased on neural networks. The neural network applied is the multi-layer\nperceptron with backpropagation algorithm for learning. The identification of\nthese surface fluxes is an inverse problem. A comparison between the new\nneural-inversion and regularized inverse solution id performed. The results\nobtained from the neural networks are significantly better. In addition, the\ninversion with the neural netwroks is fster than regularized approaches, after\ntraining."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0912.2310v1", 
    "title": "NeuralNetwork Based 3D Surface Reconstruction", 
    "arxiv-id": "0912.2310v1", 
    "author": "Shalini Bhatia", 
    "publish": "2009-12-11T18:40:06Z", 
    "summary": "This paper proposes a novel neural-network-based adaptive hybrid-reflectance\nthree-dimensional (3-D) surface reconstruction model. The neural network\ncombines the diffuse and specular components into a hybrid model. The proposed\nmodel considers the characteristics of each point and the variant albedo to\nprevent the reconstructed surface from being distorted. The neural network\ninputs are the pixel values of the two-dimensional images to be reconstructed.\nThe normal vectors of the surface can then be obtained from the output of the\nneural network after supervised learning, where the illuminant direction does\nnot have to be known in advance. Finally, the obtained normal vectors can be\napplied to integration method when reconstructing 3-D objects. Facial images\nwere used for training in the proposed approach"
},{
    "category": "cs.NE", 
    "doi": "10.1145/1389095.1389204", 
    "link": "http://arxiv.org/pdf/0912.3960v2", 
    "title": "Optimal Design of Fuzzy Based Power System Stabilizer Self Tuned by   Robust Search Algorithm", 
    "arxiv-id": "0912.3960v2", 
    "author": "N. Kesavan Nair", 
    "publish": "2009-12-20T02:21:40Z", 
    "summary": "In the interconnected power system network, instability problems are caused\nmainly by the low frequency oscillations of 0.2 to 2.5 Hz. The supplementary\ncontrol signal in addition with AVR and high gain excitation systems are\nprovided by means of Power System Stabilizer (PSS). Conventional power system\nstabilizers provide effective damping only on a particular operating point. But\nfuzzy based PSS provides good damping for a wide range of operating points. The\nbottlenecks faced in designing a fuzzy logic controller can be minimized by\nusing appropriate optimization techniques like Genetic Algorithm, Particle Swam\nOptimization, Ant Colony Optimization etc.In this paper the membership\nfunctions of FLC are optimized by the new breed optimization technique called\nGenetic Algorithm. This design methodology is implemented on a Single Machine\nInfinite Bus (SMIB) system. Simulation results on SMIB show the effectiveness\nand robustness of the proposed PSS over a wide range of operating conditions\nand system configurations."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1001.2097v1", 
    "title": "Predictability of PV power grid performance on insular sites without   weather stations: use of artificial neural networks", 
    "arxiv-id": "1001.2097v1", 
    "author": "P. Haurant", 
    "publish": "2010-01-13T08:11:12Z", 
    "summary": "The official meteorological network is poor on the island of Corsica: only\nthree sites being about 50 km apart are equipped with pyranometers which enable\nmeasurements by hourly and daily step. These sites are Ajaccio (41\\degree 55'N\nand 8\\degree 48'E, seaside), Bastia (42\\degree 33'N, 9\\degree 29'E, seaside)\nand Corte (42\\degree 30'N, 9\\degree 15'E average altitude of 486 meters). This\nlack of weather station makes difficult the predictability of PV power grid\nperformance. This work intends to study a methodology which can predict global\nsolar irradiation using data available from another location for daily and\nhourly horizon. In order to achieve this prediction, we have used Artificial\nNeural Network which is a popular artificial intelligence technique in the\nforecasting domain. A simulator has been obtained using data available for the\nstation of Ajaccio that is the only station for which we have a lot of data: 16\nyears from 1972 to 1987. Then we have tested the efficiency of this simulator\nin two places with different geographical features: Corte, a mountainous region\nand Bastia, a coastal region. On daily horizon, the relocation has implied\nfewer errors than a \"na\\\"ive\" prediction method based on the persistence\n(RMSE=1468 Vs 1383Wh/m^2 to Bastia and 1325 Vs 1213Wh/m^2 to Corte). On hourly\ncase, the results were still satisfactory, and widely better than persistence\n(RMSE=138.8 Vs 109.3 Wh/m^2 to Bastia and 135.1 Vs 114.7 Wh/m^2 to Corte). The\nlast experiment was to evaluate the accuracy of our simulator on a PV power\ngrid localized at 10 km from the station of Ajaccio. We got errors very\nsuitable (nRMSE=27.9%, RMSE=99.0 W.h) compared to those obtained with the\npersistence (nRMSE=42.2%, RMSE=149.7 W.h)."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1001.3491v1", 
    "title": "Particle Swarm Optimization Based Reactive Power Optimization", 
    "arxiv-id": "1001.3491v1", 
    "author": "M. Mary Linda", 
    "publish": "2010-01-20T07:57:02Z", 
    "summary": "Reactive power plays an important role in supporting the real power transfer\nby maintaining voltage stability and system reliability. It is a critical\nelement for a transmission operator to ensure the reliability of an electric\nsystem while minimizing the cost associated with it. The traditional objectives\nof reactive power dispatch are focused on the technical side of reactive\nsupport such as minimization of transmission losses. Reactive power cost\ncompensation to a generator is based on the incurred cost of its reactive power\ncontribution less the cost of its obligation to support the active power\ndelivery. In this paper an efficient Particle Swarm Optimization (PSO) based\nreactive power optimization approach is presented. The optimal reactive power\ndispatch problem is a nonlinear optimization problem with several constraints.\nThe objective of the proposed PSO is to minimize the total support cost from\ngenerators and reactive compensators. It is achieved by maintaining the whole\nsystem power loss as minimum thereby reducing cost allocation. The purpose of\nreactive power dispatch is to determine the proper amount and location of\nreactive support. Reactive Optimal Power Flow (ROPF) formulation is developed\nas an analysis tool and the validity of proposed method is examined using an\nIEEE-14 bus system."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1001.3741v1", 
    "title": "Application of Artificial Neural Networks in Aircraft Maintenance,   Repair and Overhaul Solutions", 
    "arxiv-id": "1001.3741v1", 
    "author": "T. R. Gopalakrishnan Nair", 
    "publish": "2010-01-21T08:36:23Z", 
    "summary": "This paper reviews application of Artificial Neural Networks in Aircraft\nMaintenance, Repair and Overhaul (MRO). MRO solutions are designed to\nfacilitate the authoring and delivery of maintenance and repair information to\nthe line maintenance technicians who need to improve aircraft repair turn\naround time, optimize the efficiency and consistency of fleet maintenance and\nensure regulatory compliance. The technical complexity of aircraft systems,\nespecially in avionics, has increased to the point at which it poses a\nsignificant troubleshotting and repair challenge for MRO personnel. As per the\nexisting scenario, the MRO systems in place are inefficient. In this paper, we\npropose the centralization and integration of the MRO database to increase its\nefficiency. Moreover the implementation of Artificial Neural Networks in this\nsystem can rid the system of many of its deficiencies. In order to make the\nsystem more efficient we propose to integrate all the modules so as to reduce\nthe efficacy of repair."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.1164v1", 
    "title": "Existence and Global Logarithmic Stability of Impulsive Neural Networks   with Time Delay", 
    "arxiv-id": "1002.1164v1", 
    "author": "C. Mallick", 
    "publish": "2010-02-05T09:20:51Z", 
    "summary": "The stability and convergence of the neural networks are the fundamental\ncharacteristics in the Hopfield type networks. Since time delay is ubiquitous\nin most physical and biological systems, more attention is being made for the\ndelayed neural networks. The inclusion of time delay into a neural model is\nnatural due to the finite transmission time of the interactions. The stability\nanalysis of the neural networks depends on the Lyapunov function and hence it\nmust be constructed for the given system. In this paper we have made an attempt\nto establish the logarithmic stability of the impulsive delayed neural networks\nby constructing suitable Lyapunov function."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.1176v1", 
    "title": "Phase-Only Planar Antenna Array Synthesis with Fuzzy Genetic Algorithms", 
    "arxiv-id": "1002.1176v1", 
    "author": "Fethi Tarik Bendimerad", 
    "publish": "2010-02-05T10:08:22Z", 
    "summary": "This paper describes a new method for the synthesis of planar antenna arrays\nusing fuzzy genetic algorithms (FGAs) by optimizing phase excitation\ncoefficients to best meet a desired radiation pattern. We present the\napplication of a rigorous optimization technique based on fuzzy genetic\nalgorithms (FGAs), the optimizing algorithm is obtained by adjusting control\nparameters of a standard version of genetic algorithm (SGAs) using a fuzzy\ncontroller (FLC) depending on the best individual fitness and the population\ndiversity measurements (PDM). The presented optimization algorithms were\npreviously checked on specific mathematical test function and show their\nsuperior capabilities with respect to the standard version (SGAs). A planar\narray with rectangular cells using a probe feed is considered. Included example\nusing FGA demonstrates the good agreement between the desired and calculated\nradiation patterns than those obtained by a SGA."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.1184v1", 
    "title": "Implementation of an Innovative Bio Inspired GA and PSO Algorithm for   Controller design considering Steam GT Dynamics", 
    "arxiv-id": "1002.1184v1", 
    "author": "R. Lakshmipathi", 
    "publish": "2010-02-05T10:27:13Z", 
    "summary": "The Application of Bio Inspired Algorithms to complicated Power System\nStability Problems has recently attracted the researchers in the field of\nArtificial Intelligence. Low frequency oscillations after a disturbance in a\nPower system, if not sufficiently damped, can drive the system unstable. This\npaper provides a systematic procedure to damp the low frequency oscillations\nbased on Bio Inspired Genetic (GA) and Particle Swarm Optimization (PSO)\nalgorithms. The proposed controller design is based on formulating a System\nDamping ratio enhancement based Optimization criterion to compute the optimal\ncontroller parameters for better stability. The Novel and contrasting feature\nof this work is the mathematical modeling and simulation of the Synchronous\ngenerator model including the Steam Governor Turbine (GT) dynamics. To show the\nrobustness of the proposed controller, Non linear Time domain simulations have\nbeen carried out under various system operating conditions. Also, a detailed\nComparative study has been done to show the superiority of the Bio inspired\nalgorithm based controllers over the Conventional Lead lag controller."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.2012v1", 
    "title": "Implementing Genetic Algorithms on Arduino Micro-Controllers", 
    "arxiv-id": "1002.2012v1", 
    "author": "Nuno Alves", 
    "publish": "2010-02-10T01:16:21Z", 
    "summary": "Since their conception in 1975, Genetic Algorithms have been an extremely\npopular approach to find exact or approximate solutions to optimization and\nsearch problems. Over the last years there has been an enhanced interest in the\nfield with related techniques, such as grammatical evolution, being developed.\nUnfortunately, work on developing genetic optimizations for low-end embedded\narchitectures hasn't embraced the same enthusiasm. This short paper tackles\nthat situation by demonstrating how genetic algorithms can be implemented in\nArduino Duemilanove, a 16 MHz open-source micro-controller, with limited\ncomputation power and storage resources. As part of this short paper, the\nlibraries used in this implementation are released into the public domain under\na GPL license."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.2195v1", 
    "title": "Multi Product Inventory Optimization using Uniform Crossover Genetic   Algorithm", 
    "arxiv-id": "1002.2195v1", 
    "author": "G. Sathish", 
    "publish": "2010-02-10T20:00:15Z", 
    "summary": "Inventory management is considered to be an important field in Supply Chain\nManagement because the cost of inventories in a supply chain accounts for about\n30 percent of the value of the product. The service provided to the customer\neventually gets enhanced once the efficient and effective management of\ninventory is carried out all through the supply chain. The precise estimation\nof optimal inventory is essential since shortage of inventory yields to lost\nsales, while excess of inventory may result in pointless storage costs. Thus\nthe determination of the inventory to be held at various levels in a supply\nchain becomes inevitable so as to ensure minimal cost for the supply chain. The\nminimization of the total supply chain cost can only be achieved when\noptimization of the base stock level is carried out at each member of the\nsupply chain. This paper deals with the problem of determination of base stock\nlevels in a ten member serial supply chain with multiple products produced by\nfactories using Uniform Crossover Genetic Algorithms. The complexity of the\nproblem increases when more distribution centers and agents and multiple\nproducts were involved. These considerations leading to very complex inventory\nmanagement process has been resolved in this work."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.2196v1", 
    "title": "Efficient Inventory Optimization of Multi Product, Multiple Suppliers   with Lead Time using PSO", 
    "arxiv-id": "1002.2196v1", 
    "author": "G. Sathish", 
    "publish": "2010-02-10T20:03:20Z", 
    "summary": "With information revolution, increased globalization and competition, supply\nchain has become longer and more complicated than ever before. These\ndevelopments bring supply chain management to the forefront of the managements\nattention. Inventories are very important in a supply chain. The total\ninvestment in inventories is enormous, and the management of inventory is\ncrucial to avoid shortages or delivery delays for the customers and serious\ndrain on a companys financial resources. The supply chain cost increases\nbecause of the influence of lead times for supplying the stocks as well as the\nraw materials. Practically, the lead times will not be same through out all the\nperiods. Maintaining abundant stocks in order to avoid the impact of high lead\ntime increases the holding cost. Similarly, maintaining fewer stocks because of\nballpark lead time may lead to shortage of stocks. This also happens in the\ncase of lead time involved in supplying raw materials. A better optimization\nmethodology that utilizes the Particle Swarm Optimization algorithm, one of the\nbest optimization algorithms, is proposed to overcome the impasse in\nmaintaining the optimal stock levels in each member of the supply chain. Taking\ninto account the stock levels thus obtained from the proposed methodology, an\nappropriate stock levels to be maintained in the approaching periods that will\nminimize the supply chain inventory cost can be arrived at."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.4004v1", 
    "title": "Nature inspired artificial intelligence based adaptive traffic flow   distribution in computer network", 
    "arxiv-id": "1002.4004v1", 
    "author": "Manoj Kumar Singh", 
    "publish": "2010-02-21T19:41:45Z", 
    "summary": "Because of the stochastic nature of traffic requirement matrix, it is very\ndifficult to get the optimal traffic distribution to minimize the delay even\nwith adaptive routing protocol in a fixed connection network where capacity\nalready defined for each link. Hence there is a requirement to define such a\nmethod, which could generate the optimal solution very quickly and efficiently.\nThis paper presenting a new concept to provide the adaptive optimal traffic\ndistribution for dynamic condition of traffic matrix using nature based\nintelligence methods. With the defined load and fixed capacity of links,\naverage delay for packet has minimized with various variations of evolutionary\nprogramming and particle swarm optimization. Comparative study has given over\ntheir performance in terms of converging speed. Universal approximation\ncapability, the key feature of feed forward neural network has applied to\npredict the flow distribution on each link to minimize the average delay for a\ntotal load available at present on the network. For any variation in the total\nload, the new flow distribution can be generated by neural network immediately,\nwhich could generate minimum delay in the network. With the inclusion of this\ninformation, performance of routing protocol will be improved very much."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1002.4831v1", 
    "title": "On Analysis and Evaluation of Multi-Sensory Cognitive Learning of a   Mathematical Topic Using Artificial Neural Networks", 
    "arxiv-id": "1002.4831v1", 
    "author": "A. Al-Hamadi", 
    "publish": "2010-02-25T17:15:50Z", 
    "summary": "This piece of research belongs to the field of educational assessment issue\nbased upon the cognitive multimedia theory. Considering that theory; visual and\nauditory material should be presented simultaneously to reinforce the retention\nof a mathematical learned topic, a carefully computer-assisted learning (CAL)\nmodule is designed for development of a multimedia tutorial for our suggested\nmathematical topic. The designed CAL module is a multimedia tutorial computer\npackage with visual and/or auditory material. So, via suggested computer\npackage, Multi-Sensory associative memories and classical conditioning theories\nare practically applicable at an educational field (a children classroom). It\nis noticed that comparative practical results obtained are interesting for\nfield application of CAL package with and without associated teacher's voice.\nFinally, the presented study highly recommends application of a novel teaching\ntrend aiming to improve quality of children mathematical learning performance."
},{
    "category": "cs.NE", 
    "doi": "10.4229/24thEUPVSEC2009-5BV.2.35", 
    "link": "http://arxiv.org/pdf/1003.1457v2", 
    "title": "The Comparison of Methods Artificial Neural Network with Linear   Regression Using Specific Variables for Prediction Stock Price in Tehran   Stock Exchange", 
    "arxiv-id": "1003.1457v2", 
    "author": "Hassan Pournaghshband", 
    "publish": "2010-03-07T12:05:22Z", 
    "summary": "In this paper, researchers estimated the stock price of activated companies\nin Tehran (Iran) stock exchange. It is used Linear Regression and Artificial\nNeural Network methods and compared these two methods. In Artificial Neural\nNetwork, of General Regression Neural Network method (GRNN) for architecture is\nused. In this paper, first, researchers considered 10 macro economic variables\nand 30 financial variables and then they obtained seven final variables\nincluding 3 macro economic variables and 4 financial variables to estimate the\nstock price using Independent components Analysis (ICA). So, we presented an\nequation for two methods and compared their results which shown that artificial\nneural network method is more efficient than linear regression method."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1004.0514v1", 
    "title": "Superior Exploration-Exploitation Balance with Quantum-Inspired Hadamard   Walks", 
    "arxiv-id": "1004.0514v1", 
    "author": "Ashish Ranjan Hota", 
    "publish": "2010-04-04T15:38:48Z", 
    "summary": "This paper extends the analogies employed in the development of\nquantum-inspired evolutionary algorithms by proposing quantum-inspired Hadamard\nwalks, called QHW. A novel quantum-inspired evolutionary algorithm, called\nHQEA, for solving combinatorial optimization problems, is also proposed. The\nnovelty of HQEA lies in it's incorporation of QHW Remote Search and QHW Local\nSearch - the quantum equivalents of classical mutation and local search, that\nthis paper defines. The intuitive reasoning behind this approach, and the\nexploration-exploitation balance thus occurring is explained. From the results\nof the experiments carried out on the 0,1-knapsack problem, HQEA performs\nsignificantly better than a conventional genetic algorithm, CGA, and two\nquantum-inspired evolutionary algorithms - QEA and NQEA, in terms of\nconvergence speed and accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1004.3557v1", 
    "title": "Neuroevolutionary optimization", 
    "arxiv-id": "1004.3557v1", 
    "author": "Eva Volna", 
    "publish": "2010-04-20T20:17:41Z", 
    "summary": "This paper presents an application of evolutionary search procedures to\nartificial neural networks. Here, we can distinguish among three kinds of\nevolution in artificial neural networks, i.e. the evolution of connection\nweights, of architectures, and of learning rules. We review each kind of\nevolution in detail and analyse critical issues related to different\nevolutions. This article concentrates on finding the suitable way of using\nevolutionary algorithms for optimizing the artificial neural network\nparameters."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1004.3725v1", 
    "title": "A Gibbs distribution that learns from GA dynamics", 
    "arxiv-id": "1004.3725v1", 
    "author": "Jun-ichi Inoue", 
    "publish": "2010-04-21T15:24:02Z", 
    "summary": "A general procedure of average-case performance evaluation for population\ndynamics such as genetic algorithms (GAs) is proposed and its validity is\nnumerically examined. We introduce a learning algorithm of Gibbs distributions\nfrom training sets which are gene configurations (strings) generated by GA in\norder to figure out the statistical properties of GA from the view point of\nthermodynamics. The learning algorithm is constructed by means of minimization\nof the Kullback-Leibler information between a parametric Gibbs distribution and\nthe empirical distribution of gene configurations. The formulation is applied\nto the solvable probabilistic models having multi-valley energy landscapes,\nnamely, the spin glass chain and the Sherrington-Kirkpatrick model. By using\ncomputer simulations, we discuss the asymptotic behaviour of the effective\ntemperature scheduling and the residual energy induced by the GA dynamics."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1004.4610v1", 
    "title": "Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks", 
    "arxiv-id": "1004.4610v1", 
    "author": "Farouk Kamoun", 
    "publish": "2010-04-26T19:18:48Z", 
    "summary": "Mobility prediction allows estimating the stability of paths in a mobile\nwireless Ad Hoc networks. Identifying stable paths helps to improve routing by\nreducing the overhead and the number of connection interruptions. In this\npaper, we introduce a neural network based method for mobility prediction in Ad\nHoc networks. This method consists of a multi-layer and recurrent neural\nnetwork using back propagation through time algorithm for training."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1005.0965v1", 
    "title": "Artificial Neural Network based Diagnostic Model For Causes of Success   and Failures", 
    "arxiv-id": "1005.0965v1", 
    "author": "Himanshu Aggarwal", 
    "publish": "2010-05-06T10:22:40Z", 
    "summary": "In this paper an attempt has been made to identify most important human\nresource factors and propose a diagnostic model based on the back-propagation\nand connectionist model approaches of artificial neural network (ANN). The\nfocus of the study is on the mobile -communication industry of India. The ANN\nbased approach is particularly important because conventional approaches (such\nas algorithmic) to the problem solving have their inherent disadvantages. The\nalgorithmic approach is well-suited to the problems that are well-understood\nand known solution(s). On the other hand the ANNs have learning by example and\nprocessing capabilities similar to that of a human brain. ANN has been followed\ndue to its inherent advantage over conversion algorithmic like approaches and\nhaving capabilities, training and human like intuitive decision making\ncapabilities. Therefore, this ANN based approach is likely to help researchers\nand organizations to reach a better solution to the problem of managing the\nhuman resource. The study is particularly important as many studies have been\ncarried in developed countries but there is a shortage of such studies in\ndeveloping nations like India. Here, a model has been derived using\nconnectionist-ANN approach and improved and verified via back-propagation\nalgorithm. This suggested ANN based model can be used for testing the success\nand failure human factors in any of the communication Industry. Results have\nbeen obtained on the basis of connectionist model, which has been further\nrefined by BPNN to an accuracy of 99.99%. Any company to predict failure due to\nHR factors can directly deploy this model."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1005.5115v1", 
    "title": "Improving GPS/INS Integration through Neural Networks", 
    "arxiv-id": "1005.5115v1", 
    "author": "C. Zhou", 
    "publish": "2010-05-27T16:46:13Z", 
    "summary": "The Global Positioning Systems (GPS) and Inertial Navigation System (INS)\ntechnology have attracted a considerable importance recently because of its\nlarge number of solutions serving both military as well as civilian\napplications. This paper aims to develop a more efficient and especially a\nfaster method for processing the GPS signal in case of INS signal loss without\nlosing the accuracy of the data. The conventional or usual method consists of\nprocessing data through a neural network and obtaining accurate positioning\noutput data. The new or improved method adds selective filtering at the\nlow-band frequency, the mid-band frequency and the high band frquency, before\nprocessing the GPS data through the neural network, so that the processing time\nis decreased significantly while the accuracy remains the same."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1006.0448v1", 
    "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local   Receptive Fields", 
    "arxiv-id": "1006.0448v1", 
    "author": "Yann LeCun", 
    "publish": "2010-06-02T17:08:29Z", 
    "summary": "We introduce a new neural architecture and an unsupervised algorithm for\nlearning invariant representations from temporal sequence of images. The system\nuses two groups of complex cells whose outputs are combined multiplicatively:\none that represents the content of the image, constrained to be constant over\nseveral consecutive frames, and one that represents the precise location of\nfeatures, which is allowed to vary over time but constrained to be sparse. The\narchitecture uses an encoder to extract features, and a decoder to reconstruct\nthe input from the features. The method was applied to patches extracted from\nconsecutive movie frames and produces orientation and frequency selective units\nanalogous to the complex cells in V1. An extension of the method is proposed to\ntrain a network composed of units with local receptive field spread over a\nlarge image of arbitrary size. A layer of complex cells, subject to sparsity\nconstraints, pool feature units over overlapping local neighborhoods, which\ncauses the feature units to organize themselves into pinwheel patterns of\norientation-selective receptive fields, similar to those observed in the\nmammalian visual cortex. A feed-forward encoder efficiently computes the\nfeature representation of full images."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1006.1543v1", 
    "title": "Efficient Discovery of Large Synchronous Events in Neural Spike Streams", 
    "arxiv-id": "1006.1543v1", 
    "author": "K. P. Unnikrishnan", 
    "publish": "2010-06-08T13:03:45Z", 
    "summary": "We address the problem of finding patterns from multi-neuronal spike trains\nthat give us insights into the multi-neuronal codes used in the brain and help\nus design better brain computer interfaces. We focus on the synchronous firings\nof groups of neurons as these have been shown to play a major role in coding\nand communication. With large electrode arrays, it is now possible to\nsimultaneously record the spiking activity of hundreds of neurons over large\nperiods of time. Recently, techniques have been developed to efficiently count\nthe frequency of synchronous firing patterns. However, when the number of\nneurons being observed grows they suffer from the combinatorial explosion in\nthe number of possible patterns and do not scale well. In this paper, we\npresent a temporal data mining scheme that overcomes many of these problems. It\ngenerates a set of candidate patterns from frequent patterns of smaller size;\nall possible patterns are not counted. Also we count only a certain well\ndefined subset of occurrences and this makes the process more efficient. We\nhighlight the computational advantage that this approach offers over the\nexisting methods through simulations.\n  We also propose methods for assessing the statistical significance of the\ndiscovered patterns. We detect only those patterns that repeat often enough to\nbe significant and thus be able to automatically fix the threshold for the\ndata-mining application. Finally we discuss the usefulness of these methods for\nbrain computer interfaces."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1006.4754v1", 
    "title": "Active Sites model for the B-Matrix Approach", 
    "arxiv-id": "1006.4754v1", 
    "author": "Krishna Chaithanya Lingashetty", 
    "publish": "2010-06-24T11:28:30Z", 
    "summary": "This paper continues on the work of the B-Matrix approach in hebbian learning\nproposed by Dr. Kak. It reports the results on methods of improving the memory\nretrieval capacity of the hebbian neural network which implements the B-Matrix\napproach. Previously, the approach to retrieving the memories from the network\nwas to clamp all the individual neurons separately and verify the integrity of\nthese memories. Here we present a network with the capability to identify the\n\"active sites\" in the network during the training phase and use these \"active\nsites\" to generate the memories retrieved from these neurons. Three methods are\nproposed for obtaining the update order of the network from the proximity\nmatrix when multiple neurons are to be clamped. We then present a comparison\nbetween the new methods to the classical case and also among the methods\nthemselves."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1008.0063v1", 
    "title": "Evolutionary Approach to Test Generation for Functional BIST", 
    "arxiv-id": "1008.0063v1", 
    "author": "J. Raik", 
    "publish": "2010-07-31T07:31:41Z", 
    "summary": "In the paper, an evolutionary approach to test generation for functional BIST\nis considered. The aim of the proposed scheme is to minimize the test data\nvolume by allowing the device's microprogram to test its logic, providing an\nobservation structure to the system, and generating appropriate test data for\nthe given architecture. Two methods of deriving a deterministic test set at\nfunctional level are suggested. The first method is based on the classical\ngenetic algorithm with binary and arithmetic crossover and mutation operators.\nThe second one uses genetic programming, where test is represented as a\nsequence of microoperations. In the latter case, we apply two-point crossover\nbased on exchanging test subsequences and mutation implemented as random\nreplacement of microoperations or operands. Experimental data of the program\nrealization showing the efficiency of the proposed methods are presented."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1008.3450v3", 
    "title": "Bottleneck of using single memristor as a synapse and its solution", 
    "arxiv-id": "1008.3450v3", 
    "author": "Iman Esmaili Paeen Afrakoti", 
    "publish": "2010-08-20T07:43:24Z", 
    "summary": "It is now widely accepted that memristive devices are perfect candidates for\nthe emulation of biological synapses in neuromorphic systems. This is mainly\nbecause of the fact that like the strength of synapse, memristance of the\nmemristive device can be tuned actively (e.g., by the application of volt- age\nor current). In addition, it is also possible to fabricate very high density of\nmemristive devices (comparable to the number of synapses in real biological\nsystem) through the nano-crossbar structures. However, in this paper we will\nshow that there are some problems associated with memristive synapses\n(memristive devices which are playing the role of biological synapses). For\nexample, we show that the variation rate of the memristance of memristive\ndevice depends completely on the current memristance of the device and\ntherefore it can change significantly with time during the learning phase. This\nphenomenon can degrade the performance of learning methods like Spike\nTiming-Dependent Plasticity (STDP) and cause the corresponding neuromorphic\nsystems to become unstable. Finally, at the end of this paper, we illustrate\nthat using two serially connected memristive devices with different polarities\nas a synapse can somewhat fix the aforementioned problem."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1008.4049v2", 
    "title": "Discriminating between Nasal and Mouth Breathing", 
    "arxiv-id": "1008.4049v2", 
    "author": "Damian Coyle", 
    "publish": "2010-08-24T14:12:07Z", 
    "summary": "The recommendation to change breathing patterns from the mouth to the nose\ncan have a significantly positive impact upon the general well being of the\nindividual. We classify nasal and mouth breathing by using an acoustic sensor\nand intelligent signal processing techniques. The overall purpose is to\ninvestigate the possibility of identifying the differences in patterns between\nnasal and mouth breathing in order to integrate this information into a\ndecision support system which will form the basis of a patient monitoring and\nmotivational feedback system to recommend the change from mouth to nasal\nbreathing. Our findings show that the breath pattern can be discriminated in\ncertain places of the body both by visual spectrum analysis and with a Back\nPropagation neural network classifier. The sound file recoded from the sensor\nplaced on the hollow in the neck shows the most promising accuracy which is as\nhigh as 90%."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.0915v1", 
    "title": "Results of Evolution Supervised by Genetic Algorithms", 
    "arxiv-id": "1009.0915v1", 
    "author": "Radu E. Sestra\u015f", 
    "publish": "2010-09-05T13:17:01Z", 
    "summary": "A series of results of evolution supervised by genetic algorithms with\ninterest to agricultural and horticultural fields are reviewed. New obtained\noriginal results from the use of genetic algorithms on structure-activity\nrelationships are reported."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.1513v2", 
    "title": "Artificial Neural Networks, Symmetries and Differential Evolution", 
    "arxiv-id": "1009.1513v2", 
    "author": "Orhan Arikan", 
    "publish": "2010-09-08T12:33:03Z", 
    "summary": "Neuroevolution is an active and growing research field, especially in times\nof increasingly parallel computing architectures. Learning methods for\nArtificial Neural Networks (ANN) can be divided into two groups. Neuroevolution\nis mainly based on Monte-Carlo techniques and belongs to the group of global\nsearch methods, whereas other methods such as backpropagation belong to the\ngroup of local search methods. ANN's comprise important symmetry properties,\nwhich can influence Monte-Carlo methods. On the other hand, local search\nmethods are generally unaffected by these symmetries. In the literature,\ndealing with the symmetries is generally reported as being not effective or\neven yielding inferior results. In this paper, we introduce the so called\nMinimum Global Optimum Proximity principle derived from theoretical\nconsiderations for effective symmetry breaking, applied to offline supervised\nlearning. Using Differential Evolution (DE), which is a popular and robust\nevolutionary global optimization method, we experimentally show significant\nglobal search efficiency improvements by symmetry breaking."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4318v1", 
    "title": "Performance Analysis of Estimation of Distribution Algorithm and Genetic   Algorithm in Zone Routing Protocol", 
    "arxiv-id": "1009.4318v1", 
    "author": "Md. Iqbal Hossain Suvo", 
    "publish": "2010-09-22T10:14:31Z", 
    "summary": "In this paper, Estimation of Distribution Algorithm (EDA) is used for Zone\nRouting Protocol (ZRP) in Mobile Ad-hoc Network (MANET) instead of Genetic\nAlgorithm (GA). It is an evolutionary approach, and used when the network size\ngrows and the search space increases. When the destination is outside the zone,\nEDA is applied to find the route with minimum cost and time. The implementation\nof proposed method is compared with Genetic ZRP, i.e., GZRP and the result\ndemonstrates better performance for the proposed method. Since the method\nprovides a set of paths to the destination, it results in load balance to the\nnetwork. As both EDA and GA use random search method to reach the optimal\npoint, the searching cost reduced significantly, especially when the number of\ndata is large."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4495v1", 
    "title": "Unary Coding for Neural Network Learning", 
    "arxiv-id": "1009.4495v1", 
    "author": "Subhash Kak", 
    "publish": "2010-09-22T22:32:37Z", 
    "summary": "This paper presents some properties of unary coding of significance for\nbiological learning and instantaneously trained neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4564v1", 
    "title": "A Constructive Algorithm for Feedforward Neural Networks for Medical   Diagnostic Reasoning", 
    "arxiv-id": "1009.4564v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-23T10:25:04Z", 
    "summary": "This research is to search for alternatives to the resolution of complex\nmedical diagnosis where human knowledge should be apprehended in a general\nfashion. Successful application examples show that human diagnostic\ncapabilities are significantly worse than the neural diagnostic system. Our\nresearch describes a constructive neural network algorithm with\nbackpropagation; offer an approach for the incremental construction of\nnearminimal neural network architectures for pattern classification. The\nalgorithm starts with minimal number of hidden units in the single hidden\nlayer; additional units are added to the hidden layer one at a time to improve\nthe accuracy of the network and to get an optimal size of a neural network. Our\nalgorithm was tested on several benchmarking classification problems including\nCancer1, Heart, and Diabetes with good generalization ability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4566v1", 
    "title": "An Algorithm to Extract Rules from Artificial Neural Networks for   Medical Diagnosis Problems", 
    "arxiv-id": "1009.4566v1", 
    "author": "Md. Monirul Islam", 
    "publish": "2010-09-23T10:30:55Z", 
    "summary": "Artificial neural networks (ANNs) have been successfully applied to solve a\nvariety of classification and function approximation problems. Although ANNs\ncan generally predict better than decision trees for pattern classification\nproblems, ANNs are often regarded as black boxes since their predictions cannot\nbe explained clearly like those of decision trees. This paper presents a new\nalgorithm, called rule extraction from ANNs (REANN), to extract rules from\ntrained ANNs for medical diagnosis problems. A standard three-layer feedforward\nANN with four-phase training is the basis of the proposed algorithm. In the\nfirst phase, the number of hidden nodes in ANNs is determined automatically by\na constructive algorithm. In the second phase, irrelevant connections and input\nnodes are removed from trained ANNs without sacrificing the predictive accuracy\nof ANNs. The continuous activation values of the hidden nodes are discretized\nby using an efficient heuristic clustering algorithm in the third phase.\nFinally, rules are extracted from compact ANNs by examining the discretized\nactivation values of the hidden nodes. Extensive experimental studies on three\nbenchmark classification problems, i.e. breast cancer, diabetes and lenses,\ndemonstrate that REANN can generate high quality rules from ANNs, which are\ncomparable with other methods in terms of number of rules, average number of\nconditions for a rule, and predictive accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4570v1", 
    "title": "Extraction of Symbolic Rules from Artificial Neural Networks", 
    "arxiv-id": "1009.4570v1", 
    "author": "Md. Monirul Islam", 
    "publish": "2010-09-23T10:37:54Z", 
    "summary": "Although backpropagation ANNs generally predict better than decision trees do\nfor pattern classification problems, they are often regarded as black boxes,\ni.e., their predictions cannot be explained as those of decision trees. In many\napplications, it is desirable to extract knowledge from trained ANNs for the\nusers to gain a better understanding of how the networks solve the problems. A\nnew rule extraction algorithm, called rule extraction from artificial neural\nnetworks (REANN) is proposed and implemented to extract symbolic rules from\nANNs. A standard three-layer feedforward ANN is the basis of the algorithm. A\nfour-phase training algorithm is proposed for backpropagation learning.\nExplicitness of the extracted rules is supported by comparing them to the\nsymbolic rules generated by other methods. Extracted rules are comparable with\nother methods in terms of number of rules, average number of conditions for a\nrule, and predictive accuracy. Extensive experimental studies on several\nbenchmarks classification problems, such as breast cancer, iris, diabetes, and\nseason classification problems, demonstrate the effectiveness of the proposed\napproach with good generalization ability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4572v1", 
    "title": "Medical diagnosis using neural network", 
    "arxiv-id": "1009.4572v1", 
    "author": "Md. Ehsanul Hoque Mazumder", 
    "publish": "2010-09-23T10:44:24Z", 
    "summary": "This research is to search for alternatives to the resolution of complex\nmedical diagnosis where human knowledge should be apprehended in a general\nfashion. Successful application examples show that human diagnostic\ncapabilities are significantly worse than the neural diagnostic system. This\npaper describes a modified feedforward neural network constructive algorithm\n(MFNNCA), a new algorithm for medical diagnosis. The new constructive algorithm\nwith backpropagation; offer an approach for the incremental construction of\nnear-minimal neural network architectures for pattern classification. The\nalgorithm starts with minimal number of hidden units in the single hidden\nlayer; additional units are added to the hidden layer one at a time to improve\nthe accuracy of the network and to get an optimal size of a neural network. The\nMFNNCA was tested on several benchmarking classification problems including the\ncancer, heart disease and diabetes. Experimental results show that the MFNNCA\ncan produce optimal neural network architecture with good generalization\nability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4962v1", 
    "title": "RGANN: An Efficient Algorithm to Extract Rules from ANNs", 
    "arxiv-id": "1009.4962v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-25T00:59:18Z", 
    "summary": "This paper describes an efficient rule generation algorithm, called rule\ngeneration from artificial neural networks (RGANN) to generate symbolic rules\nfrom ANNs. Classification rules are sought in many areas from automatic\nknowledge acquisition to data mining and ANN rule extraction. This is because\nclassification rules possess some attractive features. They are explicit,\nunderstandable and verifiable by domain experts, and can be modified, extended\nand passed on as modular knowledge. A standard three-layer feedforward ANN is\nthe basis of the algorithm. A four-phase training algorithm is proposed for\nbackpropagation learning. Comparing them to the symbolic rules generated by\nother methods supports explicitness of the generated rules. Generated rules are\ncomparable with other methods in terms of number of rules, average number of\nconditions for a rule, and predictive accuracy. Extensive experimental studies\non several benchmarks classification problems, including breast cancer, wine,\nseason, golf-playing, and lenses classification demonstrate the effectiveness\nof the proposed approach with good generalization ability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4978v1", 
    "title": "Extracting Symbolic Rules for Medical Diagnosis Problem", 
    "arxiv-id": "1009.4978v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-25T06:22:30Z", 
    "summary": "Neural networks (NNs) have been successfully applied to solve a variety of\napplication problems involving classification and function approximation.\nAlthough backpropagation NNs generally predict better than decision trees do\nfor pattern classification problems, they are often regarded as black boxes,\ni.e., their predictions cannot be explained as those of decision trees. In many\napplications, it is desirable to extract knowledge from trained NNs for the\nusers to gain a better understanding of how the networks solve the problems. An\nalgorithm is proposed and implemented to extract symbolic rules for medical\ndiagnosis problem. Empirical study on three benchmarks classification problems,\nsuch as breast cancer, diabetes, and lenses demonstrates that the proposed\nalgorithm generates high quality rules from NNs comparable with other methods\nin terms of number of rules, average number of conditions for a rule, and\npredictive accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4983v1", 
    "title": "Pattern Classification using Simplified Neural Networks", 
    "arxiv-id": "1009.4983v1", 
    "author": "Ahmed Ryadh Hasan", 
    "publish": "2010-09-25T07:00:25Z", 
    "summary": "In recent years, many neural network models have been proposed for pattern\nclassification, function approximation and regression problems. This paper\npresents an approach for classifying patterns from simplified NNs. Although the\npredictive accuracy of ANNs is often higher than that of other methods or human\nexperts, it is often said that ANNs are practically \"black boxes\", due to the\ncomplexity of the networks. In this paper, we have an attempted to open up\nthese black boxes by reducing the complexity of the network. The factor makes\nthis possible is the pruning algorithm. By eliminating redundant weights,\nredundant input and hidden units are identified and removed from the network.\nUsing the pruning algorithm, we have been able to prune networks such that only\na few input units, hidden units and connections left yield a simplified\nnetwork. Experimental results on several benchmarks problems in neural networks\nshow the effectiveness of the proposed approach with good generalization\nability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4984v1", 
    "title": "Rule Extraction using Artificial Neural Networks", 
    "arxiv-id": "1009.4984v1", 
    "author": "Ahmed Ryadh Hasan", 
    "publish": "2010-09-25T07:05:29Z", 
    "summary": "Artificial neural networks have been successfully applied to a variety of\nbusiness application problems involving classification and regression. Although\nbackpropagation neural networks generally predict better than decision trees do\nfor pattern classification problems, they are often regarded as black boxes,\ni.e., their predictions are not as interpretable as those of decision trees. In\nmany applications, it is desirable to extract knowledge from trained neural\nnetworks so that the users can gain a better understanding of the solution.\nThis paper presents an efficient algorithm to extract rules from artificial\nneural networks. We use two-phase training algorithm for backpropagation\nlearning. In the first phase, the number of hidden nodes of the network is\ndetermined automatically in a constructive fashion by adding nodes one after\nanother based on the performance of the network on training data. In the second\nphase, the number of relevant input units of the network is determined using\npruning algorithm. The pruning process attempts to eliminate as many\nconnections as possible from the network. Relevant and irrelevant attributes of\nthe data are distinguished during the training process. Those that are relevant\nwill be kept and others will be automatically discarded. From the simplified\nnetworks having small number of connections and nodes we may easily able to\nextract symbolic rules using the proposed algorithm. Extensive experimental\nresults on several benchmarks problems in neural networks demonstrate the\neffectiveness of the proposed approach with good generalization ability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.4988v1", 
    "title": "REx: An Efficient Rule Generator", 
    "arxiv-id": "1009.4988v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-25T07:33:44Z", 
    "summary": "This paper describes an efficient algorithm REx for generating symbolic rules\nfrom artificial neural network (ANN). Classification rules are sought in many\nareas from automatic knowledge acquisition to data mining and ANN rule\nextraction. This is because classification rules possess some attractive\nfeatures. They are explicit, understandable and verifiable by domain experts,\nand can be modified, extended and passed on as modular knowledge. REx exploits\nthe first order information in the data and finds shortest sufficient\nconditions for a rule of a class that can differentiate it from patterns of\nother classes. It can generate concise and perfect rules in the sense that the\nerror rate of the rules is not worse than the inconsistency rate found in the\noriginal data. An important feature of rule extraction algorithm, REx, is its\nrecursive nature. They are concise, comprehensible, order insensitive and do\nnot involve any weight values. Extensive experimental studies on several\nbenchmark classification problems, such as breast cancer, iris, season, and\ngolf-playing, demonstrate the effectiveness of the proposed approach with good\ngeneralization ability."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1009.5031v1", 
    "title": "A Genetic Algorithm for the Multi-Pickup and Delivery Problem with time   windows", 
    "arxiv-id": "1009.5031v1", 
    "author": "Pierre Borne", 
    "publish": "2010-09-25T19:44:51Z", 
    "summary": "In This paper we present a genetic algorithm for the multi-pickup and\ndelivery problem with time windows (m-PDPTW). The m-PDPTW is an optimization\nvehicles routing problem which must meet requests for transport between\nsuppliers and customers satisfying precedence, capacity and time constraints.\nThis paper purposes a brief literature review of the PDPTW, present our\napproach based on genetic algorithms to minimizing the total travel distance\nand thereafter the total travel cost, by showing that an encoding represents\nthe parameters of each individual."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1010.0771v2", 
    "title": "Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and   Delivery Problem with Time Windows", 
    "arxiv-id": "1010.0771v2", 
    "author": "Pierre Borne", 
    "publish": "2010-10-05T06:01:24Z", 
    "summary": "In This paper we present a genetic algorithm for mulicriteria optimization of\na multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW is\nan optimization vehicles routing problem which must meet requests for transport\nbetween suppliers and customers satisfying precedence, capacity and time\nconstraints. This paper purposes a brief literature review of the PDPTW,\npresent an approach based on genetic algorithms and Pareto dominance method to\ngive a set of satisfying solutions to the m-PDPTW minimizing total travel cost,\ntotal tardiness time and the vehicles number."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1010.0979v1", 
    "title": "Un Algorithme g\u00e9n\u00e9tique pour le probl\u00e8me de ramassage et de   livraison avec fen\u00eatres de temps \u00e0 plusieurs v\u00e9hicules", 
    "arxiv-id": "1010.0979v1", 
    "author": "Mekki Ksouri", 
    "publish": "2010-10-05T18:54:43Z", 
    "summary": "The PDPTW is an optimization vehicles routing problem which must meet\nrequests for transport between suppliers and customers satisfying precedence,\ncapacity and time constraints. We present, in this paper, a genetic algorithm\nfor optimization of a multi pickup and delivery problem with time windows\n(m-PDPTW). We purposes a brief literature review of the PDPTW, present an\napproach based on genetic algorithms to give a satisfying solution to the\nm-PDPTW minimizing the total travel cost."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1010.0980v1", 
    "title": "Approche Multicrit\u00e8re pour le Probl\u00e8me de Ramassage et de Livraison   avec Fen\u00eatres de Temps \u00e0 Plusieurs V\u00e9hicules", 
    "arxiv-id": "1010.0980v1", 
    "author": "Pierre Borne", 
    "publish": "2010-10-05T18:55:13Z", 
    "summary": "Nowadays, the transport goods problem occupies an important place in the\neconomic life of modern societies. The pickup and delivery problem with time\nwindows (PDPTW) is one of the problems which a large part of the research was\ninterested. In this paper, we present a a brief literature review of the VRP\nand the PDPTW, propose our multicriteria approach based on genetic algorithms\nwhich allows minimize the compromise between the vehicles number, the total\ntardiness time and the total travel cost. And this, by treating the case where\na customer can have multiple suppliers and one supplier can have multiple\ncustomers"
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1010.1429v2", 
    "title": "Optimizing Monotone Functions Can Be Difficult", 
    "arxiv-id": "1010.1429v2", 
    "author": "Christine Zarges", 
    "publish": "2010-10-07T13:21:28Z", 
    "summary": "Extending previous analyses on function classes like linear functions, we\nanalyze how the simple (1+1) evolutionary algorithm optimizes pseudo-Boolean\nfunctions that are strictly monotone. Contrary to what one would expect, not\nall of these functions are easy to optimize. The choice of the constant $c$ in\nthe mutation probability $p(n) = c/n$ can make a decisive difference.\n  We show that if $c < 1$, then the (1+1) evolutionary algorithm finds the\noptimum of every such function in $\\Theta(n \\log n)$ iterations. For $c=1$, we\ncan still prove an upper bound of $O(n^{3/2})$. However, for $c > 33$, we\npresent a strictly monotone function such that the (1+1) evolutionary algorithm\nwith overwhelming probability does not find the optimum within $2^{\\Omega(n)}$\niterations. This is the first time that we observe that a constant factor\nchange of the mutation probability changes the run-time by more than constant\nfactors."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1010.4138v1", 
    "title": "Sparse and silent coding in neural circuits", 
    "arxiv-id": "1010.4138v1", 
    "author": "G\u00e1bor Szirtes", 
    "publish": "2010-10-20T09:29:03Z", 
    "summary": "Sparse coding algorithms are about finding a linear basis in which signals\ncan be represented by a small number of active (non-zero) coefficients. Such\ncoding has many applications in science and engineering and is believed to play\nan important role in neural information processing. However, due to the\ncomputational complexity of the task, only approximate solutions provide the\nrequired efficiency (in terms of time). As new results show, under particular\nconditions there exist efficient solutions by minimizing the magnitude of the\ncoefficients (`$l_1$-norm') instead of minimizing the size of the active subset\nof features (`$l_0$-norm'). Straightforward neural implementation of these\nsolutions is not likely, as they require \\emph{a priori} knowledge of the\nnumber of active features. Furthermore, these methods utilize iterative\nre-evaluation of the reconstruction error, which in turn implies that final\nsparse forms (featuring `population sparseness') can only be reached through\nthe formation of a series of non-sparse representations, which is in contrast\nwith the overall sparse functioning of the neural systems (`lifetime\nsparseness'). In this article we present a novel algorithm which integrates our\nprevious `$l_0$-norm' model on spike based probabilistic optimization for\nsparse coding with ideas coming from novel `$l_1$-norm' solutions.\n  The resulting algorithm allows neurally plausible implementation and does not\nrequire an exactly defined sparseness level thus it is suitable for\nrepresenting natural stimuli with a varying number of features. We also\ndemonstrate that the combined method significantly extends the domain where\noptimal solutions can be found by `$l_1$-norm' based algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1830761.1830878", 
    "link": "http://arxiv.org/pdf/1011.0800v1", 
    "title": "Soil Classification Using GATree", 
    "arxiv-id": "1011.0800v1", 
    "author": "S. Jyothi", 
    "publish": "2010-11-03T05:07:02Z", 
    "summary": "This paper details the application of a genetic programming framework for\nclassification of decision tree of Soil data to classify soil texture. The\ndatabase contains measurements of soil profile data. We have applied GATree for\ngenerating classification decision tree. GATree is a decision tree builder that\nis based on Genetic Algorithms (GAs). The idea behind it is rather simple but\npowerful. Instead of using statistic metrics that are biased towards specific\ntrees we use a more flexible, global metric of tree quality that try to\noptimize accuracy and size. GATree offers some unique features not to be found\nin any other tree inducers while at the same time it can produce better results\nfor many difficult problems. Experimental results are presented which\nillustrate the performance of generating best decision tree for classifying\nsoil texture for soil data set."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.artint.2012.01.001", 
    "link": "http://arxiv.org/pdf/1011.4028v4", 
    "title": "On the approximation ability of evolutionary optimization with   application to minimum set cover", 
    "arxiv-id": "1011.4028v4", 
    "author": "Zhi-Hua Zhou", 
    "publish": "2010-11-17T19:08:42Z", 
    "summary": "Evolutionary algorithms (EAs) are heuristic algorithms inspired by natural\nevolution. They are often used to obtain satisficing solutions in practice. In\nthis paper, we investigate a largely underexplored issue: the approximation\nperformance of EAs in terms of how close the solution obtained is to an optimal\nsolution. We study an EA framework named simple EA with isolated population\n(SEIP) that can be implemented as a single- or multi-objective EA. We analyze\nthe approximation performance of SEIP using the partial ratio, which\ncharacterizes the approximation ratio that can be guaranteed. Specifically, we\nanalyze SEIP using a set cover problem that is NP-hard. We find that in a\nsimple configuration, SEIP efficiently achieves an $H_n$-approximation ratio,\nthe asymptotic lower bound, for the unbounded set cover problem. We also find\nthat SEIP efficiently achieves an $(H_k-\\frac{k-1}/{8k^9})$-approximation\nratio, the currently best-achievable result, for the k-set cover problem.\nMoreover, for an instance class of the k-set cover problem, we disclose how\nSEIP, using either one-bit or bit-wise mutation, can overcome the difficulty\nthat limits the greedy algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.artint.2012.01.001", 
    "link": "http://arxiv.org/pdf/1011.6022v4", 
    "title": "DXNN Platform: The Shedding of Biological Inefficiencies", 
    "arxiv-id": "1011.6022v4", 
    "author": "Gene I. Sher", 
    "publish": "2010-11-28T09:10:11Z", 
    "summary": "This paper introduces a novel type of memetic algorithm based Topology and\nWeight Evolving Artificial Neural Network (TWEANN) system called DX Neural\nNetwork (DXNN). DXNN implements a number of interesting features, amongst which\nis: a simple and database friendly tuple based encoding method, a 2 phase\nneuroevolutionary approach aimed at removing the need for speciation due to its\nintrinsic population diversification effects, a new \"Targeted Tuning Phase\"\naimed at dealing with \"the curse of dimensionality\", and a new Random Intensity\nMutation (RIM) method that removes the need for crossover algorithms. The paper\nwill discuss DXNN's architecture, mutation operators, and its built in feature\nselection method that allows for the evolved systems to expand and incorporate\nnew sensors and actuators. I then compare DXNN to other state of the art\nTWEANNs on the standard double pole balancing benchmark, and demonstrate its\nsuperior ability to evolve highly compact solutions faster than its\ncompetitors. Then a set of oblation experiments is performed to demonstrate how\neach feature of DXNN effects its performance, followed by a set of experiments\nwhich demonstrate the platform's ability to create NN populations with\nexceptionally high diversity profiles. Finally, DXNN is used to evolve\nartificial robots in a set of two dimensional open-ended food gathering and\npredator-prey simulations, demonstrating the system's ability to produce ever\nmore complex Neural Networks, and the system's applicability to the domain of\nrobotics, artificial life, and coevolution."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.artint.2012.01.001", 
    "link": "http://arxiv.org/pdf/1012.0952v1", 
    "title": "Faster Black-Box Algorithms Through Higher Arity Operators", 
    "arxiv-id": "1012.0952v1", 
    "author": "Carola Winzen", 
    "publish": "2010-12-04T22:11:48Z", 
    "summary": "We extend the work of Lehre and Witt (GECCO 2010) on the unbiased black-box\nmodel by considering higher arity variation operators. In particular, we show\nthat already for binary operators the black-box complexity of \\leadingones\ndrops from $\\Theta(n^2)$ for unary operators to $O(n \\log n)$. For \\onemax, the\n$\\Omega(n \\log n)$ unary black-box complexity drops to O(n) in the binary case.\nFor $k$-ary operators, $k \\leq n$, the \\onemax-complexity further decreases to\n$O(n/\\log k)$."
},{
    "category": "cs.NE", 
    "doi": "10.1109/NABIC.2010.5716320", 
    "link": "http://arxiv.org/pdf/1101.0362v1", 
    "title": "An Adaptive Quantum-inspired Differential Evolution Algorithm for 0-1   Knapsack Problem", 
    "arxiv-id": "1101.0362v1", 
    "author": "Ankit Pat", 
    "publish": "2011-01-01T18:26:29Z", 
    "summary": "Differential evolution (DE) is a population based evolutionary algorithm\nwidely used for solving multidimensional global optimization problems over\ncontinuous spaces. However, the design of its operators makes it unsuitable for\nmany real-life constrained combinatorial optimization problems which operate on\nbinary space. On the other hand, the quantum inspired evolutionary algorithm\n(QEA) is very well suitable for handling such problems by applying several\nquantum computing techniques such as Q-bit representation and rotation gate\noperator, etc. This paper extends the concept of differential operators with\nadaptive parameter control to the quantum paradigm and proposes the adaptive\nquantum-inspired differential evolution algorithm (AQDE). The performance of\nAQDE is found to be significantly superior as compared to QEA and a discrete\nversion of DE on the standard 0-1 knapsack problem for all the considered test\ncases."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1101.0776v1", 
    "title": "Multiplicative Drift Analysis", 
    "arxiv-id": "1101.0776v1", 
    "author": "Carola Winzen", 
    "publish": "2011-01-04T17:44:56Z", 
    "summary": "In this work, we introduce multiplicative drift analysis as a suitable way to\nanalyze the runtime of randomized search heuristics such as evolutionary\nalgorithms.\n  We give a multiplicative version of the classical drift theorem. This allows\neasier analyses in those settings where the optimization progress is roughly\nproportional to the current distance to the optimum.\n  To display the strength of this tool, we regard the classical problem how the\n(1+1) Evolutionary Algorithm optimizes an arbitrary linear pseudo-Boolean\nfunction. Here, we first give a relatively simple proof for the fact that any\nlinear function is optimized in expected time $O(n \\log n)$, where $n$ is the\nlength of the bit string. Afterwards, we show that in fact any such function is\noptimized in expected time at most ${(1+o(1)) 1.39 \\euler n\\ln (n)}$, again\nusing multiplicative drift analysis. We also prove a corresponding lower bound\nof ${(1-o(1))e n\\ln(n)}$ which actually holds for all functions with a unique\nglobal optimum.\n  We further demonstrate how our drift theorem immediately gives natural proofs\n(with better constants) for the best known runtime bounds for the (1+1)\nEvolutionary Algorithm on combinatorial problems like finding minimum spanning\ntrees, shortest paths, or Euler tours."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1101.4445v1", 
    "title": "Spectrum Management for Cognitive Radio based on Genetics Algorithm", 
    "arxiv-id": "1101.4445v1", 
    "author": "Dr. Krishna Chandra Roy", 
    "publish": "2011-01-24T05:21:33Z", 
    "summary": "Spectrum scarceness is one of the major challenges that the present world is\nfacing. The efficient use of existing licensed spectrum is becoming most\ncritical as growing demand of the radio spectrum. Different researches show\nthat the use of licensed are not utilized inefficiently. It has been also shown\nthat primary user does not use more than 70% of the licensed frequency band\nmost of the time. Many researchers are trying to found the techniques that\nefficiently utilize the under-utilized licensed spectrum. One of the approaches\nis the use of \"Cognitive Radio\". This allows the radio to learn from its\nenvironment, changing certain parameters. Based on this knowledge the radio can\ndynamically exploit the spectrum holes in the licensed band of the spectrum.\nThis paper w i l l focus on the performance of spectrum allocation technique,\nbased on popular meta-heuristics Genetics Algorithm and analyzing the\nperformance of this technique using Mat Lab."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1101.5997v1", 
    "title": "New Model for Multi-Objective Evolutionary Algorithms", 
    "arxiv-id": "1101.5997v1", 
    "author": "Yuanxiang Li", 
    "publish": "2011-01-31T15:47:05Z", 
    "summary": "Multi-Objective Evolutionary Algorithms (MOEAs) have been proved efficient to\ndeal with Multi-objective Optimization Problems (MOPs). Until now tens of MOEAs\nhave been proposed. The unified mode would provide a more systematic approach\nto build new MOEAs. Here a new model is proposed which includes two sub-models\nbased on two classes of different schemas of MOEAs. According to the new model,\nsome representatives algorithms are decomposed and some interesting issues are\ndiscussed."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1102.2559v1", 
    "title": "Toward Measuring the Scaling of Genetic Programming", 
    "arxiv-id": "1102.2559v1", 
    "author": "Mike Stimpson", 
    "publish": "2011-02-13T05:09:32Z", 
    "summary": "Several genetic programming systems are created, each solving a different\nproblem. In these systems, the median number of generations G needed to evolve\na working program is measured. The behavior of G is observed as the difficulty\nof the problem is increased. In these systems, the density D of working\nprograms in the universe of all possible programs is measured. The relationship\nG ~ 1/sqrt(D) is observed to approximately hold for two program-like systems.\nFor parallel systems (systems that look like several independent programs\nevolving in parallel), the relationship G ~ 1/(n ln n) is observed to\napproximately hold. Finally, systems that are anti-parallel are considered."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1102.5757v1", 
    "title": "Improving the character recognition efficiency of feed forward BP neural   network", 
    "arxiv-id": "1102.5757v1", 
    "author": "Rahul Rishi", 
    "publish": "2011-02-28T19:58:20Z", 
    "summary": "This work is focused on improving the character recognition capability of\nfeed-forward back-propagation neural network by using one, two and three hidden\nlayers and the modified additional momentum term. 182 English letters were\ncollected for this work and the equivalent binary matrix form of these\ncharacters was applied to the neural network as training patterns. While the\nnetwork was getting trained, the connection weights were modified at each epoch\nof learning. For each training sample, the error surface was examined for\nminima by computing the gradient descent. We started the experiment by using\none hidden layer and the number of hidden layers was increased up to three and\nit has been observed that accuracy of the network was increased with low mean\nsquare error but at the cost of training time. The recognition accuracy was\nimproved further when modified additional momentum term was used."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1103.0087v1", 
    "title": "Cost effective approach on feature selection using genetic algorithms   and fuzzy logic for diabetes diagnosis", 
    "arxiv-id": "1103.0087v1", 
    "author": "E. P. Ephzibah", 
    "publish": "2011-03-01T06:10:18Z", 
    "summary": "A way to enhance the performance of a model that combines genetic algorithms\nand fuzzy logic for feature selection and classification is proposed. Early\ndiagnosis of any disease with less cost is preferable. Diabetes is one such\ndisease. Diabetes has become the fourth leading cause of death in developed\ncountries and there is substantial evidence that it is reaching epidemic\nproportions in many developing and newly industrialized nations. In medical\ndiagnosis, patterns consist of observable symptoms along with the results of\ndiagnostic tests. These tests have various associated costs and risks. In the\nautomated design of pattern classification, the proposed system solves the\nfeature subset selection problem. It is a task of identifying and selecting a\nuseful subset of pattern-representing features from a larger set of features.\nUsing fuzzy rule-based classification system, the proposed system proves to\nimprove the classification accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1103.2741v1", 
    "title": "Memory Retrieval in the B-Matrix Neural Network", 
    "arxiv-id": "1103.2741v1", 
    "author": "Prerana Laddha", 
    "publish": "2011-03-14T18:58:59Z", 
    "summary": "This paper is an extension to the memory retrieval procedure of the B-Matrix\napproach [6],[17] to neural network learning. The B-Matrix is a part of the\ninterconnection matrix generated from the Hebbian neural network, and in memory\nretrieval, the B-matrix is clamped with a small fragment of the memory. The\nfragment gradually enlarges by means of feedback, until the entire vector is\nobtained. In this paper, we propose the use of delta learning to enhance the\nretrieval rate of the stored memories."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1103.4820v1", 
    "title": "Design and classification of dynamic multi-objective optimization   problems", 
    "arxiv-id": "1103.4820v1", 
    "author": "Pascal Bouvry", 
    "publish": "2011-03-24T17:59:10Z", 
    "summary": "In this work we provide a formal model for the different time-dependent\ncomponents that can appear in dynamic multi-objective optimization problems,\nalong with a classification of these components. Four main classes are\nidentified, corresponding to the influence of the parameters, objective\nfunctions, previous states of the dynamic system and, last, environment\nchanges, which in turn lead to online optimization problems. For illustration\npurposes, examples are provided for each class identified - by no means\nstanding as the most representative ones or exhaustive in scope."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1103.5081v2", 
    "title": "Using Variable Threshold to Increase Capacity in a Feedback Neural   Network", 
    "arxiv-id": "1103.5081v2", 
    "author": "Praveen Kuruvada", 
    "publish": "2011-03-25T20:59:13Z", 
    "summary": "The article presents new results on the use of variable thresholds to\nincrease the capacity of a feedback neural network. Non-binary networks are\nalso considered in this analysis."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1103.5797v2", 
    "title": "Computational Complexity Results for Genetic Programming and the Sorting   Problem", 
    "arxiv-id": "1103.5797v2", 
    "author": "Frank Neumann", 
    "publish": "2011-03-29T23:52:30Z", 
    "summary": "Genetic Programming (GP) has found various applications. Understanding this\ntype of algorithm from a theoretical point of view is a challenging task. The\nfirst results on the computational complexity of GP have been obtained for\nproblems with isolated program semantics. With this paper, we push forward the\ncomputational complexity analysis of GP on a problem with dependent program\nsemantics. We study the well-known sorting problem in this context and analyze\nrigorously how GP can deal with different measures of sortedness."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1105.0332v1", 
    "title": "Recalling of Images using Hopfield Neural Network Model", 
    "arxiv-id": "1105.0332v1", 
    "author": "Dr. K. S. Shreedhara", 
    "publish": "2011-05-02T13:54:40Z", 
    "summary": "In the present paper, an effort has been made for storing and recalling\nimages with Hopfield Neural Network Model of auto-associative memory. Images\nare stored by calculating a corresponding weight matrix. Thereafter, starting\nfrom an arbitrary configuration, the memory will settle on exactly that stored\nimage, which is nearest to the starting configuration in terms of Hamming\ndistance. Thus given an incomplete or corrupted version of a stored image, the\nnetwork is able to recall the corresponding original image. The storing of the\nobjects has been performed according to the Hopfield algorithm explained below.\nOnce the net has completely learnt this set of input patterns, a set of testing\npatterns containing degraded images will be given to the net. Then the Hopfield\nnet will tend to recall the closest matching pattern for the given degraded\nimage. The simulated results show that Hopfield model is the best for storing\nand recalling images."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1105.0355v1", 
    "title": "A Novel Crossover Operator for Genetic Algorithms: Ring Crossover", 
    "arxiv-id": "1105.0355v1", 
    "author": "Ramazan Tek\\D{j}n", 
    "publish": "2011-05-02T15:22:36Z", 
    "summary": "The genetic algorithm (GA) is an optimization and search technique based on\nthe principles of genetics and natural selection. A GA allows a population\ncomposed of many individuals to evolve under specified selection rules to a\nstate that maximizes the \"fitness\" function. In that process, crossover\noperator plays an important role. To comprehend the GAs as a whole, it is\nnecessary to understand the role of a crossover operator. Today, there are a\nnumber of different crossover operators that can be used in GAs. However, how\nto decide what operator to use for solving a problem? A number of test\nfunctions with various levels of difficulty has been selected as a test polygon\nfor determine the performance of crossover operators. In this paper, a novel\ncrossover operator called 'ring crossover' is proposed. In order to evaluate\nthe efficiency and feasibility of the proposed operator, a comparison between\nthe results of this study and results of different crossover operators used in\nGAs is made through a number of test functions with various levels of\ndifficulty. Results of this study clearly show significant differences between\nthe proposed operator and the other crossover operators."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00453-012-9622-x", 
    "link": "http://arxiv.org/pdf/1105.1641v1", 
    "title": "Neural network to identify individuals at health risk", 
    "arxiv-id": "1105.1641v1", 
    "author": "Dejan Magoc", 
    "publish": "2011-05-09T11:45:04Z", 
    "summary": "The risk of diseases such as heart attack and high blood pressure could be\nreduced by adequate physical activity. However, even though majority of general\npopulation claims to perform some physical exercise, only a minority exercises\nenough to keep a healthy living style. Thus, physical inactivity has become one\nof the major concerns of public health in the past decade. Research shows that\nthe highest decrease in physical activity is noticed from high school to\ncollege. Thus, it is of great importance to quickly identify college students\nat health risk due to physical inactivity. Research also shows that the level\nof physical activity of an individual is highly correlated to demographic\nfeatures such as race and gender, as well as self motivation and support from\nfamily and friends. This information could be collected from each student via a\n20 minute questionnaire, but the time needed to distribute and analyze each\nquestionnaire is infeasible on a collegiate campus. Thus, we propose an\nautomatic identifier of students at risk, so that these students could easier\nbe targeted by collegiate campuses and physical activity promotion departments.\nWe present in this paper preliminary results of a supervised backpropagation\nmultilayer neural network for classifying students into at-risk or not at-risk\ngroup."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.1901v1", 
    "title": "Convergence Analysis of Differential Evolution Variants on Unconstrained   Global Optimization Functions", 
    "arxiv-id": "1105.1901v1", 
    "author": "G. Jeyakumar C. Shanmugavelayutham", 
    "publish": "2011-05-10T10:32:01Z", 
    "summary": "In this paper, we present an empirical study on convergence nature of\nDifferential Evolution (DE) variants to solve unconstrained global optimization\nproblems. The aim is to identify the competitive nature of DE variants in\nsolving the problem at their hand and compare. We have chosen fourteen\nbenchmark functions grouped by feature: unimodal and separable, unimodal and\nnonseparable, multimodal and separable, and multimodal and nonseparable.\nFourteen variants of DE were implemented and tested on fourteen benchmark\nproblems for dimensions of 30. The competitiveness of the variants are\nidentified by the Mean Objective Function value, they achieved in 100 runs. The\nconvergence nature of the best and worst performing variants are analyzed by\nmeasuring their Convergence Speed (Cs) and Quality Measure (Qm)."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.2894v2", 
    "title": "Ant Colony Optimization and Hypergraph Covering Problems", 
    "arxiv-id": "1105.2894v2", 
    "author": "Ashish Ranjan Hota", 
    "publish": "2011-05-14T12:22:42Z", 
    "summary": "Ant Colony Optimization (ACO) is a very popular metaheuristic for solving\ncomputationally hard combinatorial optimization problems. Runtime analysis of\nACO with respect to various pseudo-boolean functions and different graph based\ncombinatorial optimization problems has been taken up in recent years. In this\npaper, we investigate the runtime behavior of an MMAS*(Max-Min Ant System) ACO\nalgorithm on some well known hypergraph covering problems that are NP-Hard. In\nparticular, we have addressed the Minimum Edge Cover problem, the Minimum\nVertex Cover problem and the Maximum Weak- Independent Set problem. The\ninfluence of pheromone values and heuristic information on the running time is\nanalysed. The results indicate that the heuristic information has greater\nimpact towards improving the expected optimization time as compared to\npheromone values. For certain instances of hypergraphs, we show that the MMAS*\nalgorithm gives a constant order expected optimization time when the dominance\nof heuristic information is suitably increased."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.3538v1", 
    "title": "The Exact Schema Theorem", 
    "arxiv-id": "1105.3538v1", 
    "author": "Alden H. Wright", 
    "publish": "2011-05-18T05:37:36Z", 
    "summary": "A schema is a naturally defined subset of the space of fixed-length binary\nstrings. The Holland Schema Theorem gives a lower bound on the expected\nfraction of a population in a schema after one generation of a simple genetic\nalgorithm. This paper gives formulas for the exact expected fraction of a\npopulation in a schema after one generation of the simple genetic algorithm.\nHolland's schema theorem has three parts, one for selection, one for crossover,\nand one for mutation. The selection part is exact, whereas the crossover and\nmutation parts are approximations. This paper shows how the crossover and\nmutation parts can be made exact. Holland's schema theorem follows naturally as\na corollary. There is a close relationship between schemata and the\nrepresentation of the population in the Walsh basis. This relationship is used\nin the derivation of the results, and can also make computation of the schema\naverages more efficient. This paper gives a version of the Vose infinite\npopulation model where crossover and mutation are separated into two functions\nrather than a single \"mixing\" function."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.4971v1", 
    "title": "Distributed Evolutionary Computation using REST", 
    "arxiv-id": "1105.4971v1", 
    "author": "J. J. Merelo", 
    "publish": "2011-05-25T09:13:31Z", 
    "summary": "This paper analises distributed evolutionary computation based on the\nRepresentational State Transfer (REST) protocol, which overlays a farming model\non evolutionary computation. An approach to evolutionary distributed\noptimisation of multilayer perceptrons (MLP) using REST and language Perl has\nbeen done. In these experiments, a master-slave based evolutionary algorithm\n(EA) has been implemented, where slave processes evaluate the costly fitness\nfunction (training a MLP to solve a classification problem). Obtained results\nshow that the parallel version of the developed programs obtains similar or\nbetter results using much less time than the sequential version, obtaining a\ngood speedup."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.4978v1", 
    "title": "SOAP vs REST: Comparing a master-slave GA implementation", 
    "arxiv-id": "1105.4978v1", 
    "author": "P. Garcia-Sanchez", 
    "publish": "2011-05-25T09:44:17Z", 
    "summary": "In this paper, a high-level comparison of both SOAP (Simple Object Access\nProtocol) and REST (Representational State Transfer) is made. These are the two\nmain approaches for interfacing to the web with web services. Both approaches\nare different and present some advantages and disadvantages for interfacing to\nweb services: SOAP is conceptually more difficult (has a steeper learning\ncurve) and more \"heavy-weight\" than REST, although it lacks of standards\nsupport for security. In order to test their eficiency (in time), two\nexperiments have been performed using both technologies: a client-server model\nimplementation and a master-slave based genetic algorithm (GA). The results\nobtained show clear differences in time between SOAP and REST implementations.\nAlthough both techniques are suitable for developing parallel systems, SOAP is\nheavier than REST, mainly due to the verbosity of SOAP communications (XML\nincreases the time taken to parse the messages)."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1105.5540v1", 
    "title": "Finite First Hitting Time versus Stochastic Convergence in Particle   Swarm Optimisation", 
    "arxiv-id": "1105.5540v1", 
    "author": "Carsten Witt", 
    "publish": "2011-05-27T12:21:29Z", 
    "summary": "We reconsider stochastic convergence analyses of particle swarm optimisation,\nand point out that previously obtained parameter conditions are not always\nsufficient to guarantee mean square convergence to a local optimum. We show\nthat stagnation can in fact occur for non-trivial configurations in non-optimal\nparts of the search space, even for simple functions like SPHERE. The\nconvergence properties of the basic PSO may in these situations be detrimental\nto the goal of optimisation, to discover a sufficiently good solution within\nreasonable time. To characterise optimisation ability of algorithms, we suggest\nthe expected first hitting time (FHT), i.e., the time until a search point in\nthe vicinity of the optimum is visited. It is shown that a basic PSO may have\ninfinite expected FHT, while an algorithm introduced here, the Noisy PSO, has\nfinite expected FHT on some functions."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1106.0190v1", 
    "title": "Evolution of Things", 
    "arxiv-id": "1106.0190v1", 
    "author": "S. Kernbach", 
    "publish": "2011-06-01T14:32:58Z", 
    "summary": "Evolution is one of the major omnipresent powers in the universe that has\nbeen studied for about two centuries. Recent scientific and technical\ndevelopments make it possible to make the transition from passively\nunderstanding to actively mastering evolution. As of today, the only area where\nhuman experimenters can design and manipulate evolutionary processes in full is\nthat of Evolutionary Computing, where evolutionary processes are carried out in\na digital space, inside computers, in simulation. We argue that in the near\nfuture it will be possible to move evolutionary computing outside such\nimaginary spaces and make it physically embodied. In other words, we envision\nthe \"Evolution of Things\", rather than just the evolution of code, leading to a\nnew field of Embodied Artificial Evolution (EAE). The main objective of the\npresent paper is to offer an umbrella term and vision in order to aid the\ndevelopment of this high potential research area. To this end, we introduce the\nnotion of EAE, discuss a few examples and applications, and elaborate on the\nexpected benefits as well as the grand challenges this developing field will\nhave to address."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1106.1570v1", 
    "title": "A Neural Network Model for Construction Projects Site Overhead Cost   Estimating in Egypt", 
    "arxiv-id": "1106.1570v1", 
    "author": "Mohammed Abdel Razek", 
    "publish": "2011-06-08T14:29:41Z", 
    "summary": "Estimating of the overhead costs of building construction projects is an\nimportant task in the management of these projects. The quality of construction\nmanagement depends heavily on their accurate cost estimation. Construction\ncosts prediction is a very difficult and sophisticated task especially when\nusing manual calculation methods. This paper uses Artificial Neural Network\n(ANN) approach to develop a parametric cost-estimating model for site overhead\ncost in Egypt. Fifty-two actual real-life cases of building projects\nconstructed in Egypt during the seven year period 2002-2009 were used as\ntraining materials. The neural network architecture is presented for the\nestimation of the site overhead costs as a percentage from the total project\nprice."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1106.2113v1", 
    "title": "Using Hopfield to Solve Resource-Leveling Problem", 
    "arxiv-id": "1106.2113v1", 
    "author": "Yueming Hu", 
    "publish": "2011-05-28T00:55:18Z", 
    "summary": "Although the traditional permute matrix coming along with Hopfield is able to\ndescribe many common problems, it seems to have limitation in solving more\ncomplicated problem with more constrains, like resource leveling which is\nactually a NP problem. This paper tries to find a better solution for it by\nusing neural network. In order to give the neural network description of\nresource leveling problem, a new description method called Augmented permute\nmatrix is proposed by expending the ability of the traditional one. An Embedded\nHybrid Model combining Hopfield model and SA are put forward to improve the\noptimization in essence in which Hopfield servers as State Generator for the\nSA. The experiment results show that Augmented permute matrix is able to\ncompletely and appropriately describe the application. The energy function and\nhybrid model given in this study are also highly efficient in solving resource\nleveling problem."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1106.2156v1", 
    "title": "A Computational Framework for Nonlinear Dimensionality Reduction of   Large Data Sets: The Exploratory Inspection Machine (XIM)", 
    "arxiv-id": "1106.2156v1", 
    "author": "Axel Wism\u00fcller", 
    "publish": "2011-06-10T19:48:01Z", 
    "summary": "In this paper, we present a novel computational framework for nonlinear\ndimensionality reduction which is specifically suited to process large data\nsets: the Exploratory Inspection Machine (XIM). XIM introduces a conceptual\ncross-link between hitherto separate domains of machine learning, namely\ntopographic vector quantization and divergence-based neighbor embedding\napproaches. There are three ways to conceptualize XIM, namely (i) as the\ninversion of the Exploratory Observation Machine (XOM) and its variants, such\nas Neighbor Embedding XOM (NE-XOM), (ii) as a powerful optimization scheme for\ndivergence-based neighbor embedding cost functions inspired by Stochastic\nNeighbor Embedding (SNE) and its variants, such as t-distributed SNE (t-SNE),\nand (iii) as an extension of topographic vector quantization methods, such as\nthe Self-Organizing Map (SOM). By preserving both global and local data\nstructure, XIM combines the virtues of classical and advanced recent embedding\nmethods. It permits direct visualization of large data collections without the\nneed for prior data reduction. Finally, XIM can contribute to many application\ndomains of data analysis and visualization important throughout the sciences\nand engineering, such as pattern matching, constrained incremental learning,\ndata clustering, and the analysis of non-metric dissimilarity data."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2209", 
    "link": "http://arxiv.org/pdf/1106.2312v1", 
    "title": "Evolutionary Biclustering of Clickstream Data", 
    "arxiv-id": "1106.2312v1", 
    "author": "J. Bagyamani", 
    "publish": "2011-06-12T14:34:16Z", 
    "summary": "Biclustering is a two way clustering approach involving simultaneous\nclustering along two dimensions of the data matrix. Finding biclusters of web\nobjects (i.e. web users and web pages) is an emerging topic in the context of\nweb usage mining. It overcomes the problem associated with traditional\nclustering methods by allowing automatic discovery of browsing pattern based on\na subset of attributes. A coherent bicluster of clickstream data is a local\nbrowsing pattern such that users in bicluster exhibit correlated browsing\npattern through a subset of pages of a web site. This paper proposed a new\napplication of biclustering to web data using a combination of heuristics and\nmeta-heuristics such as K-means, Greedy Search Procedure and Genetic Algorithms\nto identify the coherent browsing pattern. Experiment is conducted on the\nbenchmark clickstream msnbc dataset from UCI repository. Results demonstrate\nthe efficiency and beneficial outcome of the proposed method by correlating the\nusers and pages of a web site in high degree.This approach shows excellent\nperformance at finding high degree of overlapped coherent biclusters from web\ndata."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-012-9322-0", 
    "link": "http://arxiv.org/pdf/1106.6223v1", 
    "title": "Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on   the Law of Gravity", 
    "arxiv-id": "1106.6223v1", 
    "author": "Roderich Gross", 
    "publish": "2011-06-30T13:38:35Z", 
    "summary": "Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on the Law\nof Gravity"
},{
    "category": "cs.NE", 
    "doi": "10.1109/MFI.2008.4648056", 
    "link": "http://arxiv.org/pdf/1107.4414v1", 
    "title": "Frequency based Classification of Activities using Accelerometer Data", 
    "arxiv-id": "1107.4414v1", 
    "author": "Young-Dong Lee Young-Sook Lee Wan-Young Chung", 
    "publish": "2011-07-22T04:41:13Z", 
    "summary": "This work presents, the classification of user activities such as Rest, Walk\nand Run, on the basis of frequency component present in the acceleration data\nin a wireless sensor network environment. As the frequencies of the above\nmentioned activities differ slightly for different person, so it gives a more\naccurate result. The algorithm uses just one parameter i.e. the frequency of\nthe body acceleration data of the three axes for classifying the activities in\na set of data. The algorithm includes a normalization step and hence there is\nno need to set a different value of threshold value for magnitude for different\ntest person. The classification is automatic and done on a block by block\nbasis."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1107.4429v1", 
    "title": "High Accuracy Human Activity Monitoring using Neural network", 
    "arxiv-id": "1107.4429v1", 
    "author": "Wan-Young Chung", 
    "publish": "2011-07-22T06:48:01Z", 
    "summary": "This paper presents the designing of a neural network for the classification\nof Human activity. A Triaxial accelerometer sensor, housed in a chest worn\nsensor unit, has been used for capturing the acceleration of the movements\nassociated. All the three axis acceleration data were collected at a base\nstation PC via a CC2420 2.4GHz ISM band radio (zigbee wireless compliant),\nprocessed and classified using MATLAB. A neural network approach for\nclassification was used with an eye on theoretical and empirical facts. The\nwork shows a detailed description of the designing steps for the classification\nof human body acceleration data. A 4-layer back propagation neural network,\nwith Levenberg-marquardt algorithm for training, showed best performance among\nthe other neural network training algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1107.4470v1", 
    "title": "Symmetry Breaking in Neuroevolution: A Technical Report", 
    "arxiv-id": "1107.4470v1", 
    "author": "Orhan Arikan", 
    "publish": "2011-07-22T10:08:58Z", 
    "summary": "Artificial Neural Networks (ANN) comprise important symmetry properties,\nwhich can influence the performance of Monte Carlo methods in Neuroevolution.\nThe problem of the symmetries is also known as the competing conventions\nproblem or simply as the permutation problem. In the literature, symmetries are\nmainly addressed in Genetic Algoritm based approaches. However, investigations\nin this direction based on other Evolutionary Algorithms (EA) are rare or\nmissing. Furthermore, there are different and contradictionary reports on the\nefficacy of symmetry breaking. By using a novel viewpoint, we offer a possible\nexplanation for this issue. As a result, we show that a strategy which is\ninvariant to the global optimum can only be successfull on certain problems,\nwhereas it must fail to improve the global convergence on others. We introduce\nthe \\emph{Minimum Global Optimum Proximity} principle as a generalized and\nadaptive strategy to symmetry breaking, which depends on the location of the\nglobal optimum. We apply the proposed principle to Differential Evolution (DE)\nand Covariance Matrix Adaptation Evolution Strategies (CMA-ES), which are two\npopular and conceptually different global optimization methods. Using a wide\nrange of feedforward ANN problems, we experimentally illustrate significant\nimprovements in the global search efficiency by the proposed symmetry breaking\ntechnique."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.0342v1", 
    "title": "Black-Box Complexities of Combinatorial Problems", 
    "arxiv-id": "1108.0342v1", 
    "author": "Carola Winzen", 
    "publish": "2011-08-01T15:44:21Z", 
    "summary": "Black-box complexity is a complexity theoretic measure for how difficult a\nproblem is to be optimized by a general purpose optimization algorithm. It is\nthus one of the few means trying to understand which problems are tractable for\ngenetic algorithms and other randomized search heuristics.\n  Most previous work on black-box complexity is on artificial test functions.\nIn this paper, we move a step forward and give a detailed analysis for the two\ncombinatorial problems minimum spanning tree and single-source shortest paths.\nBesides giving interesting bounds for their black-box complexities, our work\nreveals that the choice of how to model the optimization problem is non-trivial\nhere. This in particular comes true where the search space does not consist of\nbit strings and where a reasonable definition of unbiasedness has to be agreed\non."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.1530v1", 
    "title": "Evolving A-Type Artificial Neural Networks", 
    "arxiv-id": "1108.1530v1", 
    "author": "Ben Martin", 
    "publish": "2011-08-07T07:23:32Z", 
    "summary": "We investigate Turing's notion of an A-type artificial neural network. We\nstudy a refinement of Turing's original idea, motivated by work of Teuscher,\nBull, Preen and Copeland. Our A-types can process binary data by accepting and\noutputting sequences of binary vectors; hence we can associate a function to an\nA-type, and we say the A-type {\\em represents} the function. There are two\nmodes of data processing: clamped and sequential. We describe an evolutionary\nalgorithm, involving graph-theoretic manipulations of A-types, which searches\nfor A-types representing a given function. The algorithm uses both mutation and\ncrossover operators. We implemented the algorithm and applied it to three\nbenchmark tasks. We found that the algorithm performed much better than a\nrandom search. For two out of the three tasks, the algorithm with crossover\nperformed better than a mutation-only version."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.3489v1", 
    "title": "A Novel and Robust Evolution Algorithm for Optimizing Complicated   Functions", 
    "arxiv-id": "1108.3489v1", 
    "author": "Ge Zhao", 
    "publish": "2011-08-17T14:22:25Z", 
    "summary": "In this paper, a novel mutation operator of differential evolution algorithm\nis proposed. A new algorithm called divergence differential evolution algorithm\n(DDEA) is developed by combining the new mutation operator with divergence\noperator and assimilation operator (divergence operator divides population,\nand, assimilation operator combines population), which can detect multiple\nsolutions and robustness in noisy environment. The new algorithm is applied to\noptimize Michalewicz Function and to track changing of rain-induced-attenuation\nprocess. The results based on DDEA are compared with those based on\nDifferential Evolution Algorithm (DEA). It shows that DDEA algorithm gets\nbetter results than DEA does in the same premise. The new algorithm is\nsignificant for optimizing and tracking the characteristics of MIMO (Multiple\nInput Multiple Output) channel at millimeter waves."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4080v1", 
    "title": "Convergence Properties of Two (\u03bc + \u03bb) Evolutionary   Algorithms On OneMax and Royal Roads Test Functions", 
    "arxiv-id": "1108.4080v1", 
    "author": "Stephen Marsland", 
    "publish": "2011-08-20T02:11:57Z", 
    "summary": "We present a number of bounds on convergence time for two elitist\npopulation-based Evolutionary Algorithms using a recombination operator\nk-Bit-Swap and a mainstream Randomized Local Search algorithm. We study the\neffect of distribution of elite species and population size."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4083v2", 
    "title": "Convergence of a Recombination-Based Elitist Evolutionary Algorithm on   the Royal Roads Test Function", 
    "arxiv-id": "1108.4083v2", 
    "author": "Stephen Marsland", 
    "publish": "2011-08-20T02:40:33Z", 
    "summary": "We present an analysis of the performance of an elitist Evolutionary\nalgorithm using a recombination operator known as 1-Bit-Swap on the Royal Roads\ntest function based on a population. We derive complete, approximate and\nasymptotic convergence rates for the algorithm. The complete model shows the\nbenefit of the size of the population and re- combination pool."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4386v2", 
    "title": "Tight Bounds on the Optimization Time of the (1+1) EA on Linear   Functions", 
    "arxiv-id": "1108.4386v2", 
    "author": "Carsten Witt", 
    "publish": "2011-08-22T17:43:35Z", 
    "summary": "The analysis of randomized search heuristics on classes of functions is\nfundamental for the understanding of the underlying stochastic process and the\ndevelopment of suitable proof techniques. Recently, remarkable progress has\nbeen made in bounding the expected optimization time of the simple (1+1) EA on\nthe class of linear functions. We improve the best known bound in this setting\nfrom $(1.39+o(1))en\\ln n$ to $en\\ln n+O(n)$ in expectation and with high\nprobability, which is tight up to lower-order terms. Moreover, upper and lower\nbounds for arbitrary mutations probabilities $p$ are derived, which imply\nexpected polynomial optimization time as long as $p=O((\\ln n)/n)$ and which are\ntight if $p=c/n$ for a constant $c$. As a consequence, the standard mutation\nprobability $p=1/n$ is optimal for all linear functions, and the (1+1) EA is\nfound to be an optimal mutation-based algorithm. The proofs are based on\nadaptive drift functions and the recent multiplicative drift theorem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4531v4", 
    "title": "Novel Analysis of Population Scalability in Evolutionary Algorithms", 
    "arxiv-id": "1108.4531v4", 
    "author": "Boris Mitavskiy", 
    "publish": "2011-08-23T09:12:04Z", 
    "summary": "Population-based evolutionary algorithms (EAs) have been widely applied to\nsolve various optimization problems. The question of how the performance of a\npopulation-based EA depends on the population size arises naturally. The\nperformance of an EA may be evaluated by different measures, such as the\naverage convergence rate to the optimal set per generation or the expected\nnumber of generations to encounter an optimal solution for the first time.\nPopulation scalability is the performance ratio between a benchmark EA and\nanother EA using identical genetic operators but a larger population size.\nAlthough intuitively the performance of an EA may improve if its population\nsize increases, currently there exist only a few case studies for simple\nfitness functions. This paper aims at providing a general study for discrete\noptimisation. A novel approach is introduced to analyse population scalability\nusing the fundamental matrix. The following two contributions summarize the\nmajor results of the current article. (1) We demonstrate rigorously that for\nelitist EAs with identical global mutation, using a lager population size\nalways increases the average rate of convergence to the optimal set; and yet,\nsometimes, the expected number of generations needed to find an optimal\nsolution (measured by either the maximal value or the average value) may\nincrease, rather than decrease. (2) We establish sufficient and/or necessary\nconditions for the superlinear scalability, that is, when the average\nconvergence rate of a $(\\mu+\\mu)$ EA (where $\\mu\\ge2$) is bigger than $\\mu$\ntimes that of a $(1+1)$ EA."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4548v1", 
    "title": "Ant Colony Optimization of Rough Set for HV Bushings Fault Detection", 
    "arxiv-id": "1108.4548v1", 
    "author": "T. Marwala", 
    "publish": "2011-08-23T10:47:23Z", 
    "summary": "Most transformer failures are attributed to bushings failures. Hence it is\nnecessary to monitor the condition of bushings. In this paper three methods are\ndeveloped to monitor the condition of oil filled bushing. Multi-layer\nperceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are\ndeveloped and combined through majority voting to form a committee. The MLP\nperforms better that the RBF and the RS is terms of classification accuracy.\nThe RBF is the fasted to train. The committee performs better than the\nindividual models. The diversity of models is measured to evaluate their\nsimilarity when used in the committee."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1108.4618v1", 
    "title": "Artificial Neural Network and Rough Set for HV Bushings Condition   Monitoring", 
    "arxiv-id": "1108.4618v1", 
    "author": "T. Marwala", 
    "publish": "2011-08-23T14:44:44Z", 
    "summary": "Most transformer failures are attributed to bushings failures. Hence it is\nnecessary to monitor the condition of bushings. In this paper three methods are\ndeveloped to monitor the condition of oil filled bushing. Multi-layer\nperceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are\ndeveloped and combined through majority voting to form a committee. The MLP\nperforms better that the RBF and the RS is terms of classification accuracy.\nThe RBF is the fasted to train. The committee performs better than the\nindividual models. The diversity of models is measured to evaluate their\nsimilarity when used in the committee."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1110.1038v1", 
    "title": "Using Genetic Algorithm in the Evolutionary Design of Sequential Logic   Circuits", 
    "arxiv-id": "1110.1038v1", 
    "author": "Mahdi Bagheri", 
    "publish": "2011-10-05T16:52:26Z", 
    "summary": "Evolvable hardware (EHW) is a set of techniques that are based on the idea of\ncombining reconfiguration hardware systems with evolutionary algorithms. In\nother word, EHW has two sections; the reconfigurable hardware and evolutionary\nalgorithm where the configurations are under the control of an evolutionary\nalgorithm. This paper, suggests a method to design and optimize the synchronous\nsequential circuits. Genetic algorithm (GA) was applied as evolutionary\nalgorithm. In this approach, for building input combinational logic circuit of\neach DFF, and also output combinational logic circuit, the cell arrays have\nbeen used. The obtained results show that our method can reduce the average\nnumber of generations by limitation the search space."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1111.0907v2", 
    "title": "Towards Analyzing Crossover Operators in Evolutionary Search via General   Markov Chain Switching Theorem", 
    "arxiv-id": "1111.0907v2", 
    "author": "Zhi-Hua Zhou", 
    "publish": "2011-11-03T16:38:02Z", 
    "summary": "Evolutionary algorithms (EAs), simulating the evolution process of natural\nspecies, are used to solve optimization problems. Crossover (also called\nrecombination), originated from simulating the chromosome exchange phenomena in\nzoogamy reproduction, is widely employed in EAs to generate offspring\nsolutions, of which the effectiveness has been examined empirically in\napplications. However, due to the irregularity of crossover operators and the\ncomplicated interactions to mutation, crossover operators are hard to analyze\nand thus have few theoretical results. Therefore, analyzing crossover not only\nhelps in understanding EAs, but also helps in developing novel techniques for\nanalyzing sophisticated metaheuristic algorithms.\n  In this paper, we derive the General Markov Chain Switching Theorem (GMCST)\nto facilitate theoretical studies of crossover-enabled EAs. The theorem allows\nus to analyze the running time of a sophisticated EA from an easy-to-analyze\nEA. Using this tool, we analyze EAs with several crossover operators on the\nLeadingOnes and OneMax problems, which are noticeably two well studied problems\nfor mutation-only EAs but with few results for crossover-enabled EAs. We first\nderive the bounds of running time of the (2+2)-EA with crossover operators;\nthen we study the running time gap between the mutation-only (2:2)-EA and the\n(2:2)-EA with crossover operators; finally, we develop strategies that apply\ncrossover operators only when necessary, which improve from the mutation-only\nas well as the crossover-all-the-time (2:2)-EA. The theoretical results are\nverified by experiments."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCIT.2008.394", 
    "link": "http://arxiv.org/pdf/1111.1353v1", 
    "title": "An efficient implementation of the simulated annealing heuristic for the   quadratic assignment problem", 
    "arxiv-id": "1111.1353v1", 
    "author": "Gerald Paul", 
    "publish": "2011-11-05T21:43:56Z", 
    "summary": "The quadratic assignment problem (QAP) is one of the most difficult\ncombinatorial optimization problems. One of the most powerful and commonly used\nheuristics to obtain approximations to the optimal solution of the QAP is\nsimulated annealing (SA). We present an efficient implementation of the SA\nheuristic which performs more than 100 times faster then existing\nimplementations for large problem sizes and a large number of SA iterations."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2011.2302", 
    "link": "http://arxiv.org/pdf/1111.1564v1", 
    "title": "Particle Swarm Optimization Framework for Low Power Testing of VLSI   Circuits", 
    "arxiv-id": "1111.1564v1", 
    "author": "Arun Khosla", 
    "publish": "2011-11-07T13:04:48Z", 
    "summary": "Power dissipation in sequential circuits is due to increased toggling count\nof Circuit under Test, which depends upon test vectors applied. If successive\ntest vectors sequences have more toggling nature then it is sure that toggling\nrate of flip flops is higher. Higher toggling for flip flops results more power\ndissipation. To overcome this problem, one method is to use GA to have test\nvectors of high fault coverage in short interval, followed by Hamming distance\nmanagement on test patterns. This approach is time consuming and needs more\nefforts. Another method which is purposed in this paper is a PSO based Frame\nWork to optimize power dissipation. Here target is to set the entire test\nvector in a frame for time period 'T', so that the frame consists of all those\nvectors strings which not only provide high fault coverage but also arrange\nvectors in frame to produce minimum toggling."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1112.1517v4", 
    "title": "Pure Strategy or Mixed Strategy?", 
    "arxiv-id": "1112.1517v4", 
    "author": "Hongbin Dong", 
    "publish": "2011-12-07T10:37:14Z", 
    "summary": "Mixed strategy EAs aim to integrate several mutation operators into a single\nalgorithm. However few theoretical analysis has been made to answer the\nquestion whether and when the performance of mixed strategy EAs is better than\nthat of pure strategy EAs. In theory, the performance of EAs can be measured by\nasymptotic convergence rate and asymptotic hitting time. In this paper, it is\nproven that given a mixed strategy (1+1) EAs consisting of several mutation\noperators, its performance (asymptotic convergence rate and asymptotic hitting\ntime)is not worse than that of the worst pure strategy (1+1) EA using one\nmutation operator; if these mutation operators are mutually complementary, then\nit is possible to design a mixed strategy (1+1) EA whose performance is better\nthan that of any pure strategy (1+1) EA using one mutation operator."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1112.2183v1", 
    "title": "The Expert System Designed to Improve Customer Satisfaction", 
    "arxiv-id": "1112.2183v1", 
    "author": "S. P. Rajagopalan", 
    "publish": "2011-12-09T18:49:28Z", 
    "summary": "Customer Relationship Management becomes a leading business strategy in\nhighly competitive business environment. It aims to enhance the performance of\nthe businesses by improving the customer satisfaction and loyalty. The\nobjective of this paper is to improve customer satisfaction on product's colors\nand design with the help of the expert system developed by using Artificial\nNeural Networks. The expert system's role is to capture the knowledge of the\nexperts and the data from the customer requirements, and then, process the\ncollected data and form the appropriate rules for choosing product's colors and\ndesign. In order to identify the hidden pattern of the customer's needs, the\nArtificial Neural Networks technique has been applied to classify the colors\nand design based upon a list of selected information. Moreover, the expert\nsystem has the capability to make decisions in ranking the scores of the colors\nand design presented in the selection. In addition, the expert system has been\nvalidated with a different customer types."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1112.4323v2", 
    "title": "Between theory and practice: guidelines for an optimization scheme with   genetic algorithms - Part I: single-objective continuous global optimization", 
    "arxiv-id": "1112.4323v2", 
    "author": "Loris Serafino", 
    "publish": "2011-12-19T13:00:23Z", 
    "summary": "The rapid advances in the field of optimization methods in many pure and\napplied science pose the difficulty of keeping track of the developments as\nwell as selecting an appropriate technique that best suits the problem in-hand.\nFrom a practitioner point of view is rightful to wander \"which optimization\nmethod is the best for my problem?\". Looking at the optimization process as a\n\"system\" of intercon- nected parts, in this paper are collected some ideas\nabout how to tackle an optimization problem using a class of tools from\nevolutionary computations called Genetic Algorithms. Despite the number of\noptimization techniques available nowadays the author of this paper thinks that\nGenetic Algorithms still play a central role for their versatility, robustness,\ntheoretical framework and simplicity of use. The paper can be considered a\n\"collection of tips\" (from literature and personal experience) for the\nnon-computer-scientist that has to deal with optimization problems both in the\nscience and engineering practice. No original methods or algorithms are\nproposed."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1112.5980v2", 
    "title": "Search space analysis with Wang-Landau sampling and slow adaptive walks", 
    "arxiv-id": "1112.5980v2", 
    "author": "Susan Khor", 
    "publish": "2011-12-27T15:56:06Z", 
    "summary": "Two complementary techniques for analyzing search spaces are proposed: (i) an\nalgorithm to detect search points with potential to be local optima; and (ii) a\nslightly adjusted Wang-Landau sampling algorithm to explore larger search\nspaces. The detection algorithm assumes that local optima are points which are\neasier to reach and harder to leave by a slow adaptive walker. A slow adaptive\nwalker moves to a nearest fitter point. Thus, points with larger outgoing step\nsizes relative to incoming step sizes are marked using the local optima score\nformulae as potential local optima points (PLOPs). Defining local optima in\nthese more general terms allows their detection within the closure of a subset\nof a search space, and the sampling of a search space unshackled by a\nparticular move set. Tests are done with NK and HIFF problems to confirm that\nPLOPs detected in the manner proposed retain characteristics of local optima,\nand that the adjusted Wang-Landau samples are more representative of the search\nspace than samples produced by choosing points uniformly at random. While our\napproach shows promise, more needs to be done to reduce its computation cost\nthat it may pave a way toward analyzing larger search spaces of practical\nmeaning."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1201.2100v1", 
    "title": "Biologically inspired design framework for Robot in Dynamic Environments   using Framsticks", 
    "arxiv-id": "1201.2100v1", 
    "author": "P. Raviraj", 
    "publish": "2012-01-10T16:19:23Z", 
    "summary": "Robot design complexity is increasing day by day especially in automated\nindustries. In this paper we propose biologically inspired design framework for\nrobots in dynamic world on the basis of Co-Evolution, Virtual Ecology, Life\ntime learning which are derived from biological creatures. We have created a\nvirtual khepera robot in Framsticks and tested its operational credibility in\nterms hardware and software components by applying the above suggested\ntechniques. Monitoring complex and non complex behaviors in different\nenvironments and obtaining the parameters that influence software and hardware\ndesign of the robot that influence anticipated and unanticipated failures,\ncontrol programs of robot generation are the major concerns of our techniques."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1201.4737v1", 
    "title": "Production System Rules as Protein Complexes from Genetic Regulatory   Networks", 
    "arxiv-id": "1201.4737v1", 
    "author": "Larry Bull", 
    "publish": "2012-01-20T20:30:59Z", 
    "summary": "This short paper introduces a new way by which to design production system\nrules. An indirect encoding scheme is presented which views such rules as\nprotein complexes produced by the temporal behaviour of an artificial genetic\nregulatory network. This initial study begins by using a simple Boolean\nregulatory network to produce traditional ternary-encoded rules before moving\nto a fuzzy variant to produce real-valued rules. Competitive performance is\nshown with related genetic regulatory networks and rule-based systems on\nbenchmark problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1201.4908v1", 
    "title": "Self-Organisation of Evolving Agent Populations in Digital Ecosystems", 
    "arxiv-id": "1201.4908v1", 
    "author": "Philippe De Wilde", 
    "publish": "2012-01-24T03:16:43Z", 
    "summary": "We investigate the self-organising behaviour of Digital Ecosystems, because a\nprimary motivation for our research is to exploit the self-organising\nproperties of biological ecosystems. We extended a definition for the\ncomplexity, grounded in the biological sciences, providing a measure of the\ninformation in an organism's genome. Next, we extended a definition for the\nstability, originating from the computer sciences, based upon convergence to an\nequilibrium distribution. Finally, we investigated a definition for the\ndiversity, relative to the selection pressures provided by the user requests.\nWe conclude with a summary and discussion of the achievements, including the\nexperimental results."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29124-1_19", 
    "link": "http://arxiv.org/pdf/1202.0678v2", 
    "title": "Influence of Topological Features on Spatially-Structured Evolutionary   Algorithms Dynamics", 
    "arxiv-id": "1202.0678v2", 
    "author": "Stefano Panzieri", 
    "publish": "2012-02-03T12:24:37Z", 
    "summary": "In the last decades, complex networks theory significantly influenced other\ndisciplines on the modeling of both static and dynamic aspects of systems\nobserved in nature. This work aims to investigate the effects of networks'\ntopological features on the dynamics of an evolutionary algorithm, considering\nin particular the ability to find a large number of optima on multi-modal\nproblems. We introduce a novel spatially-structured evolutionary algorithm and\nwe apply it on two combinatorial problems: ONEMAX and the multi-modal NMAX.\nConsidering three different network models we investigate the relationships\nbetween their features, algorithm's convergence and its ability to find\nmultiple optima (for the multi-modal problem). In order to perform a deeper\nanalysis we investigate the introduction of weighted graphs with time-varying\nweights. The results show that networks with a large Average Path Length lead\nto an higher number of optima and a consequent slow exploration dynamics (i.e.\nlow First Hitting Time). Furthermore, the introduction of weighted networks\nshows the possibility to tune algorithm's dynamics during its execution with\nthe parameter related with weights' change. This work gives a first answer\nabout the effects of various graph topologies on the diversity of evolutionary\nalgorithms and it describes a simple but powerful algorithmic framework which\nallows to investigate many aspects of ssEAs dynamics."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1202.1708v2", 
    "title": "A Polynomial Time Approximation Scheme for a Single Machine Scheduling   Problem Using a Hybrid Evolutionary Algorithm", 
    "arxiv-id": "1202.1708v2", 
    "author": "Jun He", 
    "publish": "2012-02-08T14:30:55Z", 
    "summary": "Nowadays hybrid evolutionary algorithms, i.e, heuristic search algorithms\ncombining several mutation operators some of which are meant to implement\nstochastically a well known technique designed for the specific problem in\nquestion while some others playing the role of random search, have become\nrather popular for tackling various NP-hard optimization problems. While\nempirical studies demonstrate that hybrid evolutionary algorithms are\nfrequently successful at finding solutions having fitness sufficiently close to\nthe optimal, many fewer articles address the computational complexity in a\nmathematically rigorous fashion. This paper is devoted to a mathematically\nmotivated design and analysis of a parameterized family of evolutionary\nalgorithms which provides a polynomial time approximation scheme for one of the\nwell-known NP-hard combinatorial optimization problems, namely the \"single\nmachine scheduling problem without precedence constraints\". The authors hope\nthat the techniques and ideas developed in this article may be applied in many\nother situations."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1203.0197v2", 
    "title": "Statistical Approach for Selecting Elite Ants", 
    "arxiv-id": "1203.0197v2", 
    "author": "N. Prasanna Kumar", 
    "publish": "2012-03-01T14:25:50Z", 
    "summary": "Applications of ACO algorithms to obtain better solutions for combinatorial\noptimization problems have become very popular in recent years. In ACO\nalgorithms, group of agents repeatedly perform well defined actions and\ncollaborate with other ants in order to accomplish the defined task. In this\npaper, we introduce new mechanisms for selecting the Elite ants dynamically\nbased on simple statistical tools. We also investigate the performance of newly\nproposed mechanisms."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1203.3847v1", 
    "title": "Handwritten digit Recognition using Support Vector Machine", 
    "arxiv-id": "1203.3847v1", 
    "author": "Anshuman Sharma", 
    "publish": "2012-03-17T09:17:21Z", 
    "summary": "Handwritten Numeral recognition plays a vital role in postal automation\nservices especially in countries like India where multiple languages and\nscripts are used Discrete Hidden Markov Model (HMM) and hybrid of Neural\nNetwork (NN) and HMM are popular methods in handwritten word recognition\nsystem. The hybrid system gives better recognition result due to better\ndiscrimination capability of the NN. A major problem in handwriting recognition\nis the huge variability and distortions of patterns. Elastic models based on\nlocal observations and dynamic programming such HMM are not efficient to absorb\nthis variability. But their vision is local. But they cannot face to length\nvariability and they are very sensitive to distortions. Then the SVM is used to\nestimate global correlations and classify the pattern. Support Vector Machine\n(SVM) is an alternative to NN. In Handwritten recognition, SVM gives a better\nrecognition result. The aim of this paper is to develop an approach which\nimprove the efficiency of handwritten recognition using artificial neural\nnetwork"
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1203.4111v1", 
    "title": "Reducing the Arity in Unbiased Black-Box Complexity", 
    "arxiv-id": "1203.4111v1", 
    "author": "Carola Winzen", 
    "publish": "2012-03-19T14:12:52Z", 
    "summary": "We show that for all $1<k \\leq \\log n$ the $k$-ary unbiased black-box\ncomplexity of the $n$-dimensional $\\onemax$ function class is $O(n/k)$. This\nindicates that the power of higher arity operators is much stronger than what\nthe previous $O(n/\\log k)$ bound by Doerr et al. (Faster black-box algorithms\nthrough higher arity operators, Proc. of FOGA 2011, pp. 163--172, ACM, 2011)\nsuggests.\n  The key to this result is an encoding strategy, which might be of independent\ninterest. We show that, using $k$-ary unbiased variation operators only, we may\nsimulate an unrestricted memory of size $O(2^k)$ bits."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1203.4881v1", 
    "title": "Computational Complexity Analysis of Multi-Objective Genetic Programming", 
    "arxiv-id": "1203.4881v1", 
    "author": "Frank Neumann", 
    "publish": "2012-03-22T04:22:36Z", 
    "summary": "The computational complexity analysis of genetic programming (GP) has been\nstarted recently by analyzing simple (1+1) GP algorithms for the problems ORDER\nand MAJORITY. In this paper, we study how taking the complexity as an\nadditional criteria influences the runtime behavior. We consider\ngeneralizations of ORDER and MAJORITY and present a computational complexity\nanalysis of (1+1) GP using multi-criteria fitness functions that take into\naccount the original objective and the complexity of a syntax tree as a\nsecondary measure. Furthermore, we study the expected time until\npopulation-based multi-objective genetic programming algorithms have computed\nthe Pareto front when taking the complexity of a syntax tree as an equally\nimportant objective."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256166", 
    "link": "http://arxiv.org/pdf/1203.5028v1", 
    "title": "Hybridizing PSM and RSM Operator for Solving NP-Complete Problems:   Application to Travelling Salesman Problem", 
    "arxiv-id": "1203.5028v1", 
    "author": "Jaafar Abouchabka", 
    "publish": "2012-03-22T16:09:51Z", 
    "summary": "In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),\nfor a genetic algorithm that generates high quality solutions to the Traveling\nSalesman Problem (TSP). The Hybrid Mutation operator constructs an offspring\nfrom a pair of parents by hybridizing two mutation operators, PSM and RSM. The\nefficiency of the HPRM is compared as against some existing mutation operators;\nnamely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for\nBERLIN52 as instance of TSPLIB. Experimental results show that the new mutation\noperator is better than the RSM and PSM."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TEVC.2014.2318025", 
    "link": "http://arxiv.org/pdf/1203.6286v5", 
    "title": "On the Easiest and Hardest Fitness Functions", 
    "arxiv-id": "1203.6286v5", 
    "author": "Xin Yao", 
    "publish": "2012-03-28T15:01:53Z", 
    "summary": "The hardness of fitness functions is an important research topic in the field\nof evolutionary computation. In theory, the study can help understanding the\nability of evolutionary algorithms. In practice, the study may provide a\nguideline to the design of benchmarks. The aim of this paper is to answer the\nfollowing research questions: Given a fitness function class, which functions\nare the easiest with respect to an evolutionary algorithm? Which are the\nhardest? How are these functions constructed? The paper provides theoretical\nanswers to these questions. The easiest and hardest fitness functions are\nconstructed for an elitist (1+1) evolutionary algorithm to maximise a class of\nfitness functions with the same optima. It is demonstrated that the unimodal\nfunctions are the easiest and deceptive functions are the hardest in terms of\nthe time-fitness landscape. The paper also reveals that the easiest fitness\nfunction to one algorithm may become the hardest to another algorithm, and vice\nversa."
},{
    "category": "cs.NE", 
    "doi": "10.1134/S1064562411030197", 
    "link": "http://arxiv.org/pdf/1205.0732v1", 
    "title": "Discretization of a matrix in the problem of quadratic functional binary   minimization", 
    "arxiv-id": "1205.0732v1", 
    "author": "Magomed Malsagov", 
    "publish": "2012-05-03T15:15:21Z", 
    "summary": "The capability of discretization of matrix elements in the problem of\nquadratic functional minimization with linear member built on matrix in\nN-dimensional configuration space with discrete coordinates is researched. It\nis shown, that optimal procedure of replacement matrix elements by the integer\nquantities with the limited number of gradations exist, and the efficient of\nminimization does not reduce. Parameter depends on matrix properties, which\nallows estimate the capability of using described procedure for given type of\nmatrix, is found. Computational complexities of algorithm and RAM requirements\nare reduced by 16 times, correct using of integer elements allows increase\nminimization algorithm speed by the orders."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsea.2012.2204", 
    "link": "http://arxiv.org/pdf/1205.2797v1", 
    "title": "Forecasting of Indian Rupee (INR) / US Dollar (USD) Currency Exchange   Rate Using Artificial Neural Network", 
    "arxiv-id": "1205.2797v1", 
    "author": "Asif Perwej", 
    "publish": "2012-05-12T17:26:49Z", 
    "summary": "A large part of the workforce, and growing every day, is originally from\nIndia. India one of the second largest populations in the world, they have a\nlot to offer in terms of jobs. The sheer number of IT workers makes them a\nformidable travelling force as well, easily picking up employment in English\nspeaking countries. The beginning of the economic crises since 2008 September,\nmany Indians have return homeland, and this has had a substantial impression on\nthe Indian Rupee (INR) as liken to the US Dollar (USD). We are using\nnumerational knowledge based techniques for forecasting has been proved highly\nsuccessful in present time. The purpose of this paper is to examine the effects\nof several important neural network factors on model fitting and forecasting\nthe behaviours. In this paper, Artificial Neural Network has successfully been\nused for exchange rate forecasting. This paper examines the effects of the\nnumber of inputs and hidden nodes and the size of the training sample on the\nin-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar\n(USD) is used for detailed examinations. The number of input nodes has a\ngreater impact on performance than the number of hidden nodes, while a large\nnumber of observations do reduce forecast errors."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsea.2012.2204", 
    "link": "http://arxiv.org/pdf/1206.0730v1", 
    "title": "Theoretical foundation for CMA-ES from information geometric perspective", 
    "arxiv-id": "1206.0730v1", 
    "author": "Shigenobu Kobayashi", 
    "publish": "2012-06-04T17:20:58Z", 
    "summary": "This paper explores the theoretical basis of the covariance matrix adaptation\nevolution strategy (CMA-ES) from the information geometry viewpoint.\n  To establish a theoretical foundation for the CMA-ES, we focus on a geometric\nstructure of a Riemannian manifold of probability distributions equipped with\nthe Fisher metric. We define a function on the manifold which is the\nexpectation of fitness over the sampling distribution, and regard the goal of\nupdate of the parameters of sampling distribution in the CMA-ES as maximization\nof the expected fitness. We investigate the steepest ascent learning for the\nexpected fitness maximization, where the steepest ascent direction is given by\nthe natural gradient, which is the product of the inverse of the Fisher\ninformation matrix and the conventional gradient of the function.\n  Our first result is that we can obtain under some types of parameterization\nof multivariate normal distribution the natural gradient of the expected\nfitness without the need for inversion of the Fisher information matrix. We\nfind that the update of the distribution parameters in the CMA-ES is the same\nas natural gradient learning for expected fitness maximization. Our second\nresult is that we derive the range of learning rates such that a step in the\ndirection of the exact natural gradient improves the parameters in the expected\nfitness. We see from the close relation between the CMA-ES and natural gradient\nlearning that the default setting of learning rates in the CMA-ES seems\nsuitable in terms of monotone improvement in expected fitness. Then, we discuss\nthe relation to the expectation-maximization framework and provide an\ninformation geometric interpretation of the CMA-ES."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsea.2012.2204", 
    "link": "http://arxiv.org/pdf/1206.0974v1", 
    "title": "Black-box optimization benchmarking of IPOP-saACM-ES on the BBOB-2012   noisy testbed", 
    "arxiv-id": "1206.0974v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2012-04-24T06:22:19Z", 
    "summary": "In this paper, we study the performance of IPOP-saACM-ES, recently proposed\nself-adaptive surrogate-assisted Covariance Matrix Adaptation Evolution\nStrategy. The algorithm was tested using restarts till a total number of\nfunction evaluations of $10^6D$ was reached, where $D$ is the dimension of the\nfunction search space. The experiments show that the surrogate model control\nallows IPOP-saACM-ES to be as robust as the original IPOP-aCMA-ES and\noutperforms the latter by a factor from 2 to 3 on 6 benchmark problems with\nmoderate noise. On 15 out of 30 benchmark problems in dimension 20,\nIPOP-saACM-ES exceeds the records observed during BBOB-2009 and BBOB-2010."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.1012v1", 
    "title": "A Hybrid Artificial Bee Colony Algorithm for Graph 3-Coloring", 
    "arxiv-id": "1206.1012v1", 
    "author": "Janez Brest", 
    "publish": "2012-05-06T18:44:15Z", 
    "summary": "The Artificial Bee Colony (ABC) is the name of an optimization algorithm that\nwas inspired by the intelligent behavior of a honey bee swarm. It is widely\nrecognized as a quick, reliable, and efficient methods for solving optimization\nproblems. This paper proposes a hybrid ABC (HABC) algorithm for graph\n3-coloring, which is a well-known discrete optimization problem. The results of\nHABC are compared with results of the well-known graph coloring algorithms of\ntoday, i.e. the Tabucol and Hybrid Evolutionary algorithm (HEA) and results of\nthe traditional evolutionary algorithm with SAW method (EA-SAW). Extensive\nexperimentations has shown that the HABC matched the competitive results of the\nbest graph coloring algorithms, and did better than the traditional heuristics\nEA-SAW when solving equi-partite, flat, and random generated medium-sized\ngraphs."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.1443v1", 
    "title": "On applying Neuro - Computing in E-com Domain", 
    "arxiv-id": "1206.1443v1", 
    "author": "Asif Perwej", 
    "publish": "2012-06-07T10:48:48Z", 
    "summary": "Prior studies have generally suggested that Artificial Neural Networks (ANNs)\nare superior to conventional statistical models in predicting consumer buying\nbehavior. There are, however, contradicting findings which raise question over\nusefulness of ANNs. This paper discusses development of three neural networks\nfor modeling consumer e-commerce behavior and compares the findings to\nequivalent logistic regression models. The results showed that ANNs predict\ne-commerce adoption slightly more accurately than logistic models but this is\nhardly justifiable given the added complexity. Further, ANNs seem to be highly\nadaptive, particularly when a small sample is coupled with a large number of\nnodes in hidden layers which, in turn, limits the neural networks'\ngeneralisability."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.1971v2", 
    "title": "A Connectionist Network Approach to Find Numerical Solutions of   Diophantine Equations", 
    "arxiv-id": "1206.1971v2", 
    "author": "Mukund Sanglikar", 
    "publish": "2012-06-09T20:24:09Z", 
    "summary": "The paper introduces a connectionist network approach to find numerical\nsolutions of Diophantine equations as an attempt to address the famous\nHilbert's tenth problem. The proposed methodology uses a three layer feed\nforward neural network with back propagation as sequential learning procedure\nto find numerical solutions of a class of Diophantine equations. It uses a\ndynamically constructed network architecture where number of nodes in the input\nlayer is chosen based on the number of variables in the equation. The powers of\nthe given Diophantine equation are taken as input to the input layer. The\ntraining of the network starts with initial random integral weights. The\nweights are updated based on the back propagation of the error values at the\noutput layer. The optimization of weights is augmented by adding a momentum\nfactor into the network. The optimized weights of the connection between the\ninput layer and the hidden layer are taken as numerical solution of the given\nDiophantine equation. The procedure is validated using different Diophantine\nEquations of different number of variables and different powers."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.3522v1", 
    "title": "General Upper Bounds on the Running Time of Parallel Evolutionary   Algorithms", 
    "arxiv-id": "1206.3522v1", 
    "author": "Dirk Sudholt", 
    "publish": "2012-06-15T17:28:51Z", 
    "summary": "We present a new method for analyzing the running time of parallel\nevolutionary algorithms with spatially structured populations. Based on the\nfitness-level method, it yields upper bounds on the expected parallel running\ntime. This allows to rigorously estimate the speedup gained by parallelization.\nTailored results are given for common migration topologies: ring graphs, torus\ngraphs, hypercubes, and the complete graph. Example applications for\npseudo-Boolean optimization show that our method is easy to apply and that it\ngives powerful results. In our examples the possible speedup increases with the\ndensity of the topology. Surprisingly, even sparse topologies like ring graphs\nlead to a significant speedup for many functions while not increasing the total\nnumber of function evaluations by more than a constant factor. We also identify\nwhich number of processors yield asymptotically optimal speedups, thus giving\nhints on how to parametrize parallel evolutionary algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.5559v1", 
    "title": "Speeding up the construction of slow adaptive walks", 
    "arxiv-id": "1206.5559v1", 
    "author": "Susan Khor", 
    "publish": "2012-06-25T02:16:01Z", 
    "summary": "An algorithm (bliss) is proposed to speed up the construction of slow\nadaptive walks. Slow adaptive walks are adaptive walks biased towards closer\npoints or smaller move steps. They were previously introduced to explore a\nsearch space, e.g. to detect potential local optima or to assess the ruggedness\nof a fitness landscape. To avoid the quadratic cost of computing Hamming\ndistance (HD) for all-pairs of strings in a set in order to find the set of\nclosest strings for each string, strings are sorted and clustered by bliss such\nthat similar strings are more likely to get paired off for HD computation. To\nefficiently arrange the strings by similarity, bliss employs the idea of shared\nnon-overlapping position specific subsequences between strings which is\ninspired by an alignment-free protein sequence comparison algorithm. Tests are\nperformed to evaluate the quality of b-walks, i.e. slow adaptive walks\nconstructed from the output of bliss, on enumerated search spaces. Finally,\nb-walks are applied to explore larger search spaces with the help of\nWang-Landau sampling."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.5651v2", 
    "title": "Optimization of Real, Hermitian Quadratic Forms: Real, Complex   Hopfield-Amari Neural Network", 
    "arxiv-id": "1206.5651v2", 
    "author": "Bondalapati Nischal", 
    "publish": "2012-06-25T11:30:44Z", 
    "summary": "In this research paper, the problem of optimization of quadratic forms\nassociated with the dynamics of Hopfield-Amari neural network is considered. An\nelegant (and short) proof of the states at which local/global minima of\nquadratic form are attained is provided. A theorem associated with local/global\nminimization of quadratic energy function using the Hopfield-Amari neural\nnetwork is discussed. The results are generalized to a \"Complex Hopfield neural\nnetwork\" dynamics over the complex hypercube (using a \"complex signum\nfunction\"). It is also reasoned through two theorems that there is no loss of\ngenerality in assuming the threshold vector to be a zero vector in the case of\nreal as well as a \"Complex Hopfield neural network\". Some structured quadratic\nforms like Toeplitz form and Complex Toeplitz form are discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1206.5780v1", 
    "title": "Black-box optimization benchmarking of IPOP-saACM-ES and BIPOP-saACM-ES   on the BBOB-2012 noiseless testbed", 
    "arxiv-id": "1206.5780v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2012-04-24T06:23:46Z", 
    "summary": "In this paper, we study the performance of IPOP-saACM-ES and BIPOP-saACM-ES,\nrecently proposed self-adaptive surrogate-assisted Covariance Matrix Adaptation\nEvolution Strategies. Both algorithms were tested using restarts till a total\nnumber of function evaluations of $10^6D$ was reached, where $D$ is the\ndimension of the function search space. We compared surrogate-assisted\nalgorithms with their surrogate-less versions IPOP-saACM-ES and BIPOP-saACM-ES,\ntwo algorithms with one of the best overall performance observed during the\nBBOB-2009 and BBOB-2010. The comparison shows that the surrogate-assisted\nversions outperform the original CMA-ES algorithms by a factor from 2 to 4 on 8\nout of 24 noiseless benchmark problems, showing the best results among all\nalgorithms of the BBOB-2009 and BBOB-2010 on Ellipsoid, Discus, Bent Cigar,\nSharp Ridge and Sum of different powers functions."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.2214v1", 
    "title": "Curved Space Optimization: A Random Search based on General Relativity   Theory", 
    "arxiv-id": "1208.2214v1", 
    "author": "Mohamed Cheriet", 
    "publish": "2012-08-10T16:53:57Z", 
    "summary": "Designing a fast and efficient optimization method with local optima\navoidance capability on a variety of optimization problems is still an open\nproblem for many researchers. In this work, the concept of a new global\noptimization method with an open implementation area is introduced as a Curved\nSpace Optimization (CSO) method, which is a simple probabilistic optimization\nmethod enhanced by concepts of general relativity theory. To address global\noptimization challenges such as performance and convergence, this new method is\ndesigned based on transformation of a random search space into a new search\nspace based on concepts of space-time curvature in general relativity theory.\nIn order to evaluate the performance of our proposed method, an implementation\nof CSO is deployed and its results are compared on benchmark functions with\nstate-of-the art optimization methods. The results show that the performance of\nCSO is promising on unimodal and multimodal benchmark functions with different\nsearch space dimension sizes."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.2345v1", 
    "title": "A Large Population Size Can Be Unhelpful in Evolutionary Algorithms", 
    "arxiv-id": "1208.2345v1", 
    "author": "Xin Yao", 
    "publish": "2012-08-11T13:55:53Z", 
    "summary": "The utilization of populations is one of the most important features of\nevolutionary algorithms (EAs). There have been many studies analyzing the\nimpact of different population sizes on the performance of EAs. However, most\nof such studies are based computational experiments, except for a few cases.\nThe common wisdom so far appears to be that a large population would increase\nthe population diversity and thus help an EA. Indeed, increasing the population\nsize has been a commonly used strategy in tuning an EA when it did not perform\nas well as expected for a given problem. He and Yao (2002) showed theoretically\nthat for some problem instance classes, a population can help to reduce the\nruntime of an EA from exponential to polynomial time. This paper analyzes the\nrole of population further in EAs and shows rigorously that large populations\nmay not always be useful. Conditions, under which large populations can be\nharmful, are discussed in this paper. Although the theoretical analysis was\ncarried out on one multi-modal problem using a specific type of EAs, it has\nmuch wider implications. The analysis has revealed certain problem\ncharacteristics, which can be either the problem considered here or other\nproblems, that lead to the disadvantages of large population sizes. The\nanalytical approach developed in this paper can also be applied to analyzing\nEAs on other problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.2437v1", 
    "title": "An Efficient Genetic Programming System with Geometric Semantic   Operators and its Application to Human Oral Bioavailability Prediction", 
    "arxiv-id": "1208.2437v1", 
    "author": "Leonardo Vanneschi", 
    "publish": "2012-08-12T16:01:54Z", 
    "summary": "Very recently new genetic operators, called geometric semantic operators,\nhave been defined for genetic programming. Contrarily to standard genetic\noperators, which are uniquely based on the syntax of the individuals, these new\noperators are based on their semantics, meaning with it the set of input-output\npairs on training data. Furthermore, these operators present the interesting\nproperty of inducing a unimodal fitness landscape for every problem that\nconsists in finding a match between given input and output data (for instance\nregression and classification). Nevertheless, the current definition of these\noperators has a serious limitation: they impose an exponential growth in the\nsize of the individuals in the population, so their use is impossible in\npractice. This paper is intended to overcome this limitation, presenting a new\ngenetic programming system that implements geometric semantic operators in an\nextremely efficient way. To demonstrate the power of the proposed system, we\nuse it to solve a complex real-life application in the field of\npharmacokinetic: the prediction of the human oral bioavailability of potential\nnew drugs. Besides the excellent performances on training data, which were\nexpected because the fitness landscape is unimodal, we also report an excellent\ngeneralization ability of the proposed system, at least for the studied\napplication. In fact, it outperforms standard genetic programming and a wide\nset of other well-known machine learning methods."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.4009v1", 
    "title": "Learning sparse messages in networks of neural cliques", 
    "arxiv-id": "1208.4009v1", 
    "author": "Xiaoran Jiang", 
    "publish": "2012-08-20T13:54:26Z", 
    "summary": "An extension to a recently introduced binary neural network is proposed in\norder to allow the learning of sparse messages, in large numbers and with high\nmemory efficiency. This new network is justified both in biological and\ninformational terms. The learning and retrieval rules are detailed and\nillustrated by various simulation results."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.4583v1", 
    "title": "A novel Hopfield neural network approach for minimizing total weighted   tardiness of jobs scheduled on identical machines", 
    "arxiv-id": "1208.4583v1", 
    "author": "J. Levendovszky", 
    "publish": "2012-07-26T19:18:29Z", 
    "summary": "This paper explores fast, polynomial time heuristic approximate solutions to\nthe NP-hard problem of scheduling jobs on N identical machines. The jobs are\nindependent and are allowed to be stopped and restarted on another machine at a\nlater time. They have well-defined deadlines, and relative priorities\nquantified by non-negative real weights. The objective is to find schedules\nwhich minimize the total weighted tardiness (TWT) of all jobs. We show how this\nproblem can be mapped into quadratic form and present a polynomial time\nheuristic solution based on the Hopfield Neural Network (HNN) approach. It is\ndemonstrated, through the results of extensive numerical simulations, that this\nsolution outperforms other popular heuristic methods. The proposed heuristic is\nboth theoretically and empirically shown to be scalable to large problem sizes\n(over 100 jobs to be scheduled), which makes it applicable to grid computing\nscheduling, arising in fields such as computational biology, chemistry and\nfinance."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.6025v1", 
    "title": "Feasibility of Genetic Algorithm for Textile Defect Classification Using   Neural Network", 
    "arxiv-id": "1208.6025v1", 
    "author": "M. Rokonuzzaman", 
    "publish": "2012-08-11T21:45:46Z", 
    "summary": "The global market for textile industry is highly competitive nowadays.\nQuality control in production process in textile industry has been a key factor\nfor retaining existence in such competitive market. Automated textile\ninspection systems are very useful in this respect, because manual inspection\nis time consuming and not accurate enough. Hence, automated textile inspection\nsystems have been drawing plenty of attention of the researchers of different\ncountries in order to replace manual inspection. Defect detection and defect\nclassification are the two major problems that are posed by the research of\nautomated textile inspection systems. In this paper, we perform an extensive\ninvestigation on the applicability of genetic algorithm (GA) in the context of\ntextile defect classification using neural network (NN). We observe the effect\nof tuning different network parameters and explain the reasons. We empirically\nfind a suitable NN model in the context of textile defect classification. We\ncompare the performance of this model with that of the classification models\nimplemented by others."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1208.6028v1", 
    "title": "Design of Low Noise Amplifiers Using Particle Swarm Optimization", 
    "arxiv-id": "1208.6028v1", 
    "author": "Sadik Ulker", 
    "publish": "2012-08-13T11:32:03Z", 
    "summary": "This short paper presents a work on the design of low noise microwave\namplifiers using particle swarm optimization (PSO) technique. Particle Swarm\nOptimization is used as a method that is applied to a single stage amplifier\ncircuit to meet two criteria: desired gain and desired low noise. The aim is to\nget the best optimized design using the predefined constraints for gain and low\nnoise values. The code is written to apply the algorithm to meet the desired\ngoals and the obtained results are verified using different simulators. The\nresults obtained show that PSO can be applied very efficiently for this kind of\ndesign problems with multiple constraints."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-29353-5_8", 
    "link": "http://arxiv.org/pdf/1209.0167v3", 
    "title": "Automatic ECG Beat Arrhythmia Detection", 
    "arxiv-id": "1209.0167v3", 
    "author": "M. Abedi", 
    "publish": "2012-09-02T11:29:03Z", 
    "summary": "Background: In recent years automated data analysis techniques have drawn\ngreat attention and are used in almost every field of research including\nbiomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided-\nDiagnosis tools which are used extensively by advances in computer hardware\ntechnology. The application of these techniques for disease diagnosis has made\ngreat progress and is widely used by physicians. An Electrocardiogram carries\nvital information about heart activity and physicians use this signal for\ncardiac disease diagnosis which was the great motivation towards our study.\nMethods: In this study we are using Probabilistic Neural Networks (PNN) as an\nautomatic technique for ECG signal analysis along with a Genetic Algorithm\n(GA). As every real signal recorded by the equipment can have different\nartifacts, we need to do some preprocessing steps before feeding it to the ANN.\nWavelet transform is used for extracting the morphological parameters and\nmedian filter for data reduction of the ECG signal. The subset of morphological\nparameters are chosen and optimized using GA. We had two approaches in our\ninvestigation, the first one uses the whole signal with 289 normalized and\nde-noised data points as input to the ANN. In the second approach after\napplying all the preprocessing steps the signal is reduced to 29 data points\nand also their important parameters extracted to form the ANN input with 35\ndata points. Results: The outcome of the two approaches for 8 types of\narrhythmia shows that the second approach is superior than the first one with\nan average accuracy of %99.42."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1209.2717v1", 
    "title": "Comparison Study for Clonal Selection Algorithm and Genetic Algorithm", 
    "arxiv-id": "1209.2717v1", 
    "author": "Sadik Ulker", 
    "publish": "2012-09-12T20:17:05Z", 
    "summary": "Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and\nGenetic Algorithms are classified as computational systems inspired by\ntheoretical immunology and genetics mechanisms. In this work we examine the\ncomparative performances of two algorithms. A special selection algorithm,\nClonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune\nSystems, and Genetic Algorithms are tested with certain benchmark functions. It\nis shown that depending on type of a function Clonal Selection Algorithm and\nGenetic Algorithm have better performance over each other."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1209.4855v1", 
    "title": "The Future of Neural Networks", 
    "arxiv-id": "1209.4855v1", 
    "author": "G. Ramakrishna", 
    "publish": "2012-09-20T14:14:59Z", 
    "summary": "The paper describes some recent developments in neural networks and discusses\nthe applicability of neural networks in the development of a machine that\nmimics the human brain. The paper mentions a new architecture, the pulsed\nneural network that is being considered as the next generation of neural\nnetworks. The paper also explores the use of memristors in the development of a\nbrain-like computer called the MoNETA. A new model, multi/infinite dimensional\nneural networks, are a recent development in the area of advanced neural\nnetworks. The paper concludes that the need of neural networks in the\ndevelopment of human-like technology is essential and may be non-expendable for\nit."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1209.4895v1", 
    "title": "A Neuro-Fuzzy Technique for Implementing the Half-Adder Circuit Using   the CANFIS Model", 
    "arxiv-id": "1209.4895v1", 
    "author": "Anubhav Sharma", 
    "publish": "2012-09-20T13:52:32Z", 
    "summary": "A Neural Network, in general, is not considered to be a good solver of\nmathematical and binary arithmetic problems. However, networks have been\ndeveloped for such problems as the XOR circuit. This paper presents a technique\nfor the implementation of the Half-adder circuit using the CoActive Neuro-Fuzzy\nInference System (CANFIS) Model and attempts to solve the problem using the\nNeuroSolutions 5 Simulator. The paper gives the experimental results along with\nthe interpretations and possible applications of the technique."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1209.5218v2", 
    "title": "A New Continuous-Time Equality-Constrained Optimization Method to Avoid   Singularity", 
    "arxiv-id": "1209.5218v2", 
    "author": "Kai-Yuan Cai", 
    "publish": "2012-09-24T10:26:19Z", 
    "summary": "In equality-constrained optimization, a standard regularity assumption is\noften associated with feasible point methods, namely the gradients of\nconstraints are linearly independent. In practice, the regularity assumption\nmay be violated. To avoid such a singularity, we propose a new projection\nmatrix, based on which a feasible point method for the continuous-time,\nequality-constrained optimization problem is developed. First, the equality\nconstraint is transformed into a continuous-time dynamical system with\nsolutions that always satisfy the equality constraint. Then, the singularity is\nexplained in detail and a new projection matrix is proposed to avoid\nsingularity. An update (or say a controller) is subsequently designed to\ndecrease the objective function along the solutions of the transformed system.\nThe invariance principle is applied to analyze the behavior of the solution. We\nalso propose a modified approach for addressing cases in which solutions do not\nsatisfy the equality constraint. Finally, the proposed optimization approaches\nare applied to two examples to demonstrate its effectiveness."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1209.5339v1", 
    "title": "Developing Improved Greedy Crossover to Solve Symmetric Traveling   Salesman Problem", 
    "arxiv-id": "1209.5339v1", 
    "author": "Kamran Zamanifar", 
    "publish": "2012-09-24T17:26:29Z", 
    "summary": "The Traveling Salesman Problem (TSP) is one of the most famous optimization\nproblems. Greedy crossover designed by Greffenstette et al, can be used while\nSymmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have\nproposed several versions of greedy crossover. Here we propose improved version\nof it. We compare our greedy crossover with some of recent crossovers, we use\nour greedy crossover and some recent crossovers in GA then compare crossovers\non speed and accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2012.4410", 
    "link": "http://arxiv.org/pdf/1210.0118v1", 
    "title": "Self-Delimiting Neural Networks", 
    "arxiv-id": "1210.0118v1", 
    "author": "Juergen Schmidhuber", 
    "publish": "2012-09-29T15:38:53Z", 
    "summary": "Self-delimiting (SLIM) programs are a central concept of theoretical computer\nscience, particularly algorithmic information & probability theory, and\nasymptotically optimal program search (AOPS). To apply AOPS to (possibly\nrecurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical\nSLIM NN have threshold activation functions. During a computational episode,\nactivations are spreading from input neurons through the SLIM NN until the\ncomputation activates a special halt neuron. Weights of the NN's used\nconnections define its program. Halting programs form a prefix code. The reset\nof the initial NN state does not cost more than the latest program execution.\nSince prefixes of SLIM programs influence their suffixes (weight changes\noccurring early in an episode influence which weights are considered later),\nSLIM NN learning algorithms (LAs) should execute weight changes online during\nactivation spreading. This can be achieved by applying AOPS to growing SLIM\nNNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly\nclassifying many different patterns, or solving many different robot control\ntasks, each connection keeps a list of tasks it is used for. The lists may be\nefficiently updated during training. To evaluate the overall effect of\ncurrently tested weight changes, a SLIM NN LA needs to re-test performance only\non the efficiently computable union of tasks potentially affected by the\ncurrent weight changes. Future SLIM NNs will be implemented on 3-dimensional\nbrain-like multi-processor hardware. Their LAs will minimize task-specific\ntotal wire length of used connections, to encourage efficient solutions of\nsubtasks by subsets of neurons that are physically close. The novel class of\nSLIM NN LAs is currently being probed in ongoing experiments to be reported in\nseparate papers."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-37213-1_12", 
    "link": "http://arxiv.org/pdf/1210.3210v2", 
    "title": "Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms", 
    "arxiv-id": "1210.3210v2", 
    "author": "Martyn Amos", 
    "publish": "2012-10-11T12:40:36Z", 
    "summary": "A significant challenge in nature-inspired algorithmics is the identification\nof specific characteristics of problems that make them harder (or easier) to\nsolve using specific methods. The hope is that, by identifying these\ncharacteristics, we may more easily predict which algorithms are best-suited to\nproblems sharing certain features. Here, we approach this problem using fitness\nlandscape analysis. Techniques already exist for measuring the \"difficulty\" of\nspecific landscapes, but these are often designed solely with evolutionary\nalgorithms in mind, and are generally specific to discrete optimisation. In\nthis paper we develop an approach for comparing a wide range of continuous\noptimisation algorithms. Using a fitness landscape generation technique, we\ncompare six different nature-inspired algorithms and identify which methods\nperform best on landscapes exhibiting specific features."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-37213-1_12", 
    "link": "http://arxiv.org/pdf/1210.3569v2", 
    "title": "Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics", 
    "arxiv-id": "1210.3569v2", 
    "author": "Yulia Sandamirskaya", 
    "publish": "2012-10-12T16:41:58Z", 
    "summary": "We introduce a dynamic neural algorithm called Dynamic Neural (DN)\nSARSA(\\lambda) for learning a behavioral sequence from delayed reward.\nDN-SARSA(\\lambda) combines Dynamic Field Theory models of behavioral sequence\nrepresentation, classical reinforcement learning, and a computational\nneuroscience model of working memory, called Item and Order working memory,\nwhich serves as an eligibility trace. DN-SARSA(\\lambda) is implemented on both\na simulated and real robot that must learn a specific rewarding sequence of\nelementary behaviors from exploration. Results show DN-SARSA(\\lambda) performs\non the level of the discrete SARSA(\\lambda), validating the feasibility of\ngeneral reinforcement learning without compromising neural dynamics."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-37213-1_12", 
    "link": "http://arxiv.org/pdf/1210.4502v1", 
    "title": "Comparing several heuristics for a packing problem", 
    "arxiv-id": "1210.4502v1", 
    "author": "Mara Hajdu-Macelaru", 
    "publish": "2012-10-10T11:08:40Z", 
    "summary": "Packing problems are in general NP-hard, even for simple cases. Since now\nthere are no highly efficient algorithms available for solving packing\nproblems. The two-dimensional bin packing problem is about packing all given\nrectangular items, into a minimum size rectangular bin, without overlapping.\nThe restriction is that the items cannot be rotated. The current paper is\ncomparing a greedy algorithm with a hybrid genetic algorithm in order to see\nwhich technique is better for the given problem. The algorithms are tested on\ndifferent sizes data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-37213-1_12", 
    "link": "http://arxiv.org/pdf/1211.0660v1", 
    "title": "Generation of Two-Layer Monotonic Functions", 
    "arxiv-id": "1211.0660v1", 
    "author": "Kiyonori Miyasaki", 
    "publish": "2012-11-04T04:49:24Z", 
    "summary": "The problem of implementing a class of functions with particular conditions\nby using monotonic multilayer functions is considered. A genetic algorithm is\nused to create monotonic functions of a certain class, and these are\nimplemented with two-layer monotonic functions. The existence of a solution to\nthe given problem suggests that from two monotone functions, a monotonic\nfunction with the same dimensions can be created. A new algorithm based on the\ngenetic algorithm is proposed, which easily implemented two-layer monotonic\nfunctions of a specific class for up to six variables."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-37213-1_12", 
    "link": "http://arxiv.org/pdf/1211.0730v1", 
    "title": "Intelligent Algorithm for Optimum Solutions Based on the Principles of   Bat Sonar", 
    "arxiv-id": "1211.0730v1", 
    "author": "Mohammed Ali Tawfeeq", 
    "publish": "2012-11-04T22:42:39Z", 
    "summary": "This paper presents a new intelligent algorithm that can solve the problems\nof finding the optimum solution in the state space among which the desired\nsolution resides. The algorithm mimics the principles of bat sonar in finding\nits targets. The algorithm introduces three search approaches. The first search\napproach considers a single sonar unit (SSU) with a fixed beam length and a\nsingle starting point. In this approach, although the results converge toward\nthe optimum fitness, it is not guaranteed to find the global optimum solution\nespecially for complex problems; it is satisfied with finding 'acceptably good'\nsolutions to these problems. The second approach considers multisonar units\n(MSU) working in parallel in the same state space. Each unit has its own\nstarting point and tries to find the optimum solution. In this approach the\nprobability that the algorithm converges toward the optimum solution is\nsignificantly increased. It is found that this approach is suitable for complex\nfunctions and for problems of wide state space. In the third approach, a single\nsonar unit with a moment (SSM) is used in order to handle the problem of\nconvergence toward a local optimum rather than a global optimum. The momentum\nterm is added to the length of the transmitted beams. This will give the chance\nto find the best fitness in a wider range within the state space. In this paper\na comparison between the proposed algorithm and genetic algorithm (GA) has been\nmade. It showed that both of the algorithms can catch approximately the optimum\nsolutions for all of the testbed functions except for the function that has a\nlocal minimum, in which the proposed algorithm's result is much better than\nthat of the GA algorithm. On the other hand, the comparison showed that the\nrequired execution time to obtain the optimum solution using the proposed\nalgorithm is much less than that of the GA algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-014-9416-y", 
    "link": "http://arxiv.org/pdf/1211.1119v1", 
    "title": "A Survey on Techniques of Improving Generalization Ability of Genetic   Programming Solutions", 
    "arxiv-id": "1211.1119v1", 
    "author": "Sanjay Chaudhary", 
    "publish": "2012-11-06T06:15:28Z", 
    "summary": "In the field of empirical modeling using Genetic Programming (GP), it is\nimportant to evolve solution with good generalization ability. Generalization\nability of GP solutions get affected by two important issues: bloat and\nover-fitting. We surveyed and classified existing literature related to\ndifferent techniques used by GP research community to deal with these issues.\nWe also point out limitation of these techniques, if any. Moreover, the\nclassification of different bloat control approaches and measures for bloat and\nover-fitting are also discussed. We believe that this work will be useful to GP\npractitioners in following ways: (i) to better understand concepts of\ngeneralization in GP (ii) comparing existing bloat and over-fitting control\ntechniques and (iii) selecting appropriate approach to improve generalization\nability of GP evolved solutions."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-014-9416-y", 
    "link": "http://arxiv.org/pdf/1211.1733v1", 
    "title": "Linear Antenna Array with Suppressed Sidelobe and Sideband Levels using   Time Modulation", 
    "arxiv-id": "1211.1733v1", 
    "author": "Subrata Mitra", 
    "publish": "2012-11-08T00:29:11Z", 
    "summary": "In this paper, the goal is to achieve an ultra low sidelobe level (SLL) and\nsideband levels (SBL) of a time modulated linear antenna array. The approach\nfollowed here is not to give fixed level of excitation to the elements of an\narray, but to change it dynamically with time. The excitation levels of the\ndifferent array elements over time are varied to get the low sidelobe and\nsideband levels. The mathematics of getting the SLL and SBL furnished in detail\nand simulation is done using the mathematical results. The excitation pattern\nover time is optimized using Genetic Algorithm (GA). Since, the amplitudes of\nthe excitations of the elements are varied within a finite limit, results show\nit gives better sidelobe and sideband suppression compared to previous time\nmodulated arrays with uniform amplitude excitations."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-014-9416-y", 
    "link": "http://arxiv.org/pdf/1211.2361v2", 
    "title": "Genetic Algorithm for Designing a Convenient Facility Layout for a   Circular Flow Path", 
    "arxiv-id": "1211.2361v2", 
    "author": "Mohammad Modarres", 
    "publish": "2012-11-11T00:26:22Z", 
    "summary": "In this paper, we present a heuristic for designing facility layouts that are\nconvenient for designing a unidirectional loop for material handling. We use\ngenetic algorithm where the objective function and crossover and mutation\noperators have all been designed specifically for this purpose. Our design is\nmade under flexible bay structure and comparisons are made with other layouts\nfrom the literature that were designed under flexible bay structure."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-014-9416-y", 
    "link": "http://arxiv.org/pdf/1211.3451v1", 
    "title": "Memory Capacity of a Random Neural Network", 
    "arxiv-id": "1211.3451v1", 
    "author": "Matt Stowe", 
    "publish": "2012-11-14T22:28:56Z", 
    "summary": "This paper considers the problem of information capacity of a random neural\nnetwork. The network is represented by matrices that are square and\nsymmetrical. The matrices have a weight which determines the highest and lowest\npossible value found in the matrix. The examined matrices are randomly\ngenerated and analyzed by a computer program. We find the surprising result\nthat the capacity of the network is a maximum for the binary random neural\nnetwork and it does not change as the number of quantization levels associated\nwith the weights increases."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pone.0048710", 
    "link": "http://arxiv.org/pdf/1211.3845v1", 
    "title": "A Bayesian Interpretation of the Particle Swarm Optimization and Its   Kernel Extension", 
    "arxiv-id": "1211.3845v1", 
    "author": "Peter Andras", 
    "publish": "2012-11-16T10:09:52Z", 
    "summary": "Particle swarm optimization is a popular method for solving difficult\noptimization problems. There have been attempts to formulate the method in\nformal probabilistic or stochastic terms (e.g. bare bones particle swarm) with\nthe aim to achieve more generality and explain the practical behavior of the\nmethod. Here we present a Bayesian interpretation of the particle swarm\noptimization. This interpretation provides a formal framework for incorporation\nof prior knowledge about the problem that is being solved. Furthermore, it also\nallows to extend the particle optimization method through the use of kernel\nfunctions that represent the intermediary transformation of the data into a\ndifferent space where the optimization problem is expected to be easier to be\nresolved, such transformation can be seen as a form of prior knowledge about\nthe nature of the optimization problem. We derive from the general Bayesian\nformulation the commonly used particle swarm methods as particular cases."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.06.008", 
    "link": "http://arxiv.org/pdf/1211.4520v2", 
    "title": "Storing cycles in Hopfield-type networks with pseudoinverse learning   rule: admissibility and network topology", 
    "arxiv-id": "1211.4520v2", 
    "author": "Iuliana Oprea", 
    "publish": "2012-11-19T17:45:25Z", 
    "summary": "Cyclic patterns of neuronal activity are ubiquitous in animal nervous\nsystems, and partially responsible for generating and controlling rhythmic\nmovements such as locomotion, respiration, swallowing and so on. Clarifying the\nrole of the network connectivities for generating cyclic patterns is\nfundamental for understanding the generation of rhythmic movements. In this\npaper, the storage of binary cycles in neural networks is investigated. We call\na cycle $\\Sigma$ admissible if a connectivity matrix satisfying the cycle's\ntransition conditions exists, and construct it using the pseudoinverse learning\nrule. Our main focus is on the structural features of admissible cycles and\ncorresponding network topology. We show that $\\Sigma$ is admissible if and only\nif its discrete Fourier transform contains exactly $r={rank}(\\Sigma)$ nonzero\ncolumns. Based on the decomposition of the rows of $\\Sigma$ into loops, where a\nloop is the set of all cyclic permutations of a row, cycles are classified as\nsimple cycles, separable or inseparable composite cycles. Simple cycles contain\nrows from one loop only, and the network topology is a feedforward chain with\nfeedback to one neuron if the loop-vectors in $\\Sigma$ are cyclic permutations\nof each other. Composite cycles contain rows from at least two disjoint loops,\nand the neurons corresponding to the rows in $\\Sigma$ from the same loop are\nidentified with a cluster. Networks constructed from separable composite cycles\ndecompose into completely isolated clusters. For inseparable composite cycles\nat least two clusters are connected, and the cluster-connectivity is related to\nthe intersections of the spaces spanned by the loop-vectors of the clusters.\nSimulations showing successfully retrieved cycles in continuous-time\nHopfield-type networks and in networks of spiking neurons are presented."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.06.008", 
    "link": "http://arxiv.org/pdf/1211.4971v1", 
    "title": "A Hybrid Bacterial Foraging Algorithm For Solving Job Shop Scheduling   Problems", 
    "arxiv-id": "1211.4971v1", 
    "author": "T. Amudha", 
    "publish": "2012-11-21T09:12:12Z", 
    "summary": "Bio-Inspired computing is the subset of Nature-Inspired computing. Job Shop\nScheduling Problem is categorized under popular scheduling problems. In this\nresearch work, Bacterial Foraging Optimization was hybridized with Ant Colony\nOptimization and a new technique Hybrid Bacterial Foraging Optimization for\nsolving Job Shop Scheduling Problem was proposed. The optimal solutions\nobtained by proposed Hybrid Bacterial Foraging Optimization algorithms are much\nbetter when compared with the solutions obtained by Bacterial Foraging\nOptimization algorithm for well-known test problems of different sizes. From\nthe implementation of this research work, it could be observed that the\nproposed Hybrid Bacterial Foraging Optimization was effective than Bacterial\nForaging Optimization algorithm in solving Job Shop Scheduling Problems. Hybrid\nBacterial Foraging Optimization is used to implement real world Job Shop\nScheduling Problems."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.06.008", 
    "link": "http://arxiv.org/pdf/1211.5400v1", 
    "title": "Ecosystem-Oriented Distributed Evolutionary Computing", 
    "arxiv-id": "1211.5400v1", 
    "author": "Philippe De Wilde", 
    "publish": "2012-11-23T01:04:26Z", 
    "summary": "We create a novel optimisation technique inspired by natural ecosystems,\nwhere the optimisation works at two levels: a first optimisation, migration of\ngenes which are distributed in a peer-to-peer network, operating continuously\nin time; this process feeds a second optimisation based on evolutionary\ncomputing that operates locally on single peers and is aimed at finding\nsolutions to satisfy locally relevant constraints. We consider from the domain\nof computer science distributed evolutionary computing, with the relevant\ntheory from the domain of theoretical biology, including the fields of\nevolutionary and ecological theory, the topological structure of ecosystems,\nand evolutionary processes within distributed environments. We then define\necosystem- oriented distributed evolutionary computing, imbibed with the\nproperties of self-organisation, scalability and sustainability from natural\necosystems, including a novel form of distributed evolu- tionary computing.\nFinally, we conclude with a discussion of the apparent compromises resulting\nfrom the hybrid model created, such as the network topology."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.06.008", 
    "link": "http://arxiv.org/pdf/1211.7184v1", 
    "title": "Erratum: Simplified Drift Analysis for Proving Lower Bounds in   Evolutionary Computation", 
    "arxiv-id": "1211.7184v1", 
    "author": "Carsten Witt", 
    "publish": "2012-11-30T08:49:46Z", 
    "summary": "This erratum points out an error in the simplified drift theorem (SDT)\n[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification\nof one of its conditions is sufficient to establish a valid result. In many\nrespects, the new theorem is more general than before. We no longer assume a\nMarkov process nor a finite search space. Furthermore, the proof of the theorem\nis more compact than the previous ones. Finally, previous applications of the\nSDT are revisited. It turns out that all of these either meet the modified\ncondition directly or by means of few additional arguments."
},{
    "category": "cs.NE", 
    "doi": "10.5120/9380-3731", 
    "link": "http://arxiv.org/pdf/1212.0215v1", 
    "title": "Artificial Neural Network for Performance Modeling and Optimization of   CMOS Analog Circuits", 
    "arxiv-id": "1212.0215v1", 
    "author": "Mriganka Chakraborty", 
    "publish": "2012-12-02T15:07:56Z", 
    "summary": "This paper presents an implementation of multilayer feed forward neural\nnetworks (NN) to optimize CMOS analog circuits. For modeling and design\nrecently neural network computational modules have got acceptance as an\nunorthodox and useful tool. To achieve high performance of active or passive\ncircuit component neural network can be trained accordingly. A well trained\nneural network can produce more accurate outcome depending on its learning\ncapability. Neural network model can replace empirical modeling solutions\nlimited by range and accuracy.[2] Neural network models are easy to obtain for\nnew circuits or devices which can replace analytical methods. Numerical\nmodeling methods can also be replaced by neural network model due to their\ncomputationally expansive behavior.[2][10][20]. The pro- posed implementation\nis aimed at reducing resource requirement, without much compromise on the\nspeed. The NN ensures proper functioning by assigning the appropriate inputs,\nweights, biases, and excitation function of the layer that is currently being\ncomputed. The concept used is shown to be very effective in reducing resource\nrequirements and enhancing speed."
},{
    "category": "cs.NE", 
    "doi": "10.5120/9380-3731", 
    "link": "http://arxiv.org/pdf/1212.0639v1", 
    "title": "Evaluation of Particle Swarm Optimization Algorithms for Weighted   Max-Sat Problem: Technical Report", 
    "arxiv-id": "1212.0639v1", 
    "author": "Osama Khalil", 
    "publish": "2012-12-04T08:48:29Z", 
    "summary": "An experimental evaluation is conducted to asses the performance of 4\ndifferent Particle Swarm Optimization neighborhood structures in solving\nMax-Sat problem. The experiment has shown that none of the algorithms achieves\nstatistically significant performance over the others under confidence level of\n0.05."
},{
    "category": "cs.NE", 
    "doi": "10.5120/9749-3332", 
    "link": "http://arxiv.org/pdf/1212.1752v1", 
    "title": "Hybrid Optimized Back propagation Learning Algorithm For Multi-layer   Perceptron", 
    "arxiv-id": "1212.1752v1", 
    "author": "Arka Ghosh", 
    "publish": "2012-12-08T02:47:40Z", 
    "summary": "Standard neural network based on general back propagation learning using\ndelta method or gradient descent method has some great faults like poor\noptimization of error-weight objective function, low learning rate, instability\n.This paper introduces a hybrid supervised back propagation learning algorithm\nwhich uses trust-region method of unconstrained optimization of the error\nobjective function by using quasi-newton method .This optimization leads to\nmore accurate weight update system for minimizing the learning error during\nlearning phase of multi-layer perceptron.[13][14][15] In this paper augmented\nline search is used for finding points which satisfies Wolfe condition. In this\npaper, This hybrid back propagation algorithm has strong global convergence\nproperties & is robust & efficient in practice."
},{
    "category": "cs.NE", 
    "doi": "10.5120/9749-3332", 
    "link": "http://arxiv.org/pdf/1212.5250v1", 
    "title": "A genetic algorithm applied to the validation of building thermal models", 
    "arxiv-id": "1212.5250v1", 
    "author": "Alain Bastide", 
    "publish": "2012-12-18T07:27:27Z", 
    "summary": "This paper presents the coupling of a building thermal simulation code with\ngenetic algorithms (GAs). GAs are randomized search algorithms that are based\non the mechanisms of natural selection and genetics. We show that this coupling\nallows the location of defective sub-models of a building thermal model i.e.\nparts of model that are responsible for the disagreements between measurements\nand model predictions. The method first of all is checked and validated on the\nbasis of a numerical model of a building taken as reference. It is then applied\nto a real building case. The results show that the method could constitute an\nefficient tool when checking the model validity."
},{
    "category": "cs.NE", 
    "doi": "10.5120/9749-3332", 
    "link": "http://arxiv.org/pdf/1302.0324v1", 
    "title": "A New Constructive Method to Optimize Neural Network Architecture and   Generalization", 
    "arxiv-id": "1302.0324v1", 
    "author": "Moon Ho Lee", 
    "publish": "2013-02-02T00:43:42Z", 
    "summary": "In this paper, after analyzing the reasons of poor generalization and\noverfitting in neural networks, we consider some noise data as a singular value\nof a continuous function - jump discontinuity point. The continuous part can be\napproximated with the simplest neural networks, which have good generalization\nperformance and optimal network architecture, by traditional algorithms such as\nconstructive algorithm for feed-forward neural networks with incremental\ntraining, BP algorithm, ELM algorithm, various constructive algorithm, RBF\napproximation and SVM. At the same time, we will construct RBF neural networks\nto fit the singular value with every error in, and we prove that a function\nwith jumping discontinuity points can be approximated by the simplest neural\nnetworks with a decay RBF neural networks in by each error, and a function with\njumping discontinuity point can be constructively approximated by a decay RBF\nneural networks in by each error and the constructive part have no\ngeneralization influence to the whole machine learning system which will\noptimize neural network architecture and generalization performance, reduce the\noverfitting phenomenon by avoid fitting the noisy data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32711-7_6", 
    "link": "http://arxiv.org/pdf/1302.0797v1", 
    "title": "Comparison of Ant-Inspired Gatherer Allocation Approaches using   Memristor-Based Environmental Models", 
    "arxiv-id": "1302.0797v1", 
    "author": "Andrew Adamatzky", 
    "publish": "2013-02-04T18:50:14Z", 
    "summary": "Memristors are used to compare three gathering techniques in an\nalready-mapped environment where resource locations are known. The All Site\nmodel, which apportions gatherers based on the modeled memristance of that\npath, proves to be good at increasing overall efficiency and decreasing time to\nfully deplete an environment, however it only works well when the resources are\nof similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour,\nassigns all gatherers first to the best resource, and once depleted, uses the\nAll Site model to spread them out amongst the rest. The Leaf Cutter model is\nbetter at increasing resource influx in the short-term and vastly out-performs\nthe All Site model in a more varied environments. It is demonstrated that\nmemristor based abstractions of gatherer models provide potential methods for\nboth the comparison and implementation of agent controls."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32711-7_6", 
    "link": "http://arxiv.org/pdf/1302.1156v2", 
    "title": "A Non-Binary Associative Memory with Exponential Pattern Retrieval   Capacity and Iterative Learning: Extended Results", 
    "arxiv-id": "1302.1156v2", 
    "author": "Amin Shokrollahi", 
    "publish": "2013-02-05T19:14:39Z", 
    "summary": "We consider the problem of neural association for a network of non-binary\nneurons. Here, the task is to first memorize a set of patterns using a network\nof neurons whose states assume values from a finite number of integer levels.\nLater, the same network should be able to recall previously memorized patterns\nfrom their noisy versions. Prior work in this area consider storing a finite\nnumber of purely random patterns, and have shown that the pattern retrieval\ncapacities (maximum number of patterns that can be memorized) scale only\nlinearly with the number of neurons in the network.\n  In our formulation of the problem, we concentrate on exploiting redundancy\nand internal structure of the patterns in order to improve the pattern\nretrieval capacity. Our first result shows that if the given patterns have a\nsuitable linear-algebraic structure, i.e. comprise a sub-space of the set of\nall possible patterns, then the pattern retrieval capacity is in fact\nexponential in terms of the number of neurons. The second result extends the\nprevious finding to cases where the patterns have weak minor components, i.e.\nthe smallest eigenvalues of the correlation matrix tend toward zero. We will\nuse these minor components (or the basis vectors of the pattern null space) to\nboth increase the pattern retrieval capacity and error correction capabilities.\n  An iterative algorithm is proposed for the learning phase, and two simple\nneural update algorithms are presented for the recall phase. Using analytical\nresults and simulations, we show that the proposed methods can tolerate a fair\namount of errors in the input while being able to memorize an exponentially\nlarge number of patterns."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32711-7_6", 
    "link": "http://arxiv.org/pdf/1302.3541v1", 
    "title": "An analysis of NK and generalized NK landscapes", 
    "arxiv-id": "1302.3541v1", 
    "author": "Jeffrey Dinitz", 
    "publish": "2013-02-14T20:30:46Z", 
    "summary": "Simulated landscapes have been used for decades to evaluate search strategies\nwhose goal is to find the landscape location with maximum fitness. Applications\ninclude modeling the capacity of enzymes to catalyze reactions and the clinical\neffectiveness of medical treatments. Understanding properties of landscapes is\nimportant for understanding search difficulty. This paper presents a novel and\ntransparent characterization of NK landscapes.\n  We prove that NK landscapes can be represented by parametric linear\ninteraction models where model coefficients have meaningful interpretations. We\nderive the statistical properties of the model coefficients, providing insight\ninto how the NK algorithm parses importance to main effects and interactions.\nAn important insight derived from the linear model representation is that the\nrank of the linear model defined by the NK algorithm is correlated with the\nnumber of local optima, a strong determinant of landscape complexity and search\ndifficulty. We show that the maximal rank for an NK landscape is achieved\nthrough epistatic interactions that form partially balanced incomplete block\ndesigns. Finally, an analytic expression representing the expected number of\nlocal optima on the landscape is derived, providing a way to quickly compute\nthe expected number of local optima for very large landscapes."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32711-7_6", 
    "link": "http://arxiv.org/pdf/1302.6310v1", 
    "title": "Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining   Techniques", 
    "arxiv-id": "1302.6310v1", 
    "author": "O. Osibanjo", 
    "publish": "2013-02-26T04:22:44Z", 
    "summary": "Industrial pollution is often considered to be one of the prime factors\ncontributing to air, water and soil pollution. Sectoral pollution loads\n(ton/yr) into different media (i.e. air, water and land) in Lagos were\nestimated using Industrial Pollution Projected System (IPPS). These were\nfurther studied using Artificial neural Networks (ANNs), a data mining\ntechnique that has the ability of detecting and describing patterns in large\ndata sets with variables that are non- linearly related. Time Lagged Recurrent\nNetwork (TLRN) appeared as the best Neural Network model among all the neural\nnetworks considered which includes Multilayer Perceptron (MLP) Network,\nGeneralized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)\nNetwork and Recurrent Network (RN). TLRN modelled the data-sets better than the\nothers in terms of the mean average error (MAE) (0.14), time (39 s) and linear\ncorrelation coefficient (0.84). The results showed that Artificial Neural\nNetworks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also\napplicable and effective in environmental assessment study. Keywords:\nArtificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution\nProjection System (IPPS), Pollution load, Pollution Intensity."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32711-7_6", 
    "link": "http://arxiv.org/pdf/1302.6426v1", 
    "title": "Segmentation of Alzheimers Disease in PET scan datasets using MATLAB", 
    "arxiv-id": "1302.6426v1", 
    "author": "K. Raja", 
    "publish": "2013-02-26T13:19:26Z", 
    "summary": "Positron Emission Tomography (PET) scan images are one of the bio medical\nimaging techniques similar to that of MRI scan images but PET scan images are\nhelpful in finding the development of tumors.The PET scan images requires\nexpertise in the segmentation where clustering plays an important role in the\nautomation process.The segmentation of such images is manual to automate the\nprocess clustering is used.Clustering is commonly known as unsupervised\nlearning process of n dimensional data sets are clustered into k groups so as\nto maximize the inter cluster similarity and to minimize the intra cluster\nsimilarity.This paper is proposed to implement the commonly used K Means and\nFuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix\nLABoratory (MATLAB) and tested with sample PET scan image. The sample data is\ncollected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image\nProcessing and Visualization Tool (MIPAV) are used to compare the resultant\nimages."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IAdCC.2013.6514315", 
    "link": "http://arxiv.org/pdf/1302.6615v1", 
    "title": "PSO based Neural Networks vs. Traditional Statistical Models for   Seasonal Time Series Forecasting", 
    "arxiv-id": "1302.6615v1", 
    "author": "Laxmi Kant", 
    "publish": "2013-02-26T22:25:16Z", 
    "summary": "Seasonality is a distinctive characteristic which is often observed in many\npractical time series. Artificial Neural Networks (ANNs) are a class of\npromising models for efficiently recognizing and forecasting seasonal patterns.\nIn this paper, the Particle Swarm Optimization (PSO) approach is used to\nenhance the forecasting strengths of feedforward ANN (FANN) as well as Elman\nANN (EANN) models for seasonal data. Three widely popular versions of the basic\nPSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.\nThe empirical analysis is conducted on three real-world seasonal time series.\nResults clearly show that each version of the PSO algorithm achieves notably\nbetter forecasting accuracies than the standard Backpropagation (BP) training\nmethod for both FANN and EANN models. The neural network forecasting results\nare also compared with those from the three traditional statistical models,\nviz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters\n(HW) and Support Vector Machine (SVM). The comparison demonstrates that both\nPSO and BP based neural networks outperform SARIMA, HW and SVM models for all\nthree time series datasets. The forecasting performances of ANNs are further\nimproved through combining the outputs from the three PSO based models."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IAdCC.2013.6514315", 
    "link": "http://arxiv.org/pdf/1302.7051v2", 
    "title": "Polyploidy and Discontinuous Heredity Effect on Evolutionary   Multi-Objective Optimization", 
    "arxiv-id": "1302.7051v2", 
    "author": "Ahmed Bahgat", 
    "publish": "2013-02-28T01:32:28Z", 
    "summary": "This paper examines the effect of mimicking discontinuous heredity caused by\ncarrying more than one chromosome in some living organisms cells in\nEvolutionary Multi-Objective Optimization algorithms. In this representation,\nthe phenotype may not fully reflect the genotype. By doing so we are mimicking\nliving organisms inheritance mechanism, where traits may be silently carried\nfor many generations to reappear later. Representations with different number\nof chromosomes in each solution vector are tested on different benchmark\nproblems with high number of decision variables and objectives. A comparison\nwith Non-Dominated Sorting Genetic Algorithm-II is done on all problems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISIE.2008.4677254", 
    "link": "http://arxiv.org/pdf/1302.7080v1", 
    "title": "Parameter Identification of Induction Motor Using Modified Particle   Swarm Optimization Algorithm", 
    "arxiv-id": "1302.7080v1", 
    "author": "Ahmed Bahgat", 
    "publish": "2013-02-28T04:41:53Z", 
    "summary": "This paper presents a new technique for induction motor parameter\nidentification. The proposed technique is based on a simple startup test using\na standard V/F inverter. The recorded startup currents are compared to that\nobtained by simulation of an induction motor model. A Modified PSO optimization\nis used to find out the best model parameter that minimizes the sum square\nerror between the measured and the simulated currents. The performance of the\nmodified PSO is compared with other optimization methods including line search,\nconventional PSO and Genetic Algorithms. Simulation results demonstrate the\nability of the proposed technique to capture the true values of the machine\nparameters and the superiority of the results obtained using the modified PSO\nover other optimization techniques."
},{
    "category": "cs.NE", 
    "doi": "10.1109/SIS.2007.367950", 
    "link": "http://arxiv.org/pdf/1303.0323v1", 
    "title": "Clubs-based Particle Swarm Optimization", 
    "arxiv-id": "1303.0323v1", 
    "author": "Ahmed Bahgat", 
    "publish": "2013-03-02T00:05:26Z", 
    "summary": "This paper introduces a new dynamic neighborhood network for particle swarm\noptimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO)\nalgorithm, each particle initially joins a default number of what we call\n'clubs'. Each particle is affected by its own experience and the experience of\nthe best performing member of the clubs it is a member of. Clubs membership is\ndynamic, where the worst performing particles socialize more by joining more\nclubs to learn from other particles and the best performing particles are made\nto socialize less by leaving clubs to reduce their strong influence on other\nmembers. Particles return gradually to default membership level when they stop\nshowing extreme performance. Inertia weights of swarm members are made random\nwithin a predefined range. This proposed dynamic neighborhood algorithm is\ncompared with other two algorithms having static neighborhood topologies on a\nset of classic benchmark problems. The results showed superior performance for\nC-PSO regarding escaping local optima and convergence speed."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijdps.2011.2604", 
    "link": "http://arxiv.org/pdf/1303.0462v1", 
    "title": "Distributed Evolutionary Computation: A New Technique for Solving Large   Number of Equations", 
    "arxiv-id": "1303.0462v1", 
    "author": "Gazi Abdullah Shahriar", 
    "publish": "2013-03-03T05:38:41Z", 
    "summary": "Evolutionary computation techniques have mostly been used to solve various\noptimization and learning problems successfully. Evolutionary algorithm is more\neffective to gain optimal solution(s) to solve complex problems than\ntraditional methods. In case of problems with large set of parameters,\nevolutionary computation technique incurs a huge computational burden for a\nsingle processing unit. Taking this limitation into account, this paper\npresents a new distributed evolutionary computation technique, which decomposes\ndecision vectors into smaller components and achieves optimal solution in a\nshort time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)\nHybrid Evolutionary Algorithm is distributed incorporating cluster computation.\nMoreover, two new selection methods named Best All Selection (BAS) and Twin\nSelection (TS) are introduced for selecting best fit solution vector.\nExperimental results show that optimal solution is achieved for different kinds\nof problems having huge parameters and a considerable speedup is obtained in\nproposed distributed system."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijdps.2011.2604", 
    "link": "http://arxiv.org/pdf/1303.1051v1", 
    "title": "A Genetic algorithm to solve the container storage space allocation   problem", 
    "arxiv-id": "1303.1051v1", 
    "author": "Ecole Nationale des Ingenieurs de Tunis", 
    "publish": "2013-03-05T14:45:30Z", 
    "summary": "This paper presented a genetic algorithm (GA) to solve the container storage\nproblem in the port. This problem is studied with different container types\nsuch as regular, open side, open top, tank, empty and refrigerated containers.\nThe objective of this problem is to determine an optimal containers\narrangement, which respects customers delivery deadlines, reduces the rehandle\noperations of containers and minimizes the stop time of the container ship. In\nthis paper, an adaptation of the genetic algorithm to the container storage\nproblem is detailed and some experimental results are presented and discussed.\nThe proposed approach was compared to a Last In First Out (LIFO) algorithm\napplied to the same problem and has recorded good results"
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.1243v1", 
    "title": "A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on   Particle Swarm Theory with Arithmetic Crossover", 
    "arxiv-id": "1303.1243v1", 
    "author": "M. M. A. Hashem", 
    "publish": "2013-03-06T02:55:20Z", 
    "summary": "This paper proposes a generalized Hybrid Real-coded Quantum Evolutionary\nAlgorithm (HRCQEA) for optimizing complex functions as well as combinatorial\noptimization. The main idea of HRCQEA is to devise a new technique for mutation\nand crossover operators. Using the evolutionary equation of PSO a\nSingle-Multiple gene Mutation (SMM) is designed and the concept of Arithmetic\nCrossover (AC) is used in the new Crossover operator. In HRCQEA, each triploid\nchromosome represents a particle and the position of the particle is updated\nusing SMM and Quantum Rotation Gate (QRG), which can make the balance between\nexploration and exploitation. Crossover is employed to expand the search space,\nHill Climbing Selection (HCS) and elitism help to accelerate the convergence\nspeed. Simulation results on Knapsack Problem and five benchmark complex\nfunctions with high dimension show that HRCQEA performs better in terms of\nability to discover the global optimum and convergence speed."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.1913v1", 
    "title": "Design and Development of Artificial Neural Networking (ANN) system   using sigmoid activation function to predict annual rice production in   Tamilnadu", 
    "arxiv-id": "1303.1913v1", 
    "author": "K. Baskaran", 
    "publish": "2013-03-08T08:53:15Z", 
    "summary": "Prediction of annual rice production in all the 31 districts of Tamilnadu is\nan important decision for the Government of Tamilnadu. Rice production is a\ncomplex process and non linear problem involving soil, crop, weather, pest,\ndisease, capital, labour and management parameters. ANN software was designed\nand developed with Feed Forward Back Propagation (FFBP) network to predict rice\nproduction. The input layer has six independent variables like area of\ncultivation and rice production in three seasons like Kuruvai, Samba and Kodai.\nThe popular sigmoid activation function was adopted to convert input data into\nsigmoid values. The hidden layer computes the summation of six sigmoid values\nwith six sets of weightages. The final output was converted into sigmoid values\nusing a sigmoid transfer function. ANN outputs are the predicted results. The\nerror between original data and ANN output values were computed. A threshold\nvalue of 10-9 was used to test whether the error is greater than the threshold\nlevel. If the error is greater than threshold then updating of weights was done\nall summations were done by back propagation. This process was repeated until\nerror equal to zero. The predicted results were printed and it was found to be\nexactly matching with the expected values. It shows that the ANN prediction was\n100% accurate."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.2215v1", 
    "title": "Expensive Optimisation: A Metaheuristics Perspective", 
    "arxiv-id": "1303.2215v1", 
    "author": "Maumita Bhattacharya", 
    "publish": "2013-03-09T14:41:24Z", 
    "summary": "Stochastic, iterative search methods such as Evolutionary Algorithms (EAs)\nare proven to be efficient optimizers. However, they require evaluation of the\ncandidate solutions which may be prohibitively expensive in many real world\noptimization problems. Use of approximate models or surrogates is being\nexplored as a way to reduce the number of such evaluations. In this paper we\ninvestigated three such methods. The first method (DAFHEA) partially replaces\nan expensive function evaluation by its approximate model. The approximation is\nrealized with support vector machine (SVM) regression models. The second method\n(DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertain\nenvironments. The third one uses surrogate ranking with preference learning or\nordinal regression. The fitness of the candidates is estimated by modeling\ntheir rank. The techniques' performances on some of the benchmark numerical\noptimization problems have been reported. The comparative benefits and\nshortcomings of both techniques have been identified."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.2745v1", 
    "title": "Evolutionary Approaches to Expensive Optimisation", 
    "arxiv-id": "1303.2745v1", 
    "author": "Maumita Bhattacharya", 
    "publish": "2013-03-12T01:39:11Z", 
    "summary": "Surrogate assisted evolutionary algorithms (EA) are rapidly gaining\npopularity where applications of EA in complex real world problem domains are\nconcerned. Although EAs are powerful global optimizers, finding optimal\nsolution to complex high dimensional, multimodal problems often require very\nexpensive fitness function evaluations. Needless to say, this could brand any\npopulation-based iterative optimization technique to be the most crippling\nchoice to handle such problems. Use of approximate model or surrogates provides\na much cheaper option. However, naturally this cheaper option comes with its\nown price. This paper discusses some of the key issues involved with use of\napproximation in evolutionary algorithm, possible best practices and solutions.\nAnswers to the following questions have been sought: what type of fitness\napproximation to be used; which approximation model to use; how to integrate\nthe approximation model in EA; how much approximation to use; and how to ensure\nreliable approximation."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.3145v2", 
    "title": "Convex Hull-Based Multi-objective Genetic Programming for Maximizing ROC   Performance", 
    "arxiv-id": "1303.3145v2", 
    "author": "Xin Yao", 
    "publish": "2013-03-13T12:51:31Z", 
    "summary": "ROC is usually used to analyze the performance of classifiers in data mining.\nROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empirical\nROC curve, and covers potential optima for the given set of classifiers.\nGenerally, ROC performance maximization could be considered to maximize the\nROCCH, which also means to maximize the true positive rate (tpr) and minimize\nthe false positive rate (fpr) for each classifier in the ROC space. However,\ntpr and fpr are conflicting with each other in the ROCCH optimization process.\nThough ROCCH maximization problem seems like a multi-objective optimization\nproblem (MOP), the special characters make it different from traditional MOP.\nIn this work, we will discuss the difference between them and propose convex\nhull-based multi-objective genetic programming (CH-MOGP) to solve ROCCH\nmaximization problems. Convex hull-based sort is an indicator based selection\nscheme that aims to maximize the area under convex hull, which serves as a\nunary indicator for the performance of a set of points. A selection procedure\nis described that can be efficiently implemented and follows similar design\nprinciples than classical hyper-volume based optimization algorithms. It is\nhypothesized that by using a tailored indicator-based selection scheme CH-MOGP\ngets more efficient for ROC convex hull approximation than algorithms which\ncompute all Pareto optimal points. To test our hypothesis we compare the new\nCH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D)\nand SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machine\nlearning algorithms such as C4.5, Naive Bayes and Prie. Experimental results\nbased on 22 well-known UCI data sets show that CH-MOGP outperforms\nsignificantly traditional EMOAs."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.3469v1", 
    "title": "Hybrid Evolutionary Computation for Continuous Optimization", 
    "arxiv-id": "1303.3469v1", 
    "author": "Richard S. Neville", 
    "publish": "2013-03-14T14:59:32Z", 
    "summary": "Hybrid optimization algorithms have gained popularity as it has become\napparent there cannot be a universal optimization strategy which is globally\nmore beneficial than any other. Despite their popularity, hybridization\nframeworks require more detailed categorization regarding: the nature of the\nproblem domain, the constituent algorithms, the coupling schema and the\nintended area of application. This report proposes a hybrid algorithm for\nsolving small to large-scale continuous global optimization problems. It\ncomprises evolutionary computation (EC) algorithms and a sequential quadratic\nprogramming (SQP) algorithm; combined in a collaborative portfolio. The SQP is\na gradient based local search method. To optimize the individual contributions\nof the EC and SQP algorithms for the overall success of the proposed hybrid\nsystem, improvements were made in key features of these algorithms. The report\nproposes enhancements in: i) the evolutionary algorithm, ii) a new convergence\ndetection mechanism was proposed; and iii) in the methods for evaluating the\nsearch directions and step sizes for the SQP local search algorithm. The\nproposed hybrid design aim was to ensure that the two algorithms complement\neach other by exploring and exploiting the problem search space. Preliminary\nresults justify that an adept hybridization of evolutionary algorithms with a\nsuitable local search method, could yield a robust and efficient means of\nsolving wide range of global optimization problems. Finally, a discussion of\nthe outcomes of the initial investigation and a review of the associated\nchallenges and inherent limitations of the proposed method is presented to\ncomplete the investigation. The report highlights extensive research,\nparticularly, some potential case studies and application areas."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.3901v2", 
    "title": "Efficient Evolutionary Algorithm for Single-Objective Bilevel   Optimization", 
    "arxiv-id": "1303.3901v2", 
    "author": "Kalyanmoy Deb", 
    "publish": "2013-03-15T20:30:57Z", 
    "summary": "Bilevel optimization problems are a class of challenging optimization\nproblems, which contain two levels of optimization tasks. In these problems,\nthe optimal solutions to the lower level problem become possible feasible\ncandidates to the upper level problem. Such a requirement makes the\noptimization problem difficult to solve, and has kept the researchers busy\ntowards devising methodologies, which can efficiently handle the problem.\nDespite the efforts, there hardly exists any effective methodology, which is\ncapable of handling a complex bilevel problem. In this paper, we introduce\nbilevel evolutionary algorithm based on quadratic approximations (BLEAQ) of\noptimal lower level variables with respect to the upper level variables. The\napproach is capable of handling bilevel problems with different kinds of\ncomplexities in relatively smaller number of function evaluations. Ideas from\nclassical optimization have been hybridized with evolutionary methods to\ngenerate an efficient optimization algorithm for generic bilevel problems. The\nefficacy of the algorithm has been shown on two sets of test problems. The\nfirst set is a recently proposed SMD test set, which contains problems with\ncontrollable complexities, and the second set contains standard test problems\ncollected from the literature. The proposed method has been evaluated against\ntwo benchmarks, and the performance gain is observed to be significant."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2010.2415", 
    "link": "http://arxiv.org/pdf/1303.6310v3", 
    "title": "A hybrid bat algorithm", 
    "arxiv-id": "1303.6310v3", 
    "author": "Xin-She Yang", 
    "publish": "2013-03-25T20:53:09Z", 
    "summary": "Swarm intelligence is a very powerful technique to be used for optimization\npurposes. In this paper we present a new swarm intelligence algorithm, based on\nthe bat algorithm. The Bat algorithm is hybridized with differential evolution\nstrategies. Besides showing very promising results of the standard benchmark\nfunctions, this hybridization also significantly improves the original bat\nalgorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1088/1742-6596/490/1/012040", 
    "link": "http://arxiv.org/pdf/1308.1940v1", 
    "title": "Time series modeling with pruned multi-layer perceptron and 2-stage   damped least-squares method", 
    "arxiv-id": "1308.1940v1", 
    "author": "Gilles Notton", 
    "publish": "2013-08-08T19:18:05Z", 
    "summary": "A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks\noften used in TS modeling and forecasting. Because of its \"black box\" aspect,\nmany researchers refuse to use it. Moreover, the optimization (often based on\nthe exhaustive approach where \"all\" configurations are tested) and learning\nphases of this artificial intelligence tool (often based on the\nLevenberg-Marquardt algorithm; LMA) are weaknesses of this approach\n(exhaustively and local minima). These two tasks must be repeated depending on\nthe knowledge of each new problem studied, making the process, long, laborious\nand not systematically robust. In this paper a pruning process is proposed.\nThis method allows, during the training phase, to carry out an inputs selecting\nmethod activating (or not) inter-nodes connections in order to verify if\nforecasting is improved. We propose to use iteratively the popular damped\nleast-squares method to activate inputs and neurons. A first pass is applied to\n10% of the learning sample to determine weights significantly different from 0\nand delete other. Then a classical batch process based on LMA is used with the\nnew MLP. The validation is done using 25 measured meteorological TS and\ncross-comparing the prediction results of the classical LMA and the 2-stage\nLMA."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.apenergy.2011.12.085", 
    "link": "http://arxiv.org/pdf/1308.2375v1", 
    "title": "A radial basis function neural network based approach for the electrical   characteristics estimation of a photovoltaic module", 
    "arxiv-id": "1308.2375v1", 
    "author": "Giuseppe Marco Tina", 
    "publish": "2013-08-11T08:17:35Z", 
    "summary": "The design process of photovoltaic (PV) modules can be greatly enhanced by\nusing advanced and accurate models in order to predict accurately their\nelectrical output behavior. The main aim of this paper is to investigate the\napplication of an advanced neural network based model of a module to improve\nthe accuracy of the predicted output I--V and P--V curves and to keep in\naccount the change of all the parameters at different operating conditions.\nRadial basis function neural networks (RBFNN) are here utilized to predict the\noutput characteristic of a commercial PV module, by reading only the data of\nsolar irradiation and temperature. A lot of available experimental data were\nused for the training of the RBFNN, and a backpropagation algorithm was\nemployed. Simulation and experimental validation is reported."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.apenergy.2011.12.085", 
    "link": "http://arxiv.org/pdf/1308.3080v4", 
    "title": "Average Drift Analysis and Population Scalability", 
    "arxiv-id": "1308.3080v4", 
    "author": "Xin Yao", 
    "publish": "2013-08-14T10:21:35Z", 
    "summary": "This paper aims to study how the population size affects the computation time\nof evolutionary algorithms in a rigorous way. The computation time of an\nevolutionary algorithm can be measured by either the expected number of\ngenerations (hitting time) or the expected number of fitness evaluations\n(running time) to find an optimal solution. Population scalability is the ratio\nof the expected hitting time between a benchmark algorithm and an algorithm\nusing a larger population size. Average drift analysis is presented for\ncomparing the expected hitting time of two algorithms and estimating lower and\nupper bounds on population scalability. Several intuitive beliefs are\nrigorously analysed. It is prove that (1) using a population sometimes\nincreases rather than decreases the expected hitting time; (2) using a\npopulation cannot shorten the expected running time of any elitist evolutionary\nalgorithm on unimodal functions in terms of the time-fitness landscape, but\nthis is not true in terms of the distance-based fitness landscape; (3) using a\npopulation cannot always reduce the expected running time on fully-deceptive\nfunctions, which depends on the benchmark algorithm using elitist selection or\nrandom selection."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2012.2216546", 
    "link": "http://arxiv.org/pdf/1308.3524v1", 
    "title": "Innovative Second-Generation Wavelets Construction With Recurrent Neural   Networks for Solar Radiation Forecasting", 
    "arxiv-id": "1308.3524v1", 
    "author": "Francesco Bonanno", 
    "publish": "2013-08-15T23:42:02Z", 
    "summary": "Solar radiation prediction is an important challenge for the electrical\nengineer because it is used to estimate the power developed by commercial\nphotovoltaic modules. This paper deals with the problem of solar radiation\nprediction based on observed meteorological data. A 2-day forecast is obtained\nby using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNS\nare used to exploit the correlation between solar radiation and\ntimescale-related variations of wind speed, humidity, and temperature. The\ninput to the selected WRNN is provided by timescale-related bands of wavelet\ncoefficients obtained from meteorological time series. The experimental setup\navailable at the University of Catania, Italy, provided this information. The\nnovelty of this approach is that the proposed WRNN performs the prediction in\nthe wavelet domain and, in addition, also performs the inverse wavelet\ntransform, giving the predicted signal as output. The obtained simulation\nresults show a very low root-mean-square error compared to the results of the\nsolar radiation prediction approaches obtained by hybrid neural networks\nreported in the recent literature."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2012.2216546", 
    "link": "http://arxiv.org/pdf/1308.4506v1", 
    "title": "A study of retrieval algorithms of sparse messages in networks of neural   cliques", 
    "arxiv-id": "1308.4506v1", 
    "author": "Xiaoran Jiang", 
    "publish": "2013-08-21T08:19:14Z", 
    "summary": "Associative memories are data structures addressed using part of the content\nrather than an index. They offer good fault reliability and biological\nplausibility. Among different families of associative memories, sparse ones are\nknown to offer the best efficiency (ratio of the amount of bits stored to that\nof bits used by the network itself). Their retrieval process performance has\nbeen shown to benefit from the use of iterations. However classical algorithms\nrequire having prior knowledge about the data to retrieve such as the number of\nnonzero symbols. We introduce several families of algorithms to enhance the\nretrieval process performance in recently proposed sparse associative memories\nbased on binary neural networks. We show that these algorithms provide better\nperformance, along with better plausibility than existing techniques. We also\nanalyze the required number of iterations and derive corresponding curves."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2012.2216546", 
    "link": "http://arxiv.org/pdf/1308.4675v1", 
    "title": "Genetic Algorithm for Solving Simple Mathematical Equality Problem", 
    "arxiv-id": "1308.4675v1", 
    "author": "Denny Hermawanto", 
    "publish": "2013-08-16T04:36:03Z", 
    "summary": "This paper explains genetic algorithm for novice in this field. Basic\nphilosophy of genetic algorithm and its flowchart are described. Step by step\nnumerical computation of genetic algorithm for solving simple mathematical\nequality problem will be briefly explained"
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2012.2216546", 
    "link": "http://arxiv.org/pdf/1308.5033v1", 
    "title": "A hybrid evolutionary algorithm with importance sampling for   multi-dimensional optimization", 
    "arxiv-id": "1308.5033v1", 
    "author": "Zhifeng Pan", 
    "publish": "2013-08-23T03:11:54Z", 
    "summary": "A hybrid evolutionary algorithm with importance sampling method is proposed\nfor multi-dimensional optimization problems in this paper. In order to make use\nof the information provided in the search process, a set of visited solutions\nis selected to give scores for intervals in each dimension, and they are\nupdated as algorithm proceeds. Those intervals with higher scores are regarded\nas good intervals, which are used to estimate the joint distribution of optimal\nsolutions through an interaction between the pool of good genetics, which are\nthe individuals with smaller fitness values. And the sampling probabilities for\ngood genetics are determined through an interaction between those estimated\ngood intervals. It is a cross validation mechanism which determines the\nsampling probabilities for good intervals and genetics, and the resulted\nprobabilities are used to design crossover, mutation and other stochastic\noperators with importance sampling method. As the selection of genetics and\nintervals is not directly dependent on the values of fitness, the resulted\noffsprings may avoid the trap of local optima. And a purely random EA is also\ncombined into the proposed algorithm to maintain the diversity of population.\n30 benchmark test functions are used to evaluate the performance of the\nproposed algorithm, and it is found that the proposed hybrid algorithm is an\nefficient algorithm for multi-dimensional optimization problems considered in\nthis paper."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2012.2216546", 
    "link": "http://arxiv.org/pdf/1308.5269v2", 
    "title": "A comparative analysis of methods for estimating axon diameter using DWI", 
    "arxiv-id": "1308.5269v2", 
    "author": "Hamed Yousefi Mesri", 
    "publish": "2013-08-24T00:57:26Z", 
    "summary": "The importance of studying the brain microstructure is described and the\nexisting and state of the art non-invasive methods for the investigation of the\nbrain microstructure using Diffusion Weighted Magnetic Resonance Imaging (DWI)\nis studied. In the next step, Cramer-Rao Lower Bound (CRLB) analysis is\ndescribed and utilised for assessment of the minimum estimation error and\nuncertainty level of different Diffusion Weighted Magnetic Resonance (DWMR)\nsignal decay models. The analyses are performed considering the best scenario\nthrough which, we assume that the models are the appropriate representation of\nthe measured phenomena. This includes the study of the sensitivity of the\nestimations to the measurement and model parameters. It is demonstrated that\nnone of the existing models can achieve a reasonable minimum uncertainty level\nunder typical measurement setup. At the end, the practical obstacles for\nachieving higher performance in clinical and experimental environments are\nstudied and their effects on feasibility of the methods are discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pone.0083242", 
    "link": "http://arxiv.org/pdf/1309.0719v2", 
    "title": "Understanding Evolutionary Potential in Virtual CPU Instruction Set   Architectures", 
    "arxiv-id": "1309.0719v2", 
    "author": "Charles Ofria", 
    "publish": "2013-09-03T15:17:07Z", 
    "summary": "We investigate fundamental decisions in the design of instruction set\narchitectures for linear genetic programs that are used as both model systems\nin evolutionary biology and underlying solution representations in evolutionary\ncomputation. We subjected digital organisms with each tested architecture to\nseven different computational environments designed to present a range of\nevolutionary challenges. Our goal was to engineer a general purpose\narchitecture that would be effective under a broad range of evolutionary\nconditions. We evaluated six different types of architectural features for the\nvirtual CPUs: (1) genetic flexibility: we allowed digital organisms to more\nprecisely modify the function of genetic instructions, (2) memory: we provided\nan increased number of registers in the virtual CPUs, (3) decoupled sensors and\nactuators: we separated input and output operations to enable greater control\nover data flow. We also tested a variety of methods to regulate expression: (4)\nexplicit labels that allow programs to dynamically refer to specific genome\npositions, (5) position-relative search instructions, and (6) multiple new flow\ncontrol instructions, including conditionals and jumps. Each of these features\nalso adds complication to the instruction set and risks slowing evolution due\nto epistatic interactions. Two features (multiple argument specification and\nseparated I/O) demonstrated substantial improvements int the majority of test\nenvironments. Some of the remaining tested modifications were detrimental,\nthought most exhibit no systematic effects on evolutionary potential,\nhighlighting the robustness of digital evolution. Combined, these observations\nenhance our understanding of how instruction architecture impacts evolutionary\npotential, enabling the creation of architectures that support more rapid\nevolution of complex solutions to a broad range of challenges."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pone.0083242", 
    "link": "http://arxiv.org/pdf/1309.3214v1", 
    "title": "Modeling Based on Elman Wavelet Neural Network for Class-D Power   Amplifiers", 
    "arxiv-id": "1309.3214v1", 
    "author": "Reza Malekian", 
    "publish": "2013-09-12T16:35:35Z", 
    "summary": "In Class-D Power Amplifiers (CDPAs), the power supply noise can intermodulate\nwith the input signal, manifesting into power-supply induced intermodulation\ndistortion (PS-IMD) and due to the memory effects of the system, there exist\nasymmetries in the PS-IMDs. In this paper, a new behavioral modeling based on\nthe Elman Wavelet Neural Network (EWNN) is proposed to study the nonlinear\ndistortion of the CDPAs. In EWNN model, the Morlet wavelet functions are\nemployed as the activation function and there is a normalized operation in the\nhidden layer, the modification of the scale factor and translation factor in\nthe wavelet functions are ignored to avoid the fluctuations of the error\ncurves. When there are 30 neurons in the hidden layer, to achieve the same\nsquare sum error (SSE) $\\epsilon_{min}=10^{-3}$, EWNN needs 31 iteration steps,\nwhile the basic Elman neural network (BENN) model needs 86 steps. The\nVolterra-Laguerre model has 605 parameters to be estimated but still can't\nachieve the same magnitude accuracy of EWNN. Simulation results show that the\nproposed approach of EWNN model has fewer parameters and higher accuracy than\nthe Volterra-Laguerre model and its convergence rate is much faster than the\nBENN model."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pone.0083242", 
    "link": "http://arxiv.org/pdf/1309.3816v1", 
    "title": "Multiplicative Approximations, Optimal Hypervolume Distributions, and   the Choice of the Reference Point", 
    "arxiv-id": "1309.3816v1", 
    "author": "Christian Thyssen", 
    "publish": "2013-09-16T02:16:56Z", 
    "summary": "Many optimization problems arising in applications have to consider several\nobjective functions at the same time. Evolutionary algorithms seem to be a very\nnatural choice for dealing with multi-objective problems as the population of\nsuch an algorithm can be used to represent the trade-offs with respect to the\ngiven objective functions. In this paper, we contribute to the theoretical\nunderstanding of evolutionary algorithms for multi-objective problems. We\nconsider indicator-based algorithms whose goal is to maximize the hypervolume\nfor a given problem by distributing {\\mu} points on the Pareto front. To gain\nnew theoretical insights into the behavior of hypervolume-based algorithms we\ncompare their optimization goal to the goal of achieving an optimal\nmultiplicative approximation ratio. Our studies are carried out for different\nPareto front shapes of bi-objective problems. For the class of linear fronts\nand a class of convex fronts, we prove that maximizing the hypervolume gives\nthe best possible approximation ratio when assuming that the extreme points\nhave to be included in both distributions of the points on the Pareto front.\nFurthermore, we investigate the choice of the reference point on the\napproximation behavior of hypervolume-based approaches and examine Pareto\nfronts of different shapes by numerical calculations."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pone.0083242", 
    "link": "http://arxiv.org/pdf/1309.5676v3", 
    "title": "Implementation of a language driven Backpropagation algorithm", 
    "arxiv-id": "1309.5676v3", 
    "author": "C. I. Ciuluvica", 
    "publish": "2013-09-23T01:12:04Z", 
    "summary": "Inspired by the importance of both communication and feedback on errors in\nhuman learning, our main goal was to implement a similar mechanism in\nsupervised learning of artificial neural networks. The starting point in our\nstudy was the observation that words should accompany the input vectors\nincluded in the training set, thus extending the ANN input space. This had as\nconsequence the necessity to take into consideration a modified sigmoid\nactivation function for neurons in the first hidden layer (in agreement with a\nspecific MLP apartment structure), and also a modified version of the\nBackpropagation algorithm, which allows using of unspecified (null) desired\noutput components. Following the belief that basic concepts should be tested on\nsimple examples, the previous mentioned mechanism was applied on both the XOR\nproblem and a didactic color case study. In this context, we noticed the\ninteresting fact that the ANN was capable to categorize all desired input\nvectors in the absence of their corresponding words, even though the training\nset included only word accompanied inputs, in both positive and negative\nexamples. Further analysis along applying this approach to more complex\nscenarios is currently in progress, as we consider the proposed language-driven\nalgorithm might contribute to a better understanding of learning in humans,\nopening as well the possibility to create a specific category of artificial\nneural networks, with abstraction capabilities."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-04772-5_102", 
    "link": "http://arxiv.org/pdf/1309.5896v1", 
    "title": "On the Success Rate of Crossover Operators for Genetic Programming with   Offspring Selection", 
    "arxiv-id": "1309.5896v1", 
    "author": "Stefan Wagner", 
    "publish": "2013-09-23T17:54:22Z", 
    "summary": "Genetic programming is a powerful heuristic search technique that is used for\na number of real world applications to solve among others regression,\nclassification, and time-series forecasting problems. A lot of progress towards\na theoretic description of genetic programming in form of schema theorems has\nbeen made, but the internal dynamics and success factors of genetic programming\nare still not fully understood. In particular, the effects of different\ncrossover operators in combination with offspring selection are largely\nunknown.\n  This contribution sheds light on the ability of well-known GP crossover\noperators to create better offspring when applied to benchmark problems. We\nconclude that standard (sub-tree swapping) crossover is a good default choice\nin combination with offspring selection, and that GP with offspring selection\nand random selection of crossover operators can improve the performance of the\nalgorithm in terms of best solution quality when no solution size constraints\nare applied."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-27549-4_51", 
    "link": "http://arxiv.org/pdf/1309.5931v1", 
    "title": "Data Mining using Unguided Symbolic Regression on a Blast Furnace   Dataset", 
    "arxiv-id": "1309.5931v1", 
    "author": "Michael Affenzeller", 
    "publish": "2013-09-23T19:35:29Z", 
    "summary": "In this paper a data mining approach for variable selection and knowledge\nextraction from datasets is presented. The approach is based on unguided\nsymbolic regression (every variable present in the dataset is treated as the\ntarget variable in multiple regression runs) and a novel variable relevance\nmetric for genetic programming. The relevance of each input variable is\ncalculated and a model approximating the target variable is created. The\ngenetic programming configurations with different target variables are executed\nmultiple times to reduce stochastic effects and the aggregated results are\ndisplayed as a variable interaction network. This interaction network\nhighlights important system components and implicit relations between the\nvariables. The whole approach is tested on a blast furnace dataset, because of\nthe complexity of the blast furnace and the many interrelations between the\nvariables. Finally the achieved results are discussed with respect to existing\nknowledge about the blast furnace process."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-27549-4_51", 
    "link": "http://arxiv.org/pdf/1311.0460v1", 
    "title": "An Adaptive Amoeba Algorithm for Shortest Path Tree Computation in   Dynamic Graphs", 
    "arxiv-id": "1311.0460v1", 
    "author": "Yong Deng", 
    "publish": "2013-11-03T12:30:37Z", 
    "summary": "This paper presents an adaptive amoeba algorithm to address the shortest path\ntree (SPT) problem in dynamic graphs. In dynamic graphs, the edge weight\nupdates consists of three categories: edge weight increases, edge weight\ndecreases, the mixture of them. Existing work on this problem solve this issue\nthrough analyzing the nodes influenced by the edge weight updates and recompute\nthese affected vertices. However, when the network becomes big, the process\nwill become complex. The proposed method can overcome the disadvantages of the\nexisting approaches. The most important feature of this algorithm is its\nadaptivity. When the edge weight changes, the proposed algorithm can recognize\nthe affected vertices and reconstruct them spontaneously. To evaluate the\nproposed adaptive amoeba algorithm, we compare it with the Label Setting\nalgorithm and Bellman-Ford algorithm. The comparison results demonstrate the\neffectiveness of the proposed method."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-27549-4_51", 
    "link": "http://arxiv.org/pdf/1311.0598v1", 
    "title": "Q-Gaussian Swarm Quantum Particle Intelligence on Predicting Global   Minimum of Potential Energy Function", 
    "arxiv-id": "1311.0598v1", 
    "author": "Hiqmet Kamberaj", 
    "publish": "2013-11-04T07:11:13Z", 
    "summary": "We present a newly developed -Gaussian Swarm Quantum-like Particle\nOptimization (q-GSQPO) algorithm to determine the global minimum of the\npotential energy function. Swarm Quantum-like Particle Optimization (SQPO)\nalgorithms have been derived using different attractive potential fields to\nrepresent swarm particles moving in a quantum environment, where the one which\nuses a harmonic oscillator potential as attractive field is considered as an\nimproved version. In this paper, we propose a new SQPO that uses -Gaussian\nprobability density function for the attractive potential field (q-GSQPO)\nrather than Gaussian one (GSQPO) which corresponds to harmonic potential. The\nperformance of the q-GSQPO is compared against the GSQPO. The new algorithm\noutperforms the GSQPO on most of the time in convergence to the global optimum\nby increasing the efficiency of sampling the phase space and avoiding the\npremature convergence to local minima. Moreover, the computational efforts were\ncomparable for both algorithms. We tested the algorithm to determine the lowest\nenergy configurations of a particle moving in a 2, 5, 10, and 50 dimensional\nspaces."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-27549-4_51", 
    "link": "http://arxiv.org/pdf/1311.1090v1", 
    "title": "Polyhedrons and Perceptrons Are Functionally Equivalent", 
    "arxiv-id": "1311.1090v1", 
    "author": "Daniel Crespin", 
    "publish": "2013-11-05T15:33:30Z", 
    "summary": "Mathematical definitions of polyhedrons and perceptron networks are\ndiscussed. The formalization of polyhedrons is done in a rather traditional\nway. For networks, previously proposed systems are developed. Perceptron\nnetworks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)\nare introduced. The main theme is that single output perceptron neural networks\nand characteristic functions of polyhedrons are one and the same class of\nfunctions. A rigorous formulation and proof that three layers suffice is\nobtained. The various constructions and results are among several steps\nrequired for algorithms that replace incremental and statistical learning with\nmore efficient, direct and exact geometric methods for calculation of\nperceptron architecture and weights."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijaia.2014.5110", 
    "link": "http://arxiv.org/pdf/1402.0708v1", 
    "title": "Microstrip Coupler Design Using Bat Algorithm", 
    "arxiv-id": "1402.0708v1", 
    "author": "Sadik Ulker", 
    "publish": "2014-02-04T12:25:31Z", 
    "summary": "Evolutionary and swarm algorithms have found many applications in design\nproblems since todays computing power enables these algorithms to find\nsolutions to complicated design problems very fast. Newly proposed hybrid\nalgorithm, bat algorithm, has been applied for the design of microwave\nmicrostrip couplers for the first time. Simulation results indicate that the\nbat algorithm is a very fast algorithm and it produces very reliable results."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1402.0808v1", 
    "title": "Associative Memories Based on Multiple-Valued Sparse Clustered Networks", 
    "arxiv-id": "1402.0808v1", 
    "author": "Warren J. Gross", 
    "publish": "2014-02-03T16:23:33Z", 
    "summary": "Associative memories are structures that store data patterns and retrieve\nthem given partial inputs. Sparse Clustered Networks (SCNs) are\nrecently-introduced binary-weighted associative memories that significantly\nimprove the storage and retrieval capabilities over the prior state-of-the art.\nHowever, deleting or updating the data patterns result in a significant\nincrease in the data retrieval error probability. In this paper, we propose an\nalgorithm to address this problem by incorporating multiple-valued weights for\nthe interconnections used in the network. The proposed algorithm lowers the\nerror rate by an order of magnitude for our sample network with 60% deleted\ncontents. We then investigate the advantages of the proposed algorithm for\nhardware implementations."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1402.1931v1", 
    "title": "MCA Learning Algorithm for Incident Signals Estimation: A Review", 
    "arxiv-id": "1402.1931v1", 
    "author": "John A. Avaritsiotis", 
    "publish": "2014-02-09T09:55:13Z", 
    "summary": "Recently there has been many works on adaptive subspace filtering in the\nsignal processing literature. Most of them are concerned with tracking the\nsignal subspace spanned by the eigenvectors corresponding to the eigenvalues of\nthe covariance matrix of the signal plus noise data. Minor Component Analysis\n(MCA) is important tool and has a wide application in telecommunications,\nantenna array processing, statistical parametric estimation, etc. As an\nimportant feature extraction technique, MCA is a statistical method of\nextracting the eigenvector associated with the smallest eigenvalue of the\ncovariance matrix. In this paper, we will present a MCA learning algorithm to\nextract minor component from input signals, and the learning rate parameter is\nalso presented, which ensures fast convergence of the algorithm, because it has\ndirect effect on the convergence of the weight vector and the error level is\naffected by this value. MCA is performed to determine the estimated DOA.\nSimulation results will be furnished to illustrate the theoretical results\nachieved."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1402.5428v1", 
    "title": "An Evolutionary approach for solving Shr\u00f6dinger Equation", 
    "arxiv-id": "1402.5428v1", 
    "author": "Abdelaziz Elmoujahid", 
    "publish": "2014-02-21T21:18:58Z", 
    "summary": "The purpose of this paper is to present a method of solving the Shr\\\"odinger\nEquation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms\ngenerations of trial solutions expressed in an analytical form. We illustrate\nthe effectiveness of this method providing, for example, the results of its\napplication to a quantum system minimal energy, and we compare these results\nwith those produced by traditional analytical methods"
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1402.6428v1", 
    "title": "Clustering Multidimensional Data with PSO based Algorithm", 
    "arxiv-id": "1402.6428v1", 
    "author": "Vishakha A. Metre", 
    "publish": "2014-02-26T06:08:27Z", 
    "summary": "Data clustering is a recognized data analysis method in data mining whereas\nK-Means is the well known partitional clustering method, possessing pleasant\nfeatures. We observed that, K-Means and other partitional clustering techniques\nsuffer from several limitations such as initial cluster centre selection,\npreknowledge of number of clusters, dead unit problem, multiple cluster\nmembership and premature convergence to local optima. Several optimization\nmethods are proposed in the literature in order to solve clustering\nlimitations, but Swarm Intelligence (SI) has achieved its remarkable position\nin the concerned area. Particle Swarm Optimization (PSO) is the most popular SI\ntechnique and one of the favorite areas of researchers. In this paper, we\npresent a brief overview of PSO and applicability of its variants to solve\nclustering challenges. Also, we propose an advanced PSO algorithm named as\nSubtractive Clustering based Boundary Restricted Adaptive Particle Swarm\nOptimization (SC-BR-APSO) algorithm for clustering multidimensional data. For\ncomparison purpose, we have studied and analyzed various algorithms such as\nK-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposed\nalgorithm on nine different datasets. The motivation behind proposing\nSC-BR-APSO algorithm is to deal with multidimensional data clustering, with\nminimum error rate and maximum convergence rate."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1402.6888v1", 
    "title": "CriPS: Critical Dynamics in Particle Swarm Optimization", 
    "arxiv-id": "1402.6888v1", 
    "author": "J Michael Herrmann", 
    "publish": "2014-02-27T12:35:27Z", 
    "summary": "Particle Swarm Optimisation (PSO) makes use of a dynamical system for solving\na search task. Instead of adding search biases in order to improve performance\nin certain problems, we aim to remove algorithm-induced scales by controlling\nthe swarm with a mechanism that is scale-free except possibly for a suppression\nof scales beyond the system size. In this way a very promising performance is\nachieved due to the balance of large-scale exploration and local search. The\nresulting algorithm shows evidence for self-organised criticality, brought\nabout via the intrinsic dynamics of the swarm as it interacts with the\nobjective function, rather than being explicitly specified. The Critical\nParticle Swarm (CriPS) can be easily combined with many existing extensions\nsuch as chaotic exploration, additional force terms or non-trivial topologies."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1404.0554v1", 
    "title": "From ADP to the Brain: Foundations, Roadmap, Challenges and Research   Priorities", 
    "arxiv-id": "1404.0554v1", 
    "author": "Paul J Werbos", 
    "publish": "2014-04-02T13:46:23Z", 
    "summary": "This paper defines and discusses Mouse Level Computational Intelligence\n(MLCI) as a grand challenge for the coming century. It provides a specific\nroadmap to reach that target, citing relevant work and review papers and\ndiscussing the relation to funding priorities in two NSF funding activities:\nthe ongoing Energy, Power and Adaptive Systems program (EPAS) and the recent\ninitiative in Cognitive Optimization and Prediction (COPN). It elaborates on\nthe first step, vector intelligence, a challenge in the development of\nuniversal learning systems, which itself will require considerable new research\nto attain. This in turn is a crucial prerequisite to true functional\nunderstanding of how mammal brains achieve such general learning capabilities."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISMVL.2014.44", 
    "link": "http://arxiv.org/pdf/1404.0868v1", 
    "title": "A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack   Problem", 
    "arxiv-id": "1404.0868v1", 
    "author": "Hongbin Dong", 
    "publish": "2014-04-03T11:46:42Z", 
    "summary": "The 0-1 knapsack problem is a well-known combinatorial optimisation problem.\nApproximation algorithms have been designed for solving it and they return\nprovably good solutions within polynomial time. On the other hand, genetic\nalgorithms are well suited for solving the knapsack problem and they find\nreasonably good solutions quickly. A naturally arising question is whether\ngenetic algorithms are able to find solutions as good as approximation\nalgorithms do. This paper presents a novel multi-objective optimisation genetic\nalgorithm for solving the 0-1 knapsack problem. Experiment results show that\nthe new algorithm outperforms its rivals, the greedy algorithm, mixed strategy\ngenetic algorithm, and greedy algorithm + mixed strategy genetic algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900442", 
    "link": "http://arxiv.org/pdf/1404.3520v1", 
    "title": "A Theoretical Assessment of Solution Quality in Evolutionary Algorithms   for the Knapsack Problem", 
    "arxiv-id": "1404.3520v1", 
    "author": "Yuren Zhou", 
    "publish": "2014-04-14T10:00:48Z", 
    "summary": "Evolutionary algorithms are well suited for solving the knapsack problem.\nSome empirical studies claim that evolutionary algorithms can produce good\nsolutions to the 0-1 knapsack problem. Nonetheless, few rigorous investigations\naddress the quality of solutions that evolutionary algorithms may produce for\nthe knapsack problem. The current paper focuses on a theoretical investigation\nof three types of (N+1) evolutionary algorithms that exploit bitwise mutation,\ntruncation selection, plus different repair methods for the 0-1 knapsack\nproblem. It assesses the solution quality in terms of the approximation ratio.\nOur work indicates that the solution produced by pure strategy and mixed\nstrategy evolutionary algorithms is arbitrarily bad. Nevertheless, the\nevolutionary algorithm using helper objectives may produce 1/2-approximation\nsolutions to the 0-1 knapsack problem."
},{
    "category": "cs.NE", 
    "doi": "10.1504/IJPM.2012.045647", 
    "link": "http://arxiv.org/pdf/1404.4067v1", 
    "title": "An effective AHP-based metaheuristic approach to solve supplier   selection problem", 
    "arxiv-id": "1404.4067v1", 
    "author": "Pranab K Dan", 
    "publish": "2014-04-15T20:21:31Z", 
    "summary": "The supplier selection problem is based on electing the best supplier from a\ngroup of pre-specified candidates, is identified as a Multi Criteria Decision\nMaking (MCDM), is proportionately significant in terms of qualitative and\nquantitative attributes. It is a fundamental issue to achieve a trade-off\nbetween such quantifiable and unquantifiable attributes with an aim to\naccomplish the best solution to the abovementioned problem. This article\nportrays a metaheuristic based optimization model to solve this NP-Complete\nproblem. Initially the Analytic Hierarchy Process (AHP) is implemented to\ngenerate an initial feasible solution of the problem. Thereafter a Simulated\nAnnealing (SA) algorithm is exploited to improve the quality of the obtained\nsolution. The Taguchi robust design method is exploited to solve the critical\nissues on the subject of the parameter selection of the SA technique. In order\nto verify the proposed methodology the numerical results are demonstrated based\non tangible industry data."
},{
    "category": "cs.NE", 
    "doi": "10.1504/IJPM.2012.045647", 
    "link": "http://arxiv.org/pdf/1404.5144v1", 
    "title": "Influence of the learning method in the performance of feedforward   neural networks when the activity of neurons is modified", 
    "arxiv-id": "1404.5144v1", 
    "author": "G. M. Sacha", 
    "publish": "2014-04-21T09:00:19Z", 
    "summary": "A method that allows us to give a different treatment to any neuron inside\nfeedforward neural networks is presented. The algorithm has been implemented\nwith two very different learning methods: a standard Back-propagation (BP)\nprocedure and an evolutionary algorithm. First, we have demonstrated that the\nEA training method converges faster and gives more accurate results than BP.\nThen we have made a full analysis of the effects of turning off different\ncombinations of neurons after the training phase. We demonstrate that EA is\nmuch more robust than BP for all the cases under study. Even in the case when\ntwo hidden neurons are lost, EA training is still able to give good average\nresults. This difference implies that we must be very careful when pruning or\nredundancy effects are being studied since the network performance when losing\nneurons strongly depends on the training method. Moreover, the influence of the\nindividual inputs will also depend on the training algorithm. Since EA keeps a\ngood classification performance when units are lost, this method could be a\ngood way to simulate biological learning systems since they must be robust\nagainst deficient neuron performance. Although biological systems are much more\ncomplex than the simulations shown in this article, we propose that a smart\ntraining strategy such as the one shown here could be considered as a first\nprotection against the losing of a certain number of neurons."
},{
    "category": "cs.NE", 
    "doi": "10.1504/IJPM.2012.045647", 
    "link": "http://arxiv.org/pdf/1404.5520v1", 
    "title": "A Computationally Efficient Limited Memory CMA-ES for Large Scale   Optimization", 
    "arxiv-id": "1404.5520v1", 
    "author": "Ilya Loshchilov", 
    "publish": "2014-04-21T06:10:51Z", 
    "summary": "We propose a computationally efficient limited memory Covariance Matrix\nAdaptation Evolution Strategy for large scale optimization, which we call the\nLM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for\nnumerical optimization of non-linear, non-convex optimization problems in\ncontinuous domain. Inspired by the limited memory BFGS method of Liu and\nNocedal (1989), the LM-CMA-ES samples candidate solutions according to a\ncovariance matrix reproduced from $m$ direction vectors selected during the\noptimization process. The decomposition of the covariance matrix into Cholesky\nfactors allows to reduce the time and memory complexity of the sampling to\n$O(mn)$, where $n$ is the number of decision variables. When $n$ is large\n(e.g., $n$ > 1000), even relatively small values of $m$ (e.g., $m=20,30$) are\nsufficient to efficiently solve fully non-separable problems and to reduce the\noverall run-time."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900272", 
    "link": "http://arxiv.org/pdf/1404.5767v1", 
    "title": "Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates", 
    "arxiv-id": "1404.5767v1", 
    "author": "Hendrik Richter", 
    "publish": "2014-04-23T09:53:47Z", 
    "summary": "Coevolutionary minimal substrates are simple and abstract models that allow\nstudying the relationships and codynamics between objective and subjective\nfitness. Using these models an approach is presented for defining and analyzing\nfitness landscapes of coevolutionary problems. We devise similarity measures of\ncodynamic fitness landscapes and experimentally study minimal substrates of\ntest--based and compositional problems for both cooperative and competitive\ninteraction."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900272", 
    "link": "http://arxiv.org/pdf/1404.6334v5", 
    "title": "Input anticipating critical reservoirs show power law forgetting of   unexpected input events", 
    "arxiv-id": "1404.6334v5", 
    "author": "Norbert Michael Mayer", 
    "publish": "2014-04-25T06:36:28Z", 
    "summary": "Usually, reservoir computing shows an exponential memory decay. This paper\ninvestigates under which circumstances echo state networks can show a power law\nforgetting. That means traces of earlier events can be found in the reservoir\nfor very long time spans. Such a setting requires critical connectivity exactly\nat the limit of what is permissible according the echo state condition.\nHowever, for general matrices the limit cannot be determined exactly from\ntheory. In addition, the behavior of the network is strongly influenced by the\ninput flow. Results are presented that use certain types of restricted\nrecurrent connectivity and anticipation learning with regard to the input,\nwhere indeed power law forgetting can be achieved."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s12065-014-0119-1", 
    "link": "http://arxiv.org/pdf/1404.7765v2", 
    "title": "A semantic network-based evolutionary algorithm for computational   creativity", 
    "arxiv-id": "1404.7765v2", 
    "author": "Santiago Ontanon", 
    "publish": "2014-04-30T15:45:01Z", 
    "summary": "We introduce a novel evolutionary algorithm (EA) with a semantic\nnetwork-based representation. For enabling this, we establish new formulations\nof EA variation operators, crossover and mutation, that we adapt to work on\nsemantic networks. The algorithm employs commonsense reasoning to ensure all\noperations preserve the meaningfulness of the networks, using ConceptNet and\nWordNet knowledge bases. The algorithm can be interpreted as a novel memetic\nalgorithm (MA), given that (1) individuals represent pieces of information that\nundergo evolution, as in the original sense of memetics as it was introduced by\nDawkins; and (2) this is different from existing MA, where the word \"memetic\"\nhas been used as a synonym for local refinement after global optimization. For\nevaluating the approach, we introduce an analogical similarity-based fitness\nmeasure that is computed through structure mapping. This setup enables the\nopen-ended generation of networks analogous to a given base network."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.1691v1", 
    "title": "Towards a Better Understanding of the Local Attractor in Particle Swarm   Optimization: Speed and Solution Quality", 
    "arxiv-id": "1406.1691v1", 
    "author": "Rolf Wanka", 
    "publish": "2014-06-06T14:00:46Z", 
    "summary": "Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic\nfor solving continuous optimization problems. Although this technique is widely\nused, the understanding of the mechanisms that make swarms so successful is\nstill limited. We present the first substantial experimental investigation of\nthe influence of the local attractor on the quality of exploration and\nexploitation. We compare in detail classical PSO with the social-only variant\nwhere local attractors are ignored. To measure the exploration capabilities, we\ndetermine how frequently both variants return results in the neighborhood of\nthe global optimum. We measure the quality of exploitation by considering only\nfunction values from runs that reached a search point sufficiently close to the\nglobal optimum and then comparing in how many digits such values still deviate\nfrom the global minimum value. It turns out that the local attractor\nsignificantly improves the exploration, but sometimes reduces the quality of\nthe exploitation. As a compromise, we propose and evaluate a hybrid PSO which\nswitches off its local attractors at a certain point in time. The effects\nmentioned can also be observed by measuring the potential of the swarm."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.2539v1", 
    "title": "Maximizing Diversity for Multimodal Optimization", 
    "arxiv-id": "1406.2539v1", 
    "author": "Fabricio Olivetti de Franca", 
    "publish": "2014-06-10T13:22:13Z", 
    "summary": "Most multimodal optimization algorithms use the so called \\textit{niching\nmethods}~\\cite{mahfoud1995niching} in order to promote diversity during\noptimization, while others, like \\textit{Artificial Immune\nSystems}~\\cite{de2010conceptual} try to find multiple solutions as its main\nobjective. One of such algorithms, called\n\\textit{dopt-aiNet}~\\cite{de2005artificial}, introduced the Line Distance that\nmeasures the distance between two solutions regarding their basis of\nattraction. In this short abstract I propose the use of the Line Distance\nmeasure as the main objective-function in order to locate multiple optima at\nonce in a population."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.2613v1", 
    "title": "Simulation based Hardness Evaluation of a Multi-Objective Genetic   Algorithm", 
    "arxiv-id": "1406.2613v1", 
    "author": "Sameen Mansha", 
    "publish": "2014-06-07T20:13:58Z", 
    "summary": "Studies have shown that multi-objective optimization problems are hard\nproblems. Such problems either require longer time to converge to an optimum\nsolution, or may not converge at all. Recently some researchers have claimed\nthat real culprit for increasing the hardness of multi-objective problems are\nnot the number of objectives themselves rather it is the increased size of\nsolution set, incompatibility of solutions, and high probability of finding\nsuboptimal solution due to increased number of local maxima. In this work, we\nhave setup a simple framework for the evaluation of hardness of multi-objective\ngenetic algorithms (MOGA). The algorithm is designed for a pray-predator game\nwhere a player is to improve its lifespan, challenging level and usability of\nthe game arena through number of generations. A rigorous set of experiments are\nperformed for quantifying the hardness in terms of evolution for increasing\nnumber of objective functions. In genetic algorithm, crossover and mutation\nwith equal probability are applied to create offspring in each generation.\nFirst, each objective function is maximized individually by ranking the\ncompeting players on the basis of the fitness (cost) function, and then a\nmulti-objective cost function (sum of individual cost functions) is maximized\nwith ranking, and also without ranking where dominated solutions are also\nallowed to evolve."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.2614v4", 
    "title": "Application and Verification of Algorithm Learning Based Neural Network", 
    "arxiv-id": "1406.2614v4", 
    "author": "Moomal Qureshi", 
    "publish": "2014-06-07T20:20:11Z", 
    "summary": "This paper has been withdrawn by the author due to a crucial accuracy error\nin Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work\nin the following article. Yoon, H., Park, C. S., Kim, J. S., & Baek, J. G.\n(2013). Algorithm learning based neural network integrating feature selection\nand classification. Expert Systems with Applications, 40(1), 231-241.\nhttp://www.sciencedirect.com/science/article/pii/S0957417412008731"
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.2671v1", 
    "title": "Conceptors: an easy introduction", 
    "arxiv-id": "1406.2671v1", 
    "author": "Herbert Jaeger", 
    "publish": "2014-06-10T19:21:33Z", 
    "summary": "Conceptors provide an elementary neuro-computational mechanism which sheds a\nfresh and unifying light on a diversity of cognitive phenomena. A number of\ndemanding learning and processing tasks can be solved with unprecedented ease,\nrobustness and accuracy. Some of these tasks were impossible to solve before.\nThis entirely informal paper introduces the basic principles of conceptors and\nhighlights some of their usages."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.2889v1", 
    "title": "Explicit Computation of Input Weights in Extreme Learning Machines", 
    "arxiv-id": "1406.2889v1", 
    "author": "Andr\u00e9 van Schaik", 
    "publish": "2014-06-11T12:39:10Z", 
    "summary": "We present a closed form expression for initializing the input weights in a\nmulti-layer perceptron, which can be used as the first step in synthesis of an\nExtreme Learning Ma-chine. The expression is based on the standard function for\na separating hyperplane as computed in multilayer perceptrons and linear\nSupport Vector Machines; that is, as a linear combination of input data\nsamples. In the absence of supervised training for the input weights, random\nlinear combinations of training data samples are used to project the input data\nto a higher dimensional hidden layer. The hidden layer weights are solved in\nthe standard ELM fashion by computing the pseudoinverse of the hidden layer\noutputs and multiplying by the desired output values. All weights for this\nmethod can be computed in a single pass, and the resulting networks are more\naccurate and more consistent on some standard problems than regular ELM\nnetworks of the same size."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-11298-5_10", 
    "link": "http://arxiv.org/pdf/1406.3282v1", 
    "title": "A swarm optimization algorithm inspired in the behavior of the   social-spider", 
    "arxiv-id": "1406.3282v1", 
    "author": "Marco Perez", 
    "publish": "2014-06-12T16:29:14Z", 
    "summary": "Swarm intelligence is a research field that models the collective behavior in\nswarms of insects or animals. Several algorithms arising from such models have\nbeen proposed to solve a wide range of complex optimization problems. In this\npaper, a novel swarm algorithm called the Social Spider Optimization (SSO) is\nproposed for solving optimization tasks. The SSO algorithm is based on the\nsimulation of cooperative behavior of social-spiders. In the proposed\nalgorithm, individuals emulate a group of spiders which interact to each other\nbased on the biological laws of the cooperative colony. The algorithm considers\ntwo different search agents (spiders): males and females. Depending on gender,\neach individual is conducted by a set of different evolutionary operators which\nmimic different cooperative behaviors that are typically found in the colony.\nIn order to illustrate the proficiency and robustness of the proposed approach,\nit is compared to other well-known evolutionary methods. The comparison\nexamines several standard benchmark functions that are commonly considered\nwithin the literature of evolutionary algorithms. The outcome shows a high\nperformance of the proposed method for searching a global optimum with several\nbenchmark functions."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22315381/IJETT-V11P286", 
    "link": "http://arxiv.org/pdf/1406.4237v1", 
    "title": "An Evolutionary Approach for Optimal Citing and Sizing of Micro-Grid in   Radial Distribution Systems", 
    "arxiv-id": "1406.4237v1", 
    "author": "Dr. S. Jeyadevi", 
    "publish": "2014-06-17T04:52:23Z", 
    "summary": "This Paper presents the methodology of penetration of Micro-Grids (MG) in the\nradial distribution system (RDS). The aim of this paper is to minimize a total\nreal power loss that descends the performance of the radial distribution system\nby integrating various renewable resources as Distributed Generation (DG). The\ncombination of different types of renewable energy resources contributes a\nsustainable MG. These resources are optimally sized and located using\nevolutionary approach in various penetration levels. The optimal solutions are\nexperimented with IEEE 33 radial distribution system using Particle Swarm\nOptimization (PSO) technique. The results are quite promising and authenticate\nits potential to solve problem in radial distribution system effectively."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22315381/IJETT-V11P286", 
    "link": "http://arxiv.org/pdf/1406.4518v1", 
    "title": "A Heuristic Method to Generate Better Initial Population for   Evolutionary Methods", 
    "arxiv-id": "1406.4518v1", 
    "author": "Amin Satlikh Mohammadi", 
    "publish": "2014-06-15T15:20:17Z", 
    "summary": "Initial population plays an important role in heuristic algorithms such as GA\nas it help to decrease the time those algorithms need to achieve an acceptable\nresult. Furthermore, it may influence the quality of the final answer given by\nevolutionary algorithms. In this paper, we shall introduce a heuristic method\nto generate a target based initial population which possess two mentioned\ncharacteristics. The efficiency of the proposed method has been shown by\npresenting the results of our tests on the benchmarks."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22315381/IJETT-V11P286", 
    "link": "http://arxiv.org/pdf/1406.5633v1", 
    "title": "Thermodynamic-RAM Technology Stack", 
    "arxiv-id": "1406.5633v1", 
    "author": "Timothy W. Molter", 
    "publish": "2014-06-21T17:03:11Z", 
    "summary": "We introduce a technology stack or specification describing the multiple\nlevels of abstraction and specialization needed to implement a neuromorphic\nprocessor based on the theory of AHaH Computing. This specific implementation\nis called Thermodynamic-RAM (kT-RAM). Bringing us closer to brain-like neural\ncomputation, kT-RAM will provide a general-purpose adaptive hardware resource\nto existing computing platforms enabling fast and low-power machine learning\ncapabilities that are currently hampered by the separation of memory and\nprocessing. The motivation for defining the technology stack is two-fold.\nFirst, explaining kT-RAM is much easier if it is broken down into smaller, more\nmanageable pieces. Secondly, groups interested in realizing kT-RAM can choose a\nlevel to contribute to that matches their interest and expertise. The levels of\nthe Thermodynamic-RAM technology stack include the memristor, Knowm-Synapse,\nAHaH Node, kT-RAM, kT-RAM instruction set, sparse spike encoding, kT-RAM\nemulator, and SENSE Server."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22315381/IJETT-V11P286", 
    "link": "http://arxiv.org/pdf/1406.7811v1", 
    "title": "An optimization algorithm for multimodal functions inspired by   collective animal behavior", 
    "arxiv-id": "1406.7811v1", 
    "author": "Mauricio Gonzalez", 
    "publish": "2014-06-30T16:48:05Z", 
    "summary": "Interest in multimodal function optimization is expanding rapidly since real\nworld optimization problems often demand locating multiple optima within a\nsearch space. This article presents a new multimodal optimization algorithm\nnamed as the Collective Animal Behavior (CAB). Animal groups, such as schools\nof fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit a\nvariety of behaviors including swarming about a food source, milling around a\ncentral location or migrating over large distances in aligned groups. These\ncollective behaviors are often advantageous to groups, allowing them to\nincrease their harvesting efficiency to follow better migration routes, to\nimprove their aerodynamic and to avoid predation. In the proposed algorithm,\nsearcher agents are a group of animals which interact to each other based on\nthe biological laws of collective motion. Experimental results demonstrate that\nthe proposed algorithm is capable of finding global and local optima of\nbenchmark multimodal optimization problems with a higher efficiency in\ncomparison to other methods reported in the literature."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.0101v1", 
    "title": "Memetic Search in Differential Evolution Algorithm", 
    "arxiv-id": "1408.0101v1", 
    "author": "Rajani Kumari", 
    "publish": "2014-08-01T08:45:14Z", 
    "summary": "Differential Evolution (DE) is a renowned optimization stratagem that can\neasily solve nonlinear and comprehensive problems. DE is a well known and\nuncomplicated population based probabilistic approach for comprehensive\noptimization. It has apparently outperformed a number of Evolutionary\nAlgorithms and further search heuristics in the vein of Particle Swarm\nOptimization at what time of testing over both yardstick and actual world\nproblems. Nevertheless, DE, like other probabilistic optimization algorithms,\nfrom time to time exhibits precipitate convergence and stagnates at suboptimal\nposition. In order to stay away from stagnation behavior while maintaining an\nexcellent convergence speed, an innovative search strategy is introduced, named\nmemetic search in DE. In the planned strategy, positions update equation\ncustomized as per a memetic search stratagem. In this strategy a better\nsolution participates more times in the position modernize procedure. The\nposition update equation is inspired from the memetic search in artificial bee\ncolony algorithm. The proposed strategy is named as Memetic Search in\nDifferential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is\ntested over 8 benchmark optimization problems and three real world optimization\nproblems. A comparative analysis has also been carried out among proposed MSDE\nand original DE. Results show that the anticipated algorithm go one better than\nthe basic DE and its recent deviations in a good number of the experiments."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.0102v1", 
    "title": "Randomized Memetic Artificial Bee Colony Algorithm", 
    "arxiv-id": "1408.0102v1", 
    "author": "Rajani Kumari", 
    "publish": "2014-08-01T08:49:25Z", 
    "summary": "Artificial Bee Colony (ABC) optimization algorithm is one of the recent\npopulation based probabilistic approach developed for global optimization. ABC\nis simple and has been showed significant improvement over other Nature\nInspired Algorithms (NIAs) when tested over some standard benchmark functions\nand for some complex real world optimization problems. Memetic Algorithms also\nbecome one of the key methodologies to solve the very large and complex\nreal-world optimization problems. The solution search equation of Memetic ABC\nis based on Golden Section Search and an arbitrary value which tries to balance\nexploration and exploitation of search space. But still there are some chances\nto skip the exact solution due to its step size. In order to balance between\ndiversification and intensification capability of the Memetic ABC, it is\nrandomized the step size in Memetic ABC. The proposed algorithm is named as\nRandomized Memetic ABC (RMABC). In RMABC, new solutions are generated nearby\nthe best so far solution and it helps to increase the exploitation capability\nof Memetic ABC. The experiments on some test problems of different complexities\nand one well known engineering optimization application show that the proposed\nalgorithm outperforms over Memetic ABC (MeABC) and some other variant of ABC\nalgorithm(like Gbest guided ABC (GABC),Hooke Jeeves ABC (HJABC), Best-So-Far\nABC (BSFABC) and Modified ABC (MABC) in case of almost all the problems."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.0689v2", 
    "title": "Real-Time Traffic Signal Control for Modern Roundabouts by Using   Particle Swarm Optimization-Based Fuzzy Controller", 
    "arxiv-id": "1408.0689v2", 
    "author": "Jun Zhang", 
    "publish": "2014-08-04T14:08:53Z", 
    "summary": "Due to that the existing traffic facilities can hardly be extended,\ndeveloping traffic signal control methods is the most important way to improve\nthe traffic efficiency of modern roundabouts. This paper proposes a novel\ntraffic signal controller with two fuzzy layers for signalizing the roundabout.\nThe outer layer of the controller computes urgency degrees of all the phase\nsubsets and then activates the most urgent subset. This mechanism helps to\ninstantly respond to the current traffic condition of the roundabout so as to\nimprove real-timeness. The inner layer of the controller computes extension\ntime of the current phase. If the extension value is larger than a threshold\nvalue, the current phase is maintained; otherwise the next phase in the running\nphase subset (selected by the outer layer) is activated. The inner layer adopts\nwell-designed phase sequences, which helps to smooth the traffic flows and to\navoid traffic jam. In general, the proposed traffic signal controller is\ncapable of improving real-timeness as well as reducing traffic congestion.\nMoreover, an offline particle swarm optimization (PSO) algorithm is developed\nto optimize the membership functions adopted in the proposed controller. By\nusing optimal membership functions, the performance of the controller can be\nfurther improved. Simulation results demonstrate that the proposed controller\noutperforms previous traffic signal controllers in terms of improving the\ntraffic efficiency of modern roundabouts."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.0998v1", 
    "title": "The Case for a Mixed-Initiative Collaborative Neuroevolution Approach", 
    "arxiv-id": "1408.0998v1", 
    "author": "Julian Togelius", 
    "publish": "2014-08-05T15:18:13Z", 
    "summary": "It is clear that the current attempts at using algorithms to create\nartificial neural networks have had mixed success at best when it comes to\ncreating large networks and/or complex behavior. This should not be unexpected,\nas creating an artificial brain is essentially a design problem. Human design\ningenuity still surpasses computational design for most tasks in most domains,\nincluding architecture, game design, and authoring literary fiction. This leads\nus to ask which the best way is to combine human and machine design capacities\nwhen it comes to designing artificial brains. Both of them have their strengths\nand weaknesses; for example, humans are much too slow to manually specify\nthousands of neurons, let alone the billions of neurons that go into a human\nbrain, but on the other hand they can rely on a vast repository of common-sense\nunderstanding and design heuristics that can help them perform a much better\nguided search in design space than an algorithm. Therefore, in this paper we\nargue for a mixed-initiative approach for collaborative online brain building\nand present first results towards this goal."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.1297v1", 
    "title": "New crossover operators for multiple subset selection tasks", 
    "arxiv-id": "1408.1297v1", 
    "author": "Craig B. Laramee", 
    "publish": "2014-08-06T14:41:04Z", 
    "summary": "We have introduced two crossover operators, MMX-BLXexploit and\nMMX-BLXexplore, for simultaneously solving multiple feature/subset selection\nproblems where the features may have numeric attributes and the subset sizes\nare not predefined. These operators differ on the level of exploration and\nexploitation they perform; one is designed to produce convergence controlled\nmutation and the other exhibits a quasi-constant mutation rate. We illustrate\nthe characteristic of these operators by evolving pattern detectors to\ndistinguish alcoholics from controls using their visually evoked response\npotentials (VERPs). This task encapsulates two groups of subset selection\nproblems; choosing a subset of EEG leads along with the lead-weights (features\nwith attributes) and the other that defines the temporal pattern that\ncharacterizes the alcoholic VERPs. We observed better generalization\nperformance from MMX-BLXexplore. Perhaps, MMX-BLXexploit was handicapped by not\nhaving a restart mechanism. These operators are novel and appears to hold\npromise for solving simultaneous feature selection problems."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1408.5350v1", 
    "title": "Structural bias in population-based algorithms", 
    "arxiv-id": "1408.5350v1", 
    "author": "Fabio Caraffini", 
    "publish": "2014-08-22T16:25:43Z", 
    "summary": "Challenging optimisation problems are abundant in all areas of science. Since\nthe 1950s, scientists have developed ever-diversifying families of black box\noptimisation algorithms designed to address any optimisation problem, requiring\nonly that quality of a candidate solution is calculated via a fitness function\nspecific to the problem. For such algorithms to be successful, at least three\nproperties are required: an effective informed sampling strategy, that guides\ngeneration of new candidates on the basis of fitnesses and locations of\npreviously visited candidates; mechanisms to ensure efficiency, so that same\ncandidates are not repeatedly visited; absence of structural bias, which, if\npresent, would predispose the algorithm towards limiting its search to some\nregions of solution space. The first two of these properties have been\nextensively investigated, however the third is little understood. In this\narticle we provide theoretical and empirical analyses that contribute to the\nunderstanding of structural bias. We prove a theorem concerning dynamics of\npopulation variance in the case of real-valued search spaces. This reveals how\nstructural bias can manifest as non-uniform clustering of population over time.\nTheory predicts that structural bias is exacerbated with increasing population\nsize and problem difficulty. These predictions reveal two previously\nunrecognised aspects of structural bias. Respectively, increasing population\nsize, though ostensibly promoting diversity, will magnify any inherent\nstructural bias, and effects of structural bias are more apparent when faced\nwith difficult problems. Our theoretical result also suggests that two commonly\nused approaches to enhancing exploration, increasing population size and\nincreasing disruptiveness of search operators, have quite distinct implications\nin terms of structural bias."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.0280v1", 
    "title": "Towards a Calculus of Echo State Networks", 
    "arxiv-id": "1409.0280v1", 
    "author": "Darko Stefanovic", 
    "publish": "2014-09-01T02:12:04Z", 
    "summary": "Reservoir computing is a recent trend in neural networks which uses the\ndynamical perturbations on the phase space of a system to compute a desired\ntarget function. We present how one can formulate an expectation of system\nperformance in a simple class of reservoir computing called echo state\nnetworks. In contrast with previous theoretical frameworks, which only reveal\nan upper bound on the total memory in the system, we analytically calculate the\nentire memory curve as a function of the structure of the system and the\nproperties of the input and the target function. We demonstrate the precision\nof our framework by validating its result for a wide range of system sizes and\nspectral radii. Our analytical calculation agrees with numerical simulations.\nTo the best of our knowledge this work presents the first exact analytical\ncharacterization of the memory curve in echo state networks."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.0334v1", 
    "title": "Storing sequences in binary tournament-based neural networks", 
    "arxiv-id": "1409.0334v1", 
    "author": "Michael Rabbat", 
    "publish": "2014-09-01T08:59:27Z", 
    "summary": "An extension to a recently introduced architecture of clique-based neural\nnetworks is presented. This extension makes it possible to store sequences with\nhigh efficiency. To obtain this property, network connections are provided with\norientation and with flexible redundancy carried by both spatial and temporal\nredundancy, a mechanism of anticipation being introduced in the model. In\naddition to the sequence storage with high efficiency, this new scheme also\noffers biological plausibility. In order to achieve accurate sequence\nretrieval, a double layered structure combining hetero-association and\nauto-association is also proposed."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.1143v1", 
    "title": "Tunably Rugged Landscapes with Known Maximum and Minimum", 
    "arxiv-id": "1409.1143v1", 
    "author": "Jeffrey S. Buzas", 
    "publish": "2014-09-03T16:20:43Z", 
    "summary": "We propose NM landscapes as a new class of tunably rugged benchmark problems.\nNM landscapes are well-defined on alphabets of any arity, including both\ndiscrete and real-valued alphabets, include epistasis in a natural and\ntransparent manner, are proven to have known value and location of the global\nmaximum and, with some additional constraints, are proven to also have a known\nglobal minimum. Empirical studies are used to illustrate that, when\ncoefficients are selected from a recommended distribution, the ruggedness of NM\nlandscapes is smoothly tunable and correlates with several measures of search\ndifficulty. We discuss why these properties make NM landscapes preferable to\nboth NK landscapes and Walsh polynomials as benchmark landscape models with\ntunable epistasis."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.1715v1", 
    "title": "An Experimental Study of Adaptive Control for Evolutionary Algorithms", 
    "arxiv-id": "1409.1715v1", 
    "author": "Fr\u00e9d\u00e9ric Saubion", 
    "publish": "2014-09-05T10:01:41Z", 
    "summary": "The balance of exploration versus exploitation (EvE) is a key issue on\nevolutionary computation. In this paper we will investigate how an adaptive\ncontroller aimed to perform Operator Selection can be used to dynamically\nmanage the EvE balance required by the search, showing that the search\nstrategies determined by this control paradigm lead to an improvement of\nsolution quality found by the evolutionary algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.2329v5", 
    "title": "Recurrent Neural Network Regularization", 
    "arxiv-id": "1409.2329v5", 
    "author": "Oriol Vinyals", 
    "publish": "2014-09-08T13:08:00Z", 
    "summary": "We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful\ntechnique for regularizing neural networks, does not work well with RNNs and\nLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show\nthat it substantially reduces overfitting on a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.2697v1", 
    "title": "Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of   Multilevel Inverter Fed Induction Motor", 
    "arxiv-id": "1409.2697v1", 
    "author": "D. D. Neema", 
    "publish": "2014-09-05T19:49:04Z", 
    "summary": "The Particle Swarm Optimized (PSO) fuzzy controller has been proposed for\nindirect vector control of induction motor. In this proposed scheme a Neutral\nPoint Clamped (NPC) multilevel inverter is used and hysteresis current control\ntechnique has been adopted for switching the IGBTs. A Mamdani type fuzzy\ncontroller is used in place of conventional PI controller. To ensure better\nperformance of fuzzy controller all parameters such as membership functions,\nnormalizing and de-normalizing parameters are optimized using PSO. The\nperformance of proposed controller is investigated under various load and speed\nconditions. The simulation results show its stability and robustness for high\nperformance derives applications."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15582-4406", 
    "link": "http://arxiv.org/pdf/1409.2710v1", 
    "title": "eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification", 
    "arxiv-id": "1409.2710v1", 
    "author": "Gopinath Chennupati", 
    "publish": "2014-09-09T12:24:22Z", 
    "summary": "Ant Colony Optimization (ACO) has been applied in supervised learning in\norder to induce classification rules as well as decision trees, named\nAnt-Miners. Although these are competitive classifiers, the stability of these\nclassifiers is an important concern that owes to their stochastic nature. In\nthis paper, to address this issue, an acclaimed machine learning technique\nnamed, ensemble of classifiers is applied, where an ACO classifier is used as a\nbase classifier to prepare the ensemble. The main trade-off is, the predictions\nin the new approach are determined by discovering a group of models as opposed\nto the single model classification. In essence, we prepare multiple models from\nthe randomly replaced samples of training data from which, a unique model is\nprepared by aggregating the models to test the unseen data points. The main\nobjective of this new approach is to increase the stability of the Ant-Miner\nresults there by improving the performance of ACO classification. We found that\nthe ensemble Ant-Miners significantly improved the stability by reducing the\nclassification error on unseen data."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcseit.2014.4405", 
    "link": "http://arxiv.org/pdf/1409.3078v1", 
    "title": "An improved genetic algorithm with a local optimization strategy and an   extra mutation level for solving traveling salesman problem", 
    "arxiv-id": "1409.3078v1", 
    "author": "Vahid Haji Hashemi", 
    "publish": "2014-09-10T14:15:29Z", 
    "summary": "The Traveling salesman problem (TSP) is proved to be NP-complete in most\ncases. The genetic algorithm (GA) is one of the most useful algorithms for\nsolving this problem. In this paper a conventional GA is compared with an\nimproved hybrid GA in solving TSP. The improved or hybrid GA consist of\nconventional GA and two local optimization strategies. The first strategy is\nextracting all sequential groups including four cities of samples and changing\nthe two central cities with each other. The second local optimization strategy\nis similar to an extra mutation process. In this step with a low probability a\nsample is selected. In this sample two random cities are defined and the path\nbetween these cities is reversed. The computation results show that the\nproposed method also finds better paths than the conventional GA within an\nacceptable computation time."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcseit.2014.4405", 
    "link": "http://arxiv.org/pdf/1409.4244v1", 
    "title": "An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem", 
    "arxiv-id": "1409.4244v1", 
    "author": "Ana Carolina Olivera", 
    "publish": "2014-09-12T12:07:47Z", 
    "summary": "The lane reversal has proven to be a useful method to mitigate traffic\ncongestion during rush hour or in case of specific events that affect high\ntraffic volumes. In this work we propose a methodology that is placed within\noptimization via Simulation, by means of which a multi-objective genetic\nalgorithm and simulations of traffic are used to determine the configuration of\nideal lane reversal."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1409.4727v1", 
    "title": "Selection of Most Appropriate Backpropagation Training Algorithm in Data   Pattern Recognition", 
    "arxiv-id": "1409.4727v1", 
    "author": "Agus Harjoko", 
    "publish": "2014-09-11T09:03:38Z", 
    "summary": "There are several training algorithms for backpropagation method in neural\nnetwork. Not all of these algorithms have the same accuracy level demonstrated\nthrough the percentage level of suitability in recognizing patterns in the\ndata. In this research tested 12 training algorithms specifically in recognize\ndata patterns of test validity. The basic network parameters used are the\nmaximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.\nOf the twelve training algorithms each performed 20 times looping. The test\nresults obtained that the percentage rate of the great match is trainlm\nalgorithm with alpha 5% have adequate levels of suitability of 87.5% at the\nlevel of significance of 0.000. This means the most appropriate training\nalgorithm in recognizing the the data pattern of test validity is the trainlm\nalgorithm."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1409.6023v1", 
    "title": "A High-Level Model of Neocortical Feedback Based on an Event Window   Segmentation Algorithm", 
    "arxiv-id": "1409.6023v1", 
    "author": "Jerry R. Van Aken", 
    "publish": "2014-09-21T18:14:48Z", 
    "summary": "The author previously presented an event window segmentation (EWS) algorithm\n[5] that uses purely statistical methods to learn to recognize recurring\npatterns in an input stream of events. In the following discussion, the EWS\nalgorithm is first extended to make predictions about future events. Next, this\nextended algorithm is used to construct a high-level, simplified model of a\nneocortical hierarchy. An event stream enters at the bottom of the hierarchy,\nand drives processing activity upward in the hierarchy. Successively higher\nregions in the hierarchy learn to recognize successively deeper levels of\npatterns in these events as they propagate from the bottom of the hierarchy.\nThe lower levels in the hierarchy use the predictions from the levels above to\nstrengthen their own predictions. A C++ source code listing of the model\nimplementation and test program is included as an appendix."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1409.7478v1", 
    "title": "An Analysis on Selection for High-Resolution Approximations in   Many-Objective Optimization", 
    "arxiv-id": "1409.7478v1", 
    "author": "Kiyoshi Tanaka", 
    "publish": "2014-09-26T06:32:52Z", 
    "summary": "This work studies the behavior of three elitist multi- and many-objective\nevolutionary algorithms generating a high-resolution approximation of the\nPareto optimal set. Several search-assessment indicators are defined to trace\nthe dynamics of survival selection and measure the ability to simultaneously\nkeep optimal solutions and discover new ones under different population sizes,\nset as a fraction of the size of the Pareto optimal set."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1409.7758v1", 
    "title": "Combating Corrupt Messages in Sparse Clustered Associative Memories", 
    "arxiv-id": "1409.7758v1", 
    "author": "Michael Rabbat", 
    "publish": "2014-09-27T03:34:02Z", 
    "summary": "In this paper we analyze and extend the neural network based associative\nmemory proposed by Gripon and Berrou. This associative memory resembles the\ncelebrated Willshaw model with an added partite cluster structure. In the\nliterature, two retrieving schemes have been proposed for the network dynamics,\nnamely sum-of-sum and sum-of-max. They both offer considerably better\nperformance than Willshaw and Hopfield networks, when comparable retrieval\nscenarios are considered. Former discussions and experiments concentrate on the\nerasure scenario, where a partial message is used as a probe to the network, in\nthe hope of retrieving the full message. In this regard, sum-of-max outperforms\nsum-of-sum in terms of retrieval rate by a large margin. However, we observe\nthat when noise and errors are present and the network is queried by a corrupt\nprobe, sum-of-max faces a severe limitation as its stringent activation rule\nprevents a neuron from reviving back into play once deactivated. In this\nmanuscript, we categorize and analyze different error scenarios so that both\nthe erasure and the corrupt scenarios can be treated consistently. We make an\namendment to the network structure to improve the retrieval rate, at the cost\nof an extra scalar per neuron. Afterwards, five different approaches are\nproposed to deal with corrupt probes. As a result, we extend the network\ncapability, and also increase the robustness of the retrieving procedure. We\nthen experimentally compare all these proposals and discuss pros and cons of\neach approach under different types of errors. Simulation results show that if\ncarefully designed, the network is able to preserve both a high retrieval rate\nand a low running time simultaneously, even when queried by a corrupt probe."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1501.00436v1", 
    "title": "An Experimental Analysis of the Echo State Network Initialization Using   the Particle Swarm Optimization", 
    "arxiv-id": "1501.00436v1", 
    "author": "V\u00e1clav Sn\u00e1\u0161el", 
    "publish": "2015-01-02T16:49:29Z", 
    "summary": "This article introduces a robust hybrid method for solving supervised\nlearning tasks, which uses the Echo State Network (ESN) model and the Particle\nSwarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network with\nthe hidden-hidden weights fixed in the learning process. The recurrent part of\nthe network stores the input information in internal states of the network.\nAnother structure forms a free-memory method used as supervised learning tool.\nThe setting procedure for initializing the recurrent structure of the ESN model\ncan impact on the model performance. On the other hand, the PSO has been shown\nto be a successful technique for finding optimal points in complex spaces.\nHere, we present an approach to use the PSO for finding some initial\nhidden-hidden weights of the ESN model. We present empirical results that\ncompare the canonical ESN model with this hybrid method on a wide range of\nbenchmark problems."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1501.02128v1", 
    "title": "Introduction and Ranking Results of the ICSI 2014 Competition on Single   Objective Optimization", 
    "arxiv-id": "1501.02128v1", 
    "author": "Zhongyang Zheng", 
    "publish": "2015-01-09T13:21:11Z", 
    "summary": "This technical report includes the introduction and ranking results of the\nICSI 2014 Competition on Single Objective Optimization."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1007.0417v1", 
    "title": "Delta Learning Rule for the Active Sites Model", 
    "arxiv-id": "1007.0417v1", 
    "author": "Krishna Chaithanya Lingashetty", 
    "publish": "2010-07-02T18:05:29Z", 
    "summary": "This paper reports the results on methods of comparing the memory retrieval\ncapacity of the Hebbian neural network which implements the B-Matrix approach,\nby using the Widrow-Hoff rule of learning. We then, extend the recently\nproposed Active Sites model by developing a delta rule to increase memory\ncapacity. Also, this paper extends the binary neural network to a multi-level\n(non-binary) neural network."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1007.4221v2", 
    "title": "Building Blocks Propagation in Quantum-Inspired Genetic Algorithm", 
    "arxiv-id": "1007.4221v2", 
    "author": "Jacek Kucharski", 
    "publish": "2010-07-23T21:30:18Z", 
    "summary": "This paper presents an analysis of building blocks propagation in\nQuantum-Inspired Genetic Algorithm, which belongs to a new class of\nmetaheuristics drawing their inspiration from both biological evolution and\nunitary evolution of quantum systems. The expected number of quantum\nchromosomes matching a schema has been analyzed and a random variable\ncorresponding to this issue has been introduced. The results have been compared\nwith Simple Genetic Algorithm. Also, it has been presented how selected binary\nquantum chromosomes cover a domain of one-dimensional fitness function."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1007.4707v1", 
    "title": "Simple Max-Min Ant Systems and the Optimization of Linear Pseudo-Boolean   Functions", 
    "arxiv-id": "1007.4707v1", 
    "author": "Markus Wagner", 
    "publish": "2010-07-27T13:07:24Z", 
    "summary": "With this paper, we contribute to the understanding of ant colony\noptimization (ACO) algorithms by formally analyzing their runtime behavior. We\nstudy simple MAX-MIN ant systems on the class of linear pseudo-Boolean\nfunctions defined on binary strings of length 'n'. Our investigations point out\nhow the progress according to function values is stored in pheromone. We\nprovide a general upper bound of O((n^3 \\log n)/ \\rho) for two ACO variants on\nall linear functions, where (\\rho) determines the pheromone update strength.\nFurthermore, we show improved bounds for two well-known linear pseudo-Boolean\nfunctions called OneMax and BinVal and give additional insights using an\nexperimental study."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1104.0283v1", 
    "title": "Evolving a New Feature for a Working Program", 
    "arxiv-id": "1104.0283v1", 
    "author": "Mike Stimpson", 
    "publish": "2011-04-02T03:33:02Z", 
    "summary": "A genetic programming system is created. A first fitness function f1 is used\nto evolve a program that implements a first feature. Then the fitness function\nis switched to a second function f2, which is used to evolve a program that\nimplements a second feature while still maintaining the first feature. The\nmedian number of generations G1 and G2 needed to evolve programs that work as\ndefined by f1 and f2 are measured. The behavior of G1 and G2 are observed as\nthe difficulty of the problem is increased.\n  In these systems, the density D1 of programs that work (for fitness function\nf1) is measured in the general population of programs. The relationship\nG1~1/sqrt(D1) is observed to approximately hold. Also, the density D2 of\nprograms that work (for fitness function f2) is measured in the general\npopulation of programs. The relationship G2~1/sqrt(D2) is observed to\napproximately hold."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1104.0775v2", 
    "title": "Evolving Pacing Strategies for Team Pursuit Track Cycling", 
    "arxiv-id": "1104.0775v2", 
    "author": "Frank Neumann", 
    "publish": "2011-04-05T08:46:33Z", 
    "summary": "Team pursuit track cycling is a bicycle racing sport held on velodromes and\nis part of the Summer Olympics. It involves the use of strategies to minimize\nthe overall time that a team of cyclists needs to complete a race. We present\nan optimisation framework for team pursuit track cycling and show how to evolve\nstrategies using metaheuristics for this interesting real-world problem. Our\nexperimental results show that these heuristics lead to significantly better\nstrategies than state-of-art strategies that are currently used by teams of\ncyclists."
},{
    "category": "cs.NE", 
    "doi": "10.14445/22312803/IJCTT-V14P120", 
    "link": "http://arxiv.org/pdf/1104.2644v1", 
    "title": "Idealized Dynamic Population Sizing for Uniformly Scaled Problems", 
    "arxiv-id": "1104.2644v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2011-04-13T23:24:49Z", 
    "summary": "This paper explores an idealized dynamic population sizing strategy for\nsolving additive decomposable problems of uniform scale. The method is designed\non top of the foundations of existing population sizing theory for this class\nof problems, and is carefully compared with an optimal fixed population sized\ngenetic algorithm. The resulting strategy should be close to a lower bound in\nterms of what can be achieved, performance-wise, by self-adjusting population\nsizing algorithms for this class of problems."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2011.3409", 
    "link": "http://arxiv.org/pdf/1109.0085v1", 
    "title": "Self-Adaptation Mechanism to Control the Diversity of the Population in   Genetic Algorithm", 
    "arxiv-id": "1109.0085v1", 
    "author": "Prabhas Chongstitvatana", 
    "publish": "2011-09-01T04:01:27Z", 
    "summary": "One of the problems in applying Genetic Algorithm is that there is some\nsituation where the evolutionary process converges too fast to a solution which\ncauses it to be trapped in local optima. To overcome this problem, a proper\ndiversity in the candidate solutions must be determined. Most existing\ndiversity-maintenance mechanisms require a problem specific knowledge to setup\nparameters properly. This work proposes a method to control diversity of the\npopulation without explicit parameter setting. A self-adaptation mechanism is\nproposed based on the competition of preference characteristic in mating. It\ncan adapt the population toward proper diversity for the problems. The\nexperiments are carried out to measure the effectiveness of the proposed method\nbased on nine well-known test problems. The performance of the adaptive method\nis comparable to traditional Genetic Algorithm with the best parameter setting."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2011.3409", 
    "link": "http://arxiv.org/pdf/1109.1074v1", 
    "title": "A Framework for Predicting Phishing Websites using Neural Networks", 
    "arxiv-id": "1109.1074v1", 
    "author": "Dr. V. Prasanna Venkatesan", 
    "publish": "2011-09-06T06:05:12Z", 
    "summary": "In India many people are now dependent on online banking. This raises\nsecurity concerns as the banking websites are forged and fraud can be committed\nby identity theft. These forged websites are called as Phishing websites and\ncreated by malicious people to mimic web pages of real websites and it attempts\nto defraud people of their personal information. Detecting and identifying\nphishing websites is a really complex and dynamic problem involving many\nfactors and criteria. This paper discusses about the prediction of phishing\nwebsites using neural networks. A neural network is a multilayer system which\nreduces the error and increases the performance. This paper describes a\nframework to better classify and predict the phishing sites using neural\nnetworks."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2011.3409", 
    "link": "http://arxiv.org/pdf/1109.1211v1", 
    "title": "An Efficient Preprocessing Methodology for Discovering Patterns and   Clustering of Web Users using a Dynamic ART1 Neural Network", 
    "arxiv-id": "1109.1211v1", 
    "author": "G. Kavitha", 
    "publish": "2011-09-06T15:19:48Z", 
    "summary": "In this paper, a complete preprocessing methodology for discovering patterns\nin web usage mining process to improve the quality of data by reducing the\nquantity of data has been proposed. A dynamic ART1 neural network clustering\nalgorithm to group users according to their Web access patterns with its neat\narchitecture is also proposed. Several experiments are conducted and the\nresults show the proposed methodology reduces the size of Web log files down to\n73-82% of the initial size and the proposed ART1 algorithm is dynamic and\nlearns relatively stable quality clusters."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2011.3409", 
    "link": "http://arxiv.org/pdf/1109.1504v2", 
    "title": "A New Method for Lower Bounds on the Running Time of Evolutionary   Algorithms", 
    "arxiv-id": "1109.1504v2", 
    "author": "Dirk Sudholt", 
    "publish": "2011-09-07T16:08:28Z", 
    "summary": "We present a new method for proving lower bounds on the expected running time\nof evolutionary algorithms. It is based on fitness-level partitions and an\nadditional condition on transition probabilities between fitness levels. The\nmethod is versatile, intuitive, elegant, and very powerful. It yields exact or\nnear-exact lower bounds for LO, OneMax, long k-paths, and all functions with a\nunique optimum. Most lower bounds are very general: they hold for all\nevolutionary algorithms that only use bit-flip mutation as variation\noperator---i.e. for all selection operators and population models. The lower\nbounds are stated with their dependence on the mutation rate.\n  These results have very strong implications. They allow to determine the\noptimal mutation-based algorithm for LO and OneMax, i.e., which algorithm\nminimizes the expected number of fitness evaluations. This includes the choice\nof the optimal mutation rate."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2011.3409", 
    "link": "http://arxiv.org/pdf/1109.1766v1", 
    "title": "Analysis of Speedups in Parallel Evolutionary Algorithms for   Combinatorial Optimization", 
    "arxiv-id": "1109.1766v1", 
    "author": "Dirk Sudholt", 
    "publish": "2011-09-08T16:40:49Z", 
    "summary": "Evolutionary algorithms are popular heuristics for solving various\ncombinatorial problems as they are easy to apply and often produce good\nresults. Island models parallelize evolution by using different populations,\ncalled islands, which are connected by a graph structure as communication\ntopology. Each island periodically communicates copies of good solutions to\nneighboring islands in a process called migration.\n  We consider the speedup gained by island models in terms of the parallel\nrunning time for problems from combinatorial optimization: sorting (as\nmaximization of sortedness), shortest paths, and Eulerian cycles. Different\nsearch operators are considered. The results show in which settings and up to\nwhat degree evolutionary algorithms can be parallelized efficiently. Along the\nway, we also investigate how island models deal with plateaus. In particular,\nwe show that natural settings lead to exponential vs. logarithmic speedups,\ndepending on the frequency of migration."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1109.2146v1", 
    "title": "CIXL2: A Crossover Operator for Evolutionary Algorithms Based on   Population Features", 
    "arxiv-id": "1109.2146v1", 
    "author": "D. Ortiz-Boyer", 
    "publish": "2011-09-09T20:32:23Z", 
    "summary": "In this paper we propose a crossover operator for evolutionary algorithms\nwith real values that is based on the statistical theory of population\ndistributions. The operator is based on the theoretical distribution of the\nvalues of the genes of the best individuals in the population. The proposed\noperator takes into account the localization and dispersion features of the\nbest individuals of the population with the objective that these features would\nbe inherited by the offspring. Our aim is the optimization of the balance\nbetween exploration and exploitation in the search process. In order to test\nthe efficiency and robustness of this crossover, we have used a set of\nfunctions to be optimized with regard to different criteria, such as,\nmultimodality, separability, regularity and epistasis. With this set of\nfunctions we can extract conclusions in function of the problem at hand. We\nanalyze the results using ANOVA and multiple comparison statistical tests. As\nan example of how our crossover can be used to solve artificial intelligence\nproblems, we have applied the proposed model to the problem of obtaining the\nweight of each network in a ensemble of neural networks. The results obtained\nare above the performance of standard methods."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1109.2788v1", 
    "title": "Developing a supervised training algorithm for limited precision   feed-forward spiking neural networks", 
    "arxiv-id": "1109.2788v1", 
    "author": "Evangelos Stromatias", 
    "publish": "2011-09-13T13:58:07Z", 
    "summary": "Spiking neural networks have been referred to as the third generation of\nartificial neural networks where the information is coded as time of the\nspikes. There are a number of different spiking neuron models available and\nthey are categorized based on their level of abstraction. In addition, there\nare two known learning methods, unsupervised and supervised learning. This\nthesis focuses on supervised learning where a new algorithm is proposed, based\non genetic algorithms. The proposed algorithm is able to train both synaptic\nweights and delays and also allow each neuron to emit multiple spikes thus\ntaking full advantage of the spatial-temporal coding power of the spiking\nneurons. In addition, limited synaptic precision is applied; only six bits are\nused to describe and train a synapse, three bits for the weights and three bits\nfor the delays. Two limited precision schemes are investigated. The proposed\nalgorithm is tested on the XOR classification problem where it produces better\nresults for even smaller network architectures than the proposed ones.\nFurthermore, the algorithm is benchmarked on the Fisher iris classification\nproblem where it produces higher classification accuracies compared to\nSpikeProp, QuickProp and Rprop. Finally, a hardware implementation on a\nmicrocontroller is done for the XOR problem as a proof of concept. Keywords:\nSpiking neural networks, supervised learning, limited synaptic precision,\ngenetic algorithms, hardware implementation."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1109.6441v1", 
    "title": "Memetic Algorithms: Parametrization and Balancing Local and Global   Search", 
    "arxiv-id": "1109.6441v1", 
    "author": "Dirk Sudholt", 
    "publish": "2011-09-29T08:53:36Z", 
    "summary": "This is a preprint of a book chapter from the Handbook of Memetic Algorithms,\nStudies in Computational Intelligence, Vol. 379, ISBN 978-3-642-23246-6,\nSpringer, edited by F. Neri, C. Cotta, and P. Moscato. It is devoted to the\nparametrization of memetic algorithms and how to find a good balance between\nglobal and local search."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.0183v1", 
    "title": "Neural Network Model for Path-Planning of Robotic Rover Systems", 
    "arxiv-id": "1204.0183v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-04-01T09:24:19Z", 
    "summary": "Today, robotics is an auspicious and fast-growing branch of technology that\ninvolves the manufacturing, design, and maintenance of robot machines that can\noperate in an autonomous fashion and can be used in a wide variety of\napplications including space exploration, weaponry, household, and\ntransportation. More particularly, in space applications, a common type of\nrobots has been of widespread use in the recent years. It is called planetary\nrover which is a robot vehicle that moves across the surface of a planet and\nconducts detailed geological studies pertaining to the properties of the\nlanding cosmic environment. However, rovers are always impeded by obstacles\nalong the traveling path which can destabilize the rover's body and prevent it\nfrom reaching its goal destination. This paper proposes an ANN model that\nallows rover systems to carry out autonomous path-planning to successfully\nnavigate through challenging planetary terrains and follow their goal location\nwhile avoiding dangerous obstacles. The proposed ANN is a multilayer network\nmade out of three layers: an input, a hidden, and an output layer. The network\nis trained in offline mode using back-propagation supervised learning\nalgorithm. A software-simulated rover was experimented and it revealed that it\nwas able to follow the safest trajectory despite existing obstacles. As future\nwork, the proposed ANN is to be parallelized so as to speed-up the execution\ntime of the training process."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.0262v2", 
    "title": "Managing contextual artificial neural networks with a service-based   mediator", 
    "arxiv-id": "1204.0262v2", 
    "author": "Greg Fish", 
    "publish": "2012-04-01T20:17:32Z", 
    "summary": "Today, a wide variety of probabilistic and expert AI systems used to analyze\nreal world inputs such as unstructured text, sounds, images, and statistical\ndata. However, all these systems exist on different platforms, with different\nimplementations, and with very different, often very specific goals in mind.\nThis paper introduces a concept for a mediator framework for such systems and\nseeks to show several architectures which would support it, potential benefits\nin combining the signals of disparate networks for formalized, high level logic\nand signal processing, and its possible academic and industrial uses."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.1706v1", 
    "title": "Efficient Design of Triplet Based Spike-Timing Dependent Plasticity", 
    "arxiv-id": "1204.1706v1", 
    "author": "Derek Abbott", 
    "publish": "2012-04-08T04:02:52Z", 
    "summary": "Spike-Timing Dependent Plasticity (STDP) is believed to play an important\nrole in learning and the formation of computational function in the brain. The\nclassical model of STDP which considers the timing between pairs of\npre-synaptic and post-synaptic spikes (p-STDP) is incapable of reproducing\nsynaptic weight changes similar to those seen in biological experiments which\ninvestigate the effect of either higher order spike trains (e.g. triplet and\nquadruplet of spikes), or, simultaneous effect of the rate and timing of spike\npairs on synaptic plasticity. In this paper, we firstly investigate synaptic\nweight changes using a p-STDP circuit and show how it fails to reproduce the\nmentioned complex biological experiments. We then present a new STDP VLSI\ncircuit which acts based on the timing among triplets of spikes (t-STDP) that\nis able to reproduce all the mentioned experimental results. We believe that\nour new STDP VLSI circuit improves upon previous circuits, whose learning\ncapacity exceeds current designs due to its capability of mimicking the\noutcomes of biological experiments more closely; thus plays a significant role\nin future VLSI implementation of neuromorphic systems."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.2139v1", 
    "title": "Affine Image Registration Transformation Estimation Using a Real Coded   Genetic Algorithm with SBX", 
    "arxiv-id": "1204.2139v1", 
    "author": "Hamid Reza Shahbazkia", 
    "publish": "2012-04-10T13:19:45Z", 
    "summary": "This paper describes the application of a real coded genetic algorithm (GA)\nto align two or more 2-D images by means of image registration. The proposed\nsearch strategy is a transformation parameters-based approach involving the\naffine transform. The real coded GA uses Simulated Binary Crossover (SBX), a\nparent-centric recombination operator that has shown to deliver a good\nperformance in many optimization problems in the continuous domain. In\naddition, we propose a new technique for matching points between a warped and\nstatic images by using a randomized ordering when visiting the points during\nthe matching procedure. This new technique makes the evaluation of the\nobjective function somewhat noisy, but GAs and other population-based search\nalgorithms have been shown to cope well with noisy fitness evaluations. The\nresults obtained are competitive to those obtained by state-of-the-art\nclassical methods in image registration, confirming the usefulness of the\nproposed noisy objective function and the suitability of SBX as a recombination\noperator for this type of problem."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.2356v1", 
    "title": "Self-Adaptive Surrogate-Assisted Covariance Matrix Adaptation Evolution   Strategy", 
    "arxiv-id": "1204.2356v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2012-04-11T07:00:31Z", 
    "summary": "This paper presents a novel mechanism to adapt surrogate-assisted\npopulation-based algorithms. This mechanism is applied to ACM-ES, a recently\nproposed surrogate-assisted variant of CMA-ES. The resulting algorithm,\nsaACM-ES, adjusts online the lifelength of the current surrogate model (the\nnumber of CMA-ES generations before learning a new surrogate) and the surrogate\nhyper-parameters. Both heuristics significantly improve the quality of the\nsurrogate model, yielding a significant speed-up of saACM-ES compared to the\nACM-ES and CMA-ES baselines. The empirical validation of saACM-ES on the\nBBOB-2012 noiseless testbed demonstrates the efficiency and the scalability\nw.r.t the problem dimension and the population size of the proposed approach,\nthat reaches new best results on some of the benchmark problems."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1204.4560v1", 
    "title": "A Fast and Effective Local Search Algorithm for Optimizing the Placement   of Wind Turbines", 
    "arxiv-id": "1204.4560v1", 
    "author": "Frank Neumann", 
    "publish": "2012-04-20T08:20:41Z", 
    "summary": "The placement of wind turbines on a given area of land such that the wind\nfarm produces a maximum amount of energy is a challenging optimization problem.\nIn this article, we tackle this problem, taking into account wake effects that\nare produced by the different turbines on the wind farm. We significantly\nimprove upon existing results for the minimization of wake effects by\ndeveloping a new problem-specific local search algorithm. One key step in the\nspeed-up of our algorithm is the reduction in computation time needed to assess\na given wind farm layout compared to previous approaches. Our new method allows\nthe optimization of large real-world scenarios within a single night on a\nstandard computer, whereas weeks on specialized computing servers were required\nfor previous approaches."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1207.0369v1", 
    "title": "More Effective Crossover Operators for the All-Pairs Shortest Path   Problem", 
    "arxiv-id": "1207.0369v1", 
    "author": "Madeleine Theile", 
    "publish": "2012-07-02T13:14:14Z", 
    "summary": "The all-pairs shortest path problem is the first non-artificial problem for\nwhich it was shown that adding crossover can significantly speed up a\nmutation-only evolutionary algorithm. Recently, the analysis of this algorithm\nwas refined and it was shown to have an expected optimization time (w.r.t. the\nnumber of fitness evaluations) of $\\Theta(n^{3.25}(\\log n)^{0.25})$.\n  In contrast to this simple algorithm, evolutionary algorithms used in\npractice usually employ refined recombination strategies in order to avoid the\ncreation of infeasible offspring. We study extensions of the basic algorithm by\ntwo such concepts which are central in recombination, namely \\emph{repair\nmechanisms} and \\emph{parent selection}. We show that repairing infeasible\noffspring leads to an improved expected optimization time of\n$\\mathord{O}(n^{3.2}(\\log n)^{0.2})$. As a second part of our study we prove\nthat choosing parents that guarantee feasible offspring results in an even\nbetter optimization time of $\\mathord{O}(n^{3}\\log n)$.\n  Both results show that already simple adjustments of the recombination\noperator can asymptotically improve the runtime of evolutionary algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1613/jair.1660", 
    "link": "http://arxiv.org/pdf/1207.0702v1", 
    "title": "Meme as Building Block for Evolutionary Optimization of Problem   Instances", 
    "arxiv-id": "1207.0702v1", 
    "author": "Ivor Wai-Hung Tsang", 
    "publish": "2012-07-03T14:44:27Z", 
    "summary": "A significantly under-explored area of evolutionary optimization in the\nliterature is the study of optimization methodologies that can evolve along\nwith the problems solved. Particularly, present evolutionary optimization\napproaches generally start their search from scratch or the ground-zero state\nof knowledge, independent of how similar the given new problem of interest is\nto those optimized previously. There has thus been the apparent lack of\nautomated knowledge transfers and reuse across problems. Taking the cue, this\npaper introduces a novel Memetic Computational Paradigm for search, one that\nmodels after how human solves problems, and embarks on a study towards\nintelligent evolutionary optimization of problems through the transfers of\nstructured knowledge in the form of memes learned from previous problem-solving\nexperiences, to enhance future evolutionary searches. In particular, the\nproposed memetic search paradigm is composed of four culture-inspired\noperators, namely, Meme Learning, Meme Selection, Meme Variation and Meme\nImitation. The learning operator mines for memes in the form of latent\nstructures derived from past experiences of problem-solving. The selection\noperator identifies the fit memes that replicate and transmit across problems,\nwhile the variation operator introduces innovations into the memes. The\nimitation operator, on the other hand, defines how fit memes assimilate into\nthe search process of newly encountered problems, thus gearing towards\nefficient and effective evolutionary optimization. Finally, comprehensive\nstudies on two widely studied challenging well established NP-hard routing\nproblem domains, particularly, the capacitated vehicle routing (CVR) and\ncapacitated arc routing (CAR), confirm the high efficacy of the proposed\nmemetic computational search paradigm for intelligent evolutionary optimization\nof problems."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2012.2302", 
    "link": "http://arxiv.org/pdf/1207.2630v1", 
    "title": "Nugget Discovery with a Multi-objective Cultural Algorithm", 
    "arxiv-id": "1207.2630v1", 
    "author": "Sivakumar Ramakrishnan", 
    "publish": "2012-07-11T13:18:55Z", 
    "summary": "Partial classification popularly known as nugget discovery comes under\ndescriptive knowledge discovery. It involves mining rules for a target class of\ninterest. Classification \"If-Then\" rules are the most sought out by decision\nmakers since they are the most comprehensible form of knowledge mined by data\nmining techniques. The rules have certain properties namely the rule metrics\nwhich are used to evaluate them. Mining rules with user specified properties\ncan be considered as a multi-objective optimization problem since the rules\nhave to satisfy more than one property to be used by the user. Cultural\nalgorithm (CA) with its knowledge sources have been used in solving many\noptimization problems. However research gap exists in using cultural algorithm\nfor multi-objective optimization of rules. In the current study a\nmulti-objective cultural algorithm is proposed for partial classification.\nResults of experiments on benchmark data sets reveal good performance."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.02.008", 
    "link": "http://arxiv.org/pdf/1207.3368v1", 
    "title": "Learning the Pseudoinverse Solution to Network Weights", 
    "arxiv-id": "1207.3368v1", 
    "author": "Andre van Schaik", 
    "publish": "2012-07-13T21:28:17Z", 
    "summary": "The last decade has seen the parallel emergence in computational neuroscience\nand machine learning of neural network structures which spread the input signal\nrandomly to a higher dimensional space; perform a nonlinear activation; and\nthen solve for a regression or classification output by means of a mathematical\npseudoinverse operation. In the field of neuromorphic engineering, these\nmethods are increasingly popular for synthesizing biologically plausible neural\nnetworks, but the \"learning method\" - computation of the pseudoinverse by\nsingular value decomposition - is problematic both for biological plausibility\nand because it is not an online or an adaptive method. We present an online or\nincremental method of computing the pseudoinverse, which we argue is\nbiologically plausible as a learning method, and which can be made adaptable\nfor non-stationary data streams. The method is significantly more\nmemory-efficient than the conventional computation of pseudoinverses by\nsingular value decomposition."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.02.008", 
    "link": "http://arxiv.org/pdf/1207.4318v1", 
    "title": "Empirical review of standard benchmark functions using evolutionary   global optimization", 
    "arxiv-id": "1207.4318v1", 
    "author": "Bernd Hartke", 
    "publish": "2012-07-18T09:44:22Z", 
    "summary": "We have employed a recent implementation of genetic algorithms to study a\nrange of standard benchmark functions for global optimization. It turns out\nthat some of them are not very useful as challenging test functions, since they\nneither allow for a discrimination between different variants of genetic\noperators nor exhibit a dimensionality scaling resembling that of real-world\nproblems, for example that of global structure optimization of atomic and\nmolecular clusters. The latter properties seem to be simulated better by two\nother types of benchmark functions. One type is designed to be deceptive,\nexemplified here by Lunacek's function. The other type offers additional\nadvantages of markedly increased complexity and of broad tunability in search\nspace characteristics. For the latter type, we use an implementation based on\nrandomly distributed Gaussians. We advocate the use of the latter types of test\nfunctions for algorithm development and benchmarking."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2001576.2001842", 
    "link": "http://arxiv.org/pdf/1207.4626v1", 
    "title": "The Road to VEGAS: Guiding the Search over Neutral Networks", 
    "arxiv-id": "1207.4626v1", 
    "author": "S\u00e9bastien Verel", 
    "publish": "2012-07-19T12:05:15Z", 
    "summary": "VEGAS (Varying Evolvability-Guided Adaptive Search) is a new methodology\nproposed to deal with the neutrality property of some optimization problems. ts\nmain feature is to consider the whole neutral network rather than an arbitrary\nsolution. Moreover, VEGAS is designed to escape from plateaus based on the\nevolvability of solution and a multi-armed bandit. Experiments are conducted on\nNK-landscapes with neutrality. Results show the importance of considering the\nwhole neutral network and of guiding the search cleverly. The impact of the\nlevel of neutrality and of the exploration-exploitation trade-off are deeply\nanalyzed."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2001576.2001842", 
    "link": "http://arxiv.org/pdf/1207.6682v1", 
    "title": "Exploring Promising Stepping Stones by Combining Novelty Search with   Interactive Evolution", 
    "arxiv-id": "1207.6682v1", 
    "author": "Kenneth O. Stanley", 
    "publish": "2012-07-28T03:11:41Z", 
    "summary": "The field of evolutionary computation is inspired by the achievements of\nnatural evolution, in which there is no final objective. Yet the pursuit of\nobjectives is ubiquitous in simulated evolution. A significant problem is that\nobjective approaches assume that intermediate stepping stones will increasingly\nresemble the final objective when in fact they often do not. The consequence is\nthat while solutions may exist, searching for such objectives may not discover\nthem. This paper highlights the importance of leveraging human insight during\nsearch as an alternative to articulating explicit objectives. In particular, a\nnew approach called novelty-assisted interactive evolutionary computation\n(NA-IEC) combines human intuition with novelty search for the first time to\nfacilitate the serendipitous discovery of agent behaviors. In this approach,\nthe human user directs evolution by selecting what is interesting from the\non-screen population of behaviors. However, unlike in typical IEC, the user can\nnow request that the next generation be filled with novel descendants. The\nexperimental results demonstrate that combining human insight with novelty\nsearch finds solutions significantly faster and at lower genomic complexities\nthan fully-automated processes, including pure novelty search, suggesting an\nimportant role for human users in the search for solutions."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2001576.2001842", 
    "link": "http://arxiv.org/pdf/1301.0048v1", 
    "title": "Generating High-Order Threshold Functions with Multiple Thresholds", 
    "arxiv-id": "1301.0048v1", 
    "author": "Kiyonori Miyasaki", 
    "publish": "2013-01-01T02:20:31Z", 
    "summary": "In this paper, we consider situations in which a given logical function is\nrealized by a multithreshold threshold function. In such situations, constant\nfunctions can be easily obtained from multithreshold threshold functions, and\ntherefore, we can show that it becomes possible to optimize a class of\nhigh-order neural networks. We begin by proposing a generating method for\nthreshold functions in which we use a vector that determines the boundary\nbetween the linearly separable function and the high-order threshold function.\nBy applying this method to high-order threshold functions, we show that\nfunctions with the same weight as, but a different threshold than, a threshold\nfunction generated by the generation process can be easily obtained. We also\nshow that the order of the entire network can be extended while maintaining the\nstructure of given functions."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2001576.2001842", 
    "link": "http://arxiv.org/pdf/1301.0785v1", 
    "title": "Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio", 
    "arxiv-id": "1301.0785v1", 
    "author": "Dilip S Aldar", 
    "publish": "2013-01-04T18:08:30Z", 
    "summary": "Radio Spectrum is most precious and scarce resource and must be utilized\nefficiently and effectively. Cognitive radio is the promising solutions for the\noptimum utilization of the scared natural resource. The spectrum owned by the\nprimary user should be shared among the secondary user, but primary user should\nnot be interfered by the secondary user. In order to utilize the primary user\nspectrum, secondary user must detect accurately, the existence of primary in\nthe band of interest. In cooperative spectrum sensing, the channel between the\nsecondary users and the cognitive radio base station is non stationary and\ncauses interference in the decision in decision fusion and in information in\ninformation due to multipath fading. In this paper neural network based\ncooperative spectrum sensing method is proposed, the performance of proposed\nmethod is evaluated and observed that, the neural network based scheme\nperformance improve significantly over the AND,OR and Majority rule"
},{
    "category": "cs.NE", 
    "doi": "10.1145/2001576.2001842", 
    "link": "http://arxiv.org/pdf/1301.0929v1", 
    "title": "Hybridization of Evolutionary Algorithms", 
    "arxiv-id": "1301.0929v1", 
    "author": "Janez Brest", 
    "publish": "2013-01-05T18:38:14Z", 
    "summary": "Evolutionary algorithms are good general problem solver but suffer from a\nlack of domain specific knowledge. However, the problem specific knowledge can\nbe added to evolutionary algorithms by hybridizing. Interestingly, all the\nelements of the evolutionary algorithms can be hybridized. In this chapter, the\nhybridization of the three elements of the evolutionary algorithms is\ndiscussed: the objective function, the survivor selection operator and the\nparameter settings. As an objective function, the existing heuristic function\nthat construct the solution of the problem in traditional way is used. However,\nthis function is embedded into the evolutionary algorithm that serves as a\ngenerator of new solutions. In addition, the objective function is improved by\nlocal search heuristics. The new neutral selection operator has been developed\nthat is capable to deal with neutral solutions, i.e. solutions that have the\ndifferent representation but expose the equal values of objective function. The\naim of this operator is to directs the evolutionary search into a new\nundiscovered regions of the search space. To avoid of wrong setting of\nparameters that control the behavior of the evolutionary algorithm, the\nself-adaptation is used. Finally, such hybrid self-adaptive evolutionary\nalgorithm is applied to the two real-world NP-hard problems: the graph\n3-coloring and the optimization of markers in the clothing industry. Extensive\nexperiments shown that these hybridization improves the results of the\nevolutionary algorithms a lot. Furthermore, the impact of the particular\nhybridizations is analyzed in details as well."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10589-012-9496-5", 
    "link": "http://arxiv.org/pdf/1301.0939v1", 
    "title": "Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm", 
    "arxiv-id": "1301.0939v1", 
    "author": "Bogdan Filipi\u010d", 
    "publish": "2013-01-05T20:11:43Z", 
    "summary": "This paper proposes a hybrid self-adaptive evolutionary algorithm for graph\ncoloring that is hybridized with the following novel elements: heuristic\ngenotype-phenotype mapping, a swap local search heuristic, and a neutral\nsurvivor selection operator. This algorithm was compared with the evolutionary\nalgorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz\nand de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. The\nperformance of these algorithms were tested on a test suite consisting of\nrandomly generated 3-colorable graphs of various structural features, such as\ngraph size, type, edge density, and variability in sizes of color classes.\nFurthermore, the test graphs were generated including the phase transition\nwhere the graphs are hard to color. The purpose of the extensive experimental\nwork was threefold: to investigate the behavior of the tested algorithms in the\nphase transition, to identify what impact hybridization with the DSatur\ntraditional heuristic has on the evolutionary algorithm, and to show how graph\nstructural features influence the performance of the graph-coloring algorithms.\nThe results indicate that the performance of the hybrid self-adaptive\nevolutionary algorithm is comparable with, or better than, the performance of\nthe hybrid evolutionary algorithm which is one of the best graph-coloring\nalgorithms today. Moreover, the fact that all the considered algorithms\nperformed poorly on flat graphs confirms that this type of graphs is really the\nhardest to color."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10589-012-9496-5", 
    "link": "http://arxiv.org/pdf/1301.4662v1", 
    "title": "Recurrent Neural Network Method in Arabic Words Recognition System", 
    "arxiv-id": "1301.4662v1", 
    "author": "Yusuf Perwej", 
    "publish": "2013-01-20T14:29:56Z", 
    "summary": "The recognition of unconstrained handwriting continues to be a difficult task\nfor computers despite active research for several decades. This is because\nhandwritten text offers great challenges such as character and word\nsegmentation, character recognition, variation between handwriting styles,\ndifferent character size and no font constraints as well as the background\nclarity. In this paper primarily discussed Online Handwriting Recognition\nmethods for Arabic words which being often used among then across the Middle\nEast and North Africa people. Because of the characteristic of the whole body\nof the Arabic words, namely connectivity between the characters, thereby the\nsegmentation of An Arabic word is very difficult. We introduced a recurrent\nneural network to online handwriting Arabic word recognition. The key\ninnovation is a recently produce recurrent neural networks objective function\nknown as connectionist temporal classification. The system consists of an\nadvanced recurrent neural network with an output layer designed for sequence\nlabeling, partially combined with a probabilistic language model. Experimental\nresults show that unconstrained Arabic words achieve recognition rates about\n79%, which is significantly higher than the about 70% using a previously\ndeveloped hidden markov model based recognition system."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.03.003", 
    "link": "http://arxiv.org/pdf/1304.0090v1", 
    "title": "A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic   Plasticity", 
    "arxiv-id": "1304.0090v1", 
    "author": "Nicolangelo Iannella", 
    "publish": "2013-03-30T11:25:35Z", 
    "summary": "Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerful\nsynaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP).\nHere, the TSTDP is capable of reproducing the outcomes from a variety of\nbiological experiments, while the PSTDP rule fails to reproduce them.\nAdditionally, it has been shown that the behaviour inherent to the spike\nrate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can also\nemerge from the TSTDP rule. This paper proposes an analog implementation of the\nTSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 um\nCMOS process and has been simulated using design kits for Synopsys and Cadence\ntools. Simulation results demonstrate how well the proposed circuit can alter\nsynaptic weights according to the timing difference amongst a set of different\npatterns of spikes. Furthermore, the circuit is shown to give rise to a\nBCM-like learning rule, which is a rate-based rule. To mimic implementation\nenvironment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposed\ncircuit. The presented MC simulation analysis and the simulation result from\nfine-tuned circuits show that, it is possible to mitigate the effect of process\nvariations in the proof of concept circuit, however, a practical variation\naware design technique is required to promise a high circuit performance in a\nlarge scale neural network. We believe that the proposed design can play a\nsignificant role in future VLSI implementations of both spike timing and rate\nbased neuromorphic learning systems."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.03.003", 
    "link": "http://arxiv.org/pdf/1304.0751v1", 
    "title": "A Cumulative Multi-Niching Genetic Algorithm for Multimodal Function   Optimization", 
    "arxiv-id": "1304.0751v1", 
    "author": "Matthew Hall", 
    "publish": "2013-03-03T00:47:21Z", 
    "summary": "This paper presents a cumulative multi-niching genetic algorithm (CMN GA),\ndesigned to expedite optimization problems that have computationally-expensive\nmultimodal objective functions. By never discarding individuals from the\npopulation, the CMN GA makes use of the information from every objective\nfunction evaluation as it explores the design space. A fitness-related\npopulation density control over the design space reduces unnecessary objective\nfunction evaluations. The algorithm's novel arrangement of genetic operations\nprovides fast and robust convergence to multiple local optima. Benchmark tests\nalongside three other multi-niching algorithms show that the CMN GA has a\ngreater convergence ability and provides an order-of-magnitude reduction in the\nnumber of objective function evaluations required to achieve a given level of\nconvergence."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2013.03.003", 
    "link": "http://arxiv.org/pdf/1304.2467v1", 
    "title": "Evolutionary Design of Digital Circuits Using Genetic Programming", 
    "arxiv-id": "1304.2467v1", 
    "author": "M. M. A. Hashem", 
    "publish": "2013-04-09T06:40:59Z", 
    "summary": "For simple digital circuits, conventional method of designing circuits can\neasily be applied. But for complex digital circuits, the conventional method of\ndesigning circuits is not fruitfully applicable because it is time-consuming.\nOn the contrary, Genetic Programming is used mostly for automatic program\ngeneration. The modern approach for designing Arithmetic circuits, commonly\ndigital circuits, is based on Graphs. This graph-based evolutionary design of\narithmetic circuits is a method of optimized designing of arithmetic circuits.\nIn this paper, a new technique for evolutionary design of digital circuits is\nproposed using Genetic Programming (GP) with Subtree Mutation in place of\nGraph-based design. The results obtained using this technique demonstrates the\npotential capability of genetic programming in digital circuit design with\nlimited computer algorithms. The proposed technique, helps to simplify and\nspeed up the process of designing digital circuits, discovers a variation in\nthe field of digital circuit design where optimized digital circuits can be\nsuccessfully and effectively designed."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICCCE.2010.5556865", 
    "link": "http://arxiv.org/pdf/1304.2543v1", 
    "title": "A New Distributed Evolutionary Computation Technique for Multi-Objective   Optimization", 
    "arxiv-id": "1304.2543v1", 
    "author": "M. M. A. Hashem", 
    "publish": "2013-04-09T11:43:13Z", 
    "summary": "Now-a-days, it is important to find out solutions of Multi-Objective\nOptimization Problems (MOPs). Evolutionary Strategy helps to solve such real\nworld problems efficiently and quickly. But sequential Evolutionary Algorithms\n(EAs) require an enormous computation power to solve such problems and it takes\nmuch time to solve large problems. To enhance the performance for solving this\ntype of problems, this paper presents a new Distributed Novel Evolutionary\nStrategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESA\napplies the divide-and-conquer approach to decompose population into smaller\nsub-population and involves multiple solutions in the form of cooperative\nsub-populations. In DNESA, the server distributes the total computation load to\nall associate clients and simulation results show that the time for solving\nlarge problems is much less than sequential EAs. Also DNESA shows better\nperformance in convergence test when compared with other three well-known EAs."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463556", 
    "link": "http://arxiv.org/pdf/1304.3138v1", 
    "title": "Sustainable Cooperative Coevolution with a Multi-Armed Bandit", 
    "arxiv-id": "1304.3138v1", 
    "author": "Denis Laurendeau", 
    "publish": "2013-04-10T20:24:31Z", 
    "summary": "This paper proposes a self-adaptation mechanism to manage the resources\nallocated to the different species comprising a cooperative coevolutionary\nalgorithm. The proposed approach relies on a dynamic extension to the\nwell-known multi-armed bandit framework. At each iteration, the dynamic\nmulti-armed bandit makes a decision on which species to evolve for a\ngeneration, using the history of progress made by the different species to\nguide the decisions. We show experimentally, on a benchmark and a real-world\nproblem, that evolving the different populations at different paces allows not\nonly to identify solutions more rapidly, but also improves the capacity of\ncooperative coevolution to solve more complex problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11721-013-0081-z", 
    "link": "http://arxiv.org/pdf/1304.3362v1", 
    "title": "Evolution of Swarm Robotics Systems with Novelty Search", 
    "arxiv-id": "1304.3362v1", 
    "author": "Anders Lyhne Christensen", 
    "publish": "2013-04-11T16:39:40Z", 
    "summary": "Novelty search is a recent artificial evolution technique that challenges\ntraditional evolutionary approaches. In novelty search, solutions are rewarded\nbased on their novelty, rather than their quality with respect to a predefined\nobjective. The lack of a predefined objective precludes premature convergence\ncaused by a deceptive fitness function. In this paper, we apply novelty search\ncombined with NEAT to the evolution of neural controllers for homogeneous\nswarms of robots. Our empirical study is conducted in simulation, and we use a\ncommon swarm robotics task - aggregation, and a more challenging task - sharing\nof an energy recharging station. Our results show that novelty search is\nunaffected by deception, is notably effective in bootstrapping the evolution,\ncan find solutions with lower complexity than fitness-based evolution, and can\nfind a broad diversity of solutions for the same task. Even in non-deceptive\nsetups, novelty search achieves solution qualities similar to those obtained in\ntraditional fitness-based evolution. Our study also encompasses variants of\nnovelty search that work in concert with fitness-based evolution to combine the\nexploratory character of novelty search with the exploitatory character of\nobjective-based evolution. We show that these variants can further improve the\nperformance of novelty search. Overall, our study shows that novelty search is\na promising alternative for the evolution of controllers for robotic swarms."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3393v1", 
    "title": "Generic Behaviour Similarity Measures for Evolutionary Swarm Robotics", 
    "arxiv-id": "1304.3393v1", 
    "author": "Anders Lyhne Christensen", 
    "publish": "2013-04-11T18:58:13Z", 
    "summary": "Novelty search has shown to be a promising approach for the evolution of\ncontrollers for swarm robotics. In existing studies, however, the experimenter\nhad to craft a domain dependent behaviour similarity measure to use novelty\nsearch in swarm robotics applications. The reliance on hand-crafted similarity\nmeasures places an additional burden to the experimenter and introduces a bias\nin the evolutionary process. In this paper, we propose and compare two\ntask-independent, generic behaviour similarity measures: combined state count\nand sampled average state. The proposed measures use the values of sensors and\neffectors recorded for each individual robot of the swarm. The characterisation\nof the group-level behaviour is then obtained by combining the sensor-effector\nvalues from all the robots. We evaluate the proposed measures in an aggregation\ntask and in a resource sharing task. We show that the generic measures match\nthe performance of domain dependent measures in terms of solution quality. Our\nresults indicate that the proposed generic measures operate as effective\nbehaviour similarity measures, and that it is possible to leverage the benefits\nof novelty search without having to craft domain specific similarity measures."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3610v1", 
    "title": "Modified Soft Brood Crossover in Genetic Programming", 
    "arxiv-id": "1304.3610v1", 
    "author": "Vipul K. Dabhi", 
    "publish": "2013-04-12T11:54:35Z", 
    "summary": "Premature convergence is one of the important issues while using Genetic\nProgramming for data modeling. It can be avoided by improving population\ndiversity. Intelligent genetic operators can help to improve the population\ndiversity. Crossover is an important operator in Genetic Programming. So, we\nhave analyzed number of intelligent crossover operators and proposed an\nalgorithm with the modification of soft brood crossover operator. It will help\nto improve the population diversity and reduce the premature convergence. We\nhave performed experiments on three different symbolic regression problems.\nThen we made the performance comparison of our proposed crossover (Modified\nSoft Brood Crossover) with the existing soft brood crossover and subtree\ncrossover operators."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3612v1", 
    "title": "A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems", 
    "arxiv-id": "1304.3612v1", 
    "author": "V. Ravibabu", 
    "publish": "2013-04-12T12:08:07Z", 
    "summary": "This paper represents the metaheuristics proposed for solving a class of Shop\nScheduling problem. The Bacterial Foraging Optimization algorithm is featured\nwith Ant Colony Optimization algorithm and proposed as a natural inspired\ncomputing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop\nis the combination of Job Shop, Flow Shop and Open Shop scheduling problems.\nThe sample instances for all mentioned Shop problems are used as test data and\nMixed Shop survive its computational complexity to minimize the makespan. The\ncomputational results show that the proposed algorithm is gentler to solve and\nperforms better than the existing algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3779v1", 
    "title": "Improving Generalization Ability of Genetic Programming: Comparative   Study", 
    "arxiv-id": "1304.3779v1", 
    "author": "Vipul K. Dabhi", 
    "publish": "2013-04-13T05:16:54Z", 
    "summary": "In the field of empirical modeling using Genetic Programming (GP), it is\nimportant to evolve solution with good generalization ability. Generalization\nability of GP solutions get affected by two important issues: bloat and\nover-fitting. Bloat is uncontrolled growth of code without any gain in fitness\nand important issue in GP. We surveyed and classified existing literature\nrelated to different techniques used by GP research community to deal with the\nissue of bloat. Moreover, the classifications of different bloat control\napproaches and measures for bloat are discussed. Next, we tested four bloat\ncontrol methods: Tarpeian, double tournament, lexicographic parsimony pressure\nwith direct bucketing and ratio bucketing on six different problems and\nidentified where each bloat control method performs well on per problem basis.\nBased on the analysis of each method, we combined two methods: double\ntournament (selection method) and Tarpeian method (works before evaluation) to\navoid bloated solutions and compared with the results obtained from individual\nperformance of double tournament method. It was found that the results were\nimproved with this combination of two methods."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3792v1", 
    "title": "Solving Linear Equations Using a Jacobi Based Time-Variant Adaptive   Hybrid Evolutionary Algorithm", 
    "arxiv-id": "1304.3792v1", 
    "author": "Md. Bazlar Rahman", 
    "publish": "2013-04-13T09:12:07Z", 
    "summary": "Large set of linear equations, especially for sparse and structured\ncoefficient (matrix) equations, solutions using classical methods become\narduous. And evolutionary algorithms have mostly been used to solve various\noptimization and learning problems. Recently, hybridization of classical\nmethods (Jacobi method and Gauss-Seidel method) with evolutionary computation\ntechniques have successfully been applied in linear equation solving. In the\nboth above hybrid evolutionary methods, uniform adaptation (UA) techniques are\nused to adapt relaxation factor. In this paper, a new Jacobi Based Time-Variant\nAdaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm,\na Time-Variant Adaptive (TVA) technique of relaxation factor is introduced\naiming at both improving the fine local tuning and reducing the disadvantage of\nuniform adaptation of relaxation factors. This algorithm integrates the Jacobi\nbased SR method with time variant adaptive evolutionary algorithm. The\nconvergence theorems of the proposed algorithm are proved theoretically. And\nthe performance of the proposed algorithm is compared with JBUA hybrid\nevolutionary algorithm and classical methods in the experimental domain. The\nproposed algorithm outperforms both the JBUA hybrid algorithm and classical\nmethods in terms of convergence speed and effectiveness."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.3892v1", 
    "title": "An accelerated CLPSO algorithm", 
    "arxiv-id": "1304.3892v1", 
    "author": "Asrar Ul Haq Sheikh", 
    "publish": "2013-04-14T08:56:10Z", 
    "summary": "The particle swarm approach provides a low complexity solution to the\noptimization problem among various existing heuristic algorithms. Recent\nadvances in the algorithm resulted in improved performance at the cost of\nincreased computational complexity, which is undesirable. Literature shows that\nthe particle swarm optimization algorithm based on comprehensive learning\nprovides the best complexity-performance trade-off. We show how to reduce the\ncomplexity of this algorithm further, with a slight but acceptable performance\nloss. This enhancement allows the application of the algorithm in time critical\napplications, such as, real-time tracking, equalization etc."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.4055v2", 
    "title": "Multiobjective optimization in Gene Expression Programming for Dew Point", 
    "arxiv-id": "1304.4055v2", 
    "author": "Vipul Dabhi", 
    "publish": "2013-04-15T11:28:06Z", 
    "summary": "The processes occurring in climatic change evolution and their variations\nplay a major role in environmental engineering. Different techniques are used\nto model the relationship between temperatures, dew point and relative\nhumidity. Gene expression programming is capable of modelling complex realities\nwith great accuracy, allowing, at the same time, the extraction of knowledge\nfrom the evolved models compared to other learning algorithms. This research\naims to use Gene Expression Programming for modelling of dew point. Generally,\naccuracy of the model is the only objective used by selection mechanism of GEP.\nThis will evolve large size models with low training error. To avoid this\nsituation, use of multiple objectives, like accuracy and size of the model are\npreferred by Genetic Programming practitioners. Multi-objective problem finds a\nset of solutions satisfying the objectives given by decision maker.\nMultiobjective based GEP will be used to evolve simple models. Various\nalgorithms widely used for multi objective optimization like NSGA II and SPEA 2\nare tested for different test cases. The results obtained thereafter gives idea\nthat SPEA 2 is better algorithm compared to NSGA II based on the features like\nexecution time, number of solutions obtained and convergence rate. Thus\ncompared to models obtained by GEP, multi-objective algorithms fetch better\nsolutions considering the dual objectives of fitness and size of the equation.\nThese simple models can be used to predict dew point."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463398", 
    "link": "http://arxiv.org/pdf/1304.5594v2", 
    "title": "Dew Point modelling using GEP based multi objective optimization", 
    "arxiv-id": "1304.5594v2", 
    "author": "Vipul Dabhi", 
    "publish": "2013-04-20T06:56:04Z", 
    "summary": "Different techniques are used to model the relationship between temperatures,\ndew point and relative humidity. Gene expression programming is capable of\nmodelling complex realities with great accuracy, allowing at the same time, the\nextraction of knowledge from the evolved models compared to other learning\nalgorithms. We aim to use Gene Expression Programming for modelling of dew\npoint. Generally, accuracy of the model is the only objective used by selection\nmechanism of GEP. This will evolve large size models with low training error.\nTo avoid this situation, use of multiple objectives, like accuracy and size of\nthe model are preferred by Genetic Programming practitioners. Solution to a\nmulti-objective problem is a set of solutions which satisfies the objectives\ngiven by decision maker. Multi objective based GEP will be used to evolve\nsimple models. Various algorithms widely used for multi objective optimization,\nlike NSGA II and SPEA 2, are tested on different test problems. The results\nobtained thereafter gives idea that SPEA 2 is better than NSGA II based on the\nfeatures like execution time, number of solutions obtained and convergence\nrate. We selected SPEA 2 for dew point prediction. The multi-objective base GEP\nproduces accurate and simpler (smaller) solutions compared to solutions\nproduced by plain GEP for dew point predictions. Thus multi objective base GEP\nproduces better solutions by considering the dual objectives of fitness and\nsize of the solution. These simple models can be used to predict future values\nof dew point."
},{
    "category": "cs.NE", 
    "doi": "10.7551/978-0-262-31709-2-ch138", 
    "link": "http://arxiv.org/pdf/1305.0763v2", 
    "title": "Quantifying the Impact of Parameter Tuning on Nature-Inspired Algorithms", 
    "arxiv-id": "1305.0763v2", 
    "author": "Martyn Amos", 
    "publish": "2013-05-03T16:15:20Z", 
    "summary": "The problem of parameterization is often central to the effective deployment\nof nature-inspired algorithms. However, finding the optimal set of parameter\nvalues for a combination of problem instance and solution method is highly\nchallenging, and few concrete guidelines exist on how and when such tuning may\nbe performed. Previous work tends to either focus on a specific algorithm or\nuse benchmark problems, and both of these restrictions limit the applicability\nof any findings. Here, we examine a number of different algorithms, and study\nthem in a \"problem agnostic\" fashion (i.e., one that is not tied to specific\ninstances) by considering their performance on fitness landscapes with varying\ncharacteristics. Using this approach, we make a number of observations on which\nalgorithms may (or may not) benefit from tuning, and in which specific\ncircumstances."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2013.6557808", 
    "link": "http://arxiv.org/pdf/1305.2490v2", 
    "title": "Combining Drift Analysis and Generalized Schema Theory to Design   Efficient Hybrid and/or Mixed Strategy EAs", 
    "arxiv-id": "1305.2490v2", 
    "author": "Jun He", 
    "publish": "2013-05-11T09:57:15Z", 
    "summary": "Hybrid and mixed strategy EAs have become rather popular for tackling various\ncomplex and NP-hard optimization problems. While empirical evidence suggests\nthat such algorithms are successful in practice, rather little theoretical\nsupport for their success is available, not mentioning a solid mathematical\nfoundation that would provide guidance towards an efficient design of this type\nof EAs. In the current paper we develop a rigorous mathematical framework that\nsuggests such designs based on generalized schema theory, fitness levels and\ndrift analysis. An example-application for tackling one of the classical\nNP-hard problems, the \"single-machine scheduling problem\" is presented."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-013-9395-4", 
    "link": "http://arxiv.org/pdf/1305.2504v1", 
    "title": "Geiringer Theorems: From Population Genetics to Computational   Intelligence, Memory Evolutive Systems and Hebbian Learning", 
    "arxiv-id": "1305.2504v1", 
    "author": "Jun He", 
    "publish": "2013-05-11T13:43:12Z", 
    "summary": "The classical Geiringer theorem addresses the limiting frequency of\noccurrence of various alleles after repeated application of crossover. It has\nbeen adopted to the setting of evolutionary algorithms and, a lot more\nrecently, reinforcement learning and Monte-Carlo tree search methodology to\ncope with a rather challenging question of action evaluation at the chance\nnodes. The theorem motivates novel dynamic parallel algorithms that are\nexplicitly described in the current paper for the first time. The algorithms\ninvolve independent agents traversing a dynamically constructed directed graph\nthat possibly has loops. A rather elegant and profound category-theoretic model\nof cognition in biological neural networks developed by a well-known French\nmathematician, professor Andree Ehresmann jointly with a neurosurgeon, Jan Paul\nVanbremeersch over the last thirty years provides a hint at the connection\nbetween such algorithms and Hebbian learning."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-013-9395-4", 
    "link": "http://arxiv.org/pdf/1305.2830v1", 
    "title": "Performance Enhancement of Distributed Quasi Steady-State Genetic   Algorithm", 
    "arxiv-id": "1305.2830v1", 
    "author": "Anil N. Jaiswal", 
    "publish": "2013-05-10T08:14:03Z", 
    "summary": "This paper proposes a new scheme for performance enhancement of distributed\ngenetic algorithm (DGA). Initial population is divided in two classes i.e.\nfemale and male. Simple distance based clustering is used for cluster formation\naround females. For reclustering self-adaptive K-means is used, which produces\nwell distributed and well separated clusters. The self-adaptive K-means used\nfor reclustering automatically locates initial position of centroids and number\nof clusters. Four plans of co-evolution are applied on these clusters\nindependently. Clusters evolve separately. Merging of clusters takes place\ndepending on their performance. For experimentation unimodal and multimodal\ntest functions have been used. Test result show that the new scheme of\ndistribution of population has given better performance."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1305.4947v1", 
    "title": "Improving NSGA-II with an Adaptive Mutation Operator", 
    "arxiv-id": "1305.4947v1", 
    "author": "Aluizio F. R. Araujo", 
    "publish": "2013-05-21T20:13:53Z", 
    "summary": "The performance of a Multiobjective Evolutionary Algorithm (MOEA) is\ncrucially dependent on the parameter setting of the operators. The most desired\ncontrol of such parameters presents the characteristic of adaptiveness, i.e.,\nthe capacity of changing the value of the parameter, in distinct stages of the\nevolutionary process, using feedbacks from the search for determining the\ndirection and/or magnitude of changing. Given the great popularity of the\nalgorithm NSGA-II, the objective of this research is to create adaptive\ncontrols for each parameter existing in this MOEA. With these controls, we\nexpect to improve even more the performance of the algorithm.\n  In this work, we propose an adaptive mutation operator that has an adaptive\ncontrol which uses information about the diversity of candidate solutions for\ncontrolling the magnitude of the mutation. A number of experiments considering\ndifferent problems suggest that this mutation operator improves the ability of\nthe NSGA-II for reaching the Pareto optimal Front and for getting a better\ndiversity among the final solutions."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1305.7056v1", 
    "title": "Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen", 
    "arxiv-id": "1305.7056v1", 
    "author": "Uwe Aickelin", 
    "publish": "2013-05-30T10:28:06Z", 
    "summary": "This thesis investigates the use of problem-specific knowledge to enhance a\ngenetic algorithm approach to multiple-choice optimisation problems. It shows\nthat such information can significantly enhance performance, but that the\nchoice of information and the way it is included are important factors for\nsuccess."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1305.7434v1", 
    "title": "Motif Detection Inspired by Immune Memory (JORS)", 
    "arxiv-id": "1305.7434v1", 
    "author": "Uwe Aickelin", 
    "publish": "2013-05-31T14:55:45Z", 
    "summary": "The search for patterns or motifs in data represents an area of key interest\nto many researchers. In this paper we present the Motif Tracking Algorithm, a\nnovel immune inspired pattern identification tool that is able to identify\nvariable length unknown motifs which repeat within time series data. The\nalgorithm searches from a neutral perspective that is independent of the data\nbeing analysed and the underlying motifs. In this paper we test the flexibility\nof the motif tracking algorithm by applying it to the search for patterns in\ntwo industrial data sets. The algorithm is able to identify a population of\nmeaningful motifs in both cases, and the value of these motifs is discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.0090v1", 
    "title": "Harmony search algorithm for the container storage problem", 
    "arxiv-id": "1306.0090v1", 
    "author": "Lacs Ecole Nationale des Ingenieurs de Tunis", 
    "publish": "2013-06-01T09:44:01Z", 
    "summary": "Recently a new metaheuristic called harmony search was developed. It mimics\nthe behaviors of musicians improvising to find the better state harmony. In\nthis paper, this algorithm is described and applied to solve the container\nstorage problem in the harbor. The objective of this problem is to determine a\nvalid containers arrangement, which meets customers delivery deadlines, reduces\nthe number of container rehandlings and minimizes the ship idle time. In this\npaper, an adaptation of the harmony search algorithm to the container storage\nproblem is detailed and some experimental results are presented and discussed.\nThe proposed approach was compared to a genetic algorithm previously applied to\nthe same problem and recorded a good results."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.0442v1", 
    "title": "Evolutionary Approach for the Containers Bin-Packing Problem", 
    "arxiv-id": "1306.0442v1", 
    "author": "P. Borne", 
    "publish": "2013-06-03T14:47:11Z", 
    "summary": "This paper deals with the resolution of combinatorial optimization problems,\nparticularly those concerning the maritime transport scheduling. We are\ninterested in the management platforms in a river port and more specifically in\ncontainer organisation operations with a view to minimizing the number of\ncontainer rehandlings. Subsequently, we rmeet customers delivery deadlines and\nwe reduce ship stoppage time In this paper, we propose a genetic algorithm to\nsolve this problem and we present some experiments and results."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.0897v1", 
    "title": "Urban ozone concentration forecasting with artificial neural network in   Corsica", 
    "arxiv-id": "1306.0897v1", 
    "author": "Aur\u00e9lia Balu", 
    "publish": "2013-06-04T18:42:57Z", 
    "summary": "Atmospheric pollutants concentration forecasting is an important issue in air\nquality monitoring. Qualitair Corse, the organization responsible for\nmonitoring air quality in Corsica (France) region, needs to develop a\nshort-term prediction model to lead its mission of information towards the\npublic. Various deterministic models exist for meso-scale or local forecasting,\nbut need powerful large variable sets, a good knowledge of atmospheric\nprocesses, and can be inaccurate because of local climatical or geographical\nparticularities, as observed in Corsica, a mountainous island located in a\nMediterranean Sea. As a result, we focus in this study on statistical models,\nand particularly Artificial Neural Networks (ANN) that have shown good results\nin the prediction of ozone concentration at horizon h+1 with data measured\nlocally. The purpose of this study is to build a predictor to realize\npredictions of ozone and PM10 at horizon d+1 in Corsica in order to be able to\nanticipate pollution peak formation and to take appropriated prevention\nmeasures. Specific meteorological conditions are known to lead to particular\npollution event in Corsica (e.g. Saharan dust event). Therefore, several ANN\nmodels will be used, for meteorological conditions clustering and for\noperational forecasting."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.2257v1", 
    "title": "Using the quaternion's representation of individuals in swarm   intelligence and evolutionary computation", 
    "arxiv-id": "1306.2257v1", 
    "author": "Iztok Fister Jr", 
    "publish": "2013-06-10T17:43:28Z", 
    "summary": "This paper introduces a novel idea for representation of individuals using\nquaternions in swarm intelligence and evolutionary algorithms. Quaternions are\na number system, which extends complex numbers. They are successfully applied\nto problems of theoretical physics and to those areas needing fast rotation\ncalculations. We propose the application of quaternions in optimization, more\nprecisely, we have been using quaternions for representation of individuals in\nBat algorithm. The preliminary results of our experiments when optimizing a\ntest-suite consisting of ten standard functions showed that this new algorithm\nsignificantly improved the results of the original Bat algorithm. Moreover, the\nobtained results are comparable with other swarm intelligence and evolutionary\nalgorithms, like the artificial bees colony, and differential evolution. We\nbelieve that this representation could also be successfully applied to other\nswarm intelligence and evolutionary algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.3018v1", 
    "title": "Second Order Swarm Intelligence", 
    "arxiv-id": "1306.3018v1", 
    "author": "Jorge Lou\u00e7\u00e3", 
    "publish": "2013-06-13T04:07:52Z", 
    "summary": "An artificial Ant Colony System (ACS) algorithm to solve general-purpose\ncombinatorial Optimization Problems (COP) that extends previous AC models [21]\nby the inclusion of a negative pheromone, is here described. Several Travelling\nSalesman Problem (TSP) were used as benchmark. We show that by using two\ndifferent sets of pheromones, a second-order co-evolved compromise between\npositive and negative feedbacks achieves better results than single positive\nfeedback systems. The algorithm was tested against known NP-complete\ncombinatorial Optimization Problems, running on symmetrical TSP's. We show that\nthe new algorithm compares favourably against these benchmarks, accordingly to\nrecent biological findings by Robinson [26,27], and Gruter [28] where \"No\nentry\" signals and negative feedback allows a colony to quickly reallocate the\nmajority of its foragers to superior food patches. This is the first time an\nextended ACS algorithm is implemented with these successful characteristics."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.4621v1", 
    "title": "English Character Recognition using Artificial Neural Network", 
    "arxiv-id": "1306.4621v1", 
    "author": "Tanistha Nayak", 
    "publish": "2013-06-19T17:26:26Z", 
    "summary": "This work focuses on development of a Offline Hand Written English Character\nRecognition algorithm based on Artificial Neural Network (ANN). The ANN\nimplemented in this work has single output neuron which shows whether the\ntested character belongs to a particular cluster or not. The implementation is\ncarried out completely in 'C' language. Ten sets of English alphabets\n(small-26, capital-26) were used to train the ANN and 5 sets of English\nalphabets were used to test the network. The characters were collected from\ndifferent persons over duration of about 25 days. The algorithm was tested with\n5 capital letters and 5 small letter sets. However, the result showed that the\nalgorithm recognized English alphabet patterns with maximum accuracy of 92.59%\nand False Rejection Rate (FRR) of 0%."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.4622v1", 
    "title": "Solution to Quadratic Equation Using Genetic Algorithm", 
    "arxiv-id": "1306.4622v1", 
    "author": "Tirtharaj Dash", 
    "publish": "2013-06-19T17:31:47Z", 
    "summary": "Solving Quadratic equation is one of the intrinsic interests as it is the\nsimplest nonlinear equations. A novel approach for solving Quadratic Equation\nbased on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a\ntechnique to solve problems which need optimization. Generation of trial\nsolutions have been formed by this method. Many examples have been worked out,\nand in most cases we find out the exact solution. We have discussed the effect\nof different parameters on the performance of the developed algorithm. The\nresults are concluded after rigorous testing on different equations."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1306.5702v1", 
    "title": "Modeling The Stable Operating Envelope For Partially Stable Combustion   Engines Using Class Imbalance Learning", 
    "arxiv-id": "1306.5702v1", 
    "author": "Dennis Assanis", 
    "publish": "2013-06-24T18:34:28Z", 
    "summary": "Advanced combustion technologies such as homogeneous charge compression\nignition (HCCI) engines have a narrow stable operating region defined by\ncomplex control strategies such as exhaust gas recirculation (EGR) and variable\nvalve timing among others. For such systems, it is important to identify the\noperating envelope or the boundary of stable operation for diagnostics and\ncontrol purposes. Obtaining a good model of the operating envelope using\nphysics becomes intractable owing to engine transient effects. In this paper, a\nmachine learning based approach is employed to identify the stable operating\nboundary of HCCI combustion directly from experimental data. Owing to imbalance\nin class proportions in the data, two approaches are considered. A re-sampling\n(under-sampling, over-sampling) based approach is used to develop models using\nexisting algorithms while a cost-sensitive approach is used to modify the\nlearning algorithm without modifying the data set. Support vector machines and\nrecently developed extreme learning machines are used for model development and\nresults compared against linear classification methods show that cost-sensitive\nversions of ELM and SVM algorithms are well suited to model the HCCI operating\nenvelope. The prediction results indicate that the models have the potential to\nbe used for predicting HCCI instability based on sensor measurement history."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.0841v1", 
    "title": "Comparing various regression methods on ensemble strategies in   differential evolution", 
    "arxiv-id": "1307.0841v1", 
    "author": "Janez Brest", 
    "publish": "2013-07-02T20:47:26Z", 
    "summary": "Differential evolution possesses a multitude of various strategies for\ngenerating new trial solutions. Unfortunately, the best strategy is not known\nin advance. Moreover, this strategy usually depends on the problem to be\nsolved. This paper suggests using various regression methods (like random\nforest, extremely randomized trees, gradient boosting, decision trees, and a\ngeneralized linear model) on ensemble strategies in differential evolution\nalgorithm by predicting the best differential evolution strategy during the\nrun. Comparing the preliminary results of this algorithm by optimizing a suite\nof five well-known functions from literature, it was shown that using the\nrandom forest regression method substantially outperformed the results of the\nother regression methods."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.2559v1", 
    "title": "General Drift Analysis with Tail Bounds", 
    "arxiv-id": "1307.2559v1", 
    "author": "Carsten Witt", 
    "publish": "2013-07-09T19:40:15Z", 
    "summary": "Drift analysis is one of the state-of-the-art techniques for the runtime\nanalysis of randomized search heuristics. In recent years, many different drift\ntheorems, including additive, multiplicative and variable drift, have been\ndeveloped, applied and partly generalized or adapted to particular processes. A\ncomprehensive overview article was missing.\n  We provide not only such an overview but also present a universal drift\ntheorem that generalizes virtually all existing drift theorems found in the\nliterature. On the one hand, the new theorem bounds the expected first hitting\ntime of optimal states in the underlying stochastic process. On the other hand,\nit also allows for general upper and lower tail bounds on the hitting time,\nwhich were not known before except for the special case of upper bounds in\nmultiplicative drift scenarios. As a proof of concept, the new tail bounds are\napplied to prove very precise sharp-concentration results on the running time\nof the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover,\nuser-friendly specializations of the general drift theorem are given."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.3463v4", 
    "title": "Non-Elitist Genetic Algorithm as a Local Search Method", 
    "arxiv-id": "1307.3463v4", 
    "author": "Anton Eremeev", 
    "publish": "2013-07-12T14:07:09Z", 
    "summary": "Sufficient conditions are found under which the iterated non-elitist genetic\nalgorithm with tournament selection first visits a local optimum in\npolynomially bounded time on average. It is shown that these conditions are\nsatisfied on a class of problems with guaranteed local optima (GLO) if\nappropriate parameters of the algorithm are chosen."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.4186v1", 
    "title": "A Brief Review of Nature-Inspired Algorithms for Optimization", 
    "arxiv-id": "1307.4186v1", 
    "author": "Du\u0161an Fister", 
    "publish": "2013-07-16T08:04:01Z", 
    "summary": "Swarm intelligence and bio-inspired algorithms form a hot topic in the\ndevelopments of new algorithms inspired by nature. These nature-inspired\nmetaheuristic algorithms can be based on swarm intelligence, biological\nsystems, physical and chemical systems. Therefore, these algorithms can be\ncalled swarm-intelligence-based, bio-inspired, physics-based and\nchemistry-based, depending on the sources of inspiration. Though not all of\nthem are efficient, a few algorithms have proved to be very efficient and thus\nhave become popular tools for solving real-world problems. Some algorithms are\ninsufficiently studied. The purpose of this review is to present a relatively\ncomprehensive list of all the algorithms in the literature, so as to inspire\nfurther research."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.4274v1", 
    "title": "The Fitness Level Method with Tail Bounds", 
    "arxiv-id": "1307.4274v1", 
    "author": "Carsten Witt", 
    "publish": "2013-07-16T13:45:24Z", 
    "summary": "The fitness-level method, also called the method of f-based partitions, is an\nintuitive and widely used technique for the running time analysis of randomized\nsearch heuristics. It was originally defined to prove upper and lower bounds on\nthe expected running time. Recently, upper tail bounds were added to the\ntechnique; however, these tail bounds only apply to running times that are at\nleast twice as large as the expectation.\n  We remove this restriction and supplement the fitness-level method with sharp\ntail bounds, including lower tails. As an exemplary application, we prove that\nthe running time of randomized local search on OneMax is sharply concentrated\naround n ln n - 0.1159 n."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.7435v1", 
    "title": "A new approach in dynamic traveling salesman problem: a hybrid of ant   colony optimization and descending gradient", 
    "arxiv-id": "1307.7435v1", 
    "author": "Seyyed Reza Khaze", 
    "publish": "2013-07-29T01:39:01Z", 
    "summary": "Nowadays swarm intelligence-based algorithms are being used widely to\noptimize the dynamic traveling salesman problem (DTSP). In this paper, we have\nused mixed method of Ant Colony Optimization (AOC)and gradient descent to\noptimize DTSP which differs with ACO algorithm in evaporation rate and\ninnovative data. This approach prevents premature convergence and scape from\nlocal optimum spots and also makes it possible to find better solutions for\nalgorithm. In this paper, we are going to offer gradient descent and ACO\nalgorithm which in comparison to some former methods it shows that algorithm\nhas significantly improved routes optimization."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1307.8104v1", 
    "title": "Neural Network Capacity for Multilevel Inputs", 
    "arxiv-id": "1307.8104v1", 
    "author": "Subhash Kak", 
    "publish": "2013-07-30T19:51:12Z", 
    "summary": "This paper examines the memory capacity of generalized neural networks.\nHopfield networks trained with a variety of learning techniques are\ninvestigated for their capacity both for binary and non-binary alphabets. It is\nshown that the capacity can be much increased when multilevel inputs are used.\nNew learning strategies are proposed to increase Hopfield network capacity, and\nthe scalability of these methods is also examined in respect to size of the\nnetwork. The ability to recall entire patterns from stimulation of a single\nneuron is examined for the increased capacity networks."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1310.1227v1", 
    "title": "The Novel Approach of Adaptive Twin Probability for Genetic Algorithm", 
    "arxiv-id": "1310.1227v1", 
    "author": "Shaila Subbaraman", 
    "publish": "2013-10-04T11:31:40Z", 
    "summary": "The performance of GA is measured and analyzed in terms of its performance\nparameters against variations in its genetic operators and associated\nparameters. Since last four decades huge numbers of researchers have been\nworking on the performance of GA and its enhancement. This earlier research\nwork on analyzing the performance of GA enforces the need to further\ninvestigate the exploration and exploitation characteristics and observe its\nimpact on the behavior and overall performance of GA. This paper introduces the\nnovel approach of adaptive twin probability associated with the advanced twin\noperator that enhances the performance of GA. The design of the advanced twin\noperator is extrapolated from the twin offspring birth due to single ovulation\nin natural genetic systems as mentioned in the earlier works. The twin\nprobability of this operator is adaptively varied based on the fitness of best\nindividual thereby relieving the GA user from statically defining its value.\nThis novel approach of adaptive twin probability is experimented and tested on\nthe standard benchmark optimization test functions. The experimental results\nshow the increased accuracy in terms of the best individual and reduced\nconvergence time."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1310.3805v1", 
    "title": "Green Heron Swarm Optimization Algorithm - State-of-the-Art of a New   Nature Inspired Discrete Meta-Heuristics", 
    "arxiv-id": "1310.3805v1", 
    "author": "Anupam Shukla", 
    "publish": "2013-10-14T19:42:26Z", 
    "summary": "Many real world problems are NP-Hard problems are a very large part of them\ncan be represented as graph based problems. This makes graph theory a very\nimportant and prevalent field of study. In this work a new bio-inspired\nmeta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm is\nbeing introduced which is inspired by the fishing skills of the bird. The\nalgorithm basically suited for graph based problems like combinatorial\noptimization etc. However introduction of an adaptive mathematical variation\noperator called Location Based Neighbour Influenced Variation (LBNIV) makes it\nsuitable for high dimensional continuous domain problems. The new algorithm is\nbeing operated on the traditional benchmark equations and the results are\ncompared with Genetic Algorithm and Particle Swarm Optimization. The algorithm\nis also operated on Travelling Salesman Problem, Quadratic Assignment Problem,\nKnapsack Problem dataset. The procedure to operate the algorithm on the\nResource Constraint Shortest Path and road network optimization is also\ndiscussed. The results clearly demarcates the GHOSA algorithm as an efficient\nalgorithm specially considering that the number of algorithms for the discrete\noptimization is very low and robust and more explorative algorithm is required\nin this age of social networking and mostly graph based problem scenarios."
},{
    "category": "cs.NE", 
    "doi": "10.1145/1570256.1570387", 
    "link": "http://arxiv.org/pdf/1312.1858v2", 
    "title": "How Santa Fe Ants Evolve", 
    "arxiv-id": "1312.1858v2", 
    "author": "Devinder Kaur", 
    "publish": "2013-12-06T13:37:37Z", 
    "summary": "The Santa Fe Ant model problem has been extensively used to investigate, test\nand evaluate Evolutionary Computing systems and methods over the past two\ndecades. There is however no literature on its program structures that are\nsystematically used for fitness improvement, the geometries of those structures\nand their dynamics during optimization. This paper analyzes the Santa Fe Ant\nProblem using a new phenotypic schema and landscape analysis based on executed\ninstruction sequences. For the first time we detail systematic structural\nfeatures that give high fitness and the evolutionary dynamics of such\nstructures. The new schema avoids variances due to introns. We develop a\nphenotypic variation method that tests the new understanding of the landscape.\nWe also develop a modified function set that tests newly identified\nsynchronization constraints. We obtain favorable computational efforts compared\nto those in the literature, on testing the new variation and function set on\nboth the Santa Fe Trail, and the more computationally demanding Los Altos\nTrail. Our findings suggest that for the Santa Fe Ant problem, a perspective of\nprogram assembly from repetition of highly fit responses to trail conditions\nleads to better analysis and performance."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.2366v1", 
    "title": "A preliminary survey on optimized multiobjective metaheuristic methods   for data clustering using evolutionary approaches", 
    "arxiv-id": "1312.2366v1", 
    "author": "Dr. AV Dattareya Rao", 
    "publish": "2013-12-09T10:23:49Z", 
    "summary": "The present survey provides the state-of-the-art of research, copiously\ndevoted to Evolutionary Approach (EAs) for clustering exemplified with a\ndiversity of evolutionary computations. The Survey provides a nomenclature that\nhighlights some aspects that are very important in the context of evolutionary\ndata clustering. The paper missions the clustering trade-offs branched out with\nwide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally,\nthis study addresses the potential challenges of MOEA design and data\nclustering, along with conclusions and recommendations for novice and\nresearchers by positioning most promising paths of future research. MOEAs have\nsubstantial success across a variety of MOP applications, from pedagogical\nmultifunction optimization to real-world engineering design. The survey paper\nnoticeably organizes the developments witnessed in the past three decades for\nEAs based metaheuristics to solve multiobjective optimization problems (MOP)\nand to derive significant progression in ruling high quality elucidations in a\nsingle run. Data clustering is an exigent task, whose intricacy is caused by a\nlack of unique and precise definition of a cluster. The discrete optimization\nproblem uses the cluster space to derive a solution for Multiobjective data\nclustering. Discovery of a majority or all of the clusters (of illogical\nshapes) present in the data is a long-standing goal of unsupervised predictive\nlearning problems or exploratory pattern analysis."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.4044v1", 
    "title": "CACO : Competitive Ant Colony Optimization, A Nature-Inspired   Metaheuristic For Large-Scale Global Optimization", 
    "arxiv-id": "1312.4044v1", 
    "author": "M. A. El-Dosuky", 
    "publish": "2013-12-14T13:33:48Z", 
    "summary": "Large-scale problems are nonlinear problems that need metaheuristics, or\nglobal optimization algorithms. This paper reviews nature-inspired\nmetaheuristics, then it introduces a framework named Competitive Ant Colony\nOptimization inspired by the chemical communications among insects. Then a case\nstudy is presented to investigate the proposed framework for large-scale global\noptimization."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.4078v1", 
    "title": "A natural-inspired optimization machine based on the annual migration of   salmons in nature", 
    "arxiv-id": "1312.4078v1", 
    "author": "Alireza Fathi", 
    "publish": "2013-12-14T19:05:11Z", 
    "summary": "Bio inspiration is a branch of artificial simulation science that shows\npervasive contributions to variety of engineering fields such as automate\npattern recognition, systematic fault detection and applied optimization. In\nthis paper, a new metaheuristic optimizing algorithm that is the simulation of\nThe Great Salmon Run(TGSR) is developed. The obtained results imply on the\nacceptable performance of implemented method in optimization of complex non\nconvex, multi dimensional and multi-modal problems. To prove the superiority of\nTGSR in both robustness and quality, it is also compared with most of the well\nknown proposed optimizing techniques such as Simulated Annealing (SA), Parallel\nMigrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA),\nParticle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony\n(ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained results\nconfirm the acceptable performance of the proposed method in both robustness\nand quality for different bench-mark optimizing problems and also prove the\nauthors claim."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.4132v1", 
    "title": "An introduction to synchronous self-learning Pareto strategy", 
    "arxiv-id": "1312.4132v1", 
    "author": "Alireza Fathi", 
    "publish": "2013-12-15T10:21:05Z", 
    "summary": "In last decades optimization and control of complex systems that possessed\nvarious conflicted objectives simultaneously attracted an incremental interest\nof scientists. This is because of the vast applications of these systems in\nvarious fields of real life engineering phenomena that are generally multi\nmodal, non convex and multi criterion. Hence, many researchers utilized\nversatile intelligent models such as Pareto based techniques, game theory\n(cooperative and non cooperative games), neuro evolutionary systems, fuzzy\nlogic and advanced neural networks for handling these types of problems. In\nthis paper a novel method called Synchronous Self Learning Pareto Strategy\nAlgorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),\nSwarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map\n(CSOM) simultaneously incorporating with a data shuffling behavior.\nEvolutionary Algorithms (EA) which attempt to simulate the phenomenon of\nnatural evolution are powerful numerical optimization algorithms that reach an\napproximate global maximum of a complex multi variable function over a wide\nsearch space and swarm base technique can improved the intensity and the\nrobustness in EA. CSOM is a neural network capable of learning and can improve\nthe quality of obtained optimal Pareto front. To prove the efficient\nperformance of proposed algorithm, authors utilized some well known benchmark\ntest functions. Obtained results indicate that the cited method is best suit in\nthe case of vector optimization."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.4149v1", 
    "title": "Autonomous Quantum Perceptron Neural Network", 
    "arxiv-id": "1312.4149v1", 
    "author": "Mohammed Zidan", 
    "publish": "2013-12-15T13:57:16Z", 
    "summary": "Recently, with the rapid development of technology, there are a lot of\napplications require to achieve low-cost learning. However the computational\npower of classical artificial neural networks, they are not capable to provide\nlow-cost learning. In contrast, quantum neural networks may be representing a\ngood computational alternate to classical neural network approaches, based on\nthe computational power of quantum bit (qubit) over the classical bit. In this\npaper we present a new computational approach to the quantum perceptron neural\nnetwork can achieve learning in low-cost computation. The proposed approach has\nonly one neuron can construct self-adaptive activation operators capable to\naccomplish the learning process in a limited number of iterations and, thereby,\nreduce the overall computational cost. The proposed approach is capable to\nconstruct its own set of activation operators to be applied widely in both\nquantum and classical applications to overcome the linearity limitation of\nclassical perceptron. The computational power of the proposed approach is\nillustrated via solving variety of problems where promising and comparable\nresults are given."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsit.2013.5504", 
    "link": "http://arxiv.org/pdf/1312.5548v1", 
    "title": "My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013", 
    "arxiv-id": "1312.5548v1", 
    "author": "J\u00fcrgen Schmidhuber", 
    "publish": "2013-12-19T13:45:45Z", 
    "summary": "Deep Learning has attracted significant attention in recent years. Here I\npresent a brief overview of my first Deep Learner of 1991, and its historic\ncontext, with a timeline of Deep Learning highlights."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijcsea.2013.3502", 
    "link": "http://arxiv.org/pdf/1312.5814v1", 
    "title": "Optimal parameter selection for unsupervised neural network using   genetic algorithm", 
    "arxiv-id": "1312.5814v1", 
    "author": "Raveendra Babu Bhogapathi", 
    "publish": "2013-12-20T05:39:51Z", 
    "summary": "K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised\nneural network requires two parameters: tolerance and vigilance. Best\nClustering results are feasible only by finest parameters specified to the\nneural network. Selecting optimal values for these parameters is a major\nproblem. To solve this issue, Genetic Algorithm (GA) is used to determine\noptimal parameters of K-FLANN for finding groups in multidimensional data.\nK-FLANN is a simple topological network, in which output nodes grows\ndynamically during the clustering process on receiving input patterns. Original\nK-FLANN is enhanced to select winner unit out of the matched nodes so that\nstable clusters are formed with in a less number of epochs. The experimental\nresults show that the GA is efficient in finding optimal values of parameters\nfrom the large search space and is tested using artificial and synthetic data\nsets."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1312.6609v1", 
    "title": "A comprehensive review of firefly algorithms", 
    "arxiv-id": "1312.6609v1", 
    "author": "Janez Brest", 
    "publish": "2013-12-23T17:16:46Z", 
    "summary": "The firefly algorithm has become an increasingly important tool of Swarm\nIntelligence that has been applied in almost all areas of optimization, as well\nas engineering practice. Many problems from various areas have been\nsuccessfully solved using the firefly algorithm and its variants. In order to\nuse the algorithm to solve diverse problems, the original firefly algorithm\nneeds to be modified or hybridized. This paper carries out a comprehensive\nreview of this living and evolving discipline of Swarm Intelligence, in order\nto show that the firefly algorithm could be applied to every problem arising in\npractice. On the other hand, it encourages new researchers and algorithm\ndevelopers to use this simple and yet very efficient algorithm for problem\nsolving. It often guarantees that the obtained results will meet the\nexpectations."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.0523v1", 
    "title": "Solving Poisson Equation by Genetic Algorithms", 
    "arxiv-id": "1401.0523v1", 
    "author": "Abdelaziz El Moujahid", 
    "publish": "2014-01-02T20:15:00Z", 
    "summary": "This paper deals with a method for solving Poisson Equation (PE) based on\ngenetic algorithms and grammatical evolution. The method forms generations of\nsolutions expressed in an analytical form. Several examples of PE are tested\nand in most cases the exact solution is recovered. But, when the solution\ncannot be expressed in an analytical form, our method produces a satisfactory\nsolution with a good level of accuracy"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.0546v1", 
    "title": "Low-Complexity Particle Swarm Optimization for Time-Critical   Applications", 
    "arxiv-id": "1401.0546v1", 
    "author": "Asrar Ul Haq Sheikh", 
    "publish": "2014-01-02T21:09:55Z", 
    "summary": "Particle swam optimization (PSO) is a popular stochastic optimization method\nthat has found wide applications in diverse fields. However, PSO suffers from\nhigh computational complexity and slow convergence speed. High computational\ncomplexity hinders its use in applications that have limited power resources\nwhile slow convergence speed makes it unsuitable for time critical\napplications. In this paper, we propose two techniques to overcome these\nlimitations. The first technique reduces the computational complexity of PSO\nwhile the second technique speeds up its convergence. These techniques can be\napplied, either separately or in conjunction, to any existing PSO variant. The\nproposed techniques are robust to the number of dimensions of the optimization\nproblem. Simulation results are presented for the proposed techniques applied\nto the standard PSO as well as to several PSO variants. The results show that\nthe use of both these techniques in conjunction results in a reduction in the\nnumber of computations required as well as faster convergence speed while\nmaintaining an acceptable error performance for time-critical applications."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.0858v1", 
    "title": "Multimodal Optimization by Sparkling Squid Populations", 
    "arxiv-id": "1401.0858v1", 
    "author": "Videh Seksaria", 
    "publish": "2014-01-05T01:37:45Z", 
    "summary": "The swarm intelligence of animals is a natural paradigm to apply to\noptimization problems. Ant colony, bee colony, firefly and bat algorithms are\namongst those that have been demonstrated to efficiently to optimize complex\nconstraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for\nmultimodal optimization, inspired by the intelligent swarm behavior of its\nnamesake. After an introduction, formulation and discussion of its\nimplementation, it will be compared to other popular metaheuristics. Finally,\napplications to well - known problems such as image registration and the\ntraveling salesperson problem will be discussed."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.0886v1", 
    "title": "Spectrum Hole Prediction Based On Historical Data: A Neural Network   Approach", 
    "arxiv-id": "1401.0886v1", 
    "author": "Mohammed Dikko Almustapha", 
    "publish": "2014-01-05T11:41:43Z", 
    "summary": "The concept of cognitive radio pioneered by Mitola promises to change the\nfuture of wireless communication especially in the area of spectrum management.\nCurrently, the command and control strategy employed in spectrum assignment is\ntoo rigid and needs to be reviewed. Recent studies have shown that assigned\nspectrum is underutilized spectrally and temporally. Cognitive radio provides a\nviable solution whereby licensed users can share the spectrum with unlicensed\nusers opportunistically without causing interference. Unlicensed users must be\nable to sense weather the channel is busy or idle, failure to do so will lead\nto interference to the licensed user. In this paper, a neural network based\nprediction model for predicting the channel status using historical data\nobtained during a spectrum occupancy measurement is presented. Genetic\nalgorithm is combined with LM BP for increasing the probability of obtaining\nthe best weights thus optimizing the network. The results obtained indicate\nhigh prediction accuracy over all bands considered"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.1124v2", 
    "title": "A binary differential evolution algorithm learning from explored   solutions", 
    "arxiv-id": "1401.1124v2", 
    "author": "Xiufen Zou", 
    "publish": "2014-01-06T15:54:16Z", 
    "summary": "Although real-coded differential evolution (DE) algorithms can perform well\non continuous optimization problems (CoOPs), it is still a challenging task to\ndesign an efficient binary-coded DE algorithm. Inspired by the learning\nmechanism of particle swarm optimization (PSO) algorithms, we propose a binary\nlearning differential evolution (BLDE) algorithm that can efficiently locate\nthe global optimal solutions by learning from the last population. Then, we\ntheoretically prove the global convergence of BLDE, and compare it with some\nexisting binary-coded evolutionary algorithms (EAs) via numerical experiments.\nNumerical results show that BLDE is competitive to the compared EAs, and\nmeanwhile, further study is performed via the change curves of a renewal metric\nand a refinement metric to investigate why BLDE cannot outperform some compared\nEAs for several selected benchmark problems. Finally, we employ BLDE solving\nthe unit commitment problem (UCP) in power systems to show its applicability in\npractical problems."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.1333v1", 
    "title": "Time series forecasting using neural networks", 
    "arxiv-id": "1401.1333v1", 
    "author": "\u015eTefan Cristian Ciucu", 
    "publish": "2014-01-07T10:29:24Z", 
    "summary": "Recent studies have shown the classification and prediction power of the\nNeural Networks. It has been demonstrated that a NN can approximate any\ncontinuous function. Neural networks have been successfully used for\nforecasting of financial data series. The classical methods used for time\nseries prediction like Box-Jenkins or ARIMA assumes that there is a linear\nrelationship between inputs and outputs. Neural Networks have the advantage\nthat can approximate nonlinear functions. In this paper we compared the\nperformances of different feed forward and recurrent neural networks and\ntraining algorithms for predicting the exchange rate EUR/RON and USD/RON. We\nused data series with daily exchange rates starting from 2005 until 2013."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.1905v1", 
    "title": "A Parameterized Complexity Analysis of Bi-level Optimisation with   Evolutionary Algorithms", 
    "arxiv-id": "1401.1905v1", 
    "author": "Mojgan Pourhassan", 
    "publish": "2014-01-09T07:00:20Z", 
    "summary": "Bi-level optimisation problems have gained increasing interest in the field\nof combinatorial optimisation in recent years. With this paper, we start the\nruntime analysis of evolutionary algorithms for bi-level optimisation problems.\nWe examine two NP-hard problems, the generalised minimum spanning tree problem\n(GMST), and the generalised travelling salesman problem (GTSP) in the context\nof parameterised complexity.\n  For the generalised minimum spanning tree problem, we analyse the two\napproaches presented by Hu and Raidl (2012) with respect to the number of\nclusters that distinguish each other by the chosen representation of possible\nsolutions. Our results show that a (1+1) EA working with the spanning nodes\nrepresentation is not a fixed-parameter evolutionary algorithm for the problem,\nwhereas the global structure representation enables to solve the problem in\nfixed-parameter time. We present hard instances for each approach and show that\nthe two approaches are highly complementary by proving that they solve each\nother's hard instances very efficiently.\n  For the generalised travelling salesman problem, we analyse the problem with\nrespect to the number of clusters in the problem instance. Our results show\nthat a (1+1) EA working with the global structure representation is a\nfixed-parameter evolutionary algorithm for the problem."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.2468v1", 
    "title": "N2Sky - Neural Networks as Services in the Clouds", 
    "arxiv-id": "1401.2468v1", 
    "author": "Erwin Mann", 
    "publish": "2014-01-10T21:09:36Z", 
    "summary": "We present the N2Sky system, which provides a framework for the exchange of\nneural network specific knowledge, as neural network paradigms and objects, by\na virtual organization environment. It follows the sky computing paradigm\ndelivering ample resources by the usage of federated Clouds. N2Sky is a novel\nCloud-based neural network simulation environment, which follows a pure service\noriented approach. The system implements a transparent environment aiming to\nenable both novice and experienced users to do neural network research easily\nand comfortably. N2Sky is built using the RAVO reference architecture of\nvirtual organizations which allows itself naturally integrating into the Cloud\nservice stack (SaaS, PaaS, and IaaS) of service oriented architectures."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.swevo.2013.06.001", 
    "link": "http://arxiv.org/pdf/1401.2651v1", 
    "title": "An Overview of Schema Theory", 
    "arxiv-id": "1401.2651v1", 
    "author": "David White", 
    "publish": "2014-01-12T17:52:57Z", 
    "summary": "The purpose of this paper is to give an introduction to the field of Schema\nTheory written by a mathematician and for mathematicians. In particular, we\nendeavor to to highlight areas of the field which might be of interest to a\nmathematician, to point out some related open problems, and to suggest some\nlarge-scale projects. Schema theory seeks to give a theoretical justification\nfor the efficacy of the field of genetic algorithms, so readers who have\nstudied genetic algorithms stand to gain the most from this paper. However,\nnothing beyond basic probability theory is assumed of the reader, and for this\nreason we write in a fairly informal style.\n  Because the mathematics behind the theorems in schema theory is relatively\nelementary, we focus more on the motivation and philosophy. Many of these\nresults have been proven elsewhere, so this paper is designed to serve a\nprimarily expository role. We attempt to cast known results in a new light,\nwhich makes the suggested future directions natural. This involves devoting a\nsubstantial amount of time to the history of the field.\n  We hope that this exposition will entice some mathematicians to do research\nin this area, that it will serve as a road map for researchers new to the\nfield, and that it will help explain how schema theory developed. Furthermore,\nwe hope that the results collected in this document will serve as a useful\nreference. Finally, as far as the author knows, the questions raised in the\nfinal section are new."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.3376v3", 
    "title": "Across neighbourhood search for numerical optimization", 
    "arxiv-id": "1401.3376v3", 
    "author": "Guohua Wu", 
    "publish": "2014-01-14T22:35:48Z", 
    "summary": "Population-based search algorithms (PBSAs), including swarm intelligence\nalgorithms (SIAs) and evolutionary algorithms (EAs), are competitive\nalternatives for solving complex optimization problems and they have been\nwidely applied to real-world optimization problems in different fields. In this\nstudy, a novel population-based across neighbourhood search (ANS) is proposed\nfor numerical optimization. ANS is motivated by two straightforward assumptions\nand three important issues raised in improving and designing efficient PBSAs.\nIn ANS, a group of individuals collaboratively search the solution space for an\noptimal solution of the optimization problem considered. A collection of\nsuperior solutions found by individuals so far is maintained and updated\ndynamically. At each generation, an individual directly searches across the\nneighbourhoods of multiple superior solutions with the guidance of a Gaussian\ndistribution. This search manner is referred to as across neighbourhood search.\nThe characteristics of ANS are discussed and the concept comparisons with other\nPBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for\nimplementation and application with three parameters being required to tune.\nExtensive experiments on 18 benchmark optimization functions of different types\nshow that ANS has well balanced exploration and exploitation capabilities and\nperforms competitively compared with many efficient PBSAs (Related Matlab codes\nused in the experiments are available from\nhttp://guohuawunudt.gotoip2.com/publications.html)."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.4660v2", 
    "title": "On the Resilience of an Ant-based System in Fuzzy Environments. An   Empirical Study", 
    "arxiv-id": "1401.4660v2", 
    "author": "Petrica C. Pop", 
    "publish": "2014-01-19T12:17:06Z", 
    "summary": "The current work describes an empirical study conducted in order to\ninvestigate the behavior of an optimization method in a fuzzy environment.\nMAX-MIN Ant System, an efficient implementation of a heuristic method is used\nfor solving an optimization problem derived from the Traveling Salesman Problem\n(TSP). Several publicly-available symmetric TSP instances and their fuzzy\nvariants are tested in order to extract some general features. The entry data\nwas adapted by introducing a two-dimensional systematic degree of fuzziness,\nproportional with the number of nodes, the dimension of the instance and also\nwith the distances between nodes, the scale of the instance. The results show\nthat our proposed method can handle the data uncertainty, showing good\nresilience and adaptability."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.4674v1", 
    "title": "Evolving Accuracy: A Genetic Algorithm to Improve Election Night   Forecasts", 
    "arxiv-id": "1401.4674v1", 
    "author": "Christoph Waldhauser", 
    "publish": "2014-01-19T14:46:09Z", 
    "summary": "In this paper, we apply genetic algorithms to the field of electoral studies.\nForecasting election results is one of the most exciting and demanding tasks in\nthe area of market research, especially due to the fact that decisions have to\nbe made within seconds on live television. We show that the proposed method\noutperforms currently applied approaches and thereby provide an argument to\ntighten the intersection between computer science and social science,\nespecially political science, further. We scrutinize the performance of our\nalgorithm's runtime behavior to evaluate its applicability in the field.\nNumerical results with real data from a local election in the Austrian province\nof Styria from 2010 substantiate the applicability of the proposed approach."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.4696v1", 
    "title": "Evolutionary Optimization for Decision Making under Uncertainty", 
    "arxiv-id": "1401.4696v1", 
    "author": "Ronald Hochreiter", 
    "publish": "2014-01-19T16:42:57Z", 
    "summary": "Optimizing decision problems under uncertainty can be done using a variety of\nsolution methods. Soft computing and heuristic approaches tend to be powerful\nfor solving such problems. In this overview article, we survey Evolutionary\nOptimization techniques to solve Stochastic Programming problems - both for the\nsingle-stage and multi-stage case."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.4714v1", 
    "title": "Revolutionary Algorithms", 
    "arxiv-id": "1401.4714v1", 
    "author": "Christoph Waldhauser", 
    "publish": "2014-01-19T18:27:41Z", 
    "summary": "The optimization of dynamic problems is both widespread and difficult. When\nconducting dynamic optimization, a balance between reinitialization and\ncomputational expense has to be found. There are multiple approaches to this.\nIn parallel genetic algorithms, multiple sub-populations concurrently try to\noptimize a potentially dynamic problem. But as the number of sub-population\nincreases, their efficiency decreases. Cultural algorithms provide a framework\nthat has the potential to make optimizations more efficient. But they adapt\nslowly to changing environments. We thus suggest a confluence of these\napproaches: revolutionary algorithms. These algorithms seek to extend the\nevolutionary and cultural aspects of the former to approaches with a notion of\nthe political. By modeling how belief systems are changed by means of\nrevolution, these algorithms provide a framework to model and optimize dynamic\nproblems in an efficient fashion."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.4848v1", 
    "title": "An Evolutionary Approach towards Clustering Airborne Laser Scanning Data", 
    "arxiv-id": "1401.4848v1", 
    "author": "Christoph Waldhauser", 
    "publish": "2014-01-20T10:22:41Z", 
    "summary": "In land surveying, the generation of maps was greatly simplified with the\nintroduction of orthophotos and at a later stage with airborne LiDAR laser\nscanning systems. While the original purpose of LiDAR systems was to determine\nthe altitude of ground elevations, newer full wave systems provide additional\ninformation that can be used on classifying the type of ground cover and the\ngeneration of maps. The LiDAR resulting point clouds are huge, multidimensional\ndata sets that need to be grouped in classes of ground cover. We propose a\ngenetic algorithm that aids in classifying these data sets and thus make them\nusable for map generation. A key feature are tailor-made genetic operators and\nfitness functions for the subject. The algorithm is compared to a traditional\nk-means clustering."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.5246v1", 
    "title": "Genetic Algorithms and its use with back-propagation network", 
    "arxiv-id": "1401.5246v1", 
    "author": "H. M. K. Mahdi", 
    "publish": "2014-01-21T10:01:27Z", 
    "summary": "Genetic algorithms are considered as one of the most efficient search\ntechniques. Although they do not offer an optimal solution, their ability to\nreach a suitable solution in considerably short time gives them their\nrespectable role in many AI techniques. This work introduces genetic algorithms\nand describes their characteristics. Then a novel method using genetic\nalgorithm in best training set generation and selection for a back-propagation\nnetwork is proposed. This work also offers a new extension to the original\ngenetic algorithms"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1401.5808v1", 
    "title": "Reducing the Computational Cost in Multi-objective Evolutionary   Algorithms by Filtering Worthless Individuals", 
    "arxiv-id": "1401.5808v1", 
    "author": "Ali Hamzeh", 
    "publish": "2014-01-02T10:28:53Z", 
    "summary": "The large number of exact fitness function evaluations makes evolutionary\nalgorithms to have computational cost. In some real-world problems, reducing\nnumber of these evaluations is much more valuable even by increasing\ncomputational complexity and spending more time. To fulfill this target, we\nintroduce an effective factor, in spite of applied factor in Adaptive Fuzzy\nFitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filter\nout worthless individuals more precisely. Our proposed approach is compared\nwith respect to Adaptive Fuzzy Fitness Granulation with Non-dominated Sorting\nGenetic Algorithm-II, using the Hyper volume and the Inverted Generational\nDistance performance measures. The proposed method is applied to 1 traditional\nand 1 state-of-the-art benchmarks with considering 3 different dimensions. From\nan average performance view, the results indicate that although decreasing the\nnumber of fitness evaluations leads to have performance reduction but it is not\ntangible compared to what we gain."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1403.1073v1", 
    "title": "Artificial Neuron Modelling Based on Wave Shape", 
    "arxiv-id": "1403.1073v1", 
    "author": "Kieran Greer", 
    "publish": "2014-03-05T11:05:16Z", 
    "summary": "This paper describes a new model for an artificial neural network processing\nunit or neuron. It is slightly different to a traditional feedforward network\nby the fact that it favours a mechanism of trying to match the wave-like\n'shape' of the input with the shape of the output against specific value error\ncorrections. The expectation is then that a best fit shape can be transposed\ninto the desired output values more easily. This allows for notions of\nreinforcement through resonance and also the construction of synapses."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.ins.2015.09.051", 
    "link": "http://arxiv.org/pdf/1403.1727v1", 
    "title": "On the Sequence of State Configurations in the Garden of Eden", 
    "arxiv-id": "1403.1727v1", 
    "author": "Kiyonori Miyasaki", 
    "publish": "2014-03-07T11:49:23Z", 
    "summary": "Autonomous threshold element circuit networks are used to investigate the\nstructure of neural networks. With these circuits, as the transition functions\nare threshold functions, it is necessary to consider the existence of sequences\nof state configurations that cannot be transitioned. In this study, we focus on\nall logical functions of four or fewer variables, and we discuss the periodic\nsequences and transient series that transition from all sequences of state\nconfigurations. Furthermore, by using the sequences of state configurations in\nthe Garden of Eden, we show that it is easy to obtain functions that determine\nthe operation of circuit networks."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.2842v1", 
    "title": "Application of Particle Swarm Optimization to Microwave Tapered   Microstrip Lines", 
    "arxiv-id": "1403.2842v1", 
    "author": "Sadik Ulker", 
    "publish": "2014-03-12T08:37:03Z", 
    "summary": "Application of metaheuristic algorithms has been of continued interest in the\nfield of electrical engineering because of their powerful features. In this\nwork special design is done for a tapered transmission line used for matching\nan arbitrary real load to a 50{\\Omega} line. The problem at hand is to match\nthis arbitrary load to 50 {\\Omega} line using three section tapered\ntransmission line with impedances in decreasing order from the load. So the\nproblem becomes optimizing an equation with three unknowns with various\nconditions. The optimized values are obtained using Particle Swarm\nOptimization. It can easily be shown that PSO is very strong in solving this\nkind of multiobjective optimization problems."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.3115v1", 
    "title": "Memory Capacity of Neural Networks using a Circulant Weight Matrix", 
    "arxiv-id": "1403.3115v1", 
    "author": "Vamsi Sashank Kotagiri", 
    "publish": "2014-03-12T21:19:26Z", 
    "summary": "This paper presents results on the memory capacity of a generalized feedback\nneural network using a circulant matrix. Children are capable of learning soon\nafter birth which indicates that the neural networks of the brain have prior\nlearnt capacity that is a consequence of the regular structures in the brain's\norganization. Motivated by this idea, we consider the capacity of circulant\nmatrices as weight matrices in a feedback network."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.3305v1", 
    "title": "Noise Facilitation in Associative Memories of Exponential Capacity", 
    "arxiv-id": "1403.3305v1", 
    "author": "Lav R. Varshney", 
    "publish": "2014-03-13T15:46:27Z", 
    "summary": "Recent advances in associative memory design through structured pattern sets\nand graph-based inference algorithms have allowed reliable learning and recall\nof an exponential number of patterns. Although these designs correct external\nerrors in recall, they assume neurons that compute noiselessly, in contrast to\nthe highly variable neurons in brain regions thought to operate associatively\nsuch as hippocampus and olfactory cortex.\n  Here we consider associative memories with noisy internal computations and\nanalytically characterize performance. As long as the internal noise level is\nbelow a specified threshold, the error probability in the recall phase can be\nmade exceedingly small. More surprisingly, we show that internal noise actually\nimproves the performance of the recall phase while the pattern retrieval\ncapacity remains intact, i.e., the number of stored patterns does not reduce\nwith noise (up to a threshold). Computational experiments lend additional\nsupport to our theoretical analysis. This work suggests a functional benefit to\nnoisy neurons in biological neuronal networks."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.3369v2", 
    "title": "Controlling Recurrent Neural Networks by Conceptors", 
    "arxiv-id": "1403.3369v2", 
    "author": "Herbert Jaeger", 
    "publish": "2014-03-13T18:58:37Z", 
    "summary": "The human brain is a dynamical system whose extremely complex sensor-driven\nneural processes give rise to conceptual, logical cognition. Understanding the\ninterplay between nonlinear neural dynamics and concept-level cognition remains\na major scientific challenge. Here I propose a mechanism of neurodynamical\norganization, called conceptors, which unites nonlinear dynamics with basic\nprinciples of conceptual abstraction and logic. It becomes possible to learn,\nstore, abstract, focus, morph, generalize, de-noise and recognize a large\nnumber of dynamical patterns within a single neural system; novel patterns can\nbe added without interfering with previously acquired ones; neural noise is\nautomatically filtered. Conceptors help explaining how conceptual-level\ninformation processing emerges naturally and robustly in neural systems, and\nremove a number of roadblocks in the theory and applications of recurrent\nneural networks."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.5345v1", 
    "title": "A Physarum-Inspired Approach to Optimal Supply Chain Network Design at   Minimum Total Cost with Demand Satisfaction", 
    "arxiv-id": "1403.5345v1", 
    "author": "Yong Deng", 
    "publish": "2014-03-21T02:32:20Z", 
    "summary": "A supply chain is a system which moves products from a supplier to customers.\nThe supply chains are ubiquitous. They play a key role in all economic\nactivities. Inspired by biological principles of nutrients' distribution in\nprotoplasmic networks of slime mould Physarum polycephalum we propose a novel\nalgorithm for a supply chain design. The algorithm handles the supply networks\nwhere capacity investments and product flows are variables. The networks are\nconstrained by a need to satisfy product demands. Two features of the slime\nmould are adopted in our algorithm. The first is the continuity of a flux\nduring the iterative process, which is used in real-time update of the costs\nassociated with the supply links. The second feature is adaptivity. The supply\nchain can converge to an equilibrium state when costs are changed. Practicality\nand flexibility of our algorithm is illustrated on numerical examples."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.7178v1", 
    "title": "Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm:   A different perspective", 
    "arxiv-id": "1403.7178v1", 
    "author": "Zhifang Wang", 
    "publish": "2014-03-27T19:16:50Z", 
    "summary": "In this paper we study the problem of optimal layout of an offshore wind farm\nto minimize the wake effect impacts. Considering the specific requirements of\nconcerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)\nwhich introduces location swaps to replace random crossovers in conventional\nGAs. That way the total number of turbines in the resulting layout will be\neffectively kept to the initially specified value. We experiment the proposed\nAGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical\noffshore wind distribution setting respectively. Numerical results verify the\neffectiveness of our proposed algorithm which achieves a much faster\nconvergence compared to conventional GA algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1403.7806v2", 
    "title": "Unbiased Black-Box Complexities of Jump Functions", 
    "arxiv-id": "1403.7806v2", 
    "author": "Timo K\u00f6tzing", 
    "publish": "2014-03-30T19:59:44Z", 
    "summary": "We analyze the unbiased black-box complexity of jump functions with small,\nmedium, and large sizes of the fitness plateau surrounding the optimal\nsolution.\n  Among other results, we show that when the jump size is $(1/2 -\n\\varepsilon)n$, that is, only a small constant fraction of the fitness values\nis visible, then the unbiased black-box complexities for arities $3$ and higher\nare of the same order as those for the simple \\textsc{OneMax} function. Even\nfor the extreme jump function, in which all but the two fitness values $n/2$\nand $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased)\nblack-box optimization algorithms exist. This is quite surprising given that\nfor the extreme jump function almost the whole search space (all but a\n$\\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness.\n  To prove these results, we introduce new tools for the analysis of unbiased\nblack-box complexities, for example, selecting the new parent individual not by\ncomparing the fitnesses of the competing search points, but also by taking into\naccount the (empirical) expected fitnesses of their offspring."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1405.1445v1", 
    "title": "Pulling back error to the hidden-node parameter technology:   Single-hidden-layer feedforward network without output weight", 
    "arxiv-id": "1405.1445v1", 
    "author": "Yaonan Wang", 
    "publish": "2014-05-06T20:18:49Z", 
    "summary": "According to conventional neural network theories, the feature of\nsingle-hidden-layer feedforward neural networks(SLFNs) resorts to parameters of\nthe weighted connections and hidden nodes. SLFNs are universal approximators\nwhen at least the parameters of the networks including hidden-node parameter\nand output weight are exist. Unlike above neural network theories, this paper\nindicates that in order to let SLFNs work as universal approximators, one may\nsimply calculate the hidden node parameter only and the output weight is not\nneeded at all. In other words, this proposed neural network architecture can be\nconsidered as a standard SLFNs with fixing output weight equal to an unit\nvector. Further more, this paper presents experiments which show that the\nproposed learning method tends to extremely reduce network output error to a\nvery small number with only 1 hidden node. Simulation results demonstrate that\nthe proposed method can provide several to thousands of times faster than other\nlearning algorithm including BP, SVM/SVR and other ELM methods."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1405.4510v1", 
    "title": "A Memetic Algorithm for the Linear Ordering Problem with Cumulative   Costs", 
    "arxiv-id": "1405.4510v1", 
    "author": "Jin-Kao Hao", 
    "publish": "2014-05-18T14:25:56Z", 
    "summary": "This paper introduces an effective memetic algorithm for the linear ordering\nproblem with cumulative costs. The proposed algorithm combines an order-based\nrecombination operator with an improved forward-backward local search procedure\nand employs a solution quality based replacement criterion for pool updating.\nExtensive experiments on 118 well-known benchmark instances show that the\nproposed algorithm achieves competitive results by identifying 46 new upper\nbounds. Furthermore, some critical ingredients of our algorithm are analyzed to\nunderstand the source of its performance."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1405.4894v1", 
    "title": "Optimization of OFDM radar waveforms using genetic algorithms", 
    "arxiv-id": "1405.4894v1", 
    "author": "Amit Kumar Mishra", 
    "publish": "2014-04-25T12:59:40Z", 
    "summary": "In this paper, we present our investigations on the use of single objective\nand multiobjective genetic algorithms based optimisation algorithms to improve\nthe design of OFDM pulses for radar. We discuss these optimization procedures\nin the scope of a waveform design intended for two different radar processing\nsolutions. Lastly, we show how the encoding solution is suited to permit the\noptimizations of waveform for OFDM radar related challenges such as enhanced\ndetection."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2014.4106", 
    "link": "http://arxiv.org/pdf/1405.5050v1", 
    "title": "A Genetic Algorithm for solving Quadratic Assignment Problem(QAP)", 
    "arxiv-id": "1405.5050v1", 
    "author": "Reza Babazadeh", 
    "publish": "2014-05-20T11:59:45Z", 
    "summary": "The Quadratic Assignment Problem (QAP) is one of the models used for the\nmulti-row layout problem with facilities of equal area. There are a set of n\nfacilities and a set of n locations. For each pair of locations, a distance is\nspecified and for each pair of facilities a weight or flow is specified (e.g.,\nthe amount of supplies transported between the two facilities). The problem is\nto assign all facilities to different locations with the aim of minimizing the\nsum of the distances multiplied by the corresponding flows. The QAP is among\nthe most difficult NP-hard combinatorial optimization problems. Because of\nthis, this paper presents an efficient Genetic algorithm (GA) to solve this\nproblem in reasonable time. For validation the proposed GA some examples are\nselected from QAP library. The obtained results in reasonable time show the\nefficiency of proposed GA."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1405.7349v1", 
    "title": "Radial basis function process neural network training based on   generalized frechet distance and GA-SA hybrid strategy", 
    "arxiv-id": "1405.7349v1", 
    "author": "Xiao-hong Yu", 
    "publish": "2014-01-09T10:24:00Z", 
    "summary": "For learning problem of Radial Basis Function Process Neural Network\n(RBF-PNN), an optimization training method based on GA combined with SA is\nproposed in this paper. Through building generalized Fr\\'echet distance to\nmeasure similarity between time-varying function samples, the learning problem\nof radial basis centre functions and connection weights is converted into the\ntraining on corresponding discrete sequence coefficients. Network training\nobjective function is constructed according to the least square error\ncriterion, and global optimization solving of network parameters is implemented\nin feasible solution space by use of global optimization feature of GA and\nprobabilistic jumping property of SA . The experiment results illustrate that\nthe training algorithm improves the network training efficiency and stability."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1405.7777v1", 
    "title": "Online and Adaptive Pseudoinverse Solutions for ELM Weights", 
    "arxiv-id": "1405.7777v1", 
    "author": "Jonathan Tapson", 
    "publish": "2014-05-30T05:24:22Z", 
    "summary": "The ELM method has become widely used for classification and regressions\nproblems as a result of its accuracy, simplicity and ease of use. The solution\nof the hidden layer weights by means of a matrix pseudoinverse operation is a\nsignificant contributor to the utility of the method; however, the conventional\ncalculation of the pseudoinverse by means of a singular value decomposition\n(SVD) is not always practical for large data sets or for online updates to the\nsolution. In this paper we discuss incremental methods for solving the\npseudoinverse which are suitable for ELM. We show that careful choice of\nmethods allows us to optimize for accuracy, ease of computation, or\nadaptability of the solution."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1405.7780v1", 
    "title": "ELM Solutions for Event-Based Systems", 
    "arxiv-id": "1405.7780v1", 
    "author": "Andr\u00e9 van Schaik", 
    "publish": "2014-05-30T05:29:39Z", 
    "summary": "Whilst most engineered systems use signals that are continuous in time, there\nis a domain of systems in which signals consist of events. Events, like Dirac\ndelta functions, have no meaningful time duration. Many important real-world\nsystems are intrinsically event-based, including the mammalian brain, in which\nthe primary packets of data are spike events, or action potentials. In this\ndomain, signal processing requires responses to spatio-temporal patterns of\nevents. We show that some straightforward modifications to the standard ELM\ntopology produce networks that are able to perform spatio-temporal event\nprocessing online with a high degree of accuracy. The modifications involve the\nre-definition of hidden layer units as synaptic kernels, in which the input\ndelta functions are transformed into continuous-valued signals using a variety\nof impulse-response functions. This permits the use of linear solution methods\nin the output layer, which can produce events as output, if modeled as a\nclassifier; the output classes are 'event' or 'no event'. We illustrate the\nmethod in application to a spike-processing problem."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.0007v1", 
    "title": "Information Transfer in Swarms with Leaders", 
    "arxiv-id": "1407.0007v1", 
    "author": "Upul Senanayake", 
    "publish": "2014-06-30T02:18:17Z", 
    "summary": "Swarm dynamics is the study of collections of agents that interact with one\nanother without central control. In natural systems, insects, birds, fish and\nother large mammals function in larger units to increase the overall fitness of\nthe individuals. Their behavior is coordinated through local interactions to\nenhance mate selection, predator detection, migratory route identification and\nso forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010;\nPartridge 1982; Sumpter et al. 2008]. In artificial systems, swarms of\nautonomous agents can augment human activities such as search and rescue, and\nenvironmental monitoring by covering large areas with multiple nodes [Alami et\nal. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley et\nal. 2002]. In this paper, we explore the interplay between swarm dynamics,\ncovert leadership and theoretical information transfer. A leader is a member of\nthe swarm that acts upon information in addition to what is provided by local\ninteractions. Depending upon the leadership model, leaders can use their\nexternal information either all the time or in response to local conditions\n[Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that is\ntreated no differently than others in the swarm, so leaders and followers\nparticipate equally in whatever interaction model is used [Rossi et al. 2007].\nIn this study, we use theoretical information transfer as a means of analyzing\nswarm interactions to explore whether or not it is possible to distinguish\nbetween followers and leaders based on interactions within the swarm. We find\nthat covert leaders can be distinguished from followers in a swarm because they\nreceive less transfer entropy than followers."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.0008v1", 
    "title": "Navigating Robot Swarms Using Collective Intelligence Learned from   Golden Shiner Fish", 
    "arxiv-id": "1407.0008v1", 
    "author": "Grace Gao", 
    "publish": "2014-06-30T02:38:10Z", 
    "summary": "Navigating networked robot swarms often requires knowing where to go, sensing\nthe environment, and path-planning based on the destination and barriers in the\nenvironment. Such a process is computationally intensive. Moreover, as the\nnetwork scales up, the computational load increases quadratically, or even\nexponentially. Unlike these man-made systems, most biological systems scale\nlinearly in complexity. Furthermore, the scale of a biological swarm can even\nenable collective intelligence. One example comes from observations of golden\nshiner fish. Golden shiners naturally prefer darkness and school together. Each\nindividual golden shiner does not know where the darkness is. Neither does it\nsense the light gradients in the environment. However, by moving together as a\nschool, they always end up in the shady area. We apply such collective\nintelligence learned from golden shiner fish to navigating robot swarms. Each\nindividual robot's dynamic is based on the gold shiners' movement strategy---a\nrandom walk with its speed modulated by the light intensity and its direction\naffected by its neighbors. The theoretical analysis and simulation results show\nthat our method 1) promises to navigate a robot swarm with little situational\nknowledge, 2) simplifies control and decision-making for each individual robot,\n3) requires minimal or even no information exchange within the swarm, and 4) is\nhighly distributed, adaptive, and robust."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.0014v1", 
    "title": "Dispersion and Line Formation in Artificial Swarm Intelligence", 
    "arxiv-id": "1407.0014v1", 
    "author": "Kiju Lee", 
    "publish": "2014-06-30T13:40:02Z", 
    "summary": "One of the major motifs in collective or swarm intelligence is that, even\nthough individuals follow simple rules, the resulting global behavior can be\ncomplex and intelligent. In artificial swarm systems, such as swarm robots, the\ngoal is to use systems that are as simple and cheap as possible, deploy many of\nthem, and coordinate them to conduct complex tasks that each individual cannot\naccomplish. Shape formation in artificial intelligence systems is usually\nrequired for specific task-oriented performance, including 1) forming sensing\ngrids, 2) exploring and mapping in space, underwater, or hazardous\nenvironments, and 3) forming a barricade for surveillance or protecting an area\nor a person. This paper presents a dynamic model of an artificial swarm system\nbased on a virtual spring damper model and algorithms for dispersion without a\nleader and line formation with an interim leader using only the distance\nestimation among the neighbors."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.0061v1", 
    "title": "Block matching algorithm for motion estimation based on Artificial Bee   Colony (ABC)", 
    "arxiv-id": "1407.0061v1", 
    "author": "Valentin Osuna", 
    "publish": "2014-06-30T21:23:11Z", 
    "summary": "Block matching (BM) motion estimation plays a very important role in video\ncoding. In a BM approach, image frames in a video sequence are divided into\nblocks. For each block in the current frame, the best matching block is\nidentified inside a region of the previous frame, aiming to minimize the sum of\nabsolute differences (SAD). Unfortunately, the SAD evaluation is\ncomputationally expensive and represents the most consuming operation in the BM\nprocess. Therefore, BM motion estimation can be approached as an optimization\nproblem, where the goal is to find the best matching block within a search\nspace. The simplest available BM method is the full search algorithm (FSA)\nwhich finds the most accurate motion vector through an exhaustive computation\nof SAD values for all elements of the search window. Recently, several fast BM\nalgorithms have been proposed to reduce the number of SAD operations by\ncalculating only a fixed subset of search locations at the price of poor\naccuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)\noptimization is proposed to reduce the number of search locations in the BM\nprocess. In our algorithm, the computation of search locations is drastically\nreduced by considering a fitness calculation strategy which indicates when it\nis feasible to calculate or only estimate new search locations. Since the\nproposed algorithm does not consider any fixed search pattern or any other\nmovement assumption as most of other BM approaches do, a high probability for\nfinding the true minimum (accurate motion vector) is expected. Conducted\nsimulations show that the proposed method achieves the best balance over other\nfast BM algorithms, in terms of both estimation accuracy and computational\ncost."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.0265v1", 
    "title": "Supervised learning in Spiking Neural Networks with Limited Precision:   SNN/LP", 
    "arxiv-id": "1407.0265v1", 
    "author": "John Marsland", 
    "publish": "2014-07-01T14:50:36Z", 
    "summary": "A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural\nNetworks. This novel algorithm uses limited precision for both synaptic weights\nand synaptic delays; 3 bits in each case. Also a genetic algorithm is used for\nthe supervised training. The results are comparable or better than previously\npublished work. The results are applicable to the realization of large scale\nhardware neural networks. One of the trained networks is implemented in\nprogrammable hardware."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.3000v1", 
    "title": "A Proposed Infrastructure for Adding Online Interaction to Any   Evolutionary Domain", 
    "arxiv-id": "1407.3000v1", 
    "author": "Kenneth O. Stanley", 
    "publish": "2014-07-11T01:11:32Z", 
    "summary": "To address the difficulty of creating online collaborative evolutionary\nsystems, this paper presents a new prototype library called Worldwide\nInfrastructure for Neuroevolution (WIN) and its accompanying site WIN Online\n(http://winark.org/). The WIN library is a collection of software packages\nbuilt on top of Node.js that reduce the complexity of creating fully\npersistent, online, and interactive (or automated) evolutionary platforms\naround any domain. WIN Online is the public interface for WIN, providing an\nonline collection of domains built with the WIN library that lets novice and\nexpert users browse and meaningfully contribute to ongoing experiments. The\nlong term goal of WIN is to make it trivial to connect any platform to the\nworld, providing both a stream of online users, and archives of data and\ndiscoveries for later extension by humans or computers."
},{
    "category": "cs.NE", 
    "doi": "10.5121/cseij.2013.3601", 
    "link": "http://arxiv.org/pdf/1407.3077v1", 
    "title": "Charge Scheduling of an Energy Storage System under Time-of-use Pricing   and a Demand Charge", 
    "arxiv-id": "1407.3077v1", 
    "author": "Yong-Hyuk Kim", 
    "publish": "2014-07-11T09:18:20Z", 
    "summary": "A real-coded genetic algorithm is used to schedule the charging of an energy\nstorage system (ESS), operated in tandem with renewable power by an electricity\nconsumer who is subject to time-of-use pricing and a demand charge. Simulations\nbased on load and generation profiles of typical residential customers show\nthat an ESS scheduled by our algorithm can reduce electricity costs by\napproximately 17%, compared to a system without an ESS, and by 8% compared to a\nscheduling algorithm based on net power."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICIEA.2014.6931307", 
    "link": "http://arxiv.org/pdf/1407.4000v2", 
    "title": "Uncertainty And Evolutionary Optimization: A Novel Approach", 
    "arxiv-id": "1407.4000v2", 
    "author": "A. Mahmood", 
    "publish": "2014-07-15T14:05:24Z", 
    "summary": "Evolutionary algorithms (EA) have been widely accepted as efficient solvers\nfor complex real world optimization problems, including engineering\noptimization. However, real world optimization problems often involve uncertain\nenvironment including noisy and/or dynamic environments, which pose major\nchallenges to EA-based optimization. The presence of noise interferes with the\nevaluation and the selection process of EA, and thus adversely affects its\nperformance. In addition, as presence of noise poses challenges to the\nevaluation of the fitness function, it may need to be estimated instead of\nbeing evaluated. Several existing approaches attempt to address this problem,\nsuch as introduction of diversity (hyper mutation, random immigrants, special\noperators) or incorporation of memory of the past (diploidy, case based\nmemory). However, these approaches fail to adequately address the problem. In\nthis paper we propose a Distributed Population Switching Evolutionary Algorithm\n(DPSEA) method that addresses optimization of functions with noisy fitness\nusing a distributed population switching architecture, to simulate a\ndistributed self-adaptive memory of the solution space. Local regression is\nused in the pseudo-populations to estimate the fitness. Successful applications\nto benchmark test problems ascertain the proposed method's superior performance\nin terms of both robustness and accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICIEA.2014.6931307", 
    "link": "http://arxiv.org/pdf/1407.5739v1", 
    "title": "Global optimization using L\u00e9vy flights", 
    "arxiv-id": "1407.5739v1", 
    "author": "Hoang Linh Nguyen", 
    "publish": "2014-07-22T05:48:08Z", 
    "summary": "This paper studies a class of enhanced diffusion processes in which random\nwalkers perform L\\'evy flights and apply it for global optimization. L\\'evy\nflights offer controlled balance between exploitation and exploration. We\ndevelop four optimization algorithms based on such properties. We compare new\nalgorithms with the well-known Simulated Annealing on hard test functions and\nthe results are very promising."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15579-4304", 
    "link": "http://arxiv.org/pdf/1407.5753v1", 
    "title": "Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm", 
    "arxiv-id": "1407.5753v1", 
    "author": "Rajani Kumari", 
    "publish": "2014-07-22T06:40:09Z", 
    "summary": "Artificial Bee Colony (ABC) is a distinguished optimization strategy that can\nresolve nonlinear and multifaceted problems. It is comparatively a\nstraightforward and modern population based probabilistic approach for\ncomprehensive optimization. In the vein of the other population based\nalgorithms, ABC is moreover computationally classy due to its slow nature of\nsearch procedure. The solution exploration equation of ABC is extensively\ninfluenced by a arbitrary quantity which helps in exploration at the cost of\nexploitation of the better search space. In the solution exploration equation\nof ABC due to the outsized step size the chance of skipping the factual\nsolution is high. Therefore, here this paper improve onlooker bee phase with\nhelp of a local search strategy inspired by memetic algorithm to balance the\ndiversity and convergence capability of the ABC. The proposed algorithm is\nnamed as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 well\nknown un-biased test problems of diverse complexities and two engineering\noptimization problems; results show that the anticipated algorithm go one\nbetter than the basic ABC and its recent deviations in a good number of the\nexperiments."
},{
    "category": "cs.NE", 
    "doi": "10.5120/15579-4304", 
    "link": "http://arxiv.org/pdf/1407.5949v2", 
    "title": "Deep Recurrent Neural Networks for Time Series Prediction", 
    "arxiv-id": "1407.5949v2", 
    "author": "Piyush Prasad", 
    "publish": "2014-07-22T17:25:50Z", 
    "summary": "Ability of deep networks to extract high level features and of recurrent\nnetworks to perform time-series inference have been studied. In view of\nuniversality of one hidden layer network at approximating functions under weak\nconstraints, the benefit of multiple layers is to enlarge the space of\ndynamical systems approximated or, given the space, reduce the number of units\nrequired for a certain error. Traditionally shallow networks with manually\nengineered features are used, back-propagation extent is limited to one and\nattempt to choose a large number of hidden units to satisfy the Markov\ncondition is made. In case of Markov models, it has been shown that many\nsystems need to be modeled as higher order. In the present work, we present\ndeep recurrent networks with longer backpropagation through time extent as a\nsolution to modeling systems that are high order and to predicting ahead. We\nstudy epileptic seizure suppression electro-stimulator. Extraction of manually\nengineered complex features and prediction employing them has not allowed small\nlow-power implementations as, to avoid possibility of surgery, extraction of\nany features that may be required has to be included. In this solution, a\nrecurrent neural network performs both feature extraction and prediction. We\nprove analytically that adding hidden layers or increasing backpropagation\nextent increases the rate of decrease of approximation error. A Dynamic\nProgramming (DP) training procedure employing matrix operations is derived. DP\nand use of matrix operations makes the procedure efficient particularly when\nusing data-parallel computing. The simulation studies show the geometry of the\nparameter space, that the network learns the temporal structure, that\nparameters converge while model output displays same dynamic behavior as the\nsystem and greater than .99 Average Detection Rate on all real seizure data\ntried."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1407.6637v1", 
    "title": "Trainable and Dynamic Computing: Error Backpropagation through Physical   Media", 
    "arxiv-id": "1407.6637v1", 
    "author": "Peter Bienstman", 
    "publish": "2014-07-24T16:14:55Z", 
    "summary": "Machine learning algorithms, and more in particular neural networks, arguably\nexperience a revolution in terms of performance. Currently, the best systems we\nhave for speech recognition, computer vision and similar problems are based on\nneural networks, trained using the half-century old backpropagation algorithm.\nDespite the fact that neural networks are a form of analog computers, they are\nstill implemented digitally for reasons of convenience and availability. In\nthis paper we demonstrate how we can design physical linear dynamic systems\nwith non-linear feedback as a generic platform for dynamic, neuro-inspired\nanalog computing. We show that a crucial advantage of this setup is that the\nerror backpropagation can be performed physically as well, which greatly speeds\nup the optimisation process. As we show in this paper, using one experimentally\nvalidated and one conceptual example, such systems may be the key to providing\na relatively straightforward mechanism for constructing highly scalable, fully\ndynamic analog computers."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1407.7399v1", 
    "title": "A Numerical Optimization Algorithm Inspired by the Strawberry Plant", 
    "arxiv-id": "1407.7399v1", 
    "author": "F. Merrikh-Bayat", 
    "publish": "2014-07-28T12:58:41Z", 
    "summary": "This paper proposes a new numerical optimization algorithm inspired by the\nstrawberry plant for solving complicated engineering problems. Plants like\nstrawberry develop both runners and roots for propagation and search for water\nresources and minerals. In these plants, runners and roots can be thought of as\ntools for global and local searches, respectively. The proposed algorithm has\nthree main differences with the trivial nature-inspired optimization\nalgorithms: duplication-elimination of the computational agents at all\niterations, subjecting all agents to both small and large movements from the\nbeginning to end, and the lack of communication (information exchange) between\nagents. Moreover, it has the advantage of using only three parameters to be\ntuned by user. This algorithm is applied to standard test functions and the\nresults are compared with GA and PSO. The proposed algorithm is also used to\nsolve an open problem in the field of robust control theory. These simulations\nshow that the proposed algorithm can very effectively solve complicated\noptimization problems."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1407.7737v1", 
    "title": "A CUDA-Based Real Parameter Optimization Benchmark", 
    "arxiv-id": "1407.7737v1", 
    "author": "Ying Tan", 
    "publish": "2014-07-29T14:26:57Z", 
    "summary": "Benchmarking is key for developing and comparing optimization algorithms. In\nthis paper, a CUDA-based real parameter optimization benchmark (cuROB) is\nintroduced. Test functions of diverse properties are included within cuROB and\nimplemented efficiently with CUDA. Speedup of one order of magnitude can be\nachieved in comparison with CPU-based benchmark of CEC'14."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.0162v1", 
    "title": "Reservoir Computing using Cellular Automata", 
    "arxiv-id": "1410.0162v1", 
    "author": "Ozgur Yilmaz", 
    "publish": "2014-10-01T09:36:19Z", 
    "summary": "We introduce a novel framework of reservoir computing. Cellular automaton is\nused as the reservoir of dynamical systems. Input is randomly projected onto\nthe initial conditions of automaton cells and nonlinear computation is\nperformed on the input via application of a rule in the automaton for a period\nof time. The evolution of the automaton creates a space-time volume of the\nautomaton state space, and it is used as the reservoir. The proposed framework\nis capable of long short-term memory and it requires orders of magnitude less\ncomputation compared to Echo State Networks. Also, for additive cellular\nautomaton rules, reservoir features can be combined using Boolean operations,\nwhich provides a direct way for concept building and symbolic processing, and\nit is much more efficient compared to state-of-the-art approaches."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.4343v2", 
    "title": "Enhanced Multiobjective Evolutionary Algorithm based on Decomposition   for Solving the Unit Commitment Problem", 
    "arxiv-id": "1410.4343v2", 
    "author": "Dipti Srinivasan", 
    "publish": "2014-10-16T09:17:51Z", 
    "summary": "The unit commitment (UC) problem is a nonlinear, high-dimensional, highly\nconstrained, mixed-integer power system optimization problem and is generally\nsolved in the literature considering minimizing the system operation cost as\nthe only objective. However, due to increasing environmental concerns, the\nrecent attention has shifted to incorporating emission in the problem\nformulation. In this paper, a multi-objective evolutionary algorithm based on\ndecomposition (MOEA/D) is proposed to solve the UC problem as a multi-objective\noptimization problem considering minimizing cost and emission as the multiple\nobjec- tives. Since, UC problem is a mixed-integer optimization problem\nconsisting of binary UC variables and continuous power dispatch variables, a\nnovel hybridization strategy is proposed within the framework of MOEA/D such\nthat genetic algorithm (GA) evolves the binary variables while differential\nevolution (DE) evolves the continuous variables. Further, a novel non-uniform\nweight vector distribution strategy is proposed and a parallel island model\nbased on combination of MOEA/D with uniform and non-uniform weight vector\ndistribution strategy is implemented to enhance the performance of the\npresented algorithm. Extensive case studies are presented on different test\nsystems and the effectiveness of the proposed hybridization strategy, the\nnon-uniform weight vector distribution strategy and parallel island model is\nverified through stringent simulated results. Further, exhaustive benchmarking\nagainst the algorithms proposed in the literature is presented to demonstrate\nthe superiority of the proposed algorithm in obtaining significantly better\nconverged and uniformly distributed trade-off solutions."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.4985v2", 
    "title": "Evolvability signatures of generative encodings: beyond standard   performance benchmarks", 
    "arxiv-id": "1410.4985v2", 
    "author": "Jean-Baptiste Mouret", 
    "publish": "2014-10-18T18:16:26Z", 
    "summary": "Evolutionary robotics is a promising approach to autonomously synthesize\nmachines with abilities that resemble those of animals, but the field suffers\nfrom a lack of strong foundations. In particular, evolutionary systems are\ncurrently assessed solely by the fitness score their evolved artifacts can\nachieve for a specific task, whereas such fitness-based comparisons provide\nlimited insights about how the same system would evaluate on different tasks,\nand its adaptive capabilities to respond to changes in fitness (e.g., from\ndamages to the machine, or in new situations). To counter these limitations, we\nintroduce the concept of \"evolvability signatures\", which picture the\npost-mutation statistical distribution of both behavior diversity (how\ndifferent are the robot behaviors after a mutation?) and fitness values (how\ndifferent is the fitness after a mutation?). We tested the relevance of this\nconcept by evolving controllers for hexapod robot locomotion using five\ndifferent genotype-to-phenotype mappings (direct encoding, generative encoding\nof open-loop and closed-loop central pattern generators, generative encoding of\nneural networks, and single-unit pattern generators (SUPG)). We observed a\npredictive relationship between the evolvability signature of each encoding and\nthe number of generations required by hexapods to adapt from incurred damages.\nOur study also reveals that, across the five investigated encodings, the SUPG\nscheme achieved the best evolvability signature, and was always foremost in\nrecovering an effective gait following robot damages. Overall, our evolvability\nsignatures neatly complement existing task-performance benchmarks, and pave the\nway for stronger foundations for research in evolutionary robotics."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.5401v2", 
    "title": "Neural Turing Machines", 
    "arxiv-id": "1410.5401v2", 
    "author": "Ivo Danihelka", 
    "publish": "2014-10-20T19:28:26Z", 
    "summary": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.5652v1", 
    "title": "Improvement of PSO algorithm by memory based gradient search -   application in inventory management", 
    "arxiv-id": "1410.5652v1", 
    "author": "J\u00e1nos Abonyi", 
    "publish": "2014-10-21T13:40:23Z", 
    "summary": "Advanced inventory management in complex supply chains requires effective and\nrobust nonlinear optimization due to the stochastic nature of supply and demand\nvariations. Application of estimated gradients can boost up the convergence of\nParticle Swarm Optimization (PSO) algorithm but classical gradient calculation\ncannot be applied to stochastic and uncertain systems. In these situations\nMonte-Carlo (MC) simulation can be applied to determine the gradient. We\ndeveloped a memory based algorithm where instead of generating and evaluating\nnew simulated samples the stored and shared former function evaluations of the\nparticles are sampled to estimate the gradients by local weighted least squares\nregression. The performance of the resulted regional gradient-based PSO is\nverified by several benchmark problems and in a complex application example\nwhere optimal reorder points of a supply chain are determined."
},{
    "category": "cs.NE", 
    "doi": "10.1038/ncomms7729", 
    "link": "http://arxiv.org/pdf/1410.7326v3", 
    "title": "Neuroevolution in Games: State of the Art and Open Challenges", 
    "arxiv-id": "1410.7326v3", 
    "author": "Julian Togelius", 
    "publish": "2014-10-27T17:46:28Z", 
    "summary": "This paper surveys research on applying neuroevolution (NE) to games. In\nneuroevolution, artificial neural networks are trained through evolutionary\nalgorithms, taking inspiration from the way biological brains evolved. We\nanalyse the application of NE in games along five different axes, which are the\nrole NE is chosen to play in a game, the different types of neural networks\nused, the way these networks are evolved, how the fitness is determined and\nwhat type of input the network receives. The article also highlights important\nopen research challenges in the field."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.0217v1", 
    "title": "Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex   Algorithm Applied to Optimization of Photovoltaic Cells", 
    "arxiv-id": "1411.0217v1", 
    "author": "Fahhad H. Alharbi", 
    "publish": "2014-11-02T07:47:26Z", 
    "summary": "A new hybridization of the Cuckoo Search (CS) is developed and applied to\noptimize multi-cell solar systems; namely multi-junction and split spectrum\ncells. The new approach consists of combining the CS with the Nelder-Mead\nmethod. More precisely, instead of using single solutions as nests for the CS,\nwe use the concept of a simplex which is used in the Nelder-Mead algorithm.\nThis makes it possible to use the flip operation introduces in the Nelder-Mead\nalgorithm instead of the Levy flight which is a standard part of the CS. In\nthis way, the hybridized algorithm becomes more robust and less sensitive to\nparameter tuning which exists in CS. The goal of our work was to optimize the\nperformance of multi-cell solar systems. Although the underlying problem\nconsists of the minimization of a function of a relatively small number of\nparameters, the difficulty comes from the fact that the evaluation of the\nfunction is complex and only a small number of evaluations is possible. In our\ntest, we show that the new method has a better performance when compared to\nsimilar but more compex hybridizations of Nelder-Mead algorithm using genetic\nalgorithms or particle swarm optimization on standard benchmark functions.\nFinally, we show that the new method outperforms some standard meta-heuristics\nfor the problem of interest."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.2897v1", 
    "title": "Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic   Operator", 
    "arxiv-id": "1411.2897v1", 
    "author": "Hassan Ismkhan", 
    "publish": "2014-11-11T17:42:26Z", 
    "summary": "This paper research review Ant colony optimization (ACO) and Genetic\nAlgorithm (GA), both are two powerful meta-heuristics. This paper explains some\nmajor defects of these two algorithm at first then proposes a new model for ACO\nin which, artificial ants use a quick genetic operator and accelerate their\nactions in selecting next state. Experimental results show that proposed hybrid\nalgorithm is effective and its performance including speed and accuracy beats\nother version."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.3277v1", 
    "title": "Using Ants as a Genetic Crossover Operator in GLS to Solve STSP", 
    "arxiv-id": "1411.3277v1", 
    "author": "Hassan Ismkhan", 
    "publish": "2014-11-12T18:56:45Z", 
    "summary": "Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two\noptimization algorithms that have been successfully applied to the Traveling\nSalesman Problem (TSP). In this paper we define new crossover operator then\nredefine ACAs ants as operate according to defined crossover operator then put\nforward our GLS that uses these ants to solve Symmetric TSP (STSP) instances."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.4148v1", 
    "title": "Diversity Handling In Evolutionary Landscape", 
    "arxiv-id": "1411.4148v1", 
    "author": "Maumita Bhattacharya", 
    "publish": "2014-11-15T14:19:22Z", 
    "summary": "The search ability of an Evolutionary Algorithm (EA) depends on the variation\namong the individuals in the population. Maintaining an optimal level of\ndiversity in the EA population is imperative to ensure that progress of the EA\nsearch is unhindered by premature convergence to suboptimal solutions. Clearer\nunderstanding of the concept of population diversity, in the context of\nevolutionary search and premature convergence in particular, is the key to\ndesigning efficient EAs. To this end, this paper first presents a comprehensive\nanalysis of the EA population diversity issues. Next we present an\ninvestigation on a counter-niching EA technique that introduces and maintains\nconstructive diversity in the population. The proposed approach uses informed\ngenetic operations to reach promising, but un-explored or under-explored areas\nof the search space, while discouraging premature local convergence. Simulation\nruns on a number of standard benchmark test functions with Genetic Algorithm\n(GA) implementation shows promising results."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.4297v1", 
    "title": "Application of Multi-core Parallel Programming to a Combination of Ant   Colony Optimization and Genetic Algorithm", 
    "arxiv-id": "1411.4297v1", 
    "author": "Rishita Kalyani", 
    "publish": "2014-11-05T15:58:47Z", 
    "summary": "This Paper will deal with a combination of Ant Colony and Genetic Programming\nAlgorithm to optimize Travelling Salesmen problem (NP-Hard). However, the\ncomplexity of the algorithm requires considerable computational time and\nresources. Parallel implementation can reduce the computational time. In this\npaper, emphasis in the parallelizing section is given to Multi-core\narchitecture and Multi-Processor Systems which is developed and used almost\neverywhere today and hence, multi-core parallelization to the combination of\nalgorithm is achieved by OpenMP library by Intel Corporation."
},{
    "category": "cs.NE", 
    "doi": "10.18576/amis/100314", 
    "link": "http://arxiv.org/pdf/1411.5053v1", 
    "title": "Model of Interaction between Learning and Evolution", 
    "arxiv-id": "1411.5053v1", 
    "author": "Vladimir G. Red'ko", 
    "publish": "2014-11-18T22:13:48Z", 
    "summary": "The model of interaction between learning and evolutionary optimization is\ndesigned and investigated. The evolving population of modeled organisms is\nconsidered. The mechanism of the genetic assimilation of the acquired features\nduring a number of generations of Darwinian evolution is studied. It is shown\nthat the genetic assimilation takes place as follows: phenotypes of modeled\norganisms move towards the optimum at learning; then the selection takes place;\ngenotypes of selected organisms also move towards the optimum. The hiding\neffect is also studied; this effect means that strong learning can inhibit the\nevolutionary search for the optimal genotype. The mechanism of influence of the\nlearning load on the interaction between learning and evolution is analyzed. It\nis shown that the learning load can lead to a significant acceleration of\nevolution."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1411.6757v9", 
    "title": "Echo State Condition at the Critical Point", 
    "arxiv-id": "1411.6757v9", 
    "author": "Norbert Michael Mayer", 
    "publish": "2014-11-25T08:09:43Z", 
    "summary": "Recurrent networks with transfer functions that fulfill the Lipschitz\ncontinuity with K=1 may be echo state networks if certain limitations on the\nrecurrent connectivity are applied. It has been shown that it is sufficient if\nthe largest singular value of the recurrent connectivity is smaller than 1. The\nmain achievement of this paper is a proof under which conditions the network is\nan echo state network even if the largest singular value is one. It turns out\nthat in this critical case the exact shape of the transfer function plays a\ndecisive role in determining whether the network still fulfills the echo state\ncondition. In addition, several examples with one neuron networks are outlined\nto illustrate effects of critical connectivity. Moreover, within the manuscript\na mathematical definition for a critical echo state network is suggested."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1411.7542v1", 
    "title": "Scalability of using Restricted Boltzmann Machines for Combinatorial   Optimization", 
    "arxiv-id": "1411.7542v1", 
    "author": "J\u00f6rn Grahl", 
    "publish": "2014-11-27T10:49:19Z", 
    "summary": "Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Restricted Boltzmann\nMachines (RBMs) are generative neural networks with these desired properties.\nWe integrate an RBM into an EDA and evaluate the performance of this system in\nsolving combinatorial optimization problems with a single objective. We assess\nhow the number of fitness evaluations and the CPU time scale with problem size\nand with problem complexity. The results are compared to the Bayesian\nOptimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires\nlarger population sizes and a larger number of fitness evaluations, it\noutperforms BOA in terms of CPU times, in particular if the problem is large or\ncomplex. RBM-EDA requires less time for model building than BOA. These results\nhighlight the potential of using generative neural networks for combinatorial\noptimization."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.0307v1", 
    "title": "Seeding the Initial Population of Multi-Objective Evolutionary   Algorithms: A Computational Study", 
    "arxiv-id": "1412.0307v1", 
    "author": "Markus Wagner", 
    "publish": "2014-11-30T23:37:11Z", 
    "summary": "Most experimental studies initialize the population of evolutionary\nalgorithms with random genotypes. In practice, however, optimizers are\ntypically seeded with good candidate solutions either previously known or\ncreated according to some problem-specific method. This \"seeding\" has been\nstudied extensively for single-objective problems. For multi-objective\nproblems, however, very little literature is available on the approaches to\nseeding and their individual benefits and disadvantages. In this article, we\nare trying to narrow this gap via a comprehensive computational study on common\nreal-valued test functions. We investigate the effect of two seeding techniques\nfor five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8\nobjectives. We observe that some functions (e.g., DTLZ4 and the LZ family)\nbenefit significantly from seeding, while others (e.g., WFG) profit less. The\nadvantage of seeding also depends on the examined algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.4210v2", 
    "title": "Learning Precise Spike Train to Spike Train Transformations in   Multilayer Feedforward Neuronal Networks", 
    "arxiv-id": "1412.4210v2", 
    "author": "Arunava Banerjee", 
    "publish": "2014-12-13T09:30:57Z", 
    "summary": "We derive a synaptic weight update rule for learning temporally precise spike\ntrain to spike train transformations in multilayer feedforward networks of\nspiking neurons. The framework, aimed at seamlessly generalizing error\nbackpropagation to the deterministic spiking neuron setting, is based strictly\non spike timing and avoids invoking concepts pertaining to spike rates or\nprobabilistic models of spiking. The derivation is founded on two innovations.\nFirst, an error functional is proposed that compares the spike train emitted by\nthe output neuron of the network to the desired spike train by way of their\nputative impact on a virtual postsynaptic neuron. This formulation sidesteps\nthe need for spike alignment and leads to closed form solutions for all\nquantities of interest. Second, virtual assignment of weights to spikes rather\nthan synapses enables a perturbation analysis of individual spike times and\nsynaptic weights of the output as well as all intermediate neurons in the\nnetwork, which yields the gradients of the error functional with respect to the\nsaid entities. Learning proceeds via a gradient descent mechanism that\nleverages these quantities. Simulation experiments demonstrate the efficacy of\nthe proposed learning framework. The experiments also highlight asymmetries\nbetween synapses on excitatory and inhibitory neurons."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.4967v1", 
    "title": "Sparse, guided feature connections in an Abstract Deep Network", 
    "arxiv-id": "1412.4967v1", 
    "author": "Alan Blair", 
    "publish": "2014-12-16T12:04:35Z", 
    "summary": "We present a technique for developing a network of re-used features, where\nthe topology is formed using a coarse learning method, that allows\ngradient-descent fine tuning, known as an Abstract Deep Network (ADN). New\nfeatures are built based on observed co-occurrences, and the network is\nmaintained using a selection process related to evolutionary algorithms. This\nallows coarse ex- ploration of the problem space, effective for irregular\ndomains, while gradient descent allows pre- cise solutions. Accuracy on\nstandard UCI and Protein-Structure Prediction problems is comparable with\nbenchmark SVM and optimized GBML approaches, and shows scalability for\naddressing large problems. The discrete implementation is symbolic, allowing\ninterpretability, while the continuous method using fine-tuning shows improved\naccuracy. The binary multiplexer problem is explored, as an irregular domain\nthat does not support gradient descent learning, showing solution to the bench-\nmark 135-bit problem. A convolutional implementation is demonstrated on image\nclassification, showing an error-rate of 0.79% on the MNIST problem, without a\npre-defined topology. The ADN system provides a method for developing a very\nsparse, deep feature topology, based on observed relationships between\nfeatures, that is able to find solutions in irregular domains, and initialize a\nnetwork prior to gradient descent learning."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.5067v1", 
    "title": "Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling   Problem with Setups", 
    "arxiv-id": "1412.5067v1", 
    "author": "Ju. V. Kovalenko", 
    "publish": "2014-12-16T16:29:34Z", 
    "summary": "In this paper, we perform an experimental study of optimal recombination\noperator for makespan minimization problem on single machine with\nsequence-dependent setup times ($1|s_{vu}|C_{\\max}$). The computational\nexperiment on benchmark problems from TSPLIB library indicates practical\napplicability of optimal recombination in crossover operator of genetic\nalgorithm for $1|s_{vu}|C_{\\max}$."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.6144v1", 
    "title": "The Computational Theory of Intelligence: Applications to Genetic   Programming and Turing Machines", 
    "arxiv-id": "1412.6144v1", 
    "author": "Daniel Kovach", 
    "publish": "2014-12-14T23:12:05Z", 
    "summary": "In this paper, we continue the efforts of the Computational Theory of\nIntelligence (CTI) by extending concepts to include computational processes in\nterms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,\nand Hybrid Computational Intelligence processes are also introduced and\ndiscussed. We consider the ramifications of the assumptions of CTI with regard\nto the qualities of reproduction and virility. Applications to Biology,\nComputer Science and Cyber Security are also discussed."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.6567v4", 
    "title": "Classifier with Hierarchical Topographical Maps as Internal   Representation", 
    "arxiv-id": "1412.6567v4", 
    "author": "Pitoyo Hartono", 
    "publish": "2014-12-20T00:58:18Z", 
    "summary": "In this study we want to connect our previously proposed context-relevant\ntopographical maps with the deep learning community. Our architecture is a\nclassifier with hidden layers that are hierarchical two-dimensional\ntopographical maps. These maps differ from the conventional self-organizing\nmaps in that their organizations are influenced by the context of the data\nlabels in a top-down manner. In this way bottom-up and top-down learning are\ncombined in a biologically relevant representational learning setting. Compared\nto our previous work, we are here specifically elaborating the model in a more\nchallenging setting compared to our previous experiments and to advance more\nhidden representation layers to bring our discussions into the context of deep\nrepresentational learning."
},{
    "category": "cs.NE", 
    "doi": "10.3390/e19010003", 
    "link": "http://arxiv.org/pdf/1412.7774v1", 
    "title": "Improved Parameter Identification Method Based on Moving Rate", 
    "arxiv-id": "1412.7774v1", 
    "author": "Jong Won Ha", 
    "publish": "2014-12-25T01:11:11Z", 
    "summary": "To improve the problem that the parameter identification for fuzzy neural\nnetwork has many time complexities in calculating, an improved T-S fuzzy\ninference method and an parameter identification method for fuzzy neural\nnetwork are proposed. It mainly includes three parts. First, improved fuzzy\ninference method based on production term for T-S Fuzzy model is explained.\nThen, compared with existing Sugeno fuzzy inference based on Compositional\nrules and type-distance fuzzy inference method, the proposed fuzzy inference\nalgorithm has a less amount of complexity in calculating and the calculating\nprocess is simple. Next, a parameter identification method for FNN based on\nproduction inference is proposed. Finally, the proposed method is applied for\nthe precipitation forecast and security situation prediction. Test results\nshowed that the proposed method significantly improved the effectiveness of\nidentification, reduced the learning order, time complexity and learning error."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2011.5949872", 
    "link": "http://arxiv.org/pdf/1502.00193v1", 
    "title": "Evolutionary Artificial Neural Network Based on Chemical Reaction   Optimization", 
    "arxiv-id": "1502.00193v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-02-01T04:39:30Z", 
    "summary": "Evolutionary algorithms (EAs) are very popular tools to design and evolve\nartificial neural networks (ANNs), especially to train them. These methods have\nadvantages over the conventional backpropagation (BP) method because of their\nlow computational requirement when searching in a large solution space. In this\npaper, we employ Chemical Reaction Optimization (CRO), a newly developed global\noptimization method, to replace BP in training neural networks. CRO is a\npopulation-based metaheuristics mimicking the transition of molecules and their\ninteractions in a chemical reaction. Simulation results show that CRO\noutperforms many EA strategies commonly used to train neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6252925", 
    "link": "http://arxiv.org/pdf/1502.00194v1", 
    "title": "Real-Coded Chemical Reaction Optimization with Different Perturbation   Functions", 
    "arxiv-id": "1502.00194v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-02-01T04:45:43Z", 
    "summary": "Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimics\nthe interactions of molecules in chemical reactions to search for the global\noptimum. The perturbation function greatly influences the performance of CRO on\nsolving different continuous problems. In this paper, we study four different\nprobability distributions, namely, the Gaussian distribution, the Cauchy\ndistribution, the exponential distribution, and a modified Rayleigh\ndistribution, for the perturbation function of CRO. Different distributions\nhave different impacts on the solutions. The distributions are tested by a set\nof well-known benchmark functions and simulation results show that problems\nwith different characteristics have different preference on the distribution\nfunction. Our study gives guidelines to design CRO for different types of\noptimization problems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2012.6256495", 
    "link": "http://arxiv.org/pdf/1502.00195v1", 
    "title": "Sensor Deployment for Air Pollution Monitoring Using Public   Transportation System", 
    "arxiv-id": "1502.00195v1", 
    "author": "Albert Y. S. Lam", 
    "publish": "2015-02-01T04:48:18Z", 
    "summary": "Air pollution monitoring is a very popular research topic and many monitoring\nsystems have been developed. In this paper, we formulate the Bus Sensor\nDeployment Problem (BSDP) to select the bus routes on which sensors are\ndeployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is\na recently proposed metaheuristic designed to solve a wide range of\noptimization problems. Using the real world data, namely Hong Kong Island bus\nroute data, we perform a series of simulations and the results show that CRO is\ncapable of solving this optimization problem efficiently."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2013.6557596", 
    "link": "http://arxiv.org/pdf/1502.00196v1", 
    "title": "Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using   Chemical Reaction Optimization", 
    "arxiv-id": "1502.00196v1", 
    "author": "Albert Y. S. Lam", 
    "publish": "2015-02-01T04:51:13Z", 
    "summary": "An electric vehicle (EV) may be used as energy storage which allows the\nbi-directional electricity flow between the vehicle's battery and the electric\npower grid. In order to flatten the load profile of the electricity system, EV\nscheduling has become a hot research topic in recent years. In this paper, we\npropose a new formulation of the joint scheduling of EV and Unit Commitment\n(UC), called EVUC. Our formulation considers the characteristics of EVs while\noptimizing the system total running cost. We employ Chemical Reaction\nOptimization (CRO), a general-purpose optimization algorithm to solve this\nproblem and the simulation results on a widely used set of instances indicate\nthat CRO can effectively optimize this problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900234", 
    "link": "http://arxiv.org/pdf/1502.00197v1", 
    "title": "An Inter-molecular Adaptive Collision Scheme for Chemical Reaction   Optimization", 
    "arxiv-id": "1502.00197v1", 
    "author": "Albert Y. S. Lam", 
    "publish": "2015-02-01T04:53:48Z", 
    "summary": "Optimization techniques are frequently applied in science and engineering\nresearch and development. Evolutionary algorithms, as a kind of general-purpose\nmetaheuristic, have been shown to be very effective in solving a wide range of\noptimization problems. A recently proposed chemical-reaction-inspired\nmetaheuristic, Chemical Reaction Optimization (CRO), has been applied to solve\nmany global optimization problems. However, the functionality of the\ninter-molecular ineffective collision operator in the canonical CRO design\noverlaps that of the on-wall ineffective collision operator, which can\npotential impair the overall performance. In this paper we propose a new\ninter-molecular ineffective collision operator for CRO for global optimization.\nTo fully utilize our newly proposed operator, we also design a scheme to adapt\nthe algorithm to optimization problems with different search space\ncharacteristics. We analyze the performance of our proposed algorithm with a\nnumber of widely used benchmark functions. The simulation results indicate that\nthe new algorithm has superior performance over the canonical CRO."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.00199v1", 
    "title": "Chemical Reaction Optimization for the Set Covering Problem", 
    "arxiv-id": "1502.00199v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-02-01T04:56:13Z", 
    "summary": "The set covering problem (SCP) is one of the representative combinatorial\noptimization problems, having many practical applications. This paper\ninvestigates the development of an algorithm to solve SCP by employing chemical\nreaction optimization (CRO), a general-purpose metaheuristic. It is tested on a\nwide range of benchmark instances of SCP. The simulation results indicate that\nthis algorithm gives outstanding performance compared with other heuristics and\nmetaheuristics in solving SCP."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.00718v2", 
    "title": "Product Reservoir Computing: Time-Series Computation with Multiplicative   Neurons", 
    "arxiv-id": "1502.00718v2", 
    "author": "Darko Stefanovic", 
    "publish": "2015-02-03T03:04:33Z", 
    "summary": "Echo state networks (ESN), a type of reservoir computing (RC) architecture,\nare efficient and accurate artificial neural systems for time series processing\nand learning. An ESN consists of a core of recurrent neural networks, called a\nreservoir, with a small number of tunable parameters to generate a\nhigh-dimensional representation of an input, and a readout layer which is\neasily trained using regression to produce a desired output from the reservoir\nstates. Certain computational tasks involve real-time calculation of high-order\ntime correlations, which requires nonlinear transformation either in the\nreservoir or the readout layer. Traditional ESN employs a reservoir with\nsigmoid or tanh function neurons. In contrast, some types of biological neurons\nobey response curves that can be described as a product unit rather than a sum\nand threshold. Inspired by this class of neurons, we introduce a RC\narchitecture with a reservoir of product nodes for time series computation. We\nfind that the product RC shows many properties of standard ESN such as\nshort-term memory and nonlinear capacity. On standard benchmarks for chaotic\nprediction tasks, the product RC maintains the performance of a standard\nnonlinear ESN while being more amenable to mathematical analysis. Our study\nprovides evidence that such networks are powerful in highly nonlinear tasks\nowing to high-order statistics generated by the recurrent product node\nreservoir."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.02407v1", 
    "title": "A Social Spider Algorithm for Global Optimization", 
    "arxiv-id": "1502.02407v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-02-09T09:46:40Z", 
    "summary": "The growing complexity of real-world problems has motivated computer\nscientists to search for efficient problem-solving methods. Metaheuristics\nbased on evolutionary computation and swarm intelligence are outstanding\nexamples of nature-inspired solution techniques. Inspired by the social\nspiders, we propose a novel Social Spider Algorithm to solve global\noptimization problems. This algorithm is mainly based on the foraging strategy\nof social spiders, utilizing the vibrations on the spider web to determine the\npositions of preys. Different from the previously proposed swarm intelligence\nalgorithms, we introduce a new social animal foraging strategy model to solve\noptimization problems. In addition, we perform preliminary parameter\nsensitivity analysis for our proposed algorithm, developing guidelines for\nchoosing the parameter values. The Social Spider Algorithm is evaluated by a\nseries of widely-used benchmark functions, and our proposed algorithm has\nsuperior performance compared with other state-of-the-art metaheuristics."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.02444v1", 
    "title": "On the Dynamics of a Recurrent Hopfield Network", 
    "arxiv-id": "1502.02444v1", 
    "author": "Moncef Gabbouj", 
    "publish": "2015-02-09T11:45:02Z", 
    "summary": "In this research paper novel real/complex valued recurrent Hopfield Neural\nNetwork (RHNN) is proposed. The method of synthesizing the energy landscape of\nsuch a network and the experimental investigation of dynamics of Recurrent\nHopfield Network is discussed. Parallel modes of operation (other than fully\nparallel mode) in layered RHNN is proposed. Also, certain potential\napplications are proposed."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.02793v1", 
    "title": "The Benefit of Sex in Noisy Evolutionary Search", 
    "arxiv-id": "1502.02793v1", 
    "author": "Andrew M. Sutton", 
    "publish": "2015-02-10T06:26:15Z", 
    "summary": "The benefit of sexual recombination is one of the most fundamental questions\nboth in population genetics and evolutionary computation. It is widely believed\nthat recombination helps solving difficult optimization problems. We present\nthe first result, which rigorously proves that it is beneficial to use sexual\nrecombination in an uncertain environment with a noisy fitness function. For\nthis, we model sexual recombination with a simple estimation of distribution\nalgorithm called the Compact Genetic Algorithm (cGA), which we compare with the\nclassical $\\mu+1$ EA. For a simple noisy fitness function with additive\nGaussian posterior noise $\\mathcal{N}(0,\\sigma^2)$, we prove that the\nmutation-only $\\mu+1$ EA typically cannot handle noise in polynomial time for\n$\\sigma^2$ large enough while the cGA runs in polynomial time as long as the\npopulation size is not too small. This shows that in this uncertain environment\nsexual recombination is provably beneficial. We observe the same behavior in a\nsmall empirical study."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.03699v1", 
    "title": "Analysis of Solution Quality of a Multiobjective Optimization-based   Evolutionary Algorithm for Knapsack Problem", 
    "arxiv-id": "1502.03699v1", 
    "author": "Yuren Zhou", 
    "publish": "2015-02-12T15:24:19Z", 
    "summary": "Multi-objective optimisation is regarded as one of the most promising ways\nfor dealing with constrained optimisation problems in evolutionary\noptimisation. This paper presents a theoretical investigation of a\nmulti-objective optimisation evolutionary algorithm for solving the 0-1\nknapsack problem. Two initialisation methods are considered in the algorithm:\nlocal search initialisation and greedy search initialisation. Then the solution\nquality of the algorithm is analysed in terms of the approximation ratio."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2014.6900233", 
    "link": "http://arxiv.org/pdf/1502.04423v2", 
    "title": "Exploring Transfer Function Nonlinearity in Echo State Networks", 
    "arxiv-id": "1502.04423v2", 
    "author": "Darko Stefanovic", 
    "publish": "2015-02-16T05:52:23Z", 
    "summary": "Supralinear and sublinear pre-synaptic and dendritic integration is\nconsidered to be responsible for nonlinear computation power of biological\nneurons, emphasizing the role of nonlinear integration as opposed to nonlinear\noutput thresholding. How, why, and to what degree the transfer function\nnonlinearity helps biologically inspired neural network models is not fully\nunderstood. Here, we study these questions in the context of echo state\nnetworks (ESN). ESN is a simple neural network architecture in which a fixed\nrecurrent network is driven with an input signal, and the output is generated\nby a readout layer from the measurements of the network states. ESN\narchitecture enjoys efficient training and good performance on certain\nsignal-processing tasks, such as system identification and time series\nprediction. ESN performance has been analyzed with respect to the connectivity\npattern in the network structure and the input bias. However, the effects of\nthe transfer function in the network have not been studied systematically.\nHere, we use an approach tanh on the Taylor expansion of a frequently used\ntransfer function, the hyperbolic tangent function, to systematically study the\neffect of increasing nonlinearity of the transfer function on the memory,\nnonlinear capacity, and signal processing performance of ESN. Interestingly, we\nfind that a quadratic approximation is enough to capture the computational\npower of ESN with tanh function. The results of this study apply to both\nsoftware and hardware implementation of ESN."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00789", 
    "link": "http://arxiv.org/pdf/1502.06094v2", 
    "title": "Positive Neural Networks in Discrete Time Implement Monotone-Regular   Behaviors", 
    "arxiv-id": "1502.06094v2", 
    "author": "Jan Van den Bussche", 
    "publish": "2015-02-21T11:17:08Z", 
    "summary": "We study the expressive power of positive neural networks. The model uses\npositive connection weights and multiple input neurons. Different behaviors can\nbe expressed by varying the connection weights. We show that in discrete time,\nand in absence of noise, the class of positive neural networks captures the\nso-called monotone-regular behaviors, that are based on regular languages. A\nfiner picture emerges if one takes into account the delay by which a\nmonotone-regular behavior is implemented. Each monotone-regular behavior can be\nimplemented by a positive neural network with a delay of one time unit. Some\nmonotone-regular behaviors can be implemented with zero delay. And,\ninterestingly, some simple monotone-regular behaviors can not be implemented\nwith zero delay."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00789", 
    "link": "http://arxiv.org/pdf/1503.00505v1", 
    "title": "A neuromorphic hardware framework based on population coding", 
    "arxiv-id": "1503.00505v1", 
    "author": "Andr\u00e9 van Schaik", 
    "publish": "2015-03-02T12:55:54Z", 
    "summary": "In the biological nervous system, large neuronal populations work\ncollaboratively to encode sensory stimuli. These neuronal populations are\ncharacterised by a diverse distribution of tuning curves, ensuring that the\nentire range of input stimuli is encoded. Based on these principles, we have\ndesigned a neuromorphic system called a Trainable Analogue Block (TAB), which\nencodes given input stimuli using a large population of neurons with a\nheterogeneous tuning curve profile. Heterogeneity of tuning curves is achieved\nusing random device mismatches in VLSI (Very Large Scale Integration) process\nand by adding a systematic offset to each hidden neuron. Here, we present\nmeasurement results of a single test cell fabricated in a 65nm technology to\nverify the TAB framework. We have mimicked a large population of neurons by\nre-using measurement results from the test cell by varying offset. We thus\ndemonstrate the learning capability of the system for various regression tasks.\nThe TAB system may pave the way to improve the design of analogue circuits for\ncommercial applications, by rendering circuits insensitive to random mismatch\nthat arises due to the manufacturing process."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00789", 
    "link": "http://arxiv.org/pdf/1503.01524v1", 
    "title": "Genetic optimization of the Hyperloop route through the Grapevine", 
    "arxiv-id": "1503.01524v1", 
    "author": "Casey J. Handmer", 
    "publish": "2015-03-05T03:29:16Z", 
    "summary": "We demonstrate a genetic algorithm that employs a versatile fitness function\nto optimize route selection for the Hyperloop, a proposed high speed passenger\ntransportation system."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.nonrwa.2009.04.006", 
    "link": "http://arxiv.org/pdf/1503.01847v1", 
    "title": "Estimation of the parameters of an infectious disease model using neural   networks", 
    "arxiv-id": "1503.01847v1", 
    "author": "M. Naresh Kumar", 
    "publish": "2015-03-06T04:48:00Z", 
    "summary": "In this paper, we propose a realistic mathematical model taking into account\nthe mutual interference among the interacting populations. This model attempts\nto describe the control (vaccination) function as a function of the number of\ninfective individuals, which is an improvement over the existing susceptible\ninfective epidemic models. Regarding the growth of the epidemic as a nonlinear\nphenomenon we have developed a neural network architecture to estimate the\nvital parameters associated with this model. This architecture is based on a\nrecently developed new class of neural networks known as co-operative and\nsupportive neural networks. The application of this architecture to the present\nstudy involves preprocessing of the input data, and this renders an efficient\nestimation of the rate of spread of the epidemic. It is observed that the\nproposed new neural network outperforms a simple feed-forward neural network\nand polynomial regression."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.nonrwa.2009.04.006", 
    "link": "http://arxiv.org/pdf/1503.01954v2", 
    "title": "Denoising Autoencoders for fast Combinatorial Black Box Optimization", 
    "arxiv-id": "1503.01954v2", 
    "author": "Malte Probst", 
    "publish": "2015-03-06T13:47:58Z", 
    "summary": "Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Autoencoders (AE) are\ngenerative stochastic networks with these desired properties. We integrate a\nspecial type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluate\nthe performance of DAE-EDA on several combinatorial optimization problems with\na single objective. We asses the number of fitness evaluations as well as the\nrequired CPU times. We compare the results to the performance to the Bayesian\nOptimization Algorithm (BOA) and RBM-EDA, another EDA which is based on a\ngenerative neural network which has proven competitive with BOA. For the\nconsidered problem instances, DAE-EDA is considerably faster than BOA and\nRBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations is\nhigher than for BOA, but competitive with RBM-EDA. These results show that DAEs\ncan be useful tools for problems with low but non-negligible fitness evaluation\ncosts."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.nonrwa.2009.04.006", 
    "link": "http://arxiv.org/pdf/1503.03011v1", 
    "title": "Technical Analysis on Financial Forecasting", 
    "arxiv-id": "1503.03011v1", 
    "author": "Kishore Kumar Sahu", 
    "publish": "2015-03-10T17:48:17Z", 
    "summary": "Financial forecasting is an estimation of future financial outcomes for a\ncompany, industry, country using historical internal accounting and sales data.\nWe may predict the future outcome of BSE_SENSEX practically by some soft\ncomputing techniques and can also optimized using PSO (Particle Swarm\nOptimization), EA (Evolutionary Algorithm) or DEA (Differential Evolutionary\nAlgorithm) etc. PSO is a biologically inspired computational search &\noptimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based on\nthe social behaviors of fish schooling or birds flocking. PSO is a promising\nmethod to train Artificial Neural Network (ANN). It is easy to implement then\nGenetic Algorithm except few parameters are adjusted. PSO is a random & pattern\nsearch technique based on populating of particle. In PSO, the particles are\nhaving some position and velocity in the search space. Two terms are used in\nPSO one is Local Best and another one is Global Best. To optimize problems that\nare like Irregular, Noisy, Change over time, Static etc. PSO uses a classic\noptimization method such as Gradient Decent & Quasi-Newton Methods. The\nobservation and review of few related studies in the last few years, focusing\non function of PSO, modification of PSO and operation that have implemented\nusing PSO like function optimization, ANN Training & Fuzzy Control etc.\nDifferential Evolution is an efficient EA technique for optimization of\nnumerical problems, financial problems etc. PSO technique is introduced due to\nthe swarming behavior of animals which is the collective behavior of similar\nsize that aggregates together."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.nonrwa.2009.04.006", 
    "link": "http://arxiv.org/pdf/1503.03175v1", 
    "title": "Benchmarking NLopt and state-of-art algorithms for Continuous Global   Optimization via Hybrid IACO$_\\mathbb{R}$", 
    "arxiv-id": "1503.03175v1", 
    "author": "Jayadeva", 
    "publish": "2015-03-11T04:59:12Z", 
    "summary": "This paper presents a comparative analysis of the performance of the\nIncremental Ant Colony algorithm for continuous optimization\n($IACO_\\mathbb{R}$), with different algorithms provided in the NLopt library.\nThe key objective is to understand how the various algorithms in the NLopt\nlibrary perform in combination with the Multi Trajectory Local Search (Mtsls1)\ntechnique. A hybrid approach has been introduced in the local search strategy\nby the use of a parameter which allows for probabilistic selection between\nMtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch is\nmade based on the algorithm being used in the previous iteration. The paper\npresents an exhaustive comparison on the performance of these approaches on\nSoft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014\nbenchmarks. For both benchmarks, we conclude that the best performing algorithm\nis a hybrid variant of Mtsls1 with BFGS for local search."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.nonrwa.2009.04.006", 
    "link": "http://arxiv.org/pdf/1503.04475v1", 
    "title": "Simulation of Genetic Algorithm: Traffic Light Efficiency", 
    "arxiv-id": "1503.04475v1", 
    "author": "Eric Lienert", 
    "publish": "2015-03-15T20:56:04Z", 
    "summary": "Traffic is a problem in many urban areas worldwide. Traffic flow is dictated\nby certain devices such as traffic lights. The traffic lights signal when each\nlane is able to pass through the intersection. Often, static schedules\ninterfere with ideal traffic flow. The purpose of this project was to find a\nway to make intersections controlled with traffic lights more efficient. This\ngoal was accomplished through the creation of a genetic algorithm, which\nenhances an input algorithm through genetic principles to produce the fittest\nalgorithm. The program was comprised of two major elements: coding in Java and\ncoding in Simulation of Urban Mobility (SUMO), which is an environment that\nsimulates real traffic. The Java code called upon the SUMO simulation via a\ncommand prompt which ran the simulation, received the output, altered the\nalgorithm, and looped. The SUMO component initialized a simulation in which a 1\nx 1 street layout was created, each intersection with its own traffic light.\nEach loop enhanced the input algorithm by altering the scheduling string\n(dictates the light changes). After the looped simulations were executed, the\ndata was then analyzed. This was accomplished by creating an algorithm based\nupon regular practice, timed traffic lights, and comparing the output which was\ncomprised of the total time it took for all vehicles to exit the system and the\naverage time it took each individual vehicle to exit the system. These\ndifferent variables: the time it took the average vehicle to exit the system\nand total time for all vehicles to exit the system, where then graphed together\nto provide a visual aid. The genetic algorithm did improve traffic light and\ntraffic flow efficiency in comparison to traditional scheduling methods."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JSEN.2009.2038124", 
    "link": "http://arxiv.org/pdf/1503.05272v1", 
    "title": "Improved Calibration of Near-Infrared Spectra by Using Ensembles of   Neural Network Models", 
    "arxiv-id": "1503.05272v1", 
    "author": "S. Bonenfant", 
    "publish": "2015-03-18T02:54:04Z", 
    "summary": "IR or near-infrared (NIR) spectroscopy is a method used to identify a\ncompound or to analyze the composition of a material. Calibration of NIR\nspectra refers to the use of the spectra as multivariate descriptors to predict\nconcentrations of the constituents. To build a calibration model,\nstate-of-the-art software predominantly uses linear regression techniques. For\nnonlinear calibration problems, neural network-based models have proved to be\nan interesting alternative. In this paper, we propose a novel extension of the\nconventional neural network-based approach, the use of an ensemble of neural\nnetwork models. The individual neural networks are obtained by resampling the\navailable training data with bootstrapping or cross-validation techniques. The\nresults obtained for a realistic calibration example show that the\nensemble-based approach produces a significantly more accurate and robust\ncalibration model than conventional regression methods."
},{
    "category": "cs.NE", 
    "doi": "10.1007/11760023_190", 
    "link": "http://arxiv.org/pdf/1503.06004v1", 
    "title": "Feeder Load Balancing using Neural Network", 
    "arxiv-id": "1503.06004v1", 
    "author": "J. Jordaan", 
    "publish": "2015-03-20T06:48:14Z", 
    "summary": "The distribution system problems, such as planning, loss minimization, and\nenergy restoration, usually involve the phase balancing or network\nreconfiguration procedures. The determination of an optimal phase balance is,\nin general, a combinatorial optimization problem. This paper proposes optimal\nreconfiguration of the phase balancing using the neural network, to switch on\nand off the different switches, allowing the three phases supply by the\ntransformer to the end-users to be balanced. This paper presents the\napplication examples of the proposed method using the real and simulated test\ndata."
},{
    "category": "cs.NE", 
    "doi": "10.1007/11760023_190", 
    "link": "http://arxiv.org/pdf/1503.06866v4", 
    "title": "Study of all the periods of a Neuronal Recurrence Equation", 
    "arxiv-id": "1503.06866v4", 
    "author": "Ren\u00e8 Ndoundam", 
    "publish": "2015-03-23T22:30:39Z", 
    "summary": "We characterize the structure of the periods of a neuronal recurrence\nequation. Firstly, we give a characterization of k-chains in 0-1 periodic\nsequences. Secondly, we characterize the periods of all cycles of some neuronal\nrecurrence equation. Thirdly, we explain how these results can be used to\ndeduce the existence of the generalized period-halving bifurcation."
},{
    "category": "cs.NE", 
    "doi": "10.1007/11760023_190", 
    "link": "http://arxiv.org/pdf/1503.07793v2", 
    "title": "Gibbs Sampling with Low-Power Spiking Digital Neurons", 
    "arxiv-id": "1503.07793v2", 
    "author": "Ken Kreutz-Delgado", 
    "publish": "2015-03-26T17:14:00Z", 
    "summary": "Restricted Boltzmann Machines and Deep Belief Networks have been successfully\nused in a wide variety of applications including image classification and\nspeech recognition. Inference and learning in these algorithms uses a Markov\nChain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms\nthe kernel of this sampler which can be realized from the firing statistics of\nnoisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper\ndemonstrates such an implementation on an array of digital spiking neurons with\nstochastic leak and threshold properties for inference tasks and presents some\nkey performance metrics for such a hardware-based sampler in both the\ngenerative and discriminative contexts."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2015.7280550", 
    "link": "http://arxiv.org/pdf/1503.08322v1", 
    "title": "Some Further Evidence about Magnification and Shape in Neural Gas", 
    "arxiv-id": "1503.08322v1", 
    "author": "Marco Piastra", 
    "publish": "2015-03-28T16:33:20Z", 
    "summary": "Neural gas (NG) is a robust vector quantization algorithm with a well-known\nmathematical model. According to this, the neural gas samples the underlying\ndata distribution following a power law with a magnification exponent that\ndepends on data dimensionality only. The effects of shape in the input data\ndistribution, however, are not entirely covered by the NG model above, due to\nthe technical difficulties involved. The experimental work described here shows\nthat shape is indeed relevant in determining the overall NG behavior; in\nparticular, some experiments reveal richer and complex behaviors induced by\nshape that cannot be explained by the power law alone. Although a more\ncomprehensive analytical model remains to be defined, the evidence collected in\nthese experiments suggests that the NG algorithm has an interesting potential\nfor detecting complex shapes in noisy datasets."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1503.09129v1", 
    "title": "Encoding Spike Patterns in Multilayer Spiking Neural Networks", 
    "arxiv-id": "1503.09129v1", 
    "author": "Andr\u00e9 Gr\u00fcning", 
    "publish": "2015-03-31T17:12:07Z", 
    "summary": "Information encoding in the nervous system is supported through the precise\nspike-timings of neurons; however, an understanding of the underlying processes\nby which such representations are formed in the first place remains unclear.\nHere we examine how networks of spiking neurons can learn to encode for input\npatterns using a fully temporal coding scheme. To this end, we introduce a\nlearning rule for spiking networks containing hidden neurons which optimizes\nthe likelihood of generating desired output spiking patterns. We show the\nproposed learning rule allows for a large number of accurate input-output spike\npattern mappings to be learnt, which outperforms other existing learning rules\nfor spiking neural networks: both in the number of mappings that can be learnt\nas well as the complexity of spike train encodings that can be utilised. The\nlearning rule is successful even in the presence of input noise, is\ndemonstrated to solve the linearly non-separable XOR computation and\ngeneralizes well on an example dataset. We further present a biologically\nplausible implementation of backpropagated learning in multilayer spiking\nnetworks, and discuss the neural mechanisms that might underlie its function.\nOur approach contributes both to a systematic understanding of how pattern\nencodings might take place in the nervous system, and a learning rule that\ndisplays strong technical capability."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.00154v1", 
    "title": "A New Repair Operator for Multi-objective Evolutionary Algorithm in   Constrained Optimization Problems", 
    "arxiv-id": "1504.00154v1", 
    "author": "Erik Goodman", 
    "publish": "2015-04-01T09:07:13Z", 
    "summary": "In this paper, we design a set of multi-objective constrained optimization\nproblems (MCOPs) and propose a new repair operator to address them. The\nproposed repair operator is used to fix the solutions that violate the box\nconstraints. More specifically, it employs a reversed correction strategy that\ncan effectively avoid the population falling into local optimum. In addition,\nwe integrate the proposed repair operator into two classical multi-objective\nevolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is\ncompared with other two kinds of commonly used repair operators on benchmark\nproblems CTPs and MCOPs. The experiment results demonstrate that our proposed\napproach is very effective in terms of convergence and diversity."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.02590v1", 
    "title": "Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic   Algorithm, Using Symmetric Travelling Salesman Problem", 
    "arxiv-id": "1504.02590v1", 
    "author": "Kamran Zamanifar", 
    "publish": "2015-04-10T08:53:03Z", 
    "summary": "The Travelling Salesman Problem (TSP) is one of the most famous optimization\nproblems. The Genetic Algorithm (GA) is one of metaheuristics that have been\napplied to TSP. The Crossover and mutation operators are two important elements\nof GA. There are many TSP solver crossover operators. In this paper, we state\nimplementation of some recent TSP solver crossovers at first and then we use\neach of them in GA to solve some Symmetric TSP (STSP) instances and finally\ncompare their effects on speed and accuracy of presented GA."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.03212v1", 
    "title": "Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th   Rule in Discrete Settings", 
    "arxiv-id": "1504.03212v1", 
    "author": "Carola Doerr", 
    "publish": "2015-04-13T15:16:00Z", 
    "summary": "While evolutionary algorithms are known to be very successful for a broad\nrange of applications, the algorithm designer is often left with many\nalgorithmic choices, for example, the size of the population, the mutation\nrates, and the crossover rates of the algorithm. These parameters are known to\nhave a crucial influence on the optimization time, and thus need to be chosen\ncarefully, a task that often requires substantial efforts. Moreover, the\noptimal parameters can change during the optimization process. It is therefore\nof great interest to design mechanisms that dynamically choose best-possible\nparameters. An example for such an update mechanism is the one-fifth success\nrule for step-size adaption in evolutionary strategies. While in continuous\ndomains this principle is well understood also from a mathematical point of\nview, no comparable theory is available for problems in discrete domains.\n  In this work we show that the one-fifth success rule can be effective also in\ndiscrete settings. We regard the $(1+(\\lambda,\\lambda))$~GA proposed in\n[Doerr/Doerr/Ebel: From black-box complexity to designing new genetic\nalgorithms, TCS 2015]. We prove that if its population size is chosen according\nto the one-fifth success rule then the expected optimization time on\n\\textsc{OneMax} is linear. This is better than what \\emph{any} static\npopulation size $\\lambda$ can achieve and is asymptotically optimal also among\nall adaptive parameter choices."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.04421v1", 
    "title": "Feasibility Preserving Constraint-Handling Strategies for Real Parameter   Evolutionary Optimization", 
    "arxiv-id": "1504.04421v1", 
    "author": "Kalyanmoy Deb", 
    "publish": "2015-04-17T01:55:23Z", 
    "summary": "Evolutionary Algorithms (EAs) are being routinely applied for a variety of\noptimization tasks, and real-parameter optimization in the presence of\nconstraints is one such important area. During constrained optimization EAs\noften create solutions that fall outside the feasible region; hence a viable\nconstraint- handling strategy is needed. This paper focuses on the class of\nconstraint-handling strategies that repair infeasible solutions by bringing\nthem back into the search space and explicitly preserve feasibility of the\nsolutions. Several existing constraint-handling strategies are studied, and two\nnew single parameter constraint-handling methodologies based on parent-centric\nand inverse parabolic probability (IP) distribution are proposed. The existing\nand newly proposed constraint-handling methods are first studied with PSO, DE,\nGAs, and simulation results on four scalable test-problems under different\nlocation settings of the optimum are presented. The newly proposed\nconstraint-handling methods exhibit robustness in terms of performance and also\nsucceed on search spaces comprising up-to 500 variables while locating the\noptimum within an error of 10$^{-10}$. The working principle of the IP based\nmethods is also demonstrated on (i) some generic constrained optimization\nproblems, and (ii) a classic `Weld' problem from structural design and\nmechanics. The successful performance of the proposed methods clearly exhibits\ntheir efficacy as a generic constrained-handling strategy for a wide range of\napplications."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.05158v1", 
    "title": "Multi-swarm PSO algorithm for the Quadratic Assignment Problem: a   massive parallel implementation on the OpenCL platform", 
    "arxiv-id": "1504.05158v1", 
    "author": "Wojciech Chmiel", 
    "publish": "2015-04-20T18:58:08Z", 
    "summary": "This paper presents a multi-swarm PSO algorithm for the Quadratic Assignment\nProblem (QAP) implemented on OpenCL platform. Our work was motivated by results\nof time efficiency tests performed for single-swarm algorithm implementation\nthat showed clearly that the benefits of a parallel execution platform can be\nfully exploited, if the processed population is large. The described algorithm\ncan be executed in two modes: with independent swarms or with migration. We\ndiscuss the algorithm construction, as well as we report results of tests\nperformed on several problem instances from the QAPLIB library. During the\nexperiments the algorithm was configured to process large populations. This\nallowed us to collect statistical data related to values of goal function\nreached by individual particles. We use them to demonstrate on two test cases\nthat although single particles seem to behave chaotically during the\noptimization process, when the whole population is analyzed, the probability\nthat a particle will select a near-optimal solution grows."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.05766v1", 
    "title": "Honeybees-inspired heuristic algorithms for numerical optimisation", 
    "arxiv-id": "1504.05766v1", 
    "author": "Muharrem D\u00fc\u011fenci", 
    "publish": "2015-04-22T12:46:07Z", 
    "summary": "Swarm intelligence is all about developing collective behaviours to solve\ncomplex, ill-structured and large-scale problems. Efficiency in collective\nbehaviours depends on how to harmonise the individual contributions so that a\ncomplementary collective effort can be achieved to offer a useful solution. The\nmain points in organising the harmony remains as managing the diversification\nand intensification actions appropriately, where the efficiency of collective\nbehaviours depends on blending these two actions appropriately. In this study,\ntwo swarm intelligence algorithms inspired of natural honeybee colonies have\nbeen overviewed with many respects and two new revisions and a hybrid version\nhave been studied to improve the efficiencies in solving numerical optimisation\nproblems, which are well-known hard benchmarks. Consequently, the revisions and\nespecially the hybrid algorithm proposed have outperformed the two original bee\nalgorithms in solving these very hard numerical optimisation benchmarks."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.05767v1", 
    "title": "Rounding Methods for Neural Networks with Low Resolution Synaptic   Weights", 
    "arxiv-id": "1504.05767v1", 
    "author": "Giacomo Indiveri", 
    "publish": "2015-04-22T12:47:32Z", 
    "summary": "Neural network algorithms simulated on standard computing platforms typically\nmake use of high resolution weights, with floating-point notation. However, for\ndedicated hardware implementations of such algorithms, fixed-point synaptic\nweights with low resolution are preferable. The basic approach of reducing the\nresolution of the weights in these algorithms by standard rounding methods\nincurs drastic losses in performance. To reduce the resolution further, in the\nextreme case even to binary weights, more advanced techniques are necessary. To\nthis end, we propose two methods for mapping neural network algorithms with\nhigh resolution weights to corresponding algorithms that work with low\nresolution weights and demonstrate that their performance is substantially\nbetter than standard rounding. We further use these methods to investigate the\nperformance of three common neural network algorithms under fixed memory size\nof the weight matrix with different weight resolutions. We show that dedicated\nhardware systems, whose technology dictates very low weight resolutions (be\nthey electronic or biological) could in principle implement the algorithms we\nstudy."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.06260v2", 
    "title": "First Steps Towards a Runtime Comparison of Natural and Artificial   Evolution", 
    "arxiv-id": "1504.06260v2", 
    "author": "Barbora Trubenov\u00e1", 
    "publish": "2015-04-23T17:13:31Z", 
    "summary": "Evolutionary algorithms (EAs) form a popular optimisation paradigm inspired\nby natural evolution. In recent years the field of evolutionary computation has\ndeveloped a rigorous analytical theory to analyse their runtime on many\nillustrative problems. Here we apply this theory to a simple model of natural\nevolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime the\ntime between occurrence of new mutations is much longer than the time it takes\nfor a new beneficial mutation to take over the population. In this situation,\nthe population only contains copies of one genotype and evolution can be\nmodelled as a (1+1)-type process where the probability of accepting a new\ngenotype (improvements or worsenings) depends on the change in fitness.\n  We present an initial runtime analysis of SSWM, quantifying its performance\nfor various parameters and investigating differences to the (1+1)EA. We show\nthat SSWM can have a moderate advantage over the (1+1)EA at crossing fitness\nvalleys and study an example where SSWM outperforms the (1+1)EA by taking\nadvantage of information on the fitness gradient."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.06859v1", 
    "title": "When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization", 
    "arxiv-id": "1504.06859v1", 
    "author": "Mosab Bazargani", 
    "publish": "2015-04-26T18:13:30Z", 
    "summary": "It has been shown in the past that a multistart hillclimbing strategy\ncompares favourably to a standard genetic algorithm with respect to solving\ninstances of the multimodal problem generator. We extend that work and verify\nif the utilization of diversity preservation techniques in the genetic\nalgorithm changes the outcome of the comparison. We do so under two scenarios:\n(1) when the goal is to find the global optimum, (2) when the goal is to find\nall optima.\n  A mathematical analysis is performed for the multistart hillclimbing\nalgorithm and a through empirical study is conducted for solving instances of\nthe multimodal problem generator with increasing number of optima, both with\nthe hillclimbing strategy as well as with genetic algorithms with niching.\nAlthough niching improves the performance of the genetic algorithm, it is still\ninferior to the multistart hillclimbing strategy on this class of problems.\n  An idealized niching strategy is also presented and it is argued that its\nperformance should be close to a lower bound of what any evolutionary algorithm\ncan do on this class of problems."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.07278v1", 
    "title": "Optimal Convergence Rate in Feed Forward Neural Networks using HJB   Equation", 
    "arxiv-id": "1504.07278v1", 
    "author": "Ajay Pratap Yadav", 
    "publish": "2015-04-27T21:09:15Z", 
    "summary": "A control theoretic approach is presented in this paper for both batch and\ninstantaneous updates of weights in feed-forward neural networks. The popular\nHamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimal\nweight update law. The remarkable contribution in this paper is that closed\nform solutions for both optimal cost and weight update can be achieved for any\nfeed-forward network using HJB equation in a simple yet elegant manner. The\nproposed approach has been compared with some of the existing best performing\nlearning algorithms. It is found as expected that the proposed approach is\nfaster in convergence in terms of computational time. Some of the benchmark\ntest data such as 8-bit parity, breast cancer and credit approval, as well as\n2D Gabor function have been used to validate our claims. The paper also\ndiscusses issues related to global optimization. The limitations of popular\ndeterministic weight update laws are critiqued and the possibility of global\noptimization using HJB formulation is discussed. It is hoped that the proposed\nalgorithm will bring in a lot of interest in researchers working in developing\nfast learning algorithms and global optimization."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.07327v1", 
    "title": "Toward Smart Power Grids: Communication Network Design for Power Grids   Synchronization", 
    "arxiv-id": "1504.07327v1", 
    "author": "Siamak Talebi", 
    "publish": "2015-04-28T01:51:15Z", 
    "summary": "In smart power grids, keeping the synchronicity of generators and the\ncorresponding controls is of great importance. To do so, a simple model is\nemployed in terms of swing equation to represent the interactions among\ndynamics of generators and feedback control. In case of having a communication\nnetwork available, the control can be done based on the transmitted\nmeasurements by the communication network. The stability of system is denoted\nby the largest eigenvalue of the weighted sum of the Laplacian matrices of the\ncommunication infrastructure and power network. In this work, we use graph\ntheory to model the communication network as a graph problem. Then, Ant Colony\nSystem (ACS) is employed for optimum design of above graph for synchronization\nof power grids. Performance evaluation of the proposed method for the 39-bus\nNew England power system versus methods such as exhaustive search and Rayleigh\nquotient approximation indicates feasibility and effectiveness of our method\nfor even large scale smart power grids."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.07329v1", 
    "title": "Combined A*-Ants Algorithm: A New Multi-Parameter Vehicle Navigation   Scheme", 
    "arxiv-id": "1504.07329v1", 
    "author": "Fereydoun Farrahi-Moghaddam", 
    "publish": "2015-04-28T01:54:49Z", 
    "summary": "In this paper a multi-parameter A*(A- star)-ants based algorithm is proposed\nin order to find the best optimized multi-parameter path between two desired\npoints in regions. This algorithm recognizes paths, according to user desired\nparameters using electronic maps. The proposed algorithm is a combination of A*\nand ants algorithm in which the proposed A* algorithm is the prologue to the\nsuggested ant based algorithm .In fact, this A* algorithm invigorates some\npaths pheromones in ants algorithm. As one of implementations of this method,\nthis algorithm was applied on a part of Kerman city, Iran as a multi-parameter\nvehicle navigator. It finds the best optimized multi-parameter direction\nbetween two desired junctions based on city traveler parameters. Comparison\nresults between the proposed method and ants algorithm demonstrates efficiency\nand lower cost function results of the proposed method versus ants algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1504.08117v3", 
    "title": "Average Convergence Rate of Evolutionary Algorithms", 
    "arxiv-id": "1504.08117v3", 
    "author": "Guangming Lin", 
    "publish": "2015-04-30T08:35:47Z", 
    "summary": "In evolutionary optimization, it is important to understand how fast\nevolutionary algorithms converge to the optimum per generation, or their\nconvergence rate. This paper proposes a new measure of the convergence rate,\ncalled average convergence rate. It is a normalised geometric mean of the\nreduction ratio of the fitness difference per generation. The calculation of\nthe average convergence rate is very simple and it is applicable for most\nevolutionary algorithms on both continuous and discrete optimization. A\ntheoretical study of the average convergence rate is conducted for discrete\noptimization. Lower bounds on the average convergence rate are derived. The\nlimit of the average convergence rate is analysed and then the asymptotic\naverage convergence rate is proposed."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.00075v1", 
    "title": "A Cooperative Framework for Fireworks Algorithm", 
    "arxiv-id": "1505.00075v1", 
    "author": "Ying Tan", 
    "publish": "2015-05-01T02:56:42Z", 
    "summary": "This paper presents a cooperative framework for fireworks algorithm (CoFFWA).\nA detailed analysis of existing fireworks algorithm (FWA) and its recently\ndeveloped variants has revealed that (i) the selection strategy lead to the\ncontribution of the firework with the best fitness (core firework) for the\noptimization overwhelms the contributions of the rest of fireworks (non-core\nfireworks) in the explosion operator, (ii) the Gaussian mutation operator is\nnot as effective as it is designed to be. To overcome these limitations, the\nCoFFWA is proposed, which can greatly enhance the exploitation ability of\nnon-core fireworks by using independent selection operator and increase the\nexploration capacity by crowdness-avoiding cooperative strategy among the\nfireworks. Experimental results on the CEC2013 benchmark functions suggest that\nCoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony,\ndifferential evolution, the standard particle swarm optimization (SPSO) in 2007\nand the most recent SPSO in 2011 in term of convergence performance."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.00444v1", 
    "title": "Some Theoretical Properties of a Network of Discretely Firing Neurons", 
    "arxiv-id": "1505.00444v1", 
    "author": "Stephen Luttrell", 
    "publish": "2015-05-03T16:11:26Z", 
    "summary": "The problem of optimising a network of discretely firing neurons is\naddressed. An objective function is introduced which measures the average\nnumber of bits that are needed for the network to encode its state. When this\nis minimised, it is shown that this leads to a number of results, such as\ntopographic mappings, piecewise linear dependence on the input of the\nprobability of a neuron firing, and factorial encoder networks."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.01474v1", 
    "title": "Retaining Experience and Growing Solutions", 
    "arxiv-id": "1505.01474v1", 
    "author": "Robyn Ffrancon", 
    "publish": "2015-05-06T19:40:26Z", 
    "summary": "Generally, when genetic programming (GP) is used for function synthesis any\nvaluable experience gained by the system is lost from one problem to the next,\neven when the problems are closely related. With the aim of developing a system\nwhich retains beneficial experience from problem to problem, this paper\nintroduces the novel Node-by-Node Growth Solver (NNGS) algorithm which features\na component, called the controller, which can be adapted and improved for use\nacross a set of related problems. NNGS grows a single solution tree from root\nto leaves. Using semantic backpropagation and acting locally on each node in\nturn, the algorithm employs the controller to assign subsequent child nodes\nuntil a fully formed solution is generated.\n  The aim of this paper is to pave a path towards the use of a neural network\nas the controller component and also, separately, towards the use of meta-GP as\na mechanism for improving the controller component. A proof-of-concept\ncontroller is discussed which demonstrates the success and potential of the\nNNGS algorithm. In this case, the controller constitutes a set of hand written\nrules which can be used to deterministically and greedily solve standard\nBoolean function synthesis benchmarks. Even before employing machine learning\nto improve the controller, the algorithm vastly outperforms other well known\nrecent algorithms on run times, maintains comparable solution sizes, and has a\n100% success rate on all Boolean function synthesis benchmarks tested so far."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.01887v1", 
    "title": "Optimal Neuron Selection: NK Echo State Networks for Reinforcement   Learning", 
    "arxiv-id": "1505.01887v1", 
    "author": "Francisco Chicano", 
    "publish": "2015-05-07T22:58:28Z", 
    "summary": "This paper introduces the NK Echo State Network. The problem of learning in\nthe NK Echo State Network is reduced to the problem of optimizing a special\nform of a Spin Glass Problem known as an NK Landscape. No weight adjustment is\nused; all learning is accomplished by spinning up (turning on) or spinning down\n(turning off) neurons in order to find a combination of neurons that work\ntogether to achieve the desired computation. For special types of NK\nLandscapes, an exact global solution can be obtained in polynomial time using\ndynamic programming. The NK Echo State Network is applied to a reinforcement\nlearning problem requiring a recurrent network: balancing two poles on a cart\ngiven no velocity information. Empirical results shows that the NK Echo State\nNetwork learns very rapidly and yields very good generalization."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.02495v1", 
    "title": "An Online Learning Algorithm for Neuromorphic Hardware Implementation", 
    "arxiv-id": "1505.02495v1", 
    "author": "Andre van Schaik", 
    "publish": "2015-05-11T06:23:15Z", 
    "summary": "We propose a sign-based online learning (SOL) algorithm for a neuromorphic\nhardware framework called Trainable Analogue Block (TAB). The TAB framework\nutilises the principles of neural population coding, implying that it encodes\nthe input stimulus using a large pool of nonlinear neurons. The SOL algorithm\nis a simple weight update rule that employs the sign of the hidden layer\nactivation and the sign of the output error, which is the difference between\nthe target output and the predicted output. The SOL algorithm is easily\nimplementable in hardware, and can be used in any artificial neural network\nframework that learns weights by minimising a convex cost function. We show\nthat the TAB framework can be trained for various regression tasks using the\nSOL algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.03917v1", 
    "title": "General Riemannian SOM", 
    "arxiv-id": "1505.03917v1", 
    "author": "Jascha A. Schewtschenko", 
    "publish": "2015-05-14T23:21:40Z", 
    "summary": "Kohonen's Self-Organizing Maps (SOMs) have proven to be a successful\ndata-reduction method to identify the intrinsic lower-dimensional sub-manifold\nof a data set that is scattered in the higher-dimensional feature space.\nMotivated by the possibly non-Euclidian nature of the feature space and of the\nintrinsic geometry of the data set, we extend the definition of classic SOMs to\nobtain the General Riemannian SOM (GRiSOM). We additionally provide an\nimplementation as a proof-of-concept for geometries with constant curvature. We\nfurthermore perform the analytic and numerical analysis of the stability limits\nof certain (GRi)SOM configurations covering the different possible regular\ntessellation of the map space in each geometry. A deviation between the\nnumerical and analytic stability limit has been observed for the square and\nhexagonal Euclidean maps for very small neighbourhoods in the map space as well\nas agreement in case of longer-ranged relations between the map nodes."
},{
    "category": "cs.NE", 
    "doi": "10.1162/NECO_a_00790", 
    "link": "http://arxiv.org/pdf/1505.04211v2", 
    "title": "Discontinuous Piecewise Polynomial Neural Networks", 
    "arxiv-id": "1505.04211v2", 
    "author": "John Loverich", 
    "publish": "2015-05-15T22:21:39Z", 
    "summary": "An artificial neural network is presented based on the idea of connections\nbetween units that are only active for a specific range of input values and\nzero outside that range (and so are not evaluated outside the active range).\nThe connection function is represented by a polynomial with compact support.\nThe finite range of activation allows for great activation sparsity in the\nnetwork and means that theoretically you are able to add computational power to\nthe network without increasing the computational time required to evaluate the\nnetwork for a given input. The polynomial order ranges from first to fifth\norder. Unit dropout is used for regularization and a parameter free weight\nupdate is used. Better performance is obtained by moving from piecewise linear\nconnections to piecewise quadratic, even better performance can be obtained by\nmoving to higher order polynomials. The algorithm is tested on the MAGIC Gamma\nray data set as well as the MNIST data set."
},{
    "category": "cs.NE", 
    "doi": "10.1162/EVCO_a_00103", 
    "link": "http://arxiv.org/pdf/1505.04357v1", 
    "title": "Evolving Spiking Networks with Variable Resistive Memories", 
    "arxiv-id": "1505.04357v1", 
    "author": "Ella Gale", 
    "publish": "2015-05-17T05:23:07Z", 
    "summary": "Neuromorphic computing is a brainlike information processing paradigm that\nrequires adaptive learning mechanisms. A spiking neuro-evolutionary system is\nused for this purpose; plastic resistive memories are implemented as synapses\nin spiking neural networks. The evolutionary design process exploits parameter\nself-adaptation and allows the topology and synaptic weights to be evolved for\neach network in an autonomous manner. Variable resistive memories are the focus\nof this research; each synapse has its own conductance profile which modifies\nthe plastic behaviour of the device and may be altered during evolution. These\nvariable resistive networks are evaluated on a noisy robotic dynamic-reward\nscenario against two static resistive memories and a system containing standard\nconnections only. Results indicate that the extra behavioural degrees of\nfreedom available to the networks incorporating variable resistive memories\nenable them to outperform the comparative synapse types."
},{
    "category": "cs.NE", 
    "doi": "10.1162/EVCO_a_00103", 
    "link": "http://arxiv.org/pdf/1505.04618v1", 
    "title": "Fractally-organized Connectionist Networks: Conjectures and Preliminary   Results", 
    "arxiv-id": "1505.04618v1", 
    "author": "Vincenzo De Florio", 
    "publish": "2015-05-18T13:04:40Z", 
    "summary": "A strict interpretation of connectionism mandates complex networks of simple\ncomponents. The question here is, is this simplicity to be interpreted in\nabsolute terms? I conjecture that absolute simplicity might not be an essential\nattribute of connectionism, and that it may be effectively exchanged with a\nrequirement for relative simplicity, namely simplicity with respect to the\ncurrent organizational level. In this paper I provide some elements to the\nanalysis of the above question. In particular I conjecture that fractally\norganized connectionist networks may provide a convenient means to achive what\nLeibniz calls an \"art of complication\", namely an effective way to encapsulate\ncomplexity and practically extend the applicability of connectionism to domains\nsuch as sociotechnical system modeling and design. Preliminary evidence to my\nclaim is brought by considering the design of the software architecture\ndesigned for the telemonitoring service of Flemish project \"Little Sister\"."
},{
    "category": "cs.NE", 
    "doi": "10.1371/journal.pcbi.1004829", 
    "link": "http://arxiv.org/pdf/1505.06353v1", 
    "title": "The evolutionary origins of hierarchy", 
    "arxiv-id": "1505.06353v1", 
    "author": "Jeff Clune", 
    "publish": "2015-05-23T17:16:32Z", 
    "summary": "Hierarchical organization -- the recursive composition of sub-modules -- is\nubiquitous in biological networks, including neural, metabolic, ecological, and\ngenetic regulatory networks, and in human-made systems, such as large\norganizations and the Internet. To date, most research on hierarchy in networks\nhas been limited to quantifying this property. However, an open, important\nquestion in evolutionary biology is why hierarchical organization evolves in\nthe first place. It has recently been shown that modularity evolves because of\nthe presence of a cost for network connections. Here we investigate whether\nsuch connection costs also tend to cause a hierarchical organization of such\nmodules. In computational simulations, we find that networks without a\nconnection cost do not evolve to be hierarchical, even when the task has a\nhierarchical structure. However, with a connection cost, networks evolve to be\nboth modular and hierarchical, and these networks exhibit higher overall\nperformance and evolvability (i.e. faster adaptation to new environments).\nAdditional analyses confirm that hierarchy independently improves adaptability\nafter controlling for modularity. Overall, our results suggest that the same\nforce--the cost of connections--promotes the evolution of both hierarchy and\nmodularity, and that these properties are important drivers of network\nperformance and adaptability. In addition to shedding light on the emergence of\nhierarchy across the many domains in which it appears, these findings will also\naccelerate future research into evolving more complex, intelligent\ncomputational brains in the fields of artificial intelligence and robotics."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TCSII.2015.2456372", 
    "link": "http://arxiv.org/pdf/1505.07814v2", 
    "title": "A CMOS Spiking Neuron for Brain-Inspired Neural Networks with Resistive   Synapses and In-Situ Learning", 
    "arxiv-id": "1505.07814v2", 
    "author": "Sakkarapani Balagopal", 
    "publish": "2015-05-28T19:30:32Z", 
    "summary": "Nanoscale resistive memories are expected to fuel dense integration of\nelectronic synapses for large-scale neuromorphic system. To realize such a\nbrain-inspired computing chip, a compact CMOS spiking neuron that performs\nin-situ learning and computing while driving a large number of resistive\nsynapses is desired. This work presents a novel leaky integrate-and-fire neuron\ndesign which implements the dual-mode operation of current integration and\nsynaptic drive, with a single opamp and enables in-situ learning with crossbar\nresistive synapses. The proposed design was implemented in a 0.18 $\\mu$m CMOS\ntechnology. Measurements show neuron's ability to drive a thousand resistive\nsynapses, and demonstrate an in-situ associative learning. The neuron circuit\noccupies a small area of 0.01 mm$^2$ and has an energy-efficiency of 9.3\npJ$/$spike$/$synapse."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TCSII.2015.2456372", 
    "link": "http://arxiv.org/pdf/1506.02361v1", 
    "title": "Microscopic approach of a time elapsed neural model", 
    "arxiv-id": "1506.02361v1", 
    "author": "Patricia Reynaud-Bouret", 
    "publish": "2015-06-08T06:41:37Z", 
    "summary": "The spike trains are the main components of the information processing in the\nbrain. To model spike trains several point processes have been investigated in\nthe literature. And more macroscopic approaches have also been studied, using\npartial differential equation models. The main aim of the present article is to\nbuild a bridge between several point processes models (Poisson, Wold, Hawkes)\nthat have been proved to statistically fit real spike trains data and\nage-structured partial differential equations as introduced by Pakdaman,\nPerthame and Salort."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.03264v1", 
    "title": "Memory and information processing in neuromorphic systems", 
    "arxiv-id": "1506.03264v1", 
    "author": "Shih-Chii Liu", 
    "publish": "2015-06-10T11:42:28Z", 
    "summary": "A striking difference between brain-inspired neuromorphic processors and\ncurrent von Neumann processors architectures is the way in which memory and\nprocessing is organized. As Information and Communication Technologies continue\nto address the need for increased computational power through the increase of\ncores within a digital processor, neuromorphic engineers and scientists can\ncomplement this need by building processor architectures where memory is\ndistributed with the processing. In this paper we present a survey of\nbrain-inspired processor architectures that support models of cortical networks\nand deep neural networks. These architectures range from serial clocked\nimplementations of multi-neuron systems to massively parallel asynchronous ones\nand from purely digital systems to mixed analog/digital systems which implement\nmore biological-like models of neurons and synapses together with a suite of\nadaptation and learning mechanisms analogous to the ones found in biological\nnervous systems. We describe the advantages of the different approaches being\npursued and present the challenges that need to be addressed for building\nartificial neural processing systems that can display the richness of behaviors\nseen in biological systems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.05082v2", 
    "title": "A review of landmark articles in the field of co-evolutionary computing", 
    "arxiv-id": "1506.05082v2", 
    "author": "Noe Casas", 
    "publish": "2015-06-10T15:38:17Z", 
    "summary": "Coevolution is a powerful tool in evolutionary computing that mitigates some\nof its endemic problems, namely stagnation in local optima and lack of\nconvergence in high dimensionality problems. Since its inception in 1990, there\nare multiple articles that have contributed greatly to the development and\nimprovement of the coevolutionary techniques. In this report we review some of\nthose landmark articles dwelving in the techniques they propose and how they\nfit to conform robust evolutionary algorithms"
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.05212v1", 
    "title": "Learning Spike time codes through Morphological Learning with Binary   Synapses", 
    "arxiv-id": "1506.05212v1", 
    "author": "Arindam Basu", 
    "publish": "2015-06-17T05:49:50Z", 
    "summary": "In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapses\nthat is able to learn temporal features of spike input patterns is considered.\nSince binary synapses are considered, learning happens through formation and\nelimination of connections between the inputs and the dendritic branches to\nmodify the structure or \"morphology\" of the NNLD. A morphological learning\nalgorithm inspired by the 'Tempotron', i.e., a recently proposed temporal\nlearning algorithm-is presented in this work. Unlike 'Tempotron', the proposed\nlearning rule uses a technique to automatically adapt the NNLD threshold during\ntraining. Experimental results indicate that our NNLD with 1-bit synapses can\nobtain similar accuracy as a traditional Tempotron with 4-bit synapses in\nclassifying single spike random latency and pair-wise synchrony patterns.\nHence, the proposed method is better suited for robust hardware implementation\nin the presence of statistical variations. We also present results of applying\nthis rule to real life spike classification problems from the field of tactile\nsensing."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.05913v1", 
    "title": "Solving Problems with Unknown Solution Length at (Almost) No Extra Cost", 
    "arxiv-id": "1506.05913v1", 
    "author": "Timo K\u00f6tzing", 
    "publish": "2015-06-19T08:43:36Z", 
    "summary": "Most research in the theory of evolutionary computation assumes that the\nproblem at hand has a fixed problem size. This assumption does not always apply\nto real-world optimization challenges, where the length of an optimal solution\nmay be unknown a priori.\n  Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we\nanalyze variants of the (1+1) evolutionary algorithm for problems with unknown\nsolution length. For their setting, in which the solution length is sampled\nfrom a geometric distribution, we provide mutation rates that yield an expected\noptimization time that is of the same order as that of the (1+1) EA knowing the\nsolution length.\n  We then show that almost the same run times can be achieved even if \\emph{no}\na priori information on the solution length is available.\n  Finally, we provide mutation rates suitable for settings in which neither the\nsolution length nor the positions of the relevant bits are known. Again we\nobtain almost optimal run times for the \\textsc{OneMax} and\n\\textsc{LeadingOnes} test functions, thus solving an open problem from\nCathabard et al."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.05937v1", 
    "title": "A Tight Runtime Analysis of the $(1+(\u03bb, \u03bb))$ Genetic   Algorithm on OneMax", 
    "arxiv-id": "1506.05937v1", 
    "author": "Carola Doerr", 
    "publish": "2015-06-19T09:51:43Z", 
    "summary": "Understanding how crossover works is still one of the big challenges in\nevolutionary computation research, and making our understanding precise and\nproven by mathematical means might be an even bigger one. As one of few\nexamples where crossover provably is useful, the $(1+(\\lambda, \\lambda))$\nGenetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015].\nUsing the fitness level method, the expected optimization time on general\nOneMax functions was analyzed and a $O(\\max\\{n\\log(n)/\\lambda, \\lambda n\\})$\nbound was proven for any offspring population size $\\lambda \\in [1..n]$.\n  We improve this work in several ways, leading to sharper bounds and a better\nunderstanding of how the use of crossover speeds up the runtime in this\nalgorithm. We first improve the upper bound on the runtime to\n$O(\\max\\{n\\log(n)/\\lambda, n\\lambda \\log\\log(\\lambda)/\\log(\\lambda)\\})$. This\nimprovement is made possible from observing that in the parallel generation of\n$\\lambda$ offspring via crossover (but not mutation), the best of these often\nis better than the expected value, and hence several fitness levels can be\ngained in one iteration.\n  We then present the first lower bound for this problem. It matches our upper\nbound for all values of $\\lambda$. This allows to determine the asymptotically\noptimal value for the population size. It is $\\lambda =\n\\Theta(\\sqrt{\\log(n)\\log\\log(n)/\\log\\log\\log(n)})$, which gives an optimization\ntime of $\\Theta(n \\sqrt{\\log(n)\\log\\log\\log(n)/\\log\\log(n)})$. Hence the\nimproved runtime analysis gives a better runtime guarantee along with a better\nsuggestion for the parameter $\\lambda$.\n  We finally give a tail bound for the upper tail of the runtime distribution,\nwhich shows that the actual runtime exceeds our runtime guarantee by a factor\nof $(1+\\delta)$ with probability $O((n/\\lambda^2)^{-\\delta})$ only."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.06848v1", 
    "title": "A Feature-Based Analysis on the Impact of Set of Constraints for   e-Constrained Differential Evolution", 
    "arxiv-id": "1506.06848v1", 
    "author": "FranK Neumann", 
    "publish": "2015-06-23T03:24:05Z", 
    "summary": "Different types of evolutionary algorithms have been developed for\nconstrained continuous optimization. We carry out a feature-based analysis of\nevolved constrained continuous optimization instances to understand the\ncharacteristics of constraints that make problems hard for evolutionary\nalgorithm. In our study, we examine how various sets of constraints can\ninfluence the behaviour of e-Constrained Differential Evolution. Investigating\nthe evolved instances, we obtain knowledge of what type of constraints and\ntheir features make a problem difficult for the examined algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.07545v1", 
    "title": "Learning Representations from Deep Networks Using Mode Synthesizers", 
    "arxiv-id": "1506.07545v1", 
    "author": "P. Enyindah", 
    "publish": "2015-06-24T20:22:40Z", 
    "summary": "Deep learning Networks play a crucial role in the evolution of a vast number\nof current machine learning models for solving a variety of real world\nnon-trivial tasks. Such networks use big data which is generally unlabeled\nunsupervised and multi-layered requiring no form of supervision for training\nand learning data and has been used to successfully build automatic supervisory\nneural networks. However the question still remains how well the learned data\nrepresents interestingness, and their effectiveness i.e. efficiency in deep\nlearning models or applications. If the output of a network of deep learning\nmodels can be beamed unto a scene of observables, we could learn the\nvariational frequencies of these stacked networks in a parallel and\ndistributive way.This paper seeks to discover and represent interesting\npatterns in an efficient and less complex way by incorporating the concept of\nMode synthesizers in the deep learning process models"
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1506.08004v1", 
    "title": "ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for   Continuous Variables", 
    "arxiv-id": "1506.08004v1", 
    "author": "Jayanta Basak", 
    "publish": "2015-06-26T09:13:57Z", 
    "summary": "Stochastic optimization is an important task in many optimization problems\nwhere the tasks are not expressible as convex optimization problems. In the\ncase of non-convex optimization problems, various different stochastic\nalgorithms like simulated annealing, evolutionary algorithms, and tabu search\nare available. Most of these algorithms require user-defined parameters\nspecific to the problem in order to find out the optimal solution. Moreover, in\nmany situations, iterative fine-tunings are required for the user-defined\nparameters, and therefore these algorithms cannot adapt if the search space and\nthe optima changes over time. In this paper we propose an \\underline{a}daptive\nparameter-free \\underline{s}tochastic \\underline{o}ptimization technique for\n\\underline{c}ontinuous random variables called ASOC."
},{
    "category": "cs.NE", 
    "doi": "10.1109/JPROC.2015.2444094", 
    "link": "http://arxiv.org/pdf/1507.00088v1", 
    "title": "Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded   Representation", 
    "arxiv-id": "1507.00088v1", 
    "author": "Robert Sabourin", 
    "publish": "2015-07-01T01:54:43Z", 
    "summary": "Numerous genotypic diversity measures (GDMs) are available in the literature\nto assess the convergence status of an evolutionary algorithm (EA) or describe\nits search behavior. In a recent study, the authors of this paper drew\nattention to the need for a GDM validation framework. In response, this study\nproposes three requirements (monotonicity in individual varieties, twinning,\nand monotonicity in distance) that can clearly portray any GDMs. These\ndiversity requirements are analysed by means of controlled population\narrangements. In this paper four GDMs are evaluated with the proposed\nvalidation framework. The results confirm that properly evaluating population\ndiversity is a rather difficult task, as none of the analysed GDMs complies\nwith all the diversity requirements."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.01687v1", 
    "title": "Developing Postfix-GP Framework for Symbolic Regression Problems", 
    "arxiv-id": "1507.01687v1", 
    "author": "Sanjay Chaudhary", 
    "publish": "2015-07-07T06:54:47Z", 
    "summary": "This paper describes Postfix-GP system, postfix notation based Genetic\nProgramming (GP), for solving symbolic regression problems. It presents an\nobject-oriented architecture of Postfix-GP framework. It assists the user in\nunderstanding of the implementation details of various components of\nPostfix-GP. Postfix-GP provides graphical user interface which allows user to\nconfigure the experiment, to visualize evolved solutions, to analyze GP run,\nand to perform out-of-sample predictions. The use of Postfix-GP is demonstrated\nby solving the benchmark symbolic regression problem. Finally, features of\nPostfix-GP framework are compared with that of other GP systems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.01889v1", 
    "title": "Design of OFDM radar pulses using genetic algorithm based techniques", 
    "arxiv-id": "1507.01889v1", 
    "author": "Michael Inggs", 
    "publish": "2015-06-19T13:51:21Z", 
    "summary": "The merit of evolutionary algorithms (EA) to solve convex optimization\nproblems is widely acknowledged. In this paper, a genetic algorithm (GA)\noptimization based waveform design framework is used to improve the features of\nradar pulses relying on the orthogonal frequency division multiplexing (OFDM)\nstructure. Our optimization techniques focus on finding optimal phase code\nsequences for the OFDM signal. Several optimality criteria are used since we\nconsider two different radar processing solutions which call either for single\nor multiple-objective optimizations. When minimization of the so-called\npeak-to-mean envelope power ratio (PMEPR) single-objective is tackled, we\ncompare our findings with existing methods and emphasize on the merit of our\napproach. In the scope of the two-objective optimization, we first address\nPMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach based\non the non-dominated sorting genetic algorithm-II (NSGA-II) provides design\nsolutions with noticeable improvements as opposed to random sets of phase\ncodes. We then look at another case of interest where the objective functions\nare two measures of the sidelobe level, namely PSLR and the integrated-sidelobe\nlevel ratio (ISLR) and propose to modify the NSGA-II to include a constrain on\nthe PMEPR instead. In the last part, we illustrate via a case study how our\nencoding solution makes it possible to minimize the single objective PMEPR\nwhile enabling a target detection enhancement strategy, when the SNR metric\nwould be chosen for the detection framework."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.02491v1", 
    "title": "Parameter Sensitivity Analysis of Social Spider Algorithm", 
    "arxiv-id": "1507.02491v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-07-09T13:10:38Z", 
    "summary": "Social Spider Algorithm (SSA) is a recently proposed general-purpose\nreal-parameter metaheuristic designed to solve global numerical optimization\nproblems. This work systematically benchmarks SSA on a suite of 11 functions\nwith different control parameters. We conduct parameter sensitivity analysis of\nSSA using advanced non-parametric statistical tests to generate statistically\nsignificant conclusion on the best performing parameter settings. The\nconclusion can be adopted in future work to reduce the effort in parameter\ntuning. In addition, we perform a success rate test to reveal the impact of the\ncontrol parameters on the convergence speed of the algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.02492v1", 
    "title": "Adaptive Chemical Reaction Optimization for Global Numerical   Optimization", 
    "arxiv-id": "1507.02492v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-07-09T13:11:13Z", 
    "summary": "A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction\nOptimization (CRO), has been applied to many optimization problems in both\ndiscrete and continuous domains. To alleviate the effort in tuning parameters,\nthis paper reduces the number of optimization parameters in canonical CRO and\ndevelops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)\nadapts better to different optimization problems. We perform simulations with\nACRO on a widely-used benchmark of continuous problems. The simulation results\nshow that ACRO has superior performance over canonical CRO."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.02835v1", 
    "title": "A Trainable Neuromorphic Integrated Circuit that Exploits Device   Mismatch", 
    "arxiv-id": "1507.02835v1", 
    "author": "Andre van Schaik", 
    "publish": "2015-07-10T10:29:01Z", 
    "summary": "Random device mismatch that arises as a result of scaling of the CMOS\n(complementary metal-oxide semi-conductor) technology into the deep submicron\nregime degrades the accuracy of analogue circuits. Methods to combat this\nincrease the complexity of design. We have developed a novel neuromorphic\nsystem called a Trainable Analogue Block (TAB), which exploits device mismatch\nas a means for random projections of the input to a higher dimensional space.\nThe TAB framework is inspired by the principles of neural population coding\noperating in the biological nervous system. Three neuronal layers, namely\ninput, hidden, and output, constitute the TAB framework, with the number of\nhidden layer neurons far exceeding the input layer neurons. Here, we present\nmeasurement results of the first prototype TAB chip built using a 65nm process\ntechnology and show its learning capability for various regression tasks. Our\nTAB chip exploits inherent randomness and variability arising due to the\nfabrication process to perform various learning tasks. Additionally, we\ncharacterise each neuron and discuss the statistical variability of its tuning\ncurve that arises due to random device mismatch, a desirable property for the\nlearning capability of the TAB. We also discuss the effect of the number of\nhidden neurons and the resolution of output weights on the accuracy of the\nlearning capability of the TAB."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.05695v1", 
    "title": "A neuromorphic hardware architecture using the Neural Engineering   Framework for pattern recognition", 
    "arxiv-id": "1507.05695v1", 
    "author": "Andre van Schaik", 
    "publish": "2015-07-21T03:48:04Z", 
    "summary": "We present a hardware architecture that uses the Neural Engineering Framework\n(NEF) to implement large-scale neural networks on Field Programmable Gate\nArrays (FPGAs) for performing pattern recognition in real time. NEF is a\nframework that is capable of synthesising large-scale cognitive systems from\nsubnetworks. We will first present the architecture of the proposed neural\nnetwork implemented using fixed-point numbers and demonstrate a routine that\ncomputes the decoding weights by using the online pseudoinverse update method\n(OPIUM) in a parallel and distributed manner. The proposed system is\nefficiently implemented on a compact digital neural core. This neural core\nconsists of 64 neurons that are instantiated by a single physical neuron using\na time-multiplexing approach. As a proof of concept, we combined 128 identical\nneural cores together to build a handwritten digit recognition system using the\nMNIST database and achieved a recognition rate of 96.55%. The system is\nimplemented on a state-of-the-art FPGA and can process 5.12 million digits per\nsecond. The architecture is not limited to handwriting recognition, but is\ngenerally applicable as an extremely fast pattern recognition processor for\nvarious kinds of patterns such as speech and images."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ACCT.2015.114", 
    "link": "http://arxiv.org/pdf/1507.06222v1", 
    "title": "STICK: Spike Time Interval Computational Kernel, A Framework for General   Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony", 
    "arxiv-id": "1507.06222v1", 
    "author": "Ryad Benosman", 
    "publish": "2015-07-22T15:09:07Z", 
    "summary": "There has been significant research over the past two decades in developing\nnew platforms for spiking neural computation. Current neural computers are\nprimarily developed to mimick biology. They use neural networks which can be\ntrained to perform specific tasks to mainly solve pattern recognition problems.\nThese machines can do more than simulate biology, they allow us to re-think our\ncurrent paradigm of computation. The ultimate goal is to develop brain inspired\ngeneral purpose computation architectures that can breach the current\nbottleneck introduced by the Von Neumann architecture. This work proposes a new\nframework for such a machine. We show that the use of neuron like units with\nprecise timing representation, synaptic diversity, and temporal delays allows\nus to set a complete, scalable compact computation framework. The presented\nframework provides both linear and non linear operations, allowing us to\nrepresent and solve any function. We show usability in solving real use cases\nfrom simple differential equations to sets of non-linear differential equations\nleading to chaotic attractors."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2821650.2821672", 
    "link": "http://arxiv.org/pdf/1507.06594v3", 
    "title": "Neural NILM: Deep Neural Networks Applied to Energy Disaggregation", 
    "arxiv-id": "1507.06594v3", 
    "author": "William Knottenbelt", 
    "publish": "2015-07-23T18:18:49Z", 
    "summary": "Energy disaggregation estimates appliance-by-appliance electricity\nconsumption from a single meter that measures the whole home's electricity\ndemand. Recently, deep neural networks have driven remarkable improvements in\nclassification performance in neighbouring machine learning fields such as\nimage classification and automatic speech recognition. In this paper, we adapt\nthree deep neural network architectures to energy disaggregation: 1) a form of\nrecurrent neural network called `long short-term memory' (LSTM); 2) denoising\nautoencoders; and 3) a network which regresses the start time, end time and\naverage power demand of each appliance activation. We use seven metrics to test\nthe performance of these algorithms on real aggregate power data from five\nappliances. Tests are performed against a house not seen during training and\nagainst houses seen during training. We find that all three neural nets achieve\nbetter F1 scores (averaged over all five appliances) than either combinatorial\noptimisation or factorial hidden Markov models and that our neural net\nalgorithms generalise well to an unseen house."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2821650.2821672", 
    "link": "http://arxiv.org/pdf/1507.06877v1", 
    "title": "Multi-objective analysis of computational models", 
    "arxiv-id": "1507.06877v1", 
    "author": "Jo\u00ebl Chaskalovic", 
    "publish": "2015-07-24T15:16:19Z", 
    "summary": "Computational models are of increasing complexity and their behavior may in\nparticular emerge from the interaction of different parts. Studying such models\nbecomes then more and more difficult and there is a need for methods and tools\nsupporting this process. Multi-objective evolutionary algorithms generate a set\nof trade-off solutions instead of a single optimal solution. The availability\nof a set of solutions that have the specificity to be optimal relative to\ncarefully chosen objectives allows to perform data mining in order to better\nunderstand model features and regularities. We review the corresponding work,\npropose a unifying framework, and highlight its potential use. Typical\nquestions that such a methodology allows to address are the following: what are\nthe most critical parameters of the model? What are the relations between the\nparameters and the objectives? What are the typical behaviors of the model? Two\nexamples are provided to illustrate the capabilities of the methodology. The\nfeatures of a flapping-wing robot are thus evaluated to find out its\nspeed-energy relation, together with the criticality of its parameters. A\nneurocomputational model of the Basal Ganglia brain nuclei is then considered\nand its most salient features according to this methodology are presented and\ndiscussed."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2821650.2821672", 
    "link": "http://arxiv.org/pdf/1507.07200v1", 
    "title": "A Neural Prototype for a Virtual Chemical Spectrophotometer", 
    "arxiv-id": "1507.07200v1", 
    "author": "Elmer Rico E. Mojica", 
    "publish": "2015-07-26T14:13:29Z", 
    "summary": "A virtual chemical spectrophotometer for the simultaneous analysis of nickel\n(Ni) and cobalt (Co) was developed based on an artificial neural network (ANN).\nThe developed ANN correlates the respective concentrations of Co and Ni given\nthe absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtual\nchemical spectrometer was trained using a 3-layer jump connection neural\nnetwork model (NNM) with 126 input nodes corresponding to the 126 absorbance\nreadings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logistic\nactivation function, and 2 nodes in the output layer with a logistic function.\nTest result shows that the NNM has correlation coefficients of 0.9953 and\n0.9922 when predicting [Co] and [Ni], respectively. We observed, however, that\nthe NNM has a duality property and that there exists a real-world practical\napplication in solving the dual problem: Predict the Co-Ni mixture's absorbance\nprofile given [Co] and [Ni]. It turns out that the dual problem is much harder\nto solve because the intended output has a much bigger cardinality than that of\nthe input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2\ninput nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in the\nhidden layer, and 126 output nodes corresponding to the 126 absorbance readings\nfrom 250 nm to 600 nm. Test result shows that the dual NNM has correlation\ncoefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nm\nwith the maximum coefficient observed at 480 nm. This means that the dual ANN\ncan be used to predict the absorbance profile given the respective Co-Ni\nconcentrations which can be of importance in creating academic models for a\nvirtual chemical spectrophotometer."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1507.07301v1", 
    "title": "A Social Spider Algorithm for Solving the Non-convex Economic Load   Dispatch Problem", 
    "arxiv-id": "1507.07301v1", 
    "author": "Victor O. K. Li", 
    "publish": "2015-07-27T05:30:15Z", 
    "summary": "Economic Load Dispatch (ELD) is one of the essential components in power\nsystem control and operation. Although conventional ELD formulation can be\nsolved using mathematical programming techniques, modern power system\nintroduces new models of the power units which are non-convex,\nnon-differentiable, and sometimes non-continuous. In order to solve such\nnon-convex ELD problems, in this paper we propose a new approach based on the\nSocial Spider Algorithm (SSA). The classical SSA is modified and enhanced to\nadapt to the unique characteristics of ELD problems, e.g., valve-point effects,\nmulti-fuel operations, prohibited operating zones, and line losses. To\ndemonstrate the superiority of our proposed approach, five widely-adopted test\nsystems are employed and the simulation results are compared with the\nstate-of-the-art algorithms. In addition, the parameter sensitivity is\nillustrated by a series of simulations. The simulation results show that SSA\ncan solve ELD problems effectively and efficiently."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1507.08007v2", 
    "title": "On Proportions of Fit Individuals in Population of Evolutionary   Algorithm with Tournament Selection", 
    "arxiv-id": "1507.08007v2", 
    "author": "Anton Eremeev", 
    "publish": "2015-07-29T02:24:55Z", 
    "summary": "In this paper, we consider a fitness-level model of a non-elitist\nmutation-only evolutionary algorithm (EA) with tournament selection. The model\nprovides upper and lower bounds for the expected proportion of the individuals\nwith fitness above given thresholds. In the case of so-called monotone\nmutation, the obtained bounds imply that increasing the tournament size\nimproves the EA performance. As corollaries, we obtain an exponentially\nvanishing tail bound for the Randomized Local Search on unimodal functions and\npolynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family\nof Set Cover problems proposed by E. Balas."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1508.00097v1", 
    "title": "The Interactive Effects of Operators and Parameters to GA Performance   Under Different Problem Sizes", 
    "arxiv-id": "1508.00097v1", 
    "author": "Elizer A. Albacea", 
    "publish": "2015-08-01T08:01:50Z", 
    "summary": "The complex effect of genetic algorithm's (GA) operators and parameters to\nits performance has been studied extensively by researchers in the past but\nnone studied their interactive effects while the GA is under different problem\nsizes. In this paper, We present the use of experimental model (1)~to\ninvestigate whether the genetic operators and their parameters interact to\naffect the offline performance of GA, (2)~to find what combination of genetic\noperators and parameter settings will provide the optimum performance for GA,\nand (3)~to investigate whether these operator-parameter combination is\ndependent on the problem size. We designed a GA to optimize a family of\ntraveling salesman problems (TSP), with their optimal solutions known for\nconvenient benchmarking. Our GA was set to use different algorithms in\nsimulating selection ($\\Omega_s$), different algorithms ($\\Omega_c$) and\nparameters ($p_c$) in simulating crossover, and different parameters ($p_m$) in\nsimulating mutation. We used several $n$-city TSPs ($n=\\{5, 7, 10, 100,\n1000\\}$) to represent the different problem sizes (i.e., size of the resulting\nsearch space as represented by GA schemata). Using analysis of variance of\n3-factor factorial experiments, we found out that GA performance is affected by\n$\\Omega_s$ at small problem size (5-city TSP) where the algorithm Partially\nMatched Crossover significantly outperforms Cycle Crossover at $95\\%$\nconfidence level."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1508.01008v1", 
    "title": "INsight: A Neuromorphic Computing System for Evaluation of Large Neural   Networks", 
    "arxiv-id": "1508.01008v1", 
    "author": "Yongshin Kang", 
    "publish": "2015-08-05T09:11:06Z", 
    "summary": "Deep neural networks have been demonstrated impressive results in various\ncognitive tasks such as object detection and image classification. In order to\nexecute large networks, Von Neumann computers store the large number of weight\nparameters in external memories, and processing elements are timed-shared,\nwhich leads to power-hungry I/O operations and processing bottlenecks. This\npaper describes a neuromorphic computing system that is designed from the\nground up for the energy-efficient evaluation of large-scale neural networks.\nThe computing system consists of a non-conventional compiler, a neuromorphic\narchitecture, and a space-efficient microarchitecture that leverages existing\nintegrated circuit design methodologies. The compiler factorizes a trained,\nfeedforward network into a sparsely connected network, compresses the weights\nlinearly, and generates a time delay neural network reducing the number of\nconnections. The connections and units in the simplified network are mapped to\nsilicon synapses and neurons. We demonstrate an implementation of the\nneuromorphic computing system based on a field-programmable gate array that\nperforms the MNIST hand-written digit classification with 97.64% accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1508.02774v1", 
    "title": "Benchmarking of LSTM Networks", 
    "arxiv-id": "1508.02774v1", 
    "author": "Thomas M. Breuel", 
    "publish": "2015-08-11T23:31:49Z", 
    "summary": "LSTM (Long Short-Term Memory) recurrent neural networks have been highly\nsuccessful in a number of application areas. This technical report describes\nthe use of the MNIST and UW3 databases for benchmarking LSTM networks and\nexplores the effect of different architectural and hyperparameter choices on\nperformance. Significant findings include: (1) LSTM performance depends\nsmoothly on learning rates, (2) batching and momentum has no significant effect\non performance, (3) softmax training outperforms least square training, (4)\npeephole units are not useful, (5) the standard non-linearities (tanh and\nsigmoid) perform best, (6) bidirectional training combined with CTC performs\nbetter than other methods."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2015.07.037", 
    "link": "http://arxiv.org/pdf/1508.05342v1", 
    "title": "Genetic Algorithms for multimodal optimization: a review", 
    "arxiv-id": "1508.05342v1", 
    "author": "Noe Casas", 
    "publish": "2015-06-10T15:29:25Z", 
    "summary": "In this article we provide a comprehensive review of the different\nevolutionary algorithm techniques used to address multimodal optimization\nproblems, classifying them according to the nature of their approach. On the\none hand there are algorithms that address the issue of the early convergence\nto a local optimum by differentiating the individuals of the population into\ngroups and limiting their interaction, hence having each group evolve with a\nhigh degree of independence. On the other hand other approaches are based on\ndirectly addressing the lack of genetic diversity of the population by\nintroducing elements into the evolutionary dynamics that promote new niches of\nthe genotypical space to be explored. Finally, we study multi-objective\noptimization genetic algorithms, that handle the situations where multiple\ncriteria have to be satisfied with no penalty for any of them. Very rich\nliterature has arised over the years on these topics, and we aim at offering an\noverview of the most important techniques of each branch of the field."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICDM.2015.65", 
    "link": "http://arxiv.org/pdf/1508.06483v1", 
    "title": "Population Synthesis via k-Nearest Neighbor Crossover Kernel", 
    "arxiv-id": "1508.06483v1", 
    "author": "Hideyuki Kikuchi", 
    "publish": "2015-08-26T13:22:37Z", 
    "summary": "The recent development of multi-agent simulations brings about a need for\npopulation synthesis. It is a task of reconstructing the entire population from\na sampling survey of limited size (1% or so), supplying the initial conditions\nfrom which simulations begin. This paper presents a new kernel density\nestimator for this task. Our method is an analogue of the classical\nBreiman-Meisel-Purcell estimator, but employs novel techniques that harness the\nhuge degree of freedom which is required to model high-dimensional nonlinearly\ncorrelated datasets: the crossover kernel, the k-nearest neighbor restriction\nof the kernel construction set and the bagging of kernels. The performance as a\nstatistical estimator is examined through real and synthetic datasets. We\nprovide an \"optimization-free\" parameter selection rule for our method, a\ntheory of how our method works and a computational cost analysis. To\ndemonstrate the usefulness as a population synthesizer, our method is applied\nto a household synthesis task for an urban micro-simulator."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICDM.2015.65", 
    "link": "http://arxiv.org/pdf/1508.07700v1", 
    "title": "A Cognitive Architecture Based on a Learning Classifier System with   Spiking Classifiers", 
    "arxiv-id": "1508.07700v1", 
    "author": "Pier-Luca Lanzi", 
    "publish": "2015-08-31T06:35:01Z", 
    "summary": "Learning Classifier Systems (LCS) are population-based reinforcement learners\nthat were originally designed to model various cognitive phenomena. This paper\npresents an explicitly cognitive LCS by using spiking neural networks as\nclassifiers, providing each classifier with a measure of temporal dynamism. We\nemploy a constructivist model of growth of both neurons and synaptic\nconnections, which permits a Genetic Algorithm (GA) to automatically evolve\nsufficiently-complex neural structures. The spiking classifiers are coupled\nwith a temporally-sensitive reinforcement learning algorithm, which allows the\nsystem to perform temporal state decomposition by appropriately rewarding\n\"macro-actions,\" created by chaining together multiple atomic actions. The\ncombination of temporal reinforcement learning and neural information\nprocessing is shown to outperform benchmark neural classifier systems, and\nsuccessfully solve a robotic navigation task."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICDM.2015.65", 
    "link": "http://arxiv.org/pdf/1509.00105v1", 
    "title": "Evolving Unipolar Memristor Spiking Neural Networks", 
    "arxiv-id": "1509.00105v1", 
    "author": "Ben De Lacy Costello", 
    "publish": "2015-09-01T01:00:57Z", 
    "summary": "Neuromorphic computing --- brainlike computing in hardware --- typically\nrequires myriad CMOS spiking neurons interconnected by a dense mesh of\nnanoscale plastic synapses. Memristors are frequently citepd as strong synapse\ncandidates due to their statefulness and potential for low-power\nimplementations. To date, plentiful research has focused on the bipolar\nmemristor synapse, which is capable of incremental weight alterations and can\nprovide adaptive self-organisation under a Hebbian learning scheme. In this\npaper we consider the Unipolar memristor synapse --- a device capable of\nnon-Hebbian switching between only two states (conductive and resistive)\nthrough application of a suitable input voltage --- and discuss its suitability\nfor neuromorphic systems. A self-adaptive evolutionary process is used to\nautonomously find highly fit network configurations. Experimentation on a two\nrobotics tasks shows that unipolar memristor networks evolve task-solving\ncontrollers faster than both bipolar memristor networks and networks containing\nconstant nonplastic connections whilst performing at least comparably."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2016.2537300", 
    "link": "http://arxiv.org/pdf/1509.00174v1", 
    "title": "A Telescopic Binary Learning Machine for Training Neural Networks", 
    "arxiv-id": "1509.00174v1", 
    "author": "Roberto Battiti", 
    "publish": "2015-09-01T08:22:33Z", 
    "summary": "This paper proposes a new algorithm based on multi-scale stochastic local\nsearch with binary representation for training neural networks.\n  In particular, we study the effects of neighborhood evaluation strategies,\nthe effect of the number of bits per weight and that of the maximum weight\nrange used for mapping binary strings to real values. Following this\npreliminary investigation, we propose a telescopic multi-scale version of local\nsearch where the number of bits is increased in an adaptive manner, leading to\na faster search and to local minima of better quality. An analysis related to\nadapting the number of bits in a dynamic way is also presented. The control on\nthe number of bits, which happens in a natural manner in the proposed method,\nis effective to increase the generalization performance. Benchmark tasks\ninclude a highly non-linear artificial problem, a control problem requiring\neither feed-forward or recurrent architectures for feedback control, and\nchallenging real-world tasks in different application domains.\n  The results demonstrate the effectiveness of the proposed method."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2016.2537300", 
    "link": "http://arxiv.org/pdf/1509.00962v1", 
    "title": "A compact aVLSI conductance-based silicon neuron", 
    "arxiv-id": "1509.00962v1", 
    "author": "Andre van Schaik", 
    "publish": "2015-09-03T07:22:38Z", 
    "summary": "We present an analogue Very Large Scale Integration (aVLSI) implementation\nthat uses first-order lowpass filters to implement a conductance-based silicon\nneuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma\n(cell body) and a single synapse, which is capable of linearly summing both the\nexcitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated by\nthe spikes arriving from different sources. Rather than biasing the silicon\nneuron with different parameters for different spiking patterns, as is\ntypically done, we provide digital control signals, generated by an FPGA, to\nthe silicon neuron to obtain different spiking behaviours. The proposed neuron\nis only ~26.5 um2 in the IBM 130nm process and thus can be integrated at very\nhigh density. Circuit simulations show that this neuron can emulate different\nspiking behaviours observed in biological neurons."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2016.2537300", 
    "link": "http://arxiv.org/pdf/1509.00967v1", 
    "title": "A Reconfigurable Mixed-signal Implementation of a Neuromorphic ADC", 
    "arxiv-id": "1509.00967v1", 
    "author": "Andre van Schaik", 
    "publish": "2015-09-03T07:51:24Z", 
    "summary": "We present a neuromorphic Analogue-to-Digital Converter (ADC), which uses\nintegrate-and-fire (I&F) neurons as the encoders of the analogue signal, with\nmodulated inhibitions to decohere the neuronal spikes trains. The architecture\nconsists of an analogue chip and a control module. The analogue chip comprises\ntwo scan chains and a twodimensional integrate-and-fire neuronal array.\nIndividual neurons are accessed via the chains one by one without any encoder\ndecoder or arbiter. The control module is implemented on an FPGA (Field\nProgrammable Gate Array), which sends scan enable signals to the scan chains\nand controls the inhibition for individual neurons. Since the control module is\nimplemented on an FPGA, it can be easily reconfigured. Additionally, we propose\na pulse width modulation methodology for the lateral inhibition, which makes\nuse of different pulse widths indicating different strengths of inhibition for\neach individual neuron to decohere neuronal spikes. Software simulations in\nthis paper tested the robustness of the proposed ADC architecture to fixed\nrandom noise. A circuit simulation using ten neurons shows the performance and\nthe feasibility of the architecture."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2016.2537300", 
    "link": "http://arxiv.org/pdf/1509.01126v1", 
    "title": "Training of CC4 Neural Network with Spread Unary Coding", 
    "arxiv-id": "1509.01126v1", 
    "author": "Pushpa Sree Potluri", 
    "publish": "2015-09-03T15:28:55Z", 
    "summary": "This paper adapts the corner classification algorithm (CC4) to train the\nneural networks using spread unary inputs. This is an important problem as\nspread unary appears to be at the basis of data representation in biological\nlearning. The modified CC4 algorithm is tested using the pattern classification\nexperiment and the results are found to be good. Specifically, we show that the\nnumber of misclassified points is not particularly sensitive to the chosen\nradius of generalization."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TNNLS.2016.2537300", 
    "link": "http://arxiv.org/pdf/1509.02807v1", 
    "title": "Transfer learning approach for financial applications", 
    "arxiv-id": "1509.02807v1", 
    "author": "Michael S. C. Thomas", 
    "publish": "2015-09-09T15:22:21Z", 
    "summary": "Artificial neural networks learn how to solve new problems through a\ncomputationally intense and time consuming process. One way to reduce the\namount of time required is to inject preexisting knowledge into the network. To\nmake use of past knowledge, we can take advantage of techniques that transfer\nthe knowledge learned from one task, and reuse it on another (sometimes\nunrelated) task. In this paper we propose a novel selective breeding technique\nthat extends the transfer learning with behavioural genetics approach proposed\nby Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial\ndata. Numerical evidence demonstrates the credibility of the new approach. We\nprovide insights on the operation of transfer learning and highlight the\nbenefits of using behavioural principles and selective breeding when tackling a\nset of diverse financial applications problems."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2016.03.003", 
    "link": "http://arxiv.org/pdf/1509.04438v2", 
    "title": "Regular expressions for decoding of neural network outputs", 
    "arxiv-id": "1509.04438v2", 
    "author": "Roger Labahn", 
    "publish": "2015-09-15T08:24:37Z", 
    "summary": "This article proposes a convenient tool for decoding the output of neural\nnetworks trained by Connectionist Temporal Classification (CTC) for handwritten\ntext recognition. We use regular expressions to describe the complex structures\nexpected in the writing. The corresponding finite automata are employed to\nbuild a decoder. We analyze theoretically which calculations are relevant and\nwhich can be avoided. A great speed-up results from an approximation. We\nconclude that the approximation most likely fails if the regular expression\ndoes not match the ground truth which is not harmful for many applications\nsince the low probability will be even underestimated. The proposed decoder is\nvery efficient compared to other decoding methods. The variety of applications\nreaches from information retrieval to full text recognition. We refer to\napplications where we integrated the proposed decoder successfully."
},{
    "category": "cs.NE", 
    "doi": "10.5120/ijca2015907021", 
    "link": "http://arxiv.org/pdf/1509.05177v4", 
    "title": "Some Theorems for Feed Forward Neural Networks", 
    "arxiv-id": "1509.05177v4", 
    "author": "Vishwajeet Singh", 
    "publish": "2015-09-17T09:17:59Z", 
    "summary": "In this paper we introduce a new method which employs the concept of\n\"Orientation Vectors\" to train a feed forward neural network and suitable for\nproblems where large dimensions are involved and the clusters are\ncharacteristically sparse. The new method is not NP hard as the problem size\nincreases. We `derive' the method by starting from Kolmogrov's method and then\nrelax some of the stringent conditions. We show for most classification\nproblems three layers are sufficient and the network size depends on the number\nof clusters. We prove as the number of clusters increase from N to N+dN the\nnumber of processing elements in the first layer only increases by d(logN), and\nare proportional to the number of classes, and the method is not NP hard.\n  Many examples are solved to demonstrate that the method of Orientation\nVectors requires much less computational effort than Radial Basis Function\nmethods and other techniques wherein distance computations are required, in\nfact the present method increases logarithmically with problem size compared to\nthe Radial Basis Function method and the other methods which depend on distance\ncomputations e.g statistical methods where probabilistic distances are\ncalculated. A practical method of applying the concept of Occum's razor to\nchoose between two architectures which solve the same classification problem\nhas been described. The ramifications of the above findings on the field of\nDeep Learning have also been briefly investigated and we have found that it\ndirectly leads to the existence of certain types of NN architectures which can\nbe used as a \"mapping engine\", which has the property of \"invertibility\", thus\nimproving the prospect of their deployment for solving problems involving Deep\nLearning and hierarchical classification. The latter possibility has a lot of\nfuture scope in the areas of machine learning and cloud computing."
},{
    "category": "cs.NE", 
    "doi": "10.5120/ijca2015907021", 
    "link": "http://arxiv.org/pdf/1509.06535v2", 
    "title": "Deep Boltzmann Machines in Estimation of Distribution Algorithms for   Combinatorial Optimization", 
    "arxiv-id": "1509.06535v2", 
    "author": "Franz Rothlauf", 
    "publish": "2015-09-22T10:03:43Z", 
    "summary": "Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Deep Boltzmann Machines\n(DBMs) are generative neural networks with these desired properties. We\nintegrate a DBM into an EDA and evaluate the performance of this system in\nsolving combinatorial optimization problems with a single objective. We compare\nthe results to the Bayesian Optimization Algorithm. The performance of DBM-EDA\nwas superior to BOA for difficult additively decomposable functions, i.e.,\nconcatenated deceptive traps of higher order. For most other benchmark\nproblems, DBM-EDA cannot clearly outperform BOA, or other neural network-based\nEDAs. In particular, it often yields optimal solutions for a subset of the runs\n(with fewer evaluations than BOA), but is unable to provide reliable\nconvergence to the global optimum competitively. At the same time, the model\nbuilding process is computationally more expensive than that of other EDAs\nusing probabilistic models from the neural network family, such as DAE-EDA."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijfcst.2015.5503", 
    "link": "http://arxiv.org/pdf/1509.08302v1", 
    "title": "A hybrid COA$\u03b5$-constraint method for solving multi-objective   problems", 
    "arxiv-id": "1509.08302v1", 
    "author": "Niloofar Jahani", 
    "publish": "2015-09-25T11:26:39Z", 
    "summary": "In this paper, a hybrid method for solving multi-objective problem has been\nprovided. The proposed method is combining the {\\epsilon}-Constraint and the\nCuckoo algorithm. First the multi objective problem transfers into a\nsingle-objective problem using $\\epsilon$-Constraint, then the Cuckoo\noptimization algorithm will optimize the problem in each task. At last the\noptimized Pareto frontier will be drawn. The advantage of this method is the\nhigh accuracy and the dispersion of its Pareto frontier. In order to testing\nthe efficiency of the suggested method, a lot of test problems have been solved\nusing this method. Comparing the results of this method with the results of\nother similar methods shows that the Cuckoo algorithm is more suitable for\nsolving the multi-objective problems."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijfcst.2015.5503", 
    "link": "http://arxiv.org/pdf/1509.09060v2", 
    "title": "Multi-objective Differential Evolution with Helper Functions for   Constrained Optimization", 
    "arxiv-id": "1509.09060v2", 
    "author": "Jun He", 
    "publish": "2015-09-30T08:19:04Z", 
    "summary": "Solving constrained optimization problems by multi-objective evolutionary\nalgorithms has scored tremendous achievements in the last decade. Standard\nmulti-objective schemes usually aim at minimizing the objective function and\nalso the degree of constraint violation simultaneously. This paper proposes a\nnew multi-objective method for solving constrained optimization problems. The\nnew method keeps two standard objectives: the original objective function and\nthe sum of degrees of constraint violation. But besides them, four more\nobjectives are added. One is based on the feasible rule. The other three come\nfrom the penalty functions. This paper conducts an initial experimental study\non thirteen benchmark functions. A simplified version of CMODE is applied to\nsolving multi-objective optimization problems. Our initial experimental results\nconfirm our expectation that adding more helper functions could be useful. The\nperformance of SMODE with more helper functions (four or six) is better than\nthat with only two helper functions."
},{
    "category": "cs.NE", 
    "doi": "10.5121/ijfcst.2015.5503", 
    "link": "http://arxiv.org/pdf/1509.09235v2", 
    "title": "Generative Adversarial Networks in Estimation of Distribution Algorithms   for Combinatorial Optimization", 
    "arxiv-id": "1509.09235v2", 
    "author": "Malte Probst", 
    "publish": "2015-09-30T16:02:59Z", 
    "summary": "Estimation of Distribution Algorithms (EDAs) require flexible probability\nmodels that can be efficiently learned and sampled. Generative Adversarial\nNetworks (GAN) are generative neural networks which can be trained to\nimplicitly model the probability distribution of given data, and it is possible\nto sample this distribution. We integrate a GAN into an EDA and evaluate the\nperformance of this system when solving combinatorial optimization problems\nwith a single objective. We use several standard benchmark problems and compare\nthe results to state-of-the-art multivariate EDAs. GAN-EDA doe not yield\ncompetitive results - the GAN lacks the ability to quickly learn a good\napproximation of the probability distribution. A key reason seems to be the\nlarge amount of noise present in the first EDA generations."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICMLA.2015.97", 
    "link": "http://arxiv.org/pdf/1510.00419v1", 
    "title": "An Asynchronous Implementation of the Limited Memory CMA-ES", 
    "arxiv-id": "1510.00419v1", 
    "author": "Anatoly Shalyto", 
    "publish": "2015-10-01T20:34:37Z", 
    "summary": "We present our asynchronous implementation of the LM-CMA-ES algorithm, which\nis a modern evolution strategy for solving complex large-scale continuous\noptimization problems. Our implementation brings the best results when the\nnumber of cores is relatively high and the computational complexity of the\nfitness function is also high. The experiments with benchmark functions show\nthat it is able to overcome its origin on the Sphere function, reaches certain\nthresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly\nperforms much better than the original version on the Rastrigin function."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.02513v2", 
    "title": "A novel mutation operator based on the union of fitness and design   spaces information for Differential Evolution", 
    "arxiv-id": "1510.02513v2", 
    "author": "K. Shojaei", 
    "publish": "2015-10-08T21:17:37Z", 
    "summary": "Differential Evolution (DE) is one of the most successful and powerful\nevolutionary algorithms for global optimization problem. The most important\noperator in this algorithm is mutation operator which parents are selected\nrandomly to participate in it. Recently, numerous papers are tried to make this\noperator more intelligent by selection of parents for mutation intelligently.\nThe intelligent selection for mutation vectors is performed by applying design\nspace (also known as decision space) criterion or fitness space criterion,\nhowever, in both cases, half of valuable information of the problem space is\ndisregarded. In this article, a Universal Differential Evolution (UDE) is\nproposed which takes advantage of both design and fitness spaces criteria for\nintelligent selection of mutation vectors. The experimental analysis on UDE are\nperformed on CEC2005 benchmarks and the results stated that UDE significantly\nimproved the performance of differential evolution in comparison with other\nmethods that only use one criterion for intelligent selection."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.02516v2", 
    "title": "Differential Evolution with Generalized Mutation Operator for Parameters   Optimization in Gene Selection for Cancer Classification", 
    "arxiv-id": "1510.02516v2", 
    "author": "K. Shojaei", 
    "publish": "2015-10-08T21:21:40Z", 
    "summary": "Differential Evolution (DE) proved to be one of the most successful\nevolutionary algorithms for global optimization purposes in continuous\nproblems. The core operator in DE is mutation which can provide the algorithm\nwith both exploration and exploitation. In this article, a new notation for DE\nis proposed which has a formula that can be utilized for generating and\nextracting novel mutations and by applying this new notation, four novel\nmutations are proposed. More importantly, by combining these novel trial vector\ngeneration strategies and four other well-known ones, we proposed Generalized\nMutation Differential Evolution (GMDE) that takes advantage of two mutation\npools that have both explorative and exploitative strategies inside them.\nResults and experimental analysis are performed on CEC2005 benchmarks and the\nresults stated that GMDE is surprisingly competitive and significantly improved\nthe performance of this algorithm. Finally, GMDE is also applied to parameters\noptimization, modification and improvement of a feature selection method for\ncancer classification purposes over gene expression microarray profiles."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.03891v1", 
    "title": "Nonlinear memory capacity of parallel time-delay reservoir computers in   the processing of multidimensional signals", 
    "arxiv-id": "1510.03891v1", 
    "author": "Juan-Pablo Ortega", 
    "publish": "2015-10-13T20:56:49Z", 
    "summary": "This paper addresses the reservoir design problem in the context of\ndelay-based reservoir computers for multidimensional input signals, parallel\narchitectures, and real-time multitasking. First, an approximating reservoir\nmodel is presented in those frameworks that provides an explicit functional\nlink between the reservoir parameters and architecture and its performance in\nthe execution of a specific task. Second, the inference properties of the ridge\nregression estimator in the multivariate context is used to assess the impact\nof finite sample training on the decrease of the reservoir capacity. Finally,\nan empirical study is conducted that shows the adequacy of the theoretical\nresults with the empirical performances exhibited by various reservoir\narchitectures in the execution of several nonlinear tasks with multidimensional\ninputs. Our results confirm the robustness properties of the parallel reservoir\narchitecture with respect to task misspecification and parameter choice that\nhad already been documented in the literature."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.05328v5", 
    "title": "Exploring the Space of Adversarial Images", 
    "arxiv-id": "1510.05328v5", 
    "author": "Eduardo Valle", 
    "publish": "2015-10-19T00:54:37Z", 
    "summary": "Adversarial examples have raised questions regarding the robustness and\nsecurity of deep neural networks. In this work we formalize the problem of\nadversarial images given a pretrained classifier, showing that even in the\nlinear case the resulting optimization problem is nonconvex. We generate\nadversarial images using shallow and deep classifiers on the MNIST and ImageNet\ndatasets. We probe the pixel space of adversarial images using noise of varying\nintensity and distribution. We bring novel visualizations that showcase the\nphenomenon and its high variability. We show that adversarial images appear in\nlarge regions in the pixel space, but that, for the same task, a shallow\nclassifier seems more robust to adversarial images than a deep convolutional\nnetwork."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.07163v1", 
    "title": "Evolutionary Landscape and Management of Population Diversity", 
    "arxiv-id": "1510.07163v1", 
    "author": "Maumita Bhattacharya", 
    "publish": "2015-10-24T16:56:11Z", 
    "summary": "The search ability of an Evolutionary Algorithm (EA) depends on the variation\namong the individuals in the population [3, 4, 8]. Maintaining an optimal level\nof diversity in the EA population is imperative to ensure that progress of the\nEA search is unhindered by premature convergence to suboptimal solutions.\nClearer understanding of the concept of population diversity, in the context of\nevolutionary search and premature convergence in particular, is the key to\ndesigning efficient EAs. To this end, this paper first presents a brief\nanalysis of the EA population diversity issues. Next we present an\ninvestigation on a counter-niching EA technique [4] that introduces and\nmaintains constructive diversity in the population. The proposed approach uses\ninformed genetic operations to reach promising, but unexplored or\nunder-explored areas of the search space, while discouraging premature local\nconvergence. Simulation runs on a suite of standard benchmark test functions\nwith Genetic Algorithm (GA) implementation shows promising results."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1510.07957v1", 
    "title": "Increasing Behavioral Complexity for Evolved Virtual Creatures with the   ESP Method", 
    "arxiv-id": "1510.07957v1", 
    "author": "Sebastian Risi", 
    "publish": "2015-10-27T16:16:22Z", 
    "summary": "Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)\nhave employed the coevolution of morphology and control to produce high-impact\nwork in multiple fields, including graphics, evolutionary computation,\nrobotics, and artificial life. However, in contrast to fixed-morphology\ncreatures, there has been no clear increase in the behavioral complexity of\nEVCs in those two decades. This paper describes a method for moving beyond this\nlimit, making use of high-level human input in the form of a syllabus of\nintermediate learning tasks--along with mechanisms for preservation, reuse, and\ncombination of previously learned tasks. This method--named ESP for its three\ncomponents: encapsulation, syllabus, and pandemonium--is presented in two\ncomplementary versions: Fast ESP, which constrains later morphological changes\nto achieve linear growth in computation time as behavioral complexity is added,\nand General ESP, which allows this restriction to be removed when sufficient\ncomputational resources are available. Experiments demonstrate that the ESP\nmethod allows evolved virtual creatures to reach new levels of behavioral\ncomplexity in the co-evolution of morphology and control, approximately\ndoubling the previous state of the art."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1511.00540v2", 
    "title": "Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem   Solvers", 
    "arxiv-id": "1511.00540v2", 
    "author": "Michael Pfeiffer", 
    "publish": "2015-11-02T15:16:43Z", 
    "summary": "Solving constraint satisfaction problems (CSPs) is a notoriously expensive\ncomputational task. Recently, it has been proposed that efficient stochastic\nsolvers can be obtained through appropriately configured spiking neural\nnetworks performing Markov Chain Monte Carlo (MCMC) sampling. The possibility\nto run such models on massively parallel, low-power neuromorphic hardware holds\ngreat promise; however, previously proposed networks are based on\nprobabilistically spiking neurons, and thus rely on random number generators or\nexternal noise sources to achieve the necessary stochasticity, leading to\nsignificant overhead in the implementation. Here we show how stochasticity can\nbe achieved by implementing deterministic models of integrate and fire neurons\nusing subthreshold analog circuits that are affected by thermal noise. We\npresent an efficient implementation of spike-based CSP solvers using a\nreconfigurable neural network VLSI device, and the device's intrinsic noise as\na source of randomness. To illustrate the overall concept, we implement a\ngeneric Sudoku solver based on our approach and demonstrate its operation. We\nestablish a link between the neuron parameters and the system dynamics,\nallowing for a simple temperature control mechanism."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-016-2359-8", 
    "link": "http://arxiv.org/pdf/1511.01427v1", 
    "title": "Turing Computation with Recurrent Artificial Neural Networks", 
    "arxiv-id": "1511.01427v1", 
    "author": "Serafim Rodrigues", 
    "publish": "2015-11-04T18:40:46Z", 
    "summary": "We improve the results by Siegelmann & Sontag (1995) by providing a novel and\nparsimonious constructive mapping between Turing Machines and Recurrent\nArtificial Neural Networks, based on recent developments of Nonlinear Dynamical\nAutomata. The architecture of the resulting R-ANNs is simple and elegant,\nstemming from its transparent relation with the underlying NDAs. These\ncharacteristics yield promise for developments in machine learning methods and\nsymbolic computation with continuous time dynamical systems. A framework is\nprovided to directly program the R-ANNs from Turing Machine descriptions, in\nabsence of network training. At the same time, the network can potentially be\ntrained to perform algorithmic tasks, with exciting possibilities in the\nintegration of approaches akin to Google DeepMind's Neural Turing Machines."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2016.7744345", 
    "link": "http://arxiv.org/pdf/1511.03483v3", 
    "title": "An Analytic Expression of Relative Approximation Error for a Class of   Evolutionary Algorithms", 
    "arxiv-id": "1511.03483v3", 
    "author": "Jun He", 
    "publish": "2015-11-11T13:02:36Z", 
    "summary": "An important question in evolutionary computation is how good solutions\nevolutionary algorithms can produce. This paper aims to provide an analytic\nanalysis of solution quality in terms of the relative approximation error,\nwhich is defined by the error between 1 and the approximation ratio of the\nsolution found by an evolutionary algorithm. Since evolutionary algorithms are\niterative methods, the relative approximation error is a function of\ngenerations. With the help of matrix analysis, it is possible to obtain an\nexact expression of such a function. In this paper, an analytic expression for\ncalculating the relative approximation error is presented for a class of\nevolutionary algorithms, that is, (1+1) strictly elitist evolution algorithms.\nFurthermore, analytic expressions of the fitness value and the average\nconvergence rate in each generation are also derived for this class of\nevolutionary algorithms. The approach is promising, and it can be extended to\nnon-elitist or population-based algorithms too."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.04484v2", 
    "title": "Stochastic Synapses Enable Efficient Brain-Inspired Learning Machines", 
    "arxiv-id": "1511.04484v2", 
    "author": "Gert Cauwenberghs", 
    "publish": "2015-11-14T00:27:37Z", 
    "summary": "Recent studies have shown that synaptic unreliability is a robust and\nsufficient mechanism for inducing the stochasticity observed in cortex. Here,\nwe introduce Synaptic Sampling Machines, a class of neural network models that\nuses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised\nlearning. Similar to the original formulation of Boltzmann machines, these\nmodels can be viewed as a stochastic counterpart of Hopfield networks, but\nwhere stochasticity is induced by a random mask over the connections. Synaptic\nstochasticity plays the dual role of an efficient mechanism for sampling, and a\nregularizer during learning akin to DropConnect. A local synaptic plasticity\nrule implementing an event-driven form of contrastive divergence enables the\nlearning of generative models in an on-line fashion. Synaptic sampling machines\nperform equally well using discrete-timed artificial units (as in Hopfield\nnetworks) or continuous-timed leaky integrate & fire neurons. The learned\nrepresentations are remarkably sparse and robust to reductions in bit precision\nand synapse pruning: removal of more than 75% of the weakest connections\nfollowed by cursory re-learning causes a negligible performance loss on\nbenchmark classification tasks. The spiking neuron-based synaptic sampling\nmachines outperform existing spike-based unsupervised learners, while\npotentially offering substantial advantages in terms of power and complexity,\nand are thus promising models for on-line learning in brain-inspired hardware."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.05552v4", 
    "title": "Recurrent Neural Networks Hardware Implementation on FPGA", 
    "arxiv-id": "1511.05552v4", 
    "author": "Eugenio Culurciello", 
    "publish": "2015-11-17T02:20:37Z", 
    "summary": "Recurrent Neural Networks (RNNs) have the ability to retain memory and learn\ndata sequences. Due to the recurrent nature of RNNs, it is sometimes hard to\nparallelize all its computations on conventional hardware. CPUs do not\ncurrently offer large parallelism, while GPUs offer limited parallelism due to\nsequential components of RNN models. In this paper we present a hardware\nimplementation of Long-Short Term Memory (LSTM) recurrent network on the\nprogrammable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$\nlayers and $128$ hidden units in hardware and it has been tested using a\ncharacter level language model. The implementation is more than $21\\times$\nfaster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can\npotentially evolve to a RNN co-processor for future mobile devices."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.05625v1", 
    "title": "MOEA/D-GM: Using probabilistic graphical models in MOEA/D for solving   combinatorial optimization problems", 
    "arxiv-id": "1511.05625v1", 
    "author": "Alexander Mendiburu", 
    "publish": "2015-11-18T00:04:35Z", 
    "summary": "Evolutionary algorithms based on modeling the statistical dependencies\n(interactions) between the variables have been proposed to solve a wide range\nof complex problems. These algorithms learn and sample probabilistic graphical\nmodels able to encode and exploit the regularities of the problem. This paper\ninvestigates the effect of using probabilistic modeling techniques as a way to\nenhance the behavior of MOEA/D framework. MOEA/D is a decomposition based\nevolutionary algorithm that decomposes a multi-objective optimization problem\n(MOP) in a number of scalar single-objective subproblems and optimizes them in\na collaborative manner. MOEA/D framework has been widely used to solve several\nMOPs. The proposed algorithm, MOEA/D using probabilistic Graphical Models\n(MOEA/D-GM) is able to instantiate both univariate and multi-variate\nprobabilistic models for each subproblem. To validate the introduced framework\nalgorithm, an experimental study is conducted on a multi-objective version of\nthe deceptive function Trap5. The results show that the variant of the\nframework (MOEA/D-Tree), where tree models are learned from the matrices of the\nmutual information between the variables, is able to capture the structure of\nthe problem. MOEA/D-Tree is able to achieve significantly better results than\nboth MOEA/D using genetic operators and MOEA/D using univariate probability\nmodels, in terms of the approximation to the true Pareto front."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.06248v1", 
    "title": "Critical Parameters in Particle Swarm Optimisation", 
    "arxiv-id": "1511.06248v1", 
    "author": "Thomas Joyce", 
    "publish": "2015-11-19T16:47:01Z", 
    "summary": "Particle swarm optimisation is a metaheuristic algorithm which finds\nreasonable solutions in a wide range of applied problems if suitable parameters\nare used. We study the properties of the algorithm in the framework of random\ndynamical systems which, due to the quasi-linear swarm dynamics, yields\nanalytical results for the stability properties of the particles. Such\nconsiderations predict a relationship between the parameters of the algorithm\nthat marks the edge between convergent and divergent behaviours. Comparison\nwith simulations indicates that the algorithm performs best near this margin of\ninstability."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.06987v1", 
    "title": "Evolutionary algorithms", 
    "arxiv-id": "1511.06987v1", 
    "author": "Anton V. Eremeev", 
    "publish": "2015-11-22T10:05:33Z", 
    "summary": "This manuscript contains an outline of lectures course \"Evolutionary\nAlgorithms\" read by the author in Omsk State University n.a. F.M.Dostoevsky.\nThe course covers Canonic Genetic Algorithm and various other genetic\nalgorithms as well as evolutioanry algorithms in general. Some facts, such as\nthe Rotation Property of crossover, the Schemata Theorem, GA performance as a\nlocal search and \"almost surely\" convergence of evolutionary algorithms are\ngiven with complete proofs. The text is in Russian."
},{
    "category": "cs.NE", 
    "doi": "10.3389/fnins.2016.00241", 
    "link": "http://arxiv.org/pdf/1511.07889v2", 
    "title": "rnn : Recurrent Library for Torch", 
    "arxiv-id": "1511.07889v2", 
    "author": "Jin-Hwa Kim", 
    "publish": "2015-11-24T21:18:33Z", 
    "summary": "The rnn package provides components for implementing a wide range of\nRecurrent Neural Networks. It is built withing the framework of the Torch\ndistribution for use with the nn package. The components have evolved from 3\niterations, each adding to the flexibility and capability of the package. All\ncomponent modules inherit either the AbstractRecurrent or AbstractSequencer\nclasses. Strong unit testing, continued backwards compatibility and access to\nsupporting material are the principles followed during its development. The\npackage is compared against existing implementations of two published papers."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1511.08366v1", 
    "title": "On randomization of neural networks as a form of post-learning strategy", 
    "arxiv-id": "1511.08366v1", 
    "author": "J. M. Sellier", 
    "publish": "2015-11-26T12:11:48Z", 
    "summary": "Today artificial neural networks are applied in various fields - engineering,\ndata analysis, robotics. While they represent a successful tool for a variety\nof relevant applications, mathematically speaking they are still far from being\nconclusive. In particular, they suffer from being unable to find the best\nconfiguration possible during the training process (local minimum problem). In\nthis paper, we focus on this issue and suggest a simple, but effective,\npost-learning strategy to allow the search for improved set of weights at a\nrelatively small extra computational cost. Therefore, we introduce a novel\ntechnique based on analogy with quantum effects occurring in nature as a way to\nimprove (and sometimes overcome) this problem. Several numerical experiments\nare presented to validate the approach."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.00708v1", 
    "title": "Duelist Algorithm: An Algorithm Inspired by How Duelist Improve Their   Capabilities in a Duel", 
    "arxiv-id": "1512.00708v1", 
    "author": "Hairul Huda", 
    "publish": "2015-12-02T14:29:13Z", 
    "summary": "This paper proposes an optimization algorithm based on how human fight and\nlearn from each duelist. Since this algorithm is based on population, the\nproposed algorithm starts with an initial set of duelists. The duel is to\ndetermine the winner and loser. The loser learns from the winner, while the\nwinner try their new skill or technique that may improve their fighting\ncapabilities. A few duelists with highest fighting capabilities are called as\nchampion. The champion train a new duelists such as their capabilities. The new\nduelist will join the tournament as a representative of each champion. All\nduelist are re-evaluated, and the duelists with worst fighting capabilities is\neliminated to maintain the amount of duelists. Two optimization problem is\napplied for the proposed algorithm, together with genetic algorithm, particle\nswarm optimization and imperialist competitive algorithm. The results show that\nthe proposed algorithm is able to find the better global optimum and faster\niteration."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.00883v1", 
    "title": "Cleaning Schedule Optimization of Heat Exchanger Networks Using Particle   Swarm Optimization", 
    "arxiv-id": "1512.00883v1", 
    "author": "Sarwono", 
    "publish": "2015-12-02T22:08:09Z", 
    "summary": "Oil refinery is one of industries that require huge energy consumption. The\ntoday technology advance requires energy saving. Heat integration is a method\nused to minimize the energy comsumption though the implementation of Heat\nExchanger Network (HEN). CPT is one of types of Heat Exchanger Network (HEN)\nthat functions to recover the heat in the flow of product or waste. HEN\ncomprises a number of heat exchangers (HEs) that are serially connected.\nHowever, the presence of fouling in the heat exchanger has caused the decline\nof the performance of both heat exchangers and all heat exchanger networks.\nFouling can not be avoided. However, it can be mitigated. In industry, periodic\nheat exchanger cleaning is the most effective and widely used mitigation\ntechnique. On the other side, a very frequent cleaning of heat exchanger can be\nmuch costly in maintenance and lost of production. In this way, an accurate\noptimization technique of cleaning schedule interval of heat exchanger is very\nessential. Commonly, this technique involves three elements: model to simulate\nthe heat exchanger network, representative fouling model to describe the\nfouling behavior and suitable optimization algorithm to solve the problem of\nclening schedule interval for heat exchanger network. This paper describe the\noptimization of interval cleaning schedule of HEN within the 44-month period\nusing PSO (particle swarm optimization). The number of iteration used to\nachieve the convergent is 100 iterations and the fitness value in PSO\ncorrelated with the amount of heat recovery, cleaning cost, and additional\npumping cost. The saving after the optimization of cleaning schedule of HEN in\nthis research achieved at $ 1.236 millions or 23% of maximum potential savings."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.00961v3", 
    "title": "Triplet Spike Time Dependent Plasticity: A floating-gate Implementation", 
    "arxiv-id": "1512.00961v3", 
    "author": "Arindam Basu", 
    "publish": "2015-12-03T06:12:05Z", 
    "summary": "Synapse plays an important role of learning in a neural network; the learning\nrules which modify the synaptic strength based on the timing difference between\nthe pre- and post-synaptic spike occurrence is termed as Spike Time Dependent\nPlasticity (STDP). The most commonly used rule posits weight change based on\ntime difference between one pre- and one post spike and is hence termed doublet\nSTDP (DSTDP). However, D-STDP could not reproduce results of many biological\nexperiments; a triplet STDP (T-STDP) that considers triplets of spikes as the\nfundamental unit has been proposed recently to explain these observations. This\npaper describes the compact implementation of a synapse using single\nfloating-gate (FG) transistor that can store a weight in a nonvolatile manner\nand demonstrate the triplet STDP (T-STDP) learning rule by modifying drain\nvoltages according to triplets of spikes. We describe a mathematical procedure\nto obtain control voltages for the FG device for T-STDP and also show\nmeasurement results from a FG synapse fabricated in TSMC 0.35um CMOS process to\nsupport the theory. Possible VLSI implementation of drain voltage waveform\ngenerator circuits are also presented with simulation results."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.01314v1", 
    "title": "An Online Unsupervised Structural Plasticity Algorithm for Spiking   Neural Networks", 
    "arxiv-id": "1512.01314v1", 
    "author": "Arindam Basu", 
    "publish": "2015-12-04T04:39:11Z", 
    "summary": "In this article, we propose a novel Winner-Take-All (WTA) architecture\nemploying neurons with nonlinear dendrites and an online unsupervised\nstructural plasticity rule for training it. Further, to aid hardware\nimplementations, our network employs only binary synapses. The proposed\nlearning rule is inspired by spike time dependent plasticity (STDP) but differs\nfor each dendrite based on its activation level. It trains the WTA network\nthrough formation and elimination of connections between inputs and synapses.\nTo demonstrate the performance of the proposed network and learning rule, we\nemploy it to solve two, four and six class classification of random Poisson\nspike time inputs. The results indicate that by proper tuning of the inhibitory\ntime constant of the WTA, a trade-off between specificity and sensitivity of\nthe network can be achieved. We use the inhibitory time constant to set the\nnumber of subpatterns per pattern we want to detect. We show that while the\npercentage of successful trials are 92%, 88% and 82% for two, four and six\nclass classification when no pattern subdivisions are made, it increases to\n100% when each pattern is subdivided into 5 or 10 subpatterns. However, the\nformer scenario of no pattern subdivision is more jitter resilient than the\nlater ones."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.02047v2", 
    "title": "Level-Based Analysis of Genetic Algorithms for Combinatorial   Optimization", 
    "arxiv-id": "1512.02047v2", 
    "author": "Per Kristian Lehre", 
    "publish": "2015-12-07T14:03:43Z", 
    "summary": "The paper is devoted to upper bounds on run-time of Non-Elitist Genetic\nAlgorithms until some target subset of solutions is visited for the first time.\nIn particular, we consider the sets of optimal solutions and the sets of local\noptima as the target subsets. Previously known upper bounds are improved by\nmeans of drift analysis. Finally, we propose conditions ensuring that a\nNon-Elitist Genetic Algorithm efficiently finds approximate solutions with\nconstant approximation ratio on the class of combinatorial optimization\nproblems with guaranteed local optima (GLO)."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.02100v1", 
    "title": "Digital Genesis: Computers, Evolution and Artificial Life", 
    "arxiv-id": "1512.02100v1", 
    "author": "Kevin Korb", 
    "publish": "2015-12-07T15:53:48Z", 
    "summary": "The application of evolution in the digital realm, with the goal of creating\nartificial intelligence and artificial life, has a history as long as that of\nthe digital computer itself. We illustrate the intertwined history of these\nideas, starting with the early theoretical work of John von Neumann and the\npioneering experimental work of Nils Aall Barricelli. We argue that\nevolutionary thinking and artificial life will continue to play an integral\nrole in the future development of the digital world."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.03466v1", 
    "title": "Computing factorized approximations of Pareto-fronts using   mNM-landscapes and Boltzmann distributions", 
    "arxiv-id": "1512.03466v1", 
    "author": "Jose A. Lozano", 
    "publish": "2015-12-10T22:21:03Z", 
    "summary": "NM-landscapes have been recently introduced as a class of tunable rugged\nmodels. They are a subset of the general interaction models where all the\ninteractions are of order less or equal $M$. The Boltzmann distribution has\nbeen extensively applied in single-objective evolutionary algorithms to\nimplement selection and study the theoretical properties of model-building\nalgorithms. In this paper we propose the combination of the multi-objective\nNM-landscape model and the Boltzmann distribution to obtain Pareto-front\napproximations. We investigate the joint effect of the parameters of the\nNM-landscapes and the probabilistic factorizations in the shape of the Pareto\nfront approximations."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.05702v2", 
    "title": "Synthesis of recurrent neural networks for dynamical system simulation", 
    "arxiv-id": "1512.05702v2", 
    "author": "Gabriele MT D'Eleuterio", 
    "publish": "2015-12-17T18:08:33Z", 
    "summary": "We review several of the most widely used techniques for training recurrent\nneural networks to approximate dynamical systems, then describe a novel\nalgorithm for this task. The algorithm is based on an earlier theoretical\nresult that guarantees the quality of the network approximation. We show that a\nfeedforward neural network can be trained on the vector field representation of\na given dynamical system using backpropagation, then recast, using matrix\nmanipulations, as a recurrent network that replicates the original system's\ndynamics. After detailing this algorithm and its relation to earlier\napproaches, we present numerical examples that demonstrate its capabilities.\nOne of the distinguishing features of our approach is that both the original\ndynamical systems and the recurrent networks that simulate them operate in\ncontinuous time."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.07980v2", 
    "title": "Diversity Enhancement for Micro-Differential Evolution", 
    "arxiv-id": "1512.07980v2", 
    "author": "Hamid R. Tizhoosh", 
    "publish": "2015-12-25T09:12:52Z", 
    "summary": "The differential evolution (DE) algorithm suffers from high computational\ntime due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithms\nemploy a very small population size, which can converge faster to a reasonable\nsolution. However, these algorithms are vulnerable to a premature convergence\nas well as to high risk of stagnation. In this paper, MDE algorithm with\nvectorized random mutation factor (MDEVM) is proposed, which utilizes the small\nsize population benefit while empowers the exploration ability of mutation\nfactor through randomizing it in the decision variable level. The idea is\nsupported by analyzing mutation factor using Monte-Carlo based simulations. To\nfacilitate the usage of MDE algorithms with very-small population sizes, new\nmutation schemes for population sizes less than four are also proposed.\nFurthermore, comprehensive comparative simulations and analysis on performance\nof the MDE algorithms over various mutation schemes, population sizes, problem\ntypes (i.e. uni-modal, multi-modal, and composite), problem dimensionalities,\nand mutation factor ranges are conducted by considering population diversity\nanalysis for stagnation and trapping in local optimum situations. The studies\nare conducted on 28 benchmark functions provided for the IEEE CEC-2013\ncompetition. Experimental results demonstrate high performance and convergence\nspeed of the proposed MDEVM algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1512.08301v2", 
    "title": "Feedforward Sequential Memory Networks: A New Structure to Learn   Long-term Dependency", 
    "arxiv-id": "1512.08301v2", 
    "author": "Yu Hu", 
    "publish": "2015-12-28T02:07:48Z", 
    "summary": "In this paper, we propose a novel neural network structure, namely\n\\emph{feedforward sequential memory networks (FSMN)}, to model long-term\ndependency in time series without using recurrent feedback. The proposed FSMN\nis a standard fully-connected feedforward neural network equipped with some\nlearnable memory blocks in its hidden layers. The memory blocks use a\ntapped-delay line structure to encode the long context information into a\nfixed-size representation as short-term memory mechanism. We have evaluated the\nproposed FSMNs in several standard benchmark tasks, including speech\nrecognition and language modelling. Experimental results have shown FSMNs\nsignificantly outperform the conventional recurrent neural networks (RNN),\nincluding LSTMs, in modeling sequential signals like speech or language.\nMoreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs\ndue to the inherent non-recurrent model structure."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1601.00191v1", 
    "title": "An Improved Intelligent Agent for Mining Real-Time Databases Using   Modified Cortical Learning Algorithms", 
    "arxiv-id": "1601.00191v1", 
    "author": "N. E. Osegi", 
    "publish": "2016-01-02T17:21:14Z", 
    "summary": "Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTM\nhave been developed by Numenta Incorporation from which variations and\nmodifications are currently being investigated upon. HTM offers better promises\nas a future computational model of the neocortex the seat of intelligence in\nthe brain. Currently, intelligent agents are embedded in almost every modern\nday electronic system found in homes, offices and industries worldwide. In this\npaper, we present a first step in realising useful HTM like applications\nspecifically for mining a synthetic and real time dataset based on a novel\nintelligent agent framework, and demonstrate how a modified version of this\nvery important computational technique will lead to improved recognition."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1601.03481v1", 
    "title": "A Fuzzy MLP Approach for Non-linear Pattern Classification", 
    "arxiv-id": "1601.03481v1", 
    "author": "H. S. Behera", 
    "publish": "2015-09-19T12:45:19Z", 
    "summary": "In case of decision making problems, classification of pattern is a complex\nand crucial task. Pattern classification using multilayer perceptron (MLP)\ntrained with back propagation learning becomes much complex with increase in\nnumber of layers, number of nodes and number of epochs and ultimate increases\ncomputational time [31]. In this paper, an attempt has been made to use fuzzy\nMLP and its learning algorithm for pattern classification. The time and space\ncomplexities of the algorithm have been analyzed. A training performance\ncomparison has been carried out between MLP and the proposed fuzzy-MLP model by\nconsidering six cases. Results are noted against different learning rates\nranging from 0 to 1. A new performance evaluation factor 'convergence gain' has\nbeen introduced. It is observed that the number of epochs drastically reduced\nand performance increased compared to MLP. The average and minimum gain has\nbeen found to be 93% and 75% respectively. The best gain is found to be 95% and\nis obtained by setting the learning rate to 0.55."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1601.04187v1", 
    "title": "Conversion of Artificial Recurrent Neural Networks to Spiking Neural   Networks for Low-power Neuromorphic Hardware", 
    "arxiv-id": "1601.04187v1", 
    "author": "Emre Neftci", 
    "publish": "2016-01-16T17:48:34Z", 
    "summary": "In recent years the field of neuromorphic low-power systems that consume\norders of magnitude less power gained significant momentum. However, their\nwider use is still hindered by the lack of algorithms that can harness the\nstrengths of such architectures. While neuromorphic adaptations of\nrepresentation learning algorithms are now emerging, efficient processing of\ntemporal sequences or variable length-inputs remain difficult. Recurrent neural\nnetworks (RNN) are widely used in machine learning to solve a variety of\nsequence learning tasks. In this work we present a train-and-constrain\nmethodology that enables the mapping of machine learned (Elman) RNNs on a\nsubstrate of spiking neurons, while being compatible with the capabilities of\ncurrent and near-future neuromorphic systems. This \"train-and-constrain\" method\nconsists of first training RNNs using backpropagation through time, then\ndiscretizing the weights and finally converting them to spiking RNNs by\nmatching the responses of artificial neurons with those of the spiking neurons.\nWe demonstrate our approach by mapping a natural language processing task\n(question classification), where we demonstrate the entire mapping process of\nthe recurrent layer of the network on IBM's Neurosynaptic System \"TrueNorth\", a\nspike-based digital neuromorphic hardware architecture. TrueNorth imposes\nspecific constraints on connectivity, neural and synaptic parameters. To\nsatisfy these constraints, it was necessary to discretize the synaptic weights\nand neural activities to 16 levels, and to limit fan-in to 64 inputs. We find\nthat short synaptic delays are sufficient to implement the dynamical (temporal)\naspect of the RNN in the question classification task. The hardware-constrained\nmodel achieved 74% accuracy in question classification while using less than\n0.025% of the cores on one TrueNorth chip, resulting in an estimated power\nconsumption of ~17 uW."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1601.05911v4", 
    "title": "Orthogonal Echo State Networks and stochastic evaluations of likelihoods", 
    "arxiv-id": "1601.05911v4", 
    "author": "Ying-Hao Yu", 
    "publish": "2016-01-22T09:01:45Z", 
    "summary": "We report about probabilistic likelihood estimates that are performed on time\nseries using an echo state network with orthogonal recurrent connectivity. The\nresults from tests using synthetic stochastic input time series with temporal\ninference indicate that the capability of the network to infer depends on the\nbalance between input strength and recurrent activity. This balance has an\ninfluence on the network with regard to the quality of inference from the short\nterm input history versus inference that accounts for influences that date back\na long time. Sensitivity of such networks against noise and the finite accuracy\nof network states in the recurrent layer are investigated. In addition, a\nmeasure based on mutual information between the output time series and the\nreservoir is introduced. Finally, different types of recurrent connectivity are\nevaluated. Orthogonal matrices show the best results of all investigated\nconnectivity types overall, but also in the way how the network performance\nscales with the size of the recurrent layer."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.01321v1", 
    "title": "A continuum among logarithmic, linear, and exponential functions, and   its potential to improve generalization in neural networks", 
    "arxiv-id": "1602.01321v1", 
    "author": "Michael S. Gashler", 
    "publish": "2016-02-03T14:46:35Z", 
    "summary": "We present the soft exponential activation function for artificial neural\nnetworks that continuously interpolates between logarithmic, linear, and\nexponential functions. This activation function is simple, differentiable, and\nparameterized so that it can be trained as the rest of the network is trained.\nWe hypothesize that soft exponential has the potential to improve neural\nnetwork learning, as it can exactly calculate many natural operations that\ntypical neural networks can only approximate, including addition,\nmultiplication, inner product, distance, polynomials, and sinusoids."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.01510v1", 
    "title": "Unsupervised Regenerative Learning of Hierarchical Features in Spiking   Deep Networks for Object Recognition", 
    "arxiv-id": "1602.01510v1", 
    "author": "Kaushik Roy", 
    "publish": "2016-02-03T23:51:22Z", 
    "summary": "We present a spike-based unsupervised regenerative learning scheme to train\nSpiking Deep Networks (SpikeCNN) for object recognition problems using\nbiologically realistic leaky integrate-and-fire neurons. The training\nmethodology is based on the Auto-Encoder learning model wherein the\nhierarchical network is trained layer wise using the encoder-decoder principle.\nRegenerative learning uses spike-timing information and inherent latencies to\nupdate the weights and learn representative levels for each convolutional layer\nin an unsupervised manner. The features learnt from the final layer in the\nhierarchy are then fed to an output layer. The output layer is trained with\nsupervision by showing a fraction of the labeled training dataset and performs\nthe overall classification of the input. Our proposed methodology yields\n0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparable\nwith state-of-the-art results. The proposed methodology also introduces\nsparsity in the hierarchical feature representations on account of event-based\ncoding resulting in computationally efficient learning."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.02009v2", 
    "title": "Computing with hardware neurons: spiking or classical? Perspectives of   applied Spiking Neural Networks from the hardware side", 
    "arxiv-id": "1602.02009v2", 
    "author": "Masoud Daneshtalab", 
    "publish": "2016-02-05T13:06:44Z", 
    "summary": "While classical neural networks take a position of a leading method in the\nmachine learning community, spiking neuromorphic systems bring attention and\nlarge projects in neuroscience. Spiking neural networks were shown to be able\nto substitute networks of classical neurons in applied tasks. This work\nexplores recent hardware designs focusing on perspective applications (like\nconvolutional neural networks) for both neuron types from the energy efficiency\nside to analyse whether there is a possibility for spiking neuromorphic\nhardware to grow up for a wider use. Our comparison shows that spiking hardware\nis at least on the same level of energy efficiency or even higher than\nnon-spiking on a level of basic operations. However, on a system level, spiking\nsystems are outmatched and consume much more energy due to inefficient data\nrepresentation with a long series of spikes. If spike-driven applications,\nminimizing an amount of spikes, are developed, spiking neural systems may reach\nthe energy efficiency level of classical neural systems. However, in the near\nfuture, both type of neuromorphic systems may benefit from emerging memory\ntechnologies, minimizing the energy consumption of computation and memory for\nboth neuron types. That would make infrastructure and data transfer energy\ndominant on the system level. We expect that spiking neurons have some\nbenefits, which would allow achieving better energy results. Still the problem\nof an amount of spikes will still be the major bottleneck for spiking hardware\nsystems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.02237v1", 
    "title": "Reducing training requirements through evolutionary based dimension   reduction and subject transfer", 
    "arxiv-id": "1602.02237v1", 
    "author": "David M. W. Powersa", 
    "publish": "2016-02-06T11:12:44Z", 
    "summary": "Training Brain Computer Interface (BCI) systems to understand the intention\nof a subject through Electroencephalogram (EEG) data currently requires\nmultiple training sessions with a subject in order to develop the necessary\nexpertise to distinguish signals for different tasks. Conventionally the task\nof training the subject is done by introducing a training and calibration stage\nduring which some feedback is presented to the subject. This training session\ncan take several hours which is not appropriate for on-line EEG-based BCI\nsystems. An alternative approach is to use previous recording sessions of the\nsame person or some other subjects that performed the same tasks (subject\ntransfer) for training the classifiers. The main aim of this study is to\ngenerate a methodology that allows the use of data from other subjects while\nreducing the dimensions of the data. The study investigates several\npossibilities for reducing the necessary training and calibration period in\nsubjects and the classifiers and addresses the impact of i) evolutionary\nsubject transfer and ii) adapting previously trained methods (retraining) using\nother subjects data. Our results suggest reduction to 40% of target subject\ndata is sufficient for training the classifier. Our results also indicate the\nsuperiority of the approaches that incorporated evolutionary subject transfer\nand highlights the feasibility of adapting a system trained on other subjects."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.02862v1", 
    "title": "A Feature-Based Prediction Model of Algorithm Selection for Constrained   Continuous Optimisation", 
    "arxiv-id": "1602.02862v1", 
    "author": "Frank Neumann", 
    "publish": "2016-02-09T05:15:24Z", 
    "summary": "With this paper, we contribute to the growing research area of feature-based\nanalysis of bio-inspired computing. In this research area, problem instances\nare classified according to different features of the underlying problem in\nterms of their difficulty of being solved by a particular algorithm. We\ninvestigate the impact of different sets of evolved instances for building\nprediction models in the area of algorithm selection. Building on the work of\nPoursoltan and Neumann [11,10], we consider how evolved instances can be used\nto predict the best performing algorithm for constrained continuous\noptimisation from a set of bio-inspired computing methods, namely high\nperforming variants of differential evolution, particle swarm optimization, and\nevolution strategies. Our experimental results show that instances evolved with\na multi-objective approach in combination with random instances of the\nunderlying problem allow to build a model that accurately predicts the best\nperforming algorithm for a wide range of problem instances."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.03032v2", 
    "title": "Associative Long Short-Term Memory", 
    "arxiv-id": "1602.03032v2", 
    "author": "Alex Graves", 
    "publish": "2016-02-09T15:26:26Z", 
    "summary": "We investigate a new method to augment recurrent neural networks with extra\nmemory without increasing the number of network parameters. The system has an\nassociative memory based on complex-valued vectors and is closely related to\nHolographic Reduced Representations and Long Short-Term Memory networks.\nHolographic Reduced Representations have limited capacity: as they store more\ninformation, each retrieval becomes noisier due to interference. Our system in\ncontrast creates redundant copies of stored information, which enables\nretrieval with reduced noise. Experiments demonstrate faster learning on\nmultiple memorization tasks."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.04335v1", 
    "title": "Learning Over Long Time Lags", 
    "arxiv-id": "1602.04335v1", 
    "author": "Hojjat Salehinejad", 
    "publish": "2016-02-13T14:09:41Z", 
    "summary": "The advantage of recurrent neural networks (RNNs) in learning dependencies\nbetween time-series data has distinguished RNNs from other deep learning\nmodels. Recently, many advances are proposed in this emerging field. However,\nthere is a lack of comprehensive review on memory models in RNNs in the\nliterature. This paper provides a fundamental review on RNNs and long short\nterm memory (LSTM) model. Then, provides a surveys of recent advances in\ndifferent memory enhancements and learning techniques for capturing long term\ndependencies in RNNs."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.04933v1", 
    "title": "Greedy Ants Colony Optimization Strategy for Solving the Curriculum   Based University Course Timetabling Problem", 
    "arxiv-id": "1602.04933v1", 
    "author": "Godswill Zipamone", 
    "publish": "2016-02-16T08:02:49Z", 
    "summary": "Timetabling is a problem faced in all higher education institutions. The\nInternational Timetabling Competition (ITC) has published a dataset that can be\nused to test the quality of methods used to solve this problem. A number of\nmeta-heuristic approaches have obtained good results when tested on the ITC\ndataset, however few have used the ant colony optimization technique,\nparticularly on the ITC 2007 curriculum based university course timetabling\nproblem. This study describes an ant system that solves the curriculum based\nuniversity course timetabling problem and the quality of the algorithm is\ntested on the ITC 2007 dataset. The ant system was able to find feasible\nsolutions in all instances of the dataset and close to optimal solutions in\nsome instances. The ant system performs better than some published approaches,\nhowever results obtained are not as good as those obtained by the best\npublished approaches. This study may be used as a benchmark for ant based\nalgorithms that solve the curriculum based university course timetabling\nproblem."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.05996v1", 
    "title": "A Nonparametric Framework for Quantifying Generative Inference on   Neuromorphic Systems", 
    "arxiv-id": "1602.05996v1", 
    "author": "Kenneth Kreutz-Delgado", 
    "publish": "2016-02-18T22:47:08Z", 
    "summary": "Restricted Boltzmann Machines and Deep Belief Networks have been successfully\nused in probabilistic generative model applications such as image occlusion\nremoval, pattern completion and motion synthesis. Generative inference in such\nalgorithms can be performed very efficiently on hardware using a Markov Chain\nMonte Carlo procedure called Gibbs sampling, where stochastic samples are drawn\nfrom noisy integrate and fire neurons implemented on neuromorphic substrates.\nCurrently, no satisfactory metrics exist for evaluating the generative\nperformance of such algorithms implemented on high-dimensional data for\nneuromorphic platforms. This paper demonstrates the application of\nnonparametric goodness-of-fit testing to both quantify the generative\nperformance as well as provide decision-directed criteria for choosing the\nparameters of the neuromorphic Gibbs sampler and optimizing usage of hardware\nresources used during sampling."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.07884v1", 
    "title": "Firefly Algorithm for optimization problems with non-continuous   variables: A Review and Analysis", 
    "arxiv-id": "1602.07884v1", 
    "author": "Jean Medard T Ngnotchouye", 
    "publish": "2016-02-25T11:04:09Z", 
    "summary": "Firefly algorithm is a swarm based metaheuristic algorithm inspired by the\nflashing behavior of fireflies. It is an effective and an easy to implement\nalgorithm. It has been tested on different problems from different disciplines\nand found to be effective. Even though the algorithm is proposed for\noptimization problems with continuous variables, it has been modified and used\nfor problems with non-continuous variables, including binary and integer valued\nproblems. In this paper a detailed review of this modifications of firefly\nalgorithm for problems with non-continuous variables will be discussed. The\nstrength and weakness of the modifications along with possible future works\nwill be presented."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.08323v2", 
    "title": "Deep Spiking Networks", 
    "arxiv-id": "1602.08323v2", 
    "author": "Max Welling", 
    "publish": "2016-02-26T13:54:47Z", 
    "summary": "We introduce an algorithm to do backpropagation on a spiking network. Our\nnetwork is \"spiking\" in the sense that our neurons accumulate their activation\ninto a potential over time, and only send out a signal (a \"spike\") when this\npotential crosses a threshold and the neuron is reset. Neurons only update\ntheir states when receiving signals from other neurons. Total computation of\nthe network thus scales with the number of spikes caused by an input rather\nthan network size. We show that the spiking Multi-Layer Perceptron behaves\nidentically, during both prediction and training, to a conventional deep\nnetwork of rectified-linear units, in the limiting case where we run the\nspiking network for a long time. We apply this architecture to a conventional\nclassification problem (MNIST) and achieve performance very close to that of a\nconventional Multi-Layer Perceptron with the same architecture. Our network is\na natural architecture for learning based on streaming event-based data, and is\na stepping stone towards using spiking neural networks to learn efficiently on\nstreaming data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.08556v1", 
    "title": "Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic   Storage in Artificial Neural Networks", 
    "arxiv-id": "1602.08556v1", 
    "author": "Kaushik Roy", 
    "publish": "2016-02-27T05:36:42Z", 
    "summary": "Multilayered artificial neural networks (ANN) have found widespread utility\nin classification and recognition applications. The scale and complexity of\nsuch networks together with the inadequacies of general purpose computing\nplatforms have led to a significant interest in the development of efficient\nhardware implementations. In this work, we focus on designing energy efficient\non-chip storage for the synaptic weights. In order to minimize the power\nconsumption of typical digital CMOS implementations of such large-scale\nnetworks, the digital neurons could be operated reliably at scaled voltages by\nreducing the clock frequency. On the contrary, the on-chip synaptic storage\ndesigned using a conventional 6T SRAM is susceptible to bitcell failures at\nreduced voltages. However, the intrinsic error resiliency of NNs to small\nsynaptic weight perturbations enables us to scale the operating voltage of the\n6TSRAM. Our analysis on a widely used digit recognition dataset indicates that\nthe voltage can be scaled by 200mV from the nominal operating voltage (950mV)\nfor practically no loss (less than 0.5%) in accuracy (22nm predictive\ntechnology). Scaling beyond that causes substantial performance degradation\nowing to increased probability of failures in the MSBs of the synaptic weights.\nWe, therefore propose a significance driven hybrid 8T-6T SRAM, wherein the\nsensitive MSBs are stored in 8T bitcells that are robust at scaled voltages due\nto decoupled read and write paths. In an effort to further minimize the area\npenalty, we present a synaptic-sensitivity driven hybrid memory architecture\nconsisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulation\nframework shows that the proposed synaptic-sensitivity driven architecture\nprovides a 30.91% reduction in the memory access power with a 10.41% area\noverhead, for less than 1% loss in the classification accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.08557v1", 
    "title": "Multiplier-less Artificial Neurons Exploiting Error Resiliency for   Energy-Efficient Neural Computing", 
    "arxiv-id": "1602.08557v1", 
    "author": "Kaushik Roy", 
    "publish": "2016-02-27T05:37:44Z", 
    "summary": "Large-scale artificial neural networks have shown significant promise in\naddressing a wide range of classification and recognition applications.\nHowever, their large computational requirements stretch the capabilities of\ncomputing platforms. The fundamental components of these neural networks are\nthe neurons and its synapses. The core of a digital hardware neuron consists of\nmultiplier, accumulator and activation function. Multipliers consume most of\nthe processing energy in the digital neurons, and thereby in the hardware\nimplementations of artificial neural networks. We propose an approximate\nmultiplier that utilizes the notion of computation sharing and exploits error\nresilience of neural network applications to achieve improved energy\nconsumption. We also propose Multiplier-less Artificial Neuron (MAN) for even\nlarger improvement in energy consumption and adapt the training process to\nensure minimal degradation in accuracy. We evaluated the proposed design on 5\nrecognition applications. The results show, 35% and 60% reduction in energy\nconsumption, for neuron sizes of 8 bits and 12 bits, respectively, with a\nmaximum of ~2.83% loss in network accuracy, compared to a conventional neuron\nimplementation. We also achieve 37% and 62% reduction in area for a neuron size\nof 8 bits and 12 bits, respectively, under iso-speed conditions."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1602.09046v1", 
    "title": "On Complex Valued Convolutional Neural Networks", 
    "arxiv-id": "1602.09046v1", 
    "author": "Nitzan Guberman", 
    "publish": "2016-02-29T17:13:47Z", 
    "summary": "Convolutional neural networks (CNNs) are the cutting edge model for\nsupervised machine learning in computer vision. In recent years CNNs have\noutperformed traditional approaches in many computer vision tasks such as\nobject detection, image classification and face recognition. CNNs are\nvulnerable to overfitting, and a lot of research focuses on finding\nregularization methods to overcome it. One approach is designing task specific\nmodels based on prior knowledge.\n  Several works have shown that properties of natural images can be easily\ncaptured using complex numbers. Motivated by these works, we present a\nvariation of the CNN model with complex valued input and weights. We construct\nthe complex model as a generalization of the real model. Lack of order over the\ncomplex field raises several difficulties both in the definition and in the\ntraining of the network. We address these issues and suggest possible\nsolutions.\n  The resulting model is shown to be a restricted form of a real valued CNN\nwith twice the parameters. It is sensitive to phase structure, and we suggest\nit serves as a regularized model for problems where such structure is\nimportant. This suggestion is verified empirically by comparing the performance\nof a complex and a real network in the problem of cell detection. The two\nnetworks achieve comparable results, and although the complex model is hard to\ntrain, it is significantly less vulnerable to overfitting. We also demonstrate\nthat the complex network detects meaningful phase structure in the data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-015-1949-1", 
    "link": "http://arxiv.org/pdf/1603.03149v1", 
    "title": "Real time error detection in metal arc welding process using Artificial   Neural Netwroks", 
    "arxiv-id": "1603.03149v1", 
    "author": "S. Rajeswari", 
    "publish": "2016-03-10T05:00:26Z", 
    "summary": "Quality assurance in production line demands reliable weld joints. Human made\nerrors is a major cause of faulty production. Promptly Identifying errors in\nthe weld while welding is in progress will decrease the post inspection cost\nspent on the welding process. Electrical parameters generated during welding,\ncould able to characterize the process efficiently. Parameter values are\ncollected using high speed data acquisition system. Time series analysis tasks\nsuch as filtering, pattern recognition etc. are performed over the collected\ndata. Filtering removes the unwanted noisy signal components and pattern\nrecognition task segregate error patterns in the time series based upon\nsimilarity, which is performed by Self Organized mapping clustering algorithm.\nWelder quality is thus compared by detecting and counting number of error\npatterns appeared in his parametric time series. Moreover, Self Organized\nmapping algorithm provides the database in which patterns are segregated into\ntwo classes either desirable or undesirable. Database thus generated is used to\ntrain the classification algorithms, and thereby automating the real time error\ndetection task. Multi Layer Perceptron and Radial basis function are the two\nclassification algorithms used, and their performance has been compared based\non metrics such as specificity, sensitivity, accuracy and time required in\ntraining."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISCAS.2016.7538989", 
    "link": "http://arxiv.org/pdf/1603.04080v1", 
    "title": "A Stochastic Approach to STDP", 
    "arxiv-id": "1603.04080v1", 
    "author": "Andr\u00e9 van Schaik", 
    "publish": "2016-03-13T21:44:22Z", 
    "summary": "We present a digital implementation of the Spike Timing Dependent Plasticity\n(STDP) learning rule. The proposed digital implementation consists of an\nexponential decay generator array and a STDP adaptor array. On the arrival of a\npre- and post-synaptic spike, the STDP adaptor will send a digital spike to the\ndecay generator. The decay generator will then generate an exponential decay,\nwhich will be used by the STDP adaptor to perform the weight adaption. The\nexponential decay, which is computational expensive, is efficiently implemented\nby using a novel stochastic approach, which we analyse and characterise here.\nWe use a time multiplexing approach to achieve 8192 (8k) virtual STDP adaptors\nand decay generators with only one physical implementation of each. We have\nvalidated our stochastic STDP approach with measurement results of a balanced\nexcitation/inhibition experiment. Our stochastic approach is ideal for\nimplementing the STDP learning rule in large-scale spiking neural networks\nrunning in real time."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISCAS.2016.7538989", 
    "link": "http://arxiv.org/pdf/1603.06788v1", 
    "title": "Adaptive Parameter Selection in Evolutionary Algorithms by Reinforcement   Learning with Dynamic Discretization of Parameter Range", 
    "arxiv-id": "1603.06788v1", 
    "author": "Arina Buzdalova", 
    "publish": "2016-03-22T13:40:05Z", 
    "summary": "Online parameter controllers for evolutionary algorithms adjust values of\nparameters during the run of an evolutionary algorithm. Recently a new\nefficient parameter controller based on reinforcement learning was proposed by\nKarafotias et al. In this method ranges of parameters are discretized into\nseveral intervals before the run. However, performing adaptive discretization\nduring the run may increase efficiency of an evolutionary algorithm. Aleti et\nal. proposed another efficient controller with adaptive discretization.\n  In the present paper we propose a parameter controller based on reinforcement\nlearning with adaptive discretization. The proposed controller is compared with\nthe existing parameter adjusting methods on several test problems using\ndifferent configurations of an evolutionary algorithm. For the test problems,\nwe consider four continuous functions, namely the sphere function, the\nRosenbrock function, the Levi function and the Rastrigin function. Results show\nthat the new controller outperforms the other controllers on most of the\nconsidered test problems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ISCAS.2016.7538989", 
    "link": "http://arxiv.org/pdf/1603.08146v1", 
    "title": "A Draft Memory Model on Spiking Neural Assemblies", 
    "arxiv-id": "1603.08146v1", 
    "author": "Pedro J. Ishimaru", 
    "publish": "2016-03-26T21:41:04Z", 
    "summary": "A draft memory model (DM) for neural networks with spike propagation delay\n(SNNwD) is described. Novelty in this approach are that the DM learns\nimmediately, with stimuli presented once, without synaptic weight changes, and\nwithout external learning algorithm. Basal on this model is to trap spikes\nwithin neural loops. In order to construct the DM we developed two functional\nblocks, also described herein. The decoder block receives input from a single\nspikes source and connect it to one among many outputs. The selector block\noperates in the opposite direction, receiving many spikes sources and\nconnecting one of them to a single output. We realized conceptual proofs by\ntesting the DM in the prime numbers classifying task. This activation-based\nmemory can be used as immediate and short-term memory."
},{
    "category": "cs.NE", 
    "doi": "10.1073/pnas.1604850113", 
    "link": "http://arxiv.org/pdf/1603.08270v2", 
    "title": "Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing", 
    "arxiv-id": "1603.08270v2", 
    "author": "Dharmendra S. Modha", 
    "publish": "2016-03-28T00:15:35Z", 
    "summary": "Deep networks are now able to achieve human-level performance on a broad\nspectrum of recognition tasks. Independently, neuromorphic computing has now\ndemonstrated unprecedented energy-efficiency through a new chip architecture\nbased on spiking neurons, low precision synapses, and a scalable communication\nnetwork. Here, we demonstrate that neuromorphic computing, despite its novel\narchitectural primitives, can implement deep convolution networks that i)\napproach state-of-the-art classification accuracy across 8 standard datasets,\nencompassing vision and speech, ii) perform inference while preserving the\nhardware's underlying energy-efficiency and high throughput, running on the\naforementioned datasets at between 1200 and 2600 frames per second and using\nbetween 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be\nspecified and trained using backpropagation with the same ease-of-use as\ncontemporary deep learning. For the first time, the algorithmic power of deep\nlearning can be merged with the efficiency of neuromorphic processors, bringing\nthe promise of embedded, intelligent, brain-inspired computing one step closer."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1603.08538v2", 
    "title": "Hybrid Ant Colony Optimization in solving Multi-Skill   Resource-Constrained Project Scheduling Problem", 
    "arxiv-id": "1603.08538v2", 
    "author": "Krzysztof O\u015bliz\u0142o", 
    "publish": "2016-03-28T20:15:53Z", 
    "summary": "In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solving\nMulti--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) has\nbeen presented. We have proposed hybrid approach that links classical heuristic\npriority rules for project scheduling with Ant Colony Optimization (ACO).\nFurthermore, a novel approach for updating pheromone value has been proposed,\nbased on both the best and worst solutions stored by ants. The objective of\nthis paper is to research the usability and robustness of ACO and its hybrids\nwith priority rules in solving MS--RCPSP. Experiments have been performed using\nartificially created dataset instances, based on real--world ones. We published\nthose instances that can be used as a benchmark. Presented results show that\nACO--based hybrid method is an efficient approach. More directed search process\nby hybrids makes this approach more stable and provides mostly better results\nthan classical ACO."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1603.08983v6", 
    "title": "Adaptive Computation Time for Recurrent Neural Networks", 
    "arxiv-id": "1603.08983v6", 
    "author": "Alex Graves", 
    "publish": "2016-03-29T22:09:00Z", 
    "summary": "This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1603.09002v1", 
    "title": "Dataflow Matrix Machines as a Generalization of Recurrent Neural   Networks", 
    "arxiv-id": "1603.09002v1", 
    "author": "Andrey Radul", 
    "publish": "2016-03-29T23:48:27Z", 
    "summary": "Dataflow matrix machines are a powerful generalization of recurrent neural\nnetworks. They work with multiple types of arbitrary linear streams, multiple\ntypes of powerful neurons, and allow to incorporate higher-order constructions.\nWe expect them to be useful in machine learning and probabilistic programming,\nand in the synthesis of dynamic systems and of deterministic and probabilistic\nprograms."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.00552v1", 
    "title": "pH Prediction by Artificial Neural Networks for the Drinking Water of   the Distribution System of Hyderabad City", 
    "arxiv-id": "1604.00552v1", 
    "author": "Abdul Khalique Ansari", 
    "publish": "2016-04-02T20:14:18Z", 
    "summary": "In this research, feedforward ANN (Artificial Neural Network) model is\ndeveloped and validated for predicting the pH at 10 different locations of the\ndistribution system of drinking water of Hyderabad city. The developed model is\nMLP (Multilayer Perceptron) with back propagation algorithm.The data for the\ntraining and testing of the model are collected through an experimental\nanalysis on weekly basis in a routine examination for maintaining the quality\nof drinking water in the city. 17 parameters are taken into consideration\nincluding pH. These all parameters are taken as input variables for the model\nand then pH is predicted for 03 phases;raw water of river Indus,treated water\nin the treatment plants and then treated water in the distribution system of\ndrinking water. The training and testing results of this model reveal that MLP\nneural networks are exceedingly extrapolative for predicting the pH of river\nwater, untreated and treated water at all locations of the distribution system\nof drinking water of Hyderabad city. The optimum input and output weights are\ngenerated with minimum MSE (Mean Square Error) < 5%.Experimental, predicted and\ntested values of pH are plotted and the effectiveness of the model is\ndetermined by calculating the coefficient of correlation (R2=0.999) of trained\nand tested results."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.00558v1", 
    "title": "Channel Equalization Using Multilayer Perceptron Networks", 
    "arxiv-id": "1604.00558v1", 
    "author": "Mukhtiar Ali Unar", 
    "publish": "2016-04-02T21:00:54Z", 
    "summary": "In most digital communication systems, bandwidth limited channel along with\nmultipath propagation causes ISI (Inter Symbol Interference) to occur. This\nphenomenon causes distortion of the given transmitted symbol due to other\ntransmitted symbols. With the help of equalization ISI can be reduced. This\npaper presents a solution to the ISI problem by performing blind equalization\nusing ANN (Artificial Neural Networks). The simulated network is a multilayer\nfeedforward Perceptron ANN, which has been trained by utilizing the error\nback-propagation algorithm. The weights of the network are updated in\naccordance with training of the network. This paper presents a very effective\nmethod for blind channel equalization, being more efficient than the\npre-existing algorithms. The obtained results show a visible reduction in the\nnoise content."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.01643v2", 
    "title": "Information Utilization Ratio in Heuristic Optimization Algorithms", 
    "arxiv-id": "1604.01643v2", 
    "author": "Ying Tan", 
    "publish": "2016-04-06T14:35:11Z", 
    "summary": "Heuristic algorithms are able to optimize objective functions efficiently\nbecause they use intelligently the information about the objective functions.\nThus, information utilization is critical to the performance of heuristics.\nHowever, the concept of information utilization has remained vague and abstract\nbecause there is no reliable metric to reflect the extent to which the\ninformation about the objective function is utilized by heuristic algorithms.\nIn this paper, the metric of information utilization ratio (IUR) is defined,\nwhich is the ratio of the utilized information quantity over the acquired\ninformation quantity in the search process. The IUR proves to be well-defined.\nSeveral examples of typical heuristic algorithms are given to demonstrate the\nprocedure of calculating the IUR. Empirical evidences on the correlation\nbetween the IUR and the performance of a heuristic are also provided. The IUR\ncan be an index of how finely an algorithm is designed and guide the invention\nof new heuristics and the improvement of existing ones."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.02313v5", 
    "title": "Norm-preserving Orthogonal Permutation Linear Unit Activation Functions   (OPLU)", 
    "arxiv-id": "1604.02313v5", 
    "author": "Dimitri Nowicki", 
    "publish": "2016-04-08T11:39:31Z", 
    "summary": "We propose a novel activation function that implements piece-wise orthogonal\nnon-linear mappings based on permutations. It is straightforward to implement,\nand very computationally efficient, also it has little memory requirements. We\ntested it on two toy problems for feedforward and recurrent networks, it shows\nsimilar performance to tanh and ReLU. OPLU activation function ensures norm\npreservance of the backpropagated gradients, therefore it is potentially good\nfor the training of deep, extra deep, and recurrent neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.02910v3", 
    "title": "Deep Gate Recurrent Neural Network", 
    "arxiv-id": "1604.02910v3", 
    "author": "Dorota Glowacka", 
    "publish": "2016-04-11T12:14:52Z", 
    "summary": "This paper introduces two recurrent neural network structures called Simple\nGated Unit (SGU) and Deep Simple Gated Unit (DSGU), which are general\nstructures for learning long term dependencies. Compared to traditional Long\nShort-Term Memory (LSTM) and Gated Recurrent Unit (GRU), both structures\nrequire fewer parameters and less computation time in sequence classification\ntasks. Unlike GRU and LSTM, which require more than one gates to control\ninformation flow in the network, SGU and DSGU only use one multiplicative gate\nto control the flow of information. We show that this difference can accelerate\nthe learning speed in tasks that require long dependency information. We also\nshow that DSGU is more numerically stable than SGU. In addition, we also\npropose a standard way of representing inner structure of RNN called RNN\nConventional Graph (RCG), which helps analyzing the relationship between input\nunits and hidden units of RNN."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s00500-014-1455-x", 
    "link": "http://arxiv.org/pdf/1604.04153v1", 
    "title": "Learning to Generate Genotypes with Neural Networks", 
    "arxiv-id": "1604.04153v1", 
    "author": "Chrisantha Fernando", 
    "publish": "2016-04-14T13:48:26Z", 
    "summary": "Neural networks and evolutionary computation have a rich intertwined history.\nThey most commonly appear together when an evolutionary algorithm optimises the\nparameters and topology of a neural network for reinforcement learning\nproblems, or when a neural network is applied as a surrogate fitness function\nto aid the evolutionary optimisation of expensive fitness functions. In this\npaper we take a different approach, asking the question of whether a neural\nnetwork can be used to provide a mutation distribution for an evolutionary\nalgorithm, and what advantages this approach may offer? Two modern neural\nnetwork models are investigated, a Denoising Autoencoder modified to produce\nstochastic outputs and the Neural Autoregressive Distribution Estimator.\nResults show that the neural network approach to learning genotypes is able to\nsolve many difficult discrete problems, such as MaxSat and HIFF, and regularly\noutperforms other evolutionary techniques."
},{
    "category": "cs.NE", 
    "doi": "10.5120/21245-4034", 
    "link": "http://arxiv.org/pdf/1604.05008v1", 
    "title": "Forecasting Volatility in Indian Stock Market using Artificial Neural   Network with Multiple Inputs and Outputs", 
    "arxiv-id": "1604.05008v1", 
    "author": "Indranil Ghosh", 
    "publish": "2016-04-18T06:29:01Z", 
    "summary": "Volatility in stock markets has been extensively studied in the applied\nfinance literature. In this paper, Artificial Neural Network models based on\nvarious back propagation algorithms have been constructed to predict volatility\nin the Indian stock market through volatility of NIFTY returns and volatility\nof gold returns. This model considers India VIX, CBOE VIX, volatility of crude\noil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAX\nreturns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility of\nNikkei returns (NIKKEISDR) as predictor variables. Three sets of experiments\nhave been performed over three time periods to judge the effectiveness of the\napproach."
},{
    "category": "cs.NE", 
    "doi": "10.5120/21245-4034", 
    "link": "http://arxiv.org/pdf/1604.05459v1", 
    "title": "An Online Structural Plasticity Rule for Generating Better Reservoirs", 
    "arxiv-id": "1604.05459v1", 
    "author": "Arindam Basu", 
    "publish": "2016-04-19T07:30:46Z", 
    "summary": "In this article, a novel neuro-inspired low-resolution online unsupervised\nlearning rule is proposed to train the reservoir or liquid of Liquid State\nMachine. The liquid is a sparsely interconnected huge recurrent network of\nspiking neurons. The proposed learning rule is inspired from structural\nplasticity and trains the liquid through formation and elimination of synaptic\nconnections. Hence, the learning involves rewiring of the reservoir connections\nsimilar to structural plasticity observed in biological neural networks. The\nnetwork connections can be stored as a connection matrix and updated in memory\nby using Address Event Representation (AER) protocols which are generally\nemployed in neuromorphic systems. On investigating the 'pairwise separation\nproperty' we find that trained liquids provide 1.36 $\\pm$ 0.18 times more\ninter-class separation while retaining similar intra-class separation as\ncompared to random liquids. Moreover, analysis of the 'linear separation\nproperty' reveals that trained liquids are 2.05 $\\pm$ 0.27 times better than\nrandom liquids. Furthermore, we show that our liquids are able to retain the\n'generalization' ability and 'generality' of random liquids. A memory analysis\nshows that trained liquids have 83.67 $\\pm$ 5.79 ms longer fading memory than\nrandom liquids which have shown 92.8 $\\pm$ 5.03 ms fading memory for a\nparticular type of spike train inputs. We also throw some light on the dynamics\nof the evolution of recurrent connections within the liquid. Moreover, compared\nto 'Separation Driven Synaptic Modification' - a recently proposed algorithm\nfor iteratively refining reservoirs, our learning rule provides 9.30%, 15.21%\nand 12.52% more liquid separations and 2.8%, 9.1% and 7.9% better\nclassification accuracies for four, eight and twelve class pattern recognition\ntasks respectively."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1604.05792v1", 
    "title": "Multi-agent evolutionary systems for the generation of complex virtual   worlds", 
    "arxiv-id": "1604.05792v1", 
    "author": "Andy M. Connor", 
    "publish": "2016-04-20T02:43:47Z", 
    "summary": "Modern films, games and virtual reality applications are dependent on\nconvincing computer graphics. Highly complex models are a requirement for the\nsuccessful delivery of many scenes and environments. While workflows such as\nrendering, compositing and animation have been streamlined to accommodate\nincreasing demands, modelling complex models is still a laborious task. This\npaper introduces the computational benefits of an Interactive Genetic Algorithm\n(IGA) to computer graphics modelling while compensating the effects of user\nfatigue, a common issue with Interactive Evolutionary Computation. An\nintelligent agent is used in conjunction with an IGA that offers the potential\nto reduce the effects of user fatigue by learning from the choices made by the\nhuman designer and directing the search accordingly. This workflow accelerates\nthe layout and distribution of basic elements to form complex models. It\ncaptures the designer's intent through interaction, and encourages playful\ndiscovery."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1604.06187v1", 
    "title": "Evolutionary Image Transition Based on Theoretical Insights of Random   Processes", 
    "arxiv-id": "1604.06187v1", 
    "author": "Frank Neumann", 
    "publish": "2016-04-21T05:47:05Z", 
    "summary": "Evolutionary algorithms have been widely studied from a theoretical\nperspective. In particular, the area of runtime analysis has contributed\nsignificantly to a theoretical understanding and provided insights into the\nworking behaviour of these algorithms. We study how these insights into\nevolutionary processes can be used for evolutionary art. We introduce the\nnotion of evolutionary image transition which transfers a given starting image\ninto a target image through an evolutionary process. Combining standard\nmutation effects known from the optimization of the classical benchmark\nfunction OneMax and different variants of random walks, we present ways of\nperforming evolutionary image transition with different artistic effects."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1604.06607v1", 
    "title": "K-Bit-Swap: A New Operator For Real-Coded Evolutionary Algorithms", 
    "arxiv-id": "1604.06607v1", 
    "author": "Stephen Marsland", 
    "publish": "2016-04-22T11:09:18Z", 
    "summary": "There has been a variety of crossover operators proposed for Real-Coded\nGenetic Algorithms (RCGAs), which recombine values from the same location in\npairs of strings. In this article we present a recombination operator for RC-\nGAs that selects the locations randomly in both parents, and compare it to\nmainstream crossover operators in a set of experiments on a range of standard\nmultidimensional optimization problems and a clustering problem. We present two\nvariants of the operator, either selecting both bits uniformly at random in the\nstrings, or sampling the second bit from a normal distribution centered at the\nselected location in the first string. While the operator is biased towards\nexploitation of fitness space, the random selection of the second bit for swap-\nping makes it slightly less exploitation-biased. Extensive statistical analysis\nusing a non-parametric test shows the advantage of the new recombination\noperators on a range of test functions."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1604.06751v1", 
    "title": "evt_MNIST: A spike based version of traditional MNIST", 
    "arxiv-id": "1604.06751v1", 
    "author": "Philippe Devienne", 
    "publish": "2016-04-22T17:06:31Z", 
    "summary": "Benchmarks and datasets have important role in evaluation of machine learning\nalgorithms and neural network implementations. Traditional dataset for images\nsuch as MNIST is applied to evaluate efficiency of different training\nalgorithms in neural networks. This demand is different in Spiking Neural\nNetworks (SNN) as they require spiking inputs. It is widely believed, in the\nbiological cortex the timing of spikes is irregular. Poisson distributions\nprovide adequate descriptions of the irregularity in generating appropriate\nspikes. Here, we introduce a spike-based version of MNSIT (handwritten digits\ndataset),using Poisson distribution and show the Poissonian property of the\ngenerated streams. We introduce a new version of evt_MNIST which can be used\nfor neural network evaluation."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1604.06929v1", 
    "title": "Memory and Information Processing in Recurrent Neural Networks", 
    "arxiv-id": "1604.06929v1", 
    "author": "Darko Stefanovic", 
    "publish": "2016-04-23T17:36:12Z", 
    "summary": "Recurrent neural networks (RNN) are simple dynamical systems whose\ncomputational power has been attributed to their short-term memory. Short-term\nmemory of RNNs has been previously studied analytically only for the case of\northogonal networks, and only under annealed approximation, and uncorrelated\ninput. Here for the first time, we present an exact solution to the memory\ncapacity and the task-solving performance as a function of the structure of a\ngiven network instance, enabling direct determination of the\nfunction--structure relation in RNNs. We calculate the memory capacity for\narbitrary networks with exponentially correlated input and further related it\nto the performance of the system on signal processing tasks in a supervised\nlearning setup. We compute the expected error and the worst-case error bound as\na function of the spectra of the network and the correlation structure of its\ninputs and outputs. Our results give an explanation for learning and\ngeneralization of task solving using short-term memory, which is crucial for\nbuilding alternative computer architectures using physical phenomena based on\nthe short-term memory principle."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1605.00097v1", 
    "title": "Application of artificial neural networks and genetic algorithms for   crude fractional distillation process modeling", 
    "arxiv-id": "1605.00097v1", 
    "author": "Lukasz Pater", 
    "publish": "2016-04-30T11:46:58Z", 
    "summary": "This work presents the application of the artificial neural networks, trained\nand structurally optimized by genetic algorithms, for modeling of crude\ndistillation process at PKN ORLEN S.A. refinery. Models for the main\nfractionator distillation column products were developed using historical data.\nQuality of the fractions were predicted based on several chosen process\nvariables. The performance of the model was validated using test data. Neural\nnetworks used in companion with genetic algorithms proved that they can\naccurately predict fractions quality shifts, reproducing the results of the\nstandard laboratory analysis. Simple knowledge extraction method from neural\nnetwork model built was also performed. Genetic algorithms can be successfully\nutilized in efficient training of large neural networks and finding their\noptimal structures."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1605.01514v1", 
    "title": "Fitness-based Adaptive Control of Parameters in Genetic Programming:   Adaptive Value Setting of Mutation Rate and Flood Mechanisms", 
    "arxiv-id": "1605.01514v1", 
    "author": "Juraj Spalek", 
    "publish": "2016-05-05T07:30:02Z", 
    "summary": "This paper concerns applications of genetic algorithms and genetic\nprogramming to tasks for which it is difficult to find a representation that\ndoes not map to a highly complex and discontinuous fitness landscape. In such\ncases the standard algorithm is prone to getting trapped in local extremes. The\npaper proposes several adaptive mechanisms that are useful in preventing the\nsearch from getting trapped."
},{
    "category": "cs.NE", 
    "doi": "10.4108/eai.20-10-2015.150099", 
    "link": "http://arxiv.org/pdf/1605.01746v1", 
    "title": "Biobjective Performance Assessment with the COCO Platform", 
    "arxiv-id": "1605.01746v1", 
    "author": "Anne Auger", 
    "publish": "2016-05-05T20:15:47Z", 
    "summary": "This document details the rationales behind assessing the performance of\nnumerical black-box optimizers on multi-objective problems within the COCO\nplatform and in particular on the biobjective test suite bbob-biobj. The\nevaluation is based on a hypervolume of all non-dominated solutions in the\narchive of candidate solutions and measures the runtime until the hypervolume\nvalue succeeds prescribed target values."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.01855v1", 
    "title": "Resource allocation using metaheuristic search", 
    "arxiv-id": "1605.01855v1", 
    "author": "Amit Shah", 
    "publish": "2016-05-06T08:14:13Z", 
    "summary": "This research is focused on solving problems in the area of software project\nmanagement using metaheuristic search algorithms and as such is research in the\nfield of search based software engineering. The main aim of this research is to\nevaluate the performance of different metaheuristic search techniques in\nresource allocation and scheduling problems that would be typical of software\ndevelopment projects. This paper reports a set of experiments which evaluate\nthe performance of three algorithms, namely simulated annealing, tabu search\nand genetic algorithms. The experimental results indicate that all of the\nmetaheuristics search techniques can be used to solve problems in resource\nallocation and scheduling within a software project. Finally, a comparative\nanalysis suggests that overall the genetic algorithm had performed better than\nsimulated annealing and tabu search."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.01988v2", 
    "title": "LSTM with Working Memory", 
    "arxiv-id": "1605.01988v2", 
    "author": "Siwei Lyu", 
    "publish": "2016-05-06T16:11:45Z", 
    "summary": "LSTM is arguably the most successful RNN architecture for many tasks that\ninvolve sequential information. In the past few years there have been several\nproposed improvements to LSTM. We propose an improvement to LSTM which allows\ncommunication between memory cells in different blocks and allows an LSTM layer\nto carry out internal computation within its memory."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.02486v1", 
    "title": "Efficiency Evaluation of Character-level RNN Training Schedules", 
    "arxiv-id": "1605.02486v1", 
    "author": "Bart Dhoedt", 
    "publish": "2016-05-09T09:12:11Z", 
    "summary": "We present four training and prediction schedules from the same\ncharacter-level recurrent neural network. The efficiency of these schedules is\ntested in terms of model effectiveness as a function of training time and\namount of training data seen. We show that the choice of training and\nprediction schedule potentially has a considerable impact on the prediction\neffectiveness for a given training budget."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.02720v1", 
    "title": "Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES   (HMO-CMA-ES)", 
    "arxiv-id": "1605.02720v1", 
    "author": "Tobias Glasmachers", 
    "publish": "2016-05-09T19:58:29Z", 
    "summary": "We propose a multi-objective optimization algorithm aimed at achieving good\nanytime performance over a wide range of problems. Performance is assessed in\nterms of the hypervolume metric. The algorithm called HMO-CMA-ES represents a\nhybrid of several old and new variants of CMA-ES, complemented by BOBYQA as a\nwarm start. We benchmark HMO-CMA-ES on the recently introduced bi-objective\nproblem suite of the COCO framework (COmparing Continuous Optimizers),\nconsisting of 55 scalable continuous optimization problems, which is used by\nthe Black-Box Optimization Benchmarking (BBOB) Workshop 2016."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.03560v1", 
    "title": "COCO: Performance Assessment", 
    "arxiv-id": "1605.03560v1", 
    "author": "Tea Tu\u0161ar", 
    "publish": "2016-05-11T19:49:43Z", 
    "summary": "We present an any-time performance assessment for benchmarking numerical\noptimization algorithms in a black-box scenario, applied within the COCO\nbenchmarking platform. The performance assessment is based on runtimes measured\nin number of objective function evaluations to reach one or several quality\nindicator target values. We argue that runtime is the only available measure\nwith a generic, meaningful, and quantitative interpretation. We discuss the\nchoice of the target values, runlength-based targets, and the aggregation of\nresults by using simulated restarts, averages, and empirical distribution\nfunctions."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.03764v1", 
    "title": "Direct Method for Training Feed-forward Neural Networks using Batch   Extended Kalman Filter for Multi-Step-Ahead Predictions", 
    "arxiv-id": "1605.03764v1", 
    "author": "Artem Chernodub", 
    "publish": "2016-05-12T11:39:14Z", 
    "summary": "This paper is dedicated to the long-term, or multi-step-ahead, time series\nprediction problem. We propose a novel method for training feed-forward neural\nnetworks, such as multilayer perceptrons, with tapped delay lines. Special\nbatch calculation of derivatives called Forecasted Propagation Through Time and\nbatch modification of the Extended Kalman Filter are introduced. Experiments\nwere carried out on well-known time series benchmarks, the Mackey-Glass chaotic\nprocess and the Santa Fe Laser Data Series. Recurrent and feed-forward neural\nnetworks were evaluated."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.05216v1", 
    "title": "Combinatorially Generated Piecewise Activation Functions", 
    "arxiv-id": "1605.05216v1", 
    "author": "Justin Chen", 
    "publish": "2016-05-17T15:45:08Z", 
    "summary": "In the neuroevolution literature, research has primarily focused on evolving\nthe number of nodes, connections, and weights in artificial neural networks.\nFew attempts have been made to evolve activation functions. Research in\nevolving activation functions has mainly focused on evolving function\nparameters, and developing heterogeneous networks by selecting from a fixed\npool of activation functions. This paper introduces a novel technique for\nevolving heterogeneous artificial neural networks through combinatorially\ngenerating piecewise activation functions to enhance expressive power. I\ndemonstrate this technique on NeuroEvolution of Augmenting Topologies using\nArcTan and Sigmoid, and show that it outperforms the original algorithm on\nnon-Markovian double pole balancing. This technique expands the landscape of\nunconventional activation functions by demonstrating that they are competitive\nwith canonical choices, and introduces a purview for further exploration of\nautomatic model selection for artificial neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.06710v1", 
    "title": "Chess Player by Co-Evolutionary Algorithm", 
    "arxiv-id": "1605.06710v1", 
    "author": "Agostinho C Rosa", 
    "publish": "2016-05-21T23:45:38Z", 
    "summary": "A co-evolutionary algorithm (CA) based chess player is presented.\nImplementation details of the algorithms, namely coding, population, variation\noperators are described. The alpha-beta or mini-max like behaviour of the\nplayer is achieved through two competitive or cooperative populations. Special\nattention is given to the fitness function evaluation (the heart of the\nsolution). Test results on algorithms vs. algorithms or human player is\nprovided."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.06714v1", 
    "title": "Evolutionary Demographic Algorithms", 
    "arxiv-id": "1605.06714v1", 
    "author": "Agostinho C Rosa", 
    "publish": "2016-05-22T00:09:04Z", 
    "summary": "Most of the problems in genetic algorithms are very complex and demand a\nlarge amount of resources that current technology can not offer. Our purpose\nwas to develop a Java-JINI distributed library that implements Genetic\nAlgorithms with sub-populations (coarse grain) and a graphical interface in\norder to configure and follow the evolution of the search. The sub-populations\nare simulated/evaluated in personal computers connected trough a network,\nkeeping in mind different models of sub-populations, migration policies and\nnetwork topologies. We show that this model delays the convergence of the\npopulation keeping a higher level of genetic diversity and allows a much\ngreater number of evaluations since they are distributed among several\ncomputers compared with the traditional Genetic Algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1605.07740v1", 
    "title": "Improving energy efficiency and classification accuracy of neuromorphic   chips by learning binary synaptic crossbars", 
    "arxiv-id": "1605.07740v1", 
    "author": "Jianbin Tang", 
    "publish": "2016-05-25T06:09:42Z", 
    "summary": "Deep Neural Networks (DNN) have achieved human level performance in many\nimage analytics tasks but DNNs are mostly deployed to GPU platforms that\nconsume a considerable amount of power. Brain-inspired spiking neuromorphic\nchips consume low power and can be highly parallelized. However, for deploying\nDNNs to energy efficient neuromorphic chips the incompatibility between\ncontinuous neurons and synaptic weights of traditional DNNs, discrete spiking\nneurons and synapses of neuromorphic chips has to be overcome. Previous work\nhas achieved this by training a network to learn continuous probabilities and\ndeployment to a neuromorphic architecture by random sampling these\nprobabilities. An ensemble of sampled networks is needed to approximate the\nperformance of the trained network.\n  In the work presented in this paper, we have extended previous research by\ndirectly learning binary synaptic crossbars. Results on MNIST show that better\nperformance can be achieved with a small network in one time step (92.7%\nmaximum observed accuracy vs 95.98% accuracy in our work). Top results on a\nlarger network are similar to previously published results (99.42% maximum\nobserved accuracy vs 99.45% accuracy in our work). More importantly, in our\nwork a smaller ensemble is needed to achieve similar or better accuracy than\nprevious work, which translates into significantly decreased energy consumption\nfor both networks. Results of our work are stable since they do not require\nrandom sampling."
},{
    "category": "cs.NE", 
    "doi": "10.5121/csit.2014.4230", 
    "link": "http://arxiv.org/pdf/1606.00601v1", 
    "title": "On the performance of different mutation operators of a   subpopulation-based genetic algorithm for multi-robot task allocation   problems", 
    "arxiv-id": "1606.00601v1", 
    "author": "Andreas Kroll", 
    "publish": "2016-06-02T09:57:52Z", 
    "summary": "The performance of different mutation operators is usually evaluated in\nconjunc-tion with specific parameter settings of genetic algorithms and target\nproblems. Most studies focus on the classical genetic algorithm with different\nparameters or on solving unconstrained combinatorial optimization problems such\nas the traveling salesman problems. In this paper, a subpopulation-based\ngenetic al-gorithm that uses only mutation and selection is developed to solve\nmulti-robot task allocation problems. The target problems are constrained\ncombinatorial optimization problems, and are more complex if cooperative tasks\nare involved as these introduce additional spatial and temporal constraints.\nThe proposed genetic algorithm can obtain better solutions than classical\ngenetic algorithms with tournament selection and partially mapped crossover.\nThe performance of different mutation operators in solving problems\nwithout/with cooperative tasks is evaluated. The results imply that inversion\nmutation performs better than others when solving problems without cooperative\ntasks, and the swap-inversion combination performs better than others when\nsolving problems with cooperative tasks."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2017.01.088", 
    "link": "http://arxiv.org/pdf/1606.00802v3", 
    "title": "A Spiking Network that Learns to Extract Spike Signatures from Speech   Signals", 
    "arxiv-id": "1606.00802v3", 
    "author": "Anthony S Maida", 
    "publish": "2016-06-02T18:54:25Z", 
    "summary": "Spiking neural networks (SNNs) with adaptive synapses reflect core properties\nof biological neural networks. Speech recognition, as an application involving\naudio coding and dynamic learning, provides a good test problem to study SNN\nfunctionality. We present a simple, novel, and efficient nonrecurrent SNN that\nlearns to convert a speech signal into a spike train signature. The signature\nis distinguishable from signatures for other speech signals representing\ndifferent words, thereby enabling digit recognition and discrimination in\ndevices that use only spiking neurons. The method uses a small, nonrecurrent\nSNN consisting of Izhikevich neurons equipped with spike timing dependent\nplasticity (STDP) and biologically realistic synapses. This approach introduces\nan efficient and fast network without error-feedback training, although it does\nrequire supervised training. The new simulation results produce discriminative\nspike train patterns for spoken digits in which highly correlated spike trains\nbelong to the same category and low correlated patterns belong to different\ncategories. The proposed SNN is evaluated using a spoken digit recognition task\nwhere a subset of the Aurora speech dataset is used. The experimental results\nshow that the network performs well in terms of accuracy rate and complexity."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neucom.2017.01.088", 
    "link": "http://arxiv.org/pdf/1606.00825v2", 
    "title": "Training a Hidden Markov Model with a Bayesian Spiking Neural Network", 
    "arxiv-id": "1606.00825v2", 
    "author": "Anthony S Maida", 
    "publish": "2016-06-02T19:48:22Z", 
    "summary": "It is of some interest to understand how statistically based mechanisms for\nsignal processing might be integrated with biologically motivated mechanisms\nsuch as neural networks. This paper explores a novel hybrid approach for\nclassifying segments of sequential data, such as individual spoken works. The\napproach combines a hidden Markov model (HMM) with a spiking neural network\n(SNN). The HMM, consisting of states and transitions, forms a fixed backbone\nwith nonadaptive transition probabilities. The SNN, however, implements a\nbiologically based Bayesian computation that derives from the spike\ntiming-dependent plasticity (STDP) learning rule. The emission (observation)\nprobabilities of the HMM are represented in the SNN and trained with the STDP\nrule. A separate SNN, each with the same architecture, is associated with each\nof the states of the HMM. Because of the STDP training, each SNN implements an\nexpectation maximization algorithm to learn the emission probabilities for one\nHMM state. The model was studied on synthesized spike-train data and also on\nspoken word data. Preliminary results suggest its performance compares\nfavorably with other biologically motivated approaches. Because of the model's\nuniqueness and initial promise, it warrants further study. It provides some new\nideas on how the brain might implement the equivalent of an HMM in a neural\ncircuit."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.01102v2", 
    "title": "Acquisition of Visual Features Through Probabilistic   Spike-Timing-Dependent Plasticity", 
    "arxiv-id": "1606.01102v2", 
    "author": "Anthony S Maida", 
    "publish": "2016-06-03T14:30:38Z", 
    "summary": "The final version of this paper has been published in IEEEXplore available at\nhttp://ieeexplore.ieee.org/document/7727213. Please cite this paper as:\nAmirhossein Tavanaei, Timothee Masquelier, and Anthony Maida, Acquisition of\nvisual features through probabilistic spike-timing-dependent plasticity. IEEE\nInternational Joint Conference on Neural Networks. pp. 307-314, IJCNN 2016.\n  This paper explores modifications to a feedforward five-layer spiking\nconvolutional network (SCN) of the ventral visual stream [Masquelier, T.,\nThorpe, S., Unsupervised learning of visual features through spike timing\ndependent plasticity. PLoS Computational Biology, 3(2), 247-257]. The original\nmodel showed that a spike-timing-dependent plasticity (STDP) learning algorithm\nembedded in an appropriately selected SCN could perform unsupervised feature\ndiscovery. The discovered features where interpretable and could effectively be\nused to perform rapid binary decisions in a classifier. In order to study the\nrobustness of the previous results, the present research examines the effects\nof modifying some of the components of the original model. For improved\nbiological realism, we replace the original non-leaky integrate-and-fire\nneurons with Izhikevich-like neurons. We also replace the original STDP rule\nwith a novel rule that has a probabilistic interpretation. The probabilistic\nSTDP slightly but significantly improves the performance for both types of\nmodel neurons. Use of the Izhikevich-like neuron was not found to improve\nperformance although performance was still comparable to the IF neuron. This\nshows that the model is robust enough to handle more biologically realistic\nneurons. We also conclude that the underlying reasons for stable performance in\nthe model are preserved despite the overt changes to the explicit components of\nthe model."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.03674v2", 
    "title": "Critical Echo State Networks that Anticipate Input using Morphable   Transfer Functions", 
    "arxiv-id": "1606.03674v2", 
    "author": "Norbert Michael Mayer", 
    "publish": "2016-06-12T07:22:58Z", 
    "summary": "The paper investigates a new type of truly critical echo state networks where\nindividual transfer functions for every neuron can be modified to anticipate\nthe expected next input. Deviations from expected input are only forgotten\nslowly in power law fashion. The paper outlines the theory, numerically\nanalyzes a one neuron model network and finally discusses technical and also\nbiological implications of this type of approach."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.04306v1", 
    "title": "Viral Search algorithm", 
    "arxiv-id": "1606.04306v1", 
    "author": "Matteo Gardini", 
    "publish": "2016-06-14T10:55:53Z", 
    "summary": "The article, after a brief introduction on genetic algorithms and their\nfunctioning, presents a kind of genetic algorithm called Viral Search. We\npresent the key concepts, we formally derive the algorithm and we perform\nnumerical tests designed to illustrate the potential and limits."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.04466v1", 
    "title": "Neural Networks and Continuous Time", 
    "arxiv-id": "1606.04466v1", 
    "author": "Florian Ruh", 
    "publish": "2016-06-14T17:26:27Z", 
    "summary": "The fields of neural computation and artificial neural networks have\ndeveloped much in the last decades. Most of the works in these fields focus on\nimplementing and/or learning discrete functions or behavior. However,\ntechnical, physical, and also cognitive processes evolve continuously in time.\nThis cannot be described directly with standard architectures of artificial\nneural networks such as multi-layer feed-forward perceptrons. Therefore, in\nthis paper, we will argue that neural networks modeling continuous time are\nneeded explicitly for this purpose, because with them the synthesis and\nanalysis of continuous and possibly periodic processes in time are possible\n(e.g. for robot behavior) besides computing discrete classification functions\n(e.g. for logical reasoning). We will relate possible neural network\narchitectures with (hybrid) automata models that allow to express continuous\nprocesses."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.05169v1", 
    "title": "Learning from Non-Stationary Stream Data in Multiobjective Evolutionary   Algorithm", 
    "arxiv-id": "1606.05169v1", 
    "author": "Qingfu Zhang", 
    "publish": "2016-06-16T12:58:43Z", 
    "summary": "Evolutionary algorithms (EAs) have been well acknowledged as a promising\nparadigm for solving optimisation problems with multiple conflicting objectives\nin the sense that they are able to locate a set of diverse approximations of\nPareto optimal solutions in a single run. EAs drive the search for approximated\nsolutions through maintaining a diverse population of solutions and by\nrecombining promising solutions selected from the population. Combining machine\nlearning techniques has shown great potentials since the intrinsic structure of\nthe Pareto optimal solutions of an multiobjective optimisation problem can be\nlearned and used to guide for effective recombination. However, existing\nmultiobjective EAs (MOEAs) based on structure learning spend too much\ncomputational resources on learning. To address this problem, we propose to use\nan online learning scheme. Based on the fact that offsprings along evolution\nare streamy, dependent and non-stationary (which implies that the intrinsic\nstructure, if any, is temporal and scale-variant), an online agglomerative\nclustering algorithm is applied to adaptively discover the intrinsic structure\nof the Pareto optimal solution set; and to guide effective offspring\nrecombination. Experimental results have shown significant improvement over\nfive state-of-the-art MOEAs on a set of well-known benchmark problems with\ncomplicated Pareto sets and complex Pareto fronts."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.05784v2", 
    "title": "Hitting times of local and global optima in genetic algorithms with very   high selection pressure", 
    "arxiv-id": "1606.05784v2", 
    "author": "Anton Eremeev", 
    "publish": "2016-06-18T17:36:28Z", 
    "summary": "The paper is devoted to upper bounds on the expected first hitting times of\nthe sets of local or global optima for non-elitist genetic algorithms with very\nhigh selection pressure. The results of this paper extend the range of\nsituations where the upper bounds on the expected runtime are known for genetic\nalgorithms and apply, in particular, to the Canonical Genetic Algorithm. The\nobtained bounds do not require the probability of fitness-decreasing mutation\nto be bounded by a constant less than one."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.06216v3", 
    "title": "Neural networks with differentiable structure", 
    "arxiv-id": "1606.06216v3", 
    "author": "Thomas Miconi", 
    "publish": "2016-06-20T17:29:01Z", 
    "summary": "While gradient descent has proven highly successful in learning connection\nweights for neural networks, the actual structure of these networks is usually\ndetermined by hand, or by other optimization algorithms. Here we describe a\nsimple method to make network structure differentiable, and therefore\naccessible to gradient descent. We test this method on recurrent neural\nnetworks applied to simple sequence prediction problems. Starting with initial\nnetworks containing only one node, the method automatically builds networks\nthat successfully solve the tasks. The number of nodes in the final network\ncorrelates with task difficulty. The method can dynamically increase network\nsize in response to an abrupt complexification in the task; however, reduction\nin network size in response to task simplification is not evident for\nreasonable meta-parameters. The method does not penalize network performance\nfor these test tasks: variable-size networks actually reach better performance\nthan fixed-size networks of higher, lower or identical size. We conclude by\ndiscussing how this method could be applied to more complex networks, such as\nfeedforward layered networks, or multiple-area networks of arbitrary shape."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1606.06818v1", 
    "title": "Evolutionary computation for multicomponent problems: opportunities and   future directions", 
    "arxiv-id": "1606.06818v1", 
    "author": "Markus Wagner", 
    "publish": "2016-06-22T05:08:52Z", 
    "summary": "Over the past 30 years many researchers in the field of evolutionary\ncomputation have put a lot of effort to introduce various approaches for\nsolving hard problems. Most of these problems have been inspired by major\nindustries so that solving them, by providing either optimal or near optimal\nsolution, was of major significance. Indeed, this was a very promising\ntrajectory as advances in these problem-solving approaches could result in\nadding values to major industries. In this paper we revisit this trajectory to\nfind out whether the attempts that started three decades ago are still aligned\nwith the same goal, as complexities of real-world problems increased\nsignificantly. We present some examples of modern real-world problems, discuss\nwhy they might be difficult to solve, and whether there is any mismatch between\nthese examples and the problems that are investigated in the evolutionary\ncomputation area."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.01691v1", 
    "title": "A Modified Activation Function with Improved Run-Times For Neural   Networks", 
    "arxiv-id": "1607.01691v1", 
    "author": "Emmanuel Ndidi Osegi", 
    "publish": "2016-07-06T16:05:52Z", 
    "summary": "In this paper we present a modified version of the Hyperbolic Tangent\nActivation Function as a learning unit generator for neural networks. The\nfunction uses an integer calibration constant as an approximation to the Euler\nnumber, e, based on a quadratic Real Number Formula (RNF) algorithm and an\nadaptive normalization constraint on the input activations to avoid the\nvanishing gradient. We demonstrate the effectiveness of the proposed\nmodification using a hypothetical and real world dataset and show that lower\nrun-times can be achieved by learning algorithms using this function leading to\nimproved speed-ups and learning accuracies during training."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.02717v1", 
    "title": "The BioDynaMo Project", 
    "arxiv-id": "1607.02717v1", 
    "author": "Max Talanov", 
    "publish": "2016-07-10T08:54:30Z", 
    "summary": "Computer simulations have become a very powerful tool for scientific\nresearch. Given the vast complexity that comes with many open scientific\nquestions, a purely analytical or experimental approach is often not viable.\nFor example, biological systems (such as the human brain) comprise an extremely\ncomplex organization and heterogeneous interactions across different spatial\nand temporal scales. In order to facilitate research on such problems, the\nBioDynaMo project (\\url{https://biodynamo.web.cern.ch/}) aims at a general\nplatform for computer simulations for biological research. Since the scientific\ninvestigations require extensive computer resources, this platform should be\nexecutable on hybrid cloud computing systems, allowing for the efficient use of\nstate-of-the-art computing technology. This paper describes challenges during\nthe early stages of the software development process. In particular, we\ndescribe issues regarding the implementation and the highly interdisciplinary\nas well as international nature of the collaboration. Moreover, we explain the\nmethodologies, the approach, and the lessons learnt by the team during these\nfirst stages."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.03070v2", 
    "title": "Forward Table-Based Presynaptic Event-Triggered Spike-Timing-Dependent   Plasticity", 
    "arxiv-id": "1607.03070v2", 
    "author": "Gert Cauwenberghs", 
    "publish": "2016-07-11T18:37:12Z", 
    "summary": "Spike-timing-dependent plasticity (STDP) incurs both causal and acausal\nsynaptic weight updates, for negative and positive time differences between\npre-synaptic and post-synaptic spike events. For realizing such updates in\nneuromorphic hardware, current implementations either require forward and\nreverse lookup access to the synaptic connectivity table, or rely on\nmemory-intensive architectures such as crossbar arrays. We present a novel\nmethod for realizing both causal and acausal weight updates using only forward\nlookup access of the synaptic connectivity table, permitting memory-efficient\nimplementation. A simplified implementation in FPGA, using a single timer\nvariable for each neuron, closely approximates exact STDP cumulative weight\nupdates for neuron refractory periods greater than 10 ms, and reduces to exact\nSTDP for refractory periods greater than the STDP time window. Compared to\nconventional crossbar implementation, the forward table-based implementation\nleads to substantial memory savings for sparsely connected networks supporting\nscalable neuromorphic systems with fully reconfigurable synaptic connectivity\nand plasticity."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.04063v2", 
    "title": "Update Strength in EDAs and ACO: How to Avoid Genetic Drift", 
    "arxiv-id": "1607.04063v2", 
    "author": "Carsten Witt", 
    "publish": "2016-07-14T10:11:59Z", 
    "summary": "We provide a rigorous runtime analysis concerning the update strength, a\nvital parameter in probabilistic model-building GAs such as the step size $1/K$\nin the compact Genetic Algorithm (cGA) and the evaporation factor $\\rho$ in\nACO. While a large update strength is desirable for exploitation, there is a\ngeneral trade-off: too strong updates can lead to genetic drift and poor\nperformance. We demonstrate this trade-off for the cGA and a simple MMAS ACO\nalgorithm on the OneMax function. More precisely, we obtain lower bounds on the\nexpected runtime of $\\Omega(K\\sqrt{n} + n \\log n)$ and $\\Omega(\\sqrt{n}/\\rho +\nn \\log n)$, respectively, showing that the update strength should be limited to\n$1/K, \\rho = O(1/(\\sqrt{n} \\log n))$. In fact, choosing $1/K, \\rho \\sim\n1/(\\sqrt{n}\\log n)$ both algorithms efficiently optimize OneMax in expected\ntime $O(n \\log n)$. Our analyses provide new insights into the stochastic\nbehavior of probabilistic model-building GAs and propose new guidelines for\nsetting the update strength in global optimization."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.04267v1", 
    "title": "Enhanced Boolean Correlation Matrix Memory", 
    "arxiv-id": "1607.04267v1", 
    "author": "Mario Mastriani", 
    "publish": "2016-07-11T16:22:47Z", 
    "summary": "This paper introduces an Enhanced Boolean version of the Correlation Matrix\nMemory (CMM), which is useful to work with binary memories. A novel Boolean\nOrthonormalization Process (BOP) is presented to convert a non-orthonormal\nBoolean basis, i.e., a set of non-orthonormal binary vectors (in a Boolean\nsense) to an orthonormal Boolean basis, i.e., a set of orthonormal binary\nvectors (in a Boolean sense). This work shows that it is possible to improve\nthe performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm\nhas a lot of additional fields of applications, e.g.: Steganography, Hopfield\nNetworks, Bi-level image processing, etc. Finally, it is important to mention\nthat the BOP is an extremely stable and fast algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.05213v1", 
    "title": "mpEAd: Multi-Population EA Diagrams", 
    "arxiv-id": "1607.05213v1", 
    "author": "Mark Wineberg", 
    "publish": "2016-07-18T17:33:34Z", 
    "summary": "Multi-population evolutionary algorithms are, by nature, highly complex and\ndifficult to describe. Even two populations working in concert (or opposition)\npresent a myriad of potential configurations that are often difficult to relate\nusing text alone. Little effort has been made, however, to depict these kinds\nof systems, relying solely on the simple structural connections (related using\nad hoc diagrams) between populations and often leaving out crucial details. In\nthis paper, we propose a notation and accompanying formalism for consistently\nand powerfully depicting these structures and the relationships within them in\nan intuitive and consistent way. Using our notation, we examine simple\nco-evolutionary systems and discover new configurations by the simple process\nof \"drawing on a whiteboard\". Finally, we demonstrate that even complex,\nhighly-interconnected systems with large numbers of populations can be\nunderstood with ease using the advanced features of our formalism"
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.05390v1", 
    "title": "Genetic Transfer or Population Diversification? Deciphering the Secret   Ingredients of Evolutionary Multitask Optimization", 
    "arxiv-id": "1607.05390v1", 
    "author": "Yew-Soon Ong", 
    "publish": "2016-07-19T03:33:06Z", 
    "summary": "Evolutionary multitasking has recently emerged as a novel paradigm that\nenables the similarities and/or latent complementarities (if present) between\ndistinct optimization tasks to be exploited in an autonomous manner simply by\nsolving them together with a unified solution representation scheme. An\nimportant matter underpinning future algorithmic advancements is to develop a\nbetter understanding of the driving force behind successful multitask\nproblem-solving. In this regard, two (seemingly disparate) ideas have been put\nforward, namely, (a) implicit genetic transfer as the key ingredient\nfacilitating the exchange of high-quality genetic material across tasks, and\n(b) population diversification resulting in effective global search of the\nunified search space encompassing all tasks. In this paper, we present some\nempirical results that provide a clearer picture of the relationship between\nthe two aforementioned propositions. For the numerical experiments we make use\nof Sudoku puzzles as case studies, mainly because of their feature that\noutwardly unlike puzzle statements can often have nearly identical final\nsolutions. The experiments reveal that while on many occasions genetic transfer\nand population diversity may be viewed as two sides of the same coin, the wider\nimplication of genetic transfer, as shall be shown herein, captures the true\nessence of evolutionary multitasking to the fullest."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.05690v1", 
    "title": "Stochastic Backpropagation through Mixture Density Distributions", 
    "arxiv-id": "1607.05690v1", 
    "author": "Alex Graves", 
    "publish": "2016-07-19T18:37:00Z", 
    "summary": "The ability to backpropagate stochastic gradients through continuous latent\ndistributions has been crucial to the emergence of variational autoencoders and\nstochastic gradient variational Bayes. The key ingredient is an unbiased and\nlow-variance way of estimating gradients with respect to distribution\nparameters from gradients evaluated at distribution samples. The\n\"reparameterization trick\" provides a class of transforms yielding such\nestimators for many continuous distributions, including the Gaussian and other\nmembers of the location-scale family. However the trick does not readily extend\nto mixture density models, due to the difficulty of reparameterizing the\ndiscrete distribution over mixture weights. This report describes an\nalternative transform, applicable to any continuous multivariate distribution\nwith a differentiable density function from which samples can be drawn, and\nuses it to derive an unbiased estimator for mixture density weight derivatives.\nCombined with the reparameterization trick applied to the individual mixture\ncomponents, this estimator makes it straightforward to train variational\nautoencoders with mixture-distributed latent variables, or to perform\nstochastic variational inference with a mixture density variational posterior."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1607.07078v1", 
    "title": "Effective Connectivity-Based Neural Decoding: A Causal   Interaction-Driven Approach", 
    "arxiv-id": "1607.07078v1", 
    "author": "Hamid Krim", 
    "publish": "2016-07-24T18:44:54Z", 
    "summary": "We propose a geometric model-free causality measurebased on multivariate\ndelay embedding that can efficiently detect linear and nonlinear causal\ninteractions between time series with no prior information. We then exploit the\nproposed causal interaction measure in real MEG data analysis. The results are\nused to construct effective connectivity maps of brain activity to decode\ndifferent categories of visual stimuli. Moreover, we discovered that the\nMEG-based effective connectivity maps as a response to structured images\nexhibit more geometric patterns, as disclosed by analyzing the evolution of\ntoplogical structures of the underlying networks using persistent homology.\nExtensive simulation and experimental result have been carried out to\nsubstantiate the capabilities of the proposed approach."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.00138v1", 
    "title": "Heterogeneous Strategy Particle Swarm Optimization", 
    "arxiv-id": "1608.00138v1", 
    "author": "Xian-Bin Cao", 
    "publish": "2016-07-30T16:06:18Z", 
    "summary": "PSO is a widely recognized optimization algorithm inspired by social swarm.\nIn this brief we present a heterogeneous strategy particle swarm optimization\n(HSPSO), in which a proportion of particles adopt a fully informed strategy to\nenhance the converging speed while the rest are singly informed to maintain the\ndiversity. Our extensive numerical experiments show that HSPSO algorithm is\nable to obtain satisfactory solutions, outperforming both PSO and the fully\ninformed PSO. The evolution process is examined from both structural and\nmicroscopic points of view. We find that the cooperation between two types of\nparticles can facilitate a good balance between exploration and exploitation,\nyielding better performance. We demonstrate the applicability of HSPSO on the\nfilter design problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.01783v1", 
    "title": "The Evolutionary Process of Image Transition in Conjunction with Box and   Strip Mutation", 
    "arxiv-id": "1608.01783v1", 
    "author": "Frank Neumann", 
    "publish": "2016-08-05T07:15:38Z", 
    "summary": "Evolutionary algorithms have been used in many ways to generate digital art.\nWe study how evolutionary processes are used for evolutionary art and present a\nnew approach to the transition of images. Our main idea is to define\nevolutionary processes for digital image transition, combining different\nvariants of mutation and evolutionary mechanisms. We introduce box and strip\nmutation operators which are specifically designed for image transition. Our\nexperimental results show that the process of an evolutionary algorithm in\ncombination with these mutation operators can be used as a valuable way to\nproduce unique generative art."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.01818v1", 
    "title": "The BioDynaMo Project: a platform for computer simulations of biological   dynamics", 
    "arxiv-id": "1608.01818v1", 
    "author": "Max Talanov", 
    "publish": "2016-08-05T09:55:59Z", 
    "summary": "This paper is a brief update on developments in the BioDynaMo project, a new\nplatform for computer simulations for biological research. We will discuss the\nnew capabilities of the simulator, important new concepts simulation\nmethodology as well as its numerous applications to the computational biology\nand nanoscience communities."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.03757v2", 
    "title": "Student's t Distribution based Estimation of Distribution Algorithms for   Derivative-free Global Optimization", 
    "arxiv-id": "1608.03757v2", 
    "author": "Yuhui Shi", 
    "publish": "2016-08-12T11:36:18Z", 
    "summary": "In this paper, we are concerned with a branch of evolutionary algorithms\ntermed estimation of distribution (EDA), which has been successfully used to\ntackle derivative-free global optimization problems. For existent EDA\nalgorithms, it is a common practice to use a Gaussian distribution or a mixture\nof Gaussian components to represent the statistical property of available\npromising solutions found so far. Observing that the Student's t distribution\nhas heavier and longer tails than the Gaussian, which may be beneficial for\nexploring the solution space, we propose a novel EDA algorithm termed ESTDA, in\nwhich the Student's t distribution, rather than Gaussian, is employed. To\naddress hard multimodal and deceptive problems, we extend ESTDA further by\nsubstituting a single Student's t distribution with a mixture of Student's t\ndistributions. The resulting algorithm is named as estimation of mixture of\nStudent's t distribution algorithm (EMSTDA). Both ESTDA and EMSTDA are\nevaluated through extensive and in-depth numerical experiments using over a\ndozen of benchmark objective functions. Empirical results demonstrate that the\nproposed algorithms provide remarkably better performance than their Gaussian\ncounterparts."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.06514v2", 
    "title": "Dynamic Multi-Objectives Optimization with a Changing Number of   Objectives", 
    "arxiv-id": "1608.06514v2", 
    "author": "Xin Yao", 
    "publish": "2016-08-23T13:57:54Z", 
    "summary": "Existing studies on dynamic multi-objective optimization focus on problems\nwith time-dependent objective functions, while the ones with a changing number\nof objectives have rarely been considered in the literature. Instead of\nchanging the shape or position of the Pareto-optimal front/set when having\ntime-dependent objective functions, increasing or decreasing the number of\nobjectives usually leads to the expansion or contraction of the dimension of\nthe Pareto-optimal front/set manifold. Unfortunately, most existing dynamic\nhandling techniques can hardly be adapted to this type of dynamics. In this\npaper, we report our attempt toward tackling the dynamic multi-objective\noptimization problems with a changing number of objectives. We implement a new\ntwo-archive evolutionary algorithm which maintains two co-evolving populations\nsimultaneously. In particular, these two populations are complementary to each\nother: one concerns more about the convergence while the other concerns more\nabout the diversity. The compositions of these two populations are adaptively\nreconstructed once the environment changes. In addition, these two populations\ninteract with each other via a mating selection mechanism. Comprehensive\nexperiments are conducted on various benchmark problems with a time-dependent\nnumber of objectives. Empirical results fully demonstrate the effectiveness of\nour proposed algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.06902v2", 
    "title": "Recurrent Neural Networks With Limited Numerical Precision", 
    "arxiv-id": "1608.06902v2", 
    "author": "Yoshua Bengio", 
    "publish": "2016-08-24T17:15:29Z", 
    "summary": "Recurrent Neural Networks (RNNs) produce state-of-art performance on many\nmachine learning tasks but their demand on resources in terms of memory and\ncomputational power are often high. Therefore, there is a great interest in\noptimizing the computations performed with these models especially when\nconsidering development of specialized low-power hardware for deep networks.\nOne way of reducing the computational needs is to limit the numerical precision\nof the network weights and biases. This has led to different proposed rounding\nmethods which have been applied so far to only Convolutional Neural Networks\nand Fully-Connected Networks. This paper addresses the question of how to best\nreduce weight precision during training in the case of RNNs. We present results\nfrom the use of different stochastic and deterministic reduced precision\ntraining methods applied to three major RNN types which are then tested on\nseveral datasets. The results show that the weight binarization methods do not\nwork with the RNNs. However, the stochastic and deterministic ternarization,\nand pow2-ternarization methods gave rise to low-precision RNNs that produce\nsimilar and even higher accuracy on certain datasets therefore providing a path\ntowards training more efficient implementations of RNNs in specialized\nhardware."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.08265v1", 
    "title": "About Learning in Recurrent Bistable Gradient Networks", 
    "arxiv-id": "1608.08265v1", 
    "author": "S. Lackner", 
    "publish": "2016-08-29T22:02:39Z", 
    "summary": "Recurrent Bistable Gradient Networks are attractor based neural networks\ncharacterized by bistable dynamics of each single neuron. Coupled together\nusing linear interaction determined by the interconnection weights, these\nnetworks do not suffer from spurious states or very limited capacity anymore.\nVladimir Chinarov and Michael Menzinger, who invented these networks, trained\nthem using Hebb's learning rule. We show, that this way of computing the\nweights leads to unwanted behaviour and limitations of the networks\ncapabilities. Furthermore we evince, that using the first order of Hintons\nContrastive Divergence algorithm leads to a quite promising recurrent neural\nnetwork. These findings are tested by learning images of the MNIST database for\nhandwritten numbers."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.08607v2", 
    "title": "Matching-Based Selection with Incomplete Lists for Decomposition   Multi-Objective Optimization", 
    "arxiv-id": "1608.08607v2", 
    "author": "Qingfu Zhang", 
    "publish": "2016-08-30T19:30:25Z", 
    "summary": "The balance between convergence and diversity is a key issue of evolutionary\nmulti-objective optimization. The recently proposed stable matching-based\nselection provides a new perspective to handle this balance under the framework\nof decomposition multi-objective optimization. In particular, the stable\nmatching between subproblems and solutions, which achieves an equilibrium\nbetween their mutual preferences, implicitly strikes a balance between the\nconvergence and diversity. Nevertheless, the original stable matching model has\na high risk of matching a solution with a unfavorable subproblem which finally\nleads to an imbalanced selection result. In this paper, we propose an adaptive\ntwo-level stable matching-based selection for decomposition multi-objective\noptimization. Specifically, borrowing the idea of stable matching with\nincomplete lists, we match each solution with one of its favorite subproblems\nby restricting the length of its preference list during the first-level stable\nmatching. During the second-level stable matching, the remaining subproblems\nare thereafter matched with their favorite solutions according to the classic\nstable matching model. In particular, we develop an adaptive mechanism to\nautomatically set the length of preference list for each solution according to\nits local competitiveness. The performance of our proposed method is validated\nand compared with several state-of-the-art evolutionary multi-objective\noptimization algorithms on 62 benchmark problem instances. Empirical results\nfully demonstrate the competitive performance of our proposed method on\nproblems with complicated Pareto sets and those with more than three\nobjectives."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1608.08782v1", 
    "title": "Training Deep Spiking Neural Networks using Backpropagation", 
    "arxiv-id": "1608.08782v1", 
    "author": "Michael Pfeiffer", 
    "publish": "2016-08-31T09:21:17Z", 
    "summary": "Deep spiking neural networks (SNNs) hold great potential for improving the\nlatency and energy efficiency of deep neural networks through event-based\ncomputation. However, training such networks is difficult due to the\nnon-differentiable nature of asynchronous spike events. In this paper, we\nintroduce a novel technique, which treats the membrane potentials of spiking\nneurons as differentiable signals, where discontinuities at spike times are\nonly considered as noise. This enables an error backpropagation mechanism for\ndeep SNNs, which works directly on spike signals and membrane potentials. Thus,\ncompared with previous methods relying on indirect training and conversion, our\ntechnique has the potential to capture the statics of spikes more precisely.\nOur novel framework outperforms all previously reported results for SNNs on the\npermutation invariant MNIST benchmark, as well as the N-MNIST benchmark\nrecorded with event-based vision sensors."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1609.01982v1", 
    "title": "Uniform Transformation of Non-Separable Probability Distributions", 
    "arxiv-id": "1609.01982v1", 
    "author": "Eric Kee", 
    "publish": "2016-08-16T02:18:33Z", 
    "summary": "A theoretical framework is developed to describe the transformation that\ndistributes probability density functions uniformly over space. In one\ndimension, the cumulative distribution can be used, but does not generalize to\nhigher dimensions, or non-separable distributions. A potential function is\nshown to link probability density functions to their transformation, and to\ngeneralize the cumulative. A numerical method is developed to compute the\npotential, and examples are shown in two dimensions."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2016.7727213", 
    "link": "http://arxiv.org/pdf/1609.02053v1", 
    "title": "Fast and Efficient Asynchronous Neural Computation with Adapting Spiking   Neural Networks", 
    "arxiv-id": "1609.02053v1", 
    "author": "Sander M. Bohte", 
    "publish": "2016-09-07T16:30:01Z", 
    "summary": "Biological neurons communicate with a sparing exchange of pulses - spikes. It\nis an open question how real spiking neurons produce the kind of powerful\nneural computation that is possible with deep artificial neural networks, using\nonly so very few spikes to communicate. Building on recent insights in\nneuroscience, we present an Adapting Spiking Neural Network (ASNN) based on\nadaptive spiking neurons. These spiking neurons efficiently encode information\nin spike-trains using a form of Asynchronous Pulsed Sigma-Delta coding while\nhomeostatically optimizing their firing rate. In the proposed paradigm of\nspiking neuron computation, neural adaptation is tightly coupled to synaptic\nplasticity, to ensure that downstream neurons can correctly decode upstream\nspiking neurons. We show that this type of network is inherently able to carry\nout asynchronous and event-driven neural computation, while performing\nidentical to corresponding artificial neural networks (ANNs). In particular, we\nshow that these adaptive spiking neurons can be drop in replacements for ReLU\nneurons in standard feedforward ANNs comprised of such units. We demonstrate\nthat this can also be successfully applied to a ReLU based deep convolutional\nneural network for classifying the MNIST dataset. The ASNN thus outperforms\ncurrent Spiking Neural Networks (SNNs) implementations, while responding (up\nto) an order of magnitude faster and using an order of magnitude fewer spikes.\nAdditionally, in a streaming setting where frames are continuously classified,\nwe show that the ASNN requires substantially fewer network updates as compared\nto the corresponding ANN."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1609.04846v1", 
    "title": "A Tutorial about Random Neural Networks in Supervised Learning", 
    "arxiv-id": "1609.04846v1", 
    "author": "Gerardo Rubino", 
    "publish": "2016-09-15T20:21:30Z", 
    "summary": "Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can\nalso be seen as a specific type of queuing network. They have been successfully\nused in several domains during the last 25 years, as queuing networks to\nanalyze the performance of resource sharing in many engineering areas, as\nlearning tools and in combinatorial optimization, where they are seen as neural\nsystems, and also as models of neurological aspects of living beings. In this\narticle we focus on their learning capabilities, and more specifically, we\npresent a practical guide for using the RNN to solve supervised learning\nproblems. We give a general description of these models using almost\nindistinctly the terminology of Queuing Theory and the neural one. We present\nthe standard learning procedures used by RNNs, adapted from similar\nwell-established improvements in the standard NN field. We describe in\nparticular a set of learning algorithms covering techniques based on the use of\nfirst order and, then, of second order derivatives. We also discuss some issues\nrelated to these objects and present new perspectives about their use in\nsupervised learning problems. The tutorial describes their most relevant\napplications, and also provides a large bibliography."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1609.05132v4", 
    "title": "Low Complexity Multiply Accumulate Unit for Weight-Sharing Convolutional   Neural Networks", 
    "arxiv-id": "1609.05132v4", 
    "author": "David Gregg", 
    "publish": "2016-08-30T13:41:41Z", 
    "summary": "Convolutional Neural Networks (CNNs) are one of the most successful deep\nmachine learning technologies for processing image, voice and video data. CNNs\nrequire large amounts of processing capacity and memory, which can exceed the\nresources of low power mobile and embedded systems. Several designs for\nhardware accelerators have been proposed for CNNs which typically contain large\nnumbers of Multiply Accumulate (MAC) units. One approach to reducing data sizes\nand memory traffic in CNN accelerators is \"weight sharing\", where the full\nrange of values in a trained CNN are put in bins and the bin index is stored\ninstead of the original weight value. In this paper we propose a novel MAC\ncircuit that exploits binning in weight-sharing CNNs. Rather than computing the\nMAC directly we instead count the frequency of each weight and place it in a\nbin. We then compute the accumulated value in a subsequent multiply phase. This\nallows hardware multipliers in the MAC circuit to be replaced with adders and\nselection logic. Experiments show that for the same clock speed our approach\nresults in fewer gates, smaller logic, and reduced power."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1609.07722v1", 
    "title": "Sooner than Expected: Hitting the Wall of Complexity in Evolution", 
    "arxiv-id": "1609.07722v1", 
    "author": "Heiko Hamann", 
    "publish": "2016-09-25T10:00:41Z", 
    "summary": "In evolutionary robotics an encoding of the control software, which maps\nsensor data (input) to motor control values (output), is shaped by stochastic\noptimization methods to complete a predefined task. This approach is assumed to\nbe beneficial compared to standard methods of controller design in those cases\nwhere no a-priori model is available that could help to optimize performance.\nAlso for robots that have to operate in unpredictable environments, an\nevolutionary robotics approach is favorable. We demonstrate here that such a\nmodel-free approach is not a free lunch, as already simple tasks can represent\nunsolvable barriers for fully open-ended uninformed evolutionary computation\ntechniques. We propose here the 'Wankelmut' task as an objective for an\nevolutionary approach that starts from scratch without pre-shaped controller\nsoftware or any other informed approach that would force the behavior to be\nevolved in a desired way. Our focal claim is that 'Wankelmut' represents the\nsimplest set of problems that makes plain-vanilla evolutionary computation\nfail. We demonstrate this by a series of simple standard evolutionary\napproaches using different fitness functions and standard artificial neural\nnetworks as well as continuous-time recurrent neural networks. All our tested\napproaches failed. We claim that any other evolutionary approach will also fail\nthat does per-se not favor or enforce modularity and does not freeze or protect\nalready evolved functionalities. Thus we propose a hard-to-pass benchmark and\nmake a strong statement for self-complexifying and generative approaches in\nevolutionary computation. We anticipate that defining such a 'simplest task to\nfail' is a valuable benchmark for promoting future development in the field of\nartificial intelligence, evolutionary robotics and artificial life."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1609.08414v1", 
    "title": "Reactive Collision Avoidance using Evolutionary Neural Networks", 
    "arxiv-id": "1609.08414v1", 
    "author": "Mohamed Moustafa", 
    "publish": "2016-09-27T13:26:10Z", 
    "summary": "Collision avoidance systems can play a vital role in reducing the number of\naccidents and saving human lives. In this paper, we introduce and validate a\nnovel method for vehicles reactive collision avoidance using evolutionary\nneural networks (ENN). A single front-facing rangefinder sensor is the only\ninput required by our method. The training process and the proposed method\nanalysis and validation are carried out using simulation. Extensive experiments\nare conducted to analyse the proposed method and evaluate its performance.\nFirstly, we experiment the ability to learn collision avoidance in a static\nfree track. Secondly, we analyse the effect of the rangefinder sensor\nresolution on the learning process. Thirdly, we experiment the ability of a\nvehicle to individually and simultaneously learn collision avoidance. Finally,\nwe test the generality of the proposed method. We used a more realistic and\npowerful simulation environment (CarMaker), a camera as an alternative input\nsensor, and lane keeping as an extra feature to learn. The results are\nencouraging; the proposed method successfully allows vehicles to learn\ncollision avoidance in different scenarios that are unseen during training. It\nalso generalizes well if any of the input sensor, the simulator, or the task to\nbe learned is changed."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1609.09158v1", 
    "title": "Proposal for a Leaky-Integrate-Fire Spiking Neuron based on   Magneto-Electric Switching of Ferro-magnets", 
    "arxiv-id": "1609.09158v1", 
    "author": "Kaushik Roy", 
    "publish": "2016-09-29T00:03:50Z", 
    "summary": "The efficiency of the human brain in performing classification tasks has\nattracted considerable research interest in brain-inspired neuromorphic\ncomputing. Hardware implementations of a neuromorphic system aims to mimic the\ncomputations in the brain through interconnection of neurons and synaptic\nweights. A leaky-integrate-fire (LIF) spiking model is widely used to emulate\nthe dynamics of neuronal action potentials. In this work, we propose a spin\nbased LIF spiking neuron using the magneto-electric (ME) switching of\nferro-magnets. The voltage across the ME oxide exhibits a typical\nleaky-integrate behavior, which in turn switches an underlying ferro-magnet.\nDue to the effect of thermal noise, the ferro-magnet exhibits probabilistic\nswitching dynamics, which is reminiscent of the stochasticity exhibited by\nbiological neurons. The energy-efficiency of the ME switching mechanism coupled\nwith the intrinsic non-volatility of ferro-magnets result in lower energy\nconsumption, when compared to a CMOS LIF neuron. A device to system-level\nsimulation framework has been developed to investigate the feasibility of the\nproposed LIF neuron for a hand-written digit recognition problem"
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.00790v1", 
    "title": "Adaptive Neuron Apoptosis for Accelerating Deep Learning on Large Scale   Systems", 
    "arxiv-id": "1610.00790v1", 
    "author": "Abhinav Vishnu", 
    "publish": "2016-10-03T23:23:34Z", 
    "summary": "We present novel techniques to accelerate the convergence of Deep Learning\nalgorithms by conducting low overhead removal of redundant neurons -- apoptosis\nof neurons -- which do not contribute to model learning, during the training\nphase itself. We provide in-depth theoretical underpinnings of our heuristics\n(bounding accuracy loss and handling apoptosis of several neuron types), and\npresent the methods to conduct adaptive neuron apoptosis. Specifically, we are\nable to improve the training time for several datasets by 2-3x, while reducing\nthe number of parameters by up to 30x (4-5x on average) on datasets such as\nImageNet classification. For the Higgs Boson dataset, our implementation\nimproves the accuracy (measured by Area Under Curve (AUC)) for classification\nfrom 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in\ncomparison to existing literature. The proposed methods achieve a 2.44x speedup\nin comparison to the default (no apoptosis) algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.01430v2", 
    "title": "LAYERS: Yet another Neural Network toolkit", 
    "arxiv-id": "1610.01430v2", 
    "author": "Jos\u00e9-Miguel Bened\u00ed", 
    "publish": "2016-10-05T14:14:51Z", 
    "summary": "Layers is an open source neural network toolkit aim at providing an easy way\nto implement modern neural networks. The main user target are students and to\nthis end layers provides an easy scriptting language that can be early adopted.\nThe user has to focus only on design details as network totpology and parameter\ntunning."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.01439v1", 
    "title": "Nonlinear Systems Identification Using Deep Dynamic Neural Networks", 
    "arxiv-id": "1610.01439v1", 
    "author": "Nicholas Gans", 
    "publish": "2016-10-05T14:26:27Z", 
    "summary": "Neural networks are known to be effective function approximators. Recently,\ndeep neural networks have proven to be very effective in pattern recognition,\nclassification tasks and human-level control to model highly nonlinear\nrealworld systems. This paper investigates the effectiveness of deep neural\nnetworks in the modeling of dynamical systems with complex behavior. Three deep\nneural network structures are trained on sequential data, and we investigate\nthe effectiveness of these networks in modeling associated characteristics of\nthe underlying dynamical systems. We carry out similar evaluations on select\npublicly available system identification datasets. We demonstrate that deep\nneural networks are effective model estimators from input-output data"
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.02732v1", 
    "title": "Investigating the effects Diversity Mechanisms have on Evolutionary   Algorithms in Dynamic Environments", 
    "arxiv-id": "1610.02732v1", 
    "author": "Matthew Hughes", 
    "publish": "2016-10-09T22:16:32Z", 
    "summary": "Evolutionary algorithms have been successfully applied to a variety of\noptimisation problems in stationary environments. However, many real world\noptimisation problems are set in dynamic environments where the success\ncriteria shifts regularly. Population diversity affects algorithmic\nperformance, particularly on multiobjective and dynamic problems. Diversity\nmechanisms are methods of altering evolutionary algorithms in a way that\npromotes the maintenance of population diversity. This project intends to\nmeasure and compare the performance effect a variety of diversity mechanisms\nhave on an evolutionary algorithm when facing an assortment of dynamic\nproblems."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.05231v1", 
    "title": "Evolving the Structure of Evolution Strategies", 
    "arxiv-id": "1610.05231v1", 
    "author": "Thomas B\u00e4ck", 
    "publish": "2016-10-17T17:56:28Z", 
    "summary": "Various variants of the well known Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES) have been proposed recently, which improve the empirical\nperformance of the original algorithm by structural modifications. However, in\npractice it is often unclear which variation is best suited to the specific\noptimization problem at hand. As one approach to tackle this issue, algorithmic\nmechanisms attached to CMA-ES variants are considered and extracted as\nfunctional \\emph{modules}, allowing for combinations of them. This leads to a\nconfiguration space over ES structures, which enables the exploration of\nalgorithm structures and paves the way toward novel algorithm generation.\nSpecifically, eleven modules are incorporated in this framework with two or\nthree alternative configurations for each module, resulting in $4\\,608$\nalgorithms. A self-adaptive Genetic Algorithm (GA) is used to efficiently\nevolve effective ES-structures for given classes of optimization problems,\noutperforming any classical CMA-ES variants from literature. The proposed\napproach is evaluated on noiseless functions from BBOB suite. Furthermore, such\nan observation is again confirmed on different function groups and\ndimensionality, indicating the feasibility of ES configuration on real-world\nproblem classes."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.05729v1", 
    "title": "Scaling Up MAP-Elites Using Centroidal Voronoi Tessellations", 
    "arxiv-id": "1610.05729v1", 
    "author": "Jean-Baptiste Mouret", 
    "publish": "2016-10-18T18:18:47Z", 
    "summary": "The recently introduced Multi-dimensional Archive of Phenotypic Elites\n(MAP-Elites) is an evolutionary algorithm capable of producing a large archive\nof diverse, high-performing solutions in a single run. It works by discretizing\na continuous feature space into unique regions according to the desired\ndiscretization per dimension. While simple, this algorithm has a main drawback:\nit cannot scale to high-dimensional feature spaces since the number of regions\nincrease exponentially with the number of dimensions. In this paper, we address\nthis limitation by introducing a simple extension of MAP-Elites that has a\nconstant, pre-defined number of regions irrespective of the dimensionality of\nthe feature space. Our main insight is that methods from computational geometry\ncould partition a high-dimensional space into well-spread geometric regions. In\nparticular, our algorithm uses a centroidal Voronoi tessellation (CVT) to\ndivide the feature space into a desired number of regions; it then places every\ngenerated individual in its closest region, replacing a less fit one if the\nregion is already occupied. We demonstrate the effectiveness of the new\n\"CVT-MAP-Elites\" algorithm in high-dimensional feature spaces through\ncomparisons against MAP-Elites in a hexapod locomotion task."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1610.09460v1", 
    "title": "Building Energy Load Forecasting using Deep Neural Networks", 
    "arxiv-id": "1610.09460v1", 
    "author": "Milos Manic", 
    "publish": "2016-10-29T06:02:03Z", 
    "summary": "Ensuring sustainability demands more efficient energy management with\nminimized energy wastage. Therefore, the power grid of the future should\nprovide an unprecedented level of flexibility in energy management. To that\nend, intelligent decision making requires accurate predictions of future energy\ndemand/load, both at aggregate and individual site level. Thus, energy load\nforecasting have received increased attention in the recent past, however has\nproven to be a difficult problem. This paper presents a novel energy load\nforecasting methodology based on Deep Neural Networks, specifically Long Short\nTerm Memory (LSTM) algorithms. The presented work investigates two variants of\nthe LSTM: 1) standard LSTM and 2) LSTM-based Sequence to Sequence (S2S)\narchitecture. Both methods were implemented on a benchmark data set of\nelectricity consumption data from one residential customer. Both architectures\nwhere trained and tested on one hour and one-minute time-step resolution\ndatasets. Experimental results showed that the standard LSTM failed at\none-minute resolution data while performing well in one-hour resolution data.\nIt was shown that S2S architecture performed well on both datasets. Further, it\nwas shown that the presented methods produced comparable results with the other\ndeep learning methods for energy forecasting in literature."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.00260v1", 
    "title": "Surrogate-Assisted Partial Order-based Evolutionary Optimisation", 
    "arxiv-id": "1611.00260v1", 
    "author": "Boris Naujoks", 
    "publish": "2016-11-01T15:00:52Z", 
    "summary": "In this paper, we propose a novel approach (SAPEO) to support the survival\nselection process in multi-objective evolutionary algorithms with surrogate\nmodels - it dynamically chooses individuals to evaluate exactly based on the\nmodel uncertainty and the distinctness of the population. We introduce variants\nthat differ in terms of the risk they allow when doing survival selection.\nHere, the anytime performance of different SAPEO variants is evaluated in\nconjunction with an SMS-EMOA using the BBOB bi-objective benchmark. We compare\nthe obtained results with the performance of the regular SMS-EMOA, as well as\nanother surrogate-assisted approach. The results open up general questions\nabout the applicability and required conditions for surrogate-assisted\nmulti-objective evolutionary algorithms to be tackled in the future."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.01590v3", 
    "title": "Alternating Direction Method of Multipliers for Sparse Convolutional   Neural Networks", 
    "arxiv-id": "1611.01590v3", 
    "author": "Mahdieh Abbasi", 
    "publish": "2016-11-05T02:51:24Z", 
    "summary": "The storage and computation requirements of Convolutional Neural Networks\n(CNNs) can be prohibitive for exploiting these models over low-power or\nembedded devices. This paper reduces the computational complexity of the CNNs\nby minimizing an objective function, including the recognition loss that is\naugmented with a sparsity-promoting penalty term. The sparsity structure of the\nnetwork is identified using the Alternating Direction Method of Multipliers\n(ADMM), which is widely used in large optimization problems. This method\nalternates between promoting the sparsity of the network and optimizing the\nrecognition performance, which allows us to exploit the two-part structure of\nthe corresponding objective functions. In particular, we take advantage of the\nseparability of the sparsity-inducing penalty functions to decompose the\nminimization problem into sub-problems that can be solved sequentially.\nApplying our method to a variety of state-of-the-art CNN models, our proposed\nmethod is able to simplify the original model, generating models with less\ncomputation and fewer parameters, while maintaining and often improving\ngeneralization performance. Accomplishments on a variety of models strongly\nverify that our proposed ADMM-based method can be a very useful tool for\nsimplifying and improving deep CNNs."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.02024v2", 
    "title": "Sigma Delta Quantized Networks", 
    "arxiv-id": "1611.02024v2", 
    "author": "Max Welling", 
    "publish": "2016-11-07T12:45:23Z", 
    "summary": "Deep neural networks can be obscenely wasteful. When processing video, a\nconvolutional network expends a fixed amount of computation for each frame with\nno regard to the similarity between neighbouring frames. As a result, it ends\nup repeatedly doing very similar computations. To put an end to such waste, we\nintroduce Sigma-Delta networks. With each new input, each layer in this network\nsends a discretized form of its change in activation to the next layer. Thus\nthe amount of computation that the network does scales with the amount of\nchange in the input and layer activations, rather than the size of the network.\nWe introduce an optimization method for converting any pre-trained deep network\ninto an optimally efficient Sigma-Delta network, and show that our algorithm,\nif run on the appropriate hardware, could cut at least an order of magnitude\nfrom the computational cost of processing video data."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.03000v1", 
    "title": "Bio-Inspired Spiking Convolutional Neural Network using Layer-wise   Sparse Coding and STDP Learning", 
    "arxiv-id": "1611.03000v1", 
    "author": "Anthony S. Maida", 
    "publish": "2016-11-09T16:25:41Z", 
    "summary": "Hierarchical feature discovery using non-spiking convolutional neural\nnetworks (CNNs) has attracted much recent interest in machine learning and\ncomputer vision. However, it is still not well understood how to create spiking\ndeep networks with multi-layer, unsupervised learning. One advantage of spiking\nCNNs is their bio-realism. Another advantage is that they represent information\nusing sparse spike-trains which enable power-efficient implementation. This\npaper explores a novel bio-inspired spiking CNN that is trained in a greedy,\nlayer-wise fashion. The proposed network consists of a spiking\nconvolutional-pooling layer followed by a feature discovery layer. Kernels for\nthe convolutional layer are trained using local learning. The learning is\nimplemented using a sparse, spiking auto-encoder representing primary visual\nfeatures. The feature discovery layer is equipped with a probabilistic\nspike-timing-dependent plasticity (STDP) learning rule. This layer represents\ncomplex visual features using probabilistic leaky, integrate-and-fire (LIF)\nneurons. Our results show that the convolutional layer is stack-admissible,\nenabling it to support a multi-layer learning. The visual features obtained\nfrom the proposed probabilistic LIF neurons in the feature discovery layer are\nutilized for training a classifier. Classification results contribute to the\nindependent and informative visual features extracted in a hierarchy of\nconvolutional and feature discovery layers. The proposed model is evaluated on\nthe MNIST digit dataset using clean and noisy images. The recognition\nperformance for clean images is above 98%. The performance loss for recognizing\nthe noisy images is in the range 0.1% to 8.5% depending on noise types and\ndensities. This level of performance loss indicates that the network is robust\nto additive noise."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.03321v1", 
    "title": "Computing threshold functions using dendrites", 
    "arxiv-id": "1611.03321v1", 
    "author": "Alain Destexhe", 
    "publish": "2016-11-10T14:47:23Z", 
    "summary": "Neurons, modeled as linear threshold unit (LTU), can in theory compute all\nthresh- old functions. In practice, however, some of these functions require\nsynaptic weights of arbitrary large precision. We show here that dendrites can\nalleviate this requirement. We introduce here the non-Linear Threshold Unit\n(nLTU) that integrates synaptic input sub-linearly within distinct subunits to\ntake into account local saturation in dendrites. We systematically search\nparameter space of the nTLU and TLU to compare them. Firstly, this shows that\nthe nLTU can compute all threshold functions with smaller precision weights\nthan the LTU. Secondly, we show that a nLTU can compute significantly more\nfunctions than a LTU when an input can only make a single synapse. This work\npaves the way for a new generation of network made of nLTU with binary\nsynapses."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.04687v2", 
    "title": "Intrinsic Geometric Information Transfer Learning on Multiple   Graph-Structured Datasets", 
    "arxiv-id": "1611.04687v2", 
    "author": "Sungroh Yoon", 
    "publish": "2016-11-15T03:17:15Z", 
    "summary": "Graphs provide a powerful means for representing complex interactions between\nentities. Recently, deep learning approaches are emerging for representing and\nmodeling graph-structured data, although the conventional deep learning methods\n(such as convolutional neural networks and recurrent neural networks) have\nmainly focused on grid-structured inputs (image and audio). Leveraged by the\ncapability of representation learning, deep learning based techniques are\nreporting promising results for graph applications by detecting structural\ncharacteristics of graphs in an automated fashion. In this paper, we attempt to\nadvance deep learning for graph-structured data by incorporating another\ncomponent, transfer learning. By transferring the intrinsic geometric\ninformation learned in the source domain, our approach can help us to construct\na model for a new but related task in the target domain without collecting new\ndata and without training a new model from scratch. We thoroughly test our\napproach with large-scale real corpora and confirm the effectiveness of the\nproposed transfer learning framework for deep learning on graphs. According to\nour experiments, transfer learning is most effective when the source and target\ndomains bear a high level of structural similarity in their graph\nrepresentations."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.04766v1", 
    "title": "Differentiable Genetic Programming", 
    "arxiv-id": "1611.04766v1", 
    "author": "Alessio Mereta", 
    "publish": "2016-11-15T10:01:20Z", 
    "summary": "We introduce the use of high order automatic differentiation, implemented via\nthe algebra of truncated Taylor polynomials, in genetic programming. Using the\nCartesian Genetic Programming encoding we obtain a high-order Taylor\nrepresentation of the program output that is then used to back-propagate errors\nduring learning. The resulting machine learning framework is called\ndifferentiable Cartesian Genetic Programming (dCGP). In the context of symbolic\nregression, dCGP offers a new approach to the long unsolved problem of constant\nrepresentation in GP expressions. On several problems of increasing complexity\nwe find that dCGP is able to find the exact form of the symbolic expression as\nwell as the constants values. We also demonstrate the use of dCGP to solve a\nlarge class of differential equations and to find prime integrals of dynamical\nsystems, presenting, in both cases, results that confirm the efficacy of our\napproach."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1611.07065v2", 
    "title": "Recurrent Neural Networks With Limited Numerical Precision", 
    "arxiv-id": "1611.07065v2", 
    "author": "Yoshua Bengio", 
    "publish": "2016-11-21T21:24:45Z", 
    "summary": "Recurrent Neural Networks (RNNs) produce state-of-art performance on many\nmachine learning tasks but their demand on resources in terms of memory and\ncomputational power are often high. Therefore, there is a great interest in\noptimizing the computations performed with these models especially when\nconsidering development of specialized low-power hardware for deep networks.\nOne way of reducing the computational needs is to limit the numerical precision\nof the network weights and biases, and this will be addressed for the case of\nRNNs. We present results from the use of different stochastic and deterministic\nreduced precision training methods applied to two major RNN types, which are\nthen tested on three datasets. The results show that the stochastic and\ndeterministic ternarization, pow2- ternarization, and exponential quantization\nmethods gave rise to low-precision RNNs that produce similar and even higher\naccuracy on certain datasets, therefore providing a path towards training more\nefficient implementations of RNNs in specialized hardware."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.00369v3", 
    "title": "New Ideas for Brain Modelling 3", 
    "arxiv-id": "1612.00369v3", 
    "author": "Kieran Greer", 
    "publish": "2016-11-22T09:01:16Z", 
    "summary": "This paper considers a process for the creation and subsequent firing of\nsequences of neuronal patterns, as might be found in the human brain. The scale\nis one of larger patterns emerging from an ensemble mass, possibly through some\ntype of energy equation and a reduction procedure. The links between the\npatterns can be formed naturally, as a residual effect of the pattern creation\nitself. If the process is valid, then the pattern creation can be relatively\nsimplistic and automatic, where the neuron does not have to do anything\nparticularly intelligent. The pattern interfaces become slightly abstract\nwithout firm boundaries and exact structure is determined more by averages or\nratios. This paper follows-on closely from the earlier research, including two\nearlier papers in the series and uses the ideas of entropy and cohesion. With a\nsmall addition, it is possible to show how the inter-pattern links can be\ndetermined. A new compact Grid form of an earlier Counting Mechanism is also\ndemonstrated. Finally, it is possible to explain how a very basic repeating\nstructure can form the arbitrary patterns and activation sequences between\nthem."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.02233v1", 
    "title": "A simple and efficient SNN and its performance & robustness evaluation   method to enable hardware implementation", 
    "arxiv-id": "1612.02233v1", 
    "author": "Udayan Ganguly", 
    "publish": "2016-12-07T13:11:27Z", 
    "summary": "Spiking Neural Networks (SNN) are more closely related to brain-like\ncomputation and inspire hardware implementation. This is enabled by small\nnetworks that give high performance on standard classification problems. In\nliterature, typical SNNs are deep and complex in terms of network structure,\nweight update rules and learning algorithms. This makes it difficult to\ntranslate them into hardware. In this paper, we first develop a simple\n2-layered network in software which compares with the state of the art on four\ndifferent standard data-sets within SNNs and has improved efficiency. For\nexample, it uses lower number of neurons (3 x), synapses (3.5 x) and epochs for\ntraining (30 x) for the Fisher Iris classification problem. The efficient\nnetwork is based on effective population coding and synapse-neuron co-design.\nSecond, we develop a computationally efficient (15000 x) and accurate\n(correlation of 0.98) method to evaluate the performance of the network without\nstandard recognition tests. Third, we show that the method produces a\nrobustness metric that can be used to evaluate noise tolerance."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.02336v1", 
    "title": "Neural Turing Machines: Convergence of Copy Tasks", 
    "arxiv-id": "1612.02336v1", 
    "author": "Janez Ale\u0161", 
    "publish": "2016-12-07T17:23:26Z", 
    "summary": "The architecture of neural Turing machines is differentiable end to end and\nis trainable with gradient descent methods. Due to their large unfolded depth\nNeural Turing Machines are hard to train and because of their linear access of\ncomplete memory they do not scale. Other architectures have been studied to\novercome these difficulties. In this report we focus on improving the quality\nof prediction of the original linear memory architecture on copy and repeat\ncopy tasks. Copy task predictions on sequences of length six times larger than\nthose the neural Turing machine was trained on prove to be highly accurate and\nso do predictions of repeat copy tasks for sequences with twice the repetition\nnumber and twice the sequence length neural Turing machine was trained on."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.03402v2", 
    "title": "Improved Quick Hypervolume Algorithm", 
    "arxiv-id": "1612.03402v2", 
    "author": "Andrzej Jaszkiewicz", 
    "publish": "2016-12-11T12:10:52Z", 
    "summary": "In this paper, we present an improved version of recently proposed Quick\nHypervolume algorithm for calculating exact hypervolume of the space dominated\nby a set of d-dimensional points. This value is often used as a quality\nindicator in multiobjective evolutionary algorithms and the efficiency of\ncalculating this indicator is of crucial importance especially in the case of\nlarge set or many dimensional spaces. We use a similar divide and conquer\nscheme as in the original Quick Hypervolume algorithm, we modify, however, the\nway the problem is split into smaller sub-problems. Through both theoretical\nanalysis and computational study we show that our approach improves\ncomputational complexity of the algorithm and running time of its\nimplementation."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.03707v1", 
    "title": "Empirical Evaluation of A New Approach to Simplifying Long Short-term   Memory (LSTM)", 
    "arxiv-id": "1612.03707v1", 
    "author": "Yuzhen Lu", 
    "publish": "2016-12-12T14:36:22Z", 
    "summary": "The standard LSTM, although it succeeds in the modeling long-range\ndependences, suffers from a highly complex structure that can be simplified\nthrough modifications to its gate units. This paper was to perform an empirical\ncomparison between the standard LSTM and three new simplified variants that\nwere obtained by eliminating input signal, bias and hidden unit signal from\nindividual gates, on the tasks of modeling two sequence datasets. The\nexperiments show that the three variants, with reduced parameters, can achieve\ncomparable performance with the standard LSTM. Due attention should be paid to\nturning the learning rate to achieve high accuracies"
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.03940v1", 
    "title": "Understanding the Impact of Precision Quantization on the Accuracy and   Energy of Neural Networks", 
    "arxiv-id": "1612.03940v1", 
    "author": "Sherief Reda", 
    "publish": "2016-12-12T21:36:48Z", 
    "summary": "Deep neural networks are gaining in popularity as they are used to generate\nstate-of-the-art results for a variety of computer vision and machine learning\napplications. At the same time, these networks have grown in depth and\ncomplexity in order to solve harder problems. Given the limitations in power\nbudgets dedicated to these networks, the importance of low-power, low-memory\nsolutions has been stressed in recent years. While a large number of dedicated\nhardware using different precisions has recently been proposed, there exists no\ncomprehensive study of different bit precisions and arithmetic in both inputs\nand network parameters. In this work, we address this issue and perform a study\nof different bit-precisions in neural networks (from floating-point to\nfixed-point, powers of two, and binary). In our evaluation, we consider and\nanalyze the effect of precision scaling on both network accuracy and hardware\nmetrics including memory footprint, power and energy consumption, and design\narea. We also investigate training-time methodologies to compensate for the\nreduction in accuracy due to limited bit precision and demonstrate that in most\ncases, precision scaling can deliver significant benefits in design metrics at\nthe cost of very modest decreases in network accuracy. In addition, we propose\nthat a small portion of the benefits achieved when using lower precisions can\nbe forfeited to increase the network size and therefore the accuracy. We\nevaluate our experiments, using three well-recognized networks and datasets to\nshow its generality. We investigate the trade-offs and highlight the benefits\nof using lower precisions in terms of energy and memory footprint."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.05571v1", 
    "title": "Delta Networks for Optimized Recurrent Network Computation", 
    "arxiv-id": "1612.05571v1", 
    "author": "Shih-Chii Liu", 
    "publish": "2016-12-16T17:57:15Z", 
    "summary": "Many neural networks exhibit stability in their activation patterns over time\nin response to inputs from sensors operating under real-world conditions. By\ncapitalizing on this property of natural signals, we propose a Recurrent Neural\nNetwork (RNN) architecture called a delta network in which each neuron\ntransmits its value only when the change in its activation exceeds a threshold.\nThe execution of RNNs as delta networks is attractive because their states must\nbe stored and fetched at every timestep, unlike in convolutional neural\nnetworks (CNNs). We show that a naive run-time delta network implementation\noffers modest improvements on the number of memory accesses and computes, but\noptimized training techniques confer higher accuracy at higher speedup. With\nthese optimizations, we demonstrate a 9X reduction in cost with negligible loss\nof accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on\nthe large Wall Street Journal speech recognition benchmark even existing\nnetworks can be greatly accelerated as delta networks, and a 5.7x improvement\nwith negligible loss of accuracy can be obtained through training. Finally, on\nan end-to-end CNN trained for steering angle prediction in a driving dataset,\nthe RNN cost can be reduced by a substantial 100X."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1612.06093v1", 
    "title": "Transfer Learning based Dynamic Multiobjective Optimization Algorithms", 
    "arxiv-id": "1612.06093v1", 
    "author": "Gary G. Yen", 
    "publish": "2016-12-19T09:49:28Z", 
    "summary": "One of the major distinguishing features of the dynamic multiobjective\noptimization problems (DMOPs) is the optimization objectives will change over\ntime, thus tracking the varying Pareto-optimal front becomes a challenge. One\nof the promising solutions is reusing the \"experiences\" to construct a\nprediction model via statistical machine learning approaches. However most of\nthe existing methods ignore the non-independent and identically distributed\nnature of data used to construct the prediction model. In this paper, we\npropose an algorithmic framework, called Tr-DMOEA, which integrates transfer\nlearning and population-based evolutionary algorithm for solving the DMOPs.\nThis approach takes the transfer learning method as a tool to help reuse the\npast experience for speeding up the evolutionary process, and at the same time,\nany population based multiobjective algorithms can benefit from this\nintegration without any extensive modifications. To verify this, we incorporate\nthe proposed approach into the development of three well-known algorithms,\nnondominated sorting genetic algorithm II (NSGA-II), multiobjective particle\nswarm optimization (MOPSO), and the regularity model-based multiobjective\nestimation of distribution algorithm (RM-MEDA), and then employ twelve\nbenchmark functions to test these algorithms as well as compare with some\nchosen state-of-the-art designs. The experimental results confirm the\neffectiveness of the proposed method through exploiting machine learning\ntechnology."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1701.00879v1", 
    "title": "PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective Optimization", 
    "arxiv-id": "1701.00879v1", 
    "author": "Yaochu Jin", 
    "publish": "2017-01-04T00:52:49Z", 
    "summary": "Over the last three decades, a large number of evolutionary algorithms have\nbeen developed for solving multiobjective optimization problems. However, there\nlacks an up-to-date and comprehensive software platform for researchers to\nproperly benchmark existing algorithms and for practitioners to apply selected\nalgorithms to solve their real-world problems. The demand of such a common tool\nbecomes even more urgent, when the source code of many proposed algorithms has\nnot been made publicly available. To address these issues, we have developed a\nMATLAB platform for evolutionary multi-objective optimization in this paper,\ncalled PlatEMO, which includes more than 50 multi-objective evolutionary\nalgorithms and more than 100 multi-objective test problems, along with several\nwidely used performance indicators. With a user-friendly graphical user\ninterface, PlatEMO enables users to easily compare several evolutionary\nalgorithms at one time and collect statistical results in Excel or LaTeX files.\nMore importantly, PlatEMO is completely open source, such that users are able\nto develop new algorithms on the basis of it. This paper introduces the main\nfeatures of PlatEMO and illustrates how to use it for performing comparative\nexperiments, embedding new algorithms, creating new test problems, and\ndeveloping performance indicators. Source code of PlatEMO is now available at:\nhttp://bimk.ahu.edu.cn/index.php?s=/Index/Software/index.html."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1701.01271v1", 
    "title": "Subpopulation Diversity Based Selecting Migration Moment in Distributed   Evolutionary Algorithms", 
    "arxiv-id": "1701.01271v1", 
    "author": "Jia Wu", 
    "publish": "2017-01-05T10:32:24Z", 
    "summary": "In distributed evolutionary algorithms, migration interval is used to decide\nmigration moments. Nevertheless, migration moments predetermined by intervals\ncannot match the dynamic situation of evolution. In this paper, a scheme of\nsetting the success rate of migration based on subpopulation diversity at each\ninterval is proposed. With the scheme, migration still occurs at intervals, but\nthe probability of immigrants entering the target subpopulation will be\ndetermined by the diversity of this subpopulation according to a proposed\nformula. An analysis shows that the time consumption of our scheme is\nacceptable. In our experiments, the basement of parallelism is an evolutionary\nalgorithm for the traveling salesman problem. Under different value\ncombinations of parameters for the formula, outcomes for eight benchmark\ninstances of the distributed evolutionary algorithm with the proposed scheme\nare compared with those of a traditional one, respectively. Results show that\nthe distributed evolutionary algorithm based on our scheme has a significant\nadvantage on solutions especially for high difficulty instances. Moreover, it\ncan be seen that the algorithm with the scheme has the most outstanding\nperformance under three value combinations of above-mentioned parameters for\nthe formula."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1701.01495v1", 
    "title": "Membrane-Dependent Neuromorphic Learning Rule for Unsupervised Spike   Pattern Detection", 
    "arxiv-id": "1701.01495v1", 
    "author": "Gert Cauwenberghs", 
    "publish": "2017-01-05T22:57:04Z", 
    "summary": "Several learning rules for synaptic plasticity, that depend on either spike\ntiming or internal state variables, have been proposed in the past imparting\nvarying computational capabilities to Spiking Neural Networks. Due to design\ncomplications these learning rules are typically not implemented on\nneuromorphic devices leaving the devices to be only capable of inference. In\nthis work we propose a unidirectional post-synaptic potential dependent\nlearning rule that is only triggered by pre-synaptic spikes, and easy to\nimplement on hardware. We demonstrate that such a learning rule is functionally\ncapable of replicating computational capabilities of pairwise STDP. Further\nmore, we demonstrate that this learning rule can be used to learn and classify\nspatio-temporal spike patterns in an unsupervised manner using individual\nneurons. We argue that this learning rule is computationally powerful and also\nideal for hardware implementations due to its unidirectional memory access."
},{
    "category": "cs.NE", 
    "doi": "10.14311/NNW.2015.25.024", 
    "link": "http://arxiv.org/pdf/1701.01791v1", 
    "title": "Classification Accuracy Improvement for Neuromorphic Computing Systems   with One-level Precision Synapses", 
    "arxiv-id": "1701.01791v1", 
    "author": "Hai Li", 
    "publish": "2017-01-07T05:01:15Z", 
    "summary": "Brain inspired neuromorphic computing has demonstrated remarkable advantages\nover traditional von Neumann architecture for its high energy efficiency and\nparallel data processing. However, the limited resolution of synaptic weights\ndegrades system accuracy and thus impedes the use of neuromorphic systems. In\nthis work, we propose three orthogonal methods to learn synapses with one-level\nprecision, namely, distribution-aware quantization, quantization regularization\nand bias tuning, to make image classification accuracy comparable to the\nstate-of-the-art. Experiments on both multi-layer perception and convolutional\nneural networks show that the accuracy drop can be well controlled within 0.19%\n(5.53%) for MNIST (CIFAR-10) database, compared to an ideal system without\nquantization."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1701.05730v1", 
    "title": "Using LLVM-based JIT Compilation in Genetic Programming", 
    "arxiv-id": "1701.05730v1", 
    "author": "Juraj Spalek", 
    "publish": "2017-01-20T09:17:52Z", 
    "summary": "The paper describes an approach to implementing genetic programming, which\nuses the LLVM library to just-in-time compile/interpret the evolved abstract\nsyntax trees. The solution is described in some detail, including a parser\n(based on FlexC++ and BisonC++) that can construct the trees from a simple toy\nlanguage with C-like syntax. The approach is compared with a previous\nimplementation (based on direct execution of trees using polymorphic functors)\nin terms of execution speed."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1701.05935v1", 
    "title": "Integration of Preferences in Decomposition Multi-Objective Optimization", 
    "arxiv-id": "1701.05935v1", 
    "author": "Xin Yao", 
    "publish": "2017-01-20T22:05:09Z", 
    "summary": "Most existing studies on evolutionary multi-objective optimization focus on\napproximating the whole Pareto-optimal front. Nevertheless, rather than the\nwhole front, which demands for too many points (especially in a\nhigh-dimensional space), the decision maker might only interest in a partial\nregion, called the region of interest. In this case, solutions outside this\nregion can be noisy to the decision making procedure. Even worse, there is no\nguarantee that we can find the preferred solutions when tackling problems with\ncomplicated properties or a large number of objectives. In this paper, we\ndevelop a systematic way to incorporate the decision maker's preference\ninformation into the decomposition-based evolutionary multi-objective\noptimization methods. Generally speaking, our basic idea is a non-uniform\nmapping scheme by which the originally uniformly distributed reference points\non a canonical simplex can be mapped to the new positions close to the\naspiration level vector specified by the decision maker. By these means, we are\nable to steer the search process towards the region of interest either directly\nor in an interactive manner and also handle a large number of objectives. In\nthe meanwhile, the boundary solutions can be approximated given the decision\nmaker's requirements. Furthermore, the extent of the region of the interest is\nintuitively understandable and controllable in a closed form. Extensive\nexperiments, both proof-of-principle and on a variety of problems with 3 to 10\nobjectives, fully demonstrate the effectiveness of our proposed method for\napproximating the preferred solutions in the region of interest."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.00993v1", 
    "title": "Robust Particle Swarm Optimizer based on Chemomimicry", 
    "arxiv-id": "1702.00993v1", 
    "author": "Karl S. Booksh", 
    "publish": "2017-02-03T13:07:35Z", 
    "summary": "A particle swarm optimizer (PSO) loosely based on the phenomena of\ncrystallization and a chaos factor which follows the complimentary error\nfunction is described. The method features three phases: diffusion, directed\nmotion, and nucleation. During the diffusion phase random walk is the only\ncontributor to particle motion. As the algorithm progresses the contribution\nfrom chaos decreases and movement toward global best locations is pursued until\nconvergence has occurred. The algorithm was found to be more robust to local\nminima in multimodal test functions than a standard PSO algorithm and is\ndesigned for problems which feature experimental precision."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.02217v1", 
    "title": "Multitask Evolution with Cartesian Genetic Programming", 
    "arxiv-id": "1702.02217v1", 
    "author": "Kenneth A. De Jong", 
    "publish": "2017-02-07T22:20:43Z", 
    "summary": "We introduce a genetic programming method for solving multiple Boolean\ncircuit synthesis tasks simultaneously. This allows us to solve a set of\nelementary logic functions twice as easily as with a direct, single-task\napproach."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.03180v4", 
    "title": "Stochastic Configuration Networks: Fundamentals and Algorithms", 
    "arxiv-id": "1702.03180v4", 
    "author": "Ming Li", 
    "publish": "2017-02-10T14:24:16Z", 
    "summary": "This paper contributes to a development of randomized methods for neural\nnetworks. The proposed learner model is generated incrementally by stochastic\nconfiguration (SC) algorithms, termed as Stochastic Configuration Networks\n(SCNs). In contrast to the existing randomised learning algorithms for single\nlayer feed-forward neural networks (SLFNNs), we randomly assign the input\nweights and biases of the hidden nodes in the light of a supervisory mechanism,\nand the output weights are analytically evaluated in either constructive or\nselective manner. As fundamentals of SCN-based data modelling techniques, we\nestablish some theoretical results on the universal approximation property.\nThree versions of SC algorithms are presented for regression problems\n(applicable for classification problems as well) in this work. Simulation\nresults concerning both function approximation and real world data regression\nindicate some remarkable merits of our proposed SCNs in terms of less human\nintervention on the network size setting, the scope adaptation of random\nparameters, fast learning and sound generalization."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.03389v1", 
    "title": "Whale swarm algorithm for function optimization", 
    "arxiv-id": "1702.03389v1", 
    "author": "Xinyu Li", 
    "publish": "2017-02-11T06:39:38Z", 
    "summary": "Increasing nature-inspired metaheuristic algorithms are applied to solving\nthe real-world optimization problems, as they have some advantages over the\nclassical methods of numerical optimization. This paper has proposed a new\nnature-inspired metaheuristic called Whale Swarm Algorithm for function\noptimization, which is inspired by the whales behavior of communicating with\neach other via ultrasound for hunting. The proposed Whale Swarm Algorithm has\nbeen compared with several popular metaheuristic algorithms on comprehensive\nperformance metrics. According to the experimental results, Whale Swarm\nAlgorithm has a quite competitive performance when compared with other\nalgorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.05308v1", 
    "title": "Hierarchy Influenced Differential Evolution: A Motor Operation Inspired   Approach", 
    "arxiv-id": "1702.05308v1", 
    "author": "Om Prakash Verma", 
    "publish": "2017-02-17T11:47:50Z", 
    "summary": "Operational maturity of biological control systems have fuelled the\ninspiration for a large number of mathematical and logical models for control,\nautomation and optimisation. The human brain represents the most sophisticated\ncontrol architecture known to us and is a central motivation for several\nresearch attempts across various domains. In the present work, we introduce an\nalgorithm for mathematical optimisation that derives its intuition from the\nhierarchical and distributed operations of the human motor system. The system\ncomprises global leaders, local leaders and an effector population that adapt\ndynamically to attain global optimisation via a feedback mechanism coupled with\nthe structural hierarchy. The hierarchical system operation is distributed into\nlocal control for movement and global controllers that facilitate gross motion\nand decision making. We present our algorithm as a variant of the classical\nDifferential Evolution algorithm, introducing a hierarchical crossover\noperation. The discussed approach is tested exhaustively on standard test\nfunctions from the CEC 2017 benchmark. Our algorithm significantly outperforms\nvarious standard algorithms as well as their popular variants as discussed in\nthe results."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.05441v1", 
    "title": "Toward Abstraction from Multi-modal Data: Empirical Studies on Multiple   Time-scale Recurrent Models", 
    "arxiv-id": "1702.05441v1", 
    "author": "Tetsuya Ogata", 
    "publish": "2017-02-07T09:31:41Z", 
    "summary": "The abstraction tasks are challenging for multi- modal sequences as they\nrequire a deeper semantic understanding and a novel text generation for the\ndata. Although the recurrent neural networks (RNN) can be used to model the\ncontext of the time-sequences, in most cases the long-term dependencies of\nmulti-modal data make the back-propagation through time training of RNN tend to\nvanish in the time domain. Recently, inspired from Multiple Time-scale\nRecurrent Neural Network (MTRNN), an extension of Gated Recurrent Unit (GRU),\ncalled Multiple Time-scale Gated Recurrent Unit (MTGRU), has been proposed to\nlearn the long-term dependencies in natural language processing. Particularly\nit is also able to accomplish the abstraction task for paragraphs given that\nthe time constants are well defined. In this paper, we compare the MTRNN and\nMTGRU in terms of its learning performances as well as their abstraction\nrepresentation on higher level (with a slower neural activation). This was done\nby conducting two studies based on a smaller data- set (two-dimension time\nsequences from non-linear functions) and a relatively large data-set\n(43-dimension time sequences from iCub manipulation tasks with multi-modal\ndata). We conclude that gated recurrent mechanisms may be necessary for\nlearning long-term dependencies in large dimension multi-modal data-sets (e.g.\nlearning of robot manipulation), even when natural language commands was not\ninvolved. But for smaller learning tasks with simple time-sequences, generic\nversion of recurrent models, such as MTRNN, were sufficient to accomplish the\nabstraction task."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.07805v2", 
    "title": "Revisiting NARX Recurrent Neural Networks for Long-Term Dependencies", 
    "arxiv-id": "1702.07805v2", 
    "author": "Gregory D. Hager", 
    "publish": "2017-02-24T23:48:11Z", 
    "summary": "Recurrent neural networks (RNNs) have shown success for many\nsequence-modeling tasks, but learning long-term dependencies from data remains\ndifficult. This is often attributed to the vanishing gradient problem, which\nshows that gradient components relating a loss at time $t$ to time $t - \\tau$\ntend to decay exponentially with $\\tau$.\n  Long short-term memory (LSTM) and gated recurrent units (GRUs), the most\nwidely-used RNN architectures, attempt to remedy this problem by making the\ndecay's base closer to 1. NARX RNNs take an orthogonal approach: by including\ndirect connections, or delays, from the past, NARX RNNs make the decay's\nexponent closer to 0. However, as introduced, NARX RNNs reduce the decay's\nexponent only by a factor of $n_d$, the number of delays, and simultaneously\nincrease computation by this same factor.\n  We introduce a new variant of NARX RNNs, called MIxed hiSTory RNNs, which\naddresses these drawbacks. We show that for $\\tau \\leq 2^{n_d-1}$, MIST RNNs\nreduce the decay's worst-case exponent from $\\tau / n_d$ to $\\log \\tau$, while\nmaintaining computational complexity that is similar to LSTM and GRUs. We\ncompare MIST RNNs to simple RNNs, LSTM, and GRUs across 4 diverse tasks. MIST\nRNNs outperform all other methods in 2 cases, and in all cases are competitive."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1702.08727v1", 
    "title": "Improving the Neural GPU Architecture for Algorithm Learning", 
    "arxiv-id": "1702.08727v1", 
    "author": "Renars Liepins", 
    "publish": "2017-02-28T10:19:51Z", 
    "summary": "Algorithm learning is a core problem in artificial intelligence with\nsignificant implications on automation level that can be achieved by machines.\nRecently deep learning methods are emerging for synthesizing an algorithm from\nits input-output examples, the most successful being the Neural GPU, capable of\nlearning multiplication. We present several improvements to the Neural GPU that\nsubstantially reduces training time and improves generalization. We introduce a\ntechnique of general applicability to use hard nonlinearities with saturation\ncost. We also introduce a technique of diagonal gates that can be applied to\nactive-memory models. The proposed architecture is the first capable of\nlearning decimal multiplication end-to-end."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.01887v1", 
    "title": "Co-evolutionary multi-task learning for dynamic time series prediction", 
    "arxiv-id": "1703.01887v1", 
    "author": "Chi-Keong Goh", 
    "publish": "2017-02-27T02:19:51Z", 
    "summary": "Multi-task learning employs shared representation of knowledge for learning\nmultiple instances from the same or related problems. Time series prediction\nconsists of several instances that are defined by the way they are broken down\ninto fixed windows known as embedding dimension. Finding the optimal values for\nembedding dimension is a computationally intensive task. Therefore, we\nintroduce a new category of problem called dynamic time series prediction that\nrequires a trained model to give prediction when presented with different\nvalues of the embedding dimension. This can be seen a new class of time series\nprediction where dynamic prediction is needed. In this paper, we propose a\nco-evolutionary multi-task learning method that provides a synergy between\nmulti-task learning and coevolution. This enables neural networks to retain\nmodularity during training for building blocks of knowledge for different\ninstances of the problem. The effectiveness of the proposed method is\ndemonstrated using one-step-ahead chaotic time series problems. The results\nshow that the proposed method can effectively be used for different instances\nof the related time series problems while providing improved generalisation\nperformance."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.01909v1", 
    "title": "Neuromorphic Hardware In The Loop: Training a Deep Spiking Network on   the BrainScaleS Wafer-Scale System", 
    "arxiv-id": "1703.01909v1", 
    "author": "Karlheinz Meier", 
    "publish": "2017-03-06T14:55:59Z", 
    "summary": "Emulating spiking neural networks on analog neuromorphic hardware offers\nseveral advantages over simulating them on conventional computers, particularly\nin terms of speed and energy consumption. However, this usually comes at the\ncost of reduced control over the dynamics of the emulated networks. In this\npaper, we demonstrate how iterative training of a hardware-emulated network can\ncompensate for anomalies induced by the analog substrate. We first convert a\ndeep neural network trained in software to a spiking network on the BrainScaleS\nwafer-scale neuromorphic system, thereby enabling an acceleration factor of 10\n000 compared to the biological time domain. This mapping is followed by the\nin-the-loop training, where in each training step, the network activity is\nfirst recorded in hardware and then used to compute the parameter updates in\nsoftware via backpropagation. An essential finding is that the parameter\nupdates do not have to be precise, but only need to approximately follow the\ncorrect gradient, which simplifies the computation of updates. Using this\napproach, after only several tens of iterations, the spiking network shows an\naccuracy close to the ideal software-emulated prototype. The presented\ntechniques show that deep spiking networks emulated on analog neuromorphic\ndevices can attain good computational performance despite the inherent\nvariations of the analog substrate."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.03334v2", 
    "title": "Fast Genetic Algorithms", 
    "arxiv-id": "1703.03334v2", 
    "author": "Ta Duy Nguyen", 
    "publish": "2017-03-09T16:45:25Z", 
    "summary": "For genetic algorithms using a bit-string representation of length~$n$, the\ngeneral recommendation is to take $1/n$ as mutation rate. In this work, we\ndiscuss whether this is really justified for multimodal functions. Taking jump\nfunctions and the $(1+1)$ evolutionary algorithm as the simplest example, we\nobserve that larger mutation rates give significantly better runtimes. For the\n$\\jump_{m,n}$ function, any mutation rate between $2/n$ and $m/n$ leads to a\nspeed-up at least exponential in $m$ compared to the standard choice.\n  The asymptotically best runtime, obtained from using the mutation rate $m/n$\nand leading to a speed-up super-exponential in $m$, is very sensitive to small\nchanges of the mutation rate. Any deviation by a small $(1 \\pm \\eps)$ factor\nleads to a slow-down exponential in $m$. Consequently, any fixed mutation rate\ngives strongly sub-optimal results for most jump functions.\n  Building on this observation, we propose to use a random mutation rate\n$\\alpha/n$, where $\\alpha$ is chosen from a power-law distribution. We prove\nthat the $(1+1)$ EA with this heavy-tailed mutation rate optimizes any\n$\\jump_{m,n}$ function in a time that is only a small polynomial (in~$m$)\nfactor above the one stemming from the optimal rate for this $m$.\n  Our heavy-tailed mutation operator yields similar speed-ups (over the best\nknown performance guarantees) for the vertex cover problem in bipartite graphs\nand the matching problem in general graphs.\n  Following the example of fast simulated annealing, fast evolution strategies,\nand fast evolutionary programming, we propose to call genetic algorithms using\na heavy-tailed mutation operator \\emph{fast genetic algorithms}."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.03773v1", 
    "title": "Evolutionary Image Composition Using Feature Covariance Matrices", 
    "arxiv-id": "1703.03773v1", 
    "author": "Frank Neumann", 
    "publish": "2017-03-10T17:31:36Z", 
    "summary": "Evolutionary algorithms have recently been used to create a wide range of\nartistic work. In this paper, we propose a new approach for the composition of\nnew images from existing ones, that retain some salient features of the\noriginal images. We introduce evolutionary algorithms that create new images\nbased on a fitness function that incorporates feature covariance matrices\nassociated with different parts of the images. This approach is very flexible\nin that it can work with a wide range of features and enables targeting\nspecific regions in the images. For the creation of the new images, we propose\na population-based evolutionary algorithm with mutation and crossover operators\nbased on random walks. Our experimental results reveal a spectrum of\naesthetically pleasing images that can be obtained with the aid of our\nevolutionary process."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.04496v1", 
    "title": "Comparison of echo state network output layer classification methods on   noisy data", 
    "arxiv-id": "1703.04496v1", 
    "author": "Ashley Prater", 
    "publish": "2017-03-13T17:25:52Z", 
    "summary": "Echo state networks are a recently developed type of recurrent neural network\nwhere the internal layer is fixed with random weights, and only the output\nlayer is trained on specific data. Echo state networks are increasingly being\nused to process spatiotemporal data in real-world settings, including speech\nrecognition, event detection, and robot control. A strength of echo state\nnetworks is the simple method used to train the output layer - typically a\ncollection of linear readout weights found using a least squares approach.\nAlthough straightforward to train and having a low computational cost to use,\nthis method may not yield acceptable accuracy performance on noisy data.\n  This study compares the performance of three echo state network output layer\nmethods to perform classification on noisy data: using trained linear weights,\nusing sparse trained linear weights, and using trained low-rank approximations\nof reservoir states. The methods are investigated experimentally on both\nsynthetic and natural datasets. The experiments suggest that using regularized\nleast squares to train linear output weights is superior on data with low\nnoise, but using the low-rank approximations may significantly improve accuracy\non datasets contaminated with higher noise levels."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.05422v1", 
    "title": "Large Scale Evolution of Convolutional Neural Networks Using Volunteer   Computing", 
    "arxiv-id": "1703.05422v1", 
    "author": "Travis Desell", 
    "publish": "2017-03-15T23:17:24Z", 
    "summary": "This work presents a new algorithm called evolutionary exploration of\naugmenting convolutional topologies (EXACT), which is capable of evolving the\nstructure of convolutional neural networks (CNNs). EXACT is in part modeled\nafter the neuroevolution of augmenting topologies (NEAT) algorithm, with\nnotable exceptions to allow it to scale to large scale distributed computing\nenvironments and evolve networks with convolutional filters. In addition to\nmultithreaded and MPI versions, EXACT has been implemented as part of a BOINC\nvolunteer computing project, allowing large scale evolution. During a period of\ntwo months, over 4,500 volunteered computers on the Citizen Science Grid\ntrained over 120,000 CNNs and evolved networks reaching 98.32% test data\naccuracy on the MNIST handwritten digits dataset. These results are even\nstronger as the backpropagation strategy used to train the CNNs was fairly\nrudimentary (ReLU units, L2 regularization and Nesterov momentum) and these\nwere initial test runs done without refinement of the backpropagation\nhyperparameters. Further, the EXACT evolutionary strategy is independent of the\nmethod used to train the CNNs, so they could be further improved by advanced\ntechniques like elastic distortions, pretraining and dropout. The evolved\nnetworks are also quite interesting, showing \"organic\" structures and\nsignificant differences from standard human designed architectures."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/1703.05807v1", 
    "title": "Reservoir Computing and Extreme Learning Machines using Pairs of   Cellular Automata Rules", 
    "arxiv-id": "1703.05807v1", 
    "author": "Nathan McDonald", 
    "publish": "2017-03-16T19:33:57Z", 
    "summary": "A framework for implementing reservoir computing (RC) and extreme learning\nmachines (ELMs), two types of artificial neural networks, based on 1D\nelementary Cellular Automata (CA) is presented, in which two separate CA rules\nexplicitly implement the minimum computational requirements of the reservoir\nlayer: hyperdimensional projection and short-term memory. CAs are cell-based\nstate machines, which evolve in time in accordance with local rules based on a\ncells current state and those of its neighbors. Notably, simple single cell\nshift rules as the memory rule in a fixed edge CA afforded reasonable success\nin conjunction with a variety of projection rules, potentially significantly\nreducing the optimal solution search space. Optimal iteration counts for the CA\nrule pairs can be estimated for some tasks based upon the category of the\nprojection rule. Initial results support future hardware realization, where CAs\npotentially afford orders of magnitude reduction in size, weight, and power\n(SWaP) requirements compared with floating point RC implementations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9809019v1", 
    "title": "Distributed Computation as Hierarchy", 
    "arxiv-id": "cs/9809019v1", 
    "author": "Michael Manthey", 
    "publish": "1998-09-14T15:25:42Z", 
    "summary": "This paper presents a new distributed computational model of distributed\nsystems called the phase web that extends V. Pratt's orthocurrence relation\nfrom 1986. The model uses mutual-exclusion to express sequence, and a new kind\nof hierarchy to replace event sequences, posets, and pomsets. The model\nexplicitly connects computation to a discrete Clifford algebra that is in turn\nextended into homology and co-homology, wherein the recursive nature of objects\nand boundaries becomes apparent and itself subject to hierarchical recursion.\nTopsy, a programming environment embodying the phase web, is available from\nwww.cs.auc.dk/topsy."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9809111v1", 
    "title": "Evolution of Neural Networks to Play the Game of Dots-and-Boxes", 
    "arxiv-id": "cs/9809111v1", 
    "author": "Terry Bossomaier", 
    "publish": "1998-09-28T03:48:22Z", 
    "summary": "Dots-and-Boxes is a child's game which remains analytically unsolved. We\nimplement and evolve artificial neural networks to play this game, evaluating\nthem against simple heuristic players. Our networks do not evaluate or predict\nthe final outcome of the game, but rather recommend moves at each stage.\nSuperior generalisation of play by co-evolved populations is found, and a\ncomparison made with networks trained by back-propagation using simple\nheuristics as an oracle."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9809123v1", 
    "title": "A role of constraint in self-organization", 
    "arxiv-id": "cs/9809123v1", 
    "author": "Tadashi Yamazaki", 
    "publish": "1998-09-30T04:06:36Z", 
    "summary": "In this paper we introduce a neural network model of self-organization. This\nmodel uses a variation of Hebb rule for updating its synaptic weights, and\nsurely converges to the equilibrium status. The key point of the convergence is\nthe update rule that constrains the total synaptic weight and this seems to\nmake the model stable. We investigate the role of the constraint and show that\nit is the constraint that makes the model stable. For analyzing this setting,\nwe propose a simple probabilistic game that models the neural network and the\nself-organization process. Then, we investigate the characteristics of this\ngame, namely, the probability that the game becomes stable and the number of\nthe steps it takes."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9811030v1", 
    "title": "Generating Segment Durations in a Text-To-Speech System: A Hybrid   Rule-Based/Neural Network Approach", 
    "arxiv-id": "cs/9811030v1", 
    "author": "Orhan Karaali", 
    "publish": "1998-11-24T22:51:20Z", 
    "summary": "A combination of a neural network with rule firing information from a\nrule-based system is used to generate segment durations for a text-to-speech\nsystem. The system shows a slight improvement in performance over a neural\nnetwork system without the rule firing information. Synthesized speech using\nsegment durations was accepted by listeners as having about the same quality as\nspeech generated using segment durations extracted from natural speech."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9811031v1", 
    "title": "Speech Synthesis with Neural Networks", 
    "arxiv-id": "cs/9811031v1", 
    "author": "Ira Gerson", 
    "publish": "1998-11-24T23:33:12Z", 
    "summary": "Text-to-speech conversion has traditionally been performed either by\nconcatenating short samples of speech or by using rule-based systems to convert\na phonetic representation of speech into an acoustic representation, which is\nthen converted into speech. This paper describes a system that uses a\ntime-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,\nwith another neural network to control the timing of the generated speech. The\nneural network system requires less memory than a concatenation system, and\nperformed well in tests comparing it to commercial systems using other\ntechnologies."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ELEKTRO.2016.7512108", 
    "link": "http://arxiv.org/pdf/cs/9811032v1", 
    "title": "Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN   Approach", 
    "arxiv-id": "cs/9811032v1", 
    "author": "Noel Massey", 
    "publish": "1998-11-24T23:51:56Z", 
    "summary": "This paper describes the design of a neural network that performs the\nphonetic-to-acoustic mapping in a speech synthesis system. The use of a\ntime-domain neural network architecture limits discontinuities that occur at\nphone boundaries. Recurrent data input also helps smooth the output parameter\ntracks. Independent testing has demonstrated that the voice quality produced by\nthis system compares favorably with speech from existing commercial\ntext-to-speech systems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/9812006v1", 
    "title": "A High Quality Text-To-Speech System Composed of Multiple Neural   Networks", 
    "arxiv-id": "cs/9812006v1", 
    "author": "Andrew Mackie", 
    "publish": "1998-12-05T00:00:57Z", 
    "summary": "While neural networks have been employed to handle several different\ntext-to-speech tasks, ours is the first system to use neural networks\nthroughout, for both linguistic and acoustic processing. We divide the\ntext-to-speech task into three subtasks, a linguistic module mapping from text\nto a linguistic representation, an acoustic module mapping from the linguistic\nrepresentation to speech, and a video module mapping from the linguistic\nrepresentation to animated images. The linguistic module employs a\nletter-to-sound neural network and a postlexical neural network. The acoustic\nmodule employs a duration neural network and a phonetic neural network. The\nvisual neural network is employed in parallel to the acoustic module to drive a\ntalking head. The use of neural networks that can be retrained on the\ncharacteristics of different voices and languages affords our system a degree\nof adaptability and naturalness heretofore unavailable."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/9812013v1", 
    "title": "The Self-Organizing Symbiotic Agent", 
    "arxiv-id": "cs/9812013v1", 
    "author": "Makoto Amamiya", 
    "publish": "1998-12-11T20:51:38Z", 
    "summary": "In [N. A. Baas, Emergence, Hierarchies, and Hyper-structures, in C.G. Langton\ned., Artificial Life III, Addison Wesley, 1994.] a general framework for the\nstudy of Emergence and hyper-structure was presented. This approach is mostly\nconcerned with the description of such systems. In this paper we will try to\nbring forth a different aspect of this model we feel will be useful in the\nengineering of agent based solutions, namely the symbiotic approach. In this\napproach a self-organizing method of dividing the more complex \"main-problem\"\nto a hyper-structure of \"sub-problems\" with the aim of reducing complexity is\ndesired. A description of the general problem will be given along with some\ninstances of related work. This paper is intended to serve as an introductory\nchallenge for general solutions to the described problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/9905012v1", 
    "title": "Linear and Order Statistics Combiners for Pattern Classification", 
    "arxiv-id": "cs/9905012v1", 
    "author": "Joydeep Ghosh", 
    "publish": "1999-05-20T20:15:13Z", 
    "summary": "Several researchers have experimentally shown that substantial improvements\ncan be obtained in difficult pattern recognition problems by combining or\nintegrating the outputs of multiple classifiers. This chapter provides an\nanalytical framework to quantify the improvements in classification results due\nto combining. The results apply to both linear combiners and order statistics\ncombiners. We first show that to a first order approximation, the error rate\nobtained over and above the Bayes error rate, is directly proportional to the\nvariance of the actual decision boundaries around the Bayes optimum boundary.\nCombining classifiers in output space reduces this variance, and hence reduces\nthe \"added\" error. If N unbiased classifiers are combined by simple averaging,\nthe added error rate can be reduced by a factor of N if the individual errors\nin approximating the decision boundaries are uncorrelated. Expressions are then\nderived for linear combiners which are biased or correlated, and the effect of\noutput correlations on ensemble performance is quantified. For order statistics\nbased non-linear combiners, we derive expressions that indicate how much the\nmedian, the maximum and in general the ith order statistic can improve\nclassifier performance. The analysis presented here facilitates the\nunderstanding of the relationships among error rates, classifier boundary\ndistributions, and combining in output space. Experimental results on several\npublic domain data sets are provided to illustrate the benefits of combining\nand to support the analytical results."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0001027v1", 
    "title": "Pattern Discovery and Computational Mechanics", 
    "arxiv-id": "cs/0001027v1", 
    "author": "James P. Crutchfield", 
    "publish": "2000-01-29T01:23:54Z", 
    "summary": "Computational mechanics is a method for discovering, describing and\nquantifying patterns, using tools from statistical physics. It constructs\noptimal, minimal models of stochastic processes and their underlying causal\nstructures. These models tell us about the intrinsic computation embedded\nwithin a process---how it stores and transforms information. Here we summarize\nthe mathematics of computational mechanics, especially recent optimality and\nuniqueness results. We also expound the principles and motivations underlying\ncomputational mechanics, emphasizing its connections to the minimum description\nlength principle, PAC theory, and other aspects of machine learning."
},{
    "category": "cs.RO", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0006006v1", 
    "title": "A Real-Time Novelty Detector for a Mobile Robot", 
    "arxiv-id": "cs/0006006v1", 
    "author": "Jonathan Shapiro", 
    "publish": "2000-06-02T12:00:15Z", 
    "summary": "Recognising new or unusual features of an environment is an ability which is\npotentially very useful to a robot. This paper demonstrates an algorithm which\nachieves this task by learning an internal representation of `normality' from\nsonar scans taken as a robot explores the environment. This model of the\nenvironment is used to evaluate the novelty of each sonar scan presented to it\nwith relation to the model. Stimuli which have not been seen before, and\ntherefore have more novelty, are highlighted by the filter. The filter has the\nability to forget about features which have been learned, so that stimuli which\nare seen only rarely recover their response over time. A number of robot\nexperiments are presented which demonstrate the operation of the filter."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0006039v1", 
    "title": "Orthogonal Least Squares Algorithm for the Approximation of a Map and   its Derivatives with a RBF Network", 
    "arxiv-id": "cs/0006039v1", 
    "author": "Davide Rocchesso", 
    "publish": "2000-06-28T10:17:43Z", 
    "summary": "Radial Basis Function Networks (RBFNs) are used primarily to solve\ncurve-fitting problems and for non-linear system modeling. Several algorithms\nare known for the approximation of a non-linear curve from a sparse data set by\nmeans of RBFNs. However, there are no procedures that permit to define\nconstrains on the derivatives of the curve. In this paper, the Orthogonal Least\nSquares algorithm for the identification of RBFNs is modified to provide the\napproximation of a non-linear 1-in 1-out map along with its derivatives, given\na set of training data. The interest on the derivatives of non-linear functions\nconcerns many identification and control tasks where the study of system\nstability and robustness is addressed. The effectiveness of the proposed\nalgorithm is demonstrated by a study on the stability of a single loop feedback\nsystem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0010002v1", 
    "title": "Noise Effects in Fuzzy Modelling Systems", 
    "arxiv-id": "cs/0010002v1", 
    "author": "J. A. Dente", 
    "publish": "2000-09-30T14:37:23Z", 
    "summary": "Noise is source of ambiguity for fuzzy systems. Although being an important\naspect, the effects of noise in fuzzy modeling have been little investigated.\nThis paper presents a set of tests using three well-known fuzzy modeling\nalgorithms. These evaluate perturbations in the extracted rule-bases caused by\nnoise polluting the learning data, and the corresponding deformations in each\nlearned functional relation. We present results to show: 1) how these fuzzy\nmodeling systems deal with noise; 2) how the established fuzzy model structure\ninfluences noise sensitivity of each algorithm; and 3) whose characteristics of\nthe learning algorithms are relevant to noise attenuation."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0012020v1", 
    "title": "Creativity and Delusions: A Neurocomputational Approach", 
    "arxiv-id": "cs/0012020v1", 
    "author": "Luis Alfredo Vidal de Carvalho", 
    "publish": "2000-12-22T12:00:07Z", 
    "summary": "Thinking is one of the most interesting mental processes. Its complexity is\nsometimes simplified and its different manifestations are classified into\nnormal and abnormal, like the delusional and disorganized thought or the\ncreative one. The boundaries between these facets of thinking are fuzzy causing\ndifficulties in medical, academic, and philosophical discussions. Considering\nthe dopaminergic signal-to-noise neuronal modulation in the central nervous\nsystem, and the existence of semantic maps in human brain, a self-organizing\nneural network model was developed to unify the different thought processes\ninto a single neurocomputational substrate. Simulations were performed varying\nthe dopaminergic modulation and observing the different patterns that emerged\nat the semantic map. Assuming that the thought process is the total pattern\nelicited at the output layer of the neural network, the model shows how the\nnormal and abnormal thinking are generated and that there are no borders\nbetween their different manifestations. Actually, a continuum of different\nqualitative reasoning, ranging from delusion to disorganization of thought, and\npassing through the normal and the creative thinking, seems to be more\nplausible. The model is far from explaining the complexities of human thinking\nbut, at least, it seems to be a good metaphorical and unifying view of the many\nfacets of this phenomenon usually studied in separated settings."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0102014v1", 
    "title": "On the predictability of Rainfall in Kerala- An application of ABF   Neural Network", 
    "arxiv-id": "cs/0102014v1", 
    "author": "K. Babu Joseph", 
    "publish": "2001-02-18T19:17:18Z", 
    "summary": "Rainfall in Kerala State, the southern part of Indian Peninsula in particular\nis caused by the two monsoons and the two cyclones every year. In general,\nclimate and rainfall are highly nonlinear phenomena in nature giving rise to\nwhat is known as the `butterfly effect'. We however attempt to train an ABF\nneural network on the time series rainfall data and show for the first time\nthat in spite of the fluctuations resulting from the nonlinearity in the\nsystem, the trends in the rainfall pattern in this corner of the globe have\nremained unaffected over the past 87 years from 1893 to 1980. We also\nsuccessfully filter out the chaotic part of the system and illustrate that its\neffects are marginal over long term predictions."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0102027v3", 
    "title": "Gene Expression Programming: a New Adaptive Algorithm for Solving   Problems", 
    "arxiv-id": "cs/0102027v3", 
    "author": "Candida Ferreira", 
    "publish": "2001-02-25T19:29:55Z", 
    "summary": "Gene expression programming, a genotype/phenotype genetic algorithm (linear\nand ramified), is presented here for the first time as a new technique for the\ncreation of computer programs. Gene expression programming uses character\nlinear chromosomes composed of genes structurally organized in a head and a\ntail. The chromosomes function as a genome and are subjected to modification by\nmeans of mutation, transposition, root transposition, gene transposition, gene\nrecombination, and one- and two-point recombination. The chromosomes encode\nexpression trees which are the object of selection. The creation of these\nseparate entities (genome and expression tree) with distinct functions allows\nthe algorithm to perform with high efficiency that greatly surpasses existing\nadaptive techniques. The suite of problems chosen to illustrate the power and\nversatility of gene expression programming includes symbolic regression,\nsequence induction with and without constant creation, block stacking, cellular\nautomata rules for the density-classification problem, and two problems of\nboolean concept learning: the 11-multiplexer and the GP rule problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0104011v1", 
    "title": "Potholes on the Royal Road", 
    "arxiv-id": "cs/0104011v1", 
    "author": "Theodore C. Belding", 
    "publish": "2001-04-06T21:15:31Z", 
    "summary": "It is still unclear how an evolutionary algorithm (EA) searches a fitness\nlandscape, and on what fitness landscapes a particular EA will do well. The\nvalidity of the building-block hypothesis, a major tenet of traditional genetic\nalgorithm theory, remains controversial despite its continued use to justify\nclaims about EAs. This paper outlines a research program to begin to answer\nsome of these open questions, by extending the work done in the royal road\nproject. The short-term goal is to find a simple class of functions which the\nsimple genetic algorithm optimizes better than other optimization methods, such\nas hillclimbers. A dialectical heuristic for searching for such a class is\nintroduced. As an example of using the heuristic, the simple genetic algorithm\nis compared with a set of hillclimbers on a simple subset of the\nhyperplane-defined functions, the pothole functions."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0108009v1", 
    "title": "Artificial Neurons with Arbitrarily Complex Internal Structures", 
    "arxiv-id": "cs/0108009v1", 
    "author": "G. A. Kohring", 
    "publish": "2001-08-17T11:30:12Z", 
    "summary": "Artificial neurons with arbitrarily complex internal structure are\nintroduced. The neurons can be described in terms of a set of internal\nvariables, a set activation functions which describe the time evolution of\nthese variables and a set of characteristic functions which control how the\nneurons interact with one another. The information capacity of attractor\nnetworks composed of these generalized neurons is shown to reach the maximum\nallowed bound. A simple example taken from the domain of pattern recognition\ndemonstrates the increased computational power of these neurons. Furthermore, a\nspecific class of generalized neurons gives rise to a simple transformation\nrelating attractor networks of generalized neurons to standard three layer\nfeed-forward networks. Given this correspondence, we conjecture that the\nmaximum information capacity of a three layer feed-forward network is 2 bits\nper weight."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0202007v1", 
    "title": "Steady State Resource Allocation Analysis of the Stochastic Diffusion   Search", 
    "arxiv-id": "cs/0202007v1", 
    "author": "Mark J. Bishop", 
    "publish": "2002-02-07T14:11:18Z", 
    "summary": "This article presents the long-term behaviour analysis of Stochastic\nDiffusion Search (SDS), a distributed agent-based system for best-fit pattern\nmatching. SDS operates by allocating simple agents into different regions of\nthe search space. Agents independently pose hypotheses about the presence of\nthe pattern in the search space and its potential distortion. Assuming a\ncompositional structure of hypotheses about pattern matching agents perform an\ninference on the basis of partial evidence from the hypothesised solution.\nAgents posing mutually consistent hypotheses about the pattern support each\nother and inhibit agents with inconsistent hypotheses. This results in the\nemergence of a stable agent population identifying the desired solution.\nPositive feedback via diffusion of information between the agents significantly\ncontributes to the speed with which the solution population is formed. The\nformulation of the SDS model in terms of interacting Markov Chains enables its\ncharacterisation in terms of the allocation of agents, or computational\nresources. The analysis characterises the stationary probability distribution\nof the activity of agents, which leads to the characterisation of the solution\npopulation in terms of its similarity to the target pattern."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0202009v1", 
    "title": "Non-negative sparse coding", 
    "arxiv-id": "cs/0202009v1", 
    "author": "Patrik O. Hoyer", 
    "publish": "2002-02-11T11:04:08Z", 
    "summary": "Non-negative sparse coding is a method for decomposing multivariate data into\nnon-negative sparse components. In this paper we briefly describe the\nmotivation behind this type of data representation and its relation to standard\nsparse coding and non-negative matrix factorization. We then give a simple yet\nefficient multiplicative algorithm for finding the optimal values of the hidden\ncomponents. In addition, we show how the basis vectors can be learned from the\nobserved data. Simulations demonstrate the effectiveness of the proposed\nmethod."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0206001v1", 
    "title": "Neural Net Model for Featured Word Extraction", 
    "arxiv-id": "cs/0206001v1", 
    "author": "C. Gershenson", 
    "publish": "2002-06-01T15:10:56Z", 
    "summary": "Search engines perform the task of retrieving information related to the\nuser-supplied query words. This task has two parts; one is finding \"featured\nwords\" which describe an article best and the other is finding a match among\nthese words to user-defined search terms. There are two main independent\napproaches to achieve this task. The first one, using the concepts of\nsemantics, has been implemented partially. For more details see another paper\nof Marko et al., 2002. The second approach is reported in this paper. It is a\ntheoretical model based on using Neural Network (NN). Instead of using keywords\nor reading from the first few lines from papers/articles, the present model\ngives emphasis on extracting \"featured words\" from an article. Obviously we\npropose to exclude prepositions, articles and so on, that is, English words\nlike \"of, the, are, so, therefore, \" etc. from such a list. A neural model is\ntaken with its nodes pre-assigned energies. Whenever a match is found with\nfeatured words and userdefined search words, the node is fired and jumps to a\nhigher energy. This firing continues until the model attains a steady energy\nlevel and total energy is now calculated. Clearly, higher match will generate\nhigher energy; so on the basis of total energy, a ranking is done to the\narticle indicating degree of relevance to the user's interest. Another\nimportant feature of the proposed model is incorporating a semantic module to\nrefine the search words; like finding association among search words, etc. In\nthis manner, information retrieval can be improved markedly."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0209030v1", 
    "title": "Extremal Optimization: an Evolutionary Local-Search Algorithm", 
    "arxiv-id": "cs/0209030v1", 
    "author": "Allon G. Percus", 
    "publish": "2002-09-26T14:16:15Z", 
    "summary": "A recently introduced general-purpose heuristic for finding high-quality\nsolutions for many hard optimization problems is reviewed. The method is\ninspired by recent progress in understanding far-from-equilibrium phenomena in\nterms of {\\em self-organized criticality,} a concept introduced to describe\nemergent complexity in physical systems. This method, called {\\em extremal\noptimization,} successively replaces the value of extremely undesirable\nvariables in a sub-optimal solution with new, random ones. Large,\navalanche-like fluctuations in the cost function self-organize from this\ndynamics, effectively scaling barriers to explore local optima in distant\nneighborhoods of the configuration space while eliminating the need to tune\nparameters. Drawing upon models used to simulate the dynamics of granular\nmedia, evolution, or geology, extremal optimization complements approximation\nmethods inspired by equilibrium statistical physics, such as {\\em simulated\nannealing}. It may be but one example of applying new insights into {\\em\nnon-equilibrium phenomena} systematically to hard optimization problems. This\nmethod is widely applicable and so far has proved competitive with -- and even\nsuperior to -- more elaborate general-purpose heuristics on testbeds of\nconstrained optimization problems with up to $10^5$ variables, such as\nbipartitioning, coloring, and satisfiability. Analysis of a suitable model\npredicts the only free parameter of the method in accordance with all\nexperimental results."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0212010v1", 
    "title": "JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space", 
    "arxiv-id": "cs/0212010v1", 
    "author": "Robert Ewaschuk", 
    "publish": "2002-12-08T00:26:49Z", 
    "summary": "JohnnyVon is an implementation of self-replicating automata in continuous\ntwo-dimensional space. Two types of particles drift about in a virtual liquid.\nThe particles are automata with discrete internal states but continuous\nexternal relationships. Their internal states are governed by finite state\nmachines but their external relationships are governed by a simulated physics\nthat includes brownian motion, viscosity, and spring-like attractive and\nrepulsive forces. The particles can be assembled into patterns that can encode\narbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern\nis put in a \"soup\" of separate individual particles, the pattern will replicate\nby assembling the individual particles into copies of itself. We also show\nthat, given sufficient time, a soup of separate individual particles will\neventually spontaneously form self-replicating patterns. We discuss the\nimplications of JohnnyVon for research in nanotechnology, theoretical biology,\nand artificial life."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0212023v1", 
    "title": "How to Shift Bias: Lessons from the Baldwin Effect", 
    "arxiv-id": "cs/0212023v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-10T18:19:54Z", 
    "summary": "An inductive learning algorithm takes a set of data as input and generates a\nhypothesis as output. A set of data is typically consistent with an infinite\nnumber of hypotheses; therefore, there must be factors other than the data that\ndetermine the output of the learning algorithm. In machine learning, these\nother factors are called the bias of the learner. Classical learning algorithms\nhave a fixed bias, implicit in their design. Recently developed learning\nalgorithms dynamically adjust their bias as they search for a hypothesis.\nAlgorithms that shift bias in this manner are not as well understood as\nclassical algorithms. In this paper, we show that the Baldwin effect has\nimplications for the design and analysis of bias shifting algorithms. The\nBaldwin effect was proposed in 1896, to explain how phenomena that might appear\nto require Lamarckian evolution (inheritance of acquired characteristics) can\narise from purely Darwinian evolution. Hinton and Nowlan presented a\ncomputational model of the Baldwin effect in 1987. We explore a variation on\ntheir model, which we constructed explicitly to illustrate the lessons that the\nBaldwin effect has for research in bias shifting algorithms. The main lesson is\nthat it appears that a good strategy for shift of bias in a learning algorithm\nis to begin with a weak bias and gradually shift to a strong bias."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0212036v1", 
    "title": "Myths and Legends of the Baldwin Effect", 
    "arxiv-id": "cs/0212036v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-11T21:34:18Z", 
    "summary": "This position paper argues that the Baldwin effect is widely misunderstood by\nthe evolutionary computation community. The misunderstandings appear to fall\ninto two general categories. Firstly, it is commonly believed that the Baldwin\neffect is concerned with the synergy that results when there is an evolving\npopulation of learning individuals. This is only half of the story. The full\nstory is more complicated and more interesting. The Baldwin effect is concerned\nwith the costs and benefits of lifetime learning by individuals in an evolving\npopulation. Several researchers have focussed exclusively on the benefits, but\nthere is much to be gained from attention to the costs. This paper explains the\ntwo sides of the story and enumerates ten of the costs and benefits of lifetime\nlearning by individuals in an evolving population. Secondly, there is a cluster\nof misunderstandings about the relationship between the Baldwin effect and\nLamarckian inheritance of acquired characteristics. The Baldwin effect is not\nLamarckian. A Lamarckian algorithm is not better for most evolutionary\ncomputing problems than a Baldwinian algorithm. Finally, Lamarckian inheritance\nis not a better model of memetic (cultural) evolution than the Baldwin effect."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0212039v1", 
    "title": "Low Size-Complexity Inductive Logic Programming: The East-West Challenge   Considered as a Problem in Cost-Sensitive Classification", 
    "arxiv-id": "cs/0212039v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-12T18:51:06Z", 
    "summary": "The Inductive Logic Programming community has considered proof-complexity and\nmodel-complexity, but, until recently, size-complexity has received little\nattention. Recently a challenge was issued \"to the international computing\ncommunity\" to discover low size-complexity Prolog programs for classifying\ntrains. The challenge was based on a problem first proposed by Ryszard\nMichalski, 20 years ago. We interpreted the challenge as a problem in\ncost-sensitive classification and we applied a recently developed\ncost-sensitive classifier to the competition. Our algorithm was relatively\nsuccessful (we won a prize). This paper presents our algorithm and analyzes the\nresults of the competition."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305013v1", 
    "title": "On Nonspecific Evidence", 
    "arxiv-id": "cs/0305013v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T13:39:19Z", 
    "summary": "When simultaneously reasoning with evidences about several different events\nit is necessary to separate the evidence according to event. These events\nshould then be handled independently. However, when propositions of evidences\nare weakly specified in the sense that it may not be certain to which event\nthey are referring, this may not be directly possible. In this paper a\ncriterion for partitioning evidences into subsets representing events is\nestablished. This criterion, derived from the conflict within each subset,\ninvolves minimising a criterion function for the overall conflict of the\npartition. An algorithm based on characteristics of the criterion function and\nan iterative optimisation among partitionings of evidences is proposed."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305015v1", 
    "title": "Finding a Posterior Domain Probability Distribution by Specifying   Nonspecific Evidence", 
    "arxiv-id": "cs/0305015v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T14:36:03Z", 
    "summary": "This article is an extension of the results of two earlier articles. In [J.\nSchubert, On nonspecific evidence, Int. J. Intell. Syst. 8 (1993) 711-725] we\nestablished within Dempster-Shafer theory a criterion function called the\nmetaconflict function. With this criterion we can partition into subsets a set\nof several pieces of evidence with propositions that are weakly specified in\nthe sense that it may be uncertain to which event a proposition is referring.\nIn a second article [J. Schubert, Specifying nonspecific evidence, in\nCluster-based specification techniques in Dempster-Shafer theory for an\nevidential intelligence analysis of multiple target tracks, Ph.D. Thesis,\nTRITA-NA-9410, Royal Institute of Technology, Stockholm, 1994, ISBN\n91-7170-801-4] we not only found the most plausible subset for each piece of\nevidence, we also found the plausibility for every subset that this piece of\nevidence belongs to the subset. In this article we aim to find a posterior\nprobability distribution regarding the number of subsets. We use the idea that\neach piece of evidence in a subset supports the existence of that subset to the\ndegree that this piece of evidence supports anything at all. From this we can\nderive a bpa that is concerned with the question of how many subsets we have.\nThat bpa can then be combined with a given prior domain probability\ndistribution in order to obtain the sought-after posterior domain distribution."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305017v1", 
    "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory", 
    "arxiv-id": "cs/0305017v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T14:46:37Z", 
    "summary": "When reasoning with uncertainty there are many situations where evidences are\nnot only uncertain but their propositions may also be weakly specified in the\nsense that it may not be certain to which event a proposition is referring. It\nis then crucial not to combine such evidences in the mistaken belief that they\nare referring to the same event. This situation would become manageable if the\nevidences could be clustered into subsets representing events that should be\nhandled separately. In an earlier article we established within Dempster-Shafer\ntheory a criterion function called the metaconflict function. With this\ncriterion we can partition a set of evidences into subsets. Each subset\nrepresenting a separate event. In this article we will not only find the most\nplausible subset for each piece of evidence, we will also find the plausibility\nfor every subset that the evidence belongs to the subset. Also, when the number\nof subsets are uncertain we aim to find a posterior probability distribution\nregarding the number of subsets."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305018v1", 
    "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory for an   Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)", 
    "arxiv-id": "cs/0305018v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T14:59:45Z", 
    "summary": "In Intelligence Analysis it is of vital importance to manage uncertainty.\nIntelligence data is almost always uncertain and incomplete, making it\nnecessary to reason and taking decisions under uncertainty. One way to manage\nthe uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis\ncontains five results regarding multiple target tracks and intelligence\nspecification."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305020v1", 
    "title": "Specifying nonspecific evidence", 
    "arxiv-id": "cs/0305020v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T15:13:22Z", 
    "summary": "In an earlier article [J. Schubert, On nonspecific evidence, Int. J. Intell.\nSyst. 8(6), 711-725 (1993)] we established within Dempster-Shafer theory a\ncriterion function called the metaconflict function. With this criterion we can\npartition into subsets a set of several pieces of evidence with propositions\nthat are weakly specified in the sense that it may be uncertain to which event\na proposition is referring. Each subset in the partitioning is representing a\nseparate event. The metaconflict function was derived as the plausibility that\nthe partitioning is correct when viewing the conflict in Dempster's rule within\neach subset as a newly constructed piece of metalevel evidence with a\nproposition giving support against the entire partitioning. In this article we\nextend the results of the previous article. We will not only find the most\nplausible subset for each piece of evidence as was done in the earlier article.\nIn addition we will specify each piece of nonspecific evidence, in the sense\nthat we find to which events the proposition might be referring, by finding the\nplausibility for every subset that this piece of evidence belong to the subset.\nIn doing this we will automatically receive indication that some evidence might\nbe false. We will then develop a new methodology to exploit these newly\nspecified pieces of evidence in a subsequent reasoning process. This will\ninclude methods to discount evidence based on their degree of falsity and on\ntheir degree of credibility due to a partial specification of affiliation, as\nwell as a refined method to infer the event of each subset."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305021v1", 
    "title": "Creating Prototypes for Fast Classification in Dempster-Shafer   Clustering", 
    "arxiv-id": "cs/0305021v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T15:32:14Z", 
    "summary": "We develop a classification method for incoming pieces of evidence in\nDempster-Shafer theory. This methodology is based on previous work with\nclustering and specification of originally nonspecific evidence. This\nmethodology is here put in order for fast classification of future incoming\npieces of evidence by comparing them with prototypes representing the clusters,\ninstead of making a full clustering of all evidence. This method has a\ncomputational complexity of O(M * N) for each new piece of evidence, where M is\nthe maximum number of subsets and N is the number of prototypes chosen for each\nsubset. That is, a computational complexity independent of the total number of\npreviously arrived pieces of evidence. The parameters M and N are typically\nfixed and domain dependent in any application."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305023v1", 
    "title": "Fast Dempster-Shafer clustering using a neural network structure", 
    "arxiv-id": "cs/0305023v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T15:48:24Z", 
    "summary": "In this paper we study a problem within Dempster-Shafer theory where 2**n - 1\npieces of evidence are clustered by a neural structure into n clusters. The\nclustering is done by minimizing a metaconflict function. Previously we\ndeveloped a method based on iterative optimization. However, for large scale\nproblems we need a method with lower computational complexity. The neural\nstructure was found to be effective and much faster than iterative optimization\nfor larger problems. While the growth in metaconflict was faster for the neural\nstructure compared with iterative optimization in medium sized problems, the\nmetaconflict per cluster and evidence was moderate. The neural structure was\nable to find a global minimum over ten runs for problem sizes up to six\nclusters."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICASSP.1998.675495", 
    "link": "http://arxiv.org/pdf/cs/0305024v1", 
    "title": "A neural network and iterative optimization hybrid for Dempster-Shafer   clustering", 
    "arxiv-id": "cs/0305024v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T15:54:46Z", 
    "summary": "In this paper we extend an earlier result within Dempster-Shafer theory\n[\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc.\nSeventh Int. Conf. Information Processing and Management of Uncertainty in\nKnowledge-Based Systems (IPMU 98)] where a large number of pieces of evidence\nare clustered into subsets by a neural network structure. The clustering is\ndone by minimizing a metaconflict function. Previously we developed a method\nbased on iterative optimization. While the neural method had a much lower\ncomputation time than iterative optimization its average clustering performance\nwas not as good. Here, we develop a hybrid of the two methods. We let the\nneural structure do the initial clustering in order to achieve a high\ncomputational performance. Its solution is fed as the initial state to the\niterative optimization in order to improve the clustering performance."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305025v1", 
    "title": "Simultaneous Dempster-Shafer clustering and gradual determination of   number of clusters using a neural network structure", 
    "arxiv-id": "cs/0305025v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:00:23Z", 
    "summary": "In this paper we extend an earlier result within Dempster-Shafer theory\n[\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc.\nSeventh Int. Conf. Information Processing and Management of Uncertainty in\nKnowledge-Based Systems (IPMU'98)] where several pieces of evidence were\nclustered into a fixed number of clusters using a neural structure. This was\ndone by minimizing a metaconflict function. We now develop a method for\nsimultaneous clustering and determination of number of clusters during\niteration in the neural structure. We let the output signals of neurons\nrepresent the degree to which a pieces of evidence belong to a corresponding\ncluster. From these we derive a probability distribution regarding the number\nof clusters, which gradually during the iteration is transformed into a\ndetermination of number of clusters. This gradual determination is fed back\ninto the neural structure at each iteration to influence the clustering\nprocess."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305026v1", 
    "title": "Fast Dempster-Shafer clustering using a neural network structure", 
    "arxiv-id": "cs/0305026v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:06:22Z", 
    "summary": "In this article we study a problem within Dempster-Shafer theory where 2**n -\n1 pieces of evidence are clustered by a neural structure into n clusters. The\nclustering is done by minimizing a metaconflict function. Previously we\ndeveloped a method based on iterative optimization. However, for large scale\nproblems we need a method with lower computational complexity. The neural\nstructure was found to be effective and much faster than iterative optimization\nfor larger problems. While the growth in metaconflict was faster for the neural\nstructure compared with iterative optimization in medium sized problems, the\nmetaconflict per cluster and evidence was moderate. The neural structure was\nable to find a global minimum over ten runs for problem sizes up to six\nclusters."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305027v1", 
    "title": "Managing Inconsistent Intelligence", 
    "arxiv-id": "cs/0305027v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:16:17Z", 
    "summary": "In this paper we demonstrate that it is possible to manage intelligence in\nconstant time as a pre-process to information fusion through a series of\nprocesses dealing with issues such as clustering reports, ranking reports with\nrespect to importance, extraction of prototypes from clusters and immediate\nclassification of newly arriving intelligence reports. These methods are used\nwhen intelligence reports arrive which concerns different events which should\nbe handled independently, when it is not known a priori to which event each\nintelligence report is related. We use clustering that runs as a back-end\nprocess to partition the intelligence into subsets representing the events, and\nin parallel, a fast classification that runs as a front-end process in order to\nput the newly arriving intelligence into its correct information fusion\nprocess."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305028v1", 
    "title": "Dempster-Shafer clustering using Potts spin mean field theory", 
    "arxiv-id": "cs/0305028v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:28:20Z", 
    "summary": "In this article we investigate a problem within Dempster-Shafer theory where\n2**q - 1 pieces of evidence are clustered into q clusters by minimizing a\nmetaconflict function, or equivalently, by minimizing the sum of weight of\nconflict over all clusters. Previously one of us developed a method based on a\nHopfield and Tank model. However, for very large problems we need a method with\nlower computational complexity. We demonstrate that the weight of conflict of\nevidence can, as an approximation, be linearized and mapped to an\nantiferromagnetic Potts Spin model. This facilitates efficient numerical\nsolution, even for large problem sizes. Optimal or nearly optimal solutions are\nfound for Dempster-Shafer clustering benchmark tests with a time complexity of\napproximately O(N**2 log**2 N). Furthermore, an isomorphism between the\nantiferromagnetic Potts spin model and a graph optimization problem is shown.\nThe graph model has dynamic variables living on the links, which have a priori\nprobabilities that are directly related to the pairwise conflict between pieces\nof evidence. Hence, the relations between three different models are shown."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305029v1", 
    "title": "Conflict-based Force Aggregation", 
    "arxiv-id": "cs/0305029v1", 
    "author": "Johan Walter", 
    "publish": "2003-05-16T16:37:01Z", 
    "summary": "In this paper we present an application where we put together two methods for\nclustering and classification into a force aggregation method. Both methods are\nbased on conflicts between elements. These methods work with different type of\nelements (intelligence reports, vehicles, military units) on different\nhierarchical levels using specific conflict assessment methods on each level.\nWe use Dempster-Shafer theory for conflict calculation between elements,\nDempster-Shafer clustering for clustering these elements, and templates for\nclassification. The result of these processes is a complete force aggregation\non all levels handled."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305030v1", 
    "title": "Reliable Force Aggregation Using a Refined Evidence Specification from   Dempster-Shafer Clustering", 
    "arxiv-id": "cs/0305030v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:45:38Z", 
    "summary": "In this paper we develop methods for selection of templates and use these\ntemplates to recluster an already performed Dempster-Shafer clustering taking\ninto account intelligence to template fit during the reclustering phase. By\nthis process the risk of erroneous force aggregation based on some misplace\npieces of evidence from the first clustering process is greatly reduced.\nFinally, a more reliable force aggregation is performed using the result of the\nsecond clustering. These steps are taken in order to maintain most of the\nexcellent computational performance of Dempster-Shafer clustering, while at the\nsame time improve on the clustering result by including some higher relations\namong intelligence reports described by the templates. The new improved\nalgorithm has a computational complexity of O(n**3 log**2 n) compared to O(n**2\nlog**2 n) of standard Dempster-Shafer clustering using Potts spin mean field\ntheory."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305031v1", 
    "title": "Clustering belief functions based on attracting and conflicting   metalevel evidence", 
    "arxiv-id": "cs/0305031v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:52:48Z", 
    "summary": "In this paper we develop a method for clustering belief functions based on\nattracting and conflicting metalevel evidence. Such clustering is done when the\nbelief functions concern multiple events, and all belief functions are mixed\nup. The clustering process is used as the means for separating the belief\nfunctions into subsets that should be handled independently. While the\nconflicting metalevel evidence is generated internally from pairwise conflicts\nof all belief functions, the attracting metalevel evidence is assumed given by\nsome external source."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0305032v1", 
    "title": "Robust Report Level Cluster-to-Track Fusion", 
    "arxiv-id": "cs/0305032v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T16:58:38Z", 
    "summary": "In this paper we develop a method for report level tracking based on\nDempster-Shafer clustering using Potts spin neural networks where clusters of\nincoming reports are gradually fused into existing tracks, one cluster for each\ntrack. Incoming reports are put into a cluster and continuous reclustering of\nolder reports is made in order to obtain maximum association fit within the\ncluster and towards the track. Over time, the oldest reports of the cluster\nleave the cluster for the fixed track at the same rate as new incoming reports\nare put into it. Fusing reports to existing tracks in this fashion allows us to\ntake account of both existing tracks and the probable future of each track, as\nrepresented by younger reports within the corresponding cluster. This gives us\na robust report-to-track association. Compared to clustering of all available\nreports this approach is computationally faster and has a better\nreport-to-track association than simple step-by-step association."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0307060v2", 
    "title": "Neural realisation of the SP theory: cell assemblies revisited", 
    "arxiv-id": "cs/0307060v2", 
    "author": "J. Gerard Wolff", 
    "publish": "2003-07-27T22:53:12Z", 
    "summary": "This paper describes how the elements of the SP theory (Wolff, 2003a) may be\nrealised with neural structures and processes. To the extent that this is\nsuccessful, the insights that have been achieved in the SP theory - the\nintegration and simplification of a range of phenomena in perception and\ncognition - may be incorporated in a neural view of brain function.\n  These proposals may be seen as a development of Hebb's (1949) concept of a\n'cell assembly'. By contrast with that concept and variants of it, the version\ndescribed in this paper proposes that any one neuron can belong in one assembly\nand only one assembly. A distinctive feature of the present proposals is that\nany neuron or cluster of neurons within a cell assembly may serve as a proxy or\nreference for another cell assembly or class of cell assemblies. This device\nprovides solutions to many of the problems associated with cell assemblies, it\nallows information to be stored in a compressed form, and it provides a robust\nmechanism by which assemblies may be connected to form hierarchies, grammars\nand other kinds of knowledge structure.\n  Drawing on insights derived from the SP theory, the paper also describes how\nunsupervised learning may be achieved with neural structures and processes.\nThis theory of learning overcomes weaknesses in the Hebbian concept of learning\nand it is, at the same time, compatible with the observations that Hebb's\ntheory was designed to explain."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0308031v1", 
    "title": "Artificial Neural Networks for Beginners", 
    "arxiv-id": "cs/0308031v1", 
    "author": "Carlos Gershenson", 
    "publish": "2003-08-20T09:40:25Z", 
    "summary": "The scope of this teaching package is to make a brief induction to Artificial\nNeural Networks (ANNs) for people who have no previous knowledge of them. We\nfirst make a brief introduction to models of networks, for then describing in\ngeneral terms ANNs. As an application, we explain the backpropagation\nalgorithm, since it is widely used and many other algorithms are derived from\nit. The user should know algebra and the handling of functions and vectors.\nDifferential calculus is recommendable, but not necessary. The contents of this\npackage should be understood by people with high school education. It would be\nuseful for people who are just curious about what are ANNs, or for people who\nwant to become familiar with them, so when they study them more fully, they\nwill already have clear notions of ANNs. Also, people who only want to apply\nthe backpropagation algorithm without a detailed and formal explanation of it\nwill find this material useful. This work should not be seen as \"Nets for\ndummies\", but of course it is not a treatise. Much of the formality is skipped\nfor the sake of simplicity. Detailed explanations and demonstrations can be\nfound in the referred readings. The included exercises complement the\nunderstanding of the theory. The on-line resources are highly recommended for\nextending this brief induction."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0309009v1", 
    "title": "What Is Working Memory and Mental Imagery? A Robot that Learns to   Perform Mental Computations", 
    "arxiv-id": "cs/0309009v1", 
    "author": "Victor Eliashberg", 
    "publish": "2003-09-08T07:31:55Z", 
    "summary": "This paper goes back to Turing (1936) and treats his machine as a cognitive\nmodel (W,D,B), where W is an \"external world\" represented by memory device (the\ntape divided into squares), and (D,B) is a simple robot that consists of the\nsensory-motor devices, D, and the brain, B. The robot's sensory-motor devices\n(the \"eye\", the \"hand\", and the \"organ of speech\") allow the robot to simulate\nthe work of any Turing machine. The robot simulates the internal states of a\nTuring machine by \"talking to itself.\" At the stage of training, the teacher\nforces the robot (by acting directly on its motor centers) to perform several\nexamples of an algorithm with different input data presented on tape. Two\neffects are achieved: 1) The robot learns to perform the shown algorithm with\nany input data using the tape. 2) The robot learns to perform the algorithm\n\"mentally\" using an \"imaginary tape.\" The model illustrates the simplest\nconcept of a universal learning neurocomputer, demonstrates universality of\nassociative learning as the mechanism of programming, and provides a\nsimplified, but nontrivial neurobiologically plausible explanation of the\nphenomena of working memory and mental imagery. The model is implemented as a\nuser-friendly program for Windows called EROBOT. The program is available at\nwww.brain0.com/software.html."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0312003v1", 
    "title": "Hybrid LQG-Neural Controller for Inverted Pendulum System", 
    "arxiv-id": "cs/0312003v1", 
    "author": "R. L. Klein", 
    "publish": "2003-11-30T00:19:19Z", 
    "summary": "The paper presents a hybrid system controller, incorporating a neural and an\nLQG controller. The neural controller has been optimized by genetic algorithms\ndirectly on the inverted pendulum system. The failure free optimization process\nstipulated a relatively small region of the asymptotic stability of the neural\ncontroller, which is concentrated around the regulation point. The presented\nhybrid controller combines benefits of a genetically optimized neural\ncontroller and an LQG controller in a single system controller. High quality of\nthe regulation process is achieved through utilization of the neural\ncontroller, while stability of the system during transient processes and a wide\nrange of operation are assured through application of the LQG controller. The\nhybrid controller has been validated by applying it to a simulation model of an\ninherently unstable system of inverted pendulum."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IDC.1999.754191", 
    "link": "http://arxiv.org/pdf/cs/0312009v1", 
    "title": "Failure-Free Genetic Algorithm Optimization of a System Controller Using   SAFE/LEARNING Controllers in Tandem", 
    "arxiv-id": "cs/0312009v1", 
    "author": "R. L. Klein", 
    "publish": "2003-12-03T22:29:01Z", 
    "summary": "The paper presents a method for failure free genetic algorithm optimization\nof a system controller. Genetic algorithms present a powerful tool that\nfacilitates producing near-optimal system controllers. Applied to such methods\nof computational intelligence as neural networks or fuzzy logic, these methods\nare capable of combining the non-linear mapping capabilities of the latter with\nlearning the system behavior directly, that is, without a prior model. At the\nsame time, genetic algorithms routinely produce solutions that lead to the\nfailure of the controlled system. Such solutions are generally unacceptable for\napplications where safe operation must be guaranteed. We present here a method\nof design, which allows failure-free application of genetic algorithms through\nutilization of SAFE and LEARNING controllers in tandem, where the SAFE\ncontroller recovers the system from dangerous states while the LEARNING\ncontroller learns its behavior. The method has been validated by applying it to\nan inherently unstable system of inverted pendulum."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2003.1223347", 
    "link": "http://arxiv.org/pdf/cs/0401020v1", 
    "title": "Presynaptic modulation as fast synaptic switching: state-dependent   modulation of task performance", 
    "arxiv-id": "cs/0401020v1", 
    "author": "Johann Schumann", 
    "publish": "2004-01-25T04:22:00Z", 
    "summary": "Neuromodulatory receptors in presynaptic position have the ability to\nsuppress synaptic transmission for seconds to minutes when fully engaged. This\neffectively alters the synaptic strength of a connection. Much work on\nneuromodulation has rested on the assumption that these effects are uniform at\nevery neuron. However, there is considerable evidence to suggest that\npresynaptic regulation may be in effect synapse-specific. This would define a\nsecond \"weight modulation\" matrix, which reflects presynaptic receptor efficacy\nat a given site. Here we explore functional consequences of this hypothesis. By\nanalyzing and comparing the weight matrices of networks trained on different\naspects of a task, we identify the potential for a low complexity \"modulation\nmatrix\", which allows to switch between differently trained subtasks while\nretaining general performance characteristics for the task. This means that a\ngiven network can adapt itself to different task demands by regulating its\nrelease of neuromodulators. Specifically, we suggest that (a) a network can\nprovide optimized responses for related classification tasks without the need\nto train entirely separate networks and (b) a network can blend a \"memory mode\"\nwhich aims at reproducing memorized patterns and a \"novelty mode\" which aims to\nfacilitate classification of new patterns. We relate this work to the known\neffects of neuromodulators on brain-state dependent processing."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2003.1223347", 
    "link": "http://arxiv.org/pdf/cs/0402030v1", 
    "title": "Computational complexity and simulation of rare events of Ising spin   glasses", 
    "arxiv-id": "cs/0402030v1", 
    "author": "Fabien Alet", 
    "publish": "2004-02-15T06:58:09Z", 
    "summary": "We discuss the computational complexity of random 2D Ising spin glasses,\nwhich represent an interesting class of constraint satisfaction problems for\nblack box optimization. Two extremal cases are considered: (1) the +/- J spin\nglass, and (2) the Gaussian spin glass. We also study a smooth transition\nbetween these two extremal cases. The computational complexity of all studied\nspin glass systems is found to be dominated by rare events of extremely hard\nspin glass samples. We show that complexity of all studied spin glass systems\nis closely related to Frechet extremal value distribution. In a hybrid\nalgorithm that combines the hierarchical Bayesian optimization algorithm (hBOA)\nwith a deterministic bit-flip hill climber, the number of steps performed by\nboth the global searcher (hBOA) and the local searcher follow Frechet\ndistributions. Nonetheless, unlike in methods based purely on local search, the\nparameters of these distributions confirm good scalability of hBOA with local\nsearch. We further argue that standard performance measures for optimization\nalgorithms--such as the average number of evaluations until convergence--can be\nmisleading. Finally, our results indicate that for highly multimodal constraint\nsatisfaction problems, such as Ising spin glasses, recombination-based search\ncan provide qualitatively better results than mutation-based search."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2003.1223347", 
    "link": "http://arxiv.org/pdf/cs/0402031v1", 
    "title": "Parameter-less hierarchical BOA", 
    "arxiv-id": "cs/0402031v1", 
    "author": "Tz-Kai Lin", 
    "publish": "2004-02-15T06:56:45Z", 
    "summary": "The parameter-less hierarchical Bayesian optimization algorithm (hBOA)\nenables the use of hBOA without the need for tuning parameters for solving each\nproblem instance. There are three crucial parameters in hBOA: (1) the selection\npressure, (2) the window size for restricted tournaments, and (3) the\npopulation size. Although both the selection pressure and the window size\ninfluence hBOA performance, performance should remain low-order polynomial with\nstandard choices of these two parameters. However, there is no standard\npopulation size that would work for all problems of interest and the population\nsize must thus be eliminated in a different way. To eliminate the population\nsize, the parameter-less hBOA adopts the population-sizing technique of the\nparameter-less genetic algorithm. Based on the existing theory, the\nparameter-less hBOA should be able to solve nearly decomposable and\nhierarchical problems in quadratic or subquadratic number of function\nevaluations without the need for setting any parameters whatsoever. A number of\nexperiments are presented to verify scalability of the parameter-less hBOA."
},{
    "category": "cs.OH", 
    "doi": "10.1109/IJCNN.2003.1223347", 
    "link": "http://arxiv.org/pdf/cs/0403027v2", 
    "title": "An approach to membrane computing under inexactitude", 
    "arxiv-id": "cs/0403027v2", 
    "author": "Francesc Rossello", 
    "publish": "2004-03-16T09:02:39Z", 
    "summary": "In this paper we introduce a fuzzy version of symport/antiport membrane\nsystems. Our fuzzy membrane systems handle possibly inexact copies of reactives\nand their rules are endowed with threshold functions that determine whether a\nrule can be applied or not to a given set of objects, depending of the degree\nof accuracy of these objects to the reactives specified in the rule. We prove\nthat these fuzzy membrane systems generate exactly the recursively enumerable\nfinite-valued fuzzy sets of natural numbers."
},{
    "category": "cs.PF", 
    "doi": "10.1109/IJCNN.2003.1223347", 
    "link": "http://arxiv.org/pdf/cs/0404001v1", 
    "title": "On the Practicality of Intrinsic Reconfiguration As a Fault Recovery   Method in Analog Systems", 
    "arxiv-id": "cs/0404001v1", 
    "author": "Garrison W. Greenwood", 
    "publish": "2004-04-01T16:54:42Z", 
    "summary": "Evolvable hardware combines the powerful search capability of evolutionary\nalgorithms with the flexibility of reprogrammable devices, thereby providing a\nnatural framework for reconfiguration. This framework has generated an interest\nin using evolvable hardware for fault-tolerant systems because reconfiguration\ncan effectively deal with hardware faults whenever it is impossible to provide\nspares. But systems cannot tolerate faults indefinitely, which means\nreconfiguration does have a deadline. The focus of previous evolvable hardware\nresearch relating to fault-tolerance has been primarily restricted to restoring\nfunctionality, with no real consideration of time constraints. In this paper we\nare concerned with evolvable hardware performing reconfiguration under deadline\nconstraints. In particular, we investigate reconfigurable hardware that\nundergoes intrinsic evolution. We show that fault recovery done by intrinsic\nreconfiguration has some restrictions, which designers cannot ignore."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548001", 
    "link": "http://arxiv.org/pdf/cs/0404017v1", 
    "title": "Exploring tradeoffs in pleiotropy and redundancy using evolutionary   computing", 
    "arxiv-id": "cs/0404017v1", 
    "author": "Derek Abbott", 
    "publish": "2004-04-07T06:23:18Z", 
    "summary": "Evolutionary computation algorithms are increasingly being used to solve\noptimization problems as they have many advantages over traditional\noptimization algorithms. In this paper we use evolutionary computation to study\nthe trade-off between pleiotropy and redundancy in a client-server based\nnetwork. Pleiotropy is a term used to describe components that perform multiple\ntasks, while redundancy refers to multiple components performing one same task.\nPleiotropy reduces cost but lacks robustness, while redundancy increases\nnetwork reliability but is more costly, as together, pleiotropy and redundancy\nbuild flexibility and robustness into systems. Therefore it is desirable to\nhave a network that contains a balance between pleiotropy and redundancy. We\nexplore how factors such as link failure probability, repair rates, and the\nsize of the network influence the design choices that we explore using genetic\nalgorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0404019v1", 
    "title": "Optimizing genetic algorithm strategies for evolving networks", 
    "arxiv-id": "cs/0404019v1", 
    "author": "Derek Abbott", 
    "publish": "2004-04-07T07:08:09Z", 
    "summary": "This paper explores the use of genetic algorithms for the design of networks,\nwhere the demands on the network fluctuate in time. For varying network\nconstraints, we find the best network using the standard genetic algorithm\noperators such as inversion, mutation and crossover. We also examine how the\nchoice of genetic algorithm operators affects the quality of the best network\nfound. Such networks typically contain redundancy in servers, where several\nservers perform the same task and pleiotropy, where servers perform multiple\ntasks. We explore this trade-off between pleiotropy versus redundancy on the\ncost versus reliability as a measure of the quality of the network."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0404042v2", 
    "title": "Extraction of topological features from communication network   topological patterns using self-organizing feature maps", 
    "arxiv-id": "cs/0404042v2", 
    "author": "F. Alavi", 
    "publish": "2004-04-21T16:05:27Z", 
    "summary": "Different classes of communication network topologies and their\nrepresentation in the form of adjacency matrix and its eigenvalues are\npresented. A self-organizing feature map neural network is used to map\ndifferent classes of communication network topological patterns. The neural\nnetwork simulation results are reported."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0404045v2", 
    "title": "Speculation on graph computation architectures and computing via   synchronization", 
    "arxiv-id": "cs/0404045v2", 
    "author": "Bayle Shanks", 
    "publish": "2004-04-22T07:29:19Z", 
    "summary": "A speculative overview of a future topic of research. The paper is a\ncollection of ideas concerning two related areas:\n  1) Graph computation machines (\"computing with graphs\"). This is the class of\nmodels of computation in which the state of the computation is represented as a\ngraph or network.\n  2) Arc-based neural networks, which store information not as activation in\nthe nodes, but rather by adding and deleting arcs. Sometimes the arcs may be\ninterpreted as synchronization.\n  Warnings to readers: this is not the sort of thing that one might submit to a\njournal or conference. No proofs are presented. The presentation is informal,\nand written at an introductory level. You'll probably want to wait for a more\nconcise presentation."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0405027v1", 
    "title": "Evolution of a Subsumption Architecture Neurocontroller", 
    "arxiv-id": "cs/0405027v1", 
    "author": "Julian Togelius", 
    "publish": "2004-05-06T19:37:07Z", 
    "summary": "An approach to robotics called layered evolution and merging features from\nthe subsumption architecture into evolutionary robotics is presented, and its\nadvantages are discussed. This approach is used to construct a layered\ncontroller for a simulated robot that learns which light source to approach in\nan environment with obstacles. The evolvability and performance of layered\nevolution on this task is compared to (standard) monolithic evolution,\nincremental and modularised evolution. To corroborate the hypothesis that a\nlayered controller performs at least as well as an integrated one, the evolved\nlayers are merged back into a single network. On the grounds of the test\nresults, it is argued that layered evolution provides a superior approach for\nmany tasks, and it is suggested that this approach may be the key to scaling up\nevolutionary robotics."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0406007v1", 
    "title": "Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis", 
    "arxiv-id": "cs/0406007v1", 
    "author": "Martin Pelikan", 
    "publish": "2004-06-03T15:43:46Z", 
    "summary": "Estimation of Distribution Algorithms have been proposed as a new paradigm\nfor evolutionary optimization. This paper focuses on the parallelization of\nEstimation of Distribution Algorithms. More specifically, the paper discusses\nhow to predict performance of parallel Mixed Bayesian Optimization Algorithm\n(MBOA) that is based on parallel construction of Bayesian networks with\ndecision trees. We determine the time complexity of parallel Mixed Bayesian\nOptimization Algorithm and compare this complexity with experimental results\nobtained by solving the spin glass optimization problem. The empirical results\nfit well the theoretical time complexity, so the scalability and efficiency of\nparallel Mixed Bayesian Optimization Algorithm for unknown instances of spin\nglass benchmarks can be predicted. Furthermore, we derive the guidelines that\ncan be used to design effective parallel Estimation of Distribution Algorithms\nwith the speedup proportional to the number of variables in the problem."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0406017v2", 
    "title": "Using Self-Organising Mappings to Learn the Structure of Data Manifolds", 
    "arxiv-id": "cs/0406017v2", 
    "author": "Stephen Luttrell", 
    "publish": "2004-06-08T14:45:45Z", 
    "summary": "In this paper it is shown how to map a data manifold into a simpler form by\nprogressively discarding small degrees of freedom. This is the key to\nself-organising data fusion, where the raw data is embedded in a very\nhigh-dimensional space (e.g. the pixel values of one or more images), and the\nrequirement is to isolate the important degrees of freedom which lie on a\nlow-dimensional manifold. A useful advantage of the approach used in this paper\nis that the computations are arranged as a feed-forward processing chain, where\nall the details of the processing in each stage of the chain are learnt by\nself-organisation. This approach is demonstrated using hierarchically\ncorrelated data, which causes the processing chain to split the data into\nseparate processing channels, and then to progressively merge these channels\nwherever they are correlated with each other. This is the key to\nself-organising data fusion."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0408049v1", 
    "title": "Using Stochastic Encoders to Discover Structure in Data", 
    "arxiv-id": "cs/0408049v1", 
    "author": "Stephen Luttrell", 
    "publish": "2004-08-21T19:40:24Z", 
    "summary": "In this paper a stochastic generalisation of the standard Linde-Buzo-Gray\n(LBG) approach to vector quantiser (VQ) design is presented, in which the\nencoder is implemented as the sampling of a vector of code indices from a\nprobability distribution derived from the input vector, and the decoder is\nimplemented as a superposition of reconstruction vectors. This stochastic VQ\n(SVQ) is optimised using a minimum mean Euclidean reconstruction distortion\ncriterion, as in the LBG case. Numerical simulations are used to demonstrate\nhow this leads to self-organisation of the SVQ, where different stochastically\nsampled code indices become associated with different input subspaces."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0408050v1", 
    "title": "Invariant Stochastic Encoders", 
    "arxiv-id": "cs/0408050v1", 
    "author": "Stephen Luttrell", 
    "publish": "2004-08-21T23:06:45Z", 
    "summary": "The theory of stochastic vector quantisers (SVQ) has been extended to allow\nthe quantiser to develop invariances, so that only \"large\" degrees of freedom\nin the input vector are represented in the code. This has been applied to the\nproblem of encoding data vectors which are a superposition of a \"large\" jammer\nand a \"small\" signal, so that only the jammer is represented in the code. This\nallows the jammer to be subtracted from the total input vector (i.e. the jammer\nis nulled), leaving a residual that contains only the underlying signal. The\nmain advantage of this approach to jammer nulling is that little prior\nknowledge of the jammer is assumed, because these properties are automatically\ndiscovered by the SVQ as it is trained on examples of input vectors."
},{
    "category": "cs.LG", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0408058v1", 
    "title": "Non-negative matrix factorization with sparseness constraints", 
    "arxiv-id": "cs/0408058v1", 
    "author": "Patrik O. Hoyer", 
    "publish": "2004-08-25T20:25:43Z", 
    "summary": "Non-negative matrix factorization (NMF) is a recently developed technique for\nfinding parts-based, linear representations of non-negative data. Although it\nhas successfully been applied in several applications, it does not always\nresult in parts-based representations. In this paper, we show how explicitly\nincorporating the notion of `sparseness' improves the found decompositions.\nAdditionally, we provide complete MATLAB code both for standard NMF and for our\nextension. Our hope is that this will further the application of these methods\nto solving novel data-analysis problems."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0410020v1", 
    "title": "Adaptive Cluster Expansion (ACE): A Hierarchical Bayesian Network", 
    "arxiv-id": "cs/0410020v1", 
    "author": "Stephen Luttrell", 
    "publish": "2004-10-10T18:30:03Z", 
    "summary": "Using the maximum entropy method, we derive the \"adaptive cluster expansion\"\n(ACE), which can be trained to estimate probability density functions in high\ndimensional spaces. The main advantage of ACE over other Bayesian networks is\nits ability to capture high order statistics after short training times, which\nit achieves by making use of a hierarchical vector quantisation of the input\ndata. We derive a scheme for representing the state of an ACE network as a\n\"probability image\", which allows us to identify statistically anomalous\nregions in an otherwise statistically homogeneous image, for instance. Finally,\nwe present some probability images that we obtained after training ACE on some\nBrodatz texture images - these demonstrate the ability of ACE to detect subtle\ntextural anomalies."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0411052v1", 
    "title": "Spontaneous Dynamics of Asymmetric Random Recurrent Spiking Neural   Networks", 
    "arxiv-id": "cs/0411052v1", 
    "author": "O. Mazet", 
    "publish": "2004-11-17T10:04:55Z", 
    "summary": "We study in this paper the effect of an unique initial stimulation on random\nrecurrent networks of leaky integrate and fire neurons. Indeed given a\nstochastic connectivity this so-called spontaneous mode exhibits various non\ntrivial dynamics. This study brings forward a mathematical formalism that\nallows us to examine the variability of the afterward dynamics according to the\nparameters of the weight distribution. Provided independence hypothesis (e.g.\nin the case of very large networks) we are able to compute the average number\nof neurons that fire at a given time -- the spiking activity. In accordance\nwith numerical simulations, we prove that this spiking activity reaches a\nsteady-state, we characterize this steady-state and explore the transients."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0412023v1", 
    "title": "Multidimensional data classification with artificial neural networks", 
    "arxiv-id": "cs/0412023v1", 
    "author": "A. De Angelis", 
    "publish": "2004-12-06T20:23:15Z", 
    "summary": "Multi-dimensional data classification is an important and challenging problem\nin many astro-particle experiments. Neural networks have proved to be versatile\nand robust in multi-dimensional data classification. In this article we shall\nstudy the classification of gamma from the hadrons for the MAGIC Experiment.\nTwo neural networks have been used for the classification task. One is\nMulti-Layer Perceptron based on supervised learning and other is\nSelf-Organising Map (SOM), which is based on unsupervised learning technique.\nThe results have been shown and the possible ways of combining these networks\nhave been proposed to yield better and faster classification results."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.548122", 
    "link": "http://arxiv.org/pdf/cs/0412059v1", 
    "title": "Vector Symbolic Architectures answer Jackendoff's challenges for   cognitive neuroscience", 
    "arxiv-id": "cs/0412059v1", 
    "author": "Ross W. Gayler", 
    "publish": "2004-12-13T08:00:55Z", 
    "summary": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and\nrules of language present to theories of brain function. The essence of these\nproblems is the question of how to neurally instantiate the rapid construction\nand transformation of the compositional structures that are typically taken to\nbe the domain of symbolic processing. He contended that typical connectionist\napproaches fail to meet these challenges and that the dialogue between\nlinguistic theory and cognitive neuroscience will be relatively unproductive\nuntil the importance of these problems is widely recognised and the challenges\nanswered by some technical innovation in connectionist modelling. This paper\nclaims that a little-known family of connectionist models (Vector Symbolic\nArchitectures) are able to meet Jackendoff's challenges."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299832", 
    "link": "http://arxiv.org/pdf/cs/0412071v1", 
    "title": "Web Usage Mining Using Artificial Ant Colony Clustering and Genetic   Programming", 
    "arxiv-id": "cs/0412071v1", 
    "author": "Vitorino Ramos", 
    "publish": "2004-12-17T14:58:38Z", 
    "summary": "The rapid e-commerce growth has made both business community and customers\nface a new situation. Due to intense competition on one hand and the customer's\noption to choose from several alternatives business community has realized the\nnecessity of intelligent marketing strategies and relationship management. Web\nusage mining attempts to discover useful knowledge from the secondary data\nobtained from the interactions of the users with the Web. Web usage mining has\nbecome very critical for effective Web site management, creating adaptive Web\nsites, business and support services, personalization, network traffic flow\nanalysis and so on. The study of ant colonies behavior and their\nself-organizing capabilities is of interest to knowledge retrieval/management\nand decision support systems sciences, because it provides models of\ndistributed adaptive organization, which are useful to solve difficult\noptimization, classification, and distributed control problems, among others.\nIn this paper, we propose an ant clustering algorithm to discover Web usage\npatterns (data clusters) and a linear genetic programming approach to analyze\nthe visitor trends. Empirical results clearly shows that ant colony clustering\nperforms well when compared to a self-organizing map (for clustering Web usage\npatterns) even though the performance accuracy is not that efficient when\ncomparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web\nUsage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear\nGenetic Programming."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412072v1", 
    "title": "Swarms on Continuous Data", 
    "arxiv-id": "cs/0412072v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-12-17T15:05:40Z", 
    "summary": "While being it extremely important, many Exploratory Data Analysis (EDA)\nsystems have the inhability to perform classification and visualization in a\ncontinuous basis or to self-organize new data-items into the older ones\n(evenmore into new labels if necessary), which can be crucial in KDD -\nKnowledge Discovery, Retrieval and Data Mining Systems (interactive and online\nforms of Web Applications are just one example). This disadvantge is also\npresent in more recent approaches using Self-Organizing Maps. On the present\nwork, and exploiting past sucesses in recently proposed Stigmergic Ant Systems\na robust online classifier is presented, which produces class decisions on a\ncontinuous stream data, allowing for continuous mappings. Results show that\nincreasingly better results are achieved, as demonstraded by other authors in\ndifferent areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy,\nData-Mining, Exploratory Data Analysis, Image Retrieval, Continuous\nClassification."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412080v1", 
    "title": "The Biological Concept of Neoteny in Evolutionary Colour Image   Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms", 
    "arxiv-id": "cs/0412080v1", 
    "author": "Vitorino Ramos", 
    "publish": "2004-12-17T16:39:33Z", 
    "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as\nthe retention by an organism of juvenile or even larval traits into later life.\nIn some species, all morphological development is retarded; the organism is\njuvenilized but sexually mature. Such shifts of reproductive capability would\nappear to have adaptive significance to organisms that exhibit it. In terms of\nevolutionary theory, the process of paedomorphosis suggests that larval stages\nand developmental phases of existing organisms may give rise, under certain\ncircumstances, to wholly new organisms. Although the present work does not\npretend to model or simulate the biological details of such a concept in any\nway, these ideas were incorporated by a rather simple abstract computational\nstrategy, in order to allow (if possible) for faster convergence into simple\nnon-memetic Genetic Algorithms, i.e. without using local improvement procedures\n(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic\nAlgorithm was used for colour image segmentation purposes by using K-mean\nunsupervised clustering methods, namely for guiding the evolutionary algorithm\nin his search for finding the optimal or sub-optimal data partition. Average\nresults suggest that the use of neotonic strategies by employing juvenile\ngenotypes into the later generations and the use of linear-dynamic mutation\nrates instead of constant, can increase fitness values by 58% comparing to\nclassical Genetic Algorithms, independently from the starting population\ncharacteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial\nNeoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,\nClassification, Clustering."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412081v1", 
    "title": "Artificial Neoteny in Evolutionary Image Segmentation", 
    "arxiv-id": "cs/0412081v1", 
    "author": "Vitorino Ramos", 
    "publish": "2004-12-17T16:44:54Z", 
    "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as\nthe retention by an organism of juvenile or even larval traits into later life.\nIn some species, all morphological development is retarded; the organism is\njuvenilized but sexually mature. Such shifts of reproductive capability would\nappear to have adaptive significance to organisms that exhibit it. In terms of\nevolutionary theory, the process of paedomorphosis suggests that larval stages\nand developmental phases of existing organisms may give rise, under certain\ncircumstances, to wholly new organisms. Although the present work does not\npretend to model or simulate the biological details of such a concept in any\nway, these ideas were incorporated by a rather simple abstract computational\nstrategy, in order to allow (if possible) for faster convergence into simple\nnon-memetic Genetic Algorithms, i.e. without using local improvement procedures\n(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic\nAlgorithm was used for colour image segmentation purposes by using K-mean\nunsupervised clustering methods, namely for guiding the evolutionary algorithm\nin his search for finding the optimal or sub-optimal data partition. Average\nresults suggest that the use of neotonic strategies by employing juvenile\ngenotypes into the later generations and the use of linear-dynamic mutation\nrates instead of constant, can increase fitness values by 58% comparing to\nclassical Genetic Algorithms, independently from the starting population\ncharacteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial\nNeoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,\nClassification, Clustering."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412084v1", 
    "title": "Map Segmentation by Colour Cube Genetic K-Mean Clustering", 
    "arxiv-id": "cs/0412084v1", 
    "author": "Fernando Muge", 
    "publish": "2004-12-17T17:04:16Z", 
    "summary": "Segmentation of a colour image composed of different kinds of texture regions\ncan be a hard problem, namely to compute for an exact texture fields and a\ndecision of the optimum number of segmentation areas in an image when it\ncontains similar and/or unstationary texture fields. In this work, a method is\ndescribed for evolving adaptive procedures for these problems. In many real\nworld applications data clustering constitutes a fundamental issue whenever\nbehavioural or feature domains can be mapped into topological domains. We\nformulate the segmentation problem upon such images as an optimisation problem\nand adopt evolutionary strategy of Genetic Algorithms for the clustering of\nsmall regions in colour feature space. The present approach uses k-Means\nunsupervised clustering methods into Genetic Algorithms, namely for guiding\nthis last Evolutionary Algorithm in his search for finding the optimal or\nsub-optimal data partition, task that as we know, requires a non-trivial search\nbecause of its NP-complete nature. To solve this task, the appropriate genetic\ncoding is also discussed, since this is a key aspect in the implementation. Our\npurpose is to demonstrate the efficiency of Genetic Algorithms to automatic and\nunsupervised texture segmentation. Some examples in Colour Maps are presented\nand overall results discussed. KEYWORDS: Genetic Algorithms, Artificial\nNeoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,\nClassification, Clustering."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412109v1", 
    "title": "Global minimization of a quadratic functional: neural network approach", 
    "arxiv-id": "cs/0412109v1", 
    "author": "B. M. Magomedov", 
    "publish": "2004-12-24T13:41:10Z", 
    "summary": "The problem of finding out the global minimum of a multiextremal functional\nis discussed. One frequently faces with such a functional in various\napplications. We propose a procedure, which depends on the dimensionality of\nthe problem polynomially. In our approach we use the eigenvalues and\neigenvectors of the connection matrix."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0412110v1", 
    "title": "Q-valued neural network as a system of fast identification and pattern   recognition", 
    "arxiv-id": "cs/0412110v1", 
    "author": "A. B. Fonarev", 
    "publish": "2004-12-24T13:48:44Z", 
    "summary": "An effective neural network algorithm of the perceptron type is proposed. The\nalgorithm allows us to identify strongly distorted input vector reliably. It is\nshown that its reliability and processing speed are orders of magnitude higher\nthan that of full connected neural networks. The processing speed of our\nalgorithm exceeds the one of the stack fast-access retrieval algorithm that is\nmodified for working when there are noises in the input channel."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502006v1", 
    "title": "Neural network ensembles: Evaluation of aggregation algorithms", 
    "arxiv-id": "cs/0502006v1", 
    "author": "H. A. Ceccatto", 
    "publish": "2005-02-01T21:22:24Z", 
    "summary": "Ensembles of artificial neural networks show improved generalization\ncapabilities that outperform those of single networks. However, for aggregation\nto be effective, the individual networks must be as accurate and diverse as\npossible. An important problem is, then, how to tune the aggregate members in\norder to have an optimal compromise between these two conflicting conditions.\nWe present here an extensive evaluation of several algorithms for ensemble\nconstruction, including new proposals and comparing them with standard methods\nin the literature. We also discuss a potential problem with sequential\naggregation algorithms: the non-frequent but damaging selection through their\nheuristics of particularly bad ensemble members. We introduce modified\nalgorithms that cope with this problem by allowing individual weighting of\naggregate members. Our algorithms and their weighted modifications are\nfavorably tested against other methods in the literature, producing a sensible\nimprovement in performance on most of the standard statistical databases used\nas benchmarks."
},{
    "category": "cs.CE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502007v1", 
    "title": "Identification of complex systems in the basis of wavelets", 
    "arxiv-id": "cs/0502007v1", 
    "author": "Alexander Shaydurov", 
    "publish": "2005-02-02T00:12:45Z", 
    "summary": "In this paper is proposed the method of the identification of complex dynamic\nsystems. Method can be used for the identification of linear and nonlinear\ncomplex dynamic systems for the determined or stochastic signals at the inputs\nand the outputs. It is proposed to use a basis of wavelets for obtaining the\nimpulse transient function (ITF) of system. ITF is considered in the form of\nsurface in the 3D space. Are given the results of experiments on the\nidentification of systems in the basis of wavelets."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502020v1", 
    "title": "Population Sizing for Genetic Programming Based Upon Decision Making", 
    "arxiv-id": "cs/0502020v1", 
    "author": "D. E. Goldberg", 
    "publish": "2005-02-04T03:58:26Z", 
    "summary": "This paper derives a population sizing relationship for genetic programming\n(GP). Following the population-sizing derivation for genetic algorithms in\nGoldberg, Deb, and Clark (1992), it considers building block decision making as\na key facet. The analysis yields a GP-unique relationship because it has to\naccount for bloat and for the fact that GP solutions often use subsolution\nmultiple times. The population-sizing relationship depends upon tree size,\nsolution complexity, problem difficulty and building block expression\nprobability. The relationship is used to analyze and empirically investigate\npopulation sizing for three model GP problems named ORDER, ON-OFF and LOUD.\nThese problems exhibit bloat to differing extents and differ in whether their\nsolutions require the use of a building block multiple times."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502021v1", 
    "title": "Oiling the Wheels of Change: The Role of Adaptive Automatic Problem   Decomposition in Non--Stationary Environments", 
    "arxiv-id": "cs/0502021v1", 
    "author": "D. E. Goldberg", 
    "publish": "2005-02-04T04:15:15Z", 
    "summary": "Genetic algorithms (GAs) that solve hard problems quickly, reliably and\naccurately are called competent GAs. When the fitness landscape of a problem\nchanges overtime, the problem is called non--stationary, dynamic or\ntime--variant problem. This paper investigates the use of competent GAs for\noptimizing non--stationary optimization problems. More specifically, we use an\ninformation theoretic approach based on the minimum description length\nprinciple to adaptively identify regularities and substructures that can be\nexploited to respond quickly to changes in the environment. We also develop a\nspecial type of problems with bounded difficulties to test non--stationary\noptimization problems. The results provide new insights into non-stationary\noptimization problems and show that a search algorithm which automatically\nidentifies and exploits possible decompositions is more robust and responds\nquickly to changes than a simple genetic algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502022v1", 
    "title": "Sub-Structural Niching in Non-Stationary Environments", 
    "arxiv-id": "cs/0502022v1", 
    "author": "D. E. Goldberg", 
    "publish": "2005-02-04T04:29:15Z", 
    "summary": "Niching enables a genetic algorithm (GA) to maintain diversity in a\npopulation. It is particularly useful when the problem has multiple optima\nwhere the aim is to find all or as many as possible of these optima. When the\nfitness landscape of a problem changes overtime, the problem is called\nnon--stationary, dynamic or time--variant problem. In these problems, niching\ncan maintain useful solutions to respond quickly, reliably and accurately to a\nchange in the environment. In this paper, we present a niching method that\nworks on the problem substructures rather than the whole solution, therefore it\nhas less space complexity than previously known niching mechanisms. We show\nthat the method is responding accurately when environmental changes occur."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502023v1", 
    "title": "Sub-structural Niching in Estimation of Distribution Algorithms", 
    "arxiv-id": "cs/0502023v1", 
    "author": "D. D. Johnson", 
    "publish": "2005-02-04T04:46:04Z", 
    "summary": "We propose a sub-structural niching method that fully exploits the problem\ndecomposition capability of linkage-learning methods such as the estimation of\ndistribution algorithms and concentrate on maintaining diversity at the\nsub-structural level. The proposed method consists of three key components: (1)\nProblem decomposition and sub-structure identification, (2) sub-structure\nfitness estimation, and (3) sub-structural niche preservation. The\nsub-structural niching method is compared to restricted tournament selection\n(RTS)--a niching method used in hierarchical Bayesian optimization\nalgorithm--with special emphasis on sustained preservation of multiple global\nsolutions of a class of boundedly-difficult, additively-separable multimodal\nproblems. The results show that sub-structural niching successfully maintains\nmultiple global optima over large number of generations and does so with\nsignificantly less population than RTS. Additionally, the market share of each\nof the niche is much closer to the expected level in sub-structural niching\nwhen compared to RTS."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502029v1", 
    "title": "Scalability of Genetic Programming and Probabilistic Incremental Program   Evolution", 
    "arxiv-id": "cs/0502029v1", 
    "author": "Kumara Sastry", 
    "publish": "2005-02-07T19:40:01Z", 
    "summary": "This paper discusses scalability of standard genetic programming (GP) and the\nprobabilistic incremental program evolution (PIPE). To investigate the need for\nboth effective mixing and linkage learning, two test problems are considered:\nORDER problem, which is rather easy for any recombination-based GP, and TRAP or\nthe deceptive trap problem, which requires the algorithm to learn interactions\namong subsets of terminals. The scalability results show that both GP and PIPE\nscale up polynomially with problem size on the simple ORDER problem, but they\nboth scale up exponentially on the deceptive problem. This indicates that while\nstandard recombination is sufficient when no interactions need to be\nconsidered, for some problems linkage learning is necessary. These results are\nin agreement with the lessons learned in the domain of binary-string genetic\nalgorithms (GAs). Furthermore, the paper investigates the effects of\nintroducing utnnecessary and irrelevant primitives on the performance of GP and\nPIPE."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502034v1", 
    "title": "Multiobjective hBOA, Clustering, and Scalability", 
    "arxiv-id": "cs/0502034v1", 
    "author": "David E. Goldberg", 
    "publish": "2005-02-07T05:26:13Z", 
    "summary": "This paper describes a scalable algorithm for solving multiobjective\ndecomposable problems by combining the hierarchical Bayesian optimization\nalgorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and\nclustering in the objective space. It is first argued that for good\nscalability, clustering or some other form of niching in the objective space is\nnecessary and the size of each niche should be approximately equal.\nMultiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and\nclustering in the objective space. The algorithm mohBOA differs from the\nmultiobjective variants of BOA and hBOA proposed in the past by including\nclustering in the objective space and allocating an approximately equally sized\nportion of the population to each cluster. The algorithm mohBOA is shown to\nscale up well on a number of problems on which standard multiobjective\nevolutionary algorithms perform poorly."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502057v1", 
    "title": "Decomposable Problems, Niching, and Scalability of Multiobjective   Estimation of Distribution Algorithms", 
    "arxiv-id": "cs/0502057v1", 
    "author": "David E. Goldberg", 
    "publish": "2005-02-12T22:29:45Z", 
    "summary": "The paper analyzes the scalability of multiobjective estimation of\ndistribution algorithms (MOEDAs) on a class of boundedly-difficult\nadditively-separable multiobjective optimization problems. The paper\nillustrates that even if the linkage is correctly identified, massive\nmultimodality of the search problems can easily overwhelm the nicher and lead\nto exponential scale-up. Facetwise models are subsequently used to propose a\ngrowth rate of the number of differing substructures between the two objectives\nto avoid the niching method from being overwhelmed and lead to polynomial\nscalability of MOEDAs."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0502096v1", 
    "title": "Property analysis of symmetric travelling salesman problem instances   acquired through evolution", 
    "arxiv-id": "cs/0502096v1", 
    "author": "J. I. van Hemert", 
    "publish": "2005-02-28T16:40:34Z", 
    "summary": "We show how an evolutionary algorithm can successfully be used to evolve a\nset of difficult to solve symmetric travelling salesman problem instances for\ntwo variants of the Lin-Kernighan algorithm. Then we analyse the instances in\nthose sets to guide us towards deferring general knowledge about the efficiency\nof the two variants in relation to structural properties of the symmetric\ntravelling sale sman problem."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504035v1", 
    "title": "Fitness Uniform Deletion: A Simple Way to Preserve Diversity", 
    "arxiv-id": "cs/0504035v1", 
    "author": "Marcus Hutter", 
    "publish": "2005-04-11T10:42:41Z", 
    "summary": "A commonly experienced problem with population based optimisation methods is\nthe gradual decline in population diversity that tends to occur over time. This\ncan slow a system's progress or even halt it completely if the population\nconverges on a local optimum from which it cannot escape. In this paper we\npresent the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat\nunconventional approach to this problem. Under FUDS the deletion operation is\nmodified to only delete those individuals which are \"common\" in the sense that\nthere exist many other individuals of similar fitness in the population. This\nmakes it impossible for the population to collapse to a collection of highly\nrelated individuals with similar fitness. Our experimental results on a range\nof optimisation problems confirm this, in particular for deceptive optimisation\nproblems the performance is significantly more robust to variation in the\nselection intensity."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504041v1", 
    "title": "Learning Polynomial Networks for Classification of Clinical   Electroencephalograms", 
    "arxiv-id": "cs/0504041v1", 
    "author": "Joachim Schult", 
    "publish": "2005-04-11T17:11:29Z", 
    "summary": "We describe a polynomial network technique developed for learning to classify\nclinical electroencephalograms (EEGs) presented by noisy features. Using an\nevolutionary strategy implemented within Group Method of Data Handling, we\nlearn classification models which are comprehensively described by sets of\nshort-term polynomials. The polynomial models were learnt to classify the EEGs\nrecorded from Alzheimer and healthy patients and recognize the EEG artifacts.\nComparing the performances of our technique and some machine learning methods\nwe conclude that our technique can learn well-suited polynomial models which\nexperts can find easy-to-understand."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504052v1", 
    "title": "Learning Multi-Class Neural-Network Models from Electroencephalograms", 
    "arxiv-id": "cs/0504052v1", 
    "author": "Valery Kuriakin", 
    "publish": "2005-04-13T13:22:49Z", 
    "summary": "We describe a new algorithm for learning multi-class neural-network models\nfrom large-scale clinical electroencephalograms (EEGs). This algorithm trains\nhidden neurons separately to classify all the pairs of classes. To find best\npairwise classifiers, our algorithm searches for input variables which are\nrelevant to the classification problem. Despite patient variability and heavily\noverlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns\ncorrectly classified 80.8% of the training and 80.1% of the testing examples.\nAdditionally, the neural-network model provides a probabilistic interpretation\nof decisions."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504054v1", 
    "title": "Learning from Web: Review of Approaches", 
    "arxiv-id": "cs/0504054v1", 
    "author": "Vitaly Schetinin", 
    "publish": "2005-04-13T13:40:38Z", 
    "summary": "Knowledge discovery is defined as non-trivial extraction of implicit,\npreviously unknown and potentially useful information from given data.\nKnowledge extraction from web documents deals with unstructured, free-format\ndocuments whose number is enormous and rapidly growing. The artificial neural\nnetworks are well suitable to solve a problem of knowledge discovery from web\ndocuments because trained networks are able more accurately and easily to\nclassify the learning and testing examples those represent the text mining\ndomain. However, the neural networks that consist of large number of weighted\nconnections and activation units often generate the incomprehensible and\nhard-to-understand models of text classification. This problem may be also\naddressed to most powerful recurrent neural networks that employ the feedback\nlinks from hidden or output units to their input units. Due to feedback links,\nrecurrent neural networks are able take into account of a context in document.\nTo be useful for data mining, self-organizing neural network techniques of\nknowledge extraction have been explored and developed. Self-organization\nprinciples were used to create an adequate neural-network structure and reduce\na dimensionality of features used to describe text documents. The use of these\nprinciples seems interesting because ones are able to reduce a neural-network\nredundancy and considerably facilitate the knowledge representation."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504055v1", 
    "title": "A Learning Algorithm for Evolving Cascade Neural Networks", 
    "arxiv-id": "cs/0504055v1", 
    "author": "Vitaly Schetinin", 
    "publish": "2005-04-13T13:57:56Z", 
    "summary": "A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is\ndescribed. An ECNN starts to learn with one input node and then adding new\ninputs as well as new hidden neurons evolves it. The trained ECNN has a nearly\nminimal number of input and hidden neurons as well as connections. The\nalgorithm was successfully applied to classify artifacts and normal segments in\nclinical electroencephalograms (EEGs). The EEG segments were visually labeled\nby EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing\nsegments. It is slightly better than a standard fully connected neural network."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504056v1", 
    "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity", 
    "arxiv-id": "cs/0504056v1", 
    "author": "V. Schetinin", 
    "publish": "2005-04-13T13:59:55Z", 
    "summary": "The principles of self-organizing the neural networks of optimal complexity\nis considered under the unrepresentative learning set. The method of\nself-organizing the multi-layered neural networks is offered and used to train\nthe logical neural networks which were applied to the medical diagnostics."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504057v1", 
    "title": "Diagnostic Rule Extraction Using Neural Networks", 
    "arxiv-id": "cs/0504057v1", 
    "author": "Anatoly Brazhnikov", 
    "publish": "2005-04-13T14:03:02Z", 
    "summary": "The neural networks have trained on incomplete sets that a doctor could\ncollect. Trained neural networks have correctly classified all the presented\ninstances. The number of intervals entered for encoding the quantitative\nvariables is equal two. The number of features as well as the number of neurons\nand layers in trained neural networks was minimal. Trained neural networks are\nadequately represented as a set of logical formulas that more comprehensible\nand easy-to-understand. These formulas are as the syndrome-complexes, which may\nbe easily tabulated and represented as a diagnostic table that the doctors\nusually use. Decision rules provide the evaluations of their confidence in\nwhich interested a doctor. Conducted clinical researches have shown that\niagnostic decisions produced by symbolic rules have coincided with the doctor's\nconclusions."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504058v1", 
    "title": "Polynomial Neural Networks Learnt to Classify EEG Signals", 
    "arxiv-id": "cs/0504058v1", 
    "author": "Vitaly Schetinin", 
    "publish": "2005-04-13T14:06:32Z", 
    "summary": "A neural network based technique is presented, which is able to successfully\nextract polynomial classification rules from labeled electroencephalogram (EEG)\nsignals. To represent the classification rules in an analytical form, we use\nthe polynomial neural networks trained by a modified Group Method of Data\nHandling (GMDH). The classification rules were extracted from clinical EEG data\nthat were recorded from an Alzheimer patient and the sudden death risk\npatients. The third data is EEG recordings that include the normal and artifact\nsegments. These EEG data were visually identified by medical experts. The\nextracted polynomial rules verified on the testing EEG data allow to correctly\nclassify 72% of the risk group patients and 96.5% of the segments. These rules\nperforms slightly better than standard feedforward neural networks."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504059v1", 
    "title": "A Neural Network Decision Tree for Learning Concepts from EEG Data", 
    "arxiv-id": "cs/0504059v1", 
    "author": "Vitaly Schetinin", 
    "publish": "2005-04-13T14:28:48Z", 
    "summary": "To learn the multi-class conceptions from the electroencephalogram (EEG) data\nwe developed a neural network decision tree (DT), that performs the linear\ntests, and a new training algorithm. We found that the known methods fail\ninducting the classification models when the data are presented by the features\nsome of them are irrelevant, and the classes are heavily overlapped. To train\nthe DT, our algorithm exploits a bottom up search of the features that provide\nthe best classification accuracy of the linear tests. We applied the developed\nalgorithm to induce the DT from the large EEG dataset consisted of 65 patients\nbelonging to 16 age groups. In these recordings each EEG segment was\nrepresented by 72 calculated features. The DT correctly classified 80.8% of the\ntraining and 80.1% of the testing examples. Correspondingly it correctly\nclassified 89.2% and 87.7% of the EEG recordings."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504067v1", 
    "title": "An Evolving Cascade Neural Network Technique for Cleaning Sleep   Electroencephalograms", 
    "arxiv-id": "cs/0504067v1", 
    "author": "Vitaly Schetinin", 
    "publish": "2005-04-14T10:36:54Z", 
    "summary": "Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable\nof selecting informative features are described. The ECNN initially learns with\none input node and then evolves by adding new inputs as well as new hidden\nneurons. The resultant ECNN has a near minimal number of hidden neurons and\ninputs. The algorithm is successfully used for training ECNN to recognise\nartefacts in sleep electroencephalograms (EEGs) which were visually labelled by\nEEG-viewers. In our experiments, the ECNN outperforms the standard\nneural-network as well as evolutionary techniques."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0504068v1", 
    "title": "Self-Organization of the Neuron Collective of Optimal Complexity", 
    "arxiv-id": "cs/0504068v1", 
    "author": "A. Kostunin", 
    "publish": "2005-04-14T10:45:06Z", 
    "summary": "The optimal complexity of neural networks is achieved when the\nself-organization principles is used to eliminate the contradictions existing\nin accordance with the K. Godel theorem about incompleteness of the systems\nbased on axiomatics. The principle of S. Beer exterior addition the Heuristic\nGroup Method of Data Handling by A. Ivakhnenko realized is used."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0505051v1", 
    "title": "Sub-Optimum Signal Linear Detector Using Wavelets and Support Vector   Machines", 
    "arxiv-id": "cs/0505051v1", 
    "author": "Diego Andina", 
    "publish": "2005-05-20T14:54:40Z", 
    "summary": "The problem of known signal detection in Additive White Gaussian Noise is\nconsidered. In previous work, a new detection scheme was introduced by the\nauthors, and it was demonstrated that optimum performance cannot be reached in\na real implementation. In this paper we analyse Support Vector Machines (SVM)\nas an alternative, evaluating the results in terms of Probability of detection\ncurves for a fixed Probability of false alarm."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0505052v1", 
    "title": "Upgrading Pulse Detection with Time Shift Properties Using Wavelets and   Support Vector Machines", 
    "arxiv-id": "cs/0505052v1", 
    "author": "Juan Seijas", 
    "publish": "2005-05-20T15:01:20Z", 
    "summary": "Current approaches in pulse detection use domain transformations so as to\nconcentrate frequency related information that can be distinguishable from\nnoise. In real cases we do not know when the pulse will begin, so we need a\ntime search process in which time windows are scheduled and analysed. Each\nwindow can contain the pulsed signal (either complete or incomplete) and / or\nnoise. In this paper a simple search process will be introduced, allowing the\nalgorithm to process more information, upgrading the capabilities in terms of\nprobability of detection (Pd) and probability of false alarm (Pfa)."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0505053v1", 
    "title": "Wavelet Time Shift Properties Integration with Support Vector Machines", 
    "arxiv-id": "cs/0505053v1", 
    "author": "Juan Seijas", 
    "publish": "2005-05-20T15:06:40Z", 
    "summary": "This paper presents a short evaluation about the integration of information\nderived from wavelet non-linear-time-invariant (non-LTI) projection properties\nusing Support Vector Machines (SVM). These properties may give additional\ninformation for a classifier trying to detect known patterns hidden by noise.\nIn the experiments we present a simple electromagnetic pulsed signal\nrecognition scheme, where some improvement is achieved with respect to previous\nwork. SVMs are used as a tool for information integration, exploiting some\nunique properties not easily found in neural networks."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0506078v1", 
    "title": "Dynamical Neural Network: Information and Topology", 
    "arxiv-id": "cs/0506078v1", 
    "author": "Francisco B. Rodriguez", 
    "publish": "2005-06-20T14:54:41Z", 
    "summary": "A neural network works as an associative memory device if it has large\nstorage capacity and the quality of the retrieval is good enough. The learning\nand attractor abilities of the network both can be measured by the mutual\ninformation (MI), between patterns and retrieval states. This paper deals with\na search for an optimal topology, of a Hebb network, in the sense of the\nmaximal MI. We use small-world topology. The connectivity $\\gamma$ ranges from\nan extremely diluted to the fully connected network; the randomness $\\omega$\nranges from purely local to completely random neighbors. It is found that,\nwhile stability implies an optimal $MI(\\gamma,\\omega)$ at\n$\\gamma_{opt}(\\omega)\\to 0$, for the dynamics, the optimal topology holds at\ncertain $\\gamma_{opt}>0$ whenever $0\\leq\\omega<0.3$."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0508117v1", 
    "title": "Long-term neuronal behavior caused by two synaptic modification   mechanisms", 
    "arxiv-id": "cs/0508117v1", 
    "author": "Philippe De Wilde", 
    "publish": "2005-08-26T10:38:36Z", 
    "summary": "We report the first results of simulating the coupling of neuronal,\nastrocyte, and cerebrovascular activity. It is suggested that the dynamics of\nthe system is different from systems that only include neurons. In the\nneuron-vascular coupling, distribution of synapse strengths affects neuronal\nbehavior and thus balance of the blood flow; oscillations are induced in the\nneuron-to-astrocyte coupling."
},{
    "category": "cs.CV", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0510008v1", 
    "title": "Accurate and robust image superresolution by neural processing of local   image representations", 
    "arxiv-id": "cs/0510008v1", 
    "author": "Francisco B. Rodriguez", 
    "publish": "2005-10-03T19:42:55Z", 
    "summary": "Image superresolution involves the processing of an image sequence to\ngenerate a still image with higher resolution. Classical approaches, such as\nbayesian MAP methods, require iterative minimization procedures, with high\ncomputational costs. Recently, the authors proposed a method to tackle this\nproblem, based on the use of a hybrid MLP-PNN architecture. In this paper, we\npresent a novel superresolution method, based on an evolution of this concept,\nto incorporate the use of local image models. A neural processing stage\nreceives as input the value of model coefficients on local windows. The data\ndimensionality is firstly reduced by application of PCA. An MLP, trained on\nsynthetic sequences with various amounts of noise, estimates the\nhigh-resolution image data. The effect of varying the dimension of the network\ninput space is examined, showing a complex, structured behavior. Quantitative\nresults are presented showing the accuracy and robustness of the proposed\nmethod."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0512002v1", 
    "title": "On Self-Regulated Swarms, Societal Memory, Speed and Dynamics", 
    "arxiv-id": "cs/0512002v1", 
    "author": "Agostinho C. Rosa", 
    "publish": "2005-12-01T03:10:09Z", 
    "summary": "We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the\nadvantageous characteristics of Swarm Intelligence as the emergence of a\nsocietal environmental memory or cognitive map via collective pheromone laying\nin the landscape (properly balancing the exploration/exploitation nature of our\ndynamic search strategy), with a simple Evolutionary mechanism that trough a\ndirect reproduction procedure linked to local environmental features is able to\nself-regulate the above exploratory swarm population, speeding it up globally.\nIn order to test his adaptive response and robustness, we have recurred to\ndifferent dynamic multimodal complex functions as well as to Dynamic\nOptimization Control problems, measuring reaction speeds and performance. Final\ncomparisons were made with standard Genetic Algorithms (GAs), Bacterial\nForaging strategies (BFOA), as well as with recent Co-Evolutionary approaches.\nSRS's were able to demonstrate quick adaptive responses, while outperforming\nthe results obtained by the other approaches. Additionally, some successful\nbehaviors were found. One of the most interesting illustrate that the present\nSRS collective swarm of bio-inspired ant-like agents is able to track about 65%\nof moving peaks traveling up to ten times faster than the velocity of a single\nindividual composing that precise swarm tracking system."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CEC.2003.1299828", 
    "link": "http://arxiv.org/pdf/cs/0512018v2", 
    "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework", 
    "arxiv-id": "cs/0512018v2", 
    "author": "H\u00e9l\u00e8ne Paugam-Moisy", 
    "publish": "2005-12-05T06:57:39Z", 
    "summary": "In a Spiking Neural Networks (SNN), spike emissions are sparsely and\nirregularly distributed both in time and in the network architecture. Since a\ncurrent feature of SNNs is a low average activity, efficient implementations of\nSNNs are usually based on an Event-Driven Simulation (EDS). On the other hand,\nsimulations of large scale neural networks can take advantage of distributing\nthe neurons on a set of processors (either workstation cluster or parallel\ncomputer). This article presents DAMNED, a large scale SNN simulation framework\nable to gather the benefits of EDS and parallel computing. Two levels of\nparallelism are combined: Distributed mapping of the neural topology, at the\nnetwork level, and local multithreaded allocation of resources for simultaneous\nprocessing of events, at the neuron level. Based on the causality of events, a\ndistributed solution is proposed for solving the complex problem of scheduling\nwithout synchronization barrier."
},{
    "category": "cs.NE", 
    "doi": "10.1140/epjb/e2006-00137-6", 
    "link": "http://arxiv.org/pdf/cs/0512037v2", 
    "title": "Evolving Stochastic Learning Algorithm Based on Tsallis Entropic Index", 
    "arxiv-id": "cs/0512037v2", 
    "author": "George D. Magoulas", 
    "publish": "2005-12-09T20:30:02Z", 
    "summary": "In this paper, inspired from our previous algorithm, which was based on the\ntheory of Tsallis statistical mechanics, we develop a new evolving stochastic\nlearning algorithm for neural networks. The new algorithm combines\ndeterministic and stochastic search steps by employing a different adaptive\nstepsize for each network weight, and applies a form of noise that is\ncharacterized by the nonextensive entropic index q, regulated by a weight decay\nterm. The behavior of the learning algorithm can be made more stochastic or\ndeterministic depending on the trade off between the temperature T and the q\nvalues. This is achieved by introducing a formula that defines a\ntime--dependent relationship between these two important learning parameters.\nOur experimental study verifies that there are indeed improvements in the\nconvergence speed of this new evolving stochastic learning algorithm, which\nmakes learning faster than using the original Hybrid Learning Scheme (HLS). In\naddition, experiments are conducted to explore the influence of the entropic\nindex q and temperature T on the convergence speed and stability of the\nproposed method."
},{
    "category": "cs.AI", 
    "doi": "10.1140/epjb/e2006-00137-6", 
    "link": "http://arxiv.org/pdf/cs/0512071v1", 
    "title": "\"Going back to our roots\": second generation biocomputing", 
    "arxiv-id": "cs/0512071v1", 
    "author": "Andy Tyrrell", 
    "publish": "2005-12-16T16:42:25Z", 
    "summary": "Researchers in the field of biocomputing have, for many years, successfully\n\"harvested and exploited\" the natural world for inspiration in developing\nsystems that are robust, adaptable and capable of generating novel and even\n\"creative\" solutions to human-defined problems. However, in this position paper\nwe argue that the time has now come for a reassessment of how we exploit\nbiology to generate new computational systems. Previous solutions (the \"first\ngeneration\" of biocomputing techniques), whilst reasonably effective, are crude\nanalogues of actual biological systems. We believe that a new, inherently\ninter-disciplinary approach is needed for the development of the emerging\n\"second generation\" of bio-inspired methods. This new modus operandi will\nrequire much closer interaction between the engineering and life sciences\ncommunities, as well as a bidirectional flow of concepts, applications and\nexpertise. We support our argument by examining, in this new light, three\nexisting areas of biocomputing (genetic programming, artificial immune systems\nand evolvable hardware), as well as an emerging area (natural genetic\nengineering) which may provide useful pointers as to the way forward."
},{
    "category": "cs.IR", 
    "doi": "10.1140/epjb/e2006-00137-6", 
    "link": "http://arxiv.org/pdf/cs/0601047v1", 
    "title": "Automatic Detection of Trends in Dynamical Text: An Evolutionary   Approach", 
    "arxiv-id": "cs/0601047v1", 
    "author": "Juan J. Merelo", 
    "publish": "2006-01-12T20:23:06Z", 
    "summary": "This paper presents an evolutionary algorithm for modeling the arrival dates\nof document streams, which is any time-stamped collection of documents, such as\nnewscasts, e-mails, IRC conversations, scientific journals archives and weblog\npostings. This algorithm assigns frequencies (number of document arrivals per\ntime unit) to time intervals so that it produces an optimal fit to the data.\nThe optimization is a trade off between accurately fitting the data and\navoiding too many frequency changes; this way the analysis is able to find fits\nwhich ignore the noise. Classical dynamic programming algorithms are limited by\nmemory and efficiency requirements, which can be a problem when dealing with\nlong streams. This suggests to explore alternative search methods which allow\nfor some degree of uncertainty to achieve tractability. Experiments have shown\nthat the designed evolutionary algorithm is able to reach the same solution\nquality as those classical dynamic programming algorithms in a shorter time. We\nhave also explored different probabilistic models to optimize the fitting of\nthe date streams, and applied these algorithms to infer whether a new arrival\nincreases or decreases {\\em interest} in the topic the document stream is\nabout."
},{
    "category": "cs.NE", 
    "doi": "10.1140/epjb/e2006-00137-6", 
    "link": "http://arxiv.org/pdf/cs/0601129v1", 
    "title": "Instantaneously Trained Neural Networks", 
    "arxiv-id": "cs/0601129v1", 
    "author": "Abhilash Ponnath", 
    "publish": "2006-01-30T22:02:47Z", 
    "summary": "This paper presents a review of instantaneously trained neural networks\n(ITNNs). These networks trade learning time for size and, in the basic model, a\nnew hidden node is created for each training sample. Various versions of the\ncorner-classification family of ITNNs, which have found applications in\nartificial intelligence (AI), are described. Implementation issues are also\nconsidered."
},{
    "category": "cs.AI", 
    "doi": "10.1140/epjb/e2006-00137-6", 
    "link": "http://arxiv.org/pdf/cs/0601132v1", 
    "title": "A Study on the Global Convergence Time Complexity of Estimation of   Distribution Algorithms", 
    "arxiv-id": "cs/0601132v1", 
    "author": "M. R. Meybodi", 
    "publish": "2006-01-31T07:10:45Z", 
    "summary": "The Estimation of Distribution Algorithm is a new class of population based\nsearch methods in that a probabilistic model of individuals is estimated based\non the high quality individuals and used to generate the new individuals. In\nthis paper we compute 1) some upper bounds on the number of iterations required\nfor global convergence of EDA 2) the exact number of iterations needed for EDA\nto converge to global optima."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0603090v2", 
    "title": "Topological Grammars for Data Approximation", 
    "arxiv-id": "cs/0603090v2", 
    "author": "A. Y. Zinovyev", 
    "publish": "2006-03-22T22:52:23Z", 
    "summary": "A method of {\\it topological grammars} is proposed for multidimensional data\napproximation. For data with complex topology we define a {\\it principal cubic\ncomplex} of low dimension and given complexity that gives the best\napproximation for the dataset. This complex is a generalization of linear and\nnon-linear principal manifolds and includes them as particular cases. The\nproblem of optimal principal complex construction is transformed into a series\nof minimization problems for quadratic functionals. These quadratic functionals\nhave a physically transparent interpretation in terms of elastic energy. For\nthe energy computation, the whole complex is represented as a system of nodes\nand springs. Topologically, the principal complex is a product of\none-dimensional continuums (represented by graphs), and the grammars describe\nhow these continuums transform during the process of optimal complex\nconstruction. This factorization of the whole process onto one-dimensional\ntransformations using minimization of quadratic energy functionals allow us to\nconstruct efficient algorithms."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0604046v1", 
    "title": "Concerning the differentiability of the energy function in vector   quantization algorithms", 
    "arxiv-id": "cs/0604046v1", 
    "author": "Michael Aupetit", 
    "publish": "2006-04-11T14:00:22Z", 
    "summary": "The adaptation rule for Vector Quantization algorithms, and consequently the\nconvergence of the generated sequence, depends on the existence and properties\nof a function called the energy function, defined on a topological manifold.\nOur aim is to investigate the conditions of existence of such a function for a\nclass of algorithms examplified by the initial ''K-means'' and Kohonen\nalgorithms. The results presented here supplement previous studies and show\nthat the energy function is not always a potential but at least the uniform\nlimit of a series of potential functions which we call a pseudo-potential. Our\nwork also shows that a large number of existing vector quantization algorithms\ndevelopped by the Artificial Neural Networks community fall into this category.\nThe framework we define opens the way to study the convergence of all the\ncorresponding adaptation rules at once, and a theorem gives promising insights\nin that direction. We also demonstrate that the ''K-means'' energy function is\na pseudo-potential but not a potential in general. Consequently, the energy\nfunction associated to the ''Neural-Gas'' is not a potential in general."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0606126v1", 
    "title": "May We Have Your Attention: Analysis of a Selective Attention Task", 
    "arxiv-id": "cs/0606126v1", 
    "author": "Randall D. Beer", 
    "publish": "2006-06-29T22:33:31Z", 
    "summary": "In this paper we present a deeper analysis than has previously been carried\nout of a selective attention problem, and the evolution of continuous-time\nrecurrent neural networks to solve it. We show that the task has a rich\nstructure, and agents must solve a variety of subproblems to perform well. We\nconsider the relationship between the complexity of an agent and the ease with\nwhich it can evolve behavior that generalizes well across subproblems, and\ndemonstrate a shaping protocol that improves generalization."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0607007v4", 
    "title": "Theory of sexes by Geodakian as it is advanced by Iskrin", 
    "arxiv-id": "cs/0607007v4", 
    "author": "Boris D. Lubachevsky", 
    "publish": "2006-07-03T04:03:23Z", 
    "summary": "In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for\nevolutionary adaptation of the species to changing environmental conditions. In\n2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new\nand interesting explanation to several phenomena which involve sex, and sex\nratio, including the war-years phenomena. He also introduced a new concept of\nthe \"catastrophic sex ratio.\" This note is an attempt to digest technical\naspects of the new ideas by Iskrin."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0608073v1", 
    "title": "Parametrical Neural Networks and Some Other Similar Architectures", 
    "arxiv-id": "cs/0608073v1", 
    "author": "Leonid B. Litinskii", 
    "publish": "2006-08-18T08:28:23Z", 
    "summary": "A review of works on associative neural networks accomplished during last\nfour years in the Institute of Optical Neural Technologies RAS is given. The\npresentation is based on description of parametrical neural networks (PNN). For\ntoday PNN have record recognizing characteristics (storage capacity, noise\nimmunity and speed of operation). Presentation of basic ideas and principles is\naccentuated."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0608078v1", 
    "title": "Searching for Globally Optimal Functional Forms for Inter-Atomic   Potentials Using Parallel Tempering and Genetic Programming", 
    "arxiv-id": "cs/0608078v1", 
    "author": "M. D. Peters", 
    "publish": "2006-08-18T23:17:32Z", 
    "summary": "We develop a Genetic Programming-based methodology that enables discovery of\nnovel functional forms for classical inter-atomic force-fields, used in\nmolecular dynamics simulations. Unlike previous efforts in the field, that fit\nonly the parameters to the fixed functional forms, we instead use a novel\nalgorithm to search the space of many possible functional forms. While a\nfollow-on practical procedure will use experimental and {\\it ab inito} data to\nfind an optimal functional form for a forcefield, we first validate the\napproach using a manufactured solution. This validation has the advantage of a\nwell-defined metric of success. We manufactured a training set of atomic\ncoordinate data with an associated set of global energies using the well-known\nLennard-Jones inter-atomic potential. We performed an automatic functional form\nfitting procedure starting with a population of random functions, using a\ngenetic programming functional formulation, and a parallel tempering\nMetropolis-based optimization algorithm. Our massively-parallel method\nindependently discovered the Lennard-Jones function after searching for several\nhours on 100 processors and covering a miniscule portion of the configuration\nspace. We find that the method is suitable for unsupervised discovery of\nfunctional forms for inter-atomic potentials/force-fields. We also find that\nour parallel tempering Metropolis-based approach significantly improves the\noptimization convergence time, and takes good advantage of the parallel cluster\narchitecture."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.aml.2006.04.022", 
    "link": "http://arxiv.org/pdf/cs/0608115v1", 
    "title": "Neural Network Clustering Based on Distances Between Objects", 
    "arxiv-id": "cs/0608115v1", 
    "author": "Dmitry E. Romanov", 
    "publish": "2006-08-29T13:24:37Z", 
    "summary": "We present an algorithm of clustering of many-dimensional objects, where only\nthe distances between objects are used. Centers of classes are found with the\naid of neuron-like procedure with lateral inhibition. The result of clustering\ndoes not depend on starting conditions. Our algorithm makes it possible to give\nan idea about classes that really exist in the empirical data. The results of\ncomputer simulations are presented."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TEVC.2005.863127", 
    "link": "http://arxiv.org/pdf/cs/0610126v1", 
    "title": "Fitness Uniform Optimization", 
    "arxiv-id": "cs/0610126v1", 
    "author": "Shane Legg", 
    "publish": "2006-10-20T16:37:11Z", 
    "summary": "In evolutionary algorithms, the fitness of a population increases with time\nby mutating and recombining individuals and by a biased selection of more fit\nindividuals. The right selection pressure is critical in ensuring sufficient\noptimization progress on the one hand and in preserving genetic diversity to be\nable to escape from local optima on the other hand. Motivated by a universal\nsimilarity relation on the individuals, we propose a new selection scheme,\nwhich is uniform in the fitness values. It generates selection pressure toward\nsparsely populated fitness regions, not necessarily toward higher fitness, as\nis the case for all other selection schemes. We show analytically on a simple\nexample that the new selection scheme can be much more effective than standard\nselection schemes. We also propose a new deletion scheme which achieves a\nsimilar result via deletion and show how such a scheme preserves genetic\ndiversity more effectively than standard approaches. We compare the performance\nof the new schemes to tournament selection and random deletion on an artificial\ndeceptive problem and a range of NP-hard problems: traveling salesman, set\ncovering and satisfiability."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2005.1556028", 
    "link": "http://arxiv.org/pdf/cs/0611020v1", 
    "title": "An associative memory for the on-line recognition and prediction of   temporal sequences", 
    "arxiv-id": "cs/0611020v1", 
    "author": "J. L. Shapiro", 
    "publish": "2006-11-05T01:15:01Z", 
    "summary": "This paper presents the design of an associative memory with feedback that is\ncapable of on-line temporal sequence learning. A framework for on-line sequence\nlearning has been proposed, and different sequence learning models have been\nanalysed according to this framework. The network model is an associative\nmemory with a separate store for the sequence context of a symbol. A sparse\ndistributed memory is used to gain scalability. The context store combines the\nfunctionality of a neural layer with a shift register. The sensitivity of the\nmachine to the sequence context is controllable, resulting in different\ncharacteristic behaviours. The model can store and predict on-line sequences of\nvarious types and length. Numerical simulations on the model have been carried\nout to determine its properties."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2005.1556028", 
    "link": "http://arxiv.org/pdf/cs/0611077v1", 
    "title": "Evolutionary Optimization in an Algorithmic Setting", 
    "arxiv-id": "cs/0611077v1", 
    "author": "Eugene Eberbach", 
    "publish": "2006-11-16T03:27:16Z", 
    "summary": "Evolutionary processes proved very useful for solving optimization problems.\nIn this work, we build a formalization of the notion of cooperation and\ncompetition of multiple systems working toward a common optimization goal of\nthe population using evolutionary computation techniques. It is justified that\nevolutionary algorithms are more expressive than conventional recursive\nalgorithms. Three subclasses of evolutionary algorithms are proposed here:\nbounded finite, unbounded finite and infinite types. Some results on\ncompleteness, optimality and search decidability for the above classes are\npresented. A natural extension of Evolutionary Turing Machine model developed\nin this paper allows one to mathematically represent and study properties of\ncooperation and competition in a population of optimized species."
},{
    "category": "cs.NI", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0611079v4", 
    "title": "Managing network congestion with a Kohonen-based RED queue", 
    "arxiv-id": "cs/0611079v4", 
    "author": "Bruno Talavera", 
    "publish": "2006-11-16T11:26:22Z", 
    "summary": "The behaviour of the TCP AIMD algorithm is known to cause queue length\noscillations when congestion occurs at a router output link. Indeed, due to\nthese queueing variations, end-to-end applications experience large delay\njitter. Many studies have proposed efficient Active Queue Management (AQM)\nmechanisms in order to reduce queue oscillations and stabilize the queue\nlength. These AQM are mostly improvements of the Random Early Detection (RED)\nmodel. Unfortunately, these enhancements do not react in a similar manner for\nvarious network conditions and are strongly sensitive to their initial setting\nparameters. Although this paper proposes a solution to overcome the\ndifficulties of setting these parameters by using a Kohonen neural network\nmodel, another goal of this study is to investigate whether cognitive\nintelligence could be placed in the core network to solve such stability\nproblem. In our context, we use results from the neural network area to\ndemonstrate that our proposal, named Kohonen-RED (KRED), enables a stable queue\nlength without complex parameters setting and passive measurements."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0611104v1", 
    "title": "Learning and discrimination through STDP in a top-down modulated   associative memory", 
    "arxiv-id": "cs/0611104v1", 
    "author": "H\u00e9l\u00e8ne Paugam-Moisy", 
    "publish": "2006-11-21T12:54:29Z", 
    "summary": "This article underlines the learning and discrimination capabilities of a\nmodel of associative memory based on artificial networks of spiking neurons.\nInspired from neuropsychology and neurobiology, the model implements top-down\nmodulations, as in neocortical layer V pyramidal neurons, with a learning rule\nbased on synaptic plasticity (STDP), for performing a multimodal association\nlearning task. A temporal correlation method of analysis proves the ability of\nthe model to associate specific activity patterns to different samples of\nstimulation. Even in the absence of initial learning and with continuously\nvarying weights, the activity patterns become stable enough for discrimination."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0611140v1", 
    "title": "On the Benefits of Inoculation, an Example in Train Scheduling", 
    "arxiv-id": "cs/0611140v1", 
    "author": "Marc Schoenauer", 
    "publish": "2006-11-28T12:44:43Z", 
    "summary": "The local reconstruction of a railway schedule following a small perturbation\nof the traffic, seeking minimization of the total accumulated delay, is a very\ndifficult and tightly constrained combinatorial problem. Notoriously enough,\nthe railway company's public image degrades proportionally to the amount of\ndaily delays, and the same goes for its profit! This paper describes an\ninoculation procedure which greatly enhances an evolutionary algorithm for\ntrain re-scheduling. The procedure consists in building the initial population\naround a pre-computed solution based on problem-related information available\nbeforehand. The optimization is performed by adapting times of departure and\narrival, as well as allocation of tracks, for each train at each station. This\nis achieved by a permutation-based evolutionary algorithm that relies on a\nsemi-greedy heuristic scheduler to gradually reconstruct the schedule by\ninserting trains one after another. Experimental results are presented on\nvarious instances of a large real-world case involving around 500 trains and\nmore than 1 million constraints. In terms of competition with commercial math\nematical programming tool ILOG CPLEX, it appears that within a large class of\ninstances, excluding trivial instances as well as too difficult ones, and with\nvery few exceptions, a clever initialization turns an encouraging failure into\na clear-cut success auguring of substantial financial savings."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0612104v2", 
    "title": "Sufficient Conditions for Coarse-Graining Evolutionary Dynamics", 
    "arxiv-id": "cs/0612104v2", 
    "author": "Keki Burjorjee", 
    "publish": "2006-12-21T02:18:17Z", 
    "summary": "It is commonly assumed that the ability to track the frequencies of a set of\nschemata in the evolving population of an infinite population genetic algorithm\n(IPGA) under different fitness functions will advance efforts to obtain a\ntheory of adaptation for the simple GA. Unfortunately, for IPGAs with long\ngenomes and non-trivial fitness functions there do not currently exist\ntheoretical results that allow such a study. We develop a simple framework for\nanalyzing the dynamics of an infinite population evolutionary algorithm (IPEA).\nThis framework derives its simplicity from its abstract nature. In particular\nwe make no commitment to the data-structure of the genomes, the kind of\nvariation performed, or the number of parents involved in a variation\noperation. We use this framework to derive abstract conditions under which the\ndynamics of an IPEA can be coarse-grained. We then use this result to derive\nconcrete conditions under which it becomes computationally feasible to closely\napproximate the frequencies of a family of schemata of relatively low order\nover multiple generations, even when the bitstsrings in the evolving population\nof the IPGA are long."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0701115v1", 
    "title": "Browser-based distributed evolutionary computation: performance and   scaling behavior", 
    "arxiv-id": "cs/0701115v1", 
    "author": "Fernando Tricas", 
    "publish": "2007-01-18T09:23:29Z", 
    "summary": "The challenge of ad-hoc computing is to find the way of taking advantage of\nspare cycles in an efficient way that takes into account all capabilities of\nthe devices and interconnections available to them. In this paper we explore\ndistributed evolutionary computation based on the Ruby on Rails framework,\nwhich overlays a Model-View-Controller on evolutionary computation. It allows\nanybody with a web browser (that is, mostly everybody connected to the\nInternet) to participate in an evolutionary computation experiment. Using a\nstraightforward farming model, we consider different factors, such as the size\nof the population used. We are mostly interested in how they impact on\nperformance, but also the scaling behavior when a non-trivial number of\ncomputers is applied to the problem. Experiments show the impact of different\npacket sizes on performance, as well as a quite limited scaling behavior, due\nto the characteristics of the server. Several solutions for that problem are\nproposed."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0702038v1", 
    "title": "Genetic Representations for Evolutionary Minimization of Network Coding   Resources", 
    "arxiv-id": "cs/0702038v1", 
    "author": "Wonsik Kim", 
    "publish": "2007-02-07T05:54:00Z", 
    "summary": "We demonstrate how a genetic algorithm solves the problem of minimizing the\nresources used for network coding, subject to a throughput constraint, in a\nmulticast scenario. A genetic algorithm avoids the computational complexity\nthat makes the problem NP-hard and, for our experiments, greatly improves on\nsub-optimal solutions of established methods. We compare two different genotype\nencodings, which tradeoff search space size with fitness landscape, as well as\nthe associated genetic operators. Our finding favors a smaller encoding despite\nits fewer intermediate solutions and demonstrates the impact of the modularity\nenforced by genetic operators on the performance of the algorithm."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.engappai.2010.10.012", 
    "link": "http://arxiv.org/pdf/cs/0702096v1", 
    "title": "Overcoming Hierarchical Difficulty by Hill-Climbing the Building Block   Structure", 
    "arxiv-id": "cs/0702096v1", 
    "author": "Dan Dumitrescu", 
    "publish": "2007-02-16T21:47:04Z", 
    "summary": "The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are\nwell-suited for hierarchical problems, where efficient solving requires proper\nproblem decomposition and assembly of solution from sub-solution with strong\nnon-linear interdependencies. The paper proposes a hill-climber operating over\nthe building block (BB) space that can efficiently address hierarchical\nproblems. The new Building Block Hill-Climber (BBHC) uses past hill-climb\nexperience to extract BB information and adapts its neighborhood structure\naccordingly. The perpetual adaptation of the neighborhood structure allows the\nmethod to climb the hierarchical structure solving successively the\nhierarchical levels. It is expected that for fully non deceptive hierarchical\nBB structures the BBHC can solve hierarchical problems in linearithmic time.\nEmpirical results confirm that the proposed method scales almost linearly with\nthe problem size thus clearly outperforms population based recombinative\nmethods."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.pbiomolbio.2010.01.005", 
    "link": "http://arxiv.org/pdf/cs/0703002v9", 
    "title": "Integral Biomathics: A Post-Newtonian View into the Logos of Bios (On   the New Meaning, Relations and Principles of Life in Science)", 
    "arxiv-id": "cs/0703002v9", 
    "author": "Plamen L. Simeonov", 
    "publish": "2007-02-28T22:52:43Z", 
    "summary": "This work is an attempt for a state-of-the-art survey of natural and life\nsciences with the goal to define the scope and address the central questions of\nan original research program. It is focused on the phenomena of emergence,\nadaptive dynamics and evolution of self-assembling, self-organizing,\nself-maintaining and self-replicating biosynthetic systems viewed from a\nnewly-arranged perspective and understanding of computation and communication\nin the living nature."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ALIFE.2007.367659", 
    "link": "http://arxiv.org/pdf/cs/0703149v1", 
    "title": "Exploring Logic Artificial Chemistries: An Illogical Attempt?", 
    "arxiv-id": "cs/0703149v1", 
    "author": "Christof Teuscher", 
    "publish": "2007-03-29T20:57:00Z", 
    "summary": "Robustness to a wide variety of negative factors and the ability to\nself-repair is an inherent and natural characteristic of all life forms on\nearth. As opposed to nature, man-made systems are in most cases not inherently\nrobust and a significant effort has to be made in order to make them resistant\nagainst failures. This can be done in a wide variety of ways and on various\nsystem levels. In the field of digital systems, for example, techniques such as\ntriple modular redundancy (TMR) are frequently used, which results in a\nconsiderable hardware overhead. Biologically-inspired computing by means of\nbio-chemical metaphors offers alternative paradigms, which need to be explored\nand evaluated.\n  Here, we are interested to evaluate the potential of nature-inspired\nartificial chemistries and membrane systems as an alternative information\nrepresenting and processing paradigm in order to obtain robust and spatially\nextended Boolean computing systems in a distributed environment. We investigate\nconceptual approaches inspired by artificial chemistries and membrane systems\nand compare proof-of-concepts. First, we show, that elementary logical\nfunctions can be implemented. Second, we illustrate how they can be made more\nrobust and how they can be assembled to larger-scale systems. Finally, we\ndiscuss the implications for and paths to possible genuine implementations.\nCompared to the main body of work in artificial chemistries, we take a very\npragmatic and implementation-oriented approach and are interested in realizing\nBoolean computations only. The results emphasize that artificial chemistries\ncan be used to implement Boolean logic in a spatially extended and distributed\nenvironment and can also be made highly robust, but at a significant price."
},{
    "category": "hep-lat", 
    "doi": "10.1109/ALIFE.2007.367659", 
    "link": "http://arxiv.org/pdf/hep-lat/9808001v2", 
    "title": "Genetic Algorithm for SU(N) gauge theory on a lattice", 
    "arxiv-id": "hep-lat/9808001v2", 
    "author": "Yamaguchi Azusa", 
    "publish": "1998-08-02T13:26:31Z", 
    "summary": "An Algorithm is proposed for the simulation of pure SU(N) lattice gauge\ntheories based on Genetic Algorithms(GAs). Main difference between GAs and\nMetropolis methods(MPs) is that GAs treat a population of points at once, while\nMPs treat only one point in the searching space. This provides GAs with\ninformation about the assortment as well as the fitness of the evolution\nfunction and producting a better solution. We apply GAs to SU(2) pure gauge\ntheory on a 2 dimensional lattice and show the results are consistent with\nthose given by MP and Heatbath methods(HBs). Thermalization speed of GAs is\nespecially faster than the simple MPs."
},{
    "category": "hep-lat", 
    "doi": "10.1016/S0920-5632(99)85221-9", 
    "link": "http://arxiv.org/pdf/hep-lat/9809068v1", 
    "title": "Genetic Algorithm for SU(2) Gauge Theory on a 2-dimensional Lattice", 
    "arxiv-id": "hep-lat/9809068v1", 
    "author": "A. Yamaguchi", 
    "publish": "1998-09-11T05:44:42Z", 
    "summary": "An algorithm is proposed for the simulation of pure SU(N) lattice gauge\ntheories based on Genetic Algorithms(GAs). We apply GAs to SU(2) pure gauge\ntheory on a 2 dimensional lattice and show the results, the action per\nplaquette and Wilson loops, are consistent with those by Metropolis method(MP)s\nand Heatbath method(HB)s. Thermalization speed of GAs is especially faster than\nthe simple MPs."
},{
    "category": "math.LO", 
    "doi": "10.1016/S0920-5632(99)85221-9", 
    "link": "http://arxiv.org/pdf/math/9801152v1", 
    "title": "On the classifiability of cellular automata", 
    "arxiv-id": "math/9801152v1", 
    "author": "Saharon Shelah", 
    "publish": "1998-01-15T00:00:00Z", 
    "summary": "Based on computer simulations Wolfram presented in several papers conjectured\nclassifications of cellular automata into 4 types. He distinguishes the 4\nclasses of cellular automata by the evolution of the pattern generated by\napplying a cellular automaton to a finite input. Wolfram's qualitative\nclassification is based on the examination of a large number of simulations. In\naddition to this classification based on the rate of growth, he conjectured a\nsimilar classification according to the eventual pattern. We consider here one\nformalization of his rate of growth suggestion. After completing our major\nresults (based only on Wolfram's work), we investigated other contributions to\nthe area and we report the relation of some of them to our discoveries."
},{
    "category": "nlin.AO", 
    "doi": "10.1016/S0920-5632(99)85221-9", 
    "link": "http://arxiv.org/pdf/nlin/0001057v1", 
    "title": "Numerical Replication of Computer Simulations: Some Pitfalls and How To   Avoid Them", 
    "arxiv-id": "nlin/0001057v1", 
    "author": "Theodore C. Belding", 
    "publish": "2000-01-26T03:08:08Z", 
    "summary": "A computer simulation, such as a genetic algorithm, that uses IEEE standard\nfloating-point arithmetic may not produce exactly the same results in two\ndifferent runs, even if it is rerun on the same computer with the same input\nand random number seeds. Researchers should not simply assume that the results\nfrom one run replicate those from another but should verify this by actually\ncomparing the data. However, researchers who are aware of this pitfall can\nreliably replicate simulations, in practice. This paper discusses the problem\nand suggests solutions."
},{
    "category": "nlin.AO", 
    "doi": "10.1016/S0920-5632(99)85221-9", 
    "link": "http://arxiv.org/pdf/nlin/0408040v1", 
    "title": "Notes on information geometry and evolutionary processes", 
    "arxiv-id": "nlin/0408040v1", 
    "author": "Marc Toussaint", 
    "publish": "2004-08-20T14:42:51Z", 
    "summary": "In order to analyze and extract different structural properties of\ndistributions, one can introduce different coordinate systems over the manifold\nof distributions. In Evolutionary Computation, the Walsh bases and the Building\nBlock Bases are often used to describe populations, which simplifies the\nanalysis of evolutionary operators applying on populations. Quite independent\nfrom these approaches, information geometry has been developed as a geometric\nway to analyze different order dependencies between random variables (e.g.,\nneural activations or genes).\n  In these notes I briefly review the essentials of various coordinate bases\nand of information geometry. The goal is to give an overview and make the\napproaches comparable. Besides introducing meaningful coordinate bases,\ninformation geometry also offers an explicit way to distinguish different order\ninteractions and it offers a geometric view on the manifold and thereby also on\noperators that apply on the manifold. For instance, uniform crossover can be\ninterpreted as an orthogonal projection of a population along an m-geodesic,\nmonotonously reducing the theta-coordinates that describe interactions between\ngenes."
},{
    "category": "nlin.AO", 
    "doi": "10.1016/S0920-5632(99)85221-9", 
    "link": "http://arxiv.org/pdf/nlin/0702001v1", 
    "title": "Bistability: a common feature in some \"aggregates\" of logistic maps", 
    "arxiv-id": "nlin/0702001v1", 
    "author": "Daniele Fournier-Prunaret", 
    "publish": "2007-02-01T13:56:58Z", 
    "summary": "As it was argued by Anderson [Science 177, 393 (1972)], the \"reductionist\"\nhypothesis does not by any means imply a \"constructionist\" one. Hence, in\ngeneral, the behavior of large and complex aggregates of elementary components\ncan not be understood nor extrapolated from the properties of a few components.\nFollowing this insight, we have simulated different \"aggregates\" of logistic\nmaps according to a particular coupling scheme. All these aggregates show a\nsimilar pattern of dynamical properties, concretely a bistable behavior, that\nis also found in a network of many units of the same type, independently of the\nnumber of components and of the interconnection topology. A qualitative\nrelationship with brain-like systems is suggested."
},{
    "category": "physics.class-ph", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/physics/0606053v2", 
    "title": "Optimal estimation for Large-Eddy Simulation of turbulence and   application to the analysis of subgrid models", 
    "arxiv-id": "physics/0606053v2", 
    "author": "Jean-Pierre Bertoglio", 
    "publish": "2006-06-06T11:36:51Z", 
    "summary": "The tools of optimal estimation are applied to the study of subgrid models\nfor Large-Eddy Simulation of turbulence. The concept of optimal estimator is\nintroduced and its properties are analyzed in the context of applications to a\npriori tests of subgrid models. Attention is focused on the Cook and Riley\nmodel in the case of a scalar field in isotropic turbulence. Using DNS data,\nthe relevance of the beta assumption is estimated by computing (i) generalized\noptimal estimators and (ii) the error brought by this assumption alone. Optimal\nestimators are computed for the subgrid variance using various sets of\nvariables and various techniques (histograms and neural networks). It is shown\nthat optimal estimators allow a thorough exploration of models. Neural networks\nare proved to be relevant and very efficient in this framework, and further\nusages are suggested."
},{
    "category": "q-bio.NC", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/q-bio/0403011v1", 
    "title": "Memorization in a neural network with adjustable transfer function and   conditional gating", 
    "arxiv-id": "q-bio/0403011v1", 
    "author": "Gabriele Scheler", 
    "publish": "2004-03-07T23:27:32Z", 
    "summary": "The main problem about replacing LTP as a memory mechanism has been to find\nother highly abstract, easily understandable principles for induced plasticity.\nIn this paper we attempt to lay out such a basic mechanism, namely intrinsic\nplasticity. Important empirical observations with theoretical significance are\ntime-layering of neural plasticity mediated by additional constraints to enter\ninto later stages, various manifestations of intrinsic neural properties, and\nconditional gating of synaptic connections. An important consequence of the\nproposed mechanism is that it can explain the usually latent nature of\nmemories."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.0047v1", 
    "title": "Intelligent location of simultaneously active acoustic emission sources:   Part I", 
    "arxiv-id": "0704.0047v1", 
    "author": "I. Grabec", 
    "publish": "2007-04-01T13:06:50Z", 
    "summary": "The intelligent acoustic emission locator is described in Part I, while Part\nII discusses blind source separation, time delay estimation and location of two\nsimultaneously active continuous acoustic emission sources.\n  The location of acoustic emission on complicated aircraft frame structures is\na difficult problem of non-destructive testing. This article describes an\nintelligent acoustic emission source locator. The intelligent locator comprises\na sensor antenna and a general regression neural network, which solves the\nlocation problem based on learning from examples. Locator performance was\ntested on different test specimens. Tests have shown that the accuracy of\nlocation depends on sound velocity and attenuation in the specimen, the\ndimensions of the tested area, and the properties of stored data. The location\naccuracy achieved by the intelligent locator is comparable to that obtained by\nthe conventional triangulation method, while the applicability of the\nintelligent locator is more general since analysis of sonic ray paths is\navoided. This is a promising method for non-destructive testing of aircraft\nframe structures by the acoustic emission method."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.0050v1", 
    "title": "Intelligent location of simultaneously active acoustic emission sources:   Part II", 
    "arxiv-id": "0704.0050v1", 
    "author": "I. Grabec", 
    "publish": "2007-04-01T18:53:13Z", 
    "summary": "Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\n  Acoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.0985v1", 
    "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems", 
    "arxiv-id": "0704.0985v1", 
    "author": "R. M. Vinay", 
    "publish": "2007-04-07T13:40:49Z", 
    "summary": "Advances in semiconductor technology are contributing to the increasing\ncomplexity in the design of embedded systems. Architectures with novel\ntechniques such as evolvable nature and autonomous behavior have engrossed lot\nof attention. This paper demonstrates conceptually evolvable embedded systems\ncan be characterized basing on acausal nature. It is noted that in acausal\nsystems, future input needs to be known, here we make a mechanism such that the\nsystem predicts the future inputs and exhibits pseudo acausal nature. An\nembedded system that uses theoretical framework of acausality is proposed. Our\nmethod aims at a novel architecture that features the hardware evolability and\nautonomous behavior alongside pseudo acausality. Various aspects of this\narchitecture are discussed in detail along with the limitations."
},{
    "category": "cs.CG", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.1196v1", 
    "title": "Novel algorithm to calculate hypervolume indicator of Pareto   approximation set", 
    "arxiv-id": "0704.1196v1", 
    "author": "Shengchao Ding", 
    "publish": "2007-04-10T07:21:02Z", 
    "summary": "Hypervolume indicator is a commonly accepted quality measure for comparing\nPareto approximation set generated by multi-objective optimizers. The best\nknown algorithm to calculate it for $n$ points in $d$-dimensional space has a\nrun time of $O(n^{d/2})$ with special data structures. This paper presents a\nrecursive, vertex-splitting algorithm for calculating the hypervolume indicator\nof a set of $n$ non-comparable points in $d>2$ dimensions. It splits out\nmultiple child hyper-cuboids which can not be dominated by a splitting\nreference point. In special, the splitting reference point is carefully chosen\nto minimize the number of points in the child hyper-cuboids. The complexity\nanalysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time\nand $O(dn^2)$ space complexity in the worst case."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.1198v1", 
    "title": "A Doubly Distributed Genetic Algorithm for Network Coding", 
    "arxiv-id": "0704.1198v1", 
    "author": "Muriel Medard", 
    "publish": "2007-04-10T13:36:44Z", 
    "summary": "We present a genetic algorithm which is distributed in two novel ways: along\ngenotype and temporal axes. Our algorithm first distributes, for every member\nof the population, a subset of the genotype to each network node, rather than a\nsubset of the population to each. This genotype distribution is shown to offer\na significant gain in running time. Then, for efficient use of the\ncomputational resources in the network, our algorithm divides the candidate\nsolutions into pipelined sets and thus the distribution is in the temporal\ndomain, rather that in the spatial domain. This temporal distribution may lead\nto temporal inconsistency in selection and replacement, however our experiments\nyield better efficiency in terms of the time to convergence without incurring\nsignificant penalties."
},{
    "category": "stat.AP", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.1709v1", 
    "title": "Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen", 
    "arxiv-id": "0704.1709v1", 
    "author": "Patrick Letr\u00e9my", 
    "publish": "2007-04-13T07:33:15Z", 
    "summary": "Nous montrons comment il est possible d'utiliser l'algorithme d'auto\norganisation de Kohonen pour traiter des donn\\'ees avec valeurs manquantes et\nestimer ces derni\\`eres. Apr\\`es un rappel m\\'ethodologique, nous illustrons\nnotre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.\n  -----\n  We show how it is possible to use the Kohonen self-organizing algorithm to\ndeal with data which contain missing values and to estimate them. After a\nmethodological recall, we illustrate our purpose from three real databases\napplications."
},{
    "category": "cs.RO", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0704.3268v1", 
    "title": "2D Path Solutions from a Single Layer Excitable CNN Model", 
    "arxiv-id": "0704.3268v1", 
    "author": "Koray Karahaliloglu", 
    "publish": "2007-04-24T20:20:46Z", 
    "summary": "An easily implementable path solution algorithm for 2D spatial problems,\nbased on excitable/programmable characteristics of a specific cellular\nnonlinear network (CNN) model is presented and numerically investigated. The\nnetwork is a single layer bioinspired model which was also implemented in CMOS\ntechnology. It exhibits excitable characteristics with regionally bistable\ncells. The related response realizes propagations of trigger autowaves, where\nthe excitable mode can be globally preset and reset. It is shown that, obstacle\ndistributions in 2D space can also be directly mapped onto the coupled cell\narray in the network. Combining these two features, the network model can serve\nas the main block in a 2D path computing processor. The related algorithm and\nconfigurations are numerically experimented with circuit level parameters and\nperformance estimations are also presented. The simplicity of the model also\nallows alternative technology and device level implementation, which may become\ncritical in autonomous processor design of related micro or nanoscale robotic\napplications."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.1214v1", 
    "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm", 
    "arxiv-id": "0705.1214v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-09T07:08:58Z", 
    "summary": "A method based on Bayesian neural networks and genetic algorithm is proposed\nto control the fermentation process. The relationship between input and output\nvariables is modelled using Bayesian neural network that is trained using\nhybrid Monte Carlo method. A feedback loop based on genetic algorithm is used\nto change input variables so that the output variables are as close to the\ndesired target as possible without the loss of confidence level on the\nprediction that the neural network gives. The proposed procedure is found to\nreduce the distance between the desired target and measured outputs\nsignificantly."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.1680v1", 
    "title": "Option Pricing Using Bayesian Neural Networks", 
    "arxiv-id": "0705.1680v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-11T15:55:31Z", 
    "summary": "Options have provided a field of much study because of the complexity\ninvolved in pricing them. The Black-Scholes equations were developed to price\noptions but they are only valid for European styled options. There is added\ncomplexity when trying to price American styled options and this is why the use\nof neural networks has been proposed. Neural Networks are able to predict\noutcomes based on past data. The inputs to the networks here are stock\nvolatility, strike price and time to maturity with the output of the network\nbeing the call option price. There are two techniques for Bayesian neural\nnetworks used. One is Automatic Relevance Determination (for Gaussian\nApproximation) and one is a Hybrid Monte Carlo method, both used with\nMulti-Layer Perceptrons."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.1760v1", 
    "title": "Dynamic Model Updating Using Particle Swarm Optimization Method", 
    "arxiv-id": "0705.1760v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-12T10:27:07Z", 
    "summary": "This paper proposes the use of particle swarm optimization method (PSO) for\nfinite element (FE) model updating. The PSO method is compared to the existing\nmethods that use simulated annealing (SA) or genetic algorithms (GA) for FE\nmodel for model updating. The proposed method is tested on an unsymmetrical\nH-shaped structure. It is observed that the proposed method gives updated\nnatural frequencies the most accurate and followed by those given by an updated\nmodel that was obtained using the GA and a full FE model. It is also observed\nthat the proposed method gives updated mode shapes that are best correlated to\nthe measured ones, followed by those given by an updated model that was\nobtained using the SA and a full FE model. Furthermore, it is observed that the\nPSO achieves this accuracy at a computational speed that is faster than that by\nthe GA and a full FE model which is faster than the SA and a full FE model."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.2305v1", 
    "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings", 
    "arxiv-id": "0705.2305v1", 
    "author": "Thokozani Majozi", 
    "publish": "2007-05-16T09:06:19Z", 
    "summary": "The work proposes the application of fuzzy set theory (FST) to diagnose the\ncondition of high voltage bushings. The diagnosis uses dissolved gas analysis\n(DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil\nimpregnated paper (OIP) bushings. FST and neural networks are compared in terms\nof accuracy and computational efficiency. Both FST and NN simulations were able\nto diagnose the bushings condition with 10% error. By using fuzzy theory, the\nmaintenance department can classify bushings and know the extent of degradation\nin the component."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.2307v1", 
    "title": "A Study in a Hybrid Centralised-Swarm Agent Community", 
    "arxiv-id": "0705.2307v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-16T09:12:09Z", 
    "summary": "This paper describes a systems architecture for a hybrid Centralised/Swarm\nbased multi-agent system. The issue of local goal assignment for agents is\ninvestigated through the use of a global agent which teaches the agents\nresponses to given situations. We implement a test problem in the form of a\nPursuit game, where the Multi-Agent system is a set of captor agents. The\nagents learn solutions to certain board positions from the global agent if they\nare unable to find a solution. The captor agents learn through the use of\nmulti-layer perceptron neural networks. The global agent is able to solve board\npositions through the use of a Genetic Algorithm. The cooperation between\nagents and the results of the simulation are discussed here. ."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.2516v1", 
    "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data   Using Evolutionary Computing", 
    "arxiv-id": "0705.2516v1", 
    "author": "Tshilidzi Marwala**", 
    "publish": "2007-05-17T11:33:34Z", 
    "summary": "The work proposes the application of neural networks with particle swarm\noptimisation (PSO) and genetic algorithms (GA) to compensate for missing data\nin classifying high voltage bushings. The classification is done using DGA data\nfrom 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates\nmethods for oil impregnated paper (OIP) bushings. PSO and GA were compared in\nterms of accuracy and computational efficiency. Both GA and PSO simulations\nwere able to estimate missing data values to an average 95% accuracy when only\none variable was missing. However PSO rapidly deteriorated to 66% accuracy with\ntwo variables missing simultaneously, compared to 84% for GA. The data\nestimated using GA was found to classify the conditions of bushings than the\nPSO."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0705.3766v1", 
    "title": "On complexity of optimized crossover for binary representations", 
    "arxiv-id": "0705.3766v1", 
    "author": "Anton Eremeev", 
    "publish": "2007-05-25T13:07:18Z", 
    "summary": "We consider the computational complexity of producing the best possible\noffspring in a crossover, given two solutions of the parents. The crossover\noperators are studied on the class of Boolean linear programming problems,\nwhere the Boolean vector of variables is used as the solution representation.\nBy means of efficient reductions of the optimized gene transmitting crossover\nproblems (OGTC) we show the polynomial solvability of the OGTC for the maximum\nweight set packing problem, the minimum weight set partition problem and for\none of the versions of the simple plant location problem. We study a connection\nbetween the OGTC for linear Boolean programming problem and the maximum weight\nindependent set problem on 2-colorable hypergraph and prove the NP-hardness of\nseveral special cases of the OGTC problem in Boolean linear programming."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0706.0457v1", 
    "title": "Challenges and Opportunities of Evolutionary Robotics", 
    "arxiv-id": "0706.0457v1", 
    "author": "A. C. Schultz", 
    "publish": "2007-06-04T16:08:22Z", 
    "summary": "Robotic hardware designs are becoming more complex as the variety and number\nof on-board sensors increase and as greater computational power is provided in\never-smaller packages on-board robots. These advances in hardware, however, do\nnot automatically translate into better software for controlling complex\nrobots. Evolutionary techniques hold the potential to solve many difficult\nproblems in robotics which defy simple conventional approaches, but present\nmany challenges as well. Numerous disciplines including artificial life,\ncognitive science and neural networks, rule-based systems, behavior-based\ncontrol, genetic algorithms and other forms of evolutionary computation have\ncontributed to shaping the current state of evolutionary robotics. This paper\nprovides an overview of developments in the emerging field of evolutionary\nrobotics, and discusses some of the opportunities and challenges which\ncurrently face practitioners in the field."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0706.1061v1", 
    "title": "Design, Implementation, and Cooperative Coevolution of an Autonomous/   Teleoperated Control System for a Serpentine Robotic Manipulator", 
    "arxiv-id": "0706.1061v1", 
    "author": "Gerald Chiang", 
    "publish": "2007-06-07T19:27:12Z", 
    "summary": "Design, implementation, and machine learning issues associated with\ndeveloping a control system for a serpentine robotic manipulator are explored.\nThe controller developed provides autonomous control of the serpentine robotic\nmanipulatorduring operation of the manipulator within an enclosed environment\nsuch as an underground storage tank. The controller algorithms make use of both\nlow-level joint angle control employing force/position feedback constraints,\nand high-level coordinated control of end-effector positioning. This approach\nhas resulted in both high-level full robotic control and low-level telerobotic\ncontrol modes, and provides a high level of dexterity for the operator."
},{
    "category": "cs.NE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0707.3979v1", 
    "title": "Clifford Algebra of the Vector Space of Conics for decision boundary   Hyperplanes in m-Euclidean Space", 
    "arxiv-id": "0707.3979v1", 
    "author": "J. Refugio Vallejo", 
    "publish": "2007-07-26T18:03:23Z", 
    "summary": "In this paper we embed $m$-dimensional Euclidean space in the geometric\nalgebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of\nincidence in the geometric algebra to generalize the notion of separator to a\ndecision boundary hyperconic in the Clifford algebra of hyperconic sections\ndenoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear\nperceptron or the spherical perceptron in conformal geometry and introduce the\nmore general conic perceptron, namely the {elliptical perceptron}. Using\nClifford duality a vector orthogonal to the decision boundary hyperplane is\ndetermined. Experimental results are shown in 2-dimensional Euclidean space\nwhere we separate data that are naturally separated by some typical plane conic\nseparators by this procedure. This procedure is more general in the sense that\nit is independent of the dimension of the input data and hence we can speak of\nthe hyperconic elliptic perceptron."
},{
    "category": "cs.CR", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0707.4032v1", 
    "title": "One-way Hash Function Based on Neural Network", 
    "arxiv-id": "0707.4032v1", 
    "author": "Zhiquan Wang", 
    "publish": "2007-07-27T02:38:56Z", 
    "summary": "A hash function is constructed based on a three-layer neural network. The\nthree neuron-layers are used to realize data confusion, diffusion and\ncompression respectively, and the multi-block hash mode is presented to support\nthe plaintext with variable length. Theoretical analysis and experimental\nresults show that this hash function is one-way, with high key sensitivity and\nplaintext sensitivity, and secure against birthday attacks or\nmeet-in-the-middle attacks. Additionally, the neural network's property makes\nit practical to realize in a parallel way. These properties make it a suitable\nchoice for data signature or authentication."
},{
    "category": "cs.MM", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0707.4524v1", 
    "title": "Image Authentication Based on Neural Networks", 
    "arxiv-id": "0707.4524v1", 
    "author": "Shiguo Lian", 
    "publish": "2007-07-31T02:27:10Z", 
    "summary": "Neural network has been attracting more and more researchers since the past\ndecades. The properties, such as parameter sensitivity, random similarity,\nlearning ability, etc., make it suitable for information protection, such as\ndata encryption, data authentication, intrusion detection, etc. In this paper,\nby investigating neural networks' properties, the low-cost authentication\nmethod based on neural networks is proposed and used to authenticate images or\nvideos. The authentication method can detect whether the images or videos are\nmodified maliciously. Firstly, this chapter introduces neural networks'\nproperties, such as parameter sensitivity, random similarity, diffusion\nproperty, confusion property, one-way property, etc. Secondly, the chapter\ngives an introduction to neural network based protection methods. Thirdly, an\nimage or video authentication scheme based on neural networks is presented, and\nits performances, including security, robustness and efficiency, are analyzed.\nFinally, conclusions are drawn, and some open issues in this field are\npresented."
},{
    "category": "cs.CY", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0708.2021v1", 
    "title": "Who is the best connected EC researcher? Centrality analysis of the   complex network of authors in evolutionary computation", 
    "arxiv-id": "0708.2021v1", 
    "author": "Carlos Cotta", 
    "publish": "2007-08-15T10:13:03Z", 
    "summary": "Co-authorship graphs (that is, the graph of authors linked by co-authorship\nof papers) are complex networks, which expresses the dynamics of a complex\nsystem. Only recently its study has started to draw interest from the EC\ncommunity, the first paper dealing with it having been published two years ago.\nIn this paper we will study the co-authorship network of EC at a microscopic\nlevel. Our objective is ascertaining which are the most relevant nodes (i.e.\nauthors) in it. For this purpose, we examine several metrics defined in the\ncomplex-network literature, and analyze them both in isolation and combined\nwithin a Pareto-dominance approach. The result of our analysis indicates that\nthere are some well-known researchers that appear systematically in top\nrankings. This also provides some hints on the social behavior of our\ncommunity."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0708.3463v1", 
    "title": "A Neural Networks Model of the Venezuelan Economy", 
    "arxiv-id": "0708.3463v1", 
    "author": "Juan Gonzalez", 
    "publish": "2007-08-26T05:10:29Z", 
    "summary": "Besides an indicator of the GDP, the Central Bank of Venezuela generates the\nso called Monthly Economic Activity General Indicator. The a priori knowledge\nof this indicator, which represents and sometimes even anticipates the\neconomy's fluctuations, could be helpful in developing public policies and in\ninvestment decision making. The purpose of this study is forecasting the IGAEM\nthrough non parametric methods, an approach that has proven effective in a wide\nvariety of problems in economics and finance."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0708.3464v1", 
    "title": "A Non Parametric Study of the Volatility of the Economy as a Country   Risk Predictor", 
    "arxiv-id": "0708.3464v1", 
    "author": "William Moreno", 
    "publish": "2007-08-26T05:30:18Z", 
    "summary": "This paper intends to explain Venezuela's country spread behavior through the\nNeural Networks analysis of a monthly economic activity general index of\neconomic indicators constructed by the Central Bank of Venezuela, a measure of\nthe shocks affecting country risk of emerging markets and the U.S. short term\ninterest rate. The use of non parametric methods allowed the finding of non\nlinear relationship between these inputs and the country risk. The networks\nperformance was evaluated using the method of excess predictability."
},{
    "category": "cs.CE", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0708.3829v1", 
    "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices", 
    "arxiv-id": "0708.3829v1", 
    "author": "Hender Prato", 
    "publish": "2007-08-28T18:29:55Z", 
    "summary": "A neural net model for forecasting the prices of Venezuelan crude oil is\nproposed. The inputs of the neural net are selected by reference to a dynamic\nsystem model of oil prices by Mashayekhi (1995, 2001) and its performance is\nevaluated using two criteria: the Excess Profitability test by Anatoliev and\nGerko (2005) and the characteristics of the equity curve generated by a trading\nstrategy based on the neural net predictions.\n  -----\n  Se introduce aqui un modelo no parametrico para pronosticar los precios del\npetroleo Venezolano cuyos insumos son seleccionados en base a un sistema\ndinamico que explica los precios en terminos de dichos insumos. Se describe el\nproceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se\nevaluan sus pronosticos a traves de un test estadistico de predictibilidad y de\nlas caracteristicas del Equity Curve inducido por la estrategia de compraventa\nbursatil generada por dichos pronosticos."
},{
    "category": "cs.CC", 
    "doi": "10.1063/1.2357974", 
    "link": "http://arxiv.org/pdf/0709.0883v5", 
    "title": "Liquid State Machines in Adbiatic Quantum Computers for General   Computation", 
    "arxiv-id": "0709.0883v5", 
    "author": "Joshua Jay Herman", 
    "publish": "2007-09-06T16:04:42Z", 
    "summary": "Major mistakes do not read"
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2006.05.002", 
    "link": "http://arxiv.org/pdf/0709.3461v1", 
    "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps", 
    "arxiv-id": "0709.3461v1", 
    "author": "A\u00efcha El Golli", 
    "publish": "2007-09-21T15:20:07Z", 
    "summary": "In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2006.05.002", 
    "link": "http://arxiv.org/pdf/0709.3586v1", 
    "title": "Une adaptation des cartes auto-organisatrices pour des donn\u00e9es   d\u00e9crites par un tableau de dissimilarit\u00e9s", 
    "arxiv-id": "0709.3586v1", 
    "author": "Yves Lechevallier", 
    "publish": "2007-09-22T15:53:54Z", 
    "summary": "Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.neunet.2006.05.002", 
    "link": "http://arxiv.org/pdf/0709.3587v1", 
    "title": "Self-organizing maps and symbolic data", 
    "arxiv-id": "0709.3587v1", 
    "author": "Fabrice Rossi", 
    "publish": "2007-09-22T15:54:37Z", 
    "summary": "In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data."
},{
    "category": "cs.CE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0709.4464v2", 
    "title": "Adaptive Investment Strategies For Periodic Environments", 
    "arxiv-id": "0709.4464v2", 
    "author": "J. -Emeterio Navarro", 
    "publish": "2007-09-27T19:04:00Z", 
    "summary": "In this paper, we present an adaptive investment strategy for environments\nwith periodic returns on investment. In our approach, we consider an investment\nmodel where the agent decides at every time step the proportion of wealth to\ninvest in a risky asset, keeping the rest of the budget in a risk-free asset.\nEvery investment is evaluated in the market via a stylized return on investment\nfunction (RoI), which is modeled by a stochastic process with unknown\nperiodicities and levels of noise. For comparison reasons, we present two\nreference strategies which represent the case of agents with zero-knowledge and\ncomplete-knowledge of the dynamics of the returns. We consider also an\ninvestment strategy based on technical analysis to forecast the next return by\nfitting a trend line to previous received returns. To account for the\nperformance of the different strategies, we perform some computer experiments\nto calculate the average budget that can be obtained with them over a certain\nnumber of time steps. To assure for fair comparisons, we first tune the\nparameters of each strategy. Afterwards, we compare the performance of these\nstrategies for RoIs with different periodicities and levels of noise."
},{
    "category": "cs.NE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0710.0213v1", 
    "title": "Optimising the topology of complex neural networks", 
    "arxiv-id": "0710.0213v1", 
    "author": "Marc Schoenauer", 
    "publish": "2007-10-01T06:51:42Z", 
    "summary": "In this paper, we study instances of complex neural networks, i.e. neural\nnetwo rks with complex topologies. We use Self-Organizing Map neural networks\nwhose n eighbourhood relationships are defined by a complex network, to\nclassify handwr itten digits. We show that topology has a small impact on\nperformance and robus tness to neuron failures, at least at long learning\ntimes. Performance may howe ver be increased (by almost 10%) by artificial\nevolution of the network topo logy. In our experimental conditions, the evolved\nnetworks are more random than their parents, but display a more heterogeneous\ndegree distribution."
},{
    "category": "cs.NE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0710.2227v1", 
    "title": "A System for Predicting Subcellular Localization of Yeast Genome Using   Neural Network", 
    "arxiv-id": "0710.2227v1", 
    "author": "K. Chandra Sekaran", 
    "publish": "2007-10-11T12:09:33Z", 
    "summary": "The subcellular location of a protein can provide valuable information about\nits function. With the rapid increase of sequenced genomic data, the need for\nan automated and accurate tool to predict subcellular localization becomes\nincreasingly important. Many efforts have been made to predict protein\nsubcellular localization. This paper aims to merge the artificial neural\nnetworks and bioinformatics to predict the location of protein in yeast genome.\nWe introduce a new subcellular prediction method based on a backpropagation\nneural network. The results show that the prediction within an error limit of 5\nto 10 percentage can be achieved with the system."
},{
    "category": "cs.NE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0710.2782v2", 
    "title": "Effective linkage learning using low-order statistics and clustering", 
    "arxiv-id": "0710.2782v2", 
    "author": "Aurora Pozo", 
    "publish": "2007-10-15T15:28:47Z", 
    "summary": "The adoption of probabilistic models for the best individuals found so far is\na powerful approach for evolutionary computation. Increasingly more complex\nmodels have been used by estimation of distribution algorithms (EDAs), which\noften result better effectiveness on finding the global optima for hard\noptimization problems. Supervised and unsupervised learning of Bayesian\nnetworks are very effective options, since those models are able to capture\ninteractions of high order among the variables of a problem. Diversity\npreservation, through niching techniques, has also shown to be very important\nto allow the identification of the problem structure as much as for keeping\nseveral global optima. Recently, clustering was evaluated as an effective\nniching technique for EDAs, but the performance of simpler low-order EDAs was\nnot shown to be much improved by clustering, except for some simple multimodal\nproblems. This work proposes and evaluates a combination operator guided by a\nmeasure from information theory which allows a clustered low-order EDA to\neffectively solve a comprehensive range of benchmark optimization problems."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0710.4734v1", 
    "title": "Computational Intelligence Characterization Method of Semiconductor   Device", 
    "arxiv-id": "0710.4734v1", 
    "author": "Doris Schmitt-Landsiedel", 
    "publish": "2007-10-25T09:41:43Z", 
    "summary": "Characterization of semiconductor devices is used to gather as much data\nabout the device as possible to determine weaknesses in design or trends in the\nmanufacturing process. In this paper, we propose a novel multiple trip point\ncharacterization concept to overcome the constraint of single trip point\nconcept in device characterization phase. In addition, we use computational\nintelligence techniques (e.g. neural network, fuzzy and genetic algorithm) to\nfurther manipulate these sets of multiple trip point values and tests based on\nsemiconductor test equipments, Our experimental results demonstrate an\nexcellent design parameter variation analysis in device characterization phase,\nas well as detection of a set of worst case tests that can provoke the worst\ncase variation, while traditional approach was not capable of detecting them."
},{
    "category": "cs.NE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0711.1401v2", 
    "title": "Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm", 
    "arxiv-id": "0711.1401v2", 
    "author": "Keki Burjorjee", 
    "publish": "2007-11-09T02:28:12Z", 
    "summary": "The pace of progress in the fields of Evolutionary Computation and Machine\nLearning is currently limited -- in the former field, by the improbability of\nmaking advantageous extensions to evolutionary algorithms when their capacity\nfor adaptation is poorly understood, and in the latter by the difficulty of\nfinding effective semi-principled reductions of hard real-world problems to\nrelatively simple optimization problems. In this paper we explain why a theory\nwhich can accurately explain the simple genetic algorithm's remarkable capacity\nfor adaptation has the potential to address both these limitations. We describe\nwhat we believe to be the impediments -- historic and analytic -- to the\ndiscovery of such a theory and highlight the negative role that the building\nblock hypothesis (BBH) has played. We argue based on experimental results that\na fundamental limitation which is widely believed to constrain the SGA's\nadaptive ability (and is strongly implied by the BBH) is in fact illusionary\nand does not exist. The SGA therefore turns out to be more powerful than it is\ncurrently thought to be. We give conditions under which it becomes feasible to\nnumerically approximate and study the multivariate marginals of the search\ndistribution of an infinite population SGA over multiple generations even when\nits genomes are long, and explain why this analysis is relevant to the riddle\nof the SGA's remarkable adaptive abilities."
},{
    "category": "cs.NE", 
    "doi": "10.1142/S0219525908001933", 
    "link": "http://arxiv.org/pdf/0711.2478v1", 
    "title": "A Compact Self-organizing Cellular Automata-based Genetic Algorithm", 
    "arxiv-id": "0711.2478v1", 
    "author": "Gary F. Dargush", 
    "publish": "2007-11-15T18:19:39Z", 
    "summary": "A Genetic Algorithm (GA) is proposed in which each member of the population\ncan change schemata only with its neighbors according to a rule. The rule\nmethodology and the neighborhood structure employ elements from the Cellular\nAutomata (CA) strategies. Each member of the GA population is assigned to a\ncell and crossover takes place only between adjacent cells, according to the\npredefined rule. Although combinations of CA and GA approaches have appeared\npreviously, here we rely on the inherent self-organizing features of CA, rather\nthan on parallelism. This conceptual shift directs us toward the evolution of\ncompact populations containing only a handful of members. We find that the\nresulting algorithm can search the design space more efficiently than\ntraditional GA strategies due to its ability to exploit mutations within this\ncompact self-organizing population. Consequently, premature convergence is\navoided and the final results often are more accurate. In order to reinforce\nthe superior mutation capability, a re-initialization strategy also is\nimplemented. Ten test functions and two benchmark structural engineering truss\ndesign problems are examined in order to demonstrate the performance of the\nmethod."
},{
    "category": "cs.NE", 
    "doi": "10.1057/palgrave.jors.2602308", 
    "link": "http://arxiv.org/pdf/0711.3591v2", 
    "title": "An Estimation of Distribution Algorithm with Intelligent Local Search   for Rule-based Nurse Rostering", 
    "arxiv-id": "0711.3591v2", 
    "author": "Jingpeng Li", 
    "publish": "2007-11-22T15:16:21Z", 
    "summary": "This paper proposes a new memetic evolutionary algorithm to achieve explicit\nlearning in rule-based nurse rostering, which involves applying a set of\nheuristic rules for each nurse's assignment. The main framework of the\nalgorithm is an estimation of distribution algorithm, in which an ant-miner\nmethodology improves the individual solutions produced in each generation.\nUnlike our previous work (where learning is implicit), the learning in the\nmemetic estimation of distribution algorithm is explicit, i.e. we are able to\nidentify building blocks directly. The overall approach learns by building a\nprobabilistic model, i.e. an estimation of the probability distribution of\nindividual nurse-rule pairs that are used to construct schedules. The local\nsearch processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive\nhigher rewards. A challenging real world nurse rostering problem is used as the\ntest problem. Computational results show that the proposed approach outperforms\nmost existing approaches. It is suggested that the learning methodologies\nsuggested in this paper may be applied to other scheduling problems where\nschedules are built systematically according to specific rules"
},{
    "category": "cs.NE", 
    "doi": "10.1057/palgrave.jors.2602308", 
    "link": "http://arxiv.org/pdf/0712.2630v1", 
    "title": "Evolving XSLT stylesheets", 
    "arxiv-id": "0712.2630v1", 
    "author": "J. J. Merelo", 
    "publish": "2007-12-17T19:59:42Z", 
    "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT\nprograms (usually called stylesheets or logicsheets). XSLT is a general\npurpose, document-oriented functional language, generally used to transform XML\ndocuments (or, in general, solve any problem that can be coded as an XML\ndocument). The proposed solution uses a tree representation for the stylesheets\nas well as diverse specific operators in order to obtain, in the studied cases\nand a reasonable time, a XSLT stylesheet that performs the transformation.\nSeveral types of representation have been compared, resulting in different\nperformance and degree of success."
},{
    "category": "cs.NE", 
    "doi": "10.1109/DEST.2007.372015", 
    "link": "http://arxiv.org/pdf/0712.4153v2", 
    "title": "Biology of Applied Digital Ecosystems", 
    "arxiv-id": "0712.4153v2", 
    "author": "G. Paperin", 
    "publish": "2007-12-26T21:56:52Z", 
    "summary": "A primary motivation for our research in Digital Ecosystems is the desire to\nexploit the self-organising properties of biological ecosystems. Ecosystems are\nthought to be robust, scalable architectures that can automatically solve\ncomplex, dynamic problems. However, the biological processes that contribute to\nthese properties have not been made explicit in Digital Ecosystems research.\nHere, we discuss how biological properties contribute to the self-organising\nfeatures of biological ecosystems, including population dynamics, evolution, a\ncomplex dynamic environment, and spatial distributions for generating local\ninteractions. The potential for exploiting these properties in artificial\nsystems is then considered. We suggest that several key features of biological\necosystems have not been fully explored in existing digital ecosystems, and\ndiscuss how mimicking these features may assist in developing robust, scalable\nself-organising architectures. An example architecture, the Digital Ecosystem,\nis considered in detail. The Digital Ecosystem is then measured experimentally\nthrough simulations, with measures originating from theoretical ecology, to\nconfirm its likeness to a biological ecosystem. Including the responsiveness to\nrequests for applications from the user base, as a measure of the 'ecological\nsuccession' (development)."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.0830v9", 
    "title": "Evolution of central pattern generators for the control of a five-link   bipedal walking mechanism", 
    "arxiv-id": "0801.0830v9", 
    "author": "Atilim Gunes Baydin", 
    "publish": "2008-01-06T00:20:25Z", 
    "summary": "Central pattern generators (CPGs), with a basis is neurophysiological\nstudies, are a type of neural network for the generation of rhythmic motion.\nWhile CPGs are being increasingly used in robot control, most applications are\nhand-tuned for a specific task and it is acknowledged in the field that generic\nmethods and design principles for creating individual networks for a given task\nare lacking. This study presents an approach where the connectivity and\noscillatory parameters of a CPG network are determined by an evolutionary\nalgorithm with fitness evaluations in a realistic simulation with accurate\nphysics. We apply this technique to a five-link planar walking mechanism to\ndemonstrate its feasibility and performance. In addition, to see whether\nresults from simulation can be acceptably transferred to real robot hardware,\nthe best evolved CPG network is also tested on a real mechanism. Our results\nalso confirm that the biologically inspired CPG model is well suited for legged\nlocomotion, since a diverse manifestation of networks have been observed to\nsucceed in fitness simulations during evolution."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3111v1", 
    "title": "Analysis of Estimation of Distribution Algorithms and Genetic Algorithms   on NK Landscapes", 
    "arxiv-id": "0801.3111v1", 
    "author": "Martin Pelikan", 
    "publish": "2008-01-21T00:20:50Z", 
    "summary": "This study analyzes performance of several genetic and evolutionary\nalgorithms on randomly generated NK fitness landscapes with various values of n\nand k. A large number of NK problem instances are first generated for each n\nand k, and the global optimum of each instance is obtained using the\nbranch-and-bound algorithm. Next, the hierarchical Bayesian optimization\nalgorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and\nthe simple genetic algorithm (GA) with uniform and two-point crossover\noperators are applied to all generated instances. Performance of all algorithms\nis then analyzed and compared, and the results are discussed."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3113v1", 
    "title": "iBOA: The Incremental Bayesian Optimization Algorithm", 
    "arxiv-id": "0801.3113v1", 
    "author": "David E. Goldberg", 
    "publish": "2008-01-21T00:34:55Z", 
    "summary": "This paper proposes the incremental Bayesian optimization algorithm (iBOA),\nwhich modifies standard BOA by removing the population of solutions and using\nincremental updates of the Bayesian network. iBOA is shown to be able to learn\nand exploit unrestricted Bayesian networks using incremental techniques for\nupdating both the structure as well as the parameters of the probabilistic\nmodel. This represents an important step toward the design of competent\nincremental estimation of distribution algorithms that can solve difficult\nnearly decomposable problems scalably and reliably."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3209v2", 
    "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering   Strategies for Scheduling Problems", 
    "arxiv-id": "0801.3209v2", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-21T15:55:22Z", 
    "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes amongst the agents on solution\nquality are examined for two multiple-choice optimisation problems. It is shown\nthat partnering strategies that exploit problem-specific knowledge are superior\nand can counter inappropriate (sub-) fitness measurements."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3539v3", 
    "title": "On the Effects of Idiotypic Interactions for Recommendation Communities   in Artificial Immune Systems", 
    "arxiv-id": "0801.3539v3", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-23T09:59:06Z", 
    "summary": "It has previously been shown that a recommender based on immune system\nidiotypic principles can out perform one based on correlation alone. This paper\nreports the results of work in progress, where we undertake some investigations\ninto the nature of this beneficial effect. The initial findings are that the\nimmune system recommender tends to produce different neighbourhoods, and that\nthe superior performance of this recommender is due partly to the different\nneighbourhoods, and partly to the way that the idiotypic effect is used to\nweight each neighbours recommendations."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3547v2", 
    "title": "A Recommender System based on the Immune Network", 
    "arxiv-id": "0801.3547v2", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-23T10:42:49Z", 
    "summary": "The immune system is a complex biological system with a highly distributed,\nadaptive and self-organising nature. This paper presents an artificial immune\nsystem (AIS) that exploits some of these characteristics and is applied to the\ntask of film recommendation by collaborative filtering (CF). Natural evolution\nand in particular the immune system have not been designed for classical\noptimisation. However, for this problem, we are not interested in finding a\nsingle optimum. Rather we intend to identify a sub-set of good matches on which\nrecommendations can be based. It is our hypothesis that an AIS built on two\ncentral aspects of the biological immune system will be an ideal candidate to\nachieve this: Antigen - antibody interaction for matching and antibody -\nantibody interaction for diversity. Computational results are presented in\nsupport of this conjecture and compared to those found by other CF techniques."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3550v2", 
    "title": "Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary   Algorithm", 
    "arxiv-id": "0801.3550v2", 
    "author": "Larry Bull", 
    "publish": "2008-01-23T11:12:39Z", 
    "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes for (sub-)fitness evaluation\npurposes are examined for two multiple-choice optimisation problems. It is\nshown that random partnering strategies perform best by providing better\nsampling and more diversity."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.3971v3", 
    "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem", 
    "arxiv-id": "0801.3971v3", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-25T16:07:25Z", 
    "summary": "A Bayesian optimization algorithm for the nurse scheduling problem is\npresented, which involves choosing a suitable scheduling rule from a set for\neach nurses assignment. Unlike our previous work that used Gas to implement\nimplicit learning, the learning in the proposed algorithm is explicit, ie.\nEventually, we will be able to identify and mix building blocks directly. The\nBayesian optimization algorithm is applied to implement such explicit learning\nby building a Bayesian network of the joint distribution of solutions. The\nconditional probability of each variable in the network is computed according\nto an initial set of promising solutions. Subsequently, each new instance for\neach variable is generated, ie in our case, a new rule string has been\nobtained. Another set of rule strings will be generated in this way, some of\nwhich will replace previous strings based on fitness selection. If stopping\nconditions are not met, the conditional probabilities for all nodes in the\nBayesian network are updated again using the current set of promising rule\nstrings. Computational results from 52 real data instances demonstrate the\nsuccess of this approach. It is also suggested that the learning mechanism in\nthe proposed approach might be suitable for other scheduling problems."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.4119v3", 
    "title": "Strategic Alert Throttling for Intrusion Detection Systems", 
    "arxiv-id": "0801.4119v3", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-28T15:36:56Z", 
    "summary": "Network intrusion detection systems are themselves becoming targets of\nattackers. Alert flood attacks may be used to conceal malicious activity by\nhiding it among a deluge of false alerts sent by the attacker. Although these\ntypes of attacks are very hard to stop completely, our aim is to present\ntechniques that improve alert throughput and capacity to such an extent that\nthe resources required to successfully mount the attack become prohibitive. The\nkey idea presented is to combine a token bucket filter with a realtime\ncorrelation algorithm. The proposed algorithm throttles alert output from the\nIDS when an attack is detected. The attack graph used in the correlation\nalgorithm is used to make sure that alerts crucial to forming strategies are\nnot discarded by throttling."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.4287v2", 
    "title": "Movie Recommendation Systems Using An Artificial Immune System", 
    "arxiv-id": "0801.4287v2", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-28T14:19:12Z", 
    "summary": "We apply the Artificial Immune System (AIS) technology to the Collaborative\nFiltering (CF) technology when we build the movie recommendation system. Two\ndifferent affinity measure algorithms of AIS, Kendall tau and Weighted Kappa,\nare used to calculate the correlation coefficients for this movie\nrecommendation system. From the testing we think that Weighted Kappa is more\nsuitable than Kendall tau for movie problems."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.4312v3", 
    "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In   Changing Environments", 
    "arxiv-id": "0801.4312v3", 
    "author": "Aniza Din", 
    "publish": "2008-01-28T15:26:59Z", 
    "summary": "Artificial immune system can be used to generate schedules in changing\nenvironments and it has been proven to be more robust than schedules developed\nusing a genetic algorithm. Good schedules can be produced especially when the\nnumber of the antigens is increased. However, an increase in the range of the\nantigens had somehow affected the fitness of the immune system. In this\nresearch, we are trying to improve the result of the system by rescheduling the\nsame problem using the same method while at the same time maintaining the\nrobustness of the schedules."
},{
    "category": "cs.NE", 
    "doi": "10.2478/s13230-012-0019-y", 
    "link": "http://arxiv.org/pdf/0801.4314v3", 
    "title": "Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision   Making", 
    "arxiv-id": "0801.4314v3", 
    "author": "Uwe Aickelin", 
    "publish": "2008-01-28T15:32:05Z", 
    "summary": "Over the last few years, more and more heuristic decision making techniques\nhave been inspired by nature, e.g. evolutionary algorithms, ant colony\noptimisation and simulated annealing. More recently, a novel computational\nintelligence technique inspired by immunology has emerged, called Artificial\nImmune Systems (AIS). This immune system inspired technique has already been\nuseful in solving some computational problems. In this keynote, we will very\nbriefly describe the immune system metaphors that are relevant to AIS. We will\nthen give some illustrative real-world problems suitable for AIS use and show a\nstep-by-step algorithm walkthrough. A comparison of AIS to other well-known\nalgorithms and areas for future work will round this keynote off. It should be\nnoted that as AIS is still a young and evolving field, there is not yet a fixed\nalgorithm template and hence actual implementations might differ somewhat from\nthe examples given here."
},{
    "category": "cs.NE", 
    "doi": "10.1080/01431160902788636", 
    "link": "http://arxiv.org/pdf/0802.1412v1", 
    "title": "Extreme Learning Machine for land cover classification", 
    "arxiv-id": "0802.1412v1", 
    "author": "Mahesh Pal", 
    "publish": "2008-02-11T11:12:06Z", 
    "summary": "This paper explores the potential of extreme learning machine based\nsupervised classification algorithm for land cover classification. In\ncomparison to a backpropagation neural network, which requires setting of\nseveral user-defined parameters and may produce local minima, extreme learning\nmachine require setting of one parameter and produce a unique solution. ETM+\nmultispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural\nnetwork was used to compare its performance in term of classification accuracy\nand computational cost. Results suggest that the extreme learning machine\nperform equally well to back propagation neural network in term of\nclassification accuracy with this data set. The computational cost using\nextreme learning machine is very small in comparison to back propagation neural\nnetwork."
},{
    "category": "cs.NE", 
    "doi": "10.1002/(SICI)1099-1425(200005/06)3:3<139::AID-JOS41>3.0.CO;2-2", 
    "link": "http://arxiv.org/pdf/0802.2001v3", 
    "title": "Exploiting problem structure in a genetic algorithm approach to a nurse   rostering problem", 
    "arxiv-id": "0802.2001v3", 
    "author": "Kathryn Dowsland", 
    "publish": "2008-02-14T11:25:37Z", 
    "summary": "There is considerable interest in the use of genetic algorithms to solve\nproblems arising in the areas of scheduling and timetabling. However, the\nclassical genetic algorithm paradigm is not well equipped to handle the\nconflict between objectives and constraints that typically occurs in such\nproblems. In order to overcome this, successful implementations frequently make\nuse of problem specific knowledge. This paper is concerned with the development\nof a GA for a nurse rostering problem at a major UK hospital. The structure of\nthe constraints is used as the basis for a co-evolutionary strategy using\nco-operating sub-populations. Problem specific knowledge is also used to define\na system of incentives and disincentives, and a complementary mutation\noperator. Empirical results based on 52 weeks of live data show how these\nfeatures are able to improve an unsuccessful canonical GA to the point where it\nis able to provide a practical solution to the problem"
},{
    "category": "cs.NE", 
    "doi": "10.1080/01431160802007624", 
    "link": "http://arxiv.org/pdf/0802.2138v1", 
    "title": "Support Vector classifiers for Land Cover Classification", 
    "arxiv-id": "0802.2138v1", 
    "author": "Paul M. Mather", 
    "publish": "2008-02-15T04:53:33Z", 
    "summary": "Support vector machines represent a promising development in machine learning\nresearch that is not widely used within the remote sensing community. This\npaper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral\nDAIS)data in which multi-class SVMs are compared with maximum likelihood and\nartificial neural network methods in terms of classification accuracy. Our\nresults show that the SVM achieves a higher level of classification accuracy\nthan either the maximum likelihood or the neural classifier, and that the\nsupport vector machine can be used with small training datasets and\nhigh-dimensional data."
},{
    "category": "cs.NE", 
    "doi": "10.1080/01431160802007624", 
    "link": "http://arxiv.org/pdf/0802.2411v1", 
    "title": "Multiclass Approaches for Support Vector Machine Based Land Cover   Classification", 
    "arxiv-id": "0802.2411v1", 
    "author": "Mahesh Pal", 
    "publish": "2008-02-18T03:47:45Z", 
    "summary": "SVMs were initially developed to perform binary classification; though,\napplications of binary classification are very limited. Most of the practical\napplications involve multiclass classification, especially in remote sensing\nland cover classification. A number of methods have been proposed to implement\nSVMs to produce multiclass classification. A number of methods to generate\nmulticlass SVMs from binary SVMs have been proposed by researchers and is still\na continuing research topic. This paper compares the performance of six\nmulti-class approaches to solve classification problem with remote sensing data\nin term of classification accuracy and computational cost. One vs. one, one vs.\nrest, Directed Acyclic Graph (DAG), and Error Corrected Output Coding (ECOC)\nbased multiclass approaches creates many binary classifiers and combines their\nresults to determine the class label of a test pixel. Another catogery of multi\nclass approach modify the binary class objective function and allows\nsimultaneous computation of multiclass classification by solving a single\noptimisation problem. Results from this study conclude the usefulness of One\nvs. One multi class approach in term of accuracy and computational cost over\nother multi class approaches."
},{
    "category": "cs.DB", 
    "doi": "10.1080/01431160802007624", 
    "link": "http://arxiv.org/pdf/0802.3582v1", 
    "title": "Neural Networks and Database Systems", 
    "arxiv-id": "0802.3582v1", 
    "author": "Erich Schikuta", 
    "publish": "2008-02-25T09:57:31Z", 
    "summary": "Object-oriented database systems proved very valuable at handling and\nadministrating complex objects. In the following guidelines for embedding\nneural networks into such systems are presented. It is our goal to treat\nnetworks as normal data in the database system. From the logical point of view,\na neural network is a complex data value and can be stored as a normal data\nobject. It is generally accepted that rule-based reasoning will play an\nimportant role in future database applications. The knowledge base consists of\nfacts and rules, which are both stored and handled by the underlying database\nsystem. Neural networks can be seen as representation of intensional knowledge\nof intelligent database systems. So they are part of a rule based knowledge\npool and can be used like conventional rules. The user has a unified view about\nhis knowledge base regardless of the origin of the unique rules."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.istr.2007.10.003", 
    "link": "http://arxiv.org/pdf/0802.4002v3", 
    "title": "Sensing Danger: Innate Immunology for Intrusion Detection", 
    "arxiv-id": "0802.4002v3", 
    "author": "Julie Greensmith", 
    "publish": "2008-02-27T12:15:08Z", 
    "summary": "The immune system provides an ideal metaphor for anomaly detection in general\nand computer security in particular. Based on this idea, artificial immune\nsystems have been used for a number of years for intrusion detection,\nunfortunately so far with little success. However, these previous systems were\nlargely based on immunological theory from the 1970s and 1980s and over the\nlast decade our understanding of immunological processes has vastly improved.\nIn this paper we present two new immune inspired algorithms based on the latest\nimmunological discoveries, such as the behaviour of Dendritic Cells. The\nresultant algorithms are applied to real world intrusion problems and show\nencouraging results. Overall, we believe there is a bright future for these\nnext generation artificial immune algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.istr.2007.10.003", 
    "link": "http://arxiv.org/pdf/0803.1576v1", 
    "title": "Simulation Optimization of the Crossdock Door Assignment Problem", 
    "arxiv-id": "0803.1576v1", 
    "author": "Adrian Adewunmi", 
    "publish": "2008-03-11T12:56:51Z", 
    "summary": "The purpose of this report is to present the Crossdock Door Assignment\nProblem, which involves assigning destinations to outbound dock doors of\nCrossdock centres such that travel distance by material handling equipment is\nminimized. We propose a two fold solution; simulation and optimization of the\nsimulation model simulation optimization. The novel aspect of our solution\napproach is that we intend to use simulation to derive a more realistic\nobjective function and use Memetic algorithms to find an optimal solution. The\nmain advantage of using Memetic algorithms is that it combines a local search\nwith Genetic Algorithms. The Crossdock Door Assignment Problem is a new domain\napplication to Memetic Algorithms and it is yet unknown how it will perform."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.istr.2007.10.003", 
    "link": "http://arxiv.org/pdf/0803.1596v1", 
    "title": "Using Intelligent Agents to understand organisational behaviour", 
    "arxiv-id": "0803.1596v1", 
    "author": "Christine Sprigg", 
    "publish": "2008-03-11T14:19:33Z", 
    "summary": "This paper introduces two ongoing research projects which seek to apply\ncomputer modelling techniques in order to simulate human behaviour within\norganisations. Previous research in other disciplines has suggested that\ncomplex social behaviours are governed by relatively simple rules which, when\nidentified, can be used to accurately model such processes using computer\ntechnology. The broad objective of our research is to develop a similar\ncapability within organisational psychology."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.istr.2007.10.003", 
    "link": "http://arxiv.org/pdf/0803.1626v1", 
    "title": "Genetic-Algorithm Seeding Of Idiotypic Networks For Mobile-Robot   Navigation", 
    "arxiv-id": "0803.1626v1", 
    "author": "Jonathan Garibaldi", 
    "publish": "2008-03-11T16:26:47Z", 
    "summary": "Robot-control designers have begun to exploit the properties of the human\nimmune system in order to produce dynamic systems that can adapt to complex,\nvarying, real-world tasks. Jernes idiotypic-network theory has proved the most\npopular artificial-immune-system (AIS) method for incorporation into\nbehaviour-based robotics, since idiotypic selection produces highly adaptive\nresponses. However, previous efforts have mostly focused on evolving the\nnetwork connections and have often worked with a single, pre-engineered set of\nbehaviours, limiting variability. This paper describes a method for encoding\nbehaviours as a variable set of attributes, and shows that when the encoding is\nused with a genetic algorithm (GA), multiple sets of diverse behaviours can\ndevelop naturally and rapidly, providing much greater scope for flexible\nbehaviour-selection. The algorithm is tested extensively with a simulated\ne-puck robot that navigates around a maze by tracking colour. Results show that\nhighly successful behaviour sets can be generated within about 25 minutes, and\nthat much greater diversity can be obtained when multiple autonomous\npopulations are used, rather than a single one."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-76931-6_31", 
    "link": "http://arxiv.org/pdf/0803.1728v1", 
    "title": "Investigating a Hybrid Metaheuristic For Job Shop Rescheduling", 
    "arxiv-id": "0803.1728v1", 
    "author": "Rong Qu", 
    "publish": "2008-03-12T09:26:47Z", 
    "summary": "Previous research has shown that artificial immune systems can be used to\nproduce robust schedules in a manufacturing environment. The main goal is to\ndevelop building blocks (antibodies) of partial schedules that can be used to\nconstruct backup solutions (antigens) when disturbances occur during\nproduction. The building blocks are created based upon underpinning ideas from\nartificial immune systems and evolved using a genetic algorithm (Phase I). Each\npartial schedule (antibody) is assigned a fitness value and the best partial\nschedules are selected to be converted into complete schedules (antigens). We\nfurther investigate whether simulated annealing and the great deluge algorithm\ncan improve the results when hybridised with our artificial immune system\n(Phase II). We use ten fixed solutions as our target and measure how well we\ncover these specific scenarios."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-76931-6_31", 
    "link": "http://arxiv.org/pdf/0803.1926v1", 
    "title": "Improved evolutionary generation of XSLT stylesheets", 
    "arxiv-id": "0803.1926v1", 
    "author": "J. J. Merelo", 
    "publish": "2008-03-13T11:43:25Z", 
    "summary": "This paper introduces a procedure based on genetic programming to evolve XSLT\nprograms (usually called stylesheets or logicsheets). XSLT is a general\npurpose, document-oriented functional language, generally used to transform XML\ndocuments (or, in general, solve any problem that can be coded as an XML\ndocument). The proposed solution uses a tree representation for the stylesheets\nas well as diverse specific operators in order to obtain, in the studied cases\nand a reasonable time, a XSLT stylesheet that performs the transformation.\nSeveral types of representation have been compared, resulting in different\nperformance and degree of success."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-76931-6_31", 
    "link": "http://arxiv.org/pdf/0803.1985v1", 
    "title": "An Investigation of the Sequential Sampling Method for Crossdocking   Simulation Output Variance Reduction", 
    "arxiv-id": "0803.1985v1", 
    "author": "Mike Byrne", 
    "publish": "2008-03-13T15:02:48Z", 
    "summary": "This paper investigates the reduction of variance associated with a\nsimulation output performance measure, using the Sequential Sampling method\nwhile applying minimum simulation replications, for a class of JIT (Just in\nTime) warehousing system called crossdocking. We initially used the Sequential\nSampling method to attain a desired 95% confidence interval half width of\nplus/minus 0.5 for our chosen performance measure (Total usage cost, given the\nmean maximum level of 157,000 pounds and a mean minimum level of 149,000\npounds). From our results, we achieved a 95% confidence interval half width of\nplus/minus 2.8 for our chosen performance measure (Total usage cost, with an\naverage mean value of 115,000 pounds). However, the Sequential Sampling method\nrequires a huge number of simulation replications to reduce variance for our\nsimulation output value to the target level. Arena (version 11) simulation\nsoftware was used to conduct this study."
},{
    "category": "cs.NE", 
    "doi": "10.1007/11844297_19", 
    "link": "http://arxiv.org/pdf/0803.1993v1", 
    "title": "Improved Squeaky Wheel Optimisation for Driver Scheduling", 
    "arxiv-id": "0803.1993v1", 
    "author": "Jingpeng Li", 
    "publish": "2008-03-13T15:28:06Z", 
    "summary": "This paper presents a technique called Improved Squeaky Wheel Optimisation\nfor driver scheduling problems. It improves the original Squeaky Wheel\nOptimisations effectiveness and execution speed by incorporating two additional\nsteps of Selection and Mutation which implement evolution within a single\nsolution. In the ISWO, a cycle of\nAnalysis-Selection-Mutation-Prioritization-Construction continues until\nstopping conditions are reached. The Analysis step first computes the fitness\nof a current solution to identify troublesome components. The Selection step\nthen discards these troublesome components probabilistically by using the\nfitness measure, and the Mutation step follows to further discard a small\nnumber of components at random. After the above steps, an input solution\nbecomes partial and thus the resulting partial solution needs to be repaired.\nThe repair is carried out by using the Prioritization step to first produce\npriorities that determine an order by which the following Construction step\nthen schedules the remaining components. Therefore, the optimisation in the\nISWO is achieved by solution disruption, iterative improvement and an iterative\nconstructive repair process performed. Encouraging experimental results are\nreported."
},{
    "category": "cs.NE", 
    "doi": "10.1007/b100601", 
    "link": "http://arxiv.org/pdf/0803.1994v1", 
    "title": "The Application of Bayesian Optimization and Classifier Systems in Nurse   Scheduling", 
    "arxiv-id": "0803.1994v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-03-13T15:43:34Z", 
    "summary": "Two ideas taken from Bayesian optimization and classifier systems are\npresented for personnel scheduling based on choosing a suitable scheduling rule\nfrom a set for each persons assignment. Unlike our previous work of using\ngenetic algorithms whose learning is implicit, the learning in both approaches\nis explicit, i.e. we are able to identify building blocks directly. To achieve\nthis target, the Bayesian optimization algorithm builds a Bayesian network of\nthe joint probability distribution of the rules used to construct solutions,\nwhile the adapted classifier system assigns each rule a strength value that is\nconstantly updated according to its usefulness in the current situation.\nComputational results from 52 real data instances of nurse scheduling\ndemonstrate the success of both approaches. It is also suggested that the\nlearning mechanism in the proposed approaches might be suitable for other\nscheduling problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/b100601", 
    "link": "http://arxiv.org/pdf/0803.2675v4", 
    "title": "Digital Ecosystems: Self-Organisation of Evolving Agent Populations", 
    "arxiv-id": "0803.2675v4", 
    "author": "Philippe De Wilde", 
    "publish": "2008-03-18T16:59:12Z", 
    "summary": "A primary motivation for our research in Digital Ecosystems is the desire to\nexploit the self-organising properties of biological ecosystems. Ecosystems are\nthought to be robust, scalable architectures that can automatically solve\ncomplex, dynamic problems. Self-organisation is perhaps one of the most\ndesirable features in the systems that we engineer, and it is important for us\nto be able to measure self-organising behaviour. We investigate the\nself-organising aspects of Digital Ecosystems, created through the application\nof evolutionary computing to Multi-Agent Systems (MASs), aiming to determine a\nmacroscopic variable to characterise the self-organisation of the evolving\nagent populations within. We study a measure for the self-organisation called\nPhysical Complexity; based on statistical physics, automata theory, and\ninformation theory, providing a measure of information relative to the\nrandomness in an organism's genome, by calculating the entropy in a population.\nWe investigate an extension to include populations of variable length, and then\nbuilt upon this to construct an efficiency measure to investigate clustering\nwithin evolving agent populations. Overall an insight has been achieved into\nwhere and how self-organisation occurs in our Digital Ecosystem, and how it can\nbe quantified."
},{
    "category": "cs.NE", 
    "doi": "10.1007/b100601", 
    "link": "http://arxiv.org/pdf/0803.2695v1", 
    "title": "KohonAnts: A Self-Organizing Ant Algorithm for Clustering and Pattern   Classification", 
    "arxiv-id": "0803.2695v1", 
    "author": "J. L. J. Laredo", 
    "publish": "2008-03-18T18:27:14Z", 
    "summary": "In this paper we introduce a new ant-based method that takes advantage of the\ncooperative self-organization of Ant Colony Systems to create a naturally\ninspired clustering and pattern recognition method. The approach considers each\ndata item as an ant, which moves inside a grid changing the cells it goes\nthrough, in a fashion similar to Kohonen's Self-Organizing Maps. The resulting\nalgorithm is conceptually more simple, takes less free parameters than other\nant-based clustering algorithms, and, after some parameter tuning, yields very\ngood results on some benchmark problems."
},{
    "category": "cs.NE", 
    "doi": "10.1023/A:1016536623961,", 
    "link": "http://arxiv.org/pdf/0803.2957v1", 
    "title": "Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall   Layout and Tenant Selection Problem", 
    "arxiv-id": "0803.2957v1", 
    "author": "Kathryn Dowsland", 
    "publish": "2008-03-20T10:19:01Z", 
    "summary": "During our earlier research, it was recognised that in order to be successful\nwith an indirect genetic algorithm approach using a decoder, the decoder has to\nstrike a balance between being an optimiser in its own right and finding\nfeasible solutions. Previously this balance was achieved manually. Here we\nextend this by presenting an automated approach where the genetic algorithm\nitself, simultaneously to solving the problem, sets weights to balance the\ncomponents out. Subsequently we were able to solve a complex and non-linear\nscheduling problem better than with a standard direct genetic algorithm\nimplementation."
},{
    "category": "cs.NE", 
    "doi": "10.1057/palgrave.jors.2601317", 
    "link": "http://arxiv.org/pdf/0803.2965v1", 
    "title": "An Indirect Genetic Algorithm for Set Covering Problems", 
    "arxiv-id": "0803.2965v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-03-20T10:58:32Z", 
    "summary": "This paper presents a new type of genetic algorithm for the set covering\nproblem. It differs from previous evolutionary approaches first because it is\nan indirect algorithm, i.e. the actual solutions are found by an external\ndecoder function. The genetic algorithm itself provides this decoder with\npermutations of the solution variables and other parameters. Second, it will be\nshown that results can be further improved by adding another indirect\noptimisation layer. The decoder will not directly seek out low cost solutions\nbut instead aims for good exploitable solutions. These are then post optimised\nby another hill-climbing algorithm. Although seemingly more complicated, we\nwill show that this three-stage approach has advantages in terms of solution\nquality, speed and adaptability to new types of problems over more direct\napproaches. Extensive computational results are presented and compared to the\nlatest evolutionary and other heuristic approaches to the same data instances."
},{
    "category": "cs.NE", 
    "doi": "10.1057/palgrave.jors.2601317", 
    "link": "http://arxiv.org/pdf/0803.2966v1", 
    "title": "On the Application of Hierarchical Coevolutionary Genetic Algorithms:   Recombination and Evaluation Partners", 
    "arxiv-id": "0803.2966v1", 
    "author": "Larry Bull", 
    "publish": "2008-03-20T11:09:39Z", 
    "summary": "This paper examines the use of a hierarchical coevolutionary genetic\nalgorithm under different partnering strategies. Cascading clusters of\nsub-populations are built from the bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\npotentially search a larger search space with a lower resolution whilst\nlower-level sub-populations search a smaller search space with a higher\nresolution. The effects of different partner selection schemes amongst the\nsub-populations on solution quality are examined for two constrained\noptimisation problems. We examine a number of recombination partnering\nstrategies in the construction of higher-level individuals and a number of\nrelated schemes for evaluating sub-solutions. It is shown that partnering\nstrategies that exploit problem-specific knowledge are superior and can counter\ninappropriate (sub)fitness measurements."
},{
    "category": "cs.NE", 
    "doi": "10.1023/B:ANOR.0000019103.31340.a6", 
    "link": "http://arxiv.org/pdf/0803.2967v1", 
    "title": "Building Better Nurse Scheduling Algorithms", 
    "arxiv-id": "0803.2967v1", 
    "author": "Paul White", 
    "publish": "2008-03-20T11:15:37Z", 
    "summary": "The aim of this research is twofold: Firstly, to model and solve a complex\nnurse scheduling problem with an integer programming formulation and\nevolutionary algorithms. Secondly, to detail a novel statistical method of\ncomparing and hence building better scheduling algorithms by identifying\nsuccessful algorithm modifications. The comparison method captures the results\nof algorithms in a single figure that can then be compared using traditional\nstatistical techniques. Thus, the proposed method of comparing algorithms is an\nobjective procedure designed to assist in the process of improving an\nalgorithm. This is achieved even when some results are non-numeric or missing\ndue to infeasibility. The final algorithm outperforms all previous evolutionary\nalgorithms, which relied on human expertise for modification."
},{
    "category": "cs.NE", 
    "doi": "10.1016/S0305-0548(03)00034-0", 
    "link": "http://arxiv.org/pdf/0803.2969v1", 
    "title": "An Indirect Genetic Algorithm for a Nurse Scheduling Problem", 
    "arxiv-id": "0803.2969v1", 
    "author": "Kathryn Dowsland", 
    "publish": "2008-03-20T11:21:19Z", 
    "summary": "This paper describes a Genetic Algorithms approach to a manpower-scheduling\nproblem arising at a major UK hospital. Although Genetic Algorithms have been\nsuccessfully used for similar problems in the past, they always had to overcome\nthe limitations of the classical Genetic Algorithms paradigm in handling the\nconflict between objectives and constraints. The approach taken here is to use\nan indirect coding based on permutations of the nurses, and a heuristic decoder\nthat builds schedules from these permutations. Computational experiments based\non 52 weeks of live data are used to evaluate three different decoders with\nvarying levels of intelligence, and four well-known crossover operators.\nResults are further enhanced by introducing a hybrid crossover operator and by\nmaking use of simple bounds to reduce the size of the solution space. The\nresults reveal that the proposed algorithm is able to find high quality\nsolutions and is both faster and more flexible than a recently published Tabu\nSearch approach."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10852-004-5336-7", 
    "link": "http://arxiv.org/pdf/0803.2970v2", 
    "title": "A Recommender System based on Idiotypic Artificial Immune Networks", 
    "arxiv-id": "0803.2970v2", 
    "author": "Uwe Aickelin", 
    "publish": "2008-03-20T11:27:56Z", 
    "summary": "The immune system is a complex biological system with a highly distributed,\nadaptive and self-organising nature. This paper presents an Artificial Immune\nSystem (AIS) that exploits some of these characteristics and is applied to the\ntask of film recommendation by Collaborative Filtering (CF). Natural evolution\nand in particular the immune system have not been designed for classical\noptimisation. However, for this problem, we are not interested in finding a\nsingle optimum. Rather we intend to identify a sub-set of good matches on which\nrecommendations can be based. It is our hypothesis that an AIS built on two\ncentral aspects of the biological immune system will be an ideal candidate to\nachieve this: Antigen-antibody interaction for matching and idiotypic\nantibody-antibody interaction for diversity. Computational results are\npresented in support of this conjecture and compared to those found by other CF\ntechniques."
},{
    "category": "cs.NE", 
    "doi": "10.1504/IJESDF.2007.013596,", 
    "link": "http://arxiv.org/pdf/0803.2973v2", 
    "title": "Rule Generalisation in Intrusion Detection Systems using Snort", 
    "arxiv-id": "0803.2973v2", 
    "author": "Thomas Hesketh-Roberts", 
    "publish": "2008-03-20T11:59:27Z", 
    "summary": "Intrusion Detection Systems (ids)provide an important layer of security for\ncomputer systems and networks, and are becoming more and more necessary as\nreliance on Internet services increases and systems with sensitive data are\nmore commonly open to Internet access. An ids responsibility is to detect\nsuspicious or unacceptable system and network activity and to alert a systems\nadministrator to this activity. The majority of ids use a set of signatures\nthat define what suspicious traffic is, and Snort is one popular and actively\ndeveloping open-source ids that uses such a set of signatures known as Snort\nrules. Our aim is to identify a way in which Snort could be developed further\nby generalising rules to identify novel attacks. In particular, we attempted to\nrelax and vary the conditions and parameters of current Snort rules, using a\nsimilar approach to classic rule learning operators such as generalisation and\nspecialisation. We demonstrate the effectiveness of our approach through\nexperiments with standard datasets and show that we are able to detect\npreviously undeleted variants of various attacks. We conclude by discussing the\ngeneral effectiveness and appropriateness of generalisation in Snort based ids\nrule processing."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.2975v2", 
    "title": "An Estimation of Distribution Algorithm for Nurse Scheduling", 
    "arxiv-id": "0803.2975v2", 
    "author": "Jingpeng Li", 
    "publish": "2008-03-20T12:07:26Z", 
    "summary": "Schedules can be built in a similar way to a human scheduler by using a set\nof rules that involve domain knowledge. This paper presents an Estimation of\nDistribution Algorithm (eda) for the nurse scheduling problem, which involves\nchoosing a suitable scheduling rule from a set for the assignment of each\nnurse. Unlike previous work that used Genetic Algorithms (ga) to implement\nimplicit learning, the learning in the proposed algorithm is explicit, i.e. we\nidentify and mix building blocks directly. The eda is applied to implement such\nexplicit learning by building a Bayesian network of the joint distribution of\nsolutions. The conditional probability of each variable in the network is\ncomputed according to an initial set of promising solutions. Subsequently, each\nnew instance for each variable is generated by using the corresponding\nconditional probabilities, until all variables have been generated, i.e. in our\ncase, a new rule string has been obtained. Another set of rule strings will be\ngenerated in this way, some of which will replace previous strings based on\nfitness selection. If stopping conditions are not met, the conditional\nprobabilities for all nodes in the Bayesian network are updated again using the\ncurrent set of promising rule strings. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed approach might be suitable for other\nscheduling problems."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3186v1", 
    "title": "Towards a human eye behavior model by applying Data Mining Techniques on   Gaze Information from IEC", 
    "arxiv-id": "0803.3186v1", 
    "author": "Thierry Baccino", 
    "publish": "2008-03-21T15:38:25Z", 
    "summary": "In this paper, we firstly present what is Interactive Evolutionary\nComputation (IEC) and rapidly how we have combined this artificial intelligence\ntechnique with an eye-tracker for visual optimization. Next, in order to\ncorrectly parameterize our application, we present results from applying data\nmining techniques on gaze information coming from experiments conducted on\nabout 80 human individuals."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3539v1", 
    "title": "Reinforcement Learning by Value Gradients", 
    "arxiv-id": "0803.3539v1", 
    "author": "Michael Fairbank", 
    "publish": "2008-03-25T11:57:15Z", 
    "summary": "The concept of the value-gradient is introduced and developed in the context\nof reinforcement learning. It is shown that by learning the value-gradients\nexploration or stochastic behaviour is no longer needed to find locally optimal\ntrajectories. This is the main motivation for using value-gradients, and it is\nargued that learning value-gradients is the actual objective of any\nvalue-function learning algorithm for control problems. It is also argued that\nlearning value-gradients is significantly more efficient than learning just the\nvalues, and this argument is supported in experiments by efficiency gains of\nseveral orders of magnitude, in several problem domains. Once value-gradients\nare introduced into learning, several analyses become possible. For example, a\nsurprising equivalence between a value-gradient learning algorithm and a\npolicy-gradient learning algorithm is proven, and this provides a robust\nconvergence proof for control problems using a value function with a general\nfunction approximator."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3746v1", 
    "title": "Cluster Approach to the Domains Formation", 
    "arxiv-id": "0803.3746v1", 
    "author": "Leonid B. Litinskii", 
    "publish": "2008-03-26T15:14:33Z", 
    "summary": "As a rule, a quadratic functional depending on a great number of binary\nvariables has a lot of local minima. One of approaches allowing one to find in\naveraged deeper local minima is aggregation of binary variables into larger\nblocks/domains. To minimize the functional one has to change the states of\naggregated variables (domains). In the present publication we discuss methods\nof domains formation. It is shown that the best results are obtained when\ndomains are formed by variables that are strongly connected with each other."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3838v2", 
    "title": "Recorded Step Directional Mutation for Faster Convergence", 
    "arxiv-id": "0803.3838v2", 
    "author": "Ted Dunning", 
    "publish": "2008-03-26T22:49:40Z", 
    "summary": "Two meta-evolutionary optimization strategies described in this paper\naccelerate the convergence of evolutionary programming algorithms while still\nretaining much of their ability to deal with multi-modal problems. The\nstrategies, called directional mutation and recorded step in this paper, can\noperate independently but together they greatly enhance the ability of\nevolutionary programming algorithms to deal with fitness landscapes\ncharacterized by long narrow valleys. The directional mutation aspect of this\ncombined method uses correlated meta-mutation but does not introduce a full\ncovariance matrix. These new methods are thus much more economical in terms of\nstorage for problems with high dimensionality. Additionally, directional\nmutation is rotationally invariant which is a substantial advantage over\nself-adaptive methods which use a single variance per coordinate for problems\nwhere the natural orientation of the problem is not oriented along the axes."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3900v1", 
    "title": "A Component Based Heuristic Search method with Adaptive Perturbations   for Hospital Personnel Scheduling", 
    "arxiv-id": "0803.3900v1", 
    "author": "Edmund Burke", 
    "publish": "2008-03-27T12:15:43Z", 
    "summary": "Nurse rostering is a complex scheduling problem that affects hospital\npersonnel on a daily basis all over the world. This paper presents a new\ncomponent-based approach with adaptive perturbations, for a nurse scheduling\nproblem arising at a major UK hospital. The main idea behind this technique is\nto decompose a schedule into its components (i.e. the allocated shift pattern\nof each nurse), and then mimic a natural evolutionary process on these\ncomponents to iteratively deliver better schedules. The worthiness of all\ncomponents in the schedule has to be continuously demonstrated in order for\nthem to remain there. This demonstration employs a dynamic evaluation function\nwhich evaluates how well each component contributes towards the final\nobjective. Two perturbation steps are then applied: the first perturbation\neliminates a number of components that are deemed not worthy to stay in the\ncurrent schedule; the second perturbation may also throw out, with a low level\nof probability, some worthy components. The eliminated components are\nreplenished with new ones using a set of constructive heuristics using local\noptimality criteria. Computational results using 52 data instances demonstrate\nthe applicability of the proposed approach in solving real-world problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0803.3905v1", 
    "title": "Introduction to Multi-Agent Simulation", 
    "arxiv-id": "0803.3905v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-03-27T12:38:17Z", 
    "summary": "When designing systems that are complex, dynamic and stochastic in nature,\nsimulation is generally recognised as one of the best design support\ntechnologies, and a valuable aid in the strategic and tactical decision making\nprocess. A simulation model consists of a set of rules that define how a system\nchanges over time, given its current state. Unlike analytical models, a\nsimulation model is not solved but is run and the changes of system states can\nbe observed at any point in time. This provides an insight into system dynamics\nrather than just predicting the output of a system based on specific inputs.\nSimulation is not a decision making tool but a decision support tool, allowing\nbetter informed decisions to be made. Due to the complexity of the real world,\na simulation model can only be an approximation of the target system. The\nessence of the art of simulation modelling is abstraction and simplification.\nOnly those characteristics that are important for the study and analysis of the\ntarget system should be included in the simulation model."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0804.0352v1", 
    "title": "Permeability Analysis based on information granulation theory", 
    "arxiv-id": "0804.0352v1", 
    "author": "E. Bakhtavar", 
    "publish": "2008-04-02T13:45:51Z", 
    "summary": "This paper describes application of information granulation theory, on the\nanalysis of \"lugeon data\". In this manner, using a combining of Self Organizing\nMap (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are\nobtained. Balancing of crisp granules and sub- fuzzy granules, within non fuzzy\ninformation (initial granulation), is rendered in open-close iteration. Using\ntwo criteria, \"simplicity of rules \"and \"suitable adaptive threshold error\nlevel\", stability of algorithm is guaranteed. In other part of paper, rough set\ntheory (RST), to approximate analysis, has been employed >.Validation of the\nproposed methods, on the large data set of in-situ permeability in rock masses,\nin the Shivashan dam, Iran, has been highlighted. By the implementation of the\nproposed algorithm on the lugeon data set, was proved the suggested method,\nrelating the approximate analysis on the permeability, could be applied."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10479-007-0214-0", 
    "link": "http://arxiv.org/pdf/0804.0353v1", 
    "title": "Graphical Estimation of Permeability Using RST&NFIS", 
    "arxiv-id": "0804.0353v1", 
    "author": "K. Shahriar W. Pedrycz", 
    "publish": "2008-04-02T13:56:10Z", 
    "summary": "This paper pursues some applications of Rough Set Theory (RST) and\nneural-fuzzy model to analysis of \"lugeon data\". In the manner, using Self\nOrganizing Map (SOM) as a pre-processing the data are scaled and then the\ndominant rules by RST, are elicited. Based on these rules variations of\npermeability in the different levels of Shivashan dam, Iran has been\nhighlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy\nInference System (NFIS) another analysis on the data was carried out. Finally,\na brief comparison between the obtained results of RST and SOM-NFIS (briefly\nSONFIS) has been rendered."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-34954-9_14", 
    "link": "http://arxiv.org/pdf/0804.0524v1", 
    "title": "Bayesian Optimisation Algorithm for Nurse Scheduling", 
    "arxiv-id": "0804.0524v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-04-03T11:14:11Z", 
    "summary": "Our research has shown that schedules can be built mimicking a human\nscheduler by using a set of rules that involve domain knowledge. This chapter\npresents a Bayesian Optimization Algorithm (BOA) for the nurse scheduling\nproblem that chooses such suitable scheduling rules from a set for each nurses\nassignment. Based on the idea of using probabilistic models, the BOA builds a\nBayesian network for the set of promising solutions and samples these networks\nto generate new candidate solutions. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed algorithm may be suitable for other\nscheduling problems."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-34954-9_14", 
    "link": "http://arxiv.org/pdf/0804.0573v1", 
    "title": "An Artificial Immune System as a Recommender System for Web Sites", 
    "arxiv-id": "0804.0573v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-04-03T14:43:44Z", 
    "summary": "Artificial Immune Systems have been used successfully to build recommender\nsystems for film databases. In this research, an attempt is made to extend this\nidea to web site recommendation. A collection of more than 1000 individuals web\nprofiles (alternatively called preferences / favourites / bookmarks file) will\nbe used. URLs will be classified using the DMOZ (Directory Mozilla) database of\nthe Open Directory Project as our ontology. This will then be used as the data\nfor the Artificial Immune Systems rather than the actual addresses. The first\nattempt will involve using a simple classification code number coupled with the\nnumber of pages within that classification code. However, this implementation\ndoes not make use of the hierarchical tree-like structure of DMOZ.\nConsideration will then be given to the construction of a similarity measure\nfor web profiles that makes use of this hierarchical information to build a\nbetter-informed Artificial Immune System."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-540-34954-9_14", 
    "link": "http://arxiv.org/pdf/0804.0580v1", 
    "title": "Explicit Learning: an Effort towards Human Scheduling Algorithms", 
    "arxiv-id": "0804.0580v1", 
    "author": "Uwe Aickelin", 
    "publish": "2008-04-03T15:31:52Z", 
    "summary": "Scheduling problems are generally NP-hard combinatorial problems, and a lot\nof research has been done to solve these problems heuristically. However, most\nof the previous approaches are problem-specific and research into the\ndevelopment of a general scheduling algorithm is still in its infancy.\n  Mimicking the natural evolutionary process of the survival of the fittest,\nGenetic Algorithms (GAs) have attracted much attention in solving difficult\nscheduling problems in recent years. Some obstacles exist when using GAs: there\nis no canonical mechanism to deal with constraints, which are commonly met in\nmost real-world scheduling problems, and small changes to a solution are\ndifficult. To overcome both difficulties, indirect approaches have been\npresented (in [1] and [2]) for nurse scheduling and driver scheduling, where\nGAs are used by mapping the solution space, and separate decoding routines then\nbuild solutions to the original problem."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.1266v1", 
    "title": "Immune System Approaches to Intrusion Detection - A Review", 
    "arxiv-id": "0804.1266v1", 
    "author": "Jamie Twycross", 
    "publish": "2008-04-08T11:44:38Z", 
    "summary": "The use of artificial immune systems in intrusion detection is an appealing\nconcept for two reasons. Firstly, the human immune system provides the human\nbody with a high level of protection from invading pathogens, in a robust,\nself-organised and distributed manner. Secondly, current techniques used in\ncomputer security are not able to cope with the dynamic and increasingly\ncomplex nature of computer systems and their security. It is hoped that\nbiologically inspired approaches in this area, including the use of\nimmune-based systems will be able to meet this challenge. Here we review the\nalgorithms used, the development of the systems and the outcome of their\nimplementation. We provide an introduction and analysis of the key developments\nwithin this field, in addition to making suggestions for future research."
},{
    "category": "cs.CR", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.1281v2", 
    "title": "Data Reduction in Intrusion Alert Correlation", 
    "arxiv-id": "0804.1281v2", 
    "author": "Uwe Aickelin", 
    "publish": "2008-04-08T14:15:34Z", 
    "summary": "Network intrusion detection sensors are usually built around low level models\nof network traffic. This means that their output is of a similarly low level\nand as a consequence, is difficult to analyze. Intrusion alert correlation is\nthe task of automating some of this analysis by grouping related alerts\ntogether. Attack graphs provide an intuitive model for such analysis.\nUnfortunately alert flooding attacks can still cause a loss of service on\nsensors, and when performing attack graph correlation, there can be a large\nnumber of extraneous alerts included in the output graph. This obscures the\nfine structure of genuine attacks and makes them more difficult for human\noperators to discern. This paper explores modified correlation algorithms which\nattempt to minimize the impact of this attack."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.3269v1", 
    "title": "Phoneme recognition in TIMIT with BLSTM-CTC", 
    "arxiv-id": "0804.3269v1", 
    "author": "Juergen Schmidhuber", 
    "publish": "2008-04-21T15:38:45Z", 
    "summary": "We compare the performance of a recurrent neural network with the best\nresults published so far on phoneme recognition in the TIMIT database. These\npublished results have been obtained with a combination of classifiers.\nHowever, in this paper we apply a single recurrent neural network to the same\ntask. Our recurrent neural network attains an error rate of 24.6%. This result\nis not significantly different from that obtained by the other best methods,\nbut they rely on a combination of classifiers for achieving comparable\nperformance."
},{
    "category": "cs.LO", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.4071v1", 
    "title": "Logic Mining Using Neural Networks", 
    "arxiv-id": "0804.4071v1", 
    "author": "Wan Ahmad Tajuddin Wan Abdullah", 
    "publish": "2008-04-25T09:30:28Z", 
    "summary": "Knowledge could be gained from experts, specialists in the area of interest,\nor it can be gained by induction from sets of data. Automatic induction of\nknowledge from data sets, usually stored in large databases, is called data\nmining. Data mining methods are important in the management of complex systems.\nThere are many technologies available to data mining practitioners, including\nArtificial Neural Networks, Regression, and Decision Trees. Neural networks\nhave been successfully applied in wide range of supervised and unsupervised\nlearning applications. Neural network methods are not commonly used for data\nmining tasks, because they often produce incomprehensible models, and require\nlong training times. One way in which the collective properties of a neural\nnetwork may be used to implement a computational task is by way of the concept\nof energy minimization. The Hopfield network is well-known example of such an\napproach. The Hopfield network is useful as content addressable memory or an\nanalog computer for solving combinatorial-type optimization problems. Wan\nAbdullah [1] proposed a method of doing logic programming on a Hopfield neural\nnetwork. Optimization of logical inconsistency is carried out by the network\nafter the connection strengths are defined from the logic program; the network\nrelaxes to neural states corresponding to a valid interpretation. In this\narticle, we describe how Hopfield network is able to induce logical rules from\nlarge database by using reverse analysis method: given the values of the\nconnections of a network, we can hope to know what logical rules are entrenched\nin the database."
},{
    "category": "cs.LO", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.4075v1", 
    "title": "Logic Learning in Hopfield Networks", 
    "arxiv-id": "0804.4075v1", 
    "author": "Wan Ahmad Tajuddin Wan Abdullah", 
    "publish": "2008-04-25T09:46:46Z", 
    "summary": "Synaptic weights for neurons in logic programming can be calculated either by\nusing Hebbian learning or by Wan Abdullah's method. In other words, Hebbian\nlearning for governing events corresponding to some respective program clauses\nis equivalent with learning using Wan Abdullah's method for the same respective\nprogram clauses. In this paper we will evaluate experimentally the equivalence\nbetween these two types of learning through computer simulations."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0804.4237v1", 
    "title": "Explaining the Logical Nature of Electrical Solitons in Neural Circuits", 
    "arxiv-id": "0804.4237v1", 
    "author": "John Robert Burger", 
    "publish": "2008-04-26T16:55:59Z", 
    "summary": "Neurons are modeled electrically based on ferroelectric membranes thin enough\nto permit charge transfer, conjectured to be the tunneling result of thermally\nenergetic ions and random electrons. These membranes can be triggered to\nproduce electrical solitons, the main signals for brain associative memory and\nlogical processing. Dendritic circuits are modeled, and electrical solitons are\nsimulated to demonstrate the nature of soliton propagation, soliton reflection,\nthe collision of solitons, as well as soliton OR gates, AND gates, XOR gates\nand NOT gates."
},{
    "category": "cond-mat.dis-nn", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.0197v1", 
    "title": "Flatness of the Energy Landscape for Horn Clauses", 
    "arxiv-id": "0805.0197v1", 
    "author": "Wan Ahmad Tajuddin Wan Abdullah", 
    "publish": "2008-05-02T09:20:11Z", 
    "summary": "The Little-Hopfield neural network programmed with Horn clauses is studied.\nWe argue that the energy landscape of the system, corresponding to the\ninconsistency function for logical interpretations of the sets of Horn clauses,\nhas minimal ruggedness. This is supported by computer simulations."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.1153v1", 
    "title": "Contact state analysis using NFIS and SOM", 
    "arxiv-id": "0805.1153v1", 
    "author": "H. Owladeghaffari", 
    "publish": "2008-05-08T12:10:21Z", 
    "summary": "This paper reports application of neuro- fuzzy inference system (NFIS) and\nself organizing feature map neural networks (SOM) on detection of contact state\nin a block system. In this manner, on a simple system, the evolution of contact\nstates, by parallelization of DDA, has been investigated. So, a comparison\nbetween NFIS and SOM results has been presented. The results show applicability\nof the proposed methods, by different accuracy, on detection of contact's\ndistribution."
},{
    "category": "cs.DL", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.1154v2", 
    "title": "Clustering of scientific citations in Wikipedia", 
    "arxiv-id": "0805.1154v2", 
    "author": "Finn Aarup Nielsen", 
    "publish": "2008-05-08T12:29:36Z", 
    "summary": "The instances of templates in Wikipedia form an interesting data set of\nstructured information. Here I focus on the cite journal template that is\nprimarily used for citation to articles in scientific journals. These citations\ncan be extracted and analyzed: Non-negative matrix factorization is performed\non a (article x journal) matrix resulting in a soft clustering of Wikipedia\narticles and scientific journals, each cluster more or less representing a\nscientific topic."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.1296v1", 
    "title": "A Simple Dynamic Mind-map Framework To Discover Associative   Relationships in Transactional Data Streams", 
    "arxiv-id": "0805.1296v1", 
    "author": "Christoph Schommer", 
    "publish": "2008-05-09T08:10:09Z", 
    "summary": "In this paper, we informally introduce dynamic mind-maps that represent a new\napproach on the basis of a dynamic construction of connectionist structures\nduring the processing of a data stream. This allows the representation and\nprocessing of recursively defined structures and avoids the problem of a more\ntraditional, fixed-size architecture with the processing of input structures of\nunknown size. For a data stream analysis with association discovery, the\nincremental analysis of data leads to results on demand. Here, we describe a\nframework that uses symbolic cells to calculate associations based on\ntransactional data streams as it exists in e.g. bibliographic databases. We\nfollow a natural paradigm of applying simple operations on cells yielding on a\nmind-map structure that adapts over time."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.1696v2", 
    "title": "Grammatical Evolution with Restarts for Fast Fractal Generation", 
    "arxiv-id": "0805.1696v2", 
    "author": "Alfonso Ortega", 
    "publish": "2008-05-12T17:55:59Z", 
    "summary": "In a previous work, the authors proposed a Grammatical Evolution algorithm to\nautomatically generate Lindenmayer Systems which represent fractal curves with\na pre-determined fractal dimension. This paper gives strong statistical\nevidence that the probability distributions of the execution time of that\nalgorithm exhibits a heavy tail with an hyperbolic probability decay for long\nexecutions, which explains the erratic performance of different executions of\nthe algorithm. Three different restart strategies have been incorporated in the\nalgorithm to mitigate the problems associated to heavy tail distributions: the\nfirst assumes full knowledge of the execution time probability distribution,\nthe second and third assume no knowledge. These strategies exploit the fact\nthat the probability of finding a solution in short executions is\nnon-negligible and yield a severe reduction, both in the expected execution\ntime (up to one order of magnitude) and in its variance, which is reduced from\nan infinite to a finite value."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.3126v1", 
    "title": "Cognitive Architecture for Direction of Attention Founded on Subliminal   Memory Searches, Pseudorandom and Nonstop", 
    "arxiv-id": "0805.3126v1", 
    "author": "J. R. Burger", 
    "publish": "2008-05-20T17:37:31Z", 
    "summary": "By way of explaining how a brain works logically, human associative memory is\nmodeled with logical and memory neurons, corresponding to standard digital\ncircuits. The resulting cognitive architecture incorporates basic psychological\nelements such as short term and long term memory. Novel to the architecture are\nmemory searches using cues chosen pseudorandomly from short term memory.\nRecalls alternated with sensory images, many tens per second, are analyzed\nsubliminally as an ongoing process, to determine a direction of attention in\nshort term memory."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0805.3800v1", 
    "title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from   Underrepresented Data", 
    "arxiv-id": "0805.3800v1", 
    "author": "Carsten Maple", 
    "publish": "2008-05-24T23:37:51Z", 
    "summary": "The use of multiple Decision Models (DMs) enables to enhance the accuracy in\ndecisions and at the same time allows users to evaluate the confidence in\ndecision making. In this paper we explore the ability of multiple DMs to learn\nfrom a small amount of verified data. This becomes important when data samples\nare difficult to collect and verify. We propose an evolutionary-based approach\nto solving this problem. The proposed technique is examined on a few clinical\nproblems presented by a small amount of data."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0806.2843v2", 
    "title": "MultiKulti Algorithm: Migrating the Most Different Genotypes in an   Island Model", 
    "arxiv-id": "0806.2843v2", 
    "author": "Francisco Fernandez de Vega", 
    "publish": "2008-06-17T17:19:13Z", 
    "summary": "Migration policies in distributed evolutionary algorithms has not been an\nactive research area until recently. However, in the same way as operators have\nan impact on performance, the choice of migrants is due to have an impact too.\nIn this paper we propose a new policy (named multikulti) for choosing the\nindividuals that are going to be sent to other nodes, based on\nmulticulturality: the individual sent should be as different as possible to the\nreceiving population. We have checked this policy on different discrete\noptimization problems, and found that, in average or in median, this policy\noutperforms classical ones like sending the best or a random individual."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0806.3646v2", 
    "title": "Round Trip Time Prediction Using the Symbolic Function Network Approach", 
    "arxiv-id": "0806.3646v2", 
    "author": "Sung Goo Yoo", 
    "publish": "2008-06-23T10:04:14Z", 
    "summary": "In this paper, we develop a novel approach to model the Internet round trip\ntime using a recently proposed symbolic type neural network model called\nsymbolic function network. The developed predictor is shown to have good\ngeneralization performance and simple representation compared to the multilayer\nperceptron based predictors."
},{
    "category": "q-bio.NC", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0807.1513v1", 
    "title": "A First-Order Non-Homogeneous Markov Model for the Response of Spiking   Neurons Stimulated by Small Phase-Continuous Signals", 
    "arxiv-id": "0807.1513v1", 
    "author": "R. Etienne-Cummings", 
    "publish": "2008-07-09T18:36:48Z", 
    "summary": "We present a first-order non-homogeneous Markov model for the\ninterspike-interval density of a continuously stimulated spiking neuron. The\nmodel allows the conditional interspike-interval density and the stationary\ninterspike-interval density to be expressed as products of two separate\nfunctions, one of which describes only the neuron characteristics, and the\nother of which describes only the signal characteristics. This allows the use\nof this model to predict the response when the underlying neuron model is not\nknown or well determined. The approximation shows particularly clearly that\nsignal autocorrelations and cross-correlations arise as natural features of the\ninterspike-interval density, and are particularly clear for small signals and\nmoderate noise. We show that this model simplifies the design of spiking neuron\ncross-correlation systems, and describe a four-neuron mutual inhibition network\nthat generates a cross-correlation output for two input signals."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0807.2928v1", 
    "title": "Visual Grouping by Neural Oscillators", 
    "arxiv-id": "0807.2928v1", 
    "author": "Jean-Jacques Slotine", 
    "publish": "2008-07-18T11:23:27Z", 
    "summary": "Distributed synchronization is known to occur at several scales in the brain,\nand has been suggested as playing a key functional role in perceptual grouping.\nState-of-the-art visual grouping algorithms, however, seem to give\ncomparatively little attention to neural synchronization analogies. Based on\nthe framework of concurrent synchronization of dynamic systems, simple networks\nof neural oscillators coupled with diffusive connections are proposed to solve\nvisual grouping problems. Multi-layer algorithms and feedback mechanisms are\nalso studied. The same algorithm is shown to achieve promising results on\nseveral classical visual grouping problems, including point clustering, contour\nintegration and image segmentation."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s11047-006-9026-4", 
    "link": "http://arxiv.org/pdf/0808.1378v1", 
    "title": "A Novel Symbolic Type Neural Network Model- Application to River Flow   Forecasting", 
    "arxiv-id": "0808.1378v1", 
    "author": "Amir F. Atiya", 
    "publish": "2008-08-09T22:05:48Z", 
    "summary": "In this paper we introduce a new symbolic type neural tree network called\nsymbolic function network (SFN) that is based on using elementary functions to\nmodel systems in a symbolic form. The proposed formulation permits feature\nselection, functional selection, and flexible structure. We applied this model\non the River Flow forecasting problem. The results found to be superior in both\nfitness and sparsity."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.comgeo.2010.03.004", 
    "link": "http://arxiv.org/pdf/0809.0835v2", 
    "title": "Approximating the volume of unions and intersections of high-dimensional   geometric objects", 
    "arxiv-id": "0809.0835v2", 
    "author": "Tobias Friedrich", 
    "publish": "2008-09-04T16:14:09Z", 
    "summary": "We consider the computation of the volume of the union of high-dimensional\ngeometric objects. While showing that this problem is #P-hard already for very\nsimple bodies (i.e., axis-parallel boxes), we give a fast FPRAS for all objects\nwhere one can: (1) test whether a given point lies inside the object, (2)\nsample a point uniformly, (3) calculate the volume of the object in polynomial\ntime. All three oracles can be weak, that is, just approximate. This implies\nthat Klee's measure problem and the hypervolume indicator can be approximated\nefficiently even though they are #P-hard and hence cannot be solved exactly in\ntime polynomial in the number of dimensions unless P=NP. Our algorithm also\nallows to approximate efficiently the volume of the union of convex bodies\ngiven by weak membership oracles.\n  For the analogous problem of the intersection of high-dimensional geometric\nobjects we prove #P-hardness for boxes and show that there is no multiplicative\npolynomial-time $2^{d^{1-\\epsilon}}$-approximation for certain boxes unless\nNP=BPP, but give a simple additive polynomial-time $\\epsilon$-approximation."
},{
    "category": "q-bio.NC", 
    "doi": "10.1162/neco.2008.03-08-734", 
    "link": "http://arxiv.org/pdf/0809.4296v1", 
    "title": "State dependent computation using coupled recurrent networks", 
    "arxiv-id": "0809.4296v1", 
    "author": "Rodney J. Douglas", 
    "publish": "2008-09-24T22:52:05Z", 
    "summary": "Although conditional branching between possible behavioural states is a\nhallmark of intelligent behavior, very little is known about the neuronal\nmechanisms that support this processing. In a step toward solving this problem\nwe demonstrate by theoretical analysis and simulation how networks of richly\ninter-connected neurons, such as those observed in the superficial layers of\nthe neocortex, can embed reliable robust finite state machines. We show how a\nmulti-stable neuronal network containing a number of states can be created very\nsimply, by coupling two recurrent networks whose synaptic weights have been\nconfigured for soft winner-take-all (sWTA) performance. These two sWTAs have\nsimple, homogenous locally recurrent connectivity except for a small fraction\nof recurrent cross-connections between them, which are used to embed the\nrequired states. This coupling between the maps allows the network to continue\nto express the current state even after the input that elicted that state is\nwithdrawn. In addition, a small number of 'transition neurons' implement the\nnecessary input-driven transitions between the embedded states. We provide\nsimple rules to systematically design and construct neuronal state machines of\nthis kind. The significance of our finding is that it offers a method whereby\nthe cortex could construct networks supporting a broad range of sophisticated\nprocessing by applying only small specializations to the same generic neuronal\ncircuit."
},{
    "category": "cs.HC", 
    "doi": "10.1162/neco.2008.03-08-734", 
    "link": "http://arxiv.org/pdf/0810.4884v2", 
    "title": "The adaptability of physiological systems optimizes performance: new   directions in augmentation", 
    "arxiv-id": "0810.4884v2", 
    "author": "Bradly Alicea", 
    "publish": "2008-10-27T17:42:05Z", 
    "summary": "This paper contributes to the human-machine interface community in two ways:\nas a critique of the closed-loop AC (augmented cognition) approach, and as a\nway to introduce concepts from complex systems and systems physiology into the\nfield. Of particular relevance is a comparison of the inverted-U (or Gaussian)\nmodel of optimal performance and multidimensional fitness landscape model.\nHypothetical examples will be given from human physiology and learning and\nmemory. In particular, a four-step model will be introduced that is proposed as\na better means to characterize multivariate systems during behavioral processes\nwith complex dynamics such as learning. Finally, the alternate approach\npresented herein is considered as a preferable design alternate in\nhuman-machine systems. It is within this context that future directions are\ndiscussed."
},{
    "category": "cs.NE", 
    "doi": "10.1162/neco.2008.03-08-734", 
    "link": "http://arxiv.org/pdf/0811.0823v1", 
    "title": "Distributed Constrained Optimization with Semicoordinate Transformations", 
    "arxiv-id": "0811.0823v1", 
    "author": "David Wolpert", 
    "publish": "2008-11-05T21:35:57Z", 
    "summary": "Recent work has shown how information theory extends conventional\nfull-rationality game theory to allow bounded rational agents. The associated\nmathematical framework can be used to solve constrained optimization problems.\nThis is done by translating the problem into an iterated game, where each agent\ncontrols a different variable of the problem, so that the joint probability\ndistribution across the agents' moves gives an expected value of the objective\nfunction. The dynamics of the agents is designed to minimize a Lagrangian\nfunction of that joint distribution. Here we illustrate how the updating of the\nLagrange parameters in the Lagrangian is a form of automated annealing, which\nfocuses the joint distribution more and more tightly about the joint moves that\noptimize the objective function. We then investigate the use of\n``semicoordinate'' variable transformations. These separate the joint state of\nthe agents from the variables of the optimization problem, with the two\nconnected by an onto mapping. We present experiments illustrating the ability\nof such transformations to facilitate optimization. We focus on the special\nkind of transformation in which the statistically independent states of the\nagents induces a mixture distribution over the optimization variables. Computer\nexperiment illustrate this for $k$-sat constraint satisfaction problems and for\nunconstrained minimization of $NK$ functions."
},{
    "category": "cs.AI", 
    "doi": "10.1162/neco.2008.03-08-734", 
    "link": "http://arxiv.org/pdf/0812.2535v1", 
    "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks", 
    "arxiv-id": "0812.2535v1", 
    "author": "K. Eswaran", 
    "publish": "2008-12-13T09:21:31Z", 
    "summary": "In this paper, we present a new kind of learning implementation to recognize\nthe patterns using the concept of Mirroring Neural Network (MNN) which can\nextract information from distinct sensory input patterns and perform pattern\nrecognition tasks. It is also capable of being used as an advanced associative\nmemory wherein image data is associated with voice inputs in an unsupervised\nmanner. Since the architecture is hierarchical and modular it has the potential\nof being used to devise learning engines of ever increasing complexity."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0812.2969v2", 
    "title": "A Growing Self-Organizing Network for Reconstructing Curves and Surfaces", 
    "arxiv-id": "0812.2969v2", 
    "author": "Marco Piastra", 
    "publish": "2008-12-16T15:59:36Z", 
    "summary": "Self-organizing networks such as Neural Gas, Growing Neural Gas and many\nothers have been adopted in actual applications for both dimensionality\nreduction and manifold learning. Typically, in these applications, the\nstructure of the adapted network yields a good estimate of the topology of the\nunknown subspace from where the input data points are sampled. The approach\npresented here takes a different perspective, namely by assuming that the input\nspace is a manifold of known dimension. In return, the new type of growing\nself-organizing network presented gains the ability to adapt itself in way that\nmay guarantee the effective and stable recovery of the exact topological\nstructure of the input manifold."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0812.4170v1", 
    "title": "Finding Still Lifes with Memetic/Exact Hybrid Algorithms", 
    "arxiv-id": "0812.4170v1", 
    "author": "Antonio J. Fernandez", 
    "publish": "2008-12-22T13:09:11Z", 
    "summary": "The maximum density still life problem (MDSLP) is a hard constraint\noptimization problem based on Conway's game of life. It is a prime example of\nweighted constrained optimization problem that has been recently tackled in the\nconstraint-programming community. Bucket elimination (BE) is a complete\ntechnique commonly used to solve this kind of constraint satisfaction problem.\nWhen the memory required to apply BE is too high, a heuristic method based on\nit (denominated mini-buckets) can be used to calculate bounds for the optimal\nsolution. Nevertheless, the curse of dimensionality makes these techniques\nunpractical for large size problems. In response to this situation, we present\na memetic algorithm for the MDSLP in which BE is used as a mechanism for\nrecombining solutions, providing the best possible child from the parental set.\nSubsequently, a multi-level model in which this exact/metaheuristic hybrid is\nfurther hybridized with branch-and-bound techniques and mini-buckets is\nstudied. Extensive experimental results analyze the performance of these models\nand multi-parent recombination. The resulting algorithm consistently finds\noptimal patterns for up to date solved instances in less time than current\napproaches. Moreover, it is shown that this proposal provides new best known\nsolutions for very large instances."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0812.4360v2", 
    "title": "Driven by Compression Progress: A Simple Principle Explains Essential   Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention,   Curiosity, Creativity, Art, Science, Music, Jokes", 
    "arxiv-id": "0812.4360v2", 
    "author": "Juergen Schmidhuber", 
    "publish": "2008-12-23T10:14:18Z", 
    "summary": "I argue that data becomes temporarily interesting by itself to some\nself-improving, but computationally limited, subjective observer once he learns\nto predict or compress the data in a better way, thus making it subjectively\nsimpler and more beautiful. Curiosity is the desire to create or discover more\nnon-random, non-arbitrary, regular data that is novel and surprising not in the\ntraditional sense of Boltzmann and Shannon but in the sense that it allows for\ncompression progress because its regularity was not yet known. This drive\nmaximizes interestingness, the first derivative of subjective beauty or\ncompressibility, that is, the steepness of the learning curve. It motivates\nexploring infants, pure mathematicians, composers, artists, dancers, comedians,\nyourself, and (since 1990) artificial systems."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0901.0317v1", 
    "title": "Design of a P System based Artificial Graph Chemistry", 
    "arxiv-id": "0901.0317v1", 
    "author": "Janardan Misra", 
    "publish": "2009-01-03T17:35:49Z", 
    "summary": "Artificial Chemistries (ACs) are symbolic chemical metaphors for the\nexploration of Artificial Life, with specific focus on the origin of life. In\nthis work we define a P system based artificial graph chemistry to understand\nthe principles leading to the evolution of life-like structures in an AC set up\nand to develop a unified framework to characterize and classify symbolic\nartificial chemistries by devising appropriate formalism to capture semantic\nand organizational information. An extension of P system is considered by\nassociating probabilities with the rules providing the topological framework\nfor the evolution of a labeled undirected graph based molecular reaction\nsemantics."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0901.0597v4", 
    "title": "On the Optimal Convergence Probability of Univariate Estimation of   Distribution Algorithms", 
    "arxiv-id": "0901.0597v4", 
    "author": "Reza Rastegar", 
    "publish": "2009-01-06T06:36:54Z", 
    "summary": "In this paper, we obtain bounds on the probability of convergence to the\noptimal solution for the compact Genetic Algorithm (cGA) and the Population\nBased Incremental Learning (PBIL). We also give a sufficient condition for\nconvergence of these algorithms to the optimal solution and compute a range of\npossible values of the parameters of these algorithms for which they converge\nto the optimal solution with a confidence level."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0901.0598v1", 
    "title": "A Step Forward in Studying the Compact Genetic Algorithm", 
    "arxiv-id": "0901.0598v1", 
    "author": "Arash Hariri", 
    "publish": "2009-01-06T07:07:00Z", 
    "summary": "The compact Genetic Algorithm (cGA) is an Estimation of Distribution\nAlgorithm that generates offspring population according to the estimated\nprobabilistic model of the parent population instead of using traditional\nrecombination and mutation operators. The cGA only needs a small amount of\nmemory; therefore, it may be quite useful in memory-constrained applications.\nThis paper introduces a theoretical framework for studying the cGA from the\nconvergence point of view in which, we model the cGA by a Markov process and\napproximate its behavior using an Ordinary Differential Equation (ODE). Then,\nwe prove that the corresponding ODE converges to local optima and stays there.\nConsequently, we conclude that the cGA will converge to the local optima of the\nfunction to be optimized."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0901.1152v1", 
    "title": "A nonclassical symbolic theory of working memory, mental computations,   and mental set", 
    "arxiv-id": "0901.1152v1", 
    "author": "Victor Eliashberg", 
    "publish": "2009-01-08T23:42:45Z", 
    "summary": "The paper tackles four basic questions associated with human brain as a\nlearning system. How can the brain learn to (1) mentally simulate different\nexternal memory aids, (2) perform, in principle, any mental computations using\nimaginary memory aids, (3) recall the real sensory and motor events and\nsynthesize a combinatorial number of imaginary events, (4) dynamically change\nits mental set to match a combinatorial number of contexts? We propose a\nuniform answer to (1)-(4) based on the general postulate that the human\nneocortex processes symbolic information in a \"nonclassical\" way. Instead of\nmanipulating symbols in a read/write memory, as the classical symbolic systems\ndo, it manipulates the states of dynamical memory representing different\ntemporary attributes of immovable symbolic structures stored in a long-term\nmemory. The approach is formalized as the concept of E-machine. Intuitively, an\nE-machine is a system that deals mainly with characteristic functions\nrepresenting subsets of memory pointers rather than the pointers themselves.\nThis nonclassical symbolic paradigm is Turing universal, and, unlike the\nclassical one, is efficiently implementable in homogeneous neural networks with\ntemporal modulation topologically resembling that of the neocortex."
},{
    "category": "cs.NE", 
    "doi": "10.1109/IJCNN.2009.5178709", 
    "link": "http://arxiv.org/pdf/0901.1610v1", 
    "title": "Towards a Framework for Observing Artificial Evolutionary Systems", 
    "arxiv-id": "0901.1610v1", 
    "author": "Janardan Misra", 
    "publish": "2009-01-12T16:35:58Z", 
    "summary": "Establishing the emergence of evolutionary behavior as a defining\ncharacteristic of 'life' is a major step in the Artificial life (ALife)\nstudies. We present here an abstract formal framework for this aim based upon\nthe notion of high-level observations made on the ALife model at hand during\nits simulations. An observation process is defined as a computable\ntransformation from the underlying dynamic structure of the model universe to a\ntuple consisting of abstract components needed to establish the evolutionary\nprocesses in the model. Starting with defining entities and their evolutionary\nrelationships observed during the simulations of the model, the framework\nprescribes a series of definitions, followed by the axioms (conditions) that\nmust be met in order to establish the level of evolutionary behavior in the\nmodel. The examples of Cellular Automata based Langton Loops and Lambda\ncalculus based Algorithmic Chemistry are used to illustrate the framework.\nGeneric design suggestions for the ALife research are also drawn based upon the\nframework design and case study analysis."
},{
    "category": "cs.NE", 
    "doi": "10.1002/nme.1150", 
    "link": "http://arxiv.org/pdf/0902.1037v2", 
    "title": "Optimal design and optimal control of structures undergoing finite   rotations and elastic deformations", 
    "arxiv-id": "0902.1037v2", 
    "author": "P. Villon", 
    "publish": "2009-02-06T09:43:09Z", 
    "summary": "In this work we deal with the optimal design and optimal control of\nstructures undergoing large rotations. In other words, we show how to find the\ncorresponding initial configuration and the corresponding set of multiple load\nparameters in order to recover a desired deformed configuration or some\ndesirable features of the deformed configuration as specified more precisely by\nthe objective or cost function. The model problem chosen to illustrate the\nproposed optimal design and optimal control methodologies is the one of\ngeometrically exact beam. First, we present a non-standard formulation of the\noptimal design and optimal control problems, relying on the method of Lagrange\nmultipliers in order to make the mechanics state variables independent from\neither design or control variables and thus provide the most general basis for\ndeveloping the best possible solution procedure. Two different solution\nprocedures are then explored, one based on the diffuse approximation of\nresponse function and gradient method and the other one based on genetic\nalgorithm. A number of numerical examples are given in order to illustrate both\nthe advantages and potential drawbacks of each of the presented procedures."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cam.2009.01.016", 
    "link": "http://arxiv.org/pdf/0902.1040v2", 
    "title": "Fast solving of Weighted Pairing Least-Squares systems", 
    "arxiv-id": "0902.1040v2", 
    "author": "Pierre Courrieu", 
    "publish": "2009-02-06T09:51:09Z", 
    "summary": "This paper presents a generalization of the \"weighted least-squares\" (WLS),\nnamed \"weighted pairing least-squares\" (WPLS), which uses a rectangular weight\nmatrix and is suitable for data alignment problems. Two fast solving methods,\nsuitable for solving full rank systems as well as rank deficient systems, are\nstudied. Computational experiments clearly show that the best method, in terms\nof speed, accuracy, and numerical stability, is based on a special {1, 2,\n3}-inverse, whose computation reduces to a very simple generalization of the\nusual \"Cholesky factorization-backward substitution\" method for solving linear\nsystems."
},{
    "category": "cs.NE", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/0902.1629v1", 
    "title": "Improvements of real coded genetic algorithms based on differential   operators preventing premature convergence", 
    "arxiv-id": "0902.1629v1", 
    "author": "A. Kucerova", 
    "publish": "2009-02-10T11:02:50Z", 
    "summary": "This paper presents several types of evolutionary algorithms (EAs) used for\nglobal optimization on real domains. The interest has been focused on\nmultimodal problems, where the difficulties of a premature convergence usually\noccurs. First the standard genetic algorithm (SGA) using binary encoding of\nreal values and its unsatisfactory behavior with multimodal problems is briefly\nreviewed together with some improvements of fighting premature convergence. Two\ntypes of real encoded methods based on differential operators are examined in\ndetail: the differential evolution (DE), a very modern and effective method\nfirstly published by R. Storn and K. Price, and the simplified real-coded\ndifferential genetic algorithm SADE proposed by the authors. In addition, an\nimprovement of the SADE method, called CERAF technology, enabling the\npopulation of solutions to escape from local extremes, is examined. All methods\nare tested on an identical set of objective functions and a systematic\ncomparison based on a reliable methodology is presented. It is confirmed that\nreal coded methods generally exhibit better behavior on real domains than the\nbinary algorithms, even when extended by several improvements. Furthermore, the\npositive influence of the differential operators due to their possibility of\nself-adaptation is demonstrated. From the reliability point of view, it seems\nthat the real encoded differential algorithm, improved by the technology\ndescribed in this paper, is a universal and reliable method capable of solving\nall proposed test problems."
}]