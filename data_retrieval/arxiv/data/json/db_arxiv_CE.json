[{
    "category": "cs.CE", 
    "author": "Lawrence Camilletti", 
    "title": "Chip-level CMP Modeling and Smart Dummy for HDP and Conformal CVD Films", 
    "publish": "2000-11-09T20:39:25Z", 
    "summary": "Chip-level CMP modeling is investigated to obtain the post-CMP film profile\nthickness across a die from its design layout file and a few film deposition\nand CMP parameters. The work covers both HDP and conformal CVD film. The\nexperimental CMP results agree well with the modeled results. Different\nalgorithms for filling of dummy structure are compared. A smart algorithm for\ndummy filling is presented, which achieves maximal pattern-density uniformity\nand CMP planarity.", 
    "link": "http://arxiv.org/pdf/cs/0011014v1", 
    "arxiv-id": "cs/0011014v1"
},{
    "category": "cs.CE", 
    "author": "Leonid A. Levin", 
    "title": "The Equity Tax and Shelter", 
    "publish": "2000-12-18T03:02:34Z", 
    "summary": "Taxes have major costs beyond the collected revenue: deadweight from\ndistorted incentives, compliance and enforcement costs, etc. A simple market\nmechanism, the Equity Tax, avoids these problems for the trickiest cases:\ncorporate, dividend, and capital gains taxes.\n  It exploits the ability of the share prices to reflect the expected true\nannual return (as perceived by investors, not as defined by law) and works only\nfor publicly held corporations. Since going or staying public cannot be forced,\nand for some constitutional reasons too, the conversion to equity tax must be a\nvoluntary contract. Repeated reconversions would be costly (all capital gains\nare realized) and thus rare. The converts and their shareholders pay no income,\ndividend, or capital gain taxes. Instead, they give the IRS, say, 2% of stock\nper year to auction promptly. Debts are the lender's assets: its status, not\nthe debtor's, determines their equity-tax or income-tax treatment.\n  The system looks too simple to be right. However, it does have no loopholes\n(thus lowering the revenue-neutral tax rate), no compliance costs, requires\nlittle regulation, and leaves all business decisions tax neutral. The total\ncapital the equity taxed sector absorbs is the only thing the tax could\npossibly distort. The rates should match so as to minimize this distortion. The\nequity tax enlarges the pre-tax profit since this is what the taxpayers\nmaximize, not a different after-tax net. The wealth shelter is paid for by\nefficiency, not by lost tax.", 
    "link": "http://arxiv.org/pdf/cs/0012013v16", 
    "arxiv-id": "cs/0012013v16"
},{
    "category": "cs.CE", 
    "author": "Shuba Raghavan", 
    "title": "Fast Pricing of European Asian Options with Provable Accuracy:   Single-stock and Basket Options", 
    "publish": "2001-02-02T20:52:36Z", 
    "summary": "This paper develops three polynomial-time pricing techniques for European\nAsian options with provably small errors, where the stock prices follow\nbinomial trees or trees of higher-degree. The first technique is the first\nknown Monte Carlo algorithm with analytical error bounds suitable for pricing\nsingle-stock options with meaningful confidence and speed. The second technique\nis a general recursive bucketing-based scheme that can use the\nAingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and\npossibly others as the base-case subroutine. This scheme enables robust\ntrade-offs between accuracy and time over subtrees of different sizes. For\nlong-term options or high frequency price averaging, it can price single-stock\noptions with smaller errors in less time than the base-case algorithms\nthemselves. The third technique combines Fast Fourier Transform with\nbucketing-based schemes for pricing basket options. This technique takes\npolynomial time in the number of days and the number of stocks, and does not\nadd any errors to those already incurred in the companion bucketing scheme.\nThis technique assumes that the price of each underlying stock moves\nindependently.", 
    "link": "http://arxiv.org/pdf/cs/0102003v1", 
    "arxiv-id": "cs/0102003v1"
},{
    "category": "cs.CE", 
    "author": "Koichiro Matsuno", 
    "title": "Shooting Over or Under the Mark: Towards a Reliable and Flexible   Anticipation in the Economy", 
    "publish": "2001-04-09T06:43:12Z", 
    "summary": "The real monetary economy is grounded upon monetary flow equilibration or the\nactivity of actualizing monetary flow continuity at each economic agent except\nfor the central bank. Every update of monetary flow continuity at each agent\nconstantly causes monetary flow equilibration at the neighborhood agents. Every\nmonetary flow equilibration as the activity of shooting the mark identified as\nmonetary flow continuity turns out to be off the mark, and constantly generate\nthe similar activities in sequence. Monetary flow equilibration ceaselessly\nreverberating in the economy performs two functions. One is to seek an\norganization on its own, and the other is to perturb the ongoing organization.\nMonetary flow equilibration as the agency of seeking and perturbing its\norganization also serves as a means of predicting its behavior. The likely\norganizational behavior could be the one that remains most robust against\nmonetary flow equilibration as an agency of applying perturbations.", 
    "link": "http://arxiv.org/pdf/cs/0104013v1", 
    "arxiv-id": "cs/0104013v1"
},{
    "category": "cs.CE", 
    "author": "Koichiro Matsuno", 
    "title": "Tracing a Faint Fingerprint of the Invisible Hand?", 
    "publish": "2001-04-09T08:49:57Z", 
    "summary": "Any economic agent constituting the monetary economy maintains the activity\nof monetary flow equilibration for fulfilling the condition of monetary flow\ncontinuity in the record, except at the central bank. At the same time,\nmonetary flow equilibration at one economic agent constantly induces at other\nagents in the economy further flow disequilibrium to be eliminated\nsubsequently. We propose the rate of monetary flow disequilibration as a figure\nmeasuring the progressive movement of the economy. The rate of disequilibration\nwas read out of both the Japanese and the United States monetary economy\nrecorded over the last fifty years.", 
    "link": "http://arxiv.org/pdf/cs/0104014v1", 
    "arxiv-id": "cs/0104014v1"
},{
    "category": "cs.CE", 
    "author": "Marcus Rickert", 
    "title": "Parallel implementation of the TRANSIMS micro-simulation", 
    "publish": "2001-05-02T12:43:39Z", 
    "summary": "This paper describes the parallel implementation of the TRANSIMS traffic\nmicro-simulation. The parallelization method is domain decomposition, which\nmeans that each CPU of the parallel computer is responsible for a different\ngeographical area of the simulated region. We describe how information between\ndomains is exchanged, and how the transportation network graph is partitioned.\nAn adaptive scheme is used to optimize load balancing. We then demonstrate how\ncomputing speeds of our parallel micro-simulations can be systematically\npredicted once the scenario and the computer architecture are known. This makes\nit possible, for example, to decide if a certain study is feasible with a\ncertain computing budget, and how to invest that budget. The main ingredients\nof the prediction are knowledge about the parallel implementation of the\nmicro-simulation, knowledge about the characteristics of the partitioning of\nthe transportation network graph, and knowledge about the interaction of these\nquantities with the computer system. In particular, we investigate the\ndifferences between switched and non-switched topologies, and the effects of 10\nMbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel\ncomputing, transportation planning, TRANSIMS", 
    "link": "http://arxiv.org/pdf/cs/0105004v1", 
    "arxiv-id": "cs/0105004v1"
},{
    "category": "cs.CE", 
    "author": "Fedor S. Kilin", 
    "title": "Analysis of Investment Policy in Belarus", 
    "publish": "2001-10-31T20:18:35Z", 
    "summary": "The optimal planning trajectory is analyzed on the basis of the growth model\nwith effectiveness. The saving per capital value has to be rather high\ninitially with smooth decrement in the future years.", 
    "link": "http://arxiv.org/pdf/cs/0110067v1", 
    "arxiv-id": "cs/0110067v1"
},{
    "category": "cs.CE", 
    "author": "Gemunu H. Gunaratne", 
    "title": "An Empirical Model for Volatility of Returns and Option Pricing", 
    "publish": "2002-01-29T18:03:39Z", 
    "summary": "In a seminal paper in 1973, Black and Scholes argued how expected\ndistributions of stock prices can be used to price options. Their model assumed\na directed random motion for the returns and consequently a lognormal\ndistribution of asset prices after a finite time. We point out two problems\nwith their formulation. First, we show that the option valuation is not\nuniquely determined; in particular, stratergies based on the delta-hedge and\nCAMP (Capital Asset Pricing Model) are shown to provide different valuations of\nan option. Second, asset returns are known not to be Gaussian distributed.\nEmpirically, distributions of returns are seen to be much better approximated\nby an exponential distribution. This exponential distribution of asset prices\ncan be used to develop a new pricing model for options that is shown to provide\nvaluations that agree very well with those used by traders. We show how the\nFokker-Planck formulation of fluctuations (i.e., the dynamics of the\ndistribution) can be modified to provide an exponential distribution for\nreturns. We also show how a singular volatility can be used to go smoothly from\nexponential to Gaussian returns and thereby illustrate why exponential returns\ncannot be reached perturbatively starting from Gaussian ones, and explain how\nthe theory of 'stochastic volatility' can be obtained from our model by making\na bad approximation. Finally, we show how to calculate put and call prices for\na stretched exponential density.", 
    "link": "http://arxiv.org/pdf/cs/0201026v1", 
    "arxiv-id": "cs/0201026v1"
},{
    "category": "cs.CE", 
    "author": "Magnus Boman", 
    "title": "Agent trade servers in financial exchange systems", 
    "publish": "2002-03-19T10:05:58Z", 
    "summary": "New services based on the best-effort paradigm could complement the current\ndeterministic services of an electronic financial exchange. Four crucial\naspects of such systems would benefit from a hybrid stance: proper use of\nprocessing resources, bandwidth management, fault tolerance, and exception\nhandling. We argue that a more refined view on Quality-of-Service control for\nexchange systems, in which the principal ambition of upholding a fair and\norderly marketplace is left uncompromised, would benefit all interested\nparties.", 
    "link": "http://arxiv.org/pdf/cs/0203023v1", 
    "arxiv-id": "cs/0203023v1"
},{
    "category": "cs.CE", 
    "author": "David Lyback", 
    "title": "Parrondo Strategies for Artificial Traders", 
    "publish": "2002-04-26T12:20:08Z", 
    "summary": "On markets with receding prices, artificial noise traders may consider\nalternatives to buy-and-hold. By simulating variations of the Parrondo\nstrategy, using real data from the Swedish stock market, we produce first\nindications of a buy-low-sell-random Parrondo variation outperforming\nbuy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms\nthe traditional value and trend investor strategies. We measure the success of\nthe Parrondo variations not only through their performance compared to other\nkinds of strategies, but also relative to varying levels of perfect\ninformation, received through messages within a multi-agent system of\nartificial traders.", 
    "link": "http://arxiv.org/pdf/cs/0204051v1", 
    "arxiv-id": "cs/0204051v1"
},{
    "category": "cs.CE", 
    "author": "David Lyback", 
    "title": "Trading Agents for Roaming Users", 
    "publish": "2002-04-29T12:20:11Z", 
    "summary": "Some roaming users need services to manipulate autonomous processes. Trading\nagents running on agent trade servers are used as a case in point. We present a\nsolution that provides the agent owners with means to upkeeping their desktop\nenvironment, and maintaining their agent trade server processes, via a\nbriefcase service.", 
    "link": "http://arxiv.org/pdf/cs/0204056v1", 
    "arxiv-id": "cs/0204056v1"
},{
    "category": "cs.CE", 
    "author": "H. Yusupov", 
    "title": "Symbolic Methodology in Numeric Data Mining: Relational Techniques for   Financial Applications", 
    "publish": "2002-08-15T03:45:36Z", 
    "summary": "Currently statistical and artificial neural network methods dominate in\nfinancial data mining. Alternative relational (symbolic) data mining methods\nhave shown their effectiveness in robotics, drug design and other applications.\nTraditionally symbolic methods prevail in the areas with significant\nnon-numeric (symbolic) knowledge, such as relative location in robot\nnavigation. At first glance, stock market forecast looks as a pure numeric area\nirrelevant to symbolic methods. One of our major goals is to show that\nfinancial time series can benefit significantly from relational data mining\nbased on symbolic methods. The paper overviews relational data mining\nmethodology and develops this techniques for financial data mining.", 
    "link": "http://arxiv.org/pdf/cs/0208022v1", 
    "arxiv-id": "cs/0208022v1"
},{
    "category": "cs.CE", 
    "author": "Theodore S. Rappaport", 
    "title": "Using Hierarchical Data Mining to Characterize Performance of Wireless   System Configurations", 
    "publish": "2002-08-25T16:28:33Z", 
    "summary": "This paper presents a statistical framework for assessing wireless systems\nperformance using hierarchical data mining techniques. We consider WCDMA\n(wideband code division multiple access) systems with two-branch STTD (space\ntime transmit diversity) and 1/2 rate convolutional coding (forward error\ncorrection codes). Monte Carlo simulation estimates the bit error probability\n(BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A\nperformance database of simulation runs is collected over a targeted space of\nsystem configurations. This database is then mined to obtain regions of the\nconfiguration space that exhibit acceptable average performance. The shape of\nthe mined regions illustrates the joint influence of configuration parameters\non system performance. The role of data mining in this application is to\nprovide explainable and statistically valid design conclusions. The research\nissue is to define statistically meaningful aggregation of data in a manner\nthat permits efficient and effective data mining algorithms. We achieve a good\ncompromise between these goals and help establish the applicability of data\nmining for characterizing wireless systems performance.", 
    "link": "http://arxiv.org/pdf/cs/0208040v1", 
    "arxiv-id": "cs/0208040v1"
},{
    "category": "cs.CE", 
    "author": "W Chen", 
    "title": "Positive time fractional derivative", 
    "publish": "2002-10-07T19:28:50Z", 
    "summary": "In mathematical modeling of the non-squared frequency-dependent diffusions,\nalso known as the anomalous diffusions, it is desirable to have a positive real\nFourier transform for the time derivative of arbitrary fractional or odd\ninteger order. The Fourier transform of the fractional time derivative in the\nRiemann-Liouville and Caputo senses, however, involves a complex power function\nof the fractional order. In this study, a positive time derivative of\nfractional or odd integer order is introduced to respect the positivity in\nmodeling the anomalous diffusions.", 
    "link": "http://arxiv.org/pdf/cs/0210005v1", 
    "arxiv-id": "cs/0210005v1"
},{
    "category": "cs.CE", 
    "author": "Alexandre d'Aspremont", 
    "title": "Interest Rate Model Calibration Using Semidefinite Programming", 
    "publish": "2003-02-25T02:48:42Z", 
    "summary": "We show that, for the purpose of pricing Swaptions, the Swap rate and the\ncorresponding Forward rates can be considered lognormal under a single\nmartingale measure. Swaptions can then be priced as options on a basket of\nlognormal assets and an approximation formula is derived for such options. This\nformula is centered around a Black-Scholes price with an appropriate\nvolatility, plus a correction term that can be interpreted as the expected\ntracking error. The calibration problem can then be solved very efficiently\nusing semidefinite programming.", 
    "link": "http://arxiv.org/pdf/cs/0302034v2", 
    "arxiv-id": "cs/0302034v2"
},{
    "category": "cs.CE", 
    "author": "Alexandre d'Aspremont", 
    "title": "Risk-Management Methods for the Libor Market Model Using Semidefinite   Programming", 
    "publish": "2003-02-25T03:09:11Z", 
    "summary": "When interest rate dynamics are described by the Libor Market Model as in\nBGM97, we show how some essential risk-management results can be obtained from\nthe dual of the calibration program. In particular, if the objetive is to\nmaximize another swaption's price, we show that the optimal dual variables\ndescribe a hedging portfolio in the sense of \\cite{Avel96}. In the general\ncase, the local sensitivity of the covariance matrix to all market movement\nscenarios can be directly computed from the optimal dual solution. We also show\nhow semidefinite programming can be used to manage the Gamma exposure of a\nportfolio.", 
    "link": "http://arxiv.org/pdf/cs/0302035v2", 
    "arxiv-id": "cs/0302035v2"
},{
    "category": "cs.CE", 
    "author": "Gilles Daniel", 
    "title": "Stochastic Volatility in a Quantitative Model of Stock Market Returns", 
    "publish": "2003-04-07T17:37:55Z", 
    "summary": "Standard quantitative models of the stock market predict a log-normal\ndistribution for stock returns (Bachelier 1900, Osborne 1959), but it is\nrecognised (Fama 1965) that empirical data, in comparison with a Gaussian,\nexhibit leptokurtosis (it has more probability mass in its tails and centre)\nand fat tails (probabilities of extreme events are underestimated). Different\nattempts to explain this departure from normality have coexisted. In\nparticular, since one of the strong assumptions of the Gaussian model concerns\nthe volatility, considered finite and constant, the new models were built on a\nnon finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)\nvolatility. We investigate in this thesis a very recent model (Dragulescu et\nal. 2002) based on a Brownian motion process for the returns, and a stochastic\nmean-reverting process for the volatility. In this model, the forward\nKolmogorov equation that governs the time evolution of returns is solved\nanalytically. We test this new theory against different stock indexes (Dow\nJones Industrial Average, Standard and Poor s and Footsie), over different\nperiods (from 20 to 105 years). Our aim is to compare this model with the\nclassical Gaussian and with a simple Neural Network, used as a benchmark. We\nperform the usual statistical tests on the kurtosis and tails of the expected\ndistributions, paying particular attention to the outliers. As claimed by the\nauthors, the new model outperforms the Gaussian for any time lag, but is\nartificially too complex for medium and low frequencies, where the Gaussian is\npreferable. Moreover this model is still rejected for high frequencies, at a\n0.05 level of significance, due to the kurtosis, incorrectly handled.", 
    "link": "http://arxiv.org/pdf/cs/0304009v1", 
    "arxiv-id": "cs/0304009v1"
},{
    "category": "cs.CE", 
    "author": "Jan-Ove Palmberg", 
    "title": "Using Dynamic Simulation in the Development of Construction Machinery", 
    "publish": "2003-05-19T20:51:50Z", 
    "summary": "As in the car industry for quite some time, dynamic simulation of complete\nvehicles is being practiced more and more in the development of off-road\nmachinery. However, specific questions arise due not only to company structure\nand size, but especially to the type of product. Tightly coupled, non-linear\nsubsystems of different domains make prediction and optimisation of the\ncomplete system's dynamic behaviour a challenge. Furthermore, the demand for\nversatile machines leads to sometimes contradictory target requirements and can\nturn the design process into a hunt for the least painful compromise. This can\nbe avoided by profound system knowledge, assisted by simulation-driven product\ndevelopment. This paper gives an overview of joint research into this issue by\nVolvo Wheel Loaders and Linkoping University on that matter, lists the results\nof a related literature review and introduces the term \"operateability\". Rather\nthan giving detailed answers, the problem space for ongoing and future research\nis examined and possible solutions are sketched.", 
    "link": "http://arxiv.org/pdf/cs/0305036v4", 
    "arxiv-id": "cs/0305036v4"
},{
    "category": "cs.CE", 
    "author": "Gilles Daniel", 
    "title": "Goodness-of-fit of the Heston model", 
    "publish": "2003-05-29T18:13:08Z", 
    "summary": "An analytical formula for the probability distribution of stock-market\nreturns, derived from the Heston model assuming a mean-reverting stochastic\nvolatility, was recently proposed by Dragulescu and Yakovenko in Quantitative\nFinance 2002. While replicating their results, we found two significant\nweaknesses in their method to pre-process the data, which cast a shadow over\nthe effective goodness-of-fit of the model. We propose a new method, more truly\ncapturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square\ntest on the resulting probability distribution. The results raise some\nsignificant questions for large time lags -- 40 to 250 days -- where the\nsmoothness of the data does not require such a complex model; nevertheless, we\nalso provide some statistical evidence in favour of the Heston model for small\ntime lags -- 1 and 5 days -- compared with the traditional Gaussian model\nassuming constant volatility.", 
    "link": "http://arxiv.org/pdf/cs/0305055v1", 
    "arxiv-id": "cs/0305055v1"
},{
    "category": "cs.CE", 
    "author": "Andrea Formica", 
    "title": "Design, implementation and deployment of the Saclay muon reconstruction   algorithms (Muonbox/y) in the Athena software framework of the ATLAS   experiment", 
    "publish": "2003-06-17T07:01:43Z", 
    "summary": "This paper gives an overview of a reconstruction algorithm for muon events in\nATLAS experiment at CERN. After a short introduction on ATLAS Muon\nSpectrometer, we will describe the procedure performed by the algorithms\nMuonbox and Muonboy (last version) in order to achieve correctly the\nreconstruction task. These algorithms have been developed in Fortran language\nand are working in the official C++ framework Athena, as well as in stand alone\nmode. A description of the interaction between Muonboy and Athena will be\ngiven, together with the reconstruction performances (efficiency and momentum\nresolution) obtained with MonteCarlo data.", 
    "link": "http://arxiv.org/pdf/cs/0306105v1", 
    "arxiv-id": "cs/0306105v1"
},{
    "category": "cs.CE", 
    "author": "Audris Kalnins", 
    "title": "Modeling Business", 
    "publish": "2003-07-17T15:41:13Z", 
    "summary": "Business concepts are studied using a metamodel-based approach, using UML\n2.0. The Notation Independent Business concepts metamodel is introduced. The\napproach offers a mapping between different business modeling notations which\ncould be used for bridging BM tools and boosting the MDA approach.", 
    "link": "http://arxiv.org/pdf/cs/0307039v1", 
    "arxiv-id": "cs/0307039v1"
},{
    "category": "cs.CE", 
    "author": "Emilio Torrente-Lujan", 
    "title": "Hamevol1.0: a C++ code for differential equations based on Runge-Kutta   algorithm. An application to matter enhanced neutrino oscillation", 
    "publish": "2003-07-23T19:30:25Z", 
    "summary": "We present a C++ implementation of a fifth order semi-implicit Runge-Kutta\nalgorithm for solving Ordinary Differential Equations. This algorithm can be\nused for studying many different problems and in particular it can be applied\nfor computing the evolution of any system whose Hamiltonian is known. We\nconsider in particular the problem of calculating the neutrino oscillation\nprobabilities in presence of matter interactions. The time performance and the\naccuracy of this implementation is competitive with respect to the other\nanalytical and numerical techniques used in literature. The algorithm design\nand the salient features of the code are presented and discussed and some\nexplicit examples of code application are given.", 
    "link": "http://arxiv.org/pdf/cs/0307053v1", 
    "arxiv-id": "cs/0307053v1"
},{
    "category": "cs.CE", 
    "author": "Anna Sandin", 
    "title": "Implementing an Agent Trade Server", 
    "publish": "2003-07-29T12:58:45Z", 
    "summary": "An experimental server for stock trading autonomous agents is presented and\nmade available, together with an agent shell for swift development. The server,\nwritten in Java, was implemented as proof-of-concept for an agent trade server\nfor a real financial exchange.", 
    "link": "http://arxiv.org/pdf/cs/0307064v1", 
    "arxiv-id": "cs/0307064v1"
},{
    "category": "cs.CE", 
    "author": "Gert R. G. Lanckriet", 
    "title": "A direct formulation for sparse PCA using semidefinite programming", 
    "publish": "2004-06-16T01:23:03Z", 
    "summary": "We examine the problem of approximating, in the Frobenius-norm sense, a\npositive, semidefinite symmetric matrix by a rank-one matrix, with an upper\nbound on the cardinality of its eigenvector. The problem arises in the\ndecomposition of a covariance matrix into sparse factors, and has wide\napplications ranging from biology to finance. We use a modification of the\nclassical variational representation of the largest eigenvalue of a symmetric\nmatrix, where cardinality is constrained, and derive a semidefinite programming\nbased relaxation for our problem. We also discuss Nesterov's smooth\nminimization technique applied to the SDP arising in the direct sparse PCA\nmethod.", 
    "link": "http://arxiv.org/pdf/cs/0406021v3", 
    "arxiv-id": "cs/0406021v3"
},{
    "category": "cs.CE", 
    "author": "Alexandre d'Aspremont", 
    "title": "Static versus Dynamic Arbitrage Bounds on Multivariate Option Prices", 
    "publish": "2004-07-10T16:17:26Z", 
    "summary": "We compare static arbitrage price bounds on basket calls, i.e. bounds that\nonly involve buy-and-hold trading strategies, with the price range obtained\nwithin a multi-variate generalization of the Black-Scholes model. While there\nis no gap between these two sets of prices in the univariate case, we observe\nhere that contrary to our intuition about model risk for at-the-money calls,\nthere is a somewhat large gap between model prices and static arbitrage prices,\nhence a similarly large set of prices on which a multivariate Black-Scholes\nmodel cannot be calibrated but where no conclusion can be drawn on the presence\nor not of a static arbitrage opportunity.", 
    "link": "http://arxiv.org/pdf/cs/0407029v1", 
    "arxiv-id": "cs/0407029v1"
},{
    "category": "cs.CE", 
    "author": "J. Balic", 
    "title": "Intelligent Computer Numerical Control unit for machine tools", 
    "publish": "2004-10-25T15:55:43Z", 
    "summary": "The paper describes a new CNC control unit for machining centres with\nlearning ability and automatic intelligent generating of NC programs on the\nbases of a neural network, which is built-in into a CNC unit as special device.\nThe device performs intelligent and completely automatically the NC part\nprograms only on the bases of 2D, 2,5D or 3D computer model of prismatic part.\nIntervention of the operator is not needed. The neural network for milling,\ndrilling, reaming, threading and operations alike has learned to generate NC\nprograms in the learning module, which is a part of intelligent CAD/CAM system.", 
    "link": "http://arxiv.org/pdf/cs/0410064v1", 
    "arxiv-id": "cs/0410064v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The Features of the Complex CAD system of Reconstruction of the   Industrial Plants", 
    "publish": "2004-12-08T08:52:28Z", 
    "summary": "The features of designing of reconstruction of the acting plant by its design\ndepartment are considered: the results of work are drawings corresponding with\nthe national standards; large number of the small projects for different acting\nobjects; variety of the types of the drawings in one project; large paper\narchive. The models and methods of developing of the complex CAD system with\nfriend uniform environment of designing, with setting a profile of operations,\nwith usage of the general parts of the project, with a series of\nproblem-oriented subsystems are described on an example of a CAD system\nTechnoCAD GlassX", 
    "link": "http://arxiv.org/pdf/cs/0412031v1", 
    "arxiv-id": "cs/0412031v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The modelling of the build constructions in a CAD of the renovation of   the enterprises by means of units in the drawings", 
    "publish": "2004-12-08T09:01:20Z", 
    "summary": "The parametric model of build constructions and features of design operations\nare described for making drawings, which are the common component of the\ndifferent parts of the projects of renovation of enterprises. The key moment of\nthe deep design automation is the using of so-called units in the drawings,\nwhich are joining a visible graphic part and invisible parameters. The model\nhas passed check during designing of several hundreds of drawings", 
    "link": "http://arxiv.org/pdf/cs/0412033v1", 
    "arxiv-id": "cs/0412033v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The informatization of design works at industry firm during its   renovation", 
    "publish": "2004-12-08T11:08:48Z", 
    "summary": "The characteristic of design works on firm at its renovation and of the\ncommon directions of their informatization is given. The implantation of a CAD\nis selected as the key direction, and the requirements to a complex CAD-system\nare stated. The methods of such a CAD-system development are featured, and the\nconnectedness of this development with the process of integration of\ninformation space of design department of the firm is characterized. The\nexperience of development and implantation of a complex CAD of renovation of\nfirms TechnoCAD GlassX lies in a basis of this reviewing", 
    "link": "http://arxiv.org/pdf/cs/0412034v1", 
    "arxiv-id": "cs/0412034v1"
},{
    "category": "cs.CE", 
    "author": "O. Riabinina", 
    "title": "The Peculiarities of Nonstationary Formation of Inhomogeneous Structures   of Charged Particles in the Electrodiffusion Processes", 
    "publish": "2005-03-30T07:00:29Z", 
    "summary": "In this paper the distribution of charged particles is constructed under the\napproximation of ambipolar diffusion. The results of mathematical modelling in\ntwo-dimensional case taking into account the velocities of the system are\npresented.", 
    "link": "http://arxiv.org/pdf/cs/0503084v1", 
    "arxiv-id": "cs/0503084v1"
},{
    "category": "cs.CE", 
    "author": "Jan-Ove Palmberg", 
    "title": "Dynamic Simulation of Construction Machinery: Towards an Operator Model", 
    "publish": "2005-03-30T20:52:11Z", 
    "summary": "In dynamic simulation of complete wheel loaders, one interesting aspect,\nspecific for the working task, is the momentary power distribution between\ndrive train and hydraulics, which is balanced by the operator.\n  This paper presents the initial results to a simulation model of a human\noperator. Rather than letting the operator model follow a predefined path with\ncontrol inputs at given points, it follows a collection of general rules that\ntogether describe the machine's working cycle in a generic way. The advantage\nof this is that the working task description and the operator model itself are\nindependent of the machine's technical parameters. Complete sub-system\ncharacteristics can thus be changed without compromising the relevance and\nvalidity of the simulation. Ultimately, this can be used to assess a machine's\ntotal performance, fuel efficiency and operability already in the concept phase\nof the product development process.", 
    "link": "http://arxiv.org/pdf/cs/0503087v4", 
    "arxiv-id": "cs/0503087v4"
},{
    "category": "cs.CE", 
    "author": "Silvio R. Dahmen", 
    "title": "Modelling investment in artificial stock markets: Analytical and   Numerical Results", 
    "publish": "2005-04-29T20:38:42Z", 
    "summary": "In this article we study the behavior of a group of economic agents in the\ncontext of cooperative game theory, interacting according to rules based on the\nPotts Model with suitable modifications. Each agent can be thought of as\nbelonging to a chain, where agents can only interact with their nearest\nneighbors (periodic boundary conditions are imposed). Each agent can invest an\namount &#963;_{i}=0,...,q-1. Using the transfer matrix method we study\nanalytically, among other things, the behavior of the investment as a function\nof a control parameter (denoted &#946;) for the cases q=2 and 3. For q>3\nnumerical evaluation of eigenvalues and high precision numerical derivatives\nare used in order to assess this information.", 
    "link": "http://arxiv.org/pdf/cs/0505001v1", 
    "arxiv-id": "cs/0505001v1"
},{
    "category": "cs.CE", 
    "author": "Reno Filla", 
    "title": "An Event-driven Operator Model for Dynamic Simulation of Construction   Machinery", 
    "publish": "2005-06-10T10:35:14Z", 
    "summary": "Prediction and optimisation of a wheel loader's dynamic behaviour is a\nchallenge due to tightly coupled, non-linear subsystems of different technical\ndomains. Furthermore, a simulation regarding performance, efficiency, and\noperability cannot be limited to the machine itself, but has to include\noperator, environment, and work task. This paper presents some results of our\napproach to an event-driven simulation model of a human operator. Describing\nthe task and the operator model independently of the machine's technical\nparameters, gives the possibility to change whole sub-system characteristics\nwithout compromising the relevance and validity of the simulation.", 
    "link": "http://arxiv.org/pdf/cs/0506033v4", 
    "arxiv-id": "cs/0506033v4"
},{
    "category": "cs.CE", 
    "author": "Eric Moulines", 
    "title": "Comparison of Resampling Schemes for Particle Filtering", 
    "publish": "2005-07-08T15:14:51Z", 
    "summary": "This contribution is devoted to the comparison of various resampling\napproaches that have been proposed in the literature on particle filtering. It\nis first shown using simple arguments that the so-called residual and\nstratified methods do yield an improvement over the basic multinomial\nresampling approach. A simple counter-example showing that this property does\nnot hold true for systematic resampling is given. Finally, some results on the\nlarge-sample behavior of the simple bootstrap filter algorithm are given. In\nparticular, a central limit theorem is established for the case where\nresampling is performed using the residual approach.", 
    "link": "http://arxiv.org/pdf/cs/0507025v1", 
    "arxiv-id": "cs/0507025v1"
},{
    "category": "cs.CE", 
    "author": "A. S. Siver", 
    "title": "ReacProc: A Tool to Process Reactions Describing Particle Interactions", 
    "publish": "2005-07-21T14:17:47Z", 
    "summary": "ReacProc is a program written in C/C++ programming language which can be used\n(1) to check out of reactions describing particles interactions against\nconservation laws and (2) to reduce input reaction into some canonical form. A\ntable with particles properties is available within ReacProc package.", 
    "link": "http://arxiv.org/pdf/cs/0507055v2", 
    "arxiv-id": "cs/0507055v2"
},{
    "category": "cs.CE", 
    "author": "Hong Tat", 
    "title": "Investigations of Process Damping Forces in Metal Cutting", 
    "publish": "2005-08-23T19:52:38Z", 
    "summary": "Using finite element software developed for metal cutting by Third Wave\nSystems we investigate the forces involved in chatter, a self-sustained\noscillation of the cutting tool. The phenomena is decomposed into a vibrating\ntool cutting a flat surface work piece, and motionless tool cutting a work\npiece with a wavy surface. While cutting the wavy surface, the shearplane was\nseen to oscillate in advance of the oscillation of the depth of cut, as were\nthe cutting, thrust, and shear plane forces. The vibrating tool was used to\ninvestigate process damping through the interaction of the relief face of the\ntool and the workpiece. Crushing forces are isolated and compared to the\ncontact length between the tool and workpiece. We found that the wavelength\ndependence of the forces depended on the relative size of the wavelength to the\nlength of the relief face of the tool. The results indicate that the damping\nforce from crushing will be proportional to the cutting speed for short tools,\nand inversely proportional for long tools.", 
    "link": "http://arxiv.org/pdf/cs/0508102v1", 
    "arxiv-id": "cs/0508102v1"
},{
    "category": "cs.CE", 
    "author": "Bazil P\u00e2rv", 
    "title": "COMODI: Architecture for a Component-Based Scientific Computing System", 
    "publish": "2005-08-31T22:48:40Z", 
    "summary": "The COmputational MODule Integrator (COMODI) is an initiative aiming at a\ncomponent based framework, component developer tool and component repository\nfor scientific computing. We identify the main ingredients to a solution that\nwould be sufficiently appealing to scientists and engineers to consider\nalternatives to their deeply rooted programming traditions. The overall\nstructure of the complete solution is sketched with special emphasis on the\nComponent Developer Tool standing at the basis of COMODI.", 
    "link": "http://arxiv.org/pdf/cs/0509003v1", 
    "arxiv-id": "cs/0509003v1"
},{
    "category": "cs.CE", 
    "author": "T. Suslo", 
    "title": "Kriging Scenario For Capital Markets", 
    "publish": "2005-09-05T08:04:06Z", 
    "summary": "An introduction to numerical statistics.", 
    "link": "http://arxiv.org/pdf/cs/0509012v5", 
    "arxiv-id": "cs/0509012v5"
},{
    "category": "cs.CE", 
    "author": "Alexandre d'Aspremont", 
    "title": "A Market Test for the Positivity of Arrow-Debreu Prices", 
    "publish": "2005-10-11T13:40:17Z", 
    "summary": "We derive tractable necessary and sufficient conditions for the absence of\nbuy-and-hold arbitrage opportunities in a perfectly liquid, one period market.\nWe formulate the positivity of Arrow-Debreu prices as a generalized moment\nproblem to show that this no arbitrage condition is equivalent to the positive\nsemidefiniteness of matrices formed by the market price of tradeable securities\nand their products. We apply this result to a market with multiple assets and\nbasket call options.", 
    "link": "http://arxiv.org/pdf/cs/0510027v2", 
    "arxiv-id": "cs/0510027v2"
},{
    "category": "cs.CE", 
    "author": "Croissant Olivier", 
    "title": "Heat kernel expansion for a family of stochastic volatility models :   delta-geometry", 
    "publish": "2005-11-04T18:31:49Z", 
    "summary": "In this paper, we study a family of stochastic volatility processes; this\nfamily features a mean reversion term for the volatility and a double CEV-like\nexponent that generalizes SABR and Heston's models. We derive approximated\nclosed form formulas for the digital prices, the local and implied\nvolatilities. Our formulas are efficient for small maturities.\n  Our method is based on differential geometry, especially small time\ndiffusions on riemanian spaces. This geometrical point of view can be extended\nto other processes, and is very accurate to produce variate smiles for small\nmaturities and small moneyness.", 
    "link": "http://arxiv.org/pdf/cs/0511024v1", 
    "arxiv-id": "cs/0511024v1"
},{
    "category": "cs.CE", 
    "author": "Daniele C. Struppa", 
    "title": "Simplicial models of social aggregation I", 
    "publish": "2006-04-23T19:28:07Z", 
    "summary": "This paper presents the foundational ideas for a new way of modeling social\naggregation. Traditional approaches have been using network theory, and the\ntheory of random networks. Under that paradigm, every social agent is\nrepresented by a node, and every social interaction is represented by a segment\nconnecting two nodes. Early work in family interactions, as well as more recent\nwork in the study of terrorist organizations, shows that network modeling may\nbe insufficient to describe the complexity of human social structures.\nSpecifically, network theory does not seem to have enough flexibility to\nrepresent higher order aggregations, where several agents interact as a group,\nrather than as a collection of pairs. The model we present here uses a well\nestablished mathematical theory, the theory of simplicial complexes, to address\nthis complex issue prevalent in interpersonal and intergroup communication. The\ntheory enables us to provide a richer graphical representation of social\ninteractions, and to determine quantitative mechanisms to describe the\nrobustness of a social structure. We also propose a methodology to create\nrandom simplicial complexes, with the purpose of providing a new method to\nsimulate computationally the creation and disgregation of social structures.\nFinally, we propose several measures which could be taken and observed in order\nto describe and study an actual social aggregation occurring in interpersonal\nand intergroup contexts.", 
    "link": "http://arxiv.org/pdf/cs/0604090v1", 
    "arxiv-id": "cs/0604090v1"
},{
    "category": "cs.CE", 
    "author": "Edward Hanna", 
    "title": "Feynman Checkerboard as a Model of Discrete Space-Time", 
    "publish": "2006-07-06T16:35:38Z", 
    "summary": "In 1965, Feynman wrote of using a lattice containing one dimension of space\nand one dimension of time to derive aspects of quantum mechanics. Instead of\nsumming the behavior of all possible paths as he did, this paper will consider\nthe motion of single particles within this discrete Space-Time lattice,\nsometimes called Feynman's Checkerboard. This empirical approach yielded\nseveral predicted emergent properties for a discrete Space-Time lattice, one of\nwhich is novel and testable.", 
    "link": "http://arxiv.org/pdf/cs/0607018v2", 
    "arxiv-id": "cs/0607018v2"
},{
    "category": "cs.CE", 
    "author": "Anton Stoilov", 
    "title": "Mathematical Modelling of the Thermal Accumulation in Hot Water Solar   Systems", 
    "publish": "2006-07-18T08:07:48Z", 
    "summary": "Mathematical modelling and defining useful recommendations for construction\nand regimes of exploitation for hot water solar installation with thermal\nstratification is the main purpose of this work. A special experimental solar\nmodule for hot water was build and equipped with sufficient measure apparatus.\nThe main concept of investigation is to optimise the stratified regime of\nthermal accumulation and constructive parameters of heat exchange equipment\n(heat serpentine in tank). Accumulation and heat exchange processes were\ninvestigated by theoretical end experimental means. Special mathematical model\nwas composed to simulate the energy transfer in stratified tank. Computer\nprogram was developed to solve mathematical equations for thermal accumulation\nand energy exchange. Extensive numerical and experimental tests were carried\nout. A good correspondence between theoretical and experimental data was\narrived. Keywords: Mathematical modelling, accumulation", 
    "link": "http://arxiv.org/pdf/cs/0607083v1", 
    "arxiv-id": "cs/0607083v1"
},{
    "category": "cs.CE", 
    "author": "Anton Stoilov", 
    "title": "Finite element method for thermal analysis of concentrating solar   receivers", 
    "publish": "2006-07-19T06:58:37Z", 
    "summary": "Application of finite element method and heat conductivity transfer model for\ncalculation of temperature distribution in receiver for dish-Stirling\nconcentrating solar system is described. The method yields discretized\nequations that are entirely local to the elements and provides complete\ngeometric flexibility. A computer program solving the finite element method\nproblem is created and great number of numerical experiments is carried out.\nIllustrative numerical results are given for an array of triangular elements in\nreceiver for dish-Stirling system.", 
    "link": "http://arxiv.org/pdf/cs/0607091v1", 
    "arxiv-id": "cs/0607091v1"
},{
    "category": "cs.CE", 
    "author": "Leszek Skoczylas", 
    "title": "A comparative analysis of the geometrical surface texture of a real and   virtual model of a tooth flank of a cylindrical gear", 
    "publish": "2006-09-15T14:05:43Z", 
    "summary": "The paper presents the methodology of modelling tooth flanks of cylindrical\ngears in the Cad environment. The modelling consists in a computer simulation\nof gear generation. A model of tooth flanks is an envelope curve of a family of\nenvelopes that originate from the rolling motion of a solid tool model in\nrelation to a solid model of the cylindrical gear. The surface stereometry and\ntopography of the tooth flanks, hobbed and chiselled by Fellows method, are\ncompared to their numerical models. Metrological measurements of the real gears\nwere carried out using a coordinated measuring machine and a two - and a\nthree-dimensional profilometer. A computer simulation of the gear generation\nwas performed in the Mechanical Desktop environment.", 
    "link": "http://arxiv.org/pdf/cs/0609087v1", 
    "arxiv-id": "cs/0609087v1"
},{
    "category": "cs.CE", 
    "author": "Laurent El Ghaoui", 
    "title": "A Semidefinite Relaxation for Air Traffic Flow Scheduling", 
    "publish": "2006-09-26T15:34:40Z", 
    "summary": "We first formulate the problem of optimally scheduling air traffic low with\nsector capacity constraints as a mixed integer linear program. We then use\nsemidefinite relaxation techniques to form a convex relaxation of that problem.\nFinally, we present a randomization algorithm to further improve the quality of\nthe solution. Because of the specific structure of the air traffic flow\nproblem, the relaxation has a single semidefinite constraint of size dn where d\nis the maximum delay and n the number of flights.", 
    "link": "http://arxiv.org/pdf/cs/0609145v1", 
    "arxiv-id": "cs/0609145v1"
},{
    "category": "cs.CE", 
    "author": "Rustem R. Kafiyatullov", 
    "title": "Protection of the information in a complex CAD system of renovation of   industrial firms", 
    "publish": "2006-11-10T07:43:33Z", 
    "summary": "The threats to security of the information originating owing to involuntary\noperations of the users of a CAD, and methods of its protection implemented in\na complex CAD system of renovation of firms are considered: rollback, autosave,\nautomatic backup copying and electronic subscript. The specificity of a complex\nCAD is reflected in necessity of rollback and autosave both of the draw and the\nparametric representations of its parts, which are the information models of\nthe problem-oriented extensions of the CAD", 
    "link": "http://arxiv.org/pdf/cs/0611044v1", 
    "arxiv-id": "cs/0611044v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The evolution of the parametric models of drawings (modules) in the   enterprises reconstruction CAD system", 
    "publish": "2006-11-10T07:56:19Z", 
    "summary": "Progressing methods of drawings creating automation is discussed on the basis\nof so-called modules containing parametric representation of a part of the\ndrawing and the geometrical elements. The stages of evolution of modular\ntechnology of automation of engineering are describing alternatives of applying\nof moduluss for simple association of elements of the drawing without\nparametric representation with an opportunity of its commenting, for graphic\nsymbols creating in the schemas of automation and drawings of pipelines, for\nstorage of the specific properties of elements, for development of the\nspecialized parts of the project: the axonometric schemas, profiles of outboard\npipe networks etc.", 
    "link": "http://arxiv.org/pdf/cs/0611045v1", 
    "arxiv-id": "cs/0611045v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "Environment of development of the programs of parametric creating of the   drawings in CAD-system of renovation of the enterprises", 
    "publish": "2006-11-17T08:27:45Z", 
    "summary": "The main ideas, data structures, structure and realization of operations with\nthem in environment of development of the programs of parametric creating of\nthe drawings are considered for the needs of the automated design engineering\nsystem of renovation of the enterprises. The example of such program and\nexample of application of this environment for creating the drawing of the base\nfor equipment in CAD-system TechnoCAD GlassX are presented", 
    "link": "http://arxiv.org/pdf/cs/0611083v1", 
    "arxiv-id": "cs/0611083v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The specifications making in complex CAD-system of renovation of the   enterprises on the basis of modules in the drawing and electronic catalogues", 
    "publish": "2006-11-27T04:31:09Z", 
    "summary": "The experience of automation of the specifications making of the projects of\nrenovation of the industrial enterprises is described, being based on the\nspecial modules in the drawing containing the visible image and additional\nparameters, and electronic catalogues", 
    "link": "http://arxiv.org/pdf/cs/0611132v1", 
    "arxiv-id": "cs/0611132v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The modelling of the automation schemes of technological processes in   CAD-system of renovation of the enterprises", 
    "publish": "2006-11-27T04:39:09Z", 
    "summary": "According to the requirements of the Russian standards, the automation\nschemes are necessary practically in each project of renovation of industrial\nbuildings and facilities, in which any technological processes are realized.\nThe model representations of the automation schemes in CAD-system TechnoCAD\nGlassX are described. The models follow a principle \"to exclude a repeated\ninput operations\"", 
    "link": "http://arxiv.org/pdf/cs/0611133v1", 
    "arxiv-id": "cs/0611133v1"
},{
    "category": "cs.CE", 
    "author": "Nikolay P. Ivankov", 
    "title": "The framework for simulation of dynamics of mechanical aggregates", 
    "publish": "2007-01-19T12:18:14Z", 
    "summary": "A framework for simulation of dynamics of mechanical aggregates has been\ndeveloped. This framework enables us to build model of aggregate from models of\nits parts. Framework is a part of universal framework for science and\nengineering.", 
    "link": "http://arxiv.org/pdf/cs/0701119v1", 
    "arxiv-id": "cs/0701119v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir Surkov", 
    "title": "Option Valuation using Fourier Space Time Stepping", 
    "publish": "2007-03-14T19:48:42Z", 
    "summary": "It is well known that the Black-Scholes-Merton model suffers from several\ndeficiencies. Jump-diffusion and Levy models have been widely used to partially\nalleviate some of the biases inherent in this classical model. Unfortunately,\nthe resulting pricing problem requires solving a more difficult partial-integro\ndifferential equation (PIDE) and although several approaches for solving the\nPIDE have been suggested in the literature, none are entirely satisfactory. All\ntreat the integral and diffusive terms asymmetrically and are difficult to\nextend to higher dimensions. We present a new, efficient algorithm, based on\ntransform methods, which symmetrically treats the diffusive and integrals\nterms, is applicable to a wide class of path-dependent options (such as\nBermudan, barrier, and shout options) and options on multiple assets, and\nnaturally extends to regime-switching Levy models. We present a concise study\nof the precision and convergence properties of our algorithm for several\nclasses of options and Levy models and demonstrate that the algorithm is\nsecond-order in space and first-order in time for path-dependent options.", 
    "link": "http://arxiv.org/pdf/cs/0703068v2", 
    "arxiv-id": "cs/0703068v2"
},{
    "category": "cs.CE", 
    "author": "Myung-Sin Song", 
    "title": "Comparison of Discrete and Continuous Wavelet Transforms", 
    "publish": "2007-05-01T18:24:52Z", 
    "summary": "In this paper we outline several points of view on the interplay between\ndiscrete and continuous wavelet transforms; stressing both pure and applied\naspects of both. We outline some new links between the two transform\ntechnologies based on the theory of representations of generators and\nrelations. By this we mean a finite system of generators which are represented\nby operators in Hilbert space. We further outline how these representations\nyield sub-band filter banks for signal and image processing algorithms.", 
    "link": "http://arxiv.org/pdf/0705.0150v2", 
    "arxiv-id": "0705.0150v2"
},{
    "category": "cs.CE", 
    "author": "P. S. Heyns", 
    "title": "Machine and Component Residual Life Estimation through the Application   of Neural Networks", 
    "publish": "2007-05-10T05:52:22Z", 
    "summary": "This paper concerns the use of neural networks for predicting the residual\nlife of machines and components. In addition, the advantage of using\ncondition-monitoring data to enhance the predictive capability of these neural\nnetworks was also investigated. A number of neural network variations were\ntrained and tested with the data of two different reliability-related datasets.\nThe first dataset represents the renewal case where the failed unit is repaired\nand restored to a good-as-new condition. Data was collected in the laboratory\nby subjecting a series of similar test pieces to fatigue loading with a\nhydraulic actuator. The average prediction error of the various neural networks\nbeing compared varied from 431 to 841 seconds on this dataset, where test\npieces had a characteristic life of 8,971 seconds. The second dataset was\ncollected from a group of pumps used to circulate a water and magnetite\nsolution within a plant. The data therefore originated from a repaired system\naffected by reliability degradation. When optimized, the multi-layer perceptron\nneural networks trained with the Levenberg-Marquardt algorithm and the general\nregression neural network produced a sum-of-squares error within 11.1% of each\nother. The potential for using neural networks for residual life prediction and\nthe advantage of incorporating condition-based data into the model were proven\nfor both examples.", 
    "link": "http://arxiv.org/pdf/0705.1390v1", 
    "arxiv-id": "0705.1390v1"
},{
    "category": "cs.CE", 
    "author": "P. S. Heyns", 
    "title": "Principal Component Analysis and Automatic Relevance Determination in   Damage Identification", 
    "publish": "2007-05-11T15:35:22Z", 
    "summary": "This paper compares two neural network input selection schemes, the Principal\nComponent Analysis (PCA) and the Automatic Relevance Determination (ARD) based\non Mac-Kay's evidence framework. The PCA takes all the input data and projects\nit onto a lower dimension space, thereby reduc-ing the dimension of the input\nspace. This input reduction method often results with parameters that have\nsignificant influence on the dynamics of the data being diluted by those that\ndo not influence the dynamics of the data. The ARD selects the most relevant\ninput parameters and discards those that do not contribute significantly to the\ndynamics of the data being modelled. The ARD sometimes results with important\ninput parameters being discarded thereby compromising the dynamics of the data.\nThe PCA and ARD methods are implemented together with a Multi-Layer-Perceptron\n(MLP) network for fault identification in structures and the performance of the\ntwo methods is as-sessed. It is observed that ARD and PCA give similar\naccu-racy levels when used as input-selection schemes. There-fore, the choice\nof input-selection scheme is dependent on the nature of the data being\nprocessed.", 
    "link": "http://arxiv.org/pdf/0705.1672v1", 
    "arxiv-id": "0705.1672v1"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "Finite Element Model Updating Using Response Surface Method", 
    "publish": "2007-05-12T10:25:22Z", 
    "summary": "This paper proposes the response surface method for finite element model\nupdating. The response surface method is implemented by approximating the\nfinite element model surface response equation by a multi-layer perceptron. The\nupdated parameters of the finite element model were calculated using genetic\nalgorithm by optimizing the surface response equation. The proposed method was\ncompared to the existing methods that use simulated annealing or genetic\nalgorithm together with a full finite element model for finite element model\nupdating. The proposed method was tested on an unsymmetri-cal H-shaped\nstructure. It was observed that the proposed method gave the updated natural\nfrequen-cies and mode shapes that were of the same order of accuracy as those\ngiven by simulated annealing and genetic algorithm. Furthermore, it was\nobserved that the response surface method achieved these results at a\ncomputational speed that was more than 2.5 times as fast as the genetic\nalgorithm and a full finite element model and 24 times faster than the\nsimulated annealing.", 
    "link": "http://arxiv.org/pdf/0705.1759v1", 
    "arxiv-id": "0705.1759v1"
},{
    "category": "cs.CE", 
    "author": "Christina Busisiwe Vilakazi", 
    "title": "Computational Intelligence for Condition Monitoring", 
    "publish": "2007-05-17T21:20:58Z", 
    "summary": "Condition monitoring techniques are described in this chapter. Two aspects of\ncondition monitoring process are considered: (1) feature extraction; and (2)\ncondition classification. Feature extraction methods described and implemented\nare fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification\nmethods described and implemented are support vector machines (SVM), hidden\nMarkov models (HMM), Gaussian mixture models (GMM) and extension neural\nnetworks (ENN). The effectiveness of these features were tested using SVM, HMM,\nGMM and ENN on condition monitoring of bearings and are found to give good\nresults.", 
    "link": "http://arxiv.org/pdf/0705.2604v1", 
    "arxiv-id": "0705.2604v1"
},{
    "category": "cs.CE", 
    "author": "Stefan Z. Stefanov", 
    "title": "Cointegration of the Daily Electric Power System Load and the Weather", 
    "publish": "2007-06-08T07:04:24Z", 
    "summary": "The paper makes a thermal predictive analysis of the electric power system\nsecurity for a day ahead. This predictive analysis is set as a thermal\ncomputation of the expected security. This computation is obtained by\ncointegrating the daily electric power systen load and the weather, by finding\nthe daily electric power system thermodynamics and by introducing tests for\nthis thermodynamics. The predictive analysis made shows the electricity\nconsumers' wisdom.", 
    "link": "http://arxiv.org/pdf/0706.1119v2", 
    "arxiv-id": "0706.1119v2"
},{
    "category": "cs.CE", 
    "author": "Hao Xing", 
    "title": "Pricing American Options for Jump Diffusions by Iterating Optimal   Stopping Problems for Diffusions", 
    "publish": "2007-06-15T16:43:14Z", 
    "summary": "We approximate the price of the American put for jump diffusions by a\nsequence of functions, which are computed iteratively. This sequence converges\nto the price function uniformly and exponentially fast. Each element of the\napproximating sequence solves an optimal stopping problem for geometric\nBrownian motion, and can be numerically computed using the classical finite\ndifference methods. We prove the convergence of this numerical scheme and\npresent examples to illustrate its performance.", 
    "link": "http://arxiv.org/pdf/0706.2331v5", 
    "arxiv-id": "0706.2331v5"
},{
    "category": "cs.CE", 
    "author": "O. Bunyak", 
    "title": "Location and Spectral Estimation of Weak Wave Packets on Noise   Background", 
    "publish": "2007-07-02T09:47:33Z", 
    "summary": "The method of location and spectral estimation of weak signals on a noise\nbackground is being considered. The method is based on the optimized on order\nand noise dispersion autoregressive model of a sought signal. A new approach of\nmodel order determination is being offered. Available estimation of the noise\ndispersion is close to the real one. The optimized model allows to define\nfunction of empirical data spectral and dynamic features changes. The analysis\nof the signal as dynamic invariant in respect of the linear shift\ntransformation yields the function of model consistency. Use of these both\nfunctions enables to detect short-time and nonstationary wave packets at signal\nto noise ratio as from -20 dB and above.", 
    "link": "http://arxiv.org/pdf/0707.0181v1", 
    "arxiv-id": "0707.0181v1"
},{
    "category": "cs.CE", 
    "author": "Erhan Bayraktar", 
    "title": "Pricing Options on Defaultable Stocks", 
    "publish": "2007-07-03T03:28:35Z", 
    "summary": "In this note, we develop stock option price approximations for a model which\ntakes both the risk o default and the stochastic volatility into account. We\nalso let the intensity of defaults be influenced by the volatility. We show\nthat it might be possible to infer the risk neutral default intensity from the\nstock option prices. Our option price approximation has a rich implied\nvolatility surface structure and fits the data implied volatility well. Our\ncalibration exercise shows that an effective hazard rate from bonds issued by a\ncompany can be used to explain the implied volatility skew of the implied\nvolatility of the option prices issued by the same company.", 
    "link": "http://arxiv.org/pdf/0707.0336v2", 
    "arxiv-id": "0707.0336v2"
},{
    "category": "cs.CE", 
    "author": "Hao Xing", 
    "title": "Pricing Asian Options for Jump Diffusions", 
    "publish": "2007-07-17T04:55:18Z", 
    "summary": "We construct a sequence of functions that uniformly converge (on compact\nsets) to the price of Asian option, which is written on a stock whose dynamics\nfollows a jump diffusion, exponentially fast. Each of the element in this\nsequence solves a parabolic partial differen- tial equation (not an\nintegro-differential equation). As a result we obtain a fast numerical\napproximation scheme whose accuracy versus speed characteristics can be\ncontrolled. We analyze the performance of our numerical algorithm on several\nexamples.", 
    "link": "http://arxiv.org/pdf/0707.2432v7", 
    "arxiv-id": "0707.2432v7"
},{
    "category": "cs.CE", 
    "author": "Alexandre d'Aspremont", 
    "title": "Identifying Small Mean Reverting Portfolios", 
    "publish": "2007-08-22T16:25:17Z", 
    "summary": "Given multivariate time series, we study the problem of forming portfolios\nwith maximum mean reversion while constraining the number of assets in these\nportfolios. We show that it can be formulated as a sparse canonical correlation\nanalysis and study various algorithms to solve the corresponding sparse\ngeneralized eigenvalue problems. After discussing penalized parameter\nestimation procedures, we study the sparsity versus predictability tradeoff and\nthe impact of predictability in various markets.", 
    "link": "http://arxiv.org/pdf/0708.3048v2", 
    "arxiv-id": "0708.3048v2"
},{
    "category": "cs.CE", 
    "author": "Jose Llamozas", 
    "title": "An Early Warning System for Bankruptcy Prediction: lessons from the   Venezuelan Bank Crisis", 
    "publish": "2007-08-26T05:33:41Z", 
    "summary": "During 1993-94 Venezuela experienced a severe banking crisis which ended up\nwith 18 commercial banks intervened by the government. Here we develop an early\nwarning system for detecting credit related bankruptcy through discriminant\nfunctions developed on financial and macroeconomic data predating the crisis. A\nrobustness test performed on these functions shows high precision in error\nestimation. The model calibrated on pre-crisis data could detect abnormal\nfinancial tension in the late Banco Capital many months before it was\nintervened and liquidated.", 
    "link": "http://arxiv.org/pdf/0708.3465v1", 
    "arxiv-id": "0708.3465v1"
},{
    "category": "cs.CE", 
    "author": "S. Moreno", 
    "title": "Risk Minimization and Optimal Derivative Design in a Principal Agent   Game", 
    "publish": "2007-10-29T20:00:15Z", 
    "summary": "We consider the problem of Adverse Selection and optimal derivative design\nwithin a Principal-Agent framework. The principal's income is exposed to\nnon-hedgeable risk factors arising, for instance, from weather or climate\nphenomena. She evaluates her risk using a coherent and law invariant risk\nmeasure and tries minimize her exposure by selling derivative securities on her\nincome to individual agents. The agents have mean-variance preferences with\nheterogeneous risk aversion coefficients. An agent's degree of risk aversion is\nprivate information and hidden to the principal who only knows the overall\ndistribution. We show that the principal's risk minimization problem has a\nsolution and illustrate the effects of risk transfer on her income by means of\ntwo specific examples. Our model extends earlier work of Barrieu and El Karoui\n(2005) and Carlier, Ekeland and Touzi (2007).", 
    "link": "http://arxiv.org/pdf/0710.5512v1", 
    "arxiv-id": "0710.5512v1"
},{
    "category": "cs.CE", 
    "author": "Fran\u00e7ois Villeneuve", 
    "title": "A numerical approach for 3D manufacturing tolerances synthesis", 
    "publish": "2007-11-14T06:21:17Z", 
    "summary": "Making a product conform to the functional requirements indicated by the\ncustomer suppose to be able to manage the manufacturing process chosen to\nrealise the parts. A simulation step is generally performed to verify that the\nexpected generated deviations fit with these requirements. It is then necessary\nto assess the actual deviations of the process in progress. This is usually\ndone by the verification of the conformity of the workpiece to manufacturing\ntolerances at the end of each set-up. It is thus necessary to determine these\nmanufacturing tolerances. This step is called \"manufacturing tolerance\nsynthesis\". In this paper, a numerical method is proposed to perform 3D\nmanufacturing tolerances synthesis. This method uses the result of the\nnumerical analysis of tolerances to determine influent mall displacement of\nsurfaces. These displacements are described by small displacements torsors. An\nalgorithm is then proposed to determine suitable ISO manufacturing tolerances.", 
    "link": "http://arxiv.org/pdf/0711.2116v1", 
    "arxiv-id": "0711.2116v1"
},{
    "category": "cs.CE", 
    "author": "Bo Yang", 
    "title": "A Unified Framework for Pricing Credit and Equity Derivatives", 
    "publish": "2007-12-21T02:53:38Z", 
    "summary": "We propose a model which can be jointly calibrated to the corporate bond term\nstructure and equity option volatility surface of the same company. Our purpose\nis to obtain explicit bond and equity option pricing formulas that can be\ncalibrated to find a risk neutral model that matches a set of observed market\nprices. This risk neutral model can then be used to price more exotic, illiquid\nor over-the-counter derivatives. We observe that the model implied credit\ndefault swap (CDS) spread matches the market CDS spread and that our model\nproduces a very desirable CDS spread term structure. This is observation is\nworth noticing since without calibrating any parameter to the CDS spread data,\nit is matched by the CDS spread that our model generates using the available\ninformation from the equity options and corporate bond markets. We also observe\nthat our model matches the equity option implied volatility surface well since\nwe properly account for the default risk premium in the implied volatility\nsurface. We demonstrate the importance of accounting for the default risk and\nstochastic interest rate in equity option pricing by comparing our results to\nFouque, Papanicolaou, Sircar and Solna (2003), which only accounts for\nstochastic volatility.", 
    "link": "http://arxiv.org/pdf/0712.3617v2", 
    "arxiv-id": "0712.3617v2"
},{
    "category": "cs.CE", 
    "author": "Louis Jezequel", 
    "title": "M\u00e9thode de calcul du rayonnement acoustique de structures complexes", 
    "publish": "2008-04-08T06:24:49Z", 
    "summary": "In the automotive industry, predicting noise during design cycle is a\nnecessary step. Well-known methods exist to answer this issue in low frequency\ndomain. Among these, Finite Element Methods, adapted to closed domains, are\nquite easy to implement whereas Boundary Element Methods are more adapted to\ninfinite domains, but may induce singularity problems. In this article, the\ndescribed method, the SDM, allows to use both methods in their best application\ndomain. A new method is also presented to solve the SDM exterior problem.\nInstead of using Boundary Element Methods, an original use of Finite Elements\nis made. Efficiency of this new version of the Substructure Deletion Method is\ndiscussed.", 
    "link": "http://arxiv.org/pdf/0804.1187v1", 
    "arxiv-id": "0804.1187v1"
},{
    "category": "cs.CE", 
    "author": "Andreas J. Grau", 
    "title": "Accelerated Option Pricing in Multiple Scenarios", 
    "publish": "2008-07-31T17:40:55Z", 
    "summary": "This paper covers a massive acceleration of Monte-Carlo based pricing method\nfor financial products and financial derivatives. The method is applicable in\nrisk management settings, where a financial product has to be priced under a\nnumber of potential future scenarios. Instead of starting a separate nested\nMonte Carlo simulation for each scenario under consideration, the new method\ncovers the utilization of very few representative nested simulations and\nestimating the product prices at each scenario by a smoothing method based on\nthe state-space. This smoothing technique can be e.g. non-parametric regression\nor kernel smoothing.", 
    "link": "http://arxiv.org/pdf/0807.5120v2", 
    "arxiv-id": "0807.5120v2"
},{
    "category": "cs.CE", 
    "author": "Armen Bagdasaryan", 
    "title": "System Theoretic Viewpoint on Modeling of Complex Systems: Design,   Synthesis, Simulation, and Control", 
    "publish": "2008-12-24T12:07:48Z", 
    "summary": "We consider the basic features of complex dynamic and control systems,\nincluding systems having hierarchical structure. Special attention is paid to\nthe problems of design and synthesis of complex systems and control models, and\nto the development of simulation techniques and systems. A model of complex\nsystem is proposed and briefly analyzed.", 
    "link": "http://arxiv.org/pdf/0812.4523v1", 
    "arxiv-id": "0812.4523v1"
},{
    "category": "cs.CE", 
    "author": "Sourabh Saha", 
    "title": "Genetic algorithm based optimization and post optimality analysis of   multi-pass face milling", 
    "publish": "2009-02-04T18:29:21Z", 
    "summary": "This paper presents an optimization technique for the multi-pass face milling\nprocess. Genetic algorithm (GA) is used to obtain the optimum cutting\nparameters by minimizing the unit production cost for a given amount of\nmaterial removal. Cutting speed, feed and depth of cut for the finish and rough\npasses are the cutting parameters. An equal depth of cut for roughing passes\nhas been considered. A lookup table containing the feasible combinations of\ndepth of cut in finish and rough passes is generated so as to reduce the number\nof variables by one. The resulting mixed integer nonlinear optimization problem\nis solved in a single step using GA. The entire technique is demonstrated in a\ncase study. Post optimality analysis of the example problem is done to develop\na strategy for optimizing without running GA again for different values of\ntotal depth of cut.", 
    "link": "http://arxiv.org/pdf/0902.0763v1", 
    "arxiv-id": "0902.0763v1"
},{
    "category": "cs.CE", 
    "author": "Armen Bagdasaryan", 
    "title": "System approach to synthesis, modeling and control of complex dynamical   systems", 
    "publish": "2009-02-20T10:28:14Z", 
    "summary": "We consider the basic features of complex dynamical and control systems.\nSpecial attention is paid to the problems of synthesis of dynamical models of\ncomplex systems, construction of efficient control models, and to the\ndevelopment of simulation techniques. We propose an approach to the synthesis\nof dynamic models of complex systems that integrates expert knowledge with the\nprocess of modeling. A set-theoretic model of complex system is defined and\nbriefly analyzed. A mathematical model of complex dynamical system with\ncontrol, based on aggregate description, is also proposed. The structure of the\nmodel is described, and architecture of computer simulation system is\npresented, requirements to and components of computer simulation systems are\nanalyzed.", 
    "link": "http://arxiv.org/pdf/0902.3541v1", 
    "arxiv-id": "0902.3541v1"
},{
    "category": "cs.CE", 
    "author": "Alain G\u00e9rard", 
    "title": "New method to characterize a machining system: application in turning", 
    "publish": "2009-08-18T14:29:42Z", 
    "summary": "Many studies simulates the machining process by using a single degree of\nfreedom spring-mass sytem to model the tool stiffness, or the workpiece\nstiffness, or the unit tool-workpiece stiffness in modelings 2D. Others impose\nthe tool action, or use more or less complex modelings of the efforts applied\nby the tool taking account the tool geometry. Thus, all these models remain\ntwo-dimensional or sometimes partially three-dimensional. This paper aims at\ndeveloping an experimental method allowing to determine accurately the real\nthree-dimensional behaviour of a machining system (machine tool, cutting tool,\ntool-holder and associated system of force metrology six-component\ndynamometer). In the work-space model of machining, a new experimental\nprocedure is implemented to determine the machining system elastic behaviour.\nAn experimental study of machining system is presented. We propose a machining\nsystem static characterization. A decomposition in two distinct blocks of the\nsystem \"Workpiece-Tool-Machine\" is realized. The block Tool and the block\nWorkpiece are studied and characterized separately by matrix stiffness and\ndisplacement (three translations and three rotations). The Castigliano's theory\nallows us to calculate the total stiffness matrix and the total displacement\nmatrix. A stiffness center point and a plan of tool tip static displacement are\npresented in agreement with the turning machining dynamic model and especially\nduring the self induced vibration. These results are necessary to have a good\nthree-dimensional machining system dynamic characterization.", 
    "link": "http://arxiv.org/pdf/0908.2578v1", 
    "arxiv-id": "0908.2578v1"
},{
    "category": "cs.CE", 
    "author": "Shinichi Watanabe", 
    "title": "Effects of Mechanical Coupling on the Dynamics of Balancing Tasks", 
    "publish": "2009-09-03T09:38:18Z", 
    "summary": "Coupled human balancing tasks are investigated based on both pseudo-neural\ncontrollers characterized by time-delayed feedback with random gain and natural\nhuman balancing tasks. It is shown numerically that, compared to single\nbalancing tasks, balancing tasks coupled by mechanical structures exhibit\nenhanced stability against balancing errors in terms of both amplitude and\nvelocity and also improve the tracking ability of the controllers. We then\nperform an experiment in which numerical pseudo-neural controllers are replaced\nwith natural human balancing tasks carried out using computer mice. The results\nreveal that the coupling structure generates asymmetric tracking abilities in\nsubjects whose tracking abilities are nearly symmetric in their single\nbalancing tasks.", 
    "link": "http://arxiv.org/pdf/0909.0611v2", 
    "arxiv-id": "0909.0611v2"
},{
    "category": "cs.CE", 
    "author": "Hamed . O. Ghaffari", 
    "title": "Two-Phase Flow in Heterogeneous Media", 
    "publish": "2009-09-12T13:51:59Z", 
    "summary": "In this study, we investigate the appeared complexity of two-phase flow\n(air-water) in a heterogeneous soil where the supposed porous media is\nnon-deformable media which is under the time-dependent gas pressure. After\nobtaining of governing equations and considering the capillary\npressure-saturation and permeability functions, the evolution of the models\nunknown parameters were obtained. In this way, using COMSOL (FEMLAB) and fluid\nflow-script Module, the role of heterogeneity in intrinsic permeability was\nanalysed. Also, the evolution of relative permeability of wetting and\nnon-wetting fluid, capillary pressure and other parameters were elicited.", 
    "link": "http://arxiv.org/pdf/0909.2336v2", 
    "arxiv-id": "0909.2336v2"
},{
    "category": "cs.CE", 
    "author": "T. Zorikov", 
    "title": "A computational model of the bottlenose dolphin sonar:   Feature-extracting method", 
    "publish": "2009-11-16T19:24:59Z", 
    "summary": "The data describing a process of echo-image formation in bottlenose dolphin\nsonar perception were accumulated in our experimental explorations. These data\nwere formalized mathematically and used in the computational model, comparative\ntesting of which in echo-discrimination tasks revealed no less capabilities\nthen those of bottlenose dolphins.", 
    "link": "http://arxiv.org/pdf/0911.3125v2", 
    "arxiv-id": "0911.3125v2"
},{
    "category": "cs.CE", 
    "author": "Sabu M. Thampi", 
    "title": "Introduction to Bioinformatics", 
    "publish": "2009-11-22T04:07:08Z", 
    "summary": "Bioinformatics is a new discipline that addresses the need to manage and\ninterpret the data that in the past decade was massively generated by genomic\nresearch. This discipline represents the convergence of genomics, biotechnology\nand information technology, and encompasses analysis and interpretation of\ndata, modeling of biological phenomena, and development of algorithms and\nstatistics. This article presents an introduction to bioinformatics", 
    "link": "http://arxiv.org/pdf/0911.4230v1", 
    "arxiv-id": "0911.4230v1"
},{
    "category": "cs.CE", 
    "author": "Marian Kogler", 
    "title": "Drip and Mate Operations Acting in Test Tube Systems and Tissue-like P   systems", 
    "publish": "2009-11-26T00:33:04Z", 
    "summary": "The operations drip and mate considered in (mem)brane computing resemble the\noperations cut and recombination well known from DNA computing. We here\nconsider sets of vesicles with multisets of objects on their outside membrane\ninteracting by drip and mate in two different setups: in test tube systems, the\nvesicles may pass from one tube to another one provided they fulfill specific\nconstraints; in tissue-like P systems, the vesicles are immediately passed to\nspecified cells after having undergone a drip or mate operation. In both\nvariants, computational completeness can be obtained, yet with different\nconstraints for the drip and mate operations.", 
    "link": "http://arxiv.org/pdf/0911.4987v1", 
    "arxiv-id": "0911.4987v1"
},{
    "category": "cs.CE", 
    "author": "V. Grosu", 
    "title": "Expert System Models in the Companies' Financial and Accounting Domain", 
    "publish": "2010-01-20T08:03:53Z", 
    "summary": "The present paper is based on studying, analyzing and implementing the expert\nsystems in the financial and accounting domain of the companies, describing the\nuse method of the informational systems that can be used in the multi-national\ncompanies, public interest institutions, and medium and small dimension\neconomical entities, in order to optimize the managerial decisions and render\nefficient the financial-accounting functionality. The purpose of this paper is\naimed to identifying the economical exigencies of the entities, based on the\nalready used accounting instruments and the management software that could\nconsent the control of the economical processes and patrimonial assets.", 
    "link": "http://arxiv.org/pdf/1001.3495v1", 
    "arxiv-id": "1001.3495v1"
},{
    "category": "cs.CE", 
    "author": "Puneet Tandon", 
    "title": "Computer Aided Design Modeling for Heterogeneous Objects", 
    "publish": "2010-04-20T20:42:34Z", 
    "summary": "Heterogeneous object design is an active research area in recent years. The\nconventional CAD modeling approaches only provide geometry and topology of the\nobject, but do not contain any information with regard to the materials of the\nobject and so can not be used for the fabrication of heterogeneous objects (HO)\nthrough rapid prototyping. Current research focuses on computer-aided design\nissues in heterogeneous object design. A new CAD modeling approach is proposed\nto integrate the material information into geometric regions thus model the\nmaterial distributions in the heterogeneous object. The gradient references are\nused to represent the complex geometry heterogeneous objects which have\nsimultaneous geometry intricacies and accurate material distributions. The\ngradient references helps in flexible manipulability and control to\nheterogeneous objects, which guarantees the local control over gradient regions\nof developed heterogeneous objects. A systematic approach on data flow,\nprocessing, computer visualization, and slicing of heterogeneous objects for\nrapid prototyping is also presented.", 
    "link": "http://arxiv.org/pdf/1004.3571v1", 
    "arxiv-id": "1004.3571v1"
},{
    "category": "cs.CE", 
    "author": "Elmostafa Barj", 
    "title": "Optical phase extraction algorithm based on the continuous wavelet and   the Hilbert transforms", 
    "publish": "2010-05-21T17:05:23Z", 
    "summary": "In this paper we present an algorithm for optical phase evaluation based on\nthe wavelet transform technique. The main advantage of this method is that it\nrequires only one fringe pattern. This algorithm is based on the use of a\nsecond {\\pi}/2 phase shifted fringe pattern where it is calculated via the\nHilbert transform. To test its validity, the algorithm was used to demodulate a\nsimulated fringe pattern giving the phase distribution with a good accuracy.", 
    "link": "http://arxiv.org/pdf/1005.4005v1", 
    "arxiv-id": "1005.4005v1"
},{
    "category": "cs.CE", 
    "author": "Oliver Kennedy", 
    "title": "Inventory Allocation for Online Graphical Display Advertising", 
    "publish": "2010-08-20T18:37:23Z", 
    "summary": "We discuss a multi-objective/goal programming model for the allocation of\ninventory of graphical advertisements. The model considers two types of\ncampaigns: guaranteed delivery (GD), which are sold months in advance, and\nnon-guaranteed delivery (NGD), which are sold using real-time auctions. We\ninvestigate various advertiser and publisher objectives such as (a) revenue\nfrom the sale of impressions, clicks and conversions, (b) future revenue from\nthe sale of NGD inventory, and (c) \"fairness\" of allocation. While the first\ntwo objectives are monetary, the third is not. This combination of demand types\nand objectives leads to potentially many variations of our model, which we\ndelineate and evaluate. Our experimental results, which are based on\noptimization runs using real data sets, demonstrate the effectiveness and\nflexibility of the proposed model.", 
    "link": "http://arxiv.org/pdf/1008.3551v1", 
    "arxiv-id": "1008.3551v1"
},{
    "category": "cs.CE", 
    "author": "Yogendra Namjoshi", 
    "title": "Delineation of Raw Plethysmograph using Wavelets for Mobile based Pulse   Oximeters", 
    "publish": "2010-11-01T04:54:50Z", 
    "summary": "The non-invasive pulse-oximeter is a crucial parameter in continuous\nmonitoring systems. It plays a vital role from admission of the patient to\nsurgeries with general anaesthesia. The paper proposes the application of\nwavelet transform to delineate the raw plethysmograph signals obtained from\nbasic portable and mobile-powered electronic hardware. The paper primarily\nfocuses on finding peaks and baseline from noisy infrared and red waveforms\nwhich are responsible for calculating heart-rate and oxygen saturation\npercentages.", 
    "link": "http://arxiv.org/pdf/1011.0250v1", 
    "arxiv-id": "1011.0250v1"
},{
    "category": "cs.CE", 
    "author": "Mohammad Reza Rezaeinezhad", 
    "title": "Mobile Based Secure Digital Wallet for Peer to Peer Payment System", 
    "publish": "2010-11-01T09:55:23Z", 
    "summary": "E-commerce in today's conditions has the highest dependence on network\ninfrastructure of banking. However, when the possibility of communicating with\nthe Banking network is not provided, business activities will suffer. This\npaper proposes a new approach of digital wallet based on mobile devices without\nthe need to exchange physical money or communicate with banking network. A\ndigital wallet is a software component that allows a user to make an electronic\npayment in cash (such as a credit card or a digital coin), and hides the\nlow-level details of executing the payment protocol that is used to make the\npayment. The main features of proposed architecture are secure awareness, fault\ntolerance, and infrastructure-less protocol.", 
    "link": "http://arxiv.org/pdf/1011.0279v1", 
    "arxiv-id": "1011.0279v1"
},{
    "category": "cs.CE", 
    "author": "Marino Miculan", 
    "title": "Measurable Stochastics for Brane Calculus", 
    "publish": "2010-11-02T01:29:09Z", 
    "summary": "We give a stochastic extension of the Brane Calculus, along the lines of\nrecent work by Cardelli and Mardare. In this presentation, the semantics of a\nBrane process is a measure of the stochastic distribution of possible\nderivations. To this end, we first introduce a labelled transition system for\nBrane Calculus, proving its adequacy w.r.t. the usual reduction semantics.\nThen, brane systems are presented as Markov processes over the measurable space\ngenerated by terms up-to syntactic congruence, and where the measures are\nindexed by the actions of this new LTS. Finally, we provide a SOS presentation\nof this stochastic semantics, which is compositional and syntax-driven.", 
    "link": "http://arxiv.org/pdf/1011.0488v1", 
    "arxiv-id": "1011.0488v1"
},{
    "category": "cs.CE", 
    "author": "Tatjana Petrov", 
    "title": "Lumpability Abstractions of Rule-based Systems", 
    "publish": "2010-11-02T01:30:06Z", 
    "summary": "The induction of a signaling pathway is characterized by transient complex\nformation and mutual posttranslational modification of proteins. To faithfully\ncapture this combinatorial process in a mathematical model is an important\nchallenge in systems biology. Exploiting the limited context on which most\nbinding and modification events are conditioned, attempts have been made to\nreduce the combinatorial complexity by quotienting the reachable set of\nmolecular species, into species aggregates while preserving the deterministic\nsemantics of the thermodynamic limit. Recently we proposed a quotienting that\nalso preserves the stochastic semantics and that is complete in the sense that\nthe semantics of individual species can be recovered from the aggregate\nsemantics. In this paper we prove that this quotienting yields a sufficient\ncondition for weak lumpability and that it gives rise to a backward Markov\nbisimulation between the original and aggregated transition system. We\nillustrate the framework on a case study of the EGF/insulin receptor crosstalk.", 
    "link": "http://arxiv.org/pdf/1011.0496v1", 
    "arxiv-id": "1011.0496v1"
},{
    "category": "cs.CE", 
    "author": "Franck Pommereau", 
    "title": "Qualitative modelling and analysis of regulations in multi-cellular   systems using Petri nets and topological collections", 
    "publish": "2010-11-02T01:37:03Z", 
    "summary": "In this paper, we aim at modelling and analyzing the regulation processes in\nmulti-cellular biological systems, in particular tissues.\n  The modelling framework is based on interconnected logical regulatory\nnetworks a la Rene Thomas equipped with information about their spatial\nrelationships. The semantics of such models is expressed through colored Petri\nnets to implement regulation rules, combined with topological collections to\nimplement the spatial information.\n  Some constraints are put on the the representation of spatial information in\norder to preserve the possibility of an enumerative and exhaustive state space\nexploration.\n  This paper presents the modelling framework, its semantics, as well as a\nprototype implementation that allowed preliminary experimentation on some\napplications.", 
    "link": "http://arxiv.org/pdf/1011.0498v1", 
    "arxiv-id": "1011.0498v1"
},{
    "category": "cs.CE", 
    "author": "Anne Auger", 
    "title": "Using Evolution Strategy with Meta-models for Well Placement   Optimization", 
    "publish": "2010-11-24T20:08:30Z", 
    "summary": "Optimum implementation of non-conventional wells allows us to increase\nconsiderably hydrocarbon recovery. By considering the high drilling cost and\nthe potential improvement in well productivity, well placement decision is an\nimportant issue in field development. Considering complex reservoir geology and\nhigh reservoir heterogeneities, stochastic optimization methods are the most\nsuitable approaches for optimum well placement. This paper proposes an\noptimization methodology to determine optimal well location and trajectory\nbased upon the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) which\nis a variant of Evolution Strategies recognized as one of the most powerful\nderivative-free optimizers for continuous optimization. To improve the\noptimization procedure, two new techniques are investigated: (1). Adaptive\npenalization with rejection is developed to handle well placement constraints.\n(2). A meta-model, based on locally weighted regression, is incorporated into\nCMA-ES using an approximate ranking procedure. Therefore, we can reduce the\nnumber of reservoir simulations, which are computationally expensive. Several\nexamples are presented. Our new approach is compared with a Genetic Algorithm\nincorporating the Genocop III technique. It is shown that our approach\noutperforms the genetic algorithm: it leads in general to both a higher NPV and\na significant reduction of the number of reservoir simulations.", 
    "link": "http://arxiv.org/pdf/1011.5481v1", 
    "arxiv-id": "1011.5481v1"
},{
    "category": "cs.CE", 
    "author": "Yves Goussard", 
    "title": "R\u00e9gularisation et optimisation pour l'imagerie sismique des fondations   de pyl\u00f4nes", 
    "publish": "2010-12-20T15:56:28Z", 
    "summary": "This research report summarizes the progress of work carried out jointly by\nthe IRCCyN and the \\'Ecole Polytechnique de Montr\\'eal about the resolution of\nthe inverse problem for the seismic imaging of transmission overhead line\nstructure foundations. Several methods aimed at mapping the underground medium\nare considered. More particularly, we focus on methods based on a bilinear\nformulation of the forward problem on one hand (CSI, modified gradient, etc.)\nand on methods based on a \"primal\" formulation on the other hand. The\nperformances of these methods are compared using synthetic data. This work was\npartially funded by RTE (R\\'eseau de Transport d'\\'Electricit\\'e), which has\ninitiated the project, and was carried out in collaboration with EDF R&D\n(\\'Electricit\\'e de France - Recherche et D\\'eveloppement).", 
    "link": "http://arxiv.org/pdf/1012.4374v1", 
    "arxiv-id": "1012.4374v1"
},{
    "category": "cs.CE", 
    "author": "Taufik Abr\u00e3o", 
    "title": "Power-Rate Allocation in DS/CDMA Based on Discretized Verhulst   Equilibrium", 
    "publish": "2010-12-22T19:09:12Z", 
    "summary": "This paper proposes to extend the discrete Verhulst power equilibrium\napproach, previously suggested in [1], to the power-rate optimal allocation\nproblem. Multirate users associated to different types of traffic are\naggregated to distinct user' classes, with the assurance of minimum rate\nallocation per user and QoS. Herein, Verhulst power allocation algorithm was\nadapted to the single-input-single-output DS/CDMA jointly power-rate control\nproblem. The analysis was carried out taking into account the convergence time,\nquality of solution, in terms of the normalized squared error (NSE), when\ncompared with the analytical solution based on interference matrix inverse, and\ncomputational complexity. Numerical results demonstrate the validity of the\nproposed resource allocation methodology.", 
    "link": "http://arxiv.org/pdf/1012.5074v1", 
    "arxiv-id": "1012.5074v1"
},{
    "category": "cs.CE", 
    "author": "D. Pech-Oy", 
    "title": "Synthesis of Mechanism for single- and hybrid-tasks using Differential   Evolution", 
    "publish": "2011-02-10T00:50:54Z", 
    "summary": "The optimal dimensional synthesis for planar mechanisms using differential\nevolution (DE) is demonstrated. Four examples are included: in the first case,\nthe synthesis of a mechanism for hybrid-tasks, considering path generation,\nfunction generation, and motion generation, is carried out. The second and\nthird cases pertain to path generation, with and without prescribed timing.\nFinally, the synthesis of an Ackerman mechanism is reported. Order defect\nproblem is solved by manipulating individuals instead of penalizing or\ndiscretizing the search space for the parameters. A technique that consists in\napplying a transformation in order to satisfy the Grashof and crank conditions\nto generate an initial elitist population is introduced. As a result, the\nevolutionary algorithm increases its efficiency.", 
    "link": "http://arxiv.org/pdf/1102.2017v2", 
    "arxiv-id": "1102.2017v2"
},{
    "category": "cs.CE", 
    "author": "Huazhong Yang", 
    "title": "Mini-step Strategy for Transient Analysis", 
    "publish": "2011-03-12T14:30:48Z", 
    "summary": "Domain decomposition methods are widely used to solve sparse linear systems\nfrom scientific problems, but they are not suited to solve sparse linear\nsystems extracted from integrated circuits. The reason is that the sparse\nlinear system of integrated circuits may be non-diagonal-dominant, and domain\ndecomposition method might be unconvergent for these non-diagonal-dominant\nmatrices. In this paper, we propose a mini-step strategy to do the circuit\ntransient analysis. Different from the traditional large-step approach, this\nstrategy is able to generate diagonal-dominant sparse linear systems. As a\nresult, preconditioned domain decomposition methods can be used to simulate the\nlarge integrated circuits on the supercomputers and clouds.", 
    "link": "http://arxiv.org/pdf/1103.2447v1", 
    "arxiv-id": "1103.2447v1"
},{
    "category": "cs.CE", 
    "author": "Sanja Petrovic", 
    "title": "An Integer Linear Programming Model for the Radiotherapy Treatment   Scheduling Problem", 
    "publish": "2011-03-17T12:27:10Z", 
    "summary": "Radiotherapy represents an important phase of treatment for a large number of\ncancer patients. It is essential that resources used to deliver this treatment\nare employed effectively. This paper presents a new integer linear programming\nmodel for real-world radiotherapy treatment scheduling and analyses the\neffectiveness of using this model on a daily basis in a hospital. Experiments\nare conducted varying the days on which schedules can be created. Results\nobtained using real-world data from the Nottingham University Hospitals NHS\nTrust, UK, are presented and show how the proposed model can be used with\ndifferent policies in order to achieve good quality schedules.", 
    "link": "http://arxiv.org/pdf/1103.3391v1", 
    "arxiv-id": "1103.3391v1"
},{
    "category": "cs.CE", 
    "author": "P. S. Hiremath", 
    "title": "Computer Modelling of 3D Geological Surface", 
    "publish": "2011-03-24T10:31:44Z", 
    "summary": "The geological surveying presently uses methods and tools for the computer\nmodeling of 3D-structures of the geographical subsurface and geotechnical\ncharacterization as well as the application of geoinformation systems for\nmanagement and analysis of spatial data, and their cartographic presentation.\nThe objectives of this paper are to present a 3D geological surface model of\nLatur district in Maharashtra state of India. This study is undertaken through\nthe several processes which are discussed in this paper to generate and\nvisualize the automated 3D geological surface model of a projected area.", 
    "link": "http://arxiv.org/pdf/1103.4720v2", 
    "arxiv-id": "1103.4720v2"
},{
    "category": "cs.CE", 
    "author": "Hyungjun Jang", 
    "title": "Airborne TDMA for High Throughput and Fast Weather Conditions   Notification", 
    "publish": "2011-05-30T10:41:17Z", 
    "summary": "As air traffic grows significantly, aircraft accidents increase. Many\naviation accidents could be prevented if the precise aircraft positions and\nweather conditions on the aircraft's route were known. Existing studies propose\ndetermining the precise aircraft positions via a VHF channel with an air-to-air\nradio relay system that is based on mobile ad-hoc networks. However, due to the\nlong propagation delay, the existing TDMA MAC schemes underutilize the\nnetworks. The existing TDMA MAC sends data and receives ACK in one time slot,\nwhich requires two guard times in one time slot. Since aeronautical\ncommunications spans a significant distance, the guard time occupies a\nsignificantly large portion of the slot. To solve this problem, we propose a\npiggybacking mechanism ACK. Our proposed MAC has one guard time in one time\nslot, which enables the transmission of more data. Using this additional data,\nwe can send weather conditions that pertain to the aircraft's current position.\nOur analysis shows that this proposed MAC performs better than the existing\nMAC, since it offers better throughput and network utilization. In addition,\nour weather condition notification model achieves a much lower transmission\ndelay than a HF (high frequency) voice communication.", 
    "link": "http://arxiv.org/pdf/1105.5939v1", 
    "arxiv-id": "1105.5939v1"
},{
    "category": "cs.CE", 
    "author": "Reshma. p", 
    "title": "Power Management during Scan Based Sequential Circuit Testing", 
    "publish": "2011-06-02T06:52:02Z", 
    "summary": "This paper shows that not every scan cell contributes equally to the power\nconsumption during scan based test. The transitions at some scan cells cause\nmore toggles at the internal signal lines of a circuit than the transitions at\nother scan cells. Hence the transitions at these scan cells have a larger\nimpact on the power consumption during test application. These scan cells are\ncalled power sensitive scan cells.A verilog based approach is proposed to\nidentify a set of power sensitive scan cells. Additional hardware is added to\nfreeze the outputs of power sensitive scan cells during scan shifting in order\nto reduce the shift power consumption.when multiple scan chain is incorporated\nalong with freezing the power sensitive scan cell,over all power during testing\ncan be reduced to a larger extend.", 
    "link": "http://arxiv.org/pdf/1106.2794v1", 
    "arxiv-id": "1106.2794v1"
},{
    "category": "cs.CE", 
    "author": "Leonid A. Ostromuhov", 
    "title": "Models, Calculation and Optimization of Gas Networks, Equipment and   Contracts for Design, Operation, Booking and Accounting", 
    "publish": "2011-06-20T18:12:19Z", 
    "summary": "There are proposed models of contracts, technological equipment and gas\nnetworks and methods of their optimization. The flow in network undergoes\nrestrictions of contracts and equipment to be operated. The values of sources\nand sinks are provided by contracts. The contract models represent (sub-)\nnetworks. The simplest contracts represent either nodes or edges. Equipment is\nmodeled by edges. More sophisticated equipment is represented by sub-networks.\nExamples of such equipment are multi-poles and compressor stations with many\nentries and exits. The edges can be of different types corresponding to\nequipment and contracts. On such edges, there are given systems of equation and\ninequalities simulating the contracts and equipment. On this base, the methods\nproposed that allow: calculation and control of contract values for booking on\nfuture days and for accounting of sales and purchases; simulation and\noptimization of design and of operation of gas networks. These models and\nmethods are implemented in software systems ACCORD and Graphicord as well as in\nthe distributed control system used by Wingas, Germany. As numerical example,\nthe industrial computations are presented.", 
    "link": "http://arxiv.org/pdf/1106.3977v3", 
    "arxiv-id": "1106.3977v3"
},{
    "category": "cs.CE", 
    "author": "Jitesh Dundas", 
    "title": "Automaton based detection of affected cells in three dimensional   biological system", 
    "publish": "2011-06-30T20:11:58Z", 
    "summary": "The aim of this research review is to propose the logic and search mechanism\nfor the development of an artificially intelligent automaton (AIA) that can\nfind affected cells in a 3-dimensional biological system. Research on the\npossible application of such automatons to detect and control cancer cells in\nthe human body are greatly focused MRI and PET scans finds the affected regions\nat the tissue level even as we can find the affected regions at the cellular\nlevel using the framework. The AIA may be designed to ensure optimum\nutilization as they record and might control the presence of affected cells in\na human body. The proposed models and techniques can be generalized and used in\nany application where cells are injured or affected by some disease or\naccident. The best method to import AIA into the body without surgery or\ninjection is to insert small pill like automata, carrying material viz drugs or\nleukocytes that is needed to correct the infection. In this process, the AIA\ncan be compared to nano pills to deliver or support therapy. NanoHive\nsimulation software was used to validate the framework of this paper. The\nexisting nanomedicine models such as obstacle avoidance algorithm based models\n(Hla K H S et al 2008) and the framework in this model were tested in different\nsimulation based experiments. The existing models such as obstacle avoidance\nbased models failed in complex environmental conditions (such as changing\nenvironmental conditions, presence of semi-solid particles, etc) while the\nmodel in this paper executed its framework successfully.Come systems biology,\nthis field of automatons deserves a bigger leap of understanding especially\nwhen pharmacogenomics is at its peak. The results also indicate the importance\nof artificial intelligence and other computational capabilities in the proposed\nmodel for the successful detection of affected cells.", 
    "link": "http://arxiv.org/pdf/1107.0015v1", 
    "arxiv-id": "1107.0015v1"
},{
    "category": "cs.CE", 
    "author": "K. R Seeja", 
    "title": "AISMOTIF-An Artificial Immune System for DNA Motif Discovery", 
    "publish": "2011-07-05T06:01:20Z", 
    "summary": "Discovery of transcription factor binding sites is a much explored and still\nexploring area of research in functional genomics. Many computational tools\nhave been developed for finding motifs and each of them has their own\nadvantages as well as disadvantages. Most of these algorithms need prior\nknowledge about the data to construct background models. However there is not a\nsingle technique that can be considered as best for finding regulatory motifs.\nThis paper proposes an artificial immune system based algorithm for finding the\ntranscription factor binding sites or motifs and two new weighted scores for\nmotif evaluation. The algorithm is enumerative, but sufficient pruning of the\npattern search space has been incorporated using immune system concepts. The\nperformance of AISMOTIF has been evaluated by comparing it with eight state of\nart composite motif discovery algorithms and found that AISMOTIF predicts known\nmotifs as well as new motifs from the benchmark dataset without any prior\nknowledge about the data.", 
    "link": "http://arxiv.org/pdf/1107.1128v1", 
    "arxiv-id": "1107.1128v1"
},{
    "category": "cs.CE", 
    "author": "R. P. Young", 
    "title": "Fluid Flow Complexity in Fracture Networks: Analysis with Graph Theory   and LBM", 
    "publish": "2011-07-25T12:28:50Z", 
    "summary": "Through this research, embedded synthetic fracture networks in rock masses\nare studied. To analysis the fluid flow complexity in fracture networks with\nrespect to the variation of connectivity patterns, two different approaches are\nemployed, namely, the Lattice Boltzmann method and graph theory. The Lattice\nBoltzmann method is used to show the sensitivity of the permeability and fluid\nvelocity distribution to synthetic fracture networks' connectivity patterns.\nFurthermore, the fracture networks are mapped into the graphs, and the\ncharacteristics of these graphs are compared to the main spatial fracture\nnetworks. Among different characteristics of networks, we distinguish the\nmodularity of networks and sub-graphs distributions. We map the flow regimes\ninto the proper regions of the network's modularity space. Also, for each type\nof fluid regime, corresponding motifs shapes are scaled. Implemented power law\ndistributions of fracture length in spatial fracture networks yielded the same\nnode's degree distribution in transformed networks. Two general spatial\nnetworks are considered: random networks and networks with \"hubness\" properties\nmimicking a spatial damage zone (both with power law distribution of fracture\nlength). In the first case, the fractures are embedded in uniformly distributed\nfracture sets; the second case covers spatial fracture zones. We prove\nnumerically that the abnormal change (transition) in permeability is controlled\nby the hub growth rate. Also, comparing LBM results with the characteristic\nmean length of transformed networks' links shows a reverse relationship between\nthe aforementioned parameters. In addition, the abnormalities in advection\nthrough nodes are presented.", 
    "link": "http://arxiv.org/pdf/1107.4918v2", 
    "arxiv-id": "1107.4918v2"
},{
    "category": "cs.CE", 
    "author": "Lorena A. Barba", 
    "title": "cuIBM -- A GPU-accelerated Immersed Boundary Method", 
    "publish": "2011-09-16T04:02:15Z", 
    "summary": "A projection-based immersed boundary method is dominated by sparse linear\nalgebra routines. Using the open-source Cusp library, we observe a speedup\n(with respect to a single CPU core) which reflects the constraints of a\nbandwidth-dominated problem on the GPU. Nevertheless, GPUs offer the capacity\nto solve large problems on commodity hardware. This work includes validation\nand a convergence study of the GPU-accelerated IBM, and various optimizations.", 
    "link": "http://arxiv.org/pdf/1109.3524v2", 
    "arxiv-id": "1109.3524v2"
},{
    "category": "cs.CE", 
    "author": "H. G. Matthies", 
    "title": "Acceleration of Uncertainty Updating in the Description of Transport   Processes in Heterogeneous Materials", 
    "publish": "2011-10-10T14:22:16Z", 
    "summary": "The prediction of thermo-mechanical behaviour of heterogeneous materials such\nas heat and moisture transport is strongly influenced by the uncertainty in\nparameters. Such materials occur e.g. in historic buildings, and the durability\nassessment of these therefore needs a reliable and probabilistic simulation of\ntransport processes, which is related to the suitable identification of\nmaterial parameters. In order to include expert knowledge as well as\nexperimental results, one can employ an updating procedure such as Bayesian\ninference. The classical probabilistic setting of the identification process in\nBayes's form requires the solution of a stochastic forward problem via\ncomputationally expensive sampling techniques, which makes the method almost\nimpractical. In this paper novel stochastic computational techniques such as\nthe stochastic Galerkin method are applied in order to accelerate the updating\nprocedure. The idea is to replace the computationally expensive forward\nsimulation via the conventional finite element (FE) method by the evaluation of\na polynomial chaos expansion (PCE). Such an approximation of the FE model for\nthe forward simulation perfectly suits the Bayesian updating. The presented\nuncertainty updating techniques are applied to the numerical model of coupled\nheat and moisture transport in heterogeneous materials with spatially varying\ncoefficients defined by random fields.", 
    "link": "http://arxiv.org/pdf/1110.2049v1", 
    "arxiv-id": "1110.2049v1"
},{
    "category": "cs.CE", 
    "author": "S. Adhikari", 
    "title": "Sampling Techniques in Bayesian Finite Element Model Updating", 
    "publish": "2011-10-15T05:17:18Z", 
    "summary": "Recent papers in the field of Finite Element Model (FEM) updating have\nhighlighted the benefits of Bayesian techniques. The Bayesian approaches are\ndesigned to deal with the uncertainties associated with complex systems, which\nis the main problem in the development and updating of FEMs. This paper\nhighlights the complexities and challenges of implementing any Bayesian method\nwhen the analysis involves a complicated structural dynamic model. In such\nsystems an analytical Bayesian formulation might not be available in an\nanalytic form; therefore this leads to the use of numerical methods, i.e.\nsampling methods. The main challenge then is to determine an efficient sampling\nof the model parameter space. In this paper, three sampling techniques, the\nMetropolis-Hastings (MH) algorithm, Slice Sampling and the Hybrid Monte Carlo\n(HMC) technique, are tested by updating a structural beam model. The efficiency\nand limitations of each technique is investigated when the FEM updating problem\nis implemented using the Bayesian Approach. Both MH and HMC techniques are\nfound to perform better than the Slice sampling when Young's modulus is chosen\nas the updating parameter. The HMC method gives better results than MH and\nSlice sampling techniques, when the area moment of inertias and section areas\nare updated.", 
    "link": "http://arxiv.org/pdf/1110.3382v1", 
    "arxiv-id": "1110.3382v1"
},{
    "category": "cs.CE", 
    "author": "Tamnun E Mursalin", 
    "title": "A more appropriate Protein Classification using Data Mining", 
    "publish": "2011-10-12T12:18:39Z", 
    "summary": "Research in bioinformatics is a complex phenomenon as it overlaps two\nknowledge domains, namely, biological and computer sciences. This paper has\ntried to introduce an efficient data mining approach for classifying proteins\ninto some useful groups by representing them in hierarchy tree structure. There\nare several techniques used to classify proteins but most of them had few\ndrawbacks on their grouping. Among them the most efficient grouping technique\nis used by PSIMAP. Even though PSIMAP (Protein Structural Interactome Map)\ntechnique was successful to incorporate most of the protein but it fails to\nclassify the scale free property proteins. Our technique overcomes this\ndrawback and successfully maps all the protein in different groups, including\nthe scale free property proteins failed to group by PSIMAP. Our approach\nselects the six major attributes of protein: a) Structure comparison b)\nSequence Comparison c) Connectivity d) Cluster Index e) Interactivity f)\nTaxonomic to group the protein from the databank by generating a hierarchal\ntree structure. The proposed approach calculates the degree (probability) of\nsimilarity of each protein newly entered in the system against of existing\nproteins in the system by using probability theorem on each six properties of\nproteins.", 
    "link": "http://arxiv.org/pdf/1111.2514v1", 
    "arxiv-id": "1111.2514v1"
},{
    "category": "cs.CE", 
    "author": "Carlos A. Cruz-Villar", 
    "title": "Synthesis of Spherical 4R Mechanism for Path Generation using   Differential Evolution", 
    "publish": "2011-12-13T16:48:05Z", 
    "summary": "The problem of path generation for the spherical 4R mechanism is solved using\nthe Differential Evolution algorithm (DE). Formulas for the spherical geodesics\nare employed in order to obtain the parametric equation for the generated\ntrajectory. Direct optimization of the objective function gives the solution to\nthe path generation task without prescribed timing. Therefore, there is no need\nto separate this task into two stages to make the optimization. Moreover, the\norder defect problem can be solved without difficulty by means of manipulations\nof the individuals in the DE algorithm. Two examples of optimum synthesis\nshowing the simplicity and effectiveness of this approach are included.", 
    "link": "http://arxiv.org/pdf/1112.2954v2", 
    "arxiv-id": "1112.2954v2"
},{
    "category": "cs.CE", 
    "author": "Ziad G. Ghauch", 
    "title": "3D Finite Element Analysis of HMA Overlay Mix Design to Control   Reflective Cracking", 
    "publish": "2011-12-21T00:39:12Z", 
    "summary": "This study examines the effectiveness of HMA overlay design strategies for\nthe purpose of controlling the development of reflective cracking. A parametric\nstudy was conducted using a 3D Finite Element (FE) model of a rigid pavement\nsection including Linear Viscoelastic (LVE) material properties for the Hot Mix\nAsphalt (HMA) overlay and non-uniform tire-pavement contact stresses. Several\nasphalt mixtures were tested in the surface, intermediate, and leveling course\nof the HMA overlay. Results obtained show that no benefits can be anticipated\nby using either Polymer-Modified (PM) or Dense-Graded (DG) mixtures instead of\nStandard Binder (SB) mixtures in the surface or intermediate course. For the\nleveling course, the use of a PM asphalt binder was found beneficial in terms\nof mitigating reflective cracking. As compared to the SB mix, the use of PM\nasphalt mixture in the leveling course reduced the level of longitudinal\ntensile stress at the bottom of the HMA overlay above the PCC joint by\napproximately 30%.", 
    "link": "http://arxiv.org/pdf/1112.4895v3", 
    "arxiv-id": "1112.4895v3"
},{
    "category": "cs.CE", 
    "author": "Karl Henrik Johansson", 
    "title": "Computing Critical $k$-tuples in Power Networks", 
    "publish": "2012-01-02T14:10:57Z", 
    "summary": "In this paper the problem of finding the sparsest (i.e., minimum cardinality)\ncritical $k$-tuple including one arbitrarily specified measurement is\nconsidered. The solution to this problem can be used to identify weak points in\nthe measurement set, or aid the placement of new meters. The critical $k$-tuple\nproblem is a combinatorial generalization of the critical measurement\ncalculation problem. Using topological network observability results, this\npaper proposes an efficient and accurate approximate solution procedure for the\nconsidered problem based on solving a minimum-cut (Min-Cut) problem and\nenumerating all its optimal solutions. It is also shown that the sparsest\ncritical $k$-tuple problem can be formulated as a mixed integer linear\nprogramming (MILP) problem. This MILP problem can be solved exactly using\navailable solvers such as CPLEX and Gurobi. A detailed numerical study is\npresented to evaluate the efficiency and the accuracy of the proposed Min-Cut\nand MILP calculations.", 
    "link": "http://arxiv.org/pdf/1201.0469v1", 
    "arxiv-id": "1201.0469v1"
},{
    "category": "cs.CE", 
    "author": "Michal \u0160ejnoha", 
    "title": "Simple Numerical Model of Laminated Glass Beams", 
    "publish": "2012-01-17T10:57:29Z", 
    "summary": "This contribution presents a simple Finite Element model aimed at efficient\nsimulation of layered glass units. The adopted approach is based on considering\nindependent kinematics of each layer, tied together via Lagrange multipliers.\nValidation and verification of the resulting model against independent data\ndemonstrate its accuracy, showing its potential for generalization towards more\ncomplex problems.", 
    "link": "http://arxiv.org/pdf/1201.3479v1", 
    "arxiv-id": "1201.3479v1"
},{
    "category": "cs.CE", 
    "author": "Jan Zeman", 
    "title": "A framework for integrated design of algorithmic architectural forms", 
    "publish": "2012-03-12T14:26:05Z", 
    "summary": "This paper presents a methodology and software tools for parametric design of\ncomplex architectural objects, called digital or algorithmic forms. In order to\nprovide a flexible tool, the proposed design philosophy involves two open\nsource utilities Donkey and MIDAS written in Grasshopper algorithm editor and\nC++, respectively, that are to be linked with a scripting-based architectural\nmodellers Rhinoceros, IntelliCAD and the open source Finite Element solver\nOOFEM. The emphasis is put on the mechanical response in order to provide\narchitects with a consistent learning framework and an insight into structural\nbehaviour of designed objects. As demonstrated on three case studies, the\nproposed modular solution is capable of handling objects of considerable\nstructural complexity, thereby accelerating the process of finding procedural\ndesign parameters from orders of weeks to days or hours.", 
    "link": "http://arxiv.org/pdf/1203.2499v2", 
    "arxiv-id": "1203.2499v2"
},{
    "category": "cs.CE", 
    "author": "Michael Robinson", 
    "title": "Knowledge-based antenna pattern extrapolation", 
    "publish": "2012-03-12T16:02:46Z", 
    "summary": "We describe a theoretically-motivated algorithm for extrapolation of antenna\nradiation patterns from a small number of measurements. This algorithm exploits\nconstraints on the antenna's underlying design to avoid ambiguities, but is\nsufficiently general to address many different antenna types. A theoretical\nbasis for the robustness of this algorithm is developed, and its performance is\nverified in simulation using a number of popular antenna designs.", 
    "link": "http://arxiv.org/pdf/1203.2528v1", 
    "arxiv-id": "1203.2528v1"
},{
    "category": "cs.CE", 
    "author": "Steskens", 
    "title": "System Identification for Indoor Climate Control", 
    "publish": "2012-03-30T07:27:23Z", 
    "summary": "The study focuses on the applicability of system identification to identify\nbuilding and system dynamics for climate control design. The main problem\nregarding the simulation of the dynamic response of a building using building\nsimulation software is that (1) the simulation of a large complex building is\ntime consuming, and (2) simulation results often lack information regarding\nfast dynamic behaviour (in the order of seconds), since most software uses a\ndiscrete time step, usually fixed to one hour. The first objective is to study\nthe applicability of system identification to reduce computing time for the\nsimulation of large complex buildings. The second objective is to research the\napplicability of system identification to identify building dynamics based on\ndiscrete time data (one hour) for climate control design. The study illustrates\nthat system identification is applicable for the identification of building\ndynamics with a frequency that is smaller as the maximum sample frequency as\nused for identification. The research shows that system identification offers\ngood perspectives for the modelling of heat, air and moisture processes in a\nbuilding. The main advantages of system identification models compared to the\nmodelling of building dynamics using building simulation software are, that (1)\nthe computing time is reduced significantly, and (2) system identification\nmodels run in a MATLAB environment, in which many building simulation tools\nhave been developed", 
    "link": "http://arxiv.org/pdf/1203.6728v1", 
    "arxiv-id": "1203.6728v1"
},{
    "category": "cs.CE", 
    "author": "Michael Robinson", 
    "title": "Multipath-dominant, pulsed doppler analysis of rotating blades", 
    "publish": "2012-04-19T15:02:30Z", 
    "summary": "We present a novel angular fingerprinting algorithm for detecting changes in\nthe direction of rotation of a target with a monostatic, stationary sonar\nplatform. Unlike other approaches, we assume that the target's centroid is\nstationary, and exploit doppler multipath signals to resolve the otherwise\nunavoidable ambiguities that arise. Since the algorithm is based on an\nunderlying differential topological theory, it is highly robust to distortions\nin the collected data. We demonstrate performance of this algorithm\nexperimentally, by exhibiting a pulsed doppler sonar collection system that\nruns on a smartphone. The performance of this system is sufficiently good to\nboth detect changes in target rotation direction using angular fingerprints,\nand also to form high-resolution inverse synthetic aperature images of the\ntarget.", 
    "link": "http://arxiv.org/pdf/1204.4366v1", 
    "arxiv-id": "1204.4366v1"
},{
    "category": "cs.CE", 
    "author": "Mar\u00eda Fernanda Gayol", 
    "title": "Christhin: Quantitative Analysis of Thin Layer Chromatography", 
    "publish": "2012-04-23T17:15:17Z", 
    "summary": "Manual for Christhin 0.1.36 Christhin (Chromatography Riser Thin) is software\ndeveloped for the quantitative analysis of data obtained from thin-layer\nchromatographic techniques (TLC). Once installed on your computer, the program\nis very easy to use, and provides data quickly and accurately. This manual\ndescribes the program, and reading should be enough to use it properly.", 
    "link": "http://arxiv.org/pdf/1204.5174v2", 
    "arxiv-id": "1204.5174v2"
},{
    "category": "cs.CE", 
    "author": "Mohammad Tahat", 
    "title": "WM Program manual", 
    "publish": "2012-06-04T14:51:18Z", 
    "summary": "This user manual has been written to describe the open source code WM to be\ndistributed associated with a research article submitted to the information\ntechnology journal 45001-ITJ-ANSI, entitled: \"Maintenance and Reengineering of\nsoftware: Creating a Visual C++ Graphical User Interface to Perform Specific\nTasks Related to Soil Structure Interaction in Poroelastic Soil\".", 
    "link": "http://arxiv.org/pdf/1206.0638v2", 
    "arxiv-id": "1206.0638v2"
},{
    "category": "cs.CE", 
    "author": "A. W. M. van Schijndel", 
    "title": "Inverse Modeling of Climate Responses of Monumental Buildings", 
    "publish": "2012-06-20T10:07:31Z", 
    "summary": "The indoor climate conditions of monumental buildings are very important for\nthe conservation of these objects. Simplified models with physical meaning are\ndesired that are capable of simulating temperature and relative humidity. In\nthis paper we research state-space models as methodology for the inverse\nmodeling of climate responses of unheated monumental buildings. It is concluded\nthat this approach is very promising for obtaining physical models and\nparameters of indoor climate responses. Furthermore state space models can be\nsimulated very efficiently: the simulation duration time of a 100 year hourly\nbased period take less than a second on an ordinary computer.", 
    "link": "http://arxiv.org/pdf/1206.4438v1", 
    "arxiv-id": "1206.4438v1"
},{
    "category": "cs.CE", 
    "author": "Yuriy Ostapov", 
    "title": "Intellectual Management of Enterprise", 
    "publish": "2012-07-02T09:18:53Z", 
    "summary": "A new technology (in addition to ERP) is proposed to provide an increase of\nprofit and normal cash flow. This technology involves the next functions:\nforming of intellectual interface on a natural language to communicate with a\ncontrol system; joint planning of production and sales to get the maximal\nprofit; an adaptation of control system to internal and external events. The\nuse of the natural language permits to overcome a barrier between the control\nsystem and upper managers. To solve posed actual problems of management the\nselection of information from a database and call to mathematical methods are\nexecuted automatically. Optimal planning provides the maximal use of available\nresources and opportunities of market. Adaptive control implements the\nefficient reaction to critical events that lead up to a decrease of profit and\nincrease of accounts receivable.", 
    "link": "http://arxiv.org/pdf/1207.0313v1", 
    "arxiv-id": "1207.0313v1"
},{
    "category": "cs.CE", 
    "author": "Ri Suk Yun", 
    "title": "Hybrid Forecasting of Exchange Rate by Using Chaos Wavelet SVM-Markov   Model and Grey Relation Degree", 
    "publish": "2012-07-06T07:54:16Z", 
    "summary": "This paper proposes an exchange rate forecasting method by using the grey\nrelative combination approach of chaos wavelet SVM-Markov model. The problem of\nshort-term forecast of exchange rate by using the comprehensive method of the\nphase space reconstitution and SVM method has been researched. We have\nsuggested a wavelet-SVR-Markov forecasting model to predict the finance time\nseries and demonstrated that can more improve the forecasting performance by\nthe rational combination of the forecast results through various combinational\ntests. Our test result has been showed that the two-stage combination model is\nmore excellent than the normal combination model. Also we have comprehensively\nestimated the combination forecast methods according to the forecasting\nperformance indicators.The estimated result have been shown that the\ncombination forecast methods on the basic of the degree of grey relation and\nthe optimal grey relation combination have fine forecast performance.", 
    "link": "http://arxiv.org/pdf/1207.1547v1", 
    "arxiv-id": "1207.1547v1"
},{
    "category": "cs.CE", 
    "author": "Ri Suk Yun", 
    "title": "A Hybrid Forecast of Exchange Rate based on ARFIMA,Discrete Grey-Markov,   and Fractal Kalman Model", 
    "publish": "2012-07-09T02:08:26Z", 
    "summary": "We propose a hybrid forecast based on extended discrete grey Markov and\nvariable dimension Kalman model and show that our hybrid model can improve much\nmore the performance of forecast than traditional grey Markov and Kalman\nmodels. Our simulation results are given to demonstrate that our hybrid\nforecast method combined with degree of grey incidence are better than grey\nMarkov and ARFIMA model or Kalman methods.", 
    "link": "http://arxiv.org/pdf/1207.1933v1", 
    "arxiv-id": "1207.1933v1"
},{
    "category": "cs.CE", 
    "author": "Paolo Bientinesi", 
    "title": "High-throughput Genome-wide Association Analysis for Single and Multiple   Phenotypes", 
    "publish": "2012-07-09T20:25:26Z", 
    "summary": "The variance component tests used in genomewide association studies of\nthousands of individuals become computationally exhaustive when multiple traits\nare analysed in the context of omics studies. We introduce two high-throughput\nalgorithms -- CLAK-CHOL and CLAK-EIG -- for single and multiple phenotype\ngenome-wide association studies (GWAS). The algorithms, generated with the help\nof an expert system, reduce the computational complexity to the point that\nthousands of traits can be analyzed for association with millions of\npolymorphisms in a course of days on a standard workstation. By taking\nadvantage of problem specific knowledge, CLAK-CHOL and CLAK-EIG significantly\noutperform the current state-of-the-art tools in both single and multiple trait\nanalysis.", 
    "link": "http://arxiv.org/pdf/1207.2169v2", 
    "arxiv-id": "1207.2169v2"
},{
    "category": "cs.CE", 
    "author": "Ri Suk Yun", 
    "title": "A Hybrid Forecast of Exchange Rate based on Discrete Grey-Markov and   Grey Neural Network Model", 
    "publish": "2012-07-10T07:46:04Z", 
    "summary": "We propose a hybrid forecast model based on discrete grey-fuzzy Markov and\ngrey neural network model and show that our hybrid model can improve much more\nthe performance of forecast than traditional grey-Markov model and neural\nnetwork models. Our simulation results are shown that our hybrid forecast\nmethod with the combinational weight based on optimal grey relation degree\nmethod is better than the hybrid model with combinational weight based\nminimization of error-squared criterion.", 
    "link": "http://arxiv.org/pdf/1207.2254v1", 
    "arxiv-id": "1207.2254v1"
},{
    "category": "cs.CE", 
    "author": "T. V. Prasad", 
    "title": "Systematic and Integrative Analysis of Proteomic Data using   Bioinformatics Tools", 
    "publish": "2012-11-12T19:19:04Z", 
    "summary": "The analysis and interpretation of relationships between biological molecules\nis done with the help of networks. Networks are used ubiquitously throughout\nbiology to represent the relationships between genes and gene products. Network\nmodels have facilitated a shift from the study of evolutionary conservation\nbetween individual gene and gene products towards the study of conservation at\nthe level of pathways and complexes. Recent work has revealed much about\nchemical reactions inside hundreds of organisms as well as universal\ncharacteristics of metabolic networks, which shed light on the evolution of the\nnetworks. However, characteristics of individual metabolites have been\nneglected in this network. The current paper provides an overview of\nbioinformatics software used in visualization of biological networks using\nproteomic data, their main functions and limitations of the software.", 
    "link": "http://arxiv.org/pdf/1211.2743v1", 
    "arxiv-id": "1211.2743v1"
},{
    "category": "cs.CE", 
    "author": "J. L. Lacoume", 
    "title": "New algorithm for footstep localization using seismic sensors in an   indoor environment", 
    "publish": "2012-11-14T08:21:06Z", 
    "summary": "In this study, we consider the use of seismic sensors for footstep\nlocalization in indoor environments. A popular strategy of localization is to\nuse the measured differences in arrival times of source signals at multiple\npairs of receivers. In the literature, most algorithms that are based on time\ndifferences of arrival (TDOA) assume that the propagation velocity is a\nconstant as a function of the source position, which is valid for air\npropagation or even for narrow band signals. However a bounded medium such as a\nconcrete slab (encountered in indoor environement) is usually dispersive and\ndamped. In this study, we demonstrate that under such conditions, the concrete\nslab can be assimilated to a thin plate; considering a Kelvin-Voigt damping\nmodel, we introduce the notion of {\\em perceived propagation velocity}, which\ndecreases when the source-sensor distance increases. This peculiar behaviour\nprecludes any possibility to rely on existing localization methods in indoor\nenvironment. Therefore, a new localization algorithm that is adapted to a\ndamped and dispersive medium is proposed, using only on the sign of the\nmeasured TDOA (SO-TDOA). A simulation and some experimental results are\nincluded, to define the performance of this SO-TDOA algorithm.", 
    "link": "http://arxiv.org/pdf/1211.3233v2", 
    "arxiv-id": "1211.3233v2"
},{
    "category": "cs.CE", 
    "author": "Rituparna Chaki", 
    "title": "Application of Data mining in Protein sequence Classification", 
    "publish": "2012-11-20T02:59:28Z", 
    "summary": "Protein sequence classification involves feature selection for accurate\nclassification. Popular protein sequence classification techniques involve\nextraction of specific features from the sequences. Researchers apply some\nwell-known classification techniques like neural networks, Genetic algorithm,\nFuzzy ARTMAP,Rough Set Classifier etc for accurate classification. This paper\npresents a review is with three different classification models such as neural\nnetwork model, fuzzy ARTMAP model and Rough set classifier model. This is\nfollowed by a new technique for classifying protein sequences. The proposed\nmodel is typically implemented with an own designed tool and tries to reduce\nthe computational overheads encountered by earlier approaches and increase the\naccuracy of classification", 
    "link": "http://arxiv.org/pdf/1211.4654v1", 
    "arxiv-id": "1211.4654v1"
},{
    "category": "cs.CE", 
    "author": "Yuriy Ostapov", 
    "title": "Adaptive Control of Enterprise", 
    "publish": "2012-11-26T09:09:21Z", 
    "summary": "Modern progress in artificial intelligence permits to realize algorithms of\nadaptation for critical events (in addition to ERP). A production emergence, an\nappearance of new competitive goods, a major change in financial state of\npartners, a radical change in exchange rate, a change in custom and tax\nlegislation, a political and energy crisis, an ecocatastrophe can lead up to a\ndecrease of profit or bankruptcy of enterprise. Therefore it is necessary to\nassess a probability of threat and to take preventive actions. If a critical\nevent took place, one must estimate restoration expenses and possible\nconsequences as well as to prepare appropriate propositions. This is provided\nusing modern methods of diagnostics, prediction, and decision making as well as\nan inference engine and semantic analysis. Mathematical methods in use are\ncalled in algorithms of adaptation automatically. Because the enterprise is a\ncomplex system, to overcome complexity of control it is necessary to apply\nsemantic representations. Such representations are formed from descriptions of\nevents, facts, persons, organizations, goods, operations, scripts on a natural\nlanguage. Semantic representations permit as well to formulate actual problems\nand to find ways to resolve these problems.", 
    "link": "http://arxiv.org/pdf/1211.5890v1", 
    "arxiv-id": "1211.5890v1"
},{
    "category": "cs.CE", 
    "author": "Saket Srivastava", 
    "title": "Twitter Sentiment Analysis: How To Hedge Your Bets In The Stock Markets", 
    "publish": "2012-12-05T17:24:06Z", 
    "summary": "Emerging interest of trading companies and hedge funds in mining social web\nhas created new avenues for intelligent systems that make use of public opinion\nin driving investment decisions. It is well accepted that at high frequency\ntrading, investors are tracking memes rising up in microblogging forums to\ncount for the public behavior as an important feature while making short term\ninvestment decisions. We investigate the complex relationship between tweet\nboard literature (like bullishness, volume, agreement etc) with the financial\nmarket instruments (like volatility, trading volume and stock prices). We have\nanalyzed Twitter sentiments for more than 4 million tweets between June 2010\nand July 2011 for DJIA, NASDAQ-100 and 11 other big cap technological stocks.\nOur results show high correlation (upto 0.88 for returns) between stock prices\nand twitter sentiments. Further, using Granger's Causality Analysis, we have\nvalidated that the movement of stock prices and indices are greatly affected in\nthe short term by Twitter discussions. Finally, we have implemented Expert\nModel Mining System (EMMS) to demonstrate that our forecasted returns give a\nhigh value of R-square (0.952) with low Maximum Absolute Percentage Error\n(MaxAPE) of 1.76% for Dow Jones Industrial Average (DJIA). We introduce a novel\nway to make use of market monitoring elements derived from public mood to\nretain a portfolio within limited risk state (highly improved hedging bets)\nduring typical market conditions.", 
    "link": "http://arxiv.org/pdf/1212.1107v1", 
    "arxiv-id": "1212.1107v1"
},{
    "category": "cs.CE", 
    "author": "S. Guichard", 
    "title": "Interroom radiative couplings through windows and large openings in   buildings: Proposal of a simplified model", 
    "publish": "2012-12-17T08:17:14Z", 
    "summary": "A simplified model of indoor short wave radiation couplings adapted to\nmulti-zone simulations is proposed, thanks to a simplifying hypothesis and to\nthe introduction of an indoor short wave exchange matrix. The specific\nproperties of this matrix appear useful to quantify the thermal radiation\nexchanges between the zones separated by windows or large openings. Integrated\nin CODYRUN software, this module is detailed and compared to experimental\nmeasurements carried out on a real scale tropical building.", 
    "link": "http://arxiv.org/pdf/1212.3922v1", 
    "arxiv-id": "1212.3922v1"
},{
    "category": "cs.CE", 
    "author": "T. A. Mara", 
    "title": "Building ventilation: A pressure airflow model computer generation and   elements of validation", 
    "publish": "2012-12-17T08:20:43Z", 
    "summary": "The calculation of airflows is of great importance for detailed building\nthermal simulation computer codes, these airflows most frequently constituting\nan important thermal coupling between the building and the outside on one hand,\nand the different thermal zones on the other. The driving effects of air\nmovement, which are the wind and the thermal buoyancy, are briefly outlined and\nwe look closely at their coupling in the case of buildings, by exploring the\ndifficulties associated with large openings. Some numerical problems tied to\nthe resolving of the non-linear system established are also covered. Part of a\ndetailled simulation software (CODYRUN), the numerical implementation of this\nairflow model is explained, insisting on data organization and processing\nallowing the calculation of the airflows. Comparisons are then made between the\nmodel results and in one hand analytical expressions and in another and\nexperimental measurements in case of a collective dwelling.", 
    "link": "http://arxiv.org/pdf/1212.3924v1", 
    "arxiv-id": "1212.3924v1"
},{
    "category": "cs.CE", 
    "author": "J. C. Gatina", 
    "title": "Elaboration of global quality standards for natural and low energy   cooling in French tropical island buildings", 
    "publish": "2012-12-17T08:21:02Z", 
    "summary": "Electric load profiles of tropical islands in developed countries are\ncharacterised by morning, midday and evening peaks arising from all year round\nhigh power demand in the commercial and residential sectors, due mostly to air\nconditioning appliances and bad thermal conception of the building. The work\npresented in this paper has led to the conception of a global quality standards\nobtained through optimized bioclimatic urban planning and architectural design,\nthe use of passive cooling architectural components, natural ventilation and\nenergy efficient systems such as solar water heaters. We evaluated, with the\naid of an airflow and thermal building simulation software (CODYRUN), the\nimpact of each technical solution on thermal comfort within the building. These\ntechnical solutions have been implemented in 280 new pilot dwelling projects\nthrough the year 1996.", 
    "link": "http://arxiv.org/pdf/1212.3925v1", 
    "arxiv-id": "1212.3925v1"
},{
    "category": "cs.CE", 
    "author": "F. Garde", 
    "title": "A validation methodology aid for improving a thermal building model:   Case of diffuse radiation accounting in a tropical climate", 
    "publish": "2012-12-17T08:22:49Z", 
    "summary": "As part of our efforts to complete the software CODYRUN validation, we chose\nas test building a block of flats constructed in Reunion Island, which has a\nhumid tropical climate. The sensitivity analysis allowed us to study the\neffects of both diffuse and direct solar radiation on our model of this\nbuilding. With regard to the choice and location of sensors, this stage of the\nstudy also led us to measure the solar radiation falling on the windows. The\ncomparison of measured and predicted radiation clearly showed that our\npredictions over-estimated the incoming solar radiation, and we were able to\ntrace the problem to the algorithm which calculates diffuse solar radiation. By\ncalculating view factors between the windows and the associated shading\ndevices, changes to the original program allowed us to improve the predictions,\nand so this article shows the importance of sensitivity analysis in this area\nof research.", 
    "link": "http://arxiv.org/pdf/1212.3928v1", 
    "arxiv-id": "1212.3928v1"
},{
    "category": "cs.CE", 
    "author": "J. -C. Gatina", 
    "title": "Detailed weather data generator for building simulations", 
    "publish": "2012-12-17T08:23:27Z", 
    "summary": "Thermal buildings simulation softwares need meteorological files in thermal\ncomfort, energetic evaluation studies. Few tools can make significant\nmeteorological data available such as generated typical year, representative\ndays, or artificial meteorological database. This paper deals about the\npresentation of a new software, RUNEOLE, used to provide weather data in\nbuildings applications with a method adapted to all kind of climates. RUNEOLE\nassociates three modules of description, modelling and generation of weather\ndata. The statistical description of an existing meteorological database makes\ntypical representative days available and leads to the creation of model\nlibraries. The generation module leads to the generation of non existing\nsequences. This software tends to be usable for the searchers and designers, by\nmeans of interactivity, facilitated use and easy communication. The conceptual\nbasis of this tool will be exposed and we'll propose two examples of\napplications in building physics for tropical humid climates.", 
    "link": "http://arxiv.org/pdf/1212.3930v1", 
    "arxiv-id": "1212.3930v1"
},{
    "category": "cs.CE", 
    "author": "P. K. Dan", 
    "title": "Modelling of Optimal Design of Manufacturing Cell Layout Considering   Material Flow and Closeness Rating Factors", 
    "publish": "2012-12-20T15:47:50Z", 
    "summary": "Developing a group of machine cells and their corresponding part families to\nminimize the inter-cell and intra-cell material flow is the basic objective of\nthe designing of a cellular manufacturing system (CMS). Afterwards achieving a\ncompetent cell layout is essential in order to minimize the total inter-cell\npart travels, which is principally noteworthy. There are plentiful articles of\nCMS literature which considered cell formation problems; however cell layout\ntopic has rarely been addressed. Therefore this research is intended to focus\non an adapted mathematical model of the layout design problem considering\nmaterial handling cost and closeness ratings of manufacturing cells. Owing to\nthe combinatorial class of the said problem, an efficient NP-hard technique\nbased on Simulated Annealing metaheuristic is proposed henceforth. Some test\nproblems are solved using the proposed technique. Computational results show\nthat the proposed metaheuristic approach is extremely effective and efficient\nin terms of solution quality and computational complexity.", 
    "link": "http://arxiv.org/pdf/1212.5095v1", 
    "arxiv-id": "1212.5095v1"
},{
    "category": "cs.CE", 
    "author": "St\u00e9phane Guichard", 
    "title": "Development of a new model to predict indoor daylighting: Integration in   CODYRUN software and validation", 
    "publish": "2012-12-18T07:37:07Z", 
    "summary": "Many models exist in the scientific literature for determining indoor\ndaylighting values. They are classified in three categories: numerical,\nsimplified and empirical models. Nevertheless, each of these categories of\nmodels are not convenient for every application. Indeed, the numerical model\nrequires high calculation time; conditions of use of the simplified models are\nlimited, and experimental models need not only important financial resources\nbut also a perfect control of experimental devices (e.g. scale model), as well\nas climatic characteristics of the location (e.g. in situ experiment). In this\narticle, a new model based on a combination of multiple simplified models is\nestablished. The objective is to improve this category of model. The\noriginality of our paper relies on the coupling of several simplified models of\nindoor daylighting calculations. The accuracy of the simulation code,\nintroduced into CODYRUN software to simulate correctly indoor illuminance, is\nthen verified. Besides, the software consists of a numerical building\nsimulation code, developed in the Physics and Mathematical Engineering\nLaboratory for Energy and Environment (P.I.M.E.N.T) at the University of\nReunion. Initially dedicated to the thermal, airflow and hydrous phenomena in\nthe buildings, the software has been completed for the calculation of indoor\ndaylighting. New models and algorithms - which rely on a semi-detailed approach\n- will be presented in this paper. In order to validate the accuracy of the\nintegrated models, many test cases have been considered as analytical,\ninter-software comparisons and experimental comparisons. In order to prove the\naccuracy of the new model - which can properly simulate the illuminance - a\nconfrontation between the results obtained from the software (developed in this\nresearch paper) and the major made at a given place is described in details. A\nnew statistical indicator to appreciate the margins of errors - named RSD\n(Reliability of Software Degrees) - is also be defined.", 
    "link": "http://arxiv.org/pdf/1212.5253v1", 
    "arxiv-id": "1212.5253v1"
},{
    "category": "cs.CE", 
    "author": "Harry Boyer", 
    "title": "A Comparison between CODYRUN and TRNSYS, simulation models for thermal   buildings behaviour", 
    "publish": "2012-12-18T07:39:56Z", 
    "summary": "Simulation codes of thermal behaviour could significantly improve housing\nconstruction design. Among the existing software, CODYRUN and TRNSYS are\ncalculations codes of different conceptions. CODYRUN is exclusively dedicated\nto housing thermal behaviour, whereas TRNSYS is more generally used on any\nthermal system. The purpose of this article is to compare these two instruments\nin two different conditions . We will first modelize a mono-zone test cell, and\nanalyse the results by means of signal treatment methods. Then, we will\nmodelize a real case of multi-zone housing, representative of housing in wet\ntropical climates. We could so evaluate influences of meteorological and\nbuilding description data on model errors.", 
    "link": "http://arxiv.org/pdf/1212.5255v1", 
    "arxiv-id": "1212.5255v1"
},{
    "category": "cs.CE", 
    "author": "J. Brau", 
    "title": "Thermal Building Simulation and Computer Generation of Nodal Models", 
    "publish": "2012-12-18T12:37:04Z", 
    "summary": "The designer's preoccupation to reduce the energy needs and get a better\nthermal quality of ambiances helped in the development of several packages\nsimulating the dynamic behaviour of buildings. This paper shows the adaptation\nof a method of thermal analysis, the nodal analysis, linked to the case of\nbuilding's thermal behaviour. We take successively an interest in the case of\nconduction into a wall, in the coupling with superficial exchanges and finally\nin the constitution of thermal state models of the building. Big variations\nexisting from one building to another, it's necessary to build the thermal\nmodel from the building description. This article shows the chosen method in\nthe case of our thermal simulation program for buildings, CODYRUN", 
    "link": "http://arxiv.org/pdf/1212.5256v1", 
    "arxiv-id": "1212.5256v1"
},{
    "category": "cs.CE", 
    "author": "Ted Soubdhan", 
    "title": "Heat transfer in buildings : application to air solar heating and Trombe   wall design", 
    "publish": "2012-12-20T08:44:54Z", 
    "summary": "The aim of this paper is to briefly recall heat transfer modes and explain\ntheir integration within a software dedicated to building simulation (CODYRUN).\nDetailed elements of the validation of this software are presented and two\napplications are finally discussed. One concerns the modeling of a flat plate\nair collector and the second focuses on the modeling of Trombe solar walls. In\neach case, detailed modeling of heat transfer allows precise understanding of\nthermal and energetic behavior of the studied structures. Recent decades have\nseen a proliferation of tools for building thermal simulation. These\napplications cover a wide spectrum from very simplified steady state models to\ndynamic simulation ones, including computational fluid dynamics modules\n(Clarke, 2001). These tools are widely available in design offices and\nengineering firms. They are often used for the design of HVAC systems and still\nsubject to detailed research, particularly with respect to the integration of\nnew fields (specific insulation materials, lighting, pollutants transport,\netc.). Available from:\nhttp://www.intechopen.com/books/evaporation-condensation-and-heat-transfer/heat-transfer-in-buildings-application-to-solar-air-collector-and-trombe-wall-design", 
    "link": "http://arxiv.org/pdf/1212.5260v1", 
    "arxiv-id": "1212.5260v1"
},{
    "category": "cs.CE", 
    "author": "Jean Brau", 
    "title": "A multimodel approach to building thermal simulation for design and   research purposes", 
    "publish": "2012-12-20T13:25:56Z", 
    "summary": "The designers pre-occupation to reduce energy consumption and to achieve\nbetter thermal ambience levels, has favoured the setting up of numerous\nbuilding thermal dynamic simulation programs. The progress in the modelling of\nphenomenas and its transfer into the professional field has resulted in various\nnumerical approaches ranging from softwares dedicated to architects for design\nuse to tools for laboratory use by the expert thermal researcher. This analysis\nshows that each approach tends to fulfil the specific needs of a certain kind\nof manipulator only, in the building conception process. Our objective is\nnotably different as it is a tool which can be used from the very initial stage\nof a construction project, to the energy audit for the existing building. In\neach of these cases, the objective results, the precision advocated and the\ntime delay of the results are different parameters which call for a multiple\nmodel approach of the building system", 
    "link": "http://arxiv.org/pdf/1212.5262v1", 
    "arxiv-id": "1212.5262v1"
},{
    "category": "cs.CE", 
    "author": "Anis Youn\u00e8s", 
    "title": "Use of BESTEST procedure to improve a building thermal simulation   program", 
    "publish": "2012-12-20T13:28:22Z", 
    "summary": "Validation of building energy simulation programs is of major interest to\nboth users and modellers. To achieve such a task, it is essential to apply a\nmethodology based on a priori test and empirical validation. A priori test\nconsists in verifying that models embedded in a program and their\nimplementation are correct. this should be achieved before carrying out\nexperiments. The aim of this report is to present results from the application\nof the BESTEST procedure to our code. We will emphasise the way it allows to\nfind bugs in our program and also how it permits to qualify models of heat\ntransfer by conduction", 
    "link": "http://arxiv.org/pdf/1212.5263v1", 
    "arxiv-id": "1212.5263v1"
},{
    "category": "cs.CE", 
    "author": "Fabien Moutarde", 
    "title": "Statistical Traffic State Analysis in Large-scale Transportation   Networks Using Locality-Preserving Non-negative Matrix Factorization", 
    "publish": "2012-12-20T14:53:44Z", 
    "summary": "Statistical traffic data analysis is a hot topic in traffic management and\ncontrol. In this field, current research progresses focus on analyzing traffic\nflows of individual links or local regions in a transportation network. Less\nattention are paid to the global view of traffic states over the entire\nnetwork, which is important for modeling large-scale traffic scenes. Our aim is\nprecisely to propose a new methodology for extracting spatio-temporal traffic\npatterns, ultimately for modeling large-scale traffic dynamics, and long-term\ntraffic forecasting. We attack this issue by utilizing Locality-Preserving\nNon-negative Matrix Factorization (LPNMF) to derive low-dimensional\nrepresentation of network-level traffic states. Clustering is performed on the\ncompact LPNMF projections to unveil typical spatial patterns and temporal\ndynamics of network-level traffic states. We have tested the proposed method on\nsimulated traffic data generated for a large-scale road network, and reported\nexperimental results validate the ability of our approach for extracting\nmeaningful large-scale space-time traffic patterns. Furthermore, the derived\nclustering results provide an intuitive understanding of spatial-temporal\ncharacteristics of traffic flows in the large-scale network, and a basis for\npotential long-term forecasting.", 
    "link": "http://arxiv.org/pdf/1212.5264v1", 
    "arxiv-id": "1212.5264v1"
},{
    "category": "cs.CE", 
    "author": "Pranab K Dan", 
    "title": "An Effective Machine-Part Grouping Algorithm to Construct Manufacturing   Cells", 
    "publish": "2012-12-20T15:51:13Z", 
    "summary": "The machine-part cell formation problem consists of creating machine cells\nand their corresponding part families with the objective of minimizing the\ninter-cell and intra-cell movement while maximizing the machine utilization.\nThis article demonstrates a hybrid clustering approach for the cell formation\nproblem in cellular manufacturing that conjoins Sorenson s similarity\ncoefficient based method to form the production cells. Computational results\nare shown over the test datasets obtained from the past literature. The hybrid\ntechnique is shown to outperform the other methods proposed in literature and\nincluding powerful soft computing approaches such as genetic algorithms,\ngenetic programming by exceeding the solution quality on the test problems.", 
    "link": "http://arxiv.org/pdf/1212.5265v1", 
    "arxiv-id": "1212.5265v1"
},{
    "category": "cs.CE", 
    "author": "Franck Lucas", 
    "title": "A Picard Newton method to solve non linear airflow networks", 
    "publish": "2012-12-20T21:19:35Z", 
    "summary": "In detailled buiding simulation models, airflow modelling and solving are\nstill open and crucial problems, specially in the case of open buildings as\nencountered in tropical climates. As a consequence, wind speed conditioning\nindoor thermal comfort or energy needs in case of air conditionning are uneasy\nto predict. A first part of the problem is the lack of reliable and usable\nlarge opening elementary modelling and another one concerns the numerical\nsolving of airflow network. This non linear pressure system is solved by\nnumerous methods mainly based on Newton Raphson (NR) method. This paper is\nadressing this part of the difficulty, in our software CODYRUN. After model\nchecks, we propose to use Picard method (known also as fixed point) to\ninitialise zone pressures. A linear system (extracted from the non linear set\nof equations) is solved around 10 times at each time step and NR uses this\nresult for initial values. Known to be uniformly but slowly convergent, this\nmethod appears to be really powerful for the building pressure system. The\ncomparison of the methods in terms of number of iterations is illustrated using\na real test case experiment.", 
    "link": "http://arxiv.org/pdf/1212.5275v1", 
    "arxiv-id": "1212.5275v1"
},{
    "category": "cs.CE", 
    "author": "Alfred Jean Philippe Lauret", 
    "title": "CODYRUN, outil de simulation et d'aide \u00e0 la conception   thermo-a\u00e9raulique de b\u00e2timents", 
    "publish": "2012-12-20T21:21:41Z", 
    "summary": "This article presents the CODYRUN software developped by University of La\nR\\'eunion. It is a multizone thermal software, with detailled airflow and\nhumidity transfer calculations. One of its specific aspects is that it\nconstitutes a research tool, a design tool used by the lab and professionnals\nand also a teaching tool. After a presentation of the multiple model aspect,\nsome details of the tree modules associated to physical phenomenons are given.\nElements of validation are exposed in next paraghaph, and then a few details of\nthe front end.", 
    "link": "http://arxiv.org/pdf/1212.5589v1", 
    "arxiv-id": "1212.5589v1"
},{
    "category": "cs.CE", 
    "author": "Jean Brau", 
    "title": "Multiple model software for airflow and thermal building simulation. A   case study under tropical humid climate, in R\u00e9union Island", 
    "publish": "2012-12-21T13:50:14Z", 
    "summary": "The first purpose of our work has been to allow -as far as heat transfer\nmodes, airflow calculation and meteorological data reconstitution are\nconcerned- the integration of diverse interchangeable physical models in a\nsingle software tool for professional use, CODYRUN. The designer's objectives,\nprecision requested and calculation time consideration, lead us to design a\nstructure accepting selective use of models, taking into account multizone\ndescription and airflow patterns. With a building case study in Reunion Island,\nwe first analyse the sensibility of the thermal model to diffuse radiation\nreconstitution on tilted surfaces. Then, a realistic balance between precision\nrequired and calculation time leads us to select detailed models for the zone\nof main interest, but to choose simplified models for the other zones.", 
    "link": "http://arxiv.org/pdf/1212.5592v1", 
    "arxiv-id": "1212.5592v1"
},{
    "category": "cs.CE", 
    "author": "Harry Boyer", 
    "title": "Time-variant Linear reduction model approximation : application to   thermal and airflow building simulation", 
    "publish": "2012-12-21T13:52:10Z", 
    "summary": "Considering the natural ventilation, the thermal behavior of buildings can be\ndescribed by a linear time varying model. In this paper, we describe an\nimplementation of model reduction of linear time varying systems. We show the\nconsequences of the model reduction on computing time and accuracy. Finally, we\ncompare experimental measures and simulation results using the initial model or\nthe reduced model. The reduced model shows negligible difference in accuracy,\nand the computing time shortens.", 
    "link": "http://arxiv.org/pdf/1212.5593v1", 
    "arxiv-id": "1212.5593v1"
},{
    "category": "cs.CE", 
    "author": "Jean Claude Gatina", 
    "title": "Elaboration of a new tool for weather data sequences generation", 
    "publish": "2012-12-21T18:55:04Z", 
    "summary": "This paper deals about the presentation of a new software RUNEOLE used to\nprovide weather data in buildings physics. RUNEOLE associates three modules\nleading to the description, the modelling and the generation of weather data.\nThe first module is dedicated to the description of each climatic variable\nincluded in the database. Graphic representation is possible (with histograms\nfor example). Mathematical tools used to compare statistical distributions,\ndetermine daily characteristic evolutions, find typical days, and the\ncorrelations between the different climatic variables have been elaborated in\nthe second module. Artificial weather datafiles adapted to different simulation\ncodes are available at the issue of the third module. This tool can then be\nused in HVAC system evaluation, or in the study of thermal comfort. The studied\nbuildings can then be tested under different thermal, aeraulic, and radiative\nsolicitations, leading to a best understanding of their behaviour for example\nin humid climates.", 
    "link": "http://arxiv.org/pdf/1212.5599v1", 
    "arxiv-id": "1212.5599v1"
},{
    "category": "cs.CE", 
    "author": "Jean Claude Gatina", 
    "title": "Weather sequences for predicting HVAC system behaviour in residential   units located in tropical climates", 
    "publish": "2012-12-22T07:40:43Z", 
    "summary": "The purpose of our research deals with the description of a methodology for\nthe definition of specific weather sequences and their influence on the energy\nneeds of HVAC system. We'll apply the method on the tropical Reunion Island.\nThe methodological approach based on a detailed analysis of weather sequences\nleads to a classification of climatic situations that can be applied to the\nsite. These sequences have been used to simulate buildings and air handling\nsystems thanks to a thermal simulation code, CODYRUN. Results bring to the\nlight how necessary it is to have coherent meteorological data for this kind of\nsimulation.", 
    "link": "http://arxiv.org/pdf/1212.5664v1", 
    "arxiv-id": "1212.5664v1"
},{
    "category": "cs.CE", 
    "author": "Jean Brau", 
    "title": "Multiple model approach and experimental validation of a residential   air-to-air heat pump", 
    "publish": "2012-12-22T07:42:21Z", 
    "summary": "The beginning of this work is the achievement of a design tool, which is a\nmultiple model software called \" CODYRUN \", suitable for professionnals and\nusable by researchers. The original aspect of this software is that the\ndesigner has at his disposal a wide panel of choices between different heat\ntransfer models More precisely, it consists in a multizone software integrating\nboth natural ventilation and moisture tranfers . This software is developed on\nPC micro computer and gets advantage of the Microsoft WINDOWS front-end. Most\nof time, HVAC systems and specially domestic air conditioners, are taken into\naccount in a very simplified way, or in a elaborated one. On one side,they are\njust supposed to supply the demand of cooling loads with an ideal control loop\n(no delay between the sollicitations and the time response of the system), The\navailable outputs are initially the hourly cooling and heating consumptions\nwithout integrating the real caracteristics of the HVAC system This paper is\nalso following the same multiple model approach than for the building modelling\nby defining different modelling levels for the air conditionning systems, from\na very simplified one to a detailled one. An experimental validation is\nachieved in order to compare the sensitivity of each defined model and to point\nout the interaction between the thermal behaviour of the envelop and the\nelectrical system consumption. For validation purposes, we will describe the\ndata acquisition system. and the used real size test cell located in the\nUniversity of Reunion island, Indian Ocean.", 
    "link": "http://arxiv.org/pdf/1212.5665v1", 
    "arxiv-id": "1212.5665v1"
},{
    "category": "cs.CE", 
    "author": "Carlos A. Cruz-Villar", 
    "title": "A Dual Number Approach for Numerical Calculation of Velocity and   Acceleration in the Spherical 4R Mechanism", 
    "publish": "2013-01-08T04:40:27Z", 
    "summary": "This paper proposes a methodology to calculate both the first and second\nderivatives of a vector function of one variable in a single computation step.\nThe method is based on the nested application of the dual number approach for\nfirst order derivatives.\n  It has been implemented in Fortran language, a module which contains the dual\nversion of elementary functions as well as more complex functions, which are\ncommon in the field of rotational kinematics. Since we have three quantities of\ninterest, namely the function itself and its first and second derivative, our\nbasic numerical entity has three elements. Then, for a given vector function\n$f:\\mathbb{R}\\to \\mathbb{R}^m$, its dual version will have the form\n$\\tilde{f}:\\mathbb{R}^3\\to \\mathbb{R}^{3m}$.\n  As a study case, the proposed methodology is used to calculate the velocity\nand acceleration of a point moving on the coupler-point curve generated by a\nspherical four-bar mechanism.", 
    "link": "http://arxiv.org/pdf/1301.1409v3", 
    "arxiv-id": "1301.1409v3"
},{
    "category": "cs.CE", 
    "author": "Daniel Play", 
    "title": "A discrete analysis of metal-v belt drive", 
    "publish": "2013-01-20T17:52:48Z", 
    "summary": "The metal-V belt drive includes a large number of parts which interact\nbetween them to transmit power from the input to the output pulleys. A\ncompression belt composed of a great number of struts is maintained by a\ntension flat belt. Power is them shared into the two belts that moves generally\nin opposite directions. Due to the particular geometry of the elements and to\nthe great number of parts, a numerical approach achieves the global equilibrium\nof the mechanism from the elementary part equilibrium. Sliding arc on each\npulley can be thus defined both for the compression and tension belts. Finally,\npower sharing can be calculated as differential motion between the belts, is\ndefined. The first part of the paper will present the different steps of the\nquasi-static mechanical analysis and their numerical implementations. Load\ndistributions, speed profiles and sliding angle values will be discussed. The\nsecond part of the paper will deal to a systematic use of the computer\nsoftware. Speed ratio, transmitted torque, strut geometry and friction\ncoefficients effect will be analysed with the output parameter variations.\nFinally, the effect pulley deformable flanges will be discussed.", 
    "link": "http://arxiv.org/pdf/1301.5595v1", 
    "arxiv-id": "1301.5595v1"
},{
    "category": "cs.CE", 
    "author": "Anar Auda Ablahad", 
    "title": "Novel Method for Mutational Disease Prediction using Bioinformatics   Techniques and Backpropagation Algorithm", 
    "publish": "2013-03-03T18:12:23Z", 
    "summary": "Cancer is one of the most feared diseases in the world it has increased\ndisturbingly and breast cancer occurs in one out of eight women, the prediction\nof malignancies plays essential roles not only in revealing human genome, but\nalso in discovering effective prevention and treatment of cancers. Generally\ncancer disease driven by somatic mutations in an individual DNA sequence, or\ngenome that accumulates during the lifetime of person. This paper is proposed a\nnovel method can predict the disease by mutations despite The presence in gene\nsequence is not necessary it are malignant, so will be compare the protein of\npatient with the gene's protein of disease if there is difference between these\ntwo proteins then can say there is malignant mutations. This method will use\nbioinformatics techniques like FASTA, CLUSTALW, etc which shows whether\nmalignant mutations or not, then training the backpropagation algorithm using\nall expected malignant mutations for a certain genes (e.g. BRCA1 and BRCA2) of\ndisease, and using it to test whether patient is holder the disease or not.\nImplementing this novel method as the first way to predict the disease based on\nmutations in the sequence of the gene that causes the disease shows two\ndecisions are achieved successfully, the first diagnose whether the patient has\nmutations of cancer or not using bioinformatics techniques the second\nclassifying these mutations are related to breast cancer (e.g. BRCA1 and BRCA2)\nusing backpropagation with mean square rate 0.0000001. Keywords-Gene sequence;\nProtein; Deoxyribonucleic Acid DNA; Malignant mutation; Bioinformatics;\nBack-propagation algorithm.", 
    "link": "http://arxiv.org/pdf/1303.0539v1", 
    "arxiv-id": "1303.0539v1"
},{
    "category": "cs.CE", 
    "author": "Karim Abed-Meraim", 
    "title": "Exact Conditional and Unconditional Cram\u00e8r-Rao Bounds for Near Field   Localization", 
    "publish": "2013-03-07T15:54:47Z", 
    "summary": "This paper considers the Cram\\`er-Rao lower Bound (CRB) for the source\nlocalization problem in the near field. More specifically, we use the exact\nexpression of the delay parameter for the CRB derivation and show how this\nexact CRB can be significantly different from the one given in the literature\nand based on an approximate time delay expression (usually considered in the\nFresnel region). This CRB derivation is then generalized by considering the\nexact expression of the received power profile (i.e., variable gain case)\nwhich, to our best knowledge, has been ignored in the literature. Finally, we\nexploit the CRB expression to introduce the new concept of Near Field\nLocalization (NFL) region for a target localization performance associated to\nthe application at hand. We illustrate the usefulness of the proposed CRB\nderivation and its developments as well as the NFL region concept through\nnumerical simulations in different scenarios.", 
    "link": "http://arxiv.org/pdf/1303.1725v2", 
    "arxiv-id": "1303.1725v2"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Fast Computation of the Series Impedance of Power Cables with Inclusion   of Skin and Proximity Effects", 
    "publish": "2013-03-21T20:18:16Z", 
    "summary": "We present an efficient numerical technique for calculating the series\nimpedance matrix of systems with round conductors. The method is based on a\nsurface admittance operator in combination with the method of moments and it\naccurately predicts both skin and proximity effects. Application to a\nthree-phase armored cable with wire screens demonstrates a speed-up by a factor\nof about 100 compared to a finite elements computation. The inclusion of\nproximity effect in combination with the high efficiency makes the new method\nvery attractive for cable modeling within EMTP-type simulation tools.\nCurrently, these tools can only take skin effect into account.", 
    "link": "http://arxiv.org/pdf/1303.5452v2", 
    "arxiv-id": "1303.5452v2"
},{
    "category": "cs.CE", 
    "author": "Michal \u0160ejnoha", 
    "title": "Numerical model of elastic laminated glass beams under finite strain", 
    "publish": "2013-03-25T21:12:55Z", 
    "summary": "Laminated glass structures are formed by stiff layers of glass connected with\na compliant plastic interlayer. Due to their slenderness and heterogeneity,\nthey exhibit a complex mechanical response that is difficult to capture by\nsingle-layer models even in the elastic range. The purpose of this paper is to\nintroduce an efficient and reliable finite element approach to the simulation\nof the immediate response of laminated glass beams. It proceeds from a refined\nplate theory due to Mau (1973), as we treat each layer independently and\nenforce the compatibility by the Lagrange multipliers. At the layer level, we\nadopt the finite-strain shear deformable formulation of Reissner (1972) and the\nnumerical framework by Ibrahimbegovi\\'{c} and Frey (1993). The resulting system\nis solved by the Newton method with consistent linearization. By comparing the\nmodel predictions against available experimental data, analytical methods and\ntwo-dimensional finite element simulations, we demonstrate that the proposed\nformulation is reliable and provides accuracy comparable to the detailed\ntwo-dimensional finite element analyzes. As such, it offers a convenient basis\nto incorporate more refined constitutive description of the interlayer.", 
    "link": "http://arxiv.org/pdf/1303.6314v2", 
    "arxiv-id": "1303.6314v2"
},{
    "category": "cs.CE", 
    "author": "Maria V. Vasilyeva", 
    "title": "Mathematical modeling of thermal stabilization of vertical wells on high   performance computing systems", 
    "publish": "2013-04-05T07:13:58Z", 
    "summary": "Temperature stabilization of oil and gas wells is used to ensure stability\nand prevent deformation of a subgrade estuary zone. In this work, we consider\nthe numerical simulation of thermal stabilization using vertical seasonal\nfreezing columns.\n  A mathematical model of such problems is described by a time-dependent\ntemperature equation with phase transitions from water to ice. The resulting\nequation is a standard nonlinear parabolic equation.\n  Numerical implementation is based on the finite element method using the\npackage Fenics. After standard purely implicit approximation in time and simple\nlinearization, we obtain a system of linear algebraic equations. Because the\nsize of freezing columns are substantially less than the size of the modeled\narea, we obtain mesh refinement near columns. Due to this, we get a large\nsystem of equations which are solved using high performance computing systems.", 
    "link": "http://arxiv.org/pdf/1304.1625v1", 
    "arxiv-id": "1304.1625v1"
},{
    "category": "cs.CE", 
    "author": "Eric Tannier", 
    "title": "On sampling SCJ rearrangement scenarios", 
    "publish": "2013-04-08T11:22:12Z", 
    "summary": "The Single Cut or Join (SCJ) operation on genomes, generalizing chromosome\nevolution by fusions and fissions, is the computationally simplest known model\nof genome rearrangement. While most genome rearrangement problems are already\nhard when comparing three genomes, it is possible to compute in polynomial time\na most parsimonious SCJ scenario for an arbitrary number of genomes related by\na binary phylogenetic tree.\n  Here we consider the problems of sampling and counting the most parsimonious\nSCJ scenarios. We show that both the sampling and counting problems are easy\nfor two genomes, and we relate SCJ scenarios to alternating permutations.\nHowever, for an arbitrary number of genomes related by a binary phylogenetic\ntree, the counting and sampling problems become hard. We prove that if a Fully\nPolynomial Randomized Approximation Scheme or a Fully Polynomial Almost Uniform\nSampler exist for the most parsimonious SCJ scenario, then RP = NP.\n  The proof has a wider scope than genome rearrangements: the same result holds\nfor parsimonious evolutionary scenarios on any set of discrete characters.", 
    "link": "http://arxiv.org/pdf/1304.2170v1", 
    "arxiv-id": "1304.2170v1"
},{
    "category": "cs.CE", 
    "author": "M. Leps", 
    "title": "Soft computing-based calibration of microplane M4 model parameters:   Methodology and validation", 
    "publish": "2013-04-19T08:11:55Z", 
    "summary": "Constitutive models for concrete based on the microplane concept have\nrepeatedly proven their ability to well-reproduce its non-linear response on\nmaterial as well as structural scales. The major obstacle to a routine\napplication of this class of models is, however, the calibration of\nmicroplane-related constants from macroscopic data. The goal of this paper is\ntwo-fold: (i) to introduce the basic ingredients of a robust inverse procedure\nfor the determination of dominant parameters of the M4 model proposed by Bazant\nand co-workers based on cascade Artificial Neural Networks trained by\nEvolutionary Algorithm and (ii) to validate the proposed methodology against a\nrepresentative set of experimental data. The obtained results demonstrate that\nthe soft computing-based method is capable of delivering the searched response\nwith an accuracy comparable to the values obtained by expert users.", 
    "link": "http://arxiv.org/pdf/1304.6099v2", 
    "arxiv-id": "1304.6099v2"
},{
    "category": "cs.CE", 
    "author": "A. Pospelov", 
    "title": "Lay-up Optimization of Laminated Composites: Mixed Approach with Exact   Feasibility Bounds on Lamination Parameters", 
    "publish": "2013-04-26T16:48:48Z", 
    "summary": "We suggest modified bi-level approach for finding the best stacking sequence\nof laminated composite structures subject to mechanical, blending and\nmanufacturing constraints. We propose to use both the number of plies laid up\nat predefined angles and lamination parameters as independent variables at\nouter (global) stage of bi-level scheme aimed to satisfy buckling, strain and\npercentage constraints. Our formulation allows precise definition of the\nfeasible region of lamination parameters and greatly facilitates the solution\nof inner level problem of finding the optimal stacking sequence.", 
    "link": "http://arxiv.org/pdf/1304.7226v1", 
    "arxiv-id": "1304.7226v1"
},{
    "category": "cs.CE", 
    "author": "Naveed Arshad", 
    "title": "An Empirical Investigation of V-I Trajectory based Load Signatures for   Non-Intrusive Load Monitoring", 
    "publish": "2013-05-02T23:32:00Z", 
    "summary": "Choice of load signature or feature space is one of the most fundamental\ndesign choices for non-intrusive load monitoring or energy disaggregation\nproblem. Electrical power quantities, harmonic load characteristics, canonical\ntransient and steady-state waveforms are some of the typical choices of load\nsignature or load signature basis for current research addressing appliance\nclassification and prediction. This paper expands and evaluates appliance load\nsignatures based on V-I trajectory - the mutual locus of instantaneous voltage\nand current waveforms - for precision and robustness of prediction in\nclassification algorithms used to disaggregate residential overall energy use\nand predict constituent appliance profiles. We also demonstrate the use of\nvariants of differential evolution as a novel strategy for selection of optimal\nload models in context of energy disaggregation. A publicly available benchmark\ndataset REDD is employed for evaluation purposes. Our experimental evaluations\nindicate that these load signatures, in conjunction with a number of popular\nclassification algorithms, offer better or generally comparable overall\nprecision of prediction, robustness and reliability against dynamic, noisy and\nhighly similar load signatures with reference to electrical power quantities\nand harmonic content. Herein, wave-shape features are found to be an effective\nnew basis of classification and prediction for semi-automated energy\ndisaggregation and monitoring.", 
    "link": "http://arxiv.org/pdf/1305.0596v1", 
    "arxiv-id": "1305.0596v1"
},{
    "category": "cs.CE", 
    "author": "Tommaso Urli", 
    "title": "json2run: a tool for experiment design & analysis", 
    "publish": "2013-05-06T08:31:48Z", 
    "summary": "json2run is a tool to automate the running, storage and analysis of\nexperiments. The main advantage of json2run is that it allows to describe a set\nof experiments concisely as a JSON-formatted parameter tree. It also supports\nparallel execution of experiments, automatic parameter tuning through the\nF-Race framework and storage and analysis of experiments with MongoDB and R.", 
    "link": "http://arxiv.org/pdf/1305.1112v1", 
    "arxiv-id": "1305.1112v1"
},{
    "category": "cs.CE", 
    "author": "Harry Boyer", 
    "title": "Simulation of a typical house in the region of Antananarivo, Madagascar.   Determination of passive solutions using local materials", 
    "publish": "2013-05-10T12:16:41Z", 
    "summary": "This paper deals with new proposals for the design of passive solutions\nadapted to the climate of the highlands of Madagascar. While the strongest\npopulation density is located in the central highlands, the problem of thermal\ncomfort in buildings occurs mainly during winter time. Currently, people use\nraw wood to warm the poorly designed houses. This leads to a large scale\ndeforestation of the areas and causes erosion and environmental problems. The\nmethodology used consisted of the identification of a typical building and of a\ntypical meteorological year. Simulations were carried out using a thermal and\nairflow software (CODYRUN) to improve each building component (roof, walls,\nwindows, and soil) in such a way as to estimate the influence of some technical\nsolutions on each component in terms of thermal comfort. The proposed solutions\nalso took into account the use of local materials and the standard of living of\nthe country.", 
    "link": "http://arxiv.org/pdf/1305.2322v1", 
    "arxiv-id": "1305.2322v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Evaluating Different Cost-Benefit Analysis Methods for Port Security   Operations", 
    "publish": "2013-05-31T14:36:59Z", 
    "summary": "Service industries, such as ports, are attentive to their standards, a smooth\nservice flow and economic viability. Cost benefit analysis has proven itself as\na useful tool to support this type of decision making; it has been used by\nbusinesses and governmental agencies for many years. In this book chapter we\ndemonstrate different modelling methods that are used for estimating input\nfactors required for conducting cost benefit analysis based on a single case\nstudy. These methods are: scenario analysis, decision trees, Monte-Carlo\nsimulation modelling and discrete event simulation modelling. Our aims are, on\nthe one hand, to guide the analyst through the modelling processes and, on the\nother hand, to demonstrate what additional decision support information can be\nobtained from applying each of these modelling methods.", 
    "link": "http://arxiv.org/pdf/1305.7422v1", 
    "arxiv-id": "1305.7422v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Investigating the effectiveness of Variance Reduction Techniques in   Manufacturing, Call Center and Cross-docking Discrete Event Simulation Models", 
    "publish": "2013-05-31T14:39:34Z", 
    "summary": "Variance reduction techniques have been shown by others in the past to be a\nuseful tool to reduce variance in Simulation studies. However, their\napplication and success in the past has been mainly domain specific, with\nrelatively little guidelines as to their general applicability, in particular\nfor novices in this area. To facilitate their use, this study aims to\ninvestigate the robustness of individual techniques across a set of scenarios\nfrom different domains. Experimental results show that Control Variates is the\nonly technique which achieves a reduction in variance across all domains.\nFurthermore, applied individually, Antithetic Variates and Control Variates\nperform particularly well in the Cross-docking scenarios, which was previously\nunknown.", 
    "link": "http://arxiv.org/pdf/1305.7424v1", 
    "arxiv-id": "1305.7424v1"
},{
    "category": "cs.CE", 
    "author": "Ion Petre", 
    "title": "Proceedings Fourth International Workshop on Computational Models for   Cell Processes", 
    "publish": "2013-06-09T13:42:56Z", 
    "summary": "The fourth international workshop on Computational Models for Cell Processes\n(CompMod 2013) took place on June 11, 2013 at the {\\AA}bo Akademi University,\nTurku, Finland, in conjunction with iFM 2013. The first edition of the workshop\n(2008) took place in Turku, Finland, in conjunction with Formal Methods 2008,\nthe second edition (2009) took place in Eindhoven, the Netherlands, as well in\nconjunction with Formal Methods 2009, and the third one took place in Aachen,\nGermany, in conjunction with CONCUR 2013. This volume contains the final\nversions of all contributions accepted for presentation at the workshop.\n  The goal of the CompMod workshop series is to bring together researchers in\nComputer Science and Mathematics (both discrete and continuous), interested in\nthe opportunities and the challenges of Systems Biology. The Program Committee\nof CompMod 2013 selected 3 papers for presentation at the workshop. In\naddition, we had two invited talks and five informal presentations.\n  The scientific program of the workshop spans an interesting mix of approaches\nto systems and even synthetic biology, encompassing several different modeling\napproaches, ranging from quantitative to qualitative techniques, from\ncontinuous to discrete mathematics, and from deterministic to stochastic\nmethods. We thank our invited speakers Daniela Besozzi (Universita degli Studi\ndi Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for\naccepting our invitation and for presenting some of their recent results at\nCompMod 2013.\n  The technical contributions address the mathematical modeling of the PDGF\nsignalling pathway, the canonical labelling of site graphs, rule-based modeling\nof polymerization reactions, rule-based modeling as a platform for the analysis\nof synthetic self-assembled nano-systems, robustness analysis of stochastic\nsystems, an algebraic approach to gene assembly in ciliates, and large-scale\ntext mining of biomedical literature.", 
    "link": "http://arxiv.org/pdf/1306.2019v1", 
    "arxiv-id": "1306.2019v1"
},{
    "category": "cs.CE", 
    "author": "Plamen L. Simeonov", 
    "title": "On Some Recent Insights in Integral Biomathics", 
    "publish": "2013-06-11T05:45:57Z", 
    "summary": "This paper summarizes the results in Integral Biomathics obtained to this\nmoment and provides an outlook for future research in the field.", 
    "link": "http://arxiv.org/pdf/1306.2843v3", 
    "arxiv-id": "1306.2843v3"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Proximity-Aware Calculation of Cable Series Impedance for Systems of   Solid and Hollow Conductors", 
    "publish": "2013-06-13T02:49:13Z", 
    "summary": "Wide-band cable models for the prediction of electromagnetic transients in\npower systems require the accurate calculation of the cable series impedance as\nfunction of frequency. A surface current approach was recently proposed for\nsystems of round solid conductors, with inclusion of skin and proximity\neffects. In this paper we extend the approach to include tubular conductors,\nallowing to model realistic cables with tubular sheaths, armors and pipes. We\nalso include the effect of a lossy ground. A noteworthy feature of the proposed\ntechnique is the accurate prediction of proximity effects, which can be of\nmajor importance in three-phase, pipe type, and closely-packed single-core\ncables. The new approach is highly efficient compared to finite elements. In\nthe case of a cross-bonded cable system featuring three phase conductors and\nthree screens, the proposed technique computes the required 120 frequency\nsamples in only six seconds of CPU time.", 
    "link": "http://arxiv.org/pdf/1306.3011v2", 
    "arxiv-id": "1306.3011v2"
},{
    "category": "cs.CE", 
    "author": "Xun Qiu", 
    "title": "Application of particle swarm optimization for enhanced cyclic steam   stimulation in a offshore heavy oil reservoir", 
    "publish": "2013-06-18T07:42:20Z", 
    "summary": "Three different variations of PSO algorithms, i.e. Canonical, Gaussian\nBare-bone and L\\'evy Bare-bone PSO, are tested to optimize the ultimate oil\nrecovery of a large heavy oil reservoir. The performance of these algorithms\nwas compared in terms of convergence behaviour and the final optimization\nresults. It is found that, in general, all three types of PSO methods are able\nto improve the objective function. The best objective function is found by\nusing the Canonical PSO, while the other two methods give similar results. The\nGaussian Bare-bone PSO may picks positions that are far away from the optimal\nsolution. The L\\'evy Bare-bone PSO has similar convergence behaviour as the\nCanonical PSO. For the specific optimization problem investigated in this\nstudy, it is found that the temperature of the injection steam, CO2 composition\nin the injection gas, and the gas injection rates have bigger impact on the\nobjective function, while steam injection rate and the liquid production rate\nhave less impact on the objective function.", 
    "link": "http://arxiv.org/pdf/1306.4092v1", 
    "arxiv-id": "1306.4092v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Towards modelling cost and risks of infrequent events in the cargo   screening process", 
    "publish": "2013-06-21T14:49:29Z", 
    "summary": "We introduce a simulation model of the port of Calais with a focus on the\noperation of immigration controls. Our aim is to compare the cost and benefits\nof different screening policies. Methodologically, we are trying to understand\nthe limits of discrete event simulation of rare events. When will they become\n'too rare' for simulation to give meaningful results?", 
    "link": "http://arxiv.org/pdf/1306.5160v1", 
    "arxiv-id": "1306.5160v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Simulating the Dynamics of T Cell Subsets Throughout the Lifetime", 
    "publish": "2013-07-02T16:19:54Z", 
    "summary": "It is widely accepted that the immune system undergoes age-related changes\ncorrelating with increased disease in the elderly. T cell subsets have been\nimplicated. The aim of this work is firstly to implement and validate a\nsimulation of T regulatory cell (Treg) dynamics throughout the lifetime, based\non a model by Baltcheva. We show that our initial simulation produces an\ninversion between precursor and mature Treys at around 20 years of age, though\nthe output differs significantly from the original laboratory dataset.\nSecondly, this report discusses development of the model to incorporate new\ndata from a cross-sectional study of healthy blood donors addressing balance\nbetween Treys and Th17 cells with novel markers for Treg. The potential for\nsimulation to add insight into immune aging is discussed.", 
    "link": "http://arxiv.org/pdf/1307.0747v1", 
    "arxiv-id": "1307.0747v1"
},{
    "category": "cs.CE", 
    "author": "David Menachof", 
    "title": "Comparing Decison Support Tools for Cargo Screening Processes", 
    "publish": "2013-07-02T16:31:44Z", 
    "summary": "When planning to change operations at ports there are two key stake holders\nwith very different interests involved in the decision making processes. Port\noperators are attentive to their standards, a smooth service flow and economic\nviability while border agencies are concerned about national security. The time\ntaken for security checks often interferes with the compliance to service\nstandards that port operators would like to achieve. Decision support tools as\nfor example Cost-Benefit Analysis or Multi Criteria Analysis are useful helpers\nto better understand the impact of changes to a system. They allow\ninvestigating future scenarios and helping to find solutions that are\nacceptable for all parties involved in port operations. In this paper we\nevaluate two different modelling methods, namely scenario analysis and discrete\nevent simulation. These are useful for driving the decision support tools (i.e.\nthey provide the inputs the decision support tools require). Our aims are, on\nthe one hand, to guide the reader through the modelling processes and, on the\nother hand, to demonstrate what kind of decision support information one can\nobtain from the different modelling methods presented.", 
    "link": "http://arxiv.org/pdf/1307.0749v1", 
    "arxiv-id": "1307.0749v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Modelling Reactive and Proactive Behaviour in Simulation: A Case Study   in a University Organisation", 
    "publish": "2013-07-03T16:43:23Z", 
    "summary": "Simulation is a well established what-if scenario analysis tool in\nOperational Research (OR). While traditionally Discrete Event Simulation (DES)\nand System Dynamics Simulation (SDS) are the predominant simulation techniques\nin OR, a new simulation technique, namely Agent-Based Simulation (ABS), has\nemerged and is gaining more attention. In our research we focus on discrete\nsimulation methods (i.e. DES and ABS). The contribution made by this paper is\nthe comparison of DES and combined DES/ABS for modelling human reactive and\ndifferent level of detail of human proactive behaviour in service systems. The\nresults of our experiments show that the level of proactiveness considered in\nthe model has a big impact on the simulation output. However, there is not a\nbig difference between the results from the DES and the combined DES/ABS\nsimulation models. Therefore, for service systems of the type we investigated\nwe would suggest to use DES as the preferred analysis tool.", 
    "link": "http://arxiv.org/pdf/1307.1073v1", 
    "arxiv-id": "1307.1073v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detect adverse drug reactions for the drug Pravastatin", 
    "publish": "2013-07-04T16:42:02Z", 
    "summary": "Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs\nare one of most common causes to withdraw some drugs from market. Prescription\nevent monitoring (PEM) is an important approach to detect the adverse drug\nreactions. The main problem to deal with this method is how to automatically\nextract the medical events or side effects from high-throughput medical data,\nwhich are collected from day to day clinical practice. In this study we propose\nan original approach to detect the ADRs using feature matrix and feature\nselection. The experiments are carried out on the drug Pravastatin. Major side\neffects for the drug are detected. The detected ADRs are based on computerized\nmethod, further investigation is needed.", 
    "link": "http://arxiv.org/pdf/1307.1466v1", 
    "arxiv-id": "1307.1466v1"
},{
    "category": "cs.CE", 
    "author": "Stephanie Foan", 
    "title": "A Beginners Guide to Systems Simulation in Immunology", 
    "publish": "2013-07-05T12:43:04Z", 
    "summary": "Some common systems modelling and simulation approaches for immune problems\nare Monte Carlo simulations, system dynamics, discrete-event simulation and\nagent-based simulation. These methods, however, are still not widely adopted in\nimmunology research. In addition, to our knowledge, there is few research on\nthe processes for the development of simulation models for the immune system.\nHence, for this work, we have two contributions to knowledge. The first one is\nto show the importance of systems simulation to help immunological research and\nto draw the attention of simulation developers to this research field. The\nsecond contribution is the introduction of a quick guide containing the main\nsteps for modelling and simulation in immunology, together with challenges that\noccur during the model development. Further, this paper introduces an example\nof a simulation problem, where we test our guidelines.", 
    "link": "http://arxiv.org/pdf/1307.1597v2", 
    "arxiv-id": "1307.1597v2"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Extending a Microsimulation of the Port of Dover", 
    "publish": "2013-07-05T12:46:33Z", 
    "summary": "Modelling and simulating the traffic of heavily used but secure environments\nsuch as seaports and airports is of increasing importance. This paper discusses\nissues and problems that may arise when extending an existing microsimulation\nstrategy. We also discuss how extensions of these simulations can aid planners\nwith optimal physical and operational feedback. Conclusions are drawn about how\nmicrosimulations can be moved forward as a robust planning tool for the 21st\ncentury.", 
    "link": "http://arxiv.org/pdf/1307.1598v1", 
    "arxiv-id": "1307.1598v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Robust Causality Check for Sampled Scattering Parameters via a Filtered   Fourier Transform", 
    "publish": "2013-07-05T14:45:00Z", 
    "summary": "We introduce a robust numerical technique to verify the causality of sampled\nscattering parameters given on a finite bandwidth. The method is based on a\nfiltered Fourier transform and includes a rigorous estimation of the errors\ncaused by missing out-of-band samples. Compared to existing techniques, the\nmethod is simpler to implement and provides a useful insight on the time-domain\ncharacteristics of the detected violation. Through an applicative example, we\nshows its usefulness to improve the accuracy and reliability of macromodeling\ntechniques used to convert sampled scattering parameters into models for\ntransient analysis.", 
    "link": "http://arxiv.org/pdf/1307.1625v2", 
    "arxiv-id": "1307.1625v2"
},{
    "category": "cs.CE", 
    "author": "Adrian Sandu", 
    "title": "Low-rank Approximations for Computing Observation Impact in 4D-Var Data   Assimilation", 
    "publish": "2013-07-18T20:34:20Z", 
    "summary": "We present an efficient computational framework to quantify the impact of\nindividual observations in four dimensional variational data assimilation. The\nproposed methodology uses first and second order adjoint sensitivity analysis,\ntogether with matrix-free algorithms to obtain low-rank approximations of ob-\nservation impact matrix. We illustrate the application of this methodology to\nimportant applications such as data pruning and the identification of faulty\nsensors for a two dimensional shallow water test system.", 
    "link": "http://arxiv.org/pdf/1307.5076v1", 
    "arxiv-id": "1307.5076v1"
},{
    "category": "cs.CE", 
    "author": "Jingsheng Lei", 
    "title": "Household Electricity Consumption Data Cleansing", 
    "publish": "2013-07-29T22:22:56Z", 
    "summary": "Load curve data in power systems refers to users' electrical energy\nconsumption data periodically collected with meters. It has become one of the\nmost important assets for modern power systems. Many operational decisions are\nmade based on the information discovered in the data. Load curve data, however,\nusually suffers from corruptions caused by various factors, such as data\ntransmission errors or malfunctioning meters. To solve the problem, tremendous\nresearch efforts have been made on load curve data cleansing. Most existing\napproaches apply outlier detection methods from the supply side (i.e.,\nelectricity service providers), which may only have aggregated load data. In\nthis paper, we propose to seek aid from the demand side (i.e., electricity\nservice users). With the help of readily available knowledge on consumers'\nappliances, we present a new appliance-driven approach to load curve data\ncleansing. This approach utilizes data generation rules and a Sequential Local\nOptimization Algorithm (SLOA) to solve the Corrupted Data Identification\nProblem (CDIP). We evaluate the performance of SLOA with real-world trace data\nand synthetic data. The results indicate that, comparing to existing load data\ncleansing methods, such as B-spline smoothing, our approach has an overall\nbetter performance and can effectively identify consecutive corrupted data.\nExperimental results also demonstrate that our method is robust in various\ntests. Our method provides a highly feasible and reliable solution to an\nemerging industry application.", 
    "link": "http://arxiv.org/pdf/1307.7757v4", 
    "arxiv-id": "1307.7757v4"
},{
    "category": "cs.CE", 
    "author": "Jos\u00e9 Juan Gonz\u00e1lez Avil\u00e9s", 
    "title": "Mathematical model of concentrating solar cooker", 
    "publish": "2013-08-06T18:11:15Z", 
    "summary": "The main purpose of this work is to obtain a mathematical model consistent\nwith the thermal behavior of concentrating solar cookers, such as\nJorhejpataranskua. We also want to simulate different conditions respect to the\nparameters involved of several materials for its construction and efficiency.\nThe model is expressed in terms of a coupled nonlinear system of differential\nequations which are solved using Mathematica 8. The results obtained by our\nmodel are compared with measurements of solar cooker in field testing\noperation. We obtained good results in agreement with experimental data.\nMoreover, the simulation results are used by calculating cooking power and\nstandardized cooking power of solar cooker for different parameters.", 
    "link": "http://arxiv.org/pdf/1308.1365v1", 
    "arxiv-id": "1308.1365v1"
},{
    "category": "cs.CE", 
    "author": "Adrian Sandu", 
    "title": "An Optimization Framework to Improve 4D-Var Data Assimilation System   Performance", 
    "publish": "2013-08-08T14:16:28Z", 
    "summary": "This paper develops a computational framework for optimizing the parameters\nof data assimilation systems in order to improve their performance. The\napproach formulates a continuous meta-optimization problem for parameters; the\nmeta-optimization is constrained by the original data assimilation problem. The\nnumerical solution process employs adjoint models and iterative solvers. The\nproposed framework is applied to optimize observation values, data weighting\ncoefficients, and the location of sensors for a test problem. The ability to\noptimize a distributed measurement network is crucial for cutting down\noperating costs and detecting malfunctions.", 
    "link": "http://arxiv.org/pdf/1308.1860v2", 
    "arxiv-id": "1308.1860v2"
},{
    "category": "cs.CE", 
    "author": "Prasanta K Panigrahi", 
    "title": "Wind Speed Data Analysis for Various Seasons during a Decade by Wavelet   and S transform", 
    "publish": "2013-08-13T07:16:03Z", 
    "summary": "The appropriate weather prediction is a challenging task and it can be\nfeasible with proper wind speed fluctuation analysis. In this current paper\ndaubechies-4 wavelet is used to analyze the winter wind speed fluctuations due\nto lesser agitated wind data samples of winter. In summer abrupt changes in\nwind speed occurs which creates difficulty for wavelets to keep proper track of\nwind speed fluctuations. So, in that case the concept of the S-transform is\nintroduced.", 
    "link": "http://arxiv.org/pdf/1308.2773v3", 
    "arxiv-id": "1308.2773v3"
},{
    "category": "cs.CE", 
    "author": "A. W. M. van Schijndel", 
    "title": "The Mapping of Simulated Climate-Dependent Building Innovations", 
    "publish": "2013-08-22T09:20:15Z", 
    "summary": "Performances of building energy innovations are most of the time dependent on\nthe external climate conditions. This means a high performance of a specific\ninnovation in a certain part of Europe, does not imply the same performances in\nother regions. The mapping of simulated building performances at the EU scale\ncould prevent the waste of potential good ideas by identifying the best region\nfor a specific innovation. This paper presents a methodology for obtaining maps\nof performances of building innovations that are virtually spread over whole\nEurope. It is concluded that these maps are useful for finding regions at the\nEU where innovations have the highest expected performances.", 
    "link": "http://arxiv.org/pdf/1308.4801v1", 
    "arxiv-id": "1308.4801v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detect adverse drug reactions for drug Pioglitazone", 
    "publish": "2013-08-23T14:44:29Z", 
    "summary": "In this study we propose a novel method to successfully detect the ADRs using\nfeature matrix and feature selection. A feature matrix, which characterizes the\nmedical events before patients take drugs or after patients take drugs, is\ncreated from THIN database. The feature selection method of Student's t-test is\nused to detect the significant features from thousands of medical events. The\nsignificant ADRs, which are corresponding to significant features, are\ndetected. Experiments are performed on the drug Pioglitazone. Compared to other\ncomputerized methods, our proposed method achieves good performance.", 
    "link": "http://arxiv.org/pdf/1308.5144v1", 
    "arxiv-id": "1308.5144v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detect adverse drug reactions for drug Atorvastatin", 
    "publish": "2013-08-30T09:55:56Z", 
    "summary": "Adverse drug reactions (ADRs) are big concern for public health. ADRs are one\nof most common causes to withdraw some drugs from markets. Now two major\nmethods for detecting ADRs are spontaneous reporting system (SRS), and\nprescription event monitoring (PEM). The World Health Organization (WHO)\ndefines a signal in pharmacovigilance as \"any reported information on a\npossible causal relationship between an adverse event and a drug, the\nrelationship being unknown or incompletely documented previously\". For\nspontaneous reporting systems, many machine learning methods are used to detect\nADRs, such as Bayesian confidence propagation neural network (BCPNN), decision\nsupport methods, genetic algorithms, knowledge based approaches, etc. One\nlimitation is the reporting mechanism to submit ADR reports, which has serious\nunderreporting and is not able to accurately quantify the corresponding risk.\nAnother limitation is hard to detect ADRs with small number of occurrences of\neach drug-event association in the database. In this paper we propose feature\nselection approach to detect ADRs from The Health Improvement Network (THIN)\ndatabase. First a feature matrix, which represents the medical events for the\npatients before and after taking drugs, is created by linking patients'\nprescriptions and corresponding medical events together. Then significant\nfeatures are selected based on feature selection methods, comparing the feature\nmatrix before patients take drugs with one after patients take drugs. Finally\nthe significant ADRs can be detected from thousands of medical events based on\ncorresponding features. Experiments are carried out on the drug Atorvastatin.\nGood performance is achieved.", 
    "link": "http://arxiv.org/pdf/1308.6697v1", 
    "arxiv-id": "1308.6697v1"
},{
    "category": "cs.CE", 
    "author": "Bidisha Goswami", 
    "title": "Modeling Vanilla Option prices: A simulation study by an implicit method", 
    "publish": "2013-11-03T08:26:38Z", 
    "summary": "Option contracts can be valued by using the Black-Scholes equation, a partial\ndifferential equation with initial conditions. An exact solution for European\nstyle options is known. The computation time and the error need to be minimized\nsimultaneously. In this paper, the authors have solved the Black-Scholes\nequation by employing a reasonably accurate implicit method. Options with known\nanalytic solutions have been evaluated. Furthermore, an overall second order\naccurate space and time discretization is proposed in this paper Keywords:\nComputational finance, implicit methods, finite differences, call/put options.", 
    "link": "http://arxiv.org/pdf/1311.0438v2", 
    "arxiv-id": "1311.0438v2"
},{
    "category": "cs.CE", 
    "author": "John W. Peterson", 
    "title": "Accurate curve fits of IAPWS data for high-pressure, high-temperature   single-phase liquid water based on the stiffened gas equation of state", 
    "publish": "2013-11-03T21:42:26Z", 
    "summary": "We present a series of optimal (in the sense of least-squares) curve fits for\nthe stiffened gas equation of state for single-phase liquid water. At high\npressures and (subcritical) temperatures, the parameters produced by these\ncurve fits are found to have very small relative errors: less than $1\\%$ in the\npressure model, and less than $2\\%$ in the temperature model. At low pressures\nand temperatures, especially near the liquid-vapor transition line, the error\nin the curve fits increases rapidly. The smallest pressure value for which\ncurve fits are reported in the present work is 25 MPa, high enough to ensure\nthat the fluid remains a single-phase liquid up to the maximum subcritical\ntemperature of approximately 647K.", 
    "link": "http://arxiv.org/pdf/1311.0534v1", 
    "arxiv-id": "1311.0534v1"
},{
    "category": "cs.CE", 
    "author": "Balasubramaniam Shanker", 
    "title": "A Discontinuous Galerkin Time Domain Framework for Periodic Structures   Subject To Oblique Excitation", 
    "publish": "2013-11-04T17:48:27Z", 
    "summary": "A nodal Discontinuous Galerkin (DG) method is derived for the analysis of\ntime-domain (TD) scattering from doubly periodic PEC/dielectric structures\nunder oblique interrogation. Field transformations are employed to elaborate a\nformalism that is free from any issues with causality that are common when\napplying spatial periodic boundary conditions simultaneously with incident\nfields at arbitrary angles of incidence. An upwind numerical flux is derived\nfor the transformed variables, which retains the same form as it does in the\noriginal Maxwell problem for domains without explicitly imposed periodicity.\nThis, in conjunction with the amenability of the DG framework to non-conformal\nmeshes, provides a natural means of accurately solving the first order TD\nMaxwell equations for a number of periodic systems of engineering interest.\nResults are presented that substantiate the accuracy and utility of our method.", 
    "link": "http://arxiv.org/pdf/1311.0790v2", 
    "arxiv-id": "1311.0790v2"
},{
    "category": "cs.CE", 
    "author": "Prabhu Ramachandran", 
    "title": "SPH Entropy Errors and the Pressure Blip", 
    "publish": "2013-11-09T12:59:38Z", 
    "summary": "The spurious pressure jump at a contact discontinuity, in SPH simulations of\nthe compressible Euler equations is investigated. From the spatiotemporal\nbehaviour of the error, the SPH pressure jump is likened to entropy errors\nobserved for artificial viscosity based finite difference/volume schemes. The\nerror is observed to be generated at start-up and dissipation is the only\nrecourse to mitigate it's effect. We show that similar errors are generated for\nthe Lagrangian plus remap version of the Piecewise Parabolic Method (PPM)\nfinite volume code (PPMLR). Through a comparison with the direct Eulerian\nversion of the PPM code (PPMDE), we argue that a lack of diffusion across the\nmaterial wave (contact discontinuity) is responsible for the error in PPMLR. We\nverify this hypothesis by constructing a more dissipative version of the remap\ncode using a piecewise constant reconstruction. As an application to SPH, we\npropose a hybrid GSPH scheme that adds the requisite dissipation by utilizing a\nmore dissipative Riemann solver for the energy equation. The proposed\nmodification to the GSPH scheme, and it's improved treatment of the anomaly is\nverified for flows with strong shocks in one and two dimensions. The result\nthat dissipation must act across the density and energy equations provides a\nconsistent explanation for many of the hitherto proposed \"cures\" or \"fixes\" for\nthe problem.", 
    "link": "http://arxiv.org/pdf/1311.2167v2", 
    "arxiv-id": "1311.2167v2"
},{
    "category": "cs.CE", 
    "author": "Baghdad Atmani", 
    "title": "SBML for optimizing decision support's tools", 
    "publish": "2013-11-15T12:58:28Z", 
    "summary": "Many theoretical works and tools on epidemiological field reflect the\nemphasis on decision-making Tools by both public health and the scientific\ncommunity, which continues to increase. Indeed, in the epidemiological field,\nmodeling tools are proving a very important way in helping to make decision.\nHowever, the variety, the large volume of data and the nature of epidemics lead\nus to seek solutions to alleviate the heavy burden imposed on both experts and\ndevelopers. In this paper, we present a new approach: the passage of an\nepidemic model realized in Bio-PEPA to a narrative language using the basics of\nSBML language. Our goal is to allow on one hand, epidemiologists to verify and\nvalidate the model, and the other hand, developers to optimize the model in\norder to achieve a better model of decision making. We also present some\npreliminary results and some suggestions to improve the simulated model.", 
    "link": "http://arxiv.org/pdf/1311.3837v1", 
    "arxiv-id": "1311.3837v1"
},{
    "category": "cs.CE", 
    "author": "Pedro Neto", 
    "title": "Numerical modeling of friction stir welding process: a literature review", 
    "publish": "2013-11-18T22:10:14Z", 
    "summary": "This survey presents a literature review on friction stir welding (FSW)\nmodeling with a special focus on the heat generation due to the contact\nconditions between the FSW tool and the workpiece. The physical process is\ndescribed and the main process parameters that are relevant to its modeling are\nhighlighted. The contact conditions (sliding/sticking) are presented as well as\nan analytical model that allows estimating the associated heat generation. The\nmodeling of the FSW process requires the knowledge of the heat loss mechanisms,\nwhich are discussed mainly considering the more commonly adopted formulations.\nDifferent approaches that have been used to investigate the material flow are\npresented and their advantages/drawbacks are discussed. A reliable FSW process\nmodeling depends on the fine tuning of some process and material parameters.\nUsually, these parameters are achieved with base on experimental data. The\nnumerical modeling of the FSW process can help to achieve such parameters with\nless effort and with economic advantages.", 
    "link": "http://arxiv.org/pdf/1311.4570v1", 
    "arxiv-id": "1311.4570v1"
},{
    "category": "cs.CE", 
    "author": "Stephane Raynaud", 
    "title": "Using virtual parts to optimize the metrology process", 
    "publish": "2013-11-25T05:57:40Z", 
    "summary": "In the measurement process, there are many parameters affecting the\nmeasurement results: the influence of the probe system, material stiffness of\nmeasured workpiece, the calibration of the probe with a reference sphere, the\nthermal effects. We want to obtain the limits of a measurement methodology to\nbe able to validate a result. The study is applied to a simple part. We observe\nthe dispersion of the position of different drilled holes (XYZ values in a\ncoordinate system) when we change the quality of the part and the method of\ncalculation. We use the Design of Experiment (Taguchi method) to realize our\nstudy. We study the influence of the part quality on a measurement results. We\nconsider two parameters to define the part quality (flatness and\nperpendicularity). We will also study the influence of different methods of\ncalculation to determine the coordinate system. We can use two options in\nMetrolog XG software (tangent plane with or without orientation constraint).\nThe originality of this paper is that we present a method for the design of\nexperiment that uses CATIA (CAD system) to generate the measured parts. In this\nway we can realize a design of experiment with a largest number of experimental\nresults. This is a positive point for a statistical analysis. We are also free\nto define the parts we want to study without manufacturing difficulties.", 
    "link": "http://arxiv.org/pdf/1311.6215v1", 
    "arxiv-id": "1311.6215v1"
},{
    "category": "cs.CE", 
    "author": "Sabyasachi Mukhopadhyay", 
    "title": "Wavelet Transform-Based Analysis of QRS complex in ECG Signals", 
    "publish": "2013-11-25T20:59:34Z", 
    "summary": "In the present paper we have reported a wavelet based time-frequency\nmultiresolution analysis of an ECG signal. The ECG (electrocardiogram), which\nrecords hearts electrical activity, is able to provide with useful information\nabout the type of Cardiac disorders suffered by the patient depending upon the\ndeviations from normal ECG signal pattern. We have plotted the coefficients of\ncontinuous wavelet transform using Morlet wavelet. We used different ECG signal\navailable at MIT-BIH database and performed a comparative study. We\ndemonstrated that the coefficient at a particular scale represents the presence\nof QRS signal very efficiently irrespective of the type or intensity of noise,\npresence of unusually high amplitude of peaks other than QRS peaks and Base\nline drift errors. We believe that the current studies can enlighten the path\ntowards development of very lucid and time efficient algorithms for identifying\nand representing the QRS complexes that can be done with normal computers and\nprocessors.", 
    "link": "http://arxiv.org/pdf/1311.6460v1", 
    "arxiv-id": "1311.6460v1"
},{
    "category": "cs.CE", 
    "author": "Evgeny Nikulchev", 
    "title": "Reconstruction Models for Attractors in the Technical and Economic   Processes", 
    "publish": "2014-02-06T19:54:39Z", 
    "summary": "The article discusses building models based on the reconstructed attractors\nof the time series. Discusses the use of the properties of dynamical chaos,\nnamely to identify the strange attractors structure models. Here is used the\ngroup properties of differential equations, which consist in the symmetry of\nparticular solutions. Examples of modeling engineering systems are given.", 
    "link": "http://arxiv.org/pdf/1402.1467v1", 
    "arxiv-id": "1402.1467v1"
},{
    "category": "cs.CE", 
    "author": "Anna Ku\u010derov\u00e1", 
    "title": "Uncertainty Propagation in Elasto-Plastic Material", 
    "publish": "2014-02-06T20:55:49Z", 
    "summary": "Macroscopically heterogeneous materials, characterised mostly by comparable\nheterogeneity lengthscale and structural sizes, can no longer be modelled by\ndeterministic approach instead. It is convenient to introduce stochastic\napproach with uncertain material parameters quantified as random fields and/or\nrandom variables. The present contribution is devoted to propagation of these\nuncertainties in mechanical modelling of inelastic behaviour. In such case the\nMonte Carlo method is the traditional approach for solving the proposed\nproblem. Nevertheless, convergence rate is relatively slow, thus new methods\n(e.g. stochastic Galerkin method, stochastic collocation approach, etc.) have\nbeen recently developed to offer fast convergence for sufficiently smooth\nsolution in the probability space. Our goal is to accelerate the uncertainty\npropagation using a polynomial chaos expansion based on stochastic collocation\nmethod. The whole concept is demonstrated on a simple numerical example of\nuniaxial test at a material point where interesting phenomena can be clearly\nunderstood.", 
    "link": "http://arxiv.org/pdf/1402.1485v1", 
    "arxiv-id": "1402.1485v1"
},{
    "category": "cs.CE", 
    "author": "Val\u00e9rio Ramos Batista", 
    "title": "Programming plantation lines on driverless tractors", 
    "publish": "2014-02-06T23:14:58Z", 
    "summary": "Recent advances in Agricultural Engineering include image processing,\nrobotics and geographic information systems (GIS). Some tasks are still\naccomplished manually, like drawing plantation lines that optimize\nproductivity. Herewith we present an algorithm to find the optimal plantation\nlines in linear time. The algorithm is based upon classical results of Geometry\nwhich enabled a source code with only 573 lines. We have implemented it in\nMatlab for sugar cane, and it can be easily adapted to other crops like coffee,\nmaize and soy.", 
    "link": "http://arxiv.org/pdf/1402.1523v1", 
    "arxiv-id": "1402.1523v1"
},{
    "category": "cs.CE", 
    "author": "Yasuhiro Ohyama", 
    "title": "Product Evaluation In Elliptical Helical Pipe Bending", 
    "publish": "2014-02-07T13:38:20Z", 
    "summary": "This research proposes a computation approach to address the evaluation of\nend product machining accuracy in elliptical surfaced helical pipe bending\nusing 6dof parallel manipulator as a pipe bender. The target end product is\nwearable metal muscle supporters used in build-to-order welfare product\nmanufacturing. This paper proposes a product testing model that mainly corrects\nthe surface direction estimation errors of existing least squares ellipse\nfittings, followed by arc length and central angle evaluations. This\npost-machining modelling requires combination of reverse rotations and\ntranslations to a specific location before accuracy evaluation takes place,\ni.e. the reverse comparing to pre-machining product modelling. This specific\nlocation not only allows us to compute surface direction but also the amount of\nexcessive surface twisting as a rotation angle about a specified axis, i.e.\nquantification of surface torsion. At first we experimented three ellipse\nfitting methods such as, two least-squares fitting methods with Bookstein\nconstraint and Trace constraint, and one non- linear least squares method using\nGauss-Newton algorithm. From fitting results, we found that using Trace\nconstraint is more reliable and designed a correction filter for surface\ntorsion observation. Finally we apply 2D total least squares line fitting\nmethod with a rectification filter for surface direction detection.", 
    "link": "http://arxiv.org/pdf/1402.1635v1", 
    "arxiv-id": "1402.1635v1"
},{
    "category": "cs.CE", 
    "author": "Yasuhiro Ohyama", 
    "title": "Vertical Clustering of 3D Elliptical Helical Data", 
    "publish": "2014-02-07T13:42:54Z", 
    "summary": "This research proposes an effective vertical clustering strategy of 3D data\nin an elliptical helical shape based on 2D geometry. The clustering object is\nan elliptical cross-sectioned metal pipe which is been bended in to an\nelliptical helical shape which is used in wearable muscle support designing for\nwelfare industry. The aim of this proposed method is to maximize the vertical\nclustering (vertical partitioning) ability of surface data in order to run the\nproduct evaluation process addressed in research [2]. The experiment results\nprove that the proposed method outperforms the existing threshold no of\nclusters that preserves the vertical shape than applying the conventional 3D\ndata. This research also proposes a new product testing strategy that provides\nthe flexibility in computer aided testing by not restricting the sequence\ndepending measurements which apply weight on measuring process. The clustering\nalgorithms used for the experiments in this research are self-organizing map\n(SOM) and K-medoids.", 
    "link": "http://arxiv.org/pdf/1402.1637v1", 
    "arxiv-id": "1402.1637v1"
},{
    "category": "cs.CE", 
    "author": "Ulrich R\u00fcde", 
    "title": "Validation Experiments for LBM Simulations of Electron Beam Melting", 
    "publish": "2014-02-11T10:59:00Z", 
    "summary": "This paper validates 3D simulation results of electron beam melting (EBM)\nprocesses comparing experimental and numerical data. The physical setup is\npresented which is discretized by a three dimensional (3D) thermal lattice\nBoltzmann method (LBM). An experimental process window is used for the\nvalidation depending on the line energy injected into the metal powder bed and\nthe scan velocity of the electron beam. In the process window the EBM products\nare classified into the categories, porous, good and swelling, depending on the\nquality of the surface. The same parameter sets are used to generate a\nnumerical process window. A comparison of numerical and experimental process\nwindows shows a good agreement. This validates the EBM model and justifies\nsimulations for future improvements of EBM processes. In particular numerical\nsimulations can be used to explain future process window scenarios and find the\nbest parameter set for a good surface quality and dense products.", 
    "link": "http://arxiv.org/pdf/1402.2440v1", 
    "arxiv-id": "1402.2440v1"
},{
    "category": "cs.CE", 
    "author": "Vigasini B", 
    "title": "Modeling European Options", 
    "publish": "2014-02-11T16:31:46Z", 
    "summary": "Option contracts can be valued by using the Black-Scholes equation, a partial\ndifferential equation with initial conditions. An exact solution for European\nstyle options is known. The computation time and the error need to be minimized\nsimultaneously. In this paper, the authors have solved the Black-Scholes\nequation by employing a reasonably accurate implicit method. Options with known\nanalytic solutions have been evaluated. Furthermore, an overall second order\naccurate space and time discretization has been accomplished in this paper.", 
    "link": "http://arxiv.org/pdf/1402.2551v1", 
    "arxiv-id": "1402.2551v1"
},{
    "category": "cs.CE", 
    "author": "Youssef M. Marzouk", 
    "title": "Efficient Localization of Discontinuities in Complex Computational   Simulations", 
    "publish": "2014-02-12T15:16:11Z", 
    "summary": "Surrogate models for computational simulations are input-output\napproximations that allow computationally intensive analyses, such as\nuncertainty propagation and inference, to be performed efficiently. When a\nsimulation output does not depend smoothly on its inputs, the error and\nconvergence rate of many approximation methods deteriorate substantially. This\npaper details a method for efficiently localizing discontinuities in the input\nparameter domain, so that the model output can be approximated as a piecewise\nsmooth function. The approach comprises an initialization phase, which uses\npolynomial annihilation to assign function values to different regions and thus\nseed an automated labeling procedure, followed by a refinement phase that\nadaptively updates a kernel support vector machine representation of the\nseparating surface via active learning. The overall approach avoids structured\ngrids and exploits any available simplicity in the geometry of the separating\nsurface, thus reducing the number of model evaluations required to localize the\ndiscontinuity. The method is illustrated on examples of up to eleven\ndimensions, including algebraic models and ODE/PDE systems, and demonstrates\nimproved scaling and efficiency over other discontinuity localization\napproaches.", 
    "link": "http://arxiv.org/pdf/1402.2845v2", 
    "arxiv-id": "1402.2845v2"
},{
    "category": "cs.CE", 
    "author": "Ji\u0159\u00ed \u0160ejnoha", 
    "title": "Homogenization of coupled heat and moisture transport in masonry   structures including interfaces", 
    "publish": "2014-02-13T15:10:26Z", 
    "summary": "Homogenization of a simultaneous heat and moisture flow in a masonry wall is\npresented in this paper. The principle objective is to examine an impact of the\nassumed imperfect hydraulic contact on the resulting homogenized properties.\nSuch a contact is characterized by a certain mismatching resistance allowing us\nto represent a discontinuous evolution of temperature and moisture fields\nacross the interface, which is in general attributed to discontinuous capillary\npressures caused by different pore size distributions of the adjacent porous\nmaterials. In achieving this, two particular laboratory experiments were\nperformed to provide distributions of temperature and relative humidity in a\nsample of the masonry wall, which in turn served to extract the corresponding\njumps and subsequently to obtain the required interface transition parameters\nby matching numerical predictions and experimental results. The results suggest\na low importance of accounting for imperfect hydraulic contact for the\nderivation of macroscopic homogenized properties. On the other hand, they\nstrongly support the need for a fully coupled multi-scale analysis due to\nsignificant dependence of the homogenized properties on actual moisture\ngradients and corresponding values of both macroscopic temperature and relative\nhumidity.", 
    "link": "http://arxiv.org/pdf/1402.3173v1", 
    "arxiv-id": "1402.3173v1"
},{
    "category": "cs.CE", 
    "author": "J. S\u00fdkora", 
    "title": "Modeling of Degradation Processes in Historical Mortars", 
    "publish": "2014-02-13T15:10:39Z", 
    "summary": "The aim of presented paper is modeling of degradation processes in historical\nmortars exposed to moisture impact during freezing. Internal damage caused by\nice crystallization in pores is one of the most important factors limiting the\nservice life of historical structures. Coupling the transport processes with\nthe mechanical part will allow us to address the impact of moisture on the\ndurability, strength and stiffness of mortars. This should be accomplished with\nthe help of a complex thermo-hygro-mechanical model representing one of the\nprime objectives of this work. The proposed formulation is based on the\nextension of the classical poroelasticity models with the damage mechanics. An\nexample of two-dimensional moisture transport in the environment with\ntemperature below freezing point is presented to support the theoretical\nderivations.", 
    "link": "http://arxiv.org/pdf/1402.3174v1", 
    "arxiv-id": "1402.3174v1"
},{
    "category": "cs.CE", 
    "author": "Evgeny Nikulchev", 
    "title": "Geometrical approach to modeling of nonlinear systems from experimental   data", 
    "publish": "2014-02-28T17:22:50Z", 
    "summary": "This monograph presents a geometric modeling method nonlinear dynamical\nsystems from experimental data . basis method is a qualitative approach to the\nanalysis of linear models and construction of the symmetry groups of attractors\nof dynamical systems with controls . A theoretical study including the central\ntheorem manifold defining conditions of existence of the class in question\nmodels in the local area , taking into account the group properties ,\nestimation algorithms invariant characteristics , methods of constructing\nmodels and identifiable description of the results obtained using the method\nfor simulation -driven engineering processes . included two application is the\ndevelopment of the proposed approach : identification of groups symmetries on\nthe phase portraits of dynamical systems and the method of constructing neural\nnetwork predictive models", 
    "link": "http://arxiv.org/pdf/1402.7324v1", 
    "arxiv-id": "1402.7324v1"
},{
    "category": "cs.CE", 
    "author": "R. G. Ragel", 
    "title": "Hardware software co-design of the Aho-Corasick algorithm: Scalable for   protein identification?", 
    "publish": "2014-03-06T01:42:41Z", 
    "summary": "Pattern matching is commonly required in many application areas and\nbioinformatics is a major area of interest that requires both exact and\napproximate pattern matching. Much work has been done in this area, yet there\nis still a significant space for improvement in efficiency, flexibility, and\nthroughput. This paper presents a hardware software co-design of Aho-Corasick\nalgorithm in Nios II soft-processor and a study on its scalability for a\npattern matching application. A software only approach is used to compare the\nthroughput and the scalability of the hardware software co-design approach.\nAccording to the results we obtained, we conclude that the hardware software\nco-design implementation shows a maximum of 10 times speed up for pattern size\nof 1200 peptides compared to the software only implementation. The results also\nshow that the hardware software co-design approach scales well for increasing\ndata size compared to the software only approach.", 
    "link": "http://arxiv.org/pdf/1403.1317v1", 
    "arxiv-id": "1403.1317v1"
},{
    "category": "cs.CE", 
    "author": "R. G. Ragel", 
    "title": "Hardware accelerated protein inference framework", 
    "publish": "2014-03-06T01:46:05Z", 
    "summary": "Protein inference plays a vital role in the proteomics study. Two major\napproaches could be used to handle the problem of protein inference; top-down\nand bottom-up. This paper presents a framework for protein inference, which\nuses hardware accelerated protein inference framework for handling the most\nimportant step in a bottom-up approach, viz. peptide identification during the\nassembling process. In our framework, identified peptides and their\nprobabilities are used to predict the most suitable reference protein cluster\nfor a given input amino acid sequence with the probability of identified\npeptides. The framework is developed on an FPGA where hardware software\nco-design techniques are used to accelerate the computationally intensive parts\nof the protein inference process. In the paper we have measured, compared and\nreported the time taken for the protein inference process in our framework\nagainst a pure software implementation.", 
    "link": "http://arxiv.org/pdf/1403.1319v1", 
    "arxiv-id": "1403.1319v1"
},{
    "category": "cs.CE", 
    "author": "Chanabasayya M. Vastrad", 
    "title": "Non linear Prediction of Antitubercular Activity Of Oxazolines and   Oxazoles derivatives Making Use of Compact TS-Fuzzy models Through Clustering   with orthogonal least sqaure technique and Fuzzy identification system", 
    "publish": "2014-02-22T02:40:40Z", 
    "summary": "The prediction of uncertain and predictive nonlinear systems is an important\nand challenging problem. Fuzzy logic models are often a good choice to describe\nsuch systems however in many cases these become complex soon. commonlly, too\nless effort is put into descriptor selection and in the creation of suitable\nlocal rules. Moreover, in common no model reduction is applied, while this may\nanalyze the model by removing redundant data. This paper suggests a combined\nmethod that deal with these issues in order to create compact Takagi Sugeno\n(TS) models that can be effectively used to represent complex predictive\nsystems. A new fuzzy clustering method is come up with for the identification\nof compact TS-fuzzy models. The best relevant consequent variables of the TS\nmodel are choosen by an orthogonal least squares technique based on the\nobtained clusters.For the selection of the relevant antecedent (scheduling)\nvariables a new method has been developed based on Fisher's interclass\nseparability basis. This complete approach is demonstrated by means of the\nOxazolines and Oxazoles derivatives as antituberculosis agent for nonlinear\nregression benchmark. The results are compared with results obtained by\nneuro-fuzzy i.e. ANFIS algorithm and advanced fuzzyy clustering techniques i.e\nFMID toolbox .", 
    "link": "http://arxiv.org/pdf/1403.3060v1", 
    "arxiv-id": "1403.3060v1"
},{
    "category": "cs.CE", 
    "author": "Carolin K\u00f6rner", 
    "title": "Numerical Investigations on Hatching Process Strategies for Powder Bed   Based Additive Manufacturing using an Electron Beam", 
    "publish": "2014-03-13T12:53:12Z", 
    "summary": "This paper investigates in hatching process strategies for additive\nmanufacturing using an electron beam by numerical simulations. The underlying\nphysical model and the corresponding three dimensional thermal free surface\nlattice Boltzmann method of the simulation software are briefly presented. The\nsimulation software has already been validated on the basis of experiments up\nto 1.2 kW beam power by hatching a cuboid with a basic process strategy,\nwhereby the results are classified into `porous', `good' and `uneven',\ndepending on their relative density and top surface smoothness. In this paper\nwe study the limitations of this basic process strategy in terms of higher beam\npowers and scan velocities to exploit the future potential of high power\nelectron beam guns up to 10 kW. Subsequently, we introduce modified process\nstrategies, which circumvent these restrictions, to build the part as fast as\npossible under the restriction of a fully dense part with a smooth top surface.\nThese process strategies are suitable to reduce the build time and costs,\nmaximize the beam power usage and therefore use the potential of high power\nelectron beam guns.", 
    "link": "http://arxiv.org/pdf/1403.3251v2", 
    "arxiv-id": "1403.3251v2"
},{
    "category": "cs.CE", 
    "author": "Cornelia Anghel", 
    "title": "Research on Study Mechanical Vibrations with Data Acquisition Systems", 
    "publish": "2014-01-29T13:06:20Z", 
    "summary": "The paper presents a new study method of mechanic vibrations with the help of\nthe data acquisition systems. The study of vibrations with the help of data\nacquisition systems allows the solving of some engineering problems connected\nto the measurement of some parameters which are difficult to measure having in\nview the improvement of the technical performances of the industrial equipment\nor devices", 
    "link": "http://arxiv.org/pdf/1403.4508v1", 
    "arxiv-id": "1403.4508v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "MoM-SO: a Complete Method for Computing the Impedance of Cable Systems   Including Skin, Proximity, and Ground Return Effects", 
    "publish": "2014-03-24T22:03:07Z", 
    "summary": "The availability of accurate and broadband models for underground and\nsubmarine cable systems is of paramount importance for the correct prediction\nof electromagnetic transients in power grids. Recently, we proposed the MoM-SO\nmethod for extracting the series impedance of power cables while accounting for\nskin and proximity effect in the conductors. In this paper, we extend the\nmethod to include ground return effects and to handle cables placed inside a\ntunnel. Numerical tests show that the proposed method is more accurate than\nwidely-used analytic formulas, and is much faster than existing proximity-aware\napproaches like finite elements. For a three-phase cable system in a tunnel,\nthe proposed method requires only 0.3 seconds of CPU time per frequency point,\nagainst the 8.3 minutes taken by finite elements, for a speed up beyond 1000 X.", 
    "link": "http://arxiv.org/pdf/1403.6167v3", 
    "arxiv-id": "1403.6167v3"
},{
    "category": "cs.CE", 
    "author": "M. Niranjan", 
    "title": "Tile optimization for area in FPGA based hardware acceleration of   peptide identification", 
    "publish": "2014-03-28T08:04:43Z", 
    "summary": "Advances in life sciences over the last few decades have lead to the\ngeneration of a huge amount of biological data. Computing research has become a\nvital part in driving biological discovery where analysis and categorization of\nbiological data are involved. String matching algorithms can be applied for\nprotein/gene sequence matching and with the phenomenal increase in the size of\nstring databases to be analyzed, software implementations of these algorithms\nseems to have hit a hard limit and hardware acceleration is increasingly being\nsought. Several hardware platforms such as Field Programmable Gate Arrays\n(FPGA), Graphics Processing Units (GPU) and Chip Multi Processors (CMP) are\nbeing explored as hardware platforms. In this paper, we give a comprehensive\noverview of the literature on hardware acceleration of string matching\nalgorithms, we take an FPGA hardware exploration and expedite the design time\nby a design automation technique. Further, our design automation is also\noptimized for better hardware utilization through optimizing the number of\npeptides that can be represented in an FPGA tile. The results indicate\nsignificant improvements in design time and hardware utilization which are\nreported in this paper.", 
    "link": "http://arxiv.org/pdf/1403.7296v1", 
    "arxiv-id": "1403.7296v1"
},{
    "category": "cs.CE", 
    "author": "Giorgos Kouropoulos", 
    "title": "Calculation software for efficiency and penetration of a fibrous filter   medium based on the mathematical models of air filtration", 
    "publish": "2014-03-20T16:00:45Z", 
    "summary": "At this article will be created a software written in visual basic for\nefficiency and penetration calculation in a fibrous filter medium for given\nvalues of particles diameter that are retained in the filter. Initially, will\nbecome report of mathematical models of air filtration in fibrous filters media\nand then will develop the code and the graphical interface of application, that\nare the base for software creation in the visual basic platform.", 
    "link": "http://arxiv.org/pdf/1405.1300v1", 
    "arxiv-id": "1405.1300v1"
},{
    "category": "cs.CE", 
    "author": "Inampudi Ramesh Babu", 
    "title": "Clonal-Based Cellular Automata in Bioinformatics", 
    "publish": "2014-05-13T14:35:18Z", 
    "summary": "This paper aims at providing a survey on the problems that can be easily\naddressed by clonalbased cellular automata in bioinformatics. Researchers try\nto address the problems in bioinformatics independent of each problem. None of\nthe researchers has tried to relate the major problems in bioinformatics and\nfind a solution using common frame work. We tried to find various problems in\nbioinformatics which can be addressed easily by clonal based cellular automata.\nExtensive literature survey is conducted. We have considered some papers in\nvarious journals and conferences for conduct of our research. This paper\nprovides intuition towards relating various problems in bioinformatics\nlogically and tries to attain a common frame work with respect to clonal based\ncellular automata classifier for addressing the same.", 
    "link": "http://arxiv.org/pdf/1405.3166v1", 
    "arxiv-id": "1405.3166v1"
},{
    "category": "cs.CE", 
    "author": "Dario Pisignano", 
    "title": "Computational homogenization of fibrous piezoelectric materials", 
    "publish": "2014-05-13T20:44:40Z", 
    "summary": "Flexible piezoelectric devices made of polymeric materials are widely used\nfor micro- and nano-electro-mechanical systems. In particular, numerous recent\napplications concern energy harvesting. Due to the importance of computational\nmodeling to understand the influence that microscale geometry and constitutive\nvariables exert on the macroscopic behavior, a numerical approach is developed\nhere for multiscale and multiphysics modeling of thin piezoelectric sheets made\nof aligned arrays of polymeric nanofibers, manufactured by electrospinning. At\nthe microscale, the representative volume element consists in piezoelectric\npolymeric nanofibers, assumed to feature a piezoelastic behavior and subjected\nto electromechanical contact constraints. The latter are incorporated into the\nvirtual work equations by formulating suitable electric, mechanical and\ncoupling potentials and the constraints are enforced by using the penalty\nmethod. From the solution of the micro-scale boundary value problem, a suitable\nscale transition procedure leads to identifying the performance of a\nmacroscopic thin piezoelectric shell element.", 
    "link": "http://arxiv.org/pdf/1405.3302v3", 
    "arxiv-id": "1405.3302v3"
},{
    "category": "cs.CE", 
    "author": "Kenneth E. Barner", 
    "title": "Exploiting Prior Knowledge in Compressed Sensing Wireless ECG Systems", 
    "publish": "2014-05-16T15:12:34Z", 
    "summary": "Recent results in telecardiology show that compressed sensing (CS) is a\npromising tool to lower energy consumption in wireless body area networks for\nelectrocardiogram (ECG) monitoring. However, the performance of current\nCS-based algorithms, in terms of compression rate and reconstruction quality of\nthe ECG, still falls short of the performance attained by state-of-the-art\nwavelet based algorithms. In this paper, we propose to exploit the structure of\nthe wavelet representation of the ECG signal to boost the performance of\nCS-based methods for compression and reconstruction of ECG signals. More\nprecisely, we incorporate prior information about the wavelet dependencies\nacross scales into the reconstruction algorithms and exploit the high fraction\nof common support of the wavelet coefficients of consecutive ECG segments.\nExperimental results utilizing the MIT-BIH Arrhythmia Database show that\nsignificant performance gains, in terms of compression rate and reconstruction\nquality, can be obtained by the proposed algorithms compared to current\nCS-based methods.", 
    "link": "http://arxiv.org/pdf/1405.4201v2", 
    "arxiv-id": "1405.4201v2"
},{
    "category": "cs.CE", 
    "author": "Xiaoyu Wang", 
    "title": "A Revised Incremental Conductance MPPT Algorithm for Solar PV Generation   Systems", 
    "publish": "2014-05-19T20:51:16Z", 
    "summary": "A revised Incremental Conductance (IncCond) maximum power point tracking\n(MPPT) algorithm for PV generation systems is proposed in this paper. The\ncommonly adopted traditional IncCond method uses a constant step size for\nvoltage adjustment and is difficult to achieve both a good tracking performance\nand quick elimination of the oscillations, especially under the dramatic\nchanges of the environment conditions. For the revised algorithm, the\nincremental voltage change step size is adaptively adjusted based on the slope\nof the power-voltage (P-V) curve. An accelerating factor and a decelerating\nfactor are further applied to adjust the voltage step change considering\nwhether the sign of the P-V curve slope remains the same or not in a subsequent\ntracking step. In addition, the upper bound of the maximum voltage step change\nis also updated considering the information of sign changes. The revised MPPT\nalgorithm can quickly track the maximum power points (MPPs) and remove the\noscillation of the actual operation points around the real MPPs. The\neffectiveness of the revised algorithm is demonstrated using a simulation.", 
    "link": "http://arxiv.org/pdf/1405.4890v1", 
    "arxiv-id": "1405.4890v1"
},{
    "category": "cs.CE", 
    "author": "Hao Li", 
    "title": "Determination of Boiling Range of Xylene Mixed in PX Device Using   Artificial Neural Networks", 
    "publish": "2014-05-20T16:33:25Z", 
    "summary": "Determination of boiling range of xylene mixed in PX device is currently a\ncrucial topic in the practical applications because of the recent disputes of\nPX project in China. In our study, instead of determining the boiling range of\nxylene mixed by traditional approach in laboratory or industry, we successfully\nestablished two Artificial Neural Networks (ANNs) models to determine the\ninitial boiling point and final boiling point respectively. Results show that\nthe Multilayer Feedforward Neural Networks (MLFN) model with 7 nodes (MLFN-7)\nis the best model to determine the initial boiling point of xylene mixed, with\nthe RMS error 0.18; while the MLFN model with 4 nodes (MLFN-4) is the best\nmodel to determine the final boiling point of xylene mixed, with the RMS error\n0.75. The training and testing processes both indicate that the models we\ndeveloped are robust and precise. Our research can effectively avoid the damage\nof the PX device to human body and environment.", 
    "link": "http://arxiv.org/pdf/1405.5148v1", 
    "arxiv-id": "1405.5148v1"
},{
    "category": "cs.CE", 
    "author": "Adrian Sandu", 
    "title": "Optimization of Vehicle Dynamics based on Multibody Models using Adjoint   Sensitivity Analysis", 
    "publish": "2014-05-20T19:29:51Z", 
    "summary": "Multibody dynamics simulations have become widely used tools for vehicle\nsystems analysis and design. As this approach evolves, it becomes able to\nprovide additional information for various types of analyses. One very\nimportant direction is the optimization of multibody systems. Sensitivity\nanalysis of multibody system dynamics is essential for design optimization.\nDynamic sensitivities, when needed, are often calculated by means of finite\ndifferences. However, depending of the number of parameters involved, this\nprocedure can be computationally expensive. Moreover, in many cases the results\nsuffer from low accuracy when real perturbations are used. This paper develops\nthe adjoint sensitivity analysis of multibody systems in the context of penalty\nformulations. The resulting sensitivities are applied to perform dynamical\noptimization of a full vehicle system.", 
    "link": "http://arxiv.org/pdf/1405.5197v2", 
    "arxiv-id": "1405.5197v2"
},{
    "category": "cs.CE", 
    "author": "Hao Li", 
    "title": "Application of Multilayer Feedforward Neural Networks in Predicting Tree   Height and Forest Stock Volume of Chinese Fir", 
    "publish": "2014-05-20T19:52:43Z", 
    "summary": "Wood increment is critical information in forestry management. Previous\nstudies used mathematics models to describe complex growing pattern of forest\nstand, in order to determine the dynamic status of growing forest stand in\nmultiple conditions. In our research, we aimed at studying non-linear\nrelationships to establish precise and robust Artificial Neural Networks (ANN)\nmodels to predict the precise values of tree height and forest stock volume\nbased on data of Chinese fir. Results show that Multilayer Feedforward Neural\nNetworks with 4 nodes (MLFN-4) can predict the tree height with the lowest RMS\nerror (1.77); Multilayer Feedforward Neural Networks with 7 nodes (MLFN-7) can\npredict the forest stock volume with the lowest RMS error (4.95). The training\nand testing process have proved that our models are precise and robust.", 
    "link": "http://arxiv.org/pdf/1405.5206v1", 
    "arxiv-id": "1405.5206v1"
},{
    "category": "cs.CE", 
    "author": "Zhilong Xiu", 
    "title": "Application of Artificial Neural Networks in Predicting Abrasion   Resistance of Solution Polymerized Styrene-Butadiene Rubber Based Composites", 
    "publish": "2014-05-21T20:30:22Z", 
    "summary": "Abrasion resistance of solution polymerized styrene-butadiene rubber (SSBR)\nbased composites is a typical and crucial property in practical applications.\nPrevious studies show that the abrasion resistance can be calculated by the\nmultiple linear regression model. In our study, considering this relationship\ncan also be described into the non-linear conditions, a Multilayer Feed-forward\nNeural Networks model with 3 nodes (MLFN-3) was successfully established to\ndescribe the relationship between the abrasion resistance and other properties,\nusing 23 groups of data, with the RMS error 0.07. Our studies have proved that\nArtificial Neural Networks (ANN) model can be used to predict the SSBR-based\ncomposites, which is an accurate and robust process.", 
    "link": "http://arxiv.org/pdf/1405.5550v1", 
    "arxiv-id": "1405.5550v1"
},{
    "category": "cs.CE", 
    "author": "Benyuan Liu", 
    "title": "Py-oopsi: the python implementation of the fast-oopsi algorithm", 
    "publish": "2014-05-06T06:05:04Z", 
    "summary": "Fast-oopsi was developed by Joshua Vogelstein in 2009, which is now widely\nused to extract neuron spike activities from calcium fluorescence signals.\nHere, we propose detailed implementation of the fast-oopsi algorithm in python\nprogramming language. Some corrections are also made to the original fast-oopsi\npaper.", 
    "link": "http://arxiv.org/pdf/1405.6181v1", 
    "arxiv-id": "1405.6181v1"
},{
    "category": "cs.CE", 
    "author": "Geqi Qi", 
    "title": "Car-following model on two lanes and stability analysis", 
    "publish": "2014-07-12T11:21:05Z", 
    "summary": "Considering lateral influence from adjacent lane, an improved car-following\nmodel is developed in this paper. Then linear and non-linear stability analyses\nare carried out. The modified Korteweg-de Vries (MKdV) equation is derived with\nthe kink-antikink soliton solution. Numerical simulations are implemented and\nthe result shows good consistency with theoretical study.", 
    "link": "http://arxiv.org/pdf/1407.3373v1", 
    "arxiv-id": "1407.3373v1"
},{
    "category": "cs.CE", 
    "author": "Slobodan Simonovic", 
    "title": "Modeling structural change in spatial system dynamics: A Daisyworld   example", 
    "publish": "2014-07-14T14:11:42Z", 
    "summary": "System dynamics (SD) is an effective approach for helping reveal the temporal\nbehavior of complex systems. Although there have been recent developments in\nexpanding SD to include systems' spatial dependencies, most applications have\nbeen restricted to the simulation of diffusion processes; this is especially\ntrue for models on structural change (e.g. LULC modeling). To address this\nshortcoming, a Python program is proposed to tightly couple SD software to a\nGeographic Information System (GIS). The approach provides the required\ncapacities for handling bidirectional and synchronized interactions of\noperations between SD and GIS. In order to illustrate the concept and the\ntechniques proposed for simulating structural changes, a fictitious environment\ncalled Daisyworld has been recreated in a spatial system dynamics (SSD)\nenvironment. The comparison of spatial and non-spatial simulations emphasizes\nthe importance of considering spatio-temporal feedbacks. Finally, practical\napplications of structural change models in agriculture and disaster management\nare proposed.", 
    "link": "http://arxiv.org/pdf/1407.3661v1", 
    "arxiv-id": "1407.3661v1"
},{
    "category": "cs.CE", 
    "author": "Shuvasish Karmaker", 
    "title": "Protein Folding in the Hexagonal Prism Lattice with Diagonals", 
    "publish": "2014-07-17T12:12:35Z", 
    "summary": "Predicting protein secondary structure using lattice model is one of the most\nstudied computational problem in bioinformatics. Here secondary structure or\nthree dimensional structure of protein is predicted from its amino acid\nsequence. Secondary structure refers to local sub-structures of protein. Mostly\nfounded secondary structures are alpha helix and beta sheets. Since, it is a\nproblem of great potential complexity many simplified energy model have been\nproposed in literature on basis of interaction of amino acid residue in\nprotein. Here we use well researched Hydrophobic-Polar (HP) energy model. In\nthis paper, we proposed hexagonal prism lattice with diagonal that can overcome\nthe problems of other lattice structure, e.g., parity problem. We give two\napproximation algorithm for protein folding on this lattice. Our first\nalgorithm leads us to similar structure of helix structure which is commonly\nfound in protein structure. This motivated us to find next algorithm which\nimproves the algorithm ratio of 9/7.", 
    "link": "http://arxiv.org/pdf/1407.4650v1", 
    "arxiv-id": "1407.4650v1"
},{
    "category": "cs.CE", 
    "author": "Paritosh Bhattacharya", 
    "title": "A comparative study between seasonal wind speed by Fourier and Wavelet   analysis", 
    "publish": "2014-07-31T16:17:39Z", 
    "summary": "Wind Energy is a useful resource for Renewable energy purpose. Wind speed\nplays a vital role for wind energy calculation of certain location. So, it is\nvery much necessary to know the wind speed data characteristics. In this paper\nfourier and wavelet transform are applied to study the wind speed data. We have\ncompared wind speed of winter with summer by taking their speed into account\nusing various discrete wavelets namely Haar and Daubechies-4 (Db-4). Also the\nperiodicity of wind speed is checked using Continuous Wavelet Transform (MCWT)\nlike Morlet. Thereafter a comparative study is done for detecting the\nperiodicity of both summer and winter. Then wavelet coherence is checked\nbetween these two data for extracting the phase coherency information.", 
    "link": "http://arxiv.org/pdf/1407.8476v2", 
    "arxiv-id": "1407.8476v2"
},{
    "category": "cs.CE", 
    "author": "Kenji Takeda", 
    "title": "\"Share and Enjoy\": Publishing Useful and Usable Scientific Models", 
    "publish": "2014-09-01T11:16:21Z", 
    "summary": "The reproduction and replication of reported scientific results is a hot\ntopic within the academic community. The retraction of numerous studies from a\nwide range of disciplines, from climate science to bioscience, has drawn the\nfocus of many commentators, but there exists a wider socio-cultural problem\nthat pervades the scientific community. Sharing code, data and models often\nrequires extra effort; this is currently seen as a significant overhead that\nmay not be worth the time investment.\n  Automated systems, which allow easy reproduction of results, offer the\npotential to incentivise a culture change and drive the adoption of new\ntechniques to improve the efficiency of scientific exploration. In this paper,\nwe discuss the value of improved access and sharing of the two key types of\nresults arising from work done in the computational sciences: models and\nalgorithms. We propose the development of an integrated cloud-based system\nunderpinning computational science, linking together software and data\nrepositories, toolchains, workflows and outputs, providing a seamless automated\ninfrastructure for the verification and validation of scientific models and in\nparticular, performance benchmarks.", 
    "link": "http://arxiv.org/pdf/1409.0367v2", 
    "arxiv-id": "1409.0367v2"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detect Adverse Drug Reactions for Drug Aspirin", 
    "publish": "2014-09-02T10:43:25Z", 
    "summary": "Adverse drug reaction (ADR) is widely concerned for public health issue. In\nthis study we propose an original approach to detect the ADRs using feature\nmatrix and feature selection. The experiments are carried out on the drug\nAspirin. Major side effects for the drug are detected and better performance is\nachieved compared to other computerized methods. The detected ADRs are based on\nthe computerized method, further investigation is needed.", 
    "link": "http://arxiv.org/pdf/1409.0658v1", 
    "arxiv-id": "1409.0658v1"
},{
    "category": "cs.CE", 
    "author": "Tom Rodden", 
    "title": "An Approach for Assessing Clustering of Households by Electricity Usage", 
    "publish": "2014-09-02T14:12:59Z", 
    "summary": "How a household varies their regular usage of electricity is useful\ninformation for organisations to allow accurate targeting of behaviour\nmodification initiatives with the aim of improving the overall efficiency of\nthe electricity network. The variability of regular activities in a household\nis one possible indication of that household's willingness to accept incentives\nto change their behaviour.\n  An approach is presented for identifying a way of representing the\nvariability of a household's behaviour and developing an efficient way of\nclustering the households, using these measures of variability, into a few,\nusable groupings.\n  To evaluate the effectiveness of the variability measures, a number of\ncluster validity indexes are explored with regard to how the indexes vary with\nthe number of clusters, the number of attributes, and the quality of the\nattributes. The Cluster Dispersion Indicator (CDI) and the Davies-Boulden\nIndicator (DBI) are selected for future work developing various indicators of\nhousehold behaviour variability.\n  The approach is tested using data from 180 UK households monitored for over a\nyear at a sampling interval of 5 minutes. Data is taken from the evening peak\nelectricity usage period of 4pm to 8pm.", 
    "link": "http://arxiv.org/pdf/1409.0718v1", 
    "arxiv-id": "1409.0718v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detecting adverse drug reactions for the drug Simvastatin", 
    "publish": "2014-09-03T12:30:05Z", 
    "summary": "Adverse drug reactions (ADR) are widely concerning for public health issue.\nIn this study we propose an original approach to detect ADRs using a feature\nmatrix and feature selection. The experiments are carried out on the drug\nSimvastatin. Major side effects for the drug are detected and better\nperformance is achieved compared to other computerized methods. Because\ncurrently the detected ADRs are based solely on computerized methods, further\nexpert investigation is needed.", 
    "link": "http://arxiv.org/pdf/1409.1059v1", 
    "arxiv-id": "1409.1059v1"
},{
    "category": "cs.CE", 
    "author": "Luca Daniel", 
    "title": "Stochastic Testing Simulator for Integrated Circuits and MEMS:   Hierarchical and Sparse Techniques", 
    "publish": "2014-09-16T22:15:47Z", 
    "summary": "Process variations are a major concern in today's chip design since they can\nsignificantly degrade chip performance. To predict such degradation, existing\ncircuit and MEMS simulators rely on Monte Carlo algorithms, which are typically\ntoo slow. Therefore, novel fast stochastic simulators are highly desired. This\npaper first reviews our recently developed stochastic testing simulator that\ncan achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we\ndevelop a fast hierarchical stochastic spectral simulator to simulate a complex\ncircuit or system consisting of several blocks. We further present a fast\nsimulation approach based on anchored ANOVA (analysis of variance) for some\ndesign problems with many process variations. This approach can reduce the\nsimulation cost and can identify which variation sources have strong impacts on\nthe circuit's performance. The simulation results of some circuit and MEMS\nexamples are reported to show the effectiveness of our simulator", 
    "link": "http://arxiv.org/pdf/1409.4822v1", 
    "arxiv-id": "1409.4822v1"
},{
    "category": "cs.CE", 
    "author": "Luca Daniel", 
    "title": "Efficient Uncertainty Quantification for the Periodic Steady State of   Forced and Autonomous Circuits", 
    "publish": "2014-09-16T22:46:49Z", 
    "summary": "This brief paper proposes an uncertainty quantification method for the\nperiodic steady-state (PSS) analysis with both Gaussian and non-Gaussian\nvariations. Our stochastic testing formulation for the PSS problem provides\nsuperior efficiency over both Monte Carlo methods and existing spectral\nmethods. The numerical implementation of a stochastic shooting Newton solver is\npresented for both forced and autonomous circuits. Simulation results on some\nanalog/RF circuits are reported to show the effectiveness of our proposed\nalgorithms.", 
    "link": "http://arxiv.org/pdf/1409.4826v1", 
    "arxiv-id": "1409.4826v1"
},{
    "category": "cs.CE", 
    "author": "Luca Daniel", 
    "title": "Stochastic Testing Method for Transistor-Level Uncertainty   Quantification Based on Generalized Polynomial Chaos", 
    "publish": "2014-09-16T23:32:44Z", 
    "summary": "Uncertainties have become a major concern in integrated circuit design. In\norder to avoid the huge number of repeated simulations in conventional Monte\nCarlo flows, this paper presents an intrusive spectral simulator for\nstatistical circuit analysis. Our simulator employs the recently developed\ngeneralized polynomial chaos expansion to perform uncertainty quantification of\nnonlinear transistor circuits with both Gaussian and non-Gaussian random\nparameters. We modify the nonintrusive stochastic collocation (SC) method and\ndevelop an intrusive variant called stochastic testing (ST) method to\naccelerate the numerical simulation. Compared with the stochastic Galerkin (SG)\nmethod, the resulting coupled deterministic equations from our proposed ST\nmethod can be solved in a decoupled manner at each time point. At the same\ntime, ST uses fewer samples and allows more flexible time step size controls\nthan directly using a nonintrusive SC solver. These two properties make ST more\nefficient than SG and than existing SC methods, and more suitable for\ntime-domain circuit simulation. Simulation results of several digital, analog\nand RF circuits are reported. Since our algorithm is based on generic\nmathematical models, the proposed ST algorithm can be applied to many other\nengineering problems.", 
    "link": "http://arxiv.org/pdf/1409.4831v1", 
    "arxiv-id": "1409.4831v1"
},{
    "category": "cs.CE", 
    "author": "Daniele de Rigo", 
    "title": "Architecture of Environmental Risk Modelling: for a faster and more   robust response to natural disasters", 
    "publish": "2014-09-28T22:48:22Z", 
    "summary": "Demands on the disaster response capacity of the European Union are likely to\nincrease, as the impacts of disasters continue to grow both in size and\nfrequency. This has resulted in intensive research on issues concerning\nspatially-explicit information and modelling and their multiple sources of\nuncertainty. Geospatial support is one of the forms of assistance frequently\nrequired by emergency response centres along with hazard forecast and event\nmanagement assessment. Robust modelling of natural hazards requires dynamic\nsimulations under an array of multiple inputs from different sources.\nUncertainty is associated with meteorological forecast and calibration of the\nmodel parameters. Software uncertainty also derives from the data\ntransformation models (D-TM) needed for predicting hazard behaviour and its\nconsequences. On the other hand, social contributions have recently been\nrecognized as valuable in raw-data collection and mapping efforts traditionally\ndominated by professional organizations. Here an architecture overview is\nproposed for adaptive and robust modelling of natural hazards, following the\nSemantic Array Programming paradigm to also include the distributed array of\nsocial contributors called Citizen Sensor in a semantically-enhanced strategy\nfor D-TM modelling. The modelling architecture proposes a multicriteria\napproach for assessing the array of potential impacts with qualitative rapid\nassessment methods based on a Partial Open Loop Feedback Control (POLFC) schema\nand complementing more traditional and accurate a-posteriori assessment. We\ndiscuss the computational aspect of environmental risk modelling using\narray-based parallel paradigms on High Performance Computing (HPC) platforms,\nin order for the implications of urgency to be introduced into the systems\n(Urgent-HPC).", 
    "link": "http://arxiv.org/pdf/1409.7966v1", 
    "arxiv-id": "1409.7966v1"
},{
    "category": "cs.CE", 
    "author": "Jason Gillette", 
    "title": "Prognosis of Anterior Cruciate Ligament (ACL) Reconstruction: A Data   Driven Approach", 
    "publish": "2014-10-30T22:54:07Z", 
    "summary": "Individuals who suffer anterior cruciate ligament (ACL) injury are at higher\nrisk of developing knee osteoarthritis (OA) and almost 50% display symptoms 10\nto 20 years post injury. Anterior cruciate ligament reconstruction (ACLR) often\ndoes not protect against knee OA development. Accordingly, a multiscale\nformulation for Data Driven Prognosis (DDP) of post ACLR is developed. Unlike\ntraditional predictive strategies that require controlled off-line measurements\nor training for determination of constitutive parameters to derive the\ntransitional statistics, the proposed DDP algorithm relies solely on in situ\nmeasurements. The proposed DDP scheme is capable of predicting onset of\ninstabilities. Since the need for off line testing (or training) is obviated,\nit can be easily implemented for ACLR, where such controlled a priori testing\nis almost impossible to conduct. The DDP algorithm facilitates hierarchical\nhandling of the large data set, and can assess the state of recovery in post\nACLR conditions based on data collected from stair ascent and descent exercises\nof subjects. The DDP algorithm identifies inefficient knee varus motion and\nknee rotation as primary difficulties experienced by some of the post ACLR\npopulation. In such cases, levels of energy dissipation rate at the knee, and\nits fluctuation may be used as measures for assessing progress after ACL\nreconstruction.", 
    "link": "http://arxiv.org/pdf/1411.0001v1", 
    "arxiv-id": "1411.0001v1"
},{
    "category": "cs.CE", 
    "author": "Carlos A. Cruz-Villar", 
    "title": "Dual Algorithms", 
    "publish": "2014-11-11T02:35:50Z", 
    "summary": "The cubic spline interpolation method, the Runge--Kutta method, and the\nNewton-Raphson method are extended to dual versions (developed in the context\nof dual numbers). This extension allows the calculation of the derivatives of\ncomplicated compositions of functions which are not necessarily defined by a\nclosed form expression. The code for the algorithms has been written in Fortran\nand some examples are presented. Among them, we use the dual Newton--Raphson\nmethod to obtain the derivatives of the output angle in the RRRCR spatial\nmechanism; we use the dual normal cubic spline interpolation algorithm to\nobtain the thermal diffusivity using photothermal techniques; and we use the\ndual Runge--Kutta method to obtain the derivatives of functions depending on\nthe solution of the Duffing equation.", 
    "link": "http://arxiv.org/pdf/1411.2684v3", 
    "arxiv-id": "1411.2684v3"
},{
    "category": "cs.CE", 
    "author": "M. Abdel Wahab", 
    "title": "Geometrically nonlinear isogeometric analysis of laminated composite   plates based on higher-order shear deformation theory", 
    "publish": "2014-11-13T11:39:39Z", 
    "summary": "In this paper, we present an effectively numerical approach based on\nisogeometric analysis (IGA) and higher-order shear deformation theory (HSDT)\nfor geometrically nonlinear analysis of laminated composite plates. The HSDT\nallows us to approximate displacement field that ensures by itself the\nrealistic shear strain energy part without shear correction factors. IGA\nutilizing basis functions namely B-splines or non-uniform rational B-splines\n(NURBS) enables to satisfy easily the stringent continuity requirement of the\nHSDT model without any additional variables. The nonlinearity of the plates is\nformed in the total Lagrange approach based on the von-Karman strain\nassumptions. Numerous numerical validations for the isotropic, orthotropic,\ncross-ply and angle-ply laminated plates are provided to demonstrate the\neffectiveness of the proposed method.", 
    "link": "http://arxiv.org/pdf/1411.3508v1", 
    "arxiv-id": "1411.3508v1"
},{
    "category": "cs.CE", 
    "author": "Boyan S. Lazarov", 
    "title": "Robust topology optimisation of microstructural details without length   scale separation - using a spectral coarse basis preconditioner", 
    "publish": "2014-11-13T15:49:40Z", 
    "summary": "This paper applies topology optimisation to the design of structures with\nperiodic microstructural details without length scale separation, i.e.\nconsidering the complete macroscopic structure and its response, while\nresolving all microstructural details, as compared to the often used\nhomogenisation approach. The approach takes boundary conditions into account\nand ensures connected and macroscopically optimised microstructures regardless\nof the difference in micro- and macroscopic length scales. This results in\nmicrostructures tailored for specific applications rather than specific\nproperties.\n  Dealing with the complete macroscopic structure and its response is\ncomputationally challenging as very fine discretisations are needed in order to\nresolve all microstructural details. Therefore, this article shows the benefits\nof applying a contrast-independent spectral preconditioner based on the\nmultiscale finite element method (MsFEM) to large structures with\nfully-resolved microstructural details.\n  The density-based topology optimisation approach combined with a Heaviside\nprojection filter and a stochastic robust formulation is used on various\nproblems, with both periodic and layered microstructures. The presented\napproach is shown to allow for the topology optimisation of very large problems\nin \\textsc{Matlab}, specifically a problem with 26 million displacement degrees\nof freedom in 26 hours using a single computational thread.", 
    "link": "http://arxiv.org/pdf/1411.3923v1", 
    "arxiv-id": "1411.3923v1"
},{
    "category": "cs.CE", 
    "author": "Zhao Ning", 
    "title": "Dynamic aerodynamic-structural coupling numerical simulation on the   flexible wing of a cicada based on ansys", 
    "publish": "2014-11-15T03:47:16Z", 
    "summary": "Most biological flyers undergo orderly deformation in flight, and the\ndeformations of wings lead to complex fluid-structure interactions. In this\npaper, an aerodynamic-structural coupling method of flapping wing is developed\nbased on ANSYS to simulate the flapping of flexible wing. Fluent module and\nTransient Structural module are connected through the System Coupling module to\nmake a two-way fluid-structure Coupling computational framework. Comparing with\nthe rigid wing of a cicada, the coupling results of the flexible wing shows\nthat the flexible deformation can increase the aerodynamic performances of\nflapping flight.", 
    "link": "http://arxiv.org/pdf/1411.4110v2", 
    "arxiv-id": "1411.4110v2"
},{
    "category": "cs.CE", 
    "author": "Albert C. To", 
    "title": "Proportional Topology Optimization: A new non-gradient method for   solving stress constrained and minimum compliance problems and its   implementation in MATLAB", 
    "publish": "2014-11-21T15:04:31Z", 
    "summary": "A new topology optimization method called the Proportional Topology\nOptimization (PTO) is presented. As a non-gradient method, PTO is simple to\nunderstand, easy to implement, and is also efficient and accurate at the same\ntime. It is implemented into two MATLAB programs to solve the stress\nconstrained and minimum compliance problems. Descriptions of the algorithm and\ncomputer programs are provided in detail. The method is applied to solve three\nnumerical examples for both types of problems. The method shows comparable\nefficiency and accuracy with an existing gradient optimality criteria method.\nAlso, the PTO stress constrained algorithm and minimum compliance algorithm are\ncompared by feeding output from one algorithm to the other in an alternative\nmanner, where the former yields lower maximum stress and volume fraction but\nhigher compliance compared to the latter. Advantages and disadvantages of the\nproposed method and future works are discussed. The computer programs are\nself-contained and publicly shared in the website www.ptomethod.org.", 
    "link": "http://arxiv.org/pdf/1411.6884v1", 
    "arxiv-id": "1411.6884v1"
},{
    "category": "cs.CE", 
    "author": "Ryan Loxton", 
    "title": "Optimal Boundary Control for Water Hammer Suppression in Fluid   Transmission Pipelines", 
    "publish": "2014-11-27T03:45:43Z", 
    "summary": "When fluid flow in a pipeline is suddenly halted, a pressure surge or wave is\ncreated within the pipeline. This phenomenon, called water hammer, can cause\nmajor damage to pipelines, including pipeline ruptures. In this paper, we model\nthe problem of mitigating water hammer during valve closure by an optimal\nboundary control problem involving a nonlinear hyperbolic PDE system that\ndescribes the fluid flow along the pipeline. The control variable in this\nsystem represents the valve boundary actuation implemented at the pipeline\nterminus. To solve the boundary control problem, we first use {the method of\nlines} to obtain a finite-dimensional ODE model based on the original PDE\nsystem. Then, for the boundary control design, we apply the control\nparameterization method to obtain an approximate optimal parameter selection\nproblem that can be solved using nonlinear optimization techniques such as\nSequential Quadratic Programming (SQP). We conclude the paper with simulation\nresults demonstrating the capability of optimal boundary control to\nsignificantly reduce flow fluctuation.", 
    "link": "http://arxiv.org/pdf/1411.7462v1", 
    "arxiv-id": "1411.7462v1"
},{
    "category": "cs.CE", 
    "author": "Samin Ishtiaq", 
    "title": "Reproducibility as a Technical Specification", 
    "publish": "2015-04-06T15:59:49Z", 
    "summary": "Reproducibility of computationally-derived scientific discoveries should be a\ncertainty. As the product of several person-years' worth of effort, results --\nwhether disseminated through academic journals, conferences or exploited\nthrough commercial ventures -- should at some level be expected to be\nrepeatable by other researchers. While this stance may appear to be obvious and\ntrivial, a variety of factors often stand in the way of making it commonplace.\nWhilst there has been detailed cross-disciplinary discussions of the various\nsocial, cultural and ideological drivers and (potential) solutions, one factor\nwhich has had less focus is the concept of reproducibility as a technical\nchallenge. Specifically, that the definition of an unambiguous and measurable\nstandard of reproducibility would offer a significant benefit to the wider\ncomputational science community.\n  In this paper, we propose a high-level technical specification for a service\nfor reproducibility, presenting cyberinfrastructure and associated workflow for\na service which would enable such a specification to be verified and validated.\nIn addition to addressing a pressing need for the scientific community, we\nfurther speculate on the potential contribution to the wider software\ndevelopment community of services which automate de novo compilation and\ntesting of code from source. We illustrate our proposed specification and\nworkflow by using the BioModelAnalyzer tool as a running example.", 
    "link": "http://arxiv.org/pdf/1504.01310v3", 
    "arxiv-id": "1504.01310v3"
},{
    "category": "cs.CE", 
    "author": "Q. He", 
    "title": "Fast and Rigorous DC Solution in Finite Element Method for Integrated   Circuit Analysis", 
    "publish": "2015-04-24T23:23:33Z", 
    "summary": "Large scale circuit simulation, such as power delivery network analysis, has\nbecome increasingly challenge in the VLSI design verification flow. Power\ndelivery network can be simulated by both SPICE-type circuit-based model and\neletromagnetics-based model when full-wave accuracy is desired. In the early\ntime of the time domain finite element simulation for integrated circuit, the\nmodes having the highest eigenvalues supported by the numerical system will be\nexcited. Because of the band limited source, after the early time, the modes\nhaving a resonance frequency well beyond the input frequency band will die\ndown, and all physically important high-order modes and DC mode will show up\nand become dominant. Among these modes, the DC mode is the last one to show up.\nAlthough the convergence criterion is not applied on the DC mode, the existence\nof DC mode in the field solution will deteriorate the convergence rate of the\nfirst several high order modes. Therefore, this paper first analyzed the\nmathematic characteristics of the DC mode and proposed a rigorous and fast\nsolution to extract the DC mode from the numerical system in order to speed up\nthe convergence rate. Experimental results demonstrated the robustness and\nsuperior performance of this method.", 
    "link": "http://arxiv.org/pdf/1504.06664v1", 
    "arxiv-id": "1504.06664v1"
},{
    "category": "cs.CE", 
    "author": "Oleg Pianykh", 
    "title": "Perfusion Linearity and Its Applications", 
    "publish": "2010-06-01T15:49:29Z", 
    "summary": "Perfusion analysis computes blood flow parameters (blood volume, blood flow,\nmean transit time) from the observed flow of contrast agent, passing through\nthe patient's vascular system. Perfusion deconvolution has been widely accepted\nas the principal numerical tool for perfusion analysis, and is used routinely\nin clinical applications. This extensive use of perfusion in clinical\ndecision-making makes numerical stability and robustness of perfusion\ncomputations vital for accurate diagnostics and patient safety. The main goal\nof this paper is to propose a novel approach for validating numerical\nproperties of perfusion algorithms. The approach is based on Perfusion\nLinearity Property (PLP), which we find in perfusion deconvolution, as well as\nin many other perfusion techniques. PLP allows one to study perfusion values as\nweighted averages of the original imaging data. This, in turn, uncovers hidden\nproblems with the existing deconvolution techniques, and may be used to suggest\nmore reliable computational approaches and methodology.", 
    "link": "http://arxiv.org/pdf/1006.0168v1", 
    "arxiv-id": "1006.0168v1"
},{
    "category": "cs.CE", 
    "author": "Amir Banookh", 
    "title": "Robust PI Control Design Using Particle Swarm Optimization", 
    "publish": "2010-06-14T19:03:53Z", 
    "summary": "This paper presents a set of robust PI tuning formulae for a first order plus\ndead time process using particle swarm optimization. Also, tuning formulae for\nan integrating process with dead time, which is a special case of a first order\nplus dead time process, is given. The design problem considers three essential\nrequirements of control problems, namely load disturbance rejection, setpoint\nregulation and robustness of closed-loop system against model uncertainties.\nThe primary design goal is to optimize load disturbance rejection. Robustness\nis guaranteed by requiring that the maximum sensitivity is less than or equal\nto a specified value. In the first step, PI controller parameters are\ndetermined such that the IAE criterion to a load disturbance step is minimized\nand the robustness constraint on maximum sensitivity is satisfied. Using a\nstructure with two degrees of freedom which introduces an extra parameter, the\nsetpoint weight, good setpoint regulation is achieved in the second step. The\nmain advantage of the proposed method is its simplicity. Once the equivalent\nfirst order plus dead time model is determined, the PI parameters are\nexplicitly given by a set of tuning formulae. In order to show the performance\nand effectiveness of the proposed tuning formulae, they are applied to three\nsimulation examples.", 
    "link": "http://arxiv.org/pdf/1006.2805v1", 
    "arxiv-id": "1006.2805v1"
},{
    "category": "cs.CE", 
    "author": "Akhileshwar Mishra", 
    "title": "A Metaheuristic Approach for IT Projects Portfolio Optimization", 
    "publish": "2010-06-14T19:07:42Z", 
    "summary": "Optimal selection of interdependent IT Projects for implementation in multi\nperiods has been challenging in the framework of real option valuation. This\npaper presents a mathematical optimization model for multi-stage portfolio of\nIT projects. The model optimizes the value of the portfolio within a given\nbudgetary and sequencing constraints for each period. These sequencing\nconstraints are due to time wise interdependencies among projects. A\nMetaheuristic approach is well suited for solving this kind of a problem\ndefinition and in this paper a genetic algorithm model has been proposed for\nthe solution. This optimization model and solution approach can help IT\nmanagers taking optimal funding decision for projects prioritization in\nmultiple sequential periods. The model also gives flexibility to the managers\nto generate alternative portfolio by changing the maximum and minimum number of\nprojects to be implemented in each sequential period.", 
    "link": "http://arxiv.org/pdf/1006.2806v1", 
    "arxiv-id": "1006.2806v1"
},{
    "category": "cs.CE", 
    "author": "Joshua Shinavier", 
    "title": "Simulating information creation in social Semantic Web applications", 
    "publish": "2010-06-25T07:57:26Z", 
    "summary": "Appropriate ranking algorithms and incentive mechanisms are essential to the\ncreation of high-quality information by users of a social network. However,\nevaluating such mechanisms in a quantifiable way is a difficult problem.\nStudies of live social networks of limited utility, due to the subjective\nnature of ranking and the lack of experimental control. Simulation provides a\nvaluable alternative: insofar as the simulation resembles the live social\nnetwork, fielding a new algorithm within a simulated network can predict the\neffect it will have on the live network. In this paper, we propose a simulation\nmodel based on the actor-conceptinstance model of semantic social networks,\nthen we evaluate the model against a number of common ranking algorithms.We\nobserve their effects on information creation in such a network, and we extend\nour results to the evaluation of generic ranking algorithms and incentive\nmechanisms.", 
    "link": "http://arxiv.org/pdf/1006.4925v1", 
    "arxiv-id": "1006.4925v1"
},{
    "category": "cs.CE", 
    "author": "Mohamad R. Askari", 
    "title": "Defect Diagnosis in Rotors Systems by Vibrations Data Collectors Using   Trending Software", 
    "publish": "2012-08-15T14:18:01Z", 
    "summary": "Vibration measurements have been used to reliably diagnose performance\nproblems in machinery and related mechanical products. A vibration data\ncollector can be used effectively to measure and analyze the machinery\nvibration content in gearboxes, engines, turbines, fans, compressors, pumps and\nbearings. Ideally, a machine will have little or no vibration, indicating that\nthe rotating components are appropriately balanced, aligned, and well\nmaintained. Quick analysis and assessment of the vibration content can lead to\nfault diagnosis and prognosis of a machine's ability to continue running. The\naim of this research used vibration measurements to pinpoint mechanical defects\nsuch as (unbalance, misalignment, resonance, and part loosening), consequently\ndiagnosis all necessary process for engineers and technicians who desire to\nunderstand the vibration that exists in structures and machines.\n  Keywords- vibration data collectors; analysis software; rotating components.", 
    "link": "http://arxiv.org/pdf/1208.3122v1", 
    "arxiv-id": "1208.3122v1"
},{
    "category": "cs.CE", 
    "author": "Mohamad R. Askari", 
    "title": "Calculations of Frequency Response Functions (FRF) Using Computer Smart   Office Software and Nyquist Plot under Gyroscopic Effect Rotation", 
    "publish": "2012-08-16T14:37:35Z", 
    "summary": "Regenerated (FRF curves), synthesis of (FRF) curves there are two main\nrequirement in the form of response model, The first being that of regenerating\n\"Theoretical\" curve for the frequency response function actually measured and\nanalysis and the second being that of synthesising the other functions which\nwere not measured,(FRF) that isolates the inherent dynamic properties of a\nmechanical structure. Experimental modal parameters (frequency, damping, and\nmode shape) are also obtained from a set of (FRF) measurements. The (FRF)\ndescribes the input-output relationship between two points on a structure as a\nfunction of frequency. Therefore, an (FRF) is actually defined between a single\ninput DOF (point & direction), and a single output (DOF), although the FRF was\npreviously defined as a ratio of the Fourier transforms of an output and input\nsignal. In this paper we detection FRF curve using Nyquist plot under\ngyroscopic effect in revolving structure using computer smart office software.\n  Keywords - FRF curve; modal test; Nyquist plot; software engineering;\ngyroscopic effect; smart office.", 
    "link": "http://arxiv.org/pdf/1208.3681v1", 
    "arxiv-id": "1208.3681v1"
},{
    "category": "cs.CE", 
    "author": "Thao Dang", 
    "title": "Analysis of parametric biological models with non-linear dynamics", 
    "publish": "2012-08-19T16:01:06Z", 
    "summary": "In this paper we present recent results on parametric analysis of biological\nmodels. The underlying method is based on the algorithms for computing\ntrajectory sets of hybrid systems with polynomial dynamics. The method is then\napplied to two case studies of biological systems: one is a cardiac cell model\nfor studying the conditions for cardiac abnormalities, and the second is a\nmodel of insect nest-site choice.", 
    "link": "http://arxiv.org/pdf/1208.3849v1", 
    "arxiv-id": "1208.3849v1"
},{
    "category": "cs.CE", 
    "author": "Anne Auger", 
    "title": "Well Placement Optimization under Uncertainty with CMA-ES Using the   Neighborhood", 
    "publish": "2012-09-04T11:52:14Z", 
    "summary": "In the well placement problem, as well as in other field development\noptimization problems, geological uncertainty is a key source of risk affecting\nthe viability of field development projects. Well placement problems under\ngeological uncertainty are formulated as optimization problems in which the\nobjective function is evaluated using a reservoir simulator on a number of\npossible geological realizations. In this paper, we present a new approach to\nhandle geological uncertainty for the well placement problem with a reduced\nnumber of reservoir simulations. The proposed approach uses already simulated\nwell configurations in the neighborhood of each well configuration for the\nobjective function evaluation. We use thus only one single reservoir simulation\nperformed on a randomly chosen realization together with the neighborhood to\nestimate the objective function instead of using multiple simulations on\nmultiple realizations. This approach is combined with the stochastic optimizer\nCMA-ES. The proposed approach is shown on the benchmark reservoir case PUNQ-S3\nto be able to capture the geological uncertainty using a smaller number of\nreservoir simulations. This approach is compared to the reference approach\nusing all the possible realizations for each well configuration, and shown to\nbe able to reduce significantly the number of reservoir simulations (around\n80%).", 
    "link": "http://arxiv.org/pdf/1209.0616v1", 
    "arxiv-id": "1209.0616v1"
},{
    "category": "cs.CE", 
    "author": "Haibin Wang", 
    "title": "C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive   Surveillance System for Prostate Cancer", 
    "publish": "2012-09-12T15:29:12Z", 
    "summary": "Currently there are many clinical trials using paper case report forms as the\nprimary data collection tool. Cloud Computing platforms provide big potential\nfor increasing efficiency through a web-based data collection interface,\nespecially for large-scale multi-center trials. Traditionally, clinical and\nbiological data for multi-center trials are stored in one dedicated,\ncentralized database system running at a data coordinating center (DCC). This\npaper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive\nsurveillance system for prostate cancer. The prototype is developed in PHP,\nJQuery and CSS with an Oracle backend in a local Web server and database server\nand deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The\ndeploying process is fast and easy to follow. The C-PASS-PC prototype can be\naccessed through an SSL-enabled web browser. Our approach proves the concept\nthat cloud computing platforms such as GAE is a suitable and flexible solution\nin the near future for multi-center clinical trials.", 
    "link": "http://arxiv.org/pdf/1209.2641v1", 
    "arxiv-id": "1209.2641v1"
},{
    "category": "cs.CE", 
    "author": "P. M. A. Sloot", 
    "title": "Distributed simulation of city inundation by coupled surface and   subsurface porous flow for urban flood decision support system", 
    "publish": "2013-02-01T23:42:35Z", 
    "summary": "We present a decision support system for flood early warning and disaster\nmanagement. It includes the models for data-driven meteorological predictions,\nfor simulation of atmospheric pressure, wind, long sea waves and seiches; a\nmodule for optimization of flood barrier gates operation; models for stability\nassessment of levees and embankments, for simulation of city inundation\ndynamics and citizens evacuation scenarios. The novelty of this paper is a\ncoupled distributed simulation of surface and subsurface flows that can predict\ninundation of low-lying inland zones far from the submerged waterfront areas,\nas observed in St. Petersburg city during the floods. All the models are\nwrapped as software services in the CLAVIRE platform for urgent computing,\nwhich provides workflow management and resource orchestration.", 
    "link": "http://arxiv.org/pdf/1302.0317v1", 
    "arxiv-id": "1302.0317v1"
},{
    "category": "cs.CE", 
    "author": "Andre O. Falcao", 
    "title": "ThermInfo: Collecting, Retrieving, and Estimating Reliable   Thermochemical Data", 
    "publish": "2013-02-04T14:56:51Z", 
    "summary": "Standard enthalpies of formation are used for assessing the efficiency and\nsafety of chemical processes in the chemical industry. However, the number of\ncompounds for which the enthalpies of formation are available is many orders of\nmagnitude smaller than the number of known compounds. Thermochemical data\nprediction methods are therefore clearly needed. Several commercial and free\nchemical databases are currently available, the NIST WebBook being the most\nused free source. To overcome this problem a cheminformatics system was\ndesigned and built with two main objectives in mind: collecting and retrieving\ncritically evaluated thermochemical values, and estimating new data. In its\npresent version, by using cheminformatics techniques, ThermInfo allows the\nretrieval of the value of a thermochemical property, such as a gas-phase\nstandard enthalpy of formation, by inputting, for example, the molecular\nstructure or the name of a compound. The same inputs can also be used to\nestimate data (presently restricted to non-polycyclic hydrocarbons) by using\nthe Extended Laidler Bond Additivity (ELBA) method. The information system is\npublicly available at http://www.therminfo.com or\nhttp://therminfo.lasige.di.fc.ul.pt. ThermInfo's strength lies in the data\nquality, availability (free access), search capabilities, and, in particular,\nprediction ability, based on a user-friendly interface that accepts inputs in\nseveral formats.", 
    "link": "http://arxiv.org/pdf/1302.0710v1", 
    "arxiv-id": "1302.0710v1"
},{
    "category": "cs.CE", 
    "author": "Harry Boyer", 
    "title": "Optimization of thermal comfort in building through envelope design", 
    "publish": "2013-02-24T19:23:56Z", 
    "summary": "Due to the current environmental situation, energy saving has become the\nleading drive in modern research. Although the residential houses in tropical\nclimate do not use air conditioning to maintain thermal comfort in order to\navoid use of electricity. As the thermal comfort is maintained by adequate\nenvelope composition and natural ventilation, this paper shows that it is\npossible to determine the thickness of envelope layers for which the best\nthermal comfort is obtained. The building is modeled in EnergyPlus software and\nHookeJeves optimization methodology. The investigated house is a typical\nresidential house one-storey high with five thermal zones located at Reunion\nIsland, France. Three optimizations are performed such as the optimization of\nthe thickness of the concrete block layer, of the wood layer, and that of the\nthermal insulation layer. The results show optimal thickness of thermal\nenvelope layers that yield the maximum TC according to Fanger predicted mean\nvote.", 
    "link": "http://arxiv.org/pdf/1302.5941v1", 
    "arxiv-id": "1302.5941v1"
},{
    "category": "cs.CE", 
    "author": "Harry Boyer", 
    "title": "Performances of Low Temperature Radiant Heating Systems", 
    "publish": "2013-02-24T19:24:26Z", 
    "summary": "Low temperature heating panel systems offer distinctive advantages in terms\nof thermal comfort and energy consumption, allowing work with low exergy\nsources. The purpose of this paper is to compare floor, wall, ceiling, and\nfloor-ceiling panel heating systems in terms of energy, exergy and CO2\nemissions. Simulation results for each of the analyzed panel system are given\nby its energy (the consumption of gas for heating, electricity for pumps and\nprimary energy) and exergy consumption, the price of heating, and its carbon\ndioxide emission. Then, the values of the air temperatures of rooms are\ninvestigated and that of the surrounding walls and floors. It is found that the\nfloor-ceiling heating system has the lowest energy, exergy, CO2 emissions,\noperating costs, and uses boiler of the lowest power. The worst system by all\nthese parameters is the classical ceiling heating", 
    "link": "http://arxiv.org/pdf/1302.5942v1", 
    "arxiv-id": "1302.5942v1"
},{
    "category": "cs.CE", 
    "author": "A. W. M. Van Schijndel", 
    "title": "Comsol Simulations of Cracking in Point Loaded Masonry with Randomly   Distributed Material Properties", 
    "publish": "2013-09-17T19:12:06Z", 
    "summary": "This paper describes COMSOL simulations of the stress and crack development\nin the area where a masonry wall supports a floor. In these simulations one of\nthe main material properties of calcium silicate, its E-value, was assigned\nrandomly to the finite elements of the modeled specimen. Calcium silicate is a\nfrequently used building material with a relatively brittle fracture\ncharacteristic. Its initial E-value varies, as well as tensile strength and\npost peak behavior. Therefore, in the simulation, initial E-values were\nrandomly assigned to the elements of the model and a step function used for\ndescribing the descending branch. The method also allows for variation in\nstrength to be taken into account in future research. The performed non-linear\nsimulation results are compared with experimental findings. They show the\nstress distribution and cracking behavior in point loaded masonry when varying\nmaterial properties are used.", 
    "link": "http://arxiv.org/pdf/1309.4429v1", 
    "arxiv-id": "1309.4429v1"
},{
    "category": "cs.CE", 
    "author": "Shunji Homma", 
    "title": "Checkerboard Problem to Topology Optimization of Continuum Structures", 
    "publish": "2013-09-23T01:16:42Z", 
    "summary": "The area of topology optimization of continuum structures of which is allowed\nto change in order to improve the performance is now dominated by methods that\nemploy the material distribution concept. The typical methods of the topology\noptimization based on the structural optimization of two phase composites are\nthe so-called variable density ones, like the SIMP (Solid Isotropic Material\nwith Penalization) and the BESO (Bi-directional Evolutional Structure\nOptimization). The topology optimization problem refers to the saddle-point\nvariation one as well as the so-called Stokes flow problem of the compressive\nfluid. The checkerboard patterns often appear in the results computed by the\nSIMP and the BESO in which the Q1-P0 element is used for FEM (Finite Element\nMethod), since these patterns are more favourable than uniform density regions.\nComputational experiments of SIMP and BESO have shown that filtering of\nsensitivity information of the optimization problem is a highly efficient way\nthat the checkerboard patterns disappeared and to ensure mesh-independency. SIn\nthis paper, we discuss the theoretical basis for the filtering method of the\nSIMP and the BESO and as a result, the filtering method can be understood by\nthe theorem of partition of unity and the convolution operator of low-pass\nfilter.", 
    "link": "http://arxiv.org/pdf/1309.5677v1", 
    "arxiv-id": "1309.5677v1"
},{
    "category": "cs.CE", 
    "author": "Rosemary A Renaut", 
    "title": "Automatic estimation of the regularization parameter in 2-D focusing   gravity inversion: an application to the Safo manganese mine in northwest of   Iran", 
    "publish": "2013-09-30T21:43:25Z", 
    "summary": "We investigate the use of Tikhonov regularization with the minimum support\nstabilizer for underdetermined 2-D inversion of gravity data. This stabilizer\nproduces models with non-smooth properties which is useful for identifying\ngeologic structures with sharp boundaries. A very important aspect of using\nTikhonov regularization is the choice of the regularization parameter that\ncontrols the trade off between the data fidelity and the stabilizing\nfunctional. The L-curve and generalized cross validation techniques, which only\nrequire the relative sizes of the uncertainties in the observations are\nconsidered. Both criteria are applied in an iterative process for which at each\niteration a value for regularization parameter is estimated. Suitable values\nfor the regularization parameter are successfully determined in both cases for\nsynthetic but practically relevant examples. Whenever the geologic situation\npermits, it is easier and more efficient to model the subsurface with a 2-D\nalgorithm, rather than to apply a full 3-D approach. Then, because the problem\nis not large it is appropriate to use the generalized singular value\ndecomposition for solving the problem efficiently. The method is applied on a\nprofile of gravity data acquired over the Safo mining camp in Maku-Iran, which\nis well known for manganese ores. The presented results demonstrate success in\nreconstructing the geometry and density distribution of the subsurface source.", 
    "link": "http://arxiv.org/pdf/1310.0068v2", 
    "arxiv-id": "1310.0068v2"
},{
    "category": "cs.CE", 
    "author": "Dr. Manali Kshirsagar", 
    "title": "Survey on Modelling Methods Applicable to Gene Regulatory Network", 
    "publish": "2013-10-09T05:58:26Z", 
    "summary": "Gene Regulatory Network (GRN) plays an important role in knowing insight of\ncellular life cycle. It gives information about at which different\nenvironmental conditions genes of particular interest get over expressed or\nunder expressed. Modelling of GRN is nothing but finding interactive\nrelationships between genes. Interaction can be positive or negative. For\ninference of GRN, time series data provided by Microarray technology is used.\nKey factors to be considered while constructing GRN are scalability,\nrobustness, reliability and maximum detection of true positive interactions\nbetween genes. This paper gives detailed technical review of existing methods\napplied for building of GRN along with scope for future work.", 
    "link": "http://arxiv.org/pdf/1310.2361v1", 
    "arxiv-id": "1310.2361v1"
},{
    "category": "cs.CE", 
    "author": "Giulio Iovine", 
    "title": "A Probabilistic Approach to Risk Mapping for Mt. Etna", 
    "publish": "2013-10-12T11:08:54Z", 
    "summary": "We evaluate susceptibility to lava flows on Mt. Etna based on specially\ndesigned die-toss experiments using probabilities for type, time and place of\nactivation from the volcano's 400-year recorded history and current studies on\nits known fractures and fissures. The types of activations were forcast using a\ntable of probabilities for events, typed by duration and volume of ejecta.\nLengths of time were represented by the number of activations to expect within\na given time-frame, calculated assuming Poisson-distributed inter-arrival times\nfor activations. Locations of future activations were forecast with a\nprobability distribution function for activation probabilities. Most likely\nscenarios for risk and resulting topography were generated for Etna's next\nactivation (average 7.76 years), the next 25, 50 and 100 years. Forecasts for\nareas most likely affected are in good agreement with previous risk studies\nmade. Forecasts for risks of lava invasions, as well as future topographies\nmight be a first. Threats to lifelines are also discussed.", 
    "link": "http://arxiv.org/pdf/1310.3360v1", 
    "arxiv-id": "1310.3360v1"
},{
    "category": "cs.CE", 
    "author": "Prasanta K. Panigrahi", 
    "title": "Application of Fourier and Wavelet Transform for analysing 300 years   Sunspot numbers to Explain the Solar Cycles", 
    "publish": "2013-10-25T11:00:02Z", 
    "summary": "In this paper Fourier Transform and Wavelet Transform are applied in case of\nrecent 300 years of sunspot numbers to explain the solar cycles. Here basically\nparallel study of Fourier and Wavelet analysis are done and we have observed\nthat the better result can be obtained from Wavelet analysis during sunspot\nnumber analysis. We are able to show various minima and maxima in the recent\nages of solar cycles with this tool. The exact periodicity and other possible\nperiodicities in the cyclic phenomenon of sunspot activity are determined.", 
    "link": "http://arxiv.org/pdf/1310.6876v2", 
    "arxiv-id": "1310.6876v2"
},{
    "category": "cs.CE", 
    "author": "D. Wishart", 
    "title": "Competitive Fragmentation Modeling of ESI-MS/MS spectra for putative   metabolite identification", 
    "publish": "2013-12-01T19:20:57Z", 
    "summary": "Electrospray tandem mass spectrometry (ESI-MS/MS) is commonly used in high\nthroughput metabolomics. One of the key obstacles to the effective use of this\ntechnology is the difficulty in interpreting measured spectra to accurately and\nefficiently identify metabolites. Traditional methods for automated metabolite\nidentification compare the target MS or MS/MS spectrum to the spectra in a\nreference database, ranking candidates based on the closeness of the match.\nHowever the limited coverage of available databases has led to an interest in\ncomputational methods for predicting reference MS/MS spectra from chemical\nstructures.\n  This work proposes a probabilistic generative model for the MS/MS\nfragmentation process, which we call Competitive Fragmentation Modeling (CFM),\nand a machine learning approach for learning parameters for this model from\nMS/MS data. We show that CFM can be used in both a MS/MS spectrum prediction\ntask (ie, predicting the mass spectrum from a chemical structure), and in a\nputative metabolite identification task (ranking possible structures for a\ntarget MS/MS spectrum).\n  In the MS/MS spectrum prediction task, CFM shows significantly improved\nperformance when compared to a full enumeration of all peaks corresponding to\nsubstructures of the molecule. In the metabolite identification task, CFM\nobtains substantially better rankings for the correct candidate than existing\nmethods (MetFrag and FingerID) on tripeptide and metabolite data, when querying\nPubChem or KEGG for candidate structures of similar mass.", 
    "link": "http://arxiv.org/pdf/1312.0264v3", 
    "arxiv-id": "1312.0264v3"
},{
    "category": "cs.CE", 
    "author": "Chanabasayya . M. Vastrad", 
    "title": "Predictive Comparative QSAR Analysis Of As 5-Nitofuran-2-YL Derivatives   Myco bacterium tuberculosis H37RV Inhibitors Bacterium Tuberculosis H37RV   Inhibitors", 
    "publish": "2013-12-10T15:50:39Z", 
    "summary": "Antitubercular activity of 5-nitrofuran-2-yl Derivatives series were\nsubjected to Quantitative Structure Activity Relationship (QSAR) Analysis with\nan effort to derive and understand a correlation between the biological\nactivity as response variable and different molecular descriptors as\nindependent variables. QSAR models are built using 40 molecular descriptor\ndataset. Different statistical regression expressions were got using Partial\nLeast Squares (PLS),Multiple Linear Regression (MLR) and Principal Component\nRegression (PCR) techniques. The among these technique, Partial Least Square\nRegression (PLS) technique has shown very promising result as compared to MLR\ntechnique A QSAR model was build by a training set of 30 molecules with\ncorrelation coefficient ($r^2$) of 0.8484, significant cross validated\ncorrelation coefficient ($q^2$) is 0.0939, F test is 48.5187, ($r^2$) for\nexternal test set (pred$_r^2$) is -0.5604, coefficient of correlation of\npredicted data set (pred$_r^2se$) is 0.7252 and degree of freedom is 26 by\nPartial Least Squares Regression technique.", 
    "link": "http://arxiv.org/pdf/1312.2841v1", 
    "arxiv-id": "1312.2841v1"
},{
    "category": "cs.CE", 
    "author": "Chanabasayya . M. Vastrad", 
    "title": "A Robust Missing Value Imputation Method MifImpute For Incomplete   Molecular Descriptor Data And Comparative Analysis With Other Missing Value   Imputation Methods", 
    "publish": "2013-12-10T16:24:28Z", 
    "summary": "Missing data imputation is an important research topic in data mining.\nLarge-scale Molecular descriptor data may contains missing values (MVs).\nHowever, some methods for downstream analyses, including some prediction tools,\nrequire a complete descriptor data matrix. We propose and evaluate an iterative\nimputation method MiFoImpute based on a random forest. By averaging over many\nunpruned regression trees, random forest intrinsically constitutes a multiple\nimputation scheme. Using the NRMSE and NMAE estimates of random forest, we are\nable to estimate the imputation error. Evaluation is performed on two molecular\ndescriptor datasets generated from a diverse selection of pharmaceutical fields\nwith artificially introduced missing values ranging from 10% to 30%. The\nexperimental result demonstrates that missing values has a great impact on the\neffectiveness of imputation techniques and our method MiFoImpute is more robust\nto missing value than the other ten imputation methods used as benchmark.\nAdditionally, MiFoImpute exhibits attractive computational efficiency and can\ncope with high-dimensional data.", 
    "link": "http://arxiv.org/pdf/1312.2859v1", 
    "arxiv-id": "1312.2859v1"
},{
    "category": "cs.CE", 
    "author": "Chanabasayya . M. Vastrad", 
    "title": "Identification Of Outliers In Oxazolines AND Oxazoles High Dimension   Molecular Descriptor Dataset Using Principal Component Outlier Detection   Algorithm And Comparative Numerical Study Of Other Robust Estimators", 
    "publish": "2013-12-10T16:35:25Z", 
    "summary": "From the past decade outlier detection has been in use. Detection of outliers\nis an emerging topic and is having robust applications in medical sciences and\npharmaceutical sciences. Outlier detection is used to detect anomalous\nbehaviour of data. Typical problems in Bioinformatics can be addressed by\noutlier detection. A computationally fast method for detecting outliers is\nshown, that is particularly effective in high dimensions. PrCmpOut algorithm\nmake use of simple properties of principal components to detect outliers in the\ntransformed space, leading to significant computational advantages for high\ndimensional data. This procedure requires considerably less computational time\nthan existing methods for outlier detection. The properties of this estimator\n(Outlier error rate (FN), Non-Outlier error rate(FP) and computational costs)\nare analyzed and compared with those of other robust estimators described in\nthe literature through simulation studies. Numerical evidence based Oxazolines\nand Oxazoles molecular descriptor dataset shows that the proposed method\nperforms well in a variety of situations of practical interest. It is thus a\nvaluable companion to the existing outlier detection methods.", 
    "link": "http://arxiv.org/pdf/1312.2861v1", 
    "arxiv-id": "1312.2861v1"
},{
    "category": "cs.CE", 
    "author": "Klaus Dietmayer", 
    "title": "Information Maps: A Practical Approach to Position Dependent   Parameterization", 
    "publish": "2013-12-13T13:45:52Z", 
    "summary": "In this contribution a practical approach to determine and store position\ndependent parameters is presented. These parameters can be obtained, among\nothers, using experimental results or expert knowledge and are stored in\n'Information Maps'. Each Information Map can be interpreted as a kind of static\ngrid map and the framework allows to link different maps hierarchically. The\nInformation Maps can be local or global, with static and dynamic information in\nit. One application of Information Maps is the representation of position\ndependent characteristics of a sensor. Thus, for instance, it is feasible to\nstore arbitrary attributes of a sensor's preprocessing in an Information Map\nand utilize them by simply taking the map value at the current position. This\nprocedure is much more efficient than using the attributes of the sensor\nitself. Some examples where and how Information Maps can be used are presented\nin this publication. The Information Map is meant to be a simple and practical\napproach to the problem of position dependent parameterization in all kind of\nalgorithms when the analytical description is not possible or can not be\nimplemented efficiently.", 
    "link": "http://arxiv.org/pdf/1312.3808v1", 
    "arxiv-id": "1312.3808v1"
},{
    "category": "cs.CE", 
    "author": "R. C. Jain", 
    "title": "Computational impact of hydrophobicity in protein stability", 
    "publish": "2013-12-13T16:19:35Z", 
    "summary": "Among the various features of amino acids, the hydrophobic property has most\nvisible impact on stability of a sequence folding. This is mentioned in many\nprotein folding related work, in this paper we more elaborately discuss the\ncomputational impact of the well defined hydrophobic aspect in determining\nstability, approach with the help of a developed free energy computing\nalgorithm covering various aspects preprocessing of an amino acid sequence,\ngenerating the folding and calculating free energy. Later discussing its use in\nprotein structure related research work.", 
    "link": "http://arxiv.org/pdf/1312.3858v1", 
    "arxiv-id": "1312.3858v1"
},{
    "category": "cs.CE", 
    "author": "Radek \u0160tefan", 
    "title": "Hygro-thermo-mechanical analysis of spalling in concrete walls at high   temperatures as a moving boundary problem", 
    "publish": "2014-01-06T17:42:41Z", 
    "summary": "A mathematical model allowing coupled hygro-thermo-mechanical analysis of\nspalling in concrete walls at high temperatures by means of the moving boundary\nproblem is presented. A simplified mechanical approach to account for effects\nof thermal stresses and pore pressure build-up on spalling is incorporated into\nthe model. The numerical algorithm based on finite element discretization in\nspace and the semi-implicit method for discretization in time is presented. The\nvalidity of the developed model is carefully examined by a comparison between\nexperimental tests performed by Kalifa et al. (2000) and Mindeguia (2009) on\nconcrete prismatic specimens under unidirectional heating of temperature of 600\n${\\deg}$C and ISO 834 fire curve and the results obtained from the numerical\nmodel.", 
    "link": "http://arxiv.org/pdf/1401.1152v3", 
    "arxiv-id": "1401.1152v3"
},{
    "category": "cs.CE", 
    "author": "Nimal Rathnayake", 
    "title": "A Simple Software Application for Simulating Commercially Available   Solar Panels", 
    "publish": "2014-01-21T03:20:10Z", 
    "summary": "This article addresses the formulation and validation of a simple PC based\nsoftware application developed for simulating commercially available solar\npanels. The important feature of this application is its capability to produce\nspeedy results in the form of solar panel output characteristics at given\nenvironmental conditions by using minimal input data. Besides, it is able to\ndeliver critical information about the maximum power point of the panel at a\ngiven environmental condition in quick succession. The application is based on\na standard equation which governs solar panels and works by means of estimating\nunknown parameters in the equation to fit a given solar panel. The process of\nparameter estimation is described in detail with the aid of equations and data\nof a commercial solar panel. A validation of obtained results for commercial\nsolar panels is also presented by comparing the panel manufacturers' results\nwith the results generated by the application. In addition, implications of the\nobtained results are discussed along with possible improvements to the\ndeveloped software application.", 
    "link": "http://arxiv.org/pdf/1401.5162v1", 
    "arxiv-id": "1401.5162v1"
},{
    "category": "cs.CE", 
    "author": "Debadatta Dash", 
    "title": "Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG", 
    "publish": "2014-01-22T11:22:19Z", 
    "summary": "EEG monitoring has an important milestone provide valuable information of\nthose candidates who suffer from epilepsy.In this paper human normal and\nepileptic Electroencephalogram signals are analyzed with popular and efficient\nsignal processing techniques like Fourier and Wavelet transform. The delta,\ntheta, alpha, beta and gamma sub bands of EEG are obtained and studied for\ndetection of seizure and epilepsy. The extracted feature is then applied to ANN\nfor classification of the EEG signals.", 
    "link": "http://arxiv.org/pdf/1401.5791v1", 
    "arxiv-id": "1401.5791v1"
},{
    "category": "cs.CE", 
    "author": "Imene Chine", 
    "title": "Modeling the behavior of reinforced concrete walls under fire,   considering the impact of the span on firewalls", 
    "publish": "2014-01-27T07:50:29Z", 
    "summary": "Numerical modeling using computers is known to present several advantages\ncompared to experimental testing. The high cost and the amount of time required\nto prepare and to perform a test were among the main problems on the table when\nthe first tools for modeling structures in fire were developed. The discipline\nstructures-in-fire modeling is still currently the subject of important\nresearch efforts around the word, those research efforts led to develop many\nsoftware. In this paper, our task is oriented to the study of fire behavior and\nthe impact of the span reinforced concrete walls with different sections\nbelonging to a residential building braced by a system composed of porticoes\nand sails. Regarding the design and mechanical loading (compression forces and\nmoments) exerted on the walls in question, we are based on the results of a\nstudy conducted at cold. We use on this subject the software Safir witch obeys\nto the Eurocode laws, to realize this study. It was found that loading,\nheating, and sizing play a capital role in the state of failed walls. Our\nresults justify well the use of reinforced concrete walls, acting as a\nfirewall. Their role is to limit the spread of fire from one structure to\nanother structure nearby, since we get fire resistance reaching more than 10\nhours depending on the loading considered.", 
    "link": "http://arxiv.org/pdf/1401.6759v1", 
    "arxiv-id": "1401.6759v1"
},{
    "category": "cs.CE", 
    "author": "P. M. A. Sloot", 
    "title": "Slope Instability of the Earthen Levee in Boston, UK: Numerical   Simulation and Sensor Data Analysis", 
    "publish": "2014-01-29T19:27:24Z", 
    "summary": "The paper presents a slope stability analysis for a heterogeneous earthen\nlevee in Boston, UK, which is prone to occasional slope failures under tidal\nloads. Dynamic behavior of the levee under tidal fluctuations was simulated\nusing a finite element model of variably saturated linear elastic perfectly\nplastic soil. Hydraulic conductivities of the soil strata have been calibrated\naccording to piezometers readings, in order to obtain correct range of\nhydraulic loads in tidal mode. Finite element simulation was complemented with\nseries of limit equilibrium analyses. Stability analyses have shown that slope\nfailure occurs with the development of a circular slip surface located in the\nsoft clay layer. Both models (FEM and LEM) confirm that the least stable\nhydraulic condition is the combination of the minimum river levels at low tide\nwith the maximal saturation of soil layers. FEM results indicate that in winter\ntime the levee is almost at its limit state, at the margin of safety (strength\nreduction factor values are 1.03 and 1.04 for the low-tide and high-tide\nphases, respectively); these results agree with real-life observations. The\nstability analyses have been implemented as real-time components integrated\ninto the UrbanFlood early warning system for flood protection.", 
    "link": "http://arxiv.org/pdf/1401.7631v1", 
    "arxiv-id": "1401.7631v1"
},{
    "category": "cs.CE", 
    "author": "Alexei Botchkarev", 
    "title": "Estimating the Accuracy of the Return on Investment (ROI) Performance   Evaluations", 
    "publish": "2014-04-08T01:50:15Z", 
    "summary": "Return on Investment (ROI) is one of the most popular performance measurement\nand evaluation metrics. ROI analysis (when applied correctly) is a powerful\ntool in comparing solutions and making informed decisions on the acquisitions\nof information systems. The ROI sensitivity to error is a natural thought, and\ncommon sense suggests that ROI evaluations cannot be absolutely accurate.\nHowever, literature review revealed that in most publications and analyst firms\nreports, this issue is just overlooked. On the one hand, the results of the ROI\ncalculations are implied to be produced with a mathematical rigor, possibility\nof errors is not mentioned and amount of errors is not estimated. On the\ncontrary, another approach claims ROI evaluations to be absolutely inaccurate\nbecause, in view of their authors, future benefits (especially, intangible)\ncannot be estimated within any reasonable boundaries. The purpose of this study\nis to provide a systematic research of the accuracy of the ROI evaluations in\nthe context of the information systems implementations. The main contribution\nof the study is that this is the first systematic effort to evaluate ROI\naccuracy. Analytical expressions have been derived for estimating errors of the\nROI evaluations. Results of the Monte Carlo simulation will help practitioners\nin making informed decisions based on explicitly stated factors influencing the\nROI uncertainties. The results of this research are intended for researchers in\ninformation systems, technology solutions and business management, and also for\ninformation specialists, project managers, program managers, technology\ndirectors, and information systems evaluators. Most results are applicable to\nROI evaluations in a wider subject area.", 
    "link": "http://arxiv.org/pdf/1404.1990v2", 
    "arxiv-id": "1404.1990v2"
},{
    "category": "cs.CE", 
    "author": "Alexander Schliep", 
    "title": "TreQ-CG: Clustering Accelerates High-Throughput Sequencing Read Mapping", 
    "publish": "2014-04-10T16:29:09Z", 
    "summary": "As high-throughput sequencers become standard equipment outside of sequencing\ncenters, there is an increasing need for efficient methods for pre-processing\nand primary analysis. While a vast literature proposes methods for HTS data\nanalysis, we argue that significant improvements can still be gained by\nexploiting expensive pre-processing steps which can be amortized with savings\nfrom later stages. We propose a method to accelerate and improve read mapping\nbased on an initial clustering of possibly billions of high-throughput\nsequencing reads, yielding clusters of high stringency and a high degree of\noverlap. This clustering improves on the state-of-the-art in running time for\nsmall datasets and, for the first time, makes clustering high-coverage human\nlibraries feasible. Given the efficiently computed clusters, only one\nrepresentative read from each cluster needs to be mapped using a traditional\nreadmapper such as BWA, instead of individually mapping all reads. On human\nreads, all processing steps, including clustering and mapping, only require\n11%-59% of the time for individually mapping all reads, achieving speed-ups for\nall readmappers, while minimally affecting mapping quality. This accelerates a\nhighly sensitive readmapper such as Stampy to be competitive with a fast\nreadmapper such as BWA on unclustered reads.", 
    "link": "http://arxiv.org/pdf/1404.2872v1", 
    "arxiv-id": "1404.2872v1"
},{
    "category": "cs.CE", 
    "author": "Mahdi Moeini", 
    "title": "A Continuous Optimization Approach for the Financial Portfolio Selection   under Discrete Asset Choice Constraints", 
    "publish": "2014-04-12T12:30:06Z", 
    "summary": "In this paper we consider a generalization of the Markowitz's Mean-Variance\nmodel under linear transaction costs and cardinality constraints. The\ncardinality constraints are used to limit the number of assets in the optimal\nportfolio. The generalized model is formulated as a mixed integer quadratic\nprogramming (MIP) problem. The purpose of this paper is to investigate a\ncontinuous approach based on difference of convex functions (DC) programming\nfor solving the MIP model. The preliminary comparative results of the proposed\napproach versus CPLEX are presented.", 
    "link": "http://arxiv.org/pdf/1404.3286v1", 
    "arxiv-id": "1404.3286v1"
},{
    "category": "cs.CE", 
    "author": "Mahdi Moeini", 
    "title": "Portfolio Selection Under Buy-In Threshold Constraints Using DC   Programming and DCA", 
    "publish": "2014-04-12T23:50:21Z", 
    "summary": "In matter of Portfolio selection, we consider a generalization of the\nMarkowitz Mean-Variance model which includes buy-in threshold constraints.\nThese constraints limit the amount of capital to be invested in each asset and\nprevent very small investments in any asset. The new model can be converted\ninto a NP-hard mixed integer quadratic programming problem. The purpose of this\npaper is to investigate a continuous approach based on DC programming and DCA\nfor solving this new model. DCA is a local continuous approach to solve a wide\nvariety of nonconvex programs for which it provided quite often a global\nsolution and proved to be more robust and efficient than standard methods.\nPreliminary comparative results of DCA and a classical Branch-and-Bound\nalgorithm will be presented. These results show that DCA is an efficient and\npromising approach for the considered portfolio selection problem.", 
    "link": "http://arxiv.org/pdf/1404.3329v1", 
    "arxiv-id": "1404.3329v1"
},{
    "category": "cs.CE", 
    "author": "Hoai An Le Thi", 
    "title": "A DC programming approach for constrained two-dimensional non-guillotine   cutting problem", 
    "publish": "2014-04-12T23:58:20Z", 
    "summary": "We investigate a new application of Difference of Convex functions\nprogramming and DCA in solving the constrained two-dimensional non-guillotine\ncutting problem. This problem consists of cutting a number of rectangular\npieces from a large rectangular object. The cuts are done under some\nconstraints and the objective is to maximize the total value of the pieces cut.\nWe reformulate this problem as a DC program and solve it by DCA. The\nperformance of the approach is compared with the standard solver CPLEX.", 
    "link": "http://arxiv.org/pdf/1404.3330v1", 
    "arxiv-id": "1404.3330v1"
},{
    "category": "cs.CE", 
    "author": "Antoine Rousseau", 
    "title": "Modeling the wind circulation around mills with a Lagrangian stochastic   approach", 
    "publish": "2014-04-16T15:23:49Z", 
    "summary": "This work aims at introducing model methodology and numerical studies related\nto a Lagrangian stochastic approach applied to the computation of the wind\ncirculation around mills. We adapt the Lagrangian stochastic downscaling method\nthat we have introduced in [3] and [4] to the atmospheric boundary layer and we\nintroduce here a Lagrangian version of the actuator disc methods to take\naccount of the mills. We present our numerical method and numerical experiments\nin the case of non rotating and rotating actuator disc models. We also present\nsome features of our numerical method, in particular the computation of the\nprobability distribution of the wind in the wake zone, as a byproduct of the\nfluid particle model and the associated PDF method.", 
    "link": "http://arxiv.org/pdf/1404.4282v4", 
    "arxiv-id": "1404.4282v4"
},{
    "category": "cs.CE", 
    "author": "Alan Topcic", 
    "title": "Rapid prototyping for sling design optimization", 
    "publish": "2014-04-20T18:31:52Z", 
    "summary": "This paper deals with combination of two modern engineering methods in order\nto optimise the shape of a representative casting product. The product being\nanalysed is a sling, which is used to attach pulling rope in timber\ntransportation. The first step was 3D modelling and static stress/strain\nanalysis using CAD/CAE software NX4. The slinger shape optimization was\nperformed using Traction method, by means of software Optishape-TS. To define\nconstraints for shape optimization, FEA software FEMAP was used. The mould\npattern with optimized 3D shape was then prepared using Fused Deposition\nModelling (FDM) Rapid prototyping method. The sling mass decreased by 20%,\nwhile signifficantly better stress distribution was achieved, with maximum\nstress 3.5 times less than initial value. The future researches should use 3D\nscanning technology in order to provide more accurate 3D model of initial part.\nResults of this research can be used by toolmakers in order to engage FEA/RP\ntechnology to design and manufacture lighter products with acceptable stress\ndistribution.", 
    "link": "http://arxiv.org/pdf/1404.5062v1", 
    "arxiv-id": "1404.5062v1"
},{
    "category": "cs.CE", 
    "author": "Mathias Chung", 
    "title": "Simultaneous Source for non-uniform data variance and missing data", 
    "publish": "2014-04-21T17:55:34Z", 
    "summary": "The use of simultaneous sources in geophysical inverse problems has\nrevolutionized the ability to deal with large scale data sets that are obtained\nfrom multiple source experiments. However, the technique breaks when the data\nhas non-uniform standard deviation or when some data are missing. In this paper\nwe develop, study, and compare a number of techniques that enable to utilize\nadvantages of the simultaneous source framework for these cases. We show that\nthe inverse problem can still be solved efficiently by using these new\ntechniques. We demonstrate our new approaches on the Direct Current Resistivity\ninverse problem.", 
    "link": "http://arxiv.org/pdf/1404.5254v1", 
    "arxiv-id": "1404.5254v1"
},{
    "category": "cs.CE", 
    "author": "SSSN Usha Devi N", 
    "title": "A Fast Multiple Attractor Cellular Automata with Modified Clonal   Classifier for Splicing Site Prediction in Human Genome", 
    "publish": "2014-04-24T03:52:24Z", 
    "summary": "Bioinformatics encompass storing, analyzing and interpreting the biological\ndata. Most of the challenges for Machine Learning methods like Cellular\nAutomata is to furnish the functional information with the corresponding\nbiological sequences. In eukaryotes DNA is divided into introns and exons. The\nintrons will be removed to make the coding region by a process called splicing.\nBy indentifying a splice site we can easily specify the DNA sequence category\n(Donor/Accepter/Neither).Splicing sites play an important role in understanding\nthe genes. A class of CA which can handle fuzzy logic is employed with modified\nclonal algorithm is proposed to identify the splicing site. This classifier is\ntested with Irvine Primate Splice Junction Database. It is compared with\nNNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of\nprediction is quite promising.", 
    "link": "http://arxiv.org/pdf/1404.6020v1", 
    "arxiv-id": "1404.6020v1"
},{
    "category": "cs.CE", 
    "author": "Christina Boucher", 
    "title": "HyDA-Vista: Towards Optimal Guided Selection of k-mer Size for Sequence   Assembly", 
    "publish": "2014-08-24T11:26:37Z", 
    "summary": "Motivation: Intimately tied to assembly quality is the complexity of the de\nBruijn graph built by the assembler. Thus, there have been many paradigms\ndeveloped to decrease the complexity of the de Bruijn graph. One obvious\ncombinatorial paradigm for this is to allow the value of $k$ to vary; having a\nlarger value of $k$ where the graph is more complex and a smaller value of $k$\nwhere the graph would likely contain fewer spurious edges and vertices. One\nopen problem that affects the practicality of this method is how to predict the\nvalue of $k$ prior to building the de Bruijn graph. We show that optimal values\nof $k$ can be predicted prior to assembly by using the information contained in\na phylogenetically-close genome and therefore, help make the use of multiple\nvalues of $k$ practical for genome assembly.\n  Results: We present HyDA-Vista, which is a genome assembler that uses\nhomology information to choose a value of $k$ for each read prior to the de\nBruijn graph construction. The chosen $k$ is optimal if there are no sequencing\nerrors and the coverage is sufficient. Fundamental to our method is the\nconstruction of the {\\em maximal sequence landscape}, which is a data structure\nthat stores for each position in the input string, the largest repeated\nsubstring containing that position. In particular, we show the maximal sequence\nlandscape can be constructed in $O(n + n \\log n)$-time and $O(n)$-space.\nHyDA-Vista first constructs the maximal sequence landscape for a homologous\ngenome. The reads are then aligned to this reference genome, and values of $k$\nare assigned to each read using the maximal sequence landscape and the\nalignments. Eventually, all the reads are assembled by an iterative de Bruijn\ngraph construction method. Our results and comparison to other assemblers\ndemonstrate that HyDA-Vista achieves the best assembly of {\\em E. coli} before\nrepeat resolution or scaffolding.", 
    "link": "http://arxiv.org/pdf/1408.5592v1", 
    "arxiv-id": "1408.5592v1"
},{
    "category": "cs.CE", 
    "author": "Pavel Sakov", 
    "title": "EnKF-C user guide", 
    "publish": "2014-10-06T01:29:42Z", 
    "summary": "EnKF-C provides a light-weight generic framework for off-line data\nassimilation into large-scale layered geophysical models with the ensemble\nKalman filter (EnKF). It is coded in C for GNU/Linux platform and can work\neither in EnKF or ensemble optimal interpolation (EnOI) mode.", 
    "link": "http://arxiv.org/pdf/1410.1233v5", 
    "arxiv-id": "1410.1233v5"
},{
    "category": "cs.CE", 
    "author": "Wim Vanroose", 
    "title": "Constrained Runs algorithm as a lifting operator for the Boltzmann   equation", 
    "publish": "2014-10-16T12:46:27Z", 
    "summary": "Lifting operators play an important role in starting a kinetic Boltzmann\nmodel from given macroscopic information. The macroscopic variables need to be\nmapped to the distribution functions, mesoscopic variables of the Boltzmann\nmodel. A well-known numerical method for the initialization of Boltzmann models\nis the Constrained Runs algorithm. This algorithm is used in literature for the\ninitialization of lattice Boltzmann models, special discretizations of the\nBoltzmann equation. It is based on the attraction of the dynamics toward the\nslow manifold and uses lattice Boltzmann steps to converge to the desired\ndynamics on the slow manifold. We focus on applying the Constrained Runs\nalgorithm to map density, average flow velocity, and temperature, the\nmacroscopic variables, to distribution functions. Furthermore, we do not\nconsider only lattice Boltzmann models. We want to perform the algorithm for\ndifferent discretizations of the Boltzmann equation and consider a standard\nfinite volume discretization.", 
    "link": "http://arxiv.org/pdf/1410.4399v1", 
    "arxiv-id": "1410.4399v1"
},{
    "category": "cs.CE", 
    "author": "Wim Vanroose", 
    "title": "Initialization of lattice Boltzmann models with the help of the   numerical Chapman-Enskog expansion", 
    "publish": "2014-10-16T13:58:04Z", 
    "summary": "We extend the applicability of the numerical Chapman-Enskog expansion as a\nlifting operator for lattice Boltzmann models to map density and momentum to\ndistribution functions. In earlier work [Vanderhoydonc et al. Multiscale Model.\nSimul. 10(3): 766-791, 2012] such an expansion was constructed in the context\nof lifting only the zeroth order velocity moment, namely the density. A lifting\noperator is necessary to convert information from the macroscopic to the\nmesoscopic scale. This operator is used for the initialization of lattice\nBoltzmann models. Given only density and momentum, the goal is to initialize\nthe distribution functions of lattice Boltzmann models. For this\ninitialization, the numerical Chapman-Enskog expansion is used in this paper.", 
    "link": "http://arxiv.org/pdf/1410.4428v1", 
    "arxiv-id": "1410.4428v1"
},{
    "category": "cs.CE", 
    "author": "Oliva Kar", 
    "title": "Data Driven Prognosis: A multi-physics approach verified via balloon   burst experiment", 
    "publish": "2014-10-31T02:05:09Z", 
    "summary": "A multi-physics formulation for Data Driven Prognosis (DDP) is developed.\nUnlike traditional predictive strategies that require controlled off-line\nmeasurements or training for determination of constitutive parameters to derive\nthe transitional statistics, the proposed DDP algorithm relies solely on in\nsitu measurements. It utilizes a deterministic mechanics framework, but the\nstochastic nature of the solution arises naturally from the underlying\nassumptions regarding the order of the conservation potential as well as the\nnumber of dimensions involved. The proposed DDP scheme is capable of predicting\nonset of instabilities. Since the need for off-line testing (or training) is\nobviated, it can be easily implemented for systems where such a priori testing\nis difficult or even impossible to conduct. The prognosis capability is\ndemonstrated here via a balloon burst experiment where the instability is\npredicted utilizing only on-line visual observations. The DDP scheme never\nfailed to predict the incipient failure, and no false positives were issued.\nThe DDP algorithm is applicable to others types of datasets. Time horizons of\nDDP predictions can be adjusted by using memory over different time windows.\nThus, a big dataset can be parsed in time to make a range of predictions over\nvarying time horizons.", 
    "link": "http://arxiv.org/pdf/1410.8616v1", 
    "arxiv-id": "1410.8616v1"
},{
    "category": "cs.CE", 
    "author": "Michal \u0160ejnoha", 
    "title": "Finite element model based on refined plate theories for laminated glass   units", 
    "publish": "2014-10-31T09:00:37Z", 
    "summary": "Laminated glass units exhibit complex response as a result of different\nmechanical behavior and properties of glass and polymer foil. We aim to develop\na finite element model for elastic laminated glass plates based on the refined\nplate theory by Mau. For a geometrically nonlinear description of the behavior\nof units, each layer behaves according to the Reissner-Mindlin kinematics,\ncomplemented with membrane effects and the von K\\'{a}rm\\'{a}n assumptions.\nNodal Lagrange multipliers enforce the compatibility of independent layers in\nthis approach. We have derived the discretized model by the energy-minimization\narguments, assuming that the unknown fields are approximated by bi-linear\nfunctions at the element level, and solved the resulting system by the Newton\nmethod with consistent linearization. We have demonstrated through verification\nand validation examples that the proposed formulation is reliable and\naccurately reproduces the behavior of laminated glass units. This study\nrepresents a first step to the development of a comprehensive, mechanics-based\nmodel for laminated glass systems that is suitable for implementation in common\nengineering finite element solvers.", 
    "link": "http://arxiv.org/pdf/1410.8674v1", 
    "arxiv-id": "1410.8674v1"
},{
    "category": "cs.CE", 
    "author": "Hamid Abrishami Moghaddam", 
    "title": "Finite Element Method Based Modeling of Cardiac Deformation Estimation   under Abnormal Ventricular Muscle Conditions", 
    "publish": "2014-12-13T04:49:31Z", 
    "summary": "Deformation modeling of cardiac muscle is an important issue in the field of\ncardiac analysis. Many approaches have been developed to better estimate the\ncardiac muscle deformation, and to obtain a practical model to be used in\ndiagnostic procedures. But there are some conditions, like in case of\nmyocardial infarction, in which the regular modeling approaches are not useful.\nIn this article, using a point-wise approach, we try to estimate the\ndeformation under some abnormal conditions of cardiac muscle. First, the\nendocardial and epicardial contour points are ordered with respect to the\ncenter of gravity of the endocardial contour and displacement vectors of\nboundary points are extracted. Then to solve the governing equations of the\ndeformation, which is an elliptic equation, we apply boundary conditions in\naccordance with the computed displacement vectors and then the Finite Element\nmethod (FEM) will be used to solve the governing equations. Using the obtained\ndisplacement field of the cardiac muscle, strain map is extracted to show the\nmechanical behavior of the cardiac muscle. Several tests are conducted using\nphantom and real cardiac data in order to show the validity of the proposed\nmethod.", 
    "link": "http://arxiv.org/pdf/1412.4192v3", 
    "arxiv-id": "1412.4192v3"
},{
    "category": "cs.CE", 
    "author": "Sylvain Lavernhe", 
    "title": "Conversion of G-code programs for milling into STEP-NC", 
    "publish": "2014-12-16T20:34:48Z", 
    "summary": "STEP-NC (ISO 14649) is becoming a promising standard to replace or supplement\nthe conventional G-code programs based on ISO 6983 due to its feature based\nmachine independent characteristics and its centric role to enable efficient\nCAD/CAM/CNC interoperability. The re-use of G-code programs is important for\nboth manufacturing and capitalization of machining knowledge, nevertheless the\nconversion is a tedious task when carried out manually and machining knowledge\nis almost hidden in the low level G-code. Mapping G-code into STEP-NC should\nbenefit from more expressiveness of the manufacturing feature-based\ncharacteristics of this new standard. The work presented here proposes an\noverall method for G-code to STEP-NC conversion. First, G-code is converted\ninto canonical machining functions, this can make the method more applicable\nand make subsequent processes easier to implement; then these functions are\nparsed to generate the neutral format of STEP-NC Part21 toolpath file, this\nturns G-code into object instances, and can facilitate company's usage of\nlegacy programs; and finally, also optionally, machining features are extracted\nto generate Part21 CC2 (conformance class) file. The proposed extraction method\nemploys geometric information of cutting area inferred from toolpaths and\nmachining strategies, in addition to cutting tools' data and workpiece's\ndimension data. This comprehensive use of available data makes the extraction\nmore accurate and reliable. The conversion method is holistic, and can be\nextended to process a wide range of G-code programs (e.g. turning or mill-turn\ncodes) with as few user interventions as possible.", 
    "link": "http://arxiv.org/pdf/1412.5496v1", 
    "arxiv-id": "1412.5496v1"
},{
    "category": "cs.CE", 
    "author": "Lidia Dobrescu", 
    "title": "Multiprocessor System Dedicated to Multi-Rotor Mini-UAV Capable of 3D   flying", 
    "publish": "2014-12-19T11:53:38Z", 
    "summary": "The paper describes an electronic multiprocessor system that assures\nfunctionality of a miniature UAV capable of 3D flying. The apparatus consists\nof six independently controlled brushless DC motors, each having a propeller\nattached to it. Since the brushless motor requires complex algorithms in order\nto achieve maximum torque, efficiency and response time a DSP must be used. All\nthe motors are then controlled by a main microprocessor which is capable of\nreading sensors (Inertial Measurement Unit (IMU)-orientation and GPS),\nreceiving input commands (remote controller or trajectory plan) and sending\nindependent commands to each of the six motors. The apparatus contains a total\nof eight microcontrollers: the main unit, the IMU mathematical processor and\none microcontroller for each of the six brushless DC motors. Applications for\nsuch an apparatus could include not only military, but also search-and-rescue,\ngeodetics, aerial photography and aerial assistance.", 
    "link": "http://arxiv.org/pdf/1412.6306v1", 
    "arxiv-id": "1412.6306v1"
},{
    "category": "cs.CE", 
    "author": "Robert Cimrman", 
    "title": "Numerical simulation of liver perfusion: from CT scans to FE model", 
    "publish": "2014-12-19T16:12:02Z", 
    "summary": "We use a collection of Python programs for numerical simulation of liver\nperfusion. We have an application for semi-automatic generation of a finite\nelement mesh of the human liver from computed tomography scans and for\nreconstruction of the liver vascular structure. When the real vascular trees\ncan not be obtained from the CT data we generate artificial trees using the\nconstructive optimization method. The generated FE mesh and vascular trees are\nimported into SfePy (Simple Finite Elements in Python) and numerical\nsimulations are performed in order to get the pressure distribution and\nperfusion flows in the liver tissue. In the post-processing steps we calculate\ntransport of a contrast fluid through the liver parenchyma.", 
    "link": "http://arxiv.org/pdf/1412.6412v1", 
    "arxiv-id": "1412.6412v1"
},{
    "category": "cs.CE", 
    "author": "F. Penunuri", 
    "title": "Synthesis Method for the Spherical 4R Mechanism with Minimum Center of   Mass Acceleration", 
    "publish": "2014-12-22T00:55:39Z", 
    "summary": "In the mechanisms area, minimization of the magnitude of the acceleration of\nthe center of mass (ACoM) implies shaking force balancing. For a mechanism\noperating in cycles, the case when the ACoM is zero implies that the\ngravitational potential energy (GPE) is constant. This article shows an\nefficient and effective optimum synthesis method for minimum acceleration of\nthe center of mass of a spherical 4R mechanism by using dual functions and the\ncounterweights balancing method. Once the dual function for ACoM has been\nwritten, one can minimize the shaking forces from a kinematic point of view. We\npresent the synthesis of a spherical 4R mechanism for the case of a path\ngeneration task. The synthesis process involves the optimization of two\nobjective functions, this multiobjective problem is solved by using the\nweighted sum method implemented in the evolutionary algorithm known as\nDifferential Evolution.", 
    "link": "http://arxiv.org/pdf/1412.6850v2", 
    "arxiv-id": "1412.6850v2"
},{
    "category": "cs.CE", 
    "author": "M. Niranjan", 
    "title": "A Structured Hardware Software Architecture for Peptide Based Diagnosis   of Baylisascaris Procyonis Infection (ICIAfS14)", 
    "publish": "2014-12-25T11:33:59Z", 
    "summary": "The problem of inferring proteins from complex peptide cocktails (digestion\nproducts of biological samples) in shotgun proteomic workflow sets extreme\ndemands on computational resources in respect of the required very high\nprocessing throughputs, rapid processing rates and reliability of results. This\nis exacerbated by the fact that, in general, a given protein cannot be defined\nby a fixed sequence of amino acids due to the existence of splice variants and\nisoforms of that protein. Therefore, the problem of protein inference could be\nconsidered as one of identifying sequences of amino acids with some limited\ntolerance. In the current paper a model-based hardware acceleration of a\nstructured and practical inference approach is developed and validated on a\nmass spectrometry experiment of realistic size. We have achieved 10 times\nmaximum speed-up in the co-designed workflow compared to a similar\nsoftware-only workflow run on the processor used for co-design.", 
    "link": "http://arxiv.org/pdf/1412.7811v1", 
    "arxiv-id": "1412.7811v1"
},{
    "category": "cs.CE", 
    "author": "Kyeong Soo Kim", 
    "title": "Designing pricing schemes based on progressive tariff and consumer   grouping in migration to a future smart grid", 
    "publish": "2014-12-26T11:50:51Z", 
    "summary": "We study the design of pricing schemes for a group of consumers with smart\nmeters (e.g., in a Greenfield area) who are connected through a gateway to a\ntraditional electricity greed with a progressive tariff. Because the\nprogressive tariff cannot take into account the time aspect of electricity\ndemands, we apply it to consumers in both an individual and a group basis over\na shorter time period, which can flatten the overall demand over time and\nthereby reduce peak load. This scenario for the coexistence of traditional and\nsmart girds and the pricing schemes under this scenario can enable smooth\nmigration to a future smart grid.", 
    "link": "http://arxiv.org/pdf/1412.7929v2", 
    "arxiv-id": "1412.7929v2"
},{
    "category": "cs.CE", 
    "author": "Neelima Satyam", 
    "title": "DSSI for pile supported asymmetrical buildings : a review", 
    "publish": "2015-01-07T08:39:52Z", 
    "summary": "With the reference of the several documents in the field of soil structure\ninteraction a document of present and past literature has been made with the\nincluding a main focus on interaction of pile supported frames. This study\nfocuses on the complexity and excessive simplification of the model for\nfoundation system and structures, and should be carried forward for its\nsignificance. The review is carried out including analytical, experimental and\nnumerical approaches considered in the past study. The perusal of literature\nreveals that very few studies investigated on asymmetrical buildings supported\non pile foundations. In this paper, an attempt is made to understand research\ncarried out in pile soil structure interaction and research gap along with the\nscope of research has been identified to carry out the present research work.", 
    "link": "http://arxiv.org/pdf/1501.01392v1", 
    "arxiv-id": "1501.01392v1"
},{
    "category": "cs.CE", 
    "author": "A. Haque", 
    "title": "Large deformation and post-failure simulations of segmental retaining   walls using mesh-free method (SPH)", 
    "publish": "2015-01-16T14:52:42Z", 
    "summary": "Numerical methods are extremely useful in gaining insights into the behaviour\nof reinforced soil retaining walls. However, traditional numerical approaches\nsuch as limit equilibrium or finite element methods are unable to simulate\nlarge deformation and post-failure behaviour of soils and retaining wall blocks\nin the reinforced soil retaining walls system. To overcome this limitation, a\nnovel numerical approach is developed aiming to predict accurately the large\ndeformation and post-failure behaviour of soil and segmental wall blocks.\nHerein, soil is modelled using an elasto-plastic constitutive model, while\nsegmental wall blocks are assumed rigid with full degrees of freedom. A soft\ncontact model is proposed to simulate the interaction between soil-block and\nblock-block. A two dimensional experiment of reinforced soil retaining walls\ncollapse was conducted to verify the numerical results. It is shown that the\nproposed method can simulate satisfactory post-failure behaviour of segmental\nwall blocks in reinforced soil retaining wall systems. The comparison showed\nthat the proposed method can provide satisfactory agreement with experiments.", 
    "link": "http://arxiv.org/pdf/1501.04000v1", 
    "arxiv-id": "1501.04000v1"
},{
    "category": "cs.CE", 
    "author": "Raquel Garcia-Bertrand", 
    "title": "Robust Transmission Network Expansion Planning in Energy Systems:   Improving Computational Performance", 
    "publish": "2015-01-22T12:49:51Z", 
    "summary": "In recent advances in solving the problem of transmission network expansion\nplanning, the use of robust optimization techniques has been put forward, as an\nalternative to stochastic mathematical programming methods, to make the problem\ntractable in realistic systems. Different sources of uncertainty have been\nconsidered, mainly related to the capacity and availability of generation\nfacilities and demand, and making use of adaptive robust optimization models.\nThe mathematical formulations for these models give rise to three-level\nmixed-integer optimization problems, which are solved using different\nstrategies. Although it is true that these robust methods are more efficient\nthan their stochastic counterparts, it is also correct that solution times for\nmixed-integer linear programming problems increase exponentially with respect\nto the size of the problem. Because of this, practitioners and system operators\nneed to use computationally efficient methods when solving this type of\nproblem. In this paper the issue of improving computational performance by\ntaking different features from existing algorithms is addressed. In particular,\nwe replace the lower-level problem with a dual one, and solve the resulting\nbi-level problem using a primal cutting plane algorithm within a decomposition\nscheme. By using this alternative and simple approach, the computing time for\nsolving transmission expansion planning problems has been reduced drastically.\nNumerical results in an illustrative example, the IEEE-24 and IEEE 118-bus test\nsystems demonstrate that the algorithm is superior in terms of computational\nperformance with respect to existing methods.", 
    "link": "http://arxiv.org/pdf/1501.05480v2", 
    "arxiv-id": "1501.05480v2"
},{
    "category": "cs.CE", 
    "author": "Graham Sander", 
    "title": "Estimating the effects of water-induced shallow landslides on soil   erosion", 
    "publish": "2015-01-23T08:44:13Z", 
    "summary": "Rainfall induced landslides and soil erosion are part of a complex system of\nmultiple interacting processes, and both are capable of significantly affecting\nsediment budgets. These sediment mass movements also have the potential to\nsignificantly impact on a broad network of ecosystems health, functionality and\nthe services they provide. To support the integrated assessment of these\nprocesses it is necessary to develop reliable modelling architectures. This\npaper proposes a semi-quantitative integrated methodology for a robust\nassessment of soil erosion rates in data poor regions affected by landslide\nactivity. It combines heuristic, empirical and probabilistic approaches. This\nproposed methodology is based on the geospatial semantic array programming\nparadigm and has been implemented on a catchment scale methodology using\nGeographic Information Systems (GIS) spatial analysis tools and GNU Octave. The\nintegrated data-transformation model relies on a modular architecture, where\nthe information flow among modules is constrained by semantic checks. In order\nto improve computational reproducibility, the geospatial data transformations\nimplemented in ESRI ArcGis are made available in the free software GRASS GIS.\nThe proposed modelling architecture is flexible enough for future\ntransdisciplinary scenario analysis to be more easily designed. In particular,\nthe architecture might contribute as a novel component to simplify future\nintegrated analyses of the potential impact of wildfires or vegetation types\nand distributions, on sediment transport from water induced landslides and\nerosion.", 
    "link": "http://arxiv.org/pdf/1501.05739v1", 
    "arxiv-id": "1501.05739v1"
},{
    "category": "cs.CE", 
    "author": "C. Solares", 
    "title": "Truss Analysis Discussion and Interpretation Using Linear Systems of   Equalities and Inequalities", 
    "publish": "2015-01-27T19:18:00Z", 
    "summary": "This paper shows the complementary roles of mathematical and engineering\npoints of view when dealing with truss analysis problems involving systems of\nlinear equations and inequalities. After the compatibility condition and the\nmathematical structure of the general solution of a system of linear equations\nis discussed, the truss analysis problem is used to illustrate its mathematical\nand engineering multiple aspects, including an analysis of the compatibility\nconditions and a physical interpretation of the general solution, and the\ngenerators of the resulting affine space. Next, the compatibility and the\nmathematical structure of the general solution of linear systems of\ninequalities are analyzed and the truss analysis problem revisited adding some\ninequality constraints, and discussing how they affect the resulting general\nsolution and many other aspects of it. Finally, some conclusions are drawn.", 
    "link": "http://arxiv.org/pdf/1501.06873v1", 
    "arxiv-id": "1501.06873v1"
},{
    "category": "cs.CE", 
    "author": "Ru Zhu", 
    "title": "Accelerate micromagnetic simulations with GPU programming in MATLAB", 
    "publish": "2015-01-25T18:21:58Z", 
    "summary": "A finite-difference Micromagnetic simulation code written in MATLAB is\npresented with Graphics Processing Unit (GPU) acceleration. The high\nperformance of Graphics Processing Unit (GPU) is demonstrated compared to a\ntypical Central Processing Unit (CPU) based code. The speed-up of GPU to CPU is\nshown to be greater than 30 for problems with larger sizes on a mid-end GPU in\nsingle precision. The code is less than 200 lines and suitable for new\nalgorithm developing.", 
    "link": "http://arxiv.org/pdf/1501.07293v1", 
    "arxiv-id": "1501.07293v1"
},{
    "category": "cs.CE", 
    "author": "K. Sako", 
    "title": "A Study of the Matter of SPH Application to Saturated Soil Problems", 
    "publish": "2015-01-30T20:19:29Z", 
    "summary": "We present an application of SPH to saturated soilproblems. Herein, the\nstandard SPH formulation was improved to model saturated soil. It is shown that\nthe proposed formulation could yield several advantages such as: it takes into\naccount the pore-water pressure in an accurate manner, it automatically\nsatisfies the dynamics boundary conditions between submerged soil and water,\nand it reduced the computational cost. Discussions on the use of the standard\nand the new SPH formulations are also given through some numerical tests.\nFurthermore, some techniques to obtained correct SPH solution are also proposed\nand discussed. To the end, this paper suggests that the proposed SPH\nformulation should be considered as the basic formulation for further\ndevelopments of SPH for soil-water couple problems", 
    "link": "http://arxiv.org/pdf/1502.00495v1", 
    "arxiv-id": "1502.00495v1"
},{
    "category": "cs.CE", 
    "author": "Trevor M. Benson", 
    "title": "Modeling Curved Carbon Fiber Composite (CFC) Structures in the   Transmission-Line Modeling (TLM) Method", 
    "publish": "2015-02-04T15:11:57Z", 
    "summary": "A new embedded model for curved thin panels is developed in the Transmission\nLine Modeling (TLM) method. In this model, curved panels are first linearized\nand then embedded between adjacent 2D TLM nodes allowing for arbitrary\npositioning between adjacent node centers. The embedded model eliminates the\nnecessity for fine discretization thus reducing the run time and memory\nrequirements for the calculation. The accuracy and convergence of the model are\nverified by comparing the resonant frequencies of an elliptical cylinder formed\nusing carbon fiber composite (CFC) materials with those of the equivalent metal\ncylinder. Furthermore, the model is used to analyze the shielding performance\nof CFC airfoil NACA2415.", 
    "link": "http://arxiv.org/pdf/1502.01227v1", 
    "arxiv-id": "1502.01227v1"
},{
    "category": "cs.CE", 
    "author": "Snehashish Chakraverty", 
    "title": "Fuzzy finite element solution of uncertain neutron diffusion equation   for imprecisely defined homogeneous triangular bare reactor", 
    "publish": "2015-02-10T09:37:17Z", 
    "summary": "Scattering of neutron collision inside a reactor depends upon geometry of the\nreactor, diffusion coefficient and absorption coefficient etc. In general these\nparameters are not crisp and hence we may get uncertain neutron diffusion\nequation. In this paper we have investigated the above problem for a bare\ntriangular homogeneous reactor. Here the uncertain governing differential\nequation is modelled by a modified fuzzy finite element method using newly\nproposed interval arithmetic. Obtained eigenvalues by the proposed method are\nstudied in detail. Further the eigenvalues are compared with the classical\nfinite element method in special cases and various uncertain results have been\ndiscussed.", 
    "link": "http://arxiv.org/pdf/1502.02824v1", 
    "arxiv-id": "1502.02824v1"
},{
    "category": "cs.CE", 
    "author": "Ronak Sumbaly", 
    "title": "Diagnosis of diabetes using classification mining techniques", 
    "publish": "2015-02-12T19:07:19Z", 
    "summary": "Diabetes has affected over 246 million people worldwide with a majority of\nthem being women. According to the WHO report, by 2025 this number is expected\nto rise to over 380 million. The disease has been named the fifth deadliest\ndisease in the United States with no imminent cure in sight. With the rise of\ninformation technology and its continued advent into the medical and healthcare\nsector, the cases of diabetes as well as their symptoms are well documented.\nThis paper aims at finding solutions to diagnose the disease by analyzing the\npatterns found in the data through classification analysis by employing\nDecision Tree and Na\\\"ive Bayes algorithms. The research hopes to propose a\nquicker and more efficient technique of diagnosing the disease, leading to\ntimely treatment of the patients.", 
    "link": "http://arxiv.org/pdf/1502.03774v1", 
    "arxiv-id": "1502.03774v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Incorporating Spontaneous Reporting System Data to Aid Causal Inference   in Longitudinal Healthcare Data", 
    "publish": "2015-02-20T17:03:36Z", 
    "summary": "Inferring causality using longitudinal observational databases is challenging\ndue to the passive way the data are collected. The majority of associations\nfound within longitudinal observational data are often non-causal and occur due\nto confounding.\n  The focus of this paper is to investigate incorporating information from\nadditional databases to complement the longitudinal observational database\nanalysis. We investigate the detection of prescription drug side effects as\nthis is an example of a causal relationship. In previous work a framework was\nproposed for detecting side effects only using longitudinal data. In this paper\nwe combine a measure of association derived from mining a spontaneous reporting\nsystem database to previously proposed analysis that extracts domain expertise\nfeatures for causal analysis of a UK general practice longitudinal database.\n  The results show that there is a significant improvement to the performance\nof detecting prescription drug side effects when the longitudinal observation\ndata analysis is complemented by incorporating additional drug safety sources\ninto the framework. The area under the receiver operating characteristic curve\n(AUC) for correctly classifying a side effect when other data were considered\nwas 0.967, whereas without it the AUC was 0.923 However, the results of this\npaper may be biased by the evaluation and future work should overcome this by\ndeveloping an unbiased reference set.", 
    "link": "http://arxiv.org/pdf/1502.05938v1", 
    "arxiv-id": "1502.05938v1"
},{
    "category": "cs.CE", 
    "author": "Radim Blaheta", 
    "title": "An improved return-mapping scheme for nonsmooth yield surfaces: PART I -   the Haigh-Westergaard coordinates", 
    "publish": "2015-03-12T06:31:41Z", 
    "summary": "The paper is devoted to the numerical solution of elastoplastic constitutive\ninitial value problems. An improved form of the implicit return-mapping scheme\nfor nonsmooth yield surfaces is proposed that systematically builds on a\nsubdifferential formulation of the flow rule. The main advantage of this\napproach is that the treatment of singular points, such as apices or edges at\nwhich the flow direction is multivalued involves only a uniquely defined set of\nnon-linear equations, similarly to smooth yield surfaces. This paper (PART I)\nis focused on isotropic models containing: $a)$ yield surfaces with one or two\napices (singular points) laying on the hydrostatic axis; $b)$ plastic\npseudo-potentials that are independent of the Lode angle; $c)$ nonlinear\nisotropic hardening (optionally). It is shown that for some models the improved\nintegration scheme also enables to a priori decide about a type of the return\nand investigate existence, uniqueness and semismoothness of discretized\nconstitutive operators in implicit form. Further, the semismooth Newton method\nis introduced to solve incremental boundary-value problems. The paper also\ncontains numerical examples related to slope stability with available Matlab\nimplementation.", 
    "link": "http://arxiv.org/pdf/1503.03605v3", 
    "arxiv-id": "1503.03605v3"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Accurate Impedance Calculation for Underground and Submarine Power   Cables using MoM-SO and a Multilayer Ground Model", 
    "publish": "2015-03-17T18:14:04Z", 
    "summary": "An accurate knowledge of the per-unit length impedance of power cables is\nnecessary to correctly predict electromagnetic transients in power systems. In\nparticular, skin, proximity, and ground return effects must be properly\nestimated. In many applications, the medium that surrounds the cable is not\nuniform and can consist of multiple layers of different conductivity, such as\ndry and wet soil, water, or air. We introduce a multilayer ground model for the\nrecently-proposed MoM-SO method, suitable to accurately predict ground return\neffects in such scenarios. The proposed technique precisely accounts for skin,\nproximity, ground and tunnel effects, and is applicable to a variety of cable\nconfigurations, including underground and submarine cables. Numerical results\nshow that the proposed method is more accurate than analytic formulas typically\nemployed for transient analyses, and delivers an accuracy comparable to the\nfinite element method (FEM). With respect to FEM, however, MoM-SO is over 1000\ntimes faster, and can calculate the impedance of a submarine cable inside a\nthree-layer medium in 0.10~s per frequency point.", 
    "link": "http://arxiv.org/pdf/1503.05146v1", 
    "arxiv-id": "1503.05146v1"
},{
    "category": "cs.CE", 
    "author": "R. Zivanovic", 
    "title": "Abrupt Change Detection in Power System Fault Analysis using Adaptive   Whitening Filter and Wavelet Transform", 
    "publish": "2015-03-18T03:10:41Z", 
    "summary": "This paper describes the application of the adaptive whitening filter and the\nwavelet transform used to detect the abrupt changes in the signals recorded\nduring disturbances in the electrical power network in South Africa. Main focus\nhas been to estimate exactly the time-instants of the changes in the signal\nmodel parameters during the pre-fault condition and following events like\ninitiation of fault, circuit-breaker opening, auto-reclosure of the\ncircuit-breakers. The key idea is to decompose the fault signals, de-noised\nusing the adaptive whitening filter, into effective detailed and smoothed\nversion using the multiresolution signal decomposition technique based on\ndiscrete wavelet transform. Then we apply the threshold method on the\ndecomposed signals to estimate the change time-instants, segmenting the fault\nsignals into the event-specific sections for further signal processing and\nanalysis. This paper presents application on the recorded signals in the power\ntransmission network of South Africa.", 
    "link": "http://arxiv.org/pdf/1503.05275v1", 
    "arxiv-id": "1503.05275v1"
},{
    "category": "cs.CE", 
    "author": "R\u00e9gis Cottereau", 
    "title": "On damping created by heterogeneous yielding in the numerical analysis   of nonlinear reinforced concrete frame elements", 
    "publish": "2015-03-21T19:34:49Z", 
    "summary": "In the dynamic analysis of structural engineering systems, it is common\npractice to introduce damping models to reproduce experimentally observed\nfeatures. These models, for instance Rayleigh damping, account for the damping\nsources in the system altogether and often lack physical basis. We report on an\nalternative path for reproducing damping coming from material nonlinear\nresponse through the consideration of the heterogeneous character of material\nmechanical properties. The parameterization of that heterogeneity is performed\nthrough a stochastic model. It is shown that such a variability creates the\npatterns in the concrete cyclic response that are classically regarded as\nsource of damping.", 
    "link": "http://arxiv.org/pdf/1503.07122v1", 
    "arxiv-id": "1503.07122v1"
},{
    "category": "cs.CE", 
    "author": "S. Chakraverty", 
    "title": "Numerical solution of moving plate problem with uncertain parameters", 
    "publish": "2015-03-14T12:31:02Z", 
    "summary": "This paper deals with uncertain parabolic fluid flow problem where the\nuncertainty occurs due to the initial conditions and parameters involved in the\nsystem. Uncertain values are considered as fuzzy and these are handled through\na recently developed method. Here the concepts of fuzzy numbers are combined\nwith Finite Difference Method (FDM) and then Fuzzy Finite Difference Method\n(FFDM) has been proposed. The proposed FFDM has been used to solve the fluid\nflow problem bounded by two parallel plates. Finally sensitivity of the fuzzy\nparameters has also been analysed.", 
    "link": "http://arxiv.org/pdf/1503.07809v1", 
    "arxiv-id": "1503.07809v1"
},{
    "category": "cs.CE", 
    "author": "Jean-Claude Mipo", 
    "title": "Energetic Galerkin Projection of Electromagnetic Fields between   Different Meshes", 
    "publish": "2015-05-05T18:29:28Z", 
    "summary": "In order to project electromagnetic fields between different meshes with\nrespect to the conservation of energetic values, Galerkin projection\nformulations based on the energetic norm are developed in this communication.\nThe proposed formulations are applied to an academic example.", 
    "link": "http://arxiv.org/pdf/1505.01118v1", 
    "arxiv-id": "1505.01118v1"
},{
    "category": "cs.CE", 
    "author": "Phillip Lord", 
    "title": "Scaffolding the Mitochondrial Disease Ontology from extant knowledge   sources", 
    "publish": "2015-05-15T16:35:43Z", 
    "summary": "Bio-medical ontologies can contain a large number of concepts. Often many of\nthese concepts are very similar to each other, and similar or identical to\nconcepts found in other bio-medical databases. This presents both a challenge\nand opportunity: maintaining many similar concepts is tedious and fastidious\nwork, which could be substantially reduced if the data could be derived from\npre-existing knowledge sources. In this paper, we describe how we have achieved\nthis for an ontology of the mitochondria using our novel ontology development\nenvironment, the Tawny-OWL library.", 
    "link": "http://arxiv.org/pdf/1505.04114v1", 
    "arxiv-id": "1505.04114v1"
},{
    "category": "cs.CE", 
    "author": "Emanuel Diamant", 
    "title": "Advances in Bioinformatics and Computational Biology: Don't take them   too seriously anyway", 
    "publish": "2015-05-18T10:18:20Z", 
    "summary": "In the last few decades or so, we witness a paradigm shift in our nature\nstudies - from a data-processing based computational approach to an\ninformation-processing based cognitive approach. The process is restricted and\noften misguided by the lack of a clear understanding about what information is\nand how it should be treated in research applications (in general) and in\nbiological studies (in particular). The paper intend to provide some remedies\nfor this bizarre situation.", 
    "link": "http://arxiv.org/pdf/1505.04785v1", 
    "arxiv-id": "1505.04785v1"
},{
    "category": "cs.CE", 
    "author": "Raghad Zuhair Yousif", 
    "title": "Novel Mining of Cancer via Mutation in Tumor Protein P53 using Quick   Propagation Network", 
    "publish": "2015-05-20T15:46:45Z", 
    "summary": "There is multiple databases contain datasets of TP53 gene and its tumor\nprotein P53 which believed to be involved in over 50% of human cancers cases,\nthese databases are rich as datasets covered all mutations caused diseases\n(cancers), but they haven't efficient mining method can classify and diagnosis\nmutations patient's then predict the cancer of that patient. This paper\nproposed a novel mining of cancer via mutations because there is no mining\nmethod before offers friendly, effective and flexible predict or diagnosis of\ncancers via using whole common database of TP53 gene (tumor protein P53) as\ndataset and selecting a minimum number of fields in training and testing quick\npropagation algorithm which supporting this miming method. Simulating quick\npropagation network for the train dataset shows results the Correlation\n(0.9999), R-squared (0.9998) and mean of Absolute Relative Error (0.0029),\nwhile the training for the ALL datasets (train, test and validation dataset)\nhave results the Correlation (0.9993), R-squared (0.9987) and mean of Absolute\nRelative Error (0.0057).", 
    "link": "http://arxiv.org/pdf/1505.06751v1", 
    "arxiv-id": "1505.06751v1"
},{
    "category": "cs.CE", 
    "author": "Aristides T. Hatjimihail", 
    "title": "Calculation of the confidence bounds for the fraction nonconforming of   normal populations of measurements in clinical laboratory medicine", 
    "publish": "2015-06-01T17:04:23Z", 
    "summary": "The fraction nonconforming is a key quality measure used in statistical\nquality control design in clinical laboratory medicine. The confidence bounds\nof normal populations of measurements for the fraction nonconforming each of\nthe lower and upper quality specification limits when both the random and the\nsystematic error are unknown can be calculated using the noncentral\nt-distribution, as it is described in detail and illustrated with examples.", 
    "link": "http://arxiv.org/pdf/1506.00571v3", 
    "arxiv-id": "1506.00571v3"
},{
    "category": "cs.CE", 
    "author": "Erik Lindahl", 
    "title": "Tackling Exascale Software Challenges in Molecular Dynamics Simulations   with GROMACS", 
    "publish": "2015-06-02T00:37:50Z", 
    "summary": "GROMACS is a widely used package for biomolecular simulation, and over the\nlast two decades it has evolved from small-scale efficiency to advanced\nheterogeneous acceleration and multi-level parallelism targeting some of the\nlargest supercomputers in the world. Here, we describe some of the ways we have\nbeen able to realize this through the use of parallelization on all levels,\ncombined with a constant focus on absolute performance. Release 4.6 of GROMACS\nuses SIMD acceleration on a wide range of architectures, GPU offloading\nacceleration, and both OpenMP and MPI parallelism within and between nodes,\nrespectively. The recent work on acceleration made it necessary to revisit the\nfundamental algorithms of molecular simulation, including the concept of\nneighborsearching, and we discuss the present and future challenges we see for\nexascale simulation - in particular a very fine-grained task parallelism. We\nalso discuss the software management, code peer review and continuous\nintegration testing required for a project of this complexity.", 
    "link": "http://arxiv.org/pdf/1506.00716v1", 
    "arxiv-id": "1506.00716v1"
},{
    "category": "cs.CE", 
    "author": "L. M. Musabekova", 
    "title": "Modeling of through-reactors with allowance of Large-Scale Effect on   Heat and Mass Efficiency of Chemical Apparatuses", 
    "publish": "2015-06-03T10:12:55Z", 
    "summary": "This paper deals also with a problem of gas absorption accompanied by an\ninstantaneous, irreversible reaction in the liquid layer. The well-known\nmethods for calculating such processes are based usually on the certain\namendments to solutions, which are obtained disregarding the chemical reaction.\nUnlike the known work (1. D. Baetens, R. Van Keer, L.H. Hosten. Gas-liquid\nreaction: absorption accompanied by an instantaneous, irreversible reaction//\nMoving Boundaries IV, Southampton, Boston, 1997) the approach we used takes\ninto account the influence of reaction resulting product on the arising and\nvelocity of a moving reaction plane. The known results in the theory of\nchemical apparatuses scaling are devoted to apparatuses with non-regular\npackings mainly. However how the phases distribution over the regular packings\nof chemical columns effects the heat and mass efficiency is studied lesser.\nThis paper deals with the methods of simulation the scaling effects applying to\nchemical towers with regular packings of various types. The models for\ndescribing the influence of initial liquid and gas distribution in chemical\ncolumns with regular packing on the mass transfer efficiency have been\nsubmitted. The sufficiently simple methods for evaluating the influence of\nlarge-scale factor on the efficiency of mass transfer have been obtained. These\nmethods are suitable for use in engineering calculation techniques.", 
    "link": "http://arxiv.org/pdf/1506.01190v1", 
    "arxiv-id": "1506.01190v1"
},{
    "category": "cs.CE", 
    "author": "Matthew D Piggott", 
    "title": "A correction to the enhanced bottom drag parameterisation of tidal   turbines", 
    "publish": "2015-06-11T10:18:17Z", 
    "summary": "Hydrodynamic modelling is an important tool for the development of tidal\nstream energy projects. Many hydrodynamic models incorporate the effect of\ntidal turbines through an enhanced bottom drag. In this paper we show that\nalthough for coarse grid resolutions (kilometre scale) the resulting force\nexerted on the flow agrees well with the theoretical value, the force starts\ndecreasing with decreasing grid sizes when these become smaller than the length\nscale of the wake recovery. This is because the assumption that the upstream\nvelocity can be approximated by the local model velocity, is no longer valid.\nUsing linear momentum actuator disc theory however, we derive a relationship\nbetween these two velocities and formulate a correction to the enhanced bottom\ndrag formulation that consistently applies a force that remains closed to the\ntheoretical value, for all grid sizes down to the turbine scale. In addition, a\nbetter understanding of the relation between the model, upstream, and actual\nturbine velocity, as predicted by actuator disc theory, leads to an improved\nestimate of the usefully extractable energy. We show how the corrections can be\napplied (demonstrated here for the models MIKE 21 and Fluidity) by a simple\nmodification of the drag coefficient.", 
    "link": "http://arxiv.org/pdf/1506.03611v1", 
    "arxiv-id": "1506.03611v1"
},{
    "category": "cs.CE", 
    "author": "Fangxing Li", 
    "title": "An Optimal Framework for Residential Load Aggregator", 
    "publish": "2015-06-14T22:30:14Z", 
    "summary": "Due to the development of intelligent demand-side management with automatic\ncontrol, distributed populations of large residential loads, such as air\nconditioners (ACs) and electrical water heaters (EWHs), have the opportunities\nto provide effective demand-side ancillary services for load serving entities\n(LSEs) to reduce the emissions and network operating costs. Most present\napproaches are restricted to 1) the scenarios involving with efficiently\nscheduling the large number of appliances in real time, 2) the issues about\nevaluating the contributions of individual residents towards participating\ndemand response (DR) program, and fairly distributing the rewards, and 3) the\nconcerns on performing cost-effective demand reduction request (DRR) for LSEs\nwith minimal rewards costs while not affecting their living comfortableness.\nTherefore, this paper presents an optimal framework for residential load\naggregators (RLAs) which helps solve the problems mentioned above. Under this\nframework, RLAs are able to realize the DRR for LSEs to generate optimal\ncontrol strategies over residential appliances quickly and efficiently. To\nresidents, the framework is designed with probabilistic model of\ncomfortableness, which minimizes the impact of DR program to their daily life.\nTo LSEs, the framework helps minimize the total reward costs of performing\nDRRs. Moreover, the framework fairly and strategically distributes the\nfinancial rewards to residents, which may stimulate the potential capability of\nloads optimized and controlled by RLAs in demand side management. The proposed\nframework has been validated on several numerical case studies.", 
    "link": "http://arxiv.org/pdf/1506.04447v1", 
    "arxiv-id": "1506.04447v1"
},{
    "category": "cs.CE", 
    "author": "Pascal Van Hentenryck", 
    "title": "Convex Relaxations for Gas Expansion Planning", 
    "publish": "2015-06-24T00:04:44Z", 
    "summary": "Expansion of natural gas networks is a critical process involving substantial\ncapital expenditures with complex decision-support requirements. Given the\nnon-convex nature of gas transmission constraints, global optimality and\ninfeasibility guarantees can only be offered by global optimisation approaches.\nUnfortunately, state-of-the-art global optimisation solvers are unable to scale\nup to real-world size instances. In this study, we present a convex\nmixed-integer second-order cone relaxation for the gas expansion planning\nproblem under steady-state conditions. The underlying model offers tight lower\nbounds with high computational efficiency. In addition, the optimal solution of\nthe relaxation can often be used to derive high-quality solutions to the\noriginal problem, leading to provably tight optimality gaps and, in some cases,\nglobal optimal soluutions. The convex relaxation is based on a few key ideas,\nincluding the introduction of flux direction variables, exact McCormick\nrelaxations, on/off constraints, and integer cuts. Numerical experiments are\nconducted on the traditional Belgian gas network, as well as other real larger\nnetworks. The results demonstrate both the accuracy and computational speed of\nthe relaxation and its ability to produce high-quality solutions.", 
    "link": "http://arxiv.org/pdf/1506.07214v1", 
    "arxiv-id": "1506.07214v1"
},{
    "category": "cs.CE", 
    "author": "Jaderick P. Pabico", 
    "title": "Selecting the Best Traffic Scheme for the Bicutan Roundabout: A   Microsimulation Approach with Multiple Driver-Agents", 
    "publish": "2015-06-24T15:07:12Z", 
    "summary": "We present the result of our microsimulation study on the effects of six\ntraffic schemes $T=\\{t_0, t_1, \\dots, t_5\\}$ on the mean total delay time\n($\\Delta$) and mean speed ($\\Sigma$) of vehicles at the non-signalized Bicutan\nRoundabout (BR) in Upper Bicutan, Taguig City, Metro Manila, with $t_0$ as the\ncurrent traffic scheme being enforced and $t_{i>0}$ as the proposed ones. We\npresent first that our simulation approach, a hybridized multi-agent system\n(MAS) with the car-following and lane-changing models (CLM), can mimic the\ncurrent observed traffic scenario $C$ at a statistical significance of\n$\\alpha=0.05$. That is, the respective absolute differences of the $\\Delta$ and\n$\\Sigma$ between $C$ and $t_0$ are not statistically different from zero. Next,\nusing our MAS-CLM, we simulated all proposed $t_{i>0}$ and compared their\nrespective $\\Delta$ and $\\Sigma$. We found out using DMRT that the best traffic\nscheme is $t_3$ (i.e., when we converted the bi-directional 4-lane PNR-PNCC\nroad into a bi-directional 1-lane PNR-to-PNCC and 3-lane PNCC-to-PNR routes\nduring rush hours). Then, we experimented on converting BR into a signalized\njunction and re-implemented all $t_3$ with controlled stops of $S=\\{15s,\n45s\\}$. We found out that $t_3$ with a 15-s stop has the best performance.\nFinally, we simulated the effect of increased in vehicular volume $V$ due to\ntraffic improvement and we found out that $t_3$ with 15-s stop still\noutperforms the others for all increased in $V=\\{10\\%, 50\\%, 100\\%\\}$.", 
    "link": "http://arxiv.org/pdf/1506.07411v1", 
    "arxiv-id": "1506.07411v1"
},{
    "category": "cs.CE", 
    "author": "Jaderick P. Pabico", 
    "title": "Simulating the Effects of Various Road Infrastructure Improvements to   Vehicular Traffic in a Busy Three-road Fork", 
    "publish": "2015-06-24T15:36:11Z", 
    "summary": "Using microsimulations of vehicular dynamics, we studied the effects of\nseveral proposed infrastructure developments to the mean travel delay\ntime~$\\Delta$ and mean speed~$\\Sigma$ of vehicles passing a busy three-road\nfork, particularly in the non-signalized roundabout junction of Lower Bicutan,\nTaguig City, Metro Manila. We designed and implemented multi-agent-based\nmicrosimulation models to mimic the autonomous driving behavior of\nheterogeneous individuals and measured the effect of various proposed\ninfrastructure developments on~$\\Delta$ and~$\\Sigma$. Our aim is to find out\nthe best infrastructure development from among three choices being considered\nby the local government for the purpose of solving the traffic problems in the\narea. We created simulation models of the current vehicular traffic situation\nin the area using the mean travel times~$\\tau$ of statistically sampled\nvehicles to show that our model can simulate the real-world at a significance\nlevel of $\\alpha=0.05$. Based on these models, we then simulated the effect of\nthe proposed infrastructure developments on~$\\Delta$ and~$\\Sigma$ and used\nthese metrics as our basis of comparison. We found out that the proposed\nwidening of one fork from two lanes to three lanes has the most improved\nmetrics at the same $\\alpha=0.05$ compared to the metrics we observed in the\ncurrent situation. Under this infrastructure development, the~$\\Delta$\nincreases linearly ($R^2=0.98$) at the rate of 1.03~$s$, while the~$\\Sigma$\ndecreases linearly ($R^2>0.99$) at the rate of 0.14~$km/h$ per percent increase\nin the total vehicle volume~$\\mathcal{V}$.", 
    "link": "http://arxiv.org/pdf/1506.07424v1", 
    "arxiv-id": "1506.07424v1"
},{
    "category": "cs.CE", 
    "author": "Brian P. Hanley", 
    "title": "A zero-sum monetary system, interest rates, and implications", 
    "publish": "2015-06-26T22:23:22Z", 
    "summary": "To the knowledge of the author, this is the first time it has been shown that\ninterest rates that are extremely high by modern standards are necessary within\na zero-sum monetary system. Extreme interest rates persisted for long periods\nof time in many places. Prior to the invention of banking, most money was\nhard-money in the form of some type of coin. Here a model is presented that\nexamines the interest rate required to succeed as an investor in a zero-sum\nhard-money system. Even when the playing field is significantly tilted toward\nthe investor, interest rates need to be much higher than expected. In a\ncompletely fair zero-sum system, an investor cannot break even without charging\n100% interest. Even with a 5% advantage, an investor will not break even at 15%\ninterest. From this it is concluded that what we consider usurious rates today\nare, within a hard-money system, driven by necessity.\n  Cryptocurrency is a novel form of hard-currency. The inability to virtualize\nthe money creates a system close to zero-sum. Therefore, within the bounds of a\ncryptocurrency system that limits money creation, interest rates must rise to\nlevels that the modern world considers usury. It is impossible, therefore, that\na cryptocurrency that is not expandable could take over a modern economy and\nreplace modern fiat currency.", 
    "link": "http://arxiv.org/pdf/1506.08231v1", 
    "arxiv-id": "1506.08231v1"
},{
    "category": "cs.CE", 
    "author": "Parisa Shokouhi", 
    "title": "Decision-level multi-method fusion of spatially scattered data from   nondestructive inspection of ferromagnetic parts", 
    "publish": "2015-06-30T09:13:13Z", 
    "summary": "This article deals with the fusion of flaw detections from multi-sensor\nnondestructive materials testing. Because each testing method makes use of\ndifferent physical effects for defect localization, a multi-method approach is\npromising to effectively distinguish the many false alarms from actual material\ndefects. To this end, we propose a new fusion technique for scattered two- or\nthree-dimensional location data. Using a density-based approach, the proposed\nmethod is able to explicitly address the localization uncertainties such as\nregistration errors. We provide guidelines on how to set all key parameters and\ndemonstrate the technique's robustness. Finally, we apply our fusion approach\nto experimental data and demonstrate its ability to find small defects by\nsubstantially reducing false alarms under conditions where no single-sensor\nmethod is adequate.", 
    "link": "http://arxiv.org/pdf/1506.09000v1", 
    "arxiv-id": "1506.09000v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "TurboMOR: an Efficient Model Order Reduction Technique for RC Networks   with Many Ports", 
    "publish": "2015-07-01T13:12:38Z", 
    "summary": "Model order reduction (MOR) techniques play a crucial role in the\ncomputer-aided design of modern integrated circuits, where they are used to\nreduce the size of parasitic networks. Unfortunately, the efficient reduction\nof passive networks with many ports is still an open problem. Existing\ntechniques do not scale well with the number of ports, and lead to dense\nreduced models that burden subsequent simulations. In this paper, we propose\nTurboMOR, a novel MOR technique for the efficient reduction of passive RC\nnetworks. TurboMOR is based on moment-matching, achieved through efficient\ncongruence transformations based on Householder reflections. A novel feature of\nTurboMOR is the block-diagonal structure of the reduced models, that makes them\nmore efficient than the dense models produced by existing techniques. Moreover,\nthe model structure allows for an insightful interpretation of the reduction\nprocess in terms of system theory. Numerical results show that TurboMOR scales\nmore favourably than existing techniques in terms of reduction time, simulation\ntime and memory consumption.", 
    "link": "http://arxiv.org/pdf/1507.00219v1", 
    "arxiv-id": "1507.00219v1"
},{
    "category": "cs.CE", 
    "author": "Yavor Vutov", 
    "title": "On pore-scale modeling and simulation of reactive transport in 3D   geometries", 
    "publish": "2015-07-07T17:38:44Z", 
    "summary": "Pore-scale modeling and simulation of reactive flow in porous media has a\nrange of diverse applications, and poses a number of research challenges. It is\nknown that the morphology of a porous medium has significant influence on the\nlocal flow rate, which can have a substantial impact on the rate of chemical\nreactions. While there are a large number of papers and software tools\ndedicated to simulating either fluid flow in 3D computerized tomography (CT)\nimages or reactive flow using pore-network models, little attention to date has\nbeen focused on the pore-scale simulation of sorptive transport in 3D CT\nimages, which is the specific focus of this paper. Here we first present an\nalgorithm for the simulation of such reactive flows directly on images, which\nis implemented in a sophisticated software package. We then use this software\nto present numerical results in two resolved geometries, illustrating the\nimportance of pore-scale simulation and the flexibility of our software\npackage.", 
    "link": "http://arxiv.org/pdf/1507.01894v2", 
    "arxiv-id": "1507.01894v2"
},{
    "category": "cs.CE", 
    "author": "Ulrich R\u00fcde", 
    "title": "Large scale lattice Boltzmann simulation for the coupling of free and   porous media flow", 
    "publish": "2015-07-23T17:03:20Z", 
    "summary": "In this work, we investigate the interaction of free and porous media flow by\nlarge scale lattice Boltzmann simulations. We study the transport phenomena at\nthe porous interface on multiple scales, i.e., we consider both,\ncomputationally generated pore-scale geometries and homogenized models at a\nmacroscopic scale. The pore-scale results are compared to those obtained by\nusing different transmission models. Two-domain approaches with sharp interface\nconditions, e.g., of Beavers--Joseph--Saffman type, as well as a single-domain\napproach with a porosity depending viscosity are taken into account. For the\npore-scale simulations, we use a highly scalable communication-reducing scheme\nwith a robust second order boundary handling. We comment on computational\naspects of the pore-scale simulation and on how to generate pore-scale\ngeometries. The two-domain approaches depend sensitively on the choice of the\nexact position of the interface, whereas a well-designed single-domain approach\ncan significantly better recover the averaged pore-scale results.", 
    "link": "http://arxiv.org/pdf/1507.06565v1", 
    "arxiv-id": "1507.06565v1"
},{
    "category": "cs.CE", 
    "author": "Andreas Hellander", 
    "title": "MOLNs: A cloud platform for interactive, reproducible and scalable   spatial stochastic computational experiments in systems biology using PyURDME", 
    "publish": "2015-08-14T18:49:36Z", 
    "summary": "Computational experiments using spatial stochastic simulations have led to\nimportant new biological insights, but they require specialized tools, a\ncomplex software stack, as well as large and scalable compute and data analysis\nresources due to the large computational cost associated with Monte Carlo\ncomputational workflows. The complexity of setting up and managing a\nlarge-scale distributed computation environment to support productive and\nreproducible modeling can be prohibitive for practitioners in systems biology.\nThis results in a barrier to the adoption of spatial stochastic simulation\ntools, effectively limiting the type of biological questions addressed by\nquantitative modeling. In this paper, we present PyURDME, a new, user-friendly\nspatial modeling and simulation package, and MOLNs, a cloud computing appliance\nfor distributed simulation of stochastic reaction-diffusion models. MOLNs is\nbased on IPython and provides an interactive programming platform for\ndevelopment of sharable and reproducible distributed parallel computational\nexperiments.", 
    "link": "http://arxiv.org/pdf/1508.03604v1", 
    "arxiv-id": "1508.03604v1"
},{
    "category": "cs.CE", 
    "author": "Chandrajit Bajaj", 
    "title": "Quantifying and Visualizing Uncertainties in Molecular Models", 
    "publish": "2015-08-17T00:18:08Z", 
    "summary": "Computational molecular modeling and visualization has seen significant\nprogress in recent years with sev- eral molecular modeling and visualization\nsoftware systems in use today. Nevertheless the molecular biology community\nlacks techniques and tools for the rigorous analysis, quantification and\nvisualization of the associated errors in molecular structure and its\nassociated properties. This paper attempts at filling this vacuum with the\nintroduction of a systematic statistical framework where each source of\nstructural uncertainty is modeled as a ran- dom variable (RV) with a known\ndistribution, and properties of the molecules are defined as dependent RVs. The\nframework consists of a theoretical basis, and an empirical implementation\nwhere the uncertainty quantification (UQ) analysis is achieved by using\nChernoff-like bounds. The framework enables additionally the propagation of\ninput structural data uncertainties, which in the molecular protein world are\ndescribed as B-factors, saved with almost all X-ray models deposited in the\nProtein Data Bank (PDB). Our statistical framework is also able and has been\napplied to quantify and visualize the uncertainties in molecular properties,\nnamely solvation interfaces and solvation free energy estimates. For each of\nthese quantities of interest (QOI) of the molecular models we provide several\nnovel and intuitive visualizations of the input, intermediate, and final\npropagated uncertainties. These methods should enable the end user achieve a\nmore quantitative and visual evaluation of various molecular PDB models for\nstructural and property correctness, or the lack thereof.", 
    "link": "http://arxiv.org/pdf/1508.03882v2", 
    "arxiv-id": "1508.03882v2"
},{
    "category": "cs.CE", 
    "author": "Emmanuel N. Osegi", 
    "title": "PTILE: A framework for the Evaluation of Power Transformer Insulation   Life in Electric Power System", 
    "publish": "2015-08-17T18:37:58Z", 
    "summary": "In this paper, a framework is developed for power transformer (Generator Step\nup Unit) insulation life evaluation (PTILE) study on power system Network.\nParameters used for studies include real time sample data obtained from power\ntransformer field studies in the South-South Niger Delta region of Nigeria. It\nis used for performing simulations over varying number of years. Simulation\nreports shows a polynomial running time complexity and validates the stochastic\nHot Spot theory indicating that the transformers in such region should be\nreplaced sooner due to higher hot spots and transformer loading in such regions", 
    "link": "http://arxiv.org/pdf/1508.04105v1", 
    "arxiv-id": "1508.04105v1"
},{
    "category": "cs.CE", 
    "author": "Martin Cermak", 
    "title": "Subdifferential-based implicit return-mapping operators in Mohr-Coulomb   plasticity", 
    "publish": "2015-08-29T11:11:40Z", 
    "summary": "The paper is devoted to a constitutive solution, limit load analysis and\nNewton-like methods in elastoplastic problems containing the Mohr-Coulomb yield\ncriterion. Within the constitutive problem, we introduce a self-contained\nderivation of the implicit return-mapping solution scheme using a recent\nsubdifferential-based treatment. Unlike conventional techniques based on\nKoiter's rules, the presented scheme a priori detects a position of the unknown\nstress tensor on the yield surface even if the constitutive solution cannot be\nfound in closed form. This fact eliminates blind guesswork from the scheme,\nenables to analyze properties of the constitutive operator, and simplifies\nconstruction of the consistent tangent operator which is important for the\nsemismooth Newton method applied on the incremental boundary value\nelastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is\ncombined with the limit load analysis. Beside a conventional direct method of\nthe incremental limit analysis, a recent indirect one is introduced and its\nadvantages are described. The paper contains 2D and 3D numerical experiments on\nslope stability with publicly available Matlab implementations.", 
    "link": "http://arxiv.org/pdf/1508.07435v3", 
    "arxiv-id": "1508.07435v3"
},{
    "category": "cs.CE", 
    "author": "Constantine J. Hatziadoniu", 
    "title": "The Economic Dispatch for Integrated Wind Power Systems Using Particle   Swarm Optimization", 
    "publish": "2015-09-05T11:24:36Z", 
    "summary": "The economic dispatch of wind power units is quite different from that in\nconventional thermal units, since the adopted model should take into\nconsideration the intermittency nature of wind speed as well. Therefore, this\npaper uses a model that takes into account the aforementioned consideration in\naddition to whether the utility owns wind turbines or not. The economic\ndispatch is solved by using one of the modern optimization algorithms: the\nparticle swarm optimization algorithm. A 6-bus system is used and it includes\nwind-powered generators besides to thermal generators. The thorough analysis of\nthe results is also provided.", 
    "link": "http://arxiv.org/pdf/1509.01693v1", 
    "arxiv-id": "1509.01693v1"
},{
    "category": "cs.CE", 
    "author": "TI Eldho", 
    "title": "Algorithm for estimating swirl angles in multi-intake hydraulic sumps", 
    "publish": "2015-09-05T15:37:11Z", 
    "summary": "The paper has been withdrawn effective November 18, 2015.\n  Hydraulic Pump sumps are designed to provide a swirl free flow to the pump.\nThe degree of swirl is measured in physical model tests using a swirl meter and\na quantity known as swirl angle is generally measured. The present paper\npresents a novel method to compute the bulk swirl angle using the local\nvelocity field obtained from computational fluid dynamics data. The basis for\nthe present method is the conservation of angular momentum conservation. By\ncarrying out both numerical and experimental studies the novel swirl angle\ncalculation method is validated. Further the effect of vortex suppression\ndevices in reducing the swirl angle is also demonstrated.", 
    "link": "http://arxiv.org/pdf/1509.01709v2", 
    "arxiv-id": "1509.01709v2"
},{
    "category": "cs.CE", 
    "author": "R. B. Patel", 
    "title": "Agent enabled Mining of Distributed Protein Data Banks", 
    "publish": "2015-06-19T07:44:29Z", 
    "summary": "Mining biological data is an emergent area at the intersection between\nbioinformatics and data mining (DM). The intelligent agent based model is a\npopular approach in constructing Distributed Data Mining (DDM) systems to\naddress scalable mining over large scale distributed data. The nature of\nassociations between different amino acids in proteins has also been a subject\nof great anxiety. There is a strong need to develop new models and exploit and\nanalyze the available distributed biological data sources. In this study, we\nhave designed and implemented a multi-agent system (MAS) called Agent enriched\nQuantitative Association Rules Mining for Amino Acids in distributed Protein\nData Banks (AeQARM-AAPDB). Such globally strong association rules enhance\nunderstanding of protein composition and are desirable for synthesis of\nartificial proteins. A real protein data bank is used to validate the system.", 
    "link": "http://arxiv.org/pdf/1509.03198v1", 
    "arxiv-id": "1509.03198v1"
},{
    "category": "cs.CE", 
    "author": "J. M\u00e1ca", 
    "title": "The response of grandstands driven by filtered Gaussian white noise   processes", 
    "publish": "2015-09-14T19:23:06Z", 
    "summary": "This paper presents a semi-analytical estimate of the response of a\ngrandstand occupied by an active crowd and by a passive crowd. Filtered\nGaussian white noise processes are used to approximate the loading terms\nrepresenting an active crowd. Lumped biodynamic models with a single degree of\nfreedom are included to reflect passive spectators occupying the structure. The\nresponse is described in terms of the first two moments, employing the It\\^o\nformula and the state augmentation method for the stationary time domain\nsolution. The quality of the approximation is compared on the basis of three\nexamples of varying complexity using Monte Carlo simulation based on a\nsynthetic generator available in the literature. For comparative purposes,\nthere is also a brief review of frequency domain estimates.", 
    "link": "http://arxiv.org/pdf/1509.04250v1", 
    "arxiv-id": "1509.04250v1"
},{
    "category": "cs.CE", 
    "author": "Petr E. Zakharov", 
    "title": "Numerical simulation of the stress-strain state of the dental system", 
    "publish": "2015-09-17T11:19:42Z", 
    "summary": "We present mathematical models, computational algorithms and software, which\ncan be used for prediction of results of prosthetic treatment. More interest\nissue is biomechanics of the periodontal complex because any prosthesis is\naccompanied by a risk of overloading the supporting elements. Such risk can be\navoided by the proper load distribution and prediction of stresses that occur\nduring the use of dentures. We developed the mathematical model of the\nperiodontal complex and its software implementation. This model is based on\nlinear elasticity theory and allows to calculate the stress and strain fields\nin periodontal ligament and jawbone. The input parameters for the developed\nmodel can be divided into two groups. The first group of parameters describes\nthe mechanical properties of periodontal ligament, teeth and jawbone (for\nexample, elasticity of periodontal ligament etc.). The second group\ncharacterized the geometric properties of objects: the size of the teeth, their\nspatial coordinates, the size of periodontal ligament etc. The mechanical\nproperties are the same for almost all, but the input of geometrical data is\ncomplicated because of their individual characteristics. In this connection, we\ndevelop algorithms and software for processing of images obtained by computed\ntomography (CT) scanner and for constructing individual digital model of the\ntooth-periodontal ligament-jawbone system of the patient. Integration of models\nand algorithms described allows to carry out biomechanical analysis on\nthree-dimensional digital model and to select prosthesis design.", 
    "link": "http://arxiv.org/pdf/1509.05208v1", 
    "arxiv-id": "1509.05208v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Skin Effect Modeling in Conductors of Arbitrary Shape Through a Surface   Admittance Operator and the Contour Integral Method", 
    "publish": "2015-09-28T15:23:48Z", 
    "summary": "An accurate modeling of skin effect inside conductors is of capital\nimportance to solve transmission line and scattering problems. This paper\npresents a surface-based formulation to model skin effect in conductors of\narbitrary cross section, and compute the per-unit-length impedance of a\nmulticonductor transmission line. The proposed formulation is based on the\nDirichlet-Neumann operator that relates the longitudinal electric field to the\ntangential magnetic field on the boundary of a conductor. We demonstrate how\nthe surface operator can be obtained through the contour integral method for\nconductors of arbitrary shape. The proposed algorithm is simple to implement,\nefficient, and can handle arbitrary cross-sections, which is a main advantage\nover the existing approach based on eigenfunctions, which is available only for\ncanonical conductor's shapes. The versatility of the method is illustrated\nthrough a diverse set of examples, which includes transmission lines with\ntrapezoidal, curved, and V-shaped conductors. Numerical results demonstrate the\naccuracy, versatility, and efficiency of the proposed technique.", 
    "link": "http://arxiv.org/pdf/1509.08357v1", 
    "arxiv-id": "1509.08357v1"
},{
    "category": "cs.CE", 
    "author": "Jarek Duda", 
    "title": "Normalized rotation shape descriptors and lossy compression of molecular   shape", 
    "publish": "2015-09-30T15:11:33Z", 
    "summary": "There is a common need to search of molecular databases for compounds\nresembling some shape, what suggests having similar biological activity while\nsearching for new drugs. The large size of the databases requires fast methods\nfor such initial screening, for example based on feature vectors constructed to\nfulfill the requirement that similar molecules should correspond to close\nvectors. Ultrafast Shape Recognition (USR) is a popular approach of this type.\nIt uses vectors of 12 real number as 3 first moments of distances from 4\nemphasized points. These coordinates might contain unnecessary correlations and\ndoes not allow to reconstruct the approximated shape. In contrast, spherical\nharmonic (SH) decomposition uses orthogonal coordinates, suggesting their\nindependence and so lager informational content of the feature vector. There is\nusually considered rotationally invariant SH descriptors, what means discarding\nof some essential information.\n  This article discusses framework for descriptors with normalized rotation,\nfor example by using principal component analysis (PCA-SH). As one of the most\ninteresting are ligands which have to slide into a protein, we will introduce\ndescriptors optimized for such flat elongated shapes. Bent deformed cylinder\n(BDC) describes the molecule as a cylinder which was first bent, then deformed\nsuch that its cross-sections became ellipses of evolving shape. Legendre\npolynomials are used to describe the central axis of such bent cylinder.\nAdditional polynomials are used to define evolution of such elliptic\ncross-section along the main axis. There will be also discussed bent\ncylindrical harmonics (BCH), which uses cross-sections described by cylindrical\nharmonics instead of ellipses. All these normalized rotation descriptors allow\nto reconstruct (decode) the approximated representation of the shape, hence can\nbe also used for lossy compression purposes.", 
    "link": "http://arxiv.org/pdf/1509.09211v1", 
    "arxiv-id": "1509.09211v1"
},{
    "category": "cs.CE", 
    "author": "Maria Tirronen", 
    "title": "Reliability Analysis of Processes with Moving Cracked Material", 
    "publish": "2015-10-11T11:01:30Z", 
    "summary": "The reliability of processes with moving elastic and isotropic material\ncontaining initial cracks is considered in terms of fracture. The material is\nmodelled as a moving plate which is simply supported from two of its sides and\nsubjected to homogeneous tension acting in the travelling direction. For\ntension, two models are studied: i) tension is constant with respect to time,\nand ii) tension varies temporally according to an Ornstein-Uhlenbeck process.\nCracks of random length are assumed to occur in the material according to a\nstochastic counting process. For a general counting process, a representation\nof the nonfracture probability of the system is obtained that exploits\nconditional Monte Carlo simulation. Explicit formulae are derived for special\ncases. To study the reliability of the system with temporally varying tension,\na known explicit result for the first passage time of an Ornstein-Uhlenbeck\nprocess to a constant boundary is utilized. Numerical examples are provided for\nprinting presses and paper material.", 
    "link": "http://arxiv.org/pdf/1510.03035v1", 
    "arxiv-id": "1510.03035v1"
},{
    "category": "cs.CE", 
    "author": "R. M\u00ednguez", 
    "title": "Dynamic Robust Transmission Expansion Planning", 
    "publish": "2015-10-11T21:04:27Z", 
    "summary": "Recent breakthroughs in Transmission Network Expansion Planning (TNEP) have\ndemonstrated that the use of robust optimization, as opposed to stochastic\nprogramming methods, renders the expansion planning problem considering\nuncertainties computationally tractable for real systems. However, there is\nstill a yet unresolved and challenging problem as regards the resolution of the\ndynamic TNEP problem (DTNEP), which considers the year-by-year representation\nof uncertainties and investment decisions in an integrated way. This problem\nhas been considered to be a highly complex and computationally intractable\nproblem, and most research related to this topic focuses on very small case\nstudies or used heuristic methods and has lead most studies about TNEP in the\ntechnical literature to take a wide spectrum of simplifying assumptions. In\nthis paper an adaptive robust transmission network expansion planning\nformulation is proposed for keeping the full dynamic complexity of the problem.\nThe method overcomes the problem size limitations and computational\nintractability associated with dynamic TNEP for realistic cases. Numerical\nresults from an illustrative example and the IEEE 118-bus system are presented\nand discussed, demonstrating the benefits of this dynamic TNEP approach with\nrespect to classical methods.", 
    "link": "http://arxiv.org/pdf/1510.03102v3", 
    "arxiv-id": "1510.03102v3"
},{
    "category": "cs.CE", 
    "author": "Ananth Grama", 
    "title": "Triangular Alignment (TAME): A Tensor-based Approach for Higher-order   Network Alignment", 
    "publish": "2015-10-22T02:55:53Z", 
    "summary": "Network alignment has extensive applications in comparative interactomics.\nTraditional approaches aim to simultaneously maximize the number of conserved\nedges and the underlying similarity of aligned entities. We propose a novel\nformulation of the network alignment problem that extends topological\nsimilarity to higher-order structures and provides a new objective function\nthat maximizes the number of aligned substructures. This objective function\ncorresponds to an integer programming problem, which is NP-hard. Consequently,\nwe identify a closely related surrogate function whose maximization results in\na tensor eigenvector problem. Based on this formulation, we present an\nalgorithm called Triangular AlignMEnt (TAME), which attempts to maximize the\nnumber of aligned triangles across networks. Using a case study on the\nNAPAbench dataset, we show that triangular alignment is capable of producing\nmappings with high node correctness. We further evaluate our method by aligning\nyeast and human interactomes. Our results indicate that TAME outperforms the\nstate-of-art alignment methods in terms of conserved triangles. In addition, we\nshow that the number of conserved triangles is more significantly correlated,\ncompared to the conserved edge, with node correctness and co-expression of\nedges. Our formulation and resulting algorithms can be easily extended to\narbitrary motifs.", 
    "link": "http://arxiv.org/pdf/1510.06482v2", 
    "arxiv-id": "1510.06482v2"
},{
    "category": "cs.CE", 
    "author": "Shimon Wdowinski", 
    "title": "An Efficient Polyphase Filter Based Resampling Method for Unifying the   PRFs in SAR Data", 
    "publish": "2015-10-22T12:54:40Z", 
    "summary": "As current airborne and spaceborne synthetic aperture radar (SAR) systems aim\nto produce higher resolution and wider area products, their associated\ncomplexities call for handling stricter requirements. Variable and higher pulse\nrepetition frequencies (PRFs) are increasingly being used to achieve these\ndemanding requirements in modern radar systems. This paper presents a\nresampling scheme capable of unifying and downsampling variable PRFs within a\nsingle look complex (SLC) SAR acquisition and across a repeat pass sequence of\nacquisitions down to an effective lower PRF through the use of polyphase\nfilters. To evaluate the performance of this resampling scheme, we use airborne\nSAR raw data with variable PRFs. The data were processed with and without the\nproposed resampling method as part of the flow of the imaging algorithm.\nSignificant improvement in the point spread function (PSF) measurement and the\nvisible image quality after rate conversion and normalization justify the\ntheoretical basis of the proposed method and the benefits it can provide in\napplication scenarios.", 
    "link": "http://arxiv.org/pdf/1510.07020v1", 
    "arxiv-id": "1510.07020v1"
},{
    "category": "cs.CE", 
    "author": "M. Abdel Wahab", 
    "title": "Isogeometric approach for nonlinear bending and post-buckling analysis   of functionally graded plates under thermal environment", 
    "publish": "2015-11-04T16:06:46Z", 
    "summary": "In this paper, equilibrium and stability equations of functionally graded\nmaterial (FGM) plate under thermal environment are formulated based on\nisogeometric analysis (IGA) in combination with higher-order shear deformation\ntheory (HSDT). The FGM plate is made by a mixture of two distinct components,\nfor which material properties not only vary continuously through thickness\naccording to a power-law distribution but also are assumed to be a function of\ntemperature. Temperature field is assumed to be constant in any plane and\nuniform, linear and nonlinear through plate thickness, respectively. The\ngoverning equation is in nonlinear form based on von Karman assumption and\nthermal effect. A NURBS-based isogeometric finite element formulation is\nutilized to naturally fulfil the rigorous C1-continuity required by the present\nplate model. Influences of gradient indices, boundary conditions, temperature\ndistributions, material properties, length-to-thickness ratios on the behaviour\nof FGM plate are discussed in details. Numerical results demonstrate excellent\nperformance of the present approach.", 
    "link": "http://arxiv.org/pdf/1511.01380v1", 
    "arxiv-id": "1511.01380v1"
},{
    "category": "cs.CE", 
    "author": "Francesco Trevisan", 
    "title": "Modeling of anechoich chambers with equivalent materials and equivalent   sources", 
    "publish": "2015-11-26T15:09:59Z", 
    "summary": "Numerical simulation of anechoic chambers is a hot topic since it can provide\nuseful data about the performance of the EMC site. However, the mathematical\nnature of the problem, the physical dimensions of the simulated sites and the\nfrequency ranges pose nontrivial challenges to the simulation. Computational\nrequirements in particular will quickly become unmanageable if adequate\ntechniques are not employed. In this work we describe a novel approach, based\non equivalent elements, that enables the simulation of large chambers with\nmodest computational resources. The method is then validated against real\nmeasurement results.", 
    "link": "http://arxiv.org/pdf/1511.08410v1", 
    "arxiv-id": "1511.08410v1"
},{
    "category": "cs.CE", 
    "author": "Steve Steffel", 
    "title": "Wavelet Based Load Models from AMI Data", 
    "publish": "2015-12-07T19:34:58Z", 
    "summary": "A major challenge of using AMI data in power system analysis is the large\nsize of the data sets. For rapid analysis that addresses historical behavior of\nsystems consisting of a few hundred feeders, all of the AMI load data can be\nloaded into memory and used in a power flow analysis. However, if a system\ncontains thousands of feeders then the handling of the AMI data in the analysis\nbecomes more challenging. The work here seeks to demonstrate that the\ninformation contained in large AMI data sets can be compressed into accurate\nload models using wavelets. Two types of wavelet based load models are\nconsidered, the multi-resolution wavelet load model for each individual\ncustomer and the classified wavelet load model for customers that share similar\nload patterns. The multi-resolution wavelet load model compresses the data, and\nthe classified wavelet load model further compresses the data. The method of\ngrouping customers into classes using the wavelet based classification\ntechnique is illustrated.", 
    "link": "http://arxiv.org/pdf/1512.02183v1", 
    "arxiv-id": "1512.02183v1"
},{
    "category": "cs.CE", 
    "author": "Joe J. Mambretti", 
    "title": "The Design of a Community Science Cloud: The Open Science Data Cloud   Perspective", 
    "publish": "2016-01-03T19:06:43Z", 
    "summary": "In this paper we describe the design, and implementation of the Open Science\nData Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data\ncloud infrastructure and related services for scientists working with large\nquantities of data. Currently, the OSDC consists of more than 2000 cores and 2\nPB of storage distributed across four data centers connected by 10G networks.\nWe discuss some of the lessons learned during the past three years of operation\nand describe the software stacks used in the OSDC. We also describe some of the\nresearch projects in biology, the earth sciences, and social sciences enabled\nby the OSDC.", 
    "link": "http://arxiv.org/pdf/1601.00323v1", 
    "arxiv-id": "1601.00323v1"
},{
    "category": "cs.CE", 
    "author": "Phillip Sewell", 
    "title": "Extended Capability Models for Carbon Fiber Composite (CFC) Panels in   the Unstructured Transmission Line Modelling (UTLM) Method", 
    "publish": "2016-01-10T18:04:08Z", 
    "summary": "An effective model of single and multilayered thin panels, including those\nformed using carbon fiber composite (CFC) materials, is incorporated into the\nTransmission Line Modeling (TLM) method. The thin panel model is a\none-dimensional (1D) one based on analytical expansions of cotangent and\ncosecant functions that are used to describe the admittance matrix in the\nfrequency domain; these are then converted into the time domain by using\ndigital filter theory and an inverse Z transform. The model, which is extended\nto allow for material anisotropy, is executed within 1D TLM codes. And, for the\nfirst time, the two-dimensional (2D) thin surface model is embedded in\nunstructured three-dimensional (3D) TLM codes. The approach is validated by\nusing it to study some canonical structures with analytic solutions, and\nagainst results taken from the literature. It is then used to investigate\nshielding effectiveness of carbon fiber composite materials in a practical\ncurved aerospace-related structure.", 
    "link": "http://arxiv.org/pdf/1601.02241v1", 
    "arxiv-id": "1601.02241v1"
},{
    "category": "cs.CE", 
    "author": "K. B. Nakshatrala", 
    "title": "A hybrid multi-time-step framework for pore-scale and continuum-scale   modeling of solute transport in porous media", 
    "publish": "2016-01-12T01:18:15Z", 
    "summary": "In this paper, we propose a computational framework,which is based on a\ndomain decomposition technique, to employ both finite element method (which is\na popular continuum modeling approach) and lattice Boltzmann method (which is a\npopular pore-scale modeling approach) in the same computational domain. To\nbridge the gap across the disparate length and time-scales, we first propose a\nnew method to enforce continuum-scale boundary conditions (i.e., Dirichlet and\nNeumann boundary conditions) onto the numerical solution from the lattice\nBoltzmann method. This method are based on maximization of entropy and preserve\nthe non-negativity of discrete distributions under the lattice Boltzmann\nmethod. The proposed computational framework allows different grid sizes,\norders of interpolation, and time-steps in different subdomains. This allows\nfor different desired resolutions in the numerical solution in different\nsubdomains. Through numerical experiments, the effect of grid and time-step\nrefinement, disparity of time-steps in different subdomains, domain\npartitioning, and the number of iteration steps on the accuracy and rate of\nconvergence of the proposed methodology are studied. Finally, to showcase the\nperformance of this framework in porous media applications, we use it to\nsimulate the dissolution of calcium carbonate in a porous structure.", 
    "link": "http://arxiv.org/pdf/1601.02708v2", 
    "arxiv-id": "1601.02708v2"
},{
    "category": "cs.CE", 
    "author": "S. Abdykarimova", 
    "title": "The discrete analogue of the method of quickest descent for an inverse   acoustic problem in case of a smooth source", 
    "publish": "2016-01-16T20:21:41Z", 
    "summary": "The article considers the discrete analogue of the method of quickest descent\nfor an inverse Acoustics problem in case of a smooth source. The authors\nderived the gradient of functional in differential and discrete cases,\ndescribed the algorithm of solving a problem, and compared gradients of\nfunctional in continuous and discrete cases. In the article the improved\nestimates of the rates of convergence of gradient-based methods are obtained,\nwhich are very important for practice because they provide with the possibility\nto make input data errors consistent with the iteration number. There is a\npractical application of the proposed new method of deriving the gradient of\nfunctional for an Acoustics discrete problem, for it provides with calculations\nthat are more accurate. The theoretical importance of the method is the\ndeveloped technique of deriving estimates and gradients of functional at a\ndiscrete level.", 
    "link": "http://arxiv.org/pdf/1601.04207v1", 
    "arxiv-id": "1601.04207v1"
},{
    "category": "cs.CE", 
    "author": "Flavio Barbara", 
    "title": "Applying a Differential Evolutionary Algorithm to a Constraint-based   System to support Separation of OTDR Superimposed Signal after Passive   Optical Network Splitters", 
    "publish": "2016-01-21T19:04:46Z", 
    "summary": "The FTTH (Fiber To The Home) market currently needs new network maintenance\ntechnologies that can, economically and effectively, cope with massive fiber\nplants. However, operating these networks requires adequate means for an\neffective monitoring cost. Especially for troubleshooting faults that are\nassociated with the possibility of remote identification of fiber breaks, which\nmay exist in the network. Optical Time Domain Reflectometry (OTDR) techniques\nare widely used in point-to-point optical network topologies. Nevertheless, it\nhas major limitations in tree-structured PONs (Passive Optical Networks), where\nall different branches backscatter the light in just one conventional OTDR\ntrace with combined signals arriving on the OLT (Optical Line Terminal) side.\nFurthermore, passive power splitters used in FTTH networks input large\nattenuation, impoverishing the reflected signal. This makes the identification\nof the very branch affected by the problem practically impossible, when\nconsidering conventional analyses. The use of constraint-based techniques have\nbeen applied in a large amount of applications for Engineering Design, where\nthe duties imposed for graphics and equations constraints result in valued\nfeatures to CAD/CAE software capabilities. Currently, it provides a faster\ndecision making capacity for engineers. This work applies the constraint based\napproach along with a Differential Evolutionary Algorithm to separate the\nsuperimposed OTDR signals, after the splitters of a FTTH Passive Optical\nNetworks. This research introduces a new set of algorithms performing a\ncoupling to an Optical Network (ON) CAD Design with its correspondent OTDR\nmeasurement signal, considering its geographical distribution branches of\ndifferent lengths after the splitter. Results of this work are presented in a\nFTTN (Fiber To The Node) prototype arrangement, using a 1:8 passive power\nsplitter.", 
    "link": "http://arxiv.org/pdf/1601.05754v1", 
    "arxiv-id": "1601.05754v1"
},{
    "category": "cs.CE", 
    "author": "K. Mazaheri Body", 
    "title": "Numerical Solution of Cylindrically Converging Shock Waves", 
    "publish": "2016-02-08T18:26:36Z", 
    "summary": "The cylindrically converging shock wave was numerically simulated by solving\nthe Euler equations in cylindrical coordinates with TVD scheme and MUSCL\napproach, using Roe's approximate Riemann solver and super-bee nonlinear\nlimiter. The present study used the in house code developed for this purpose.\nThe behavior of the solution in the vicinity of axis is investigated and the\nresults of the numerical solution are compared with the computed data given by\nPayne, Lapidus, Abarbanel, and Goldberg, Sod, and Leutioff et al.", 
    "link": "http://arxiv.org/pdf/1602.02680v3", 
    "arxiv-id": "1602.02680v3"
},{
    "category": "cs.CE", 
    "author": "Javier Escudero", 
    "title": "Refined Multiscale Fuzzy Entropy based on Standard Deviation for   Biomedical Signal Analysis", 
    "publish": "2016-02-09T03:17:07Z", 
    "summary": "Multiscale entropy (MSE) has been a prevalent algorithm to quantify the\ncomplexity of fluctuations in the local mean value of biomedical time series.\nRecent developments in the field have tried to improve the MSE by reducing its\nvariability in large scale factors. On the other hand, there has been recent\ninterest in using other statistical moments than the mean, i.e. variance, in\nthe coarse-graining step of the MSE. Building on these trends, here we\nintroduce the so-called refined composite multiscale fuzzy entropy based on the\nstandard deviation (RCMFE{\\sigma}) to quantify the dynamical properties of\nspread over multiple time scales. We demonstrate the dependency of the\nRCMFE{\\sigma}, in comparison with other multiscale approaches, on several\nstraightforward signal processing concepts using a set of synthetic signals. We\nalso investigate the complementarity of using the standard deviation instead of\nthe mean in the coarse-graining process using magnetoencephalograms in\nAlzheimer disease and publicly available electroencephalograms recorded from\nfocal and non-focal areas in epilepsy. Our results indicate that RCMFE{\\sigma}\noffers complementary information to that revealed by classical coarse-graining\napproaches and that it has superior performance to distinguish different types\nof physiological activity.", 
    "link": "http://arxiv.org/pdf/1602.02847v1", 
    "arxiv-id": "1602.02847v1"
},{
    "category": "cs.CE", 
    "author": "Olve Mo", 
    "title": "Variable Transmission Voltage for Loss Minimization in Long Offshore   Wind Farm AC Export Cables", 
    "publish": "2016-02-09T13:44:03Z", 
    "summary": "Connection of offshore wind farms to shore requires the use of submarine\ncables. In the case of long HVAC connections, the capacitive charging currents\nlimit the transfer capability and lead to high losses. This paper shows that\nthe losses can be substantially reduced by continuously adjusting the cable\noperating voltage according to the instantaneous wind farm power\nproduction.Calculations for a 320 MW windfarm connected to shore via a 200 km\ncable at 220 kV nominal voltage shows that an annual loss reduction of 9\npercent is achievable by simply using a 15 percent tap changer voltage\nregulation on the two transformers. Allowing a larger voltage regulation range\nleads to further loss reduction (13 percent for 0.4-1.0 p.u. voltage range). If\nthe windfarm has a low utilization factor, the loss reduction potential is\ndemonstrated to be as high as 21 percent . The methodology can be applied\nwithout introducing new technology that needs to be developed or qualified.", 
    "link": "http://arxiv.org/pdf/1602.02982v1", 
    "arxiv-id": "1602.02982v1"
},{
    "category": "cs.CE", 
    "author": "Elnaz Siami-Irdemoosa", 
    "title": "Estimating the unconfined compressive strength of carbonate rocks using   gene expression programming", 
    "publish": "2016-01-26T01:35:23Z", 
    "summary": "Conventionally, many researchers have used both regression and black box\ntechniques to estimate the unconfined compressive strength (UCS) of different\nrocks. The advantage of the regression approach is that it can be used to\nrender a functional relationship between the predictive rock indices and its\nUCS. The advantage of the black box techniques is in rendering more accurate\npredictions. Gene expression programming (GEP) is proposed, in this study, as a\nrobust mathematical alternative for predicting the UCS of carbonate rocks. The\ntwo parameters of total porosity and P-wave speed were selected as predictive\nindices. The proposed GEP model had the advantage of the both traditionally\nused approaches by proposing a mathematical model, similar to a regression,\nwhile keeping the prediction errors as low as the black box methods. The GEP\noutperformed both artificial neural networks and support vector machines in\nterms of yielding more accurate estimates of UCS. Both the porosity and the\nP-wave velocity were sufficient predictive indices for estimating the UCS of\nthe carbonate rocks in this study. Nearly, 95% of the observed variation in the\nUCS values was explained by these two parameters (i.e., R2 =95%).", 
    "link": "http://arxiv.org/pdf/1602.03854v1", 
    "arxiv-id": "1602.03854v1"
},{
    "category": "cs.CE", 
    "author": "Ji-Qing Han", 
    "title": "Signal periodic decomposition with conjugate subspaces", 
    "publish": "2016-02-12T08:22:49Z", 
    "summary": "In this paper, we focus on hidden period identification and the periodic\ndecomposition of signals. Based on recent results on the Ramanujan subspace, we\nreveal the conjugate symmetry of the Ramanujan subspace with a set of complex\nexponential basis functions and represent the subspace as the union of a series\nof conjugate subspaces. With these conjugate subspaces, the signal periodic\nmodel is introduced to characterize the periodic structure of a signal. To\nachieve the decomposition of the proposed model, the conjugate subspace\nmatching pursuit (CSMP) algorithm is proposed based on two different greedy\nstrategies. The CSMP is performed iteratively in two stages. In the first\nstage, the dominant hidden period is chosen with the periodicity strategy.\nThen, the dominant conjugate subspace is chosen with the energy strategy in the\nsecond stage. Compared with the current state-of-the-art methods for hidden\nperiod identification, the main advantages provided by the CSMP are the\nfollowing: (i) the capability of identifying all the hidden periods in the\nrange from $1$ to the maximum hidden period $Q$ of a signal of any length,\nwithout truncating the signal; (ii) the ability to identify the time-varying\nhidden period with its shifted version; and (iii) the low computational cost,\nwithout generating and using a large over-complete dictionary. Moreover, we\nprovide examples and applications to demonstrate the abilities of the proposed\ntwo-stage CSMP algorithm, which include hidden period identification, signal\napproximation, time-varying period detection, and pitch detection of speech.", 
    "link": "http://arxiv.org/pdf/1602.03979v1", 
    "arxiv-id": "1602.03979v1"
},{
    "category": "cs.CE", 
    "author": "Zhangxin Chen", 
    "title": "Development of A Platform for Large-scale Reservoir Simulations on   Parallel computers", 
    "publish": "2016-02-18T18:22:18Z", 
    "summary": "This paper presents our work on designing a platform for large-scale\nreservoir simulations. Detailed components, such as grid and linear solver, and\ndata structures are introduced, which can serve as a guide to parallel\nreservoir simulations and other parallel applications. The main objective of\nplatform is to support implementation of various parallel reservoir simulators\non distributed-memory parallel systems, where MPI (Message Passing Interface)\nis employed for communications among computation nodes. It provides structured\ngrid due to its simplicity and cell-centered data is applied for each cell. The\nplatform has a distributed matrix and vector module and a map module. The\nmatrix and vector module is the base of our parallel linear systems. The map\nconnects grid and linear system modules, which defines various mappings between\ngrid and linear systems. Commonly-used Krylov subspace linear solvers are\nimplemented, including the restarted GMRES method and the BiCGSTAB method. It\nalso has an interface to a parallel algebraic multigrid solver, BoomerAMG from\nHYPRE. Parallel general-purpose preconditioners and special preconditioners for\nreservoir simulations are also developed. Various data structures are designed,\nsuch as grid, cell, data, linear solver and preconditioner, and some key\ndefault parameters are presented in this paper. The numerical experiments show\nthat our platform has excellent scalability and it can simulate giant reservoir\nmodels with hundreds of millions of grid cells using thousands of CPU cores.", 
    "link": "http://arxiv.org/pdf/1602.05901v5", 
    "arxiv-id": "1602.05901v5"
},{
    "category": "cs.CE", 
    "author": "Philippe Karamian-Surville", 
    "title": "Influence of morphological parameters in 3D composite materials on their   effective thermal properties and comparison with effective mechanical   properties", 
    "publish": "2016-02-25T09:07:19Z", 
    "summary": "In this paper we study the effective thermal behaviour of 3D representative\nvolume elements (RVEs) of two-phased composite materials constituted by a\nmatrix with cylindrical and spherical inclusions distributed randomly, with\nperiodic boundaries. Variations around the shape of inclusions have been taken\ninto account, by corrugating shapes, excavating and/or by removing pieces of\ninclusions. The effective behaviour is computed with the help of homogenization\nprocess based on an accelerated FFT-scheme giving the thermal conductivity\ntensor. Several morphological parameters are also taken into account for\ninstance the number and the volume fraction of each type of inclusions,... in\norder to analyse the behaviour of the composite for a large number of\ngeometries. We compare the results obtained for RVEs with and without\nvariations, and then with the mechanical results of such composite studied in\nour previous paper.", 
    "link": "http://arxiv.org/pdf/1602.07851v1", 
    "arxiv-id": "1602.07851v1"
},{
    "category": "cs.CE", 
    "author": "Charles F. Vardeman II", 
    "title": "Green Scale Research Tool for Multi-Criteria and Multi-Metric Energy   Analysis Performed During the Architectural Design Process", 
    "publish": "2016-02-26T20:08:18Z", 
    "summary": "Prevailing computational tools available to and used by architecture and\nengineering professionals purport to gather and present thorough and accurate\nperspectives of the environmental impacts associated with their contributions\nto the built environment. The presented research of building modeling and\nanalysis software used by the Architecture, Engineering, Construction, and\nOperations (AECO) industry reveals that many of the most heavily relied-upon\nindustry tools are isolated in functionality, utilize incomplete models and\ndata, and are disruptive to normative design and building optimization\nworkflows. This paper describes the current models and tools, their primary\nfunctions and limitations, and presents our concurrent research to develop more\nadvanced models to assess lifetime building energy consumption alongside\noperating energy use. A series of case studies describes the current\nstate-of-the-art in tools and building energy analysis followed by the research\nmodels and novel design and analysis Tool that the Green Scale Research Group\nhas developed in response. A fundamental goal of this effort is to increase the\nuse and efficacy of building impact studies conducted by architects, engineers,\nand building owners and operators during the building design process.", 
    "link": "http://arxiv.org/pdf/1602.08463v1", 
    "arxiv-id": "1602.08463v1"
},{
    "category": "cs.CE", 
    "author": "Klaus Hackl", 
    "title": "An efficient adaptive polygonal finite element method for plastic   collapse analysis of solids", 
    "publish": "2016-03-06T20:10:15Z", 
    "summary": "We propose an adaptive polygonal finite element formulation for collapse\nplastic analysis of solids. The article contributes into four crucial points:\n1) Wachspress shape functions at vertex and bubble nodes handled at a\nprimal-mesh level; 2) plastic strain rates and dissipation performed over a\ndual-mesh level; 3) a new adaptive primal-mesh strategy driven by the L^2\n-norm-based indicator of strain rates; and 4) a spatial decomposition structure\nobtained from a so-called polytree mesh scheme. We investigate both purely\ncohesive and cohesive-frictional materials. We prove numerically that the\npresent method performs well for volumetric locking problem. In addition, the\noptimization formulation of limit analysis is written by the form of\nsecond-order cone programming (SOCP) in order to exploit the high efficiency of\ninterior-point solvers. The present method retains a low number of optimization\nvariables. This convenient approach allows us to design and solve the\nlarge-scale optimization problems effectively. Numerical validations show the\nexcellent performance of the proposed method.", 
    "link": "http://arxiv.org/pdf/1603.01866v2", 
    "arxiv-id": "1603.01866v2"
},{
    "category": "cs.CE", 
    "author": "C. -M Duluc", 
    "title": "Global sensitivity analysis with 2d hydraulic codes: applied protocol   and practical tool", 
    "publish": "2016-03-24T07:57:42Z", 
    "summary": "Global Sensitivity Analysis (GSA) methods are useful tools to rank input\nparameters uncertainties regarding their impact on result variability. In\npractice, such type of approach is still at an exploratory level for studies\nrelying on 2D Shallow Water Equations (SWE) codes as GSA requires specific\ntools and deals with important computational capacity. The aim of this paper is\nto provide both a protocol and a tool to carry out a GSA for 2D hydraulic\nmodelling applications. A coupled tool between Prom{\\'e}th{\\'e}e (a parametric\ncomputation environment) and FullSWOF 2D (a code relying on 2D SWE) has been\nset up: Prom{\\'e}th{\\'e}e-FullSWOF 2D (P-FS). The main steps of our protocol\nare: i) to identify the 2D hydraulic code input parameters of interest and to\nassign them a probability density function, ii) to propagate uncertainties\nwithin the model, and iii) to rank the effects of each input parameter on the\noutput of interest. For our study case, simulations of a river flood event were\nrun with uncertainties introduced through three parameters using P-FS tool.\nTests were performed on regular computational mesh, spatially discretizing an\nurban area, using up to 17.9 million of computational points. P-FS tool has\nbeen installed on a cluster for computation. Method and P-FS tool successfully\nallow the computation of Sobol indices maps. Keywords Uncertainty, flood hazard\nmodelling, global sensitivity analysis, 2D shallow water equation, Sobol index.\nAnalyse globale de sensibilit{\\'e} en mod{\\'e}lisation hydrauliqu{\\`e} a\nsurface libre 2D : application d'un protocole et d{\\'e}veloppement d'outils\nop{\\'e}rationnels -- Les m{\\'e}thodes d'analyse de sensibilit{\\'e} permettent\nde contr{\\^o}ler la robustesse des r{\\'e}sultats de mod{\\'e}lisation ainsi que\nd'identifier le degr{\\'e} d'influence des param etres d' entr{\\'e}e sur le\nr{\\'e}sultat en sortie d'un mod ele. Le processus complet constitue une analyse\nglobale de sensibilit{\\'e} (GSA). Ce type d'approche pr{\\'e}sente un grand\nint{\\'e}r{\\^e}t pour analyser les incer-titudes de r{\\'e}sultats de\nmod{\\'e}lisation , mais est toujours a un stade exploratoire dans les etudes\nappliqu{\\'e}es mettant en jeu des codes bas{\\'e}s sur la r{\\'e}solution\nbidimensionnelle des equations de Saint-Venant. En effet, l' impl{\\'e}mentation\nd'une GSA est d{\\'e}licate car elle", 
    "link": "http://arxiv.org/pdf/1603.07919v1", 
    "arxiv-id": "1603.07919v1"
},{
    "category": "cs.CE", 
    "author": "Gabriel Delgado", 
    "title": "Optimal design of a micro-tubular fuel cell", 
    "publish": "2016-03-27T21:21:39Z", 
    "summary": "We discuss the problem of the optimal design of a micro-tubular fuel cell\napplying an inverse homogenization technique. Fuel cells are extremely clean\nand efficient electrochemical power generation devices, made up of a\ncathode/electrolyte/anode structure, whose energetic potential has not being\nfully exploited in propulsion systems in aeronautics due to their low power\ndensities. Nevertheless, thanks to the recent development of additive layer\nmanufacturing techniques (3D printing), complex structures usually impossible\nto design with conventional manufacturing techniques can be constructed with a\nlow cost, allowing notably to build porous or foam-type structures for fuel\ncells. We seek thus to come up with the micro-structure of an arrangement of\nmicro-tubular cathodes which maximizes the contact surface subject to a\npressure drop and a permeability constraint. The optimal periodic design\n(fluid/solid) emerges from the application of a shape gradient algorithm\ncoupled to a level-set method for the geometrical description of the\ncorresponding cell problem.", 
    "link": "http://arxiv.org/pdf/1603.08260v1", 
    "arxiv-id": "1603.08260v1"
},{
    "category": "cs.CE", 
    "author": "Wolfgang A. Wall", 
    "title": "A Finite Element Approach for the Line-to-Line Contact Interaction of   Thin Beams with Arbitrary Orientation", 
    "publish": "2016-03-30T14:53:02Z", 
    "summary": "The objective of this work is the development of a novel finite element\nformulation describing the contact interaction of slender beams in complex 3D\nconfigurations involving arbitrary beam-to-beam orientations. It is shown in a\nmathematically concise manner that standard beam contact models based on a\npoint-wise contact force fail to describe a considerable range of\nconfigurations, which are, however, likely to occur in practical applications.\nOn the contrary, the formulation proposed here models beam-to-beam contact by\nmeans of distributed line forces, a procedure that is shown to be applicable\nfor arbitrary geometrical configurations. The proposed formulation is based on\na Gauss-point-to-segment type contact discretization and a penalty\nregularization of the contact constraint. By means of detailed theoretical and\nnumerical investigations, it is shown that this approach is more suitable for\nbeam contact than possible alternatives based on mortar type contact\ndiscretizations or constraint enforcement by means of Lagrange multipliers. The\nproposed formulation is enhanced by a consistently linearized integration\ninterval segmentation avoiding numerical integration across strong\ndiscontinuities. In combination with a smoothed contact force law and the\nemployed C1-continuous beam elements, this procedure drastically reduces the\nnumerical integration error, an essential prerequisite for optimal spatial\nconvergence rates. The resulting line-to-line contact algorithm is supplemented\nby contact contributions of the beam endpoints, which represent boundary minima\nof the underlying minimal distance problem. Finally, a series of numerical test\ncases is analyzed in order to investigate the accuracy and consistency of the\nproposed formulation regarding integration error, spatial convergence behavior\nand resulting contact force distributions.", 
    "link": "http://arxiv.org/pdf/1603.09227v1", 
    "arxiv-id": "1603.09227v1"
},{
    "category": "cs.CE", 
    "author": "Thuc P. Vo", 
    "title": "Isogeometric analysis for functionally graded microplates based on   modified couple stress theory", 
    "publish": "2016-04-02T19:33:15Z", 
    "summary": "Analysis of static bending, free vibration and buckling behaviours of\nfunctionally graded microplates is investigated in this study. The main idea is\nto use the isogeometric analysis in associated with novel four-variable refined\nplate theory and quasi-3D theory. More importantly, the modified couple stress\ntheory with only one material length scale parameter is employed to effectively\ncapture the size-dependent effects within the microplates. Meanwhile, the\nquasi-3D theory which is constructed from a novel seventh-order shear\ndeformation refined plate theory with four unknowns is able to consider both\nshear deformations and thickness stretching effect without requiring shear\ncorrection factors. The NURBS-based isogeometric analysis is integrated to\nexactly describe the geometry and approximately calculate the unknown fields\nwith higher-order derivative and continuity requirements. The convergence and\nverification show the validity and efficiency of this proposed computational\napproach in comparison with those existing in the literature. It is further\napplied to study the static bending, free vibration and buckling responses of\nrectangular and circular functionally graded microplates with various types of\nboundary conditions. A number of investigations are also conducted to\nillustrate the effects of the material length scale, material index, and\nlength-to-thickness ratios on the responses of the microplates.", 
    "link": "http://arxiv.org/pdf/1604.00547v1", 
    "arxiv-id": "1604.00547v1"
},{
    "category": "cs.CE", 
    "author": "Pradip Sircar", 
    "title": "Two Dimensional Angle of Arrival Estimation", 
    "publish": "2016-04-03T05:21:19Z", 
    "summary": "We present a new method for the estimation of two dimensional (2D) angles of\narrival (AOAs), namely, azimuth and incidence angles of multiple narrowband\nsignals of same frequency in the far field of antenna array.", 
    "link": "http://arxiv.org/pdf/1604.00594v1", 
    "arxiv-id": "1604.00594v1"
},{
    "category": "cs.CE", 
    "author": "H. Nguyen-Xuan", 
    "title": "Isogeometric nonlinear bending and buckling analysis of   variable-thickness composite plate structures", 
    "publish": "2016-04-03T19:45:23Z", 
    "summary": "This paper investigates nonlinear bending and buckling behaviours of\ncomposite plates characterized by a thickness variation. Layer interfaces are\ndescribed as functions of inplane coordinates. Top and bottom surfaces of the\nplate are symmetric about the midplane and the plate could be considered as a\nflat surface in analysis along with thickness parameters which vary over the\nplate. The variable thickness at a certain position in the midplane is modeled\nby a set of control points (or thickness-parameters) through NURBS (Non-Uniform\nRational B-Spline) basic functions. The knot parameter space which is referred\nin modelling geometry and approximating displacement variables is employed for\napproximating thickness, simultaneously. The use of quadratic NURBS functions\nresults in C^1 continuity of modeling variable thickness and analyzing\nsolutions. Thin to moderately thick laminates in bound of first-order shear\ndeformation theory (FSDT) are taken into account. Strain-displacement relations\nin sense of von-Karman theory are employed for large deformation. Riks method\nis used for geometrically nonlinear analysis. The weak form is approximated\nnumerically by the isogeometric analysis (IGA), which has been found to be a\nrobust, stable and realistic numerical tool. Numerical results confirm the\nreliability and capacity of the propose method.", 
    "link": "http://arxiv.org/pdf/1604.01367v1", 
    "arxiv-id": "1604.01367v1"
},{
    "category": "cs.CE", 
    "author": "Robert D Moser", 
    "title": "Representing model inadequacy: A stochastic operator approach", 
    "publish": "2016-04-06T14:57:56Z", 
    "summary": "Mathematical models of physical systems are subject to many uncertainties\nsuch as measurement errors and uncertain initial and boundary conditions. After\naccounting for these uncertainties, it is often revealed that discrepancies\nbetween the model output and the observations remain; if so, the model is said\nto be inadequate. In practice, the inadequate model may be the best that is\navailable or tractable, and so despite its inadequacy the model may be used to\nmake predictions of unobserved quantities. In this case, a representation of\nthe inadequacy is necessary, so the impact of the observed discrepancy can be\ndetermined. We investigate this problem in the context of chemical kinetics and\npropose a new technique to account for model inadequacy that is both\nprobabilistic and physically meaningful. A stochastic inadequacy operator\n$\\mathcal{S}$ is introduced which is embedded in the ODEs describing the\nevolution of chemical species concentrations and which respects certain\nphysical constraints such as conservation laws. The parameters of $\\mathcal{S}$\nare governed by probability distributions, which in turn are characterized by a\nset of hyperparameters. The model parameters and hyperparameters are calibrated\nusing high-dimensional hierarchical Bayesian inference. We apply the method to\na typical problem in chemical kinetics---the reaction mechanism of hydrogen\ncombustion.", 
    "link": "http://arxiv.org/pdf/1604.01651v2", 
    "arxiv-id": "1604.01651v2"
},{
    "category": "cs.CE", 
    "author": "Jorge Ambr\u00f3sio", 
    "title": "Multibody minimum-energy trajectory with applications to protein folding", 
    "publish": "2016-04-12T01:26:44Z", 
    "summary": "This work addresses the optimal control of multibody systems being actuated\nwith control forces in order to find a dynamically feasible minimum-energy\ntrajectory of the system. The optimal control problem and its constraints are\nintegrated in a discrete version of the equation of motion allowing the\nminimization of system energy with respect to a discrete state and control\ntrajectory. The work is centred on a specific type of open-chain multibody\nsystem, with strong local propensity, where the overall system kinematics is\ndescribed essentially by the torsion around the links that connect rigid\nbodies. The coupling between the rigid body motion, and the optimal\nconformation is described as an elastic band of replicas of the original system\nwith different conformations. The band forces are used to control system's\nmotion directly, reflecting the influence of the system energy field on its\nconformation, using for that the Nudged-Elastic Band method. Here the equation\nof motion of the multibody grid are solved by using the augmented Lagrangean\nmethod. In this context, if a feasible minimum-energy trajectory of the\noriginal system exists it is a stationary state of the extended system. This\napproach is applied to the folding of a single chain protein.", 
    "link": "http://arxiv.org/pdf/1604.03195v2", 
    "arxiv-id": "1604.03195v2"
},{
    "category": "cs.CE", 
    "author": "Indranil Ghosh", 
    "title": "Using Clustering Method to Understand Indian Stock Market Volatility", 
    "publish": "2016-04-18T06:58:30Z", 
    "summary": "In this paper we use Clustering Method to understand whether stock market\nvolatility can be predicted at all, and if so, when it can be predicted. The\nexercise has been performed for the Indian stock market on daily data for two\nyears. For our analysis we map number of clusters against number of variables.\nWe then test for efficiency of clustering. Our contention is that, given a\nfixed number of variables, one of them being historic volatility of NIFTY\nreturns, if increase in the number of clusters improves clustering efficiency,\nthen volatility cannot be predicted. Volatility then becomes random as, for a\ngiven time period, it gets classified in various clusters. On the other hand,\nif efficiency falls with increase in the number of clusters, then volatility\ncan be predicted as there is some homogeneity in the data. If we fix the number\nof clusters and then increase the number of variables, this should have some\nimpact on clustering efficiency. Indeed if we can hit upon, in a sense, an\noptimum number of variables, then if the number of clusters is reasonably\nsmall, we can use these variables to predict volatility. The variables that we\nconsider for our study are volatility of NIFTY returns, volatility of gold\nreturns, India VIX, CBOE VIX, volatility of crude oil returns, volatility of\nDJIA returns, volatility of DAX returns, volatility of Hang Seng returns and\nvolatility of Nikkei returns. We use three clustering algorithms namely Kernel\nK-Means, Self Organizing Maps and Mixture of Gaussian models and two internal\nclustering validity measures, Silhouette Index and Dunn Index, to assess the\nquality of generated clusters.", 
    "link": "http://arxiv.org/pdf/1604.05015v1", 
    "arxiv-id": "1604.05015v1"
},{
    "category": "cs.CE", 
    "author": "Lubin G. Vulkov", 
    "title": "Numerical solution of a parabolic system in air pollution", 
    "publish": "2016-04-18T12:44:39Z", 
    "summary": "An air pollution model is generally described by a system of PDEs on\nunbounded domain. Transformation of the independent variable is used to convert\nthe problem for nonlinear air pollution on finite computational domain. We\ninvestigate the new, degenerated parabolic problem in Sobolev spaces with\nweights for well-posedness and positivity of the solution. Then we construct a\nfitted finite volume difference scheme. Some results from computations are\npresented.", 
    "link": "http://arxiv.org/pdf/1604.05122v1", 
    "arxiv-id": "1604.05122v1"
},{
    "category": "cs.CE", 
    "author": "Y Richet", 
    "title": "Global Sensitivity Analysis with 2D Hydraulic Codes: Application on   Uncertainties Related to High-Resolution Topographic Data", 
    "publish": "2016-03-24T07:56:43Z", 
    "summary": "Technologies such as aerial photogrammetry allow production of 3D topographic\ndata including complex environments such as urban areas. Therefore, it is\npossible to create High Resolution (HR) Digital Elevation Models (DEM)\nincorporating thin above ground elements influencing overland flow paths. Even\nthough this category of big data has a high level of accuracy, there are still\nerrors in measurements and hypothesis under DEM elaboration. Moreover,\noperators look for optimizing spatial discretization resolution in order to\nimprove flood models computation time. Errors in measurement, errors in DEM\ngeneration, and operator choices for inclusion of this data within 2D hydraulic\nmodel, might influence results of flood models simulations. These errors and\nhypothesis may influence significantly flood modelling results variability. The\npurpose of this study is to investigate uncertainties related to (i) the own\nerror of high resolution topographic data, and (ii) the modeller choices when\nincluding topographic data in hydraulic codes. The aim is to perform a Global\nSensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty\npropagation, to quantify impact of uncertainties, followed by a Sobol' indices\ncomputation, to rank influence of identified parameters on result variability.\nA process using a coupling of an environment for parametric computation\n(Prom{\\'e}th{\\'e}e) and a code relying on 2D shallow water equations (FullSWOF\n2D) has been developed (P-FS tool). The study has been performed over the lower\npart of the Var river valley using the estimated hydrograph of 1994 flood\nevent. HR topographic data has been made available for the study area, which is\n17.5 km 2 , by Nice municipality. Three uncertain parameters were studied: the\nmeasurement error (var. E), the level of details of above-ground element\nrepresentation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial\ndiscretization resolution (grid cell size for regular mesh) (var. R). Parameter\nvar. E follows a probability density function, whereas parameters var. S and\nvar. R. are discrete operator choices. Combining these parameters, a database\nof 2, 000 simulations has been produced using P-FS tool implemented on a high\nperformance computing structure. In our study case, the output of interest is\nthe maximal", 
    "link": "http://arxiv.org/pdf/1604.06777v1", 
    "arxiv-id": "1604.06777v1"
},{
    "category": "cs.CE", 
    "author": "Paul P. H. Wilson", 
    "title": "Challenging Fuel Cycle Modeling Assumptions: Facility and Time Step   Discretization Effects", 
    "publish": "2016-05-04T19:58:48Z", 
    "summary": "Due to the diversity of fuel cycle simulator modeling assumptions, direct\ncomparison and benchmarking can be difficult. In 2012 the Organisation for\nEconomic Co-operation and Development completed a benchmark study that is\nperhaps the most complete published comparison performed. Despite this, various\nresults from the simulators were often significantly different because of\ninconsistencies in modeling decisions involving reprocessing strategies,\nrefueling behavior, reactor end-of-life handling, etc. This work identifies and\nquantifies the effects of selected modeling choices that may sometimes be taken\nfor granted in the fuel cycle simulation domain. Four scenarios are compared\nusing combinations of fleet-based or individually modeled reactors with monthly\nor quarterly (3-month) time steps. The scenarios approximate a transition from\nthe current U.S. once-through light water reactor fleet to a full sodium fast\nreactor fuel cycle. The Cyclus fuel cycle simulator's plug-in capability along\nwith its market-like dynamic material routing allow it to be used as a level\nplaying field for comparing the scenarios. When under supply-constraint\npressure, the four cases exhibit noticeably different behavior. Fleet-based\nmodeling is more efficient in supply-constrained environments at the expense of\nlosing insight on issues such as realistically suboptimal fuel distribution and\nchallenges in reactor refueling cycle staggering. Finer-grained time steps\nenable more efficient material use in supply-constrained environments resulting\nin lower standing inventories of separated Pu. Large simulations with\nfleet-based reactors run much more quickly than their individual reactor\ncounterparts. Gaining a better understanding of how these and other modeling\nchoices affect fuel cycle dynamics will enable making more deliberate decisions\nwith respect to trade-offs such as computational investment vs. realism.", 
    "link": "http://arxiv.org/pdf/1605.01402v1", 
    "arxiv-id": "1605.01402v1"
},{
    "category": "cs.CE", 
    "author": "Matthias Zwicker", 
    "title": "Bifurcation Analysis of Reaction Diffusion Systems on Arbitrary Surfaces", 
    "publish": "2016-05-05T13:29:36Z", 
    "summary": "In this paper we present computational techniques to investigate the\nsolutions of two-component, nonlinear reaction-diffusion (RD) systems on\narbitrary surfaces. We build on standard techniques for linear and nonlinear\nanalysis of RD systems, and extend them to operate on large-scale meshes for\narbitrary surfaces. In particular, we use spectral techniques for a linear\nstability analysis to characterize and directly compose patterns emerging from\nhomogeneities. We develop an implementation using surface finite element\nmethods and a numerical eigenanalysis of the Laplace-Beltrami operator on\nsurface meshes. In addition, we describe a technique to explore solutions of\nthe nonlinear RD equations using numerical continuation. Here, we present a\nmultiresolution approach that allows us to trace solution branches of the\nnonlinear equations efficiently even for large-scale meshes. Finally, we\ndemonstrate the working of our framework for two RD systems with applications\nin biological pattern formation: a Brusselator model that has been used to\nmodel pattern development on growing plant tips, and a chemotactic model for\nthe formation of skin pigmentation patterns. While these models have been used\npreviously on simple geometries, our framework allows us to study the impact of\narbitrary geometries on emerging patterns.", 
    "link": "http://arxiv.org/pdf/1605.01583v1", 
    "arxiv-id": "1605.01583v1"
},{
    "category": "cs.CE", 
    "author": "HyungSeon Oh", 
    "title": "Tensors in Power System Computation I: Distributed Computation for   Optimal Power Flow, DC OPF", 
    "publish": "2016-05-22T04:43:10Z", 
    "summary": "Tensor decomposition plays a key role in identifying common features across a\ncollection of matrices in many areas of science. A fundamental need in big data\nresearch is to process data tabulated as large-scale matrices using\neigenvectors. A higher order generalized singular value decomposition technique\nsuccessfully captures the common features of the same organ from multiple\nanimals in genomic signal processing. A recent semidefinite programming\napproach to solve an AC optimal power flow was accompanied by the problem\nformulation in the Cartesian coordinate system. The collection of nodal\nKirchhoff laws introduces a 3D tensor with a common feature of individual\nmatrices to maintain local power balance. In this paper, the mathematical\nprocess is established and the common feature is identified. The common feature\nis a key element to a fully decentralized and therefore scalable algorithm to\nsolve AC optimal power flow.", 
    "link": "http://arxiv.org/pdf/1605.06735v1", 
    "arxiv-id": "1605.06735v1"
},{
    "category": "cs.CE", 
    "author": "Shantenu Jha", 
    "title": "ExTASY: Scalable and Flexible Coupling of MD Simulations and Advanced   Sampling Techniques", 
    "publish": "2016-06-01T02:01:25Z", 
    "summary": "For many macromolecular systems the accurate sampling of the relevant regions\non the potential energy surface cannot be obtained by a single, long Molecular\nDynamics (MD) trajectory. New approaches are required to promote more efficient\nsampling. We present the design and implementation of the Extensible Toolkit\nfor Advanced Sampling and analYsis (ExTASY) for building and executing advanced\nsampling workflows on HPC systems. ExTASY provides Python based \"templated\nscripts\" that interface to an interoperable and high-performance pilot-based\nrun time system, which abstracts the complexity of managing multiple\nsimulations. ExTASY supports the use of existing highly-optimised parallel MD\ncode and their coupling to analysis tools based upon collective coordinates\nwhich do not require a priori knowledge of the system to bias. We describe two\nworkflows which both couple large \"ensembles\" of relatively short MD\nsimulations with analysis tools to automatically analyse the generated\ntrajectories and identify molecular conformational structures that will be used\non-the-fly as new starting points for further \"simulation-analysis\" iterations.\nOne of the workflows leverages the Locally Scaled Diffusion Maps technique; the\nother makes use of Complementary Coordinates techniques to enhance sampling and\ngenerate start-points for the next generation of MD simulations. We show that\nthe ExTASY tools have been deployed on a range of HPC systems including ARCHER\n(Cray CX30), Blue Waters (Cray XE6/XK7), and Stampede (Linux cluster), and that\ngood strong scaling can be obtained up to 1000s of MD simulations, independent\nof the size of each simulation. We discuss how ExTASY can be easily extended or\nmodified by end-users to build their own workflows, and ongoing work to improve\nthe usability and robustness of ExTASY.", 
    "link": "http://arxiv.org/pdf/1606.00093v1", 
    "arxiv-id": "1606.00093v1"
},{
    "category": "cs.CE", 
    "author": "A. W. M. van Schijndel", 
    "title": "BES with FEM: Building Energy Simulation using Finite Element Methods", 
    "publish": "2016-06-03T08:20:32Z", 
    "summary": "An overall objective of energy efficiency in the built environment is to\nimprove building and systems performances in terms of durability, comfort and\neconomics. In order to predict, improve and meet a certain set of performance\nrequirements related to the indoor climate of buildings and the associated\nenergy demand, building energy simulation (BES) tools are indispensable. Due to\nthe rapid development of FEM software and the Multiphysics approaches, it\nshould possible to build and simulate full 3D models of buildings regarding the\nenergy demand. The paper presents a methodology for performing building energy\nsimulation with Comsol. The method was applied to an international test box\nexperiment. The results showed an almost perfect agreement between the used BES\nmodel and Comsol. These preliminary results confirm the great opportunities to\nuse FEM related software for building energy performance simulation.", 
    "link": "http://arxiv.org/pdf/1606.01243v1", 
    "arxiv-id": "1606.01243v1"
},{
    "category": "cs.CE", 
    "author": "Stephane P. A. Bordas", 
    "title": "Bayesian inference for the stochastic identification of elastoplastic   material parameters: Introduction, misconceptions and insights", 
    "publish": "2016-06-08T07:03:09Z", 
    "summary": "We discuss Bayesian inference (BI) for the probabilistic identification of\nmaterial parameters. This contribution aims to shed light on the use of BI for\nthe identification of elastoplastic material parameters. For this purpose a\nsingle spring is considered, for which the stress-strain curves are\nartificially created. Besides offering a didactic introduction to BI, this\npaper proposes an approach to incorporate statistical errors both in the\nmeasured stresses, and in the measured strains. It is assumed that the\nuncertainty is only due to measurement errors and the material is homogeneous.\nFurthermore, a number of possible misconceptions on BI are highlighted based on\nthe purely elastic case.", 
    "link": "http://arxiv.org/pdf/1606.02422v4", 
    "arxiv-id": "1606.02422v4"
},{
    "category": "cs.CE", 
    "author": "Oleksiy Kononenko", 
    "title": "Mathematical Modeling of Dynamics for Partially Filled Shells of   Revolution", 
    "publish": "2016-06-13T08:27:21Z", 
    "summary": "In this work we study the dynamic behaviour of compound shells of revolution\npartially filled with an ideal incompressible fluid based on boundary-value\nproblems. New analytical mathematical model with corresponding discrete scheme\nfor the elastic displacements and the dynamic liquid pressure is developed. The\ndiscrete scheme is based on the method of discrete singularities. A code to\nperform the numerical analysis is developed. Comprehensive benchmarking of the\nobtained results against other methods is done and good agreement is observed.\nThe convergence of the proposed numerical method is demonstrated. One of the\nadvantages of this new model is that the initial 3D problem is analytically\nreduced to a 1D integral equation. Moreover, it can handle the behaviour of the\npressure in the vicinity of the nodes explicitly and the computational\ntechnique used has a quick convergence requiring a negligible amount of CPU\ntime.", 
    "link": "http://arxiv.org/pdf/1606.03855v2", 
    "arxiv-id": "1606.03855v2"
},{
    "category": "cs.CE", 
    "author": "David Cowburn", 
    "title": "Bias Correction in Saupe Tensor Estimation", 
    "publish": "2016-06-22T15:05:23Z", 
    "summary": "Estimation of the Saupe tensor is central to the determination of molecular\nstructures from residual dipolar couplings (RDC) or chemical shift\nanisotropies. Assuming a given template structure, the singular value\ndecomposition (SVD) method proposed in Losonczi et al. 1999 has been used\ntraditionally to estimate the Saupe tensor. Despite its simplicity, whenever\nthe template structure has large structural noise, the eigenvalues of the\nestimated tensor have a magnitude systematically smaller than their actual\nvalues. This leads to systematic error when calculating the eigenvalue\ndependent parameters, magnitude and rhombicity. We propose here a Monte Carlo\nsimulation method to remove such bias. We further demonstrate the effectiveness\nof our method in the setting when the eigenvalue estimates from multiple\ntemplate protein fragments are available and their average is used as an\nimproved eigenvalue estimator. For both synthetic and experimental RDC datasets\nof ubiquitin, when using template fragments corrupted by large noise, the\nmagnitude of our proposed bias-reduced estimator generally reaches at least 90%\nof the actual value, whereas the magnitude of SVD estimator can be shrunk below\n80% of the true value.", 
    "link": "http://arxiv.org/pdf/1606.06975v1", 
    "arxiv-id": "1606.06975v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "A Dissipative Systems Theory for FDTD with Application to Stability   Analysis and Subgridding", 
    "publish": "2016-06-28T15:45:32Z", 
    "summary": "This paper establishes a far-reaching connection between the\nFinite-Difference Time-Domain method (FDTD) and the theory of dissipative\nsystems. The FDTD equations for a rectangular region are written as a dynamical\nsystem having the magnetic and electric fields on the boundary as inputs and\noutputs. Suitable expressions for the energy stored in the region and the\nenergy absorbed from the boundaries are introduced, and used to show that the\nFDTD system is dissipative under a generalized Courant-Friedrichs-Lewy\ncondition. Based on the concept of dissipation, a powerful theoretical\nframework to investigate the stability of FDTD methods is devised. The new\nmethod makes FDTD stability proofs simpler, more intuitive, and modular.\nStability conditions can indeed be given on the individual components (e.g.\nboundary conditions, meshes, embedded models) instead of the whole coupled\nsetup. As an example of application, we derive a new subgridding method with\nmaterial traverse, arbitrary grid refinement, and guaranteed stability. The\nmethod is easy to implement and has a straightforward stability proof.\nNumerical results confirm its stability, low reflections, and ability to handle\nmaterial traverse.", 
    "link": "http://arxiv.org/pdf/1606.08761v1", 
    "arxiv-id": "1606.08761v1"
},{
    "category": "cs.CE", 
    "author": "Sergey Tarasenko", 
    "title": "An Application of the EM-algorithm to Approximate Empirical   Distributions of Financial Indices with the Gaussian Mixtures", 
    "publish": "2016-06-29T12:44:14Z", 
    "summary": "In this study I briefly illustrate application of the Gaussian mixtures to\napproximate empirical distributions of financial indices (DAX, Dow Jones,\nNikkei, RTSI, S&P 500). The resulting distributions illustrate very high\nquality of approximation as evaluated by Kolmogorov-Smirnov test. This implies\nfurther study of application of the Gaussian mixtures to approximate empirical\ndistributions of financial indices.", 
    "link": "http://arxiv.org/pdf/1607.01033v1", 
    "arxiv-id": "1607.01033v1"
},{
    "category": "cs.CE", 
    "author": "Shaloo Rakheja", 
    "title": "Solving the stochastic Landau-Lifshitz-Gilbert-Slonczewski equation for   monodomain nanomagnets : A survey and analysis of numerical techniques", 
    "publish": "2016-07-15T17:44:35Z", 
    "summary": "The stochastic Landau-Lifshitz-Gilbert-Slonczewski (s-LLGS) equation is\nwidely used to study the temporal evolution of the macrospin subject to spin\ntorque and thermal noise. The numerical simulation of the s-LLGS equation\nrequires an appropriate choice of stochastic calculus and numerical integration\nscheme. In this paper, we comprehensively evaluate the accuracy and complexity\nof various numerical techniques to solve the s-LLGS equation. We focus on\nimplicit midpoint, Heun, and Euler-Heun methods that converge to the\nStratonovich solution of the s-LLGS equation. By performing numerical tests for\nboth strong (path-wise) and weak (statistical) convergence, we quantify the\naccuracy of various numerical schemes used to solve the s-LLGS equation. We\ndemonstrate a new method intended to solve Stochastic Differential Equations\n(SDEs) with small noise (RK4-Heun), and test its capability to handle the\ns-LLGS equation. We also discuss the circuit implementation of nanomagnets for\nlarge-scale SPICE-based simulations. We evaluate the efficacy of SPICE in\nhandling the stochastic dynamics of the multiplicative noise in the s-LLGS\nequation. Numerical schemes such as Euler and Gear, typically used by\nSPICE-based circuit simulators do not yield the expected outcome when solving\nthe Stratonovich s-LLGS equation. While the trapezoidal method in SPICE does\nsolve for the Stratonovich solution, its accuracy is limited by the minimum\ntime step of integration in SPICE. We implement the s-LLGS equation in both its\ncartesian and spherical coordinates form in SPICE and compare the stability and\naccuracy of the two implementations. The results in this paper will serve as\nguidelines for researchers to understand the tradeoffs between accuracy and\ncomplexity of various numerical methods and the choice of appropriate calculus\nto solve the s-LLGS equation.", 
    "link": "http://arxiv.org/pdf/1607.04596v4", 
    "arxiv-id": "1607.04596v4"
},{
    "category": "cs.CE", 
    "author": "Alexander Popp", 
    "title": "A Unified Approach for Beam-to-Beam Contact", 
    "publish": "2016-07-29T15:45:42Z", 
    "summary": "Existing beam contact formulations can be categorized in point contact models\nthat consider a discrete contact force at the closest point of the beams, and\nline contact models that assume distributed contact forces. In this work, it\nwill be shown that line contact formulations provide accurate and robust\nmechanical models in the range of small contact angles, whereas the\ncomputational efficiency considerably decreases with increasing contact angles.\nOn the other hand, point contact formulations serve as sufficiently accurate\nand very efficient models in the regime of large contact angles, while they are\nnot applicable for small contact angles as a consequence of non-unique closest\npoint projections. In order to combine the advantages of these basic\nformulations, a novel all-angle beam contact (ABC) formulation is developed\nthat applies a point contact formulation for large contact angles and a\nrecently developed line contact formulation for small contact angles, the two\nbeing smoothly connected by means of a variationally consistent model\ntransition. Based on a stringent analysis, two different transition laws are\ninvestigated, optimal algorithmic parameters are suggested and conservation of\nenergy and momentum is shown. All configuration-dependent quantities are\nconsistently linearized, thus allowing for their application within implicit\ntime integration schemes. Furthermore, a step size control of the nonlinear\nsolution scheme that allows for large displacement increments per time step and\nan efficient two-stage contact search based on dynamically adapted search\nsegments are proposed. A series of numerical test cases is analyzed in order to\nverify the accuracy and consistency of the proposed contact model transition\nregarding contact force distributions and conservation properties, but also for\nquantifying the efficiency gains as compared to standard beam contact\nformulations.", 
    "link": "http://arxiv.org/pdf/1607.08853v1", 
    "arxiv-id": "1607.08853v1"
},{
    "category": "cs.CE", 
    "author": "Karthik Duraisamy", 
    "title": "Machine Learning-augmented Predictive Modeling of Turbulent Separated   Flows over Airfoils", 
    "publish": "2016-08-13T15:07:50Z", 
    "summary": "A modeling paradigm is developed to augment predictive models of turbulence\nby effectively utilizing limited data generated from physical experiments. The\nkey components of our approach involve inverse modeling to infer the spatial\ndistribution of model discrepancies, and, machine learning to reconstruct\ndiscrepancy information from a large number of inverse problems into corrective\nmodel forms. We apply the methodology to turbulent flows over airfoils\ninvolving flow separation. Model augmentations are developed for the Spalart\nAllmaras (SA) model using adjoint-based full field inference on experimentally\nmeasured lift coefficient data. When these model forms are reconstructed using\nneural networks (NN) and embedded within a standard solver, we show that much\nimproved predictions in lift can be obtained for geometries and flow conditions\nthat were not used to train the model. The NN-augmented SA model also predicts\nsurface pressures extremely well. Portability of this approach is demonstrated\nby confirming that predictive improvements are preserved when the augmentation\nis embedded in a different commercial finite-element solver. The broader vision\nis that by incorporating data that can reveal the form of the innate model\ndiscrepancy, the applicability of data-driven turbulence models can be extended\nto more general flows.", 
    "link": "http://arxiv.org/pdf/1608.03990v3", 
    "arxiv-id": "1608.03990v3"
},{
    "category": "cs.CE", 
    "author": "Mark Walkley", 
    "title": "A Unified Finite Element Method for Fluid-Structure Interaction", 
    "publish": "2016-08-15T22:56:50Z", 
    "summary": "In this article, we present a new unified finite element method (UFEM) for\nsimulation of general Fluid-Structure interaction (FSI) which has the same\ngenerality and robustness as monolithic methods but is significantly more\ncomputationally efficient and easier to implement. Our proposed approach has\nsimilarities with classical immersed finite element methods (IFEMs), by\napproximating a single velocity and pressure field in the entire domain (i.e.\noccupied by fluid and solid) on a single mesh, but differs by treating the\ncorrections due to the solid deformation on the left-hand side of the modified\nfluid flow equations (i.e. implicitly). The method is described in detail,\nfollowed by the presentation of multiple computational examples in order to\nvalidate it across a wide range of fluid and solid parameters and interactions.", 
    "link": "http://arxiv.org/pdf/1608.04998v1", 
    "arxiv-id": "1608.04998v1"
},{
    "category": "cs.CE", 
    "author": "Oded Amir", 
    "title": "Stress-constrained continuum topology optimization: a new approach based   on elasto-plasticity", 
    "publish": "2016-08-23T21:06:28Z", 
    "summary": "A new approach for generating stress-constrained topological designs in\ncontinua is presented. The main novelty is in the use of elasto-plastic\nmodeling and in optimizing the design such that it will exhibit a\nlinear-elastic response. This is achieved by imposing a single global\nconstraint on the total sum of equivalent plastic strains, providing accurate\ncontrol over all local stress violations. The single constraint essentially\nreplaces a large number of local stress constraints or an approximate\naggregation of them--two common approaches in the literature. A classical\nrate-independent plasticity model is utilized, for which analytical adjoint\nsensitivity analysis is derived and verified. Several examples demonstrate the\ncapability of the computational procedure to generate designs that challenge\nresults from the literature, in terms of the obtained stiffness-strength-weight\ntrade-offs. A full elasto-plastic analysis of the optimized designs shows that\nprior to the initial yielding, these designs can sustain significantly higher\nloads than minimum compliance topological layouts, with only a minor compromise\non stiffness.", 
    "link": "http://arxiv.org/pdf/1608.06654v1", 
    "arxiv-id": "1608.06654v1"
},{
    "category": "cs.CE", 
    "author": "Alexander Popp", 
    "title": "Geometrically Exact Finite Element Formulations for Curved Slender   Beams: Kirchhoff-Love Theory vs. Simo-Reissner Theory", 
    "publish": "2016-09-01T06:11:46Z", 
    "summary": "The present work focuses on geometrically exact finite elements for highly\nslender beams. It aims at the proposal of novel formulations of Kirchhoff-Love\ntype, a detailed review of existing formulations of Kirchhoff-Love and\nSimo-Reissner type as well as a careful evaluation and comparison of the\nproposed and existing formulations. Two different rotation interpolation\nschemes with strong or weak Kirchhoff constraint enforcement, respectively, as\nwell as two different choices of nodal triad parametrizations in terms of\nrotation or tangent vectors are proposed. The combination of these schemes\nleads to four novel finite element variants, all of them based on a\nC1-continuous Hermite interpolation of the beam centerline. Essential\nrequirements such as representability of general 3D, large-deformation, dynamic\nproblems involving slender beams with arbitrary initial curvatures and\nanisotropic cross-section shapes or preservation of objectivity and\npath-independence will be investigated analytically and verified numerically\nfor the different formulations. It will be shown that the geometrically exact\nKirchhoff-Love beam elements proposed in this work are the first ones of this\ntype that fulfill all the considered requirements. On the contrary,\nSimo-Reissner type formulations fulfilling these requirements can be found in\nthe literature very well. However, it will be argued that the shear-free\nKirchhoff-Love formulations can provide considerable numerical advantages when\napplied to highly slender beams. Concretely, several representative numerical\ntest cases confirm that the proposed Kirchhoff-Love formulations exhibit a\nlower discretization error level as well as a considerably improved nonlinear\nsolver performance in the range of high beam slenderness ratios as compared to\ntwo representative Simo-Reissner element formulations from the literature.", 
    "link": "http://arxiv.org/pdf/1609.00119v1", 
    "arxiv-id": "1609.00119v1"
},{
    "category": "cs.CE", 
    "author": "Maarten de Hoop", 
    "title": "Inverse Problems with Invariant Multiscale Statistics", 
    "publish": "2016-09-18T15:44:04Z", 
    "summary": "We propose a new approach to linear ill-posed inverse problems. Our algorithm\nalternates between enforcing two constraints: the measurements and the\nstatistical correlation structure in some transformed space. We use a\nnon-linear multiscale scattering transform which discards the phase and thus\nexposes strong spectral correlations otherwise hidden beneath the phase\nfluctuations. As a result, both constraints may be put into effect by linear\nprojections in their respective spaces. We apply the algorithm to\nsuper-resolution and tomography and show that it outperforms ad hoc convex\nregularizers and stably recovers the missing spectrum.", 
    "link": "http://arxiv.org/pdf/1609.05502v2", 
    "arxiv-id": "1609.05502v2"
},{
    "category": "cs.CE", 
    "author": "Maria Serna", 
    "title": "Uncertainty Analysis of Simple Macroeconomic Models Using Angel-Daemon   Games", 
    "publish": "2016-09-19T12:48:52Z", 
    "summary": "We propose the use of an angel-daemon framework to perform an uncertainty\nanalysis of short-term macroeconomic models with exogenous components. An\nuncertainty profile $\\mathcal U$ is a short and macroscopic description of a\npotentially perturbed situation. The angel-daemon framework uses $\\mathcal U$\nto define a strategic game where two agents, the angel and the daemon, act\nselfishly having different goals. The Nash equilibria of those games provide\nthe stable strategies in perturbed situations, giving a natural estimation of\nuncertainty.\n  In this initial work we apply the framework in order to get an uncertainty\nanalysis of linear versions of the IS-LM and the IS-MP models. In those models,\nby considering uncertainty profiles, we can capture different economical\nsituations. Some of them can be described in terms of macroeconomic policy\ncoordination. In other cases we just analyse the results of the system under\nsome possible perturbation level. Besides providing examples of application we\nanalyse the structure of the Nash equilibria in some particular cases of\ninterest.", 
    "link": "http://arxiv.org/pdf/1609.06153v1", 
    "arxiv-id": "1609.06153v1"
},{
    "category": "cs.CE", 
    "author": "Sebastian Sch\u00f6ps", 
    "title": "Determination of Bond Wire Failure Probabilities in Microelectronic   Packages", 
    "publish": "2016-09-18T16:50:47Z", 
    "summary": "This work deals with the computation of industry-relevant bond wire failure\nprobabilities in microelectronic packages. Under operating conditions, a\npackage is subject to Joule heating that can lead to electrothermally induced\nfailures. Manufacturing tolerances result, e.g., in uncertain bond wire\ngeometries that often induce very small failure probabilities requiring a high\nnumber of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC\nsampling scheme that combines the use of an expensive computer model with a\ncheap surrogate is used. The fraction of surrogate evaluations is maximized\nusing an iterative procedure, yielding accurate results at reduced cost.\nMoreover, the scheme is non-intrusive, i.e., existing code can be reused. The\nalgorithm is used to compute the failure probability for an example package and\nthe computational savings are assessed by performing a surrogate efficiency\nstudy.", 
    "link": "http://arxiv.org/pdf/1609.06187v2", 
    "arxiv-id": "1609.06187v2"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "A Stable FDTD Method with Embedded Reduced-Order Models", 
    "publish": "2016-09-22T19:19:21Z", 
    "summary": "The computational efficiency of the Finite-Difference Time-Domain (FDTD)\nmethod can be significantly reduced by the presence of complex objects with\nfine features. Small geometrical details impose a fine mesh and a reduced time\nstep, significantly increasing computational cost. Model order reduction has\nbeen proposed as a systematic way to generate compact models for complex\nobjects, that one can then instantiate into a main FDTD mesh. However, the\nstability of FDTD with embedded reduced models remains an open problem. We\npropose a systematic method to generate reduced models for FDTD domains, and\nembed them into a main FDTD mesh with guaranteed stability. Models can be\ncreated for arbitrary domains containing inhomogeneous and lossy materials. The\nCourant-Friedrichs-Lewy (CFL) limit of the final scheme is provided by the\ntheory, and can be extended with a simple perturbation of the coefficients of\nthe reduced models. Numerical tests confirm the stability of the proposed\nmethod, and its potential to accelerate multiscale FDTD simulations.", 
    "link": "http://arxiv.org/pdf/1609.07114v1", 
    "arxiv-id": "1609.07114v1"
},{
    "category": "cs.CE", 
    "author": "Natalia Alguacil", 
    "title": "On the Solution of Large-Scale Robust Transmission Network Expansion   Planning under Uncertain Demand and Generation Capacity", 
    "publish": "2016-09-26T09:28:48Z", 
    "summary": "Two-stage robust optimization has emerged as a relevant approach to deal with\nuncertain demand and generation capacity in the transmission network expansion\nplanning problem. Unfortunately, available solution methodologies for the\nresulting trilevel robust counterpart are unsuitable for large-scale problems.\nIn order to overcome this shortcoming, this paper presents an alternative\ncolumn-and-constraint generation algorithm wherein the max-min problem\nassociated with the second stage is solved by a novel coordinate descent\nmethod. As a major salient feature, the proposed approach does not rely on the\ntransformation of the second-stage problem to a single-level equivalent. As a\nconsequence, bilinear terms involving dual variables or Lagrange multipliers do\nnot arise, thereby precluding the use of computationally expensive big-M-based\nlinearization schemes. Thus, not only is the computational effort reduced, but\nalso the typically overlooked non-trivial tuning of bounding parameters for\ndual variables or Lagrange multipliers is avoided. The practical applicability\nof the proposed methodology is confirmed by numerical testing on several\nbenchmarks including a case based on the Polish 2383-bus system, which is well\nbeyond the capability of the robust methods available in the literature.", 
    "link": "http://arxiv.org/pdf/1609.07902v1", 
    "arxiv-id": "1609.07902v1"
},{
    "category": "cs.CE", 
    "author": "Chris J. Pearce", 
    "title": "Energy consistent framework for continuously evolving 3D crack   propagation", 
    "publish": "2016-09-30T11:16:35Z", 
    "summary": "This paper presents a formulation for brittle fracture in 3D elastic solids\nwithin the context of configurational mechanics. The local form of the first\nlaw of thermodynamics provides a condition for equilibrium of the crack front.\nThe direction of the crack propagation is shown to be given by the direction of\nthe configurational forces on the crack front that maximise the local\ndissipation. The evolving crack front is continuously resolved by the finite\nelement mesh, without the need for face splitting or the use of enrichment\ntechniques. A monolithic solution strategy is adopted, solving simultaneously\nfor both the material displacements (i.e. crack extension) and the spatial\ndisplacements, is adopted. In order to trace the dissipative loading path, an\narc-length procedure is developed that controls the incremental crack area\ngrowth. In order to maintain mesh quality, smoothing of the mesh is undertaken\nas a continuous process, together with face flipping, node merging and edge\nsplitting where necessary. Hierarchical basis functions of arbitrary polynomial\norder are adopted to increase the order of approximation without the need to\nchange the finite element mesh. Performance of the formulation is demonstrated\nby means of three representative numerical simulations, demonstrating both\naccuracy and robustness.", 
    "link": "http://arxiv.org/pdf/1610.00004v1", 
    "arxiv-id": "1610.00004v1"
},{
    "category": "cs.CE", 
    "author": "Roberto Lionello", 
    "title": "Advancing parabolic operators in thermodynamic MHD models: Explicit   super time-stepping versus implicit schemes with Krylov solvers", 
    "publish": "2016-10-05T03:32:11Z", 
    "summary": "We explore the performance and advantages/disadvantages of using\nunconditionally stable explicit super time-stepping (STS) algorithms versus\nimplicit schemes with Krylov solvers for integrating parabolic operators in\nthermodynamic MHD models of the solar corona. Specifically, we compare the\nsecond-order Runge-Kutta Legendre (RKL2) STS method with the implicit backward\nEuler scheme computed using the preconditioned conjugate gradient (PCG) solver\nwith both a point-Jacobi and a non-overlapping domain decomposition ILU0\npreconditioner. The algorithms are used to integrate anisotropic Spitzer\nthermal conduction and artificial kinematic viscosity at time-steps much larger\nthan classic explicit stability criteria allow. A key component of the\ncomparison is the use of an established MHD model (MAS) to compute a real-world\nsimulation on a large HPC cluster. Special attention is placed on the parallel\nscaling of the algorithms. It is shown that, for a specific problem and model,\nthe RKL2 method is comparable or surpasses the implicit method with PCG solvers\nin performance and scaling, but suffers from some accuracy limitations. These\nlimitations, and the applicability of RKL methods are briefly discussed.", 
    "link": "http://arxiv.org/pdf/1610.01265v1", 
    "arxiv-id": "1610.01265v1"
},{
    "category": "cs.CE", 
    "author": "Carl Lundholm", 
    "title": "Simulation of flow and view with applications in computational design of   settlement layouts", 
    "publish": "2016-10-07T13:33:50Z", 
    "summary": "We present methodology, algorithms and software for evaluating flow and view\nfor architectural settlement layouts. For a given settlement layout consisting\nof a number of buildings arbitrarily positioned on a piece of land, in the\npresent study an island situated on the west coast of Sweden, the methodology\nallows for evaluation of flow patterns and for evaluating the view experienced\nfrom the buildings. The computation of flow is based on a multimesh finite\nelement method, which allows each building to be embedded in a boundary-fitted\nmesh which can be moved around freely in a fixed background mesh. The\ncomputation of view is based on a novel and objective measure of the view which\ncan be efficiently computed by rasterization.", 
    "link": "http://arxiv.org/pdf/1610.02277v1", 
    "arxiv-id": "1610.02277v1"
},{
    "category": "cs.CE", 
    "author": "Chris Pearce", 
    "title": "Multi-scale computational homogenisation to predict the long-term   durability of composite structures", 
    "publish": "2016-10-11T15:47:23Z", 
    "summary": "A coupled hygro-thermo-mechanical computational model is proposed for fibre\nreinforced polymers, formulated within the framework of Computational\nHomogenisation (CH). At each macrostructure Gauss point, constitutive matrices\nfor thermal, moisture transport and mechanical responses are calculated from CH\nof the underlying representative volume element (RVE). A degradation model,\ndeveloped from experimental data relating evolution of mechanical properties\nover time for a given exposure temperature and moisture concentration is also\ndeveloped and incorporated in the proposed computational model. A unified\napproach is used to impose the RVE boundary conditions, which allows convenient\nswitching between linear Dirichlet, uniform Neumann and periodic boundary\nconditions. A plain weave textile composite RVE consisting of yarns embedded in\na matrix is considered in this case. Matrix and yarns are considered as\nisotropic and transversely isotropic materials respectively. Furthermore, the\ncomputational framework utilises hierarchic basis functions and designed to\ntake advantage of distributed memory high-performance computing.", 
    "link": "http://arxiv.org/pdf/1610.03484v2", 
    "arxiv-id": "1610.03484v2"
},{
    "category": "cs.CE", 
    "author": "Christopher J. Vogl", 
    "title": "The Curvature-Augmented Closest Point Method with Vesicle   Inextensibility Application", 
    "publish": "2016-10-13T04:08:32Z", 
    "summary": "The Closest Point method, initially developed by Ruuth and Merriman, allows\nfor the numerical solution of surface partial differential equations without\nthe need for a parameterization of the surface itself. Surface quantities are\nembedded into the surrounding domain by assigning each value at a given spatial\nlocation to the corresponding value at the closest point on the surface. This\nembedding allows for surface derivatives to be replaced by their Cartesian\ncounterparts (e.g. $\\nabla_s = \\nabla$). This equivalence is only valid on the\nsurface, and thus, interpolation is used to enforce what is known as the side\ncondition away from the surface. To improve upon the method, this work derives\nan operator embedding that incorporates curvature information, making it valid\nin a neighborhood of the surface. With this, direct enforcement of the side\ncondition is no longer needed. Comparisons in $\\mathbb{R}^2$ and $\\mathbb{R}^3$\nshow that the resulting Curvature-Augmented Closest Point method has better\naccuracy and requires less memory, through increased matrix sparsity, than the\nClosest Point method. To demonstrate the utility of the method in a physical\napplication, simulations of inextensible, bi-lipid vesicles evolving toward\nequilibrium shapes are also included.", 
    "link": "http://arxiv.org/pdf/1610.03932v1", 
    "arxiv-id": "1610.03932v1"
},{
    "category": "cs.CE", 
    "author": "Sebastian Sch\u00f6ps", 
    "title": "Electrothermal Simulation of Bonding Wire Degradation under Uncertain   Geometries", 
    "publish": "2016-10-14T00:46:08Z", 
    "summary": "In this paper, electrothermal field phenomena in electronic components are\nconsidered. This coupling is tackled by multiphysical field simulations using\nthe Finite Integration Technique (FIT). In particular, the design of bonding\nwires with respect to thermal degradation is investigated. Instead of resolving\nthe wires by the computational grid, lumped element representations are\nintroduced as point-to-point connections in the spatially distributed model.\nFabrication tolerances lead to uncertainties of the wires' parameters and\ninfluence the operation and reliability of the final product. Based on\ngeometric measurements, the resulting variability of the wire temperatures is\ndetermined using the stochastic electrothermal field-circuit model.", 
    "link": "http://arxiv.org/pdf/1610.04303v1", 
    "arxiv-id": "1610.04303v1"
},{
    "category": "cs.CE", 
    "author": "Sebastian Sch\u00f6ps", 
    "title": "Automatic Generation of Equivalent Electrothermal SPICE Netlists from 3D   Electrothermal Field Models", 
    "publish": "2016-10-14T00:47:04Z", 
    "summary": "Starting from a 3D electrothermal field problem discretized by the Finite\nIntegration Technique, the equivalence to a circuit description is shown by\nexploiting the analogy to the Modified Nodal Analysis approach. Using this\nanalogy, an algorithm for the automatic generation of a monolithic SPICE\nnetlist is presented. Joule losses from the electrical circuit are included as\nheat sources in the thermal circuit. The thermal simulation yields nodal\ntemperatures that influence the electrical conductivity. Apart from the used\nfield discretization, this approach applies no further simplifications. An\nexample 3D chip package is used to validate the algorithm.", 
    "link": "http://arxiv.org/pdf/1610.04304v1", 
    "arxiv-id": "1610.04304v1"
},{
    "category": "cs.CE", 
    "author": "Chris J. Pearce", 
    "title": "Three-dimensional nonlinear micro/meso-mechanical response of the   fibre-reinforced polymer composites", 
    "publish": "2016-10-14T19:43:01Z", 
    "summary": "A three-dimensional multi-scale computational homogenisation framework is\ndeveloped for the prediction of nonlinear micro/meso-mechanical response of the\nfibre-reinforced polymer (FRP) composites. Two dominant damage mechanisms, i.e.\nmatrix elasto-plastic response and fibre-matrix decohesion are considered and\nmodelled using a non-associative pressure dependent paraboloidal yield\ncriterion and cohesive interface elements respectively. A linear-elastic\ntransversely isotropic material model is used to model yarns/fibres within the\nrepresentative volume element (RVE). A unified approach is used to impose the\nRVE boundary conditions, which allows convenient switching between linear\ndisplacement, uniform traction and periodic boundary conditions. The\ncomputational model is implemented within the framework of the hierarchic\nfinite element, which permits the use of arbitrary orders of approximation.\nFurthermore, the computational framework is designed to take advantage of\ndistributed memory high-performance computing. The accuracy and performance of\nthe computational framework are demonstrated with a variety of numerical\nexamples, including unidirectional FRP composite, a composite comprising a\nmulti-fibre and multi-layer RVE, with randomly generated fibres, and a single\nlayered plain weave textile composite. Results are validated against the\nreference experimental/numerical results from the literature. The computational\nframework is also used to study the effect of matrix and fibre-matrix\ninterfaces properties on the homogenised stress-strain responses.", 
    "link": "http://arxiv.org/pdf/1610.04610v1", 
    "arxiv-id": "1610.04610v1"
},{
    "category": "cs.CE", 
    "author": "J\u00fcrgen Zechner", 
    "title": "Simulation of electrical machines - A FEM-BEM coupling scheme", 
    "publish": "2016-10-18T08:09:36Z", 
    "summary": "Electrical machines commonly consist of moving and stationary parts. The\nfield simulation of such devices can be very demanding if the underlying\nnumerical scheme is solely based on a domain discretization, such as in case of\nthe Finite Element Method (FEM). Here, a coupling scheme based on FEM together\nwith Boundary Element Methods (BEM) is presented that neither hinges on\nre-meshing techniques nor deals with a special treatment of sliding interfaces.\nWhile the numerics are certainly more involved the reward is obvious: The\nmodeling costs decrease and the application engineer is provided with an\neasy-to-use, versatile, and accurate simulation tool.", 
    "link": "http://arxiv.org/pdf/1610.05472v2", 
    "arxiv-id": "1610.05472v2"
},{
    "category": "cs.CE", 
    "author": "Yakov A. Pachepsky", 
    "title": "Scale-Dependent Pedotransfer Functions Reliability for Estimating   Saturated Hydraulic Conductivity", 
    "publish": "2016-10-21T21:29:06Z", 
    "summary": "Saturated hydraulic conductivity Ksat is a fundamental characteristic in\nmodeling flow and contaminant transport in soils and sediments. Therefore, many\nmodels have been developed to estimate Ksat from easily measureable parameters,\nsuch as textural properties, bulk density, etc. However, Ksat is not only\naffected by textural and structural characteristics, but also by scale e.g.,\ninternal diameter and height. Using the UNSODA database and the contrast\npattern aided regression (CPXR) method, we recently developed scale-dependent\npedotransfer functions to estimate Ksat from textural data, bulk density, and\nsample dimensions. The main objectives of this study were evaluating the\nproposed pedotransfer functions using a larger database, and comparing them\nwith seven other models. For this purpose, we selected more than nineteen\nthousands soil samples from all around the United States. Results showed that\nthe scale-dependent pedotransfer functions estimated Ksat more accurately than\nseven other models frequently used in the literature.", 
    "link": "http://arxiv.org/pdf/1610.06958v1", 
    "arxiv-id": "1610.06958v1"
},{
    "category": "cs.CE", 
    "author": "Johannes B. Rutzmoser", 
    "title": "A Quadratic Manifold for Model Order Reduction of Nonlinear Structural   Dynamics", 
    "publish": "2016-10-31T12:57:21Z", 
    "summary": "This paper describes the use of a quadratic manifold for the model order\nreduction of structural dynamics problems featuring geometric nonlinearities.\nThe manifold is tangent to a subspace spanned by the most relevant vibration\nmodes, and its curvature is provided by modal derivatives obtained by\nsensitivity analysis of the eigenvalue problem, or its static approximation,\nalong the vibration modes. The construction of the quadratic manifold requires\nminimal computational effort once the vibration modes are known. The reduced\norder model is then obtained by Galerkin projection, where the\nconfiguration-dependent tangent space of the manifold is used to project the\ndiscretized equations of motion.", 
    "link": "http://arxiv.org/pdf/1610.09902v2", 
    "arxiv-id": "1610.09902v2"
},{
    "category": "cs.CE", 
    "author": "S. Jain", 
    "title": "Generalization of Quadratic Manifolds for Reduced Order Modeling of   Nonlinear Structural Dynamics", 
    "publish": "2016-10-31T13:13:52Z", 
    "summary": "In this paper, a generalization of a quadratic manifold approach for the\nreduction of geometrically nonlinear structural dynamics problems is presented.\nThis generalization is constructed by a linearization of the static force with\nrespect to the generalized coordinates, resulting in a shift of the quadratic\nbehavior from the force to the manifold. In this framework, static derivatives\nemerge as natural extensions to modal derivatives for displacement fields other\nthan the vibration modes, such as the Krylov subspace vectors. Here the dynamic\nproblem is projected onto the tangent space of the quadratic manifold, allowing\nfor a much less number of generalized coordinates compared to linear basis\nmethods. The potential of the quadratic manifold approach is investigated in a\nnumerical study, where several variations of the approach are compared on\ndifferent examples, indicating a clear pattern where the proposed approach is\napplicable.", 
    "link": "http://arxiv.org/pdf/1610.09906v1", 
    "arxiv-id": "1610.09906v1"
},{
    "category": "cs.CE", 
    "author": "Aurora Manicardi", 
    "title": "Hydropower optimization: an industrial approach", 
    "publish": "2016-10-31T18:34:36Z", 
    "summary": "Nowadays hydroelectric energy is one of the best energy sources: it is\ncleaner, safer and more programmable than other sources. For this reason, its\nmanage could not be done in an approssimative way, but advance mathematical\nmodels must be use. In this article we consider an overview of the problem: we\nintroduce the problem, then we show its simplest but quite exaustive\nmathematical formulation and in the end we produce numerical results under the\nipothesis that all input are deterministic.", 
    "link": "http://arxiv.org/pdf/1610.10057v1", 
    "arxiv-id": "1610.10057v1"
},{
    "category": "cs.CE", 
    "author": "Daniele Pellegrini", 
    "title": "Modal Analysis of Masonry Structures", 
    "publish": "2016-11-02T10:18:13Z", 
    "summary": "This paper presents a new numerical procedure for evaluating the vibration\nfrequencies and mode shapes of masonry buildings in the presence of cracks. The\nalgorithm has been implemented within the NOSA-ITACA code, which models masonry\nas a nonlinear elastic material with zero tensile strength. Some case studies\nare reported, and the differences between linear and nonlinear behavior\nhighlighted.", 
    "link": "http://arxiv.org/pdf/1611.00531v1", 
    "arxiv-id": "1611.00531v1"
},{
    "category": "cs.CE", 
    "author": "Karl Henning Halse", 
    "title": "Dual Quaternion Variational Integrator for Rigid Body Dynamic Simulation", 
    "publish": "2016-11-02T14:02:16Z", 
    "summary": "We introduce a symplectic dual quaternion variational integrator(DQVI) for\nsimulating single rigid body motion in all six degrees of freedom. Dual\nquaternion is used to represent rigid body kinematics and one-step Lie group\nvariational integrator is used to conserve the geometric structure, energy and\nmomentum of the system during the simulation. The combination of these two\nbecomes the first Lie group variational integrator for rigid body simulation\nwithout decoupling translations and rotations. Newton-Raphson method is used to\nsolve the recursive dynamic equation. This method is suitable for real-time\nrigid body simulations with high precision under large time step. DQVI respects\nthe symplectic structure of the system with excellent long-term conservation of\ngeometry structure, momentum and energy. It also allows the reference point and\n6-by-6 inertia matrix to be arbitrarily defined, which is very convenient for a\nvariety of engineering problems.", 
    "link": "http://arxiv.org/pdf/1611.00616v1", 
    "arxiv-id": "1611.00616v1"
},{
    "category": "cs.CE", 
    "author": "Sean V. Hum", 
    "title": "A Novel Single-Source Surface Integral Method to Compute Scattering from   Dielectric Objects", 
    "publish": "2016-11-18T23:02:35Z", 
    "summary": "Using the traditional surface integral methods, the computation of scattering\nfrom a dielectric object requires two equivalent current densities on the\nboundary of the dielectric. In this paper, we present an approach that requires\nonly a single current density. Our method is based on a surface admittance\noperator and is applicable to dielectric bodies of arbitrary shape. The\nformulation results in four times lower memory consumption and up to eight\ntimes lower time to solve the linear system than the traditional PMCHWT\nformulation. Numerical results demonstrate that the proposed technique is as\naccurate as the PMCHWT formulation.", 
    "link": "http://arxiv.org/pdf/1611.06271v1", 
    "arxiv-id": "1611.06271v1"
},{
    "category": "cs.CE", 
    "author": "Fr\u00e9d\u00e9ric Dufour", 
    "title": "Studying the influence of inclusion characteristics on the   characteristic length involved in quasi-brittle materials using the lattice   element method", 
    "publish": "2016-11-19T16:17:12Z", 
    "summary": "Unlike nonlocal models, there is no need to introduce an internal length in\nthe constitutive law for lattice model at the mesoscopic scale. Actually, the\ninternal length is not explicitly introduced but rather governed by the\nmesostructure characteristics themselves. The influence of the mesostructure on\nthe width of the fracture process zone which is assumed to be correlated to the\ncharacteristic length of the homogenized quasi-brittle material is studied. The\ninfluence of the ligament size (a structural parameter) is also investigated.\nThis analysis provides recommendations/warnings when extracting an internal\nlength required for nonlocal damage models from the material mesostructure", 
    "link": "http://arxiv.org/pdf/1611.06396v1", 
    "arxiv-id": "1611.06396v1"
},{
    "category": "cs.CE", 
    "author": "Asgeir Mjelve", 
    "title": "High-Frequency Modeling and Simulation of a Single-Phase Three-Winding   Transformer Including Taps in Regulating Winding", 
    "publish": "2016-11-17T14:27:39Z", 
    "summary": "Transformer terminal equivalents obtained via admittance measurements are\nsuitable for simulating high-frequency transient interaction between the\ntransformer and the network. This paper augments the terminal equivalent\napproach with a measurement-based voltage transfer function model which permits\ncalculation of voltages at internal points in the regulating winding. The\napproach is demonstrated for a single-phase three-winding transformer in tap\nposition Nom+ with inclusion of three internal points in the regulating winding\nthat represent the mid-point and the two extreme ends. The terminal equivalent\nmodeling makes use of additional common-mode measurements to avoid error\nmagnifications to result from the ungrounded tertiary winding. The final model\nis used in a time domain simulation where ground-fault initiation results in a\nresonant voltage build-up in the winding. It is shown that that the peak value\nof the resonant overvoltage can be higher than during the lightning impulse\ntest, with unfavorable network conditions. Additional measurements show that\nthe selected tap position affects the terminal behavior of the transformer,\nchanging the frequency and peak value of the lower resonance point in the\nvoltage transfer between windings.", 
    "link": "http://arxiv.org/pdf/1611.06868v1", 
    "arxiv-id": "1611.06868v1"
},{
    "category": "cs.CE", 
    "author": "Thomas Weiland", 
    "title": "Discretization of Maxwell's Equations for Non-inertial Observers Using   Space-Time Algebra", 
    "publish": "2016-11-21T13:11:29Z", 
    "summary": "We employ Maxwell's equations formulated in Space-Time Algebra to perform\ndiscretization of moving geometries directly in space-time. All the derivations\nare carried out without any non-relativistic assumptions, thus the application\narea of the scheme is not restricted to low velocities. The 4D mesh\nconstruction is based on a 3D mesh stemming from a conventional 3D mesh\ngenerator. The movement of the system is encoded in the 4D mesh geometry,\nenabling an easy extension of well-known 3D approaches to the space-time\nsetting. As a research example, we study a manifestation of Sagnac's effect in\na rotating ring resonator. In case of constant rotation, the space-time\napproach enhances the efficiency of the scheme, as the material matrices are\nconstant for every time step, without abandoning the relativistic framework.", 
    "link": "http://arxiv.org/pdf/1611.07368v1", 
    "arxiv-id": "1611.07368v1"
},{
    "category": "cs.CE", 
    "author": "Hui Liu", 
    "title": "Researches on Dynamic Load Balancing Algorithms and hp-Adaptivity in 3-D   Parallel Adaptive Finite Element Computations", 
    "publish": "2016-11-24T17:31:41Z", 
    "summary": "This work is related to PHG (Parallel Hierarchical Grid). PHG is a toolbox\nfor developing parallel adaptive finite element programs, which is under active\ndevelopment at the State Key Laboratory of Scientific and Engineering\nComputing. The main results of this work are as follows.\n  1) For the tetrahedral meshes used in PHG, under reasonable assumptions, we\nproved the existence of through-vertex Hamiltonian paths between arbitrary two\nvertices, as well as the existence of through-vertex Hamiltonian cycles, and\ndesigned an efficient algorithm with linear complexity for constructing\nthrough-vertex Hamiltonian paths. The resulting algorithm has been implemented\nin PHG, and is used for ordering elements in the coarsest mesh for the\nrefinement tree mesh partitioning algorithm.\n  2) We designed encoding and decoding algorithms for high dimensional Hilbert\norder. Hilbert order has good locality, and it has wide applications in various\nfields in computer science, such as memory management, database, and dynamic\nload balancing.\n  3) We implemented refinement tree and space filling curve based mesh\npartitioning algorithms in PHG, and designed the dynamic load balancing module\nof PHG. The refinement tree based partitioning algorithm was originally\nproposed by Mitchell, the one implemented in PHG was improved in several\naspects. The space filling curve based mesh partitioning function in PHG can\nuse either Hilbert or Morton space filling curve.\n  4) We studied existing hp-adaptive strategies in the literature, and proposed\na new strategy. Numerical experiments show that our new strategy achieves\nexponential convergence, and is superior, in both precision of the solutions\nand computation time, to the strategy compared. This part of the work also\nserves to validate the hp-adaptivity module of PHG.", 
    "link": "http://arxiv.org/pdf/1611.08266v1", 
    "arxiv-id": "1611.08266v1"
},{
    "category": "cs.CE", 
    "author": "R. Korakitis", 
    "title": "Geodesic equations and their numerical solutions in geodetic and   Cartesian coordinates on an oblate spheroid", 
    "publish": "2016-11-28T13:31:03Z", 
    "summary": "The direct geodesic problem on an oblate spheroid is described as an initial\nvalue problem and is solved numerically in geodetic and Cartesian coordinates.\nThe geodesic equations are formulated by means of the theory of differential\ngeometry. The initial value problem under consideration is reduced to a system\nof first-order ordinary differential equations, which is solved using a\nnumerical method. The solution provides the coordinates and the azimuths at any\npoint along the geodesic. The Clairaut constant is not assumed known but it is\ncomputed, allowing to check the precision of the method. An extended data set\nof geodesics is used, in order to evaluate the performance of the method in\neach coordinate system. The results for the direct geodesic problem are\nvalidated by comparison to Karney's method. We conclude that a complete,\nstable, precise, accurate and fast solution of the problem in Cartesian\ncoordinates is accomplished.", 
    "link": "http://arxiv.org/pdf/1612.01357v1", 
    "arxiv-id": "1612.01357v1"
},{
    "category": "cs.CE", 
    "author": "Carlos Barr\u00f3n-Romero", 
    "title": "Discrete Optimal Global Convergence of an Evolutionary Algorithm for   Clusters under the Potential of Lennard Jones", 
    "publish": "2017-01-03T00:37:08Z", 
    "summary": "A review of the properties that bond the particles under Lennard Jones\nPotential allow to states properties and conditions for building evolutive\nalgorithms using the CB lattice with other different lattices. The new lattice\nis called CB lattice and it is based on small cubes. A set of propositions\nstates convergence and optimal conditions over the CB lattice for an\nevolutionary algorithm. The evolutionary algorithm is a reload version of\nprevious genetic algorithms based in phenotypes. The novelty using CB lattice,\ntogether with the other lattices, and ad-hoc cluster segmentation and\nenumeration, is to allow the combination of genotype (DNA coding for cluster\nusing their particle's number) and phenotype (geometrical shapes using\nparticle's coordinates in 3D). A parallel version of an evolutionary algorithm\nfor determining the global optimality is depicted. The results presented are\nfrom a standalone program for a personal computer of the evolutionary\nalgorithm, which can estimate all putative optimal Lennard Jones Clusters from\n13 to 1612 particles. The novelty are the theoretical results for the\nevolutionary algorithm's efficiency, the strategies with phenotype or genotype,\nand the classification of the clusters based in an ad-hoc geometric algorithm\nfor segmenting a cluster into its nucleus and layers. Also, the standalone\nprogram is not only capable to replicate the optimal Lennard Jones clusters in\nThe Cambridge Cluster Database (CCD), but to find new ones.", 
    "link": "http://arxiv.org/pdf/1701.00557v2", 
    "arxiv-id": "1701.00557v2"
},{
    "category": "cs.CE", 
    "author": "Michal \u0160ejnoha", 
    "title": "Comparison of viscoelastic finite element models for laminated glass   beams", 
    "publish": "2017-01-13T11:53:56Z", 
    "summary": "We are concerned with finite element modeling of geometrically non-linear\nlaminated glass beams consisting of stiff elastic glass layers connected with\ncompliant polymeric interlayer of temperature-sensitive viscoelastic behavior.\nIn particular, four layerwise theories are introduced in this paper, which\ndiffer in the non-linear beam formulation used at the layer level (von\nK\\'{a}rm\\'{a}n/Reissner) and in constitutive assumptions used for interlayer\n(viscoelastic solid with time-independent bulk modulus/Poisson ratio). We show\nthat all formulations deliver practically identical responses for\nsimply-supported and fixed-end three-layer beams. For the most straightforward\nformulation, combining the von K\\'{a}rm\\'{a}n model with the assumption of\ntime-independent Poisson ratio, we perform detailed verification and validation\nstudies at different temperatures and compare its accuracy with simplified\nelastic solutions mostly used in practice. These findings provide a suitable\nbasis for extensions towards laminated plates and/or glass layer fracture,\nowing to the modular format of layerwise theories.", 
    "link": "http://arxiv.org/pdf/1701.03636v1", 
    "arxiv-id": "1701.03636v1"
},{
    "category": "cs.CE", 
    "author": "Serafim Kalliadasis", 
    "title": "Pseudospectral methods for density functional theory in bounded and   unbounded domains", 
    "publish": "2017-01-22T16:23:38Z", 
    "summary": "Classical Density Functional Theory (DFT) is a statistical-mechanical\nframework to analyze fluids, which accounts for nanoscale fluid inhomogeneities\nand non-local intermolecular interactions. DFT can be applied to a wide range\nof interfacial phenomena, as well as problems in adsorption, colloidal science\nand phase transitions in fluids. Typical DFT equations are highly non-linear,\nstiff and contain several convolution terms. We propose a novel, efficient\npseudo-spectral collocation scheme for computing the non-local terms in real\nspace with the help of a specialized Gauss quadrature. Due to the exponential\naccuracy of the quadrature and a convenient choice of collocation points near\ninterfaces, we can use grids with a significantly lower number of nodes than\nmost other reported methods. We demonstrate the capabilities of our numerical\nmethodology by studying equilibrium and dynamic two-dimensional test cases with\nsingle- and multispecies hard-sphere and hard-disc particles modelled with\nfundamental measure theory, with and without van der Waals attractive forces,\nin bounded and unbounded physical domains. We show that our results satisfy\nstatistical mechanical sum rules.", 
    "link": "http://arxiv.org/pdf/1701.06182v1", 
    "arxiv-id": "1701.06182v1"
},{
    "category": "cs.CE", 
    "author": "Zhangxin Chen", 
    "title": "A Parallel Simulator for Massive Reservoir Models Utilizing   Distributed-Memory Parallel Systems", 
    "publish": "2017-01-23T02:35:21Z", 
    "summary": "This paper presents our work on developing parallel computational methods for\ntwo-phase flow on modern parallel computers, where techniques for linear\nsolvers and nonlinear methods are studied and the standard and inexact Newton\nmethods are investigated. A multi-stage preconditioner for two-phase flow is\napplied and advanced matrix processing strategies are studied. A local\nreordering method is developed to speed the solution of linear systems.\nNumerical experiments show that these computational methods are effective and\nscalable, and are capable of computing large-scale reservoir simulation\nproblems using thousands of CPU cores on parallel computers. The nonlinear\ntechniques, preconditioner and matrix processing strategies can also be applied\nto three-phase black oil, compositional and thermal models.", 
    "link": "http://arxiv.org/pdf/1701.06254v1", 
    "arxiv-id": "1701.06254v1"
},{
    "category": "cs.CE", 
    "author": "I. Cali\u00f2", 
    "title": "Closed-form solution based Genetic Algorithm Software: Application to   multiple cracks detection on beam structures by static tests", 
    "publish": "2017-01-14T08:39:00Z", 
    "summary": "In this paper a procedure for the static identification and reconstruction of\nconcentrated damage distribution in beam-like structures, implemented in a\ndedicated software, is presented. The proposed damage identification strategy\nrelies on the solution of an optimisation problem, by means of a genetic\nalgorithm, which exploits the closed form solution based on the distribution\ntheory of multi-cracked beams subjected to static loads. Precisely, the\nadoption of the latter closed-form solution allows a straightforward evolution\nof an initial random population of chromosomes, representing different damage\ndistributions along the beam axis, towards the fittest and selected as the\nsought solution. This method allows the identification of the position and\nintensity of an arbitrary number of cracks and is limited only by the amount of\ndata experimentally measured. The proposed procedure, which has the great\nadvantage of being robust and very fast, has been implemented in the powerful\nagent based software environment NetLogo, and is here presented and validated\nwith reference to several benchmark cases of single and multi-cracked beams\nconsidering different load scenarios and boundary conditions. Sensitivity\nanalyses to assess the influence of instrumental errors are also included in\nthe study.", 
    "link": "http://arxiv.org/pdf/1701.06432v1", 
    "arxiv-id": "1701.06432v1"
},{
    "category": "cs.CE", 
    "author": "Philippe Karamian-Surville", 
    "title": "On the choice of homogenization method to achieve effective mechanical   properties of composites reinforced by ellipsoidal and spherical particles", 
    "publish": "2017-01-31T17:06:29Z", 
    "summary": "In this paper, several rigorous numerical simulations were conducted to\nexamine the relevance of mean-field micromechanical models compared to the Fast\nFourier Transform full-field computation by considering spherical or\nellipsoidal inclusions. To be more general, the numerical study was extended to\na mixture of different kind of microstructures consisting of spheroidal shapes\nwithin the same RVE. Although the Fast Fourier Transform full field calculation\nis sensitive to high contrasts, calculation time, for a combination of complex\nmicrostructures, remains reasonable compared with those obtained with\nmean-field micromechanical models. Moreover, for low volume fractions of\ninclusions, the results of the mean-field approximations and those of the Fast\nFourier Transform-based (FFTb) full-field computation are very close, whatever\nthe inclusions morphology is. For RVEs consisting of ellipsoidal or a mixture\nof ellipsoidal and spherical inclusions, when the inclusions volume fraction\nbecomes higher, one observes that Lielens' model and the FFTb full-field\ncomputation give similar estimates. The accuracy of the computational methods\ndepends on the shape of the inclusions' and their volume fraction.", 
    "link": "http://arxiv.org/pdf/1701.09131v1", 
    "arxiv-id": "1701.09131v1"
},{
    "category": "cs.CE", 
    "author": "Patrizio Neff", 
    "title": "The exponentiated Hencky energy: Anisotropic extension and biomechanical   applications", 
    "publish": "2017-02-01T18:57:38Z", 
    "summary": "In this paper we propose an anisotropic extension of the isotropic\nexponentiated Hencky en- ergy, based on logarithmic strain invariants. Unlike\nother elastic formulations, the isotropic exponentiated Hencky elastic energy\nhas been derived solely on differential geometric grounds, involving the\ngeodesic distance of the deformation gradient F to the group of rotations. We\nformally extend this approach towards anisotropy by defining additional\nanisotropic logarith- mic strain invariants with the help of suitable\nstructural tensors and consider our findings for biomechanical applications.", 
    "link": "http://arxiv.org/pdf/1702.00394v1", 
    "arxiv-id": "1702.00394v1"
},{
    "category": "cs.CE", 
    "author": "Gianluca Cusatis", 
    "title": "Adaptive Multiscale Homogenization of the Lattice Discrete Particle   Model for the Analysis of Damage and Fracture in Concrete", 
    "publish": "2017-01-31T21:59:47Z", 
    "summary": "This paper presents a new adaptive multiscale homogenization scheme for the\nsimulation of damage and fracture in concrete structures. A two-scale\nhomogenization method, coupling meso-scale discrete particle models to macro-\nscale finite element models, is formulated into an adaptive framework. A\ncontinuum multiaxial failure criterion for concrete is calibrated on the basis\nof fine-scale simulations, and it serves as the adaptive criterion in the\nmultiscale framework. Thus, in this approach, simulations start without\nassigning any material Representative Volume Element (RVE) to the macro-scale\nfinite elements. The finite elements that meet the adaptive criterion and must\nbe entered into the multiscale homogenization framework are detected on the\nfly. This leads to a substantial reduction of the computational cost especially\nfor loading conditions leading to damage localization in which only a small\nportion of the FE mesh is enriched with the homogenized RVE. Several numerical\nsimulations are carried out to investigate the capability of the developed\nadaptive homogenization method. In addition, a detailed study on the\ncomputational cost is performed.", 
    "link": "http://arxiv.org/pdf/1702.00695v1", 
    "arxiv-id": "1702.00695v1"
},{
    "category": "cs.CE", 
    "author": "Claus Leth Bak", 
    "title": "Real-Time Voltage Stability Assessment for Load Areas Based on the   Holomorphic Embedding Method", 
    "publish": "2017-02-05T23:01:24Z", 
    "summary": "This paper proposes a method of real-time voltage stability assessment for\nload areas, in which the proximity to voltage collapse point at each bus can be\naccurately evaluated. Based on the non-iterative holomorphic embedding method\n(HEM), the voltage of each bus for different loading levels in the load area is\nquickly screened out by only performing one-time power flow calculation. A\npower series derived by the HEM with a physical germ solution makes sure that\nthe P-V curve is in conformity with that from conventional continuous power\nflow. Therefore, the voltage stability margin can be evaluated by plugging a\nvalue into the embedded complex variable of the power series. An adaptive\ntwo-stage Pade approximants method is also proposed based on the power series\nto improve its convergence. Thus, on one hand, it enables more accurate\nprediction of the voltage collapse point; on the other hand, reduces the\ncomputational burden on large power systems. The proposed method is illustrated\nin detail on a 4-bus test system and then demonstrated on a load area of\nNortheast Power Coordinating Council 48-geneartor, 140-bus power system.", 
    "link": "http://arxiv.org/pdf/1702.01464v1", 
    "arxiv-id": "1702.01464v1"
},{
    "category": "cs.CE", 
    "author": "Shiwen Mao", 
    "title": "TensorBeat: Tensor Decomposition for Monitoring Multi-Person Breathing   Beats with Commodity WiFi", 
    "publish": "2017-02-06T06:58:22Z", 
    "summary": "Breathing signal monitoring can provide important clues for human's physical\nhealth problems. Comparing to existing techniques that require wearable devices\nand special equipment, a more desirable approach is to provide contact-free and\nlong-term breathing rate monitoring by exploiting wireless signals. In this\npaper, we propose TensorBeat, a system to employ channel state information\n(CSI) phase difference data to intelligently estimate breathing rates for\nmultiple persons with commodity WiFi devices. The main idea is to leverage the\ntensor decomposition technique to handle the CSI phase difference data. The\nproposed TensorBeat scheme first obtains CSI phase difference data between\npairs of antennas at the WiFi receiver to create CSI tensor data. Then\nCanonical Polyadic (CP) decomposition is applied to obtain the desired\nbreathing signals. A stable signal matching algorithm is developed to find the\ndecomposed signal pairs, and a peak detection method is applied to estimate the\nbreathing rates for multiple persons. Our experimental study shows that\nTensorBeat can achieve high accuracy under different environments for\nmulti-person breathing rate monitoring.", 
    "link": "http://arxiv.org/pdf/1702.02046v1", 
    "arxiv-id": "1702.02046v1"
},{
    "category": "cs.CE", 
    "author": "Wolfgang Bangerth", 
    "title": "High Accuracy Mantle Convection Simulation through Modern Numerical   Methods. II: Realistic Models and Problems", 
    "publish": "2017-02-16T18:29:16Z", 
    "summary": "Computations have helped elucidate the dynamics of Earth's mantle for several\ndecades already. The numerical methods that underlie these simulations have\ngreatly evolved within this time span, and today include dynamically changing\nand adaptively refined meshes, sophisticated and efficient solvers, and\nparallelization to large clusters of computers. At the same time, many of these\nmethods -- discussed in detail in a previous paper in this series -- were\ndeveloped and tested primarily using model problems that lack many of the\ncomplexities that are common to the realistic models our community wants to\nsolve today.\n  With several years of experience solving complex and realistic models, we\nhere revisit some of the algorithm designs of the earlier paper and discuss the\nincorporation of more complex physics. In particular, we re-consider time\nstepping and mesh refinement algorithms, evaluate approaches to incorporate\ncompressibility, and discuss dealing with strongly varying material\ncoefficients, latent heat, and how to track chemical compositions and\nheterogeneities. Taken together and implemented in a high-performance,\nmassively parallel code, the techniques discussed in this paper then allow for\nhigh resolution, 3d, compressible, global mantle convection simulations with\nphase transitions, strongly temperature dependent viscosity and realistic\nmaterial properties based on mineral physics data.", 
    "link": "http://arxiv.org/pdf/1702.05075v1", 
    "arxiv-id": "1702.05075v1"
},{
    "category": "cs.CE", 
    "author": "Shengwei Mei", 
    "title": "Efficient Simulation of Temperature Evolution of Overhead Transmission   Lines Based on Analytical Solution and NWP", 
    "publish": "2017-02-23T16:45:31Z", 
    "summary": "Transmission lines are vital components in power systems. Tripping of\ntransmission lines caused by over-temperature is a major threat to the security\nof system operations, so it is necessary to efficiently simulate line\ntemperature under both normal operation conditions and foreseen fault\nconditions. Existing methods based on thermal-steady-state analyses cannot\nreflect transient temperature evolution, and thus cannot provide timing\ninformation needed for taking remedial actions. Moreover, conventional\nnumerical method requires huge computational efforts and barricades system-wide\nanalysis. In this regard, this paper derives an approximate analytical solution\nof transmission-line temperature evolution enabling efficient analysis on\nmultiple operation states. Considering the uncertainties in environmental\nparameters, the region of over-temperature is constructed in the environmental\nparameter space to realize the over-temperature risk assessment in both the\nplanning stage and real-time operations. A test on a typical conductor model\nverifies the accuracy of the approximate analytical solution. Based on the\nanalytical solution and numerical weather prediction (NWP) data, an efficient\nsimulation method for temperature evolution of transmission systems under\nmultiple operation states is proposed. As demonstrated on an NPCC 140-bus\nsystem, it achieves over 1000 times of efficiency enhancement, verifying its\npotentials in online risk assessment and decision support.", 
    "link": "http://arxiv.org/pdf/1702.07284v1", 
    "arxiv-id": "1702.07284v1"
},{
    "category": "cs.CE", 
    "author": "Serge Dos Santos", 
    "title": "Simulation of detecting contact nonlinearity in carbon fibre polymer   using ultrasonic nonlinear delayed time reversal", 
    "publish": "2017-02-21T15:09:00Z", 
    "summary": "A finite element method simulation of a carbon fibre reinforced polymer block\nis used to analyse the nonlinearities arising from a contacting delamination\ngap inside the material. The ultrasonic signal is amplified and nonlinearities\nare analysed by delayed Time Reversal -- Nonlinear Elastic Wave Spectroscopy\nsignal processing method. This signal processing method allows to focus the\nwave energy onto the receiving transducer and to modify the focused wave shape,\nallowing to use several different methods, including pulse inversion, for\ndetecting the nonlinear signature of the damage. It is found that the small\ncrack with contacting acoustic nonlinearity produces a noticeable nonlinear\nsignature when using pulse inversion signal processing, and even higher\nsignature with delayed time reversal, without requiring any baseline\ninformation from an undamaged medium.", 
    "link": "http://arxiv.org/pdf/1702.07320v1", 
    "arxiv-id": "1702.07320v1"
},{
    "category": "astro-ph", 
    "author": "R. Williams", 
    "title": "Virtual Observatory: From Concept to Implementation", 
    "publish": "2005-04-01T00:57:22Z", 
    "summary": "We review the origins of the Virtual Observatory (VO) concept, and the\ncurrent status of the efforts in this field. VO is the response of the\nastronomical community to the challenges posed by the modern massive and\ncomplex data sets. It is a framework in which information technology is\nharnessed to organize, maintain, and explore the rich information content of\nthe exponentially growing data sets, and to enable a qualitatively new science\nto be done with them. VO will become a complete, open, distributed, web-based\nframework for astronomy of the early 21st century. A number of significant\nefforts worldwide are now striving to convert this vision into reality. The\ntechnological and methodological challenges posed by the information-rich\nastronomy are also common to many other fields. We see a fundamental change in\nthe way all science is done, driven by the information technology revolution.", 
    "link": "http://arxiv.org/pdf/astro-ph/0504006v1", 
    "arxiv-id": "astro-ph/0504006v1"
},{
    "category": "cs.MS", 
    "author": "Eirik Fossgaard", 
    "title": "Fast Computational Algorithms for the Discrete Wavelet Transform and   Applications of Localized Orthonormal Bases in Signal Classification", 
    "publish": "1999-01-16T16:54:01Z", 
    "summary": "We construct an algorithm for implementing the discrete wavelet transform by\nmeans of matrices in SO_2(R) for orthonormal compactly supported wavelets and\nmatrices in SL_m(R), m > = 2, for compactly supported biorthogonal wavelets. We\nshow that in 1 dimension the total operation count using this algorithm can be\nreduced to about 50% of the conventional convolution and downsampling by\n2-operation for both orthonormal and biorthogonal filters. In the special case\nof biorthogonal symmetric odd-odd filters, we show an implementation yielding a\ntotal operation count of about 38% of the conventional method. In 2 dimensions\nwe show an implementation of this algorithm yielding a reduction in the total\noperation count of about 70% when the filters are orthonormal, a reduction of\nabout 62% for general biorthogonal filters, and a reduction of about 70% if the\nfilters are symmetric odd-odd length filters. We further extend these results\nto 3 dimensions. We also show how the SO_2(R)-method for implementing the\ndiscrete wavelet transform may be exploited to compute short FIR filters, and\nwe construct edge mappings where we try to improve upon the degree of\npreservation of regularity in the conventional methods. We also consider a\ntwo-class waveform discrimination problem. A statistical space-frequency\nanalysis is performed on a training data set using the LDB-algorithm of N.Saito\nand R.Coifman. The success of the algorithm on this particular problem is\nevaluated on a disjoint test data set.", 
    "link": "http://arxiv.org/pdf/cs/9901008v1", 
    "arxiv-id": "cs/9901008v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Zakharov", 
    "title": "Algorithms of Two-Level Parallelization for DSMC of Unsteady Flows in   Molecular Gasdynamics", 
    "publish": "1999-02-11T13:13:04Z", 
    "summary": "The general scheme of two-level parallelization (TLP) for direct simulation\nMonte Carlo of unsteady gas flows on shared memory multiprocessor computers has\nbeen described. The high efficient algorithm of parallel independent runs is\nused on the first level. The data parallelization is employed for the second\none. Two versions of TLP algorithm are elaborated with static and dynamic load\nbalancing. The method of dynamic processor reallocation is used for dynamic\nload balancing. Two gasdynamic unsteady problems were used to study speedup and\nefficiency of the algorithms. The conditions of efficient application field for\nthe algorithms have been determined.", 
    "link": "http://arxiv.org/pdf/cs/9902024v1", 
    "arxiv-id": "cs/9902024v1"
},{
    "category": "cs.CE", 
    "author": "T. Zhong", 
    "title": "The Structure of Weighting Coefficient Matrices of Harmonic Differential   Quadrature and Its Applications", 
    "publish": "1999-04-11T08:21:13Z", 
    "summary": "The structure of weighting coefficient matrices of Harmonic Differential\nQuadrature (HDQ) is found to be either centrosymmetric or skew centrosymmetric\ndepending on the order of the corresponding derivatives. The properties of both\nmatrices are briefly discussed in this paper. It is noted that the\ncomputational effort of the harmonic quadrature for some problems can be\nfurther reduced up to 75 per cent by using the properties of the\nabove-mentioned matrices.", 
    "link": "http://arxiv.org/pdf/cs/9904003v1", 
    "arxiv-id": "cs/9904003v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Jacobian matrix: a bridge between linear and nonlinear polynomial-only   problems", 
    "publish": "1999-04-15T12:31:21Z", 
    "summary": "By using the Hadamard matrix product concept, this paper introduces two\ngeneralized matrix formulation forms of numerical analogue of nonlinear\ndifferential operators. The SJT matrix-vector product approach is found to be a\nsimple, efficient and accurate technique in the calculation of the Jacobian\nmatrix of the nonlinear discretization by finite difference, finite volume,\ncollocation, dual reciprocity BEM or radial functions based numerical methods.\nWe also present and prove simple underlying relationship (theorem (3.1))\nbetween general nonlinear analogue polynomials and their corresponding Jacobian\nmatrices, which forms the basis of this paper. By means of theorem 3.1,\nstability analysis of numerical solutions of nonlinear initial value problems\ncan be easily handled based on the well-known results for linear problems.\nTheorem 3.1 also leads naturally to the straightforward extension of various\nlinear iterative algorithms such as the SOR, Gauss-Seidel and Jacobi methods to\nnonlinear algebraic equations. Since an exact alternative of the quasi-Newton\nequation is established via theorem 3.1, we derive a modified BFGS quasi-Newton\nmethod. A simple formula is also given to examine the deviation between the\napproximate and exact Jacobian matrices. Furthermore, in order to avoid the\nevaluation of the Jacobian matrix and its inverse, the pseudo-Jacobian matrix\nis introduced with a general applicability of any nonlinear systems of\nequations. It should be pointed out that a large class of real-world nonlinear\nproblems can be modeled or numerically discretized polynomial-only algebraic\nsystem of equations. The results presented here are in general applicable for\nall these problems. This paper can be considered as a starting point in the\nresearch of nonlinear computation and analysis from an innovative viewpoint.", 
    "link": "http://arxiv.org/pdf/cs/9904006v1", 
    "arxiv-id": "cs/9904006v1"
},{
    "category": "cs.CE", 
    "author": "Tingxiu Zhong", 
    "title": "The Study on the Nonlinear Computations of the DQ and DC Methods", 
    "publish": "1999-04-15T13:24:56Z", 
    "summary": "This paper points out that the differential quadrature (DQ) and differential\ncubature (DC) methods due to their global domain property are more efficient\nfor nonlinear problems than the traditional numerical techniques such as finite\nelement and finite difference methods. By introducing the Hadamard product of\nmatrices, we obtain an explicit matrix formulation for the DQ and DC solutions\nof nonlinear differential and integro-differential equations. Due to its\nsimplicity and flexibility, the present Hadamard product approach makes the DQ\nand DC methods much easier to be used. Many studies on the Hadamard product can\nbe fully exploited for the DQ and DC nonlinear computations. Furthermore, we\nfirst present SJT product of matrix and vector to compute accurately and\nefficiently the Frechet derivative matrix in the Newton-Raphson method for the\nsolution of the nonlinear formulations. We also propose a simple approach to\nsimplify the DQ or DC formulations for some nonlinear differential operators\nand thus the computational efficiency of these methods is improved\nsignificantly. We give the matrix multiplication formulas to compute\nefficiently the weighting coefficient matrices of the DC method. The spherical\nharmonics are suggested as the test functions in the DC method to handle the\nnonlinear differential equations occurring in global and hemispheric weather\nforecasting problems. Some examples are analyzed to demonstrate the simplicity\nand efficiency of the presented techniques. It is emphasized that innovations\npresented are applicable to the nonlinear computations of the other numerical\nmethods as well.", 
    "link": "http://arxiv.org/pdf/cs/9904007v1", 
    "arxiv-id": "cs/9904007v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Hadamard product nonlinear formulation of Galerkin and finite element   methods", 
    "publish": "1999-04-28T13:12:47Z", 
    "summary": "A novel nonlinear formulation of the finite element and Galerkin methods is\npresented here, which leads to the Hadamard product expression of the resultant\nnonlinear algebraic analogue. The presented formulation attains the advantages\nof weak formulation in the standard finite element and Galerkin schemes and\navoids the costly repeated numerical integration of the Jacobian matrix via the\nrecently developed SJT product approach. This also provides possibility of the\nnonlinear decoupling computations.", 
    "link": "http://arxiv.org/pdf/cs/9904021v1", 
    "arxiv-id": "cs/9904021v1"
},{
    "category": "cs.CE", 
    "author": "M. Chaves", 
    "title": "Programs with Stringent Performance Objectives Will Often Exhibit   Chaotic Behavior", 
    "publish": "1999-05-27T23:58:05Z", 
    "summary": "Software for the resolution of certain kind of problems, those that rate high\nin the Stringent Performance Objectives adjustment factor (IFPUG scheme), can\nbe described using a combination of game theory and autonomous systems. From\nthis description it can be shown that some of those problems exhibit chaotic\nbehavior, an important fact in understanding the functioning of the related\nsoftware. As a relatively simple example, it is shown that chess exhibits\nchaotic behavior in its configuration space. This implies that static\nevaluators in chess programs have intrinsic limitations.", 
    "link": "http://arxiv.org/pdf/cs/9905016v1", 
    "arxiv-id": "cs/9905016v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "A Newton method without evaluation of nonlinear function values", 
    "publish": "1999-06-09T12:27:03Z", 
    "summary": "The present author recently proposed and proved a relationship theorem\nbetween nonlinear polynomial equations and the corresponding Jacobian matrix.\nBy using this theorem, this paper derives a Newton iterative formula without\nrequiring the evaluation of nonlinear function values in the solution of\nnonlinear polynomial-only problems.", 
    "link": "http://arxiv.org/pdf/cs/9906011v1", 
    "arxiv-id": "cs/9906011v1"
},{
    "category": "cs.CE", 
    "author": "W. He", 
    "title": "The application of special matrix product to differential quadrature   solution of geometrically nonlinear bending of orthotropic rectangular plates", 
    "publish": "1999-06-09T12:47:13Z", 
    "summary": "The Hadamard and SJT product of matrices are two types of special matrix\nproduct. The latter was first defined by Chen. In this study, they are applied\nto the differential quadrature (DQ) solution of geometrically nonlinear bending\nof isotropic and orthotropic rectangular plates. By using the Hadamard product,\nthe nonlinear formulations are greatly simplified, while the SJT product\napproach minimizes the effort to evaluate the Jacobian derivative matrix in the\nNewton-Raphson method for solving the resultant nonlinear formulations. In\naddition, the coupled nonlinear formulations for the present problems can\neasily be decoupled by means of the Hadamard and SJT product. Therefore, the\nsize of the simultaneous nonlinear algebraic equations is reduced by two-thirds\nand the computing effort and storage requirements are alleviated greatly. Two\nrecent approaches applying the multiple boundary conditions are employed in the\npresent DQ nonlinear computations. The solution accuracies are improved\nobviously in comparison to the previously given by Bert et al. The numerical\nresults and detailed solution procedures are provided to demonstrate the superb\nefficiency, accuracy and simplicity of the new approaches in applying DQ method\nfor nonlinear computations.", 
    "link": "http://arxiv.org/pdf/cs/9906012v1", 
    "arxiv-id": "cs/9906012v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Generalized linearization in nonlinear modeling of data", 
    "publish": "1999-07-12T11:34:56Z", 
    "summary": "The principal innovative idea in this paper is to transform the original\ncomplex nonlinear modeling problem into a combination of linear problem and\nvery simple nonlinear problems. The key step is the generalized linearization\nof nonlinear terms. This paper only presents the introductory strategy of this\nmethodology. The practical numerical experiments will be provided subsequently.", 
    "link": "http://arxiv.org/pdf/cs/9907020v2", 
    "arxiv-id": "cs/9907020v2"
},{
    "category": "cs.CE", 
    "author": "Christoph Best", 
    "title": "A simple C++ library for manipulating scientific data sets as structured   data", 
    "publish": "1999-07-30T08:35:55Z", 
    "summary": "Representing scientific data sets efficiently on external storage usually\ninvolves converting them to a byte string representation using specialized\nreader/writer routines. The resulting storage files are frequently difficult to\ninterpret without these specialized routines as they do not contain information\nabout the logical structure of the data. Avoiding such problems usually\ninvolves heavy-weight data format libraries or data base systems. We present a\nsimple C++ library that allows to create and access data files that store\nstructured data. The structure of the data is described by a data type that can\nbe built from elementary data types (integer and floating-point numbers, byte\nstrings) and composite data types (arrays, structures, unions). An abstract\ndata access class presents the data to the application. Different actual data\nfile structures can be implemented under this layer. This method is\nparticularly suited to applications that require complex data structures, e.g.\nmolecular dynamics simulations. Extensions such as late type binding and object\npersistence are discussed.", 
    "link": "http://arxiv.org/pdf/cs/9907043v1", 
    "arxiv-id": "cs/9907043v1"
},{
    "category": "cs.MS", 
    "author": "Neil J. Gunther", 
    "title": "Seeing the Forest in the Tree: Applying VRML to Mathematical Problems in   Number Theory", 
    "publish": "1999-12-31T18:36:38Z", 
    "summary": "We show how VRML (Virtual Reality Modeling Language) can provide potentially\npowerful insight into the 3x + 1 problem via the introduction of a unique\ngeometrical object, called the 'G-cell', akin to a fractal generator. We\npresent an example of a VRML world developed programmatically with the G-cell.\nThe role of VRML as a tool for furthering the understanding the 3x+1 problem is\npotentially significant for several reasons: a) VRML permits the observer to\nzoom into the geometric structure at all scales (up to limitations of the\ncomputing platform). b) VRML enables rotation to alter comparative visual\nperspective (similar to Tukey's data-spinning concept). c) VRML facilitates the\ndemonstration of interesting tree features between collaborators on the\ninternet who might otherwise have difficulty conveying their ideas\nunambiguously. d) VRML promises to reveal any dimensional dependencies among\n3x+1 sequences.", 
    "link": "http://arxiv.org/pdf/cs/9912021v2", 
    "arxiv-id": "cs/9912021v2"
},{
    "category": "cs.MS", 
    "author": "Lester Ingber", 
    "title": "Adaptive simulated annealing (ASA): Lessons learned", 
    "publish": "2000-01-23T20:36:49Z", 
    "summary": "Adaptive simulated annealing (ASA) is a global optimization algorithm based\non an associated proof that the parameter space can be sampled much more\nefficiently than by using other previous simulated annealing algorithms. The\nauthor's ASA code has been publicly available for over two years. During this\ntime the author has volunteered to help people via e-mail, and the feedback\nobtained has been used to further develop the code. Some lessons learned, in\nparticular some which are relevant to other simulated annealing algorithms, are\ndescribed.", 
    "link": "http://arxiv.org/pdf/cs/0001018v1", 
    "arxiv-id": "cs/0001018v1"
},{
    "category": "cs.CE", 
    "author": "Lusheng Wang", 
    "title": "On The Closest String and Substring Problems", 
    "publish": "2000-02-17T23:06:06Z", 
    "summary": "The problem of finding a center string that is `close' to every given string\narises and has many applications in computational biology and coding theory.\nThis problem has two versions: the Closest String problem and the Closest\nSubstring problem. Assume that we are given a set of strings ${\\cal S}=\\{s_1,\ns_2, ..., s_n\\}$ of strings, say, each of length $m$. The Closest String\nproblem asks for the smallest $d$ and a string $s$ of length $m$ which is\nwithin Hamming distance $d$ to each $s_i\\in {\\cal S}$. This problem comes from\ncoding theory when we are looking for a code not too far away from a given set\nof codes. The problem is NP-hard. Berman et al give a polynomial time algorithm\nfor constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient\napproximation algorithm using linear program relaxation technique. The best\npolynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al\nand Gasieniec et al. The Closest Substring problem looks for a string $t$ which\nis within Hamming distance $d$ away from a substring of each $s_i$. This\nproblem only has a $2- \\frac{2}{2|\\Sigma|+1}$ approximation algorithm\npreviously Lanctot et al and is much more elusive than the Closest String\nproblem, but it has many applications in finding conserved regions, genetic\ndrug target identification, and genetic probes in molecular biology. Whether\nthere are efficient approximation algorithms for both problems are major open\nquestions in this area. We present two polynomial time approxmation algorithms\nwith approximation ratio $1+ \\epsilon$ for any small $\\epsilon$ to settle both\nquestions.", 
    "link": "http://arxiv.org/pdf/cs/0002012v1", 
    "arxiv-id": "cs/0002012v1"
},{
    "category": "cs.CE", 
    "author": "J. A. Dente", 
    "title": "Fault Detection using Immune-Based Systems and Formal Language   Algorithms", 
    "publish": "2000-10-03T17:54:38Z", 
    "summary": "This paper describes two approaches for fault detection: an immune-based\nmechanism and a formal language algorithm. The first one is based on the\nfeature of immune systems in distinguish any foreign cell from the body own\ncell. The formal language approach assumes the system as a linguistic source\ncapable of generating a certain language, characterised by a grammar. Each\nalgorithm has particular characteristics, which are analysed in the paper,\nnamely in what cases they can be used with advantage. To test their\npracticality, both approaches were applied on the problem of fault detection in\nan induction motor.", 
    "link": "http://arxiv.org/pdf/cs/0010010v1", 
    "arxiv-id": "cs/0010010v1"
},{
    "category": "cs.CE", 
    "author": "Alok Kumar", 
    "title": "Towards Understanding the Predictability of Stock Markets from the   Perspective of Computational Complexity", 
    "publish": "2000-10-14T14:01:17Z", 
    "summary": "This paper initiates a study into the century-old issue of market\npredictability from the perspective of computational complexity. We develop a\nsimple agent-based model for a stock market where the agents are traders\nequipped with simple trading strategies, and their trades together determine\nthe stock prices. Computer simulations show that a basic case of this model is\nalready capable of generating price graphs which are visually similar to the\nrecent price movements of high tech stocks. In the general model, we prove that\nif there are a large number of traders but they employ a relatively small\nnumber of strategies, then there is a polynomial-time algorithm for predicting\nfuture price movements with high accuracy. On the other hand, if the number of\nstrategies is large, market prediction becomes complete in two new\ncomputational complexity classes CPP and BCPP, which are between P^NP[O(log n)]\nand PP. These computational completeness results open up a novel possibility\nthat the price graph of an actual stock could be sufficiently deterministic for\nvarious prediction goals but appear random to all polynomial-time prediction\nalgorithms.", 
    "link": "http://arxiv.org/pdf/cs/0010021v2", 
    "arxiv-id": "cs/0010021v2"
},{
    "category": "cs.CE", 
    "author": "Ming-Yang Kao", 
    "title": "Opportunity Cost Algorithms for Combinatorial Auctions", 
    "publish": "2000-10-24T17:14:01Z", 
    "summary": "Two general algorithms based on opportunity costs are given for approximating\na revenue-maximizing set of bids an auctioneer should accept, in a\ncombinatorial auction in which each bidder offers a price for some subset of\nthe available goods and the auctioneer can only accept non-intersecting bids.\nSince this problem is difficult even to approximate in general, the algorithms\nare most useful when the bids are restricted to be connected node subsets of an\nunderlying object graph that represents which objects are relevant to each\nother. The approximation ratios of the algorithms depend on structural\nproperties of this graph and are small constants for many interesting families\nof object graphs. The running times of the algorithms are linear in the size of\nthe bid graph, which describes the conflicts between bids. Extensions of the\nalgorithms allow for efficient processing of additional constraints, such as\nbudget constraints that associate bids with particular bidders and limit how\nmany bids from a particular bidder can be accepted.", 
    "link": "http://arxiv.org/pdf/cs/0010031v1", 
    "arxiv-id": "cs/0010031v1"
},{
    "category": "cs.CE", 
    "author": "Stephen R. Tate", 
    "title": "Designing Proxies for Stock Market Indices is Computationally Hard", 
    "publish": "2000-11-13T02:51:49Z", 
    "summary": "In this paper, we study the problem of designing proxies (or portfolios) for\nvarious stock market indices based on historical data. We use four different\nmethods for computing market indices, all of which are formulas used in actual\nstock market analysis. For each index, we consider three criteria for designing\nthe proxy: the proxy must either track the market index, outperform the market\nindex, or perform within a margin of error of the index while maintaining a low\nvolatility. In eleven of the twelve cases (all combinations of four indices\nwith three criteria except the problem of sacrificing return for less\nvolatility using the price-relative index) we show that the problem is NP-hard,\nand hence most likely intractable.", 
    "link": "http://arxiv.org/pdf/cs/0011016v1", 
    "arxiv-id": "cs/0011016v1"
},{
    "category": "cs.CE", 
    "author": "Hsing-Kuo Wong", 
    "title": "Optimal Buy-and-Hold Strategies for Financial Markets with Bounded Daily   Returns", 
    "publish": "2000-11-14T16:04:27Z", 
    "summary": "In the context of investment analysis, we formulate an abstract online\ncomputing problem called a planning game and develop general tools for solving\nsuch a game. We then use the tools to investigate a practical buy-and-hold\ntrading problem faced by long-term investors in stocks. We obtain the unique\noptimal static online algorithm for the problem and determine its exact\ncompetitive ratio. We also compare this algorithm with the popular dollar\naveraging strategy using actual market data.", 
    "link": "http://arxiv.org/pdf/cs/0011018v1", 
    "arxiv-id": "cs/0011018v1"
},{
    "category": "cs.CE", 
    "author": "Lei Tan", 
    "title": "Optimal Bidding Algorithms Against Cheating in Multiple-Object Auctions", 
    "publish": "2000-11-17T01:55:22Z", 
    "summary": "This paper studies some basic problems in a multiple-object auction model\nusing methodologies from theoretical computer science. We are especially\nconcerned with situations where an adversary bidder knows the bidding\nalgorithms of all the other bidders. In the two-bidder case, we derive an\noptimal randomized bidding algorithm, by which the disadvantaged bidder can\nprocure at least half of the auction objects despite the adversary's a priori\nknowledge of his algorithm. In the general $k$-bidder case, if the number of\nobjects is a multiple of $k$, an optimal randomized bidding algorithm is found.\nIf the $k-1$ disadvantaged bidders employ that same algorithm, each of them can\nobtain at least $1/k$ of the objects regardless of the bidding algorithm the\nadversary uses. These two algorithms are based on closed-form solutions to\ncertain multivariate probability distributions. In situations where a\nclosed-form solution cannot be obtained, we study a restricted class of bidding\nalgorithms as an approximation to desired optimal algorithms.", 
    "link": "http://arxiv.org/pdf/cs/0011023v1", 
    "arxiv-id": "cs/0011023v1"
},{
    "category": "cs.CE", 
    "author": "George M. Church", 
    "title": "A Dynamic Programming Approach to De Novo Peptide Sequencing via Tandem   Mass Spectrometry", 
    "publish": "2001-01-18T03:10:58Z", 
    "summary": "The tandem mass spectrometry fragments a large number of molecules of the\nsame peptide sequence into charged prefix and suffix subsequences, and then\nmeasures mass/charge ratios of these ions. The de novo peptide sequencing\nproblem is to reconstruct the peptide sequence from a given tandem mass\nspectral data of k ions. By implicitly transforming the spectral data into an\nNC-spectrum graph G=(V,E) where |V|=2k+2, we can solve this problem in\nO(|V|+|E|) time and O(|V|) space using dynamic programming. Our approach can be\nfurther used to discover a modified amino acid in O(|V||E|) time and to analyze\ndata with other types of noise in O(|V||E|) time. Our algorithms have been\nimplemented and tested on actual experimental data.", 
    "link": "http://arxiv.org/pdf/cs/0101016v1", 
    "arxiv-id": "cs/0101016v1"
},{
    "category": "cs.CE", 
    "author": "Ming-Yang Kao", 
    "title": "Tree Contractions and Evolutionary Trees", 
    "publish": "2001-01-26T21:36:30Z", 
    "summary": "An evolutionary tree is a rooted tree where each internal vertex has at least\ntwo children and where the leaves are labeled with distinct symbols\nrepresenting species. Evolutionary trees are useful for modeling the\nevolutionary history of species. An agreement subtree of two evolutionary trees\nis an evolutionary tree which is also a topological subtree of the two given\ntrees. We give an algorithm to determine the largest possible number of leaves\nin any agreement subtree of two trees T_1 and T_2 with n leaves each. If the\nmaximum degree d of these trees is bounded by a constant, the time complexity\nis O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this\nalgorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d\nsqrt(d) log^3(n)) time.", 
    "link": "http://arxiv.org/pdf/cs/0101030v1", 
    "arxiv-id": "cs/0101030v1"
},{
    "category": "cs.CE", 
    "author": "Hing-Fung Ting", 
    "title": "Cavity Matchings, Label Compressions, and Unrooted Evolutionary Trees", 
    "publish": "2001-01-26T23:59:55Z", 
    "summary": "We present an algorithm for computing a maximum agreement subtree of two\nunrooted evolutionary trees. It takes O(n^{1.5} log n) time for trees with\nunbounded degrees, matching the best known time complexity for the rooted case.\nOur algorithm allows the input trees to be mixed trees, i.e., trees that may\ncontain directed and undirected edges at the same time. Our algorithm adopts a\nrecursive strategy exploiting a technique called label compression. The\nbackbone of this technique is an algorithm that computes the maximum weight\nmatchings over many subgraphs of a bipartite graph as fast as it takes to\ncompute a single matching.", 
    "link": "http://arxiv.org/pdf/cs/0101031v2", 
    "arxiv-id": "cs/0101031v2"
},{
    "category": "cs.CE", 
    "author": "Andrea Schaerf", 
    "title": "Local Search Techniques for Constrained Portfolio Selection Problems", 
    "publish": "2001-04-18T13:42:49Z", 
    "summary": "We consider the problem of selecting a portfolio of assets that provides the\ninvestor a suitable balance of expected return and risk. With respect to the\nseminal mean-variance model of Markowitz, we consider additional constraints on\nthe cardinality of the portfolio and on the quantity of individual shares. Such\nconstraints better capture the real-world trading system, but make the problem\nmore difficult to be solved with exact methods. We explore the use of local\nsearch techniques, mainly tabu search, for the portfolio selection problem. We\ncompare and combine previous work on portfolio selection that makes use of the\nlocal search approach and we propose new algorithms that combine different\nneighborhood relations. In addition, we show how the use of randomization and\nof a simple form of adaptiveness simplifies the setting of a large number of\ncritical parameters. Finally, we show how our techniques perform on public\nbenchmarks.", 
    "link": "http://arxiv.org/pdf/cs/0104017v1", 
    "arxiv-id": "cs/0104017v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "Several new domain-type and boundary-type numerical discretization   schemes with radial basis function", 
    "publish": "2001-04-23T18:50:45Z", 
    "summary": "This paper is concerned with a few novel RBF-based numerical schemes\ndiscretizing partial differential equations. For boundary-type methods, we\nderive the indirect and direct symmetric boundary knot methods (BKM). The\nresulting interpolation matrix of both is always symmetric irrespective of\nboundary geometry and conditions. In particular, the direct BKM applies the\npractical physical variables rather than expansion coefficients and becomes\nvery competitive to the boundary element method. On the other hand, based on\nthe multiple reciprocity principle, we invent the RBF-based boundary particle\nmethod (BPM) for general inhomogeneous problems without a need using inner\nnodes. The direct and symmetric BPM schemes are also developed.\n  For domain-type RBF discretization schemes, by using the Green integral we\ndevelop a new Hermite RBF scheme called as the modified Kansa method (MKM),\nwhich differs from the symmetric Hermite RBF scheme in that the MKM discretizes\nboth governing equation and boundary conditions on the same boundary nodes. The\nlocal spline version of the MKM is named as the finite knot method (FKM). Both\nMKM and FKM significantly reduce calculation errors at nodes adjacent to\nboundary. In addition, the nonsingular high-order fundamental or general\nsolution is strongly recommended as the RBF in the domain-type methods and dual\nreciprocity method approximation of particular solution relating to the BKM.\n  It is stressed that all the above discretization methods of boundary-type and\ndomain-type are symmetric, meshless, and integration-free. The spline-based\nschemes will produce desirable symmetric sparse banded interpolation matrix. In\nappendix, we present a Hermite scheme to eliminate edge effect on the RBF\ngeometric modeling and imaging.", 
    "link": "http://arxiv.org/pdf/cs/0104018v1", 
    "arxiv-id": "cs/0104018v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "Errata and supplements to: Orthonormal RBF Wavelet and Ridgelet-like   Series and Transforms for High-Dimensional Problems", 
    "publish": "2001-05-07T16:53:05Z", 
    "summary": "In recent years some attempts have been done to relate the RBF with wavelets\nin handling high dimensional multiscale problems. To the author's knowledge,\nhowever, the orthonormal and bi-orthogonal RBF wavelets are still missing in\nthe literature. By using the nonsingular general solution and singular\nfundamental solution of differential operator, recently the present author,\nrefer. 3, made some substantial headway to derive the orthonormal RBF wavelets\nseries and transforms. The methodology can be generalized to create the RBF\nwavelets by means of the orthogonal convolution kernel function of various\nintegral operators. In particular, it is stressed that the presented RBF\nwavelets does not apply the tensor product to handle multivariate problems at\nall.\n  This note is to correct some errata in reference 3 and also to supply a few\nlatest advances in the study of orthornormal RBF wavelet transforms.", 
    "link": "http://arxiv.org/pdf/cs/0105014v1", 
    "arxiv-id": "cs/0105014v1"
},{
    "category": "cs.CE", 
    "author": "W. He", 
    "title": "A note on radial basis function computing", 
    "publish": "2001-06-03T11:41:56Z", 
    "summary": "This note carries three purposes involving our latest advances on the radial\nbasis function (RBF) approach. First, we will introduce a new scheme employing\nthe boundary knot method (BKM) to nonlinear convection-diffusion problem. It is\nstressed that the new scheme directly results in a linear BKM formulation of\nnonlinear problems by using response point-dependent RBFs, which can be solved\nby any linear solver. Then we only need to solve a single nonlinear algebraic\nequation for desirable unknown at some inner node of interest. The numerical\nresults demonstrate high accuracy and efficiency of this nonlinear BKM\nstrategy. Second, we extend the concepts of distance function, which include\ntime-space and variable transformation distance functions. Finally, we\ndemonstrate that if the nodes are symmetrically placed, the RBF coefficient\nmatrices have either centrosymmetric or skew centrosymmetric structures. The\nfactorization features of such matrices lead to a considerable reduction in the\nRBF computing effort. A simple approach is also presented to reduce the\nill-conditioning of RBF interpolation matrices in general cases.", 
    "link": "http://arxiv.org/pdf/cs/0106003v1", 
    "arxiv-id": "cs/0106003v1"
},{
    "category": "cs.NI", 
    "author": "Lars Rasmusson", 
    "title": "Pricing Virtual Paths with Quality-of-Service Guarantees as Bundle   Derivatives", 
    "publish": "2001-06-12T12:06:47Z", 
    "summary": "We describe a model of a communication network that allows us to price\ncomplex network services as financial derivative contracts based on the spot\nprice of the capacity in individual routers. We prove a theorem of a Girsanov\ntransform that is useful for pricing linear derivatives on underlying assets,\nwhich can be used to price many complex network services, and it is used to\nprice an option that gives access to one of several virtual channels between\ntwo network nodes, during a specified future time interval. We give the\ncontinuous time hedging strategy, for which the option price is independent of\nthe service providers attitude towards risk. The option price contains the\ndensity function of a sum of lognormal variables, which has to be evaluated\nnumerically.", 
    "link": "http://arxiv.org/pdf/cs/0106028v1", 
    "arxiv-id": "cs/0106028v1"
},{
    "category": "cs.CE", 
    "author": "Damon Liu", 
    "title": "Multivariant Branching Prediction, Reflection, and Retrospection", 
    "publish": "2001-10-23T22:04:24Z", 
    "summary": "In branching simulation, a novel approach to simulation presented in this\npaper, a multiplicity of plausible scenarios are concurrently developed and\nimplemented. In conventional simulations of complex systems, there arise from\ntime to time uncertainties as to which of two or more alternatives are more\nlikely to be pursued by the system being simulated. Under these conditions the\nsimulationist makes a judicious choice of one of these alternatives and embeds\nthis choice in the simulation model. By contrast, in the branching approach,\ntwo or more of such alternatives (or branches) are included in the model and\nimplemented for concurrent computer solution. The theoretical foundations for\nbranching simulation as a computational process are in the domains of\nalternating Turing machines, molecular computing, and E-machines. Branching\nsimulations constitute the development of diagrams of scenarios representing\nsignificant, alternative flows of events. Logical means for interpretation and\ninvestigation of the branching simulation and prediction are provided by the\nlogical theories of possible worlds, which have been formalized by the\nconstruction of logical varieties. Under certain conditions, the branching\napproach can considerably enhance the efficiency of computer simulations and\nprovide more complete insights into the interpretation of predictions based on\nsimulations. As an example, the concepts developed in this paper have been\napplied to a simulation task that plays an important role in radiology - the\nnoninvasive treatment of brain aneurysms.", 
    "link": "http://arxiv.org/pdf/cs/0110048v1", 
    "arxiv-id": "cs/0110048v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "Analytical solution of transient scalar wave and diffusion problems of   arbitrary dimensionality and geometry by RBF wavelet series", 
    "publish": "2001-10-28T10:28:55Z", 
    "summary": "This study applies the RBF wavelet series to the evaluation of analytical\nsolutions of linear time-dependent wave and diffusion problems of any\ndimensionality and geometry. To the best of the author's knowledge, such\nanalytical solutions have never been achieved before. The RBF wavelets can be\nunderstood an alternative for multidimensional problems to the standard Fourier\nseries via fundamental and general solutions of partial differential equation.\nThe present RBF wavelets are infinitely differential, compactly supported,\northogonal over different scales and very simple. The rigorous mathematical\nproof of completeness and convergence is still missing in this study. The\npresent work may open a new window to numerical solution and theoretical\nanalysis of many other high-dimensional time-dependent PDE problems under\narbitrary geometry.", 
    "link": "http://arxiv.org/pdf/cs/0110055v1", 
    "arxiv-id": "cs/0110055v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "New RBF collocation methods and kernel RBF with applications", 
    "publish": "2001-11-30T16:03:00Z", 
    "summary": "A few novel radial basis function (RBF) discretization schemes for partial\ndifferential equations are developed in this study. For boundary-type methods,\nwe derive the indirect and direct symmetric boundary knot methods. Based on the\nmultiple reciprocity principle, the boundary particle method is introduced for\ngeneral inhomogeneous problems without using inner nodes. For domain-type\nschemes, by using the Green integral we develop a novel Hermite RBF scheme\ncalled the modified Kansa method, which significantly reduces calculation\nerrors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the\nleast square RBF collocation scheme. Finally, five types of the kernel RBF are\nalso briefly presented.", 
    "link": "http://arxiv.org/pdf/cs/0111063v1", 
    "arxiv-id": "cs/0111063v1"
},{
    "category": "cs.CC", 
    "author": "Vijay Ramachandran", 
    "title": "DNA Self-Assembly For Constructing 3D Boxes", 
    "publish": "2001-12-08T21:36:28Z", 
    "summary": "We propose a mathematical model of DNA self-assembly using 2D tiles to form\n3D nanostructures. This is the first work to combine studies in self-assembly\nand nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Our\nmodel is a more precise superset of their Tile Assembly Model that facilitates\nbuilding scalable 3D molecules. Under our model, we present algorithms to build\na hollow cube, which is intuitively one of the simplest 3D structures to\nconstruct. We also introduce five basic measures of complexity to analyze these\nalgorithms. Our model and algorithmic techniques are applicable to more complex\n2D and 3D nanostructures.", 
    "link": "http://arxiv.org/pdf/cs/0112009v1", 
    "arxiv-id": "cs/0112009v1"
},{
    "category": "cs.AI", 
    "author": "J. P. Kropp", 
    "title": "A Qualitative Dynamical Modelling Approach to Capital Accumulation in   Unregulated Fisheries", 
    "publish": "2002-02-05T17:50:56Z", 
    "summary": "Capital accumulation has been a major issue in fisheries economics over the\nlast two decades, whereby the interaction of the fish and capital stocks were\nof particular interest. Because bio-economic systems are intrinsically complex,\nprevious efforts in this field have relied on a variety of simplifying\nassumptions. The model presented here relaxes some of these simplifications.\nProblems of tractability are surmounted by using the methodology of qualitative\ndifferential equations (QDE). The theory of QDEs takes into account that\nscientific knowledge about particular fisheries is usually limited, and\nfacilitates an analysis of the global dynamics of systems with more than two\nordinary differential equations. The model is able to trace the evolution of\ncapital and fish stock in good agreement with observed patterns, and shows that\nover-capitalization is unavoidable in unregulated fisheries.", 
    "link": "http://arxiv.org/pdf/cs/0202004v3", 
    "arxiv-id": "cs/0202004v3"
},{
    "category": "cs.CE", 
    "author": "Theodore S. Rappaport", 
    "title": "BSML: A Binding Schema Markup Language for Data Interchange in Problem   Solving Environments (PSEs)", 
    "publish": "2002-02-18T16:01:03Z", 
    "summary": "We describe a binding schema markup language (BSML) for describing data\ninterchange between scientific codes. Such a facility is an important\nconstituent of scientific problem solving environments (PSEs). BSML is designed\nto integrate with a PSE or application composition system that views model\nspecification and execution as a problem of managing semistructured data. The\ndata interchange problem is addressed by three techniques for processing\nsemistructured data: validation, binding, and conversion. We present BSML and\ndescribe its application to a PSE for wireless communications system design.", 
    "link": "http://arxiv.org/pdf/cs/0202027v1", 
    "arxiv-id": "cs/0202027v1"
},{
    "category": "cs.CE", 
    "author": "Ming-Yang Kao", 
    "title": "Fast Universalization of Investment Strategies with Provably Good   Relative Returns", 
    "publish": "2002-04-10T03:13:03Z", 
    "summary": "A universalization of a parameterized investment strategy is an online\nalgorithm whose average daily performance approaches that of the strategy\noperating with the optimal parameters determined offline in hindsight. We\npresent a general framework for universalizing investment strategies and\ndiscuss conditions under which investment strategies are universalizable. We\npresent examples of common investment strategies that fit into our framework.\nThe examples include both trading strategies that decide positions in\nindividual stocks, and portfolio strategies that allocate wealth among multiple\nstocks. This work extends Cover's universal portfolio work. We also discuss the\nruntime efficiency of universalization algorithms. While a straightforward\nimplementation of our algorithms runs in time exponential in the number of\nparameters, we show that the efficient universal portfolio computation\ntechnique of Kalai and Vempala involving the sampling of log-concave functions\ncan be generalized to other classes of investment strategies.", 
    "link": "http://arxiv.org/pdf/cs/0204019v1", 
    "arxiv-id": "cs/0204019v1"
},{
    "category": "cs.CE", 
    "author": "Chris Bailey-Kellogg", 
    "title": "Sampling Strategies for Mining in Data-Scarce Domains", 
    "publish": "2002-04-22T19:41:24Z", 
    "summary": "Data mining has traditionally focused on the task of drawing inferences from\nlarge datasets. However, many scientific and engineering domains, such as fluid\ndynamics and aircraft design, are characterized by scarce data, due to the\nexpense and complexity of associated experiments and simulations. In such\ndata-scarce domains, it is advantageous to focus the data collection effort on\nonly those regions deemed most important to support a particular data mining\nobjective. This paper describes a mechanism that interleaves bottom-up data\nmining, to uncover multi-level structures in spatial data, with top-down\nsampling, to clarify difficult decisions in the mining process. The mechanism\nexploits relevant physical properties, such as continuity, correspondence, and\nlocality, in a unified framework. This leads to effective mining and sampling\ndecisions that are explainable in terms of domain knowledge and data\ncharacteristics. This approach is demonstrated in two diverse applications --\nmining pockets in spatial data, and qualitative determination of Jordan forms\nof matrices.", 
    "link": "http://arxiv.org/pdf/cs/0204047v2", 
    "arxiv-id": "cs/0204047v2"
},{
    "category": "cs.AI", 
    "author": "Naren Ramakrishnan", 
    "title": "Qualitative Analysis of Correspondence for Experimental Algorithmics", 
    "publish": "2002-04-26T17:25:51Z", 
    "summary": "Correspondence identifies relationships among objects via similarities among\ntheir components; it is ubiquitous in the analysis of spatial datasets,\nincluding images, weather maps, and computational simulations. This paper\ndevelops a novel multi-level mechanism for qualitative analysis of\ncorrespondence. Operators leverage domain knowledge to establish\ncorrespondence, evaluate implications for model selection, and leverage\nidentified weaknesses to focus additional data collection. The utility of the\nmechanism is demonstrated in two applications from experimental algorithmics --\nmatrix spectral portrait analysis and graphical assessment of Jordan forms of\nmatrices. Results show that the mechanism efficiently samples computational\nexperiments and successfully uncovers high-level problem properties. It\novercomes noise and data sparsity by leveraging domain knowledge to detect\nmutually reinforcing interpretations of spatial data.", 
    "link": "http://arxiv.org/pdf/cs/0204053v1", 
    "arxiv-id": "cs/0204053v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Distance function wavelets - Part I: Helmholtz and convection-diffusion   transforms and series", 
    "publish": "2002-05-14T13:43:47Z", 
    "summary": "This report aims to present my research updates on distance function wavelets\n(DFW) based on the fundamental solutions and the general solutions of the\nHelmholtz, modified Helmholtz, and convection-diffusion equations, which\ninclude the isotropic Helmholtz-Fourier (HF) transform and series, the\nHelmholtz-Laplace (HL) transform, and the anisotropic convection-diffusion\nwavelets and ridgelets. The latter is set to handle discontinuous and track\ndata problems. The edge effect of the HF series is addressed. Alternative\nexistence conditions for the DFW transforms are proposed and discussed. To\nsimplify and streamline the expression of the HF and HL transforms, a new\ndimension-dependent function notation is introduced. The HF series is also used\nto evaluate the analytical solutions of linear diffusion problems of arbitrary\ndimensionality and geometry. The weakness of this report is lacking of rigorous\nmathematical analysis due to the author's limited mathematical knowledge.", 
    "link": "http://arxiv.org/pdf/cs/0205019v1", 
    "arxiv-id": "cs/0205019v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "A quasi-RBF technique for numerical discretization of PDE's", 
    "publish": "2002-05-14T13:58:48Z", 
    "summary": "Atkinson developed a strategy which splits solution of a PDE system into\nhomogeneous and particular solutions, where the former have to satisfy the\nboundary and governing equation, while the latter only need to satisfy the\ngoverning equation without concerning geometry. Since the particular solution\ncan be solved irrespective of boundary shape, we can use a readily available\nfast Fourier or orthogonal polynomial technique O(NlogN) to evaluate it in a\nregular box or sphere surrounding physical domain. The distinction of this\nstudy is that we approximate homogeneous solution with nonsingular general\nsolution RBF as in the boundary knot method. The collocation method using\ngeneral solution RBF has very high accuracy and spectral convergent speed and\nis a simple, truly meshfree approach for any complicated geometry. More\nimportantly, the use of nonsingular general solution avoids the controversial\nartificial boundary in the method of fundamental solution due to the\nsingularity of fundamental solution.", 
    "link": "http://arxiv.org/pdf/cs/0205020v1", 
    "arxiv-id": "cs/0205020v1"
},{
    "category": "cs.DS", 
    "author": "Neal Young", 
    "title": "Data-Collection for the Sloan Digital Sky Survey: a Network-Flow   Heuristic", 
    "publish": "2002-05-18T03:29:33Z", 
    "summary": "The goal of the Sloan Digital Sky Survey is ``to map in detail one-quarter of\nthe entire sky, determining the positions and absolute brightnesses of more\nthan 100 million celestial objects''. The survey will be performed by taking\n``snapshots'' through a large telescope. Each snapshot can capture up to 600\nobjects from a small circle of the sky. This paper describes the design and\nimplementation of the algorithm that is being used to determine the snapshots\nso as to minimize their number. The problem is NP-hard in general; the\nalgorithm described is a heuristic, based on Lagriangian-relaxation and\nmin-cost network flow. It gets within 5-15% of a naive lower bound, whereas\nusing a ``uniform'' cover only gets within 25-35%.", 
    "link": "http://arxiv.org/pdf/cs/0205034v1", 
    "arxiv-id": "cs/0205034v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Distance function wavelets - Part II: Extended results and conjectures", 
    "publish": "2002-05-24T12:07:28Z", 
    "summary": "Report II is concerned with the extended results of distance function\nwavelets (DFW). The fractional DFW transforms are first addressed relating to\nthe fractal geometry and fractional derivative, and then, the discrete\nHelmholtz-Fourier transform is briefly presented. The Green second identity may\nbe an alternative devise in developing the theoretical framework of the DFW\ntransform and series. The kernel solutions of the Winkler plate equation and\nthe Burger's equation are used to create the DFW transforms and series. Most\ninterestingly, it is found that the translation invariant monomial solutions of\nthe high-order Laplace equations can be used to make very simple harmonic\npolynomial DFW series. In most cases of this study, solid mathematical analysis\nis missing and results are obtained intuitively in the conjecture status.", 
    "link": "http://arxiv.org/pdf/cs/0205063v1", 
    "arxiv-id": "cs/0205063v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "High-order fundamental and general solutions of convection-diffusion   equation and their applications with boundary particle method", 
    "publish": "2002-06-08T10:46:56Z", 
    "summary": "In this study, we presented the high-order fundamental solutions and general\nsolutions of convection-diffusion equation. To demonstrate their efficacy, we\napplied the high-order general solutions to the boundary particle method (BPM)\nfor the solution of some inhomogeneous convection-diffusion problems, where the\nBPM is a new truly boundary-only meshfree collocation method based on multiple\nreciprocity principle. For the sake of completeness, the BPM is also briefly\ndescribed here.", 
    "link": "http://arxiv.org/pdf/cs/0206013v1", 
    "arxiv-id": "cs/0206013v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Distance function wavelets - Part III: \"Exotic\" transforms and series", 
    "publish": "2002-06-10T09:01:53Z", 
    "summary": "Part III of the reports consists of various unconventional distance function\nwavelets (DFW). The dimension and the order of partial differential equation\n(PDE) are first used as a substitute of the scale parameter in the DFW\ntransforms and series, especially with the space and time-space potential\nproblems. It is noted that the recursive multiple reciprocity formulation is\nthe DFW series. The Green second identity is used to avoid the singularity of\nthe zero-order fundamental solution in creating the DFW series. The fundamental\nsolutions of various composite PDEs are found very flexible and efficient to\nhandle a borad range of problems. We also discuss the underlying connections\nbetween the crucial concepts of dimension, scale and the order of PDE through\nthe analysis of dissipative acoustic wave propagation. The shape parameter of\nthe potential problems is also employed as the \"scale parameter\" to create the\nnon-orthogonal DFW. This paper also briefly discusses and conjectures the DFW\ncorrespondences of a variety of coordinate variable transforms and series.\nPractically important, the anisotropic and inhomogeneous DFW's are developed by\nusing the geodesic distance variable. The DFW and the related basis functions\nare also used in making the kernel distance sigmoidal functions, which are\npotentially useful in the artificial neural network and machine learning. As or\neven worse than the preceding two reports, this study scarifies mathematical\nrigor and in turn unfetter imagination. Most results are intuitively obtained\nwithout rigorous analysis. Follow-up research is still under way. The paper is\nintended to inspire more research into this promising area.", 
    "link": "http://arxiv.org/pdf/cs/0206016v1", 
    "arxiv-id": "cs/0206016v1"
},{
    "category": "cs.DC", 
    "author": "Ronald M. Fussell", 
    "title": "National Infrastructure Contingencies: Survey of Wireless Technology   Support", 
    "publish": "2002-07-01T18:09:44Z", 
    "summary": "In modern society, the flow of information has become the lifeblood of\ncommerce and social interaction. This movement of data supports most aspects of\nthe United States economy in particular, as well as, serving as the vehicle\nupon which governmental agencies react to social conditions. In addition, it is\nunderstood that the continuance of efficient and reliable data communications\nduring times of national or regional disaster remains a priority in the United\nStates. The coordination of emergency response and area revitalization /\nrehabilitation efforts between local, state, and federal emergency response is\nincreasingly necessary as agencies strive to work more seamlessly between the\naffected organizations. Additionally, international support is often made\navailable to react to such adverse conditions as wildfire suppression scenarios\nand therefore require the efficient management of workforce and associated\nlogistics support.\n  It is through the examination of the issues related to un-tethered data\ntransmission during infrastructure contingencies that responders may best\ntailor a unified approach to the rapid recovery after disasters occur.", 
    "link": "http://arxiv.org/pdf/cs/0207001v1", 
    "arxiv-id": "cs/0207001v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Symmetric boundary knot method", 
    "publish": "2002-07-03T20:17:31Z", 
    "summary": "The boundary knot method (BKM) is a recent boundary-type radial basis\nfunction (RBF) collocation scheme for general PDEs. Like the method of\nfundamental solution (MFS), the RBF is employed to approximate the\ninhomogeneous terms via the dual reciprocity principle. Unlike the MFS, the\nmethod uses a nonsingular general solution instead of a singular fundamental\nsolution to evaluate the homogeneous solution so as to circumvent the\ncontroversial artificial boundary outside the physical domain. The BKM is\nmeshfree, superconvergent, integration free, very easy to learn and program.\nThe original BKM, however, loses symmetricity in the presense of mixed\nboundary. In this study, by analogy with Hermite RBF interpolation, we\ndeveloped a symmetric BKM scheme. The accuracy and efficiency of the symmetric\nBKM are also numerically validated in some 2D and 3D Helmholtz and diffusion\nreaction problems under complicated geometries.", 
    "link": "http://arxiv.org/pdf/cs/0207010v1", 
    "arxiv-id": "cs/0207010v1"
},{
    "category": "cs.CE", 
    "author": "M. Tanaka", 
    "title": "New advances in dual reciprocity and boundary-only RBF methods", 
    "publish": "2002-07-04T12:10:06Z", 
    "summary": "This paper made some significant advances in the dual reciprocity and\nboundary-only RBF techniques. The proposed boundary knot method (BKM) is\ndifferent from the standard boundary element method in a number of important\naspects. Namely, it is truly meshless, exponential convergence,\nintegration-free (of course, no singular integration), boundary-only for\ngeneral problems, and leads to symmetric matrix under certain conditions (able\nto be extended to general cases after further modified). The BKM also avoids\nthe artificial boundary in the method of fundamental solution. An amazing\nfinding is that the BKM can formulate linear modeling equations for nonlinear\npartial differential systems with linear boundary conditions. This merit makes\nit circumvent all perplexing issues in the iteration solution of nonlinear\nequations. On the other hand, by analogy with Green's second identity, this\npaper also presents a general solution RBF (GSR) methodology to construct\nefficient RBFs in the dual reciprocity and domain-type RBF collocation methods.\nThe GSR approach first establishes an explicit relationship between the BEM and\nRBF itself on the ground of the weighted residual principle. This paper also\ndiscusses the RBF convergence and stability problems within the framework of\nintegral equation theory.", 
    "link": "http://arxiv.org/pdf/cs/0207015v1", 
    "arxiv-id": "cs/0207015v1"
},{
    "category": "cs.CE", 
    "author": "M. Tanaka", 
    "title": "Relationship between boundary integral equation and radial basis   function", 
    "publish": "2002-07-04T12:15:11Z", 
    "summary": "This paper aims to survey our recent work relating to the radial basis\nfunction (RBF) from some new views of points. In the first part, we established\nthe RBF on numerical integration analysis based on an intrinsic relationship\nbetween the Green's boundary integral representation and RBF. It is found that\nthe kernel function of integral equation is important to create efficient RBF.\nThe fundamental solution RBF (FS-RBF) was presented as a novel strategy\nconstructing operator-dependent RBF. We proposed a conjecture formula featuring\nthe dimension affect on error bound to show the independent-dimension merit of\nthe RBF techniques. We also discussed wavelet RBF, localized RBF schemes, and\nthe influence of node placement on the RBF solution accuracy. The\ncentrosymmetric matrix structure of the RBF interpolation matrix under\nsymmetric node placing is proved.\n  The second part of this paper is concerned with the boundary knot method\n(BKM), a new boundary-only, meshless, spectral convergent, integration-free RBF\ncollocation technique. The BKM was tested to the Helmholtz, Laplace, linear and\nnonlinear convection-diffusion problems. In particular, we introduced the\nresponse knot-dependent nonsingular general solution to calculate\nvarying-parameter and nonlinear steady convection-diffusion problems very\nefficiently. By comparing with the multiple dual reciprocity method, we\ndiscussed the completeness issue of the BKM.\n  Finally, the nonsingular solutions for some known differential operators were\ngiven in appendix. Also we expanded the RBF concepts by introducing time-space\nRBF for transient problems.", 
    "link": "http://arxiv.org/pdf/cs/0207016v1", 
    "arxiv-id": "cs/0207016v1"
},{
    "category": "cs.CE", 
    "author": "M. Tanaka", 
    "title": "New Insights in Boundary-only and Domain-type RBF Methods", 
    "publish": "2002-07-04T12:18:06Z", 
    "summary": "This paper has made some significant advances in the boundary-only and\ndomain-type RBF techniques. The proposed boundary knot method (BKM) is\ndifferent from the standard boundary element method in a number of important\naspects. Namely, it is truly meshless, exponential convergence,\nintegration-free (of course, no singular integration), boundary-only for\ngeneral problems, and leads to symmetric matrix under certain conditions (able\nto be extended to general cases after further modified). The BKM also avoids\nthe artificial boundary in the method of fundamental solution. An amazing\nfinding is that the BKM can formulate linear modeling equations for nonlinear\npartial differential systems with linear boundary conditions. This merit makes\nit circumvent all perplexing issues in the iteration solution of nonlinear\nequations. On the other hand, by analogy with Green's second identity, we also\npresents a general solution RBF (GSR) methodology to construct efficient RBFs\nin the domain-type RBF collocation method and dual reciprocity method. The GSR\napproach first establishes an explicit relationship between the BEM and RBF\nitself on the ground of the potential theory. This paper also discusses some\nessential issues relating to the RBF computing, which include time-space RBFs,\ndirect and indirect RBF schemes, finite RBF method, and the application of\nmultipole and wavelet to the RBF solution of the PDEs.", 
    "link": "http://arxiv.org/pdf/cs/0207017v1", 
    "arxiv-id": "cs/0207017v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Definitions of distance function in radial basis function approach", 
    "publish": "2002-07-04T12:20:24Z", 
    "summary": "Very few studies involve how to construct the efficient RBFs by means of\nproblem features. Recently the present author presented general solution RBF\n(GS-RBF) methodology to create operator-dependent RBFs successfully [1]. On the\nother hand, the normal radial basis function (RBF) is defined via Euclidean\nspace distance function or the geodesic distance [2]. This purpose of this note\nis to redefine distance function in conjunction with problem features, which\ninclude problem-dependent and time-space distance function.", 
    "link": "http://arxiv.org/pdf/cs/0207018v1", 
    "arxiv-id": "cs/0207018v1"
},{
    "category": "cs.CE", 
    "author": "Yongxi Yu", 
    "title": "Reducing the Computational Requirements of the Differential Quadrature   Method", 
    "publish": "2002-07-09T19:53:42Z", 
    "summary": "This paper shows that the weighting coefficient matrices of the differential\nquadrature method (DQM) are centrosymmetric or skew-centrosymmetric if the grid\nspacings are symmetric irrespective of whether they are equal or unequal. A new\nskew centrosymmetric matrix is also discussed. The application of the\nproperties of centrosymmetric and skew centrosymmetric matrix can reduce the\ncomputational effort of the DQM for calculations of the inverse, determinant,\neigenvectors and eigenvalues by 75%. This computational advantage are also\ndemonstrated via several numerical examples.", 
    "link": "http://arxiv.org/pdf/cs/0207033v1", 
    "arxiv-id": "cs/0207033v1"
},{
    "category": "cs.CE", 
    "author": "Tingxiu Zhong", 
    "title": "A Lyapunov Formulation for Efficient Solution of the Poisson and   Convection-Diffusion Equations by the Differential Quadrature Method", 
    "publish": "2002-07-09T20:26:18Z", 
    "summary": "Civan and Sliepcevich [1, 2] suggested that special matrix solver should be\ndeveloped to further reduce the computing effort in applying the differential\nquadrature (DQ) method for the Poisson and convection-diffusion equations.\nTherefore, the purpose of the present communication is to introduce and apply\nthe Lyapunov formulation which can be solved much more efficiently than the\nGaussian elimination method. Civan and Sliepcevich [2] first presented DQ\napproximate formulas in polynomial form for partial derivatives in\ntow-dimensional variable domain. For simplifying formulation effort, Chen et\nal. [3] proposed the compact matrix form of these DQ approximate formulas. In\nthis study, by using these matrix approximate formulas, the DQ formulations for\nthe Poisson and convection-diffusion equations can be expressed as the Lyapunov\nalgebraic matrix equation. The formulation effort is simplified, and a simple\nand explicit matrix formulation is obtained. A variety of fast algorithms in\nthe solution of the Lyapunov equation [4-6] can be successfully applied in the\nDQ analysis of these two-dimensional problems, and, thus, the computing effort\ncan be greatly reduced. Finally, we also point out that the present reduction\ntechnique can be easily extended to the three-dimensional cases.", 
    "link": "http://arxiv.org/pdf/cs/0207035v1", 
    "arxiv-id": "cs/0207035v1"
},{
    "category": "cs.CE", 
    "author": "W Chen", 
    "title": "Dual reciprocity BEM and dynamic programming filter for inverse   elastodynamic problems", 
    "publish": "2002-07-10T08:00:42Z", 
    "summary": "This paper presents the first coupling application of the dual reciprocity\nBEM (DRBEM) and dynamic programming filter to inverse elastodynamic problem.\nThe DRBEM is the only BEM method, which does not require domain discretization\nfor general linear and nonlinear dynamic problems. Since the size of numerical\ndiscretization system has a great effect on the computing effort of recursive\nor iterative calculations of inverse analysis, the intrinsic boundary-only\nmerit of the DRBEM causes a considerable computational saving. On the other\nhand, the strengths of the dynamic programming filter lie in its mathematical\nsimplicity, easy to program and great flexibility in the type, number and\nlocations of measurements and unknown inputs. The combination of these two\ntechniques is therefore very attractive for the solution of practical inverse\nproblems. In this study, the spatial and temporal partial derivatives of the\ngoverning equation are respectively discretized first by the DRBEM and the\nprecise integration method, and then, by using dynamic programming with\nregularization, dynamic load is estimated based on noisy measurements of\nvelocity and displacement at very few locations. Numerical experiments involved\nwith the periodic and Heaviside impact load are conducted to demonstrate the\napplicability, efficiency and simplicity of this strategy. The affect of noise\nlevel, regularization parameter, and measurement types on the estimation is\nalso investigated.", 
    "link": "http://arxiv.org/pdf/cs/0207039v1", 
    "arxiv-id": "cs/0207039v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "RBF-based meshless boundary knot method and boundary particle method", 
    "publish": "2002-07-10T20:31:29Z", 
    "summary": "This paper is concerned with the two new boundary-type radial basis function\ncollocation schemes, boundary knot method (BKM) and boundary particle method\n(BPM). The BKM is developed based on the dual reciprocity theorem, while the\nBPM employs the multiple reciprocity technique. Unlike the method of\nfundamental solution, the wto methods use the nonsingular general solutions\ninstead of singular fundamental solution to circumvent the controversial\nartificial boundary outside physical domain. Compared with the boundary element\nmethod, both the BKM and BPM are meshfree, superconvergent, meshfree,\nintegration free, symmetric, and mathematically simple collocation techniques\nfor general PDEs. In particular, the BPM does not require any inner nodes for\ninhomogeneous problems. In this study, the accuracy and efficiency of the two\nmethods are numerically demonstrated to some 2D, 3D Helmholtz and\nconvection-diffusion problems under complicated geometries.", 
    "link": "http://arxiv.org/pdf/cs/0207041v1", 
    "arxiv-id": "cs/0207041v1"
},{
    "category": "cs.CE", 
    "author": "M. Tanaka", 
    "title": "A meshless, integration-free, and boundary-only RBF technique", 
    "publish": "2002-07-11T12:19:49Z", 
    "summary": "Based on the radial basis function (RBF), non-singular general solution and\ndual reciprocity method (DRM), this paper presents an inherently meshless,\nintegration-free, boundary-only RBF collocation techniques for numerical\nsolution of various partial differential equation systems. The basic ideas\nbehind this methodology are very mathematically simple. In this study, the RBFs\nare employed to approximate the inhomogeneous terms via the DRM, while\nnon-singular general solution leads to a boundary-only RBF formulation for\nhomogenous solution. The present scheme is named as the boundary knot method\n(BKM) to differentiate it from the other numerical techniques. In particular,\ndue to the use of nonsingular general solutions rather than singular\nfundamental solutions, the BKM is different from the method of fundamental\nsolution in that the former does no require the artificial boundary and results\nin the symmetric system equations under certain conditions. The efficiency and\nutility of this new technique are validated through a number of typical\nnumerical examples. Completeness concern of the BKM due to the only use of\nnon-singular part of complete fundamental solution is also discussed.", 
    "link": "http://arxiv.org/pdf/cs/0207043v1", 
    "arxiv-id": "cs/0207043v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "Some addenda on distance function wavelets", 
    "publish": "2002-07-15T19:58:27Z", 
    "summary": "This report will add some supplements to the recently finished report series\non the distance function wavelets (DFW). First, we define the general distance\nin terms of the Riesz potential, and then, the distance function Abel wavelets\nare derived via the fractional integral and Laplacian. Second, the DFW Weyl\ntransform is found to be a shifted Laplace potential DFW. The DFW Radon\ntransform is also presented. Third, we present a conjecture on truncation error\nformula of the multiple reciprocity Laplace DFW series and discuss its error\ndistributions in terms of node density distributions. Forth, we point out that\nthe Hermite distance function interpolation can be used to replace overlapping\nin the domain decomposition in order to produce sparse matrix. Fifth, the shape\nparameter is explained as a virtual extra axis contribution in terms of the\nMQ-type Possion kernel. The report is concluded with some remarks on a range of\nother issues.", 
    "link": "http://arxiv.org/pdf/cs/0207062v1", 
    "arxiv-id": "cs/0207062v1"
},{
    "category": "cs.DB", 
    "author": "Jan vandenBerg", 
    "title": "Petabyte Scale Data Mining: Dream or Reality?", 
    "publish": "2002-08-07T22:49:56Z", 
    "summary": "Science is becoming very data intensive1. Today's astronomy datasets with\ntens of millions of galaxies already present substantial challenges for data\nmining. In less than 10 years the catalogs are expected to grow to billions of\nobjects, and image archives will reach Petabytes. Imagine having a 100GB\ndatabase in 1996, when disk scanning speeds were 30MB/s, and database tools\nwere immature. Such a task today is trivial, almost manageable with a laptop.\nWe think that the issue of a PB database will be very similar in six years. In\nthis paper we scale our current experiments in data archiving and analysis on\nthe Sloan Digital Sky Survey2,3 data six years into the future. We analyze\nthese projections and look at the requirements of performing data mining on\nsuch data sets. We conclude that the task scales rather well: we could do the\njob today, although it would be expensive. There do not seem to be any\nshow-stoppers that would prevent us from storing and using a Petabyte dataset\nsix years from today.", 
    "link": "http://arxiv.org/pdf/cs/0208013v1", 
    "arxiv-id": "cs/0208013v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "A note on fractional derivative modeling of broadband   frequency-dependent absorption: Model III", 
    "publish": "2002-08-08T07:29:10Z", 
    "summary": "By far, the fractional derivative model is mainly related to the modelling of\ncomplicated solid viscoelastic material. In this study, we try to build the\nfractional derivative PDE model for broadband ultrasound propagation through\nhuman tissues.", 
    "link": "http://arxiv.org/pdf/cs/0208016v1", 
    "arxiv-id": "cs/0208016v1"
},{
    "category": "cs.CE", 
    "author": "W Chen", 
    "title": "A direct time-domain FEM modeling of broadband frequency-dependent   absorption with the presence of matrix fractional power: Model I", 
    "publish": "2002-08-20T12:36:06Z", 
    "summary": "The frequency-dependent attenuation of broadband acoustics is often\nconfronted in many different areas. However, the related time domain simulation\nis rarely found in literature due to enormous technical difficulty. The\ncurrently popular relaxation models with the presence of convolution operation\nrequire some material parameters which are not readily available. In this\nstudy, three reports are contributed to address broadband ultrasound\nfrequency-dependent absorptions using the readily available empirical\nparameters. This report is the first in series concerned with developing a\ndirect time domain FEM formulation. The next two reports are about the\nfrequency decomposition model and the fractional derivative model.", 
    "link": "http://arxiv.org/pdf/cs/0208030v1", 
    "arxiv-id": "cs/0208030v1"
},{
    "category": "cs.CE", 
    "author": "MyungHo Kim", 
    "title": "A Novel Statistical Diagnosis of Clinical Data", 
    "publish": "2002-09-02T03:52:48Z", 
    "summary": "In this paper, we present a diagnosis method of diseases from clinical data.\nThe data are routine test such as urine test, hematology, chemistries etc.\nThough those tests have been done for people who check in medical institutes,\nhow each item of the data interacts each other and which combination of them\ncause a disease are neither understood nor studied well. Here we attack the\npractically important problem by putting the data into mathematical setup and\napplying support vector machine. Finally we present simulation results for\nfatty liver, gastritis etc and discuss about their implications.", 
    "link": "http://arxiv.org/pdf/cs/0209001v1", 
    "arxiv-id": "cs/0209001v1"
},{
    "category": "cs.NA", 
    "author": "W. Chen", 
    "title": "A new definition of the fractional Laplacian", 
    "publish": "2002-09-18T12:45:43Z", 
    "summary": "It is noted that the standard definition of the fractional Laplacian leads to\na hyper-singular convolution integral and is also obscure about how to\nimplement the boundary conditions. This purpose of this note is to introduce a\nnew definition of the fractional Laplacian to overcome these major drawbacks.", 
    "link": "http://arxiv.org/pdf/cs/0209020v1", 
    "arxiv-id": "cs/0209020v1"
},{
    "category": "cs.GR", 
    "author": "R. L. Mikkelson", 
    "title": "User software for the next generation", 
    "publish": "2002-10-19T01:27:45Z", 
    "summary": "New generations of neutron scattering sources and instrumentation are\nproviding challenges in data handling for user software. Time-of-Flight\ninstruments used at pulsed sources typically produce hundreds or thousands of\nchannels of data for each detector segment. New instruments are being designed\nwith thousands to hundreds of thousands of detector segments. High intensity\nneutron sources make possible parametric studies and texture studies which\nfurther increase data handling requirements. The Integrated Spectral Analysis\nWorkbench (ISAW) software developed at Argonne handles large numbers of spectra\nsimultaneously while providing operations to reduce, sort, combine and export\nthe data. It includes viewers to inspect the data in detail in real time. ISAW\nuses existing software components and packages where feasible and takes\nadvantage of the excellent support for user interface design and network\ncommunication in Java. The included scripting language simplifies repetitive\noperations for analyzing many files related to a given experiment. Recent\nadditions to ISAW include a contour view, a time-slice table view, routines for\nfinding and fitting peaks in data, and support for data from other facilities\nusing the NeXus format. In this paper, I give an overview of features and\nplanned improvements of ISAW. Details of some of the improvements are covered\nin other presentations at this conference.", 
    "link": "http://arxiv.org/pdf/cs/0210018v1", 
    "arxiv-id": "cs/0210018v1"
},{
    "category": "cs.DB", 
    "author": "Ani R. Thakar", 
    "title": "SkyQuery: A WebService Approach to Federate Databases", 
    "publish": "2002-11-20T04:54:19Z", 
    "summary": "Traditional science searched for new objects and phenomena that led to\ndiscoveries. Tomorrow's science will combine together the large pool of\ninformation in scientific archives and make discoveries. Scienthists are\ncurrently keen to federate together the existing scientific databases. The\nmajor challenge in building a federation of these autonomous and heterogeneous\ndatabases is system integration. Ineffective integration will result in defunct\nfederations and under utilized scientific data.\n  Astronomy, in particular, has many autonomous archives spread over the\nInternet. It is now seeking to federate these, with minimal effort, into a\nVirtual Observatory that will solve complex distributed computing tasks such as\nanswering federated spatial join queries.\n  In this paper, we present SkyQuery, a successful prototype of an evolving\nfederation of astronomy archives. It interoperates using the emerging Web\nservices standard. We describe the SkyQuery architecture and show how it\nefficiently evaluates a probabilistic federated spatial join query.", 
    "link": "http://arxiv.org/pdf/cs/0211023v1", 
    "arxiv-id": "cs/0211023v1"
},{
    "category": "cs.NE", 
    "author": "Robert Ewaschuk", 
    "title": "JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space", 
    "publish": "2002-12-08T00:26:49Z", 
    "summary": "JohnnyVon is an implementation of self-replicating automata in continuous\ntwo-dimensional space. Two types of particles drift about in a virtual liquid.\nThe particles are automata with discrete internal states but continuous\nexternal relationships. Their internal states are governed by finite state\nmachines but their external relationships are governed by a simulated physics\nthat includes brownian motion, viscosity, and spring-like attractive and\nrepulsive forces. The particles can be assembled into patterns that can encode\narbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern\nis put in a \"soup\" of separate individual particles, the pattern will replicate\nby assembling the individual particles into copies of itself. We also show\nthat, given sufficient time, a soup of separate individual particles will\neventually spontaneously form self-replicating patterns. We discuss the\nimplications of JohnnyVon for research in nanotechnology, theoretical biology,\nand artificial life.", 
    "link": "http://arxiv.org/pdf/cs/0212010v1", 
    "arxiv-id": "cs/0212010v1"
},{
    "category": "cs.CE", 
    "author": "Naren Ramakrishnan", 
    "title": "Novel Runtime Systems Support for Adaptive Compositional Modeling on the   Grid", 
    "publish": "2003-01-21T02:26:47Z", 
    "summary": "Grid infrastructures and computing environments have progressed significantly\nin the past few years. The vision of truly seamless Grid usage relies on\nruntime systems support that is cognizant of the operational issues underlying\ngrid computations and, at the same time, is flexible enough to accommodate\ndiverse application scenarios. This paper addresses the twin aspects of Grid\ninfrastructure and application support through a novel combination of two\ncomputational technologies: Weaves - a source-language independent parallel\nruntime compositional framework that operates through reverse-analysis of\ncompiled object files, and runtime recommender systems that aid in dynamic\nknowledge-based application composition. Domain-specific adaptivity is\nexploited through a novel compositional system that supports runtime\nrecommendation of code modules and a sophisticated checkpointing and runtime\nmigration solution that can be transparently deployed over Grid\ninfrastructures. A core set of \"adaptivity schemas\" are provided as templates\nfor adaptive composition of large-scale scientific computations. Implementation\nissues, motivating application contexts, and preliminary results are described.", 
    "link": "http://arxiv.org/pdf/cs/0301018v1", 
    "arxiv-id": "cs/0301018v1"
},{
    "category": "cs.CE", 
    "author": "Francesc Rossello", 
    "title": "Modelling Biochemical Operations on RNA Secondary Structures", 
    "publish": "2003-06-03T08:48:54Z", 
    "summary": "In this paper we model several simple biochemical operations on RNA molecules\nthat modify their secondary structure by means of a suitable variation of\nGro\\ss e-Rhode's Algebra Transformation Systems.", 
    "link": "http://arxiv.org/pdf/cs/0306016v1", 
    "arxiv-id": "cs/0306016v1"
},{
    "category": "cs.CE", 
    "author": "Mihai V. Putz", 
    "title": "Contributions to the Development and Improvement of a Regulatory and   Pre-Regulatory Digitally System for the Tools within Flexible Fabrication   Systems", 
    "publish": "2003-07-24T14:01:16Z", 
    "summary": "The paper reports the obtained results for the projection and realization of\na digitally system aiming to assist the equipment for a regulatory and\npre-regulatory tools and holding tools within the flexible fabrication systems\n(FFS). Moreover, based on the present results, the same methodology can be\napplied for assisting tools from the point of view of their integrity and to\nwear compensation in the FFS framework.", 
    "link": "http://arxiv.org/pdf/cs/0307054v1", 
    "arxiv-id": "cs/0307054v1"
},{
    "category": "cs.CE", 
    "author": "W. Chen", 
    "title": "Boundary knot method for Laplace and biharmonic problems", 
    "publish": "2003-07-28T09:35:52Z", 
    "summary": "The boundary knot method (BKM) [1] is a meshless boundary-type radial basis\nfunction (RBF) collocation scheme, where the nonsingular general solution is\nused instead of fundamental solution to evaluate the homogeneous solution,\nwhile the dual reciprocity method (DRM) is employed to approximation of\nparticular solution. Despite the fact that there are not nonsingular RBF\ngeneral solutions available for Laplace and biharmonic problems, this study\nshows that the method can be successfully applied to these problems. The\nhigh-order general and fundamental solutions of Burger and Winkler equations\nare also first presented here.", 
    "link": "http://arxiv.org/pdf/cs/0307061v1", 
    "arxiv-id": "cs/0307061v1"
},{
    "category": "cs.CE", 
    "author": "Fionn Murtagh", 
    "title": "The Generalized Riemann or Henstock Integral Underpinning Multivariate   Data Analysis: Application to Faint Structure Finding in Price Processes", 
    "publish": "2003-08-05T09:31:38Z", 
    "summary": "Practical data analysis involves many implicit or explicit assumptions about\nthe good behavior of the data, and excludes consideration of various\npotentially pathological or limit cases. In this work, we present a new general\ntheory of data, and of data processing, to bypass some of these assumptions.\nThe new framework presented is focused on integration, and has direct\napplicability to expectation, distance, correlation, and aggregation. In a case\nstudy, we seek to reveal faint structure in financial data. Our new foundation\nfor data encoding and handling offers increased justification for our\nconclusions.", 
    "link": "http://arxiv.org/pdf/cs/0308009v3", 
    "arxiv-id": "cs/0308009v3"
},{
    "category": "cs.CE", 
    "author": "Jules Sadefo Kamdem", 
    "title": "Value-at-Risk and Expected Shortfall for Quadratic portfolio of   securities with mixture of elliptic Distributed Risk Factors", 
    "publish": "2003-10-22T18:04:24Z", 
    "summary": "Generally, in the financial literature, the notion of quadratic VaR is\nimplicitly confused with the Delta-Gamma VaR, because more authors dealt with\nportfolios that contains derivatives instruments.\n  In this paper, we postpone to estimate the Value-at-Risk of a quadratic\nportfolio of securities (i.e equities) without the Delta and Gamma greeks, when\nthe joint log-returns changes with multivariate elliptic distribution. We have\nreduced the estimation of the quadratic VaR of such portfolio to a resolution\nof one dimensional integral equation. To illustrate our method, we give special\nattention to the mixture of normal and mixture of t-student distribution. For\ngiven VaR, when joint Risk Factors changes with elliptic distribution, we show\nhow to estimate an Expected Shortfall .", 
    "link": "http://arxiv.org/pdf/cs/0310043v3", 
    "arxiv-id": "cs/0310043v3"
},{
    "category": "cs.CE", 
    "author": "Richard F. Helm", 
    "title": "Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions", 
    "publish": "2003-11-27T18:13:38Z", 
    "summary": "We present an unusual algorithm involving classification trees where two\ntrees are grown in opposite directions so that they are matched at their\nleaves. This approach finds application in a new data mining task we formulate,\ncalled \"redescription mining\". A redescription is a shift-of-vocabulary, or a\ndifferent way of communicating information about a given subset of data; the\ngoal of redescription mining is to find subsets of data that afford multiple\ndescriptions. We highlight the importance of this problem in domains such as\nbioinformatics, which exhibit an underlying richness and diversity of data\ndescriptors (e.g., genes can be studied in a variety of ways). Our approach\nhelps integrate multiple forms of characterizing datasets, situates the\nknowledge gained from one dataset in the context of others, and harnesses\nhigh-level abstractions for uncovering cryptic and subtle features of data.\nAlgorithm design decisions, implementation details, and experimental results\nare presented.", 
    "link": "http://arxiv.org/pdf/cs/0311048v1", 
    "arxiv-id": "cs/0311048v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The modulus in the CAD system drawings as a base of developing of the   problem-oriented extensions", 
    "publish": "2004-05-12T07:55:34Z", 
    "summary": "The concept of the \"modulus\" in the CAD system drawings is characterized,\nbeing a base of developing of the problem-oriented extensions. The modulus\nconsists of visible geometric elements of the drawing and invisible parametric\nrepresentation of the modelling object. The technological advantages of\nmoduluss in a complex CAD system developing are described.", 
    "link": "http://arxiv.org/pdf/cs/0405041v1", 
    "arxiv-id": "cs/0405041v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "Modular technology of developing of the problem-oriented extensions of a   CAD system of reconstruction of the plant", 
    "publish": "2004-05-14T17:43:33Z", 
    "summary": "The modular technology of creation of the problem-oriented extensions of a\nCAD system is described, which was realised in a system TechnoCAD GlassX for\ndesigning of reconstruction of the plants. The modularity of the technology is\nexpressed in storage of all parameters of the design in one element of the\ndrawing - modulus, with automatic generation of a geometrical part of the\nmodulus from these parameters. The common principles of the system organization\nof extensions developing are described: separation of the part of the design to\nautomize in this extension, architecture of parameters in the form of the lists\nof objects with their properties and links to another objects, separation of\ncommon and special operations, stages of the developing, boundaries of\napplicability of technology.", 
    "link": "http://arxiv.org/pdf/cs/0405047v1", 
    "arxiv-id": "cs/0405047v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The model of the tables in design documentation for operating with the   electronic catalogs and for specifications making in a CAD system", 
    "publish": "2004-05-17T09:39:03Z", 
    "summary": "The hierarchic block model of the tables in design documentation as a part of\na CAD system is described, intended for automatic specifications making of\nelements of the drawings, with usage of the electronic catalogs. The model is\ncreated for needs of a CAD system of reconstruction of the industrial plants,\nwhere the result of designing are the drawings, which include the\nspecifications of different types. The adequate simulation of the specification\ntables is ensured with technology of storing in the drawing of the visible\ngeometric elements and invisible parametric representation, sufficient for\ngeneration of this elements.", 
    "link": "http://arxiv.org/pdf/cs/0405054v1", 
    "arxiv-id": "cs/0405054v1"
},{
    "category": "cs.CE", 
    "author": "Ilsur T. Safin", 
    "title": "Modular technology of developing of the extensions of a CAD system.   Axonometric piping diagrams. Parametric representation", 
    "publish": "2004-05-17T09:46:02Z", 
    "summary": "Applying the modular technology of developing of the problem-oriented\nextensions of a CAD system to a problem of automation of creating of the\naxonometric piping diagrams on an example of the program system TechnoCAD\nGlassX is described. The proximity of composition of the schemas is detected\nfor special technological pipe lines, systems of a water line and water drain,\nheating, heat supply, ventilating, air conditioning. The structured parametric\nrepresentation of the schemas, including properties of objects, their link,\ncommon settings, settings by default and the special links of compatibility is\nreviewed.", 
    "link": "http://arxiv.org/pdf/cs/0405055v1", 
    "arxiv-id": "cs/0405055v1"
},{
    "category": "cs.CE", 
    "author": "Rustem R. Kafiatullov", 
    "title": "Modular technology of developing of the extensions of a CAD system. The   axonometric piping diagrams. Common and special operations", 
    "publish": "2004-05-17T10:14:12Z", 
    "summary": "Applying the modular technology of developing of the problem-oriented\nextensions of a CAD system to a problem of automation of creating of the\naxonometric piping diagrams on an example of the program system TechnoCAD\nGlassX is described. The features of realization of common operations,\ncomposition and realization of special operations of a designing of the schemas\nof the special technological pipe lines, systems of a water line and water\ndrain, heating, heat supply, ventilating, air conditioning are reviewed.", 
    "link": "http://arxiv.org/pdf/cs/0405056v1", 
    "arxiv-id": "cs/0405056v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "Mathematical and programming toolkit of the computer aided design of the   axonometric piping diagrams", 
    "publish": "2004-05-17T10:34:16Z", 
    "summary": "The problem of the automation of the designing of the axonometric piping\ndiagrams include, as the minimum, manipulations with the flat schemas of\nthree-dimensional wireframe objects (with dimension of 2,5). The specialized\nmodel, methodical and mathematical approaches are required because of large\nbulk of calculuss. Coordinate systems, data types, common principles of\nrealization of operation with data and composition of the basic operations are\ndescribed which are realised in the complex CAD system of the reconstruction of\nthe plants TechnoCAD GlassX.", 
    "link": "http://arxiv.org/pdf/cs/0405057v1", 
    "arxiv-id": "cs/0405057v1"
},{
    "category": "cs.CE", 
    "author": "Valdis Vitolins", 
    "title": "Business Process Measures", 
    "publish": "2004-06-22T12:19:54Z", 
    "summary": "The paper proposes a new methodology for defining business process measures\nand their computation. The approach is based on metamodeling according to MOF.\nEspecially, a metamodel providing precise definitions of typical process\nmeasures for UML activity diagram-like notation is proposed, including precise\ndefinitions how measures should be aggregated for composite process elements.\nThe proposed approach allows defining values in a natural way, and measurement\nof data, which are of interest to business, without deep investigation into\nspecific technical solutions. This provides new possibilities for business\nprocess measurement, decreasing the gap between technical solutions and asset\nmanagement methodologies.", 
    "link": "http://arxiv.org/pdf/cs/0406042v1", 
    "arxiv-id": "cs/0406042v1"
},{
    "category": "cs.MA", 
    "author": "Pericles A Mitkas", 
    "title": "An agent-based intelligent environmental monitoring system", 
    "publish": "2004-07-10T11:06:57Z", 
    "summary": "Fairly rapid environmental changes call for continuous surveillance and\non-line decision making. There are two main areas where IT technologies can be\nvaluable. In this paper we present a multi-agent system for monitoring and\nassessing air-quality attributes, which uses data coming from a meteorological\nstation. A community of software agents is assigned to monitor and validate\nmeasurements coming from several sensors, to assess air-quality, and, finally,\nto fire alarms to appropriate recipients, when needed. Data mining techniques\nhave been used for adding data-driven, customized intelligence into agents. The\narchitecture of the developed system, its domain ontology, and typical agent\ninteractions are presented. Finally, the deployment of a real-world test case\nis demonstrated.", 
    "link": "http://arxiv.org/pdf/cs/0407024v1", 
    "arxiv-id": "cs/0407024v1"
},{
    "category": "cs.CE", 
    "author": "Paolo Dini", 
    "title": "Pervasive Service Architecture for a Digital Business Ecosystem", 
    "publish": "2004-08-20T16:23:19Z", 
    "summary": "In this paper we present ideas and architectural principles upon which we are\nbasing the development of a distributed, open-source infrastructure that, in\nturn, will support the expression of business models, the dynamic composition\nof software services, and the optimisation of service chains through automatic\nself-organising and evolutionary algorithms derived from biology. The target\nusers are small and medium-sized enterprises (SMEs). We call the collection of\nthe infrastructure, the software services, and the SMEs a Digital Business\nEcosystem (DBE).", 
    "link": "http://arxiv.org/pdf/cs/0408047v2", 
    "arxiv-id": "cs/0408047v2"
},{
    "category": "cs.CE", 
    "author": "Ilsur T. Safin", 
    "title": "The modular technology of development of the CAD expansions: profiles of   outside networks of water supply and water drain", 
    "publish": "2004-12-08T08:42:53Z", 
    "summary": "The modular technology of development of the problem-oriented CAD expansions\nis applied to a task of designing of profiles of outside networks of water\nsupply and water drain with realization in program system TechnoCAD GlassX. The\nunity of structure of this profiles is revealed, the system model of the\ndrawings of profiles of networks is developed including the structured\nparametric representation (properties of objects and their interdependence,\ngeneral settings and default settings) and operations with it, which\nefficiently automate designing", 
    "link": "http://arxiv.org/pdf/cs/0412029v1", 
    "arxiv-id": "cs/0412029v1"
},{
    "category": "cs.CE", 
    "author": "Ilsur T. Safin", 
    "title": "The modular technology of development of the CAD expansions: protection   of the buildings from the lightning", 
    "publish": "2004-12-08T08:49:08Z", 
    "summary": "The modular technology of development of the problem-oriented CAD expansions\nis applied to a task of designing of protection of the buildings from the\nlightning with realization in program system TechnoCAD GlassX. The system model\nof the drawings of lightning protection is developed including the structured\nparametric representation (properties of objects and their interdependence,\ngeneral settings and default settings) and operations with it, which\nefficiently automate designing", 
    "link": "http://arxiv.org/pdf/cs/0412030v1", 
    "arxiv-id": "cs/0412030v1"
},{
    "category": "cs.CE", 
    "author": "Vladimir V. Migunov", 
    "title": "The methods of support of the requirements of the Russian standards at   development of a CAD of industrial objects", 
    "publish": "2004-12-08T08:57:49Z", 
    "summary": "The methods of support of the requirements of the Russian standards in a CAD\nof industrial objects are explained, which were implemented in the CAD system\nTechnoCAD GlassX with an own graphics core and own structures of data storage.\nIt is rotined, that the binding of storage structures and program code of a CAD\nto the requirements of standards enable not only to fulfil these requirements\nin project documentation, but also to increase a degree of compactness of\nstorage of drawings both on the disk and in the RAM", 
    "link": "http://arxiv.org/pdf/cs/0412032v1", 
    "arxiv-id": "cs/0412032v1"
},{
    "category": "cs.CE", 
    "author": "Alexander Shaydurov", 
    "title": "Identification of complex systems in the basis of wavelets", 
    "publish": "2005-02-02T00:12:45Z", 
    "summary": "In this paper is proposed the method of the identification of complex dynamic\nsystems. Method can be used for the identification of linear and nonlinear\ncomplex dynamic systems for the determined or stochastic signals at the inputs\nand the outputs. It is proposed to use a basis of wavelets for obtaining the\nimpulse transient function (ITF) of system. ITF is considered in the form of\nsurface in the 3D space. Are given the results of experiments on the\nidentification of systems in the basis of wavelets.", 
    "link": "http://arxiv.org/pdf/cs/0502007v1", 
    "arxiv-id": "cs/0502007v1"
},{
    "category": "cs.DB", 
    "author": "Gerd Heber", 
    "title": "Scientific Data Management in the Coming Decade", 
    "publish": "2005-02-02T03:15:42Z", 
    "summary": "This is a thought piece on data-intensive science requirements for databases\nand science centers. It argues that peta-scale datasets will be housed by\nscience centers that provide substantial storage and processing for scientists\nwho access the data via smart notebooks. Next-generation science instruments\nand simulations will generate these peta-scale datasets. The need to publish\nand share data and the need for generic analysis and visualization tools will\nfinally create a convergence on common metadata standards. Database systems\nwill be judged by their support of these metadata standards and by their\nability to manage and access peta-scale datasets. The procedural\nstream-of-bytes-file-centric approach to data analysis is both too cumbersome\nand too serial for such large datasets. Non-procedural query and analysis of\nschematized self-describing data is both easier to use and allows much more\nparallelism.", 
    "link": "http://arxiv.org/pdf/cs/0502008v1", 
    "arxiv-id": "cs/0502008v1"
},{
    "category": "cs.DB", 
    "author": "Alexander S. Szalay", 
    "title": "Where the Rubber Meets the Sky: Bridging the Gap between Databases and   Science", 
    "publish": "2005-02-02T04:40:55Z", 
    "summary": "Scientists in all domains face a data avalanche - both from better\ninstruments and from improved simulations. We believe that computer science\ntools and computer scientists are in a position to help all the sciences by\nbuilding tools and developing techniques to manage, analyze, and visualize\npeta-scale scientific information. This article is summarizes our experiences\nover the last seven years trying to bridge the gap between database technology\nand the needs of the astronomy community in building the World-Wide Telescope.", 
    "link": "http://arxiv.org/pdf/cs/0502011v1", 
    "arxiv-id": "cs/0502011v1"
},{
    "category": "cs.SC", 
    "author": "R. Barrere", 
    "title": "Can Computer Algebra be Liberated from its Algebraic Yoke ?", 
    "publish": "2005-02-03T17:28:01Z", 
    "summary": "So far, the scope of computer algebra has been needlessly restricted to exact\nalgebraic methods. Its possible extension to approximate analytical methods is\ndiscussed. The entangled roles of functional analysis and symbolic programming,\nespecially the functional and transformational paradigms, are put forward. In\nthe future, algebraic algorithms could constitute the core of extended symbolic\nmanipulation systems including primitives for symbolic approximations.", 
    "link": "http://arxiv.org/pdf/cs/0502015v1", 
    "arxiv-id": "cs/0502015v1"
},{
    "category": "cs.IR", 
    "author": "C. -A. Thole", 
    "title": "Data Mining on Crash Simulation Data", 
    "publish": "2005-05-02T15:27:45Z", 
    "summary": "The work presented in this paper is part of the cooperative research project\nAUTO-OPT carried out by twelve partners from the automotive industries. One\nmajor work package concerns the application of data mining methods in the area\nof automotive design. Suitable methods for data preparation and data analysis\nare developed. The objective of the work is the re-use of data stored in the\ncrash-simulation department at BMW in order to gain deeper insight into the\ninterrelations between the geometric variations of the car during its design\nand its performance in crash testing. In this paper a method for data analysis\nof finite element models and results from crash simulation is proposed and\napplication to recent data from the industrial partner BMW is demonstrated. All\nnecessary steps from data pre-processing to re-integration into the working\nenvironment of the engineer are covered.", 
    "link": "http://arxiv.org/pdf/cs/0505008v1", 
    "arxiv-id": "cs/0505008v1"
},{
    "category": "cs.CE", 
    "author": "Laurent El Ghaoui", 
    "title": "Sparse Covariance Selection via Robust Maximum Likelihood Estimation", 
    "publish": "2005-06-08T21:08:38Z", 
    "summary": "We address a problem of covariance selection, where we seek a trade-off\nbetween a high likelihood against the number of non-zero elements in the\ninverse covariance matrix. We solve a maximum likelihood problem with a penalty\nterm given by the sum of absolute values of the elements of the inverse\ncovariance matrix, and allow for imposing bounds on the condition number of the\nsolution. The problem is directly amenable to now standard interior-point\nalgorithms for convex optimization, but remains challenging due to its size. We\nfirst give some results on the theoretical computational complexity of the\nproblem, by showing that a recent methodology for non-smooth convex\noptimization due to Nesterov can be applied to this problem, to greatly improve\non the complexity estimate given by interior-point algorithms. We then examine\ntwo practical algorithms aimed at solving large-scale, noisy (hence dense)\ninstances: one is based on a block-coordinate descent approach, where columns\nand rows are updated sequentially, another applies a dual version of Nesterov's\nmethod.", 
    "link": "http://arxiv.org/pdf/cs/0506023v1", 
    "arxiv-id": "cs/0506023v1"
},{
    "category": "cs.DC", 
    "author": "Kotagiri Ramamohanarao", 
    "title": "A Taxonomy of Data Grids for Distributed Data Sharing, Management and   Processing", 
    "publish": "2005-06-10T10:59:37Z", 
    "summary": "Data Grids have been adopted as the platform for scientific communities that\nneed to share, access, transport, process and manage large data collections\ndistributed worldwide. They combine high-end computing technologies with\nhigh-performance networking and wide-area storage management techniques. In\nthis paper, we discuss the key concepts behind Data Grids and compare them with\nother data sharing and distribution paradigms such as content delivery\nnetworks, peer-to-peer networks and distributed databases. We then provide\ncomprehensive taxonomies that cover various aspects of architecture, data\ntransportation, data replication and resource allocation and scheduling.\nFinally, we map the proposed taxonomy to various Data Grid systems not only to\nvalidate the taxonomy but also to identify areas for future exploration.\nThrough this taxonomy, we aim to categorise existing systems to better\nunderstand their goals and their methodology. This would help evaluate their\napplicability for solving similar problems. This taxonomy also provides a \"gap\nanalysis\" of this area through which researchers can potentially identify new\nissues for investigation. Finally, we hope that the proposed taxonomy and\nmapping also helps to provide an easy way for new practitioners to understand\nthis complex area of research.", 
    "link": "http://arxiv.org/pdf/cs/0506034v1", 
    "arxiv-id": "cs/0506034v1"
},{
    "category": "cs.CE", 
    "author": "Gunnar Rueckner", 
    "title": "Comparison of two different implementations of a   finite-difference-method for first-order pde in mathematica and matlab", 
    "publish": "2005-06-13T15:26:16Z", 
    "summary": "In this article two implementations of a symmetric finite difference\nalgorithm for a first-order partial differential equation are discussed. The\nconsidered partial differential equation discribes the time evolution of the\ncrack length distribution of microcracks in brittle materia.", 
    "link": "http://arxiv.org/pdf/cs/0506051v2", 
    "arxiv-id": "cs/0506051v2"
},{
    "category": "cs.NE", 
    "author": "Philippe De Wilde", 
    "title": "Long-term neuronal behavior caused by two synaptic modification   mechanisms", 
    "publish": "2005-08-26T10:38:36Z", 
    "summary": "We report the first results of simulating the coupling of neuronal,\nastrocyte, and cerebrovascular activity. It is suggested that the dynamics of\nthe system is different from systems that only include neurons. In the\nneuron-vascular coupling, distribution of synapse strengths affects neuronal\nbehavior and thus balance of the blood flow; oscillations are induced in the\nneuron-to-astrocyte coupling.", 
    "link": "http://arxiv.org/pdf/cs/0508117v1", 
    "arxiv-id": "cs/0508117v1"
},{
    "category": "cs.CE", 
    "author": "Simon W. de Leeuw", 
    "title": "Component Based Programming in Scientific Computing: The Viable Approach", 
    "publish": "2005-08-31T21:57:04Z", 
    "summary": "Computational scientists are facing a new era where the old ways of\ndeveloping and reusing code have to be left behind and a few daring steps are\nto be made towards new horizons. The present work analyzes the needs that drive\nthis change, the factors that contribute to the inertia of the community and\nslow the transition, the status and perspective of present attempts, the\nprinciple, practical and technical problems that are to be addressed in the\nshort and long run.", 
    "link": "http://arxiv.org/pdf/cs/0509002v1", 
    "arxiv-id": "cs/0509002v1"
},{
    "category": "cs.MA", 
    "author": "Sorin Solomon", 
    "title": "Traders imprint themselves by adaptively updating their own avatar", 
    "publish": "2005-09-06T17:02:52Z", 
    "summary": "Simulations of artificial stock markets were considered as early as 1964 and\nmulti-agent ones were introduced as early as 1989. Starting the early 90's,\ncollaborations of economists and physicists produced increasingly realistic\nsimulation platforms. Currently, the market stylized facts are easily\nreproduced and one has now to address the realistic details of the Market\nMicrostructure and of the Traders Behaviour. This calls for new methods and\ntools capable of bridging smoothly between simulations and experiments in\neconomics.\n  We propose here the following Avatar-Based Method (ABM). The subjects\nimplement and maintain their Avatars (programs encoding their personal decision\nmaking procedures) on NatLab, a market simulation platform. Once these\nprocedures are fed in a computer edible format, they can be operationally used\nas such without the need for belabouring, interpreting or conceptualising them.\nThus ABM short-circuits the usual behavioural economics experiments that search\nfor the psychological mechanisms underlying the subjects behaviour. Finally,\nABM maintains a level of objectivity close to the classical behaviourism while\nextending its scope to subjects' decision making mechanisms.\n  We report on experiments where Avatars designed and maintained by humans from\ndifferent backgrounds (including real traders) compete in a continuous\ndouble-auction market. We hope this unbiased way of capturing the adaptive\nevolution of real subjects behaviour may lead to a new kind of behavioural\neconomics experiments with a high degree of reliability, analysability and\nreproducibility.", 
    "link": "http://arxiv.org/pdf/cs/0509017v1", 
    "arxiv-id": "cs/0509017v1"
},{
    "category": "cs.CR", 
    "author": "Tina Hudson", 
    "title": "Underwater Hacker Missile Wars: A Cryptography and Engineering Contest", 
    "publish": "2005-09-18T22:11:56Z", 
    "summary": "For a recent student conference, the authors developed a day-long design\nproblem and competition suitable for engineering, mathematics and science\nundergraduates. The competition included a cryptography problem, for which a\nworkshop was run during the conference. This paper describes the competition,\nfocusing on the cryptography problem and the workshop. Notes from the workshop\nand code for the computer programs are made available via the Internet. The\nresults of a personal self-evaluation (PSE) are described.", 
    "link": "http://arxiv.org/pdf/cs/0509053v1", 
    "arxiv-id": "cs/0509053v1"
},{
    "category": "cs.CE", 
    "author": "Audris Kalnins", 
    "title": "Semantics of UML 2.0 Activity Diagram for Business Modeling by Means of   Virtual Machine", 
    "publish": "2005-09-28T12:24:42Z", 
    "summary": "The paper proposes a more formalized definition of UML 2.0 Activity Diagram\nsemantics. A subset of activity diagram constructs relevant for business\nprocess modeling is considered. The semantics definition is based on the\noriginal token flow methodology, but a more constructive approach is used. The\nActivity Diagram Virtual machine is defined by means of a metamodel, with\noperations defined by a mix of pseudocode and OCL pre- and postconditions. A\nformal procedure is described which builds the virtual machine for any activity\ndiagram. The relatively complicated original token movement rules in control\nnodes and edges are combined into paths from an action to action. A new\napproach is the use of different (push and pull) engines, which move tokens\nalong the paths. Pull engines are used for paths containing join nodes, where\nthe movement of several tokens must be coordinated. The proposed virtual\nmachine approach makes the activity semantics definition more transparent where\nthe token movement can be easily traced. However, the main benefit of the\napproach is the possibility to use the defined virtual machine as a basis for\nUML activity diagram based workflow or simulation engine.", 
    "link": "http://arxiv.org/pdf/cs/0509089v1", 
    "arxiv-id": "cs/0509089v1"
},{
    "category": "cs.DS", 
    "author": "Mikl\u00f3s Cs\u0171r\u00f6s", 
    "title": "A linear-time algorithm for finding the longest segment which scores   above a given threshold", 
    "publish": "2005-12-04T04:28:00Z", 
    "summary": "This paper describes a linear-time algorithm that finds the longest stretch\nin a sequence of real numbers (``scores'') in which the sum exceeds an input\nparameter. The algorithm also solves the problem of finding the longest\ninterval in which the average of the scores is above a fixed threshold. The\nproblem originates from molecular sequence analysis: for instance, the\nalgorithm can be employed to identify long GC-rich regions in DNA sequences.\nThe algorithm can also be used to trim low-quality ends of shotgun sequences in\na preprocessing step of whole-genome assembly.", 
    "link": "http://arxiv.org/pdf/cs/0512016v2", 
    "arxiv-id": "cs/0512016v2"
},{
    "category": "cs.MA", 
    "author": "Philippe De Wilde", 
    "title": "The emergence of knowledge exchange: an agent-based model of a software   market", 
    "publish": "2006-04-20T11:20:16Z", 
    "summary": "We investigate knowledge exchange among commercial organisations, the\nrationale behind it and its effects on the market. Knowledge exchange is known\nto be beneficial for industry, but in order to explain it, authors have used\nhigh level concepts like network effects, reputation and trust. We attempt to\nformalise a plausible and elegant explanation of how and why companies adopt\ninformation exchange and why it benefits the market as a whole when this\nhappens. This explanation is based on a multi-agent model that simulates a\nmarket of software providers. Even though the model does not include any\nhigh-level concepts, information exchange naturally emerges during simulations\nas a successful profitable behaviour. The conclusions reached by this\nagent-based analysis are twofold: (1) A straightforward set of assumptions is\nenough to give rise to exchange in a software market. (2) Knowledge exchange is\nshown to increase the efficiency of the market.", 
    "link": "http://arxiv.org/pdf/cs/0604078v1", 
    "arxiv-id": "cs/0604078v1"
},{
    "category": "cs.AI", 
    "author": "K. Konishi", 
    "title": "The meaning of manufacturing know-how", 
    "publish": "2006-05-30T05:12:35Z", 
    "summary": "This paper investigates the phenomenon of manufacturing know-how. First, the\nabstract notion of knowledge is discussed, and a terminological basis is\nintroduced to treat know-how as a kind of knowledge. Next, a brief survey of\nthe recently reported works dealt with manufacturing know-how is presented, and\nan explicit definition of know-how is formulated. Finally, the problem of\nutilizing know-how with knowledge-based systems is analyzed, and some ideas\nuseful for its solving are given.", 
    "link": "http://arxiv.org/pdf/cs/0605138v1", 
    "arxiv-id": "cs/0605138v1"
},{
    "category": "cs.CE", 
    "author": "Y. Fukuda", 
    "title": "A Framework for the Development of Manufacturing Simulators: Towards New   Generation of Simulation Systems", 
    "publish": "2006-06-01T02:36:12Z", 
    "summary": "In this paper, an attempt is made to systematically discuss the development\nof simulation systems for manufacturing system design. General requirements on\nmanufacturing simulators are formulated and a framework to address the\nrequirements is suggested. Problems of information representation as an\nactivity underlying simulation are considered. This is to form the necessary\nmathematical foundation for manufacturing simulations. The theoretical findings\nare explored through a pilot study. A conclusion about the suitability of the\nsuggested approach to the development of simulation systems for manufacturing\nsystem design is made, and implications for future research are described.", 
    "link": "http://arxiv.org/pdf/cs/0606004v1", 
    "arxiv-id": "cs/0606004v1"
},{
    "category": "cs.CE", 
    "author": "K. Konishi", 
    "title": "A Decision-Making Support System Based on Know-How", 
    "publish": "2006-06-02T03:06:07Z", 
    "summary": "The research results described are concerned with: - developing a domain\nmodeling method and tools to provide the design and implementation of\ndecision-making support systems for computer integrated manufacturing; -\nbuilding a decision-making support system based on know-how and its software\nenvironment. The research is funded by NEDO, Japan.", 
    "link": "http://arxiv.org/pdf/cs/0606010v1", 
    "arxiv-id": "cs/0606010v1"
},{
    "category": "cs.CE", 
    "author": "S. Kitamura", 
    "title": "A simulation engine to support production scheduling using   genetics-based machine learning", 
    "publish": "2006-06-06T09:30:58Z", 
    "summary": "The ever higher complexity of manufacturing systems, continually shortening\nlife cycles of products and their increasing variety, as well as the unstable\nmarket situation of the recent years require introducing grater flexibility and\nresponsiveness to manufacturing processes. From this perspective, one of the\ncritical manufacturing tasks, which traditionally attract significant attention\nin both academia and the industry, but which have no satisfactory universal\nsolution, is production scheduling. This paper proposes an approach based on\ngenetics-based machine learning (GBML) to treat the problem of flow shop\nscheduling. By the approach, a set of scheduling rules is represented as an\nindividual of genetic algorithms, and the fitness of the individual is\nestimated based on the makespan of the schedule generated by using the\nrule-set. A concept of the interactive software environment consisting of a\nsimulator and a GBML simulation engine is introduced to support human\ndecision-making during scheduling. A pilot study is underway to evaluate the\nperformance of the GBML technique in comparison with other methods (such as\nJohnson's algorithm and simulated annealing) while completing test examples.", 
    "link": "http://arxiv.org/pdf/cs/0606021v1", 
    "arxiv-id": "cs/0606021v1"
},{
    "category": "cs.CE", 
    "author": "S. Kitamura", 
    "title": "Evolutionary Design: Philosophy, Theory, and Application Tactics", 
    "publish": "2006-06-09T08:00:37Z", 
    "summary": "Although it has contributed to remarkable improvements in some specific\nareas, attempts to develop a universal design theory are generally\ncharacterized by failure. This paper sketches arguments for a new approach to\nengineering design based on Semiotics - the science about signs. The approach\nis to combine different design theories over all the product life cycle stages\ninto one coherent and traceable framework. Besides, it is to bring together the\ndesigner's and user's understandings of the notion of 'good product'. Building\non the insight from natural sciences that complex systems always exhibit a\nself-organizing meaning-influential hierarchical dynamics, objective laws\ncontrolling product development are found through an examination of design as a\nsemiosis process. These laws are then applied to support evolutionary design of\nproducts. An experiment validating some of the theoretical findings is\noutlined, and concluding remarks are given.", 
    "link": "http://arxiv.org/pdf/cs/0606039v1", 
    "arxiv-id": "cs/0606039v1"
},{
    "category": "cs.MA", 
    "author": "Peter D. Turney", 
    "title": "Self-Replication and Self-Assembly for Manufacturing", 
    "publish": "2006-07-27T17:55:16Z", 
    "summary": "It has been argued that a central objective of nanotechnology is to make\nproducts inexpensively, and that self-replication is an effective approach to\nvery low-cost manufacturing. The research presented here is intended to be a\nstep towards this vision. We describe a computational simulation of nanoscale\nmachines floating in a virtual liquid. The machines can bond together to form\nstrands (chains) that self-replicate and self-assemble into user-specified\nmeshes. There are four types of machines and the sequence of machine types in a\nstrand determines the shape of the mesh they will build. A strand may be in an\nunfolded state, in which the bonds are straight, or in a folded state, in which\nthe bond angles depend on the types of machines. By choosing the sequence of\nmachine types in a strand, the user can specify a variety of polygonal shapes.\nA simulation typically begins with an initial unfolded seed strand in a soup of\nunbonded machines. The seed strand replicates by bonding with free machines in\nthe soup. The child strands fold into the encoded polygonal shape, and then the\npolygons drift together and bond to form a mesh. We demonstrate that a variety\nof polygonal meshes can be manufactured in the simulation, by simply changing\nthe sequence of machine types in the seed.", 
    "link": "http://arxiv.org/pdf/cs/0607133v1", 
    "arxiv-id": "cs/0607133v1"
},{
    "category": "cs.NA", 
    "author": "Stephen A. Vavasis", 
    "title": "A Robust Solution Procedure for Hyperelastic Solids with Large Boundary   Deformation", 
    "publish": "2006-09-01T00:07:41Z", 
    "summary": "Compressible Mooney-Rivlin theory has been used to model hyperelastic solids,\nsuch as rubber and porous polymers, and more recently for the modeling of soft\ntissues for biomedical tissues, undergoing large elastic deformations. We\npropose a solution procedure for Lagrangian finite element discretization of a\nstatic nonlinear compressible Mooney-Rivlin hyperelastic solid. We consider the\ncase in which the boundary condition is a large prescribed deformation, so that\nmesh tangling becomes an obstacle for straightforward algorithms. Our solution\nprocedure involves a largely geometric procedure to untangle the mesh: solution\nof a sequence of linear systems to obtain initial guesses for interior nodal\npositions for which no element is inverted. After the mesh is untangled, we\ntake Newton iterations to converge to a mechanical equilibrium. The Newton\niterations are safeguarded by a line search similar to one used in\noptimization. Our computational results indicate that the algorithm is up to 70\ntimes faster than a straightforward Newton continuation procedure and is also\nmore robust (i.e., able to tolerate much larger deformations). For a few\nextremely large deformations, the deformed mesh could only be computed through\nthe use of an expensive Newton continuation method while using a tight\nconvergence tolerance and taking very small steps.", 
    "link": "http://arxiv.org/pdf/cs/0609001v2", 
    "arxiv-id": "cs/0609001v2"
},{
    "category": "cs.NA", 
    "author": "Tomasz Suslo", 
    "title": "Modern Statistics by Kriging", 
    "publish": "2006-09-14T12:34:59Z", 
    "summary": "We present statistics (S-statistics) based only on random variable (not\nrandom value) with a mean squared error of mean estimation as a concept of\nerror.", 
    "link": "http://arxiv.org/pdf/cs/0609079v4", 
    "arxiv-id": "cs/0609079v4"
},{
    "category": "cs.CE", 
    "author": "Mark Tygert", 
    "title": "Recurrence relations and fast algorithms", 
    "publish": "2006-09-14T17:51:11Z", 
    "summary": "We construct fast algorithms for evaluating transforms associated with\nfamilies of functions which satisfy recurrence relations. These include\nalgorithms both for computing the coefficients in linear combinations of the\nfunctions, given the values of these linear combinations at certain points,\nand, vice versa, for evaluating such linear combinations at those points, given\nthe coefficients in the linear combinations; such procedures are also known as\nanalysis and synthesis of series of certain special functions. The algorithms\nof the present paper are efficient in the sense that their computational costs\nare proportional to n (ln n) (ln(1/epsilon))^3, where n is the amount of input\nand output data, and epsilon is the precision of computations. Stated somewhat\nmore precisely, we find a positive real number C such that, for any positive\ninteger n > 10, the algorithms require at most C n (ln n) (ln(1/epsilon))^3\nfloating-point operations and words of memory to evaluate at n appropriately\nchosen points any linear combination of n special functions, given the\ncoefficients in the linear combination, where epsilon is the precision of\ncomputations.", 
    "link": "http://arxiv.org/pdf/cs/0609081v1", 
    "arxiv-id": "cs/0609081v1"
},{
    "category": "cs.CE", 
    "author": "Samuel Malone", 
    "title": "Towards a Bayesian framework for option pricing", 
    "publish": "2006-10-10T19:37:37Z", 
    "summary": "In this paper, we describe a general method for constructing the posterior\ndistribution of an option price. Our framework takes as inputs the prior\ndistributions of the parameters of the stochastic process followed by the\nunderlying, as well as the likelihood function implied by the observed price\nhistory for the underlying. Our work extends that of Karolyi (1993) and\nDarsinos and Satchell (2001), but with the crucial difference that the\nlikelihood function we use for inference is that which is directly implied by\nthe underlying, rather than imposed in an ad hoc manner via the introduction of\na function representing \"measurement error.\" As such, an important problem\nstill relevant for our method is that of model risk, and we address this issue\nby describing how to perform a Bayesian averaging of parameter inferences based\non the different models considered using our framework.", 
    "link": "http://arxiv.org/pdf/cs/0610053v1", 
    "arxiv-id": "cs/0610053v1"
},{
    "category": "cs.NA", 
    "author": "Messaoud Bensebti", 
    "title": "Doppler Spectrum Estimation by Ramanujan Fourier Transforms", 
    "publish": "2006-10-18T14:43:45Z", 
    "summary": "The Doppler spectrum estimation of a weather radar signal in a classic way\ncan be made by two methods, temporal one based in the autocorrelation of the\nsuccessful signals, whereas the other one uses the estimation of the power\nspectral density PSD by using Fourier transforms. We introduces a new tool of\nsignal processing based on Ramanujan sums cq(n), adapted to the analysis of\narithmetical sequences with several resonances p/q. These sums are almost\nperiodic according to time n of resonances and aperiodic according to the order\nq of resonances. New results will be supplied by the use of Ramanujan Fourier\nTransform (RFT) for the estimation of the Doppler spectrum for the weather\nradar signal.", 
    "link": "http://arxiv.org/pdf/cs/0610108v2", 
    "arxiv-id": "cs/0610108v2"
},{
    "category": "cs.CE", 
    "author": "Argyn Kuketayev", 
    "title": "On numerical stability of recursive present value computation method", 
    "publish": "2006-11-13T20:37:42Z", 
    "summary": "We analyze numerical stability of a recursive computation scheme of present\nvalue (PV) amd show that the absolute error increases exponentially for\npositive discount rates. We show that reversing the direction of calculations\nin the recurrence equation yields a robust PV computation routine.", 
    "link": "http://arxiv.org/pdf/cs/0611049v2", 
    "arxiv-id": "cs/0611049v2"
},{
    "category": "cs.CE", 
    "author": "Jan W. Dash", 
    "title": "Multivariate Integral Perturbation Techniques - I (Theory)", 
    "publish": "2006-11-14T16:58:58Z", 
    "summary": "We present a quasi-analytic perturbation expansion for multivariate\nN-dimensional Gaussian integrals. The perturbation expansion is an infinite\nseries of lower-dimensional integrals (one-dimensional in the simplest\napproximation). This perturbative idea can also be applied to multivariate\nStudent-t integrals. We evaluate the perturbation expansion explicitly through\n2nd order, and discuss the convergence, including enhancement using Pade\napproximants. Brief comments on potential applications in finance are given,\nincluding options, models for credit risk and derivatives, and correlation\nsensitivities.", 
    "link": "http://arxiv.org/pdf/cs/0611061v1", 
    "arxiv-id": "cs/0611061v1"
},{
    "category": "cs.MS", 
    "author": "Cl\u00e9ment Chavant", 
    "title": "Coupling Methodology within the Software Platform Alliances", 
    "publish": "2006-11-24T16:43:06Z", 
    "summary": "CEA, ANDRA and EDF are jointly developing the software platform ALLIANCES\nwhich aim is to produce a tool for the simulation of nuclear waste storage and\ndisposal repository. This type of simulations deals with highly coupled\nthermo-hydro-mechanical and chemical (T-H-M-C) processes. A key objective of\nAlliances is to give the capability for coupling algorithms development between\nexisting codes. The aim of this paper is to present coupling methodology use in\nthe context of this software platform.", 
    "link": "http://arxiv.org/pdf/cs/0611127v1", 
    "arxiv-id": "cs/0611127v1"
},{
    "category": "cs.CE", 
    "author": "Nikolay P. Ivankov", 
    "title": "The virtual reality framework for engineering objects", 
    "publish": "2006-12-22T19:19:41Z", 
    "summary": "A framework for virtual reality of engineering objects has been developed.\nThis framework may simulate different equipment related to virtual reality.\nFramework supports 6D dynamics, ordinary differential equations, finite\nformulas, vector and matrix operations. The framework also supports embedding\nof external software.", 
    "link": "http://arxiv.org/pdf/cs/0612126v1", 
    "arxiv-id": "cs/0612126v1"
},{
    "category": "cs.DB", 
    "author": "Jim Gray", 
    "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part I: There is Life beyond Files", 
    "publish": "2007-01-25T23:02:32Z", 
    "summary": "In this paper, we show how to use a Relational Database Management System in\nsupport of Finite Element Analysis. We believe it is a new way of thinking\nabout data management in well-understood applications to prepare them for two\nmajor challenges, - size and integration (globalization). Neither extreme size\nnor integration (with other applications over the Web) was a design concern 30\nyears ago when the paradigm for FEA implementation first was formed. On the\nother hand, database technology has come a long way since its inception and it\nis past time to highlight its usefulness to the field of scientific computing\nand computer based engineering. This series aims to widen the list of\napplications for database designers and for FEA users and application\ndevelopers to reap some of the benefits of database development.", 
    "link": "http://arxiv.org/pdf/cs/0701159v1", 
    "arxiv-id": "cs/0701159v1"
},{
    "category": "cs.DB", 
    "author": "Jim Gray", 
    "title": "Supporting Finite Element Analysis with a Relational Database Backend,   Part II: Database Design and Access", 
    "publish": "2007-01-25T23:05:40Z", 
    "summary": "This is Part II of a three article series on using databases for Finite\nElement Analysis (FEA). It discusses (1) db design, (2) data loading, (3)\ntypical use cases during grid building, (4) typical use cases during simulation\n(get and put), (5) typical use cases during analysis (also done in Part III)\nand some performance measures of these cases. It argues that using a database\nis simpler to implement than custom data schemas, has better performance\nbecause it can use data parallelism, and better supports FEA modularity and\ntool evolution because database schema evolution, data independence, and\nself-defining data.", 
    "link": "http://arxiv.org/pdf/cs/0701160v1", 
    "arxiv-id": "cs/0701160v1"
},{
    "category": "cs.DB", 
    "author": "Gyorgy Fekete", 
    "title": "Using Table Valued Functions in SQL Server 2005 To Implement a Spatial   Data Library", 
    "publish": "2007-01-26T00:00:37Z", 
    "summary": "This article explains how to add spatial search functions (point-near-point\nand point in polygon) to Microsoft SQL Server 2005 using C# and table-valued\nfunctions. It is possible to use this library to add spatial search to your\napplication without writing any special code. The library implements the\npublic-domain C# Hierarchical Triangular Mesh (HTM) algorithms from Johns\nHopkins University. That C# library is connected to SQL Server 2005 via a set\nof scalar-valued and table-valued functions. These functions act as a spatial\nindex.", 
    "link": "http://arxiv.org/pdf/cs/0701163v1", 
    "arxiv-id": "cs/0701163v1"
},{
    "category": "cs.DB", 
    "author": "Jim Gray", 
    "title": "Large-Scale Query and XMatch, Entering the Parallel Zone", 
    "publish": "2007-01-26T00:33:26Z", 
    "summary": "Current and future astronomical surveys are producing catalogs with millions\nand billions of objects. On-line access to such big datasets for data mining\nand cross-correlation is usually as highly desired as unfeasible. Providing\nthese capabilities is becoming critical for the Virtual Observatory framework.\nIn this paper we present various performance tests that show how using\nRelational Database Management Systems (RDBMS) and a Zoning algorithm to\npartition and parallelize the computation, we can facilitate large-scale query\nand cross-match.", 
    "link": "http://arxiv.org/pdf/cs/0701167v1", 
    "arxiv-id": "cs/0701167v1"
},{
    "category": "cs.DB", 
    "author": "Alex Szalay", 
    "title": "Life Under Your Feet: An End-to-End Soil Ecology Sensor Network,   Database, Web Server, and Analysis Service", 
    "publish": "2007-01-26T05:08:06Z", 
    "summary": "Wireless sensor networks can revolutionize soil ecology by providing\nmeasurements at temporal and spatial granularities previously impossible. This\npaper presents a soil monitoring system we developed and deployed at an urban\nforest in Baltimore as a first step towards realizing this vision. Motes in\nthis network measure and save soil moisture and temperature in situ every\nminute. Raw measurements are periodically retrieved by a sensor gateway and\nstored in a central database where calibrated versions are derived and stored.\nThe measurement database is published through Web Services interfaces. In\naddition, analysis tools let scientists analyze current and historical data and\nhelp manage the sensor network. The article describes the system design, what\nwe learned from the deployment, and initial results obtained from the sensors.\nThe system measures soil factors with unprecedented temporal precision.\nHowever, the deployment required device-level programming, sensor calibration\nacross space and time, and cross-referencing measurements with external\nsources. The database, web server, and data analysis design required\nconsiderable innovation and expertise. So, the ratio of computer-scientists to\necologists was 3:1. Before sensor networks can fulfill their potential as\ninstruments that can be easily deployed by scientists, these technical problems\nmust be addressed so that the ratio is one nerd per ten ecologists.", 
    "link": "http://arxiv.org/pdf/cs/0701170v1", 
    "arxiv-id": "cs/0701170v1"
},{
    "category": "cs.DB", 
    "author": "Ani Thakar", 
    "title": "Cross-Matching Multiple Spatial Observations and Dealing with Missing   Data", 
    "publish": "2007-01-26T05:18:45Z", 
    "summary": "Cross-match spatially clusters and organizes several astronomical\npoint-source measurements from one or more surveys. Ideally, each object would\nbe found in each survey. Unfortunately, the observation conditions and the\nobjects themselves change continually. Even some stationary objects are missing\nin some observations; sometimes objects have a variable light flux and\nsometimes the seeing is worse. In most cases we are faced with a substantial\nnumber of differences in object detections between surveys and between\nobservations taken at different times within the same survey or instrument.\nDealing with such missing observations is a difficult problem. The first step\nis to classify misses as ephemeral - when the object moved or simply\ndisappeared, masked - when noise hid or corrupted the object observation, or\nedge - when the object was near the edge of the observational field. This\nclassification and a spatial library to represent and manipulate observational\nfootprints help construct a Match table recording both hits and misses.\nTransitive closure clusters friends-of-friends into object bundles. The bundle\nsummary statistics are recorded in a Bundle table. This design is an evolution\nof the Sloan Digital Sky Survey cross-match design that compared overlapping\nobservations taken at different times. Cross-Matching Multiple Spatial\nObservations and Dealing with Missing Data.", 
    "link": "http://arxiv.org/pdf/cs/0701172v1", 
    "arxiv-id": "cs/0701172v1"
},{
    "category": "cs.DB", 
    "author": "Brian Yanny", 
    "title": "SkyServer Traffic Report - The First Five Years", 
    "publish": "2007-01-26T05:22:15Z", 
    "summary": "The SkyServer is an Internet portal to the Sloan Digital Sky Survey Catalog\nArchive Server. From 2001 to 2006, there were a million visitors in 3 million\nsessions generating 170 million Web hits, 16 million ad-hoc SQL queries, and 62\nmillion page views. The site currently averages 35 thousand visitors and 400\nthousand sessions per month. The Web and SQL logs are public. We analyzed\ntraffic and sessions by duration, usage pattern, data product, and client type\n(mortal or bot) over time. The analysis shows (1) the site's popularity, (2)\nthe educational website that delivered nearly fifty thousand hours of\ninteractive instruction, (3) the relative use of interactive, programmatic, and\nbatch-local access, (4) the success of offering ad-hoc SQL, personal database,\nand batch job access to scientists as part of the data publication, (5) the\ncontinuing interest in \"old\" datasets, (6) the usage of SQL constructs, and (7)\na novel approach of using the corpus of correct SQL queries to suggest similar\nbut correct statements when a user presents an incorrect SQL statement.", 
    "link": "http://arxiv.org/pdf/cs/0701173v1", 
    "arxiv-id": "cs/0701173v1"
},{
    "category": "cs.CE", 
    "author": "Jason M. Reese", 
    "title": "AICA: a New Pair Force Evaluation Method for Parallel Molecular Dynamics   in Arbitrary Geometries", 
    "publish": "2007-02-22T17:27:05Z", 
    "summary": "A new algorithm for calculating intermolecular pair forces in Molecular\nDynamics (MD) simulations on a distributed parallel computer is presented. The\nArbitrary Interacting Cells Algorithm (AICA) is designed to operate on\ngeometrical domains defined by an unstructured, arbitrary polyhedral mesh,\nwhich has been spatially decomposed into irregular portions for\nparallelisation. It is intended for nano scale fluid mechanics simulation by MD\nin complex geometries, and to provide the MD component of a hybrid MD/continuum\nsimulation. AICA has been implemented in the open-source computational toolbox\nOpenFOAM, and verified against a published MD code.", 
    "link": "http://arxiv.org/pdf/cs/0702131v1", 
    "arxiv-id": "cs/0702131v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "First Passage Time for Multivariate Jump-diffusion Stochastic Models   With Applications in Finance", 
    "publish": "2007-02-28T10:39:15Z", 
    "summary": "The ``first passage-time'' (FPT) problem is an important problem with a wide\nrange of applications in mathematics, physics, biology and finance.\nMathematically, such a problem can be reduced to estimating the probability of\na (stochastic) process first to reach a critical level or threshold. While in\nother areas of applications the FPT problem can often be solved analytically,\nin finance we usually have to resort to the application of numerical\nprocedures, in particular when we deal with jump-diffusion stochastic processes\n(JDP). In this paper, we develop a Monte-Carlo-based methodology for the\nsolution of the FPT problem in the context of a multivariate jump-diffusion\nstochastic process. The developed methodology is tested by using different\nparameters, the simulation results indicate that the developed methodology is\nmuch more efficient than the conventional Monte Carlo method. It is an\nefficient tool for further practical applications, such as the analysis of\ndefault correlation and predicting barrier options in finance.", 
    "link": "http://arxiv.org/pdf/cs/0702163v1", 
    "arxiv-id": "cs/0702163v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Monte-Carlo Simulations of the First Passage Time for Multivariate   Jump-Diffusion Processes in Financial Applications", 
    "publish": "2007-02-28T10:51:16Z", 
    "summary": "Many problems in finance require the information on the first passage time\n(FPT) of a stochastic process. Mathematically, such problems are often reduced\nto the evaluation of the probability density of the time for such a process to\ncross a certain level, a boundary, or to enter a certain region. While in other\nareas of applications the FPT problem can often be solved analytically, in\nfinance we usually have to resort to the application of numerical procedures,\nin particular when we deal with jump-diffusion stochastic processes (JDP). In\nthis paper, we propose a Monte-Carlo-based methodology for the solution of the\nfirst passage time problem in the context of multivariate (and correlated)\njump-diffusion processes. The developed technique provide an efficient tool for\na number of applications, including credit risk and option pricing. We\ndemonstrate its applicability to the analysis of the default rates and default\ncorrelations of several different, but correlated firms via a set of empirical\ndata.", 
    "link": "http://arxiv.org/pdf/cs/0702164v1", 
    "arxiv-id": "cs/0702164v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Efficient estimation of default correlation for multivariate   jump-diffusion processes", 
    "publish": "2007-02-28T11:22:18Z", 
    "summary": "Evaluation of default correlation is an important task in credit risk\nanalysis. In many practical situations, it concerns the joint defaults of\nseveral correlated firms, the task that is reducible to a first passage time\n(FPT) problem. This task represents a great challenge for jump-diffusion\nprocesses (JDP), where except for very basic cases, there are no analytical\nsolutions for such problems. In this contribution, we generalize our previous\nfast Monte-Carlo method (non-correlated jump-diffusion cases) for multivariate\n(and correlated) jump-diffusion processes. This generalization allows us, among\nother things, to evaluate the default events of several correlated assets based\non a set of empirical data. The developed technique is an efficient tool for a\nnumber of other applications, including credit risk and option pricing.", 
    "link": "http://arxiv.org/pdf/cs/0702165v1", 
    "arxiv-id": "cs/0702165v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Solving Stochastic Differential Equations with Jump-Diffusion   Efficiently: Applications to FPT Problems in Credit Risk", 
    "publish": "2007-02-28T11:48:12Z", 
    "summary": "The first passage time (FPT) problem is ubiquitous in many applications. In\nfinance, we often have to deal with stochastic processes with jump-diffusion,\nso that the FTP problem is reducible to a stochastic differential equation with\njump-diffusion. While the application of the conventional Monte-Carlo procedure\nis possible for the solution of the resulting model, it becomes computationally\ninefficient which severely restricts its applicability in many practically\ninteresting cases. In this contribution, we focus on the development of\nefficient Monte-Carlo-based computational procedures for solving the FPT\nproblem under the multivariate (and correlated) jump-diffusion processes. We\nalso discuss the implementation of the developed Monte-Carlo-based technique\nfor multivariate jump-diffusion processes driving by several compound Poisson\nshocks. Finally, we demonstrate the application of the developed methodologies\nfor analyzing the default rates and default correlations of differently rated\nfirms via historical data.", 
    "link": "http://arxiv.org/pdf/cs/0702166v1", 
    "arxiv-id": "cs/0702166v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Finite Volume Analysis of Nonlinear Thermo-mechanical Dynamics of Shape   Memory Alloys", 
    "publish": "2007-02-28T13:00:33Z", 
    "summary": "In this paper, the finite volume method is developed to analyze coupled\ndynamic problems of nonlinear thermoelasticity. The major focus is given to the\ndescription of martensitic phase transformations essential in the modelling of\nshape memory alloys. Computational experiments are carried out to study the\nthermo-mechanical wave interactions in a shape memory alloy rod, and a patch.\nBoth mechanically and thermally induced phase transformations, as well as\nhysteresis effects, in a one-dimensional structure are successfully simulated\nwith the developed methodology. In the two-dimensional case, the main focus is\ngiven to square-to-rectangular transformations and examples of martensitic\ncombinations under different mechanical loadings are provided.", 
    "link": "http://arxiv.org/pdf/cs/0702167v1", 
    "arxiv-id": "cs/0702167v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Simulation of Phase Combinations in Shape Memory Alloys Patches by   Hybrid Optimization Methods", 
    "publish": "2007-02-28T14:09:42Z", 
    "summary": "In this paper, phase combinations among martensitic variants in shape memory\nalloys patches and bars are simulated by a hybrid optimization methodology. The\nmathematical model is based on the Landau theory of phase transformations. Each\nstable phase is associated with a local minimum of the free energy function,\nand the phase combinations are simulated by minimizing the bulk energy. At low\ntemperature, the free energy function has double potential wells leading to\nnon-convexity of the optimization problem. The methodology proposed in the\npresent paper is based on an initial estimate of the global solution by a\ngenetic algorithm, followed by a refined quasi-Newton procedure to locally\nrefine the optimum. By combining the local and global search algorithms, the\nphase combinations are successfully simulated. Numerical experiments are\npresented for the phase combinations in a SMA patch under several typical\nmechanical loadings.", 
    "link": "http://arxiv.org/pdf/cs/0702168v1", 
    "arxiv-id": "cs/0702168v1"
},{
    "category": "cs.CE", 
    "author": "Roderick V. N. Melnik", 
    "title": "Numerical Model For Vibration Damping Resulting From the First Order   Phase Transformations", 
    "publish": "2007-02-28T18:31:19Z", 
    "summary": "A numerical model is constructed for modelling macroscale damping effects\ninduced by the first order martensite phase transformations in a shape memory\nalloy rod. The model is constructed on the basis of the modified\nLandau-Ginzburg theory that couples nonlinear mechanical and thermal fields.\nThe free energy function for the model is constructed as a double well function\nat low temperature, such that the external energy can be absorbed during the\nphase transformation and converted into thermal form. The Chebyshev spectral\nmethods are employed together with backward differentiation for the numerical\nanalysis of the problem. Computational experiments performed for different\nvibration energies demonstrate the importance of taking into account damping\neffects induced by phase transformations.", 
    "link": "http://arxiv.org/pdf/cs/0702172v1", 
    "arxiv-id": "cs/0702172v1"
},{
    "category": "math.NA", 
    "author": "Mario A. Storti", 
    "title": "Hot-pressing process modeling for medium density fiberboard (MDF)", 
    "publish": "2000-10-17T17:04:00Z", 
    "summary": "In this paper we present a numerical solution for the mathematical modeling\nof the hot-pressing process applied to medium density fiberboard. The model is\nbased in the work of Humphrey[82], Humphrey and Bolton[89] and Carvalho and\nCosta[98], with some modifications and extensions in order to take into account\nmainly the convective effects on the phase change term and also a conservative\nnumerical treatment of the resulting system of partial differential equations.", 
    "link": "http://arxiv.org/pdf/math/0010173v4", 
    "arxiv-id": "math/0010173v4"
},{
    "category": "math.NA", 
    "author": "Kevin T. Chu", 
    "title": "A Direct Matrix Method for Computing Analytical Jacobians of Discretized   Nonlinear Integro-differential Equations", 
    "publish": "2007-02-05T19:03:57Z", 
    "summary": "In this pedagogical article, we present a simple direct matrix method for\nanalytically computing the Jacobian of nonlinear algebraic equations that arise\nfrom the discretization of nonlinear integro-differential equations. The method\nis based on a formulation of the discretized equations in vector form using\nonly matrix-vector products and component-wise operations. By applying simple\nmatrix-based differentiation rules, the matrix form of the analytical Jacobian\ncan be calculated with little more difficulty than that required when computing\nderivatives in single-variable calculus. After describing the direct matrix\nmethod, we present numerical experiments demonstrating the computational\nperformance of the method, discuss its connection to the Newton-Kantorovich\nmethod, and apply it to illustrative 1D and 2D example problems. MATLAB code is\nprovided to demonstrate the low code complexity required by the method.", 
    "link": "http://arxiv.org/pdf/math/0702116v2", 
    "arxiv-id": "math/0702116v2"
},{
    "category": "nlin.AO", 
    "author": "Martin Pieck", 
    "title": "The dynamics of iterated transportation simulations", 
    "publish": "2000-02-23T01:42:08Z", 
    "summary": "Iterating between a router and a traffic micro-simulation is an increasibly\naccepted method for doing traffic assignment. This paper, after pointing out\nthat the analytical theory of simulation-based assignment to-date is\ninsufficient for some practical cases, presents results of simulation studies\nfrom a real world study. Specifically, we look into the issues of uniqueness,\nvariability, and robustness and validation. Regarding uniqueness, despite some\ncautionary notes from a theoretical point of view, we find no indication of\n``meta-stable'' states for the iterations. Variability however is considerable.\nBy variability we mean the variation of the simulation of a given plan set by\njust changing the random seed. We show then results from three different\nmicro-simulations under the same iteration scenario in order to test for the\nrobustness of the results under different implementations. We find the results\nencouraging, also when comparing to reality and with a traditional assignment\nresult.\n  Keywords: dynamic traffic assignment (DTA); traffic micro-simulation;\nTRANSIMS; large-scale simulations; urban planning", 
    "link": "http://arxiv.org/pdf/nlin/0002040v1", 
    "arxiv-id": "nlin/0002040v1"
},{
    "category": "quant-ph", 
    "author": "John Robert Burger", 
    "title": "New Approachs to Quantum Computer Simulaton in a Classical Supercomputer", 
    "publish": "2003-08-28T16:02:25Z", 
    "summary": "Classical simulation is important because it sets a benchmark for quantum\ncomputer performance. Classical simulation is currently the only way to\nexercise larger numbers of qubits. To achieve larger simulations, sparse matrix\nprocessing is emphasized below while trading memory for processing. It\nperformed well within NCSA supercomputers, giving a state vector in convenient\ncontinuous portions ready for post processing.", 
    "link": "http://arxiv.org/pdf/quant-ph/0308158v1", 
    "arxiv-id": "quant-ph/0308158v1"
},{
    "category": "cs.CE", 
    "author": "Enrique ter Horst", 
    "title": "Assessment and Propagation of Input Uncertainty in Tree-based Option   Pricing Models", 
    "publish": "2007-04-13T14:48:41Z", 
    "summary": "This paper aims to provide a practical example on the assessment and\npropagation of input uncertainty for option pricing when using tree-based\nmethods. Input uncertainty is propagated into output uncertainty, reflecting\nthat option prices are as unknown as the inputs they are based on. Option\npricing formulas are tools whose validity is conditional not only on how close\nthe model represents reality, but also on the quality of the inputs they use,\nand those inputs are usually not observable. We provide three alternative\nframeworks to calibrate option pricing tree models, propagating parameter\nuncertainty into the resulting option prices. We finally compare our methods\nwith classical calibration-based results assuming that there is no options\nmarket established. These methods can be applied to pricing of instruments for\nwhich there is not an options market, as well as a methodological tool to\naccount for parameter and model uncertainty in theoretical option pricing.", 
    "link": "http://arxiv.org/pdf/0704.1768v1", 
    "arxiv-id": "0704.1768v1"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm", 
    "publish": "2007-05-09T07:08:58Z", 
    "summary": "A method based on Bayesian neural networks and genetic algorithm is proposed\nto control the fermentation process. The relationship between input and output\nvariables is modelled using Bayesian neural network that is trained using\nhybrid Monte Carlo method. A feedback loop based on genetic algorithm is used\nto change input variables so that the output variables are as close to the\ndesired target as possible without the loss of confidence level on the\nprediction that the neural network gives. The proposed procedure is found to\nreduce the distance between the desired target and measured outputs\nsignificantly.", 
    "link": "http://arxiv.org/pdf/0705.1214v1", 
    "arxiv-id": "0705.1214v1"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "Evolutionary Optimisation Methods for Template Based Image Registration", 
    "publish": "2007-05-11T15:51:36Z", 
    "summary": "This paper investigates the use of evolutionary optimisation techniques to\nregister a template with a scene image. An error function is created to measure\nthe correspondence of the template to the image. The problem presented here is\nto optimise the horizontal, vertical and scaling parameters that register the\ntemplate with the scene. The Genetic Algorithm, Simulated Annealing and\nParticle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation\nwith starting points chosen in a pre-processing stage. The paper investigates\nthe precision and accuracy of each method and shows that all four methods\nperform favourably for image registration. SA is the most precise, GA is the\nmost accurate. PSO is a good mix of both and the Simplex method returns local\nminima the most. A pre-processing stage should be investigated for the\nevolutionary methods in order to improve performance. Discrete versions of the\noptimisation methods should be investigated to further improve computational\nperformance.", 
    "link": "http://arxiv.org/pdf/0705.1674v1", 
    "arxiv-id": "0705.1674v1"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "Option Pricing Using Bayesian Neural Networks", 
    "publish": "2007-05-11T15:55:31Z", 
    "summary": "Options have provided a field of much study because of the complexity\ninvolved in pricing them. The Black-Scholes equations were developed to price\noptions but they are only valid for European styled options. There is added\ncomplexity when trying to price American styled options and this is why the use\nof neural networks has been proposed. Neural Networks are able to predict\noutcomes based on past data. The inputs to the networks here are stock\nvolatility, strike price and time to maturity with the output of the network\nbeing the call option price. There are two techniques for Bayesian neural\nnetworks used. One is Automatic Relevance Determination (for Gaussian\nApproximation) and one is a Hybrid Monte Carlo method, both used with\nMulti-Layer Perceptrons.", 
    "link": "http://arxiv.org/pdf/0705.1680v1", 
    "arxiv-id": "0705.1680v1"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "Dynamic Model Updating Using Particle Swarm Optimization Method", 
    "publish": "2007-05-12T10:27:07Z", 
    "summary": "This paper proposes the use of particle swarm optimization method (PSO) for\nfinite element (FE) model updating. The PSO method is compared to the existing\nmethods that use simulated annealing (SA) or genetic algorithms (GA) for FE\nmodel for model updating. The proposed method is tested on an unsymmetrical\nH-shaped structure. It is observed that the proposed method gives updated\nnatural frequencies the most accurate and followed by those given by an updated\nmodel that was obtained using the GA and a full FE model. It is also observed\nthat the proposed method gives updated mode shapes that are best correlated to\nthe measured ones, followed by those given by an updated model that was\nobtained using the SA and a full FE model. Furthermore, it is observed that the\nPSO achieves this accuracy at a computational speed that is faster than that by\nthe GA and a full FE model which is faster than the SA and a full FE model.", 
    "link": "http://arxiv.org/pdf/0705.1760v1", 
    "arxiv-id": "0705.1760v1"
},{
    "category": "cs.CE", 
    "author": "Neil F. Johnson", 
    "title": "Inferring the Composition of a Trader Population in a Financial Market", 
    "publish": "2007-06-06T17:29:42Z", 
    "summary": "We discuss a method for predicting financial movements and finding pockets of\npredictability in the price-series, which is built around inferring the\nheterogeneity of trading strategies in a multi-agent trader population. This\nwork explores extensions to our previous framework (arXiv:physics/0506134).\nHere we allow for more intelligent agents possessing a richer strategy set, and\nwe no longer constrain the estimate for the heterogeneity of the agents to a\nprobability space. We also introduce a scheme which allows the incorporation of\nmodels with a wide variety of agent types, and discuss a mechanism for the\nremoval of bias from relevant parameters.", 
    "link": "http://arxiv.org/pdf/0706.0870v1", 
    "arxiv-id": "0706.0870v1"
},{
    "category": "cs.CE", 
    "author": "Eric Darve", 
    "title": "N-Body Simulations on GPUs", 
    "publish": "2007-06-20T21:02:14Z", 
    "summary": "Commercial graphics processors (GPUs) have high compute capacity at very low\ncost, which makes them attractive for general purpose scientific computing. In\nthis paper we show how graphics processors can be used for N-body simulations\nto obtain improvements in performance over current generation CPUs. We have\ndeveloped a highly optimized algorithm for performing the O(N^2) force\ncalculations that constitute the major part of stellar and molecular dynamics\nsimulations. In some of the calculations, we achieve sustained performance of\nnearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to\nspecialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the\ncost. Furthermore, the wide availability of GPUs has significant implications\nfor cluster computing and distributed computing efforts like Folding@Home.", 
    "link": "http://arxiv.org/pdf/0706.3060v1", 
    "arxiv-id": "0706.3060v1"
},{
    "category": "cs.CE", 
    "author": "L. A. Nunez", 
    "title": "e-Science initiatives in Venezuela", 
    "publish": "2007-07-24T12:00:43Z", 
    "summary": "Within the context of the nascent e-Science infrastructure in Venezuela, we\ndescribe several web-based scientific applications developed at the Centro\nNacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida,\nand at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas.\nThe different strategies that have been followed for implementing quantum\nchemistry and atomic physics applications are presented. We also briefly\ndiscuss a damage portal based on dynamic, nonlinear, finite elements of lumped\ndamage mechanics and a biomedical portal developed within the framework of the\n\\textit{E-Infrastructure shared between Europe and Latin America} (EELA)\ninitiative for searching common sequences and inferring their functions in\nparasitic diseases such as leishmaniasis, chagas and malaria.", 
    "link": "http://arxiv.org/pdf/0707.3531v1", 
    "arxiv-id": "0707.3531v1"
},{
    "category": "cs.CE", 
    "author": "Florian Simatos", 
    "title": "A variant of the Recoil Growth algorithm to generate multi-polymer   systems", 
    "publish": "2007-08-08T15:07:53Z", 
    "summary": "The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the\nmost efficient algorithm available in the literature to sample from a\nmulti-polymer system. Such problems are closely related to the generation of\nself-avoiding paths. In this paper, we study a variant of the original Recoil\nGrowth algorithm, where we constrain the generation of a new polymer to take\nplace on a specific class of graphs. This makes it possible to make a fine\ntrade-off between computational cost and success rate. We moreover give a\nsimple proof for a lower bound on the irreducibility of this new algorithm,\nwhich applies to the original algorithm as well.", 
    "link": "http://arxiv.org/pdf/0708.1116v3", 
    "arxiv-id": "0708.1116v3"
},{
    "category": "cs.CE", 
    "author": "D. Hui", 
    "title": "Computational Simulation and 3D Virtual Reality Engineering Tools for   Dynamical Modeling and Imaging of Composite Nanomaterials", 
    "publish": "2007-08-14T08:17:45Z", 
    "summary": "An adventure at engineering design and modeling is possible with a Virtual\nReality Environment (VRE) that uses multiple computer-generated media to let a\nuser experience situations that are temporally and spatially prohibiting. In\nthis paper, an approach to developing some advanced architecture and modeling\ntools is presented to allow multiple frameworks work together while being\nshielded from the application program. This architecture is being developed in\na framework of workbench interactive tools for next generation\nnanoparticle-reinforced damping/dynamic systems. Through the use of system, an\nengineer/programmer can respectively concentrate on tailoring an engineering\ndesign concept of novel system and the application software design while using\nexisting databases/software outputs.", 
    "link": "http://arxiv.org/pdf/0708.1818v1", 
    "arxiv-id": "0708.1818v1"
},{
    "category": "cs.CE", 
    "author": "Juan Gonzalez", 
    "title": "A Neural Networks Model of the Venezuelan Economy", 
    "publish": "2007-08-26T05:10:29Z", 
    "summary": "Besides an indicator of the GDP, the Central Bank of Venezuela generates the\nso called Monthly Economic Activity General Indicator. The a priori knowledge\nof this indicator, which represents and sometimes even anticipates the\neconomy's fluctuations, could be helpful in developing public policies and in\ninvestment decision making. The purpose of this study is forecasting the IGAEM\nthrough non parametric methods, an approach that has proven effective in a wide\nvariety of problems in economics and finance.", 
    "link": "http://arxiv.org/pdf/0708.3463v1", 
    "arxiv-id": "0708.3463v1"
},{
    "category": "cs.CE", 
    "author": "William Moreno", 
    "title": "A Non Parametric Study of the Volatility of the Economy as a Country   Risk Predictor", 
    "publish": "2007-08-26T05:30:18Z", 
    "summary": "This paper intends to explain Venezuela's country spread behavior through the\nNeural Networks analysis of a monthly economic activity general index of\neconomic indicators constructed by the Central Bank of Venezuela, a measure of\nthe shocks affecting country risk of emerging markets and the U.S. short term\ninterest rate. The use of non parametric methods allowed the finding of non\nlinear relationship between these inputs and the country risk. The networks\nperformance was evaluated using the method of excess predictability.", 
    "link": "http://arxiv.org/pdf/0708.3464v1", 
    "arxiv-id": "0708.3464v1"
},{
    "category": "cs.CE", 
    "author": "Hender Prato", 
    "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices", 
    "publish": "2007-08-28T18:29:55Z", 
    "summary": "A neural net model for forecasting the prices of Venezuelan crude oil is\nproposed. The inputs of the neural net are selected by reference to a dynamic\nsystem model of oil prices by Mashayekhi (1995, 2001) and its performance is\nevaluated using two criteria: the Excess Profitability test by Anatoliev and\nGerko (2005) and the characteristics of the equity curve generated by a trading\nstrategy based on the neural net predictions.\n  -----\n  Se introduce aqui un modelo no parametrico para pronosticar los precios del\npetroleo Venezolano cuyos insumos son seleccionados en base a un sistema\ndinamico que explica los precios en terminos de dichos insumos. Se describe el\nproceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se\nevaluan sus pronosticos a traves de un test estadistico de predictibilidad y de\nlas caracteristicas del Equity Curve inducido por la estrategia de compraventa\nbursatil generada por dichos pronosticos.", 
    "link": "http://arxiv.org/pdf/0708.3829v1", 
    "arxiv-id": "0708.3829v1"
},{
    "category": "cs.CE", 
    "author": "Michel O. Deville", 
    "title": "Solution of moving-boundary problems by the spectral element method", 
    "publish": "2007-09-04T14:51:56Z", 
    "summary": "This paper describes a novel numerical model aiming at solving\nmoving-boundary problems such as free-surface flows or fluid-structure\ninteraction. This model uses a moving-grid technique to solve the\nNavier--Stokes equations expressed in the arbitrary Lagrangian--Eulerian\nkinematics. The discretization in space is based on the spectral element\nmethod. The coupling of the fluid equations and the moving-grid equations is\nessentially done through the conditions on the moving boundaries. Two- and\nthree-dimensional simulations are presented: translation and rotation of a\ncylinder in a fluid, and large-amplitude sloshing in a rectangular tank. The\naccuracy and robustness of the present numerical model is studied and\ndiscussed.", 
    "link": "http://arxiv.org/pdf/0709.0355v1", 
    "arxiv-id": "0709.0355v1"
},{
    "category": "cs.CE", 
    "author": "J. -Emeterio Navarro", 
    "title": "Adaptive Investment Strategies For Periodic Environments", 
    "publish": "2007-09-27T19:04:00Z", 
    "summary": "In this paper, we present an adaptive investment strategy for environments\nwith periodic returns on investment. In our approach, we consider an investment\nmodel where the agent decides at every time step the proportion of wealth to\ninvest in a risky asset, keeping the rest of the budget in a risk-free asset.\nEvery investment is evaluated in the market via a stylized return on investment\nfunction (RoI), which is modeled by a stochastic process with unknown\nperiodicities and levels of noise. For comparison reasons, we present two\nreference strategies which represent the case of agents with zero-knowledge and\ncomplete-knowledge of the dynamics of the returns. We consider also an\ninvestment strategy based on technical analysis to forecast the next return by\nfitting a trend line to previous received returns. To account for the\nperformance of the different strategies, we perform some computer experiments\nto calculate the average budget that can be obtained with them over a certain\nnumber of time steps. To assure for fair comparisons, we first tune the\nparameters of each strategy. Afterwards, we compare the performance of these\nstrategies for RoIs with different periodicities and levels of noise.", 
    "link": "http://arxiv.org/pdf/0709.4464v2", 
    "arxiv-id": "0709.4464v2"
},{
    "category": "cs.CE", 
    "author": "Philip B. Alipour", 
    "title": "Theoretical Engineering and Satellite Comlink of a PTVD-SHAM System", 
    "publish": "2007-10-01T09:35:30Z", 
    "summary": "This paper focuses on super helical memory system's design, 'Engineering,\nArchitectural and Satellite Communications' as a theoretical approach of an\ninvention-model to 'store time-data'. The current release entails three\nconcepts: 1- an in-depth theoretical physics engineering of the chip including\nits, 2- architectural concept based on VLSI methods, and 3- the time-data\nversus data-time algorithm. The 'Parallel Time Varying & Data Super-helical\nAccess Memory' (PTVD-SHAM), possesses a waterfall effect in its architecture\ndealing with the process of voltage output-switch into diverse logic and\nquantum states described as 'Boolean logic & image-logic', respectively.\nQuantum dot computational methods are explained by utilizing coiled carbon\nnanotubes (CCNTs) and CNT field effect transistors (CNFETs) in the chip's\narchitecture. Quantum confinement, categorized quantum well substrate, and\nB-field flux involvements are discussed in theory. Multi-access of coherent\nsequences of 'qubit addressing' in any magnitude, gained as pre-defined, here\ne.g., the 'big O notation' asymptotically confined into singularity while\npossessing a magnitude of 'infinity' for the orientation of array displacement.\nGaussian curvature of k<0 versus k'>(k<0) is debated in aim of specifying the\n2D electron gas characteristics, data storage system for defining short and\nlong time cycles for different CCNT diameters where space-time continuum is\nfolded by chance for the particle. Precise pre/post data timing for, e.g.,\nseismic waves before earthquake mantle-reach event occurrence, including time\nvarying self-clocking devices in diverse geographic locations for radar systems\nis illustrated in the Subsections of the paper. The theoretical fabrication\nprocess, electromigration between chip's components is discussed as well.", 
    "link": "http://arxiv.org/pdf/0710.0244v1", 
    "arxiv-id": "0710.0244v1"
},{
    "category": "cs.CE", 
    "author": "Philip B. Alipour", 
    "title": "The Theory of Unified Relativity for a Biovielectroluminescence   Phenomenon via Fly's Visual and Imaging System", 
    "publish": "2007-10-01T23:55:50Z", 
    "summary": "The elucidation upon fly's neuronal patterns as a link to computer graphics\nand memory cards I/O's, is investigated for the phenomenon by propounding a\nunified theory of Einstein's two known relativities. It is conclusive that\nflies could contribute a certain amount of neuromatrices indicating an imagery\nfunction of a visual-computational system into computer graphics and storage\nsystems. The visual system involves the time aspect, whereas flies possess\nfaster pulses compared to humans' visual ability due to the E-field state on an\nactive fly's eye surface. This behaviour can be tested on a dissected fly\nspecimen at its ommatidia. Electro-optical contacts and electrodes are wired\nthrough the flesh forming organic emitter layer to stimulate light emission,\nthereby to a computer circuit. The next step is applying a threshold voltage\nwith secondary voltages to the circuit denoting an array of essential\nelectrodes for bit switch. As a result, circuit's dormant pulses versus active\npulses at the specimen's area are recorded. The outcome matrix possesses a\nconstruction of RGB and time radicals expressing the time problem in\nconsumption, allocating time into computational algorithms, enhancing the\ntechnology far beyond. The obtained formulation generates consumed distance\ncons(x), denoting circuital travel between data source/sink for pixel data and\nbendable wavelengths. Once 'image logic' is in place, incorporating this point\nof graphical acceleration permits one to enhance graphics and optimize\nimmensely central processing, data transmissions between memory and computer\nvisual system. The phenomenon can be mainly used in 360-deg. display/viewing,\n3D scanning techniques, military and medicine, a robust and cheap substitution\nfor e.g. pre-motion pattern analysis, real-time rendering and LCDs.", 
    "link": "http://arxiv.org/pdf/0710.0410v1", 
    "arxiv-id": "0710.0410v1"
},{
    "category": "cs.CE", 
    "author": "Derek Abbott", 
    "title": "Numerical removal of water-vapor effects from THz-TDS measurements", 
    "publish": "2007-10-19T02:17:35Z", 
    "summary": "One source of disturbance in a pulsed T-ray signal is attributed to ambient\nwater vapor. Water molecules in the gas phase selectively absorb T-rays at\ndiscrete frequencies corresponding to their molecular rotational transitions.\nThis results in prominent resonances spread over the T-ray spectrum, and in the\ntime domain the T-ray signal is observed as fluctuations after the main pulse.\nThese effects are generally undesired, since they may mask critical\nspectroscopic data. So, ambient water vapor is commonly removed from the T-ray\npath by using a closed chamber during the measurement. Yet, in some\napplications a closed chamber is not applicable. This situation, therefore,\nmotivates the need for another method to reduce these unwanted artifacts. This\npaper presents a study on a computational means to address the problem.\nInitially, a complex frequency response of water vapor is modeled from a\nspectroscopic catalog. Using a deconvolution technique, together with fine\ntuning of the strength of each resonance, parts of the water-vapor response are\nremoved from a measured T-ray signal, with minimal signal distortion.", 
    "link": "http://arxiv.org/pdf/0710.3621v1", 
    "arxiv-id": "0710.3621v1"
},{
    "category": "cs.CV", 
    "author": "Philip B. Alipour", 
    "title": "Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision", 
    "publish": "2007-11-06T19:41:22Z", 
    "summary": "In October 2007, a Research Proposal for the University of Sydney, Australia,\nthe author suggested that biovie-physical phenomenon as `electrodynamic\ndependant biological vision', is governed by relativistic quantum laws and\nbiovision. The phenomenon on the basis of `biovielectroluminescence', satisfies\nman/microbio/megabio/computer vision (MMMCV), as a robust candidate for\nphysical and visual sciences. The general aim of this addendum is to present a\nrefined text of Sections 1-3 of that proposal and highlighting the contents of\nits Appendix in form of a `Mechanisms' Section. We then briefly remind in an\narticle aimed for December 2007, by appending two more equations into Section\n3, a theoretical II-time scenario as a time model well-proposed for the\nphenomenon. The time model within the core of the proposal, plays a significant\nrole in emphasizing the principle points on Objectives no. 1-8, Sub-hypothesis\n3.1.2, mentioned in Article [arXiv:0710.0410]. It also expresses the time\nconcept in terms of causing quantized energy f(|E|) of time |t|, emit in regard\nto shortening the probability of particle loci as predictable patterns of\nparticle's un-occurred motion, a solution to Heisenberg's uncertainty principle\n(HUP) into a simplistic manner. We conclude that, practical frames via a time\nalgorithm to this model, fixates such predictable patterns of motion of scenery\nbodies onto recordable observation points of a MMMCV system. It even\nsuppresses/predicts superposition phenomena coming from a human subject and/or\nother bio-subjects for any decision making event, e.g., brainwave quantum\npatterns based on vision. Maintaining the existential probability of Riemann\nsurfaces of II-time scenarios in the context of biovielectroluminescence, makes\nmotion-prediction a possibility.", 
    "link": "http://arxiv.org/pdf/0711.0784v1", 
    "arxiv-id": "0711.0784v1"
},{
    "category": "cs.NE", 
    "author": "Jingpeng Li", 
    "title": "An Estimation of Distribution Algorithm with Intelligent Local Search   for Rule-based Nurse Rostering", 
    "publish": "2007-11-22T15:16:21Z", 
    "summary": "This paper proposes a new memetic evolutionary algorithm to achieve explicit\nlearning in rule-based nurse rostering, which involves applying a set of\nheuristic rules for each nurse's assignment. The main framework of the\nalgorithm is an estimation of distribution algorithm, in which an ant-miner\nmethodology improves the individual solutions produced in each generation.\nUnlike our previous work (where learning is implicit), the learning in the\nmemetic estimation of distribution algorithm is explicit, i.e. we are able to\nidentify building blocks directly. The overall approach learns by building a\nprobabilistic model, i.e. an estimation of the probability distribution of\nindividual nurse-rule pairs that are used to construct schedules. The local\nsearch processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive\nhigher rewards. A challenging real world nurse rostering problem is used as the\ntest problem. Computational results show that the proposed approach outperforms\nmost existing approaches. It is suggested that the learning methodologies\nsuggested in this paper may be applied to other scheduling problems where\nschedules are built systematically according to specific rules", 
    "link": "http://arxiv.org/pdf/0711.3591v2", 
    "arxiv-id": "0711.3591v2"
},{
    "category": "q-bio.PE", 
    "author": "Gabriel Valiente", 
    "title": "A Perl Package and an Alignment Tool for Phylogenetic Networks", 
    "publish": "2007-11-22T18:05:49Z", 
    "summary": "Phylogenetic networks are a generalization of phylogenetic trees that allow\nfor the representation of evolutionary events acting at the population level,\nlike recombination between genes, hybridization between lineages, and lateral\ngene transfer. While most phylogenetics tools implement a wide range of\nalgorithms on phylogenetic trees, there exist only a few applications to work\nwith phylogenetic networks, and there are no open-source libraries either.\n  In order to improve this situation, we have developed a Perl package that\nrelies on the BioPerl bundle and implements many algorithms on phylogenetic\nnetworks. We have also developed a Java applet that makes use of the\naforementioned Perl package and allows the user to make simple experiments with\nphylogenetic networks without having to develop a program or Perl script by\nherself.\n  The Perl package has been accepted as part of the BioPerl bundle. It can be\ndownloaded from http://dmi.uib.es/~gcardona/BioInfo/Bio-PhyloNetwork.tgz. The\nweb-based application is available at http://dmi.uib.es/~gcardona/BioInfo/. The\nPerl package includes full documentation of all its features.", 
    "link": "http://arxiv.org/pdf/0711.3628v1", 
    "arxiv-id": "0711.3628v1"
},{
    "category": "cs.CE", 
    "author": "Jinshan Zhang", 
    "title": "Report on \"American Option Pricing and Hedging Strategies\"", 
    "publish": "2007-11-27T18:34:40Z", 
    "summary": "This paper mainly discusses the American option's hedging strategies via\nbinomialmodel and the basic idea of pricing and hedging American option.\nAlthough the essential scheme of hedging is almost the same as European option,\nsmall differences may arise when simulating the process for American option\nholder has more rights, spelling that the option can be exercised at anytime\nbefore its maturity. Our method is dynamic-hedging method.", 
    "link": "http://arxiv.org/pdf/0711.4324v1", 
    "arxiv-id": "0711.4324v1"
},{
    "category": "cs.MS", 
    "author": "Benjamin Dauvergne", 
    "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation   Model OPA with the Automatic Differentiation tool TAPENADE", 
    "publish": "2007-11-28T08:04:18Z", 
    "summary": "The ocean general circulation model OPA is developed by the LODYC team at\nParis VI university. OPA has recently undergone a major rewriting, migrating to\nFORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the\nadjoint of OPA was written by hand at a high development cost. We use the\nAutomatic Differentiation tool TAPENADE to build mechanicaly the tangent and\nadjoint codes of OPA. We validate the differentiated codes by comparison with\ndivided differences, and also with an identical twin experiment. We apply\nstate-of-the-art methods to improve the performance of the adjoint code. In\nparticular we implement the Griewank and Walther's binomial checkpointing\nalgorithm which gives us an optimal trade-off between time and memory\nconsumption. We apply a specific strategy to differentiate the iterative linear\nsolver that comes from the implicit time stepping scheme", 
    "link": "http://arxiv.org/pdf/0711.4444v2", 
    "arxiv-id": "0711.4444v2"
},{
    "category": "physics.flu-dyn", 
    "author": "Damien Olivier", 
    "title": "Changing Levels of Description in a Fluid Flow Simulation", 
    "publish": "2007-12-17T07:07:06Z", 
    "summary": "We describe here our perception of complex systems, of how we feel the\ndifferent layers of description are important part of a correct complex system\nsimulation. We describe a rough models categorization between rules based and\nlaw based, of how these categories handled the levels of descriptions or\nscales. We then describe our fluid flow simulation, which combines different\nfineness of grain in a mixed approach of these categories. This simulation is\nbuilt keeping in mind an ulterior use inside a more general aquatic ecosystem.", 
    "link": "http://arxiv.org/pdf/0712.2643v1", 
    "arxiv-id": "0712.2643v1"
},{
    "category": "cs.CE", 
    "author": "Lester Ingber", 
    "title": "Trading in Risk Dimensions (TRD)", 
    "publish": "2007-12-17T18:11:52Z", 
    "summary": "Previous work, mostly published, developed two-shell recursive trading\nsystems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit\nto incoming market data. A parameterized trading-rule outer-shell uses the\nglobal optimization code Adaptive Simulated Annealing (ASA) to fit the trading\nsystem to historical data. A simple fitting algorithm, usually not requiring\nASA, is used for the inner-shell fit. An additional risk-management\nmiddle-shell has been added to create a three-shell recursive\noptimization/sampling/fitting algorithm. Portfolio-level distributions of\ncopula-transformed multivariate distributions (with constituent markets\npossessing different marginal distributions in returns space) are generated by\nMonte Carlo samplings. ASA is used to importance-sample weightings of these\nmarkets.\n  The core code, Trading in Risk Dimensions (TRD), processes Training and\nTesting trading systems on historical data, and consistently interacts with\nRealTime trading platforms at minute resolutions, but this scale can be\nmodified. This approach transforms constituent probability distributions into a\ncommon space where it makes sense to develop correlations to further develop\nprobability distributions and risk/uncertainty analyses of the full portfolio.\nASA is used for importance-sampling these distributions and for optimizing\nsystem parameters.", 
    "link": "http://arxiv.org/pdf/0712.2789v2", 
    "arxiv-id": "0712.2789v2"
},{
    "category": "cs.LO", 
    "author": "M. B. van der Zwaag", 
    "title": "Tuplix Calculus", 
    "publish": "2007-12-20T13:58:14Z", 
    "summary": "We introduce a calculus for tuplices, which are expressions that generalize\nmatrices and vectors. Tuplices have an underlying data type for quantities that\nare taken from a zero-totalized field. We start with the core tuplix calculus\nCTC for entries and tests, which are combined using conjunctive composition. We\ndefine a standard model and prove that CTC is relatively complete with respect\nto it. The core calculus is extended with operators for choice, information\nhiding, scalar multiplication, clearing and encapsulation. We provide two\nexamples of applications; one on incremental financial budgeting, and one on\nmodular financial budget design.", 
    "link": "http://arxiv.org/pdf/0712.3423v1", 
    "arxiv-id": "0712.3423v1"
},{
    "category": "cs.CE", 
    "author": "John M. Stockie", 
    "title": "A model for reactive porous transport during re-wetting of hardened   concrete", 
    "publish": "2008-01-19T18:54:01Z", 
    "summary": "A mathematical model is developed that captures the transport of liquid water\nin hardened concrete, as well as the chemical reactions that occur between the\nimbibed water and the residual calcium silicate compounds residing in the\nporous concrete matrix. The main hypothesis in this model is that the reaction\nproduct -- calcium silicate hydrate gel -- clogs the pores within the concrete\nthereby hindering water transport. Numerical simulations are employed to\ndetermine the sensitivity of the model solution to changes in various physical\nparameters, and compare to experimental results available in the literature.", 
    "link": "http://arxiv.org/pdf/0801.3046v3", 
    "arxiv-id": "0801.3046v3"
},{
    "category": "cs.NE", 
    "author": "Uwe Aickelin", 
    "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering   Strategies for Scheduling Problems", 
    "publish": "2008-01-21T15:55:22Z", 
    "summary": "This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes amongst the agents on solution\nquality are examined for two multiple-choice optimisation problems. It is shown\nthat partnering strategies that exploit problem-specific knowledge are superior\nand can counter inappropriate (sub-) fitness measurements.", 
    "link": "http://arxiv.org/pdf/0801.3209v2", 
    "arxiv-id": "0801.3209v2"
},{
    "category": "physics.ao-ph", 
    "author": "Zhen Wang", 
    "title": "Towards a Real-Time Data Driven Wildland Fire Model", 
    "publish": "2008-01-25T04:41:01Z", 
    "summary": "A wildland fire model based on semi-empirical relations for the spread rate\nof a surface fire and post-frontal heat release is coupled with the Weather\nResearch and Forecasting atmospheric model (WRF). The propagation of the fire\nfront is implemented by a level set method. Data is assimilated by a morphing\nensemble Kalman filter, which provides amplitude as well as position\ncorrections. Thermal images of a fire will provide the observations and will be\ncompared to a synthetic image from the model state.", 
    "link": "http://arxiv.org/pdf/0801.3875v2", 
    "arxiv-id": "0801.3875v2"
},{
    "category": "cs.NE", 
    "author": "Uwe Aickelin", 
    "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem", 
    "publish": "2008-01-25T16:07:25Z", 
    "summary": "A Bayesian optimization algorithm for the nurse scheduling problem is\npresented, which involves choosing a suitable scheduling rule from a set for\neach nurses assignment. Unlike our previous work that used Gas to implement\nimplicit learning, the learning in the proposed algorithm is explicit, ie.\nEventually, we will be able to identify and mix building blocks directly. The\nBayesian optimization algorithm is applied to implement such explicit learning\nby building a Bayesian network of the joint distribution of solutions. The\nconditional probability of each variable in the network is computed according\nto an initial set of promising solutions. Subsequently, each new instance for\neach variable is generated, ie in our case, a new rule string has been\nobtained. Another set of rule strings will be generated in this way, some of\nwhich will replace previous strings based on fitness selection. If stopping\nconditions are not met, the conditional probabilities for all nodes in the\nBayesian network are updated again using the current set of promising rule\nstrings. Computational results from 52 real data instances demonstrate the\nsuccess of this approach. It is also suggested that the learning mechanism in\nthe proposed approach might be suitable for other scheduling problems.", 
    "link": "http://arxiv.org/pdf/0801.3971v3", 
    "arxiv-id": "0801.3971v3"
},{
    "category": "cs.NE", 
    "author": "Aniza Din", 
    "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In   Changing Environments", 
    "publish": "2008-01-28T15:26:59Z", 
    "summary": "Artificial immune system can be used to generate schedules in changing\nenvironments and it has been proven to be more robust than schedules developed\nusing a genetic algorithm. Good schedules can be produced especially when the\nnumber of the antigens is increased. However, an increase in the range of the\nantigens had somehow affected the fitness of the immune system. In this\nresearch, we are trying to improve the result of the system by rescheduling the\nsame problem using the same method while at the same time maintaining the\nrobustness of the schedules.", 
    "link": "http://arxiv.org/pdf/0801.4312v3", 
    "arxiv-id": "0801.4312v3"
},{
    "category": "cs.NE", 
    "author": "Kathryn Dowsland", 
    "title": "Exploiting problem structure in a genetic algorithm approach to a nurse   rostering problem", 
    "publish": "2008-02-14T11:25:37Z", 
    "summary": "There is considerable interest in the use of genetic algorithms to solve\nproblems arising in the areas of scheduling and timetabling. However, the\nclassical genetic algorithm paradigm is not well equipped to handle the\nconflict between objectives and constraints that typically occurs in such\nproblems. In order to overcome this, successful implementations frequently make\nuse of problem specific knowledge. This paper is concerned with the development\nof a GA for a nurse rostering problem at a major UK hospital. The structure of\nthe constraints is used as the basis for a co-evolutionary strategy using\nco-operating sub-populations. Problem specific knowledge is also used to define\na system of incentives and disincentives, and a complementary mutation\noperator. Empirical results based on 52 weeks of live data show how these\nfeatures are able to improve an unsuccessful canonical GA to the point where it\nis able to provide a practical solution to the problem", 
    "link": "http://arxiv.org/pdf/0802.2001v3", 
    "arxiv-id": "0802.2001v3"
},{
    "category": "cs.CE", 
    "author": "Alexei Botchkarev", 
    "title": "Hospital Case Cost Estimates Modelling - Algorithm Comparison", 
    "publish": "2008-02-28T04:56:48Z", 
    "summary": "Ontario (Canada) Health System stakeholders support the idea and necessity of\nthe integrated source of data that would include both clinical (e.g. diagnosis,\nintervention, length of stay, case mix group) and financial (e.g. cost per\nweighted case, cost per diem) characteristics of the Ontario healthcare system\nactivities at the patient-specific level. At present, the actual patient-level\ncase costs in the explicit form are not available in the financial databases\nfor all hospitals. The goal of this research effort is to develop financial\nmodels that will assign each clinical case in the patient-specific data\nwarehouse a dollar value, representing the cost incurred by the Ontario health\ncare facility which treated the patient. Five mathematical models have been\ndeveloped and verified using real dataset. All models can be classified into\ntwo groups based on their underlying method: 1. Models based on using relative\nintensity weights of the cases, and 2. Models based on using cost per diem.", 
    "link": "http://arxiv.org/pdf/0802.4126v1", 
    "arxiv-id": "0802.4126v1"
},{
    "category": "cs.NE", 
    "author": "Adrian Adewunmi", 
    "title": "Simulation Optimization of the Crossdock Door Assignment Problem", 
    "publish": "2008-03-11T12:56:51Z", 
    "summary": "The purpose of this report is to present the Crossdock Door Assignment\nProblem, which involves assigning destinations to outbound dock doors of\nCrossdock centres such that travel distance by material handling equipment is\nminimized. We propose a two fold solution; simulation and optimization of the\nsimulation model simulation optimization. The novel aspect of our solution\napproach is that we intend to use simulation to derive a more realistic\nobjective function and use Memetic algorithms to find an optimal solution. The\nmain advantage of using Memetic algorithms is that it combines a local search\nwith Genetic Algorithms. The Crossdock Door Assignment Problem is a new domain\napplication to Memetic Algorithms and it is yet unknown how it will perform.", 
    "link": "http://arxiv.org/pdf/0803.1576v1", 
    "arxiv-id": "0803.1576v1"
},{
    "category": "cs.NE", 
    "author": "Rong Qu", 
    "title": "Investigating a Hybrid Metaheuristic For Job Shop Rescheduling", 
    "publish": "2008-03-12T09:26:47Z", 
    "summary": "Previous research has shown that artificial immune systems can be used to\nproduce robust schedules in a manufacturing environment. The main goal is to\ndevelop building blocks (antibodies) of partial schedules that can be used to\nconstruct backup solutions (antigens) when disturbances occur during\nproduction. The building blocks are created based upon underpinning ideas from\nartificial immune systems and evolved using a genetic algorithm (Phase I). Each\npartial schedule (antibody) is assigned a fitness value and the best partial\nschedules are selected to be converted into complete schedules (antigens). We\nfurther investigate whether simulated annealing and the great deluge algorithm\ncan improve the results when hybridised with our artificial immune system\n(Phase II). We use ten fixed solutions as our target and measure how well we\ncover these specific scenarios.", 
    "link": "http://arxiv.org/pdf/0803.1728v1", 
    "arxiv-id": "0803.1728v1"
},{
    "category": "cs.NE", 
    "author": "Mike Byrne", 
    "title": "An Investigation of the Sequential Sampling Method for Crossdocking   Simulation Output Variance Reduction", 
    "publish": "2008-03-13T15:02:48Z", 
    "summary": "This paper investigates the reduction of variance associated with a\nsimulation output performance measure, using the Sequential Sampling method\nwhile applying minimum simulation replications, for a class of JIT (Just in\nTime) warehousing system called crossdocking. We initially used the Sequential\nSampling method to attain a desired 95% confidence interval half width of\nplus/minus 0.5 for our chosen performance measure (Total usage cost, given the\nmean maximum level of 157,000 pounds and a mean minimum level of 149,000\npounds). From our results, we achieved a 95% confidence interval half width of\nplus/minus 2.8 for our chosen performance measure (Total usage cost, with an\naverage mean value of 115,000 pounds). However, the Sequential Sampling method\nrequires a huge number of simulation replications to reduce variance for our\nsimulation output value to the target level. Arena (version 11) simulation\nsoftware was used to conduct this study.", 
    "link": "http://arxiv.org/pdf/0803.1985v1", 
    "arxiv-id": "0803.1985v1"
},{
    "category": "cs.NE", 
    "author": "Jingpeng Li", 
    "title": "Improved Squeaky Wheel Optimisation for Driver Scheduling", 
    "publish": "2008-03-13T15:28:06Z", 
    "summary": "This paper presents a technique called Improved Squeaky Wheel Optimisation\nfor driver scheduling problems. It improves the original Squeaky Wheel\nOptimisations effectiveness and execution speed by incorporating two additional\nsteps of Selection and Mutation which implement evolution within a single\nsolution. In the ISWO, a cycle of\nAnalysis-Selection-Mutation-Prioritization-Construction continues until\nstopping conditions are reached. The Analysis step first computes the fitness\nof a current solution to identify troublesome components. The Selection step\nthen discards these troublesome components probabilistically by using the\nfitness measure, and the Mutation step follows to further discard a small\nnumber of components at random. After the above steps, an input solution\nbecomes partial and thus the resulting partial solution needs to be repaired.\nThe repair is carried out by using the Prioritization step to first produce\npriorities that determine an order by which the following Construction step\nthen schedules the remaining components. Therefore, the optimisation in the\nISWO is achieved by solution disruption, iterative improvement and an iterative\nconstructive repair process performed. Encouraging experimental results are\nreported.", 
    "link": "http://arxiv.org/pdf/0803.1993v1", 
    "arxiv-id": "0803.1993v1"
},{
    "category": "cs.NE", 
    "author": "Uwe Aickelin", 
    "title": "The Application of Bayesian Optimization and Classifier Systems in Nurse   Scheduling", 
    "publish": "2008-03-13T15:43:34Z", 
    "summary": "Two ideas taken from Bayesian optimization and classifier systems are\npresented for personnel scheduling based on choosing a suitable scheduling rule\nfrom a set for each persons assignment. Unlike our previous work of using\ngenetic algorithms whose learning is implicit, the learning in both approaches\nis explicit, i.e. we are able to identify building blocks directly. To achieve\nthis target, the Bayesian optimization algorithm builds a Bayesian network of\nthe joint probability distribution of the rules used to construct solutions,\nwhile the adapted classifier system assigns each rule a strength value that is\nconstantly updated according to its usefulness in the current situation.\nComputational results from 52 real data instances of nurse scheduling\ndemonstrate the success of both approaches. It is also suggested that the\nlearning mechanism in the proposed approaches might be suitable for other\nscheduling problems.", 
    "link": "http://arxiv.org/pdf/0803.1994v1", 
    "arxiv-id": "0803.1994v1"
},{
    "category": "cs.NE", 
    "author": "Kathryn Dowsland", 
    "title": "Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall   Layout and Tenant Selection Problem", 
    "publish": "2008-03-20T10:19:01Z", 
    "summary": "During our earlier research, it was recognised that in order to be successful\nwith an indirect genetic algorithm approach using a decoder, the decoder has to\nstrike a balance between being an optimiser in its own right and finding\nfeasible solutions. Previously this balance was achieved manually. Here we\nextend this by presenting an automated approach where the genetic algorithm\nitself, simultaneously to solving the problem, sets weights to balance the\ncomponents out. Subsequently we were able to solve a complex and non-linear\nscheduling problem better than with a standard direct genetic algorithm\nimplementation.", 
    "link": "http://arxiv.org/pdf/0803.2957v1", 
    "arxiv-id": "0803.2957v1"
},{
    "category": "cs.NE", 
    "author": "Paul White", 
    "title": "Building Better Nurse Scheduling Algorithms", 
    "publish": "2008-03-20T11:15:37Z", 
    "summary": "The aim of this research is twofold: Firstly, to model and solve a complex\nnurse scheduling problem with an integer programming formulation and\nevolutionary algorithms. Secondly, to detail a novel statistical method of\ncomparing and hence building better scheduling algorithms by identifying\nsuccessful algorithm modifications. The comparison method captures the results\nof algorithms in a single figure that can then be compared using traditional\nstatistical techniques. Thus, the proposed method of comparing algorithms is an\nobjective procedure designed to assist in the process of improving an\nalgorithm. This is achieved even when some results are non-numeric or missing\ndue to infeasibility. The final algorithm outperforms all previous evolutionary\nalgorithms, which relied on human expertise for modification.", 
    "link": "http://arxiv.org/pdf/0803.2967v1", 
    "arxiv-id": "0803.2967v1"
},{
    "category": "cs.NE", 
    "author": "Kathryn Dowsland", 
    "title": "An Indirect Genetic Algorithm for a Nurse Scheduling Problem", 
    "publish": "2008-03-20T11:21:19Z", 
    "summary": "This paper describes a Genetic Algorithms approach to a manpower-scheduling\nproblem arising at a major UK hospital. Although Genetic Algorithms have been\nsuccessfully used for similar problems in the past, they always had to overcome\nthe limitations of the classical Genetic Algorithms paradigm in handling the\nconflict between objectives and constraints. The approach taken here is to use\nan indirect coding based on permutations of the nurses, and a heuristic decoder\nthat builds schedules from these permutations. Computational experiments based\non 52 weeks of live data are used to evaluate three different decoders with\nvarying levels of intelligence, and four well-known crossover operators.\nResults are further enhanced by introducing a hybrid crossover operator and by\nmaking use of simple bounds to reduce the size of the solution space. The\nresults reveal that the proposed algorithm is able to find high quality\nsolutions and is both faster and more flexible than a recently published Tabu\nSearch approach.", 
    "link": "http://arxiv.org/pdf/0803.2969v1", 
    "arxiv-id": "0803.2969v1"
},{
    "category": "cs.NE", 
    "author": "Jingpeng Li", 
    "title": "An Estimation of Distribution Algorithm for Nurse Scheduling", 
    "publish": "2008-03-20T12:07:26Z", 
    "summary": "Schedules can be built in a similar way to a human scheduler by using a set\nof rules that involve domain knowledge. This paper presents an Estimation of\nDistribution Algorithm (eda) for the nurse scheduling problem, which involves\nchoosing a suitable scheduling rule from a set for the assignment of each\nnurse. Unlike previous work that used Genetic Algorithms (ga) to implement\nimplicit learning, the learning in the proposed algorithm is explicit, i.e. we\nidentify and mix building blocks directly. The eda is applied to implement such\nexplicit learning by building a Bayesian network of the joint distribution of\nsolutions. The conditional probability of each variable in the network is\ncomputed according to an initial set of promising solutions. Subsequently, each\nnew instance for each variable is generated by using the corresponding\nconditional probabilities, until all variables have been generated, i.e. in our\ncase, a new rule string has been obtained. Another set of rule strings will be\ngenerated in this way, some of which will replace previous strings based on\nfitness selection. If stopping conditions are not met, the conditional\nprobabilities for all nodes in the Bayesian network are updated again using the\ncurrent set of promising rule strings. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed approach might be suitable for other\nscheduling problems.", 
    "link": "http://arxiv.org/pdf/0803.2975v2", 
    "arxiv-id": "0803.2975v2"
},{
    "category": "cs.NE", 
    "author": "Edmund Burke", 
    "title": "A Component Based Heuristic Search method with Adaptive Perturbations   for Hospital Personnel Scheduling", 
    "publish": "2008-03-27T12:15:43Z", 
    "summary": "Nurse rostering is a complex scheduling problem that affects hospital\npersonnel on a daily basis all over the world. This paper presents a new\ncomponent-based approach with adaptive perturbations, for a nurse scheduling\nproblem arising at a major UK hospital. The main idea behind this technique is\nto decompose a schedule into its components (i.e. the allocated shift pattern\nof each nurse), and then mimic a natural evolutionary process on these\ncomponents to iteratively deliver better schedules. The worthiness of all\ncomponents in the schedule has to be continuously demonstrated in order for\nthem to remain there. This demonstration employs a dynamic evaluation function\nwhich evaluates how well each component contributes towards the final\nobjective. Two perturbation steps are then applied: the first perturbation\neliminates a number of components that are deemed not worthy to stay in the\ncurrent schedule; the second perturbation may also throw out, with a low level\nof probability, some worthy components. The eliminated components are\nreplenished with new ones using a set of constructive heuristics using local\noptimality criteria. Computational results using 52 data instances demonstrate\nthe applicability of the proposed approach in solving real-world problems.", 
    "link": "http://arxiv.org/pdf/0803.3900v1", 
    "arxiv-id": "0803.3900v1"
},{
    "category": "cs.NE", 
    "author": "Uwe Aickelin", 
    "title": "Bayesian Optimisation Algorithm for Nurse Scheduling", 
    "publish": "2008-04-03T11:14:11Z", 
    "summary": "Our research has shown that schedules can be built mimicking a human\nscheduler by using a set of rules that involve domain knowledge. This chapter\npresents a Bayesian Optimization Algorithm (BOA) for the nurse scheduling\nproblem that chooses such suitable scheduling rules from a set for each nurses\nassignment. Based on the idea of using probabilistic models, the BOA builds a\nBayesian network for the set of promising solutions and samples these networks\nto generate new candidate solutions. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed algorithm may be suitable for other\nscheduling problems.", 
    "link": "http://arxiv.org/pdf/0804.0524v1", 
    "arxiv-id": "0804.0524v1"
},{
    "category": "cs.CE", 
    "author": "Steve Gwynne", 
    "title": "Prediction and Mitigation of Crush Conditions in Emergency Evacuations", 
    "publish": "2008-05-03T13:00:42Z", 
    "summary": "Several simulation environments exist for the simulation of large-scale\nevacuations of buildings, ships, or other enclosed spaces. These offer\nsophisticated tools for the study of human behaviour, the recreation of\nenvironmental factors such as fire or smoke, and the inclusion of architectural\nor structural features, such as elevators, pillars and exits. Although such\nsimulation environments can provide insights into crowd behaviour, they lack\nthe ability to examine potentially dangerous forces building up within a crowd.\nThese are commonly referred to as crush conditions, and are a common cause of\ndeath in emergency evacuations.\n  In this paper, we describe a methodology for the prediction and mitigation of\ncrush conditions. The paper is organised as follows. We first establish the\nneed for such a model, defining the main factors that lead to crush conditions,\nand describing several exemplar case studies. We then examine current methods\nfor studying crush, and describe their limitations. From this, we develop a\nthree-stage hybrid approach, using a combination of techniques. We conclude\nwith a brief discussion of the potential benefits of our approach.", 
    "link": "http://arxiv.org/pdf/0805.0360v1", 
    "arxiv-id": "0805.0360v1"
},{
    "category": "cs.CE", 
    "author": "M. B. van der Zwaag", 
    "title": "Tuplix Calculus Specifications of Financial Transfer Networks", 
    "publish": "2008-05-13T09:05:41Z", 
    "summary": "We study the application of Tuplix Calculus in modular financial budget\ndesign. We formalize organizational structure using financial transfer\nnetworks. We consider the notion of flux of money over a network, and a way to\nenforce the matching of influx and outflux for parts of a network. We exploit\nso-called signed attribute notation to make internal streams visible through\nencapsulations. Finally, we propose a Tuplix Calculus construct for the\ndefinition of data functions.", 
    "link": "http://arxiv.org/pdf/0805.1806v1", 
    "arxiv-id": "0805.1806v1"
},{
    "category": "cs.DC", 
    "author": "Ian Stokes-Rees", 
    "title": "Parallel Pricing Algorithms for Multi--Dimensional Bermudan/American   Options using Monte Carlo methods", 
    "publish": "2008-05-13T12:34:04Z", 
    "summary": "In this paper we present two parallel Monte Carlo based algorithms for\npricing multi--dimensional Bermudan/American options. First approach relies on\ncomputation of the optimal exercise boundary while the second relies on\nclassification of continuation and exercise values. We also evaluate the\nperformance of both the algorithms in a desktop grid environment. We show the\neffectiveness of the proposed approaches in a heterogeneous computing\nenvironment, and identify scalability constraints due to the algorithmic\nstructure.", 
    "link": "http://arxiv.org/pdf/0805.1827v1", 
    "arxiv-id": "0805.1827v1"
},{
    "category": "astro-ph", 
    "author": "B. Skordovski", 
    "title": "The VO-Neural project: recent developments and some applications", 
    "publish": "2008-06-05T16:44:23Z", 
    "summary": "VO-Neural is the natural evolution of the Astroneural project which was\nstarted in 1994 with the aim to implement a suite of neural tools for data\nmining in astronomical massive data sets. At a difference with its ancestor,\nwhich was implemented under Matlab, VO-Neural is written in C++, object\noriented, and it is specifically tailored to work in distributed computing\narchitectures. We discuss the current status of implementation of VO-Neural,\npresent an application to the classification of Active Galactic Nuclei, and\noutline the ongoing work to improve the functionalities of the package.", 
    "link": "http://arxiv.org/pdf/0806.1006v1", 
    "arxiv-id": "0806.1006v1"
},{
    "category": "astro-ph", 
    "author": "G. Longo", 
    "title": "GRID-Launcher v.1.0", 
    "publish": "2008-06-06T12:35:17Z", 
    "summary": "GRID-launcher-1.0 was built within the VO-Tech framework, as a software\ninterface between the UK-ASTROGRID and a generic GRID infrastructures in order\nto allow any ASTROGRID user to launch on the GRID computing intensive tasks\nfrom the ASTROGRID Workbench or Desktop. Even though of general application, so\nfar the Grid-Launcher has been tested on a few selected softwares\n(VONeural-MLP, VONeural-SVM, Sextractor and SWARP) and on the SCOPE-GRID.", 
    "link": "http://arxiv.org/pdf/0806.1144v1", 
    "arxiv-id": "0806.1144v1"
},{
    "category": "hep-lat", 
    "author": "for the USQCD Collaboration", 
    "title": "Continuing Progress on a Lattice QCD Software Infrastructure", 
    "publish": "2008-06-13T19:22:00Z", 
    "summary": "We report on the progress of the software effort in the QCD Application Area\nof SciDAC. In particular, we discuss how the software developed under SciDAC\nenabled the aggressive exploitation of leadership computers, and we report on\nprogress in the area of QCD software for multi-core architectures.", 
    "link": "http://arxiv.org/pdf/0806.2312v1", 
    "arxiv-id": "0806.2312v1"
},{
    "category": "cs.CE", 
    "author": "Maziar Nekovee", 
    "title": "Simulations of Large-scale WiFi-based Wireless Networks:   Interdisciplinary Challenges and Applications", 
    "publish": "2008-07-09T16:04:37Z", 
    "summary": "Wireless Fidelity (WiFi) is the fastest growing wireless technology to date.\nIn addition to providing wire-free connectivity to the Internet WiFi technology\nalso enables mobile devices to connect directly to each other and form highly\ndynamic wireless adhoc networks. Such distributed networks can be used to\nperform cooperative communication tasks such ad data routing and information\ndissemination in the absence of a fixed infrastructure. Furthermore, adhoc\ngrids composed of wirelessly networked portable devices are emerging as a new\nparadigm in grid computing. In this paper we review computational and\nalgorithmic challenges of high-fidelity simulations of such WiFi-based wireless\ncommunication and computing networks, including scalable topology maintenance,\nmobility modelling, parallelisation and synchronisation. We explore\nsimilarities and differences between the simulations of these networks and\nsimulations of interacting many-particle systems, such as molecular dynamics\n(MD) simulations. We show how the cell linked-list algorithm which we have\nadapted from our MD simulations can be used to greatly improve the\ncomputational performance of wireless network simulators in the presence of\nmobility, and illustrate with an example from our simulation studies of worm\nattacks on mobile wireless adhoc networks.", 
    "link": "http://arxiv.org/pdf/0807.1475v1", 
    "arxiv-id": "0807.1475v1"
},{
    "category": "cs.NA", 
    "author": "K. D. Hjelmstad", 
    "title": "On dual Schur domain decomposition method for linear first-order   transient problems", 
    "publish": "2008-07-14T08:10:33Z", 
    "summary": "This paper addresses some numerical and theoretical aspects of dual Schur\ndomain decomposition methods for linear first-order transient partial\ndifferential equations. In this work, we consider the trapezoidal family of\nschemes for integrating the ordinary differential equations (ODEs) for each\nsubdomain and present four different coupling methods, corresponding to\ndifferent algebraic constraints, for enforcing kinematic continuity on the\ninterface between the subdomains.\n  Method 1 (d-continuity) is based on the conventional approach using\ncontinuity of the primary variable and we show that this method is unstable for\na lot of commonly used time integrators including the mid-point rule. To\nalleviate this difficulty, we propose a new Method 2 (Modified d-continuity)\nand prove its stability for coupling all time integrators in the trapezoidal\nfamily (except the forward Euler). Method 3 (v-continuity) is based on\nenforcing the continuity of the time derivative of the primary variable.\nHowever, this constraint introduces a drift in the primary variable on the\ninterface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte\nstabilization to limit this drift and we derive bounds for the stabilization\nparameter to ensure stability.\n  Our stability analysis is based on the ``energy'' method, and one of the main\ncontributions of this paper is the extension of the energy method (which was\npreviously introduced in the context of numerical methods for ODEs) to assess\nthe stability of numerical formulations for index-2 differential-algebraic\nequations (DAEs).", 
    "link": "http://arxiv.org/pdf/0807.2108v2", 
    "arxiv-id": "0807.2108v2"
},{
    "category": "cs.AI", 
    "author": "Juan Ojeda Sarmiento", 
    "title": "Electricity Demand and Energy Consumption Management System", 
    "publish": "2008-09-14T22:26:49Z", 
    "summary": "This project describes the electricity demand and energy consumption\nmanagement system and its application to Southern Peru smelter. It is composed\nof an hourly demand-forecasting module and of a simulation component for a\nplant electrical system. The first module was done using dynamic neural\nnetworks with backpropagation training algorithm; it is used to predict the\nelectric power demanded every hour, with an error percentage below of 1%. This\ninformation allows efficient management of energy peak demands before this\nhappen, distributing the raise of electric load to other hours or improving\nthose equipments that increase the demand. The simulation module is based in\nadvanced estimation techniques, such as: parametric estimation, neural network\nmodeling, statistic regression and previously developed models, which simulates\nthe electric behavior of the smelter plant. These modules facilitate\nelectricity demand and consumption proper planning, because they allow knowing\nthe behavior of the hourly demand and the consumption patterns of the plant,\nincluding the bill components, but also energy deficiencies and opportunities\nfor improvement, based on analysis of information about equipments, processes\nand production plans, as well as maintenance programs. Finally the results of\nits application in Southern Peru smelter are presented.", 
    "link": "http://arxiv.org/pdf/0809.2421v5", 
    "arxiv-id": "0809.2421v5"
},{
    "category": "cs.MA", 
    "author": "Armen Bagdasaryan", 
    "title": "Mathematical Tool of Discrete Dynamic Modeling of Complex Systems in   Control Loop", 
    "publish": "2008-09-16T11:14:20Z", 
    "summary": "In this paper we present a method of discrete modeling and analysis of\nmulti-level dynamics of complex large-scale hierarchical dynamic systems\nsubject to external dynamic control mechanism. In a model each state describes\nparallel dynamics and simultaneous trends of changes in system parameters. The\nessence of the approach is in analysis of system state dynamics while it is in\nthe control loop.", 
    "link": "http://arxiv.org/pdf/0809.2680v1", 
    "arxiv-id": "0809.2680v1"
},{
    "category": "cs.CE", 
    "author": "Armen Bagdasaryan", 
    "title": "Mathematical and computer tools of discrete dynamic modeling and   analysis of complex systems in control loop", 
    "publish": "2008-09-22T12:03:52Z", 
    "summary": "We present a method of discrete modeling and analysis of multilevel dynamics\nof complex large-scale hierarchical dynamic systems subject to external dynamic\ncontrol mechanism. Architectural model of information system supporting\nsimulation and analysis of dynamic processes and development scenarios\n(strategies) of complex large-scale hierarchical systems is also proposed.", 
    "link": "http://arxiv.org/pdf/0809.3688v1", 
    "arxiv-id": "0809.3688v1"
},{
    "category": "hep-ph", 
    "author": "P. R. Reynolds", 
    "title": "New avenue to the Parton Distribution Functions: Self-Organizing Maps", 
    "publish": "2008-10-15T05:41:51Z", 
    "summary": "Neural network algorithms have been recently applied to construct Parton\nDistribution Function (PDF) parametrizations which provide an alternative to\nstandard global fitting procedures. We propose a technique based on an\ninteractive neural network algorithm using Self-Organizing Maps (SOMs). SOMs\nare a class of clustering algorithms based on competitive learning among\nspatially-ordered neurons. Our SOMs are trained on selections of stochastically\ngenerated PDF samples. The selection criterion for every optimization iteration\nis based on the features of the clustered PDFs. Our main goal is to provide a\nfitting procedure that, at variance with the standard neural network\napproaches, allows for an increased control of the systematic bias by enabling\nuser interaction in the various stages of the process.", 
    "link": "http://arxiv.org/pdf/0810.2598v2", 
    "arxiv-id": "0810.2598v2"
},{
    "category": "cs.NE", 
    "author": "P. Villon", 
    "title": "Optimal design and optimal control of structures undergoing finite   rotations and elastic deformations", 
    "publish": "2009-02-06T09:43:09Z", 
    "summary": "In this work we deal with the optimal design and optimal control of\nstructures undergoing large rotations. In other words, we show how to find the\ncorresponding initial configuration and the corresponding set of multiple load\nparameters in order to recover a desired deformed configuration or some\ndesirable features of the deformed configuration as specified more precisely by\nthe objective or cost function. The model problem chosen to illustrate the\nproposed optimal design and optimal control methodologies is the one of\ngeometrically exact beam. First, we present a non-standard formulation of the\noptimal design and optimal control problems, relying on the method of Lagrange\nmultipliers in order to make the mechanics state variables independent from\neither design or control variables and thus provide the most general basis for\ndeveloping the best possible solution procedure. Two different solution\nprocedures are then explored, one based on the diffuse approximation of\nresponse function and gradient method and the other one based on genetic\nalgorithm. A number of numerical examples are given in order to illustrate both\nthe advantages and potential drawbacks of each of the presented procedures.", 
    "link": "http://arxiv.org/pdf/0902.1037v2", 
    "arxiv-id": "0902.1037v2"
},{
    "category": "cs.NE", 
    "author": "Z. Bittnar", 
    "title": "Novel anisotropic continuum-discrete damage model capable of   representing localized failure of massive structures. Part II: identification   from tests under heterogeneous stress field", 
    "publish": "2009-02-10T14:45:50Z", 
    "summary": "In Part I of this paper we have presented a simple model capable of\ndescribing the localized failure of a massive structure. In this part, we\ndiscuss the identification of the model parameters from two kinds of\nexperiments: a uniaxial tensile test and a three-point bending test. The former\nis used only for illustration of material parameter response dependence, and we\nfocus mostly upon the latter, discussing the inverse optimization problem for\nwhich the specimen is subjected to a heterogeneous stress field.", 
    "link": "http://arxiv.org/pdf/0902.1665v1", 
    "arxiv-id": "0902.1665v1"
},{
    "category": "nlin.CD", 
    "author": "P. I. Kogut", 
    "title": "Definition of Strange Attractor in Benard problem for Generalized   Couette Cell", 
    "publish": "2009-03-05T10:45:51Z", 
    "summary": "For movements of the viscous continuous flow in generalized Couette cell the\ndynamic system describing the central limiting variety is received.", 
    "link": "http://arxiv.org/pdf/0903.0952v1", 
    "arxiv-id": "0903.0952v1"
},{
    "category": "cs.CE", 
    "author": "A. J. Valocchi", 
    "title": "Variational structure of the optimal artificial diffusion method for the   advection-diffusion equation", 
    "publish": "2009-05-29T01:49:38Z", 
    "summary": "In this research note we provide a variational basis for the optimal\nartificial diffusion method, which has been a cornerstone in developing many\nstabilized methods. The optimal artificial diffusion method produces exact\nnodal solutions when applied to one-dimensional problems with constant\ncoefficients and forcing function. We first present a variational principle for\na multi-dimensional advective-diffusive system, and then derive a new stable\nweak formulation. When applied to one-dimensional problems with constant\ncoefficients and forcing function, this resulting weak formulation will be\nequivalent to the optimal artificial diffusion method. We present\nrepresentative numerical results to corroborate our theoretical findings.", 
    "link": "http://arxiv.org/pdf/0905.4771v3", 
    "arxiv-id": "0905.4771v3"
},{
    "category": "q-bio.QM", 
    "author": "Eric Wajnberg", 
    "title": "Activatability for simulation tractability of NP problems: Application   to Ecology", 
    "publish": "2009-06-24T11:59:29Z", 
    "summary": "Dynamics of biological-ecological systems is strongly depending on spatial\ndimensions. Most of powerful simulators in ecology take into account for system\nspatiality thus embedding stochastic processes. Due to the difficulty of\nresearching particular trajectories, biologists and computer scientists aim at\npredicting the most probable trajectories of systems under study. Doing that,\nthey considerably reduce computation times. However, because of the largeness\nof space, the execution time remains usually polynomial in time. In order to\nreduce execution times we propose an activatability-based search cycle through\nthe process space. This cycle eliminates the redundant processes on a\nstatistical basis (Generalized Linear Model), and converges to the minimal\nnumber of processes required to match simulation objectives.", 
    "link": "http://arxiv.org/pdf/0906.4454v1", 
    "arxiv-id": "0906.4454v1"
},{
    "category": "cs.CE", 
    "author": "Oktie Hassanzadeh", 
    "title": "Automated Protein Structure Classification: A Survey", 
    "publish": "2009-07-13T18:40:38Z", 
    "summary": "Classification of proteins based on their structure provides a valuable\nresource for studying protein structure, function and evolutionary\nrelationships. With the rapidly increasing number of known protein structures,\nmanual and semi-automatic classification is becoming ever more difficult and\nprohibitively slow. Therefore, there is a growing need for automated, accurate\nand efficient classification methods to generate classification databases or\nincrease the speed and accuracy of semi-automatic techniques. Recognizing this\nneed, several automated classification methods have been developed. In this\nsurvey, we overview recent developments in this area. We classify different\nmethods based on their characteristics and compare their methodology, accuracy\nand efficiency. We then present a few open problems and explain future\ndirections.", 
    "link": "http://arxiv.org/pdf/0907.1990v1", 
    "arxiv-id": "0907.1990v1"
},{
    "category": "cs.CE", 
    "author": "Petr R. Ivankov", 
    "title": "Top-down Paradigm in Engineering Software Integration", 
    "publish": "2009-08-06T12:21:22Z", 
    "summary": "The top-down approach of engineering software integration is considered in\nthis parer. A set of advantages of this approach are presented, by examples.\nAll examples are supplied by open source code.", 
    "link": "http://arxiv.org/pdf/0908.0833v1", 
    "arxiv-id": "0908.0833v1"
},{
    "category": "cs.CV", 
    "author": "A. De Cesare", 
    "title": "Non-quadratic convex regularized reconstruction of MR images from spiral   acquisitions", 
    "publish": "2009-08-22T14:03:03Z", 
    "summary": "Combining fast MR acquisition sequences and high resolution imaging is a\nmajor issue in dynamic imaging. Reducing the acquisition time can be achieved\nby using non-Cartesian and sparse acquisitions. The reconstruction of MR images\nfrom these measurements is generally carried out using gridding that\ninterpolates the missing data to obtain a dense Cartesian k-space filling. The\nMR image is then reconstructed using a conventional Fast Fourier Transform. The\nestimation of the missing data unavoidably introduces artifacts in the image\nthat remain difficult to quantify.\n  A general reconstruction method is proposed to take into account these\nlimitations. It can be applied to any sampling trajectory in k-space, Cartesian\nor not, and specifically takes into account the exact location of the measured\ndata, without making any interpolation of the missing data in k-space.\nInformation about the expected characteristics of the imaged object is\nintroduced to preserve the spatial resolution and improve the signal to noise\nratio in a regularization framework. The reconstructed image is obtained by\nminimizing a non-quadratic convex objective function. An original rewriting of\nthis criterion is shown to strongly improve the reconstruction efficiency.\nResults on simulated data and on a real spiral acquisition are presented and\ndiscussed.", 
    "link": "http://arxiv.org/pdf/0908.3252v1", 
    "arxiv-id": "0908.3252v1"
},{
    "category": "cs.IR", 
    "author": "Andri Mirzal", 
    "title": "On the Relationship between Trading Network and WWW Network: A   Preferential Attachment Perspective", 
    "publish": "2009-08-23T02:06:35Z", 
    "summary": "This paper describes the relationship between trading network and WWW network\nfrom preferential attachment mechanism perspective. This mechanism is known to\nbe the underlying principle in the network evolution and has been incorporated\nto formulate two famous web pages ranking algorithms, PageRank and HITS. We\npoint out the differences between trading network and WWW network in this\nmechanism, derive the formulation of HITS-based ranking algorithm for trading\nnetwork as a direct consequence of the differences, and apply the same\nframework when deriving the formulation back to the HITS formulation that turns\nto become a technique to accelerate its convergences.", 
    "link": "http://arxiv.org/pdf/0908.3280v2", 
    "arxiv-id": "0908.3280v2"
},{
    "category": "cs.CE", 
    "author": "Dmitry A. Karpeev", 
    "title": "Mesh Algorithms for PDE with Sieve I: Mesh Distribution", 
    "publish": "2009-08-30T21:53:01Z", 
    "summary": "We have developed a new programming framework, called Sieve, to support\nparallel numerical PDE algorithms operating over distributed meshes. We have\nalso developed a reference implementation of Sieve in C++ as a library of\ngeneric algorithms operating on distributed containers conforming to the Sieve\ninterface. Sieve makes instances of the incidence relation, or \\emph{arrows},\nthe conceptual first-class objects represented in the containers. Further,\ngeneric algorithms acting on this arrow container are systematically used to\nprovide natural geometric operations on the topology and also, through duality,\non the data. Finally, coverings and duality are used to encode not only\nindividual meshes, but all types of hierarchies underlying PDE data structures,\nincluding multigrid and mesh partitions.\n  In order to demonstrate the usefulness of the framework, we show how the mesh\npartition data can be represented and manipulated using the same fundamental\nmechanisms used to represent meshes. We present the complete description of an\nalgorithm to encode a mesh partition and then distribute a mesh, which is\nindependent of the mesh dimension, element shape, or embedding. Moreover, data\nassociated with the mesh can be similarly distributed with exactly the same\nalgorithm. The use of a high level of abstraction within the Sieve leads to\nseveral benefits in terms of code reuse, simplicity, and extensibility. We\ndiscuss these benefits and compare our approach to other existing mesh\nlibraries.", 
    "link": "http://arxiv.org/pdf/0908.4427v1", 
    "arxiv-id": "0908.4427v1"
},{
    "category": "cs.CE", 
    "author": "Ahmad Baraani Dastjerdi", 
    "title": "A Hybrid Multi Objective Particle Swarm Optimization Method to Discover   Biclusters in Microarray Data", 
    "publish": "2009-09-08T06:43:54Z", 
    "summary": "In recent years, with the development of microarray technique, discovery of\nuseful knowledge from microarray data has become very important. Biclustering\nis a very useful data mining technique for discovering genes which have similar\nbehavior. In microarray data, several objectives have to be optimized\nsimultaneously and often these objectives are in conflict with each other. A\nMulti Objective model is capable of solving such problems. Our method proposes\na Hybrid algorithm which is based on the Multi Objective Particle Swarm\nOptimization for discovering biclusters in gene expression data. In our method,\nwe will consider a low level of overlapping amongst the biclusters and try to\ncover all elements of the gene expression matrix. Experimental results in the\nbench mark database show a significant improvement in both overlap among\nbiclusters and coverage of elements in the gene expression matrix.", 
    "link": "http://arxiv.org/pdf/0909.1405v1", 
    "arxiv-id": "0909.1405v1"
},{
    "category": "cs.SE", 
    "author": "Anthony Scemama", 
    "title": "IRPF90: a programming environment for high performance computing", 
    "publish": "2009-09-28T16:46:45Z", 
    "summary": "IRPF90 is a Fortran programming environment which helps the development of\nlarge Fortran codes. In Fortran programs, the programmer has to focus on the\norder of the instructions: before using a variable, the programmer has to be\nsure that it has already been computed in all possible situations. For large\ncodes, it is common source of error. In IRPF90 most of the order of\ninstructions is handled by the pre-processor, and an automatic mechanism\nguarantees that every entity is built before being used. This mechanism relies\non the {needs/needed by} relations between the entities, which are built\nautomatically. Codes written with IRPF90 execute often faster than Fortran\nprograms, are faster to write and easier to maintain.", 
    "link": "http://arxiv.org/pdf/0909.5012v1", 
    "arxiv-id": "0909.5012v1"
},{
    "category": "cs.CE", 
    "author": "Hamed O. Ghaffari", 
    "title": "Two-Phase Flow Complexity in Heterogeneous Media", 
    "publish": "2009-09-30T11:53:18Z", 
    "summary": "In this study, we investigate the appeared complexity of two-phase flow\n(air/water) in a heterogeneous soil where the supposed porous media is\nnon-deformable media which is under the timedependent gas pressure. After\nobtaining of governing equations and considering the capillary\npressuresaturation and permeability functions, the evolution of the model\nunknown parameters were obtained. In this way, using COMSOL (FEMLAB) and fluid\nflow/script Module, the role of heterogeneity in intrinsic permeability was\nanalysed. Also, the evolution of relative permeability of wetting and\nnon-wetting fluid, capillary pressure and other parameters were elicited. In\nthe last part, a complex network approach to analysis of emerged patterns will\nbe employed.", 
    "link": "http://arxiv.org/pdf/0909.5583v2", 
    "arxiv-id": "0909.5583v2"
},{
    "category": "q-bio.QM", 
    "author": "Andrea Maggiolo-Schettini", 
    "title": "On the Interpretation of Delays in Delay Stochastic Simulation of   Biological Systems", 
    "publish": "2009-10-07T11:25:03Z", 
    "summary": "Delays in biological systems may be used to model events for which the\nunderlying dynamics cannot be precisely observed. Mathematical modeling of\nbiological systems with delays is usually based on Delay Differential Equations\n(DDEs), a kind of differential equations in which the derivative of the unknown\nfunction at a certain time is given in terms of the values of the function at\nprevious times. In the literature, delay stochastic simulation algorithms have\nbeen proposed. These algorithms follow a \"delay as duration\" approach, namely\nthey are based on an interpretation of a delay as the elapsing time between the\nstart and the termination of a chemical reaction. This interpretation is not\nsuitable for some classes of biological systems in which species involved in a\ndelayed interaction can be involved at the same time in other interactions. We\nshow on a DDE model of tumor growth that the delay as duration approach for\nstochastic simulation is not precise, and we propose a simulation algorithm\nbased on a ``purely delayed'' interpretation of delays which provides better\nresults on the considered model.", 
    "link": "http://arxiv.org/pdf/0910.1219v1", 
    "arxiv-id": "0910.1219v1"
},{
    "category": "q-bio.MN", 
    "author": "Giancarlo Mauri", 
    "title": "A study on the combined interplay between stochastic fluctuations and   the number of flagella in bacterial chemotaxis", 
    "publish": "2009-10-08T05:31:06Z", 
    "summary": "The chemotactic pathway allows bacteria to respond and adapt to environmental\nchanges, by tuning the tumbling and running motions that are due to clockwise\nand counterclockwise rotations of their flagella. The pathway is tightly\nregulated by feedback mechanisms governed by the phosphorylation and\nmethylation of several proteins. In this paper, we present a detailed\nmechanistic model for chemotaxis, that considers all of its transmembrane and\ncytoplasmic components, and their mutual interactions. Stochastic simulations\nof the dynamics of a pivotal protein, CheYp, are performed by means of tau\nleaping algorithm. This approach is then used to investigate the interplay\nbetween the stochastic fluctuations of CheYp amount and the number of cellular\nflagella. Our results suggest that the combination of these factors might\nrepresent a relevant component for chemotaxis. Moreover, we study the pathway\nunder various conditions, such as different methylation levels and ligand\namounts, in order to test its adaptation response. Some issues for future work\nare finally discussed.", 
    "link": "http://arxiv.org/pdf/0910.1415v1", 
    "arxiv-id": "0910.1415v1"
},{
    "category": "cs.CE", 
    "author": "Erik de Vink", 
    "title": "Proceedings Second International Workshop on Computational Models for   Cell Processes", 
    "publish": "2009-10-08T20:15:51Z", 
    "summary": "The second international workshop on Computational Models for Cell Processes\n(ComProc 2009) took place on November 3, 2009 at the Eindhoven University of\nTechnology, in conjunction with Formal Methods 2009. The workshop was jointly\norganized with the EC-MOAN project. This volume contains the final versions of\nall contributions accepted for presentation at the workshop.", 
    "link": "http://arxiv.org/pdf/0910.1605v2", 
    "arxiv-id": "0910.1605v2"
},{
    "category": "cs.CE", 
    "author": "Tshilidzi Marwala", 
    "title": "State of the Art Review for Applying Computational Intelligence and   Machine Learning Techniques to Portfolio Optimisation", 
    "publish": "2009-10-13T15:53:45Z", 
    "summary": "Computational techniques have shown much promise in the field of Finance,\nowing to their ability to extract sense out of dauntingly complex systems. This\npaper reviews the most promising of these techniques, from traditional\ncomputational intelligence methods to their machine learning siblings, with\nparticular view to their application in optimising the management of a\nportfolio of financial instruments. The current state of the art is assessed,\nand prospective further work is assessed and recommended", 
    "link": "http://arxiv.org/pdf/0910.2276v1", 
    "arxiv-id": "0910.2276v1"
},{
    "category": "cs.CE", 
    "author": "Sebastian Will", 
    "title": "Equivalence Classes of Optimal Structures in HP Protein Models Including   Side Chains", 
    "publish": "2009-10-20T15:18:27Z", 
    "summary": "Lattice protein models, as the Hydrophobic-Polar (HP) model, are a common\nabstraction to enable exhaustive studies on structure, function, or evolution\nof proteins. A main issue is the high number of optimal structures, resulting\nfrom the hydrophobicity-based energy function applied. We introduce an\nequivalence relation on protein structures that correlates to the energy\nfunction. We discuss the efficient enumeration of optimal representatives of\nthe corresponding equivalence classes and the application of the results.", 
    "link": "http://arxiv.org/pdf/0910.3848v1", 
    "arxiv-id": "0910.3848v1"
},{
    "category": "cs.CE", 
    "author": "Rolf Backofen", 
    "title": "Constraint-based Local Move Definitions for Lattice Protein Models   Including Side Chains", 
    "publish": "2009-10-20T15:37:52Z", 
    "summary": "The simulation of a protein's folding process is often done via stochastic\nlocal search, which requires a procedure to apply structural changes onto a\ngiven conformation. Here, we introduce a constraint-based approach to enumerate\nlattice protein structures according to k-local moves in arbitrary lattices.\nOur declarative description is much more flexible for extensions than standard\noperational formulations. It enables a generic calculation of k-local neighbors\nin backbone-only and side chain models. We exemplify the procedure using a\nsimple hierarchical folding scheme.", 
    "link": "http://arxiv.org/pdf/0910.3880v1", 
    "arxiv-id": "0910.3880v1"
},{
    "category": "cs.CE", 
    "author": "James Cusick", 
    "title": "Biological Computing Fundamentals and Futures", 
    "publish": "2009-11-09T13:16:01Z", 
    "summary": "The fields of computing and biology have begun to cross paths in new ways. In\nthis paper a review of the current research in biological computing is\npresented. Fundamental concepts are introduced and these foundational elements\nare explored to discuss the possibilities of a new computing paradigm. We\nassume the reader to possess a basic knowledge of Biology and Computer Science", 
    "link": "http://arxiv.org/pdf/0911.1672v1", 
    "arxiv-id": "0911.1672v1"
},{
    "category": "cs.LO", 
    "author": "Angelo Troina", 
    "title": "A Type System for Required/Excluded Elements in CLS", 
    "publish": "2009-11-12T08:44:27Z", 
    "summary": "The calculus of looping sequences is a formalism for describing the evolution\nof biological systems by means of term rewriting rules. We enrich this calculus\nwith a type discipline to guarantee the soundness of reduction rules with\nrespect to some biological properties deriving from the requirement of certain\nelements, and the repellency of others. As an example, we model a toy system\nwhere the repellency of a certain element is captured by our type system and\nforbids another element to exit a compartment.", 
    "link": "http://arxiv.org/pdf/0911.2323v1", 
    "arxiv-id": "0911.2323v1"
},{
    "category": "cs.NE", 
    "author": "David Wozabal", 
    "title": "Evolutionary estimation of a Coupled Markov Chain credit risk model", 
    "publish": "2009-11-19T10:43:16Z", 
    "summary": "There exists a range of different models for estimating and simulating credit\nrisk transitions to optimally manage credit risk portfolios and products. In\nthis chapter we present a Coupled Markov Chain approach to model rating\ntransitions and thereby default probabilities of companies. As the likelihood\nof the model turns out to be a non-convex function of the parameters to be\nestimated, we apply heuristics to find the ML estimators. To this extent, we\noutline the model and its likelihood function, and present both a Particle\nSwarm Optimization algorithm, as well as an Evolutionary Optimization algorithm\nto maximize the likelihood function. Numerical results are shown which suggest\na further application of evolutionary optimization techniques for credit risk\nmanagement.", 
    "link": "http://arxiv.org/pdf/0911.3753v1", 
    "arxiv-id": "0911.3753v1"
},{
    "category": "cs.SE", 
    "author": "J. -V. Peetz", 
    "title": "Modular Workflow Engine for Distributed Services using Lightweight Java   Clients", 
    "publish": "2009-12-03T01:31:36Z", 
    "summary": "In this article we introduce the concept and the first implementation of a\nlightweight client-server-framework as middleware for distributed computing. On\nthe client side an installation without administrative rights or privileged\nports can turn any computer into a worker node. Only a Java runtime environment\nand the JAR files comprising the workflow client are needed. To connect all\nclients to the engine one open server port is sufficient. The engine submits\ndata to the clients and orchestrates their work by workflow descriptions from a\ncentral database. Clients request new task descriptions periodically, thus the\nsystem is robust against network failures. In the basic set-up, data up- and\ndownloads are handled via HTTP communication with the server. The performance\nof the modular system could additionally be improved using dedicated file\nservers or distributed network file systems.\n  We demonstrate the design features of the proposed engine in real-world\napplications from mechanical engineering. We have used this system on a compute\ncluster in design-of-experiment studies, parameter optimisations and robustness\nvalidations of finite element structures.", 
    "link": "http://arxiv.org/pdf/0912.0549v1", 
    "arxiv-id": "0912.0549v1"
},{
    "category": "cs.CE", 
    "author": "A. B. Mutiara", 
    "title": "Performance Analysis on Molecular Dynamics Simulation of Protein Using   GROMACS", 
    "publish": "2009-12-04T16:47:24Z", 
    "summary": "Development of computer technology in chemistry, bring many application of\nchemistry. Not only the application to visualize the structure of molecule but\nalso to molecular dynamics simulation. One of them is Gromacs. Gromacs is an\nexample of molecular dynamics application developed by Groningen University.\nThis application is a non-commercial and able to work in the operating system\nLinux. The main ability of Gromacs is to perform molecular dynamics simulation\nand minimization energy. In this paper, the author discusses about how to work\nGromacs in molecular dynamics simulation of some protein. In the molecular\ndynamics simulation, Gromacs does not work alone. Gromacs interact with pymol\nand Grace. Pymol is an application to visualize molecule structure and Grace is\nan application in Linux to display graphs. Both applications will support\nanalysis of molecular dynamics simulation.", 
    "link": "http://arxiv.org/pdf/0912.0893v1", 
    "arxiv-id": "0912.0893v1"
},{
    "category": "cs.NE", 
    "author": "Dr. Mrs. S. Wadhwani", 
    "title": "Short Term Load Forecasting Using Multi Parameter Regression", 
    "publish": "2009-12-05T13:18:35Z", 
    "summary": "Short Term Load forecasting in this paper uses input data dependent on\nparameters such as load for current hour and previous two hours, temperature\nfor current hour and previous two hours, wind for current hour and previous two\nhours, cloud for current hour and previous two hours. Forecasting will be of\nload demand for coming hour based on input parameters at that hour. In this\npaper we are using multiparameter regression method for forecasting which has\nerror within tolerable range. Algorithms implementing these forecasting\ntechniques have been programmed using MATLAB and applied to the case study.\nOther methodologies in this area are ANN, Fuzzy and Evolutionary Algorithms for\nwhich investigations are under process. Adaptive multiparameter regression for\nload forecasting, in near future will be possible.", 
    "link": "http://arxiv.org/pdf/0912.1015v1", 
    "arxiv-id": "0912.1015v1"
},{
    "category": "cs.CE", 
    "author": "Susan Khor", 
    "title": "Application of Graph Coloring to Biological Networks", 
    "publish": "2009-12-17T17:01:13Z", 
    "summary": "We explore the application of graph coloring to biological networks,\nspecifically protein-protein interaction (PPI) networks. First, we find that\ngiven similar conditions (i.e. number of nodes, number of links, degree\ndistribution and clustering), fewer colors are needed to color disassortative\n(high degree nodes tend to connect to low degree nodes and vice versa) than\nassortative networks. Fewer colors create fewer independent sets which in turn\nimply higher concurrency potential for a network. Since PPI networks tend to be\ndisassortative, we suggest that in addition to functional specificity and\nstability proposed previously by Maslov and Sneppen (Science 296, 2002), the\ndisassortative nature of PPI networks may promote the ability of cells to\nperform multiple, crucial and functionally diverse tasks concurrently. Second,\nsince graph coloring is closely related to the presence of cliques in a graph,\nthe significance of node coloring information to the problem of identifying\nprotein complexes, i.e. dense subgraphs in a PPI network, is investigated. We\nfind that for PPI networks where 1% to 11% of nodes participate in at least one\nidentified protein complex, such as H. sapien (DIP20070219, DIP20081014 and\nHPRD070609), DSATUR (a well-known complete graph coloring algorithm) node\ncoloring information can improve the quality (homogeneity and separation) of\ninitial candidate complexes. This finding may help to improve existing protein\ncomplex detection methods, and/or suggest new methods.", 
    "link": "http://arxiv.org/pdf/0912.3461v1", 
    "arxiv-id": "0912.3461v1"
},{
    "category": "cs.CE", 
    "author": "Erman. Evgin", 
    "title": "Complexity Analysis of Unsaturated Flow in Heterogeneous Media Using a   Complex Network Approach", 
    "publish": "2009-12-26T01:11:19Z", 
    "summary": "In this study, we investigate the complexity of two-phase flow (air/water) in\na heterogeneous soil sample by using complex network theory, where the supposed\nporous media is non-deformable media, under the time-dependent gas pressure.\nBased on the different similarity measurements (i.e., correlation, Euclidean\nmetrics) over the emerged patterns from the evolution of saturation of\nnon-wetting phase of a multi-heterogeneous soil sample, the emerged complex\nnetworks are recognized. Understanding of the properties of complex networks\n(such degree distribution, mean path length, clustering coefficient) can be\nsupposed as a way to analysis of variation of saturation profiles structures\n(as the solution of finite element method on the coupled PDEs) where complexity\nis coming from the changeable connection and links between assumed nodes. Also,\nthe path of evolution of the supposed system will be illustrated on the state\nspace of networks either in correlation and Euclidean measurements. The results\nof analysis showed in a closed system the designed complex networks approach to\nsmall world network where the mean path length and clustering coefficient are\nlow and high, respectively. As another result, the evolution of macro -states\nof system (such mean velocity of air or pressure) can be scaled with\ncharacteristics of structure complexity of saturation. In other part, we tried\nto find a phase transition criterion based on the variation of non-wetting\nphase velocity profiles over a network which had been constructed over\ncorrelation distance.", 
    "link": "http://arxiv.org/pdf/0912.4991v2", 
    "arxiv-id": "0912.4991v2"
},{
    "category": "cs.CE", 
    "author": "Weichuan Yu", 
    "title": "Stable Feature Selection for Biomarker Discovery", 
    "publish": "2010-01-06T13:11:35Z", 
    "summary": "Feature selection techniques have been used as the workhorse in biomarker\ndiscovery applications for a long time. Surprisingly, the stability of feature\nselection with respect to sampling variations has long been under-considered.\nIt is only until recently that this issue has received more and more attention.\nIn this article, we review existing stable feature selection methods for\nbiomarker discovery using a generic hierarchal framework. We have two\nobjectives: (1) providing an overview on this new yet fast growing topic for a\nconvenient reference; (2) categorizing existing methods under an expandable\nframework for future research and development.", 
    "link": "http://arxiv.org/pdf/1001.0887v1", 
    "arxiv-id": "1001.0887v1"
},{
    "category": "cs.CE", 
    "author": "Feng Luo", 
    "title": "Effectively integrating information content and structural relationship   to improve the GO-based similarity measure between proteins", 
    "publish": "2010-01-06T19:57:22Z", 
    "summary": "The Gene Ontology (GO) provides a knowledge base to effectively describe\nproteins. However, measuring similarity between proteins based on GO remains a\nchallenge. In this paper, we propose a new similarity measure, information\ncoefficient similarity measure (SimIC), to effectively integrate both the\ninformation content (IC) of GO terms and the structural information of GO\nhierarchy to determine the similarity between proteins. Testing on yeast\nproteins, our results show that SimIC efficiently addresses the shallow\nannotation issue in GO, thus improves the correlations between GO similarities\nof yeast proteins and their expression similarities as well as between GO\nsimilarities of yeast proteins and their sequence similarities. Furthermore, we\ndemonstrate that the proposed SimIC is superior in predicting yeast protein\ninteractions. We predict 20484 yeast protein-protein interactions (PPIs)\nbetween 2462 proteins based on the high SimIC values of biological process (BP)\nand cellular component (CC). Examining the 214 MIPS complexes in our predicted\nPPIs shows that all members of 159 MIPS complexes can be found in our PPI\npredictions, which is more than those (120/214) found in PPIs predicted by\nrelative specificity similarity (RSS). Integrating IC and structural\ninformation of GO hierarchy can improve the effectiveness of the semantic\nsimilarity measure of GO terms. The new SimIC can effectively correct the\neffect of shallow annotation, and then provide an effective way to measure\nsimilarity between proteins based on Gene Ontology.", 
    "link": "http://arxiv.org/pdf/1001.0958v2", 
    "arxiv-id": "1001.0958v2"
},{
    "category": "cs.CE", 
    "author": "Romeo Rizzi", 
    "title": "Pure Parsimony Xor Haplotyping", 
    "publish": "2010-01-08T07:55:44Z", 
    "summary": "The haplotype resolution from xor-genotype data has been recently formulated\nas a new model for genetic studies. The xor-genotype data is a cheaply\nobtainable type of data distinguishing heterozygous from homozygous sites\nwithout identifying the homozygous alleles. In this paper we propose a\nformulation based on a well-known model used in haplotype inference: pure\nparsimony. We exhibit exact solutions of the problem by providing polynomial\ntime algorithms for some restricted cases and a fixed-parameter algorithm for\nthe general case. These results are based on some interesting combinatorial\nproperties of a graph representation of the solutions. Furthermore, we show\nthat the problem has a polynomial time k-approximation, where k is the maximum\nnumber of xor-genotypes containing a given SNP. Finally, we propose a heuristic\nand produce an experimental analysis showing that it scales to real-world large\ninstances taken from the HapMap project.", 
    "link": "http://arxiv.org/pdf/1001.1210v1", 
    "arxiv-id": "1001.1210v1"
},{
    "category": "q-bio.GN", 
    "author": "Durg Singh Chauhan", 
    "title": "DNA-MATRIX a tool for DNA motif discovery and weight matrix construction", 
    "publish": "2010-01-12T19:17:23Z", 
    "summary": "In computational molecular biology, gene regulatory binding sites prediction\nin whole genome remains a challenge for the researchers. Now a days, the genome\nwide regulatory binding site prediction tools required either direct pattern\nsequence or weight matrix. Although there are known transcription factor\nbinding sites databases available for genome wide prediction but no tool is\navailable which can construct different weight matrices as per need of user or\ntools available for large data set scanning by first aligning the input\nupstream or promoter sequences and than construct the matrices in different\nlevel and file format. Considering this, we developed a DNA MATRIX tool for\nsearching putative regulatory binding sites in gene upstream sequences. This\ntool uses the simple biological rule based heuristic algorithm for weight\nmatrix construction, which can be transformed into different formats after\nmotif alignment and therefore provides the possibility to identify the most\npotential conserved binding sites in the regulated genes. The user may\nconstruct and save specific weight or frequency matrices in different form and\nfile formats based on user based selection of conserved aligned block of short\nsequences ranges from 6 to 20 base pairs and prior nucleotide frequency before\nweight scoring.", 
    "link": "http://arxiv.org/pdf/1001.1984v2", 
    "arxiv-id": "1001.1984v2"
},{
    "category": "cs.CE", 
    "author": "Madhvi Shakya", 
    "title": "Mathematical Modeling to Study the Dynamics of A Diatomic Molecule N2 in   Water", 
    "publish": "2010-01-20T08:15:18Z", 
    "summary": "In the present work an attempt has been made to study the dynamics of a\ndiatomic molecule N2 in water. The proposed model consists of Langevin\nstochastic differential equation whose solution is obtained through Euler's\nmethod. The proposed work has been concluded by studying the behavior of\nstatistical parameters like variance in position, variance in velocity and\ncovariance between position and velocity. This model incorporates the important\nparameters like acceleration, intermolecular force, frictional force and random\nforce.", 
    "link": "http://arxiv.org/pdf/1001.3500v3", 
    "arxiv-id": "1001.3500v3"
},{
    "category": "cs.GR", 
    "author": "Lautaro Salazar Silva", 
    "title": "Modelacion y Visualizacion Tridimensional Interactiva de Variables   Electricas en Celdas de Electro-Obtencion con Electrodos Bipolares", 
    "publish": "2010-01-22T12:57:59Z", 
    "summary": "The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This paper presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow.", 
    "link": "http://arxiv.org/pdf/1001.3974v2", 
    "arxiv-id": "1001.3974v2"
},{
    "category": "cs.GR", 
    "author": "C\u00e9sar Mena Labra\u00f1a", 
    "title": "Aplicacion Grafica para el estudio de un Modelo de Celda Electrolitica   usando Tecnicas de Visualizacion de Campos Vectoriales", 
    "publish": "2010-01-22T18:23:27Z", 
    "summary": "The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This thesis presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow.", 
    "link": "http://arxiv.org/pdf/1001.4002v1", 
    "arxiv-id": "1001.4002v1"
},{
    "category": "cs.MA", 
    "author": "George J. Pappas", 
    "title": "Distributed Control of the Laplacian Spectral Moments of a Network", 
    "publish": "2010-01-23T02:31:26Z", 
    "summary": "It is well-known that the eigenvalue spectrum of the Laplacian matrix of a\nnetwork contains valuable information about the network structure and the\nbehavior of many dynamical processes run on it. In this paper, we propose a\nfully decentralized algorithm that iteratively modifies the structure of a\nnetwork of agents in order to control the moments of the Laplacian eigenvalue\nspectrum. Although the individual agents have knowledge of their local network\nstructure only (i.e., myopic information), they are collectively able to\naggregate this local information and decide on what links are most beneficial\nto be added or removed at each time step. Our approach relies on gossip\nalgorithms to distributively compute the spectral moments of the Laplacian\nmatrix, as well as ensure network connectivity in the presence of link\ndeletions. We illustrate our approach in nontrivial computer simulations and\nshow that a good final approximation of the spectral moments of the target\nLaplacian matrix is achieved for many cases of interest.", 
    "link": "http://arxiv.org/pdf/1001.4122v1", 
    "arxiv-id": "1001.4122v1"
},{
    "category": "q-bio.QM", 
    "author": "G. Sahoo", 
    "title": "A Probabilistic Model For Sequence Analysis", 
    "publish": "2010-02-11T20:00:54Z", 
    "summary": "This paper presents a probabilistic approach for DNA sequence analysis. A DNA\nsequence consists of an arrangement of the four nucleotides A, C, T and G and\ndifferent representation schemes are presented according to a probability\nmeasure associated with them. There are different ways that probability can be\nassociated with the DNA sequence: one way is when the probability of an\noccurrence of a letter does not depend on the previous one (termed as\nunsuccessive probability) and in another scheme the probability of occurrence\nof a letter depends on its previous letter (termed as successive probability).\nFurther, based on these probability measures graphical representations of the\nschemes are also presented. Using the diagram probability measure one can\neasily calculate an associated probability measure which can serve as a\nparameter to check how close is a new sequence to already existing ones.", 
    "link": "http://arxiv.org/pdf/1002.2412v1", 
    "arxiv-id": "1002.2412v1"
},{
    "category": "cs.CE", 
    "author": "Muffy Calder", 
    "title": "Modelling and Analysis of Biochemical Signalling Pathway Cross-talk", 
    "publish": "2010-02-22T06:33:00Z", 
    "summary": "Signalling pathways are abstractions that help life scientists structure the\ncoordination of cellular activity. Cross-talk between pathways accounts for\nmany of the complex behaviours exhibited by signalling pathways and is often\ncritical in producing the correct signal-response relationship. Formal models\nof signalling pathways and cross-talk in particular can aid understanding and\ndrive experimentation. We define an approach to modelling based on the concept\nthat a pathway is the (synchronising) parallel composition of instances of\ngeneric modules (with internal and external labels). Pathways are then composed\nby (synchronising) parallel composition and renaming; different types of\ncross-talk result from different combinations of synchronisation and renaming.\nWe define a number of generic modules in PRISM and five types of cross-talk:\nsignal flow, substrate availability, receptor function, gene expression and\nintracellular communication. We show that Continuous Stochastic Logic\nproperties can both detect and distinguish the types of cross-talk. The\napproach is illustrated with small examples and an analysis of the cross-talk\nbetween the TGF-b/BMP, WNT and MAPK pathways.", 
    "link": "http://arxiv.org/pdf/1002.4062v1", 
    "arxiv-id": "1002.4062v1"
},{
    "category": "cs.CE", 
    "author": "Jane Hillston", 
    "title": "Investigating modularity in the analysis of process algebra models of   biochemical systems", 
    "publish": "2010-02-22T06:36:41Z", 
    "summary": "Compositionality is a key feature of process algebras which is often cited as\none of their advantages as a modelling technique. It is certainly true that in\nbiochemical systems, as in many other systems, model construction is made\neasier in a formalism which allows the problem to be tackled compositionally.\nIn this paper we consider the extent to which the compositional structure which\nis inherent in process algebra models of biochemical systems can be exploited\nduring model solution. In essence this means using the compositional structure\nto guide decomposed solution and analysis.\n  Unfortunately the dynamic behaviour of biochemical systems exhibits strong\ninterdependencies between the components of the model making decomposed\nsolution a difficult task. Nevertheless we believe that if such decomposition\nbased on process algebras could be established it would demonstrate substantial\nbenefits for systems biology modelling. In this paper we present our\npreliminary investigations based on a case study of the pheromone pathway in\nyeast, modelling in the stochastic process algebra Bio-PEPA.", 
    "link": "http://arxiv.org/pdf/1002.4063v1", 
    "arxiv-id": "1002.4063v1"
},{
    "category": "cs.CE", 
    "author": "Adelinde M. Uhrmacher", 
    "title": "A flexible architecture for modeling and simulation of diffusional   association", 
    "publish": "2010-02-22T06:38:58Z", 
    "summary": "Up to now, it is not possible to obtain analytical solutions for complex\nmolecular association processes (e.g. Molecule recognition in Signaling or\ncatalysis). Instead Brownian Dynamics (BD) simulations are commonly used to\nestimate the rate of diffusional association, e.g. to be later used in\nmesoscopic simulations. Meanwhile a portfolio of diffusional association (DA)\nmethods have been developed that exploit BD.\n  However, DA methods do not clearly distinguish between modeling, simulation,\nand experiment settings. This hampers to classify and compare the existing\nmethods with respect to, for instance model assumptions, simulation\napproximations or specific optimization strategies for steering the computation\nof trajectories.\n  To address this deficiency we propose FADA (Flexible Architecture for\nDiffusional Association) - an architecture that allows the flexible definition\nof the experiment comprising a formal description of the model in SpacePi,\ndifferent simulators, as well as validation and analysis methods. Based on the\nNAM (Northrup-Allison-McCammon) method, which forms the basis of many existing\nDA methods, we illustrate the structure and functioning of FADA. A discussion\nof future validation experiments illuminates how the FADA can be exploited in\norder to estimate reaction rates and how validation techniques may be applied\nto validate additional features of the model.", 
    "link": "http://arxiv.org/pdf/1002.4064v1", 
    "arxiv-id": "1002.4064v1"
},{
    "category": "cs.CE", 
    "author": "Corrado Priami", 
    "title": "BlenX-based compositional modeling of complex reaction mechanisms", 
    "publish": "2010-02-22T06:40:24Z", 
    "summary": "Molecular interactions are wired in a fascinating way resulting in complex\nbehavior of biological systems. Theoretical modeling provides a useful\nframework for understanding the dynamics and the function of such networks. The\ncomplexity of the biological networks calls for conceptual tools that manage\nthe combinatorial explosion of the set of possible interactions. A suitable\nconceptual tool to attack complexity is compositionality, already successfully\nused in the process algebra field to model computer systems. We rely on the\nBlenX programming language, originated by the beta-binders process calculus, to\nspecify and simulate high-level descriptions of biological circuits. The\nGillespie's stochastic framework of BlenX requires the decomposition of\nphenomenological functions into basic elementary reactions. Systematic\nunpacking of complex reaction mechanisms into BlenX templates is shown in this\nstudy. The estimation/derivation of missing parameters and the challenges\nemerging from compositional model building in stochastic process algebras are\ndiscussed. A biological example on circadian clock is presented as a case study\nof BlenX compositionality.", 
    "link": "http://arxiv.org/pdf/1002.4065v1", 
    "arxiv-id": "1002.4065v1"
},{
    "category": "cs.CE", 
    "author": "Roberta Gori", 
    "title": "A Taxonomy of Causality-Based Biological Properties", 
    "publish": "2010-02-22T06:44:34Z", 
    "summary": "We formally characterize a set of causality-based properties of metabolic\nnetworks. This set of properties aims at making precise several notions on the\nproduction of metabolites, which are familiar in the biologists' terminology.\n  From a theoretical point of view, biochemical reactions are abstractly\nrepresented as causal implications and the produced metabolites as causal\nconsequences of the implication representing the corresponding reaction. The\nfact that a reactant is produced is represented by means of the chain of\nreactions that have made it exist. Such representation abstracts away from\nquantities, stoichiometric and thermodynamic parameters and constitutes the\nbasis for the characterization of our properties. Moreover, we propose an\neffective method for verifying our properties based on an abstract model of\nsystem dynamics. This consists of a new abstract semantics for the system seen\nas a concurrent network and expressed using the Chemical Ground Form calculus.\n  We illustrate an application of this framework to a portion of a real\nmetabolic pathway.", 
    "link": "http://arxiv.org/pdf/1002.4067v1", 
    "arxiv-id": "1002.4067v1"
},{
    "category": "cs.CE", 
    "author": "Paola Quaglia", 
    "title": "Proceedings Third Workshop From Biology To Concurrency and back", 
    "publish": "2010-02-26T03:54:16Z", 
    "summary": "This volume contains the papers presented at the 3rd Workshop \"From Biology\nTo Concurrency and back\", FBTC 2010, held in Paphos, Cyprus, on March 27, 2010,\nas satellite event of the Joint European Conference on Theory and Practice of\nSoftware, ETAPS 2010.\n  The Workshop aimed at gathering together researchers with special interest at\nthe convergence of life and computer science, with particular focus on the\napplication of techniques and methods from concurrency. The papers contained in\nthis volume present works on modelling, analysis, and validation of biological\nbehaviours using concurrency-inspired methods and platforms, and bio-inspired\nmodels and tools for describing distributed interactions.", 
    "link": "http://arxiv.org/pdf/1002.4919v1", 
    "arxiv-id": "1002.4919v1"
},{
    "category": "cs.AI", 
    "author": "Uwe Aickelin", 
    "title": "Optimisation of a Crossdocking Distribution Centre Simulation Model", 
    "publish": "2010-03-19T11:46:30Z", 
    "summary": "This paper reports on continuing research into the modelling of an order\npicking process within a Crossdocking distribution centre using Simulation\nOptimisation. The aim of this project is to optimise a discrete event\nsimulation model and to understand factors that affect finding its optimal\nperformance. Our initial investigation revealed that the precision of the\nselected simulation output performance measure and the number of replications\nrequired for the evaluation of the optimisation objective function through\nsimulation influences the ability of the optimisation technique. We\nexperimented with Common Random Numbers, in order to improve the precision of\nour simulation output performance measure, and intended to use the number of\nreplications utilised for this purpose as the initial number of replications\nfor the optimisation of our Crossdocking distribution centre simulation model.\nOur results demonstrate that we can improve the precision of our selected\nsimulation output performance measure value using Common Random Numbers at\nvarious levels of replications. Furthermore, after optimising our Crossdocking\ndistribution centre simulation model, we are able to achieve optimal\nperformance using fewer simulations runs for the simulation model which uses\nCommon Random Numbers as compared to the simulation model which does not use\nCommon Random Numbers.", 
    "link": "http://arxiv.org/pdf/1003.3775v1", 
    "arxiv-id": "1003.3775v1"
},{
    "category": "cs.MA", 
    "author": "Chris Clegg", 
    "title": "Simulating Customer Experience and Word Of Mouth in Retail - A Case   Study", 
    "publish": "2010-03-19T12:31:55Z", 
    "summary": "Agents offer a new and exciting way of understanding the world of work. In\nthis paper we describe the development of agent-based simulation models,\ndesigned to help to understand the relationship between people management\npractices and retail performance. We report on the current development of our\nsimulation models which includes new features concerning the evolution of\ncustomers over time. To test the features we have conducted a series of\nexperiments dealing with customer pool sizes, standard and noise reduction\nmodes, and the spread of customers' word of mouth. To validate and evaluate our\nmodel, we introduce new performance measure specific to retail operations. We\nshow that by varying different parameters in our model we can simulate a range\nof customer experiences leading to significant differences in performance\nmeasures. Ultimately, we are interested in better understanding the impact of\nchanges in staff behavior due to changes in store management practices. Our\nmulti-disciplinary research team draws upon expertise from work psychologists\nand computer scientists. Despite the fact we are working within a relatively\nnovel and complex domain, it is clear that intelligent agents offer potential\nfor fostering sustainable organizational capabilities in the future.", 
    "link": "http://arxiv.org/pdf/1003.3784v1", 
    "arxiv-id": "1003.3784v1"
},{
    "category": "math.OC", 
    "author": "Anna Ivanova", 
    "title": "Identification of Convection Heat Transfer Coefficient of Secondary   Cooling Zone of CCM based on Least Squares Method and Stochastic   Approximation Method", 
    "publish": "2010-03-24T14:54:22Z", 
    "summary": "The detailed mathematical model of heat and mass transfer of steel ingot of\ncurvilinear continuous casting machine is proposed. The process of heat and\nmass transfer is described by nonlinear partial differential equations of\nparabolic type. Position of phase boundary is determined by Stefan conditions.\nThe temperature of cooling water in mould channel is described by a special\nbalance equation. Boundary conditions of secondary cooling zone include radiant\nand convective components of heat exchange and account for the complex\nmechanism of heat-conducting due to airmist cooling using compressed air and\nwater. Convective heat-transfer coefficient of secondary cooling zone is\nunknown and considered as distributed parameter. To solve this problem the\nalgorithm of initial adjustment of parameter and the algorithm of operative\nadjustment are developed.", 
    "link": "http://arxiv.org/pdf/1003.4657v1", 
    "arxiv-id": "1003.4657v1"
},{
    "category": "math.OC", 
    "author": "Michael Chertkov", 
    "title": "A Majorization-Minimization Approach to Design of Power Transmission   Networks", 
    "publish": "2010-04-13T23:07:07Z", 
    "summary": "We propose an optimization approach to design cost-effective electrical power\ntransmission networks. That is, we aim to select both the network structure and\nthe line conductances (line sizes) so as to optimize the trade-off between\nnetwork efficiency (low power dissipation within the transmission network) and\nthe cost to build the network. We begin with a convex optimization method based\non the paper ``Minimizing Effective Resistance of a Graph'' [Ghosh, Boyd \\&\nSaberi]. We show that this (DC) resistive network method can be adapted to the\ncontext of AC power flow. However, that does not address the combinatorial\naspect of selecting network structure. We approach this problem as selecting a\nsubgraph within an over-complete network, posed as minimizing the (convex)\nnetwork power dissipation plus a non-convex cost on line conductances that\nencourages sparse networks where many line conductances are set to zero. We\ndevelop a heuristic approach to solve this non-convex optimization problem\nusing: (1) a continuation method to interpolate from the smooth, convex problem\nto the (non-smooth, non-convex) combinatorial problem, (2) the\nmajorization-minimization algorithm to perform the necessary intermediate\nsmooth but non-convex optimization steps. Ultimately, this involves solving a\nsequence of convex optimization problems in which we iteratively reweight a\nlinear cost on line conductances to fit the actual non-convex cost. Several\nexamples are presented which suggest that the overall method is a good\nheuristic for network design. We also consider how to obtain sparse networks\nthat are still robust against failures of lines and/or generators.", 
    "link": "http://arxiv.org/pdf/1004.2285v2", 
    "arxiv-id": "1004.2285v2"
},{
    "category": "cs.CE", 
    "author": "Malcolm Roberts", 
    "title": "Efficient Dealiased Convolutions without Padding", 
    "publish": "2010-08-07T21:36:51Z", 
    "summary": "Algorithms are developed for calculating dealiased linear convolution sums\nwithout the expense of conventional zero-padding or phase-shift techniques. For\none-dimensional in-place convolutions, the memory requirements are identical\nwith the zero-padding technique, with the important distinction that the\nadditional work memory need not be contiguous with the input data. This\ndecoupling of data and work arrays dramatically reduces the memory and\ncomputation time required to evaluate higher-dimensional in-place convolutions.\nThe technique also allows one to dealias the higher-order convolutions that\narise from Fourier transforming cubic and higher powers. Implicitly dealiased\nconvolutions can be built on top of state-of-the-art fast Fourier transform\nlibraries: vectorized multidimensional implementations for the complex and\ncentered Hermitian (pseudospectral) cases have been implemented in the\nopen-source software FFTW++.", 
    "link": "http://arxiv.org/pdf/1008.1366v2", 
    "arxiv-id": "1008.1366v2"
},{
    "category": "cs.CE", 
    "author": "Mario de J. P\u00e9rez Jim\u00e9nez", 
    "title": "Proceedings First Workshop on Applications of Membrane computing,   Concurrency and Agent-based modelling in POPulation biology", 
    "publish": "2010-08-18T18:32:44Z", 
    "summary": "This volume contains the papers presented at the first International Workshop\non Applications of Membrane Computing, Concurrency and Agent-based Modelling in\nPopulation Biology (AMCA-POP 2010) held in Jena, Germany on August 25th, 2010\nas a satellite event of the 11th Conference on Membrane Computing (CMC11).\n  The aim of the workshop is to investigate whether formal modelling and\nanalysis techniques could be applied with profit to systems of interest for\npopulation biology and ecology. The considered modelling notations include\nmembrane systems, Petri nets, agent-based notations, process calculi,\nautomata-based notations, rewriting systems and cellular automata. Such\nnotations enable the application of analysis techniques such as simulation,\nmodel checking, abstract interpretation and type systems to study systems of\ninterest in disciplines such as population biology, ecosystem science,\nepidemiology, genetics, sustainability science, evolution and other disciplines\nin which population dynamics and interactions with the environment are studied.\nPapers contain results and experiences in the modelling and analysis of systems\nof interest in these fields.", 
    "link": "http://arxiv.org/pdf/1008.3147v1", 
    "arxiv-id": "1008.3147v1"
},{
    "category": "cs.CE", 
    "author": "Giancarlo Mauri", 
    "title": "An Analysis on the Influence of Network Topologies on Local and Global   Dynamics of Metapopulation Systems", 
    "publish": "2010-08-19T14:00:57Z", 
    "summary": "Metapopulations are models of ecological systems, describing the interactions\nand the behavior of populations that live in fragmented habitats. In this\npaper, we present a model of metapopulations based on the multivolume\nsimulation algorithm tau-DPP, a stochastic class of membrane systems, that we\nutilize to investigate the influence that different habitat topologies can have\non the local and global dynamics of metapopulations. In particular, we focus\nour analysis on the migration rate of individuals among adjacent patches, and\non their capability of colonizing the empty patches in the habitat. We compare\nthe simulation results obtained for each habitat topology, and conclude the\npaper with some proposals for other research issues concerning metapopulations.", 
    "link": "http://arxiv.org/pdf/1008.3304v1", 
    "arxiv-id": "1008.3304v1"
},{
    "category": "cs.DS", 
    "author": "Luca Martini", 
    "title": "Celer: an Efficient Program for Genotype Elimination", 
    "publish": "2010-08-19T14:01:01Z", 
    "summary": "This paper presents an efficient program for checking Mendelian consistency\nin a pedigree. Since pedigrees may contain incomplete and/or erroneous\ninformation, geneticists need to pre-process them before performing linkage\nanalysis. Removing superfluous genotypes that do not respect the Mendelian\ninheritance laws can speed up the linkage analysis. We have described in a\nformal way the Mendelian consistency problem and algorithms known in\nliterature. The formalization helped to polish the algorithms and to find\nefficient data structures. The performance of the tool has been tested on a\nwide range of benchmarks. The results are promising if compared to other\nprograms that treat Mendelian consistency.", 
    "link": "http://arxiv.org/pdf/1008.3305v1", 
    "arxiv-id": "1008.3305v1"
},{
    "category": "cs.PL", 
    "author": "Paul Taylor", 
    "title": "An extensible web interface for databases and its application to storing   biochemical data", 
    "publish": "2010-09-20T11:06:25Z", 
    "summary": "This paper presents a generic web-based database interface implemented in\nProlog. We discuss the advantages of the implementation platform and\ndemonstrate the system's applicability in providing access to integrated\nbiochemical data. Our system exploits two libraries of SWI-Prolog to create a\nschema-transparent interface within a relational setting. As is expected in\ndeclarative programming, the interface was written with minimal programming\neffort due to the high level of the language and its suitability to the task.\nWe highlight two of Prolog's features that are well suited to the task at hand:\nterm representation of structured documents and relational nature of Prolog\nwhich facilitates transparent integration of relational databases. Although we\ndeveloped the system for accessing in-house biochemical and genomic data the\ninterface is generic and provides a number of extensible features. We describe\nsome of these features with references to our research databases. Finally we\noutline an in-house library that facilitates interaction between Prolog and the\nR statistical package. We describe how it has been employed in the present\ncontext to store output from statistical analysis on to the database.", 
    "link": "http://arxiv.org/pdf/1009.3771v1", 
    "arxiv-id": "1009.3771v1"
},{
    "category": "cs.DS", 
    "author": "Sanguthevar Rajasekaran", 
    "title": "A memory-efficient data structure representing exact-match overlap   graphs with application for next generation DNA assembly", 
    "publish": "2010-09-21T02:39:34Z", 
    "summary": "An exact-match overlap graph of $n$ given strings of length $\\ell$ is an\nedge-weighted graph in which each vertex is associated with a string and there\nis an edge $(x,y)$ of weight $\\omega = \\ell - |ov_{max}(x,y)|$ if and only if\n$\\omega \\leq \\lambda$, where $|ov_{max}(x,y)|$ is the length of $ov_{max}(x,y)$\nand $\\lambda$ is a given threshold. In this paper, we show that the exact-match\noverlap graphs can be represented by a compact data structure that can be\nstored using at most $(2\\lambda -1 )(2\\lceil\\log n\\rceil +\n\\lceil\\log\\lambda\\rceil)n$ bits with a guarantee that the basic operation of\naccessing an edge takes $O(\\log \\lambda)$ time.\n  Exact-match overlap graphs have been broadly used in the context of DNA\nassembly and the \\emph{shortest super string problem} where the number of\nstrings $n$ ranges from a couple of thousands to a couple of billions, the\nlength $\\ell$ of the strings is from 25 to 1000, depending on DNA sequencing\ntechnologies. However, many DNA assemblers using overlap graphs are facing a\nmajor problem of constructing and storing them. Especially, it is impossible\nfor these DNA assemblers to handle the huge amount of data produced by the next\ngeneration sequencing technologies where the number of strings $n$ is usually\nvery large ranging from hundred million to a couple of billions. In fact, to\nour best knowledge there is no DNA assemblers that can handle such a large\nnumber of strings. Fortunately, with our compact data structure, the major\nproblem of constructing and storing overlap graphs is practically solved since\nit only requires linear time and and linear memory. As a result, it opens the\ndoor of possibilities to build a DNA assembler that can handle large-scale\ndatasets efficiently.", 
    "link": "http://arxiv.org/pdf/1009.3984v1", 
    "arxiv-id": "1009.3984v1"
},{
    "category": "cs.CE", 
    "author": "Malik Magdon-Ismail", 
    "title": "Efficient Computation of Optimal Trading Strategies", 
    "publish": "2010-09-23T19:18:23Z", 
    "summary": "Given the return series for a set of instruments, a \\emph{trading strategy}\nis a switching function that transfers wealth from one instrument to another at\nspecified times. We present efficient algorithms for constructing (ex-post)\ntrading strategies that are optimal with respect to the total return, the\nSterling ratio and the Sharpe ratio. Such ex-post optimal strategies are useful\nanalysis tools. They can be used to analyze the \"profitability of a market\" in\nterms of optimal trading; to develop benchmarks against which real trading can\nbe compared; and, within an inductive framework, the optimal trades can be used\nto to teach learning systems (predictors) which are then used to identify\nfuture trading opportunities.", 
    "link": "http://arxiv.org/pdf/1009.4683v1", 
    "arxiv-id": "1009.4683v1"
},{
    "category": "math.NA", 
    "author": "Glaucio H. Paulino", 
    "title": "Dynamic Adaptive Mesh Refinement for Topology Optimization", 
    "publish": "2010-09-25T05:49:04Z", 
    "summary": "We present an improved method for topology optimization with both adaptive\nmesh refinement and derefinement. Since the total volume fraction in topology\noptimization is usually modest, after a few initial iterations the domain of\ncomputation is largely void. Hence, it is inefficient to have many small\nelements, in such regions, that contribute significantly to the overall\ncomputational cost but contribute little to the accuracy of computation and\ndesign. At the same time, we want high spatial resolution for accurate\nthree-dimensional designs to avoid postprocessing or interpretation as much as\npossible. Dynamic adaptive mesh refinement (AMR) offers the possibility to\nbalance these two requirements. We discuss requirements on AMR for topology\noptimization and the algorithmic features to implement them. The numerical\ndesign problems demonstrate (1) that our AMR strategy for topology optimization\nleads to designs that are equivalent to optimal designs on uniform meshes, (2)\nhow AMR strategies that do not satisfy the postulated requirements may lead to\nsuboptimal designs, and (3) that our AMR strategy significantly reduces the\ntime to compute optimal designs.", 
    "link": "http://arxiv.org/pdf/1009.4975v1", 
    "arxiv-id": "1009.4975v1"
},{
    "category": "cs.AI", 
    "author": "Ross Gayler", 
    "title": "A Comprehensive Survey of Data Mining-based Fraud Detection Research", 
    "publish": "2010-09-30T13:06:12Z", 
    "summary": "This survey paper categorises, compares, and summarises from almost all\npublished technical and review articles in automated fraud detection within the\nlast 10 years. It defines the professional fraudster, formalises the main types\nand subtypes of known fraud, and presents the nature of data evidence collected\nwithin affected industries. Within the business context of mining the data to\nachieve higher cost savings, this research presents methods and techniques\ntogether with their problems. Compared to all related reviews on fraud\ndetection, this survey covers much more technical articles and is the only one,\nto the best of our knowledge, which proposes alternative data and solutions\nfrom related domains.", 
    "link": "http://arxiv.org/pdf/1009.6119v1", 
    "arxiv-id": "1009.6119v1"
},{
    "category": "cs.CE", 
    "author": "Ali E. Y\u0131lmaz", 
    "title": "A Hybrid Parallelization of AIM for Multi-Core Clusters: Implementation   Details and Benchmark Results on Ranger", 
    "publish": "2010-10-07T15:35:24Z", 
    "summary": "This paper presents implementation details and empirical results for a hybrid\nmessage passing and shared memory paralleliziation of the adaptive integral\nmethod (AIM). AIM is implemented on a (near) petaflop supercomputing cluster of\nquad-core processors and its accuracy, complexity, and scalability are\ninvestigated by solving benchmark scattering problems. The timing and speedup\nresults on up to 1024 processors show that the hybrid MPI/OpenMP\nparallelization of AIM exhibits better strong scalability (fixed problem size\nspeedup) than pure MPI parallelization of it when multiple cores are used on\neach processor.", 
    "link": "http://arxiv.org/pdf/1010.1456v1", 
    "arxiv-id": "1010.1456v1"
},{
    "category": "physics.soc-ph", 
    "author": "Xue-Jiao Liu", 
    "title": "Individual and Group Dynamics in Purchasing Activity", 
    "publish": "2010-10-19T06:55:17Z", 
    "summary": "As a major part of the daily operation in an enterprise, purchasing frequency\nis of constant change. Recent approaches on the human dynamics can provide some\nnew insights into the economic behaviors of companies in the supply chain. This\npaper captures the attributes of creation times of purchase orders to an\nindividual vendor, as well as to all vendors, and further investigates whether\nthey have some kind of dynamics by applying logarithmic binning to the\nconstruction of distribution plot. It's found that the former displays a\npower-law distribution with approximate exponent 2.0, while the latter is\nfitted by a mixture distribution with both power-law and exponential\ncharacteristics. Obviously, two distinctive characteristics are presented for\nthe interval time distribution from the perspective of individual dynamics and\ngroup dynamics. Actually, this mixing feature can be attributed to the fitting\ndeviations as they are negligible for individual dynamics, but those of\ndifferent vendors are cumulated and then lead to an exponential factor for\ngroup dynamics. To better describe the mechanism generating the heterogeneity\nof purchase order assignment process from the objective company to all its\nvendors, a model driven by product life cycle is introduced, and then the\nanalytical distribution and the simulation result are obtained, which are in\ngood line with the empirical data.", 
    "link": "http://arxiv.org/pdf/1010.3815v2", 
    "arxiv-id": "1010.3815v2"
},{
    "category": "math.OC", 
    "author": "Constantine Caramanis", 
    "title": "Theory and Applications of Robust Optimization", 
    "publish": "2010-10-26T16:17:35Z", 
    "summary": "In this paper we survey the primary research, both theoretical and applied,\nin the area of Robust Optimization (RO). Our focus is on the computational\nattractiveness of RO approaches, as well as the modeling power and broad\napplicability of the methodology. In addition to surveying prominent\ntheoretical results of RO, we also present some recent results linking RO to\nadaptable models for multi-stage decision-making problems. Finally, we\nhighlight applications of RO across a wide spectrum of domains, including\nfinance, statistics, learning, and various areas of engineering.", 
    "link": "http://arxiv.org/pdf/1010.5445v1", 
    "arxiv-id": "1010.5445v1"
},{
    "category": "cs.CE", 
    "author": "L. Jason Steggles", 
    "title": "An Abstraction Theory for Qualitative Models of Biological Systems", 
    "publish": "2010-11-02T01:29:15Z", 
    "summary": "Multi-valued network models are an important qualitative modelling approach\nused widely by the biological community. In this paper we consider developing\nan abstraction theory for multi-valued network models that allows the state\nspace of a model to be reduced while preserving key properties of the model.\nThis is important as it aids the analysis and comparison of multi-valued\nnetworks and in particular, helps address the well-known problem of state space\nexplosion associated with such analysis. We also consider developing techniques\nfor efficiently identifying abstractions and so provide a basis for the\nautomation of this task. We illustrate the theory and techniques developed by\ninvestigating the identification of abstractions for two published MVN models\nof the lysis-lysogeny switch in the bacteriophage lambda.", 
    "link": "http://arxiv.org/pdf/1011.0489v1", 
    "arxiv-id": "1011.0489v1"
},{
    "category": "cs.CE", 
    "author": "Tommy White", 
    "title": "Computational Modeling for the Activation Cycle of G-proteins by   G-protein-coupled Receptors", 
    "publish": "2010-11-02T01:29:21Z", 
    "summary": "In this paper, we survey five different computational modeling methods. For\ncomparison, we use the activation cycle of G-proteins that regulate cellular\nsignaling events downstream of G-protein-coupled receptors (GPCRs) as a driving\nexample. Starting from an existing Ordinary Differential Equations (ODEs)\nmodel, we implement the G-protein cycle in the stochastic Pi-calculus using\nSPiM, as Petri-nets using Cell Illustrator, in the Kappa Language using\nCellucidate, and in Bio-PEPA using the Bio-PEPA eclipse plug in. We also\nprovide a high-level notation to abstract away from communication primitives\nthat may be unfamiliar to the average biologist, and we show how to translate\nhigh-level programs into stochastic Pi-calculus processes and chemical\nreactions.", 
    "link": "http://arxiv.org/pdf/1011.0490v1", 
    "arxiv-id": "1011.0490v1"
},{
    "category": "cs.CE", 
    "author": "Luca Tesei", 
    "title": "Multiscale Bone Remodelling with Spatial P Systems", 
    "publish": "2010-11-02T01:29:39Z", 
    "summary": "Many biological phenomena are inherently multiscale, i.e. they are\ncharacterized by interactions involving different spatial and temporal scales\nsimultaneously. Though several approaches have been proposed to provide\n\"multilayer\" models, only Complex Automata, derived from Cellular Automata,\nnaturally embed spatial information and realize multiscaling with\nwell-established inter-scale integration schemas. Spatial P systems, a variant\nof P systems in which a more geometric concept of space has been added, have\nseveral characteristics in common with Cellular Automata. We propose such a\nformalism as a basis to rephrase the Complex Automata multiscaling approach\nand, in this perspective, provide a 2-scale Spatial P system describing bone\nremodelling. The proposed model not only results to be highly faithful and\nexpressive in a multiscale scenario, but also highlights the need of a deep and\nformal expressiveness study involving Complex Automata, Spatial P systems and\nother promising multiscale approaches, such as our shape-based one already\nresulted to be highly faithful.", 
    "link": "http://arxiv.org/pdf/1011.0492v1", 
    "arxiv-id": "1011.0492v1"
},{
    "category": "cs.CE", 
    "author": "Jane Hillston", 
    "title": "Modeling biological systems with delays in Bio-PEPA", 
    "publish": "2010-11-02T01:29:42Z", 
    "summary": "Delays in biological systems may be used to model events for which the\nunderlying dynamics cannot be precisely observed, or to provide abstraction of\nsome behavior of the system resulting more compact models. In this paper we\nenrich the stochastic process algebra Bio-PEPA, with the possibility of\nassigning delays to actions, yielding a new non-Markovian process algebra:\nBio-PEPAd. This is a conservative extension meaning that the original syntax of\nBio-PEPA is retained and the delay specification which can now be associated\nwith actions may be added to existing Bio-PEPA models. The semantics of the\nfiring of the actions with delays is the delay-as-duration approach, earlier\npresented in papers on the stochastic simulation of biological systems with\ndelays. These semantics of the algebra are given in the Starting-Terminating\nstyle, meaning that the state and the completion of an action are observed as\ntwo separate events, as required by delays. Furthermore we outline how to\nperform stochastic simulation of Bio-PEPAd systems and how to automatically\ntranslate a Bio-PEPAd system into a set of Delay Differential Equations, the\ndeterministic framework for modeling of biological systems with delays. We end\nthe paper with two example models of biological systems with delays to\nillustrate the approach.", 
    "link": "http://arxiv.org/pdf/1011.0493v1", 
    "arxiv-id": "1011.0493v1"
},{
    "category": "cs.CE", 
    "author": "Albert Burger", 
    "title": "Argudas: arguing with gene expression information", 
    "publish": "2010-12-07T21:34:26Z", 
    "summary": "In situ hybridisation gene expression information helps biologists identify\nwhere a gene is expressed. However, the databases that republish the\nexperimental information are often both incomplete and inconsistent. This paper\nexamines a system, Argudas, designed to help tackle these issues. Argudas is an\nevolution of an existing system, and so that system is reviewed as a means of\nboth explaining and justifying the behaviour of Argudas. Throughout the\ndiscussion of Argudas a number of issues will be raised including the\nappropriateness of argumentation in biology and the challenges faced when\nintegrating apparently similar online biological databases.", 
    "link": "http://arxiv.org/pdf/1012.1615v1", 
    "arxiv-id": "1012.1615v1"
},{
    "category": "cs.AI", 
    "author": "Michael Krauthammer", 
    "title": "Analysis Of Cancer Omics Data In A Semantic Web Framework", 
    "publish": "2010-12-08T00:06:28Z", 
    "summary": "Our work concerns the elucidation of the cancer (epi)genome, transcriptome\nand proteome to better understand the complex interplay between a cancer cell's\nmolecular state and its response to anti-cancer therapy. To study the problem,\nwe have previously focused on data warehousing technologies and statistical\ndata integration. In this paper, we present recent work on extending our\nanalytical capabilities using Semantic Web technology. A key new component\npresented here is a SPARQL endpoint to our existing data warehouse. This\nendpoint allows the merging of observed quantitative data with existing data\nfrom semantic knowledge sources such as Gene Ontology (GO). We show how such\nvariegated quantitative and functional data can be integrated and accessed in a\nuniversal manner using Semantic Web tools. We also demonstrate how Description\nLogic (DL) reasoning can be used to infer previously unstated conclusions from\nexisting knowledge bases. As proof of concept, we illustrate the ability of our\nsetup to answer complex queries on resistance of cancer cells to Decitabine, a\ndemethylating agent.", 
    "link": "http://arxiv.org/pdf/1012.1648v1", 
    "arxiv-id": "1012.1648v1"
},{
    "category": "cs.AI", 
    "author": "Christopher Rawlings", 
    "title": "Analysis and visualisation of RDF resources in Ondex", 
    "publish": "2010-12-08T00:54:27Z", 
    "summary": "Ondex is a data integration and visualization platform developed to support\nSystems Biology Research. At its core is a data model based on two main\nprinciples: first, all information can be represented as a graph and, second,\nall elements of the graph can be annotated with ontologies. This data model is\nconformant to the Semantic Web framework, in particular to RDF, and therefore\nOndex is ideally positioned as a platform that can exploit the semantic web.", 
    "link": "http://arxiv.org/pdf/1012.1661v1", 
    "arxiv-id": "1012.1661v1"
},{
    "category": "cs.CE", 
    "author": "Garth N. Wells", 
    "title": "Energy stable and momentum conserving hybrid finite element method for   the incompressible Navier-Stokes equations", 
    "publish": "2010-12-16T19:16:58Z", 
    "summary": "A hybrid method for the incompressible Navier--Stokes equations is presented.\nThe method inherits the attractive stabilizing mechanism of upwinded\ndiscontinuous Galerkin methods when momentum advection becomes significant,\nequal-order interpolations can be used for the velocity and pressure fields,\nand mass can be conserved locally. Using continuous Lagrange multiplier spaces\nto enforce flux continuity across cell facets, the number of global degrees of\nfreedom is the same as for a continuous Galerkin method on the same mesh.\nDifferent from our earlier investigations on the approach for the\nNavier--Stokes equations, the pressure field in this work is discontinuous\nacross cell boundaries. It is shown that this leads to very good local mass\nconservation and, for an appropriate choice of finite element spaces, momentum\nconservation. Also, a new form of the momentum transport terms for the method\nis constructed such that global energy stability is guaranteed, even in the\nabsence of a point-wise solenoidal velocity field. Mass conservation, momentum\nconservation and global energy stability are proved for the time-continuous\ncase, and for a fully discrete scheme. The presented analysis results are\nsupported by a range of numerical simulations.", 
    "link": "http://arxiv.org/pdf/1012.3722v3", 
    "arxiv-id": "1012.3722v3"
},{
    "category": "cs.CE", 
    "author": "Rafael Mayo", 
    "title": "PhyloGrid: a development for a workflow in Phylogeny", 
    "publish": "2010-12-17T18:32:10Z", 
    "summary": "In this work we present the development of a workflow based on Taverna which\nis going to be implemented for calculations in Phylogeny by means of the\nMrBayes tool. It has a friendly interface developed with the Gridsphere\nframework. The user is able to define the parameters for doing the Bayesian\ncalculation, determine the model of evolution, check the accuracy of the\nresults in the intermediate stages as well as do a multiple alignment of the\nsequences previously to the final result. To do this, no knowledge from his/her\nside about the computational procedure is required.", 
    "link": "http://arxiv.org/pdf/1012.3953v1", 
    "arxiv-id": "1012.3953v1"
},{
    "category": "cs.CE", 
    "author": "Rafael Mayo", 
    "title": "Advances in the Biomedical Applications of the EELA Project", 
    "publish": "2010-12-17T18:35:43Z", 
    "summary": "In the last years an increasing demand for Grid Infrastructures has resulted\nin several international collaborations. This is the case of the EELA Project,\nwhich has brought together collaborating groups of Latin America and Europe.\nOne year ago we presented this e-infrastructure used, among others, by the\nBiomedical groups for the studies of oncological analysis, neglected diseases,\nsequence alignments and computation phylogenetics. After this period, the\nachieved advances are summarised in this paper.", 
    "link": "http://arxiv.org/pdf/1012.3956v1", 
    "arxiv-id": "1012.3956v1"
},{
    "category": "cs.CE", 
    "author": "Bruno Pinaud", 
    "title": "PORGY: Strategy-Driven Interactive Transformation of Graphs", 
    "publish": "2011-02-14T01:09:40Z", 
    "summary": "This paper investigates the use of graph rewriting systems as a modelling\ntool, and advocates the embedding of such systems in an interactive\nenvironment. One important application domain is the modelling of biochemical\nsystems, where states are represented by port graphs and the dynamics is driven\nby rules and strategies. A graph rewriting tool's capability to interactively\nexplore the features of the rewriting system provides useful insights into\npossible behaviours of the model and its properties. We describe PORGY, a\nvisual and interactive tool we have developed to model complex systems using\nport graphs and port graph rewrite rules guided by strategies, and to navigate\nin the derivation history. We demonstrate via examples some functionalities\nprovided by PORGY.", 
    "link": "http://arxiv.org/pdf/1102.2654v1", 
    "arxiv-id": "1102.2654v1"
},{
    "category": "q-bio.QM", 
    "author": "Verena Wolf", 
    "title": "Parameter Identification for Markov Models of Biochemical Reactions", 
    "publish": "2011-02-14T16:27:08Z", 
    "summary": "We propose a numerical technique for parameter inference in Markov models of\nbiological processes. Based on time-series data of a process we estimate the\nkinetic rate constants by maximizing the likelihood of the data. The\ncomputation of the likelihood relies on a dynamic abstraction of the discrete\nstate space of the Markov model which successfully mitigates the problem of\nstate space largeness. We compare two variants of our method to\nstate-of-the-art, recently published methods and demonstrate their usefulness\nand efficiency on several case studies from systems biology.", 
    "link": "http://arxiv.org/pdf/1102.2819v1", 
    "arxiv-id": "1102.2819v1"
},{
    "category": "cs.CE", 
    "author": "Jose Roberto Steiner", 
    "title": "Modelling the Dynamics of the Work-Employment System by Predator-Prey   Interactions", 
    "publish": "2011-02-22T15:01:06Z", 
    "summary": "The broad application range of the predator-prey modelling enabled us to\napply it to represent the dynamics of the work-employment system. For the\nadopted period, we conclude that this dynamics is chaotic in the beginning of\nthe time series and tends to less perturbed states, as time goes by, due to\npublic policies and hidden intrinsic system features. Basic Lotka-Volterra\napproach was revised and adapted to the reality of the study. The final aim is\nto provide managers with generalized theoretical elements that allow to a more\naccurate understanding of the behavior of the work-employment system.", 
    "link": "http://arxiv.org/pdf/1102.4528v3", 
    "arxiv-id": "1102.4528v3"
},{
    "category": "physics.flu-dyn", 
    "author": "R. P. Young", 
    "title": "Fluid flow analysis in a rough fracture (type II) using complex networks   and lattice Boltzmann method", 
    "publish": "2011-03-06T13:28:03Z", 
    "summary": "Complexity of fluid flow in a rough fracture is induced by the complex\nconfigurations of opening areas between the fracture planes. In this study, we\nmodel fluid flow in an evolvable real rock joint structure, which under certain\nnormal load is sheared. In an experimental study, information regarding about\napertures of the rock joint during consecutive 20 mm displacements and fluid\nflow (permeability) in different pressure heads have been recorded by a scanner\nlaser. Our aim in this study is to simulate the fluid flow in the mentioned\ncomplex geometries using the lattice Boltzmann method (LBM), while the\ncharacteristics of the aperture field will be compared with the modeled fluid\nflow permeability To characterize the aperture, we use a new concept in the\ngraph theory, namely: complex networks and motif analysis of the corresponding\nnetworks. In this approach, the similar aperture profile along the fluid flow\ndirection is mapped in to a network space. The modeled permeability using the\nLBM shows good correlation with the experimental measured values. Furthermore,\nthe two main characters of the obtained networks, i.e., characteristic length\nand number of edges show the same evolutionary trend with the modeled\npermeability values. Analysis of motifs through the obtained networks showed\nthe most transient sub-graphs are much more frequent in residual stages. This\ncoincides with nearly stable fluid flow and high permeability values.", 
    "link": "http://arxiv.org/pdf/1103.1124v1", 
    "arxiv-id": "1103.1124v1"
},{
    "category": "cond-mat.mtrl-sci", 
    "author": "C. J. Pearce", 
    "title": "A micromechanics-enhanced finite element formulation for modelling   heterogeneous materials", 
    "publish": "2011-03-29T13:30:29Z", 
    "summary": "In the analysis of composite materials with heterogeneous microstructures,\nfull resolution of the heterogeneities using classical numerical approaches can\nbe computationally prohibitive. This paper presents a micromechanics-enhanced\nfinite element formulation that accurately captures the mechanical behaviour of\nheterogeneous materials in a computationally efficient manner. The strategy\nexploits analytical solutions derived by Eshelby for ellipsoidal inclusions in\norder to determine the mechanical perturbation fields as a result of the\nunderlying heterogeneities. Approximation functions for these perturbation\nfields are then incorporated into a finite element formulation to augment those\nof the macroscopic fields. A significant feature of this approach is that the\nfinite element mesh does not explicitly resolve the heterogeneities and that no\nadditional degrees of freedom are introduced. In this paper, hybrid-Trefftz\nstress finite elements are utilised and performance of the proposed formulation\nis demonstrated with numerical examples. The method is restricted here to\nelastic particulate composites with ellipsoidal inclusions but it has been\ndesigned to be extensible to a wider class of materials comprising arbitrary\nshaped inclusions.", 
    "link": "http://arxiv.org/pdf/1103.5633v2", 
    "arxiv-id": "1103.5633v2"
},{
    "category": "cs.CV", 
    "author": "P. S. Hiremath", 
    "title": "An Effect of Spatial Filtering in Visualization of Coronary Arteries   Imaging", 
    "publish": "2011-03-28T12:35:47Z", 
    "summary": "At present, coronary angiography is the well known standard for the diagnosis\nof coronary artery disease. Conventional coronary angiography is an invasive\nprocedure with a small, yet inherent risk of myocardial infarction, stroke,\npotential arrhythmias, and death. Other noninvasive diagnostic tools, such as\nelectrocardiography, echocardiography, and nuclear imaging are now widely\navailable but are limited by their inability to directly visualize and quantify\ncoronary artery stenoses and predict the stability of plaques. Coronary\nmagnetic resonance angiography (MRA) is a technique that allows visualization\nof the coronary arteries by noninvasive means; however, it has not yet reached\na stage where it can be used in routine clinical practice. Although coronary\nMRA is a potentially useful diagnostic tool, it has limitations. Further\nresearch should focus on improving the diagnostic resolution and accuracy of\ncoronary MRA. This paper will helps to cardiologists to take the clear look of\nspatial filtered imaging of coronary arteries.", 
    "link": "http://arxiv.org/pdf/1104.3513v1", 
    "arxiv-id": "1104.3513v1"
},{
    "category": "cs.CE", 
    "author": "P. S. Hiremath", 
    "title": "Visualization techniques for data mining of Latur district satellite   imagery", 
    "publish": "2011-03-28T12:35:11Z", 
    "summary": "This study presents a new visualization tool for classification of satellite\nimagery. Visualization of feature space allows exploration of patterns in the\nimage data and insight into the classification process and related uncertainty.\nVisual Data Mining provides added value to image classifications as the user\ncan be involved in the classification process providing increased confidence in\nand understanding of the results. In this study, we present a prototype\nvisualization tool for visual data mining (VDM) of satellite imagery. The\nvisualization tool is showcased in a classification study of highresolution\nimageries of Latur district in Maharashtra state of India.", 
    "link": "http://arxiv.org/pdf/1104.3571v2", 
    "arxiv-id": "1104.3571v2"
},{
    "category": "physics.ins-det", 
    "author": "H. Steigmann", 
    "title": "The regularized blind tip reconstruction algorithm as a scanning probe   microscopy tip metrology method", 
    "publish": "2011-05-07T19:44:12Z", 
    "summary": "The problem of an accurate tip radius and shape characterization is very\nimportant for determination of surface mechanical and chemical properties on\nthe basis of the scanning probe microscopy measurements. We think that the most\nfavorable methods for this purpose are blind tip reconstruction methods, since\nthey do not need any calibrated characterizers and might be performed on an\nordinary SPM setup. As in many other inverse problems also in case of these\nmethods the stability of the solution in presence of vibrational and electronic\nnoise needs application of so called regularization techniques. In this paper\nthe novel regularization technique (Regularized Blind Tip Reconstruction -\nRBTR) for blind tip reconstruction algorithm is presented. It improves the\nquality of the solution in presence of isotropic and anisotropic noise. The\nsuperiority of our approach is proved on the basis of computer simulations and\nanalysis of images of the Budget Sensors TipCheck calibration standard. In case\nof characterization of real AFM probes as a reference method the high\nresolution scanning electron microscopy was chosen and we obtain good\nqualitative correspondence of both methods.", 
    "link": "http://arxiv.org/pdf/1105.1472v1", 
    "arxiv-id": "1105.1472v1"
},{
    "category": "cs.CE", 
    "author": "David A. Bader", 
    "title": "On the random access performance of Cell Broadband Engine with graph   analysis application", 
    "publish": "2011-05-30T07:07:59Z", 
    "summary": "The Cell Broad Engine (BE) Processor has unique memory access architecture\nbesides its powerful computing engines. Many computing-intensive applications\nhave been ported to Cell/BE successfully. But memory-intensive applications are\nrarely investigated except for several micro benchmarks. Since Cell/BE has\npowerful software visible DMA engine, this paper studies on whether Cell/BE is\nsuit for applica- tions with large amount of random memory accesses. Two\nbenchmarks, GUPS and SSCA#2, are used. The latter is a rather complex one that\nin representative of real world graph analysis applications. We find both\nbenchmarks have good performance on Cell/BE based IBM QS20/22. Com- pared with\n2 conventional multi-processor systems with the same core/thread number, GUPS\nis about 40-80% fast and SSCA#2 about 17-30% fast. The dynamic load balanc- ing\nand software pipeline for optimizing SSCA#2 are intro- duced. Based on the\nexperiment, the potential of Cell/BE for random access is analyzed in detail as\nwell as its limita- tions of memory controller, atomic engine and TLB manage-\nment.Our research shows although more programming effort are needed, Cell/BE\nhas the potencial for irregular memory access applications.", 
    "link": "http://arxiv.org/pdf/1105.5881v2", 
    "arxiv-id": "1105.5881v2"
},{
    "category": "cs.SI", 
    "author": "J. -Emeterio Navarro-Barrientos", 
    "title": "Formation of Common Investment Networks by Project Establishment between   Agents", 
    "publish": "2011-07-08T11:15:59Z", 
    "summary": "We present an investment model integrated with trust-reputation mechanisms\nwhere agents interact with each other to establish investment projects. We\ninvestigate the establishment of investment projects, the influence of the\ninteraction between agents in the evolution of the distribution of wealth, as\nwell as the formation of common investment networks and some of their\nproperties. Simulation results show that the wealth distribution presents a\npower law in its tail. Also, it is shown that the trust and reputation\nmechanism presented leads to the establishment of networks among agents, which\npresent some of the typical characteristics of real-life networks like a high\nclustering coefficient and short average path length.", 
    "link": "http://arxiv.org/pdf/1107.1608v1", 
    "arxiv-id": "1107.1608v1"
},{
    "category": "astro-ph.IM", 
    "author": "L. J. Greenhill", 
    "title": "Accelerating Radio Astronomy Cross-Correlation with Graphics Processing   Units", 
    "publish": "2011-07-21T13:16:30Z", 
    "summary": "We present a highly parallel implementation of the cross-correlation of\ntime-series data using graphics processing units (GPUs), which is scalable to\nhundreds of independent inputs and suitable for the processing of signals from\n\"Large-N\" arrays of many radio antennas. The computational part of the\nalgorithm, the X-engine, is implementated efficiently on Nvidia's Fermi\narchitecture, sustaining up to 79% of the peak single precision floating-point\nthroughput. We compare performance obtained for hardware- and software-managed\ncaches, observing significantly better performance for the latter. The high\nperformance reported involves use of a multi-level data tiling strategy in\nmemory and use of a pipelined algorithm with simultaneous computation and\ntransfer of data from host to device memory. The speed of code development,\nflexibility, and low cost of the GPU implementations compared to ASIC and FPGA\nimplementations have the potential to greatly shorten the cycle of correlator\ndevelopment and deployment, for cases where some power consumption penalty can\nbe tolerated.", 
    "link": "http://arxiv.org/pdf/1107.4264v2", 
    "arxiv-id": "1107.4264v2"
},{
    "category": "cs.CE", 
    "author": "Masaaki Miki", 
    "title": "Three-term Method and Dual Estimate on Static Problems of Continuum   Bodies", 
    "publish": "2011-08-05T18:35:12Z", 
    "summary": "This work aims to provide standard formulations for direct minimization\napproaches on various types of static problems of continuum mechanics.\nParticularly, form-finding problems of tension structures are discussed in the\nfirst half and the large deformation problems of continuum bodies are discussed\nin the last half. In the first half, as the standards of iterative direct\nminimization strategies, two types of simple recursive methods are presented,\nnamely the two-term method and the three-term method. The dual estimate is also\nintroduced as a powerful means of involving equally constraint conditions into\nminimization problems. As examples of direct minimization approaches on usual\nengineering issues, some form finding problems of tension structures which can\nbe solved by the presented strategies are illustrated. Additionally, it is\npointed out that while the two-term method sometimes becomes useless, the\nthree-term method always provides remarkable rate of global convergence\nefficiency. Then, to show the potential ability of the three-term method, in\nthe last part of this work, some principle of virtual works which usually\nappear in the continuum mechanics are approximated and discretized in a common\nmanner, which are suitable to be solved by the three-term method. Finally, some\nlarge deformation analyses of continuum bodies which can be solved by the\nthree-term method are presented.", 
    "link": "http://arxiv.org/pdf/1108.1331v2", 
    "arxiv-id": "1108.1331v2"
},{
    "category": "cs.LO", 
    "author": "Alexis Marechal", 
    "title": "Modelling of Genetic Regulatory Mechanisms with GReg", 
    "publish": "2011-08-17T09:31:59Z", 
    "summary": "Most available tools propose simulation frameworks to study models of\nbiological systems, but simulation only explores a few of the most probable\nbehaviours of the system. On the contrary, techniques such as model checking,\ncoming from IT-systems analysis, explore all the possible behaviours of the\nmodelled systems, thus helping to identify emergent properties. A main drawback\nfrom most model checking tools in the life sciences domain is that they take as\ninput a language designed for computer scientists, that is not easily\nunderstood by non-expert users. We propose in this article an approach based on\nDSL. It provides a comprehensible language to describe the system while\nallowing the use of complex and powerful underlying model checking techniques.", 
    "link": "http://arxiv.org/pdf/1108.3436v1", 
    "arxiv-id": "1108.3436v1"
},{
    "category": "cs.LG", 
    "author": "Tshilidzi Marwala", 
    "title": "The fuzzy gene filter: A classifier performance assesment", 
    "publish": "2011-08-23T10:34:07Z", 
    "summary": "The Fuzzy Gene Filter (FGF) is an optimised Fuzzy Inference System designed\nto rank genes in order of differential expression, based on expression data\ngenerated in a microarray experiment. This paper examines the effectiveness of\nthe FGF for feature selection using various classification architectures. The\nFGF is compared to three of the most common gene ranking algorithms: t-test,\nWilcoxon test and ROC curve analysis. Four classification schemes are used to\ncompare the performance of the FGF vis-a-vis the standard approaches: K Nearest\nNeighbour (KNN), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC)\nand Artificial Neural Network (ANN). A nested stratified Leave-One-Out Cross\nValidation scheme is used to identify the optimal number top ranking genes, as\nwell as the optimal classifier parameters. Two microarray data sets are used\nfor the comparison: a prostate cancer data set and a lymphoma data set.", 
    "link": "http://arxiv.org/pdf/1108.4545v1", 
    "arxiv-id": "1108.4545v1"
},{
    "category": "cs.LG", 
    "author": "Tshilidzi Marwala", 
    "title": "Improving the performance of the ripper in insurance risk classification   : A comparitive study using feature selection", 
    "publish": "2011-08-23T10:52:18Z", 
    "summary": "The Ripper algorithm is designed to generate rule sets for large datasets\nwith many features. However, it was shown that the algorithm struggles with\nclassification performance in the presence of missing data. The algorithm\nstruggles to classify instances when the quality of the data deteriorates as a\nresult of increasing missing data. In this paper, a feature selection technique\nis used to help improve the classification performance of the Ripper model.\nPrincipal component analysis and evidence automatic relevance determination\ntechniques are used to improve the performance. A comparison is done to see\nwhich technique helps the algorithm improve the most. Training datasets with\ncompletely observable data were used to construct the model and testing\ndatasets with missing values were used for measuring accuracy. The results\nshowed that principal component analysis is a better feature selection for the\nRipper in improving the classification performance.", 
    "link": "http://arxiv.org/pdf/1108.4551v1", 
    "arxiv-id": "1108.4551v1"
},{
    "category": "physics.flu-dyn", 
    "author": "R. Hemadri Reddy", 
    "title": "Unsteady Hydromagnetic Flow of Viscoelastic Fluid down an Open Inclined   Channel", 
    "publish": "2011-08-26T07:20:41Z", 
    "summary": "In this paper, we study the unsteady hydromagnetic flow of a Walter's fluid\n(Model B') down an open inclined channel of width 2a and depth d under gravity,\nthe walls of the channel being normal to the surface of the bottom under the\ninfluence of a uniform transverse magnetic field. A uniform tangential stress\nis applied at the free surface in the direction of flow. We have evaluated the\nvelocity distribution by using Laplace transform and finite Fourier Sine\ntransform technique. The velocity distribution has been obtained taking\ndifferent form of time dependent pressure gradient g(t), viz., i) constant ii)\nexponential decreasing function of time and iii) Cosine function of time. The\neffects of magnetic parameter M, Reynolds number R and the viscoelastic\nparameter K are discussed on the velocity distribution in three different\ncases.", 
    "link": "http://arxiv.org/pdf/1108.5593v1", 
    "arxiv-id": "1108.5593v1"
},{
    "category": "physics.flu-dyn", 
    "author": "Alexander M. Linkov", 
    "title": "Use of a speed equation for numerical simulation of hydraulic fractures", 
    "publish": "2011-08-31T07:47:52Z", 
    "summary": "The paper treats the propagation of a hydraulically driven crack. We\nexplicitly write the local speed equation, which facilitates using the theory\nof propagating interfaces. It is shown that when neglecting the lag between the\nliquid front and the crack tip, the lubrication PDE yields that a solution\nsatisfies the speed equation identically. This implies that for zero or small\nlag, the boundary value problem appears ill-posed when solved numerically. We\nsuggest e - regularization, which consists in employing the speed equation\ntogether with a prescribed BC on the front to obtain a new BC formulated at a\nsmall distance behind the front rather than on the front itself. It is shown\nthat - regularization provides accurate and stable results with reasonable time\nexpense. It is also shown that the speed equation gives a key to proper choice\nof unknown functions when solving a hydraulic fracture problem numerically.", 
    "link": "http://arxiv.org/pdf/1108.6146v1", 
    "arxiv-id": "1108.6146v1"
},{
    "category": "cs.CE", 
    "author": "Erik de Vink", 
    "title": "Proceedings Third International Workshop on Computational Models for   Cell Processes", 
    "publish": "2011-09-06T02:18:56Z", 
    "summary": "This volume contains the final versions of the papers presented at the 3rd\nInternational Workshop on Computational Models for Cell Processes (CompMod\n2011). The workshop took place on September 10, 2011 at the University of\nAachen, Germany, in conjunction with CONCUR 2011. The first edition of the\nworkshop (2008) took place in Turku, Finland, in conjunction with Formal\nMethods 2008 and the second edition (2009) took place in Eindhoven, the\nNetherlands, as well in conjunction with Formal Methods 2009. The goal of the\nCompMod workshop series is to bring together researchers in Computer Science\n(especially in Formal Methods) and Mathematics (both discrete and continuous),\ninterested in the opportunities and the challenges of Systems Biology.", 
    "link": "http://arxiv.org/pdf/1109.1044v2", 
    "arxiv-id": "1109.1044v2"
},{
    "category": "cs.LO", 
    "author": "Angelo Troina", 
    "title": "Modelling Spatial Interactions in the Arbuscular Mycorrhizal Symbiosis   using the Calculus of Wrapped Compartments", 
    "publish": "2011-09-07T06:27:06Z", 
    "summary": "Arbuscular mycorrhiza (AM) is the most wide-spread plant-fungus symbiosis on\nearth. Investigating this kind of symbiosis is considered one of the most\npromising ways to develop methods to nurture plants in more natural manners,\navoiding the complex chemical productions used nowadays to produce artificial\nfertilizers. In previous work we used the Calculus of Wrapped Compartments\n(CWC) to investigate different phases of the AM symbiosis. In this paper, we\ncontinue this line of research by modelling the colonisation of the plant root\ncells by the fungal hyphae spreading in the soil. This study requires the\ndescription of some spatial interaction. Although CWC has no explicit feature\nmodelling a spatial geometry, the compartment labelling feature can be\neffectively exploited to define a discrete surface topology outlining the\nrelevant sectors which determine the spatial properties of the system under\nconsideration. Different situations and interesting spatial properties can be\nmodelled and analysed in such a lightweight framework (which has not an\nexplicit notion of geometry with coordinates and spatial metrics), thus\nexploiting the existing CWC simulation tool.", 
    "link": "http://arxiv.org/pdf/1109.1363v1", 
    "arxiv-id": "1109.1363v1"
},{
    "category": "cs.CE", 
    "author": "Alberto Policriti", 
    "title": "Programmable models of growth and mutation of cancer-cell populations", 
    "publish": "2011-09-07T06:27:16Z", 
    "summary": "In this paper we propose a systematic approach to construct mathematical\nmodels describing populations of cancer-cells at different stages of disease\ndevelopment. The methodology we propose is based on stochastic Concurrent\nConstraint Programming, a flexible stochastic modelling language. The\nmethodology is tested on (and partially motivated by) the study of prostate\ncancer. In particular, we prove how our method is suitable to systematically\nreconstruct different mathematical models of prostate cancer growth - together\nwith interactions with different kinds of hormone therapy - at different levels\nof refinement.", 
    "link": "http://arxiv.org/pdf/1109.1364v1", 
    "arxiv-id": "1109.1364v1"
},{
    "category": "cs.CE", 
    "author": "Federica Ciocchetta", 
    "title": "A semi-quantitative equivalence for abstracting from fast reactions", 
    "publish": "2011-09-07T06:27:23Z", 
    "summary": "Semantic equivalences are used in process algebra to capture the notion of\nsimilar behaviour, and this paper proposes a semi-quantitative equivalence for\na stochastic process algebra developed for biological modelling. We consider\nabstracting away from fast reactions as suggested by the Quasi-Steady-State\nAssumption. We define a fast-slow bisimilarity based on this idea. We also show\ncongruence under an appropriate condition for the cooperation operator of\nBio-PEPA. The condition requires that there is no synchronisation over fast\nactions, and this distinguishes fast-slow bisimilarity from weak bisimilarity.\nWe also show congruence for an operator which extends the reactions available\nfor a species. We characterise models for which it is only necessary to\nconsider the matching of slow transitions and we illustrate the equivalence on\ntwo models of competitive inhibition.", 
    "link": "http://arxiv.org/pdf/1109.1365v1", 
    "arxiv-id": "1109.1365v1"
},{
    "category": "cs.CE", 
    "author": "Livio Bioglio", 
    "title": "A Minimal OO Calculus for Modelling Biological Systems", 
    "publish": "2011-09-07T06:27:29Z", 
    "summary": "In this paper we present a minimal object oriented core calculus for\nmodelling the biological notion of type that arises from biological ontologies\nin formalisms based on term rewriting. This calculus implements encapsulation,\nmethod invocation, subtyping and a simple formof overriding inheritance, and it\nis applicable to models designed in the most popular term-rewriting formalisms.\nThe classes implemented in a formalism can be used in several models, like\nprogramming libraries.", 
    "link": "http://arxiv.org/pdf/1109.1366v1", 
    "arxiv-id": "1109.1366v1"
},{
    "category": "cs.CE", 
    "author": "Victor P. Zhukov", 
    "title": "Verification, Validation and Testing of Kinetic Mechanisms of Hydrogen   Combustion in Fluid Dynamic Computations", 
    "publish": "2011-09-16T09:42:03Z", 
    "summary": "A one-step, a two-step, an abridged, a skeletal and four detailed kinetic\nschemes of hydrogen oxidation have been tested. A new skeletal kinetic scheme\nof hydrogen oxidation has been developed. The CFD calculations were carried out\nusing ANSYS CFX software. Ignition delay times and speeds of flames were\nderived from the computational results. The computational data obtained using\nANSYS CFX and CHEMKIN, and experimental data were compared. The precision,\nreliability, and range of validity of the kinetic schemes in CFD simulations\nwere estimated. The impact of kinetic scheme on the results of computations was\ndiscussed. The relationship between grid spacing, timestep, accuracy, and\ncomputational cost were analyzed.", 
    "link": "http://arxiv.org/pdf/1109.3563v2", 
    "arxiv-id": "1109.3563v2"
},{
    "category": "q-bio.QM", 
    "author": "Michel Zivy", 
    "title": "MassChroQ: A versatile tool for mass spectrometry quantification", 
    "publish": "2011-09-26T08:52:55Z", 
    "summary": "Recently, many software tools have been developed to perform quantification\nin LC-MS analyses. However, most of them are specific to either a\nquantification strategy (e.g. label-free or isotopic labelling) or a\nmass-spectrometry system (e.g. high or low resolution).\n  In this context, we have developed MassChroQ, a versatile software that\nperforms LC-MS data alignment and peptide quantification by peak area\nintegration on extracted ion chromatograms. MassChroQ is suitable for\nquantification with or without labelling and is not limited to high resolution\nsystems. Peptides of interest (for example all the identified peptides) can be\ndetermined automatically or manually by providing targeted m/z and retention\ntime values. It can handle large experiments that include protein or peptide\nfractionation (as SDS-PAGE, 2D-LC). It is fully configurable. Every processing\nstep is traceable, the produced data are in open standard format and its\nmodularity allows easy integration into proteomic pipelines. The output results\nare ready for use in statistical analyses.\n  Evaluation of MassChroQ on complex label-free data obtained from low and high\nresolution mass spectrometers showed low CVs for technical reproducibility\n(1.4%) and high coefficients of correlation to protein quantity (0.98).\n  MassChroQ is freely available under the GNU General Public Licence v3.0 at\nhttp://pappso.inra.fr/bioinfo/masschroq/.", 
    "link": "http://arxiv.org/pdf/1109.5488v1", 
    "arxiv-id": "1109.5488v1"
},{
    "category": "cs.CC", 
    "author": "Jens K Mueller", 
    "title": "The Complexity of Rooted Phylogeny Problems", 
    "publish": "2011-10-04T14:09:06Z", 
    "summary": "Several computational problems in phylogenetic reconstruction can be\nformulated as restrictions of the following general problem: given a formula in\nconjunctive normal form where the literals are rooted triples, is there a\nrooted binary tree that satisfies the formula? If the formulas do not contain\ndisjunctions, the problem becomes the famous rooted triple consistency problem,\nwhich can be solved in polynomial time by an algorithm of Aho, Sagiv,\nSzymanski, and Ullman. If the clauses in the formulas are restricted to\ndisjunctions of negated triples, Ng, Steel, and Wormald showed that the problem\nremains NP-complete. We systematically study the computational complexity of\nthe problem for all such restrictions of the clauses in the input formula. For\ncertain restricted disjunctions of triples we present an algorithm that has\nsub-quadratic running time and is asymptotically as fast as the fastest known\nalgorithm for the rooted triple consistency problem. We also show that any\nrestriction of the general rooted phylogeny problem that does not fall into our\ntractable class is NP-complete, using known results about the complexity of\nBoolean constraint satisfaction problems. Finally, we present a pebble game\nargument that shows that the rooted triple consistency problem (and also all\ngeneralizations studied in this paper) cannot be solved by Datalog.", 
    "link": "http://arxiv.org/pdf/1110.0693v2", 
    "arxiv-id": "1110.0693v2"
},{
    "category": "cs.CE", 
    "author": "Tristan van Leeuwen", 
    "title": "Robust inversion via semistochastic dimensionality reduction", 
    "publish": "2011-10-05T04:55:59Z", 
    "summary": "We consider a class of inverse problems where it is possible to aggregate the\nresults of multiple experiments. This class includes problems where the forward\nmodel is the solution operator to linear ODEs or PDEs. The tremendous size of\nsuch problems motivates dimensionality reduction techniques based on randomly\nmixing experiments. These techniques break down, however, when robust\ndata-fitting formulations are used, which are essential in cases of missing\ndata, unusually large errors, and systematic features in the data unexplained\nby the forward model. We survey robust methods within a statistical framework,\nand propose a semistochastic optimization approach that allows dimensionality\nreduction. The efficacy of the methods are demonstrated for a large-scale\nseismic inverse problem using the robust Student's t-distribution, where a\nuseful synthetic velocity model is recovered in the extreme scenario of 60%\ndata missing at random. The semistochastic approach achieves this recovery\nusing 20% of the effort required by a direct robust approach.", 
    "link": "http://arxiv.org/pdf/1110.0895v4", 
    "arxiv-id": "1110.0895v4"
},{
    "category": "cs.CE", 
    "author": "M V Stoitsov", 
    "title": "Advancing Nuclear Physics Through TOPS Solvers and Tools", 
    "publish": "2011-10-08T08:35:04Z", 
    "summary": "At the heart of many scientific applications is the solution of algebraic\nsystems, such as linear systems of equations, eigenvalue problems, and\noptimization problems, to name a few. TOPS, which stands for Towards Optimal\nPetascale Simulations, is a SciDAC applied math center focused on the\ndevelopment of solvers for tackling these algebraic systems, as well as the\ndeployment of such technologies in large-scale scientific applications of\ninterest to the U.S. Department of Energy. In this paper, we highlight some of\nthe solver technologies we have developed in optimization and matrix\ncomputations. We also describe some accomplishments achieved using these\ntechnologies in UNEDF, a SciDAC application project on nuclear physics.", 
    "link": "http://arxiv.org/pdf/1110.1708v1", 
    "arxiv-id": "1110.1708v1"
},{
    "category": "cs.CE", 
    "author": "M. Sejnoha", 
    "title": "Computational homogenization of non-stationary transport processes in   masonry structures", 
    "publish": "2011-10-10T14:34:59Z", 
    "summary": "A fully coupled transient heat and moisture transport in a masonry structure\nis examined in this paper. Supported by several successful applications in\ncivil engineering the nonlinear diffusion model proposed by K\\\"{u}nzel is\nadopted in the present study. A strong material heterogeneity together with a\nsignificant dependence of the model parameters on initial conditions as well as\nthe gradients of heat and moisture fields vindicates the use of a hierarchical\nmodeling strategy to solve the problem of this kind. Attention is limited to\nthe classical first order homogenization in a spatial domain developed here in\nthe framework of a two step (meso-macro) multi-scale computational scheme (FE^2\nproblem). Several illustrative examples are presented to investigate the\ninfluence of transient flow at the level of constituents (meso-scale) on the\nmacroscopic response including the effect of macro-scale boundary conditions. A\ntwo-dimensional section of Charles Bridge subjected to actual climatic\nconditions is analyzed next to confirm the suitability of algorithmic format of\nFE^2 scheme for the parallel computing.", 
    "link": "http://arxiv.org/pdf/1110.2055v1", 
    "arxiv-id": "1110.2055v1"
},{
    "category": "cs.CR", 
    "author": "Gene Tsudik", 
    "title": "Countering Gattaca: Efficient and Secure Testing of Fully-Sequenced   Human Genomes (Full Version)", 
    "publish": "2011-10-11T19:47:11Z", 
    "summary": "Recent advances in DNA sequencing technologies have put ubiquitous\navailability of fully sequenced human genomes within reach. It is no longer\nhard to imagine the day when everyone will have the means to obtain and store\none's own DNA sequence. Widespread and affordable availability of fully\nsequenced genomes immediately opens up important opportunities in a number of\nhealth-related fields. In particular, common genomic applications and tests\nperformed in vitro today will soon be conducted computationally, using\ndigitized genomes. New applications will be developed as genome-enabled\nmedicine becomes increasingly preventive and personalized. However, this\nprogress also prompts significant privacy challenges associated with potential\nloss, theft, or misuse of genomic data. In this paper, we begin to address\ngenomic privacy by focusing on three important applications: Paternity Tests,\nPersonalized Medicine, and Genetic Compatibility Tests. After carefully\nanalyzing these applications and their privacy requirements, we propose a set\nof efficient techniques based on private set operations. This allows us to\nimplement in in silico some operations that are currently performed via in\nvitro methods, in a secure fashion. Experimental results demonstrate that\nproposed techniques are both feasible and practical today.", 
    "link": "http://arxiv.org/pdf/1110.2478v3", 
    "arxiv-id": "1110.2478v3"
},{
    "category": "cs.PF", 
    "author": "Moncho G\u00f3mez-Gesteira", 
    "title": "Optimization strategies for parallel CPU and GPU implementations of a   meshfree particle method", 
    "publish": "2011-10-17T15:54:48Z", 
    "summary": "Much of the current focus in high performance computing (HPC) for\ncomputational fluid dynamics (CFD) deals with grid based methods. However,\nparallel implementations for new meshfree particle methods such as Smoothed\nParticle Hydrodynamics (SPH) are less studied. In this work, we present\noptimizations for both central processing unit (CPU) and graphics processing\nunit (GPU) of a SPH method. These optimization strategies can be further\napplied to many other meshfree methods. The obtained performance for each\narchitecture and a comparison between the most efficient implementations for\nCPU and GPU are shown.", 
    "link": "http://arxiv.org/pdf/1110.3711v3", 
    "arxiv-id": "1110.3711v3"
},{
    "category": "cs.MS", 
    "author": "Yiannis Andreopoulos", 
    "title": "Throughput-Distortion Computation Of Generic Matrix Multiplication:   Toward A Computation Channel For Digital Signal Processing Systems", 
    "publish": "2011-10-26T11:17:21Z", 
    "summary": "The generic matrix multiply (GEMM) function is the core element of\nhigh-performance linear algebra libraries used in many\ncomputationally-demanding digital signal processing (DSP) systems. We propose\nan acceleration technique for GEMM based on dynamically adjusting the\nimprecision (distortion) of computation. Our technique employs adaptive scalar\ncompanding and rounding to input matrix blocks followed by two forms of packing\nin floating-point that allow for concurrent calculation of multiple results.\nSince the adaptive companding process controls the increase of concurrency (via\npacking), the increase in processing throughput (and the corresponding increase\nin distortion) depends on the input data statistics. To demonstrate this, we\nderive the optimal throughput-distortion control framework for GEMM for the\nbroad class of zero-mean, independent identically distributed, input sources.\nOur approach converts matrix multiplication in programmable processors into a\ncomputation channel: when increasing the processing throughput, the output\nnoise (error) increases due to (i) coarser quantization and (ii) computational\nerrors caused by exceeding the machine-precision limitations. We show that,\nunder certain distortion in the GEMM computation, the proposed framework can\nsignificantly surpass 100% of the peak performance of a given processor. The\npractical benefits of our proposal are shown in a face recognition system and a\nmulti-layer perceptron system trained for metadata learning from a large music\nfeature database.", 
    "link": "http://arxiv.org/pdf/1110.5765v1", 
    "arxiv-id": "1110.5765v1"
},{
    "category": "cs.DS", 
    "author": "Gabriella Trucco", 
    "title": "The Binary Perfect Phylogeny with Persistent characters", 
    "publish": "2011-10-31T10:17:52Z", 
    "summary": "The binary perfect phylogeny model is too restrictive to model biological\nevents such as back mutations. In this paper we consider a natural\ngeneralization of the model that allows a special type of back mutation. We\ninvestigate the problem of reconstructing a near perfect phylogeny over a\nbinary set of characters where characters are persistent: characters can be\ngained and lost at most once. Based on this notion, we define the problem of\nthe Persistent Perfect Phylogeny (referred as P-PP). We restate the P-PP\nproblem as a special case of the Incomplete Directed Perfect Phylogeny, called\nIncomplete Perfect Phylogeny with Persistent Completion, (refereed as IP-PP),\nwhere the instance is an incomplete binary matrix M having some missing\nentries, denoted by symbol ?, that must be determined (or completed) as 0 or 1\nso that M admits a binary perfect phylogeny. We show that the IP-PP problem can\nbe reduced to a problem over an edge colored graph since the completion of each\ncolumn of the input matrix can be represented by a graph operation. Based on\nthis graph formulation, we develop an exact algorithm for solving the P-PP\nproblem that is exponential in the number of characters and polynomial in the\nnumber of species.", 
    "link": "http://arxiv.org/pdf/1110.6739v2", 
    "arxiv-id": "1110.6739v2"
},{
    "category": "q-bio.PE", 
    "author": "Jakub Truszkowski", 
    "title": "Fast reconstruction of phylogenetic trees using locality-sensitive   hashing", 
    "publish": "2011-11-02T04:21:19Z", 
    "summary": "We present the first sub-quadratic time algorithm that with high probability\ncorrectly reconstructs phylogenetic trees for short sequences generated by a\nMarkov model of evolution. Due to rapid expansion in sequence databases, such\nvery fast algorithms are becoming necessary. Other fast heuristics have been\ndeveloped for building trees from very large alignments (Price et al, and Brown\net al), but they lack theoretical performance guarantees. Our new algorithm\nruns in $O(n^{1+\\gamma(g)}\\log^2n)$ time, where $\\gamma$ is an increasing\nfunction of an upper bound on the branch lengths in the phylogeny, the upper\nbound $g$ must be below$1/2-\\sqrt{1/8} \\approx 0.15$, and $\\gamma(g)<1$ for all\n$g$. For phylogenies with very short branches, the running time of our\nalgorithm is close to linear. For example, if all branch lengths correspond to\na mutation probability of less than 0.02, the running time of our algorithm is\nroughly $O(n^{1.2}\\log^2n)$. Via a prototype and a sequence of large-scale\nexperiments, we show that many large phylogenies can be reconstructed fast,\nwithout compromising reconstruction accuracy.", 
    "link": "http://arxiv.org/pdf/1111.0379v2", 
    "arxiv-id": "1111.0379v2"
},{
    "category": "q-bio.GN", 
    "author": "Alexander Schliep", 
    "title": "SLIQ: Simple Linear Inequalities for Efficient Contig Scaffolding", 
    "publish": "2011-11-06T15:46:57Z", 
    "summary": "Scaffolding is an important subproblem in \"de novo\" genome assembly in which\nmate pair data are used to construct a linear sequence of contigs separated by\ngaps. Here we present SLIQ, a set of simple linear inequalities derived from\nthe geometry of contigs on the line that can be used to predict the relative\npositions and orientations of contigs from individual mate pair reads and thus\nproduce a contig digraph. The SLIQ inequalities can also filter out unreliable\nmate pairs and can be used as a preprocessing step for any scaffolding\nalgorithm. We tested the SLIQ inequalities on five real data sets ranging in\ncomplexity from simple bacterial genomes to complex mammalian genomes and\ncompared the results to the majority voting procedure used by many other\nscaffolding algorithms. SLIQ predicted the relative positions and orientations\nof the contigs with high accuracy in all cases and gave more accurate position\npredictions than majority voting for complex genomes, in particular the human\ngenome. Finally, we present a simple scaffolding algorithm that produces linear\nscaffolds given a contig digraph. We show that our algorithm is very efficient\ncompared to other scaffolding algorithms while maintaining high accuracy in\npredicting both contig positions and orientations for real data sets.", 
    "link": "http://arxiv.org/pdf/1111.1426v2", 
    "arxiv-id": "1111.1426v2"
},{
    "category": "cs.AI", 
    "author": "Jesus A. Lopez", 
    "title": "Control Neuronal por Modelo Inverso de un Servosistema Usando Algoritmos   de Aprendizaje Levenberg-Marquardt y Bayesiano", 
    "publish": "2011-11-18T03:26:47Z", 
    "summary": "In this paper we present the experimental results of the neural network\ncontrol of a servo-system in order to control its speed. The control strategy\nis implemented by using an inverse-model control based on Artificial Neural\nNetworks (ANNs). The network training was performed using two learning\nalgorithms: Levenberg-Marquardt and Bayesian regularization. We evaluate the\ngeneralization capability for each method according to both the correct\noperation of the controller to follow the reference signal, and the control\nefforts developed by the ANN-based controller.", 
    "link": "http://arxiv.org/pdf/1111.4267v1", 
    "arxiv-id": "1111.4267v1"
},{
    "category": "physics.ao-ph", 
    "author": "Christopher R. Johnson", 
    "title": "A wildland fire modeling and visualization environment", 
    "publish": "2011-11-20T05:50:00Z", 
    "summary": "We present an overview of a modeling environment, consisting of a coupled\natmosphere-wildfire model, utilities for visualization, data processing, and\ndiagnostics, open source software repositories, and a community wiki. The fire\nmodel, called SFIRE, is based on a fire-spread model, implemented by the\nlevel-set method, and it is coupled with the Weather Research Forecasting (WRF)\nmodel. A version with a subset of the features is distributed with WRF 3.3 as\nWRF-Fire. In each time step, the fire module takes the wind as input and\nreturns the latent and sensible heat fluxes. The software architecture uses WRF\nparallel infrastructure for massively parallel computing. Recent features of\nthe code include interpolation from an ideal logarithmic wind profile for\nnonhomogeneous fuels and ignition from a fire perimeter with an atmosphere and\nfire spin-up. Real runs use online sources for fuel maps, fine-scale\ntopography, and meteorological data, and can run faster than real time.\nVisualization pathways allow generating images and animations in many packages,\nincluding VisTrails, VAPOR, MayaVi, and Paraview, as well as output to Google\nEarth. The environment is available from openwfm.org. New diagnostic variables\nwere added to the code recently, including a new kind of fireline intensity,\nwhich takes into account also the speed of burning, unlike Byram's fireline\nintensity.", 
    "link": "http://arxiv.org/pdf/1111.4610v1", 
    "arxiv-id": "1111.4610v1"
},{
    "category": "q-bio.MN", 
    "author": "Ivo F. Sbalzarini", 
    "title": "Global parameter identification of stochastic reaction networks from   single trajectories", 
    "publish": "2011-11-21T08:30:29Z", 
    "summary": "We consider the problem of inferring the unknown parameters of a stochastic\nbiochemical network model from a single measured time-course of the\nconcentration of some of the involved species. Such measurements are available,\ne.g., from live-cell fluorescence microscopy in image-based systems biology. In\naddition, fluctuation time-courses from, e.g., fluorescence correlation\nspectroscopy provide additional information about the system dynamics that can\nbe used to more robustly infer parameters than when considering only mean\nconcentrations. Estimating model parameters from a single experimental\ntrajectory enables single-cell measurements and quantification of cell--cell\nvariability. We propose a novel combination of an adaptive Monte Carlo sampler,\ncalled Gaussian Adaptation, and efficient exact stochastic simulation\nalgorithms that allows parameter identification from single stochastic\ntrajectories. We benchmark the proposed method on a linear and a non-linear\nreaction network at steady state and during transient phases. In addition, we\ndemonstrate that the present method also provides an ellipsoidal volume\nestimate of the viable part of parameter space and is able to estimate the\nphysical volume of the compartment in which the observed reactions take place.", 
    "link": "http://arxiv.org/pdf/1111.4785v1", 
    "arxiv-id": "1111.4785v1"
},{
    "category": "cs.NE", 
    "author": "Gene I. Sher", 
    "title": "Evolving Chart Pattern Sensitive Neural Network Based Forex Trading   Agents", 
    "publish": "2011-11-25T05:11:14Z", 
    "summary": "Though machine learning has been applied to the foreign exchange market for\nalgorithmic trading for quiet some time now, and neural networks(NN) have been\nshown to yield positive results, in most modern approaches the NN systems are\noptimized through traditional methods like the backpropagation algorithm for\nexample, and their input signals are price lists, and lists composed of other\ntechnical indicator elements. The aim of this paper is twofold: the\npresentation and testing of the application of topology and weight evolving\nartificial neural network (TWEANN) systems to automated currency trading, and\nto demonstrate the performance when using Forex chart images as input to\ngeometrical regularity aware indirectly encoded neural network systems,\nenabling them to use the patterns & trends within, when trading. This paper\npresents the benchmark results of NN based automated currency trading systems\nevolved using TWEANNs, and compares the performance and generalization\ncapabilities of these direct encoded NNs which use the standard sliding-window\nbased price vector inputs, and the indirect (substrate) encoded NNs which use\ncharts as input. The TWEANN algorithm I will use in this paper to evolve these\ncurrency trading agents is the memetic algorithm based TWEANN system called\nDeus Ex Neural Network (DXNN) platform.", 
    "link": "http://arxiv.org/pdf/1111.5892v2", 
    "arxiv-id": "1111.5892v2"
},{
    "category": "cs.NA", 
    "author": "Ashish Garg", 
    "title": "Vertex-centroid finite volume scheme on tetrahedral grids for   conservation laws", 
    "publish": "2011-12-19T04:54:47Z", 
    "summary": "Vertex-centroid schemes are cell-centered finite volume schemes for\nconservation laws which make use of vertex values to construct high resolution\nschemes. The vertex values must be obtained through a consistent averaging\n(interpolation) procedure. A modified interpolation scheme is proposed which is\nbetter than existing schemes in giving positive weights in the interpolation\nformula. A simplified reconstruction scheme is also proposed which is also more\naccurate and efficient. For scalar conservation laws, we develop limited\nversions of the schemes which are stable in maximum norm by constructing\nsuitable limiters. The schemes are applied to compressible flows governed by\nthe Euler equations of inviscid gas dynamics.", 
    "link": "http://arxiv.org/pdf/1112.4238v1", 
    "arxiv-id": "1112.4238v1"
},{
    "category": "cs.CE", 
    "author": "Ulrich R\u00fcde", 
    "title": "Liquid-gas-solid flows with lattice Boltzmann: Simulation of floating   bodies", 
    "publish": "2012-01-01T13:34:55Z", 
    "summary": "This paper presents a model for the simulation of liquid-gas-solid flows by\nmeans of the lattice Boltzmann method. The approach is built upon previous\nworks for the simulation of liquid-solid particle suspensions on the one hand,\nand on a liquid-gas free surface model on the other. We show how the two\napproaches can be unified by a novel set of dynamic cell conversion rules. For\nevaluation, we concentrate on the rotational stability of non-spherical rigid\nbodies floating on a plane water surface - a classical hydrostatic problem\nknown from naval architecture. We show the consistency of our method in this\nkind of flows and obtain convergence towards the ideal solution for the\nmeasured heeling stability of a floating box.", 
    "link": "http://arxiv.org/pdf/1201.0351v1", 
    "arxiv-id": "1201.0351v1"
},{
    "category": "cs.CE", 
    "author": "Larry Bull", 
    "title": "On Natural Genetic Engineering: Structural Dynamism in Random Boolean   Networks", 
    "publish": "2012-01-16T11:27:52Z", 
    "summary": "This short paper presents an abstract, tunable model of genomic structural\nchange within the cell lifecycle and explores its use with simulated evolution.\nA well-known Boolean model of genetic regulatory networks is extended to\ninclude changes in node connectivity based upon the current cell state, e.g.,\nvia transposable elements. The underlying behaviour of the resulting dynamical\nnetworks is investigated before their evolvability is explored using a version\nof the NK model of fitness landscapes. Structural dynamism is found to be\nselected for in non-stationary environments and subsequently shown capable of\nproviding a mechanism for evolutionary innovation when such reorganizations are\ninherited.", 
    "link": "http://arxiv.org/pdf/1201.3545v1", 
    "arxiv-id": "1201.3545v1"
},{
    "category": "cs.NA", 
    "author": "Hermann G. Matthies", 
    "title": "Parameter Identification in a Probabilistic Setting", 
    "publish": "2012-01-19T13:00:34Z", 
    "summary": "Parameter identification problems are formulated in a probabilistic language,\nwhere the randomness reflects the uncertainty about the knowledge of the true\nvalues. This setting allows conceptually easily to incorporate new information,\ne.g. through a measurement, by connecting it to Bayes's theorem. The unknown\nquantity is modelled as a (may be high-dimensional) random variable. Such a\ndescription has two constituents, the measurable function and the measure. One\ngroup of methods is identified as updating the measure, the other group changes\nthe measurable function. We connect both groups with the relatively recent\nmethods of functional approximation of stochastic problems, and introduce\nespecially in combination with the second group of methods a new procedure\nwhich does not need any sampling, hence works completely deterministically. It\nalso seems to be the fastest and more reliable when compared with other\nmethods. We show by example that it also works for highly nonlinear non-smooth\nproblems with non-Gaussian measures.", 
    "link": "http://arxiv.org/pdf/1201.4049v1", 
    "arxiv-id": "1201.4049v1"
},{
    "category": "cs.CE", 
    "author": "Alvaro Juan Ojeda Garcia", 
    "title": "Mathematical and computational modeling for describing the basic   behavior of free radicals and antioxidants within epithelial cells", 
    "publish": "2012-01-21T19:12:38Z", 
    "summary": "The traditional methods of the biology, based on illustrative descriptions\nand linear logic explanations, are discussed. This work aims to improve this\napproach by introducing alternative tools to describe and represent complex\nbiological systems. Two models were developed, one mathematical and another\ncomputational, both were made in order to study the biological process between\nfree radicals and antioxidants. Each model was used to study the same process\nbut in different scenarios. The mathematical model was used to study the\nbiological process in an epithelial cells culture; this model was validated\nwith the experimental data of Anne Hanneken's research group from the\nDepartment of Molecular and Experimental Medicine, published by the journal\nInvestigative Ophthalmology and Visual Science in July 2006. The computational\nmodel was used to study the same process in an individual. The model was made\nusing C++ programming language, supported by the network theory of aging.", 
    "link": "http://arxiv.org/pdf/1201.4499v1", 
    "arxiv-id": "1201.4499v1"
},{
    "category": "cs.CV", 
    "author": "G. R. Sridhar", 
    "title": "Robust seed selection algorithm for k-means type algorithms", 
    "publish": "2012-02-08T03:07:39Z", 
    "summary": "Selection of initial seeds greatly affects the quality of the clusters and in\nk-means type algorithms. Most of the seed selection methods result different\nresults in different independent runs. We propose a single, optimal, outlier\ninsensitive seed selection algorithm for k-means type algorithms as extension\nto k-means++. The experimental results on synthetic, real and on microarray\ndata sets demonstrated that effectiveness of the new algorithm in producing the\nclustering results", 
    "link": "http://arxiv.org/pdf/1202.1585v1", 
    "arxiv-id": "1202.1585v1"
},{
    "category": "cs.CE", 
    "author": "Bernard Bourges", 
    "title": "Application of sensitivity analysis in building energy simulations:   combining first and second order elementary effects Methods", 
    "publish": "2012-03-14T11:43:21Z", 
    "summary": "Sensitivity analysis plays an important role in the understanding of complex\nmodels. It helps to identify influence of input parameters in relation to the\noutputs. It can be also a tool to understand the behavior of the model and then\ncan help in its development stage. This study aims to analyze and illustrate\nthe potential usefulness of combining first and second-order sensitivity\nanalysis, applied to a building energy model (ESP-r). Through the example of a\ncollective building, a sensitivity analysis is performed using the method of\nelementary effects (also known as Morris method), including an analysis of\ninteractions between the input parameters (second order analysis). Importance\nof higher-order analysis to better support the results of first order analysis,\nhighlighted especially in such complex model. Several aspects are tackled to\nimplement efficiently the multi-order sensitivity analysis: interval size of\nthe variables, management of non-linearity, usefulness of various outputs.", 
    "link": "http://arxiv.org/pdf/1203.3055v2", 
    "arxiv-id": "1203.3055v2"
},{
    "category": "cs.NE", 
    "author": "Jaafar Abouchabaka", 
    "title": "A Comparative Study of Adaptive Crossover Operators for Genetic   Algorithms to Resolve the Traveling Salesman Problem", 
    "publish": "2012-03-14T14:32:45Z", 
    "summary": "Genetic algorithm includes some parameters that should be adjusting so that\nthe algorithm can provide positive results. Crossover operators play very\nimportant role by constructing competitive Genetic Algorithms (GAs). In this\npaper, the basic conceptual features and specific characteristics of various\ncrossover operators in the context of the Traveling Salesman Problem (TSP) are\ndiscussed. The results of experimental comparison of more than six different\ncrossover operators for the TSP are presented. The experiment results show that\nOX operator enables to achieve a better solutions than other operators tested.", 
    "link": "http://arxiv.org/pdf/1203.3097v1", 
    "arxiv-id": "1203.3097v1"
},{
    "category": "cs.NE", 
    "author": "Chakir Tajani", 
    "title": "Analyzing the Performance of Mutation Operators to Solve the Travelling   Salesman Problem", 
    "publish": "2012-03-14T14:38:18Z", 
    "summary": "The genetic algorithm includes some parameters that should be adjusted, so as\nto get reliable results. Choosing a representation of the problem addressed, an\ninitial population, a method of selection, a crossover operator, mutation\noperator, the probabilities of crossover and mutation, and the insertion method\ncreates a variant of genetic algorithms. Our work is part of the answer to this\nperspective to find a solution for this combinatorial problem. What are the\nbest parameters to select for a genetic algorithm that creates a variety\nefficient to solve the Travelling Salesman Problem (TSP)? In this paper, we\npresent a comparative analysis of different mutation operators, surrounded by a\ndilated discussion that justifying the relevance of genetic operators chosen to\nsolving the TSP problem.", 
    "link": "http://arxiv.org/pdf/1203.3099v1", 
    "arxiv-id": "1203.3099v1"
},{
    "category": "cs.CV", 
    "author": "Mauricio Reyes", 
    "title": "Skull-stripping for Tumor-bearing Brain Images", 
    "publish": "2012-04-02T09:48:12Z", 
    "summary": "Skull-stripping separates the skull region of the head from the soft brain\ntissues. In many cases of brain image analysis, this is an essential\npreprocessing step in order to improve the final result. This is true for both\nregistration and segmentation tasks. In fact, skull-stripping of magnetic\nresonance images (MRI) is a well-studied problem with numerous publications in\nrecent years. Many different algorithms have been proposed, a summary and\ncomparison of which can be found in [Fennema-Notestine, 2006]. Despite the\nabundance of approaches, we discovered that the algorithms which had been\nsuggested so far, perform poorly when dealing with tumor-bearing brain images.\nThis is mostly due to additional difficulties in separating the brain from the\nskull in this case, especially when the lesion is located very close to the\nskull border. Additionally, images acquired according to standard clinical\nprotocols, often exhibit anisotropic resolution and only partial coverage,\nwhich further complicates the task. Therefore, we developed a method which is\ndedicated to skull-stripping for clinically acquired tumor-bearing brain\nimages.", 
    "link": "http://arxiv.org/pdf/1204.0357v1", 
    "arxiv-id": "1204.0357v1"
},{
    "category": "cs.MS", 
    "author": "Andrew G. Salinger", 
    "title": "Automating embedded analysis capabilities and managing software   complexity in multiphysics simulation part I: template-based generic   programming", 
    "publish": "2012-05-03T18:24:54Z", 
    "summary": "An approach for incorporating embedded simulation and analysis capabilities\nin complex simulation codes through template-based generic programming is\npresented. This approach relies on templating and operator overloading within\nthe C++ language to transform a given calculation into one that can compute a\nvariety of additional quantities that are necessary for many state-of-the-art\nsimulation and analysis algorithms. An approach for incorporating these ideas\ninto complex simulation codes through general graph-based assembly is also\npresented. These ideas have been implemented within a set of packages in the\nTrilinos framework and are demonstrated on a simple problem from chemical\nengineering.", 
    "link": "http://arxiv.org/pdf/1205.0790v2", 
    "arxiv-id": "1205.0790v2"
},{
    "category": "cs.CE", 
    "author": "Khalid Raza", 
    "title": "Application Of Data Mining In Bioinformatics", 
    "publish": "2012-05-05T12:19:33Z", 
    "summary": "This article highlights some of the basic concepts of bioinformatics and data\nmining. The major research areas of bioinformatics are highlighted. The\napplication of data mining in the domain of bioinformatics is explained. It\nalso highlights some of the current challenges and opportunities of data mining\nin bioinformatics.", 
    "link": "http://arxiv.org/pdf/1205.1125v1", 
    "arxiv-id": "1205.1125v1"
},{
    "category": "cs.CE", 
    "author": "Z. S. Liu", 
    "title": "High Velocity Penetration/Perforation Using Coupled Smooth Particle   Hydrodynamics-Finite Element Method", 
    "publish": "2012-05-07T15:24:56Z", 
    "summary": "Finite element method (FEM) suffers from a serious mesh distortion problem\nwhen used for high velocity impact analyses. The smooth particle hydrodynamics\n(SPH) method is appropriate for this class of problems involving severe damages\nbut at considerable computational cost. It is beneficial if the latter is\nadopted only in severely distorted regions and FEM further away. The coupled\nsmooth particle hydrodynamics - finite element method (SFM) has been adopted in\na commercial hydrocode LS-DYNA to study the perforation of Weldox 460E steel\nand AA5083-H116 aluminum plates with varying thicknesses and various projectile\nnose geometries including blunt, conical and ogival noses. Effects of the SPH\ndomain size and particle density are studied considering the friction effect\nbetween the projectile and the target materials. The simulated residual\nvelocities and the ballistic limit velocities from the SFM agree well with the\npublished experimental data. The study shows that SFM is able to emulate the\nsame failure mechanisms of the steel and aluminum plates as observed in various\nexperimental investigations for initial impact velocity of 170 m/s and higher.", 
    "link": "http://arxiv.org/pdf/1205.1428v1", 
    "arxiv-id": "1205.1428v1"
},{
    "category": "cs.CE", 
    "author": "Rafat Parveen", 
    "title": "Evolutionary algorithms in genetic regulatory networks model", 
    "publish": "2012-05-09T14:04:35Z", 
    "summary": "Genetic Regulatory Networks (GRNs) plays a vital role in the understanding of\ncomplex biological processes. Modeling GRNs is significantly important in order\nto reveal fundamental cellular processes, examine gene functions and\nunderstanding their complex relationships. Understanding the interactions\nbetween genes gives rise to develop better method for drug discovery and\ndiagnosis of the disease since many diseases are characterized by abnormal\nbehaviour of the genes. In this paper we have reviewed various evolutionary\nalgorithms-based approach for modeling GRNs and discussed various opportunities\nand challenges.", 
    "link": "http://arxiv.org/pdf/1205.1986v1", 
    "arxiv-id": "1205.1986v1"
},{
    "category": "cs.CE", 
    "author": "Zhanshan Sam Ma", 
    "title": "A Note on Extending Taylor's Power Law for Characterizing Human   Microbial Communities: Inspiration from Comparative Studies on the   Distribution Patterns of Insects and Galaxies, and as a Case Study for   Medical Ecology", 
    "publish": "2012-05-15T20:26:44Z", 
    "summary": "Many natural patterns, such as the distributions of blood particles in a\nblood sample, proteins on cell surfaces, biological populations in their\nhabitat, galaxies in the universe, the sequence of human genes, and the fitness\nin evolutionary computing, have been found to follow power law. Taylor's power\nlaw (Taylor 1961: Nature, 189:732-) is well recognized as one of the\nfundamental models in population ecology. A fundamental property of biological\npopulations, which Taylor's power law reveals, is the near universal\nheterogeneity of population abundance distribution in habitat. Obviously, the\nheterogeneity also exists at the community level, where not only the\ndistributions of population abundances but also the proportions of the species\ncomposition in the community are often heterogeneous. Nevertheless, existing\ncommunity diversity indexes such as Shannon index and Simpson index can only\nmeasure \"local\" or \"static\" diversity in the sense that they are computed for\neach habitat at a specific time point, and the indexes alone do not reflect the\ndiversity changes. In this note, I propose to extend the application scope of\nTaylor's power law to the studies of human microbial communities, specifically,\nthe community heterogeneity at both population and community levels. I further\nsuggested that population dispersion models such as Taylor (1980: Nature, 286,\n53-), which is known to generate population distribution patterns consistent\nwith the power law, should also be very useful for analyzing the distribution\npatterns of human microbes within the human body. Overall, I hope that the\napproach to human microbial community with the power law offers an example that\necological theories can play an important role in the emerging medical ecology,\nwhich aims at studying the ecology of human microbiome and its implications to\nhuman diseases and health, as well as in personalized medicine.", 
    "link": "http://arxiv.org/pdf/1205.3504v1", 
    "arxiv-id": "1205.3504v1"
},{
    "category": "cs.MS", 
    "author": "Roger Pawlowski", 
    "title": "Efficient Expression Templates for Operator Overloading-based Automatic   Differentiation", 
    "publish": "2012-05-15T20:42:23Z", 
    "summary": "Expression templates are a well-known set of techniques for improving the\nefficiency of operator overloading-based forward mode automatic differentiation\nschemes in the C++ programming language by translating the differentiation from\nindividual operators to whole expressions. However standard expression template\napproaches result in a large amount of duplicate computation, particularly for\nlarge expression trees, degrading their performance. In this paper we describe\nseveral techniques for improving the efficiency of expression templates and\ntheir implementation in the automatic differentiation package Sacado. We\ndemonstrate their improved efficiency through test functions as well as their\napplication to differentiation of a large-scale fluid dynamics simulation code.", 
    "link": "http://arxiv.org/pdf/1205.3506v1", 
    "arxiv-id": "1205.3506v1"
},{
    "category": "physics.comp-ph", 
    "author": "A. W. M. van Schijndel", 
    "title": "The Simulation and Mapping of Building Performance Indicators based on   European Weather Stations", 
    "publish": "2012-05-16T06:45:44Z", 
    "summary": "Due to the climate change debate, a lot of research and maps of external\nclimate parameters are available. However, maps of indoor climate performance\nparameters are still lacking. This paper presents a methodology for obtaining\nmaps of performances of similar buildings that are virtually spread over whole\nEurope. The produced maps are useful for analyzing regional climate influence\non building performance indicators such as energy use and indoor climate. This\nis shown using the Bestest building as a reference benchmark. An important\napplication of the mapping tool is the visualization of potential building\nmeasures over the EU. Also the performances of single building components can\nbe simulated and mapped. It is concluded that the presented method is efficient\nas it takes less than 15 minutes to simulate and produce the maps on a\n2.6GHz/4GB computer. Moreover, the approach is applicable for any type of\nbuilding.", 
    "link": "http://arxiv.org/pdf/1205.3569v1", 
    "arxiv-id": "1205.3569v1"
},{
    "category": "cs.MS", 
    "author": "Matthew L. Staten", 
    "title": "Automating embedded analysis capabilities and managing software   complexity in multiphysics simulation part II: application to partial   differential equations", 
    "publish": "2012-05-17T15:18:00Z", 
    "summary": "A template-based generic programming approach was presented in a previous\npaper that separates the development effort of programming a physical model\nfrom that of computing additional quantities, such as derivatives, needed for\nembedded analysis algorithms. In this paper, we describe the implementation\ndetails for using the template-based generic programming approach for\nsimulation and analysis of partial differential equations (PDEs). We detail\nseveral of the hurdles that we have encountered, and some of the software\ninfrastructure developed to overcome them. We end with a demonstration where we\npresent shape optimization and uncertainty quantification results for a 3D PDE\napplication.", 
    "link": "http://arxiv.org/pdf/1205.3952v1", 
    "arxiv-id": "1205.3952v1"
},{
    "category": "cs.CE", 
    "author": "H. Chandrasekharan", 
    "title": "Analytical Study of Hexapod miRNAs using Phylogenetic Methods", 
    "publish": "2012-05-22T10:28:29Z", 
    "summary": "MicroRNAs (miRNAs) are a class of non-coding RNAs that regulate gene\nexpression. Identification of total number of miRNAs even in completely\nsequenced organisms is still an open problem. However, researchers have been\nusing techniques that can predict limited number of miRNA in an organism. In\nthis paper, we have used homology based approach for comparative analysis of\nmiRNA of hexapoda group .We have used Apis mellifera, Bombyx mori, Anopholes\ngambiae and Drosophila melanogaster miRNA datasets from miRBase repository. We\nhave done pair wise as well as multiple alignments for the available miRNAs in\nthe repository to identify and analyse conserved regions among related species.\nUnfortunately, to the best of our knowledge, miRNA related literature does not\nprovide in depth analysis of hexapods. We have made an attempt to derive the\ncommonality among the miRNAs and to identify the conserved regions which are\nstill not available in miRNA repositories. The results are good approximation\nwith a small number of mismatches. However, they are encouraging and may\nfacilitate miRNA biogenesis for", 
    "link": "http://arxiv.org/pdf/1205.5024v1", 
    "arxiv-id": "1205.5024v1"
},{
    "category": "cs.CE", 
    "author": "Jan H. Jensen", 
    "title": "FragIt: A Tool to Prepare Input Files for Fragment Based Quantum   Chemical Calculations", 
    "publish": "2012-05-22T14:39:44Z", 
    "summary": "Near linear scaling fragment based quantum chemical calculations are becoming\nincreasingly popular for treating large systems with high accuracy and is an\nactive field of research. However, it remains difficult to set up these\ncalculations without expert knowledge. To facilitate the use of such methods,\nsoftware tools need to be available to support these methods and help to set up\nreasonable input files which will lower the barrier of entry for usage by\nnon-experts. Previous tools relies on specific annotations in structure files\nfor automatic and successful fragmentation such as residues in PDB files. We\npresent a general fragmentation methodology and accompanying tools called\nFragIt to help setup these calculations. FragIt uses the SMARTS language to\nlocate chemically appropriate fragments in large structures and is applicable\nto fragmentation of any molecular system given suitable SMARTS patterns. We\npresent SMARTS patterns of fragmentation for proteins, DNA and polysaccharides,\nspecifically for D-galactopyranose for use in cyclodextrins. FragIt is used to\nprepare input files for the Fragment Molecular Orbital method in the GAMESS\nprogram package, but can be extended to other computational methods easily.", 
    "link": "http://arxiv.org/pdf/1205.5025v2", 
    "arxiv-id": "1205.5025v2"
},{
    "category": "q-bio.GN", 
    "author": "Filippo Utro", 
    "title": "The Chromatin Organization of an Eukaryotic Genome : Sequence Specific+   Statistical=Combinatorial (Extended Abstract)", 
    "publish": "2012-05-27T23:49:33Z", 
    "summary": "Nucleosome organization in eukaryotic genomes has a deep impact on gene\nfunction. Although progress has been recently made in the identification of\nvarious concurring factors influencing nucleosome positioning, it is still\nunclear whether nucleosome positions are sequence dictated or determined by a\nrandom process. It has been postulated for a long time that,in the proximity of\nTSS, a barrier determines the position of the +1 nucleosome and then geometric\nconstraints alter the random positioning process determining nucleosomal\nphasing. Such a pattern fades out as one moves away from the barrier to become\nagain a random positioning process. Although this statistical model is widely\naccepted,the molecular nature of the barrier is still unknown. Moreover,we are\nfar from the identification of a set of sequence rules able:to account for the\ngenome-wide nucleosome organization;to explain the nature of the barriers on\nwhich the statistical mechanism hinges;to allow for a smooth transition from\nsequence-dictated to statistical positioning and back. We show that sequence\ncomplexity,quantified via various methods, can be the rule able to at least\npartially account for all the above.In particular, we have conducted our\nanalyses on 4 high resolution nucleosomal maps of the model eukaryotes and\nfound that nucleosome depleted regions can be well distinguished from\nnucleosome enriched regions by sequence complexity measures.In particular, (a)\nthe depleted regions are less complex than the enriched ones, (b) around TSS\ncomplexity measures alone are in striking agreement with in vivo nucleosome\noccupancy,in particular precisely indicating the positions of the +1 and -1\nnucleosomes. Those findings indicate that the intrinsic richness of\nsubsequences within sequences plays a role in nucleosomal formation in genomes,\nand that sequence complexity constitutes the molecular nature of nucleosome\nbarrier.", 
    "link": "http://arxiv.org/pdf/1205.6010v1", 
    "arxiv-id": "1205.6010v1"
},{
    "category": "cs.NE", 
    "author": "Amit Konar", 
    "title": "An Evolutionary Approach to Drug-Design Using a Novel Neighbourhood   Based Genetic Algorithm", 
    "publish": "2012-05-03T05:08:23Z", 
    "summary": "The present work provides a new approach to evolve ligand structures which\nrepresent possible drug to be docked to the active site of the target protein.\nThe structure is represented as a tree where each non-empty node represents a\nfunctional group. It is assumed that the active site configuration of the\ntarget protein is known with position of the essential residues. In this paper\nthe interaction energy of the ligands with the protein target is minimized.\nMoreover, the size of the tree is difficult to obtain and it will be different\nfor different active sites. To overcome the difficulty, a variable tree size\nconfiguration is used for designing ligands. The optimization is done using a\nnovel Neighbourhood Based Genetic Algorithm (NBGA) which uses dynamic\nneighbourhood topology. To get variable tree size, a variable-length version of\nthe above algorithm is devised. To judge the merit of the algorithm, it is\ninitially applied on the well known Travelling Salesman Problem (TSP).", 
    "link": "http://arxiv.org/pdf/1205.6412v1", 
    "arxiv-id": "1205.6412v1"
},{
    "category": "cs.GT", 
    "author": "Rushi P Bhatt", 
    "title": "Real-Time Bid Optimization for Group-Buying Ads", 
    "publish": "2012-06-03T17:25:00Z", 
    "summary": "Group-buying ads seeking a minimum number of customers before the deal expiry\nare increasingly used by the daily-deal providers. Unlike the traditional web\nads, the advertiser's profits for group-buying ads depends on the time to\nexpiry and additional customers needed to satisfy the minimum group size. Since\nboth these quantities are time-dependent, optimal bid amounts to maximize\nprofits change with every impression. Consequently, traditional static bidding\nstrategies are far from optimal. Instead, bid values need to be optimized in\nreal-time to maximize expected bidder profits. This online optimization of deal\nprofits is made possible by the advent of ad exchanges offering real-time\n(spot) bidding. To this end, we propose a real-time bidding strategy for\ngroup-buying deals based on the online optimization of bid values. We derive\nthe expected bidder profit of deals as a function of the bid amounts, and\ndynamically vary bids to maximize profits. Further, to satisfy time constraints\nof the online bidding, we present methods of minimizing computation timings.\nSubsequently, we derive the real time ad selection, admissibility, and real\ntime bidding of the traditional ads as the special cases of the proposed\nmethod. We evaluate the proposed bidding, selection and admission strategies on\na multi-million click stream of 935 ads. The proposed real-time bidding,\nselection and admissibility show significant profit increases over the existing\nstrategies. Further the experiments illustrate the robustness of the bidding\nand acceptable computation timings.", 
    "link": "http://arxiv.org/pdf/1206.0469v1", 
    "arxiv-id": "1206.0469v1"
},{
    "category": "cs.NE", 
    "author": "Jubin Hazra", 
    "title": "An Evolutionary Approach to Drug-Design Using Quantam Binary Particle   Swarm Optimization Algorithm", 
    "publish": "2012-05-03T05:06:26Z", 
    "summary": "The present work provides a new approach to evolve ligand structures which\nrepresent possible drug to be docked to the active site of the target protein.\nThe structure is represented as a tree where each non-empty node represents a\nfunctional group. It is assumed that the active site configuration of the\ntarget protein is known with position of the essential residues. In this paper\nthe interaction energy of the ligands with the protein target is minimized.\nMoreover, the size of the tree is difficult to obtain and it will be different\nfor different active sites. To overcome the difficulty, a variable tree size\nconfiguration is used for designing ligands. The optimization is done using a\nquantum discrete PSO. The result using fixed length and variable length\nconfiguration are compared.", 
    "link": "http://arxiv.org/pdf/1206.4588v1", 
    "arxiv-id": "1206.4588v1"
},{
    "category": "stat.AP", 
    "author": "David Heckerman", 
    "title": "Determining the Number of Non-Spurious Arcs in a Learned DAG Model:   Investigation of a Bayesian and a Frequentist Approach", 
    "publish": "2012-06-20T15:04:30Z", 
    "summary": "In many application domains, such as computational biology, the goal of\ngraphical model structure learning is to uncover discrete relationships between\nentities. For example, in our problem of interest concerning HIV vaccine\ndesign, we want to infer which HIV peptides interact with which immune system\nmolecules (HLA molecules). For problems of this nature, we are interested in\ndetermining the number of nonspurious arcs in a learned graphical model. We\ndescribe both a Bayesian and frequentist approach to this problem. In the\nBayesian approach, we use the posterior distribution over model structures to\ncompute the expected number of true arcs in a learned model. In the frequentist\napproach, we develop a method based on the concept of the False Discovery Rate.\nOn synthetic data sets generated from models similar to the ones learned, we\nfind that both the Bayesian and frequentist approaches yield accurate estimates\nof the number of non-spurious arcs. In addition, we speculate that the\nfrequentist approach, which is non-parametric, may outperform the parametric\nBayesian approach in situations where the models learned are less\nrepresentative of the data. Finally, we apply the frequentist approach to our\nproblem of HIV vaccine design.", 
    "link": "http://arxiv.org/pdf/1206.5269v1", 
    "arxiv-id": "1206.5269v1"
},{
    "category": "cs.PL", 
    "author": "Franck Delaplace", 
    "title": "GUBS, a Behavior-based Language for Open System Dedicated to Synthetic   Biology", 
    "publish": "2012-06-26T19:56:41Z", 
    "summary": "In this article, we propose a domain specific language, GUBS (Genomic Unified\nBehavior Specification), dedicated to the behavioral specification of synthetic\nbiological devices, viewed as discrete open dynamical systems. GUBS is a\nrule-based declarative language. By contrast to a closed system, a program is\nalways a partial description of the behavior of the system. The semantics of\nthe language accounts the existence of some hidden non-specified actions\npossibly altering the behavior of the programmed device. The compilation\nframework follows a scheme similar to automatic theorem proving, aiming at\nimproving synthetic biological design safety.", 
    "link": "http://arxiv.org/pdf/1206.6098v2", 
    "arxiv-id": "1206.6098v2"
},{
    "category": "cs.AI", 
    "author": "Linda C. van der Gaag", 
    "title": "Sensitivity Analysis for Threshold Decision Making with Dynamic Networks", 
    "publish": "2012-06-27T15:39:01Z", 
    "summary": "The effect of inaccuracies in the parameters of a dynamic Bayesian network\ncan be investigated by subjecting the network to a sensitivity analysis. Having\ndetailed the resulting sensitivity functions in our previous work, we now study\nthe effect of parameter inaccuracies on a recommended decision in view of a\nthreshold decision-making model. We detail the effect of varying a single and\nmultiple parameters from a conditional probability table and present a\ncomputational procedure for establishing bounds between which assessments for\nthese parameters can be varied without inducing a change in the recommended\ndecision. We illustrate the various concepts involved by means of a real-life\ndynamic network in the field of infectious disease.", 
    "link": "http://arxiv.org/pdf/1206.6818v1", 
    "arxiv-id": "1206.6818v1"
},{
    "category": "q-bio.TO", 
    "author": "Eric Werner", 
    "title": "The Origin, Evolution and Development of Bilateral Symmetry in   Multicellular Organisms", 
    "publish": "2012-07-13T16:06:35Z", 
    "summary": "A computational theory and model of the ontogeny and development of bilateral\nsymmetry in multicellular organisms is presented. Understanding the origin and\nevolution of bilateral organisms requires an understanding of how bilateral\nsymmetry develops, starting from a single cell. Bilateral symmetric growth of a\nmulticellular organism from a single starter cell is explained as resulting\nfrom the opposite handedness and orientation along one axis in two daughter\nfounder cells that are in equivalent developmental control network states.\nSeveral methods of establishing the initial orientation of the daughter cells\n(including oriented cell division and cell signaling) are discussed. The\norientation states of the daughter cells are epigenetically inherited by their\nprogeny. This results in mirror development with the two founding daughter\ncells generating complementary mirror image multicellular morphologies. The end\nproduct is a bilateral symmetric organism. The theory gives a unified\nexplanation of diverse phenomena including symmetry breaking, situs inversus,\ngynandromorphs, inside-out growth, bilaterally symmetric cancers, and the\nrapid, punctuated evolution of bilaterally symmetric organisms in the Cambrian\nExplosion. The theory is supported by experimental results on early embryonic\ndevelopment. The theory makes precise testable predications.", 
    "link": "http://arxiv.org/pdf/1207.3289v1", 
    "arxiv-id": "1207.3289v1"
},{
    "category": "cs.CE", 
    "author": "Ri Suk Yun", 
    "title": "Optimal Selection of Assets Investing Composition Plan based on Grey   Multi Objective Programming method", 
    "publish": "2012-07-15T01:30:56Z", 
    "summary": "The problem for selection of appropriate assets investing composition\nprojects such as assets rationalization plays an important role in promotion of\nbusiness systems. We consider the assets investing composition plan problems\nsubject to grey multiobjective programming with the grey inequality\nconstraints. In this paper, we show in detail the entire process of the\napplication from modeling the case problem to generating its solution. To solve\nthe grey multi objective programming problem, we then develop and apply an\nalgorithm of grey multiple objective programming by weighting method and an\nalgorithm of grey multiple objective programming based on q -positioned\nprogramming method. These algorithms all regard as of great importance\nuncertainty (greyness) at grey multiobjective programming and simple and easy\nthe calculating process. The calculating examples of paper also show ability\nand effectiveness of algorithms.", 
    "link": "http://arxiv.org/pdf/1207.3472v1", 
    "arxiv-id": "1207.3472v1"
},{
    "category": "cs.CE", 
    "author": "Oswaldo D. Miranda", 
    "title": "Programing Using High Level Design With Python and FORTRAN: A Study Case   in Astrophysics", 
    "publish": "2012-07-16T12:50:30Z", 
    "summary": "In this work, we present a short review about the high level design\nmethodology (HLDM), that is based on the use of very high level (VHL)\nprograming language as main, and the use of the intermediate level (IL)\nlanguage only for the critical processing time. The languages used are Python\n(VHL) and FORTRAN (IL). Moreover, this methodology, making use of the oriented\nobject programing (OOP), permits to produce a readable, portable and reusable\ncode. Also is presented the concept of computational framework, that naturally\nappears from the OOP paradigm. As an example, we present the framework called\nPYGRAWC (Python framework for Gravitational Waves from Cosmological origin).\nEven more, we show that the use of HLDM with Python and FORTRAN produces a\npowerful tool for solving astrophysical problems.", 
    "link": "http://arxiv.org/pdf/1207.3658v1", 
    "arxiv-id": "1207.3658v1"
},{
    "category": "cs.AI", 
    "author": "Z. S. Hoare", 
    "title": "Pre-Selection of Independent Binary Features: An Application to   Diagnosing Scrapie in Sheep", 
    "publish": "2012-07-11T14:54:09Z", 
    "summary": "Suppose that the only available information in a multi-class problem are\nexpert estimates of the conditional probabilities of occurrence for a set of\nbinary features. The aim is to select a subset of features to be measured in\nsubsequent data collection experiments. In the lack of any information about\nthe dependencies between the features, we assume that all features are\nconditionally independent and hence choose the Naive Bayes classifier as the\noptimal classifier for the problem. Even in this (seemingly trivial) case of\ncomplete knowledge of the distributions, choosing an optimal feature subset is\nnot straightforward. We discuss the properties and implementation details of\nSequential Forward Selection (SFS) as a feature selection procedure for the\ncurrent problem. A sensitivity analysis was carried out to investigate whether\nthe same features are selected when the probabilities vary around the estimated\nvalues. The procedure is illustrated with a set of probability estimates for\nScrapie in sheep.", 
    "link": "http://arxiv.org/pdf/1207.4141v1", 
    "arxiv-id": "1207.4141v1"
},{
    "category": "stat.AP", 
    "author": "Stefan Luther", 
    "title": "Modeling Waveform Shapes with Random Eects Segmental Hidden Markov   Models", 
    "publish": "2012-07-11T14:54:41Z", 
    "summary": "In this paper we describe a general probabilistic framework for modeling\nwaveforms such as heartbeats from ECG data. The model is based on segmental\nhidden Markov models (as used in speech recognition) with the addition of\nrandom effects to the generative model. The random effects component of the\nmodel handles shape variability across different waveforms within a general\nclass of waveforms of similar shape. We show that this probabilistic model\nprovides a unified framework for learning these models from sets of waveform\ndata as well as parsing, classification, and prediction of new waveforms. We\nderive a computationally efficient EM algorithm to fit the model on multiple\nwaveforms, and introduce a scoring method that evaluates a test waveform based\non its shape. Results on two real-world data sets demonstrate that the random\neffects methodology leads to improved accuracy (compared to alternative\napproaches) on classification and segmentation of real-world waveforms.", 
    "link": "http://arxiv.org/pdf/1207.4143v1", 
    "arxiv-id": "1207.4143v1"
},{
    "category": "cs.CE", 
    "author": "Mingyan Liu", 
    "title": "Profit Incentive In A Secondary Spectrum Market: A Contract Design   Approach", 
    "publish": "2012-07-27T04:25:02Z", 
    "summary": "In this paper we formulate a contract design problem where a primary license\nholder wishes to profit from its excess spectrum capacity by selling it to\npotential secondary users/buyers. It needs to determine how to optimally price\nthe excess spectrum so as to maximize its profit, knowing that this excess\ncapacity is stochastic in nature, does not come with exclusive access, and\ncannot provide deterministic service guarantees to a buyer. At the same time,\nbuyers are of different {\\em types}, characterized by different communication\nneeds, tolerance for the channel uncertainty, and so on, all of which a buyer's\nprivate information. The license holder must then try to design different\ncontracts catered to different types of buyers in order to maximize its profit.\nWe address this problem by adopting as a reference a traditional spectrum\nmarket where the buyer can purchase exclusive access with fixed/deterministic\nguarantees. We fully characterize the optimal solution in the cases where there\nis a single buyer type, and when multiple types of buyers share the same, known\nchannel condition as a result of the primary user activity. In the most general\ncase we construct an algorithm that generates a set of contracts in a\ncomputationally efficient manner, and show that this set is optimal when the\nbuyer types satisfy a monotonicity condition.", 
    "link": "http://arxiv.org/pdf/1207.6445v1", 
    "arxiv-id": "1207.6445v1"
},{
    "category": "cs.CE", 
    "author": "Femke van Raamsdonk", 
    "title": "Proceedings 7th International Workshop on Developments of Computational   Methods", 
    "publish": "2012-07-30T02:01:37Z", 
    "summary": "This volume contains the proceedings of the 7th International Workshop on\nDevelopments in Computational Models (DCM 2011) which was held on Sunday July\n3, 2011, in Zurich, Switzerland, as a satelite workshop of ICALP 2011.\n  Recently several new models of computation have emerged, for instance for\nbio-computing and quantum-computing, and in addition traditional models of\ncomputation have been adapted to accommodate new demands or capabilities of\ncomputer systems. The aim of DCM is to bring together researchers who are\ncurrently developing new computational models or new features for traditional\ncomputational models, in order to foster their interaction, to provide a forum\nfor presenting new ideas and work in progress, and to enable newcomers to learn\nabout current activities in this area.", 
    "link": "http://arxiv.org/pdf/1207.6821v1", 
    "arxiv-id": "1207.6821v1"
},{
    "category": "cs.CE", 
    "author": "Angelo Troina", 
    "title": "A Calculus of Looping Sequences with Local Rules", 
    "publish": "2012-07-31T02:06:26Z", 
    "summary": "In this paper we present a variant of the Calculus of Looping Sequences (CLS\nfor short) with global and local rewrite rules. While global rules, as in CLS,\nare applied anywhere in a given term, local rules can only be applied in the\ncompartment on which they are defined. Local rules are dynamic: they can be\nadded, moved and erased. We enrich the new calculus with a parallel semantics\nwhere a reduction step is lead by any number of global and local rules that\ncould be performed in parallel. A type system is developed to enforce the\nproperty that a compartment must contain only local rules with specific\nfeatures. As a running example we model some interactions happening in a cell\nstarting from its nucleus and moving towards its mitochondria.", 
    "link": "http://arxiv.org/pdf/1207.7147v1", 
    "arxiv-id": "1207.7147v1"
},{
    "category": "cs.CE", 
    "author": "V. P. Singh", 
    "title": "Adaptive Bee Colony in an Artificial Bee Colony for Solving Engineering   Design Problems", 
    "publish": "2012-10-22T15:30:04Z", 
    "summary": "A wide range of engineering design problems have been solved by the\nalgorithms that simulates collective intelligence in swarms of birds or\ninsects. The Artificial Bee Colony or ABC is one of the recent additions to the\nclass of swarm intelligence based algorithms that mimics the foraging behavior\nof honey bees. ABC consists of three groups of bees namely employed, onlooker\nand scout bees. In ABC, the food locations represent the potential candidate\nsolution. In the present study an attempt is made to generate the population of\nfood sources (Colony Size) adaptively and the variant is named as A-ABC. A-ABC\nis further enhanced to improve convergence speed and exploitation capability,\nby employing the concept of elitism, which guides the bees towards the best\nfood source. This enhanced variant is called E-ABC. The proposed algorithms are\nvalidated on a set of standard benchmark problems with varying dimensions taken\nfrom literature and on five engineering design problems. The numerical results\nare compared with the basic ABC and three recent variant of ABC. Numerically\nand statistically simulated results illustrate that the proposed method is very\nefficient and competitive.", 
    "link": "http://arxiv.org/pdf/1211.0957v1", 
    "arxiv-id": "1211.0957v1"
},{
    "category": "cs.CE", 
    "author": "Suixiang Gao", 
    "title": "Explosion prediction of oil gas using SVM and Logistic Regression", 
    "publish": "2012-11-07T12:47:57Z", 
    "summary": "The prevention of dangerous chemical accidents is a primary problem of\nindustrial manufacturing. In the accidents of dangerous chemicals, the oil gas\nexplosion plays an important role. The essential task of the explosion\nprevention is to estimate the better explosion limit of a given oil gas. In\nthis paper, Support Vector Machines (SVM) and Logistic Regression (LR) are used\nto predict the explosion of oil gas. LR can get the explicit probability\nformula of explosion, and the explosive range of the concentrations of oil gas\naccording to the concentration of oxygen. Meanwhile, SVM gives higher accuracy\nof prediction. Furthermore, considering the practical requirements, the effects\nof penalty parameter on the distribution of two types of errors are discussed.", 
    "link": "http://arxiv.org/pdf/1211.1526v2", 
    "arxiv-id": "1211.1526v2"
},{
    "category": "cond-mat.mtrl-sci", 
    "author": "Ru Zhu", 
    "title": "Low-dimensionality energy landscapes: Magnetic switching mechanisms and   rates", 
    "publish": "2012-11-07T16:56:58Z", 
    "summary": "In this paper we propose a new method for the study and visualization of\ndynamic processes in magnetic nanostructures, and for the accurate calculation\nof rates for such processes. The method is illustrated for the case of\nswitching of a grain of an exchange-coupled recording medium, which switches\nthrough domain wall nucleation and motion, but is generalizable to other rate\nprocesses such as vortex formation and annihilation. The method involves\ncalculating the most probable (lowest energy) switching path and projecting the\nmotion onto that path. The motion is conveniently visualized in a\ntwo-dimensional (2D) projection parameterized by the dipole and quadrupole\nmoment of the grain. The motion along that path can then be described by a\nLangevin equation, and its rate can be computed by the classic method of\nKramers. The rate can be evaluated numerically, or in an analytic approximation\n- interestingly, the analytic result for domain-wall switching is very similar\nto that obtained by Brown in 1963 for coherent switching, except for a factor\nproportional to the domain-wall volume. Thus in addition to its lower\ncoercivity, an exchange-coupled medium has the additional advantage (over a\nuniform medium) of greater thermal stability, for a fixed energy barrier.", 
    "link": "http://arxiv.org/pdf/1211.1599v1", 
    "arxiv-id": "1211.1599v1"
},{
    "category": "cs.CE", 
    "author": "Utpal Dholakia", 
    "title": "Does a Daily Deal Promotion Signal a Distressed Business? An Empirical   Investigation of Small Business Survival", 
    "publish": "2012-11-07T21:27:16Z", 
    "summary": "In the last four years, daily deals have emerged from nowhere to become a\nmulti-billion dollar industry world-wide. Daily deal sites such as Groupon and\nLivingsocial offer products and services at deep discounts to consumers via\nemail and social networks. As the industry matures, there are many questions\nregarding the impact of daily deals on the marketplace. Important questions in\nthis regard concern the reasons why businesses decide to offer daily deals and\ntheir longer-term impact on businesses. In the present paper, we investigate\nwhether the unobserved factors that make marketers run daily deals are\ncorrelated with the unobserved factors that influence the business, In\nparticular, we employ the framework of seemingly unrelated regression to model\nthe correlation between the errors in predicting whether a business uses a\ndaily deal and the errors in predicting the business' survival. Our analysis\nconsists of the survival of 985 small businesses that offered daily deals\nbetween January and July 2011 in the city of Chicago. Our results indicate that\nthere is a statistically significant correlation between the unobserved factors\nthat influence the business' decision to offer a daily deal and the unobserved\nfactors that impact its survival. Furthermore, our results indicate that the\ncorrelation coefficient is significant in certain business categories (e.g.\nrestaurants).", 
    "link": "http://arxiv.org/pdf/1211.1694v1", 
    "arxiv-id": "1211.1694v1"
},{
    "category": "cs.CE", 
    "author": "Akhilesh Mishra", 
    "title": "A Novel Anticlustering Filtering Algorithm for the Prediction of Genes   as a Drug Target", 
    "publish": "2012-11-09T17:41:16Z", 
    "summary": "The high-throughput data generated by microarray experiments provides\ncomplete set of genes being expressed in a given cell or in an organism under\nparticular conditions. The analysis of these enormous data has opened a new\ndimension for the researchers. In this paper we describe a novel algorithm to\nmicroarray data analysis focusing on the identification of genes that are\ndifferentially expressed in particular internal or external conditions and\nwhich could be potential drug targets. The algorithm uses the time-series gene\nexpression data as an input and recognizes genes which are expressed\ndifferentially. This algorithm implements standard statistics-based gene\nfunctional investigations, such as the log transformation, mean, log-sigmoid\nfunction, coefficient of variations, etc. It does not use clustering analysis.\nThe proposed algorithm has been implemented in Perl. The time-series gene\nexpression data on yeast Saccharomyces cerevisiae from the Stanford Microarray\nDatabase (SMD)consisting of 6154 genes have been taken for the validation of\nthe algorithm. The developed method extracted 48 genes out of total 6154 genes.\nThese genes are mostly responsible for the yeast's resistants at a high\ntemperature.", 
    "link": "http://arxiv.org/pdf/1211.2194v1", 
    "arxiv-id": "1211.2194v1"
},{
    "category": "cs.DC", 
    "author": "Peter V. Coveney", 
    "title": "Flexible composition and execution of high performance, high fidelity   multiscale biomedical simulations", 
    "publish": "2012-11-13T12:11:09Z", 
    "summary": "Multiscale simulations are essential in the biomedical domain to accurately\nmodel human physiology. We present a modular approach for designing,\nconstructing and executing multiscale simulations on a wide range of resources,\nfrom desktops to petascale supercomputers, including combinations of these. Our\nwork features two multiscale applications, in-stent restenosis and\ncerebrovascular bloodflow, which combine multiple existing single-scale\napplications to create a multiscale simulation. These applications can be\nefficiently coupled, deployed and executed on computers up to the largest\n(peta) scale, incurring a coupling overhead of 1 to 10% of the total execution\ntime.", 
    "link": "http://arxiv.org/pdf/1211.2963v2", 
    "arxiv-id": "1211.2963v2"
},{
    "category": "cs.CE", 
    "author": "Ilaria Sambarino", 
    "title": "Implementing the Stochastics Brane Calculus in a Generic Stochastic   Abstract Machine", 
    "publish": "2012-11-17T09:15:07Z", 
    "summary": "In this paper, we deal with the problem of implementing an abstract machine\nfor a stochastic version of the Brane Calculus. Instead of defining an ad hoc\nabstract machine, we consider the generic stochastic abstract machine\nintroduced by Lakin, Paulev\\'e and Phillips. The nested structure of membranes\nis flattened into a set of species where the hierarchical structure is\nrepresented by means of names. In order to reduce the overhead introduced by\nthis encoding, we modify the machine by adding a copy-on-write optimization\nstrategy. We prove that this implementation is adequate with respect to the\nstochastic structural operational semantics recently given for the Brane\nCalculus. These techniques can be ported also to other stochastic calculi\ndealing with nested structures.", 
    "link": "http://arxiv.org/pdf/1211.4094v1", 
    "arxiv-id": "1211.4094v1"
},{
    "category": "cs.CE", 
    "author": "P. M. A. Sloot", 
    "title": "Modeling Earthen Dike Stability: Sensitivity Analysis and Automatic   Calibration of Diffusivities Based on Live Sensor Data", 
    "publish": "2012-11-18T13:05:54Z", 
    "summary": "The paper describes concept and implementation details of integrating a\nfinite element module for dike stability analysis Virtual Dike into an early\nwarning system for flood protection. The module operates in real-time mode and\nincludes fluid and structural sub-models for simulation of porous flow through\nthe dike and for dike stability analysis. Real-time measurements obtained from\npore pressure sensors are fed into the simulation module, to be compared with\nsimulated pore pressure dynamics. Implementation of the module has been\nperformed for a real-world test case - an earthen levee protecting a sea-port\nin Groningen, the Netherlands. Sensitivity analysis and calibration of\ndiffusivities have been performed for tidal fluctuations. An algorithm for\nautomatic diffusivities calibration for a heterogeneous dike is proposed and\nstudied. Analytical solutions describing tidal propagation in one-dimensional\nsaturated aquifer are employed in the algorithm to generate initial estimates\nof diffusivities.", 
    "link": "http://arxiv.org/pdf/1211.4218v2", 
    "arxiv-id": "1211.4218v2"
},{
    "category": "cs.CE", 
    "author": "P. M. A. Sloot", 
    "title": "Free-surface flow simulations for discharge-based operation of hydraulic   structure gates", 
    "publish": "2012-11-19T15:32:55Z", 
    "summary": "We combine non-hydrostatic flow simulations of the free surface with a\ndischarge model based on elementary gate flow equations for decision support in\noperation of hydraulic structure gates. A water level-based gate control used\nin most of today's general practice does not take into account the fact that\ngate operation scenarios producing similar total discharged volumes and similar\nwater levels may have different local flow characteristics. Accurate and timely\nprediction of local flow conditions around hydraulic gates is important for\nseveral aspects of structure management: ecology, scour, flow-induced gate\nvibrations and waterway navigation. The modelling approach is described and\ntested for a multi-gate sluice structure regulating discharge from a river to\nthe sea. The number of opened gates is varied and the discharge is stabilized\nwith automated control by varying gate openings. The free-surface model was\nvalidated for discharge showing a correlation coefficient of 0.994 compared to\nexperimental data. Additionally, we show the analysis of CFD results for\nevaluating bed stability and gate vibrations.", 
    "link": "http://arxiv.org/pdf/1211.4464v1", 
    "arxiv-id": "1211.4464v1"
},{
    "category": "cs.CE", 
    "author": "Ashish V. Tendulkar", 
    "title": "Accurate Demarcation of Protein Domain Linkers based on Structural   Analysis of Linker Probable Region", 
    "publish": "2012-11-23T14:53:54Z", 
    "summary": "In multi-domain proteins, the domains are connected by a flexible\nunstructured region called as protein domain linker. The accurate demarcation\nof these linkers holds a key to understanding of their biochemical and\nevolutionary attributes. This knowledge helps in designing a suitable linker\nfor engineering stable multi-domain chimeric proteins. Here we propose a novel\nmethod for the demarcation of the linker based on a three-dimensional protein\nstructure and a domain definition. The proposed method is based on biological\nknowledge about structural flexibility of the linkers. We performed structural\nanalysis on a linker probable region (LPR) around domain boundary points of\nknown SCOP domains. The LPR was described using a set of overlapping peptide\nfragments of fixed size. Each peptide fragment was then described by geometric\ninvariants (GIs) and subjected to clustering process where the fragments\ncorresponding to actual linker come up as outliers. We then discover the actual\nlinkers by finding the longest continuous stretch of outlier fragments from\nLPRs. This method was evaluated on a benchmark dataset of 51 continuous\nmulti-domain proteins, where it achieves F1 score of 0.745 (0.83 precision and\n0.66 recall). When the method was applied on 725 continuous multi-domain\nproteins, it was able to identify novel linkers that were not reported\npreviously. This method can be used in combination with supervised / sequence\nbased linker prediction methods for accurate linker demarcation.", 
    "link": "http://arxiv.org/pdf/1211.5520v1", 
    "arxiv-id": "1211.5520v1"
},{
    "category": "cs.CE", 
    "author": "Hon Wai Leong", 
    "title": "A survey of computational methods for protein complex prediction from   protein interaction networks", 
    "publish": "2012-11-24T00:30:33Z", 
    "summary": "Complexes of physically interacting proteins are one of the fundamental\nfunctional units responsible for driving key biological mechanisms within the\ncell. Their identification is therefore necessary not only to understand\ncomplex formation but also the higher level organization of the cell. With the\nadvent of high-throughput techniques in molecular biology, significant amount\nof physical interaction data has been cataloged from organisms such as yeast,\nwhich has in turn fueled computational approaches to systematically mine\ncomplexes from the network of physical interactions among proteins (PPI\nnetwork). In this survey, we review, classify and evaluate some of the key\ncomputational methods developed till date for the identification of protein\ncomplexes from PPI networks. We present two insightful taxonomies that reflect\nhow these methods have evolved over the years towards improving automated\ncomplex prediction. We also discuss some open challenges facing accurate\nreconstruction of complexes, the crucial ones being presence of high proportion\nof errors and noise in current high-throughput datasets and some key aspects\noverlooked by current complex detection methods. We hope this review will not\nonly help to condense the history of computational complex detection for easy\nreference, but also provide valuable insights to drive further research in this\narea.", 
    "link": "http://arxiv.org/pdf/1211.5625v1", 
    "arxiv-id": "1211.5625v1"
},{
    "category": "math.NA", 
    "author": "Alexander Basyrov", 
    "title": "Error Bounds on Derivatives during Simulations", 
    "publish": "2012-12-03T04:02:59Z", 
    "summary": "The methods commonly used for numerical differentiation, such as the\n\"center-difference formula\" and \"four-points formula\" are unusable in\nsimulations or real-time data analysis because they require knowledge of the\nfuture. In Bard'11, an algorithm was shown that generates formulas that require\nknowledge only of the past and present values of $f(t)$ to estimate $f'(t)$.\nFurthermore, the algorithm can handle irregularly spaced data and higher-order\nderivatives. That work did not include a rigorous proof of correctness nor the\nerror bounds. In this paper, the correctness and error bounds of that algorithm\nare proven, explicit forms are given for the coefficients, and several\ninteresting corollaries are proven.", 
    "link": "http://arxiv.org/pdf/1212.0280v1", 
    "arxiv-id": "1212.0280v1"
},{
    "category": "cs.CE", 
    "author": "Ruben Specogna", 
    "title": "Physics inspired algorithms for (co)homology computation", 
    "publish": "2012-12-06T16:03:10Z", 
    "summary": "The issue of computing (co)homology generators of a cell complex is gaining a\npivotal role in various branches of science. While this issue can be rigorously\nsolved in polynomial time, it is still overly demanding for large scale\nproblems. Drawing inspiration from low-frequency electrodynamics, this paper\npresents a physics inspired algorithm for first cohomology group computations\non three-dimensional complexes. The algorithm is general and exhibits orders of\nmagnitude speed up with respect to competing ones, allowing to handle problems\nnot addressable before. In particular, when generators are employed in the\nphysical modeling of magneto-quasistatic problems, this algorithm solves one of\nthe most long-lasting problems in low-frequency computational electromagnetics.\nIn this case, the effectiveness of the algorithm and its ease of implementation\nmay be even improved by introducing the novel concept of \\textit{lazy\ncohomology generators}.", 
    "link": "http://arxiv.org/pdf/1212.1360v1", 
    "arxiv-id": "1212.1360v1"
},{
    "category": "cs.CE", 
    "author": "Lihua Wen", 
    "title": "Efficiency improvement of the frequency-domain BEM for rapid transient   elastodynamic analysis", 
    "publish": "2012-12-13T01:39:08Z", 
    "summary": "The frequency-domain fast boundary element method (BEM) combined with the\nexponential window technique leads to an efficient yet simple method for\nelastodynamic analysis. In this paper, the efficiency of this method is further\nenhanced by three strategies. Firstly, we propose to use exponential window\nwith large damping parameter to improve the conditioning of the BEM matrices.\nSecondly, the frequency domain windowing technique is introduced to alleviate\nthe severe Gibbs oscillations in time-domain responses caused by large damping\nparameters. Thirdly, a solution extrapolation scheme is applied to obtain\nbetter initial guesses for solving the sequential linear systems in the\nfrequency domain. Numerical results of three typical examples with the problem\nsize up to 0.7 million unknowns clearly show that the first and third\nstrategies can significantly reduce the computational time. The second strategy\ncan effectively eliminate the Gibbs oscillations and result in accurate\ntime-domain responses.", 
    "link": "http://arxiv.org/pdf/1212.3032v2", 
    "arxiv-id": "1212.3032v2"
},{
    "category": "cs.CE", 
    "author": "Paula Couto", 
    "title": "A Neural Network Approach to ECG Denoising", 
    "publish": "2012-12-20T20:11:30Z", 
    "summary": "We propose an ECG denoising method based on a feed forward neural network\nwith three hidden layers. Particulary useful for very noisy signals, this\napproach uses the available ECG channels to reconstruct a noisy channel. We\ntested the method, on all the records from Physionet MIT-BIH Arrhythmia\nDatabase, adding electrode motion artifact noise. This denoising method\nimproved the perfomance of publicly available ECG analysis programs on noisy\nECG signals. This is an offline method that can be used to remove noise from\nvery corrupted Holter records.", 
    "link": "http://arxiv.org/pdf/1212.5217v1", 
    "arxiv-id": "1212.5217v1"
},{
    "category": "cs.CE", 
    "author": "Robert Celaire", 
    "title": "Bringing simulation to implementation: Presentation of a global approach   in the design of passive solar buildings under humid tropical climates", 
    "publish": "2012-12-18T07:35:35Z", 
    "summary": "In early 1995, a DSM pilot initiative has been launched in the French islands\nof Guadeloupe and Reunion through a partnership between several public and\nprivate partners (the French Public Utility EDF, the University of Reunion\nIsland, low cost housing companies, architects, energy consultants, etc...) to\nset up standards to improve thermal design of new residential buildings in\ntropical climates. This partnership led to defining optimized bio-climatic\nurban planning and architectural designs featuring the use of passive cooling\narchitectural principles (solar shading, natural ventilation) and components,\nas well as energy efficient systems and technologies. The design and sizing of\neach architectural component on internal thermal comfort in building has been\nassessed with a validated thermal and airflow building simulation software\n(CODYRUN). These technical specifications have been edited in a reference\ndocument which has been used to build over 300 new pilot dwellings through the\nyears 1996-1998 in Reunion Island and in Guadeloupe. An experimental monitoring\nhas been made in these first ECODOM dwellings in 1998 and 1999. It will result\nin experimental validation of impact of the passive cooling strategies on\nthermal comfort of occupants leading to modify specifications if necessary. The\npaper present all the methodology used for the elaboration of ECODOM, from the\nsimulations to the experimental results. This follow up is important, as the\nsetting up of the ECODOM standard will be the first step towards the setting up\nof thermal regulations in the French overseas territories, by the year 2002.", 
    "link": "http://arxiv.org/pdf/1212.5252v2", 
    "arxiv-id": "1212.5252v2"
},{
    "category": "cs.LG", 
    "author": "H. Hannah Inbarani", 
    "title": "Fuzzy soft rough K-Means clustering approach for gene expression data", 
    "publish": "2012-12-21T08:43:05Z", 
    "summary": "Clustering is one of the widely used data mining techniques for medical\ndiagnosis. Clustering can be considered as the most important unsupervised\nlearning technique. Most of the clustering methods group data based on distance\nand few methods cluster data based on similarity. The clustering algorithms\nclassify gene expression data into clusters and the functionally related genes\nare grouped together in an efficient manner. The groupings are constructed such\nthat the degree of relationship is strong among members of the same cluster and\nweak among members of different clusters. In this work, we focus on a\nsimilarity relationship among genes with similar expression patterns so that a\nconsequential and simple analytical decision can be made from the proposed\nFuzzy Soft Rough K-Means algorithm. The algorithm is developed based on Fuzzy\nSoft sets and Rough sets. Comparative analysis of the proposed work is made\nwith bench mark algorithms like K-Means and Rough K-Means and efficiency of the\nproposed algorithm is illustrated in this work by using various cluster\nvalidity measures such as DB index and Xie-Beni index.", 
    "link": "http://arxiv.org/pdf/1212.5359v1", 
    "arxiv-id": "1212.5359v1"
},{
    "category": "cs.LG", 
    "author": "H. Hannah Inbarani", 
    "title": "Soft Set Based Feature Selection Approach for Lung Cancer Images", 
    "publish": "2012-12-21T10:46:24Z", 
    "summary": "Lung cancer is the deadliest type of cancer for both men and women. Feature\nselection plays a vital role in cancer classification. This paper investigates\nthe feature selection process in Computed Tomographic (CT) lung cancer images\nusing soft set theory. We propose a new soft set based unsupervised feature\nselection algorithm. Nineteen features are extracted from the segmented lung\nimages using gray level co-occurence matrix (GLCM) and gray level different\nmatrix (GLDM). In this paper, an efficient Unsupervised Soft Set based Quick\nReduct (SSUSQR) algorithm is presented. This method is used to select features\nfrom the data set and compared with existing rough set based unsupervised\nfeature selection methods. Then K-Means and Self Organizing Map (SOM)\nclustering algorithms are used to cluster the data. The performance of the\nfeature selection algorithms is evaluated based on performance of clustering\ntechniques. The results show that the proposed method effectively removes\nredundant features.", 
    "link": "http://arxiv.org/pdf/1212.5391v1", 
    "arxiv-id": "1212.5391v1"
},{
    "category": "cs.NE", 
    "author": "Harry Boyer", 
    "title": "Black box modelling of HVAC system : improving the performances of   neural networks", 
    "publish": "2012-12-21T13:53:53Z", 
    "summary": "This paper deals with neural networks modelling of HVAC systems. In order to\nincrease the neural networks performances, a method based on sensitivity\nanalysis is applied. The same technique is also used to compute the relevance\nof each input. To avoid the prediction errors in dry coil conditions, a\nmetamodel for each capacity is derived from the neural networks. The regression\ncoefficients of the polynomial forms are identified through the use of spectral\nanalysis. These methods based on sensitivity and spectral analysis lead to an\noptimized neural network model, as regard to its architecture and predictions.", 
    "link": "http://arxiv.org/pdf/1212.5594v1", 
    "arxiv-id": "1212.5594v1"
},{
    "category": "cs.LG", 
    "author": "Ghazi Belmufti", 
    "title": "Transfer Learning Using Logistic Regression in Credit Scoring", 
    "publish": "2012-12-26T12:03:26Z", 
    "summary": "The credit scoring risk management is a fast growing field due to consumer's\ncredit requests. Credit requests, of new and existing customers, are often\nevaluated by classical discrimination rules based on customers information.\nHowever, these kinds of strategies have serious limits and don't take into\naccount the characteristics difference between current customers and the future\nones. The aim of this paper is to measure credit worthiness for non customers\nborrowers and to model potential risk given a heterogeneous population formed\nby borrowers customers of the bank and others who are not. We hold on previous\nworks done in generalized gaussian discrimination and transpose them into the\nlogistic model to bring out efficient discrimination rules for non customers'\nsubpopulation.\n  Therefore we obtain several simple models of connection between parameters of\nboth logistic models associated respectively to the two subpopulations. The\nGerman credit data set is selected to experiment and to compare these models.\nExperimental results show that the use of links between the two subpopulations\nimprove the classification accuracy for the new loan applicants.", 
    "link": "http://arxiv.org/pdf/1212.6167v1", 
    "arxiv-id": "1212.6167v1"
},{
    "category": "cs.AI", 
    "author": "Doreswamy", 
    "title": "Knowledge Discovery System For Fiber Reinforced Polymer Matrix Composite   Laminate", 
    "publish": "2013-01-02T06:47:45Z", 
    "summary": "In this paper Knowledge Discovery System (KDS) is proposed and implemented\nfor the extraction of knowledge-mean stiffness of a polymer composite material\nin which when fibers are placed at different orientations. Cosine amplitude\nmethod is implemented for retrieving compatible polymer matrix and\nreinforcement fiber which is coming under predicted fiber class, from the\npolymer and reinforcement database respectively, based on the design\nrequirements. Fuzzy classification rules to classify fibers into short, medium\nand long fiber classes are derived based on the fiber length and the computed\nor derive critical length of fiber. Longitudinal and Transverse module of\nPolymer Matrix Composite consisting of seven layers with different fiber volume\nfractions and different fibers orientations at 0,15,30,45,60,75 and 90 degrees\nare analyzed through Rule-of Mixture material design model. The analysis\nresults are represented in different graphical steps and have been measured\nwith statistical parameters. This data mining application implemented here has\nfocused the mechanical problems of material design and analysis. Therefore,\nthis system is an expert decision support system for optimizing the materials\nperformance for designing light-weight and strong, and cost effective polymer\ncomposite materials.", 
    "link": "http://arxiv.org/pdf/1301.0173v1", 
    "arxiv-id": "1301.0173v1"
},{
    "category": "cs.AI", 
    "author": "M. N. Vanajakshi", 
    "title": "Similarity Measuring Approuch for Engineering Materials Selection", 
    "publish": "2013-01-02T07:07:20Z", 
    "summary": "Advanced engineering materials design involves the exploration of massive\nmultidimensional feature spaces, the correlation of materials properties and\nthe processing parameters derived from disparate sources. The search for\nalternative materials or processing property strategies, whether through\nanalytical, experimental or simulation approaches, has been a slow and arduous\ntask, punctuated by infrequent and often expected discoveries. A few systematic\nefforts have been made to analyze the trends in data as a basis for\nclassifications and predictions. This is particularly due to the lack of large\namounts of organized data and more importantly the challenging of shifting\nthrough them in a timely and efficient manner. The application of recent\nadvances in Data Mining on materials informatics is the state of art of\ncomputational and experimental approaches for materials discovery. In this\npaper similarity based engineering materials selection model is proposed and\nimplemented to select engineering materials based on the composite materials\nconstraints. The result reviewed from this model is sustainable for effective\ndecision making in advanced engineering materials design applications.", 
    "link": "http://arxiv.org/pdf/1301.0176v1", 
    "arxiv-id": "1301.0176v1"
},{
    "category": "cs.CE", 
    "author": "Hon Wai Leong", 
    "title": "Employing functional interactions for characterization and detection of   sparse complexes from yeast PPI networks", 
    "publish": "2013-01-03T02:01:10Z", 
    "summary": "Over the last few years, several computational techniques have been devised\nto recover protein complexes from the protein interaction (PPI) networks of\norganisms. These techniques model \"dense\" subnetworks within PPI networks as\ncomplexes. However, our comprehensive evaluations revealed that these\ntechniques fail to reconstruct many 'gold standard' complexes that are \"sparse\"\nin the networks (only 71 recovered out of 123 known yeast complexes embedded in\na network of 9704 interactions among 1622 proteins). In this work, we propose a\nnovel index called Component-Edge (CE) score to quantitatively measure the\nnotion of \"complex derivability\" from PPI networks. Using this index, we\ntheoretically categorize complexes as \"sparse\" or \"dense\" with respect to a\ngiven network. We then devise an algorithm SPARC that selectively employs\nfunctional interactions to improve the CE scores of predicted complexes, and\nthereby elevates many of the \"sparse\" complexes to \"dense\". This empowers\nexisting methods to detect these \"sparse\" complexes. We demonstrate that our\napproach is effective in reconstructing significantly many complexes missed\npreviously (104 recovered out of the 123 known complexes or ~47% improvement).", 
    "link": "http://arxiv.org/pdf/1301.0363v1", 
    "arxiv-id": "1301.0363v1"
},{
    "category": "cs.AI", 
    "author": "H. Hannah Inbarani", 
    "title": "Fuzzy Soft Set Based Classification for Gene Expression Data", 
    "publish": "2013-01-08T11:48:49Z", 
    "summary": "Classification is one of the major issues in Data Mining Research fields. The\nclassification problems in medical area often classify medical dataset based on\nthe result of medical diagnosis or description of medical treatment by the\nmedical practitioner. This research work discusses the classification process\nof Gene Expression data for three different cancers which are breast cancer,\nlung cancer and leukemia cancer with two classes which are cancerous stage and\nnon cancerous stage. We have applied a fuzzy soft set similarity based\nclassifier to enhance the accuracy to predict the stages among cancer genes and\nthe informative genes are selected by using Entopy filtering.", 
    "link": "http://arxiv.org/pdf/1301.1502v1", 
    "arxiv-id": "1301.1502v1"
},{
    "category": "cs.CE", 
    "author": "Yasuhiro Nakahara", 
    "title": "Parallel Computing of Discrete Element Method on GPU", 
    "publish": "2013-01-08T23:09:49Z", 
    "summary": "We investigate applicability of GPU to DEM. NVIDIA's code obtained superior\nperformance than CPU in computational time. A model of contact forces in\nNVIDIA's code is too simple for practical use. We modify this model by\nreplacing it with the practical model. The simulation shows that the practical\nmodel obtains the computing speed 6 times faster than the practical one on CPU\nwhile 7 times slower than the simple one on GPU. The result are analyzed.", 
    "link": "http://arxiv.org/pdf/1301.1714v1", 
    "arxiv-id": "1301.1714v1"
},{
    "category": "cs.CE", 
    "author": "Nobuhiro Yoshikawa", 
    "title": "A New Approach for Solving Singular Systems in Topology Optimization   Using Krylov Subspace Methods", 
    "publish": "2013-01-10T23:11:16Z", 
    "summary": "In topology optimization, the design parameter with no contribution to the\nobjective function vanishes. This causes the stiffness matrix to become\nsingular. We show that a local optimal solution is obtained by Conjugate\nResidual Method and Conjugate Gradient Method even if the stiffness matrix\nbecomes singular. We prove that CGMconverges to a local optimal solution in\nthat case. Computer simulation shows that CGM gives the same solutions obtained\nby CRM in case of a cantilever beam problem.", 
    "link": "http://arxiv.org/pdf/1301.2354v1", 
    "arxiv-id": "1301.2354v1"
},{
    "category": "q-bio.TO", 
    "author": "David Basanta", 
    "title": "Intrinsic cell factors that influence tumourigenicity in cancer stem   cells - towards hallmarks of cancer stem cells", 
    "publish": "2013-01-16T22:04:46Z", 
    "summary": "Since the discovery of a cancer initiating side population in solid tumours,\nstudies focussing on the role of so-called cancer stem cells in cancer\ninitiation and progression have abounded. The biological interrogation of these\ncells has yielded volumes of information about their behaviour, but there has,\nas of yet, not been many actionable generalised theoretical conclusions. To\naddress this point, we have created a hybrid, discrete/continuous computational\ncellular automaton model of a generalised stem-cell driven tissue and explored\nthe phenotypic traits inherent in the inciting cell and the resultant tissue\ngrowth. We identify the regions in phenotype parameter space where these\ninitiating cells are able to cause a disruption in homeostasis, leading to\ntissue overgrowth and tumour formation. As our parameters and model are\nnon-specific, they could apply to any tissue cancer stem-cell and do not assume\nspecific genetic mutations. In this way, our model suggests that targeting\nthese phenotypic traits could represent generalizable strategies across cancer\ntypes and represents a first attempt to identify the hallmarks of cancer stem\ncells.", 
    "link": "http://arxiv.org/pdf/1301.3934v3", 
    "arxiv-id": "1301.3934v3"
},{
    "category": "cs.CE", 
    "author": "Kirana Kumara P", 
    "title": "A MATLAB Code for Three Dimensional Linear Elastostatics using Constant   Boundary Elements", 
    "publish": "2013-01-20T16:29:09Z", 
    "summary": "Present work presents a code written in the very simple programming language\nMATLAB, for three dimensional linear elastostatics, using constant boundary\nelements. The code, in full or in part, is not a translation or a copy of any\nof the existing codes. Present paper explains how the code is written, and\nlists all the formulae used. Code is verified by using the code to solve a\nsimple problem which has the well known approximate analytical solution. Of\ncourse, present work does not make any contribution to research on boundary\nelements, in terms of theory. But the work is justified by the fact that, to\nthe best of author's knowledge, as of now, one cannot find an open access\nMATLAB code for three dimensional linear elastostatics using constant boundary\nelements. Author hopes this paper to be of help to beginners who wish to\nunderstand how a simple but complete boundary element code works, so that they\ncan build upon and modify the present open access code to solve complex\nengineering problems quickly and easily. The code is available online for open\naccess (as supplementary file for the present paper), and may be downloaded\nfrom the website for the present journal.", 
    "link": "http://arxiv.org/pdf/1301.4668v1", 
    "arxiv-id": "1301.4668v1"
},{
    "category": "q-bio.GN", 
    "author": "Rick B. Jenison", 
    "title": "Using Periodicity of Nucleotide Sequences", 
    "publish": "2013-01-20T02:02:30Z", 
    "summary": "Withdrawn by arXiv administrators due to content entirely plagiarized from\nother authors (not in arXiv).", 
    "link": "http://arxiv.org/pdf/1301.5273v2", 
    "arxiv-id": "1301.5273v2"
},{
    "category": "q-bio.CB", 
    "author": "Hugues Berry", 
    "title": "Localization of protein aggregation in Escherichia coli is governed by   diffusion and nucleoid macromolecular crowding effect", 
    "publish": "2013-03-08T07:53:49Z", 
    "summary": "Aggregates of misfolded proteins are a hallmark of many age-related diseases.\nRecently, they have been linked to aging of Escherichia coli (E. coli) where\nprotein aggregates accumulate at the old pole region of the aging bacterium.\nBecause of the potential of E. coli as a model organism, elucidating aging and\nprotein aggregation in this bacterium may pave the way to significant advances\nin our global understanding of aging. A first obstacle along this path is to\ndecipher the mechanisms by which protein aggregates are targeted to specific\nintercellular locations. Here, using an integrated approach based on\nindividual-based modeling, time-lapse fluorescence microscopy and automated\nimage analysis, we show that the movement of aging-related protein aggregates\nin E. coli is purely diffusive (Brownian). Using single-particle tracking of\nprotein aggregates in live E. coli cells, we estimated the average size and\ndiffusion constant of the aggregates. Our results evidence that the aggregates\npassively diffuse within the cell, with diffusion constants that depend on\ntheir size in agreement with the Stokes-Einstein law. However, the aggregate\ndisplacements along the cell long axis are confined to a region that roughly\ncorresponds to the nucleoid-free space in the cell pole, thus confirming the\nimportance of increased macromolecular crowding in the nucleoids. We thus used\n3d individual-based modeling to show that these three ingredients (diffusion,\naggregation and diffusion hindrance in the nucleoids) are sufficient and\nnecessary to reproduce the available experimental data on aggregate\nlocalization in the cells. Taken together, our results strongly support the\nhypothesis that the localization of aging-related protein aggregates in the\npoles of E. coli results from the coupling of passive diffusion- aggregation\nwith spatially non-homogeneous macromolecular crowding. They further support\nthe importance of \"soft\" intracellular structuring (based on macromolecular\ncrowding) in diffusion-based protein localization in E. coli.", 
    "link": "http://arxiv.org/pdf/1303.1904v1", 
    "arxiv-id": "1303.1904v1"
},{
    "category": "cs.CE", 
    "author": "Engelbert Mephu Nguifo", 
    "title": "Mining Representative Unsubstituted Graph Patterns Using Prior   Similarity Matrix", 
    "publish": "2013-03-08T16:57:18Z", 
    "summary": "One of the most powerful techniques to study protein structures is to look\nfor recurrent fragments (also called substructures or spatial motifs), then use\nthem as patterns to characterize the proteins under study. An emergent trend\nconsists in parsing proteins three-dimensional (3D) structures into graphs of\namino acids. Hence, the search of recurrent spatial motifs is formulated as a\nprocess of frequent subgraph discovery where each subgraph represents a spatial\nmotif. In this scope, several efficient approaches for frequent subgraph\ndiscovery have been proposed in the literature. However, the set of discovered\nfrequent subgraphs is too large to be efficiently analyzed and explored in any\nfurther process. In this paper, we propose a novel pattern selection approach\nthat shrinks the large number of discovered frequent subgraphs by selecting the\nrepresentative ones. Existing pattern selection approaches do not exploit the\ndomain knowledge. Yet, in our approach we incorporate the evolutionary\ninformation of amino acids defined in the substitution matrices in order to\nselect the representative subgraphs. We show the effectiveness of our approach\non a number of real datasets. The results issued from our experiments show that\nour approach is able to considerably decrease the number of motifs while\nenhancing their interestingness.", 
    "link": "http://arxiv.org/pdf/1303.2054v1", 
    "arxiv-id": "1303.2054v1"
},{
    "category": "cs.CE", 
    "author": "C. Titus Brown", 
    "title": "khmer: Working with Big Data in Bioinformatics", 
    "publish": "2013-03-09T15:34:25Z", 
    "summary": "We introduce design and optimization considerations for the 'khmer' package.", 
    "link": "http://arxiv.org/pdf/1303.2223v1", 
    "arxiv-id": "1303.2223v1"
},{
    "category": "cs.CE", 
    "author": "Manfred Kerber", 
    "title": "The ForMaRE Project - Formal Mathematical Reasoning in Economics", 
    "publish": "2013-03-18T09:34:10Z", 
    "summary": "The ForMaRE project applies formal mathematical reasoning to economics. We\nseek to increase confidence in economics' theoretical results, to aid in\ndiscovering new results, and to foster interest in formal methods, i.e.\ncomputer-aided reasoning, within economics. To formal methods, we seek to\ncontribute user experience feedback from new audiences, as well as new\nchallenge problems. In the first project year, we continued earlier game theory\nstudies but then focused on auctions, where we are building a toolbox of\nformalisations, and have started to study matching and financial risk.\n  In parallel to conducting research that connects economics and formal\nmethods, we organise events and provide infrastructure to connect both\ncommunities, from fostering mutual awareness to targeted matchmaking. These\nefforts extend beyond economics, towards generally enabling domain experts to\nuse mechanised reasoning.", 
    "link": "http://arxiv.org/pdf/1303.4194v3", 
    "arxiv-id": "1303.4194v3"
},{
    "category": "q-bio.PE", 
    "author": "Martin A. Nowak", 
    "title": "TTP: Tool for Tumor Progression", 
    "publish": "2013-03-21T13:17:16Z", 
    "summary": "In this work we present a flexible tool for tumor progression, which\nsimulates the evolutionary dynamics of cancer. Tumor progression implements a\nmulti-type branching process where the key parameters are the fitness\nlandscape, the mutation rate, and the average time of cell division. The\nfitness of a cancer cell depends on the mutations it has accumulated. The input\nto our tool could be any fitness landscape, mutation rate, and cell division\ntime, and the tool produces the growth dynamics and all relevant statistics.", 
    "link": "http://arxiv.org/pdf/1303.5251v1", 
    "arxiv-id": "1303.5251v1"
},{
    "category": "nlin.CG", 
    "author": "Alison J. Gilbert", 
    "title": "City versus wetland: Predicting urban growth in the Vecht area with a   cellular automaton model", 
    "publish": "2013-04-05T03:49:54Z", 
    "summary": "There are many studies dealing with the protection or restoration of wetlands\nand the sustainable economic growth of cities as separate subjects. This study\ninvestigates the conflict between the two in an area where city growth is\nthreatening a protected wetland area. We develop a stochastic cellular\nautomaton model for urban growth and apply it to the Vecht area surrounding the\ncity of Hilversum in the Netherlands, using topographic maps covering the past\n150 years. We investigate the dependence of the urban growth pattern on the\nvalues associated with the protected wetland and other types of landscape\nsurrounding the city. The conflict between city growth and wetland protection\nis projected to occur before 2035, assuming full protection of the wetland. Our\nresults also show that a milder protection policy, allowing some of the wetland\nto be sacrificed, could be beneficial for maintaining other valuable\nlandscapes. This insight would be difficult to achieve by other analytical\nmeans. We conclude that even slight changes in usage priorities of landscapes\ncan significantly affect the landscape distribution in near future. Our results\nalso point to the importance of a protection policy to take the value of\nsurrounding landscapes and the dynamic nature of urban areas into account.", 
    "link": "http://arxiv.org/pdf/1304.1609v1", 
    "arxiv-id": "1304.1609v1"
},{
    "category": "cs.AI", 
    "author": "Pier Luca Lanzi", 
    "title": "Simulated Car Racing Championship: Competition Software Manual", 
    "publish": "2013-04-05T10:42:14Z", 
    "summary": "This manual describes the competition software for the Simulated Car Racing\nChampionship, an international competition held at major conferences in the\nfield of Evolutionary Computation and in the field of Computational\nIntelligence and Games. It provides an overview of the architecture, the\ninstructions to install the software and to run the simple drivers provided in\nthe package, the description of the sensors and the actuators.", 
    "link": "http://arxiv.org/pdf/1304.1672v2", 
    "arxiv-id": "1304.1672v2"
},{
    "category": "math.OC", 
    "author": "M. D. Piggott", 
    "title": "Tidal turbine array optimisation using the adjoint approach", 
    "publish": "2013-04-05T17:20:29Z", 
    "summary": "Oceanic tides have the potential to yield a vast amount of renewable energy.\nTidal stream generators are one of the key technologies for extracting and\nharnessing this potential. In order to extract an economically useful amount of\npower, hundreds of tidal turbines must typically be deployed in an array. This\nnaturally leads to the question of how these turbines should be configured to\nextract the maximum possible power: the positioning and the individual tuning\nof the turbines could significantly influence the extracted power, and hence is\nof major economic interest. However, manual optimisation is difficult due to\nlegal site constraints, nonlinear interactions of the turbine wakes, and the\ncubic dependence of the power on the flow speed. The novel contribution of this\npaper is the formulation of this problem as an optimisation problem constrained\nby a physical model, which is then solved using an efficient gradient-based\noptimisation algorithm. In each optimisation iteration, a two-dimensional\nfinite element shallow water model predicts the flow and the performance of the\ncurrent array configuration. The gradient of the power extracted with respect\nto the turbine positions and their tuning parameters is then computed in a\nfraction of the time taken for a flow solution by solving the associated\nadjoint equations. These equations propagate causality backwards through the\ncomputation, from the power extracted back to the turbine positions and the\ntuning parameters. This yields the gradient at a cost almost independent of the\nnumber of turbines, which is crucial for any practical application. The utility\nof the approach is demonstrated by optimising turbine arrays in four idealised\nscenarios and a more realistic case with up to 256 turbines in the Inner Sound\nof the Pentland Firth, Scotland.", 
    "link": "http://arxiv.org/pdf/1304.1768v2", 
    "arxiv-id": "1304.1768v2"
},{
    "category": "cs.CE", 
    "author": "Sylvain Soliman", 
    "title": "Un mod\u00e8le bool\u00e9en pour l'\u00e9num\u00e9ration des siphons et des pi\u00e8ges   minimaux dans les r\u00e9seaux de Petri", 
    "publish": "2013-04-10T13:23:06Z", 
    "summary": "Petri-nets are a simple formalism for modeling concurrent computation.\nRecently, they have emerged as a powerful tool for the modeling and analysis of\nbiochemical reaction networks, bridging the gap between purely qualitative and\nquantitative models. These networks can be large and complex, which makes their\nstudy difficult and computationally challenging. In this paper, we focus on two\nstructural properties of Petri-nets, siphons and traps, that bring us\ninformation about the persistence of some molecular species. We present two\nmethods for enumerating all minimal siphons and traps of a Petri-net by\niterating the resolution of a boolean model interpreted as either a SAT or a\nCLP(B) program. We compare the performance of these methods with a\nstate-of-the-art dedicated algorithm of the Petri-net community. We show that\nthe SAT and CLP(B) programs are both faster. We analyze why these programs\nperform so well on the models of the repository of biological models\nbiomodels.net, and propose some hard instances for the problem of minimal\nsiphons enumeration.", 
    "link": "http://arxiv.org/pdf/1304.2948v1", 
    "arxiv-id": "1304.2948v1"
},{
    "category": "cs.CE", 
    "author": "Lester Ingber", 
    "title": "Statistical Mechanics Algorithm for Response to Targets (SMART)", 
    "publish": "2013-03-27T19:58:34Z", 
    "summary": "It is proposed to apply modern methods of nonlinear nonequilibrium\nstatistical mechanics to develop software algorithms that will optimally\nrespond to targets within short response times with minimal computer resources.\nThis Statistical Mechanics Algorithm for Response to Targets (SMART) can be\ndeveloped with a view towards its future implementation into a hardwired\nStatistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed\nof response to targets (SMART_SAM).", 
    "link": "http://arxiv.org/pdf/1304.3449v1", 
    "arxiv-id": "1304.3449v1"
},{
    "category": "cs.CE", 
    "author": "Antonio Tadeu Azevedo Gomes", 
    "title": "Expressando Atributos N\u00e3o-Funcionais em Workflows Cient\u00edficos", 
    "publish": "2013-04-18T12:16:44Z", 
    "summary": "In this paper we present OSC, a scientific workflow specification language\nbased on software architecture principles. In contrast with other approaches,\nOSC employs connectors as first-class constructs. In this way, we leverage\nreusability and compositionality in the workflow modeling process, specially in\nthe configuration of mechanisms that manage non-functional attributes.", 
    "link": "http://arxiv.org/pdf/1304.5099v1", 
    "arxiv-id": "1304.5099v1"
},{
    "category": "q-bio.MN", 
    "author": "Mark A. Ragan", 
    "title": "Computing Pathways to Systems Biology: Key Contributions of   Computational Methods in Pathway Identification", 
    "publish": "2013-04-20T00:17:03Z", 
    "summary": "Understanding large molecular networks consisting of entities such as genes,\nproteins or RNAs that interact in complex ways to drive the cellular machinery\nhas been an active focus of systems biology. Computational approaches have\nplayed a key role in systems biology by complementing theoretical and\nexperimental approaches. Here we roadmap some key contributions of\ncomputational methods developed over the last decade in the reconstruction of\nbiological pathways. We position these contributions in a 'systems biology\nperspective' to reemphasize their roles in unraveling cellular mechanisms and\nto understand 'systems biology diseases' including cancer.", 
    "link": "http://arxiv.org/pdf/1304.5565v1", 
    "arxiv-id": "1304.5565v1"
},{
    "category": "cs.CE", 
    "author": "Noah M. Daniels", 
    "title": "Remote Homology Detection in Proteins Using Graphical Models", 
    "publish": "2013-04-24T03:29:23Z", 
    "summary": "Given the amino acid sequence of a protein, researchers often infer its\nstructure and function by finding homologous, or evolutionarily-related,\nproteins of known structure and function. Since structure is typically more\nconserved than sequence over long evolutionary distances, recognizing remote\nprotein homologs from their sequence poses a challenge.\n  We first consider all proteins of known three-dimensional structure, and\nexplore how they cluster according to different levels of homology. An\nautomatic computational method reasonably approximates a human-curated\nhierarchical organization of proteins according to their degree of homology.\n  Next, we return to homology prediction, based only on the one-dimensional\namino acid sequence of a protein. Menke, Berger, and Cowen proposed a Markov\nrandom field model to predict remote homology for beta-structural proteins, but\ntheir formulation was computationally intractable on many beta-strand\ntopologies.\n  We show two different approaches to approximate this random field, both of\nwhich make it computationally tractable, for the first time, on all protein\nfolds. One method simplifies the random field itself, while the other retains\nthe full random field, but approximates the solution through stochastic search.\nBoth methods achieve improvements over the state of the art in remote homology\ndetection for beta-structural protein folds.", 
    "link": "http://arxiv.org/pdf/1304.6476v1", 
    "arxiv-id": "1304.6476v1"
},{
    "category": "q-bio.TO", 
    "author": "W. Hamish B. Wallace", 
    "title": "Ovarian volume throughout life: a validated normative model", 
    "publish": "2013-04-23T16:20:57Z", 
    "summary": "The measurement of ovarian volume has been shown to be a useful indirect\nindicator of the ovarian reserve in women of reproductive age, in the diagnosis\nand management of a number of disorders of puberty and adult reproductive\nfunction, and is under investigation as a screening tool for ovarian cancer. To\ndate there is no normative model of ovarian volume throughout life. By\nsearching the published literature for ovarian volume in healthy females, and\nusing our own data from multiple sources (combined n = 59,994) we have\ngenerated and robustly validated the first model of ovarian volume from\nconception to 82 years of age. This model shows that 69% of the variation in\novarian volume is due to age alone. We have shown that in the average case\novarian volume rises from 0.7 mL (95% CI 0.4 -- 1.1 mL) at 2 years of age to a\npeak of 7.7 mL (95% CI 6.5 -- 9.2 mL) at 20 years of age with a subsequent\ndecline to about 2.8mL (95% CI 2.7 -- 2.9 mL) at the menopause and smaller\nvolumes thereafter. Our model allows us to generate normal values and ranges\nfor ovarian volume throughout life. This is the first validated normative model\nof ovarian volume from conception to old age; it will be of use in the\ndiagnosis and management of a number of diverse gynaecological and reproductive\nconditions in females from birth to menopause and beyond.", 
    "link": "http://arxiv.org/pdf/1304.6613v1", 
    "arxiv-id": "1304.6613v1"
},{
    "category": "q-bio.GN", 
    "author": "Hamidreza Chitsaz", 
    "title": "Distilled Single Cell Genome Sequencing and De Novo Assembly for Sparse   Microbial Communities", 
    "publish": "2013-05-01T00:49:29Z", 
    "summary": "Identification of every single genome present in a microbial sample is an\nimportant and challenging task with crucial applications. It is challenging\nbecause there are typically millions of cells in a microbial sample, the vast\nmajority of which elude cultivation. The most accurate method to date is\nexhaustive single cell sequencing using multiple displacement amplification,\nwhich is simply intractable for a large number of cells. However, there is hope\nfor breaking this barrier as the number of different cell types with distinct\ngenome sequences is usually much smaller than the number of cells.\n  Here, we present a novel divide and conquer method to sequence and de novo\nassemble all distinct genomes present in a microbial sample with a sequencing\ncost and computational complexity proportional to the number of genome types,\nrather than the number of cells. The method is implemented in a tool called\nSqueezambler. We evaluated Squeezambler on simulated data. The proposed divide\nand conquer method successfully reduces the cost of sequencing in comparison\nwith the naive exhaustive approach.\n  Availability: Squeezambler and datasets are available under\nhttp://compbio.cs.wayne.edu/software/squeezambler/.", 
    "link": "http://arxiv.org/pdf/1305.0062v2", 
    "arxiv-id": "1305.0062v2"
},{
    "category": "q-bio.GN", 
    "author": "Alexander Schliep", 
    "title": "Turtle: Identifying frequent k-mers with cache-efficient algorithms", 
    "publish": "2013-05-08T15:49:39Z", 
    "summary": "Counting the frequencies of k-mers in read libraries is often a first step in\nthe analysis of high-throughput sequencing experiments. Infrequent k-mers are\nassumed to be a result of sequencing errors. The frequent k-mers constitute a\nreduced but error-free representation of the experiment, which can inform read\nerror correction or serve as the input to de novo assembly methods. Ideally,\nthe memory requirement for counting should be linear in the number of frequent\nk-mers and not in the, typically much larger, total number of k-mers in the\nread library.\n  We present a novel method that balances time, space and accuracy requirements\nto efficiently extract frequent k-mers even for high coverage libraries and\nlarge genomes such as human. Our method is designed to minimize cache-misses in\na cache-efficient manner by using a Pattern-blocked Bloom filter to remove\ninfrequent k-mers from consideration in combination with a novel\nsort-and-compact scheme, instead of a Hash, for the actual counting. While this\nincreases theoretical complexity, the savings in cache misses reduce the\nempirical running times. A variant can resort to a counting Bloom filter for\neven larger savings in memory at the expense of false negatives in addition to\nthe false positives common to all Bloom filter based approaches. A comparison\nto the state-of-the-art shows reduced memory requirements and running times.\nNote that we also provide the first competitive method to count k-mers up to\nsize 64.", 
    "link": "http://arxiv.org/pdf/1305.1861v1", 
    "arxiv-id": "1305.1861v1"
},{
    "category": "cs.CE", 
    "author": "Odemir M. Bruno", 
    "title": "Multi-q Pattern Classification of Polarization Curves", 
    "publish": "2013-05-10T04:31:49Z", 
    "summary": "Several experimental measurements are expressed in the form of\none-dimensional profiles, for which there is a scarcity of methodologies able\nto classify the pertinence of a given result to a specific group. The\npolarization curves that evaluate the corrosion kinetics of electrodes in\ncorrosive media are an application where the behavior is chiefly analyzed from\nprofiles. Polarization curves are indeed a classic method to determine the\nglobal kinetics of metallic electrodes, but the strong nonlinearity from\ndifferent metals and alloys can overlap and the discrimination becomes a\nchallenging problem. Moreover, even finding a typical curve from replicated\ntests requires subjective judgement. In this paper we used the so-called\nmulti-q approach based on the Tsallis statistics in a classification engine to\nseparate multiple polarization curve profiles of two stainless steels. We\ncollected 48 experimental polarization curves in aqueous chloride medium of two\nstainless steel types, with different resistance against localized corrosion.\nMulti-q pattern analysis was then carried out on a wide potential range, from\ncathodic up to anodic regions. An excellent classification rate was obtained,\nat a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), and\nboth potential ranges, respectively, using only 2% of the original profile\ndata. These results show the potential of the proposed approach towards\nefficient, robust, systematic and automatic classification of highly non-linear\nprofile curves.", 
    "link": "http://arxiv.org/pdf/1305.2876v1", 
    "arxiv-id": "1305.2876v1"
},{
    "category": "cs.CE", 
    "author": "Quan-gong Huo", 
    "title": "Qualitative detection of oil adulteration with machine learning   approaches", 
    "publish": "2013-05-14T13:23:19Z", 
    "summary": "The study focused on the machine learning analysis approaches to identify the\nadulteration of 9 kinds of edible oil qualitatively and answered the following\nthree questions: Is the oil sample adulterant? How does it constitute? What is\nthe main ingredient of the adulteration oil? After extracting the\nhigh-performance liquid chromatography (HPLC) data on triglyceride from 370 oil\nsamples, we applied the adaptive boosting with multi-class Hamming loss\n(AdaBoost.MH) to distinguish the oil adulteration in contrast with the support\nvector machine (SVM). Further, we regarded the adulterant oil and the pure oil\nsamples as ones with multiple labels and with only one label, respectively.\nThen multi-label AdaBoost.MH and multi-label learning vector quantization\n(ML-LVQ) model were built to determine the ingredients and their relative ratio\nin the adulteration oil. The experimental results on six measures show that\nML-LVQ achieves better performance than multi-label AdaBoost.MH.", 
    "link": "http://arxiv.org/pdf/1305.3149v1", 
    "arxiv-id": "1305.3149v1"
},{
    "category": "cs.CE", 
    "author": "Phillip Lord", 
    "title": "The Karyotype Ontology: a computational representation for human   cytogenetic patterns", 
    "publish": "2013-05-16T11:07:42Z", 
    "summary": "The karyotype ontology describes the human chromosome complement as\ndetermined cytogenetically, and is designed as an initial step toward the goal\nof replacing the current system which is based on semantically meaningful\nstrings. This ontology uses a novel, semi-programmatic methodology based around\nthe tawny library to construct many classes rapidly. Here, we describe our use\ncase, methodology and the event-based approach that we use to represent\nkaryotypes.\n  The ontology is available at http://www.purl.org/ontolink/karyotype/. The\nclojure code is available at http://code.google.com/p/karyotype-clj/.", 
    "link": "http://arxiv.org/pdf/1305.3758v1", 
    "arxiv-id": "1305.3758v1"
},{
    "category": "cs.CE", 
    "author": "Stephen S. -T. Yau", 
    "title": "Denoising the 3-Base Periodicity Walks of DNA Sequences in Gene Finding", 
    "publish": "2013-05-23T19:24:25Z", 
    "summary": "A nonlinear Tracking-Differentiator is one-input-two-output system that can\ngenerate smooth approximation of measured signals and get the derivatives of\nthe signals. The nonlinear tracking-Differentiator is explored to denoise and\ngenerate the derivatives of the walks of the 3-periodicity of DNA sequences. An\nimproved algorithm for gene finding is presented using the nonlinear\nTracking-Differentiator. The gene finding algorithm employs the 3-base\nperiodicity of coding region. The 3-base periodicity DNA walks are denoised and\ntracked using the nonlinear Tracking-Differentiator. Case studies demonstrate\nthat the nonlinear Tracking-Differentiator is an effective method to improve\nthe accuracy of the gene finding algorithm.", 
    "link": "http://arxiv.org/pdf/1305.5524v1", 
    "arxiv-id": "1305.5524v1"
},{
    "category": "cs.SY", 
    "author": "Rajni Jaiswal", 
    "title": "Reconstruction and Analysis of Cancer-specific Gene Regulatory Networks   from Gene Expression Profiles", 
    "publish": "2013-05-23T19:21:45Z", 
    "summary": "The main goal of Systems Biology research is to reconstruct biological\nnetworks for its topological analysis so that reconstructed networks can be\nused for the identification of various kinds of disease. The availability of\nhigh-throughput data generated by microarray experiments fueled researchers to\nuse whole-genome gene expression profiles to understand cancer and to\nreconstruct key cancer-specific gene regulatory network. Now, the researchers\nare taking a keen interest in the development of algorithm for the\nreconstruction of gene regulatory network from whole genome expression\nprofiles. In this study, a cancer-specific gene regulatory network (prostate\ncancer) has been constructed using a simple and novel statistics based\napproach. First, significant genes differentially expressing them self in the\ndisease condition has been identified using a two-stage filtering approach\nt-test and fold-change measure. Next, regulatory relationships between the\nidentified genes has been computed using Pearson correlation coefficient. The\nobtained results has been validated with the available databases and\nliterature. We obtained a cancer-specific regulatory network of 29 genes with a\ntotal of 55 regulatory relations in which some of the genes has been identified\nas hub genes that can act as drug target for the cancer diagnosis.", 
    "link": "http://arxiv.org/pdf/1305.5750v2", 
    "arxiv-id": "1305.5750v2"
},{
    "category": "cs.CE", 
    "author": "Eric de Sturler", 
    "title": "Efficient methods for computing observation impact in 4D-Var data   assimilation", 
    "publish": "2013-05-24T17:00:00Z", 
    "summary": "This paper presents a practical computational approach to quantify the effect\nof individual observations in estimating the state of a system. Such an\nanalysis can be used for pruning redundant measurements, and for designing\nfuture sensor networks. The mathematical approach is based on computing the\nsensitivity of the reanalysis (unconstrained optimization solution) with\nrespect to the data. The computational cost is dominated by the solution of a\nlinear system, whose matrix is the Hessian of the cost function, and is only\navailable in operator form. The right hand side is the gradient of a scalar\ncost function that quantifies the forecast error of the numerical model. The\nuse of adjoint models to obtain the necessary first and second order\nderivatives is discussed. We study various strategies to accelerate the\ncomputation, including matrix-free iterative solvers, preconditioners, and an\nin-house multigrid solver. Experiments are conducted on both a small-size\nshallow-water equations model, and on a large-scale numerical weather\nprediction model, in order to illustrate the capabilities of the new\nmethodology.", 
    "link": "http://arxiv.org/pdf/1305.5796v2", 
    "arxiv-id": "1305.5796v2"
},{
    "category": "cs.LG", 
    "author": "Mostefa Mokaddem", 
    "title": "Supervised Feature Selection for Diagnosis of Coronary Artery Disease   Based on Genetic Algorithm", 
    "publish": "2013-05-26T18:16:52Z", 
    "summary": "Feature Selection (FS) has become the focus of much research on decision\nsupport systems areas for which data sets with tremendous number of variables\nare analyzed. In this paper we present a new method for the diagnosis of\nCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes\nNaive (BN) based FS. Basically, CAD dataset contains two classes defined with\n13 features. In GA BN algorithm, GA generates in each iteration a subset of\nattributes that will be evaluated using the BN in the second step of the\nselection procedure. The final set of attribute contains the most relevant\nfeature model that increases the accuracy. The algorithm in this case produces\n85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the\nAlgorithm is then compared with the use of Support Vector Machine (SVM),\nMultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of\nclassification accuracy for those algorithms are respectively 83.5%, 83.16% and\n80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared\nwith other FS algorithms. The Obtained results have shown very promising\noutcomes for the diagnosis of CAD.", 
    "link": "http://arxiv.org/pdf/1305.6046v1", 
    "arxiv-id": "1305.6046v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Modelling Electricity Consumption in Office Buildings: An Agent Based   Approach", 
    "publish": "2013-05-31T15:01:01Z", 
    "summary": "In this paper, we develop an agent-based model which integrates four\nimportant elements, i.e. organisational energy management policies/regulations,\nenergy management technologies, electric appliances and equipment, and human\nbehaviour, to simulate the electricity consumption in office buildings. Based\non a case study, we use this model to test the effectiveness of different\nelectricity management strategies, and solve practical office electricity\nconsumption problems. This paper theoretically contributes to an integration of\nthe four elements involved in the complex organisational issue of office\nelectricity consumption, and practically contributes to an application of an\nagent-based approach for office building electricity consumption study.", 
    "link": "http://arxiv.org/pdf/1305.7437v1", 
    "arxiv-id": "1305.7437v1"
},{
    "category": "cs.NE", 
    "author": "Lindy G. Durrant", 
    "title": "Wavelet feature extraction and genetic algorithm for biomarker detection   in colorectal cancer data", 
    "publish": "2013-05-31T15:53:08Z", 
    "summary": "Biomarkers which predict patient's survival can play an important role in\nmedical diagnosis and treatment. How to select the significant biomarkers from\nhundreds of protein markers is a key step in survival analysis. In this paper a\nnovel method is proposed to detect the prognostic biomarkers of survival in\ncolorectal cancer patients using wavelet analysis, genetic algorithm, and Bayes\nclassifier. One dimensional discrete wavelet transform (DWT) is normally used\nto reduce the dimensionality of biomedical data. In this study one dimensional\ncontinuous wavelet transform (CWT) was proposed to extract the features of\ncolorectal cancer data. One dimensional CWT has no ability to reduce\ndimensionality of data, but captures the missing features of DWT, and is\ncomplementary part of DWT. Genetic algorithm was performed on extracted wavelet\ncoefficients to select the optimized features, using Bayes classifier to build\nits fitness function. The corresponding protein markers were located based on\nthe position of optimized features. Kaplan-Meier curve and Cox regression model\nwere used to evaluate the performance of selected biomarkers. Experiments were\nconducted on colorectal cancer dataset and several significant biomarkers were\ndetected. A new protein biomarker CD46 was found to significantly associate\nwith survival time.", 
    "link": "http://arxiv.org/pdf/1305.7465v1", 
    "arxiv-id": "1305.7465v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Investigating Mathematical Models of Immuno-Interactions with   Early-Stage Cancer under an Agent-Based Modelling Perspective", 
    "publish": "2013-05-31T16:09:00Z", 
    "summary": "Many advances in research regarding immuno-interactions with cancer were\ndeveloped with the help of ordinary differential equation (ODE) models. These\nmodels, however, are not effectively capable of representing problems involving\nindividual localisation, memory and emerging properties, which are common\ncharacteristics of cells and molecules of the immune system. Agent-based\nmodelling and simulation is an alternative paradigm to ODE models that\novercomes these limitations. In this paper we investigate the potential\ncontribution of agent-based modelling and simulation when compared to ODE\nmodelling and simulation. We seek answers to the following questions: Is it\npossible to obtain an equivalent agent-based model from the ODE formulation? Do\nthe outcomes differ? Are there any benefits of using one method compared to the\nother? To answer these questions, we have considered three case studies using\nestablished mathematical models of immune interactions with early-stage cancer.\nThese case studies were re-conceptualised under an agent-based perspective and\nthe simulation results were then compared with those from the ODE models. Our\nresults show that it is possible to obtain equivalent agent-based models (i.e.\nimplementing the same mechanisms); the simulation output of both types of\nmodels however might differ depending on the attributes of the system to be\nmodelled. In some cases, additional insight from using agent-based modelling\nwas obtained. Overall, we can confirm that agent-based modelling is a useful\naddition to the tool set of immunologists, as it has extra features that allow\nfor simulations with characteristics that are closer to the biological\nphenomena.", 
    "link": "http://arxiv.org/pdf/1305.7471v1", 
    "arxiv-id": "1305.7471v1"
},{
    "category": "cs.CE", 
    "author": "Angelika Sebald", 
    "title": "Genetic algorithms and solid state NMR pulse sequences", 
    "publish": "2013-06-02T11:22:21Z", 
    "summary": "The use of genetic algorithms for the optimisation of magic angle spinning\nNMR pulse sequences is discussed. The discussion uses as an example the\noptimisation of the C7 dipolar recoupling pulse sequence, aiming to achieve\nimproved efficiency for spin systems characterised by large chemical shielding\nanisotropies and/or small dipolar coupling interactions. The optimised pulse\nsequence is found to be robust over a wide range of parameters, requires only\nminimal a priori knowledge of the spin system for experimental implementations\nwith buildup rates being solely determined by the magnitude of the dipolar\ncoupling interaction, but is found to be less broadbanded than the original C7\npulse sequence. The optimised pulse sequence breaks the synchronicity between\nr.f. pulses and sample spinning.", 
    "link": "http://arxiv.org/pdf/1306.0194v1", 
    "arxiv-id": "1306.0194v1"
},{
    "category": "cs.CE", 
    "author": "Jinyou Xiao", 
    "title": "An efficient method for evaluating BEM singular integrals on curved   elements with application in acoustic analysis", 
    "publish": "2013-06-03T03:09:47Z", 
    "summary": "The polar coordinate transformation (PCT) method has been extensively used to\ntreat various singular integrals in the boundary element method (BEM). However,\nthe resultant integrands of the PCT tend to become nearly singular when (1) the\naspect ratio of the element is large or (2) the field point is closed to the\nelement boundary; thus a large number of quadrature points are needed to\nachieve a relatively high accuracy. In this paper, the first problem is\ncircumvented by using a conformal transformation so that the geometry of the\ncurved physical element is preserved in the transformed domain. The second\nproblem is alleviated by using a sigmoidal transformation, which makes the\nquadrature points more concentrated around the near singularity.\n  By combining the proposed two transformations with the Guiggiani's method in\n[M. Guiggiani, \\emph{et al}.\n  A general algorithm for the numerical solution of hypersingular boundary\nintegral equations.\n  \\emph{ASME Journal of Applied Mechanics}, 59(1992), 604-614], one obtains an\nefficient and robust numerical method for computing the weakly-, strongly- and\nhyper-singular integrals in high-order BEM with curved elements. Numerical\nintegration results show that, compared with the original PCT, the present\nmethod can reduce the number of quadrature points considerably, for given\naccuracy. For further verification, the method is incorporated into a 2-order\nNystr\\\"om BEM code for solving acoustic Burton-Miller boundary integral\nequation. It is shown that the method can retain the convergence rate of the\nBEM with much less quadrature points than the existing PCT. The method is\nimplemented in C language and freely available.", 
    "link": "http://arxiv.org/pdf/1306.0282v1", 
    "arxiv-id": "1306.0282v1"
},{
    "category": "cs.LG", 
    "author": "Uri Kartoun", 
    "title": "Identifying Pairs in Simulated Bio-Medical Time-Series", 
    "publish": "2013-05-12T22:00:09Z", 
    "summary": "The paper presents a time-series-based classification approach to identify\nsimilarities in pairs of simulated human-generated patterns. An example for a\npattern is a time-series representing a heart rate during a specific\ntime-range, wherein the time-series is a sequence of data points that represent\nthe changes in the heart rate values. A bio-medical simulator system was\ndeveloped to acquire a collection of 7,871 price patterns of financial\ninstruments. The financial instruments traded in real-time on three American\nstock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The\nsystem simulates a human in which each price pattern represents one bio-medical\nsensor. Data provided during trading hours from the stock exchanges allowed\nreal-time classification. Classification is based on new machine learning\ntechniques: self-labeling, which allows the application of supervised learning\nmethods on unlabeled time-series and similarity ranking, which applied on a\ndecision tree learning algorithm to classify time-series regardless of type and\nquantity.", 
    "link": "http://arxiv.org/pdf/1306.0541v1", 
    "arxiv-id": "1306.0541v1"
},{
    "category": "cs.GT", 
    "author": "H. H. Zeineldin", 
    "title": "A Critical Assessment of Cost-Based Nash Methods for Demand Scheduling   in Smart Grids", 
    "publish": "2013-06-04T14:35:24Z", 
    "summary": "Demand-side management (DSM) is becoming an increasingly important component\nof the envisioned smart grid. The ability to improve the efficiency of energy\nuse in the power system by altering demand is widely viewed as being not merely\npromising but in fact essential. However, while the advantages of DSM are\nclear, arriving at an efficient implementation has so far proven to be less\nstraightforward. There have recently been many proposals put forth in the\nliterature to tackle the demand scheduling aspect of DSM. One particular\napproach based on a game-theoretic treatment of the day-ahead load-scheduling\nproblem has recently gained tremendous popularity in the DSM literature. In\nthis letter, an assessment of this approach is conducted, and its main result\nis challenged.", 
    "link": "http://arxiv.org/pdf/1306.0816v1", 
    "arxiv-id": "1306.0816v1"
},{
    "category": "q-bio.QM", 
    "author": "Tomaso Aste", 
    "title": "Graph theory enables drug repurposing. How a mathematical model can   drive the discovery of hidden Mechanisms of Action", 
    "publish": "2013-06-04T20:41:23Z", 
    "summary": "We introduced a methodology to efficiently exploit natural-language expressed\nbiomedical knowledge for repurposing existing drugs towards diseases for which\nthey were not initially intended. Leveraging on developments in Computational\nLinguistics and Graph Theory, a methodology is defined to build a graph\nrepresentation of knowledge, which is automatically analysed to discover hidden\nrelations between any drug and any disease: these relations are specific paths\namong the biomedical entities of the graph, representing possible Modes of\nAction for any given pharmacological compound. These paths are ranked according\nto their relevance, exploiting a measure induced by a stochastic process\ndefined on the graph. Here we show, providing real-world examples, how the\nmethod successfully retrieves known pathophysiological Mode of Actions and\nfinds new ones by meaningfully selecting and aggregating contributions from\nknown bio-molecular interactions. Applications of this methodology are\npresented, and prove the efficacy of the method for selecting drugs as\ntreatment options for rare diseases.", 
    "link": "http://arxiv.org/pdf/1306.0924v1", 
    "arxiv-id": "1306.0924v1"
},{
    "category": "cs.CE", 
    "author": "Dongxiao Zhu", 
    "title": "SPATA: A Seeding and Patching Algorithm for Hybrid Transcriptome   Assembly", 
    "publish": "2013-06-06T19:09:18Z", 
    "summary": "Transcriptome assembly from RNA-Seq reads is an active area of bioinformatics\nresearch. The ever-declining cost and the increasing depth of RNA-Seq have\nprovided unprecedented opportunities to better identify expressed transcripts.\nHowever, the nonlinear transcript structures and the ultra-high throughput of\nRNA-Seq reads pose significant algorithmic and computational challenges to the\nexisting transcriptome assembly approaches, either reference-guided or de novo.\nWhile reference-guided approaches offer good sensitivity, they rely on\nalignment results of the splice-aware aligners and are thus unsuitable for\nspecies with incomplete reference genomes. In contrast, de novo approaches do\nnot depend on the reference genome but face a computational daunting task\nderived from the complexity of the graph built for the whole transcriptome. In\nresponse to these challenges, we present a hybrid approach to exploit an\nincomplete reference genome without relying on splice-aware aligners. We have\ndesigned a split-and-align procedure to efficiently localize the reads to\nindividual genomic loci, which is followed by an accurate de novo assembly to\nassemble reads falling into each locus. Using extensive simulation data, we\ndemonstrate a high accuracy and precision in transcriptome reconstruction by\ncomparing to selected transcriptome assembly tools. Our method is implemented\nin assemblySAM, a GUI software freely available at\nhttp://sammate.sourceforge.net.", 
    "link": "http://arxiv.org/pdf/1306.1511v1", 
    "arxiv-id": "1306.1511v1"
},{
    "category": "cs.CE", 
    "author": "Anar Auda Ablahad", 
    "title": "Enhancement of a Novel Method for Mutational Disease Prediction using   Bioinformatics Techniques and Backpropagation Algorithm", 
    "publish": "2013-06-07T21:53:25Z", 
    "summary": "The noval method for mutational disease prediction using bioinformatics tools\nand datasets for diagnosis the malignant mutations with powerful Artificial\nNeural Network (Backpropagation Network) for classifying these malignant\nmutations are related to gene(s) (like BRCA1 and BRCA2) cause a disease (breast\ncancer). This noval method did not take in consideration just like adopted for\ndealing, analyzing and treat the gene sequences for extracting useful\ninformation from the sequence, also exceeded the environment factors which play\nimportant roles in deciding and calculating some of genes features in order to\nview its functional parts and relations to diseases. This paper is proposed an\nenhancement of a novel method as a first way for diagnosis and prediction the\ndisease by mutations considering and introducing multi other features show the\nalternations, changes in the environment as well as genes, comparing sequences\nto gain information about the structure or function of a query sequence, also\nproposing optimal and more accurate system for classification and dealing with\nspecific disorder using backpropagation with mean square rate 0.000000001.\nIndex Terms (Homology sequence, GC content and AT content, Bioinformatics,\nBackpropagation Network, BLAST, DNA Sequence, Protein Sequence)", 
    "link": "http://arxiv.org/pdf/1306.1850v1", 
    "arxiv-id": "1306.1850v1"
},{
    "category": "cs.CE", 
    "author": "T. Chandrasekhar", 
    "title": "A Novel Approach for Single Gene Selection Using Clustering and   Dimensionality Reduction", 
    "publish": "2013-06-10T07:28:51Z", 
    "summary": "We extend the standard rough set-based approach to deal with huge amounts of\nnumeric attributes versus small amount of available objects. Here, a novel\napproach of clustering along with dimensionality reduction; Hybrid Fuzzy C\nMeans-Quick Reduct (FCMQR) algorithm is proposed for single gene selection.\nGene selection is a process to select genes which are more informative. It is\none of the important steps in knowledge discovery. The problem is that all\ngenes are not important in gene expression data. Some of the genes may be\nredundant, and others may be irrelevant and noisy. In this study, the entire\ndataset is divided in proper grouping of similar genes by applying Fuzzy C\nMeans (FCM) algorithm. A high class discriminated genes has been selected based\non their degree of dependence by applying Quick Reduct algorithm based on Rough\nSet Theory to all the resultant clusters. Average Correlation Value (ACV) is\ncalculated for the high class discriminated genes. The clusters which have the\nACV value a s 1 is determined as significant clusters, whose classification\naccuracy will be equal or high when comparing to the accuracy of the entire\ndataset. The proposed algorithm is evaluated using WEKA classifiers and\ncompared. Finally, experimental results related to the leukemia cancer data\nconfirm that our approach is quite promising, though it surely requires further\nresearch.", 
    "link": "http://arxiv.org/pdf/1306.2118v1", 
    "arxiv-id": "1306.2118v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Defining a Simulation Strategy for Cancer Immunocompetence", 
    "publish": "2013-06-12T17:06:40Z", 
    "summary": "Although there are various types of cancer treatments, none of these\ncurrently take into account the effect of ageing of the immune system and hence\naltered responses to cancer. Recent studies have shown that in vitro\nstimulation of T cells can help in the treatment of patients. There are many\nfactors that have to be considered when simulating an organism's\nimmunocompetence. Our particular interest lies in the study of loss of\nimmunocompetence with age. We are trying to answer questions such as: Given a\ncertain age of a patient, how fit is their immune system to fight cancer? Would\nan immune boost improve the effectiveness of a cancer treatment given the\npatient's immune phenotype and age? We believe that understanding the processes\nof immune system ageing and degradation through computer simulation may help in\nanswering these questions. Specifically, we have decided to look at the change\nin numbers of naive T cells with age, as they play a important role in\nresponses to cancer and anti-tumour vaccination. In this work we present an\nagent-based simulation model to understand the interactions which influence the\nnaive T cell populations over time. Our agent model is based on existing\nmathematical system dynamic model, but in comparisons offers better scope for\ncustomisation and detailed analysis. We believe that the results obtained can\nin future help with the modelling of T cell populations inside tumours.", 
    "link": "http://arxiv.org/pdf/1306.2898v1", 
    "arxiv-id": "1306.2898v1"
},{
    "category": "cs.CE", 
    "author": "David Menachof", 
    "title": "Scenario Analysis, Decision Trees and Simulation for Cost Benefit   Analysis of the Cargo Screening Process", 
    "publish": "2013-06-21T14:47:02Z", 
    "summary": "In this paper we present our ideas for conducting a cost benefit analysis by\nusing three different methods: scenario analysis, decision trees and\nsimulation. Then we introduce our case study and examine these methods in a\nreal world situation. We show how these tools can be used and what the results\nare for each of them. Our aim is to conduct a comparison of these different\nprobabilistic methods of estimating costs for port security risk assessment\nstudies. Methodologically, we are trying to understand the limits of all the\ntools mentioned above by focusing on rare events.", 
    "link": "http://arxiv.org/pdf/1306.5158v1", 
    "arxiv-id": "1306.5158v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Investigating Immune System Aging: System Dynamics and Agent-Based   Modeling", 
    "publish": "2013-06-26T11:32:38Z", 
    "summary": "System dynamics and agent based simulation models can both be used to model\nand understand interactions of entities within a population. Our modeling work\npresented here is concerned with understanding the suitability of the different\ntypes of simulation for the immune system aging problems and comparing their\nresults. We are trying to answer questions such as: How fit is the immune\nsystem given a certain age? Would an immune boost be of therapeutic value, e.g.\nto improve the effectiveness of a simultaneous vaccination? Understanding the\nprocesses of immune system aging and degradation may also help in development\nof therapies that reverse some of the damages caused thus improving life\nexpectancy. Therefore as a first step our research focuses on T cells; major\ncontributors to immune system functionality. One of the main factors\ninfluencing immune system aging is the output rate of naive T cells. Of further\ninterest is the number and phenotypical variety of these cells in an\nindividual, which will be the case study focused on in this paper.", 
    "link": "http://arxiv.org/pdf/1306.6206v1", 
    "arxiv-id": "1306.6206v1"
},{
    "category": "cs.CG", 
    "author": "Sk. Sarif Hassan", 
    "title": "Fractal and Mathematical Morphology in Intricate Comparison between   Tertiary Protein Structures", 
    "publish": "2013-06-11T15:20:03Z", 
    "summary": "Intricate comparison between two given tertiary structures of proteins is as\nimportant as the comparison of their functions. Several algorithms have been\ndevised to compute the similarity and dissimilarity among protein structures.\nBut, these algorithms compare protein structures by structural alignment of the\nprotein backbones which are usually unable to determine precise differences. In\nthis paper, an attempt has been made to compute the similarities and\ndissimilarities among 3D protein structures using the fundamental mathematical\nmorphology operations and fractal geometry which can resolve the problem of\nreal differences. In doing so, two techniques are being used here in\ndetermining the superficial structural (global similarity) and local similarity\nin atomic level of the protein molecules. This intricate structural difference\nwould provide insight to Biologists to understand the protein structures and\ntheir functions more precisely.", 
    "link": "http://arxiv.org/pdf/1307.0029v2", 
    "arxiv-id": "1307.0029v2"
},{
    "category": "q-bio.GN", 
    "author": "Zhao KaiYong", 
    "title": "A new DNA alignment method based on inverted index", 
    "publish": "2013-06-30T10:08:04Z", 
    "summary": "This paper presents a novel DNA sequences alignment method based on inverted\nindex. Now most large scale information retrieval system are all use inverted\nindex as the basic data structure. But its application in DNA sequence\nalignment is still not found. This paper just discuss such applications. Three\nmain problems, DNA segmenting, long DNA query search, DNA search ranking\nalgorithm and evaluation method are detailed respectively. This research\npresents a new avenue to build more effective DNA alignment methods.", 
    "link": "http://arxiv.org/pdf/1307.0194v1", 
    "arxiv-id": "1307.0194v1"
},{
    "category": "cs.DS", 
    "author": "Sanguthevar Rajasekaran", 
    "title": "Efficient Sequential and Parallel Algorithms for Planted Motif Search", 
    "publish": "2013-07-02T01:51:06Z", 
    "summary": "Motif searching is an important step in the detection of rare events\noccurring in a set of DNA or protein sequences. One formulation of the problem\nis known as (l,d)-motif search or Planted Motif Search (PMS). In PMS we are\ngiven two integers l and d and n biological sequences. We want to find all\nsequences of length l that appear in each of the input sequences with at most d\nmismatches. The PMS problem is NP-complete. PMS algorithms are typically\nevaluated on certain instances considered challenging. This paper presents an\nexact parallel PMS algorithm called PMS8. PMS8 is the first algorithm to solve\nthe challenging (l,d) instances (25,10) and (26,11). PMS8 is also efficient on\ninstances with larger l and d such as (50,21). This paper also introduces\nnecessary and sufficient conditions for 3 l-mers to have a common d-neighbor.", 
    "link": "http://arxiv.org/pdf/1307.0571v1", 
    "arxiv-id": "1307.0571v1"
},{
    "category": "cs.CE", 
    "author": "Richard B. Hubbard", 
    "title": "Investigating the Detection of Adverse Drug Events in a UK General   Practice Electronic Health-Care Database", 
    "publish": "2013-07-03T16:55:32Z", 
    "summary": "Data-mining techniques have frequently been developed for Spontaneous\nreporting databases. These techniques aim to find adverse drug events\naccurately and efficiently. Spontaneous reporting databases are prone to\nmissing information, under reporting and incorrect entries. This often results\nin a detection lag or prevents the detection of some adverse drug events. These\nlimitations do not occur in electronic health-care databases. In this paper,\nexisting methods developed for spontaneous reporting databases are implemented\non both a spontaneous reporting database and a general practice electronic\nhealth-care database and compared. The results suggests that the application of\nexisting methods to the general practice database may help find signals that\nhave gone undetected when using the spontaneous reporting system database. In\naddition the general practice database provides far more supplementary\ninformation, that if incorporated in analysis could provide a wealth of\ninformation for identifying adverse events more accurately.", 
    "link": "http://arxiv.org/pdf/1307.1078v1", 
    "arxiv-id": "1307.1078v1"
},{
    "category": "cs.CE", 
    "author": "Tom Rodden", 
    "title": "Application of a clustering framework to UK domestic electricity data", 
    "publish": "2013-07-03T17:03:31Z", 
    "summary": "This paper takes an approach to clustering domestic electricity load profiles\nthat has been successfully used with data from Portugal and applies it to UK\ndata. Clustering techniques are applied and it is found that the preferred\ntechnique in the Portuguese work (a two stage process combining Self Organised\nMaps and Kmeans) is not appropriate for the UK data. The work shows that up to\nnine clusters of households can be identified with the differences in usage\nprofiles being visually striking. This demonstrates the appropriateness of\nbreaking the electricity usage patterns down to more detail than the two load\nprofiles currently published by the electricity industry. The paper details\ninitial results using data collected in Milton Keynes around 1990. Further work\nis described and will concentrate on building accurate and meaningful clusters\nof similar electricity users in order to better direct demand side management\ninitiatives to the most relevant target customers.", 
    "link": "http://arxiv.org/pdf/1307.1079v1", 
    "arxiv-id": "1307.1079v1"
},{
    "category": "cs.CE", 
    "author": "Tom Rodden", 
    "title": "Creating Personalised Energy Plans. From Groups to Individuals using   Fuzzy C Means Clustering", 
    "publish": "2013-07-04T15:55:33Z", 
    "summary": "Changes in the UK electricity market mean that domestic users will be\nrequired to modify their usage behaviour in order that supplies can be\nmaintained. Clustering allows usage profiles collected at the household level\nto be clustered into groups and assigned a stereotypical profile which can be\nused to target marketing campaigns. Fuzzy C Means clustering extends this by\nallowing each household to be a member of many groups and hence provides the\nopportunity to make personalised offers to the household dependent on their\ndegree of membership of each group. In addition, feedback can be provided on\nhow user's changing behaviour is moving them towards more \"green\" or cost\neffective stereotypical usage.", 
    "link": "http://arxiv.org/pdf/1307.1385v1", 
    "arxiv-id": "1307.1385v1"
},{
    "category": "cs.LG", 
    "author": "Uwe Aickelin", 
    "title": "Examining the Classification Accuracy of TSVMs with ?Feature Selection   in Comparison with the GLAD Algorithm", 
    "publish": "2013-07-04T16:06:25Z", 
    "summary": "Gene expression data sets are used to classify and predict patient diagnostic\ncategories. As we know, it is extremely difficult and expensive to obtain gene\nexpression labelled examples. Moreover, conventional supervised approaches\ncannot function properly when labelled data (training examples) are\ninsufficient using Support Vector Machines (SVM) algorithms. Therefore, in this\npaper, we suggest Transductive Support Vector Machines (TSVMs) as\nsemi-supervised learning algorithms, learning with both labelled samples data\nand unlabelled samples to perform the classification of microarray data. To\nprune the superfluous genes and samples we used a feature selection method\ncalled Recursive Feature Elimination (RFE), which is supposed to enhance the\noutput of classification and avoid the local optimization problem. We examined\nthe classification prediction accuracy of the TSVM-RFE algorithm in comparison\nwith the Genetic Learning Across Datasets (GLAD) algorithm, as both are\nsemi-supervised learning methods. Comparing these two methods, we found that\nthe TSVM-RFE surpassed both a SVM using RFE and GLAD.", 
    "link": "http://arxiv.org/pdf/1307.1387v1", 
    "arxiv-id": "1307.1387v1"
},{
    "category": "cs.CE", 
    "author": "Peer-Olaf Siebers", 
    "title": "Systems Dynamics or Agent-Based Modelling for Immune Simulation?", 
    "publish": "2013-07-04T16:15:11Z", 
    "summary": "In immune system simulation there are two competing simulation approaches:\nSystem Dynamics Simulation (SDS) and Agent-Based Simulation (ABS). In the\nliterature there is little guidance on how to choose the best approach for a\nspecific immune problem. Our overall research aim is to develop a framework\nthat helps researchers with this choice. In this paper we investigate if it is\npossible to easily convert simulation models between approaches. With no\nexplicit guidelines available from the literature we develop and test our own\nset of guidelines for converting SDS models into ABS models in a non-spacial\nscenario. We also define guidelines to convert ABS into SDS considering a\nnon-spatial and a spatial scenario. After running some experiments with the\ndeveloped models we found that in all cases there are significant differences\nbetween the results produced by the different simulation methods.", 
    "link": "http://arxiv.org/pdf/1307.1390v1", 
    "arxiv-id": "1307.1390v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Detect adverse drug reactions for drug Alendronate", 
    "publish": "2013-07-04T16:24:17Z", 
    "summary": "Adverse drug reaction (ADR) is widely concerned for public health issue. In\nthis study we propose an original approach to detect the ADRs using feature\nmatrix and feature selection. The experiments are carried out on the drug\nSimvastatin. Major side effects for the drug are detected and better\nperformance is achieved compared to other computerized methods. The detected\nADRs are based on the computerized method, further investigation is needed.", 
    "link": "http://arxiv.org/pdf/1307.1394v1", 
    "arxiv-id": "1307.1394v1"
},{
    "category": "cs.LG", 
    "author": "Lindy Durrant", 
    "title": "Biomarker Clustering of Colorectal Cancer Data to Complement Clinical   Classification", 
    "publish": "2013-07-05T12:56:24Z", 
    "summary": "In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. Attempts are made\nto cluster this dataset and important subsets of it in an effort to\ncharacterize the data and validate existing standards for tumour\nclassification. It is apparent from optimal clustering that existing tumour\nclassification is largely unrelated to immunological factors within a patient\nand that there may be scope for re-evaluating treatment options and survival\nestimates based on a combination of tumour physiology and patient\nhistochemistry.", 
    "link": "http://arxiv.org/pdf/1307.1601v1", 
    "arxiv-id": "1307.1601v1"
},{
    "category": "cs.LG", 
    "author": "Eamonn Ferguson", 
    "title": "Using Clustering to extract Personality Information from socio economic   data", 
    "publish": "2013-07-08T09:25:07Z", 
    "summary": "It has become apparent that models that have been applied widely in\neconomics, including Machine Learning techniques and Data Mining methods,\nshould take into consideration principles that derive from the theories of\nPersonality Psychology in order to discover more comprehensive knowledge\nregarding complicated economic behaviours. In this work, we present a method to\nextract Behavioural Groups by using simple clustering techniques that can\npotentially reveal aspects of the Personalities for their members. We believe\nthat this is very important because the psychological information regarding the\nPersonalities of individuals is limited in real world applications and because\nit can become a useful tool in improving the traditional models of Knowledge\nEconomy.", 
    "link": "http://arxiv.org/pdf/1307.1998v1", 
    "arxiv-id": "1307.1998v1"
},{
    "category": "cs.CE", 
    "author": "Uwe Aickelin", 
    "title": "Variance in System Dynamics and Agent Based Modelling Using the SIR   Model of Infectious Disease", 
    "publish": "2013-07-08T09:28:27Z", 
    "summary": "Classical deterministic simulations of epidemiological processes, such as\nthose based on System Dynamics, produce a single result based on a fixed set of\ninput parameters with no variance between simulations. Input parameters are\nsubsequently modified on these simulations using Monte-Carlo methods, to\nunderstand how changes in the input parameters affect the spread of results for\nthe simulation. Agent Based simulations are able to produce different output\nresults on each run based on knowledge of the local interactions of the\nunderlying agents and without making any changes to the input parameters. In\nthis paper we compare the influence and effect of variation within these two\ndistinct simulation paradigms and show that the Agent Based simulation of the\nepidemiological SIR (Susceptible, Infectious, and Recovered) model is more\neffective at capturing the natural variation within SIR compared to an\nequivalent model using System Dynamics with Monte-Carlo simulation. To\ndemonstrate this effect, the SIR model is implemented using both System\nDynamics (with Monte-Carlo simulation) and Agent Based Modelling based on\npreviously published empirical data.", 
    "link": "http://arxiv.org/pdf/1307.2001v1", 
    "arxiv-id": "1307.2001v1"
},{
    "category": "cs.LG", 
    "author": "Tom Rodden", 
    "title": "Finding the creatures of habit; Clustering households based on their   flexibility in using electricity", 
    "publish": "2013-07-08T14:47:42Z", 
    "summary": "Changes in the UK electricity market, particularly with the roll out of smart\nmeters, will provide greatly increased opportunities for initiatives intended\nto change households' electricity usage patterns for the benefit of the overall\nsystem. Users show differences in their regular behaviours and clustering\nhouseholds into similar groupings based on this variability provides for\nefficient targeting of initiatives. Those people who are stuck into a regular\npattern of activity may be the least receptive to an initiative to change\nbehaviour. A sample of 180 households from the UK are clustered into four\ngroups as an initial test of the concept and useful, actionable groupings are\nfound.", 
    "link": "http://arxiv.org/pdf/1307.2111v1", 
    "arxiv-id": "1307.2111v1"
},{
    "category": "cs.CE", 
    "author": "Mohamed Cheriet", 
    "title": "Computer Simulation of 3-D Finite-Volume Liquid Transport in Fibrous   Materials: a Physical Model for Ink Seepage into Paper", 
    "publish": "2013-07-10T13:33:57Z", 
    "summary": "A physical model for the simulation ink/paper interaction at the mesoscopic\nscale is developed. It is based on the modified Ising model, and is generalized\nto consider the restriction of the finite-volume of ink and also its dynamic\nseepage. This allows the model to obtain the ink distribution within the paper\nvolume. At the mesoscopic scale, the paper is modeled using a discretized fiber\nstructure. The ink distribution is obtained by solving its equivalent\noptimization problem, which is solved using a modified genetic algorithm, along\nwith a new boundary condition and the quasi-linear technique. The model is able\nto simulate the finite-volume distribution of ink.", 
    "link": "http://arxiv.org/pdf/1307.2789v1", 
    "arxiv-id": "1307.2789v1"
},{
    "category": "cs.CE", 
    "author": "V. J. Vijayalakshmi", 
    "title": "A New Approach to the Solution of Economic Dispatch Using Particle Swarm   Optimization with Simulated Annealing", 
    "publish": "2013-07-11T08:42:30Z", 
    "summary": "A new approach to the solution of Economic Dispatch using Particle Swarm\nOptimization is presented. It is the progression of allocating production\namongst the dedicated units such that the restriction forced are fulfilled and\nthe power needs are reduced. More just, the soft computing method has received\nsupplementary concentration and was used in a quantity of successful and\nsensible applications. Here, an attempt has been made to find out the minimum\ncost by using Particle Swarm Optimization Algorithm using the data of three\ngenerating units. In this work, data has been taken such as the loss\ncoefficients with the max-min power limit and cost function. PSO and Simulated\nAnnealing are functional to put out the least amount for dissimilar energy\nrequirements. When the outputs are compared with the conventional method, PSO\nseems to give an improved result with enhanced convergence feature. All the\nmethods are executed in MATLAB environment. The effectiveness and feasibility\nof the proposed method were demonstrated by three generating units case study.\nOutput gives hopeful results, signifying that the projected method of\ncalculation is competent of economically formative advanced eminence solutions\naddressing economic dispatch problems.", 
    "link": "http://arxiv.org/pdf/1307.3014v1", 
    "arxiv-id": "1307.3014v1"
},{
    "category": "cs.CE", 
    "author": "E. N. Sathishkumar", 
    "title": "Unsupervised Gene Expression Data using Enhanced Clustering Method", 
    "publish": "2013-07-12T06:20:59Z", 
    "summary": "Microarrays are made it possible to simultaneously monitor the expression\nprofiles of thousands of genes under various experimental conditions.\nIdentification of co-expressed genes and coherent patterns is the central goal\nin microarray or gene expression data analysis and is an important task in\nbioinformatics research. Feature selection is a process to select features\nwhich are more informative. It is one of the important steps in knowledge\ndiscovery. The problem is that not all features are important. Some of the\nfeatures may be redundant, and others may be irrelevant and noisy. In this work\nthe unsupervised Gene selection method and Enhanced Center Initialization\nAlgorithm (ECIA) with K-Means algorithms have been applied for clustering of\nGene Expression Data. This proposed clustering algorithm overcomes the\ndrawbacks in terms of specifying the optimal number of clusters and\ninitialization of good cluster centroids. Gene Expression Data show that could\nidentify compact clusters with performs well in terms of the Silhouette\nCoefficients cluster measure.", 
    "link": "http://arxiv.org/pdf/1307.3337v1", 
    "arxiv-id": "1307.3337v1"
},{
    "category": "cs.CE", 
    "author": "Tijana Milenkovic", 
    "title": "Dynamic networks reveal key players in aging", 
    "publish": "2013-07-12T09:28:02Z", 
    "summary": "Motivation: Since susceptibility to diseases increases with age, studying\naging gains importance. Analyses of gene expression or sequence data, which\nhave been indispensable for investigating aging, have been limited to studying\ngenes and their protein products in isolation, ignoring their connectivities.\nHowever, proteins function by interacting with other proteins, and this is\nexactly what biological networks (BNs) model. Thus, analyzing the proteins' BN\ntopologies could contribute to understanding of aging. Current methods for\nanalyzing systems-level BNs deal with their static representations, even though\ncells are dynamic. For this reason, and because different data types can give\ncomplementary biological insights, we integrate current static BNs with\naging-related gene expression data to construct dynamic, age-specific BNs.\nThen, we apply sensitive measures of topology to the dynamic BNs to study\ncellular changes with age.\n  Results: While global BN topologies do not significantly change with age,\nlocal topologies of a number of genes do. We predict such genes as\naging-related. We demonstrate credibility of our predictions by: 1) observing\nsignificant overlap between our predicted aging-related genes and \"ground\ntruth\" aging-related genes; 2) showing that our aging-related predictions group\nby functions and diseases that are different than functions and diseases of\ngenes that are not predicted as aging-related; 3) observing significant overlap\nbetween functions and diseases that are enriched in our aging-related\npredictions and those that are enriched in \"ground truth\" aging-related data;\n4) providing evidence that diseases which are enriched in our aging-related\npredictions are linked to human aging; and 5) validating all of our\nhigh-scoring novel predictions via manual literature search.", 
    "link": "http://arxiv.org/pdf/1307.3388v1", 
    "arxiv-id": "1307.3388v1"
},{
    "category": "cs.CE", 
    "author": "E. Elayaraja", 
    "title": "Performance Analysis of Clustering Algorithms for Gene Expression Data", 
    "publish": "2013-07-12T06:43:27Z", 
    "summary": "Microarray technology is a process that allows thousands of genes\nsimultaneously monitor to various experimental conditions. It is used to\nidentify the co-expressed genes in specific cells or tissues that are actively\nused to make proteins, This method is used to analysis the gene expression, an\nimportant task in bioinformatics research. Cluster analysis of gene expression\ndata has proved to be a useful tool for identifying co-expressed genes,\nbiologically relevant groupings of genes and samples. In this paper we analysed\nK-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group\nthe microarray data sets on the basic of ISODATA. AGMFI is to generate initial\nvalues for merge and Spilt factor, maximum merge times instead of selecting\nefficient values as in ISODATA. The initial seeds for each cluster were\nnormally chosen either sequentially or randomly. The quality of the final\nclusters was found to be influenced by these initial seeds. For the real life\nproblems, the suitable number of clusters cannot be predicted. To overcome the\nabove drawback the current research focused on developing the clustering\nalgorithms without giving the initial number of clusters.", 
    "link": "http://arxiv.org/pdf/1307.3549v1", 
    "arxiv-id": "1307.3549v1"
},{
    "category": "q-bio.MN", 
    "author": "Wim Verhaegh", 
    "title": "Prior Biological Knowledge And Epigenetic Information Enhances   Prediction Accuracy Of Bayesian Wnt Pathway", 
    "publish": "2013-07-15T09:27:35Z", 
    "summary": "Computational modeling of Wnt signaling pathway has gained prominence for its\nuse as computer aided diagnostic tool to develop therapeutic cancer target\ndrugs and predict of test samples as cancerous and non cancerous. This\nmanuscript focuses on development of simple static bayesian network models of\nvarying complexity that encompasses prior partially available biological\nknowledge about intra and extra cellular factors affecting the Wnt pathway and\nincorporates epigenetic information like methylation and histone modification\nof a few genes known to have inhibitory affect on Wnt pathway. It might be\nexpected that such models not only increase cancer prediction accuracies and\nalso form basis for understanding Wnt signaling activity in different states of\ntumorigenesis. Initial results in human colorectal cancer cases indicate that\nincorporation of epigenetic information increases prediction accuracy of test\nsamples as being tumorous or normal. Receiver Operator Curves (ROC) and their\nrespective area under the curve (AUC) measurements, obtained from predictions\nof state of test sample and corresponding predictions of the state of\nactivation of transcription complex of the Wnt pathway for the test sample,\nindicate that there is significant difference between the Wnt pathway being on\n(off) and its association with the sample being tumorous (normal). Two sample\nKolmogorov-Smirnov test confirm the statistical deviation between the\ndistributions of these predictions. At a preliminary stage, use of these models\nmay help in understanding the yet unknown effect of certain factors like DKK2,\nDKK3-1 and SFRP-2/3/5 on {\\beta}-catenin transcription complex.", 
    "link": "http://arxiv.org/pdf/1307.4296v3", 
    "arxiv-id": "1307.4296v3"
},{
    "category": "cs.DS", 
    "author": "Simon J. Puglisi", 
    "title": "AliBI: An Alignment-Based Index for Genomic Datasets", 
    "publish": "2013-07-24T15:42:23Z", 
    "summary": "With current hardware and software, a standard computer can now hold in RAM\nan index for approximate pattern matching on about half a dozen human genomes.\nSequencing technologies have improved so quickly, however, that scientists will\nsoon demand indexes for thousands of genomes. Whereas most researchers who have\naddressed this problem have proposed completely new kinds of indexes, we\nrecently described a simple technique that scales standard indexes to work on\nmore genomes. Our main idea was to filter the dataset with LZ77, build a\nstandard index for the filtered file, and then create a hybrid of that standard\nindex and an LZ77-based index. In this paper we describe how to our technique\nto use alignments instead of LZ77, in order to simplify and speed up both\npreprocessing and random access.", 
    "link": "http://arxiv.org/pdf/1307.6462v1", 
    "arxiv-id": "1307.6462v1"
},{
    "category": "cs.CE", 
    "author": "\u00c1lvaro Gomes", 
    "title": "A gradient descent technique coupled with a dynamic simulation to   determine the near optimum orientation of floor plan designs", 
    "publish": "2013-07-25T21:24:53Z", 
    "summary": "A prototype tool to assist architects during the early design stage of floor\nplans has been developed, consisting of an Evolutionary Program for the Space\nAllocation Problem (EPSAP), which generates sets of floor plan alternatives\naccording to the architect's preferences; and a Floor Plan Performance\nOptimization Program (FPOP), which optimizes the selected solutions according\nto thermal performance criteria. The design variables subject to optimization\nare window position and size, overhangs, fins, wall positioning, and building\norientation. A procedure using a transformation operator with gradient descent,\nsuch as behavior, coupled with a dynamic simulation engine was developed for\nthe thermal evaluation and optimization process. However, the need to evaluate\nall possible alternatives regarding designing variables being used during the\noptimization process leads to an intensive use of thermal simulation, which\ndramatically increases the simulation time, rendering it unpractical. An\nalternative approach is a smart optimization approach, which utilizes an\noriented and adaptive search technique to efficiently find the near optimum\nsolution. This paper presents the search methodology for the building\norientation of floor plan designs, and the corresponding efficiency and\neffectiveness indicators. The calculations are based on 100 floor plan designs\ngenerated by EPSAP. All floor plans have the same design program, location, and\nweather data, changing only their geometry. Dynamic simulation of buildings was\neffectively used together with the optimization procedure in this approach to\nsignificantly improve the designs. The use of the orientation variable has been\nincluded in the algorithm.", 
    "link": "http://arxiv.org/pdf/1307.6883v2", 
    "arxiv-id": "1307.6883v2"
},{
    "category": "cs.LG", 
    "author": "Atif N Hasan", 
    "title": "A Comprehensive Evaluation of Machine Learning Techniques for Cancer   Class Prediction Based on Microarray Data", 
    "publish": "2013-07-26T14:44:16Z", 
    "summary": "Prostate cancer is among the most common cancer in males and its\nheterogeneity is well known. Its early detection helps making therapeutic\ndecision. There is no standard technique or procedure yet which is full-proof\nin predicting cancer class. The genomic level changes can be detected in gene\nexpression data and those changes may serve as standard model for any random\ncancer data for class prediction. Various techniques were implied on prostate\ncancer data set in order to accurately predict cancer class including machine\nlearning techniques. Huge number of attributes and few number of sample in\nmicroarray data leads to poor machine learning, therefore the most challenging\npart is attribute reduction or non significant gene reduction. In this work we\nhave compared several machine learning techniques for their accuracy in\npredicting the cancer class. Machine learning is effective when number of\nattributes (genes) are larger than the number of samples which is rarely\npossible with gene expression data. Attribute reduction or gene filtering is\nabsolutely required in order to make the data more meaningful as most of the\ngenes do not participate in tumor development and are irrelevant for cancer\nprediction. Here we have applied combination of statistical techniques such as\ninter-quartile range and t-test, which has been effective in filtering\nsignificant genes and minimizing noise from data. Further we have done a\ncomprehensive evaluation of ten state-of-the-art machine learning techniques\nfor their accuracy in class prediction of prostate cancer. Out of these\ntechniques, Bayes Network out performed with an accuracy of 94.11% followed by\nNavie Bayes with an accuracy of 91.17%. To cross validate our results, we\nmodified our training dataset in six different way and found that average\nsensitivity, specificity, precision and accuracy of Bayes Network is highest\namong all other techniques used.", 
    "link": "http://arxiv.org/pdf/1307.7050v1", 
    "arxiv-id": "1307.7050v1"
},{
    "category": "cs.CE", 
    "author": "Kyle T. Mandli", 
    "title": "ManyClaw: Slicing and dicing Riemann solvers for next generation highly   parallel architectures", 
    "publish": "2013-08-07T02:24:20Z", 
    "summary": "Next generation computer architectures will include order of magnitude more\nintra-node parallelism; however, many application programmers have a difficult\ntime keeping their codes current with the state-of-the-art machines. In this\ncontext, we analyze Hyperbolic PDE solvers, which are used in the solution of\nmany important applications in science and engineering. We present ManyClaw, a\nproject intended to explore the exploitation of intra-node parallelism in\nhyperbolic PDE solvers via the Clawpack software package for solving hyperbolic\nPDEs. Our goal is to separate the low level parallelism and the physical\nequations thus providing users the capability to leverage intra-node\nparallelism without explicitly writing code to take advantage of newer\narchitectures.", 
    "link": "http://arxiv.org/pdf/1308.1464v1", 
    "arxiv-id": "1308.1464v1"
},{
    "category": "cs.CE", 
    "author": "F. De Lima Neto", 
    "title": "Finite Element Model Updating Using Fish School Search Optimization   Method", 
    "publish": "2013-08-10T13:07:07Z", 
    "summary": "A recent nature inspired optimization algorithm, Fish School Search (FSS) is\napplied to the finite element model (FEM) updating problem. This method is\ntested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithm\nare compared with two other metaheuristic algorithms; Genetic Algorithm (GA)\nand Particle Swarm Optimization (PSO). It is observed that on average, the FSS\nand PSO algorithms give more accurate results than the GA. A minor modification\nto the FSS is proposed. This modification improves the performance of FSS on\nthe FEM updating problem which has a constrained search space.", 
    "link": "http://arxiv.org/pdf/1308.2307v1", 
    "arxiv-id": "1308.2307v1"
},{
    "category": "gr-qc", 
    "author": "Manuel Tiglio", 
    "title": "Fast prediction and evaluation of gravitational waveforms using   surrogate models", 
    "publish": "2013-08-16T07:13:44Z", 
    "summary": "[Abridged] We propose a solution to the problem of quickly and accurately\npredicting gravitational waveforms within any given physical model. The method\nis relevant for both real-time applications and in more traditional scenarios\nwhere the generation of waveforms using standard methods can be prohibitively\nexpensive. Our approach is based on three offline steps resulting in an\naccurate reduced-order model that can be used as a surrogate for the\ntrue/fiducial waveform family. First, a set of m parameter values is determined\nusing a greedy algorithm from which a reduced basis representation is\nconstructed. Second, these m parameters induce the selection of m time values\nfor interpolating a waveform time series using an empirical interpolant. Third,\na fit in the parameter dimension is performed for the waveform's value at each\nof these m times. The cost of predicting L waveform time samples for a generic\nparameter choice is of order m L + m c_f online operations where c_f denotes\nthe fitting function operation count and, typically, m << L. We generate\naccurate surrogate models for Effective One Body (EOB) waveforms of\nnon-spinning binary black hole coalescences with durations as long as 10^5 M,\nmass ratios from 1 to 10, and for multiple harmonic modes. We find that these\nsurrogates are three orders of magnitude faster to evaluate as compared to the\ncost of generating EOB waveforms in standard ways. Surrogate model building for\nother waveform models follow the same steps and have the same low online\nscaling cost. For expensive numerical simulations of binary black hole\ncoalescences we thus anticipate large speedups in generating new waveforms with\na surrogate. As waveform generation is one of the dominant costs in parameter\nestimation algorithms and parameter space exploration, surrogate models offer a\nnew and practical way to dramatically accelerate such studies without impacting\naccuracy.", 
    "link": "http://arxiv.org/pdf/1308.3565v2", 
    "arxiv-id": "1308.3565v2"
},{
    "category": "cs.DC", 
    "author": "Norbert Zeh", 
    "title": "QuPARA: Query-Driven Large-Scale Portfolio Aggregate Risk Analysis on   MapReduce", 
    "publish": "2013-08-16T12:15:14Z", 
    "summary": "Stochastic simulation techniques are used for portfolio risk analysis. Risk\nportfolios may consist of thousands of reinsurance contracts covering millions\nof insured locations. To quantify risk each portfolio must be evaluated in up\nto a million simulation trials, each capturing a different possible sequence of\ncatastrophic events over the course of a contractual year. In this paper, we\nexplore the design of a flexible framework for portfolio risk analysis that\nfacilitates answering a rich variety of catastrophic risk queries. Rather than\naggregating simulation data in order to produce a small set of high-level risk\nmetrics efficiently (as is often done in production risk management systems),\nthe focus here is on allowing the user to pose queries on unaggregated or\npartially aggregated data. The goal is to provide a flexible framework that can\nbe used by analysts to answer a wide variety of unanticipated but natural ad\nhoc queries. Such detailed queries can help actuaries or underwriters to better\nunderstand the multiple dimensions (e.g., spatial correlation, seasonality,\nperil features, construction features, and financial terms) that can impact\nportfolio risk. We implemented a prototype system, called QuPARA (Query-Driven\nLarge-Scale Portfolio Aggregate Risk Analysis), using Hadoop, which is Apache's\nimplementation of the MapReduce paradigm. This allows the user to take\nadvantage of large parallel compute servers in order to answer ad hoc risk\nanalysis queries efficiently even on very large data sets typically encountered\nin practice. We describe the design and implementation of QuPARA and present\nexperimental results that demonstrate its feasibility. A full portfolio risk\nanalysis run consisting of a 1,000,000 trial simulation, with 1,000 events per\ntrial, and 3,200 risk transfer contracts can be completed on a 16-node Hadoop\ncluster in just over 20 minutes.", 
    "link": "http://arxiv.org/pdf/1308.3615v1", 
    "arxiv-id": "1308.3615v1"
},{
    "category": "q-bio.GN", 
    "author": "Carl Kingsford", 
    "title": "Sailfish: Alignment-free Isoform Quantification from RNA-seq Reads using   Lightweight Algorithms", 
    "publish": "2013-08-16T19:51:34Z", 
    "summary": "RNA-seq has rapidly become the de facto technique to measure gene expression.\nHowever, the time required for analysis has not kept up with the pace of data\ngeneration. Here we introduce Sailfish, a novel computational method for\nquantifying the abundance of previously annotated RNA isoforms from RNA-seq\ndata. Sailfish entirely avoids mapping reads, which is a time-consuming step in\nall current methods. Sailfish provides quantification estimates much faster\nthan existing approaches (typically 20-times faster) without loss of accuracy.", 
    "link": "http://arxiv.org/pdf/1308.3700v1", 
    "arxiv-id": "1308.3700v1"
},{
    "category": "cs.SY", 
    "author": "Giordano Pola", 
    "title": "Proceedings Third International Workshop on Hybrid Autonomous Systems", 
    "publish": "2013-08-22T15:44:08Z", 
    "summary": "The interest on autonomous systems is increasing both in industry and\nacademia. Such systems must operate with limited human intervention in a\nchanging environment and must be able to compensate for significant system\nfailures without external intervention. The most appropriate models of\nautonomous systems can be found in the class of hybrid systems (which study\ncontinuous-state dynamic processes via discrete-state controllers) that\ninteract with their environment. This workshop brings together researchers\ninterested in all aspects of autonomy and resilience of hybrid systems.", 
    "link": "http://arxiv.org/pdf/1308.4904v1", 
    "arxiv-id": "1308.4904v1"
},{
    "category": "cs.CE", 
    "author": "Celine Lantieri Marcovici", 
    "title": "Biological effects and equivalent doses in radiotherapy: a software   solution", 
    "publish": "2013-08-23T12:08:54Z", 
    "summary": "The limits of TDF (time, dose, and fractionation) and linear quadratic models\nhave been known for a long time. Medical physicists and physicians are required\nto provide fast and reliable interpretations regarding the delivered doses or\nany future prescriptions relating to treatment changes. We therefore propose a\ncalculation interface under the GNU license to be used for equivalent doses,\nbiological doses, and normal tumor complication probability (Lyman model). The\nmethodology used draws from several sources: the linear-quadratic-linear model\nof Astrahan, the repopulation effects of Dale, and the prediction of\nmulti-fractionated treatments of Thames. The results are obtained from an\nalgorithm that minimizes an ad-hoc cost function, and then compared to the\nequivalent dose computed using standard calculators in seven French\nradiotherapy centers.", 
    "link": "http://arxiv.org/pdf/1308.5906v1", 
    "arxiv-id": "1308.5906v1"
},{
    "category": "cs.CE", 
    "author": "Abdul Sattar", 
    "title": "Mixing Energy Models in Genetic Algorithms for On-Lattice Protein   Structure Prediction", 
    "publish": "2013-11-15T13:09:19Z", 
    "summary": "Protein structure prediction (PSP) is computationally a very challenging\nproblem. The challenge largely comes from the fact that the energy function\nthat needs to be minimised in order to obtain the native structure of a given\nprotein is not clearly known. A high resolution 20x20 energy model could better\ncapture the behaviour of the actual energy function than a low resolution\nenergy model such as hydrophobic polar. However, the fine grained details of\nthe high resolution interaction energy matrix are often not very informative\nfor guiding the search. In contrast, a low resolution energy model could\neffectively bias the search towards certain promising directions. In this\npaper, we develop a genetic algorithm that mainly uses a high resolution energy\nmodel for protein structure evaluation but uses a low resolution HP energy\nmodel in focussing the search towards exploring structures that have\nhydrophobic cores. We experimentally show that this mixing of energy models\nleads to significant lower energy structures compared to the state-of-the-art\nresults.", 
    "link": "http://arxiv.org/pdf/1311.3840v1", 
    "arxiv-id": "1311.3840v1"
},{
    "category": "cs.SY", 
    "author": "Daniele de Rigo", 
    "title": "Software Uncertainty in Integrated Environmental Modelling: the role of   Semantics and Open Science", 
    "publish": "2013-11-19T15:00:26Z", 
    "summary": "Computational aspects increasingly shape environmental sciences. Actually,\ntransdisciplinary modelling of complex and uncertain environmental systems is\nchallenging computational science (CS) and also the science-policy interface.\nLarge spatial-scale problems falling within this category - i.e. wide-scale\ntransdisciplinary modelling for environment (WSTMe) - often deal with factors\n(a) for which deep-uncertainty may prevent usual statistical analysis of\nmodelled quantities and need different ways for providing policy-making with\nscience-based support. Here, practical recommendations are proposed for\ntempering a peculiar - not infrequently underestimated - source of uncertainty.\nSoftware errors in complex WSTMe may subtly affect the outcomes with possible\nconsequences even on collective environmental decision-making. Semantic\ntransparency in CS and free software are discussed as possible mitigations.", 
    "link": "http://arxiv.org/pdf/1311.4762v2", 
    "arxiv-id": "1311.4762v2"
},{
    "category": "cs.DC", 
    "author": "Andrew Rau-Chaplin", 
    "title": "High Performance Risk Aggregation: Addressing the Data Processing   Challenge the Hadoop MapReduce Way", 
    "publish": "2013-11-22T09:35:40Z", 
    "summary": "Monte Carlo simulations employed for the analysis of portfolios of\ncatastrophic risk process large volumes of data. Often times these simulations\nare not performed in real-time scenarios as they are slow and consume large\ndata. Such simulations can benefit from a framework that exploits parallelism\nfor addressing the computational challenge and facilitates a distributed file\nsystem for addressing the data challenge. To this end, the Apache Hadoop\nframework is chosen for the simulation reported in this paper so that the\ncomputational challenge can be tackled using the MapReduce model and the data\nchallenge can be addressed using the Hadoop Distributed File System. A parallel\nalgorithm for the analysis of aggregate risk is proposed and implemented using\nthe MapReduce model in this paper. An evaluation of the performance of the\nalgorithm indicates that the Hadoop MapReduce model offers a framework for\nprocessing large data in aggregate risk analysis. A simulation of aggregate\nrisk employing 100,000 trials with 1000 catastrophic events per trial on a\ntypical exposure set and contract structure is performed on multiple worker\nnodes in less than 6 minutes. The result indicates the scope and feasibility of\nMapReduce for tackling the computational and data challenge in the analysis of\naggregate risk for real-time use.", 
    "link": "http://arxiv.org/pdf/1311.5686v1", 
    "arxiv-id": "1311.5686v1"
},{
    "category": "cs.CV", 
    "author": "Prasanta K Panigrahi", 
    "title": "Wavelet and Fast Fourier Transform based analysis of Solar Image", 
    "publish": "2013-10-30T10:37:28Z", 
    "summary": "Both of Wavelet and Fast Fourier Transform are strong signal processing tools\nin the field of Data Analysis. In this paper fast fourier transform (FFT) and\nWavelet Transform are employed to observe some important features of Solar\nimage (December, 2004). We have tried to find out the periodicity and coherence\nof different sections of the solar image. We plotted the distribution of energy\nin solar surface by analyzing the solar image with scalograms and\n3D-coefficient plots.", 
    "link": "http://arxiv.org/pdf/1311.6799v2", 
    "arxiv-id": "1311.6799v2"
},{
    "category": "physics.ao-ph", 
    "author": "O. Perpi\u00f1\u00e1n", 
    "title": "Downscaling of global solar irradiation in R", 
    "publish": "2013-11-28T08:13:20Z", 
    "summary": "A methodology for downscaling solar irradiation from satellite-derived\ndatabases is described using R software. Different packages such as raster,\nparallel, solaR, gstat, sp and rasterVis are considered in this study for\nimproving solar resource estimation in areas with complex topography, in which\ndownscaling is a very useful tool for reducing inherent deviations in\nsatellite-derived irradiation databases, which lack of high global spatial\nresolution. A topographical analysis of horizon blocking and sky-view is\ndeveloped with a digital elevation model to determine what fraction of hourly\nsolar irradiation reaches the Earth's surface. Eventually, kriging with\nexternal drift is applied for a better estimation of solar irradiation\nthroughout the region analyzed. This methodology has been implemented as an\nexample within the region of La Rioja in northern Spain, and the mean absolute\nerror found is a striking 25.5% lower than with the original database.", 
    "link": "http://arxiv.org/pdf/1311.7235v1", 
    "arxiv-id": "1311.7235v1"
},{
    "category": "cs.CE", 
    "author": "Ms. Rumaisah Munir", 
    "title": "Secure Debit Card Device Model", 
    "publish": "2014-02-02T21:08:07Z", 
    "summary": "The project envisages the implementation of an e-payment system utilizing\nFIPS-201 Smart Card. The system combines hardware and software modules. The\nhardware module takes data insertions (e.g. currency notes), processes the data\nand then creates connection with the smart card using serial/USB ports to\nperform further mathematical manipulations. The hardware interacts with servers\nat the back for authentication and identification of users and for data storage\npertaining to a particular user. The software module manages database, handles\nidentities, provide authentication and secure communication between the various\nsystem components. It will also provide a component to the end users. This\ncomponent can be in the form of software for computer or executable binaries\nfor PoS devices. The idea is to receive data in the embedded system from data\nreader and smart card. After manipulations, the updated data is imprinted on\nsmart card memory and also updated in the back end servers maintaining\ndatabase. The information to be sent to a server is sent through a PoS device\nwhich has multiple transfer mediums involving wired and un-wired mediums. The\nuser device also acts as an updater; therefore, whenever the smart card is\ninserted by user, it is automatically updated by synchronizing with back-end\ndatabase. The project required expertise in embedded systems, networks, java\nand C++ (Optional).", 
    "link": "http://arxiv.org/pdf/1402.0247v1", 
    "arxiv-id": "1402.0247v1"
},{
    "category": "cs.GT", 
    "author": "Bertrand Corn\u00e9lusse", 
    "title": "A quantitative analysis of the effect of flexible loads on reserve   markets", 
    "publish": "2014-02-03T12:34:25Z", 
    "summary": "We propose and analyze a day-ahead reserve market model that handles bids\nfrom flexible loads. This pool market model takes into account the fact that a\nload modulation in one direction must usually be compensated later by a\nmodulation of the same magnitude in the opposite direction. Our analysis takes\ninto account the gaming possibilities of producers and retailers, controlling\nload flexibility, in the day-ahead energy and reserve markets, and in imbalance\nsettlement. This analysis is carried out by an agent-based approach where, for\nevery round, each actor uses linear programs to maximize its profit according\nto forecasts of the prices. The procurement of a reserve is assumed to be\ndetermined, for each period, as a fixed percentage of the total consumption\ncleared in the energy market for the same period. The results show that the\nprovision of reserves by flexible loads has a negligible impact on the energy\nmarket prices but markedly decreases the cost of reserve procurement. However,\nas the rate of flexible loads increases, the system operator has to rely more\nand more on non-contracted reserves, which may cancel out the benefits made in\nthe procurement of reserves.", 
    "link": "http://arxiv.org/pdf/1402.0362v2", 
    "arxiv-id": "1402.0362v2"
},{
    "category": "cs.CV", 
    "author": "Zeyun Yu", 
    "title": "An Optimization Method For Slice Interpolation Of Medical Images", 
    "publish": "2014-02-05T05:31:59Z", 
    "summary": "Slice interpolation is a fast growing field in medical image processing.\nIntensity-based interpolation and object-based interpolation are two major\ngroups of methods in the literature. In this paper, we describe an\nobject-oriented, optimization method based on a modified version of\ncurvature-based image registration, in which a displacement field is computed\nfor the missing slice between two known slices and used to interpolate the\nintensities of the missing slice. The proposed approach is evaluated\nquantitatively by using the Mean Squared Difference (MSD) as a metric. The\nproduced results also show visual improvement in preserving sharp edges in\nimages.", 
    "link": "http://arxiv.org/pdf/1402.0936v2", 
    "arxiv-id": "1402.0936v2"
},{
    "category": "cond-mat.stat-mech", 
    "author": "Eric Vanden-Eijnden", 
    "title": "Flows in Complex Networks: Theory, Algorithms, and Application to   Lennard-Jones Cluster Rearrangement", 
    "publish": "2014-01-29T10:00:25Z", 
    "summary": "A set of analytical and computational tools based on transition path theory\n(TPT) is proposed to analyze flows in complex networks. Specifically, TPT is\nused to study the statistical properties of the reactive trajectories by which\ntransitions occur between specific groups of nodes on the network. Sampling\ntools are built upon the outputs of TPT that allow to generate these reactive\ntrajectories directly, or even transition paths that travel from one group of\nnodes to the other without making any detour and carry the same probability\ncurrent as the reactive trajectories. These objects permit to characterize the\nmechanism of the transitions, for example by quantifying the width of the tubes\nby which these transitions occur, the location and distribution of their\ndynamical bottlenecks, etc. These tools are applied to a network modeling the\ndynamics of the Lennard-Jones cluster with 38 atoms (LJ38) and used to\nunderstand the mechanism by which this cluster rearranges itself between its\ntwo most likely states at various temperatures.", 
    "link": "http://arxiv.org/pdf/1402.1736v1", 
    "arxiv-id": "1402.1736v1"
},{
    "category": "q-bio.GN", 
    "author": "Jesse G. Meyer", 
    "title": "In silico Proteome Cleavage Reveals Iterative Digestion Strategy for   High Sequence Coverage", 
    "publish": "2014-02-07T23:13:48Z", 
    "summary": "In the post-genome era, biologists have sought to measure the complete\ncomplement of proteins, termed proteomics. Currently, the most effective method\nto measure the proteome is with shotgun, or bottom-up, proteomics, in which the\nproteome is digested into peptides that are identified followed by protein\ninference. Despite continuous improvements to all steps of the shotgun\nproteomics workflow, observed proteome coverage is often low; some proteins are\nidentified by a single peptide sequence. Complete proteome sequence coverage\nwould allow comprehensive characterization of RNA splicing variants and all\npost translational modifications, which would drastically improve the accuracy\nof biological models. There are many reasons for the sequence coverage deficit,\nbut ultimately peptide length determines sequence observability. Peptides that\nare too short are lost because they match many protein sequences and their true\norigin is ambiguous. The maximum observable peptide length is determined by\nseveral analytical challenges. This paper explores computationally how peptide\nlengths produced from several common proteome digestion methods limit\nobservable proteome coverage. Iterative proteome cleavage strategies are also\nexplored. These simulations reveal that maximized proteome coverage can be\nachieved by use of an iterative digestion protocol involving multiple proteases\nand chemical cleavages that theoretically allow 91.1% proteome coverage.", 
    "link": "http://arxiv.org/pdf/1402.1794v1", 
    "arxiv-id": "1402.1794v1"
},{
    "category": "cs.DS", 
    "author": "Steef van de Velde", 
    "title": "Tactical Fixed Job Scheduling with Spread-Time Constraints", 
    "publish": "2014-02-08T19:30:16Z", 
    "summary": "We address the tactical fixed job scheduling problem with spread-time\nconstraints. In such a problem, there are a fixed number of classes of machines\nand a fixed number of groups of jobs. Jobs of the same group can only be\nprocessed by machines of a given set of classes. All jobs have their fixed\nstart and end times. Each machine is associated with a cost according to its\nmachine class. Machines have spread-time constraints, with which each machine\nis only available for $L$ consecutive time units from the start time of the\nearliest job assigned to it. The objective is to minimize the total cost of the\nmachines used to process all the jobs. For this strongly NP-hard problem, we\ndevelop a branch-and-price algorithm, which solves instances with up to $300$\njobs, as compared with CPLEX, which cannot solve instances of $100$ jobs. We\nfurther investigate the influence of machine flexibility by computational\nexperiments. Our results show that limited machine flexibility is sufficient in\nmost situations.", 
    "link": "http://arxiv.org/pdf/1402.1881v1", 
    "arxiv-id": "1402.1881v1"
},{
    "category": "cs.CE", 
    "author": "Michele Piana", 
    "title": "Sliding window and compressive sensing for low-field dynamic magnetic   resonance imaging", 
    "publish": "2014-02-11T11:24:11Z", 
    "summary": "We describe an acquisition/processing procedure for image reconstruction in\ndynamic Magnetic Resonance Imaging (MRI). The approach requires sliding window\nto record a set of trajectories in the k-space, standard regularization to\nreconstruct an estimate of the object and compressed sensing to recover image\nresiduals. We validated this approach in the case of specific simulated\nexperiments and, in the case of real measurements, we showed that the procedure\nis reliable even in the case of data acquired by means of a low-field scanner.", 
    "link": "http://arxiv.org/pdf/1402.2453v1", 
    "arxiv-id": "1402.2453v1"
},{
    "category": "cs.CE", 
    "author": "Val\u00e9rio Ramos Batista", 
    "title": "First steps to Virtual Mammography: Simulating external compressions of   the breast with the Surface Evolver", 
    "publish": "2014-02-17T19:56:22Z", 
    "summary": "In this paper we introduce a computational modelling that reproduces the\nbreast compression processes used to obtain the mammogram. The main result is a\nprogramme in which one can track the first steps of virtual mammography. On the\none hand, our modelling enables addition of structures that represent different\ntissues, muscles and glands in the breast. On the other hand, we shall validate\nand implement it by means of laboratory tests with phantoms. To the best of our\nknowledge, these two characteristics do confer originality to our research.\nThis is because their interrelation seems not to be properly established\nelsewhere yet. We conclude that our model reproduces the same shapes and\nmeasurements really taken from the volunteer's breasts.", 
    "link": "http://arxiv.org/pdf/1402.4101v1", 
    "arxiv-id": "1402.4101v1"
},{
    "category": "q-fin.RM", 
    "author": "Lakshmi Kaligounder", 
    "title": "Densely Entangled Financial Systems", 
    "publish": "2014-02-21T05:54:10Z", 
    "summary": "In [1] Zawadoski introduces a banking network model in which the asset and\ncounter-party risks are treated separately and the banks hedge their assets\nrisks by appropriate OTC contracts. In his model, each bank has only two\ncounter-party neighbors, a bank fails due to the counter-party risk only if at\nleast one of its two neighbors default, and such a counter-party risk is a low\nprobability event. Informally, the author shows that the banks will hedge their\nasset risks by appropriate OTC contracts, and, though it may be socially\noptimal to insure against counter-party risk, in equilibrium banks will {\\em\nnot} choose to insure this low probability event.\n  In this paper, we consider the above model for more general network\ntopologies, namely when each node has exactly 2r counter-party neighbors for\nsome integer r>0. We extend the analysis of [1] to show that as the number of\ncounter-party neighbors increase the probability of counter-party risk also\nincreases, and in particular the socially optimal solution becomes privately\nsustainable when each bank hedges its risk to at least n/2 banks, where n is\nthe number of banks in the network, i.e., when 2r is at least n/2, banks not\nonly hedge their asset risk but also hedge its counter-party risk.", 
    "link": "http://arxiv.org/pdf/1402.5208v1", 
    "arxiv-id": "1402.5208v1"
},{
    "category": "cs.CE", 
    "author": "Chanabasyya M. Vastrad", 
    "title": "Predictive Comparative QSAR analysis of Sulfathiazole Analogues as   Mycobacterium Tuberculosis H37RV Inhabitors", 
    "publish": "2014-02-22T02:52:52Z", 
    "summary": "Antitubercular activity of Sulfathiazole Derivitives series were subjected to\nQuantitative Structure Activity Relationship (QSAR) Analysis with an attempt to\nderive and understand a correlation between the Biologically Activity as\ndependent variable and various descriptors as independent variables. QSAR\nmodels generated using 28 compounds. Several statistical regression expressions\nwere obtained using Partial Least Squares (PLS) Regression, Multiple Linear\nRegression (MLR) and Principal Component Regression (PCR) methods. The among\nthese methods, Partial Least Square Regression (PLS) method has shown very\npromising result as compare to other two methods. A QSAR model was generated by\na training set of 18 molecules with correlation coefficient r (r square) of\n0.9191, significant cross validated correlation coefficient (q square) of\n0.8300, F test of 53.5783, r square for external test set pred_r square\n-3.6132, coefficient of correlation of predicted data set pred_r_se square\n1.4859 and degree of freedom 14 by Partial Least Squares Regression Method.", 
    "link": "http://arxiv.org/pdf/1402.5466v1", 
    "arxiv-id": "1402.5466v1"
},{
    "category": "cs.SE", 
    "author": "the N4U Consortium", 
    "title": "An Integrated e-science Analysis Base for Computation Neuroscience   Experiments and Analysis", 
    "publish": "2014-02-24T09:14:44Z", 
    "summary": "Recent developments in data management and imaging technologies have\nsignificantly affected diagnostic and extrapolative research in the\nunderstanding of neurodegenerative diseases. However, the impact of these new\ntechnologies is largely dependent on the speed and reliability with which the\nmedical data can be visualised, analysed and interpreted. The EUs neuGRID for\nUsers (N4U) is a follow-on project to neuGRID, which aims to provide an\nintegrated environment to carry out computational neuroscience experiments.\nThis paper reports on the design and development of the N4U Analysis Base and\nrelated Information Services, which addresses existing research and practical\nchallenges by offering an integrated medical data analysis environment with the\nnecessary building blocks for neuroscientists to optimally exploit neuroscience\nworkflows, large image datasets and algorithms in order to conduct analyses.\nThe N4U Analysis Base enables such analyses by indexing and interlinking the\nneuroimaging and clinical study datasets stored on the N4U Grid infrastructure,\nalgorithms and scientific workflow definitions along with their associated\nprovenance information.", 
    "link": "http://arxiv.org/pdf/1402.5757v1", 
    "arxiv-id": "1402.5757v1"
},{
    "category": "cs.CE", 
    "author": "Mustafa Abdul Salam", 
    "title": "LSSVM-ABC Algorithm for Stock Price prediction", 
    "publish": "2014-02-25T23:02:08Z", 
    "summary": "In this paper, Artificial Bee Colony (ABC) algorithm which inspired from the\nbehavior of honey bees swarm is presented. ABC is a stochastic population-based\nevolutionary algorithm for problem solving. ABC algorithm, which is considered\none of the most recently swarm intelligent techniques, is proposed to optimize\nleast square support vector machine (LSSVM) to predict the daily stock prices.\nThe proposed model is based on the study of stocks historical data, technical\nindicators and optimizing LSSVM with ABC algorithm. ABC selects best free\nparameters combination for LSSVM to avoid over-fitting and local minima\nproblems and improve prediction accuracy. LSSVM optimized by Particle swarm\noptimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparison\nwith proposed model. Proposed model tested with twenty datasets representing\ndifferent sectors in S&P 500 stock market. Results presented in this paper show\nthat the proposed model has fast convergence speed, and it also achieves better\naccuracy than compared techniques in most cases.", 
    "link": "http://arxiv.org/pdf/1402.6366v1", 
    "arxiv-id": "1402.6366v1"
},{
    "category": "cs.CE", 
    "author": "David Lowe", 
    "title": "Analysis of Multibeam SONAR Data using Dissimilarity Representations", 
    "publish": "2014-02-19T10:21:34Z", 
    "summary": "This paper considers the problem of low-dimensional visualisation of very\nhigh dimensional information sources for the purpose of situation awareness in\nthe maritime environment. In response to the requirement for human decision\nsupport aids to reduce information overload (and specifically, data amenable to\ninter-point relative similarity measures) appropriate to the below-water\nmaritime domain, we are investigating a preliminary prototype topographic\nvisualisation model. The focus of the current paper is on the mathematical\nproblem of exploiting a relative dissimilarity representation of signals in a\nvisual informatics mapping model, driven by real-world sonar systems. An\nindependent source model is used to analyse the sonar beams from which a simple\nprobabilistic input model to represent uncertainty is mapped to a latent\nvisualisation space where data uncertainty can be accommodated. The use of\neuclidean and non-euclidean measures are used and the motivation for future use\nof non-euclidean measures is made. Concepts are illustrated using a simulated\n64 beam weak SNR dataset with realistic sonar targets.", 
    "link": "http://arxiv.org/pdf/1402.6636v1", 
    "arxiv-id": "1402.6636v1"
},{
    "category": "cs.CE", 
    "author": "Chad Myers", 
    "title": "Analysis of Barcode sequence features to find anomalies due to   amplification Bias", 
    "publish": "2014-02-27T02:59:52Z", 
    "summary": "In this paper we aim at investigating whether barcode sequence features can\npredict the read count ambiguities caused during PCR based next generation\nsequencing techniques. The methodologies we used are mutual information based\nmotif discovery and Lasso regression technique using features generated from\nthe barcode sequence. The results indicate that there is a certain degree of\ncorrelation between motifs discovered in the sequences and the read counts. Our\nmain contribution in this paper is a thorough investigation of the barcode\nfeatures that gave us useful information regarding the significance of the\nsequence features and the sequence containing the discovered motifs in\nprediction of read counts.", 
    "link": "http://arxiv.org/pdf/1402.6775v1", 
    "arxiv-id": "1402.6775v1"
},{
    "category": "cs.CE", 
    "author": "Alexandre Gramfort", 
    "title": "Data-driven HRF estimation for encoding and decoding models", 
    "publish": "2014-02-27T18:50:58Z", 
    "summary": "Despite the common usage of a canonical, data-independent, hemodynamic\nresponse function (HRF), it is known that the shape of the HRF varies across\nbrain regions and subjects. This suggests that a data-driven estimation of this\nfunction could lead to more statistical power when modeling BOLD fMRI data.\nHowever, unconstrained estimation of the HRF can yield highly unstable results\nwhen the number of free parameters is large. We develop a method for the joint\nestimation of activation and HRF using a rank constraint causing the estimated\nHRF to be equal across events/conditions, yet permitting it to be different\nacross voxels. Model estimation leads to an optimization problem that we\npropose to solve with an efficient quasi-Newton method exploiting fast gradient\ncomputations. This model, called GLM with Rank-1 constraint (R1-GLM), can be\nextended to the setting of GLM with separate designs which has been shown to\nimprove decoding accuracy in brain activity decoding experiments. We compare 10\ndifferent HRF modeling methods in terms of encoding and decoding score in two\ndifferent datasets. Our results show that the R1-GLM model significantly\noutperforms competing methods in both encoding and decoding settings,\npositioning it as an attractive method both from the points of view of accuracy\nand computational efficiency.", 
    "link": "http://arxiv.org/pdf/1402.7015v6", 
    "arxiv-id": "1402.7015v6"
},{
    "category": "cs.CE", 
    "author": "Mustafa Abdul Salam", 
    "title": "A Machine Learning Model for Stock Market Prediction", 
    "publish": "2014-02-28T19:12:50Z", 
    "summary": "Stock market prediction is the act of trying to determine the future value of\na company stock or other financial instrument traded on a financial exchange.", 
    "link": "http://arxiv.org/pdf/1402.7351v1", 
    "arxiv-id": "1402.7351v1"
},{
    "category": "cs.CE", 
    "author": "H. Nguyen-Xuan", 
    "title": "An extended isogeometric analysis for vibration of cracked FGM plates   using higher-order shear deformation theory", 
    "publish": "2014-03-03T04:09:30Z", 
    "summary": "A novel and effective formulation that combines the eXtended IsoGeometric\nApproach (XIGA) and Higher-order Shear Deformation Theory (HSDT) is proposed to\nstudy the free vibration of cracked Functionally Graded Material (FGM) plates.\nHerein, the general HSDT model with five unknown variables per node is applied\nfor calculating the stiffness matrix without needing Shear Correction Factor\n(SCF). In order to model the discontinuous and singular phenomena in the\ncracked plates, IsoGeometric Analysis (IGA) utilizing the Non-Uniform Rational\nB-Spline (NURBS) functions is incorporated with enrichment functions through\nthe partition of unity method. NURBS basis functions with their inherent\narbitrary high order smoothness permit the C1 requirement of the HSDT model.\nThe material properties of the FGM plates vary continuously through the plate\nthickness according to an exponent function. The effects of gradient index,\ncrack length, crack location, length to thickness on the natural frequencies\nand mode shapes of simply supported and clamped FGM plate are studied.\nNumerical examples are provided to show excellent performance of the proposed\nmethod compared with other published solutions in the literature.", 
    "link": "http://arxiv.org/pdf/1403.0306v1", 
    "arxiv-id": "1403.0306v1"
},{
    "category": "cs.CE", 
    "author": "H. Nguyen-Xuan", 
    "title": "Isogeometric finite element analysis of laminated composite plates based   on a four variable refined plate theory", 
    "publish": "2014-03-03T04:10:48Z", 
    "summary": "In this paper, a novel and effective formulation based on isogeometric\napproach (IGA) and Refined Plate Theory (RPT) is proposed to study the behavior\nof laminated composite plates. Using many kinds of higher-order distributed\nfunctions, RPT model naturally satisfies the traction-free boundary conditions\nat plate surfaces and describes the non-linear distribution of shear stresses\nwithout requiring shear correction factor (SCF). IGA utilizes the basis\nfunctions, namely B-splines or non-uniform rational B-splines (NURBS), which\nachieve easily the smoothness of any arbitrary order. It hence satisfies the C1\nrequirement of the RPT model. The static, dynamic and buckling analysis of\nrectangular plates is investigated for different boundary conditions. Numerical\nresults show high effectiveness of the present formulation.", 
    "link": "http://arxiv.org/pdf/1403.0307v1", 
    "arxiv-id": "1403.0307v1"
},{
    "category": "cs.CE", 
    "author": "R. G. Ragel", 
    "title": "Accelerating motif finding in DNA sequences with multicore CPUs", 
    "publish": "2014-03-06T01:24:13Z", 
    "summary": "Motif discovery in DNA sequences is a challenging task in molecular biology.\nIn computational motif discovery, Planted (l, d) motif finding is a widely\nstudied problem and numerous algorithms are available to solve it. Both\nhardware and software accelerators have been introduced to accelerate the motif\nfinding algorithms. However, the use of hardware accelerators such as FPGAs\nneeds hardware specialists to design such systems. Software based acceleration\nmethods on the other hand are easier to implement than hardware acceleration\ntechniques. Grid computing is one such software based acceleration technique\nwhich has been used in acceleration of motif finding. However, drawbacks such\nas network communication delays and the need of fast interconnection between\nnodes in the grid can limit its usage and scalability. As using multicore CPUs\nto accelerate CPU intensive tasks are becoming increasingly popular and common\nnowadays, we can employ it to accelerate motif finding and it can be a faster\nmethod than grid based acceleration. In this paper, we have explored the use of\nmulticore CPUs to accelerate motif finding. We have accelerated the Skip-Brute\nForce algorithm on multicore CPUs parallelizing it using the POSIX thread\nlibrary. Our method yielded an average speed up of 34x on a 32-core processor\ncompared to a speed up of 21x on a grid based implementation of 32 nodes.", 
    "link": "http://arxiv.org/pdf/1403.1313v1", 
    "arxiv-id": "1403.1313v1"
},{
    "category": "cs.CE", 
    "author": "Inampudi Ramesh Babu", 
    "title": "An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated   MACA based Clonal Classifier for Protein Coding and Promoter Region   Prediction)", 
    "publish": "2014-03-06T03:46:38Z", 
    "summary": "This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has\ncreated good impact on solving major problems in bioinformatics like protein\nregion identification and promoter region prediction with less time (Pokkuluri\nKiran Sree, 2014). This AIS-INMACA is now came with several variations\n(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics\nfor solving many problems in bioinformatics. So this paper will be very much\nuseful for so many researchers who are working in the domain of bioinformatics\nwith cellular automata.", 
    "link": "http://arxiv.org/pdf/1403.1336v1", 
    "arxiv-id": "1403.1336v1"
},{
    "category": "cs.CE", 
    "author": "Jiasong Wang", 
    "title": "A Novel Method for Comparative Analysis of DNA Sequences by   Ramanujan-Fourier Transform", 
    "publish": "2014-03-06T18:29:52Z", 
    "summary": "Alignment-free sequence analysis approaches provide important alternatives\nover multiple sequence alignment (MSA) in biological sequence analysis because\nalignment-free approaches have low computation complexity and are not dependent\non high level of sequence identity, however, most of the existing\nalignment-free methods do not employ true full information content of sequences\nand thus can not accurately reveal similarities and differences among DNA\nsequences. We present a novel alignment-free computational method for sequence\nanalysis based on Ramanujan-Fourier transform (RFT), in which complete\ninformation of DNA sequences is retained. We represent DNA sequences as four\nbinary indicator sequences and apply RFT on the indicator sequences to convert\nthem into frequency domain. The Euclidean distance of the complete RFT\ncoefficients of DNA sequences are used as similarity measure. To address the\ndifferent lengths in Euclidean space of RFT coefficients, we pad zeros to short\nDNA binary sequences so that the binary sequences equal the longest length in\nthe comparison sequence data. Thus, the DNA sequences are compared in the same\ndimensional frequency space without information loss. We demonstrate the\nusefulness of the proposed method by presenting experimental results on\nhierarchical clustering of genes and genomes. The proposed method opens a new\nchannel to biological sequence analysis, classification, and structural module\nidentification.", 
    "link": "http://arxiv.org/pdf/1403.1523v2", 
    "arxiv-id": "1403.1523v2"
},{
    "category": "cs.LG", 
    "author": "Mohammad Mansour Riahi Kashani", 
    "title": "Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in   Lung Cancer Dataset", 
    "publish": "2014-03-08T08:12:54Z", 
    "summary": "Classification algorithms are unable to make reliable models on the datasets\nwith huge sizes. These datasets contain many irrelevant and redundant features\nthat mislead the classifiers. Furthermore, many huge datasets have imbalanced\nclass distribution which leads to bias over majority class in the\nclassification process. In this paper combination of unsupervised\ndimensionality reduction methods with resampling is proposed and the results\nare tested on Lung-Cancer dataset. In the first step PCA is applied on\nLung-Cancer dataset to compact the dataset and eliminate irrelevant features\nand in the second step SMOTE resampling is carried out to balance the class\ndistribution and increase the variety of sample domain. Finally, Naive Bayes\nclassifier is applied on the resulting dataset and the results are compared and\nevaluation metrics are calculated. The experiments show the effectiveness of\nthe proposed method across four evaluation metrics: Overall accuracy, False\nPositive Rate, Precision, Recall.", 
    "link": "http://arxiv.org/pdf/1403.1949v1", 
    "arxiv-id": "1403.1949v1"
},{
    "category": "cs.CE", 
    "author": "Simon Kramer", 
    "title": "A Galois-Connection between Myers-Briggs' Type Indicators and Szondi's   Personality Profiles", 
    "publish": "2014-03-08T19:25:36Z", 
    "summary": "We propose a computable Galois-connection between Myers-Briggs' Type\nIndicators (MBTIs), the most widely-used personality measure for\nnon-psychiatric populations (based on C.G. Jung's personality types), and\nSzondi's personality profiles (SPPs), a less well-known but, as we show, finer\npersonality measure for psychiatric as well as non-psychiatric populations\n(conceived as a unification of the depth psychology of S. Freud, C.G. Jung, and\nA. Adler). The practical significance of our result is that our\nGalois-connection provides a pair of computable, interpreting translations\nbetween the two personality spaces of MBTIs and SPPs: one concrete from\nMBTI-space to SPP-space (because SPPs are finer) and one abstract from\nSPP-space to MBTI-space (because MBTIs are coarser). Thus Myers-Briggs' and\nSzondi's personality-test results are mutually interpretable and\ninter-translatable, even automatically by computers.", 
    "link": "http://arxiv.org/pdf/1403.2000v1", 
    "arxiv-id": "1403.2000v1"
},{
    "category": "cs.CE", 
    "author": "Ugur Ayan", 
    "title": "Time Series Analysis on Stock Market for Text Mining Correlation of   Economy News", 
    "publish": "2014-03-08T19:50:15Z", 
    "summary": "This paper proposes an information retrieval method for the economy news. The\neffect of economy news, are researched in the word level and stock market\nvalues are considered as the ground proof. The correlation between stock market\nprices and economy news is an already addressed problem for most of the\ncountries. The most well-known approach is applying the text mining approaches\nto the news and some time series analysis techniques over stock market closing\nvalues in order to apply classification or clustering algorithms over the\nfeatures extracted. This study goes further and tries to ask the question what\nare the available time series analysis techniques for the stock market closing\nvalues and which one is the most suitable? In this study, the news and their\ndates are collected into a database and text mining is applied over the news,\nthe text mining part has been kept simple with only term frequency-inverse\ndocument frequency method. For the time series analysis part, we have studied\n10 different methods such as random walk, moving average, acceleration,\nBollinger band, price rate of change, periodic average, difference, momentum or\nrelative strength index and their variation. In this study we have also\nexplained these techniques in a comparative way and we have applied the methods\nover Turkish Stock Market closing values for more than a 2 year period. On the\nother hand, we have applied the term frequency-inverse document frequency\nmethod on the economy news of one of the high-circulating newspapers in Turkey.", 
    "link": "http://arxiv.org/pdf/1403.2002v1", 
    "arxiv-id": "1403.2002v1"
},{
    "category": "cs.LG", 
    "author": "Eamonn Keogh", 
    "title": "Flying Insect Classification with Inexpensive Sensors", 
    "publish": "2014-03-11T18:36:39Z", 
    "summary": "The ability to use inexpensive, noninvasive sensors to accurately classify\nflying insects would have significant implications for entomological research,\nand allow for the development of many useful applications in vector control for\nboth medical and agricultural entomology. Given this, the last sixty years have\nseen many research efforts on this task. To date, however, none of this\nresearch has had a lasting impact. In this work, we explain this lack of\nprogress. We attribute the stagnation on this problem to several factors,\nincluding the use of acoustic sensing devices, the over-reliance on the single\nfeature of wingbeat frequency, and the attempts to learn complex models with\nrelatively little data. In contrast, we show that pseudo-acoustic optical\nsensors can produce vastly superior data, that we can exploit additional\nfeatures, both intrinsic and extrinsic to the insect's flight behavior, and\nthat a Bayesian classification approach allows us to efficiently learn\nclassification models that are very robust to over-fitting. We demonstrate our\nfindings with large scale experiments that dwarf all previous works combined,\nas measured by the number of insects and the number of species considered.", 
    "link": "http://arxiv.org/pdf/1403.2654v1", 
    "arxiv-id": "1403.2654v1"
},{
    "category": "cs.CE", 
    "author": "Hamid Abrishami Moghaddam", 
    "title": "A consistent model for cardiac deformation estimation under abnormal   ventricular muscle conditions", 
    "publish": "2014-02-23T00:14:19Z", 
    "summary": "Deformation modeling of cardiac muscle is an important issue in the field of\ncardiac analysis. For this reason, many approaches have been developed to best\nestimate the cardiac muscle deformation, and to obtain a practical model to use\nin diagnostic procedures. But there are some conditions, like in case of\nmyocardial infarction, in which the regular modeling approaches are not useful.\nIn this section, using a point-wise approach in deformation estimation, we try\nto estimate the deformation under some abnormal conditions of cardiac muscle.\nFirst, the endocardial and epicardial contour points are ordered with respect\nto the center of gravity of endocardial contour and boundary point displacement\nvectors are extracted. Then to solve the governing equation of deformation,\nwhich is an elliptic equation, we apply boundary conditions in accordance with\nthe computed displacement vectors and then the Finite Element method (FEM) will\nbe used to solve the governing equation. Using obtained displacement field\nthrough the cardiac muscle, strain map is extracted to show the mechanical\nbehavior of cardiac muscle. To validate the proposed algorithm in case of\ninfracted muscle, a non-homogeneous ring is modeled using ANSYS under a uniform\ntime varying internal pressure, which is the case in real cardiac muscle\ndeformation and then the proposed algorithm implemented in MATLAB and the\nresults for such problem are extracted.", 
    "link": "http://arxiv.org/pdf/1403.2740v2", 
    "arxiv-id": "1403.2740v2"
},{
    "category": "cs.DB", 
    "author": "Suprativ Saha", 
    "title": "Delineation of Techniques to implement on the enhanced proposed model   using data mining for protein sequence classification", 
    "publish": "2014-03-12T08:44:08Z", 
    "summary": "In post genomic era with the advent of new technologies a huge amount of\ncomplex molecular data are generated with high throughput. The management of\nthis biological data is definitely a challenging task due to complexity and\nheterogeneity of data for discovering new knowledge. Issues like managing noisy\nand incomplete data are needed to be dealt with. Use of data mining in\nbiological domain has made its inventory success. Discovering new knowledge\nfrom the biological data is a major challenge in data mining technique. The\nnovelty of the proposed model is its combined use of intelligent techniques to\nclassify the protein sequence faster and efficiently. Use of FFT, fuzzy\nclassifier, String weighted algorithm, gram encoding method, neural network\nmodel and rough set classifier in a single model and in an appropriate place\ncan enhance the quality of the classification system.Thus the primary challenge\nis to identify and classify the large protein sequences in a very fast and easy\nbut intellectual way to decrease the time complexity and space complexity.", 
    "link": "http://arxiv.org/pdf/1403.2848v1", 
    "arxiv-id": "1403.2848v1"
},{
    "category": "cs.DB", 
    "author": "Yiping Zhao", 
    "title": "Analyzing Large Biological Datasets with an Improved Algorithm for MIC", 
    "publish": "2014-03-14T07:26:33Z", 
    "summary": "A computational framework utilizes the traditional similarity measures for\nmining the significant relationships in biological annotations is recently\nproposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved\napproximation algorithm for MIC (maximal information coefficient) named IAMIC\nis suggested to perfect this framework for discovering the hidden regularities\nbetween biological annotations. Further, IAMIC is the enhanced algorithm for\napproximating a novel similarity coefficient MIC with generality and\nequitability, which makes it more appropriate for data exploration. Here it is\nshown that IAMIC is also applicable for identify the associations between\nbiological annotations.", 
    "link": "http://arxiv.org/pdf/1403.3495v1", 
    "arxiv-id": "1403.3495v1"
},{
    "category": "cs.NE", 
    "author": "Mark Shackelford", 
    "title": "Evolutionary Algorithm for Drug Discovery Interim Design Report", 
    "publish": "2014-03-19T16:24:13Z", 
    "summary": "A software program which aims to provide an exploration capability over the\nSearch Space of potential drug molecules. The program explores the search space\nby generating random molecules, determining their fitness and then breeding a\nnew generation from the fittest individuals. The search space, in theory any\ncombination of any elements in any order, is constrained by the use of a subset\nof elements and a list of fragments, molecular parts that are known to be\nuseful in drug development. The resultant molecules from each generation are\nstored in a searchable database, so that the user can browse through previous\ngenerations looking for interesting molecules.", 
    "link": "http://arxiv.org/pdf/1403.4871v1", 
    "arxiv-id": "1403.4871v1"
},{
    "category": "cs.CE", 
    "author": "Venkatesan Kanagaraj", 
    "title": "Comparing Numerical Integration Schemes for Time-Continuous   Car-Following Models", 
    "publish": "2014-03-19T16:54:23Z", 
    "summary": "When simulating trajectories by integrating time-continuous car-following\nmodels, standard integration schemes such as the forth-order Runge-Kutta method\n(RK4) are rarely used while the simple Euler's method is popular among\nresearchers. We compare four explicit methods: Euler's method, ballistic\nupdate, Heun's method (trapezoidal rule), and the standard forth-order RK4. As\nperformance metrics, we plot the global discretization error as a function of\nthe numerical complexity. We tested the methods on several time-continuous\ncar-following models in several multi-vehicle simulation scenarios with and\nwithout discontinuities such as stops or a discontinuous behavior of an\nexternal leader. We find that the theoretical advantage of RK4 (consistency\norder~4) only plays a role if both the acceleration function of the model and\nthe external data of the simulation scenario are sufficiently often\ndifferentiable. Otherwise, we obtain lower (and often fractional) consistency\norders. Although, to our knowledge, Heun's method has never been used for\nintegrating car-following models, it turns out to be the best scheme for many\npractical situations. The ballistic update always prevails Euler's method\nalthough both are of first order.", 
    "link": "http://arxiv.org/pdf/1403.4881v1", 
    "arxiv-id": "1403.4881v1"
},{
    "category": "cs.CE", 
    "author": "Inampudi Ramesh Babu", 
    "title": "AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein   Coding and Promoter Region Prediction", 
    "publish": "2014-03-24T12:37:11Z", 
    "summary": "Most of the problems in bioinformatics are now the challenges in computing.\nThis paper aims at building a classifier based on Multiple Attractor Cellular\nAutomata (MACA) which uses fuzzy logic. It is strengthened with an artificial\nImmune System Technique (AIS), Clonal algorithm for identifying a protein\ncoding and promoter region in a given DNA sequence. The proposed classifier is\nnamed as AIS-INMACA introduces a novel concept to combine CA with artificial\nimmune system to produce a better classifier which can address major problems\nin bioinformatics. This will be the first integrated algorithm which can\npredict both promoter and protein coding regions. To obtain good fitness rules\nthe basic concept of Clonal selection algorithm was used. The proposed\nclassifier can handle DNA sequences of lengths 54,108,162,252,354. This\nclassifier gives the exact boundaries of both protein and promoter regions with\nan average accuracy of 89.6%. This classifier was tested with 97,000 data\ncomponents which were taken from Fickett & Toung, MPromDb, and other sequences\nfrom a renowned medical university. This proposed classifier can handle huge\ndata sets and can find protein and promoter regions even in mixed and\noverlapped DNA sequences. This work also aims at identifying the logicality\nbetween the major problems in bioinformatics and tries to obtaining a common\nframe work for addressing major problems in bioinformatics like protein\nstructure prediction, RNA structure prediction, predicting the splicing pattern\nof any primary transcript and analysis of information content in DNA, RNA,\nprotein sequences and structure. This work will attract more researchers\ntowards application of CA as a potential pattern classifier to many important\nproblems in bioinformatics", 
    "link": "http://arxiv.org/pdf/1403.5933v1", 
    "arxiv-id": "1403.5933v1"
},{
    "category": "q-bio.QM", 
    "author": "Alex Pothen", 
    "title": "Immunophenotypes of Acute Myeloid Leukemia From Flow Cytometry Data   Using Templates", 
    "publish": "2014-03-22T02:23:28Z", 
    "summary": "Motivation: We investigate whether a template-based classification pipeline\ncould be used to identify immunophenotypes in (and thereby classify) a\nheterogeneous disease with many subtypes. The disease we consider here is Acute\nMyeloid Leukemia, which is heterogeneous at the morphologic, cytogenetic and\nmolecular levels, with several known subtypes. The prognosis and treatment for\nAML depends on the subtype.\n  Results: We apply flowMatch, an algorithmic pipeline for flow cytometry data\ncreated in earlier work, to compute templates succinctly summarizing classes of\nAML and healthy samples. We develop a scoring function that accounts for\nfeatures of the AML data such as heterogeneity to identify immunophenotypes\ncorresponding to various AML subtypes, including APL. All of the AML samples in\nthe test set are classified correctly with high confidence.\n  Availability: flowMatch is available at\nwww.bioconductor.org/packages/devel/bioc/html/flowMatch.html; programs specific\nto immunophenotyping AML are at www.cs.purdue.edu/homes/aazad/software.html.", 
    "link": "http://arxiv.org/pdf/1403.6358v1", 
    "arxiv-id": "1403.6358v1"
},{
    "category": "cs.CR", 
    "author": "Roger Wattenhofer", 
    "title": "Bitcoin Transaction Malleability and MtGox", 
    "publish": "2014-03-26T14:01:13Z", 
    "summary": "In Bitcoin, transaction malleability describes the fact that the signatures\nthat prove the ownership of bitcoins being transferred in a transaction do not\nprovide any integrity guarantee for the signatures themselves. This allows an\nattacker to mount a malleability attack in which it intercepts, modifies, and\nrebroadcasts a transaction, causing the transaction issuer to believe that the\noriginal transaction was not confirmed. In February 2014 MtGox, once the\nlargest Bitcoin exchange, closed and filed for bankruptcy claiming that\nattackers used malleability attacks to drain its accounts. In this work we use\ntraces of the Bitcoin network for over a year preceding the filing to show\nthat, while the problem is real, there was no widespread use of malleability\nattacks before the closure of MtGox.", 
    "link": "http://arxiv.org/pdf/1403.6676v1", 
    "arxiv-id": "1403.6676v1"
},{
    "category": "cs.CE", 
    "author": "Adrian Sandu", 
    "title": "A Sampling Filter for Non-Gaussian Data Assimilation", 
    "publish": "2014-03-27T17:21:08Z", 
    "summary": "Data assimilation combines information from models, measurements, and priors\nto estimate the state of a dynamical system such as the atmosphere. The\nEnsemble Kalman filter (EnKF) is a family of ensemble-based data assimilation\napproaches that has gained wide popularity due its simple formulation, ease of\nimplementation, and good practical results. Most EnKF algorithms assume that\nthe underlying probability distributions are Gaussian. Although this assumption\nis well accepted, it is too restrictive when applied to large nonlinear models,\nnonlinear observation operators, and large levels of uncertainty. Several\napproaches have been proposed in order to avoid the Gaussianity assumption. One\nof the most successful strategies is the maximum likelihood ensemble filter\n(MLEF) which computes a maximum a posteriori estimate of the state assuming the\nposterior distribution is Gaussian. MLEF is designed to work with nonlinear and\neven non-differentiable observation operators, and shows good practical\nperformance. However, there are limits to the degree of nonlinearity that MLEF\ncan handle. This paper proposes a new ensemble-based data assimilation method,\nnamed the \"sampling filter\", which obtains the analysis by sampling directly\nfrom the posterior distribution. The sampling strategy is based on a Hybrid\nMonte Carlo (HMC) approach that can handle non-Gaussian probability\ndistributions. Numerical experiments are carried out using the Lorenz-96 model\nand observation operators with different levels of non-linearity and\ndifferentiability. The proposed filter is also tested with shallow water model\non a sphere with linear observation operator. The results show that the\nsampling filter can perform well even in highly nonlinear situations were EnKF\nand MLEF filters diverge.", 
    "link": "http://arxiv.org/pdf/1403.7137v2", 
    "arxiv-id": "1403.7137v2"
},{
    "category": "cs.CE", 
    "author": "David Radford", 
    "title": "Acceleration of a Full-scale Industrial CFD Application with OP2", 
    "publish": "2014-03-27T20:14:24Z", 
    "summary": "Hydra is a full-scale industrial CFD application used for the design of\nturbomachinery at Rolls Royce plc. It consists of over 300 parallel loops with\na code base exceeding 50K lines and is capable of performing complex\nsimulations over highly detailed unstructured mesh geometries. Unlike simpler\nstructured-mesh applications, which feature high speed-ups when accelerated by\nmodern processor architectures, such as multi-core and many-core processor\nsystems, Hydra presents major challenges in data organization and movement that\nneed to be overcome for continued high performance on emerging platforms. We\npresent research in achieving this goal through the OP2 domain-specific\nhigh-level framework. OP2 targets the domain of unstructured mesh problems and\nfollows the design of an active library using source-to-source translation and\ncompilation to generate multiple parallel implementations from a single\nhigh-level application source for execution on a range of back-end hardware\nplatforms. We chart the conversion of Hydra from its original hand-tuned\nproduction version to one that utilizes OP2, and map out the key difficulties\nencountered in the process. To our knowledge this research presents the first\napplication of such a high-level framework to a full scale production code.\nSpecifically we show (1) how different parallel implementations can be achieved\nwith an active library framework, even for a highly complicated industrial\napplication such as Hydra, and (2) how different optimizations targeting\ncontrasting parallel architectures can be applied to the whole application,\nseamlessly, reducing developer effort and increasing code longevity.\nPerformance results demonstrate that not only the same runtime performance as\nthat of the hand-tuned original production code could be achieved, but it can\nbe significantly improved on conventional processor systems. Additionally, we\nachieve further...", 
    "link": "http://arxiv.org/pdf/1403.7209v1", 
    "arxiv-id": "1403.7209v1"
},{
    "category": "cs.DS", 
    "author": "Cesim Erten", 
    "title": "Constrained Alignments of a Pair of Graphs", 
    "publish": "2014-03-31T11:18:32Z", 
    "summary": "We consider the constrained graph alignment problem which has applications in\nbiological network analysis studies. Given two input graphs $G_1, G_2$, a pair\nof vertex mappings induces an edge conservation if the vertex pairs are\nadjacent in their respective graphs. In general terms the goal is to provide a\none-to-one mapping between the vertices of the input graphs such that edge\nconservation is maximized. However the allowed mappings are restricted. Let\n$m_1$ ($m_2$) denote the number of $G_2$-vertices ($G_1$-vertices) that each\n$G_1$-vertex ($G_2$-vertex) is allowed to be mapped to. All provided results\nassume $m_2=1$, except for the fixed-parameter tractability result for bounded\ndegree graphs which applies to the more general setting of any constant $m_1,\nm_2$. We present a polynomial time solution for the special case where $G_1$ is\nacyclic. Relaxing the constraint on $G_1$, for the setting of $m_1=2$, we\nprovide several structural properties that lead to polynomial-time\napproximation algorithms. We then relax the constraint on $m_1$ and consider\nany positive integer constant $m_1$. We provide further structural properties\nfor this setting which lead to several additional approximation algorithms with\napproximation ratios better than those of the previous studies. For the same\nsetting, we also show that the problem is fixed-parameter tractable,\nparameterized only by the output size. Previously the same result was known\nonly for bounded degree graphs. We finally consider the more general setting\nwhere $m_1, m_2$ can be any positive integer constant and provide an\napproximation algorithm and a fixed-parameter tractability result that applies\nto bounded degree graphs.", 
    "link": "http://arxiv.org/pdf/1403.7948v2", 
    "arxiv-id": "1403.7948v2"
},{
    "category": "cs.DC", 
    "author": "Umang Vipul", 
    "title": "Map-Reduce Parallelization of Motif Discovery", 
    "publish": "2014-05-02T07:30:45Z", 
    "summary": "Motif discovery is one of the most challenging problems in bioinformatics\ntoday. DNA sequence motifs are becoming increasingly important in analysis of\ngene regulation. Motifs are short, recurring patterns in DNA that have a\nbiological function. For example, they indicate binding sites for Transcription\nFactors (TFs) and nucleases. There are a number of Motif Discovery algorithms\nthat run sequentially. The sequential nature stops these algorithms from being\nparallelized. HOMER is one such Motif discovery tool, that we have decided to\nuse to overcome this limitation. To overcome this limitation, we propose a new\nmethodology for Motif Discovery, using HOMER, that parallelizes the task.\nParallelized version can potentially yield better scalability and performance.\nTo achieve this, we have decided to use sub-sampling and the Map Reduce model.\nAt each Map node, a sub-sampled version of the input DNA sequences is used as\ninput to HOMER. Subsampling at each map node is performed with different\nparameters to ensure that no two HOMER instances receive identical inputs. The\noutput of the map phase and the input of the reduce phase is a list of Motifs\ndiscovered using the sub-sampled sequences. The reduce phase calculates the\nmode, most frequent Motifs, and outputs them as the final discovered Motifs. We\nfound marginal speed gains with this model of execution and substantial amount\nof quality loss in discovered Motifs.", 
    "link": "http://arxiv.org/pdf/1405.0354v1", 
    "arxiv-id": "1405.0354v1"
},{
    "category": "cs.CR", 
    "author": "Nicolas T. Courtois", 
    "title": "On The Longest Chain Rule and Programmed Self-Destruction of Crypto   Currencies", 
    "publish": "2014-05-02T22:58:02Z", 
    "summary": "In this paper we revisit some major orthodoxies which lie at the heart of the\nbitcoin crypto currency and its numerous clones. In particular we look at The\nLongest Chain Rule, the monetary supply policies and the exact mechanisms which\nimplement them. We claim that these built-in properties are not as brilliant as\nthey are sometimes claimed. A closer examination reveals that they are closer\nto being... engineering mistakes which other crypto currencies have copied\nrather blindly. More precisely we show that the capacity of current crypto\ncurrencies to resist double spending attacks is poor and most current crypto\ncurrencies are highly vulnerable. Satoshi did not implement a timestamp for\nbitcoin transactions and the bitcoin software does not attempt to monitor\ndouble spending events. As a result major attacks involving hundreds of\nmillions of dollars can occur and would not even be recorded. Hundreds of\nmillions have been invested to pay for ASIC hashing infrastructure yet\ninsufficient attention was paid to network neutrality and to insure that the\nprotection layer it promises is effective and cannot be abused. In this paper\nwe develop a theory of Programmed Self-Destruction of crypto currencies. We\nobserve that most crypto currencies have mandated abrupt and sudden\ntransitions. These affect their hash rate and therefore their protection\nagainst double spending attacks which we do not limit the to the notion of 51%\nattacks which is highly misleading. In addition we show that smaller bitcoin\ncompetitors are substantially more vulnerable. In addition to small hash rate,\nmany bitcoin competitors mandate incredibly important adjustments in miner\nreward. We exhibit examples of 'alt-coins' which validate our theory and for\nwhich the process of programmed decline and rapid self-destruction has clearly\nalready started.", 
    "link": "http://arxiv.org/pdf/1405.0534v11", 
    "arxiv-id": "1405.0534v11"
},{
    "category": "cs.CE", 
    "author": "Eman AboElhamd", 
    "title": "Classification of Diabetes Mellitus using Modified Particle Swarm   Optimization and Least Squares Support Vector Machine", 
    "publish": "2014-05-03T02:31:07Z", 
    "summary": "Diabetes Mellitus is a major health problem all over the world. Many\nclassification algorithms have been applied for its diagnoses and treatment. In\nthis paper, a hybrid algorithm of Modified-Particle Swarm Optimization and\nLeast Squares- Support Vector Machine is proposed for the classification of\ntype II DM patients. LS-SVM algorithm is used for classification by finding\noptimal hyper-plane which separates various classes. Since LS-SVM is so\nsensitive to the changes of its parameter values, Modified-PSO algorithm is\nused as an optimization technique for LS-SVM parameters. This will Guarantee\nthe robustness of the hybrid algorithm by searching for the optimal values for\nLS-SVM parameters. The pro-posed Algorithm is implemented and evaluated using\nPima Indians Diabetes Data set from UCI repository of machine learning\ndatabases. It is also compared with different classifier algorithms which were\napplied on the same database. The experimental results showed the superiority\nof the proposed algorithm which could achieve an average classification\naccuracy of 97.833%.", 
    "link": "http://arxiv.org/pdf/1405.0549v1", 
    "arxiv-id": "1405.0549v1"
},{
    "category": "cs.CE", 
    "author": "Ashish Garg", 
    "title": "Numerical Investigation of Effects of Compound Angle and Length to   Diameter Ratio on Adiabatic Film Cooling Effectiveness", 
    "publish": "2014-05-03T08:15:32Z", 
    "summary": "A modification has been done in the normal injection hole of 35 degree, by\ninjecting the cold fluid at different angles(compound angle) in lateral\ndirection, providing a significant change in the shape of holes which later we\nfound in our numerical investigation giving good quality of effectiveness in\ncooling. Different L/D ratios are also studied for each compound angle. The\nnumerical simulation is performed based on Reynolds Averaged\nNavier-Stokes(RANS) equations with k-epsilon turbulence model by using\nFluent(Commercial Software). Adiabatic Film Cooling Effectiveness has been\nstudied for compound angles of (0, 30, 45 and 60 degrees) and L/D ratios of (1,\n2, 3 and 4) on a hole of 6mm diameter with blowing ratio 0.5. The findings are\nobtained from the results, concludes that the trend of laterally averaged\nadiabatic effectiveness is the function of L/D ratio and compound angle.", 
    "link": "http://arxiv.org/pdf/1405.0560v1", 
    "arxiv-id": "1405.0560v1"
},{
    "category": "cs.CE", 
    "author": "Simon Kramer", 
    "title": "A Galois-Connection between Cattell's and Szondi's Personality Profiles", 
    "publish": "2014-05-05T12:45:11Z", 
    "summary": "We propose a computable Galois-connection between, on the one hand, Cattell's\n16-Personality-Factor (16PF) Profiles, one of the most comprehensive and\nwidely-used personality measures for non-psychiatric populations and their\ncontaining PsychEval Personality Profiles (PPPs) for psychiatric populations,\nand, on the other hand, Szondi's personality profiles (SPPs), a less well-known\nbut, as we show, finer personality measure for psychiatric as well as\nnon-psychiatric populations (conceived as a unification of the depth psychology\nof S. Freud, C.G. Jung, and A. Adler). The practical significance of our result\nis that our Galois-connection provides a pair of computable, interpreting\ntranslations between the two personality spaces of PPPs (containing the 16PFs)\nand SPPs: one concrete from PPP-space to SPP-space (because SPPs are finer than\nPPPs) and one abstract from SPP-space to PPP-space (because PPPs are coarser\nthan SPPs). Thus Cattell's and Szondi's personality-test results are mutually\ninterpretable and inter-translatable, even automatically by computers.", 
    "link": "http://arxiv.org/pdf/1405.0877v1", 
    "arxiv-id": "1405.0877v1"
},{
    "category": "cs.CE", 
    "author": "Sumaira Tasnim", 
    "title": "Application of Machine Learning Techniques in Aquaculture", 
    "publish": "2014-05-03T14:26:42Z", 
    "summary": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", 
    "link": "http://arxiv.org/pdf/1405.1304v1", 
    "arxiv-id": "1405.1304v1"
},{
    "category": "cs.SY", 
    "author": "Pabitra Pal Choudhury", 
    "title": "On Analysis and Generation of some Biologically Important Boolean   Functions", 
    "publish": "2014-05-09T15:51:32Z", 
    "summary": "Boolean networks are used to model biological networks such as gene\nregulatory networks. Often Boolean networks show very chaotic behaviour which\nis sensitive to any small perturbations. In order to reduce the chaotic\nbehaviour and to attain stability in the gene regulatory network, nested\nCanalizing Functions (NCFs) are best suited. NCFs and its variants have a wide\nrange of applications in systems biology. Previously, many works were done on\nthe application of canalizing functions, but there were fewer methods to check\nif any arbitrary Boolean function is canalizing or not. In this paper, by using\nKarnaugh Map this problem is solved and also it has been shown that when the\ncanalizing functions of variable is given, all the canalizing functions of\nvariable could be generated by the method of concatenation. In this paper we\nhave uniquely identified the number of NCFs having a particular Hamming\nDistance (H.D) generated by each variable as starting canalizing input.\nPartially NCFs of 4 variables has also been studied in this paper.", 
    "link": "http://arxiv.org/pdf/1405.2271v2", 
    "arxiv-id": "1405.2271v2"
},{
    "category": "cs.SY", 
    "author": "Bertrand Corn\u00e9lusse", 
    "title": "Active network management for electrical distribution systems: problem   formulation, benchmark, and approximate solution", 
    "publish": "2014-05-12T15:31:34Z", 
    "summary": "With the increasing share of renewable and distributed generation in\nelectrical distribution systems, Active Network Management (ANM) becomes a\nvaluable option for a distribution system operator to operate his system in a\nsecure and cost-effective way without relying solely on network reinforcement.\nANM strategies are short-term policies that control the power injected by\ngenerators and/or taken off by loads in order to avoid congestion or voltage\nissues. Advanced ANM strategies imply that the system operator has to solve\nlarge-scale optimal sequential decision-making problems under uncertainty. For\nexample, decisions taken at a given moment constrain the future decisions that\ncan be taken and uncertainty must be explicitly accounted for because neither\ndemand nor generation can be accurately forecasted. We first formulate the ANM\nproblem, which in addition to be sequential and uncertain, has a nonlinear\nnature stemming from the power flow equations and a discrete nature arising\nfrom the activation of power modulation signals. This ANM problem is then cast\nas a stochastic mixed-integer nonlinear program, as well as second-order cone\nand linear counterparts, for which we provide quantitative results using state\nof the art solvers and perform a sensitivity analysis over the size of the\nsystem, the amount of available flexibility, and the number of scenarios\nconsidered in the deterministic equivalent of the stochastic program. To foster\nfurther research on this problem, we make available at\nhttp://www.montefiore.ulg.ac.be/~anm/ three test beds based on distribution\nnetworks of 5, 33, and 77 buses. These test beds contain a simulator of the\ndistribution system, with stochastic models for the generation and consumption\ndevices, and callbacks to implement and test various ANM strategies.", 
    "link": "http://arxiv.org/pdf/1405.2806v6", 
    "arxiv-id": "1405.2806v6"
},{
    "category": "cs.DB", 
    "author": "Fabio Porto", 
    "title": "$\u03a5$-DB: Managing scientific hypotheses as uncertain data", 
    "publish": "2014-05-19T05:09:50Z", 
    "summary": "In view of the paradigm shift that makes science ever more data-driven, we\nconsider deterministic scientific hypotheses as uncertain data. This vision\ncomprises a probabilistic database (p-DB) design methodology for the systematic\nconstruction and management of U-relational hypothesis DBs, viz.,\n$\\Upsilon$-DBs. It introduces hypothesis management as a promising new class of\napplications for p-DBs. We illustrate the potential of $\\Upsilon$-DB as a tool\nfor deep predictive analytics.", 
    "link": "http://arxiv.org/pdf/1405.4607v1", 
    "arxiv-id": "1405.4607v1"
},{
    "category": "cs.NE", 
    "author": "Ahmed I. Taloba", 
    "title": "An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study", 
    "publish": "2014-02-27T11:03:28Z", 
    "summary": "Clustering analysis plays an important role in scientific research and\ncommercial application. K-means algorithm is a widely used partition method in\nclustering. However, it is known that the K-means algorithm may get stuck at\nsuboptimal solutions, depending on the choice of the initial cluster centers.\nIn this article, we propose a technique to handle large scale data, which can\nselect initial clustering center purposefully using Genetic algorithms (GAs),\nreduce the sensitivity to isolated point, avoid dissevering big cluster, and\novercome deflexion of data in some degree that caused by the disproportion in\ndata partitioning owing to adoption of multi-sampling. We applied our method to\nsome public datasets these show the advantages of the proposed approach for\nexample Hepatitis C dataset that has been taken from the machine learning\nwarehouse of University of California. Our aim is to evaluate hepatitis\ndataset. In order to evaluate this dataset we did some preprocessing operation,\nthe reason to preprocessing is to summarize the data in the best and suitable\nway for our algorithm. Missing values of the instances are adjusted using local\nmean method.", 
    "link": "http://arxiv.org/pdf/1405.6173v1", 
    "arxiv-id": "1405.6173v1"
},{
    "category": "q-fin.ST", 
    "author": "Tiziana Di Matteo", 
    "title": "Relation between Financial Market Structure and the Real Economy:   Comparison between Clustering Methods", 
    "publish": "2014-06-02T20:38:32Z", 
    "summary": "We quantify the amount of information filtered by different hierarchical\nclustering methods on correlations between stock returns comparing it with the\nunderlying industrial activity structure. Specifically, we apply, for the first\ntime to financial data, a novel hierarchical clustering approach, the Directed\nBubble Hierarchical Tree and we compare it with other methods including the\nLinkage and k-medoids. In particular, by taking the industrial sector\nclassification of stocks as a benchmark partition, we evaluate how the\ndifferent methods retrieve this classification. The results show that the\nDirected Bubble Hierarchical Tree can outperform other methods, being able to\nretrieve more information with fewer clusters. Moreover, we show that the\neconomic information is hidden at different levels of the hierarchical\nstructures depending on the clustering method. The dynamical analysis on a\nrolling window also reveals that the different methods show different degrees\nof sensitivity to events affecting financial markets, like crises. These\nresults can be of interest for all the applications of clustering methods to\nportfolio optimization and risk hedging.", 
    "link": "http://arxiv.org/pdf/1406.0496v2", 
    "arxiv-id": "1406.0496v2"
},{
    "category": "cs.CE", 
    "author": "Livia King", 
    "title": "ACO Implementation for Sequence Alignment with Genetic Algorithms", 
    "publish": "2014-06-04T02:44:57Z", 
    "summary": "In this paper, we implement Ant Colony Optimization (ACO) for sequence\nalignment. ACO is a meta-heuristic recently developed for nearest neighbor\napproximations in large, NP-hard search spaces. Here we use a genetic algorithm\napproach to evolve the best parameters for an ACO designed to align two\nsequences. We then used the best parameters found to interpolate approximate\noptimal parameters for a given string length within a range. The basis of our\ncomparison is the alignment given by the Needleman-Wunsch algorithm. We found\nthat ACO can indeed be applied to sequence alignment. While it is\ncomputationally expensive compared to other equivalent algorithms, it is a\npromising algorithm that can be readily applied to a variety of other\nbiological problems.", 
    "link": "http://arxiv.org/pdf/1406.0930v1", 
    "arxiv-id": "1406.0930v1"
},{
    "category": "cs.CE", 
    "author": "Richard D. Gitlin", 
    "title": "An Innovative Wireless Cardiac Rhythm Management (iCRM) System", 
    "publish": "2014-06-03T19:29:04Z", 
    "summary": "In this paper, we propose a wireless Communicator to manage and enhance a\nCardiac Rhythm Management System. The system includes: (1) an on-body wireless\nElectrocardiogram (ECG), (2) an Intracardiac Electrogram (EGM) embedded inside\nan Implantable Cardioverter/Defibrillator, and (3) a Communicator (with a\nresident Learning System). The first two devices are existing technology\navailable in the market and are emulated using data from the Physionet\ndatabase, while the Communicator was designed and implemented by our research\nteam. The value of the information obtained by combining the information\nsupplied by (1) and (2), presented to the Communicator, improves decision\nmaking regarding use of the actuator or other actions. Preliminary results show\na high level of confidence in the decisions made by the Communicator. For\nexample, excellent accuracy is achieved in predicting atrial arrhythmia in 8\npatients using only external ECG when we used a neural network.", 
    "link": "http://arxiv.org/pdf/1406.1062v1", 
    "arxiv-id": "1406.1062v1"
},{
    "category": "cs.CE", 
    "author": "Daniele de Rigo", 
    "title": "Connectivity of Natura 2000 forest sites in Europe", 
    "publish": "2014-06-05T10:13:04Z", 
    "summary": "Background/Purpose: In the context of the European Biodiversity policy, the\nGreen Infrastructure Strategy is one supporting tool to mitigate fragmentation,\ninter-alia to increase the spatial and functional connectivity between\nprotected and unprotected areas. The Joint Research Centre has developed an\nintegrated model to provide a macro-scale set of indices to evaluate the\nconnectivity of the Natura 2000 network, which forms the backbone of a Green\nInfrastructure for Europe. The model allows a wide assessment and comparison to\nbe performed across countries in terms of structural (spatially connected or\nisolated sites) and functional connectivity (least-cost distances between sites\ninfluenced by distribution, distance and land cover).\n  Main conclusion: The Natura 2000 network in Europe shows differences among\ncountries in terms of the sizes and numbers of sites, their distribution as\nwell as distances between sites. Connectivity has been assessed on the basis of\na 500 m average inter-site distance, roads and intensive land use as barrier\neffects as well as the presence of \"green\" corridors. In all countries the\nNatura 2000 network is mostly made of sites which are not physically connected.\nHighest functional connectivity values are found for Spain, Slovakia, Romania\nand Bulgaria. The more natural landscape in Sweden and Finland does not result\nin high inter-site network connectivity due to large inter-site distances. The\ndistribution of subnets with respect to roads explains the higher share of\nisolated subnets in Portugal than in Belgium.", 
    "link": "http://arxiv.org/pdf/1406.1501v2", 
    "arxiv-id": "1406.1501v2"
},{
    "category": "cs.NE", 
    "author": "Mahish Kohli", 
    "title": "Ant Colony Optimization for Inferring Key Gene Interactions", 
    "publish": "2014-06-06T10:06:35Z", 
    "summary": "Inferring gene interaction network from gene expression data is an important\ntask in systems biology research. The gene interaction network, especially key\ninteractions, plays an important role in identifying biomarkers for disease\nthat further helps in drug design. Ant colony optimization is an optimization\nalgorithm based on natural evolution and has been used in many optimization\nproblems. In this paper, we applied ant colony optimization algorithm for\ninferring the key gene interactions from gene expression data. The algorithm\nhas been tested on two different kinds of benchmark datasets and observed that\nit successfully identify some key gene interactions.", 
    "link": "http://arxiv.org/pdf/1406.1626v1", 
    "arxiv-id": "1406.1626v1"
},{
    "category": "cs.CL", 
    "author": "Ting Wang", 
    "title": "Automatic Extraction of Protein Interaction in Literature", 
    "publish": "2014-06-08T07:41:07Z", 
    "summary": "Protein-protein interaction extraction is the key precondition of the\nconstruction of protein knowledge network, and it is very important for the\nresearch in the biomedicine. This paper extracted directional protein-protein\ninteraction from the biological text, using the SVM-based method. Experiments\nwere evaluated on the LLL05 corpus with good results. The results show that\ndependency features are import for the protein-protein interaction extraction\nand features related to the interaction word are effective for the interaction\ndirection judgment. At last, we analyzed the effects of different features and\nplaned for the next step.", 
    "link": "http://arxiv.org/pdf/1406.1953v2", 
    "arxiv-id": "1406.1953v2"
},{
    "category": "cs.CE", 
    "author": "Marco Antonio Dorantes Gonzalez", 
    "title": "Tollan-Xicocotitlan: A reconstructed City by augmented reality", 
    "publish": "2014-06-19T18:57:03Z", 
    "summary": "This project presents the analysis, design, implementation and results of\nReconstruction Xicocotitlan Tollan-through augmented reality, which will\nrelease information about the Toltec culture supplemented by presenting an\noverview of the main premises of the Xicocotitlan Tollan city supported\ndimensional models based on the augmented reality technique showing the user a\nvirtual representation of buildings in Tollan.", 
    "link": "http://arxiv.org/pdf/1406.5151v1", 
    "arxiv-id": "1406.5151v1"
},{
    "category": "cs.CE", 
    "author": "Piero Triverio", 
    "title": "Structure-Preserving Reduction of Finite-Difference Time-Domain   Equations with Controllable Stability Beyond the CFL Limit", 
    "publish": "2014-06-26T22:50:43Z", 
    "summary": "The timestep of the Finite-Difference Time-Domain method (FDTD) is\nconstrained by the stability limit known as the Courant-Friedrichs-Lewy (CFL)\ncondition. This limit can make FDTD simulations quite time consuming for\nstructures containing small geometrical details. Several methods have been\nproposed in the literature to extend the CFL limit, including implicit FDTD\nmethods and filtering techniques. In this paper, we propose a novel approach\nwhich combines model order reduction and a perturbation algorithm to accelerate\nFDTD simulations beyond the CFL barrier. We compare the proposed algorithm\nagainst existing implicit and explicit CFL extension techniques, demonstrating\nincreased accuracy and performance on a large number of test cases, including\nresonant cavities, a waveguide structure, a focusing metascreen and a\nmicrostrip filter.", 
    "link": "http://arxiv.org/pdf/1406.7042v1", 
    "arxiv-id": "1406.7042v1"
},{
    "category": "cs.CE", 
    "author": "Ru Zhu", 
    "title": "Speedup of Micromagnetic Simulations with C++ AMP On Graphics Processing   Units", 
    "publish": "2014-06-29T04:27:06Z", 
    "summary": "A finite-difference Micromagnetic solver is presented utilizing the C++\nAccelerated Massive Parallelism (C++ AMP). The high speed performance of a\nsingle Graphics Processing Unit (GPU) is demonstrated compared to a typical\nCPU-based solver. The speed-up of GPU to CPU is shown to be greater than 100\nfor problems with larger sizes. This solver is based on C++ AMP and can run on\nGPUs from various hardware vendors, such as NVIDIA, AMD and Intel, regardless\nof whether it is dedicated or integrated graphics processor.", 
    "link": "http://arxiv.org/pdf/1406.7459v1", 
    "arxiv-id": "1406.7459v1"
},{
    "category": "cs.CE", 
    "author": "Kee-Chaing Chua", 
    "title": "A Statistical Modelling and Analysis of PHEVs' Power Demand in Smart   Grids", 
    "publish": "2014-07-07T04:12:47Z", 
    "summary": "Electric vehicles (EVs) and particularly plug-in hybrid electric vehicles\n(PHEVs) are foreseen to become popular in the near future. Not only are they\nmuch more environmentally friendly than conventional internal combustion engine\n(ICE) vehicles, their fuel can also be catered from diverse energy sources and\nresources. However, they add significant load on the power grid as they become\nwidespread. The characteristics of this extra load follow the patterns of\npeople's driving behaviours. In particular, random parameters such as arrival\ntime and driven distance of the vehicles determine their expected demand\nprofile from the power grid. In this paper, we first present a model for\nuncoordinated charging power demand of PHEVs based on a stochastic process and\naccordingly we characterize the EV's expected daily power demand profile. Next,\nwe adopt different distributions for the EV's charging time following some\navailable empirical research data in the literature. Simulation results show\nthat the EV's expected daily power demand profiles obtained under the uniform,\nGaussian with positive support and Rician distributions for charging time are\nidentical when the first and second order statistics of these distributions are\nthe same. This gives us useful insights into the long-term planning for\nupgrading power systems' infrastructure to accommodate PHEVs. In addition, the\nresults from this modelling can be incorporated into designing demand response\n(DR) algorithms and evaluating the available DR techniques more accurately.", 
    "link": "http://arxiv.org/pdf/1407.1576v1", 
    "arxiv-id": "1407.1576v1"
},{
    "category": "cs.CE", 
    "author": "Ruth Nussinov", 
    "title": "MIMTool: A Tool for Drawing Molecular Interaction Maps", 
    "publish": "2014-07-08T13:23:17Z", 
    "summary": "Background: To understand protein function, it is important to study protein-\nprotein interaction networks. These networks can be represented in network\ndiagrams called protein interaction maps that can lead to better understanding\nby visualization. We address the problem of drawing of protein interactions in\nKohn's Molecular Interaction Map (MIM) notation. Even though there are some\nexisting tools for graphical visualization of protein interactions in general,\nthere is no tool that can draw protein interactions with MIM notation with full\nsupport. Results: MIMTool was developed for drawing protein interaction maps in\nKohn's MIM notation. MIMTool was developed using the Qt toolkit libraries and\nintroduces several unique features such as full interactivity, object dragging,\nability to export files in MIMML, SBML and line drawing with automatic bending\nand crossover minimization, which are not available in other diagram editors.\nMIMTool also has a unique orthogonal edge drawing method that is both easy and\nmore flexible than other orthogonal drawing methods present in other\ninteraction drawing tools. Conclusions: MIMTool facilitates faster drawing,\nupdating and exchanging of MIMs. Among its several features, it also includes a\nsemi-automatic drawing algorithm that makes use of shortest path algorithm for\nconstructing lines with small number of bends and crossings. MIMTool\ncontributes a much needed software tool that was missing and will facilitate\nwider adoption of Kohn's MIM notation.", 
    "link": "http://arxiv.org/pdf/1407.2073v1", 
    "arxiv-id": "1407.2073v1"
},{
    "category": "cs.CE", 
    "author": "Sonja J. Prohaska", 
    "title": "Analyzing Chromatin Using Tiled Binned Scatterplot Matrices", 
    "publish": "2014-07-08T13:57:28Z", 
    "summary": "Background: Over the last years, more and more biological data became\navailable. Besides the pure amount of new data, also its dimensionality - the\nnumber of different attributes per data point - increased. Recently, especially\nthe amount of data on chromatin and its modifications increased considerably.\nIn the field of epigenetics, appropriate visualization tools designed for\nhighlighting the different aspects of epigenetic data are currently not\navailable. Results: We present a tool called TiBi-Scatter enabling correlation\nanalysis in 2D. This approach allows for analyzing multidimensional data while\nkeeping the use of resources such as memory small. Thus, it is in particular\napplicable to large data sets. Conclusions: TiBi-Scatter is a resource-friendly\nand easy to use tool that allows for the hypothesis-free analysis of large\nmultidimensional biological data sets.", 
    "link": "http://arxiv.org/pdf/1407.2084v1", 
    "arxiv-id": "1407.2084v1"
},{
    "category": "cs.CE", 
    "author": "Kay Nieselt", 
    "title": "inPHAP: Interactive visualization of genotype and phased haplotype data", 
    "publish": "2014-07-08T14:14:18Z", 
    "summary": "Background: To understand individual genomes it is necessary to look at the\nvariations that lead to changes in phenotype and possibly to disease. However,\ngenotype information alone is often not sufficient and additional knowledge\nregarding the phase of the variation is needed to make correct interpretations.\nInteractive visualizations, that allow the user to explore the data in various\nways, can be of great assistance in the process of making well informed\ndecisions. But, currently there is a lack for visualizations that are able to\ndeal with phased haplotype data. Results: We present inPHAP, an interactive\nvisualization tool for genotype and phased haplotype data. inPHAP features a\nvariety of interaction possibilities such as zooming, sorting, filtering and\naggregation of rows in order to explore patterns hidden in large genetic data\nsets. As a proof of concept, we apply inPHAP to the phased haplotype data set\nof Phase 1 of the 1000 Genomes Project. Thereby, inPHAP's ability to show\ngenetic variations on the population as well as on the individuals level is\ndemonstrated for several disease related loci. Conclusions: As of today, inPHAP\nis the only visual analytical tool that allows the user to explore unphased and\nphased haplotype data interactively. Due to its highly scalable design, inPHAP\ncan be applied to large datasets with up to 100 GB of data, enabling users to\nvisualize even large scale input data. inPHAP closes the gap between common\nvisualization tools for unphased genotype data and introduces several new\nfeatures, such as the visualization of phased data.", 
    "link": "http://arxiv.org/pdf/1407.2098v1", 
    "arxiv-id": "1407.2098v1"
},{
    "category": "cs.CE", 
    "author": "Jim Austin", 
    "title": "An Algorithm for Alignment-free Sequence Comparison using Logical Match", 
    "publish": "2014-07-06T17:00:53Z", 
    "summary": "This paper proposes an algorithm for alignment-free sequence comparison using\nLogical Match. Here, we compute the score using fuzzy membership values which\ngenerate automatically from the number of matches and mismatches. We\ndemonstrate the method with both the artificial and real datum. The results\nshow the uniqueness of the proposed method by analyzing DNA sequences taken\nfrom NCBI databank with a novel computational time.", 
    "link": "http://arxiv.org/pdf/1407.2237v1", 
    "arxiv-id": "1407.2237v1"
},{
    "category": "cs.CE", 
    "author": "Anna Wojtowicz", 
    "title": "A domain-specific analysis system for examining nuclear reactor   simulation data for light-water and sodium-cooled fast reactors", 
    "publish": "2014-07-10T14:04:25Z", 
    "summary": "Building a new generation of fission reactors in the United States presents\nmany technical and regulatory challenges. One important challenge is the need\nto share and present results from new high-fidelity, high-performance\nsimulations in an easily usable way. Since modern multiscale, multi-physics\nsimulations can generate petabytes of data, they will require the development\nof new techniques and methods to reduce the data to familiar quantities of\ninterest (e.g., pin powers, temperatures) with a more reasonable resolution and\nsize. Furthermore, some of the results from these simulations may be new\nquantities for which visualization and analysis techniques are not immediately\navailable in the community and need to be developed.\n  This paper describes a new system for managing high-performance simulation\nresults in a domain-specific way that naturally exposes quantities of interest\nfor light water and sodium-cooled fast reactors. It describes requirements to\nbuild such a system and the technical challenges faced in its development at\nall levels (simulation, user interface, etc.). An example comparing results\nfrom two different simulation suites for a single assembly in a light-water\nreactor is presented, along with a detailed discussion of the system's\nrequirements and design.", 
    "link": "http://arxiv.org/pdf/1407.2795v1", 
    "arxiv-id": "1407.2795v1"
},{
    "category": "cs.DC", 
    "author": "Konstantinos G. Margaritis", 
    "title": "A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber   Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence   Database", 
    "publish": "2014-07-10T18:15:18Z", 
    "summary": "Multiple matching algorithms are used to locate the occurrences of patterns\nfrom a finite pattern set in a large input string. Aho-Corasick and Wu-Manber,\ntwo of the most well known algorithms for multiple matching require an\nincreased computing power, particularly in cases where large-size datasets must\nbe processed, as is common in computational biology applications. Over the past\nyears, Graphics Processing Units (GPUs) have evolved to powerful parallel\nprocessors outperforming Central Processing Units (CPUs) in scientific\ncalculations. Moreover, multiple GPUs can be used in parallel, forming hybrid\ncomputer cluster configurations to achieve an even higher processing\nthroughput. This paper evaluates the speedup of the parallel implementation of\nthe Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to\nprocess a snapshot of the Expressed Sequence Tags of the human genome and for\ndifferent problem parameters.", 
    "link": "http://arxiv.org/pdf/1407.2889v1", 
    "arxiv-id": "1407.2889v1"
},{
    "category": "cs.CE", 
    "author": "Luca Daniel", 
    "title": "Enabling High-Dimensional Hierarchical Uncertainty Quantification by   ANOVA and Tensor-Train Decomposition", 
    "publish": "2014-07-11T04:38:01Z", 
    "summary": "Hierarchical uncertainty quantification can reduce the computational cost of\nstochastic circuit simulation by employing spectral methods at different\nlevels. This paper presents an efficient framework to simulate hierarchically\nsome challenging stochastic circuits/systems that include high-dimensional\nsubsystems. Due to the high parameter dimensionality, it is challenging to both\nextract surrogate models at the low level of the design hierarchy and to handle\nthem in the high-level simulation. In this paper, we develop an efficient\nANOVA-based stochastic circuit/MEMS simulator to extract efficiently the\nsurrogate models at the low level. In order to avoid the curse of\ndimensionality, we employ tensor-train decomposition at the high level to\nconstruct the basis functions and Gauss quadrature points. As a demonstration,\nwe verify our algorithm on a stochastic oscillator with four MEMS capacitors\nand 184 random parameters. This challenging example is simulated efficiently by\nour simulator at the cost of only 10 minutes in MATLAB on a regular personal\ncomputer.", 
    "link": "http://arxiv.org/pdf/1407.3023v4", 
    "arxiv-id": "1407.3023v4"
},{
    "category": "cs.SE", 
    "author": "Samin Ishtiaq", 
    "title": "\"Can I Implement Your Algorithm?\": A Model for Reproducible Research   Software", 
    "publish": "2014-07-22T19:29:34Z", 
    "summary": "The reproduction and replication of novel results has become a major issue\nfor a number of scientific disciplines. In computer science and related\ncomputational disciplines such as systems biology, the issues closely revolve\naround the ability to implement novel algorithms and approaches. Taking an\napproach from the literature and applying it to a new codebase frequently\nrequires local knowledge missing from the published manuscripts and project\nwebsites. Alongside this issue, benchmarking, and the development of fair ---\nand widely available --- benchmark sets present another barrier.\n  In this paper, we outline several suggestions to address these issues, driven\nby specific examples from a range of scientific domains. Finally, based on\nthese suggestions, we propose a new open platform for scientific software\ndevelopment which effectively isolates specific dependencies from the\nindividual researcher and their workstation and allows faster, more powerful\nsharing of the results of scientific software engineering.", 
    "link": "http://arxiv.org/pdf/1407.5981v2", 
    "arxiv-id": "1407.5981v2"
},{
    "category": "q-bio.QM", 
    "author": "Nikos Vlassis", 
    "title": "Spectral Sequence Motif Discovery", 
    "publish": "2014-07-23T08:07:50Z", 
    "summary": "Sequence discovery tools play a central role in several fields of\ncomputational biology. In the framework of Transcription Factor binding\nstudies, motif finding algorithms of increasingly high performance are required\nto process the big datasets produced by new high-throughput sequencing\ntechnologies. Most existing algorithms are computationally demanding and often\ncannot support the large size of new experimental data. We present a new motif\ndiscovery algorithm that is built on a recent machine learning technique,\nreferred to as Method of Moments. Based on spectral decompositions, this method\nis robust under model misspecification and is not prone to locally optimal\nsolutions. We obtain an algorithm that is extremely fast and designed for the\nanalysis of big sequencing data. In a few minutes, we can process datasets of\nhundreds of thousand sequences and extract motif profiles that match those\ncomputed by various state-of-the-art algorithms.", 
    "link": "http://arxiv.org/pdf/1407.6125v2", 
    "arxiv-id": "1407.6125v2"
},{
    "category": "cs.CV", 
    "author": "Brian C. Lovell", 
    "title": "Discovering Discriminative Cell Attributes for HEp-2 Specimen Image   Classification", 
    "publish": "2014-07-28T06:03:03Z", 
    "summary": "Recently, there has been a growing interest in developing Computer Aided\nDiagnostic (CAD) systems for improving the reliability and consistency of\npathology test results. This paper describes a novel CAD system for the\nAnti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol on\nHuman Epithelial Type 2 (HEp-2) cells. While prior works have primarily focused\non classifying cell images extracted from ANA specimen images, this work takes\na further step by focussing on the specimen image classification problem\nitself. Our system is able to efficiently classify specimen images as well as\nproducing meaningful descriptions of ANA pattern class which helps physicians\nto understand the differences between various ANA patterns. We achieve this\ngoal by designing a specimen-level image descriptor that: (1) is highly\ndiscriminative; (2) has small descriptor length and (3) is semantically\nmeaningful at the cell level. In our work, a specimen image descriptor is\nrepresented by its overall cell attribute descriptors. As such, we propose two\nmax-margin based learning schemes to discover cell attributes whilst still\nmaintaining the discrimination of the specimen image descriptor. Our learning\nschemes differ from the existing discriminative attribute learning approaches\nas they primarily focus on discovering image-level attributes. Comparative\nevaluations were undertaken to contrast the proposed approach to various\nstate-of-the-art approaches on a novel HEp-2 cell dataset which was\nspecifically proposed for the specimen-level classification. Finally, we\nshowcase the ability of the proposed approach to provide textual descriptions\nto explain ANA patterns.", 
    "link": "http://arxiv.org/pdf/1407.7330v1", 
    "arxiv-id": "1407.7330v1"
},{
    "category": "physics.flu-dyn", 
    "author": "John M. Stockie", 
    "title": "Simulating flexible fiber suspensions using a scalable immersed boundary   algorithm", 
    "publish": "2014-07-28T19:43:11Z", 
    "summary": "We present an approach for numerically simulating the dynamics of flexible\nfibers in a three-dimensional shear flow using a scalable immersed boundary\n(IB) algorithm based on Guermond and Minev's pseudo-compressible fluid solver.\nThe fibers are treated as one-dimensional Kirchhoff rods that resist\nstretching, bending, and twisting, within the generalized IB framework. We\nperform a careful numerical comparison against experiments on single fibers\nperformed by S. G. Mason and co-workers, who categorized the fiber dynamics\ninto several distinct orbit classes. We show that the orbit class may be\ndetermined using a single dimensionless parameter for low Reynolds flows.\nLastly, we simulate dilute suspensions containing up to hundreds of fibers\nusing a distributed- memory computer cluster. These simulations serve as a\nstepping stone for studying more complex suspension dynamics including\nnon-dilute suspensions and aggregation of fibers (also known as flocculation).", 
    "link": "http://arxiv.org/pdf/1407.7514v1", 
    "arxiv-id": "1407.7514v1"
},{
    "category": "cs.CE", 
    "author": "Michael R. Metel", 
    "title": "Chance Constrained Optimization for Targeted Internet Advertising", 
    "publish": "2014-07-30T02:33:57Z", 
    "summary": "We introduce a chance constrained optimization model for the fulfillment of\nguaranteed display Internet advertising campaigns. The proposed formulation for\nthe allocation of display inventory takes into account the uncertainty of the\nsupply of Internet viewers. We discuss and present theoretical and\ncomputational features of the model via Monte Carlo sampling and convex\napproximations. Theoretical upper and lower bounds are presented along with a\nnumerical substantiation.", 
    "link": "http://arxiv.org/pdf/1407.7924v1", 
    "arxiv-id": "1407.7924v1"
},{
    "category": "cs.LG", 
    "author": "Jieping Ye", 
    "title": "Stochastic Coordinate Coding and Its Application for Drosophila Gene   Expression Pattern Annotation", 
    "publish": "2014-07-30T18:04:20Z", 
    "summary": "\\textit{Drosophila melanogaster} has been established as a model organism for\ninvestigating the fundamental principles of developmental gene interactions.\nThe gene expression patterns of \\textit{Drosophila melanogaster} can be\ndocumented as digital images, which are annotated with anatomical ontology\nterms to facilitate pattern discovery and comparison. The automated annotation\nof gene expression pattern images has received increasing attention due to the\nrecent expansion of the image database. The effectiveness of gene expression\npattern annotation relies on the quality of feature representation. Previous\nstudies have demonstrated that sparse coding is effective for extracting\nfeatures from gene expression images. However, solving sparse coding remains a\ncomputationally challenging problem, especially when dealing with large-scale\ndata sets and learning large size dictionaries. In this paper, we propose a\nnovel algorithm to solve the sparse coding problem, called Stochastic\nCoordinate Coding (SCC). The proposed algorithm alternatively updates the\nsparse codes via just a few steps of coordinate descent and updates the\ndictionary via second order stochastic gradient descent. The computational cost\nis further reduced by focusing on the non-zero components of the sparse codes\nand the corresponding columns of the dictionary only in the updating procedure.\nThus, the proposed algorithm significantly improves the efficiency and the\nscalability, making sparse coding applicable for large-scale data sets and\nlarge dictionary sizes. Our experiments on Drosophila gene expression data sets\ndemonstrate the efficiency and the effectiveness of the proposed algorithm.", 
    "link": "http://arxiv.org/pdf/1407.8147v2", 
    "arxiv-id": "1407.8147v2"
},{
    "category": "cs.LG", 
    "author": "Richard Hubbard", 
    "title": "Comparison of algorithms that detect drug side effects using electronic   healthcare databases", 
    "publish": "2014-09-02T15:16:26Z", 
    "summary": "The electronic healthcare databases are starting to become more readily\navailable and are thought to have excellent potential for generating adverse\ndrug reaction signals. The Health Improvement Network (THIN) database is an\nelectronic healthcare database containing medical information on over 11\nmillion patients that has excellent potential for detecting ADRs. In this paper\nwe apply four existing electronic healthcare database signal detecting\nalgorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the\nTHIN database for a selection of drugs from six chosen drug families. This is\nthe first comparison of ADR signalling algorithms that includes MUTARA and HUNT\nand enabled us to set a benchmark for the adverse drug reaction signalling\nability of the THIN database. The drugs were selectively chosen to enable a\ncomparison with previous work and for variety. It was found that no algorithm\nwas generally superior and the algorithms' natural thresholds act at variable\nstringencies. Furthermore, none of the algorithms perform well at detecting\nrare ADRs.", 
    "link": "http://arxiv.org/pdf/1409.0748v1", 
    "arxiv-id": "1409.0748v1"
},{
    "category": "cs.MA", 
    "author": "Uwe Aickelin", 
    "title": "Comparing Stochastic Differential Equations and Agent-Based Modelling   and Simulation for Early-stage Cancer", 
    "publish": "2014-09-02T15:36:00Z", 
    "summary": "There is great potential to be explored regarding the use of agent-based\nmodelling and simulation as an alternative paradigm to investigate early-stage\ncancer interactions with the immune system. It does not suffer from some\nlimitations of ordinary differential equation models, such as the lack of\nstochasticity, representation of individual behaviours rather than aggregates\nand individual memory. In this paper we investigate the potential contribution\nof agent-based modelling and simulation when contrasted with stochastic\nversions of ODE models using early-stage cancer examples. We seek answers to\nthe following questions: (1) Does this new stochastic formulation produce\nsimilar results to the agent-based version? (2) Can these methods be used\ninterchangeably? (3) Do agent-based models outcomes reveal any benefit when\ncompared to the Gillespie results? To answer these research questions we\ninvestigate three well-established mathematical models describing interactions\nbetween tumour cells and immune elements. These case studies were\nre-conceptualised under an agent-based perspective and also converted to the\nGillespie algorithm formulation. Our interest in this work, therefore, is to\nestablish a methodological discussion regarding the usability of different\nsimulation approaches, rather than provide further biological insights into the\ninvestigated case studies. Our results show that it is possible to obtain\nequivalent models that implement the same mechanisms; however, the incapacity\nof the Gillespie algorithm to retain individual memory of past events affects\nthe similarity of some results. Furthermore, the emergent behaviour of ABMS\nproduces extra patters of behaviour in the system, which was not obtained by\nthe Gillespie algorithm.", 
    "link": "http://arxiv.org/pdf/1409.0758v1", 
    "arxiv-id": "1409.0758v1"
},{
    "category": "cs.LG", 
    "author": "Richard B. Hubbard", 
    "title": "A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect   Discovery", 
    "publish": "2014-09-02T16:00:23Z", 
    "summary": "Drugs are frequently prescribed to patients with the aim of improving each\npatient's medical state, but an unfortunate consequence of most prescription\ndrugs is the occurrence of undesirable side effects. Side effects that occur in\nmore than one in a thousand patients are likely to be signalled efficiently by\ncurrent drug surveillance methods, however, these same methods may take decades\nbefore generating signals for rarer side effects, risking medical morbidity or\nmortality in patients prescribed the drug while the rare side effect is\nundiscovered. In this paper we propose a novel computational meta-analysis\nframework for signalling rare side effects that integrates existing methods,\nknowledge from the web, metric learning and semi-supervised clustering. The\nnovel framework was able to signal many known rare and serious side effects for\nthe selection of drugs investigated, such as tendon rupture when prescribed\nCiprofloxacin or Levofloxacin, renal failure with Naproxen and depression\nassociated with Rimonabant. Furthermore, for the majority of the drug\ninvestigated it generated signals for rare side effects at a more stringent\nsignalling threshold than existing methods and shows the potential to become a\nfundamental part of post marketing surveillance to detect rare side effects.", 
    "link": "http://arxiv.org/pdf/1409.0768v1", 
    "arxiv-id": "1409.0768v1"
},{
    "category": "cs.LG", 
    "author": "Richard B. Hubbard", 
    "title": "Signalling Paediatric Side Effects using an Ensemble of Simple Study   Designs", 
    "publish": "2014-09-02T16:17:25Z", 
    "summary": "Background: Children are frequently prescribed medication off-label, meaning\nthere has not been sufficient testing of the medication to determine its safety\nor effectiveness. The main reason this safety knowledge is lacking is due to\nethical restrictions that prevent children from being included in the majority\nof clinical trials. Objective: The objective of this paper is to investigate\nwhether an ensemble of simple study designs can be implemented to signal\nacutely occurring side effects effectively within the paediatric population by\nusing historical longitudinal data. The majority of pharmacovigilance\ntechniques are unsupervised, but this research presents a supervised framework.\nMethods: Multiple measures of association are calculated for each drug and\nmedical event pair and these are used as features that are fed into a\nclassiffier to determine the likelihood of the drug and medical event pair\ncorresponding to an adverse drug reaction. The classiffier is trained using\nknown adverse drug reactions or known non-adverse drug reaction relationships.\nResults: The novel ensemble framework obtained a false positive rate of 0:149,\na sensitivity of 0:547 and a specificity of 0:851 when implemented on a\nreference set of drug and medical event pairs. The novel framework consistently\noutperformed each individual simple study design. Conclusion: This research\nshows that it is possible to exploit the mechanism of causality and presents a\nframework for signalling adverse drug reactions effectively.", 
    "link": "http://arxiv.org/pdf/1409.0772v1", 
    "arxiv-id": "1409.0772v1"
},{
    "category": "cs.LG", 
    "author": "Uwe Aickelin", 
    "title": "Feature selection in detection of adverse drug reactions from the Health   Improvement Network (THIN) database", 
    "publish": "2014-09-02T16:25:58Z", 
    "summary": "Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs\nare one of most common causes to withdraw some drugs from market. Prescription\nevent monitoring (PEM) is an important approach to detect the adverse drug\nreactions. The main problem to deal with this method is how to automatically\nextract the medical events or side effects from high-throughput medical events,\nwhich are collected from day to day clinical practice. In this study we propose\na novel concept of feature matrix to detect the ADRs. Feature matrix, which is\nextracted from big medical data from The Health Improvement Network (THIN)\ndatabase, is created to characterize the medical events for the patients who\ntake drugs. Feature matrix builds the foundation for the irregular and big\nmedical data. Then feature selection methods are performed on feature matrix to\ndetect the significant features. Finally the ADRs can be located based on the\nsignificant features. The experiments are carried out on three drugs:\nAtorvastatin, Alendronate, and Metoclopramide. Major side effects for each drug\nare detected and better performance is achieved compared to other computerized\nmethods. The detected ADRs are based on computerized methods, further\ninvestigation is needed.", 
    "link": "http://arxiv.org/pdf/1409.0775v1", 
    "arxiv-id": "1409.0775v1"
},{
    "category": "cs.LG", 
    "author": "Lindy Durrant", 
    "title": "Ensemble Learning of Colorectal Cancer Survival Rates", 
    "publish": "2014-09-02T16:52:16Z", 
    "summary": "In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. We build on\nexisting research on clustering and machine learning facets of this data to\ndemonstrate a role for an ensemble approach to highlighting patients with\nclearer prognosis parameters. Results for survival prediction using 3 different\napproaches are shown for a subset of the data which is most difficult to model.\nThe performance of each model individually is compared with subsets of the data\nwhere some agreement is reached for multiple models. Significant improvements\nin model accuracy on an unseen test set can be achieved for patients where\nagreement between models is achieved.", 
    "link": "http://arxiv.org/pdf/1409.0788v1", 
    "arxiv-id": "1409.0788v1"
},{
    "category": "cs.LG", 
    "author": "Tom Rodden", 
    "title": "Variability of Behaviour in Electricity Load Profile Clustering; Who   Does Things at the Same Time Each Day?", 
    "publish": "2014-09-03T11:42:33Z", 
    "summary": "UK electricity market changes provide opportunities to alter households'\nelectricity usage patterns for the benefit of the overall electricity network.\nWork on clustering similar households has concentrated on daily load profiles\nand the variability in regular household behaviours has not been considered.\nThose households with most variability in regular activities may be the most\nreceptive to incentives to change timing.\n  Whether using the variability of regular behaviour allows the creation of\nmore consistent groupings of households is investigated and compared with daily\nload profile clustering. 204 UK households are analysed to find repeating\npatterns (motifs). Variability in the time of the motif is used as the basis\nfor clustering households. Different clustering algorithms are assessed by the\nconsistency of the results.\n  Findings show that variability of behaviour, using motifs, provides more\nconsistent groupings of households across different clustering algorithms and\nallows for more efficient targeting of behaviour change interventions.", 
    "link": "http://arxiv.org/pdf/1409.1043v1", 
    "arxiv-id": "1409.1043v1"
},{
    "category": "cs.LG", 
    "author": "Jonathan M. Garibaldi", 
    "title": "Tuning a Multiple Classifier System for Side Effect Discovery using   Genetic Algorithms", 
    "publish": "2014-09-03T12:11:54Z", 
    "summary": "In previous work, a novel supervised framework implementing a binary\nclassifier was presented that obtained excellent results for side effect\ndiscovery. Interestingly, unique side effects were identified when different\nbinary classifiers were used within the framework, prompting the investigation\nof applying a multiple classifier system. In this paper we investigate tuning a\nside effect multiple classifying system using genetic algorithms. The results\nof this research show that the novel framework implementing a multiple\nclassifying system trained using genetic algorithms can obtain a higher partial\narea under the receiver operating characteristic curve than implementing a\nsingle classifier. Furthermore, the framework is able to detect side effects\nefficiently and obtains a low false positive rate.", 
    "link": "http://arxiv.org/pdf/1409.1053v1", 
    "arxiv-id": "1409.1053v1"
},{
    "category": "cs.DB", 
    "author": "Christian Wagner", 
    "title": "Comparison of Distance Metrics for Hierarchical Data in Medical   Databases", 
    "publish": "2014-09-03T12:19:19Z", 
    "summary": "Distance metrics are broadly used in different research areas and\napplications, such as bio-informatics, data mining and many other fields.\nHowever, there are some metrics, like pq-gram and Edit Distance used\nspecifically for data with a hierarchical structure. Other metrics used for\nnon-hierarchical data are the geometric and Hamming metrics. We have applied\nthese metrics to The Health Improvement Network (THIN) database which has some\nhierarchical data. The THIN data has to be converted into a tree-like structure\nfor the first group of metrics. For the second group of metrics, the data are\nconverted into a frequency table or matrix, then for all metrics, all distances\nare found and normalised. Based on this particular data set, our research\nquestion: which of these metrics is useful for THIN data? This paper compares\nthe metrics, particularly the pq-gram metric on finding the similarities of\npatients' data. It also investigates the similar patients who have the same\nclose distances as well as the metrics suitability for clustering the whole\npatient population. Our results show that the two groups of metrics perform\ndifferently as they represent different structures of the data. Nevertheless,\nall the metrics could represent some similar data of patients as well as\ndiscriminate sufficiently well in clustering the patient population using\n$k$-means clustering algorithm.", 
    "link": "http://arxiv.org/pdf/1409.1055v1", 
    "arxiv-id": "1409.1055v1"
},{
    "category": "cs.CE", 
    "author": "Richard B. Hubbard", 
    "title": "Attributes for Causal Inference in Longitudinal Observational Databases", 
    "publish": "2014-09-03T09:07:53Z", 
    "summary": "The pharmaceutical industry is plagued by the problem of side effects that\ncan occur anytime a prescribed medication is ingested. There has been a recent\ninterest in using the vast quantities of medical data available in longitudinal\nobservational databases to identify causal relationships between drugs and\nmedical events. Unfortunately the majority of existing post marketing\nsurveillance algorithms measure how dependant or associated an event is on the\npresence of a drug rather than measuring causality. In this paper we\ninvestigate potential attributes that can be used in causal inference to\nidentify side effects based on the Bradford-Hill causality criteria. Potential\nattributes are developed by considering five of the causality criteria and\nfeature selection is applied to identify the most suitable of these attributes\nfor detecting side effects. We found that attributes based on the specificity\ncriterion may improve side effect signalling algorithms but the experiment and\ndosage criteria attributes investigated in this paper did not offer sufficient\nadditional information.", 
    "link": "http://arxiv.org/pdf/1409.5774v1", 
    "arxiv-id": "1409.5774v1"
},{
    "category": "cs.CE", 
    "author": "Jacques M. Bahi", 
    "title": "Finding the Core-Genes of Chloroplasts", 
    "publish": "2014-09-22T23:26:30Z", 
    "summary": "Due to the recent evolution of sequencing techniques, the number of available\ngenomes is rising steadily, leading to the possibility to make large scale\ngenomic comparison between sets of close species. An interesting question to\nanswer is: what is the common functionality genes of a collection of species,\nor conversely, to determine what is specific to a given species when compared\nto other ones belonging in the same genus, family, etc. Investigating such\nproblem means to find both core and pan genomes of a collection of species,\n\\textit{i.e.}, genes in common to all the species vs. the set of all genes in\nall species under consideration. However, obtaining trustworthy core and pan\ngenomes is not an easy task, leading to a large amount of computation, and\nrequiring a rigorous methodology. Surprisingly, as far as we know, this\nmethodology in finding core and pan genomes has not really been deeply\ninvestigated. This research work tries to fill this gap by focusing only on\nchloroplastic genomes, whose reasonable sizes allow a deep study. To achieve\nthis goal, a collection of 99 chloroplasts are considered in this article. Two\nmethodologies have been investigated, respectively based on sequence\nsimilarities and genes names taken from annotation tools. The obtained results\nwill finally be evaluated in terms of biological relevance.", 
    "link": "http://arxiv.org/pdf/1409.6369v1", 
    "arxiv-id": "1409.6369v1"
},{
    "category": "cs.CE", 
    "author": "Ru Zhu", 
    "title": "Grace: a Cross-platform Micromagnetic Simulator On Graphics Processing   Units", 
    "publish": "2014-11-10T20:12:46Z", 
    "summary": "A micromagnetic simulator running on graphics processing unit (GPU) is\npresented. It achieves significant performance boost as compared to previous\ncentral processing unit (CPU) simulators, up to two orders of magnitude for\nlarge input problems. Different from GPU implementations of other research\ngroups, this simulator is developed with C++ Accelerated Massive Parallelism\n(C++ AMP) and is hardware platform compatible. It runs on GPU from venders\ninclude NVidia, AMD and Intel, which paved the way for fast micromagnetic\nsimulation on both high-end workstations with dedicated graphics cards and\nlow-end personal computers with integrated graphics card. A copy of the\nsimulator software is publicly available.", 
    "link": "http://arxiv.org/pdf/1411.2565v1", 
    "arxiv-id": "1411.2565v1"
},{
    "category": "cs.CE", 
    "author": "M. Vijaya Kumar", 
    "title": "Identification of Helicopter Dynamics based on Flight Data using Nature   Inspired Techniques", 
    "publish": "2014-11-12T17:29:49Z", 
    "summary": "The complexity of helicopter flight dynamics makes modeling and helicopter\nsystem identification a very difficult task. Most of the traditional techniques\nrequire a model structure to be defined apriori and in case of helicopter\ndynamics, this is difficult due to its complexity and the interplay between\nvarious subsystems.To overcome this difficulty, non-parametric approaches are\ncommonly adopted for helicopter system identification. Artificial Neural\nNetwork are a widely used class of algorithms for non-parametric system\nidentification, among them, the Nonlinear Auto Regressive eXogeneous input\nnetwork (NARX) model is very popular, but it also necessitates some in depth\nknowledge regarding the system being modeled. There have been many approaches\nproposed to circumvent this and yet still retain the advantageous\ncharacteristics. In this paper we carry out an extensive study of one such\nnewly proposed approach using a modified NARX model with a two tiered,\nexternally driven recurrent neural network architecture. This is coupled with\nan outer optimization routine for evolving the order of the system. This\ngeneric architecture is comprehensively explored to ascertain its usability and\ncritically asses its potential. Different instantiations of this architecture,\nbased on nature inspired computational techniques (Artificial Bee Colony,\nArtificial Immune System and Particle Swarm Optimization) are evaluated and\ncritically compared in this paper. Simulations have been carried out for\nidentifying the longitudinally uncoupled dynamics. Results of identification\nindicate a quite close correlation between the actual and the predicted\nresponse of the helicopter for all the models.", 
    "link": "http://arxiv.org/pdf/1411.3251v1", 
    "arxiv-id": "1411.3251v1"
},{
    "category": "cs.CE", 
    "author": "Philippe Karamian-Surville", 
    "title": "Measure of combined effects of morphological parameters of inclusions   within composite materials via stochastic homogenization to determine   effective mechanical properties", 
    "publish": "2014-11-14T20:44:35Z", 
    "summary": "In our previous papers we have described efficient and reliable methods of\ngeneration of representative volume elements (RVE) perfectly suitable for\nanalysis of composite materials via stochastic homogenization.\n  In this paper we profit from these methods to analyze the influence of the\nmorphology on the effective mechanical properties of the samples. More\nprecisely, we study the dependence of main mechanical characteristics of a\ncomposite medium on various parameters of the mixture of inclusions composed of\nspheres and cylinders. On top of that we introduce various imperfections to\ninclusions and observe the evolution of effective properties related to that.\n  The main computational approach used throughout the work is the FFT-based\nhomogenization technique, validated however by comparison with the direct\nfinite elements method. We give details on the features of the method and the\nvalidation campaign as well.\n  Keywords: Composite materials, Cylindrical and spherical reinforcements,\nMechanical properties, Stochastic homogenization.", 
    "link": "http://arxiv.org/pdf/1411.4037v2", 
    "arxiv-id": "1411.4037v2"
},{
    "category": "cs.NE", 
    "author": "M. Sohel Rahman", 
    "title": "GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem   for Protein Structures", 
    "publish": "2014-11-16T11:26:06Z", 
    "summary": "Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique to\npredict the native structure of proteins. However, NMR machines are only able\nto report approximate and partial distances between pair of atoms. To build the\nprotein structure one has to solve the Euclidean distance geometry problem\ngiven the incomplete interval distance data produced by NMR machines. In this\npaper, we propose a new genetic algorithm for solving the Euclidean distance\ngeometry problem for protein structure prediction given sparse NMR data. Our\ngenetic algorithm uses a greedy mutation operator to intensify the search, a\ntwin removal technique for diversification in the population and a random\nrestart method to recover stagnation. On a standard set of benchmark dataset,\nour algorithm significantly outperforms standard genetic algorithms.", 
    "link": "http://arxiv.org/pdf/1411.4246v1", 
    "arxiv-id": "1411.4246v1"
},{
    "category": "cs.CE", 
    "author": "B. Lacarriere", 
    "title": "Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand   Using Artificial Neural Network", 
    "publish": "2014-11-17T21:40:36Z", 
    "summary": "This paper presents the building heating demand prediction model with\noccupancy profile and operational heating power level characteristics in short\ntime horizon (a couple of days) using artificial neural network. In addition,\nnovel pseudo dynamic transitional model is introduced, which consider time\ndependent attributes of operational power level characteristics and its effect\nin the overall model performance is outlined. Pseudo dynamic model is applied\nto a case study of French Institution building and compared its results with\nstatic and other pseudo dynamic neural network models. The results show the\ncoefficients of correlation in static and pseudo dynamic neural network model\nof 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning\nphase, and 0.61 and 0.85 during the prediction phase respectively. Further,\northogonal array design is applied to the pseudo dynamic model to check the\nschedule of occupancy profile and operational heating power level\ncharacteristics. The results show the new schedule and provide the robust\ndesign for pseudo dynamic model. Due to prediction in short time horizon, it\nfinds application for Energy Services Company (ESCOs) to manage the heating\nload for dynamic control of heat production system.", 
    "link": "http://arxiv.org/pdf/1411.4679v1", 
    "arxiv-id": "1411.4679v1"
},{
    "category": "q-bio.GN", 
    "author": "Christina Boucher", 
    "title": "Misassembly Detection using Paired-End Sequence Reads and Optical   Mapping Data", 
    "publish": "2014-11-20T15:34:34Z", 
    "summary": "A crucial problem in genome assembly is the discovery and correction of\nmisassembly errors in draft genomes. We develop a method that will enhance the\nquality of draft genomes by identifying and removing misassembly errors using\npaired short read sequence data and optical mapping data. We apply our method\nto various assemblies of the loblolly pine and Francisella tularensis genomes.\nOur results demonstrate that we detect more than 54% of extensively\nmisassembled contigs and more than 60% of locally misassembed contigs in an\nassembly of Francisella tularensis, and between 31% and 100% of extensively\nmisassembled contigs and between 57% and 73% of locally misassembed contigs in\nthe assemblies of loblolly pine. MISSEQUEL can be downloaded at\nhttp://www.cs.colostate.edu/seq/.", 
    "link": "http://arxiv.org/pdf/1411.5890v1", 
    "arxiv-id": "1411.5890v1"
},{
    "category": "cs.CE", 
    "author": "Kenji Takeda", 
    "title": "Top Tips to Make Your Research Irreproducible", 
    "publish": "2015-03-31T23:02:11Z", 
    "summary": "It is an unfortunate convention of science that research should pretend to be\nreproducible; our top tips will help you mitigate this fussy conventionality,\nenabling you to enthusiastically showcase your irreproducible work.", 
    "link": "http://arxiv.org/pdf/1504.00062v2", 
    "arxiv-id": "1504.00062v2"
},{
    "category": "cs.CE", 
    "author": "J. B. Bell", 
    "title": "Achieving algorithmic resilience for temporal integration through   spectral deferred corrections", 
    "publish": "2015-04-06T17:26:32Z", 
    "summary": "Spectral deferred corrections (SDC) is an iterative approach for constructing\nhigher- order accurate numerical approximations of ordinary differential\nequations. SDC starts with an initial approximation of the solution defined at\na set of Gaussian or spectral collocation nodes over a time interval and uses\nan iterative application of lower-order time discretizations applied to a\ncorrection equation to improve the solution at these nodes. Each deferred\ncorrection sweep increases the formal order of accuracy of the method up to the\nlimit inherent in the accuracy defined by the collocation points. In this\npaper, we demonstrate that SDC is well suited to recovering from soft\n(transient) hardware faults in the data. A strategy where extra correction\niterations are used to recover from soft errors and provide algorithmic\nresilience is proposed. Specifically, in this approach the iteration is\ncontinued until the residual (a measure of the error in the approximation) is\nsmall relative to the residual on the first correction iteration and changes\nslowly between successive iterations. We demonstrate the effectiveness of this\nstrategy for both canonical test problems and a comprehen- sive situation\ninvolving a mature scientific application code that solves the reacting\nNavier-Stokes equations for combustion research.", 
    "link": "http://arxiv.org/pdf/1504.01329v1", 
    "arxiv-id": "1504.01329v1"
},{
    "category": "cs.CE", 
    "author": "Qiqi Wang", 
    "title": "The swept rule for breaking the latency barrier in time advancing PDEs", 
    "publish": "2015-04-06T16:00:32Z", 
    "summary": "This article investigates the swept rule of space-time domain decomposition,\nan idea to break the latency barrier via communicating less often when\nexplicitly solving time-dependent PDEs. The swept rule decomposes space and\ntime among computing nodes in ways that exploit the domains of influence and\nthe domain of dependency, making it possible to communicate once per many\ntimesteps without redundant computation. The article presents simple\ntheoretical analysis to the performance of the swept rule which then was shown\nto be accurate by conducting numerical experiments.", 
    "link": "http://arxiv.org/pdf/1504.01380v2", 
    "arxiv-id": "1504.01380v2"
},{
    "category": "q-bio.GN", 
    "author": "S. Cenk Sahinalp", 
    "title": "Joint Inference of Genome Structure and Content in Heterogeneous Tumour   Samples", 
    "publish": "2015-04-13T07:17:36Z", 
    "summary": "For a genomically unstable cancer, a single tumour biopsy will often contain\na mixture of competing tumour clones. These tumour clones frequently differ\nwith respect to their genomic content (copy number of each gene) and structure\n(order of genes on each chromosome). Modern bulk genome sequencing mixes the\nsignals of tumour clones and contaminating normal cells, complicating inference\nof genomic content and structure. We propose a method to unmix tumour and\ncontaminating normal signals and jointly predict genomic structure and content\nof each tumour clone. We use genome graphs to represent tumour clones, and\nmodel the likelihood of the observed reads given clones and mixing proportions.\nOur use of haplotype blocks allows us to accurately measure allele specific\nread counts, and infer allele specific copy number for each clone. The proposed\nmethod is a heuristic local search based on applying incremental, locally\noptimal modifications of the genome graphs. Using simulated data, we show that\nour method predicts copy counts and gene adjacencies with reasonable accuracy.", 
    "link": "http://arxiv.org/pdf/1504.03080v2", 
    "arxiv-id": "1504.03080v2"
},{
    "category": "cond-mat.soft", 
    "author": "I. E. Sarris", 
    "title": "Computational Modeling of an MRI Guided Drug Delivery System Based on   Magnetic Nanoparticle Aggregations for the Navigation of Paramagnetic   Nanocapsules", 
    "publish": "2015-04-14T10:44:18Z", 
    "summary": "A computational method for magnetically guided drug delivery is presented and\nthe results are compared for the aggregation process of magnetic particles\nwithin a fluid environment. The model is developed for the simulation of the\naggregation patterns of magnetic nanoparticles under the influence of MRI\nmagnetic coils. A novel approach for the calculation of the drag coefficient of\naggregates is presented. The comparison against experimental and numerical\nresults from the literature is showed that the proposed method predicts well\nthe aggregations in respect to their size and pattern dependance, on the\nconcentration and the strength of the magnetic field, as well as their velocity\nwhen particles are driven through the fluid by magnetic gradients.", 
    "link": "http://arxiv.org/pdf/1504.03490v1", 
    "arxiv-id": "1504.03490v1"
},{
    "category": "cs.CV", 
    "author": "Pornchai Phukpattaranont", 
    "title": "Comparisons of wavelet functions in QRS signal to noise ratio   enhancement and detection accuracy", 
    "publish": "2015-04-15T09:22:31Z", 
    "summary": "We compare the capability of wavelet functions used for noise removal in\npreprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)\nsignal. The QRS signal to noise ratio enhancement and the detection accuracy of\neach wavelet function are evaluated using three measures: (1) the ratio of the\nmaximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of\nabsolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet\nfunctions from previous well-known publications are explored, i.e., Bior1.3,\nDb10, and Mexican hat wavelet functions. Results evaluated with the ECG signal\nfrom MIT-BIH arrhythmia database show that the Mexican hat wavelet function is\nbetter than the others. While the scale 8 of Mexican hat wavelet function can\nprovide the best enhancement in QRS signal to noise ratio, the scale 4 of\nMexican hat wavelet function can provide the best detection accuracy. These\nresults may be combined and may enable the use of a single fixed threshold for\nall ECG records leading to the reduction in computational complexity of the QRS\ndetection algorithm.", 
    "link": "http://arxiv.org/pdf/1504.03834v2", 
    "arxiv-id": "1504.03834v2"
},{
    "category": "q-bio.GN", 
    "author": "Jacques M. Bahi", 
    "title": "Improved Core Genes Prediction for Constructing well-supported   Phylogenetic Trees in large sets of Plant Species", 
    "publish": "2015-04-23T09:45:07Z", 
    "summary": "The way to infer well-supported phylogenetic trees that precisely reflect the\nevolutionary process is a challenging task that completely depends on the way\nthe related core genes have been found. In previous computational biology\nstudies, many similarity based algorithms, mainly dependent on calculating\nsequence alignment matrices, have been proposed to find them. In these kinds of\napproaches, a significantly high similarity score between two coding sequences\nextracted from a given annotation tool means that one has the same genes. In a\nprevious work article, we presented a quality test approach (QTA) that improves\nthe core genes quality by combining two annotation tools (namely NCBI, a\npartially human-curated database, and DOGMA, an efficient annotation algorithm\nfor chloroplasts). This method takes the advantages from both sequence\nsimilarity and gene features to guarantee that the core genome contains correct\nand well-clustered coding sequences (\\emph{i.e.}, genes). We then show in this\narticle how useful are such well-defined core genes for biomolecular\nphylogenetic reconstructions, by investigating various subsets of core genes at\nvarious family or genus levels, leading to subtrees with strong bootstraps that\nare finally merged in a well-supported supertree.", 
    "link": "http://arxiv.org/pdf/1504.06110v1", 
    "arxiv-id": "1504.06110v1"
},{
    "category": "cs.CE", 
    "author": "Philippe Karamian", 
    "title": "Computation of thermal properties via 3D homogenization of multiphase   materials using FFT-based accelerated scheme", 
    "publish": "2015-04-28T14:32:52Z", 
    "summary": "In this paper we study the thermal effective behaviour for 3D multiphase\ncomposite material consisting of three isotropic phases which are the matrix,\nthe inclusions and the coating media. For this purpose we use an accelerated\nFFT-based scheme initially proposed in Eyre and Milton (1999) to evaluate the\nthermal conductivity tensor. Matrix and spherical inclusions media are polymers\nwith similar properties whereas the coating medium is metallic hence better\nconducting. Thus, the contrast between the coating and the others media is very\nlarge. For our study, we use RVEs (Representative volume elements) generated by\nRSA (Random Sequential Adsorption) method developed in our previous works,\nthen, we compute effective thermal properties using an FFT-based homogenization\ntechnique validated by comparison with the direct finite elements method. We\nstudy the thermal behaviour of the 3D-multiphase composite material and we show\nwhat features should be taken into account to make the computational approach\nefficient.", 
    "link": "http://arxiv.org/pdf/1504.07499v1", 
    "arxiv-id": "1504.07499v1"
},{
    "category": "cs.DC", 
    "author": "Derek Groen", 
    "title": "An automated multiscale ensemble simulation approach for vascular blood   flow", 
    "publish": "2015-04-29T10:22:56Z", 
    "summary": "Cerebrovascular diseases such as brain aneurysms are a primary cause of adult\ndisability. The flow dynamics in brain arteries, both during periods of rest\nand increased activity, are known to be a major factor in the risk of aneurysm\nformation and rupture. The precise relation is however still an open field of\ninvestigation. We present an automated ensemble simulation method for modelling\ncerebrovascular blood flow under a range of flow regimes. By automatically\nconstructing and performing an ensemble of multiscale simulations, where we\nunidirectionally couple a 1D solver with a 3D lattice-Boltzmann code, we are\nable to model the blood flow in a patient artery over a range of flow regimes.\nWe apply the method to a model of a middle cerebral artery, and find that this\napproach helps us to fine-tune our modelling techniques, and opens up new ways\nto investigate cerebrovascular flow properties.", 
    "link": "http://arxiv.org/pdf/1504.07795v1", 
    "arxiv-id": "1504.07795v1"
},{
    "category": "cs.CE", 
    "author": "Paolo Bientinesi", 
    "title": "Large-scale linear regression: Development of high-performance routines", 
    "publish": "2015-04-29T15:24:33Z", 
    "summary": "In statistics, series of ordinary least squares problems (OLS) are used to\nstudy the linear correlation among sets of variables of interest; in many\nstudies, the number of such variables is at least in the millions, and the\ncorresponding datasets occupy terabytes of disk space. As the availability of\nlarge-scale datasets increases regularly, so does the challenge in dealing with\nthem. Indeed, traditional solvers---which rely on the use of black-box\"\nroutines optimized for one single OLS---are highly inefficient and fail to\nprovide a viable solution for big-data analyses. As a case study, in this paper\nwe consider a linear regression consisting of two-dimensional grids of related\nOLS problems that arise in the context of genome-wide association analyses, and\ngive a careful walkthrough for the development of {\\sc ols-grid}, a\nhigh-performance routine for shared-memory architectures; analogous steps are\nrelevant for tailoring OLS solvers to other applications. In particular, we\nfirst illustrate the design of efficient algorithms that exploit the structure\nof the OLS problems and eliminate redundant computations; then, we show how to\neffectively deal with datasets that do not fit in main memory; finally, we\ndiscuss how to cast the computation in terms of efficient kernels and how to\nachieve scalability. Importantly, each design decision along the way is\njustified by simple performance models. {\\sc ols-grid} enables the solution of\n$10^{11}$ correlated OLS problems operating on terabytes of data in a matter of\nhours.", 
    "link": "http://arxiv.org/pdf/1504.07890v1", 
    "arxiv-id": "1504.07890v1"
},{
    "category": "cs.CE", 
    "author": "Eric Polizzi", 
    "title": "A Density Matrix-based Algorithm for Solving Eigenvalue Problems", 
    "publish": "2009-01-17T23:36:23Z", 
    "summary": "A new numerical algorithm for solving the symmetric eigenvalue problem is\npresented. The technique deviates fundamentally from the traditional Krylov\nsubspace iteration based techniques (Arnoldi and Lanczos algorithms) or other\nDavidson-Jacobi techniques, and takes its inspiration from the contour\nintegration and density matrix representation in quantum mechanics. It will be\nshown that this new algorithm - named FEAST - exhibits high efficiency,\nrobustness, accuracy and scalability on parallel architectures. Examples from\nelectronic structure calculations of Carbon nanotubes (CNT) are presented, and\nnumerical performances and capabilities are discussed.", 
    "link": "http://arxiv.org/pdf/0901.2665v1", 
    "arxiv-id": "0901.2665v1"
},{
    "category": "cs.CE", 
    "author": "D. Bhaumik", 
    "title": "Algorithm for Predicting Protein Secondary Structure", 
    "publish": "2010-06-14T19:38:28Z", 
    "summary": "Predicting protein structure from amino acid sequence is one of the most\nimportant unsolved problems of molecular biology and biophysics.Not only would\na successful prediction algorithm be a tremendous advance in the understanding\nof the biochemical mechanisms of proteins, but, since such an algorithm could\nconceivably be used to design proteins to carry out specific\nfunctions.Prediction of the secondary structure of a protein (alpha-helix,\nbeta-sheet, coil) is an important step towards elucidating its three\ndimensional structure as well as its function. In this research, we use\ndifferent Hidden Markov models for protein secondary structure prediction. In\nthis paper we have proposed an algorithm for predicting protein secondary\nstructure. We have used Hidden Markov model with sliding window for secondary\nstructure prediction.The secondary structure has three regular forms, for each\nsecondary structural element we are using one Hidden Markov Model.", 
    "link": "http://arxiv.org/pdf/1006.2813v1", 
    "arxiv-id": "1006.2813v1"
},{
    "category": "cs.CE", 
    "author": "Massimo Torquati", 
    "title": "StochKit-FF: Efficient Systems Biology on Multicore Architectures", 
    "publish": "2010-07-11T10:48:37Z", 
    "summary": "The stochastic modelling of biological systems is an informative, and in some\ncases, very adequate technique, which may however result in being more\nexpensive than other modelling approaches, such as differential equations. We\npresent StochKit-FF, a parallel version of StochKit, a reference toolkit for\nstochastic simulations. StochKit-FF is based on the FastFlow programming\ntoolkit for multicores and exploits the novel concept of selective memory. We\nexperiment StochKit-FF on a model of HIV infection dynamics, with the aim of\nextracting information from efficiently run experiments, here in terms of\naverage and variance and, on a longer term, of more structured data.", 
    "link": "http://arxiv.org/pdf/1007.1768v1", 
    "arxiv-id": "1007.1768v1"
},{
    "category": "astro-ph.IM", 
    "author": "T. Mermet", 
    "title": "Status of GDL - GNU Data Language", 
    "publish": "2011-01-04T09:28:30Z", 
    "summary": "GNU Data Language (GDL) is an open-source interpreted language aimed at\nnumerical data analysis and visualisation. It is a free implementation of the\nInteractive Data Language (IDL) widely used in Astronomy. GDL has a full syntax\ncompatibility with IDL, and includes a large set of library routines targeting\nadvanced matrix manipulation, plotting, time-series and image analysis,\nmapping, and data input/output including numerous scientific data formats. We\nwill present the current status of the project, the key accomplishments, and\nthe weaknesses - areas where contributions are welcome !", 
    "link": "http://arxiv.org/pdf/1101.0679v1", 
    "arxiv-id": "1101.0679v1"
},{
    "category": "cs.CR", 
    "author": "Eric R. Verheul", 
    "title": "Best Effort and Practice Activation Codes", 
    "publish": "2011-01-04T10:41:27Z", 
    "summary": "Activation Codes are used in many different digital services and known by\nmany different names including voucher, e-coupon and discount code. In this\npaper we focus on a specific class of ACs that are short, human-readable,\nfixed-length and represent value. Even though this class of codes is\nextensively used there are no general guidelines for the design of Activation\nCode schemes. We discuss different methods that are used in practice and\npropose BEPAC, a new Activation Code scheme that provides both authenticity and\nconfidentiality. The small message space of activation codes introduces some\nproblems that are illustrated by an adaptive chosen-plaintext attack (CPA-2) on\na general 3-round Feis- tel network of size 2^(2n) . This attack recovers the\ncomplete permutation from at most 2^(n+2) plaintext-ciphertext pairs. For this\nreason, BEPAC is designed in such a way that authenticity and confidentiality\nare in- dependent properties, i.e. loss of confidentiality does not imply loss\nof authenticity.", 
    "link": "http://arxiv.org/pdf/1101.0698v2", 
    "arxiv-id": "1101.0698v2"
},{
    "category": "cs.DS", 
    "author": "Yu Xia", 
    "title": "Clustering Protein Sequences Given the Approximation Stability of the   Min-Sum Objective Function", 
    "publish": "2011-01-19T05:29:24Z", 
    "summary": "We study the problem of efficiently clustering protein sequences in a limited\ninformation setting. We assume that we do not know the distances between the\nsequences in advance, and must query them during the execution of the\nalgorithm. Our goal is to find an accurate clustering using few queries. We\nmodel the problem as a point set $S$ with an unknown metric $d$ on $S$, and\nassume that we have access to \\emph{one versus all} distance queries that given\na point $s \\in S$ return the distances between $s$ and all other points. Our\none versus all query represents an efficient sequence database search program\nsuch as BLAST, which compares an input sequence to an entire data set. Given a\nnatural assumption about the approximation stability of the \\emph{min-sum}\nobjective function for clustering, we design a provably accurate clustering\nalgorithm that uses few one versus all queries. In our empirical study we show\nthat our method compares favorably to well-established clustering algorithms\nwhen we compare computationally derived clusterings to gold-standard manual\nclassifications.", 
    "link": "http://arxiv.org/pdf/1101.3620v2", 
    "arxiv-id": "1101.3620v2"
},{
    "category": "cs.MS", 
    "author": "Matthew J. Patitz", 
    "title": "Simulation of Self-Assembly in the Abstract Tile Assembly Model with ISU   TAS", 
    "publish": "2011-01-27T03:16:16Z", 
    "summary": "Since its introduction by Erik Winfree in 1998, the abstract Tile Assembly\nModel (aTAM) has inspired a wealth of research. As an abstract model for tile\nbased self-assembly, it has proven to be remarkably powerful and expressive in\nterms of the structures which can self-assemble within it. As research has\nprogressed in the aTAM, the self-assembling structures being studied have\nbecome progressively more complex. This increasing complexity, along with a\nneed for standardization of definitions and tools among researchers, motivated\nthe development of the Iowa State University Tile Assembly Simulator (ISU TAS).\nISU TAS is a graphical simulator and tile set editor for designing and building\n2-D and 3-D aTAM tile assembly systems and simulating their self-assembly. This\npaper reviews the features and functionality of ISU TAS and describes how it\ncan be used to further research into the complexities of the aTAM. Software and\nsource code are available at http://www.cs.iastate.edu/~lnsa.", 
    "link": "http://arxiv.org/pdf/1101.5151v1", 
    "arxiv-id": "1101.5151v1"
},{
    "category": "cond-mat.mtrl-sci", 
    "author": "Timon Rabczuk", 
    "title": "A comparative study of two molecular mechanics models based on harmonic   potentials", 
    "publish": "2012-08-07T07:42:39Z", 
    "summary": "We show that the two molecular mechanics models, the stick-spiral and the\nbeam models, predict considerably different mechanical properties of materials\nbased on energy equivalence. The difference between the two models is\nindependent of the materials since all parameters of the beam model are\nobtained from the harmonic potentials. We demonstrate this difference for\nfinite width graphene nanoribbons and a single polyethylene chain comparing\nresults of the molecular dynamics (MD) simulations with harmonic potentials and\nthe finite element method with the beam model. We also find that the difference\nstrongly depends on the loading modes, chirality and width of the graphene\nnanoribbons, and it increases with decreasing width of the nanoribbons under\npure bending condition. The maximum difference of the predicted mechanical\nproperties using the two models can exceed 300% in different loading modes.\nComparing the two models with the MD results of AIREBO potential, we find that\nthe stick-spiral model overestimates and the beam model underestimates the\nmechanical properties in narrow armchair graphene nanoribbons under pure\nbending condition.", 
    "link": "http://arxiv.org/pdf/1208.1353v1", 
    "arxiv-id": "1208.1353v1"
},{
    "category": "cs.CE", 
    "author": "Fr\u00e9d\u00e9ric Rodriguez", 
    "title": "Technical report: CSVM dictionaries", 
    "publish": "2012-08-08T19:48:40Z", 
    "summary": "CSVM (CSV with Metadata) is a simple file format for tabular data. The\npossible application domain is the same as typical spreadsheets files, but CSVM\nis well suited for long term storage and the inter-conversion of RAW data. CSVM\nembeds different levels for data, metadata and annotations in human readable\nformat and flat ASCII files. As a proof of concept, Perl and Python toolkits\nwere designed in order to handle CSVM data and objects in workflows. These\nparsers can process CSVM files independently of data types, so it is possible\nto use same data format and parser for a lot of scientific purposes. CSVM-1 is\nthe first version of CSVM specification, an extension of CSVM-1 for\nimplementing a translation system between CSVM files is presented in this\npaper. The necessary data used to make the translation are also coded in\nanother CSVM file. This particular kind of CSVM is called a CSVM dictionary, it\nis also readable by the current CSVM parser and it is fully supported by the\nPython toolkit. This report presents a proposal for CSVM dictionaries, a\nworking example in chemistry, and some elements of Python toolkit usable to\nhandle these files.", 
    "link": "http://arxiv.org/pdf/1208.1934v1", 
    "arxiv-id": "1208.1934v1"
},{
    "category": "q-fin.GN", 
    "author": "Lakshmi Kaligounder", 
    "title": "On Global Stability of Financial Networks", 
    "publish": "2012-08-18T22:23:22Z", 
    "summary": "The recent financial crisis have generated renewed interests in fragilities\nof global financial networks among economists and regulatory authorities. In\nparticular, a potential vulnerability of the financial networks is the\n\"financial contagion\" process in which insolvencies of individual entities\npropagate through the \"web of dependencies\" to affect the entire system. In\nthis paper, we formalize an extension of a financial network model originally\nproposed by Nier et al. for scenarios such as the OTC derivatives market,\ndefine a suitable global stability measure for this model, and perform a\ncomprehensive empirical evaluation of this stability measure over more than\n700,000 combinations of networks types and parameter combinations. Based on our\nevaluations, we discover many interesting implications of our evaluations of\nthis stability measure, and derive topological properties and parameters\ncombinations that may be used to flag the network as a possible fragile\nnetwork. An interactive software FIN-STAB for computing the stability is\navailable from the website www2.cs.uic.edu/~dasgupta/financial-simulator-files", 
    "link": "http://arxiv.org/pdf/1208.3789v5", 
    "arxiv-id": "1208.3789v5"
},{
    "category": "cs.CE", 
    "author": "Guido Sanguinetti", 
    "title": "A subsystems approach for parameter estimation of ODE models of hybrid   systems", 
    "publish": "2012-08-19T16:01:13Z", 
    "summary": "We present a new method for parameter identification of ODE system\ndescriptions based on data measurements. Our method works by splitting the\nsystem into a number of subsystems and working on each of them separately,\nthereby being easily parallelisable, and can also deal with noise in the\nobservations.", 
    "link": "http://arxiv.org/pdf/1208.3850v1", 
    "arxiv-id": "1208.3850v1"
},{
    "category": "cs.CE", 
    "author": "Alberto d'Onofrio", 
    "title": "Effects of delayed immune-response in tumor immune-system interplay", 
    "publish": "2012-08-19T16:01:54Z", 
    "summary": "Tumors constitute a wide family of diseases kinetically characterized by the\nco-presence of multiple spatio-temporal scales. So, tumor cells ecologically\ninterplay with other kind of cells, e.g. endothelial cells or immune system\neffectors, producing and exchanging various chemical signals. As such, tumor\ngrowth is an ideal object of hybrid modeling where discrete stochastic\nprocesses model agents at low concentrations, and mean-field equations model\nchemical signals. In previous works we proposed a hybrid version of the\nwell-known Panetta-Kirschner mean-field model of tumor cells, effector cells\nand Interleukin-2. Our hybrid model suggested -at variance of the inferences\nfrom its original formulation- that immune surveillance, i.e. tumor elimination\nby the immune system, may occur through a sort of side-effect of large\nstochastic oscillations. However, that model did not account that, due to both\nchemical transportation and cellular differentiation/division, the\ntumor-induced recruitment of immune effectors is not instantaneous but,\ninstead, it exhibits a lag period. To capture this, we here integrate a\nmean-field equation for Interleukins-2 with a bi-dimensional delayed stochastic\nprocess describing such delayed interplay. An algorithm to realize trajectories\nof the underlying stochastic process is obtained by coupling the Piecewise\nDeterministic Markov process (for the hybrid part) with a Generalized\nSemi-Markovian clock structure (to account for delays). We (i) relate tumor\nmass growth with delays via simulations and via parametric sensitivity analysis\ntechniques, (ii) we quantitatively determine probabilistic eradication times,\nand (iii) we prove, in the oscillatory regime, the existence of a heuristic\nstochastic bifurcation resulting in delay-induced tumor eradication, which is\nneither predicted by the mean-field nor by the hybrid non-delayed models.", 
    "link": "http://arxiv.org/pdf/1208.3855v1", 
    "arxiv-id": "1208.3855v1"
},{
    "category": "cs.CE", 
    "author": "Sean Sedwards", 
    "title": "Statistical Model Checking for Stochastic Hybrid Systems", 
    "publish": "2012-08-19T16:02:05Z", 
    "summary": "This paper presents novel extensions and applications of the UPPAAL-SMC model\nchecker. The extensions allow for statistical model checking of stochastic\nhybrid systems. We show how our race-based stochastic semantics extends to\nnetworks of hybrid systems, and indicate the integration technique applied for\nimplementing this semantics in the UPPAAL-SMC simulation engine. We report on\ntwo applications of the resulting tool-set coming from systems biology and\nenergy aware buildings.", 
    "link": "http://arxiv.org/pdf/1208.3856v1", 
    "arxiv-id": "1208.3856v1"
},{
    "category": "cs.CE", 
    "author": "Paolo Magrassi", 
    "title": "How Non-linearity will Transform Information Systems", 
    "publish": "2012-08-27T07:56:06Z", 
    "summary": "One 'problem' with the 21st century world, particularly the economic and\nbusiness worlds, is the phenomenal and increasing number of interconnections\nbetween economic agents (consumers, firms, banks, markets, national economies).\nThis implies that such agents are all interacting and consequently giving raise\nto enormous degrees of non-linearity, a.k.a. complexity. Complexity often\nbrings with it unexpected phenomena, such as chaos and emerging behaviour, that\ncan become challenges for the survival of economic agents and systems.\nDeveloping econophysics approaches are beginning to apply, to the 'economic\nweb', methods and models that have been used in physics and/or systems theory\nto tackle non-linear domains. The paper gives an account of the research in\nprogress in this field and shows its implications for enteprise information\nsystems, anticipating the emergence of software that will allow to reflect the\ncomplexity of the business world, as holistic risk management becomes a mandate\nfor financial institutions and business organizations.", 
    "link": "http://arxiv.org/pdf/1208.5316v1", 
    "arxiv-id": "1208.5316v1"
},{
    "category": "cs.CE", 
    "author": "Antonio A. Gentile", 
    "title": "Review of strategies for a comprehensive simulation in sputtering   devices", 
    "publish": "2012-09-12T16:47:23Z", 
    "summary": "The development of sputtering facilities, at the moment, is mainly pursued\nthrough experimental tests, or simply by expertise in the field, and relies\nmuch less on numerical simulation of the process environment. This leads to\ngreat efforts and empirically, roughly optimized solutions: in fact, the\nsimulation of these devices, at the state of art, is quite good in predicting\nthe behavior of single steps of the overall deposition process, but it seems\nstill ahead a full integration among the tools simulating the various phenomena\ninvolved in a sputter. We summarize here the techniques and codes already\navailable for problems of interest in sputtering facilities, and we try to\noutline the possible features of a comprehensive simulation framework. This\nframework should be able to integrate the single paradigms, dealing with\naspects going from the plasma environment up to the distribution and properties\nof the deposited film, not only on the surface of the substrate, but also on\nthe walls of the process chamber.", 
    "link": "http://arxiv.org/pdf/1209.2660v1", 
    "arxiv-id": "1209.2660v1"
},{
    "category": "astro-ph.IM", 
    "author": "Sarod Yatawatta", 
    "title": "Estimation of Radio Interferometer Beam Shapes Using Riemannian   Optimization", 
    "publish": "2012-09-19T13:25:48Z", 
    "summary": "The knowledge of receiver beam shapes is essential for accurate radio\ninterferometric imaging. Traditionally, this information is obtained by\nholographic techniques or by numerical simulation. However, such methods are\nnot feasible for an observation with time varying beams, such as the beams\nproduced by a phased array radio interferometer. We propose the use of the\nobserved data itself for the estimation of the beam shapes. We use the\ndirectional gains obtained along multiple sources across the sky for the\nconstruction of a time varying beam model. The construction of this model is an\nill posed non linear optimization problem. Therefore, we propose to use\nRiemannian optimization, where we consider the constraints imposed as a\nmanifold. We compare the performance of the proposed approach with traditional\nunconstrained optimization and give results to show the superiority of the\nproposed approach.", 
    "link": "http://arxiv.org/pdf/1209.4236v1", 
    "arxiv-id": "1209.4236v1"
},{
    "category": "q-fin.ST", 
    "author": "J. B. Kim", 
    "title": "Performance Analysis of Hybrid Forecasting Model In Stock Market   Forecasting", 
    "publish": "2012-09-20T18:50:41Z", 
    "summary": "This paper presents performance analysis of hybrid model comprise of\nconcordance and Genetic Programming (GP) to forecast financial market with some\nexisting models. This scheme can be used for in depth analysis of stock market.\nDifferent measures of concordances such as Kendalls Tau, Ginis Mean Difference,\nSpearmans Rho, and weak interpretation of concordance are used to search for\nthe pattern in past that look similar to present. Genetic Programming is then\nused to match the past trend to present trend as close as possible. Then\nGenetic Program estimates what will happen next based on what had happened\nnext. The concept is validated using financial time series data (S&P 500 and\nNASDAQ indices) as sample data sets. The forecasted result is then compared\nwith standard ARIMA model and other model to analyse its performance.", 
    "link": "http://arxiv.org/pdf/1209.4608v2", 
    "arxiv-id": "1209.4608v2"
},{
    "category": "cs.PL", 
    "author": "Alan Edelman", 
    "title": "Julia: A Fast Dynamic Language for Technical Computing", 
    "publish": "2012-09-24T03:55:45Z", 
    "summary": "Dynamic languages have become popular for scientific computing. They are\ngenerally considered highly productive, but lacking in performance. This paper\npresents Julia, a new dynamic language for technical computing, designed for\nperformance from the beginning by adapting and extending modern programming\nlanguage techniques. A design based on generic functions and a rich type system\nsimultaneously enables an expressive programming model and successful type\ninference, leading to good performance for a wide range of programs. This makes\nit possible for much of the Julia library to be written in Julia itself, while\nalso incorporating best-of-breed C and Fortran libraries.", 
    "link": "http://arxiv.org/pdf/1209.5145v1", 
    "arxiv-id": "1209.5145v1"
},{
    "category": "cs.CE", 
    "author": "Samir K. Bandyopadhyay", 
    "title": "An Efficient Biological Sequence Compression Technique Using LUT And   Repeat In The Sequence", 
    "publish": "2012-09-26T11:47:33Z", 
    "summary": "Data compression plays an important role to deal with high volumes of DNA\nsequences in the field of Bioinformatics. Again data compression techniques\ndirectly affect the alignment of DNA sequences. So the time needed to\ndecompress a compressed sequence has to be given equal priorities as with\ncompression ratio. This article contains first introduction then a brief review\nof different biological sequence compression after that my proposed work then\nour two improved Biological sequence compression algorithms after that result\nfollowed by conclusion and discussion, future scope and finally references.\nThese algorithms gain a very good compression factor with higher saving\npercentage and less time for compression and decompression than the previous\nBiological Sequence compression algorithms. Keywords: Hash map table, Tandem\nrepeats, compression factor, compression time, saving percentage, compression,\ndecompression process.", 
    "link": "http://arxiv.org/pdf/1209.5905v2", 
    "arxiv-id": "1209.5905v2"
},{
    "category": "cs.LG", 
    "author": "George Runger", 
    "title": "Gene selection with guided regularized random forest", 
    "publish": "2012-09-28T04:59:33Z", 
    "summary": "The regularized random forest (RRF) was recently proposed for feature\nselection by building only one ensemble. In RRF the features are evaluated on a\npart of the training data at each tree node. We derive an upper bound for the\nnumber of distinct Gini information gain values in a node, and show that many\nfeatures can share the same information gain at a node with a small number of\ninstances and a large number of features. Therefore, in a node with a small\nnumber of instances, RRF is likely to select a feature not strongly relevant.\nHere an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In\nGRRF, the importance scores from an ordinary random forest (RF) are used to\nguide the feature selection process in RRF. Experiments on 10 gene data sets\nshow that the accuracy performance of GRRF is, in general, more robust than RRF\nwhen their parameters change. GRRF is computationally efficient, can select\ncompact feature subsets, and has competitive accuracy performance, compared to\nRRF, varSelRF and LASSO logistic regression (with evaluations from an RF\nclassifier). Also, RF applied to the features selected by RRF with the minimal\nregularization outperforms RF applied to all the features for most of the data\nsets considered here. Therefore, if accuracy is considered more important than\nthe size of the feature subset, RRF with the minimal regularization may be\nconsidered. We use the accuracy performance of RF, a strong classifier, to\nevaluate feature selection methods, and illustrate that weak classifiers are\nless capable of capturing the information contained in a feature subset. Both\nRRF and GRRF were implemented in the \"RRF\" R package available at CRAN, the\nofficial R package archive.", 
    "link": "http://arxiv.org/pdf/1209.6425v3", 
    "arxiv-id": "1209.6425v3"
},{
    "category": "cs.CE", 
    "author": "Deepak Garg", 
    "title": "Online Financial Algorithms Competitive Analysis", 
    "publish": "2012-09-28T11:46:28Z", 
    "summary": "Analysis of algorithms with complete knowledge of its inputs is sometimes not\nup to our expectations. Many times we are surrounded with such scenarios where\ninputs are generated without any prior knowledge. Online Algorithms have found\ntheir applicability in broad areas of computer engineering. Among these, an\nonline financial algorithm is one of the most important areas where lots of\nefforts have been used to produce an efficient algorithm. In this paper various\nOnline Algorithms have been reviewed for their efficiency and various\nalternative measures have been explored for analysis purposes.", 
    "link": "http://arxiv.org/pdf/1209.6489v1", 
    "arxiv-id": "1209.6489v1"
},{
    "category": "cs.CE", 
    "author": "Callum J. Corbett", 
    "title": "A computational formulation for constrained solid and liquid membranes   considering isogeometric finite elements", 
    "publish": "2012-10-17T16:45:52Z", 
    "summary": "A geometrically exact membrane formulation is presented that is based on\ncurvilinear coordinates and isogeometric finite elements, and is suitable for\nboth solid and liquid membranes. The curvilinear coordinate system is used to\ndescribe both the theory and the finite element equations of the membrane. In\nthe latter case this avoids the use of local cartesian coordinates at the\nelement level. Consequently, no transformation of derivatives is required. The\nformulation considers a split of the in-plane and out-of-plane membrane\ncontributions, which allows the construction of a stable formulation for liquid\nmembranes with constant surface tension. The proposed membrane formulation is\ngeneral, and accounts for dead and live loading, as well as enclosed volume,\narea, and contact constraints. The new formulation is illustrated by several\nchallenging examples, considering linear and quadratic Lagrange elements, as\nwell as isogeometric elements based on quadratic NURBS and cubic T-splines. It\nis seen that the isogeometric elements are much more accurate than standard\nLagrange elements. The gain is especially large for the liquid membrane\nformulation since it depends explicitly on the surface curvature.", 
    "link": "http://arxiv.org/pdf/1210.4791v1", 
    "arxiv-id": "1210.4791v1"
},{
    "category": "stat.ME", 
    "author": "Karsten Keller", 
    "title": "Detecting Change-Points in Time Series by Maximum Mean Discrepancy of   Ordinal Pattern Distributions", 
    "publish": "2012-10-16T17:51:29Z", 
    "summary": "As a new method for detecting change-points in high-resolution time series,\nwe apply Maximum Mean Discrepancy to the distributions of ordinal patterns in\ndifferent parts of a time series. The main advantage of this approach is its\ncomputational simplicity and robustness with respect to (non-linear) monotonic\ntransformations, which makes it particularly well-suited for the analysis of\nlong biophysical time series where the exact calibration of measurement devices\nis unknown or varies with time. We establish consistency of the method and\nevaluate its performance in simulation studies. Furthermore, we demonstrate the\napplication to the analysis of electroencephalography (EEG) and\nelectrocardiography (ECG) recordings.", 
    "link": "http://arxiv.org/pdf/1210.4903v1", 
    "arxiv-id": "1210.4903v1"
},{
    "category": "cs.CE", 
    "author": "William S. Noble", 
    "title": "Spectrum Identification using a Dynamic Bayesian Network Model of Tandem   Mass Spectra", 
    "publish": "2012-10-16T17:51:39Z", 
    "summary": "Shotgun proteomics is a high-throughput technology used to identify unknown\nproteins in a complex mixture. At the heart of this process is a prediction\ntask, the spectrum identification problem, in which each fragmentation spectrum\nproduced by a shotgun proteomics experiment must be mapped to the peptide\n(protein subsequence) which generated the spectrum. We propose a new algorithm\nfor spectrum identification, based on dynamic Bayesian networks, which\nsignificantly outperforms the de-facto standard tools for this task: SEQUEST\nand Mascot.", 
    "link": "http://arxiv.org/pdf/1210.4904v1", 
    "arxiv-id": "1210.4904v1"
},{
    "category": "cs.NA", 
    "author": "A. J. Valocchi", 
    "title": "A numerical framework for diffusion-controlled bimolecular-reactive   systems to enforce maximum principles and non-negative constraint", 
    "publish": "2012-10-19T01:06:02Z", 
    "summary": "We present a novel computational framework for diffusive-reactive systems\nthat satisfies the non-negative constraint and maximum principles on general\ncomputational grids. The governing equations for the concentration of reactants\nand product are written in terms of tensorial diffusion-reaction equations. %\nWe restrict our studies to fast irreversible bimolecular reactions. If one\nassumes that the reaction is diffusion-limited and all chemical species have\nthe same diffusion coefficient, one can employ a linear transformation to\nrewrite the governing equations in terms of invariants, which are unaffected by\nthe reaction. This results in two uncoupled tensorial diffusion equations in\nterms of these invariants, which are solved using a novel non-negative solver\nfor tensorial diffusion-type equations. The concentrations of the reactants and\nthe product are then calculated from invariants using algebraic manipulations.\nThe novel aspect of the proposed computational framework is that it will always\nproduce physically meaningful non-negative values for the concentrations of all\nchemical species. Several representative numerical examples are presented to\nillustrate the robustness, convergence, and the numerical performance of the\nproposed computational framework. We will also compare the proposed framework\nwith other popular formulations. In particular, we will show that the Galerkin\nformulation (which is the standard single-field formulation) does not produce\nreliable solutions, and the reason can be attributed to the fact that the\nsingle-field formulation does not guarantee non-negative solutions. We will\nalso show that the clipping procedure (which produces non-negative solutions\nbut is considered as a variational crime) does not give accurate results when\ncompared with the proposed computational framework.", 
    "link": "http://arxiv.org/pdf/1210.5290v4", 
    "arxiv-id": "1210.5290v4"
},{
    "category": "cs.AI", 
    "author": "V. P. Singh", 
    "title": "Improved Local Search in Artificial Bee Colony using Golden Section   Search", 
    "publish": "2012-10-23T04:57:04Z", 
    "summary": "Artificial bee colony (ABC), an optimization algorithm is a recent addition\nto the family of population based search algorithm. ABC has taken its\ninspiration from the collective intelligent foraging behavior of honey bees. In\nthis study we have incorporated golden section search mechanism in the\nstructure of basic ABC to improve the global convergence and prevent to stick\non a local solution. The proposed variant is termed as ILS-ABC. Comparative\nnumerical results with the state-of-art algorithms show the performance of the\nproposal when applied to the set of unconstrained engineering design problems.\nThe simulated results show that the proposed variant can be successfully\napplied to solve real life problems.", 
    "link": "http://arxiv.org/pdf/1210.6128v1", 
    "arxiv-id": "1210.6128v1"
},{
    "category": "physics.flu-dyn", 
    "author": "M. Sommerfeld", 
    "title": "Experiments and Direct Numerical Simulations of binary collisions of   miscible liquid droplets with different viscosities", 
    "publish": "2012-10-23T13:53:56Z", 
    "summary": "Binary droplet collisions are of importance in a variety of practical\napplications comprising dispersed two-phase flows. The background of our\nresearch is the prediction of properties of particulate products formed in\nspray processes. To gain a more thorough understanding of the elementary\nsub-processes inside a spray, experiments and direct numerical simulations of\nbinary droplet collisions are used. The aim of these investigations is to\ndevelop semi-analytical descriptions for the outcome of droplet collisions.\nSuch collision models can then be employed as closure terms for scale-reduced\nsimulations. In the present work we focus on the collision of droplets of\ndifferent liquids. These kinds of collisions take place in every spray drying\nprocess when droplets with different solids contents collide in recirculation\nzones. A new experimental method has been developed allowing for high spatial\nand time resolved recordings via Laser-induced fluorescence. The results\nobtained with the proposed method will be compared with DNS simulations. The\nviscosities of the droplets are different whereas the interfacial tension and\ndensity are equal. The liquids are miscible and no surface tension is acting\nbetween the two liquids. Our intention is to discover elementary phenomena\ncaused by the viscosity ratio of the droplets.", 
    "link": "http://arxiv.org/pdf/1210.6234v1", 
    "arxiv-id": "1210.6234v1"
},{
    "category": "cs.CE", 
    "author": "Minh Nhut Nguyen", 
    "title": "Predicting Near-Future Churners and Win-Backs in the Telecommunications   Industry", 
    "publish": "2012-10-24T05:56:45Z", 
    "summary": "In this work, we presented the strategies and techniques that we have\ndeveloped for predicting the near-future churners and win-backs for a telecom\ncompany. On a large-scale and real-world database containing customer profiles\nand some transaction data from a telecom company, we first analyzed the data\nschema, developed feature computation strategies and then extracted a large set\nof relevant features that can be associated with the customer churning and\nreturning behaviors. Our features include both the original driver factors as\nwell as some derived features. We evaluated our features on the imbalance\ncorrected dataset, i.e. under-sampled dataset and compare a large number of\nexisting machine learning tools, especially decision tree-based classifiers,\nfor predicting the churners and win-backs. In general, we find RandomForest and\nSimpleCart learning algorithms generally perform well and tend to provide us\nwith highly competitive prediction performance. Among the top-15 driver factors\nthat signal the churn behavior, we find that the service utilization, e.g. last\ntwo months' download and upload volume, last three months' average upload and\ndownload, and the payment related factors are the most indicative features for\npredicting if churn will happen soon. Such features can collectively tell\ndiscrepancies between the service plans, payments and the dynamically changing\nutilization needs of the customers. Our proposed features and their\ncomputational strategy exhibit reasonable precision performance to predict\nchurn behavior in near future.", 
    "link": "http://arxiv.org/pdf/1210.6891v1", 
    "arxiv-id": "1210.6891v1"
},{
    "category": "cs.CE", 
    "author": "Jorn H. Baayen", 
    "title": "Vortexje - An Open-Source Panel Method for Co-Simulation", 
    "publish": "2012-10-25T19:22:46Z", 
    "summary": "This paper discusses the use of the 3-dimensional panel method for dynamical\nsystem simulation. Specifically, the advantages and disadvantages of model\nexchange versus co-simulation of the aerodynamics and the dynamical system\nmodel are discussed. Based on a trade-off analysis, a set of recommendations\nfor a panel method implementation and for a co-simulation environment is\nproposed. These recommendations are implemented in a C++ library, offered\non-line under an open source license. This code is validated against XFLR5, and\nits suitability for co-simulation is demonstrated with an example of a tethered\nwing, i.e, a kite. The panel method implementation and the co-simulation\nenvironment are shown to be able to solve this stiff problem in a stable\nfashion.", 
    "link": "http://arxiv.org/pdf/1210.6956v3", 
    "arxiv-id": "1210.6956v3"
},{
    "category": "cs.DS", 
    "author": "Salim Chikhi", 
    "title": "A new greedy randomized adaptive search procedure for multiobjective RNA   structural alignment", 
    "publish": "2013-02-06T15:13:03Z", 
    "summary": "RNA secondary structures prediction is one of the main issues in\nbioinformatics. It seeks to elucidate structural conserved regions within a set\nof RNA sequences. Unfortunately, finding an accurate conserved structure is a\nvery hard task to do. Within the present study, the prediction problem is\nconsidered as a multiobjective optimization process in which the structural\nconservation and the sensitivity of the multiple alignment are optimized. The\nproposed method called GRASPMORSA is based on an aggregate function and GRASP\nprocedure. The initial solutions are obtained by using a random progressive\nlocal/ global algorithm, and then they are refined by an iterative realignment.\nExperiments within a large scale of data have shown the efficacy and\neffectiveness of the proposed method and its capacity to reach good quality\nsolutions.", 
    "link": "http://arxiv.org/pdf/1302.1400v1", 
    "arxiv-id": "1302.1400v1"
},{
    "category": "cs.CV", 
    "author": "H. Hannah Inbarani", 
    "title": "An Analysis of Gene Expression Data using Penalized Fuzzy C-Means   Approach", 
    "publish": "2013-01-08T17:16:39Z", 
    "summary": "With the rapid advances of microarray technologies, large amounts of\nhigh-dimensional gene expression data are being generated, which poses\nsignificant computational challenges. A first step towards addressing this\nchallenge is the use of clustering techniques, which is essential in the data\nmining process to reveal natural structures and identify interesting patterns\nin the underlying data. A robust gene expression clustering approach to\nminimize undesirable clustering is proposed. In this paper, Penalized Fuzzy\nC-Means (PFCM) Clustering algorithm is described and compared with the most\nrepresentative off-line clustering techniques: K-Means Clustering, Rough\nK-Means Clustering and Fuzzy C-Means clustering. These techniques are\nimplemented and tested for a Brain Tumor gene expression Dataset. Analysis of\nthe performance of the proposed approach is presented through qualitative\nvalidation experiments. From experimental results, it can be observed that\nPenalized Fuzzy C-Means algorithm shows a much higher usability than the other\nprojected clustering algorithms used in our comparison study. Significant and\npromising clustering results are presented using Brain Tumor Gene expression\ndataset. Thus patterns seen in genome-wide expression experiments can be\ninterpreted as indications of the status of cellular processes. In these\nclustering results, we find that Penalized Fuzzy C-Means algorithm provides\nuseful information as an aid to diagnosis in oncology.", 
    "link": "http://arxiv.org/pdf/1302.3123v1", 
    "arxiv-id": "1302.3123v1"
},{
    "category": "cs.CE", 
    "author": "Qing H. Qin", 
    "title": "Post-buckling Solutions of Hyper-elastic Beam by Canonical Dual Finite   Element Method", 
    "publish": "2013-02-17T23:53:52Z", 
    "summary": "Post buckling problem of a large deformed beam is analyzed using canonical\ndual finite element method (CD-FEM). The feature of this method is to choose\ncorrectly the canonical dual stress so that the original non-convex potential\nenergy functional is reformulated in a mixed complementary energy form with\nboth displacement and stress fields, and a pure complementary energy is\nexplicitly formulated in finite dimensional space. Based on the canonical\nduality theory and the associated triality theorem, a primal-dual algorithm is\nproposed, which can be used to find all possible solutions of this nonconvex\npost-buckling problem. Numerical results show that the global maximum of the\npure-complementary energy leads to a stable buckled configuration of the beam.\nWhile the local extrema of the pure-complementary energy present unstable\ndeformation states, especially. We discovered that the unstable buckled state\nis very sensitive to the number of total elements and the external loads.\nTheoretical results are verified through numerical examples and some\ninteresting phenomena in post-bifurcation of this large deformed beam are\nobserved.", 
    "link": "http://arxiv.org/pdf/1302.4136v1", 
    "arxiv-id": "1302.4136v1"
},{
    "category": "cs.CE", 
    "author": "Mohammadreza Ghodsi", 
    "title": "Constructing a genome assembly that has the maximum likelihood", 
    "publish": "2013-02-18T19:14:38Z", 
    "summary": "We formulate genome assembly problem as an optimization problem in which the\nobjective function is the likelihood of the assembly given the reads.", 
    "link": "http://arxiv.org/pdf/1302.4391v3", 
    "arxiv-id": "1302.4391v3"
},{
    "category": "physics.med-ph", 
    "author": "Kirana Kumara P", 
    "title": "Extracting Three Dimensional Surface Model of Human Kidney from the   Visible Human Data Set using Free Software", 
    "publish": "2013-02-19T09:44:51Z", 
    "summary": "Three dimensional digital model of a representative human kidney is needed\nfor a surgical simulator that is capable of simulating a laparoscopic surgery\ninvolving kidney. Buying a three dimensional computer model of a representative\nhuman kidney, or reconstructing a human kidney from an image sequence using\ncommercial software, both involve (sometimes significant amount of) money. In\nthis paper, author has shown that one can obtain a three dimensional surface\nmodel of human kidney by making use of images from the Visible Human Data Set\nand a few free software packages (ImageJ, ITK-SNAP, and MeshLab in particular).\nImages from the Visible Human Data Set, and the software packages used here,\nboth do not cost anything. Hence, the practice of extracting the geometry of a\nrepresentative human kidney for free, as illustrated in the present work, could\nbe a free alternative to the use of expensive commercial software or to the\npurchase of a digital model.", 
    "link": "http://arxiv.org/pdf/1302.4557v1", 
    "arxiv-id": "1302.4557v1"
},{
    "category": "cs.CE", 
    "author": "Namit Jain", 
    "title": "A Fast Template Based Heuristic For Global Multiple Sequence Alignment", 
    "publish": "2013-02-25T09:55:16Z", 
    "summary": "Advances in bio-technology have made available massive amounts of functional,\nstructural and genomic data for many biological sequences. This increased\navailability of heterogeneous biological data has resulted in biological\napplications where a multiple sequence alignment (msa) is required for aligning\nsimilar features, where a feature is described in structural, functional or\nevolutionary terms. In these applications, for a given set of sequences,\ndepending on the feature of interest the optimal msa is likely to be different,\nand sequence similarity can only be used as a rough initial estimate on the\naccuracy of an msa. This has motivated the growth in template based heuristics\nthat supplement the sequence information with evolutionary, structural and\nfunctional data and exploit feature similarity instead of sequence similarity\nto construct multiple sequence alignments that are biologically more accurate.\nHowever, current frameworks for designing template based heuristics do not\nallow the user to explicitly specify information that can help to classify\nfeatures into types and associate weights signifying the relative importance of\na feature with respect to other features. In this paper, we first provide a\nmechanism where as a part of the template information the user can explicitly\nspecify for each feature, its type, and weight. The type is to classify the\nfeatures into different categories based on their characteristics and the\nweight signifies the relative importance of a feature with respect to other\nfeatures in that sequence. Second, we exploit the above information to define\nscoring models for pair-wise sequence alignment that assume segment\nconservation as opposed to single character (residue) conservation. Finally, we\npresent a fast progressive alignment based heuristic framework that helps in\nconstructing a global msa efficiently.", 
    "link": "http://arxiv.org/pdf/1302.6030v1", 
    "arxiv-id": "1302.6030v1"
},{
    "category": "cs.CE", 
    "author": "Nick Williams", 
    "title": "An Exploratory Data Survey of Drug Name Incidence and Prevalence From   the FDA's Adverse Event Reporting System, 2004 to 2012Q2", 
    "publish": "2013-08-31T01:09:23Z", 
    "summary": "Drug Names, Population Level Surveillance and the FDA's Adverse Event\nReporting System: An Exploratory Data Survey of Drug Name Incidence and\nPrevalence, 2004-2012Q2 Purpose: To count and monitor the drug names reported\nin the publicly available version of the Federal Adverse Event Reporting System\n(FAERS) from 2004 to 2012Q2 in a maximized sensitivity relational model.\nMethods: Data mining and data modeling was conducted and event based summary\nstatistics with plots were created from over nine continuous years of\ncontinuous FAERS data. Results: This FAERS model contains 344,452 individual\ndrug names and 432,541,994 count references which occurred across 4,148,761\nhuman subjects in the 34 quarter study period. Plots for the top 100 scoring\ndrug name references are reported by year and quarter; the top 100 drug names\ncontain 143,384,240 references or 33% of all drug name references over 34\nquarters of continuous FAERS data. Conclusions: While FAERS contains many drugs\nand adverse event reports, its data pertains to very few of them. Drug name\nincidence lends timely and effective surveillance of large populations of\nAverse Event Reports and does not require the cause of the AE, nor its validity\nto be known to detect a mass poisoning.", 
    "link": "http://arxiv.org/pdf/1309.0781v1", 
    "arxiv-id": "1309.0781v1"
},{
    "category": "cs.CE", 
    "author": "Ovidiu Radulescu", 
    "title": "A hybrid mammalian cell cycle model", 
    "publish": "2013-09-03T23:41:28Z", 
    "summary": "Hybrid modeling provides an effective solution to cope with multiple time\nscales dynamics in systems biology. Among the applications of this method, one\nof the most important is the cell cycle regulation. The machinery of the cell\ncycle, leading to cell division and proliferation, combines slow growth,\nspatio-temporal re-organisation of the cell, and rapid changes of regulatory\nproteins concentrations induced by post-translational modifications. The\nadvancement through the cell cycle comprises a well defined sequence of stages,\nseparated by checkpoint transitions. The combination of continuous and discrete\nchanges justifies hybrid modelling approaches to cell cycle dynamics. We\npresent a piecewise-smooth version of a mammalian cell cycle model, obtained by\nhybridization from a smooth biochemical model. The approximate hybridization\nscheme, leading to simplified reaction rates and binary event location\nfunctions, is based on learning from a training set of trajectories of the\nsmooth model. We discuss several learning strategies for the parameters of the\nhybrid model.", 
    "link": "http://arxiv.org/pdf/1309.0870v1", 
    "arxiv-id": "1309.0870v1"
},{
    "category": "cs.CE", 
    "author": "Ouri Maler", 
    "title": "Exploring the Dynamics of Mass Action Systems", 
    "publish": "2013-09-03T23:41:36Z", 
    "summary": "We present the Populus toolkit for exploring the dynamics of mass action\nsystems under different assumptions.", 
    "link": "http://arxiv.org/pdf/1309.0871v1", 
    "arxiv-id": "1309.0871v1"
},{
    "category": "cs.CE", 
    "author": "Louise H. Kellogg", 
    "title": "Experiences with Automated Build and Test for Geodynamics Simulation   Codes", 
    "publish": "2013-09-04T22:22:52Z", 
    "summary": "The Computational Infrastructure for Geodynamics (CIG) is an NSF funded\nproject that develops, supports, and disseminates community-accessible software\nfor the geodynamics research community. CIG software supports a variety of\ncomputational geodynamic research from mantle and core dynamics, to crustal and\nearthquake dynamics, to magma migration and seismology. To support this type of\nproject a backend computational infrastructure is necessary.\n  Part of this backend infrastructure is an automated build and testing system\nto ensure codes and changes to them are compatible with multiple platforms and\nthat the changes do not significantly affect the scientific results. In this\npaper we describe the build and test infrastructure for CIG based on the BaTLab\nsystem, how it is organized, and how it assists in operations. We demonstrate\nthe use of this type of testing for a suite of geophysics codes, show why codes\nmay compile on one platform but not on another, and demonstrate how minor\nchanges may alter the computed results in unexpected ways that can influence\nthe scientific interpretation. Finally, we examine result comparison between\nplatforms and show how the compiler or operating system may affect results.", 
    "link": "http://arxiv.org/pdf/1309.1199v1", 
    "arxiv-id": "1309.1199v1"
},{
    "category": "cs.MS", 
    "author": "Barry F. Smith", 
    "title": "Achieving High Performance with Unified Residual Evaluation", 
    "publish": "2013-09-04T23:03:33Z", 
    "summary": "We examine residual evaluation, perhaps the most basic operation in numerical\nsimulation. By raising the level of abstraction in this operation, we can\neliminate specialized code, enable optimization, and greatly increase the\nextensibility of existing code.", 
    "link": "http://arxiv.org/pdf/1309.1204v2", 
    "arxiv-id": "1309.1204v2"
},{
    "category": "physics.geo-ph", 
    "author": "Shuchin Aeron", 
    "title": "Methods for Large Scale Hydraulic Fracture Monitoring", 
    "publish": "2013-09-05T21:47:04Z", 
    "summary": "In this paper we propose computationally efficient and robust methods for\nestimating the moment tensor and location of micro-seismic event(s) for large\nsearch volumes. Our contribution is two-fold. First, we propose a novel\njoint-complexity measure, namely the sum of nuclear norms which while imposing\nsparsity on the number of fractures (locations) over a large spatial volume,\nalso captures the rank-1 nature of the induced wavefield pattern. This\nwavefield pattern is modeled as the outer-product of the source signature with\nthe amplitude pattern across the receivers from a seismic source. A rank-1\nfactorization of the estimated wavefield pattern at each location can therefore\nbe used to estimate the seismic moment tensor using the knowledge of the array\ngeometry. In contrast to existing work this approach allows us to drop any\nother assumption on the source signature. Second, we exploit the recently\nproposed first-order incremental projection algorithms for a fast and efficient\nimplementation of the resulting optimization problem and develop a hybrid\nstochastic & deterministic algorithm which results in significant computational\nsavings.", 
    "link": "http://arxiv.org/pdf/1309.1496v2", 
    "arxiv-id": "1309.1496v2"
},{
    "category": "physics.ins-det", 
    "author": "Nicolas Boulant", 
    "title": "On Variant Strategies To Solve The Magnitude Least Squares Optimization   Problem In Parallel Transmission Pulse Design And Under Strict SAR And Power   Constraints", 
    "publish": "2013-09-06T08:48:31Z", 
    "summary": "Parallel transmission has been a very promising candidate technology to\nmitigate the inevitable radio-frequency field inhomogeneity in magnetic\nresonance imaging (MRI) at ultra-high field (UHF). For the first few years,\npulse design utilizing this technique was expressed as a least squares problem\nwith crude power regularizations aimed at controlling the specific absorption\nrate (SAR), hence the patient safety. This approach being suboptimal for many\napplications sensitive mostly to the magnitude of the spin excitation, and not\nits phase, the magnitude least squares (MLS) problem then was first formulated\nin 2007. Despite its importance and the availability of other powerful\nnumerical optimization methods, this problem yet has been faced exclusively by\nthe pulse designer with the so-called variable exchange method. In this paper,\nwe investigate other strategies and incorporate directly the strict SAR and\nhardware constraints. Different schemes such as sequential quadratic\nprogramming (SQP), interior point (I-P) methods, semi-definite programming\n(SDP) and magnitude squared least squares (MSLS) relaxations are studied both\nin the small and large tip angle regimes with real data sets obtained in-vivo\non a human brain at 7 Tesla. Convergence and robustness of the different\napproaches are analyzed, and recommendations to tackle this specific problem\nare finally given. Small tip angle and inversion pulses are returned in a few\nseconds and in under a minute respectively while respecting the constraints,\nallowing the use of the proposed approach in routine.", 
    "link": "http://arxiv.org/pdf/1309.1567v3", 
    "arxiv-id": "1309.1567v3"
},{
    "category": "cs.CE", 
    "author": "Richard Vuduc", 
    "title": "Sustainable Software Development for Next-Gen Sequencing (NGS)   Bioinformatics on Emerging Platforms", 
    "publish": "2013-09-07T06:51:49Z", 
    "summary": "DNA sequence analysis is fundamental to life science research. The rapid\ndevelopment of next generation sequencing (NGS) technologies, and the richness\nand diversity of applications it makes feasible, have created an enormous gulf\nbetween the potential of this technology and the development of computational\nmethods to realize this potential. Bridging this gap holds possibilities for\nbroad impacts toward multiple grand challenges and offers unprecedented\nopportunities for software innovation and research. We argue that NGS-enabled\napplications need a critical mass of sustainable software to benefit from\nemerging computing platforms' transformative potential. Accumulating the\nnecessary critical mass will require leaders in computational biology,\nbioinformatics, computer science, and computer engineering work together to\nidentify core opportunity areas, critical software infrastructure, and software\nsustainability challenges. Furthermore, due to the quickly changing nature of\nboth bioinformatics software and accelerator technology, we conclude that\ncreating sustainable accelerated bioinformatics software means constructing a\nsustainable bridge between the two fields. In particular, sustained\ncollaboration between domain developers and technology experts is needed to\ndevelop the accelerated kernels, libraries, frameworks and middleware that\ncould provide the needed flexible link from NGS bioinformatics applications to\nemerging platforms.", 
    "link": "http://arxiv.org/pdf/1309.1828v2", 
    "arxiv-id": "1309.1828v2"
},{
    "category": "cs.NE", 
    "author": "Dr. Prabin Kumar Panigrahi", 
    "title": "A Neural Network based Approach for Predicting Customer Churn in   Cellular Network Services", 
    "publish": "2013-09-16T13:20:30Z", 
    "summary": "Marketing literature states that it is more costly to engage a new customer\nthan to retain an existing loyal customer. Churn prediction models are\ndeveloped by academics and practitioners to effectively manage and control\ncustomer churn in order to retain existing customers. As churn management is an\nimportant activity for companies to retain loyal customers, the ability to\ncorrectly predict customer churn is necessary. As the cellular network services\nmarket becoming more competitive, customer churn management has become a\ncrucial task for mobile communication operators. This paper proposes a neural\nnetwork based approach to predict customer churn in subscription of cellular\nwireless services. The results of experiments indicate that neural network\nbased approach can predict customer churn.", 
    "link": "http://arxiv.org/pdf/1309.3945v1", 
    "arxiv-id": "1309.3945v1"
},{
    "category": "cs.CE", 
    "author": "Carolyn Talcott", 
    "title": "The Immune System: the ultimate fractionated cyber-physical system", 
    "publish": "2013-09-20T01:45:29Z", 
    "summary": "In this little vision paper we analyze the human immune system from a\ncomputer science point of view with the aim of understanding the architecture\nand features that allow robust, effective behavior to emerge from local sensing\nand actions. We then recall the notion of fractionated cyber-physical systems,\nand compare and contrast this to the immune system. We conclude with some\nchallenges.", 
    "link": "http://arxiv.org/pdf/1309.5145v1", 
    "arxiv-id": "1309.5145v1"
},{
    "category": "cs.CE", 
    "author": "Jan Egger", 
    "title": "Image-guided therapy system for interstitial gynecologic brachytherapy   in a multimodality operating suite", 
    "publish": "2013-09-22T07:51:10Z", 
    "summary": "In this contribution, an image-guided therapy system supporting gynecologic\nradiation therapy is introduced. The overall workflow of the presented system\nstarts with the arrival of the patient and ends with follow-up examinations by\nimaging and a superimposed visualization of the modeled device from a PACS\nsystem. Thereby, the system covers all treatments stages (pre-, intra- and\npostoperative) and has been designed and constructed by a computer scientist\nwith feedback from an interdisciplinary team of physicians and engineers. This\nintegrated medical system enables dispatch of diagnostic images directly after\nacquisition to a processing workstation that has an on-board 3D Computer Aided\nDesign model of a medical device. Thus, allowing precise identification of\ncatheter location in the 3D imaging model which later provides rapid feedback\nto the clinician regarding device location. Moreover, the system enables the\nability to perform patient-specific pre-implant evaluation by assessing the\nplacement of interstitial needles prior to an intervention via virtual template\nmatching with a diagnostic scan.", 
    "link": "http://arxiv.org/pdf/1309.5574v1", 
    "arxiv-id": "1309.5574v1"
},{
    "category": "cs.DC", 
    "author": "Enol Fern\u00e1ndez del Castillo", 
    "title": "Analysis of Scientific Cloud Computing requirements", 
    "publish": "2013-09-24T10:54:59Z", 
    "summary": "While the requirements of enterprise and web applications have driven the\ndevelopment of Cloud computing, some of its key features, such as customized\nenvironments and rapid elasticity, could also benefit scientific applications.\nHowever, neither virtualization techniques nor Cloud-like access to resources\nis common in scientific computing centers due to the negative perception of the\nimpact that virtualization techniques introduce.\n  In this paper we discuss the feasibility of the IaaS cloud model to satisfy\nsome of the computational science requirements and the main drawbacks that need\nto be addressed by cloud resource providers so that the maximum benefit can be\nobtained from a given cloud infrastructure.", 
    "link": "http://arxiv.org/pdf/1309.6109v2", 
    "arxiv-id": "1309.6109v2"
},{
    "category": "cs.CE", 
    "author": "Marco Antoniotti", 
    "title": "Proceedings Wivace 2013 - Italian Workshop on Artificial Life and   Evolutionary Computation", 
    "publish": "2013-09-27T06:27:53Z", 
    "summary": "The Wivace 2013 Electronic Proceedings in Theoretical Computer Science\n(EPTCS) contain some selected long and short articles accepted for the\npresentation at Wivace 2013 - Italian Workshop on Artificial Life and\nEvolutionary Computation, which was held at the University of Milan-Bicocca,\nMilan, on the 1st and 2nd of July, 2013.", 
    "link": "http://arxiv.org/pdf/1309.7122v1", 
    "arxiv-id": "1309.7122v1"
},{
    "category": "cs.CE", 
    "author": "Roberto Serra", 
    "title": "Recent developments in research on catalytic reaction networks", 
    "publish": "2013-09-30T01:05:00Z", 
    "summary": "Over the last years, analyses performed on a stochastic model of catalytic\nreaction networks have provided some indications about the reasons why wet-lab\nexperiments hardly ever comply with the phase transition typically predicted by\ntheoretical models with regard to the emergence of collectively\nself-replicating sets of molecule (also defined as autocatalytic sets, ACSs), a\nphenomenon that is often observed in nature and that is supposed to have played\na major role in the emergence of the primitive forms of life. The model at\nissue has allowed to reveal that the emerging ACSs are characterized by a\ngeneral dynamical fragility, which might explain the difficulty to observe them\nin lab experiments. In this work, the main results of the various analyses are\nreviewed, with particular regard to the factors able to affect the generic\nproperties of catalytic reactions network, for what concerns, not only the\nprobability of ACSs to be observed, but also the overall activity of the\nsystem, in terms of production of new species, reactions and matter.", 
    "link": "http://arxiv.org/pdf/1309.7686v1", 
    "arxiv-id": "1309.7686v1"
},{
    "category": "cs.CE", 
    "author": "Pasquale Stano", 
    "title": "Chemical communication between synthetic and natural cells: a possible   experimental design", 
    "publish": "2013-09-30T01:05:15Z", 
    "summary": "The bottom-up construction of synthetic cells is one of the most intriguing\nand interesting research arenas in synthetic biology. Synthetic cells are built\nby encapsulating biomolecules inside lipid vesicles (liposomes), allowing the\nsynthesis of one or more functional proteins. Thanks to the in situ synthesized\nproteins, synthetic cells become able to perform several biomolecular\nfunctions, which can be exploited for a large variety of applications. This\npaves the way to several advanced uses of synthetic cells in basic science and\nbiotechnology, thanks to their versatility, modularity, biocompatibility, and\nprogrammability. In the previous WIVACE (2012) we presented the\nstate-of-the-art of semi-synthetic minimal cell (SSMC) technology and\nintroduced, for the first time, the idea of chemical communication between\nsynthetic cells and natural cells. The development of a proper synthetic\ncommunication protocol should be seen as a tool for the nascent field of\nbio/chemical-based Information and Communication Technologies (bio-chem-ICTs)\nand ultimately aimed at building soft-wet-micro-robots. In this contribution\n(WIVACE, 2013) we present a blueprint for realizing this project, and show some\npreliminary experimental results. We firstly discuss how our research goal\n(based on the natural capabilities of biological systems to manipulate chemical\nsignals) finds a proper place in the current scientific and technological\ncontexts. Then, we shortly comment on the experimental approaches from the\nviewpoints of (i) synthetic cell construction, and (ii) bioengineering of\nmicroorganisms, providing up-to-date results from our laboratory. Finally, we\nshortly discuss how autopoiesis can be used as a theoretical framework for\ndefining synthetic minimal life, minimal cognition, and as bridge between\nsynthetic biology and artificial intelligence.", 
    "link": "http://arxiv.org/pdf/1309.7687v1", 
    "arxiv-id": "1309.7687v1"
},{
    "category": "cs.CE", 
    "author": "Borys Wr\u00f3bel", 
    "title": "Evolution and development of complex computational systems using the   paradigm of metabolic computing in Epigenetic Tracking", 
    "publish": "2013-09-30T01:05:30Z", 
    "summary": "Epigenetic Tracking (ET) is an Artificial Embryology system which allows for\nthe evolution and development of large complex structures built from artificial\ncells. In terms of the number of cells, the complexity of the bodies generated\nwith ET is comparable with the complexity of biological organisms. We have\npreviously used ET to simulate the growth of multicellular bodies with\narbitrary 3-dimensional shapes which perform computation using the paradigm of\n\"metabolic computing\". In this paper we investigate the memory capacity of such\ncomputational structures and analyse the trade-off between shape and\ncomputation. We now plan to build on these foundations to create a\nbiologically-inspired model in which the encoding of the phenotype is efficient\n(in terms of the compactness of the genome) and evolvable in tasks involving\nnon-trivial computation, robust to damage and capable of self-maintenance and\nself-repair.", 
    "link": "http://arxiv.org/pdf/1309.7688v1", 
    "arxiv-id": "1309.7688v1"
},{
    "category": "cs.CE", 
    "author": "Giovanni Pardini", 
    "title": "Application of a Semi-automatic Algorithm for Identification of   Molecular Components in SBML Models", 
    "publish": "2013-09-30T01:05:43Z", 
    "summary": "Reactions forming a pathway can be rewritten by making explicit the different\nmolecular components involved in them. A molecular component represents a\nbiological entity (e.g. a protein) in all its states (free, bound, degraded,\netc.). In this paper we show the application of a component identification\nalgorithm to a number of real-world models to experimentally validate the\napproach. Components identification allows subpathways to be computed to better\nunderstand the pathway functioning.", 
    "link": "http://arxiv.org/pdf/1309.7689v1", 
    "arxiv-id": "1309.7689v1"
},{
    "category": "cs.NE", 
    "author": "Giancarlo Mauri", 
    "title": "A Hybrid Monte Carlo Ant Colony Optimization Approach for Protein   Structure Prediction in the HP Model", 
    "publish": "2013-09-30T01:05:51Z", 
    "summary": "The hydrophobic-polar (HP) model has been widely studied in the field of\nprotein structure prediction (PSP) both for theoretical purposes and as a\nbenchmark for new optimization strategies. In this work we introduce a new\nheuristics based on Ant Colony Optimization (ACO) and Markov Chain Monte Carlo\n(MCMC) that we called Hybrid Monte Carlo Ant Colony Optimization (HMCACO). We\ndescribe this method and compare results obtained on well known HP instances in\nthe 3 dimensional cubic lattice to those obtained with standard ACO and\nSimulated Annealing (SA). All methods were implemented using an unconstrained\nneighborhood and a modified objective function to prevent the creation of\noverlapping walks. Results show that our methods perform better than the other\nheuristics in all benchmark instances.", 
    "link": "http://arxiv.org/pdf/1309.7690v1", 
    "arxiv-id": "1309.7690v1"
}]