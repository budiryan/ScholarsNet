[{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0002010v1", 
    "title": "Biologically Motivated Distributed Designs for Adaptive Knowledge   Management", 
    "arxiv-id": "cs/0002010v1", 
    "author": "Johan Bollen", 
    "publish": "2000-02-16T18:22:32Z", 
    "summary": "We discuss how distributed designs that draw from biological network\nmetaphors can largely improve the current state of information retrieval and\nknowledge management of distributed information systems. In particular, two\nadaptive recommendation systems named TalkMine and @ApWeb are discussed in more\ndetail. TalkMine operates at the semantic level of keywords. It leads different\ndatabases to learn new and adapt existing keywords to the categories recognized\nby its communities of users using distributed algorithms. @ApWeb operates at\nthe structural level of information resources, namely citation or hyperlink\nstructure. It relies on collective behavior to adapt such structure to the\nexpectations of users. TalkMine and @ApWeb are currently being implemented for\nthe research library of the Los Alamos National Laboratory under the Active\nRecommendation Project. Together they define a biologically motivated\ninformation retrieval system, recommending simultaneously at the level of user\nknowledge categories expressed in keywords, and at the level of individual\ndocuments and their associations to other documents. Rather than passive\ninformation retrieval, with this system, users obtain an active, evolving\ninteraction with information resources."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0003001v1", 
    "title": "Making news understandable to computers", 
    "arxiv-id": "cs/0003001v1", 
    "author": "Erik T. Mueller", 
    "publish": "2000-03-01T18:11:08Z", 
    "summary": "Computers and devices are largely unaware of events taking place in the\nworld. This could be changed if news were made available in a\ncomputer-understandable form. In this paper we present XML documents called\nNewsForms that represent the key points of 17 types of news events. We discuss\nthe benefits of computer-understandable news and present the NewsExtract\nprogram for converting text news stories into NewsForms."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0007017v1", 
    "title": "Fuzzy data: XML may handle it", 
    "arxiv-id": "cs/0007017v1", 
    "author": "J. Dudeck", 
    "publish": "2000-07-13T07:58:53Z", 
    "summary": "Data modeling is one of the most difficult tasks in application engineering.\nThe engineer must be aware of the use cases and the required application\nservices and at a certain point of time he has to fix the data model which\nforms the base for the application services. However, once the data model has\nbeen fixed it is difficult to consider changing needs. This might be a problem\nin specific domains, which are as dynamic as the healthcare domain. With fuzzy\ndata we address all those data that are difficult to organize in a single\ndatabase. In this paper we discuss a gradual and pragmatic approach that uses\nthe XML technology to conquer more model flexibility. XML may provide the clue\nbetween unstructured text data and structured database solutions and shift the\nparadigm from \"organizing the data along a given model\" towards \"organizing the\ndata along user requirements\"."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0102002v1", 
    "title": "On the Automated Classification of Web Sites", 
    "arxiv-id": "cs/0102002v1", 
    "author": "John M. Pierre", 
    "publish": "2001-02-01T23:03:49Z", 
    "summary": "In this paper we discuss several issues related to automated text\nclassification of web sites. We analyze the nature of web content and metadata\nin relation to requirements for text features. We find that HTML metatags are a\ngood source of text features, but are not in wide use despite their role in\nsearch engine rankings. We present an approach for targeted spidering including\nmetadata extraction and opportunistic crawling of specific semantic hyperlinks.\nWe describe a system for automatically classifying web sites into industry\ncategories and present performance results based on different combinations of\ntext features and training data. This system can serve as the basis for a\ngeneralized framework for automated metadata creation."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0209021v1", 
    "title": "Activities, Context and Ubiquitous Computing", 
    "arxiv-id": "cs/0209021v1", 
    "author": "Mark Burnett", 
    "publish": "2002-09-19T06:53:51Z", 
    "summary": "Context and context-awareness provides computing environments with the\nability to usefully adapt the services or information they provide. It is the\nability to implicitly sense and automatically derive the user needs that\nseparates context-aware applications from traditionally designed applications,\nand this makes them more attentive, responsive, and aware of their user's\nidentity, and their user's environment. This paper argues that context-aware\napplications capable of supporting complex, cognitive activities can be built\nfrom a model of context called Activity-Centric Context. A conceptual model of\nActivity-Centric context is presented. The model is illustrated via a detailed\nexample."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0306021v2", 
    "title": "Visualization for Periodic Population Movement between Distinct   Localities", 
    "arxiv-id": "cs/0306021v2", 
    "author": "Alexander Haubold", 
    "publish": "2003-06-04T19:40:04Z", 
    "summary": "We present a new visualization method to summarize and present periodic\npopulation movement between distinct locations, such as floors, buildings,\ncities, or the like. In the specific case of this paper, we have chosen to\nfocus on student movement between college dormitories on the Columbia\nUniversity campus. The visual information is presented to the information\nanalyst in the form of an interactive geographical map, in which specific\ntemporal periods as well as individual buildings can be singled out for\ndetailed data exploration. The navigational interface has been designed to\nspecifically meet a geographical setting."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0306026v1", 
    "title": "BdbServer++: A User Driven Data Location and Retrieval Tool", 
    "arxiv-id": "cs/0306026v1", 
    "author": "D. Boutigany", 
    "publish": "2003-06-05T12:20:32Z", 
    "summary": "The adoption of Grid technology has the potential to greatly aid the BaBar\nexperiment. BdbServer was originally designed to extract copies of data from\nthe Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple\nlocations in a variety of data formats, we are enhancing this tool. This will\nenable users to extract selected deep copies of event collections and ship them\nto the requested site using the facilities offered by the existing Grid\ninfrastructure. By building on the work done by various groups in BaBar, and\nthe European DataGrid, we have successfully expanded the capabilities of the\nBdbServer software. This should provide a framework for future work in data\ndistribution."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0306094v1", 
    "title": "BaBar - A Community Web Site in an Organizational Setting", 
    "arxiv-id": "cs/0306094v1", 
    "author": "Bebo White", 
    "publish": "2003-06-16T06:12:04Z", 
    "summary": "The BABAR Web site was established in 1993 at the Stanford Linear Accelerator\nCenter (SLAC) to support the BABAR experiment, to report its results, and to\nfacilitate communication among its scientific and engineering collaborators,\ncurrently numbering about 600 individuals from 75 collaborating institutions in\n10 countries. The BABAR Web site is, therefore, a community Web site. At the\nsame time it is hosted at SLAC and funded by agencies that demand adherence to\npolicies decided under different priorities. Additionally, the BABAR Web\nadministrators deal with the problems that arise during the course of managing\nusers, content, policies, standards, and changing technologies. Desired\nsolutions to some of these problems may be incompatible with the overall\nadministration of the SLAC Web sites and/or the SLAC policies and concerns.\nThere are thus different perspectives of the same Web site and differing\nexpectations in segments of the SLAC population which act as constraints and\nchallenges in any review or re-engineering activities. Web Engineering, which\npost-dates the BABAR Web, has aimed to provide a comprehensive understanding of\nall aspects of Web development. This paper reports on the first part of a\nrecent review of application of Web Engineering methods to the BABAR Web site,\nwhich has led to explicit user and information models of the BABAR community\nand how SLAC and the BABAR community relate and react to each other. The paper\nidentifies the issues of a community Web site in a hierarchical,\nsemi-governmental sector and formulates a strategy for periodic reviews of\nBABAR and similar sites."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0308042v1", 
    "title": "Centralized reward system gives rise to fast and efficient work sharing   for intelligent Internet agents lacking direct communication", 
    "arxiv-id": "cs/0308042v1", 
    "author": "Andras Lorincz", 
    "publish": "2003-08-27T13:32:29Z", 
    "summary": "WWW has a scale-free structure where novel information is often difficult to\nlocate. Moreover, Intelligent agents easily get trapped in this structure. Here\na novel method is put forth, which turns these traps into information\nrepositories, supplies: We populated an Internet environment with intelligent\nnews foragers. Foraging has its associated cost whereas foragers are rewarded\nif they detect not yet discovered novel information. The intelligent news\nforagers crawl by using the estimated long-term cumulated reward, and also have\na finite sized memory: the list of most promising supplies. Foragers form an\nartificial life community: the most successful ones are allowed to multiply,\nwhile unsuccessful ones die out. The specific property of this community is\nthat there is no direct communication amongst foragers but the centralized\nrewarding system. Still, fast division of work is achieved."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0402061v1", 
    "title": "A Correlation-Based Distance", 
    "arxiv-id": "cs/0402061v1", 
    "author": "Paul Albuquerque", 
    "publish": "2004-02-27T14:13:01Z", 
    "summary": "In this short technical report, we define on the sample space R^D a distance\nbetween data points which depends on their correlation. We also derive an\nexpression for the center of mass of a set of points with respect to this\ndistance."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0410055v1", 
    "title": "Mathematical knowledge management is needed", 
    "arxiv-id": "cs/0410055v1", 
    "author": "Michiel Hazewinkel", 
    "publish": "2004-10-21T15:51:20Z", 
    "summary": "In this lecture I discuss some aspects of MKM, Mathematical Knowledge\nManagement, with particuar emphasis on information storage and information\nretrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0411026v1", 
    "title": "A Search Relevancy Tuning Method Using Expert Results Content Evaluation", 
    "arxiv-id": "cs/0411026v1", 
    "author": "Boris Mark Tylevich", 
    "publish": "2004-11-08T20:49:42Z", 
    "summary": "The article presents an online relevancy tuning method using explicit user\nfeedback. The author developed and tested a method of words' weights\nmodification based on search result evaluation by user. User decides whether\nthe result is useful or not after inspecting the full result content. The\nexperiment proved that the constantly accumulated words weights base leads to\nbetter search quality in a specified data domain. The author also suggested\nfuture improvements of the method."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0503011v1", 
    "title": "Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of   Search Engine Results", 
    "arxiv-id": "cs/0503011v1", 
    "author": "Soumen Chakrabarti", 
    "publish": "2005-03-04T10:29:34Z", 
    "summary": "In-degree, PageRank, number of visits and other measures of Web page\npopularity significantly influence the ranking of search results by modern\nsearch engines. The assumption is that popularity is closely correlated with\nquality, a more elusive concept that is difficult to measure directly.\nUnfortunately, the correlation between popularity and quality is very weak for\nnewly-created pages that have yet to receive many visits and/or in-links.\nWorse, since discovery of new content is largely done by querying search\nengines, and because users usually focus their attention on the top few\nresults, newly-created but high-quality pages are effectively ``shut out,'' and\nit can take a very long time before they become popular.\n  We propose a simple and elegant solution to this problem: the introduction of\na controlled amount of randomness into search result ranking methods. Doing so\noffers new pages a chance to prove their worth, although clearly using too much\nrandomness will degrade result quality and annul any benefits achieved. Hence\nthere is a tradeoff between exploration to estimate the quality of new pages\nand exploitation of pages already known to be of high quality. We study this\ntradeoff both analytically and via simulation, in the context of an economic\nobjective function based on aggregate result quality amortized over time. We\nshow that a modest amount of randomness leads to improved search results."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0503020v1", 
    "title": "Earlier Web Usage Statistics as Predictors of Later Citation Impact", 
    "arxiv-id": "cs/0503020v1", 
    "author": "Stevan Harnad", 
    "publish": "2005-03-08T22:26:07Z", 
    "summary": "The use of citation counts to assess the impact of research articles is well\nestablished. However, the citation impact of an article can only be measured\nseveral years after it has been published. As research articles are\nincreasingly accessed through the Web, the number of times an article is\ndownloaded can be instantly recorded and counted. One would expect the number\nof times an article is read to be related both to the number of times it is\ncited and to how old the article is. This paper analyses how short-term Web\nusage impact predicts medium-term citation impact. The physics e-print archive\n(arXiv.org) is used to test this."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0503021v1", 
    "title": "Fast-Forward on the Green Road to Open Access: The Case Against Mixing   Up Green and Gold", 
    "arxiv-id": "cs/0503021v1", 
    "author": "Stevan Harnad", 
    "publish": "2005-03-08T22:44:37Z", 
    "summary": "This article is a critique of: \"The 'Green' and 'Gold' Roads to Open Access:\nThe Case for Mixing and Matching\" by Jean-Claude Guedon (in Serials Review\n30(4) 2004).\n  Open Access (OA) means: free online access to all peer-reviewed journal\narticles. Jean-Claude Guedon argues against the efficacy of author\nself-archiving of peer-reviewed journal articles (the \"Green\" road to OA). He\nsuggests instead that we should convert to Open Access Publishing (the \"Golden\"\nroad to OA) by \"mixing and matching\" Green and Gold as follows: o First,\nself-archive dissertations (not published, peer-reviewed journal articles). o\nSecond, identify and tag how those dissertations have been evaluated and\nreviewed. o Third, self-archive unrefereed preprints (not published,\npeer-reviewed journal articles). o Fourth, develop new mechanisms for\nevaluating and reviewing those unrefereed preprints, at multiple levels. The\nresult will be OA Publishing (Gold). I argue that rather than yet another 10\nyears of speculation like this, what is actually needed (and imminent) is for\nOA self-archiving to be mandated by research funders and institutions so that\nthe self-archiving of published, peer-reviewed journal articles (Green) can be\nfast-forwarded to 100% OA."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0505039v1", 
    "title": "Methods for comparing rankings of search engine results", 
    "arxiv-id": "cs/0505039v1", 
    "author": "Mark Levene", 
    "publish": "2005-05-14T17:48:07Z", 
    "summary": "In this paper we present a number of measures that compare rankings of search\nengine results. We apply these measures to five queries that were monitored\ndaily for two periods of about 21 days each. Rankings of the different search\nengines (Google, Yahoo and Teoma for text searches and Google, Yahoo and\nPicsearch for image searches) are compared on a daily basis, in addition to\nlongitudinal comparisons of the same engine for the same query over time. The\nresults and rankings of the two periods are compared as well."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0506047v1", 
    "title": "Analyse et expansion des textes en question-r\u00e9ponse", 
    "arxiv-id": "cs/0506047v1", 
    "author": "Bernard Jacquemin", 
    "publish": "2005-06-12T16:39:01Z", 
    "summary": "This paper presents an original methodology to consider question answering.\nWe noticed that query expansion is often incorrect because of a bad\nunderstanding of the question. But the automatic good understanding of an\nutterance is linked to the context length, and the question are often short.\nThis methodology proposes to analyse the documents and to construct an\ninformative structure from the results of the analysis and from a semantic text\nexpansion. The linguistic analysis identifies words (tokenization and\nmorphological analysis), links between words (syntactic analysis) and word\nsense (semantic disambiguation). The text expansion adds to each word the\nsynonyms matching its sense and replaces the words in the utterances by\nderivatives, modifying the syntactic schema if necessary. In this way, whatever\nenrichment may be, the text keeps the same meaning, but each piece of\ninformation matches many realisations. The questioning method consists in\nconstructing a local informative structure without enrichment, and matches it\nwith the documentary structure. If a sentence in the informative structure\nmatches the question structure, this sentence is the answer to the question."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0506048v1", 
    "title": "Enriching a Text by Semantic Disambiguation for Information Extraction", 
    "arxiv-id": "cs/0506048v1", 
    "author": "Claude Roux", 
    "publish": "2005-06-12T16:44:05Z", 
    "summary": "External linguistic resources have been used for a very long time in\ninformation extraction. These methods enrich a document with data that are\nsemantically equivalent, in order to improve recall. For instance, some of\nthese methods use synonym dictionaries. These dictionaries enrich a sentence\nwith words that have a similar meaning. However, these methods present some\nserious drawbacks, since words are usually synonyms only in restricted\ncontexts. The method we propose here consists of using word sense\ndisambiguation rules (WSD) to restrict the selection of synonyms to only these\nthat match a specific syntactico-semantic context. We show how WSD rules are\nbuilt and how information extraction techniques can benefit from the\napplication of these rules."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0507024v2", 
    "title": "Experiments in Clustering Homogeneous XML Documents to Validate an   Existing Typology", 
    "arxiv-id": "cs/0507024v2", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-07-08T13:42:42Z", 
    "summary": "This paper presents some experiments in clustering homogeneous XMLdocuments\nto validate an existing classification or more generally anorganisational\nstructure. Our approach integrates techniques for extracting knowledge from\ndocuments with unsupervised classification (clustering) of documents. We focus\non the feature selection used for representing documents and its impact on the\nemerging classification. We mix the selection of structured features with fine\ntextual selection based on syntactic characteristics.We illustrate and evaluate\nthis approach with a collection of Inria activity reports for the year 2003.\nThe objective is to cluster projects into larger groups (Themes), based on the\nkeywords or different chapters of these activity reports. We then compare the\nresults of clustering using different feature selections, with the official\ntheme structure used by Inria."
},{
    "category": "cs.IR", 
    "doi": "10.1177/1461444812452411", 
    "link": "http://arxiv.org/pdf/cs/0507069v1", 
    "title": "Users and Assessors in the Context of INEX: Are Relevance Dimensions   Relevant?", 
    "arxiv-id": "cs/0507069v1", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-07-28T15:02:04Z", 
    "summary": "The main aspects of XML retrieval are identified by analysing and comparing\nthe following two behaviours: the behaviour of the assessor when judging the\nrelevance of returned document components; and the behaviour of users when\ninteracting with components of XML documents. We argue that the two INEX\nrelevance dimensions, Exhaustivity and Specificity, are not orthogonal\ndimensions; indeed, an empirical analysis of each dimension reveals that the\ngrades of the two dimensions are correlated to each other. By analysing the\nlevel of agreement between the assessor and the users, we aim at identifying\nthe best units of retrieval. The results of our analysis show that the highest\nlevel of agreement is on highly relevant and on non-relevant document\ncomponents, suggesting that only the end points of the INEX 10-point relevance\nscale are perceived in the same way by both the assessor and the users. We\npropose a new definition of relevance for XML retrieval and argue that its\ncorresponding relevance scale would be a better choice for INEX."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0507070v1", 
    "title": "Hybrid XML Retrieval: Combining Information Retrieval and a Native XML   Database", 
    "arxiv-id": "cs/0507070v1", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-07-28T19:19:12Z", 
    "summary": "This paper investigates the impact of three approaches to XML retrieval:\nusing Zettair, a full-text information retrieval system; using eXist, a native\nXML database; and using a hybrid system that takes full article answers from\nZettair and uses eXist to extract elements from those articles. For the\ncontent-only topics, we undertake a preliminary analysis of the INEX 2003\nrelevance assessments in order to identify the types of highly relevant\ndocument components. Further analysis identifies two complementary sub-cases of\nrelevance assessments (\"General\" and \"Specific\") and two categories of topics\n(\"Broad\" and \"Narrow\"). We develop a novel retrieval module that for a\ncontent-only topic utilises the information from the resulting answer list of a\nnative XML database and dynamically determines the preferable units of\nretrieval, which we call \"Coherent Retrieval Elements\". The results of our\nexperiments show that -- when each of the three systems is evaluated against\ndifferent retrieval scenarios (such as different cases of relevance\nassessments, different topic categories and different choices of evaluation\nmetrics) -- the XML retrieval systems exhibit varying behaviour and the best\nperformance can be reached for different values of the retrieval parameters. In\nthe case of INEX 2003 relevance assessments for the content-only topics, our\nnewly developed hybrid XML retrieval system is substantially more effective\nthan either Zettair or eXist, and yields a robust and a very effective XML\nretrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0508017v1", 
    "title": "Enhancing Content-And-Structure Information Retrieval using a Native XML   Database", 
    "arxiv-id": "cs/0508017v1", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-08-02T15:05:18Z", 
    "summary": "Three approaches to content-and-structure XML retrieval are analysed in this\npaper: first by using Zettair, a full-text information retrieval system; second\nby using eXist, a native XML database, and third by using a hybrid XML\nretrieval system that uses eXist to produce the final answers from likely\nrelevant articles retrieved by Zettair. INEX 2003 content-and-structure topics\ncan be classified in two categories: the first retrieving full articles as\nfinal answers, and the second retrieving more specific elements within articles\nas final answers. We show that for both topic categories our initial hybrid\nsystem improves the retrieval effectiveness of a native XML database. For\nranking the final answer elements, we propose and evaluate a novel retrieval\nmodel that utilises the structural relationships between the answer elements of\na native XML database and retrieves Coherent Retrieval Elements. The final\nresults of our experiments show that when the XML retrieval task focusses on\nhighly relevant elements our hybrid XML retrieval system with the Coherent\nRetrieval Elements module is 1.8 times more effective than Zettair and 3 times\nmore effective than eXist, and yields an effective content-and-structure XML\nretrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0508036v2", 
    "title": "Exp\u00e9riences de classification d'une collection de documents XML de   structure homog\u00e8ne", 
    "arxiv-id": "cs/0508036v2", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-08-04T14:14:59Z", 
    "summary": "This paper presents some experiments in clustering homogeneous XMLdocuments\nto validate an existing classification or more generally anorganisational\nstructure. Our approach integrates techniques for extracting knowledge from\ndocuments with unsupervised classification (clustering) of documents. We focus\non the feature selection used for representing documents and its impact on the\nemerging classification. We mix the selection of structured features with fine\ntextual selection based on syntactic characteristics.We illustrate and evaluate\nthis approach with a collection of Inria activity reports for the year 2003.\nThe objective is to cluster projects into larger groups (Themes), based on the\nkeywords or different chapters of these activity reports. We then compare the\nresults of clustering using different feature selections, with the official\ntheme structure used by Inria."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0509005v1", 
    "title": "Combining Structured Corporate Data and Document Content to Improve   Expertise Finding", 
    "arxiv-id": "cs/0509005v1", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2005-09-02T08:24:07Z", 
    "summary": "In this paper, we present an algorithm for automatically building expertise\nevidence for finding experts within an organization by combining structured\ncorporate information with different content. We also describe our test data\ncollection and our evaluation method. Evaluation of the algorithm shows that\nusing organizational structure leads to a significant improvement in the\nprecision of finding an expert. Furthermore we evaluate the impact of using\ndifferent data sources on the quality of the results and conclude that Expert\nFinding is not a \"one engine fits all\" solution. It requires an analysis of the\ninformation space into which a solution will be placed and the appropriate\nselection and weighting scheme of the data sources."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0510025v1", 
    "title": "Practical Semantic Analysis of Web Sites and Documents", 
    "arxiv-id": "cs/0510025v1", 
    "author": "Thierry Despeyroux", 
    "publish": "2005-10-11T08:40:05Z", 
    "summary": "As Web sites are now ordinary products, it is necessary to explicit the\nnotion of quality of a Web site. The quality of a site may be linked to the\neasiness of accessibility and also to other criteria such as the fact that the\nsite is up to date and coherent. This last quality is difficult to insure\nbecause sites may be updated very frequently, may have many authors, may be\npartially generated and in this context proof-reading is very difficult. The\nsame piece of information may be found in different occurrences, but also in\ndata or meta-data, leading to the need for consistency checking. In this paper\nwe make a parallel between programs and Web sites. We present some examples of\nsemantic constraints that one would like to specify (constraints between the\nmeaning of categories and sub-categories in a thematic directory, consistency\nbetween the organization chart and the rest of the site in an academic site).\nWe present quickly the Natural Semantics, a way to specify the semantics of\nprogramming languages that inspires our works. Then we propose a specification\nlanguage for semantic constraints in Web sites that, in conjunction with the\nwell known ``make'' program, permits to generate some site verification tools\nby compiling the specification into Prolog code. We apply our method to a large\nXML document which is the scientific part of our institute activity report,\ntracking errors or inconsistencies and also constructing some indicators that\ncan be used by the management of the institute."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0512032v1", 
    "title": "A Software Framework for Vehicle-Infrastructure Cooperative Applications", 
    "arxiv-id": "cs/0512032v1", 
    "author": "Michel Parent", 
    "publish": "2005-12-08T09:37:49Z", 
    "summary": "A growing category of vehicle-infrastructure cooperative (VIC) applications\nrequires telematics software components distributed between an\ninfrastructure-based management center and a number of vehicles. This article\npresents an approach based on a software framework, focusing on a Telematic\nManagement System (TMS), a component suite aimed to run inside an\ninfrastructure-based operations center, in some cases interacting with legacy\nsystems like Advanced Traffic Management Systems or Vehicle Relationship\nManagement. The TMS framework provides support for modular, flexible,\nprototyping and implementation of VIC applications. This work has received the\nsupport of the European Commission in the context of the projects REACT and\nCyberCars."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0512085v1", 
    "title": "Analyzing and Visualizing the Semantic Coverage of Wikipedia and Its   Authors", 
    "arxiv-id": "cs/0512085v1", 
    "author": "Katy B\u00f6rner", 
    "publish": "2005-12-21T19:31:23Z", 
    "summary": "This paper presents a novel analysis and visualization of English Wikipedia\ndata. Our specific interest is the analysis of basic statistics, the\nidentification of the semantic structure and age of the categories in this free\nonline encyclopedia, and the content coverage of its highly productive authors.\nThe paper starts with an introduction of Wikipedia and a review of related\nwork. We then introduce a suite of measures and approaches to analyze and map\nthe semantic structure of Wikipedia. The results show that co-occurrences of\ncategories within individual articles have a power-law distribution, and when\nmapped reveal the nicely clustered semantic structure of Wikipedia. The results\nalso reveal the content coverage of the article's authors, although the roles\nthese authors play are as varied as the authors themselves. We conclude with a\ndiscussion of major results and planned future work."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0601103v1", 
    "title": "Google Web APIs - an Instrument for Webometric Analyses?", 
    "arxiv-id": "cs/0601103v1", 
    "author": "Fabio Tosques", 
    "publish": "2006-01-24T10:23:15Z", 
    "summary": "This paper introduces Google Web APIs (Google APIs) as an instrument and\nplayground for webometric studies. Several examples of Google APIs\nimplementations are given. Our examples show that this Google Web Service can\nbe used successfully for informetric Internet based studies albeit with some\nrestrictions."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-005-0748-1", 
    "link": "http://arxiv.org/pdf/cs/0606105v1", 
    "title": "Iso9000 Based Advanced Quality Approach for Continuous Improvement of   Manufacturing Processes", 
    "arxiv-id": "cs/0606105v1", 
    "author": "Beno\u00eet Iung", 
    "publish": "2006-06-26T11:51:41Z", 
    "summary": "The continuous improvement in TQM is considered as the core value by which\norganisation could maintain a competitive edge. Several techniques and tools\nare known to support this core value but most of the time these techniques are\ninformal and without modelling the interdependence between the core value and\ntools. Thus, technique formalisation is one of TQM challenges for increasing\nefficiency of quality process implementation. In that way, the paper proposes\nand experiments an advanced quality modelling approach based on meta-modelling\nthe \"process approach\" as advocated by the standard ISO9000:2000. This\nmeta-model allows formalising the interdependence between technique, tools and\ncore value"
},{
    "category": "cs.IR", 
    "doi": "10.1007/11766278\\_34", 
    "link": "http://arxiv.org/pdf/cs/0607012v1", 
    "title": "A Flexible Structured-based Representation for XML Document Mining", 
    "arxiv-id": "cs/0607012v1", 
    "author": "Yves Lechevallier", 
    "publish": "2006-07-05T12:07:16Z", 
    "summary": "This paper reports on the INRIA group's approach to XML mining while\nparticipating in the INEX XML Mining track 2005. We use a flexible\nrepresentation of XML documents that allows taking into account the structure\nonly or both the structure and content. Our approach consists of representing\nXML documents by a set of their sub-paths, defined according to some criteria\n(length, root beginning, leaf ending). By considering those sub-paths as words,\nwe can use standard methods for vocabulary reduction, and simple clustering\nmethods such as K-means that scale well. We actually use an implementation of\nthe clustering algorithm known as \"dynamic clouds\" that can work with distinct\ngroups of independent variables put in separate variables. This is useful in\nour model since embedded sub-paths are not independent: we split potentially\ndependant paths into separate variables, resulting in each of them containing\nindependant paths. Experiments with the INEX collections show good results for\nthe structure-only collections, but our approach could not scale well for large\nstructure-and-content collections."
},{
    "category": "cs.IR", 
    "doi": "10.1007/11766278\\_34", 
    "link": "http://arxiv.org/pdf/cs/0607015v1", 
    "title": "The uncovering of hidden structures by Latent Semantic Analysis", 
    "arxiv-id": "cs/0607015v1", 
    "author": "Eduardo Mizraji", 
    "publish": "2006-07-05T22:56:29Z", 
    "summary": "Latent Semantic Analysis (LSA) is a well known method for information\nretrieval. It has also been applied as a model of cognitive processing and\nword-meaning acquisition. This dual importance of LSA derives from its capacity\nto modulate the meaning of words by contexts, dealing successfully with\npolysemy and synonymy. The underlying reasons that make the method work are not\nclear enough. We propose that the method works because it detects an underlying\nblock structure (the blocks corresponding to topics) in the term by document\nmatrix. In real cases this block structure is hidden because of perturbations.\nWe propose that the correct explanation for LSA must be searched in the\nstructure of singular vectors rather than in the profile of singular values.\nUsing Perron-Frobenius theory we show that the presence of disjoint blocks of\ndocuments is marked by sign-homogeneous entries in the vectors corresponding to\nthe documents of one block and zeros elsewhere. In the case of nearly disjoint\nblocks, perturbation theory shows that if the perturbations are small the zeros\nin the leading vectors are replaced by small numbers (pseudo-zeros). Since the\nsingular values of each block might be very different in magnitude, their order\ndoes not mirror the order of blocks. When the norms of the blocks are similar,\nLSA works fine, but we propose that when the topics have different sizes, the\nusual procedure of selecting the first k singular triplets (k being the number\nof blocks) should be replaced by a method that selects the perturbed Perron\nvectors for each block."
},{
    "category": "cs.IR", 
    "doi": "10.1007/11766278\\_34", 
    "link": "http://arxiv.org/pdf/cs/0608043v1", 
    "title": "Using Users' Expectations to Adapt Business Intelligence Systems", 
    "arxiv-id": "cs/0608043v1", 
    "author": "Odile Thiery", 
    "publish": "2006-08-08T13:19:08Z", 
    "summary": "This paper takes a look at the general characteristics of business or\neconomic intelligence system. The role of the user within this type of system\nis emphasized. We propose two models which we consider important in order to\nadapt this system to the user. The first model is based on the definition of\ndecisional problem and the second on the four cognitive phases of human\nlearning. We also describe the application domain we are using to test these\nmodels in this type of system."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0608107v3", 
    "title": "The Haar Wavelet Transform of a Dendrogram", 
    "arxiv-id": "cs/0608107v3", 
    "author": "Fionn Murtagh", 
    "publish": "2006-08-28T17:05:07Z", 
    "summary": "We describe a new wavelet transform, for use on hierarchies or binary rooted\ntrees. The theoretical framework of this approach to data analysis is\ndescribed. Case studies are used to further exemplify this approach. A first\nset of application studies deals with data array smoothing, or filtering. A\nsecond set of application studies relates to hierarchical tree condensation.\nFinally, a third study explores the wavelet decomposition, and the\nreproducibility of data sets such as text, including a new perspective on the\ngeneration or computability of such data objects."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0610058v1", 
    "title": "Context-sensitive access to e-document corpus", 
    "arxiv-id": "cs/0610058v1", 
    "author": "A. S. Komarova", 
    "publish": "2006-10-11T08:02:14Z", 
    "summary": "The methodology of context-sensitive access to e-documents considers context\nas a problem model based on the knowledge extracted from the application\ndomain, and presented in the form of application ontology. Efficient access to\nan information in the text form is needed. Wiki resources as a modern text\nformat provides huge number of text in a semi formalized structure. At the\nfirst stage of the methodology, documents are indexed against the ontology\nrepresenting macro-situation. The indexing method uses a topic tree as a middle\nlayer between documents and the application ontology. At the second stage\ndocuments relevant to the current situation (the abstract and operational\ncontexts) are identified and sorted by degree of relevance. Abstract context is\na problem-oriented ontology-based model. Operational context is an\ninstantiation of the abstract context with data provided by the information\nsources. The following parts of the methodology are described: (i) metrics for\nmeasuring similarity of e-documents to ontology, (ii) a document index storing\nresults of indexing of e-documents against the ontology; (iii) a method for\nidentification of relevant e-documents based on semantic similarity measures.\nWikipedia (wiki resource) is used as a corpus of e-documents for approach\nevaluation in a case study. Text categorization, the presence of metadata, and\nan existence of a lot of articles related to different topics characterize the\ncorpus."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0611131v3", 
    "title": "Scatter Networks: A New Approach for Analyzing Information Scatter on   the Web", 
    "arxiv-id": "cs/0611131v3", 
    "author": "Xiaolin Shi", 
    "publish": "2006-11-26T22:06:23Z", 
    "summary": "Information on any given topic is often scattered across the web. Previously\nthis scatter has been characterized through the distribution of a set of facts\n(i.e. pieces of information) across web pages, showing that typically a few\npages contain many facts on the topic, while many pages contain just a few.\nWhile such approaches have revealed important scatter phenomena, they are lossy\nin that they conceal how specific facts (e.g. rare facts) occur in specific\ntypes of pages (e.g. fact-rich pages). To reveal such regularities, we\nconstruct bi-partite networks, consisting of two types of vertices: the facts\ncontained in webpages and the webpages themselves. Such a representation\nenables the application of a series of network analysis techniques, revealing\nstructural features such as connectivity, robustness, and clustering. We\ndiscuss the implications of each of these features to the users' ability to\nfind comprehensive information online. Finally, we compare the bipartite graph\nstructure of webpages and facts with the hyperlink structure between the\nwebpages."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0612062v1", 
    "title": "Unifying Lexicons in view of a Phonological and Morphological Lexical DB", 
    "arxiv-id": "cs/0612062v1", 
    "author": "Monica Monachini", 
    "publish": "2006-12-11T14:45:49Z", 
    "summary": "The present work falls in the line of activities promoted by the European\nLanguguage Resource Association (ELRA) Production Committee (PCom) and raises\nissues in methods, procedures and tools for the reusability, creation, and\nmanagement of Language Resources. A two-fold purpose lies behind this\nexperiment. The first aim is to investigate the feasibility, define methods and\nprocedures for combining two Italian lexical resources that have incompatible\nformats and complementary information into a Unified Lexicon (UL). The adopted\nstrategy and the procedures appointed are described together with the driving\ncriterion of the merging task, where a balance between human and computational\nefforts is pursued. The coverage of the UL has been maximized, by making use of\nsimple and fast matching procedures. The second aim is to exploit this newly\nobtained resource for implementing the phonological and morphological layers of\nthe CLIPS lexical database. Implementing these new layers and linking them with\nthe already exisitng syntactic and semantic layers is not a trivial task. The\nconstraints imposed by the model, the impact at the architectural level and the\nsolution adopted in order to make the whole database `speak' efficiently are\npresented. Advantages vs. disadvantages are discussed."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0701072v2", 
    "title": "Tagging, Folksonomy & Co - Renaissance of Manual Indexing?", 
    "arxiv-id": "cs/0701072v2", 
    "author": "Jakob Voss", 
    "publish": "2007-01-10T18:18:28Z", 
    "summary": "This paper gives an overview of current trends in manual indexing on the Web.\nAlong with a general rise of user generated content there are more and more\ntagging systems that allow users to annotate digital resources with tags\n(keywords) and share their annotations with other users. Tagging is frequently\nseen in contrast to traditional knowledge organization systems or as something\ncompletely new. This paper shows that tagging should better be seen as a\npopular form of manual indexing on the Web. Difference between controlled and\nfree indexing blurs with sufficient feedback mechanisms. A revised typology of\ntagging systems is presented that includes different user roles and knowledge\norganization systems with hierarchical relationships and vocabulary control. A\ndetailed bibliography of current research in collaborative tagging is included."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0701180v1", 
    "title": "Ontology from Local Hierarchical Structure in Text", 
    "arxiv-id": "cs/0701180v1", 
    "author": "K. Englmeier", 
    "publish": "2007-01-27T18:31:22Z", 
    "summary": "We study the notion of hierarchy in the context of visualizing textual data\nand navigating text collections. A formal framework for ``hierarchy'' is given\nby an ultrametric topology. This provides us with a theoretical foundation for\nconcept hierarchy creation. A major objective is {\\em scalable} annotation or\nlabeling of concept maps. Serendipitously we pursue other objectives such as\nderiving common word pair (and triplet) phrases, i.e., word 2- and 3-grams. We\nevaluate our approach using (i) a collection of texts, (ii) a single text\nsubdivided into successive parts (for which we provide an interactive\ndemonstrator), and (iii) a text subdivided at the sentence or line level. While\ndetailing a generic framework, a distinguishing feature of our work is that we\nfocus on {\\em locality} of hierarchic structure in order to extract semantic\ninformation."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0702067v1", 
    "title": "The Haar Wavelet Transform of a Dendrogram: Additional Notes", 
    "arxiv-id": "cs/0702067v1", 
    "author": "Fionn Murtagh", 
    "publish": "2007-02-10T21:26:05Z", 
    "summary": "We consider the wavelet transform of a finite, rooted, node-ranked, $p$-way\ntree, focusing on the case of binary ($p = 2$) trees. We study a Haar wavelet\ntransform on this tree. Wavelet transforms allow for multiresolution analysis\nthrough translation and dilation of a wavelet function. We explore how this\nworks in our tree context."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0702106v1", 
    "title": "Wild, Wild Wikis: A way forward", 
    "arxiv-id": "cs/0702106v1", 
    "author": "Ranmi Adigun", 
    "publish": "2007-02-19T15:12:43Z", 
    "summary": "Wikis can be considered as public domain knowledge sharing system. They\nprovide opportunity for those who may not have the privilege to publish their\nthoughts through the traditional methods. They are one of the fastest growing\nsystems of online encyclopaedia. In this study, we consider the importance of\nwikis as a way of creating, sharing and improving public knowledge. We identify\nsome of the problems associated with wikis to include, (a) identification of\nthe identities of information and its creator (b) accuracy of information (c)\njustification of the credibility of authors (d) vandalism of quality of\ninformation (e) weak control over the contents. A solution to some of these\nproblems is sought through the use of an annotation model. The model assumes\nthat contributions in wikis can be seen as annotation to the initial document.\nIt proposed a systematic control of contributors and contributions to the\ninitiative and the keeping of records of what existed and what was done to\ninitial documents. We believe that with this model, analysis can be done on the\nprogress of wiki initiatives. We assumed that using this model, wikis can be\nbetter used for creation and sharing of knowledge for public use."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0702107v1", 
    "title": "AMIEDoT: An annotation model for document tracking and recommendation   service", 
    "arxiv-id": "cs/0702107v1", 
    "author": "Charles A. Robert", 
    "publish": "2007-02-19T15:18:26Z", 
    "summary": "The primary objective of document annotation in whatever form, manual or\nelectronic is to allow those who may not have control to original document to\nprovide personal view on information source. Beyond providing personal\nassessment to original information sources, we are looking at a situation where\nannotation made can be used as additional source of information for document\ntracking and recommendation service. Most of the annotation tools existing\ntoday were conceived for their independent use with no reference to the creator\nof the annotation. We propose AMIEDoT (Annotation Model for Information\nExchange and Document Tracking) an annotation model that can assist in document\ntracking and recommendation service. The model is based on three parameters in\nthe acts of annotation. We believe that introducing document parameters, time\nand the parameters of the creator of annotation into an annotation process can\nbe a dependable source to know, who used a document, when a document was used\nand for what a document was used for. Beyond document tracking, our model can\nbe used in not only for selective dissemination of information but for\nrecommendation services. AMIEDoT can also be used for information sharing and\ninformation reuse."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00357-007-0007-9", 
    "link": "http://arxiv.org/pdf/cs/0702109v1", 
    "title": "AMIE: An annotation model for information research", 
    "arxiv-id": "cs/0702109v1", 
    "author": "David Amos", 
    "publish": "2007-02-19T15:26:05Z", 
    "summary": "The objective of most users for consulting any information database,\ninformation warehouse or the internet is to resolve one problem or the other.\nAvailable online or offline annotation tools were not conceived with the\nobjective of assisting users in their bid to resolve a decisional problem.\nApart from the objective and usage of annotation tools, how these tools are\nconceived and classified has implication on their usage. Several criteria have\nbeen used to categorize annotation concepts. Typically annotation are conceived\nbased on how it affect the organization of document been considered for\nannotation or the organization of the resulting annotation. Our approach is\nannotation that will assist in information research for decision making.\nAnnotation model for information exchange (AMIE) was conceived with the\nobjective of information sharing and reuse."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TPAMI.2008.76", 
    "link": "http://arxiv.org/pdf/cs/0703033v5", 
    "title": "Time Warp Edit Distance with Stiffness Adjustment for Time Series   Matching", 
    "arxiv-id": "cs/0703033v5", 
    "author": "Pierre-Fran\u00e7ois Marteau", 
    "publish": "2007-03-07T19:54:33Z", 
    "summary": "In a way similar to the string-to-string correction problem we address time\nseries similarity in the light of a time-series-to-time-series-correction\nproblem for which the similarity between two time series is measured as the\nminimum cost sequence of \"edit operations\" needed to transform one time series\ninto another. To define the \"edit operations\" we use the paradigm of a\ngraphical editing process and end up with a dynamic programming algorithm that\nwe call Time Warp Edit Distance (TWED). TWED is slightly different in form from\nDynamic Time Warping, Longest Common Subsequence or Edit Distance with Real\nPenalty algorithms. In particular, it highlights a parameter which drives a\nkind of stiffness of the elastic measure along the time axis. We show that the\nsimilarity provided by TWED is a metric potentially useful in time series\nretrieval applications since it could benefit from the triangular inequality\nproperty to speed up the retrieval process while tuning the parameters of the\nelastic measure. In that context, a lower bound is derived to relate the\nmatching of time series into down sampled representation spaces to the matching\ninto the original space. Empiric quality of the TWED distance is evaluated on a\nsimple classification task. Compared to Edit Distance, Dynamic Time Warping,\nLongest Common Subsequnce and Edit Distance with Real Penalty, TWED has proven\nto be quite effective on the considered experimental task."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TPAMI.2008.76", 
    "link": "http://arxiv.org/pdf/0705.1886v1", 
    "title": "Ontology-Supported and Ontology-Driven Conceptual Navigation on the   World Wide Web", 
    "arxiv-id": "0705.1886v1", 
    "author": "Sylvie Ranwez", 
    "publish": "2007-05-14T08:19:28Z", 
    "summary": "This paper presents the principles of ontology-supported and ontology-driven\nconceptual navigation. Conceptual navigation realizes the independence between\nresources and links to facilitate interoperability and reusability. An engine\nbuilds dynamic links, assembles resources under an argumentative scheme and\nallows optimization with a possible constraint, such as the user's available\ntime. Among several strategies, two are discussed in detail with examples of\napplications. On the one hand, conceptual specifications for linking and\nassembling are embedded in the resource meta-description with the support of\nthe ontology of the domain to facilitate meta-communication. Resources are like\nagents looking for conceptual acquaintances with intention. On the other hand,\nthe domain ontology and an argumentative ontology drive the linking and\nassembling strategies."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TPAMI.2008.76", 
    "link": "http://arxiv.org/pdf/0706.2797v3", 
    "title": "Extraction d'entit\u00e9s dans des collections \u00e9volutives", 
    "arxiv-id": "0706.2797v3", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2007-06-19T14:16:48Z", 
    "summary": "The goal of our work is to use a set of reports and extract named entities,\nin our case the names of Industrial or Academic partners. Starting with an\ninitial list of entities, we use a first set of documents to identify syntactic\npatterns that are then validated in a supervised learning phase on a set of\nannotated documents. The complete collection is then explored. This approach is\nsimilar to the ones used in data extraction from semi-structured documents\n(wrappers) and do not need any linguistic resources neither a large set for\ntraining. As our collection of documents would evolve over years, we hope that\nthe performance of the extraction would improve with the increased size of the\ntraining set."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WSE.2007.4380247", 
    "link": "http://arxiv.org/pdf/0708.0741v1", 
    "title": "Characterising Web Site Link Structure", 
    "arxiv-id": "0708.0741v1", 
    "author": "Vaclav Petricek", 
    "publish": "2007-08-06T11:00:58Z", 
    "summary": "The topological structures of the Internet and the Web have received\nconsiderable attention. However, there has been little research on the\ntopological properties of individual web sites. In this paper, we consider\nwhether web sites (as opposed to the entire Web) exhibit structural\nsimilarities. To do so, we exhaustively crawled 18 web sites as diverse as\ngovernmental departments, commercial companies and university departments in\ndifferent countries. These web sites consisted of as little as a few thousand\npages to millions of pages. Statistical analysis of these 18 sites revealed\nthat the internal link structure of the web sites are significantly different\nwhen measured with first and second-order topological properties, i.e.\nproperties based on the connectivity of an individual or a pairs of nodes.\nHowever, examination of a third-order topological property that consider the\nconnectivity between three nodes that form a triangle, revealed a strong\ncorrespondence across web sites, suggestive of an invariant. Comparison with\nthe Web, the AS Internet, and a citation network, showed that this third-order\nproperty is not shared across other types of networks. Nor is the property\nexhibited in generative network models such as that of Barabasi and Albert."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WSE.2007.4380247", 
    "link": "http://arxiv.org/pdf/0708.2788v1", 
    "title": "Integrating users' needs into multimedia information retrieval system", 
    "arxiv-id": "0708.2788v1", 
    "author": "Amos David", 
    "publish": "2007-08-21T09:04:49Z", 
    "summary": "The exponential growth of multimedia information and the development of\nvarious communication media generated new problems at various levels including\nthe rate of flow of information, problems of storage and management. The\ndifficulty which arises is no longer the existence of information but rather\nthe access to this information. When designing multimedia information retrieval\nsystem, it is appropriate to bear in mind the potential users and their\ninformation needs. We assumed that multimedia information representation which\ntakes into account explicitly the users' needs and the cases of use could\ncontribute to the adaptation potentials of the system for the end-users. We\nbelieve also that responses of multimedia information system would be more\nrelevant to the users' needs if the types of results to be used from the system\nwere identified before the design and development of the system. We propose the\nintegration of the users' information needs. More precisely integrating usage\ncontexts of resulting information in an information system (during creation and\nfeedback) should enhance more pertinent users' need. The first section of this\nstudy is dedicated to traditional multimedia information systems and\nspecifically the approaches of representing multimedia information. Taking into\naccount the dynamism of users, these approaches do not permit the explicit\nintegration of the users' information needs. In this paper, we will present our\nproposals based on economic intelligence approach. This approach emphasizes the\nimportance of starting any process of information retrieval witch the user\ninformation need."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0709.4669v1", 
    "title": "The Extended Edit Distance Metric", 
    "arxiv-id": "0709.4669v1", 
    "author": "Pierre-Fran\u00e7ois Marteau", 
    "publish": "2007-09-28T18:45:48Z", 
    "summary": "Similarity search is an important problem in information retrieval. This\nsimilarity is based on a distance. Symbolic representation of time series has\nattracted many researchers recently, since it reduces the dimensionality of\nthese high dimensional data objects. We propose a new distance metric that is\napplied to symbolic data objects and we test it on time series data bases in a\nclassification task. We compare it to other distances that are well known in\nthe literature for symbolic data objects. We also prove, mathematically, that\nour distance is metric."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0710.1962v1", 
    "title": "Stanford Matrix Considered Harmful", 
    "arxiv-id": "0710.1962v1", 
    "author": "Sebastiano Vigna", 
    "publish": "2007-10-10T10:03:03Z", 
    "summary": "This note argues about the validity of web-graph data used in the literature."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0711.2832v1", 
    "title": "Premi\u00e8re \u00e9tape vers une navigation r\u00e9f\u00e9rentielle par l'image   pour l'assistance \u00e0 la conception des ambiances lumineuses", 
    "arxiv-id": "0711.2832v1", 
    "author": "Gilles Halin", 
    "publish": "2007-11-19T16:10:35Z", 
    "summary": "In the first design stage, image reference plays a double role of means of\nformulation and resolution of problems. In our approach, we consider image\nreference as a support of creation activity to generate ideas and we propose a\ntool for navigation in references by image in order to assist daylight ambience\ndesign. Within this paper, we present, in a first part, the semantic indexation\nmethod to be used for the indexation of our image database. In a second part we\npropose a synthetic analysis of various modes of referential navigation in\norder to propose a tool implementing all or a part of these modes."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0711.2917v1", 
    "title": "Use of Wikipedia Categories in Entity Ranking", 
    "arxiv-id": "0711.2917v1", 
    "author": "Anne-Marie Vercoustre", 
    "publish": "2007-11-19T12:35:48Z", 
    "summary": "Wikipedia is a useful source of knowledge that has many applications in\nlanguage processing and knowledge representation. The Wikipedia category graph\ncan be compared with the class hierarchy in an ontology; it has some\ncharacteristics in common as well as some differences. In this paper, we\npresent our approach for answering entity ranking queries from the Wikipedia.\nIn particular, we explore how to make use of Wikipedia categories to improve\nentity ranking effectiveness. Our experiments show that using categories of\nexample entities works significantly better than using loosely defined target\ncategories."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0711.3128v1", 
    "title": "Entity Ranking in Wikipedia", 
    "arxiv-id": "0711.3128v1", 
    "author": "Jovan Pehcevski", 
    "publish": "2007-11-20T12:40:23Z", 
    "summary": "The traditional entity extraction problem lies in the ability of extracting\nnamed entities from plain text using natural language processing techniques and\nintensive training from large document collections. Examples of named entities\ninclude organisations, people, locations, or dates. There are many research\nactivities involving named entities; we are interested in entity ranking in the\nfield of information retrieval. In this paper, we describe our approach to\nidentifying and ranking entities from the INEX Wikipedia document collection.\nWikipedia offers a number of interesting features for entity identification and\nranking that we first introduce. We then describe the principles and the\narchitecture of our entity ranking system, and introduce our methodology for\nevaluation. Our preliminary results show that the use of categories and the\nlink structure of Wikipedia, together with entity examples, can significantly\nimprove retrieval effectiveness."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0711.3964v1", 
    "title": "Iterative Filtering for a Dynamical Reputation System", 
    "arxiv-id": "0711.3964v1", 
    "author": "Paul Van Dooren", 
    "publish": "2007-11-26T08:12:51Z", 
    "summary": "The paper introduces a novel iterative method that assigns a reputation to n\n+ m items: n raters and m objects. Each rater evaluates a subset of objects\nleading to a n x m rating matrix with a certain sparsity pattern. From this\nrating matrix we give a nonlinear formula to define the reputation of raters\nand objects. We also provide an iterative algorithm that superlinearly\nconverges to the unique vector of reputations and this for any rating matrix.\nIn contrast to classical outliers detection, no evaluation is discarded in this\nmethod but each one is taken into account with different weights for the\nreputation of the objects. The complexity of one iteration step is linear in\nthe number of evaluations, making our algorithm efficient for large data set.\nExperiments show good robustness of the reputation of the objects against\ncheaters and spammers and good detection properties of cheaters and spammers."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0712.2063v1", 
    "title": "An axiomatic approach to intrinsic dimension of a dataset", 
    "arxiv-id": "0712.2063v1", 
    "author": "Vladimir Pestov", 
    "publish": "2007-12-12T23:39:21Z", 
    "summary": "We perform a deeper analysis of an axiomatic approach to the concept of\nintrinsic dimension of a dataset proposed by us in the IJCNN'07 paper\n(arXiv:cs/0703125). The main features of our approach are that a high intrinsic\ndimension of a dataset reflects the presence of the curse of dimensionality (in\na certain mathematically precise sense), and that dimension of a discrete\ni.i.d. sample of a low-dimensional manifold is, with high probability, close to\nthat of the manifold. At the same time, the intrinsic dimension of a sample is\neasily corrupted by moderate high-dimensional noise (of the same amplitude as\nthe size of the manifold) and suffers from prohibitevely high computational\ncomplexity (computing it is an $NP$-complete problem). We outline a possible\nway to overcome these difficulties."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0801.3908v1", 
    "title": "Encoding changing country codes for the Semantic Web with ISO 3166 and   SKOS", 
    "arxiv-id": "0801.3908v1", 
    "author": "Jakob Voss", 
    "publish": "2008-01-25T10:40:27Z", 
    "summary": "This paper shows how authority files can be encoded for the Semantic Web with\nthe Simple Knowledge Organisation System (SKOS). In particular the application\nof SKOS for encoding the structure, management, and utilization of country\ncodes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use\ncase for SKOS that includes features that have only been discussed little so\nfar, such as multiple notations, nested concept schemes, changes by versioning."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0802.3522v5", 
    "title": "Time Warp Edit Distance", 
    "arxiv-id": "0802.3522v5", 
    "author": "Pierre-Fran\u00e7ois Marteau", 
    "publish": "2008-02-24T17:18:50Z", 
    "summary": "This technical report details a family of time warp distances on the set of\ndiscrete time series. This family is constructed as an editing distance whose\nelementary operations apply on linear segments. A specific parameter allows\ncontrolling the stiffness of the elastic matching. It is well suited for the\nprocessing of event data for which each data sample is associated with a\ntimestamp, not necessarily obtained according to a constant sampling rate. Some\nproperties verified by these distances are proposed and proved in this report."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0803.0822v1", 
    "title": "Website Optimization through Mining User Navigational Pattern", 
    "arxiv-id": "0803.0822v1", 
    "author": "Biswajit Biswal", 
    "publish": "2008-03-06T10:01:08Z", 
    "summary": "With the World Wide Web's ubiquity increase and the rapid development of\nvarious online businesses, the complexity of web sites grow. The analysis of\nweb user's navigational pattern within a web site can provide useful\ninformation for server performance enhancements, restructuring a website and\ndirect marketing in e-commerce etc. In this paper, an algorithm is proposed for\nmining such navigation patterns. The key insight is that users access\ninformation of interest and follow a certain path while navigating a web site.\nIf they don't find it, they would backtrack and choose among the alternate\npaths till they reach the destination. The point they backtrack is the\nIntermediate Reference Location. Identifying such Intermediate locations and\ndestinations out of the pattern will be the main endeavor in the rest of this\nreport."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0803.2220v2", 
    "title": "The Anatomy of Mitos Web Search Engine", 
    "arxiv-id": "0803.2220v2", 
    "author": "Yannis Tzitzikas", 
    "publish": "2008-03-14T19:18:15Z", 
    "summary": "Engineering a Web search engine offering effective and efficient information\nretrieval is a challenging task. This document presents our experiences from\ndesigning and developing a Web search engine offering a wide spectrum of\nfunctionalities and we report some interesting experimental results. A rather\npeculiar design choice of the engine is that its index is based on a DBMS,\nwhile some of the distinctive functionalities that are offered include advanced\nGreek language stemming, real time result clustering, and advanced link\nanalysis techniques (also for spam page detection)."
},{
    "category": "cs.IR", 
    "doi": "10.1109/CBMI.2008.4564953", 
    "link": "http://arxiv.org/pdf/0804.2057v1", 
    "title": "Comparing and Combining Methods for Automatic Query Expansion", 
    "arxiv-id": "0804.2057v1", 
    "author": "Lourdes Araujo", 
    "publish": "2008-04-13T11:38:28Z", 
    "summary": "Query expansion is a well known method to improve the performance of\ninformation retrieval systems. In this work we have tested different approaches\nto extract the candidate query terms from the top ranked documents returned by\nthe first-pass retrieval.\n  One of them is the cooccurrence approach, based on measures of cooccurrence\nof the candidate and the query terms in the retrieved documents. The other one,\nthe probabilistic approach, is based on the probability distribution of terms\nin the collection and in the top ranked set.\n  We compare the retrieval improvement achieved by expanding the query with\nterms obtained with different methods belonging to both approaches. Besides, we\nhave developed a na\\\"ive combination of both kinds of method, with which we\nhave obtained results that improve those obtained with any of them separately.\nThis result confirms that the information provided by each approach is of a\ndifferent nature and, therefore, can be used in a combined manner."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0806.4921v1", 
    "title": "Interpr\u00e9tation vague des contraintes structurelles pour la RI dans des   corpus de documents XML - \u00c9valuation d'une m\u00e9thode approch\u00e9e de RI   structur\u00e9e", 
    "arxiv-id": "0806.4921v1", 
    "author": "Pierre-Fran\u00e7ois Marteau", 
    "publish": "2008-06-30T15:25:30Z", 
    "summary": "We propose specific data structures designed to the indexing and retrieval of\ninformation elements in heterogeneous XML data bases. The indexing scheme is\nwell suited to the management of various contextual searches, expressed either\nat a structural level or at an information content level. The approximate\nsearch mechanisms are based on a modified Levenshtein editing distance and\ninformation fusion heuristics. The implementation described highlights the\nmixing of structured information presented as field/value instances and free\ntext elements. The retrieval performances of the proposed approach are\nevaluated within the INEX 2005 evaluation campaign. The evaluation results rank\nthe proposed approach among the best evaluated XML IR systems for the VVCAS\ntask."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0809.4668v1", 
    "title": "Faceted Ranking of Egos in Collaborative Tagging Systems", 
    "arxiv-id": "0809.4668v1", 
    "author": "Jos\u00e9 Ignacio Alvarez-Hamelin", 
    "publish": "2008-09-26T16:26:50Z", 
    "summary": "Multimedia uploaded content is tagged and recommended by users of\ncollaborative systems, resulting in informal classifications also known as\nfolksonomies. Faceted web ranking has been proved a reasonable alternative to a\nsingle ranking which does not take into account a personalized context. In this\npaper we analyze the online computation of rankings of users associated to\nfacets made up of multiple tags. Possible applications are user reputation\nevaluation (ego-ranking) and improvement of content quality in case of\nretrieval. We propose a solution based on PageRank as centrality measure: (i) a\nranking for each tag is computed offline on the basis of the corresponding\ntag-dependent subgraph; (ii) a faceted order is generated by merging rankings\ncorresponding to all the tags in the facet. The fundamental assumption,\nvalidated by empirical observations, is that step (i) is scalable. We also\npresent algorithms for part (ii) having time complexity O(k), where k is the\nnumber of tags in the facet, well suited to online computation."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0809.4834v1", 
    "title": "Relevance Feedback in Conceptual Image Retrieval: A User Evaluation", 
    "arxiv-id": "0809.4834v1", 
    "author": "Luis Paulo Reis", 
    "publish": "2008-09-28T10:17:20Z", 
    "summary": "The Visual Object Information Retrieval (VOIR) system described in this paper\nimplements an image retrieval approach that combines two layers, the conceptual\nand the visual layer. It uses terms from a textual thesaurus to represent the\nconceptual information and also works with image regions, the visual\ninformation. The terms are related with the image regions through a weighted\nassociation enabling the execution of concept-level queries. VOIR uses\nregion-based relevance feedback to improve the quality of the results in each\nquery session and to discover new associations between text and image. This\npaper describes a user-centred and task-oriented comparative evaluation of VOIR\nwhich was undertaken considering three distinct versions of VOIR: a full-fledge\nversion; one supporting relevance feedback only at image level; and a third\nversion not supporting relevance feedback at all. The evaluation performed\nshowed the usefulness of region based relevance feedback in the context of VOIR\nprototype."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0810.1732v1", 
    "title": "Introduction to Searching with Regular Expressions", 
    "arxiv-id": "0810.1732v1", 
    "author": "Christopher M. Frenz", 
    "publish": "2008-10-09T19:57:31Z", 
    "summary": "The explosive rate of information growth and availability often makes it\nincreasingly difficult to locate information pertinent to your needs. These\nproblems are often compounded when keyword based search methodologies are not\nadequate for describing the information you seek. In many instances,\ninformation such as Web site URLs, phone numbers, etc. can often be better\nidentified through the use of a textual pattern than by keyword. For example,\nmany more phone numbers could be picked up by a search for the pattern (XXX)\nXXX-XXXX, where X could be any digit, than would be by a search for any\nspecific phone number (i.e. the keyword approach). Programming languages\ntypically allow for the matching of textual patterns via the usage of regular\nexpressions. This tutorial will provide an introduction to the basics of\nprogramming regular expressions as well as provide an introduction to how\nregular expressions can be applied to data processing tasks such as information\nextraction and search refinement."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0811.0603v1", 
    "title": "Query Refinement by Multi Word Term expansions and semantic synonymy", 
    "arxiv-id": "0811.0603v1", 
    "author": "Eric San Juan", 
    "publish": "2008-11-04T20:43:29Z", 
    "summary": "We developed a system, TermWatch\n(https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a\nlinguistic extraction of terms, their structuring into a terminological network\nwith a clustering algorithm. In this paper we explore its ability in\nintegrating the most promising aspects of the studies on query refinement:\nchoice of meaningful text units to cluster (domain terms), choice of tight\nsemantic relations with which to cluster terms, structuring of terms in a\nnetwork enabling abetter perception of domain concepts. We have run this\nexperiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic\ndatabase (http://www.inist.fr) and compared the structured terminological\nresource automatically build by TermWarch to the English segment of TermScience\nresource (http://termsciences.inist.fr/) containing 88 211 terms."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0812.1394v1", 
    "title": "Conceptual approach through an annotation process for the representation   and the information contents enhancement in economic intelligence (EI)", 
    "arxiv-id": "0812.1394v1", 
    "author": "Sahbi Sidhom", 
    "publish": "2008-12-07T20:07:37Z", 
    "summary": "In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0812.4461v1", 
    "title": "Mining User Profiles to Support Structure and Explanation in Open Social   Networking", 
    "arxiv-id": "0812.4461v1", 
    "author": "Wolfgang Nejdl", 
    "publish": "2008-12-23T23:20:44Z", 
    "summary": "The proliferation of media sharing and social networking websites has brought\nwith it vast collections of site-specific user generated content. The result is\na Social Networking Divide in which the concepts and structure common across\ndifferent sites are hidden. The knowledge and structures from one social site\nare not adequately exploited to provide new information and resources to the\nsame or different users in comparable social sites. For music bloggers, this\nlatent structure, forces bloggers to select sub-optimal blogrolls. However, by\nintegrating the social activities of music bloggers and listeners, we are able\nto overcome this limitation: improving the quality of the blogroll\nneighborhoods, in terms of similarity, by 85 percent when using tracks and by\n120 percent when integrating tags from another site."
},{
    "category": "cs.IR", 
    "doi": "10.3166/dn.10.63-88", 
    "link": "http://arxiv.org/pdf/0901.0358v1", 
    "title": "Weighted Naive Bayes Model for Semi-Structured Document Categorization", 
    "arxiv-id": "0901.0358v1", 
    "author": "Eugen Popovici", 
    "publish": "2009-01-04T06:35:34Z", 
    "summary": "The aim of this paper is the supervised classification of semi-structured\ndata. A formal model based on bayesian classification is developed while\naddressing the integration of the document structure into classification tasks.\nWe define what we call the structural context of occurrence for unstructured\ndata, and we derive a recursive formulation in which parameters are used to\nweight the contribution of structural element relatively to the others. A\nsimplified version of this formal model is implemented to carry out textual\ndocuments classification experiments. First results show, for a adhoc weighting\nstrategy, that the structural context of word occurrences has a significant\nimpact on classification results comparing to the performance of a simple\nmultinomial naive Bayes classifier. The proposed implementation competes on the\nReuters-21578 data with the SVM classifier associated or not with the splitting\nof structural components. These results encourage exploring the learning of\nacceptable weighting strategies for this model, in particular boosting\nstrategies."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1555400.1555446", 
    "link": "http://arxiv.org/pdf/0903.0153v1", 
    "title": "Document Relevance Evaluation via Term Distribution Analysis Using   Fourier Series Expansion", 
    "arxiv-id": "0903.0153v1", 
    "author": "Bernd Freisleben", 
    "publish": "2009-03-01T17:08:17Z", 
    "summary": "In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments for a given search query. In this paper, term distribution analysis\nusing Fourier series expansion as a novel approach for calculating an abstract\nrepresentation of term positions in a document corpus is introduced. Based on\nthis approach, two methods for improving the evaluation of document relevance\nare proposed: (a) a function-based ranking optimization representing a user\ndefined document region, and (b) a query expansion technique based on\noverlapping the term distributions in the top-ranked documents. Experimental\nresults demonstrate the effectiveness of the proposed approach in providing new\npossibilities for optimizing the retrieval process."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1555400.1555446", 
    "link": "http://arxiv.org/pdf/0903.2544v1", 
    "title": "To Click or not to Click? The Role of Contextualized and User-Centric   Web Snippets", 
    "arxiv-id": "0903.2544v1", 
    "author": "I. Varlamis", 
    "publish": "2009-03-14T12:22:27Z", 
    "summary": "When searching the web, it is often possible that there are too many results\navailable for ambiguous queries. Text snippets, extracted from the retrieved\npages, are an indicator of the pages' usefulness to the query intention and can\nbe used to focus the scope of search results. In this paper, we propose a novel\nmethod for automatically extracting web page snippets that are highly relevant\nto the query intention and expressive of the pages' entire content. We show\nthat the usage of semantics, as a basis for focused retrieval, produces high\nquality text snippet suggestions. The snippets delivered by our method are\nsignificantly better in terms of retrieval performance compared to those\nderived using the pages' statistical content. Furthermore, our study suggests\nthat semantically-driven snippet generation can also be used to augment\ntraditional passage retrieval algorithms based on word overlap or statistical\nweights, since they typically differ in coverage and produce different results.\nUser clicks on the query relevant snippets can be used to refine the query\nresults and promote the most comprehensive among the relevant documents."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1555400.1555446", 
    "link": "http://arxiv.org/pdf/0903.4035v1", 
    "title": "BLOGRANK: Ranking Weblogs Based On Connectivity And Similarity Features", 
    "arxiv-id": "0903.4035v1", 
    "author": "I. Varlamis", 
    "publish": "2009-03-24T08:36:21Z", 
    "summary": "A large part of the hidden web resides in weblog servers. New content is\nproduced in a daily basis and the work of traditional search engines turns to\nbe insufficient due to the nature of weblogs. This work summarizes the\nstructure of the blogosphere and highlights the special features of weblogs. In\nthis paper we present a method for ranking weblogs based on the link graph and\non several similarity characteristics between weblogs. First we create an\nenhanced graph of connected weblogs and add new types of edges and weights\nutilising many weblog features. Then, we assign a ranking to each weblog using\nour algorithm, BlogRank, which is a modified version of PageRank. For the\nvalidation of our method we run experiments on a weblog dataset, which we\nprocess and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).\nThe results suggest that the use of the enhanced graph and the BlogRank\nalgorithm is preferred by the users."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1555400.1555446", 
    "link": "http://arxiv.org/pdf/0904.0986v1", 
    "title": "Approche conceptuelle par un processus d'annotation pour la   repr\u00e9sentation et la valorisation de contenus informationnels en   intelligence \u00e9conomique (IE)", 
    "arxiv-id": "0904.0986v1", 
    "author": "Sahbi Sidhom", 
    "publish": "2009-04-06T18:56:12Z", 
    "summary": "In the era of the information society, the impact of the information systems\non the economy of material and immaterial is certainly perceptible. With\nregards to the information resources of an organization, the annotation\ninvolved to enrich informational content, to track the intellectual activities\non a document and to set the added value on information for the benefit of\nsolving a decision-making problem in the context of economic intelligence. Our\ncontribution is distinguished by the representation of an annotation process\nand its inherent concepts to lead the decisionmaker to an anticipated decision:\nthe provision of relevant and annotated information. Such information in the\nsystem is made easy by taking into account the diversity of resources and those\nthat are well annotated so formally and informally by the EI actors. A capital\nresearch framework consist of integrating in the decision-making process the\nannotator activity, the software agent (or the reasoning mechanisms) and the\ninformation resources enhancement."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.08.036", 
    "link": "http://arxiv.org/pdf/0904.1989v1", 
    "title": "Personalized Recommendation via Integrated Diffusion on User-Item-Tag   Tripartite Graphs", 
    "arxiv-id": "0904.1989v1", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2009-04-13T19:13:03Z", 
    "summary": "Personalized recommender systems are confronting great challenges of\naccuracy, diversification and novelty, especially when the data set is sparse\nand lacks accessorial information, such as user profiles, item attributes and\nexplicit ratings. Collaborative tags contain rich information about\npersonalized preferences and item contents, and are therefore potential to help\nin providing better recommendations. In this paper, we propose a recommendation\nalgorithm based on an integrated diffusion on user-item-tag tripartite graphs.\nWe use three benchmark data sets, Del.icio.us, MovieLens and BibSonomy, to\nevaluate our algorithm. Experimental results demonstrate that the usage of tag\ninformation can significantly improve accuracy, diversification and novelty of\nrecommendations."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.08.036", 
    "link": "http://arxiv.org/pdf/0905.2501v1", 
    "title": "Macrodynamics of users' behavior in Information Retrieval", 
    "arxiv-id": "0905.2501v1", 
    "author": "Rom\u00e0n R. Zapatrin", 
    "publish": "2009-05-15T09:57:56Z", 
    "summary": "We present a method to geometrize massive data sets from search engines query\nlogs. For this purpose, a macrodynamic-like quantitative model of the\nInformation Retrieval (IR) process is developed, whose paradigm is inspired by\nbasic constructions of Einstein's general relativity theory in which all IR\nobjects are uniformly placed in a common Room. The Room has a structure similar\nto Einsteinian spacetime, namely that of a smooth manifold. Documents and\nqueries are treated as matter objects and sources of material fields.\nRelevance, the central notion of IR, becomes a dynamical issue controlled by\nboth gravitation (or, more precisely, as the motion in a curved spacetime) and\nforces originating from the interactions of matter fields. The spatio-temporal\ndescription ascribes dynamics to any document or query, thus providing a\nuniform description for documents of both initially static and dynamical\nnature. Within the IR context, the techniques presented are based on two ideas.\nThe first is the placement of all objects participating in IR into a common\ncontinuous space. The second idea is the `objectivization' of the IR process;\ninstead of expressing users' wishes, we consider the overall IR as an objective\nphysical process, representing the IR process in terms of motion in a given\nexternal-fields configuration. Various semantic environments are treated as\nvarious IR universes."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.036213", 
    "link": "http://arxiv.org/pdf/0905.4162v2", 
    "title": "Google matrix, dynamical attractors and Ulam networks", 
    "arxiv-id": "0905.4162v2", 
    "author": "O. V. Zhirov", 
    "publish": "2009-05-26T11:03:39Z", 
    "summary": "We study the properties of the Google matrix generated by a coarse-grained\nPerron-Frobenius operator of the Chirikov typical map with dissipation. The\nfinite size matrix approximant of this operator is constructed by the Ulam\nmethod. This method applied to the simple dynamical model creates the directed\nUlam networks with approximate scale-free scaling and characteristics being\nrather similar to those of the World Wide Web. The simple dynamical attractors\nplay here the role of popular web sites with a strong concentration of\nPageRank. A variation of the Google parameter $\\alpha$ or other parameters of\nthe dynamical map can drive the PageRank of the Google matrix to a delocalized\nphase with a strange attractor where the Google search becomes inefficient."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.08.011", 
    "link": "http://arxiv.org/pdf/0906.1148v1", 
    "title": "Collaborative filtering based on multi-channel diffusion", 
    "arxiv-id": "0906.1148v1", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2009-06-05T15:32:47Z", 
    "summary": "In this paper, by applying a diffusion process, we propose a new index to\nquantify the similarity between two users in a user-object bipartite graph. To\ndeal with the discrete ratings on objects, we use a multi-channel\nrepresentation where each object is mapped to several channels with the number\nof channels being equal to the number of different ratings. Each channel\nrepresents a certain rating and a user having voted an object will be connected\nto the channel corresponding to the rating. Diffusion process taking place on\nsuch a user-channel bipartite graph gives a new similarity measure of user\npairs, which is further demonstrated to be more accurate than the classical\nPearson correlation coefficient under the standard collaborative filtering\nframework."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.08.011", 
    "link": "http://arxiv.org/pdf/0906.3085v1", 
    "title": "Poset representation and similarity comparisons os systems in IR", 
    "arxiv-id": "0906.3085v1", 
    "author": "Christine Michel", 
    "publish": "2009-06-17T07:04:44Z", 
    "summary": "In this paper we are using the poset representation to describe the complex\nanswers given by IR systems after a clustering and ranking processes. The\nanswers considered may be given by cartographical representations or by\nthematic sub-lists of documents. The poset representation, with the graph\ntheory and the relational representation opens many perspectives in the\ndefinition of new similarity measures capable of taking into account both the\nclustering and ranking processes. We present a general method for constructing\nnew similarity measures and give several examples. These measures can be used\nfor semi-ordered partitions; moreover, in the comparison of two sets of\nanswers, the corresponding similarity indicator is an increasing function of\nthe ranks of presentation of common answers."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.08.011", 
    "link": "http://arxiv.org/pdf/0906.4690v1", 
    "title": "Fuzzy Logic Based Method for Improving Text Summarization", 
    "arxiv-id": "0906.4690v1", 
    "author": "Mohammed Salem Binwahlan", 
    "publish": "2009-06-25T13:19:07Z", 
    "summary": "Text summarization can be classified into two approaches: extraction and\nabstraction. This paper focuses on extraction approach. The goal of text\nsummarization based on extraction approach is sentence selection. One of the\nmethods to obtain the suitable sentences is to assign some numerical measure of\na sentence for the summary called sentence weighting and then select the best\nones. The first step in summarization by extraction is the identification of\nimportant features. In our experiment, we used 125 test documents in DUC2002\ndata set. Each document is prepared by preprocessing process: sentence\nsegmentation, tokenization, removing stop word, and word stemming. Then, we use\n8 important features and calculate their score for each sentence. We propose\ntext summarization based on fuzzy logic to improve the quality of the summary\ncreated by the general statistic method. We compare our results with the\nbaseline summarizer and Microsoft Word 2007 summarizers. The results show that\nthe best average precision, recall, and f-measure for the summaries were\nobtained by fuzzy method."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0906.5017v2", 
    "title": "Collaborative filtering with diffusion-based similarity on tripartite   graphs", 
    "arxiv-id": "0906.5017v2", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2009-06-26T23:01:47Z", 
    "summary": "Collaborative tags are playing more and more important role for the\norganization of information systems. In this paper, we study a personalized\nrecommendation model making use of the ternary relations among users, objects\nand tags. We propose a measure of user similarity based on his preference and\ntagging information. Two kinds of similarities between users are calculated by\nusing a diffusion-based process, which are then integrated for recommendation.\nWe test the proposed method in a standard collaborative filtering framework\nwith three metrics: ranking score, Recall and Precision, and demonstrate that\nit performs better than the commonly used cosine similarity."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0906.5034v1", 
    "title": "Effective Focused Crawling Based on Content and Link Structure Analysis", 
    "arxiv-id": "0906.5034v1", 
    "author": "S. C. Shrivastava", 
    "publish": "2009-06-27T03:50:59Z", 
    "summary": "A focused crawler traverses the web selecting out relevant pages to a\npredefined topic and neglecting those out of concern. While surfing the\ninternet it is difficult to deal with irrelevant pages and to predict which\nlinks lead to quality pages. In this paper a technique of effective focused\ncrawling is implemented to improve the quality of web navigation. To check the\nsimilarity of web pages w.r.t. topic keywords a similarity function is used and\nthe priorities of extracted out links are also calculated based on meta data\nand resultant pages generated from focused crawler. The proposed work also uses\na method for traversing the irrelevant pages that met during crawling to\nimprove the coverage of a specific topic."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0906.5286v1", 
    "title": "Putting Recommendations on the Map -- Visualizing Clusters and Relations", 
    "arxiv-id": "0906.5286v1", 
    "author": "Chris Volinsky", 
    "publish": "2009-06-29T15:00:43Z", 
    "summary": "For users, recommendations can sometimes seem odd or counterintuitive.\nVisualizing recommendations can remove some of this mystery, showing how a\nrecommendation is grouped with other choices. A drawing can also lead a user's\neye to other options. Traditional 2D-embeddings of points can be used to create\na basic layout, but these methods, by themselves, do not illustrate clusters\nand neighborhoods very well. In this paper, we propose the use of geographic\nmaps to enhance the definition of clusters and neighborhoods, and consider the\neffectiveness of this approach in visualizing similarities and recommendations\narising from TV shows and music selections. All the maps referenced in this\npaper can be found in http://www.research.att.com/~volinsky/maps"
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0907.1728v2", 
    "title": "Role of Weak Ties in Link Prediction of Complex Networks", 
    "arxiv-id": "0907.1728v2", 
    "author": "Tao Zhou", 
    "publish": "2009-07-10T19:25:20Z", 
    "summary": "Plenty of algorithms for link prediction have been proposed and were applied\nto various real networks. Among these works, the weights of links are rarely\ntaken into account. In this paper, we use local similarity indices to estimate\nthe likelihood of the existence of links in weighted networks, including Common\nNeighbor, Adamic-Adar Index, Resource Allocation Index, and their weighted\nversions. In both the unweighted and weighted cases, the resource allocation\nindex performs the best. To our surprise, the weighted indices perform worse,\nwhich reminds us of the well-known Weak Tie Theory. Further extensive\nexperimental study shows that the weak ties play a significant role in the link\nprediction problem, and to emphasize the contribution of weak ties can\nremarkably enhance the predicting accuracy."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0907.2209v2", 
    "title": "Related terms search based on WordNet / Wiktionary and its application   in Ontology Matching", 
    "arxiv-id": "0907.2209v2", 
    "author": "Feiyu Lin", 
    "publish": "2009-07-13T16:58:22Z", 
    "summary": "A set of ontology matching algorithms (for finding correspondences between\nconcepts) is based on a thesaurus that provides the source data for the\nsemantic distance calculations. In this wiki era, new resources may spring up\nand improve this kind of semantic search. In the paper a solution of this task\nbased on Russian Wiktionary is compared to WordNet based algorithms. Metrics\nare estimated using the test collection, containing 353 English word pairs with\na relatedness score assigned by human evaluators. The experiment shows that the\nproposed method is capable in principle of calculating a semantic distance\nbetween pair of words in any language presented in Russian Wiktionary. The\ncalculation of Wiktionary based metric had required the development of the\nopen-source Wiktionary parser software."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0907.3315v1", 
    "title": "Effective Personalized Recommendation in Collaborative Tagging Systems", 
    "arxiv-id": "0907.3315v1", 
    "author": "Tao Zhou", 
    "publish": "2009-07-19T18:56:37Z", 
    "summary": "Recently, collaborative tagging systems have attracted more and more\nattention and have been widely applied in web systems. Tags provide highly\nabstracted information about personal preferences and item content, and are\ntherefore potential to help in improving better personalized recommendations.\nIn this paper, we propose a tag-based recommendation algorithm considering the\npersonal vocabulary and evaluate it in a real-world dataset: Del.icio.us.\nExperimental results demonstrate that the usage of tag information can\nsignificantly improve the accuracy of personalized recommendations."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0907.3823v2", 
    "title": "USUM: Update Summary Generation System", 
    "arxiv-id": "0907.3823v2", 
    "author": "P Sreenivasa Kumar", 
    "publish": "2009-07-22T12:00:06Z", 
    "summary": "Huge amount of information is present in the World Wide Web and a large\namount is being added to it frequently. A query-specific summary of multiple\ndocuments is very helpful to the user in this context. Currently, few systems\nhave been proposed for query-specific, extractive multi-document summarization.\nIf a summary is available for a set of documents on a given query and if a new\ndocument is added to the corpus, generating an updated summary from the scratch\nis time consuming and many a times it is not practical/possible. In this paper\nwe propose a solution to this problem. This is especially useful in a scenario\nwhere the source documents are not accessible. We cleverly embed the sentences\nof the current summary into the new document and then perform query-specific\nsummary generation on that document. Our experimental results show that the\nperformance of the proposed approach is good in terms of both quality and\nefficiency."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0908.0912v1", 
    "title": "Evaluation of Coordination Techniques in Synchronous Collaborative   Information Retrieval", 
    "arxiv-id": "0908.0912v1", 
    "author": "Alan F. Smeaton", 
    "publish": "2009-08-06T17:23:01Z", 
    "summary": "Traditional Information Retrieval (IR) research has focussed on a single user\ninteraction modality, where a user searches to satisfy an information need.\nRecent advances in web technologies and computer hardware have enabled multiple\nusers to collaborate on many computer-supported tasks, therefore there is an\nincreasing opportunity to support two or more users searching together at the\nsame time in order to satisfy a shared information need, which we refer to as\nSynchronous Collaborative Information Retrieval (SCIR). SCIR systems represent\na significant paradigmatic shift from traditional IR systems. In order to\nsupport effective SCIR, new techniques are required to coordinate users'\nactivities. In addition, the novel domain of SCIR presents challenges for\neffective evaluations of these systems. In this paper we will propose an\neffective and re-usable evaluation methodology based on simulating users\nsearching together. We will outline how we have used this evaluation in\nempirical studies of the effects of different division of labour and sharing of\nknowledge techniques for SCIR."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0908.0919v1", 
    "title": "Collaborative Search Trails for Video Search", 
    "arxiv-id": "0908.0919v1", 
    "author": "Joemon Jose", 
    "publish": "2009-08-06T17:28:21Z", 
    "summary": "In this paper we present an approach for supporting users in the difficult\ntask of searching for video. We use collaborative feedback mined from the\ninteractions of earlier users of a video search system to help users in their\ncurrent search tasks. Our objective is to improve the quality of the results\nthat users find, and in doing so also assist users to explore a large and\ncomplex information space. It is hoped that this will lead to them considering\nsearch options that they may not have considered otherwise. We performed a user\ncentred evaluation. The results of our evaluation indicate that we achieved our\ngoals, the performance of the users in finding relevant video clips was\nenhanced with our system; users were able to explore the collection of video\nclips more and users demonstrated a preference for our system that provided\nrecommendations."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0909.0572v1", 
    "title": "A Method for Accelerating the HITS Algorithm", 
    "arxiv-id": "0909.0572v1", 
    "author": "Masashi Furukawa", 
    "publish": "2009-09-03T05:34:35Z", 
    "summary": "We present a new method to accelerate the HITS algorithm by exploiting\nhyperlink structure of the web graph. The proposed algorithm extends the idea\nof authority and hub scores from HITS by introducing two diagonal matrices\nwhich contain constants that act as weights to make authority pages more\nauthoritative and hub pages more hubby. This method works because in the web\ngraph good authorities are pointed to by good hubs and good hubs point to good\nauthorities. Consequently, these pages will collect their scores faster under\nthe proposed algorithm than under the standard HITS. We show that the authority\nand hub vectors of the proposed algorithm exist but are not necessarily be\nunique, and then give a treatment to ensure the uniqueness property of the\nvectors. The experimental results show that the proposed algorithm can improve\nHITS computations, especially for back button datasets."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.11.041", 
    "link": "http://arxiv.org/pdf/0909.2345v1", 
    "title": "Weblog Clustering in Multilinear Algebra Perspective", 
    "arxiv-id": "0909.2345v1", 
    "author": "Andri Mirzal", 
    "publish": "2009-09-12T15:53:33Z", 
    "summary": "This paper describes a clustering method to group the most similar and\nimportant weblogs with their descriptive shared words by using a technique from\nmultilinear algebra known as PARAFAC tensor decomposition. The proposed method\nfirst creates labeled-link network representation of the weblog datasets, where\nthe nodes are the blogs and the labels are the shared words. Then, 3-way\nadjacency tensor is extracted from the network and the PARAFAC decomposition is\napplied to the tensor to get pairs of node lists and label lists with scores\nattached to each list as the indication of the degree of importance. The\nclustering is done by sorting the lists in decreasing order and taking the\npairs of top ranked blogs and words. Thus, unlike standard co-clustering\nmethods, this method not only groups the similar blogs with their descriptive\nwords but also tends to produce clusters of important blogs and descriptive\nwords."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0909.2489v1", 
    "title": "PrisCrawler: A Relevance Based Crawler for Automated Data Classification   from Bulletin Board", 
    "arxiv-id": "0909.2489v1", 
    "author": "Weiran Xu", 
    "publish": "2009-09-14T09:02:01Z", 
    "summary": "Nowadays people realize that it is difficult to find information simply and\nquickly on the bulletin boards. In order to solve this problem, people propose\nthe concept of bulletin board search engine. This paper describes the\npriscrawler system, a subsystem of the bulletin board search engine, which can\nautomatically crawl and add the relevance to the classified attachments of the\nbulletin board. Priscrawler utilizes Attachrank algorithm to generate the\nrelevance between webpages and attachments and then turns bulletin board into\nclear classified and associated databases, making the search for attachments\ngreatly simplified. Moreover, it can effectively reduce the complexity of\npretreatment subsystem and retrieval subsystem and improve the search\nprecision. We provide experimental results to demonstrate the efficacy of the\npriscrawler."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0909.2496v2", 
    "title": "Pavideoge: A Metadata Markup Video Structure in Video Search Engine", 
    "arxiv-id": "0909.2496v2", 
    "author": "Guang Chen", 
    "publish": "2009-09-14T09:39:17Z", 
    "summary": "In this paper, we study the problems of video processing in video search\nengine. Video has now become a very important kind of data in Internet; while\nsearching for video is still a challenging task due to the inner properties of\nvideo: requiring enormous storage space, being independent, expressing\ninformation hiddenly. To handle the properties of video more effectively, in\nthis paper, we propose a new video processing method in video search engine. In\ndetail, the core of the new video processing method is creating pavideoge--a\nnew data type, which contains the video advantages and webpage advantages. The\npavideoge has four attributes: real link, videorank, text information and\nplaynum. Each of them combines video's properties with webpage's. Video search\nengine based on the pavideoge can retrieve video more effectively. The\nexperiment results show the encouraging performance of our approach. Based on\nthe pavideoge, our video search engine can retrieve more precise videos in\ncomparsion with previous related work."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0909.3472v2", 
    "title": "The Universal Recommender", 
    "arxiv-id": "0909.3472v2", 
    "author": "Winfried Umbrath", 
    "publish": "2009-09-18T15:54:51Z", 
    "summary": "We describe the Universal Recommender, a recommender system for semantic\ndatasets that generalizes domain-specific recommenders such as content-based,\ncollaborative, social, bibliographic, lexicographic, hybrid and other\nrecommenders. In contrast to existing recommender systems, the Universal\nRecommender applies to any dataset that allows a semantic representation. We\ndescribe the scalable three-stage architecture of the Universal Recommender and\nits application to Internet Protocol Television (IPTV). To achieve good\nrecommendation accuracy, several novel machine learning and optimization\nproblems are identified. We finally give a brief argument supporting the need\nfor machine learning recommenders."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0909.4416v2", 
    "title": "A baseline for content-based blog classification", 
    "arxiv-id": "0909.4416v2", 
    "author": "Magnus Boman", 
    "publish": "2009-09-24T12:27:02Z", 
    "summary": "A content-based network representation of web logs (blogs) using a basic\nword-overlap similarity measure is presented. Due to a strong signal in blog\ndata the approach is sufficient for accurately classifying blogs. Using Swedish\nblog data we demonstrate that blogs that treat similar subjects are organized\nin clusters that, in turn, are hierarchically organized in higher-order\nclusters. The simplicity of the representation renders it both computationally\ntractable and transparent. We therefore argue that the approach is suitable as\na baseline when developing and analyzing more advanced content-based\nrepresentations of the blogosphere."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0910.1869v2", 
    "title": "Management Of Volatile Information In Incremental Web Crawler", 
    "arxiv-id": "0910.1869v2", 
    "author": "Annu Dhankhar", 
    "publish": "2009-10-09T21:42:58Z", 
    "summary": "Paper has been withdrawn."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0910.1938v1", 
    "title": "Information Retrieval via Truncated Hilbert-Space Expansions", 
    "arxiv-id": "0910.1938v1", 
    "author": "Bernd Freisleben", 
    "publish": "2009-10-10T19:26:13Z", 
    "summary": "In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments. In this paper, a new approach for representing term positions in\ndocuments is presented. The approach allows an efficient evaluation of\nterm-positional information at query evaluation time. Three applications are\ninvestigated: a function-based ranking optimization representing a user-defined\ndocument region, a query expansion technique based on overlapping the term\ndistributions in the top-ranked documents, and cluster analysis of terms in\ndocuments. Experimental results demonstrate the effectiveness of the proposed\napproach."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0910.4769v1", 
    "title": "Enrichissement des contenus par la r\u00e9indexation des usagers : un   \u00e9tat de l'art sur la probl\u00e9matique", 
    "arxiv-id": "0910.4769v1", 
    "author": "Sahbi Sidhom", 
    "publish": "2009-10-25T21:22:04Z", 
    "summary": "Information retrieval (IR) is a user approach to obtain relevant information\nwhich meets needs with the help of a IR system (IRS). However, the IRS shows\ncertain differences between user relevance and system relevance. These gaps are\nessentially related to the imperfection of the indexing process (as approach\nrelated to the IR), to problems related to the misunderstanding of the natural\nlanguage and the non correspondence between the real needs of the user and the\nresults of his query. As idea is to think about an ?intellectual? indexing that\ntakes into account the point of view of the user. By consulting the document,\nuser can build information as added-value on the existing content: new\ninformation which grows contents and allows the semantic visibility or\nfacilitates the reading by the annotations, by links to other content, by new\ndescriptors, specific new abstracts of users: it is the reindexing of the\ncontents by the contribution or the vote of the uses"
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0911.0914v2", 
    "title": "Enhanced Trustworthy and High-Quality Information Retrieval System for   Web Search Engines", 
    "arxiv-id": "0911.0914v2", 
    "author": "Vetriselvi Ramaraj", 
    "publish": "2009-11-04T19:22:44Z", 
    "summary": "The WWW is the most important source of information. But, there is no\nguarantee for information correctness and lots of conflicting information is\nretrieved by the search engines and the quality of provided information also\nvaries from low quality to high quality. We provide enhanced trustworthiness in\nboth specific (entity) and broad (content) queries in web searching. The\nfiltering of trustworthiness is based on 5 factors: Provenance, Authority, Age,\nPopularity, and Related Links. The trustworthiness is calculated based on these\n5 factors and it is stored thereby increasing the performance in retrieving\ntrustworthy websites. The calculated trustworthiness is stored only for static\nwebsites. Quality is provided based on policies selected by the user. Quality\nbased ranking of retrieved trusted information is provided using WIQA (Web\nInformation Quality Assessment) Framework."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0911.5046v2", 
    "title": "Integrating the Probabilistic Models BM25/BM25F into Lucene", 
    "arxiv-id": "0911.5046v2", 
    "author": "Yuval Z. Feinstein", 
    "publish": "2009-11-26T10:27:14Z", 
    "summary": "This document describes the BM25 and BM25F implementation using the Lucene\nJava Framework. Both models have stood out at TREC by their performance and are\nconsidered as state-of-the-art in the IR community. BM25 is applied to\nretrieval on plain text documents, that is for documents that do not contain\nfields, while BM25F is applied to documents with structure."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0911.5378v1", 
    "title": "De la recherche sociale d'information \u00e0 la recherche collaborative   d'information", 
    "arxiv-id": "0911.5378v1", 
    "author": "Victor Odumuyiwa", 
    "publish": "2009-11-28T06:25:00Z", 
    "summary": "In this paper, we explain social information retrieval (SIR) and\ncollaborative information retrieval (CIR). We see SIR as a way of knowing who\nto collaborate with in resolving an information problem while CIR entails the\nprocess of mutual understanding and solving of an information problem among\ncollaborators. We are interested in the transition from SIR to CIR hence we\ndeveloped a communication model to facilitate knowledge sharing during CIR."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0912.1010v1", 
    "title": "Web Document Analysis for Companies Listed in Bursa Malaysia", 
    "arxiv-id": "0912.1010v1", 
    "author": "Juhana Salim", 
    "publish": "2009-12-05T12:58:43Z", 
    "summary": "This paper discusses a research on web document analysis for companies listed\non Bursa Malaysia which is the forerunner of financial and investment center in\nMalaysia. Data set used in this research are from the company web documents\nlisted in the Main Board and Second Board on Bursa Malaysia. This research has\nused the Web Resources Extraction System which was developed by the research\ngroup mainly to extract information for the web documents involved. Our\nresearch findings have shown that the level of website usage among the\ncompanies on Bursa Malaysia is still minimal. Furthermore, research has also\nfound that 60.02 percent of the image files are utilized making it the most\nused type of file in creating websites."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0912.1294v1", 
    "title": "Conception d'un outil d'aide \u00e0 l'indexation de ressources   p\u00e9dagogiques - Extraction automatique des th\u00e9matiques et des mots-clefs   de documents UNIT", 
    "arxiv-id": "0912.1294v1", 
    "author": "Jean-Pierre P\u00e9cuchet", 
    "publish": "2009-12-07T19:43:08Z", 
    "summary": "Indexing learning documents using the Learning Object Metadata (LOM) is often\ncarried out manually by archivists. Filling out the LOM fields is a long and\ndifficult task, requiring a complete reading and a full knowledge on the topic\ndealt within the document. In this paper, we present an innovative model and\nmethod to assist the archivists in finding the important concepts and keywords\nof a learning document. The application is performed using wikipedia's category\nlinks."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0912.1421v1", 
    "title": "Context and Keyword Extraction in Plain Text Using a Graph   Representation", 
    "arxiv-id": "0912.1421v1", 
    "author": "Jean-Pierre P\u00e9cuchet", 
    "publish": "2009-12-08T07:33:27Z", 
    "summary": "Document indexation is an essential task achieved by archivists or automatic\nindexing tools. To retrieve relevant documents to a query, keywords describing\nthis document have to be carefully chosen. Archivists have to find out the\nright topic of a document before starting to extract the keywords. For an\narchivist indexing specialized documents, experience plays an important role.\nBut indexing documents on different topics is much harder. This article\nproposes an innovative method for an indexing support system. This system takes\nas input an ontology and a plain text document and provides as output\ncontextualized keywords of the document. The method has been evaluated by\nexploiting Wikipedia's category links as a termino-ontological resources."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/0912.3957v1", 
    "title": "Realization of Semantic Atom Blog", 
    "arxiv-id": "0912.3957v1", 
    "author": "Sidheshwar A. Khuba", 
    "publish": "2009-12-20T02:13:10Z", 
    "summary": "Web blog is used as a collaborative platform to publish and share\ninformation. The information accumulated in the blog intrinsically contains the\nknowledge. The knowledge shared by the community of people has intangible value\nproposition. The blog is viewed as a multimedia information resource available\non the Internet. In a blog, information in the form of text, image, audio and\nvideo builds up exponentially. The multimedia information contained in an Atom\nblog does not have the capability, which is required by the software processes\nso that Atom blog content can be accessed, processed and reused over the\nInternet. This shortcoming is addressed by exploring OWL knowledge modeling,\nsemantic annotation and semantic categorization techniques in an Atom blog\nsphere. By adopting these techniques, futuristic Atom blogs can be created and\ndeployed over the Internet."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1001.3277v2", 
    "title": "On Utilization and Importance of Perl Status Reporter (SRr) in Text   Mining", 
    "arxiv-id": "1001.3277v2", 
    "author": "Hari Cohly", 
    "publish": "2010-01-19T12:16:55Z", 
    "summary": "In Bioinformatics, text mining and text data mining sometimes interchangeably\nused is a process to derive high-quality information from text. Perl Status\nReporter (SRr) is a data fetching tool from a flat text file and in this\nresearch paper we illustrate the use of SRr in text or data mining. SRr needs a\nflat text input file where the mining process to be performed. SRr reads input\nfile and derives the high quality information from it. Typically text mining\ntasks are text categorization, text clustering, concept and entity extraction,\nand document summarization. SRr can be utilized for any of these tasks with\nlittle or none customizing efforts. In our implementation we perform text\ncategorization mining operation on input file. The input file has two\nparameters of interest (firstKey and secondKey). The composition of these two\nparameters describes the uniqueness of entries in that file in the similar\nmanner as done by composite key in database. SRr reads the input file line by\nline and extracts the parameters of interest and form a composite key by\njoining them together. It subsequently generates an output file consisting of\nthe name as firstKey secondKey. SRr reads the input file and tracks the\ncomposite key. It further stores all that data lines, having the same composite\nkey, in output file generated by SRr based on that composite key."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1001.4597v3", 
    "title": "Learning to Blend by Relevance", 
    "arxiv-id": "1001.4597v3", 
    "author": "Zhaohui Zheng", 
    "publish": "2010-01-26T06:19:06Z", 
    "summary": "Emergence of various vertical search engines highlights the fact that a\nsingle ranking technology cannot deal with the complexity and scale of search\nproblems. For example, technology behind video and image search is very\ndifferent from general web search. Their ranking functions share few features.\nQuestion answering websites (e.g., Yahoo! Answer) can make use of text matching\nand click features developed for general web, but they have unique page\nstructures and rich user feedback, e.g., thumbs up and thumbs down ratings in\nYahoo! answer, which greatly benefit their own ranking. Even for those features\nshared by answer and general web, the correlation between features and\nrelevance could be very different. Therefore, dedicated functions are needed in\norder to better rank documents within individual domains. These dedicated\nfunctions are defined on distinct feature spaces. However, having one search\nbox for each domain, is neither efficient nor scalable. Rather than typing the\nsame query two times into both Yahoo! Search and Yahoo! Answer and retrieving\ntwo ranking lists, we would prefer putting it only once but receiving a\ncomprehensive list of documents from both domains on the subject. This\nsituation calls for new technology that blends documents from different sources\ninto a single ranking list. Despite the content richness of the blended list,\nit has to be sorted by relevance none the less. We call such technology\nblending, which is the main subject of this paper."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1002.0215v1", 
    "title": "Extraction de termes, reconnaissance et labellisation de relations dans   un th\u00e9saurus", 
    "arxiv-id": "1002.0215v1", 
    "author": "Mauro Gaio", 
    "publish": "2010-02-01T10:17:55Z", 
    "summary": "Within the documentary system domain, the integration of thesauri for\nindexing and retrieval information steps is usual. In libraries, documents own\nrich descriptive information made by librarians, under descriptive notice based\non Rameau thesaurus. We exploit two kinds of information in order to create a\nfirst semantic structure. A step of conceptualization allows us to define the\nvarious modules used to automatically build the semantic structure of the\nindexation work. Our current work focuses on an approach that aims to define an\nontology based on a thesaurus. We hope to integrate new knowledge\ncharacterizing the territory of our structure (adding \"toponyms\" and links\nbetween concepts) thanks to a geographic information system (GIS)."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1002.0239v1", 
    "title": "Construction et enrichissement automatique d'ontologie \u00e0 partir de   ressources externes", 
    "arxiv-id": "1002.0239v1", 
    "author": "Mauro Gaio", 
    "publish": "2010-02-01T13:09:52Z", 
    "summary": "Automatic construction of ontologies from text is generally based on\nretrieving text content. For a much more rich ontology we extend these\napproaches by taking into account the document structure and some external\nresources (like thesaurus of indexing terms of near domain). In this paper we\ndescribe how these external resources are at first analyzed and then exploited.\nThis method has been applied on a geographical domain and the benefit has been\nevaluated."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1002.0577v1", 
    "title": "Recherche de relations spatio-temporelles : une m\u00e9thode bas\u00e9e sur   l'analyse de corpus textuels", 
    "arxiv-id": "1002.0577v1", 
    "author": "Christian Sallaberry", 
    "publish": "2010-02-02T20:07:22Z", 
    "summary": "This paper presents a work package realized for the G\\'eOnto project. A new\nmethod is proposed for an enrichment of a first geographical ontology developed\nbeforehand. This method relies on text analysis by lexico-syntactic patterns.\n  From the retrieve of n-ary relations the method automatically detect those\ninvolved in a spatial and/or temporal relation in a context of a description of\njourneys."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1002.2439v1", 
    "title": "Using Web Page Titles to Rediscover Lost Web Pages", 
    "arxiv-id": "1002.2439v1", 
    "author": "Michael L. Nelson", 
    "publish": "2010-02-11T21:45:48Z", 
    "summary": "Titles are denoted by the TITLE element within a web page. We queried the\ntitle against the the Yahoo search engine to determine the page's status\n(found, not found). We conducted several tests based on elements of the title.\nThese tests were used to discern whether we could predict a pages status based\non the title. Our results increase our ability to determine bad titles but not\nour ability to determine good titles."
},{
    "category": "cs.IR", 
    "doi": "10.1109/GCIS.2009.434", 
    "link": "http://arxiv.org/pdf/1002.3238v2", 
    "title": "Exploring a Multidimensional Representation of Documents and Queries   (extended version)", 
    "arxiv-id": "1002.3238v2", 
    "author": "Keith van Rijsbergen", 
    "publish": "2010-02-17T10:25:57Z", 
    "summary": "In Information Retrieval (IR), whether implicitly or explicitly, queries and\ndocuments are often represented as vectors. However, it may be more beneficial\nto consider documents and/or queries as multidimensional objects. Our belief is\nthis would allow building \"truly\" interactive IR systems, i.e., where\ninteraction is fully incorporated in the IR framework.\n  The probabilistic formalism of quantum physics represents events and\ndensities as multidimensional objects. This paper presents our first step\ntowards building an interactive IR framework upon this formalism, by stating\nhow the first interaction of the retrieval process, when the user types a\nquery, can be formalised. Our framework depends on a number of parameters\naffecting the final document ranking. In this paper we experimentally\ninvestigate the effect of these parameters, showing that the proposed\nrepresentation of documents and queries as multidimensional objects can compete\nwith standard approaches, with the additional prospect to be applied to\ninteractive retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1002.3342v1", 
    "title": "Spectral properties of the Google matrix of the World Wide Web and other   directed networks", 
    "arxiv-id": "1002.3342v1", 
    "author": "D. L. Shepelyansky", 
    "publish": "2010-02-17T18:22:10Z", 
    "summary": "We study numerically the spectrum and eigenstate properties of the Google\nmatrix of various examples of directed networks such as vocabulary networks of\ndictionaries and university World Wide Web networks. The spectra have gapless\nstructure in the vicinity of the maximal eigenvalue for Google damping\nparameter $\\alpha$ equal to unity. The vocabulary networks have relatively\nhomogeneous spectral density, while university networks have pronounced\nspectral structures which change from one university to another, reflecting\nspecific properties of the networks. We also determine specific properties of\neigenstates of the Google matrix, including the PageRank. The fidelity of the\nPageRank is proposed as a new characterization of its stability."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1002.4041v1", 
    "title": "Improving Term Extraction Using Particle Swarm Optimization Techniques", 
    "arxiv-id": "1002.4041v1", 
    "author": "Naomie Salim", 
    "publish": "2010-02-22T03:01:49Z", 
    "summary": "Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1002.4048v1", 
    "title": "A Hough Transform based Technique for Text Segmentation", 
    "arxiv-id": "1002.4048v1", 
    "author": "Dipak Kr. Basu", 
    "publish": "2010-02-22T03:16:55Z", 
    "summary": "Text segmentation is an inherent part of an OCR system irrespective of the\ndomain of application of it. The OCR system contains a segmentation module\nwhere the text lines, words and ultimately the characters must be segmented\nproperly for its successful recognition. The present work implements a Hough\ntransform based technique for line and word segmentation from digitized images.\nThe proposed technique is applied not only on the document image dataset but\nalso on dataset for business card reader system and license plate recognition\nsystem. For standardization of the performance of the system the technique is\nalso applied on public domain dataset published in the website by CMATER,\nJadavpur University. The document images consist of multi-script printed and\nhand written text lines with variety in script and line spacing in single\ndocument image. The technique performs quite satisfactorily when applied on\nmobile camera captured business card images with low resolution. The usefulness\nof the technique is verified by applying it in a commercial project for\nlocalization of license plate of vehicles from surveillance camera images by\nthe process of segmentation itself. The accuracy of the technique for word\nsegmentation, as verified experimentally, is 85.7% for document images, 94.6%\nfor business card images and 88% for surveillance camera images."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.1048v1", 
    "title": "Tag Clusters as Information Retrieval Interfaces", 
    "arxiv-id": "1003.1048v1", 
    "author": "Wolfgang G. Stock", 
    "publish": "2010-03-04T13:53:26Z", 
    "summary": "The paper presents our design of a next generation information retrieval\nsystem based on tag co-occurrences and subsequent clustering. We help users\ngetting access to digital data through information visualization in the form of\ntag clusters. Current problems like the absence of interactivity and semantics\nbetween tags or the difficulty of adding additional search arguments are\nsolved. In the evaluation, based upon SERVQUAL and IT systems quality\nindicators, we found out that tag clusters are perceived as more useful than\ntag clouds, are much more trustworthy, and are more enjoyable to use."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.1460v1", 
    "title": "Ontology Based Query Expansion Using Word Sense Disambiguation", 
    "arxiv-id": "1003.1460v1", 
    "author": "S. Valli", 
    "publish": "2010-03-07T12:24:29Z", 
    "summary": "The existing information retrieval techniques do not consider the context of\nthe keywords present in the user's queries. Therefore, the search engines\nsometimes do not provide sufficient information to the users. New methods based\non the semantics of user keywords must be developed to search in the vast web\nspace without incurring loss of information. The semantic based information\nretrieval techniques need to understand the meaning of the concepts in the user\nqueries. This will improve the precision-recall of the search results.\nTherefore, this approach focuses on the concept based semantic information\nretrieval. This work is based on Word sense disambiguation, thesaurus WordNet\nand ontology of any domain for retrieving information in order to capture the\ncontext of particular concept(s) and discover semantic relationships between\nthem."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.1494v1", 
    "title": "Formal Concept Analysis for Information Retrieval", 
    "arxiv-id": "1003.1494v1", 
    "author": "Yassine Ennouary", 
    "publish": "2010-03-07T17:16:34Z", 
    "summary": "In this paper we describe a mechanism to improve Information Retrieval (IR)\non the web. The method is based on Formal Concepts Analysis (FCA) that it is\nmakes semantical relations during the queries, and allows a reorganizing, in\nthe shape of a lattice of concepts, the answers provided by a search engine. We\nproposed for the IR an incremental algorithm based on Galois lattice. This\nalgorithm allows a formal clustering of the data sources, and the results which\nit turns over are classified by order of relevance. The control of relevance is\nexploited in clustering, we improved the result by using ontology in field of\nimage processing, and reformulating the user queries which make it possible to\ngive more relevant documents."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.1814v1", 
    "title": "An Analytical Approach to Document Clustering Based on Internal   Criterion Function", 
    "arxiv-id": "1003.1814v1", 
    "author": "Joydip Dhar", 
    "publish": "2010-03-09T07:28:07Z", 
    "summary": "Fast and high quality document clustering is an important task in organizing\ninformation, search engine results obtaining from user query, enhancing web\ncrawling and information retrieval. With the large amount of data available and\nwith a goal of creating good quality clusters, a variety of algorithms have\nbeen developed having quality-complexity trade-offs. Among these, some\nalgorithms seek to minimize the computational complexity using certain\ncriterion functions which are defined for the whole set of clustering solution.\nIn this paper, we are proposing a novel document clustering algorithm based on\nan internal criterion function. Most commonly used partitioning clustering\nalgorithms (e.g. k-means) have some drawbacks as they suffer from local optimum\nsolutions and creation of empty clusters as a clustering solution. The proposed\nalgorithm usually does not suffer from these problems and converge to a global\noptimum, its performance enhances with the increase in number of clusters. We\nhave checked our algorithm against three different datasets for four different\nvalues of k (required number of clusters)."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.2458v1", 
    "title": "Revisiting the Examination Hypothesis with Query Specific Position Bias", 
    "arxiv-id": "1003.2458v1", 
    "author": "Rina Panigrahy", 
    "publish": "2010-03-12T01:26:25Z", 
    "summary": "Click through rates (CTR) offer useful user feedback that can be used to\ninfer the relevance of search results for queries. However it is not very\nmeaningful to look at the raw click through rate of a search result because the\nlikelihood of a result being clicked depends not only on its relevance but also\nthe position in which it is displayed. One model of the browsing behavior, the\n{\\em Examination Hypothesis} \\cite{RDR07,Craswell08,DP08}, states that each\nposition has a certain probability of being examined and is then clicked based\non the relevance of the search snippets. This is based on eye tracking studies\n\\cite{Claypool01, GJG04} which suggest that users are less likely to view\nresults in lower positions. Such a position dependent variation in the\nprobability of examining a document is referred to as {\\em position bias}. Our\nmain observation in this study is that the position bias tends to differ with\nthe kind of information the user is looking for. This makes the position bias\n{\\em query specific}. In this study, we present a model for analyzing a query\nspecific position bias from the click data and use these biases to derive\nposition independent relevance values of search results. Our model is based on\nthe assumption that for a given query, the positional click through rate of a\ndocument is proportional to the product of its relevance and a {\\em query\nspecific} position bias. We compare our model with the vanilla examination\nhypothesis model (EH) on a set of queries obtained from search logs of a\ncommercial search engine. We also compare it with the User Browsing Model (UBM)\n\\cite{DP08} which extends the cascade model of Craswell et al\\cite{Craswell08}\nby incorporating multiple clicks in a query session. We show that the our\nmodel, although much simpler to implement, consistently outperforms both EH and\nUBM on well-used measures such as relative error and cross entropy."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.2677v1", 
    "title": "Classified Ads Harvesting Agent and Notification System", 
    "arxiv-id": "1003.2677v1", 
    "author": "Mozafar Aukin", 
    "publish": "2010-03-13T05:32:33Z", 
    "summary": "The shift from an information society to a knowledge society require rapid\ninformation harvesting, reliable search and instantaneous on demand delivery.\nInformation extraction agents are used to explore and collect data available\nfrom Web, in order to effectively exploit such data for business purposes, such\nas automatic news filtering, advertisement or product searching and price\ncomparing. In this paper, we develop a real-time automatic harvesting agent for\nadverts posted on Servihoo web portal and an SMS-based notification system. It\nuses the URL of the web portal and the object model, i.e., the fields of\ninterests and a set of rules written using the HTML parsing functions to\nextract latest adverts information. The extraction engine executes the\nextraction rules and stores the information in a database to be processed for\nautomatic notification. This intelligent system helps to tremendously save\ntime. It also enables users or potential product buyers to react more quickly\nto changes and newly posted sales adverts, paving the way to real-time best buy\ndeals."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.4067v1", 
    "title": "Computation of Reducts Using Topology and Measure of Significance of   Attributes", 
    "arxiv-id": "1003.4067v1", 
    "author": "R. Bhaskaran", 
    "publish": "2010-03-22T05:34:18Z", 
    "summary": "Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed."
},{
    "category": "cs.IR", 
    "doi": "10.1103/PhysRevE.81.056109", 
    "link": "http://arxiv.org/pdf/1003.5042v1", 
    "title": "Local Popularity based Page Link Analysis", 
    "arxiv-id": "1003.5042v1", 
    "author": "C Ravindranath Chowdary", 
    "publish": "2010-03-26T05:51:30Z", 
    "summary": "In this paper we introduce the concept of dynamic link pages. A web site/page\ncontains a number of links to other pages. All the links are not equally\nimportant. Few links are more frequently visited and few rarely visited. In\nthis scenario, identifying the frequently used links and placing them in the\ntop left corner of the page will increase the user's satisfaction. This process\nwill reduce the time spent by a visitor on the page, as most of the times, the\npopular links are presented in the visible part of the screen itself. Also, a\nsite can be indexed based on the popular links in that page. This will increase\nthe efficiency of the retrieval system. We presented a model to display the\npopular links, and also proposed a method to increase the quality of retrieval\nsystem."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s00224-008-9154-6", 
    "link": "http://arxiv.org/pdf/1004.0092v1", 
    "title": "Maximal Intersection Queries in Randomized Input Models", 
    "arxiv-id": "1004.0092v1", 
    "author": "Dirk Nowotka", 
    "publish": "2010-04-01T09:34:55Z", 
    "summary": "Consider a family of sets and a single set, called the query set. How can one\nquickly find a member of the family which has a maximal intersection with the\nquery set? Time constraints on the query and on a possible preprocessing of the\nset family make this problem challenging. Such maximal intersection queries\narise in a wide range of applications, including web search, recommendation\nsystems, and distributing on-line advertisements. In general, maximal\nintersection queries are computationally expensive. We investigate two\nwell-motivated distributions over all families of sets and propose an algorithm\nfor each of them. We show that with very high probability an almost optimal\nsolution is found in time which is logarithmic in the size of the family.\nMoreover, we point out a threshold phenomenon on the probabilities of\nintersecting sets in each of our two input models which leads to the efficient\nalgorithms mentioned above."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.0816v2", 
    "title": "Nepotistic Relationships in Twitter and their Impact on Rank Prestige   Algorithms", 
    "arxiv-id": "1004.0816v2", 
    "author": "Daniel Gayo-Avello", 
    "publish": "2010-04-06T10:26:48Z", 
    "summary": "Micro-blogging services such as Twitter allow anyone to publish anything,\nanytime. Needless to say, many of the available contents can be diminished as\nbabble or spam. However, given the number and diversity of users, some valuable\npieces of information should arise from the stream of tweets. Thus, such\nservices can develop into valuable sources of up-to-date information (the\nso-called real-time web) provided a way to find the most\nrelevant/trustworthy/authoritative users is available. Hence, this makes a\nhighly pertinent question for which graph centrality methods can provide an\nanswer. In this paper the author offers a comprehensive survey of feasible\nalgorithms for ranking users in social networks, he examines their\nvulnerabilities to linking malpractice in such networks, and suggests an\nobjective criterion against which to compare such algorithms. Additionally, he\nsuggests a first step towards \"desensitizing\" prestige algorithms against\ncheating by spammers and other abusive users."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.1257v1", 
    "title": "A Survey on Preprocessing Methods for Web Usage Data", 
    "arxiv-id": "1004.1257v1", 
    "author": "Dr. Antony Selvdoss Davamani", 
    "publish": "2010-04-08T07:07:16Z", 
    "summary": "World Wide Web is a huge repository of web pages and links. It provides\nabundance of information for the Internet users. The growth of web is\ntremendous as approximately one million pages are added daily. Users' accesses\nare recorded in web logs. Because of the tremendous usage of web, the web log\nfiles are growing at a faster rate and the size is becoming huge. Web data\nmining is the application of data mining techniques in web data. Web Usage\nMining applies mining techniques in log data to extract the behavior of users\nwhich is used in various applications like personalized services, adaptive web\nsites, customer profiling, prefetching, creating attractive web sites etc., Web\nusage mining consists of three phases preprocessing, pattern discovery and\npattern analysis. Web log data is usually noisy and ambiguous and preprocessing\nis an important process before mining. For discovering patterns sessions are to\nbe constructed efficiently. This paper reviews existing work done in the\npreprocessing stage. A brief overview of various data mining techniques for\ndiscovering patterns, and pattern analysis are discussed. Finally a glimpse of\nvarious applications of web usage mining is also presented."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.1796v1", 
    "title": "Document Clustering using Sequential Information Bottleneck Method", 
    "arxiv-id": "1004.1796v1", 
    "author": "M. Punithavalli", 
    "publish": "2010-04-11T11:23:42Z", 
    "summary": "This paper illustrates the Principal Direction Divisive Partitioning (PDDP)\nalgorithm and describes its drawbacks and introduces a combinatorial framework\nof the Principal Direction Divisive Partitioning (PDDP) algorithm, then\ndescribes the simplified version of the EM algorithm called the spherical\nGaussian EM (sGEM) algorithm and Information Bottleneck method (IB) is a\ntechnique for finding accuracy, complexity and time space. The PDDP algorithm\nrecursively splits the data samples into two sub clusters using the hyper plane\nnormal to the principal direction derived from the covariance matrix, which is\nthe central logic of the algorithm. However, the PDDP algorithm can yield poor\nresults, especially when clusters are not well separated from one another. To\nimprove the quality of the clustering results problem, it is resolved by\nreallocating new cluster membership using the IB algorithm with different\nsettings. IB Method gives accuracy but time consumption is more. Furthermore,\nbased on the theoretical background of the sGEM algorithm and sequential\nInformation Bottleneck method(sIB), it can be obvious to extend the framework\nto cover the problem of estimating the number of clusters using the Bayesian\nInformation Criterion. Experimental results are given to show the effectiveness\nof the proposed algorithm with comparison to the existing algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.2719v1", 
    "title": "Is This a Good Title?", 
    "arxiv-id": "1004.2719v1", 
    "author": "Michael L. Nelson", 
    "publish": "2010-04-15T21:30:41Z", 
    "summary": "Missing web pages, URIs that return the 404 \"Page Not Found\" error or the\nHTTP response code 200 but dereference unexpected content, are ubiquitous in\ntoday's browsing experience. We use Internet search engines to relocate such\nmissing pages and provide means that help automate the rediscovery process. We\npropose querying web pages' titles against search engines. We investigate the\nretrieval performance of titles and compare them to lexical signatures which\nare derived from the pages' content. Since titles naturally represent the\ncontent of a document they intuitively change over time. We measure the edit\ndistance between current titles and titles of copies of the same pages obtained\nfrom the Internet Archive and display their evolution. We further investigate\nthe correlation between title changes and content modifications of a web page\nover time. Lastly we provide a predictive model for the quality of any given\nweb page title in terms of its discovery performance. Our results show that\ntitles return more than 60% URIs top ranked and further relevant content\nreturned in the top 10 results. We show that titles decay slowly but are far\nmore stable than the pages' content. We further distill stop titles than can\nhelp identify insufficiently performing search engine queries."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.3274v1", 
    "title": "A New Approach to Keyphrase Extraction Using Neural Networks", 
    "arxiv-id": "1004.3274v1", 
    "author": "Suranjan Ghose", 
    "publish": "2010-04-19T18:24:02Z", 
    "summary": "Keyphrases provide a simple way of describing a document, giving the reader\nsome clues about its contents. Keyphrases can be useful in a various\napplications such as retrieval engines, browsing interfaces, thesaurus\nconstruction, text mining etc.. There are also other tasks for which keyphrases\nare useful, as we discuss in this paper. This paper describes a neural network\nbased approach to keyphrase extraction from scientific articles. Our results\nshow that the proposed method performs better than some state-of-the art\nkeyphrase extraction approaches."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.3371v1", 
    "title": "Improving Update Summarization by Revisiting the MMR Criterion", 
    "arxiv-id": "1004.3371v1", 
    "author": "Marc El-B\u00e8ze", 
    "publish": "2010-04-20T07:49:07Z", 
    "summary": "This paper describes a method for multi-document update summarization that\nrelies on a double maximization criterion. A Maximal Marginal Relevance like\ncriterion, modified and so called Smmr, is used to select sentences that are\nclose to the topic and at the same time, distant from sentences used in already\nread documents. Summaries are then generated by assembling the high ranked\nmaterial and applying some ruled-based linguistic post-processing in order to\nobtain length reduction and maintain coherency. Through a participation to the\nText Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our\nmethod achieves promising results."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.4460v1", 
    "title": "Handling Overload Conditions In High Performance Trustworthy Information   Retrieval Systems", 
    "arxiv-id": "1004.4460v1", 
    "author": "Vetriselvi Ramaraj", 
    "publish": "2010-04-26T10:03:49Z", 
    "summary": "Web search engines retrieve a vast amount of information for a given search\nquery. But the user needs only trustworthy and high-quality information from\nthis vast retrieved data. The response time of the search engine must be a\nminimum value in order to satisfy the user. An optimum level of response time\nshould be maintained even when the system is overloaded. This paper proposes an\noptimal Load Shedding algorithm which is used to handle overload conditions in\nreal-time data stream applications and is adapted to the Information Retrieval\nSystem of a web search engine. Experiment results show that the proposed\nalgorithm enables a web search engine to provide trustworthy search results to\nthe user within an optimum response time, even during overload conditions."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.4462v1", 
    "title": "BiLingual Information Retrieval System for English and Tamil", 
    "arxiv-id": "1004.4462v1", 
    "author": "Kalaiyarasi. M", 
    "publish": "2010-04-26T10:08:11Z", 
    "summary": "This paper addresses the design and implementation of BiLingual Information\nRetrieval system on the domain, Festivals. A generic platform is built for\nBiLingual Information retrieval which can be extended to any foreign or Indian\nlanguage working with the same efficiency. Search for the solution of the query\nis not done in a specific predefined set of standard languages but is chosen\ndynamically on processing the user's query. This paper deals with Indian\nlanguage Tamil apart from English. The task is to retrieve the solution for the\nuser given query in the same language as that of the query. In this process, a\nOntological tree is built for the domain in such a way that there are entries\nin the above listed two languages in every node of the tree. A Part-Of-Speech\n(POS) Tagger is used to determine the keywords from the given query. Based on\nthe context, the keywords are translated to appropriate languages using the\nOntological tree. A search is performed and documents are retrieved based on\nthe keywords. With the use of the Ontological tree, Information Extraction is\ndone. Finally, the solution for the query is translated back to the query\nlanguage (if necessary) and produced to the user."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.4489v1", 
    "title": "MIREX: MapReduce Information Retrieval Experiments", 
    "arxiv-id": "1004.4489v1", 
    "author": "Claudia Hauff", 
    "publish": "2010-04-26T11:36:38Z", 
    "summary": "We propose to use MapReduce to quickly test new retrieval approaches on a\ncluster of machines by sequentially scanning all documents. We present a small\ncase study in which we use a cluster of 15 low cost ma- chines to search a web\ncrawl of 0.5 billion pages showing that sequential scanning is a viable\napproach to running large-scale information retrieval experiments with little\neffort. The code is available to other researchers at:\nhttp://mirex.sourceforge.net"
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.5168v1", 
    "title": "Efficient and Effective Spam Filtering and Re-ranking for Large Web   Datasets", 
    "arxiv-id": "1004.5168v1", 
    "author": "Charles L. A. Clarke", 
    "publish": "2010-04-29T00:54:25Z", 
    "summary": "The TREC 2009 web ad hoc and relevance feedback tasks used a new document\ncollection, the ClueWeb09 dataset, which was crawled from the general Web in\nearly 2009. This dataset contains 1 billion web pages, a substantial fraction\nof which are spam --- pages designed to deceive search engines so as to deliver\nan unwanted payload. We examine the effect of spam on the results of the TREC\n2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.\nWe show that a simple content-based classifier with minimal training is\nefficient enough to rank the \"spamminess\" of every page in the dataset using a\nstandard personal computer in 48 hours, and effective enough to yield\nsignificant and substantive improvements in the fixed-cutoff precision (estP10)\nas well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted\nruns. Moreover, using a set of \"honeypot\" queries the labeling of training data\nmay be reduced to an entirely automatic process. The results of classical\ninformation retrieval methods are particularly enhanced by filtering --- from\namong the worst to among the best."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1004.5370v1", 
    "title": "Self-Taught Hashing for Fast Similarity Search", 
    "arxiv-id": "1004.5370v1", 
    "author": "Jinsong Lu", 
    "publish": "2010-04-29T19:25:17Z", 
    "summary": "The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal $l$-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train $l$ classifiers via supervised learning\nto predict the $l$-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1005.0268v1", 
    "title": "Node-Context Network Clustering using PARAFAC Tensor Decomposition", 
    "arxiv-id": "1005.0268v1", 
    "author": "Masashi Furukawa", 
    "publish": "2010-05-03T12:28:42Z", 
    "summary": "We describe a clustering method for labeled link network (semantic graph)\nthat can be used to group important nodes (highly connected nodes) with their\nrelevant link's labels by using PARAFAC tensor decomposition. In this kind of\nnetwork, the adjacency matrix can not be used to fully describe all information\nabout the network structure. We have to expand the matrix into 3-way adjacency\ntensor, so that not only the information about to which nodes a node connects\nto but by which link's labels is also included. And by applying PARAFAC\ndecomposition on this tensor, we get two lists, nodes and link's labels with\nscores attached to each node and labels, for each decomposition group. So\nclustering process to get the important nodes along with their relevant labels\ncan be done simply by sorting the lists in decreasing order. To test the\nmethod, we construct labeled link network by using blog's dataset, where the\nblogs are the nodes and labeled links are the shared words among them. The\nsimilarity measures between the results and standard measures look promising,\nespecially for two most important tasks, finding the most relevant words to\nblogs query and finding the most similar blogs to blogs query, about 0.87."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1005.0961v1", 
    "title": "Performance Oriented Query Processing In GEO Based Location Search   Engines", 
    "arxiv-id": "1005.0961v1", 
    "author": "S. Sivasubramanian", 
    "publish": "2010-05-06T10:09:09Z", 
    "summary": "Geographic location search engines allow users to constrain and order search\nresults in an intuitive manner by focusing a query on a particular geographic\nregion. Geographic search technology, also called location search, has recently\nreceived significant interest from major search engine companies. Academic\nresearch in this area has focused primarily on techniques for extracting\ngeographic knowledge from the web. In this paper, we study the problem of\nefficient query processing in scalable geographic search engines. Query\nprocessing is a major bottleneck in standard web search engines, and the main\nreason for the thousands of machines used by the major engines. Geographic\nsearch engine query processing is different in that it requires a combination\nof text and spatial data processing techniques. We propose several algorithms\nfor efficient query processing in geographic search engines, integrate them\ninto an existing web search query processor, and evaluate them on large sets of\nreal data and query traces."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2013.06.003", 
    "link": "http://arxiv.org/pdf/1005.4270v1", 
    "title": "Clustering Time Series Data Stream - A Literature Survey", 
    "arxiv-id": "1005.4270v1", 
    "author": "M. Punithavalli", 
    "publish": "2010-05-24T07:41:29Z", 
    "summary": "Mining Time Series data has a tremendous growth of interest in today's world.\nTo provide an indication various implementations are studied and summarized to\nidentify the different problems in existing applications. Clustering time\nseries is a trouble that has applications in an extensive assortment of fields\nand has recently attracted a large amount of research. Time series data are\nfrequently large and may contain outliers. In addition, time series are a\nspecial type of data set where elements have a temporal ordering. Therefore\nclustering of such data stream is an important issue in the data mining\nprocess. Numerous techniques and clustering algorithms have been proposed\nearlier to assist clustering of time series data streams. The clustering\nalgorithms and its effectiveness on various applications are compared to\ndevelop a new method to solve the existing problem. This paper presents a\nsurvey on various clustering algorithms available for time series datasets.\nMoreover, the distinctiveness and restriction of previous research are\ndiscussed and several achievable topics for future study are recognized.\nFurthermore the areas that utilize time series clustering are also summarized."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2010.2201", 
    "link": "http://arxiv.org/pdf/1005.5271v1", 
    "title": "A Restful Approach for Managing Citizen profiles Using A Semantic   Support", 
    "arxiv-id": "1005.5271v1", 
    "author": "Luis Anido Rifon", 
    "publish": "2010-05-28T11:55:04Z", 
    "summary": "Several steps are missing in the current high-speed race towards the holistic\nsupport of citizen needs in the domain of eGovernment. This paper is focused on\nhow to provide support for the citizen profile. This profile, in a wide sense,\nincludes personal information as well documents in possession of the citizen.\nThis also involves the provision of those mechanisms required to publish,\naccess and submit the convenient information to a Public Administration in due\ncurse of a transactional services provided with the last one. Main features of\nthe system are related to interoperability and possibilities for its inclusion\nin a cost effective manner in already developed platforms. To make that\npossible, this approach will take full advantage of semantic technologies and\nthe RESTful paradigm to design the entire system. The paper presents the\noverall system with some notes on the deployment of the solution for its\nfurther reuse in similar contexts."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2010.2201", 
    "link": "http://arxiv.org/pdf/1005.5516v2", 
    "title": "On the Fly Query Entity Decomposition Using Snippets", 
    "arxiv-id": "1005.5516v2", 
    "author": "Rodrigo Garcia", 
    "publish": "2010-05-30T11:41:43Z", 
    "summary": "One of the most important issues in Information Retrieval is inferring the\nintents underlying users' queries. Thus, any tool to enrich or to better\ncontextualized queries can proof extremely valuable. Entity extraction,\nprovided it is done fast, can be one of such tools. Such techniques usually\nrely on a prior training phase involving large datasets. That training is\ncostly, specially in environments which are increasingly moving towards real\ntime scenarios where latency to retrieve fresh informacion should be minimal.\nIn this paper an `on-the-fly' query decomposition method is proposed. It uses\nsnippets which are mined by means of a na\\\"ive statistical algorithm. An\ninitial evaluation of such a method is provided, in addition to a discussion on\nits applicability to different scenarios."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.1184v1", 
    "title": "An Algorithm to Self-Extract Secondary Keywords and Their Combinations   Based on Abstracts Collected using Primary Keywords from Online Digital   Libraries", 
    "arxiv-id": "1006.1184v1", 
    "author": "Hari Cohly", 
    "publish": "2010-06-07T06:52:44Z", 
    "summary": "The high-level contribution of this paper is the development and\nimplementation of an algorithm to selfextract secondary keywords and their\ncombinations (combo words) based on abstracts collected using standard primary\nkeywords for research areas from reputed online digital libraries like IEEE\nExplore, PubMed Central and etc. Given a collection of N abstracts, we\narbitrarily select M abstracts (M<< N; M/N as low as 0.15) and parse each of\nthe M abstracts, word by word. Upon the first-time appearance of a word, we\nquery the user for classifying the word into an Accept-List or non-Accept-List.\nThe effectiveness of the training approach is evaluated by measuring the\npercentage of words for which the user is queried for classification when the\nalgorithm parses through the words of each of the M abstracts. We observed that\nas M grows larger, the percentage of words for which the user is queried for\nclassification reduces drastically. After the list of acceptable words is built\nby parsing the M abstracts, we now parse all the N abstracts, word by word, and\ncount the frequency of appearance of each of the words in Accept-List in these\nN abstracts. We also construct a Combo-Accept-List comprising of all possible\ncombinations of the single keywords in Accept-List and parse all the N\nabstracts, two successive words (combo word) at a time, and count the frequency\nof appearance of each of the combo words in the Combo-Accept-List in these N\nabstracts."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.3498v2", 
    "title": "Fast and accurate annotation of short texts with Wikipedia pages", 
    "arxiv-id": "1006.3498v2", 
    "author": "Ugo Scaiella", 
    "publish": "2010-06-17T15:43:12Z", 
    "summary": "We address the problem of cross-referencing text fragments with Wikipedia\npages, in a way that synonymy and polysemy issues are resolved accurately and\nefficiently. We take inspiration from a recent flow of work [Cucerzan 2007,\nMihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti et al 2009], and\nextend their scenario from the annotation of long documents to the annotation\nof short texts, such as snippets of search-engine results, tweets, news, blogs,\netc.. These short and poorly composed texts pose new challenges in terms of\nefficiency and effectiveness of the annotation process, that we address by\ndesigning and engineering TAGME, the first system that performs an accurate and\non-the-fly annotation of these short textual fragments. A large set of\nexperiments shows that TAGME outperforms state-of-the-art algorithms when they\nare adapted to work on short texts and it results fast and competitive on long\ntexts."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.4458v1", 
    "title": "Few Algorithms for ascertaining merit of a document and their   applications", 
    "arxiv-id": "1006.4458v1", 
    "author": "Ka. Shrinivaasan", 
    "publish": "2010-06-23T09:50:06Z", 
    "summary": "Existing models for ranking documents(mostly in world wide web) are prestige\nbased. In this article, three algorithms to objectively judge the merit of a\ndocument are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap\nbased intrinsic merit scoring and 3) Interview algorithm. A short discussion on\ngeneric judgement and its mathematical treatment is presented in introduction\nto motivate these algorithms."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.4535v1", 
    "title": "Studies on Relevance, Ranking and Results Display", 
    "arxiv-id": "1006.4535v1", 
    "author": "Jaime Carbonell", 
    "publish": "2010-06-23T14:43:22Z", 
    "summary": "This study considers the extent to which users with the same query agree as\nto what is relevant, and how what is considered relevant may translate into a\nretrieval algorithm and results display. To combine user perceptions of\nrelevance with algorithm rank and to present results, we created a prototype\ndigital library of scholarly literature. We confine studies to one population\nof scientists (paleontologists), one domain of scholarly scientific articles\n(paleo-related), and a prototype system (PaleoLit) that we built for the\npurpose. Based on the principle that users do not pre-suppose answers to a\ngiven query but that they will recognize what they want when they see it, our\nsystem uses a rules-based algorithm to cluster results into fuzzy categories\nwith three relevance levels. Our system matches at least 1/3 of our\nparticipants' relevancy ratings 87% of the time. Our subsequent usability study\nfound that participants trusted our uncertainty labels but did not value our\ncolor-coded horizontal results layout above a standard retrieval list. We posit\nthat users make such judgments in limited time, and that time optimization per\ntask might help explain some of our findings."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.4568v1", 
    "title": "Approaches, Challenges and Future Direction of Image Retrieval", 
    "arxiv-id": "1006.4568v1", 
    "author": "N. A. Ismail", 
    "publish": "2010-06-23T15:21:11Z", 
    "summary": "This paper attempts to discuss the evolution of the retrieval approaches\nfocusing on development, challenges and future direction of the image\nretrieval. It highlights both the already addressed and outstanding issues. The\nexplosive growth of image data leads to the need of research and development of\nImage Retrieval. However, Image retrieval researches are moving from keyword,\nto low level features and to semantic features. Drive towards semantic features\nis due to the problem of the keywords which can be very subjective and time\nconsuming while low level features cannot always describe high level concepts\nin the users' mind. Hence, introducing an interpretation inconsistency between\nimage descriptors and high level semantics that known as the semantic gap. This\npaper also discusses the semantic gap issues, user query mechanisms as well as\ncommon ways used to bridge the gap in image retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.4953v1", 
    "title": "Large scale link based latent Dirichlet allocation for web document   classification", 
    "arxiv-id": "1006.4953v1", 
    "author": "J\u00e1cint Szab\u00f3", 
    "publish": "2010-06-25T10:19:09Z", 
    "summary": "In this paper we demonstrate the applicability of latent Dirichlet allocation\n(LDA) for classifying large Web document collections. One of our main results\nis a novel influence model that gives a fully generative model of the document\ncontent taking linkage into account. In our setup, topics propagate along links\nin such a way that linked documents directly influence the words in the linking\ndocument. As another main contribution we develop LDA specific boosting of\nGibbs samplers resulting in a significant speedup in our experiments. The\ninferred LDA model can be applied for classification as dimensionality\nreduction similarly to latent semantic indexing. In addition, the model yields\nlink weights that can be applied in algorithms to process the Web graph; as an\nexample we deploy LDA link weights in stacked graphical learning. By using\nWeka's BayesNet classifier, in terms of the AUC of classification, we achieve\n4% improvement over plain LDA with BayesNet and 18% over tf.idf with SVM. Our\nGibbs sampling strategies yield about 5-10 times speedup with less than 1%\ndecrease in accuracy in terms of likelihood and AUC of classification."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.5040v1", 
    "title": "The comparison of Wiktionary thesauri transformed into the   machine-readable format", 
    "arxiv-id": "1006.5040v1", 
    "author": "A. A. Krizhanovsky", 
    "publish": "2010-06-25T18:51:13Z", 
    "summary": "Wiktionary is a unique, peculiar, valuable and original resource for natural\nlanguage processing (NLP). The paper describes an open-source Wiktionary\nparser: its architecture and requirements followed by a description of\nWiktionary features to be taken into account, some open problems of Wiktionary\nand the parser. The current implementation of the parser extracts the\ndefinitions, semantic relations, and translations from English and Russian\nWiktionaries. The paper's goal is to interest researchers (1) in using the\nconstructed machine-readable dictionary for different NLP tasks, (2) in\nextending the software to parse 170 still unused Wiktionaries. The comparison\nof a number and types of semantic relations, a number of definitions, and a\nnumber of translations in the English Wiktionary and the Russian Wiktionary has\nbeen carried out. It was found that the number of semantic relations in the\nEnglish Wiktionary is larger by 1.57 times than in Russian (157 and 100\nthousands). But the Russian Wiktionary has more \"rich\" entries (with a big\nnumber of semantic relations), e.g. the number of entries with three or more\nsemantic relations is larger by 1.63 times than in the English Wiktionary. Upon\ncomparison, it was found out the methodological shortcomings of the Wiktionary."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1006.5059v1", 
    "title": "Capacity Planning for Vertical Search Engines", 
    "arxiv-id": "1006.5059v1", 
    "author": "Nivio Ziviani", 
    "publish": "2010-06-25T20:22:08Z", 
    "summary": "Vertical search engines focus on specific slices of content, such as the Web\nof a single country or the document collection of a large corporation. Despite\nthis, like general open web search engines, they are expensive to maintain,\nexpensive to operate, and hard to design. Because of this, predicting the\nresponse time of a vertical search engine is usually done empirically through\nexperimentation, requiring a costly setup. An alternative is to develop a model\nof the search engine for predicting performance. However, this alternative is\nof interest only if its predictions are accurate. In this paper we propose a\nmethodology for analyzing the performance of vertical search engines. Applying\nthe proposed methodology, we present a capacity planning model based on a\nqueueing network for search engines with a scale typically suitable for the\nneeds of large corporations. The model is simple and yet reasonably accurate\nand, in contrast to previous work, considers the imbalance in query service\ntimes among homogeneous index servers. We discuss how we tune up the model and\nhow we apply it to predict the impact on the query response time when\nparameters such as CPU and disk capacities are changed. This allows a manager\nof a vertical search engine to determine a priori whether a new configuration\nof the system might keep the query response under specified performance\nconstraints."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1008.0441v1", 
    "title": "An Optimal Trade-off between Content Freshness and Refresh Cost", 
    "arxiv-id": "1008.0441v1", 
    "author": "Jie Mi", 
    "publish": "2010-08-03T02:40:10Z", 
    "summary": "Caching is an effective mechanism for reducing bandwidth usage and\nalleviating server load. However, the use of caching entails a compromise\nbetween content freshness and refresh cost. An excessive refresh allows a high\ndegree of content freshness at a greater cost of system resource. Conversely, a\ndeficient refresh inhibits content freshness but saves the cost of resource\nusages. To address the freshness-cost problem, we formulate the refresh\nscheduling problem with a generic cost model and use this cost model to\ndetermine an optimal refresh frequency that gives the best tradeoff between\nrefresh cost and content freshness. We prove the existence and uniqueness of an\noptimal refresh frequency under the assumptions that the arrival of content\nupdate is Poisson and the age-related cost monotonically increases with\ndecreasing freshness. In addition, we provide an analytic comparison of system\nperformance under fixed refresh scheduling and random refresh scheduling,\nshowing that with the same average refresh frequency two refresh schedulings\nare mathematically equivalent in terms of the long-run average cost."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1008.0716v2", 
    "title": "Cross-Lingual Adaptation using Structural Correspondence Learning", 
    "arxiv-id": "1008.0716v2", 
    "author": "Benno Stein", 
    "publish": "2010-08-04T08:42:07Z", 
    "summary": "Cross-lingual adaptation, a special case of domain adaptation, refers to the\ntransfer of classification knowledge between two languages. In this article we\ndescribe an extension of Structural Correspondence Learning (SCL), a recently\nproposed algorithm for domain adaptation, for cross-lingual adaptation. The\nproposed method uses unlabeled documents from both languages, along with a word\ntranslation oracle, to induce cross-lingual feature correspondences. From these\ncorrespondences a cross-lingual representation is created that enables the\ntransfer of classification knowledge from the source to the target language.\nThe main advantages of this approach over other approaches are its resource\nefficiency and task specificity.\n  We conduct experiments in the area of cross-language topic and sentiment\nclassification involving English as source language and German, French, and\nJapanese as target languages. The results show a significant improvement of the\nproposed method over a machine translation baseline, reducing the relative\nerror due to cross-lingual adaptation by an average of 30% (topic\nclassification) and 59% (sentiment classification). We further report on\nempirical analyses that reveal insights into the use of unlabeled data, the\nsensitivity with respect to important hyperparameters, and the nature of the\ninduced cross-lingual correspondences."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2307", 
    "link": "http://arxiv.org/pdf/1008.1335v1", 
    "title": "Designing a Dynamic Components and Agent based Approach for Semantic   Information Retrieval", 
    "arxiv-id": "1008.1335v1", 
    "author": "Detlef Gerhard", 
    "publish": "2010-08-07T12:41:26Z", 
    "summary": "In this paper based on agent and semantic web technologies we propose an\napproach .i.e., Semantic Oriented Agent Based Search (SOAS), to cope with\ncurrently existing challenges of Meta data extraction, modeling and information\nretrieval over the web. SOAS is designed by keeping four major requirements\n.i.e., Automatic user request handling, Dynamic unstructured full text reading,\nAnalysing and modeling, Semantic query generation and optimized result\nclassifier. The architecture of SOAS is consisting of an agent called Personal\nAgent (PA) and five dynamic components .i.e., Request Processing Unit (RPU),\nAgent Locator (AL), Agent Communicator (AC), List Builder (LB) and Result\nGenerator (RG). Furthermore, in this paper we briefly discuss Semantic Web and\nsome already existing in time proposed and implemented semantic based\napproaches."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1008.4815v1", 
    "title": "Recommender Systems by means of Information Retrieval", 
    "arxiv-id": "1008.4815v1", 
    "author": "Fabio Roda", 
    "publish": "2010-08-27T22:24:25Z", 
    "summary": "In this paper we present a method for reformulating the Recommender Systems\nproblem in an Information Retrieval one. In our tests we have a dataset of\nusers who give ratings for some movies; we hide some values from the dataset,\nand we try to predict them again using its remaining portion (the so-called\n\"leave-n-out approach\"). In order to use an Information Retrieval algorithm, we\nreformulate this Recommender Systems problem in this way: a user corresponds to\na document, a movie corresponds to a term, the active user (whose rating we\nwant to predict) plays the role of the query, and the ratings are used as\nweigths, in place of the weighting schema of the original IR algorithm. The\noutput is the ranking list of the documents (\"users\") relevant for the query\n(\"active user\"). We use the ratings of these users, weighted according to the\nrank, to predict the rating of the active user. We carry out the comparison by\nmeans of a typical metric, namely the accuracy of the predictions returned by\nthe algorithm, and we compare this to the real ratings from users. In our first\ntests, we use two different Information Retrieval algorithms: LSPR, a recently\nproposed model based on Discrete Fourier Transform, and a simple vector space\nmodel."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1009.4964v1", 
    "title": "Text Classification using Artificial Intelligence", 
    "arxiv-id": "1009.4964v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-25T01:08:27Z", 
    "summary": "Text classification is the process of classifying documents into predefined\ncategories based on their content. It is the automated assignment of natural\nlanguage texts to predefined categories. Text classification is the primary\nrequirement of text retrieval systems, which retrieve texts in response to a\nuser query, and text understanding systems, which transform text in some way\nsuch as producing summaries, answering questions or extracting data. Existing\nsupervised learning algorithms for classifying text need sufficient documents\nto learn accurately. This paper presents a new algorithm for text\nclassification using artificial intelligence technique that requires fewer\ndocuments for training. Instead of using words, word relation i.e. association\nrules from these words is used to derive feature set from pre-classified text\ndocuments. The concept of na\\\"ive Bayes classifier is then used on derived\nfeatures and finally only a single concept of genetic algorithm has been added\nfor final classification. A system based on the proposed algorithm has been\nimplemented and tested. The experimental results show that the proposed system\nworks as a successful text classifier."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1010.1824v1", 
    "title": "Implications of Inter-Rater Agreement on a Student Information Retrieval   Evaluation", 
    "arxiv-id": "1010.1824v1", 
    "author": "Peter Mutschke", 
    "publish": "2010-10-09T09:57:44Z", 
    "summary": "This paper is about an information retrieval evaluation on three different\nretrieval-supporting services. All three services were designed to compensate\ntypical problems that arise in metadata-driven Digital Libraries, which are not\nadequately handled by a simple tf-idf based retrieval. The services are: (1) a\nco-word analysis based query expansion mechanism and re-ranking via (2)\nBradfordizing and (3) author centrality. The services are evaluated with\nrelevance assessments conducted by 73 information science students. Since the\nstudents are neither information professionals nor domain experts the question\nof inter-rater agreement is taken into consideration. Two important\nimplications emerge: (1) the inter-rater agreement rates were mainly fair to\nmoderate and (2) after a data-cleaning step which erased the assessments with\npoor agreement rates the evaluation data shows that the three retrieval\nservices returned disjoint but still relevant result sets."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1010.3898v2", 
    "title": "Advancements in scientific data searching, sharing and retrieval", 
    "arxiv-id": "1010.3898v2", 
    "author": "Bruce Wilson", 
    "publish": "2010-10-19T13:17:39Z", 
    "summary": "The Open Archive Initiative Protocol for Metadata Handling (OAI-PMHiii) is a\nstandard that is seeing increased use as a means for exchanging structured\nmetadata. OAI-PMH implementations must support Dublin Core as a metadata\nstandard, with other metadata formats as optional. We have developed tools\nwhich enable Mercury to consume metadata from OAI-PMH services in any of the\nmetadata formats we support (Dublin Core, Darwin Core, FCDC CSDGM, GCMD DIF,\nEML, and ISO 19115/19137). We are also making ORNL DAAC metadata available\nthrough OAI-PMH for other metadata tools to utilize. This paper describes\nMercury capabilities with multiple metadata formats, in general, and, more\nspecifically, the results of our OAI-PMH implementations and the lessons\nlearned."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1010.5584v1", 
    "title": "A derivational rephrasing experiment for question answering", 
    "arxiv-id": "1010.5584v1", 
    "author": "Bernard Jacquemin", 
    "publish": "2010-10-27T06:33:06Z", 
    "summary": "In Knowledge Management, variations in information expressions have proven a\nreal challenge. In particular, classical semantic relations (e.g. synonymy) do\nnot connect words with different parts-of-speech. The method proposed tries to\naddress this issue. It consists in building a derivational resource from a\nmorphological derivation tool together with derivational guidelines from a\ndictionary in order to store only correct derivatives. This resource, combined\nwith a syntactic parser, a semantic disambiguator and some derivational\npatterns, helps to reformulate an original sentence while keeping the initial\nmeaning in a convincing manner This approach has been evaluated in three\ndifferent ways: the precision of the derivatives produced from a lemma; its\nability to provide well-formed reformulations from an original sentence,\npreserving the initial meaning; its impact on the results coping with a real\nissue, ie a question answering task . The evaluation of this approach through a\nquestion answering system shows the pros and cons of this system, while\nforeshadowing some interesting future developments."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1010.6242v1", 
    "title": "GraphDuplex: visualisation simultan\u00e9e de N r\u00e9seaux coupl\u00e9s 2 par 2", 
    "arxiv-id": "1010.6242v1", 
    "author": "Bernard Jacquemin", 
    "publish": "2010-10-29T15:11:05Z", 
    "summary": "While social network analysis often focuses on graph structure of social\nactors, an increasing number of communication networks now provide textual\ncontent within social activity (email, instant messaging, blogging,\ncollaboration networks). We present an open source visualization software,\nGraphDuplex, which brings together social structure and textual content, adding\na semantic dimension to social analysis. GraphDuplex eventually connects any\nnumber of social or semantic graphs together, and through dynamic queries\nenables user interaction and exploration across multiple graphs of different\nnature."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1988688.1988755", 
    "link": "http://arxiv.org/pdf/1011.0306v1", 
    "title": "Semantic Query Optimisation with Ontology Simulation", 
    "arxiv-id": "1011.0306v1", 
    "author": "Narina Thakur", 
    "publish": "2010-11-01T12:43:53Z", 
    "summary": "Semantic Web is, without a doubt, gaining momentum in both industry and\nacademia. The word \"Semantic\" refers to \"meaning\" - a semantic web is a web of\nmeaning. In this fast changing and result oriented practical world, gone are\nthe days where an individual had to struggle for finding information on the\nInternet where knowledge management was the major issue. The semantic web has a\nvision of linking, integrating and analysing data from various data sources and\nforming a new information stream, hence a web of databases connected with each\nother and machines interacting with other machines to yield results which are\nuser oriented and accurate. With the emergence of Semantic Web framework the\nna\\\"ive approach of searching information on the syntactic web is clich\\'e.\nThis paper proposes an optimised semantic searching of keywords exemplified by\nsimulation an ontology of Indian universities with a proposed algorithm which\nramifies the effective semantic retrieval of information which is easy to\naccess and time saving."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1011.0404v1", 
    "title": "A New Email Retrieval Ranking Approach", 
    "arxiv-id": "1011.0404v1", 
    "author": "Reem Bahgat", 
    "publish": "2010-11-01T18:20:27Z", 
    "summary": "Email Retrieval task has recently taken much attention to help the user\nretrieve the email(s) related to the submitted query. Up to our knowledge,\nexisting email retrieval ranking approaches sort the retrieved emails based on\nsome heuristic rules, which are either search clues or some predefined user\ncriteria rooted in email fields. Unfortunately, the user usually does not know\nthe effective rule that acquires best ranking related to his query. This paper\npresents a new email retrieval ranking approach to tackle this problem. It\nranks the retrieved emails based on a scoring function that depends on crucial\nemail fields, namely subject, content, and sender. The paper also proposes an\narchitecture to allow every user in a network/group of users to be able, if\npermissible, to know the most important network senders who are interested in\nhis submitted query words. The experimental evaluation on Enron corpus prove\nthat our approach outperforms known email retrieval ranking approaches"
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1011.0502v1", 
    "title": "A New Email Retrieval Ranking Approach", 
    "arxiv-id": "1011.0502v1", 
    "author": "Reem Bahgat", 
    "publish": "2010-11-02T02:32:40Z", 
    "summary": "Email Retrieval task has recently taken much attention to help the user\nretrieve the email(s) related to the submitted query. Up to our knowledge,\nexisting email retrieval ranking approaches sort the retrieved emails based on\nsome heuristic rules, which are either search clues or some predefined user\ncriteria rooted in email fields. Unfortunately, the user usually does not know\nthe effective rule that acquires best ranking related to his query. This paper\npresents a new email retrieval ranking approach to tackle this problem. It\nranks the retrieved emails based on a scoring function that depends on crucial\nemail fields, namely subject, content, and sender. The paper also proposes an\narchitecture to allow every user in a network/group of users to be able, if\npermissible, to know the most important network senders who are interested in\nhis submitted query words. The experimental evaluation on Enron corpus prove\nthat our approach outperforms known email retrieval ranking approaches."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1011.1368v1", 
    "title": "Transformation of Wiktionary entry structure into tables and relations   in a relational database schema", 
    "arxiv-id": "1011.1368v1", 
    "author": "A. A. Krizhanovsky", 
    "publish": "2010-11-05T11:01:45Z", 
    "summary": "This paper addresses the question of automatic data extraction from the\nWiktionary, which is a multilingual and multifunctional dictionary. Wiktionary\nis a collaborative project working on the same principles as the Wikipedia. The\nWiktionary entry is a plain text from the text processing point of view.\nWiktionary guidelines prescribe the entry layout and rules, which should be\nfollowed by editors of the dictionary. The presence of the structure of a\nWiktionary article and formatting rules allows transforming the Wiktionary\nentry structure into tables and relations in a relational database schema,\nwhich is a part of a machine-readable dictionary (MRD). The paper describes how\nthe flat text of the Wiktionary entry was extracted, converted, and stored in\nthe specially designed relational database. The MRD contains the definitions,\nsemantic relations, and translations extracted from the English and Russian\nWiktionaries. The parser software is released under the open source license\nagreement (GPL), to facilitate its dissemination, modification and upgrades, to\ndraw researchers and programmers into parsing other Wiktionaries, not only\nRussian and English."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1011.5364v1", 
    "title": "Optimizing On-Line Advertising", 
    "arxiv-id": "1011.5364v1", 
    "author": "Giovanni Giuffrida", 
    "publish": "2010-11-24T13:06:18Z", 
    "summary": "We want to find the optimal strategy for displaying advertisements e.g.\nbanners, videos, in given locations at given times under some realistic dynamic\nconstraints. Our primary goal is to maximize the expected revenue in a given\nperiod of time, i.e. the total profit produced by the impressions, which\ndepends on profit-generating events such as the impressions themselves, the\nensuing clicks and registrations. Moreover we must take into consideration the\npossibility that the constraints could change in time in a way that cannot\nalways be foreseen."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.0854v1", 
    "title": "Semantic Content Filtering with Wikipedia and Ontologies", 
    "arxiv-id": "1012.0854v1", 
    "author": "Pekka Korhonen", 
    "publish": "2010-12-03T21:23:21Z", 
    "summary": "The use of domain knowledge is generally found to improve query efficiency in\ncontent filtering applications. In particular, tangible benefits have been\nachieved when using knowledge-based approaches within more specialized fields,\nsuch as medical free texts or legal documents. However, the problem is that\nsources of domain knowledge are time-consuming to build and equally costly to\nmaintain. As a potential remedy, recent studies on Wikipedia suggest that this\nlarge body of socially constructed knowledge can be effectively harnessed to\nprovide not only facts but also accurate information about semantic\nconcept-similarities. This paper describes a framework for document filtering,\nwhere Wikipedia's concept-relatedness information is combined with a domain\nontology to produce semantic content classifiers. The approach is evaluated\nusing Reuters RCV1 corpus and TREC-11 filtering task definitions. In a\ncomparative study, the approach shows robust performance and appears to\noutperform content classifiers based on Support Vector Machines (SVM) and C4.5\nalgorithm."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.1609v1", 
    "title": "Building conceptual spaces for exploring and linking biomedical   resources", 
    "arxiv-id": "1012.1609v1", 
    "author": "V. Nebot", 
    "publish": "2010-12-07T21:16:59Z", 
    "summary": "The establishment of links between data (e.g., patient records) and Web\nresources (e.g., literature) and the proper visualization of such discovered\nknowledge is still a challenge in most Life Science domains (e.g.,\nbiomedicine). In this paper we present our contribution to the community in the\nform of an infrastructure to annotate information resources, to discover\nrelationships among them, and to represent and visualize the new discovered\nknowledge. Furthermore, we have also implemented a Web-based prototype tool\nwhich integrates the proposed infrastructure."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.1617v1", 
    "title": "User Centered and Ontology Based Information Retrieval System for Life   Sciences", 
    "arxiv-id": "1012.1617v1", 
    "author": "Michel Crampes", 
    "publish": "2010-12-07T21:41:17Z", 
    "summary": "Because of the increasing number of electronic data, designing efficient\ntools to retrieve and exploit documents is a major challenge. Current search\nengines suffer from two main drawbacks: there is limited interaction with the\nlist of retrieved documents and no explanation for their adequacy to the query.\nUsers may thus be confused by the selection and have no idea how to adapt their\nquery so that the results match their expectations. This paper describes a\nrequest method and an environment based on aggregating models to assess the\nrelevance of documents annotated by concepts of ontology. The selection of\ndocuments is then displayed in a semantic map to provide graphical indications\nthat make explicit to what extent they match the user's query; this man/machine\ninterface favors a more interactive exploration of data corpus."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.1663v1", 
    "title": "A Concept Annotation System for Clinical Records", 
    "arxiv-id": "1012.1663v1", 
    "author": "Jan A. Kors", 
    "publish": "2010-12-08T00:56:45Z", 
    "summary": "Unstructured information comprises a valuable source of data in clinical\nrecords. For text mining in clinical records, concept extraction is the first\nstep in finding assertions and relationships. This study presents a system\ndeveloped for the annotation of medical concepts, including medical problems,\ntests, and treatments, mentioned in clinical records. The system combines six\npublicly available named entity recognition system into one framework, and uses\na simple voting scheme that allows to tune precision and recall of the system\nto specific needs. The system provides both a web service interface and a UIMA\ninterface which can be easily used by other systems. The system was tested in\nthe fourth i2b2 challenge and achieved an F-score of 82.1% for the concept\nexact match task, a score which is among the top-ranking systems. To our\nknowledge, this is the first publicly available clinical record concept\nannotation system."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.1666v1", 
    "title": "SPARQL Assist Language-Neutral Query Composer", 
    "arxiv-id": "1012.1666v1", 
    "author": "Mark Wilkinson", 
    "publish": "2010-12-08T01:10:59Z", 
    "summary": "SPARQL query composition is difficult for the lay-person or even the\nexperienced bioinformatician in cases where the data model is unfamiliar.\nEstablished best-practices and internationalization concerns dictate that\nsemantic web ontologies should use terms with opaque identifiers, further\ncomplicating the task. We present SPARQL Assist: a web application that\naddresses these issues by providing context-sensitive type-ahead completion to\nexisting web forms. Ontological terms are suggested using their labels and\ndescriptions, leveraging existing XML support for internationalization and\nlanguage-neutrality."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.3278v1", 
    "title": "Collaborative Knowledge Creation and Management in Information Retrieval", 
    "arxiv-id": "1012.3278v1", 
    "author": "David Amos", 
    "publish": "2010-12-15T10:59:37Z", 
    "summary": "The final goal of Information Retrieval (IR) is knowledge production.\nHowever, it has been argued that knowledge production is not an individual\neffort but a collaborative effort. Collaboration in information retrieval is\ngeared towards knowledge sharing and creation of new knowledge by users. This\npaper discusses Collaborative Information Retrieval (CIR) and how it culminates\nto knowledge creation. It explains how created knowledge is organized and\nstructured. It describes a functional architecture for the development of a CIR\nprototype called MECOCIR. Some of the features of the prototype are presented\nas well as how they facilitate collaborative knowledge exploitation. Knowledge\ncreation is explained through the knowledge conversion/transformation processes\nproposed by Nonaka and CIR activities that facilitate these processes are\nhigh-lighted and discussed"
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1012.3805v1", 
    "title": "Element Retrieval using Namespace Based on keyword search over XML   Documents", 
    "arxiv-id": "1012.3805v1", 
    "author": "Xiaodi Huang", 
    "publish": "2010-12-17T04:00:10Z", 
    "summary": "Querying over XML elements using keyword search is steadily gaining\npopularity. The traditional similarity measure is widely employed in order to\neffectively retrieve various XML documents. A number of authors have already\nproposed different similarity-measure methods that take advantage of the\nstructure and content of XML documents. They do not, however, consider the\nsimilarity between latent semantic information of element texts and that of\nkeywords in a query. Although many algorithms on XML element search are\navailable, some of them have the high computational complexity due to searching\na huge number of elements. In this paper, we propose a new algorithm that makes\nuse of the semantic similarity between elements instead of between entire XML\ndocuments, considering not only the structure and content of an XML document,\nbut also semantic information of namespaces in elements. We compare our\nalgorithm with the three other algorithms by testing on the real datasets. The\nexperiments have demonstrated that our proposed method is able to improve the\nquery accuracy, as well as to reduce the running time."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1101.0766v1", 
    "title": "Information Retrieval of Jumbled Words", 
    "arxiv-id": "1101.0766v1", 
    "author": "Venkata Ravinder Paruchuri", 
    "publish": "2011-01-04T16:59:24Z", 
    "summary": "It is known that humans can easily read words where the letters have been\njumbled in a certain way. This paper examines this problem by associating a\ndistance measure with the jumbling process. Modifications to text were\ngenerated according to the Damerau-Levenshtein distance and it was checked if\nthe users are able to read it. Graphical representations of the results are\nprovided."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1101.3400v1", 
    "title": "Behavioral On-Line Advertising", 
    "arxiv-id": "1101.3400v1", 
    "author": "Calogero Zarba", 
    "publish": "2011-01-18T08:39:12Z", 
    "summary": "We present a new algorithm for behavioral targeting of banner advertisements.\nWe record different user's actions such as clicks, search queries and page\nviews. We use the collected information on the user to estimate in real time\nthe probability of a click on a banner. A banner is displayed if it either has\nthe highest probability of being clicked or if it is the one that generates the\nhighest average profit."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1101.5763v1", 
    "title": "A New Semantic Web Approach for Constructing, Searching and Modifying   Ontology Dynamically", 
    "arxiv-id": "1101.5763v1", 
    "author": "Sounak Chakravorty", 
    "publish": "2011-01-30T11:33:51Z", 
    "summary": "Semantic web is the next generation web, which concerns the meaning of web\ndocuments It has the immense power to pull out the most relevant information\nfrom the web pages, which is also meaningful to any user, using software\nagents. In today's world, agent communication is not possible if concerned\nontology is changed a little. We have pointed out this very problem and\ndeveloped an Ontology Purification System to help agent communication. In our\nsystem you can send queries and view the search results. If it can't meet the\ncriteria then it finds out the mismatched elements. Modification is done within\na second and you can see the difference. That's why we emphasis on the word\ndynamic. When Administrator is updating the system, at the same time that\nupdation is visible to the user."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1102.0676v1", 
    "title": "Architecture of A Scalable Dynamic Parallel WebCrawler with High Speed   Downloadable Capability for a Web Search Engine", 
    "arxiv-id": "1102.0676v1", 
    "author": "Young-Chon Kim", 
    "publish": "2011-02-03T13:20:51Z", 
    "summary": "Today World Wide Web (WWW) has become a huge ocean of information and it is\ngrowing in size everyday. Downloading even a fraction of this mammoth data is\nlike sailing through a huge ocean and it is a challenging task indeed. In order\nto download a large portion of data from WWW, it has become absolutely\nessential to make the crawling process parallel. In this paper we offer the\narchitecture of a dynamic parallel Web crawler, christened as \"WEB-SAILOR,\"\nwhich presents a scalable approach based on Client-Server model to speed up the\ndownload process on behalf of a Web Search Engine in a distributed Domain-set\nspecific environment. WEB-SAILOR removes the possibility of overlapping of\ndownloaded documents by multiple crawlers without even incurring the cost of\ncommunication overhead among several parallel \"client\" crawling processes."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1102.0694v1", 
    "title": "A Syntactic Classification based Web Page Ranking Algorithm", 
    "arxiv-id": "1102.0694v1", 
    "author": "Young-Chon Kim", 
    "publish": "2011-02-03T14:19:10Z", 
    "summary": "The existing search engines sometimes give unsatisfactory search result for\nlack of any categorization of search result. If there is some means to know the\npreference of user about the search result and rank pages according to that\npreference, the result will be more useful and accurate to the user. In the\npresent paper a web page ranking algorithm is being proposed based on syntactic\nclassification of web pages. Syntactic Classification does not bother about the\nmeaning of the content of a web page. The proposed approach mainly consists of\nthree steps: select some properties of web pages based on user's demand,\nmeasure them, and give different weightage to each property during ranking for\ndifferent types of pages. The existence of syntactic classification is\nsupported by running fuzzy c-means algorithm and neural network classification\non a set of web pages. The change in ranking for difference in type of pages\nbut for same query string is also being demonstrated."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsit.2010.2504", 
    "link": "http://arxiv.org/pdf/1102.0695v1", 
    "title": "A Domain Specific Ontology Based Semantic Web Search Engine", 
    "arxiv-id": "1102.0695v1", 
    "author": "Young-Chon Kim", 
    "publish": "2011-02-03T14:31:25Z", 
    "summary": "Since its emergence in the 1990s the World Wide Web (WWW) has rapidly evolved\ninto a huge mine of global information and it is growing in size everyday. The\npresence of huge amount of resources on the Web thus poses a serious problem of\naccurate search. This is mainly because today's Web is a human-readable Web\nwhere information cannot be easily processed by machine. Highly sophisticated,\nefficient keyword based search engines that have evolved today have not been\nable to bridge this gap. So comes up the concept of the Semantic Web which is\nenvisioned by Tim Berners-Lee as the Web of machine interpretable information\nto make a machine processable form for expressing information. Based on the\nsemantic Web technologies we present in this paper the design methodology and\ndevelopment of a semantic Web search engine which provides exact search results\nfor a domain specific search. This search engine is developed for an\nagricultural Website which hosts agricultural information about the state of\nWest Bengal."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2011.2102", 
    "link": "http://arxiv.org/pdf/1102.0735v1", 
    "title": "Analyzing the Impact of Visitors on Page Views with Google Analytics", 
    "arxiv-id": "1102.0735v1", 
    "author": "Najes Shokry", 
    "publish": "2011-02-03T17:18:43Z", 
    "summary": "This paper develops a flexible methodology to analyze the effectiveness of\ndifferent variables on various dependent variables which all are times series\nand especially shows how to use a time series regression on one of the most\nimportant and primary index (page views per visit) on Google analytic and in\nconjunction it shows how to use the most suitable data to gain a more accurate\nresult. Search engine visitors have a variety of impact on page views which\ncannot be described by single regression. On one hand referral visitors are\nwell-fitted on linear regression with low impact. On the other hand, direct\nvisitors made a huge impact on page views. The higher connection speed does not\nsimply imply higher impact on page views and the content of web page and the\nterritory of visitors can help connection speed to describe user behavior.\nReturning visitors have some similarities with direct visitors."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1102.1111v1", 
    "title": "Treelicious: a System for Semantically Navigating Tagged Web Pages", 
    "arxiv-id": "1102.1111v1", 
    "author": "Perry Fizzano", 
    "publish": "2011-02-05T23:54:04Z", 
    "summary": "Collaborative tagging has emerged as a popular and effective method for\norganizing and describing pages on the Web. We present Treelicious, a system\nthat allows hierarchical navigation of tagged web pages. Our system enriches\nthe navigational capabilities of standard tagging systems, which typically\nexploit only popularity and co-occurrence data. We describe a prototype that\nleverages the Wikipedia category structure to allow a user to semantically\nnavigate pages from the Delicious social bookmarking service. In our system a\nuser can perform an ordinary keyword search and browse relevant pages but is\nalso given the ability to broaden the search to more general topics and narrow\nit to more specific topics. We show that Treelicious indeed provides an\nintuitive framework that allows for improved and effective discovery of\nknowledge."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1102.1345v1", 
    "title": "Introducing a New Mechanism for Construction of an Efficient Search   Model", 
    "arxiv-id": "1102.1345v1", 
    "author": "Sukanta Sinha", 
    "publish": "2011-02-07T16:04:50Z", 
    "summary": "Search engine has become an inevitable tool for retrieving information from\nthe WWW. Web researchers introduce lots of algorithms to modify search engine\nbased on different features. Sometimes those algorithms are domain related,\nsometimes they are Web page ranking related, and sometimes they are efficiency\nrelated and so on. We are introducing such a type of algorithm which is\nmultiple domains as well as efficiency related. In this paper, we are providing\nmultilevel indexing on top of Index Based Acyclic Graph (IBAG) which support\nmultiple Ontologies as well as reduce search time. IBAG contains only domains\nrelated pages and are constructed from Relevant Page Graph (RPaG). We have also\nprovided a comparative study of time complexity for the various models."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1102.3866v1", 
    "title": "Treatment of Semantic Heterogeneity in Information Retrieval", 
    "arxiv-id": "1102.3866v1", 
    "author": "Robert Str\u00f6tgen", 
    "publish": "2011-02-18T16:11:51Z", 
    "summary": "The first step to handle semantic heterogeneity should be the attempt to\nenrich the semantic information about documents, i.e. to fill up the gaps in\nthe documents meta-data automatically. Section 2 describes a set of cascading\ndeductive and heuristic extraction rules, which were developed in the project\nCARMEN for the domain of Social Sciences. The mapping between different\nterminologies can be done by using intellectual, statistical and/or neural\nnetwork transfer modules. Intellectual transfers use cross-concordances between\ndifferent classification schemes or thesauri. Section 3 describes the creation,\nstorage and handling of such transfers."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1102.5458v1", 
    "title": "Improving Image Search based on User Created Communities", 
    "arxiv-id": "1102.5458v1", 
    "author": "Ahmed Hassan", 
    "publish": "2011-02-26T23:00:41Z", 
    "summary": "Tag-based retrieval of multimedia content is a difficult problem, not only\nbecause of the shorter length of tags associated with images and videos, but\nalso due to mismatch in the terminologies used by searcher and content creator.\nTo alleviate this problem, we propose a simple concept-driven probabilistic\nmodel for improving text-based rich-media search. While our approach is similar\nto existing topic-based retrieval and cluster-based language modeling work,\nthere are two important differences: (1) our proposed model considers not only\nthe query-generation likelihood from cluster, but explicitly accounts for the\noverall \"popularity\" of the cluster or underlying concept, and (2) we explore\nthe possibility of inferring the likely concept relevant to a rich-media\ncontent through the user-created communities that the content belongs to.\n  We implement two methods of concept extraction: a traditional cluster based\napproach, and the proposed community based approach. We evaluate these two\ntechniques for how effectively they capture the intended meaning of a term from\nthe content creator and searcher, and their overall value in improving image\nsearch. Our results show that concept-driven search, though simple, clearly\noutperforms plain search. Among the two techniques for concept-driven search,\ncommunity-based approach is more successful, as the concepts generated from\nuser communities are found to be more intuitive and appealing."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1103.2886v1", 
    "title": "Predicting User Preferences", 
    "arxiv-id": "1103.2886v1", 
    "author": "Pavel Sirotkin", 
    "publish": "2011-03-15T11:48:30Z", 
    "summary": "The many metrics employed for the evaluation of search engine results have\nnot themselves been conclusively evaluated. We propose a new measure for a\nmetric's ability to identify user preference of result lists. Using this\nmeasure, we evaluate the metrics Discounted Cumulated Gain, Mean Average\nPrecision and classical precision, finding that the former performs best. We\nalso show that considering more results for a given query can impair rather\nthan improve a metric's ability to predict user preferences."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1105.1406v1", 
    "title": "Comparison Latent Semantic and WordNet Approach for Semantic Similarity   Calculation", 
    "arxiv-id": "1105.1406v1", 
    "author": "Bambang Wahyudi", 
    "publish": "2011-05-07T01:28:09Z", 
    "summary": "Information exchange among many sources in Internet is more autonomous,\ndynamic and free. The situation drive difference view of concepts among\nsources. For example, word 'bank' has meaning as economic institution for\neconomy domain, but for ecology domain it will be defined as slope of river or\nlake. In this aper, we will evaluate latent semantic and WordNet approach to\ncalculate semantic similarity. The evaluation will be run for some concepts\nfrom different domain with reference by expert or human. Result of the\nevaluation can provide a contribution for mapping of concept, query rewriting,\ninteroperability, etc."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1105.4255v1", 
    "title": "Efficient Diversification of Web Search Results", 
    "arxiv-id": "1105.4255v1", 
    "author": "Fabrizio Silvestri", 
    "publish": "2011-05-21T12:09:46Z", 
    "summary": "In this paper we analyze the efficiency of various search results\ndiversification methods. While efficacy of diversification approaches has been\ndeeply investigated in the past, response time and scalability issues have been\nrarely addressed. A unified framework for studying performance and feasibility\nof result diversification solutions is thus proposed. First we define a new\nmethodology for detecting when, and how, query results need to be diversified.\nTo this purpose, we rely on the concept of \"query refinement\" to estimate the\nprobability of a query to be ambiguous. Then, relying on this novel ambiguity\ndetection method, we deploy and compare on a standard test set, three different\ndiversification methods: IASelect, xQuAD, and OptSelect. While the first two\nare recent state-of-the-art proposals, the latter is an original algorithm\nintroduced in this paper. We evaluate both the efficiency and the effectiveness\nof our approach against its competitors by using the standard TREC Web\ndiversification track testbed. Results shown that OptSelect is able to run two\norders of magnitude faster than the two other state-of-the-art approaches and\nto obtain comparable figures in diversification effectiveness."
},{
    "category": "cs.IR", 
    "doi": "10.1109/WI-IAT.2010.289", 
    "link": "http://arxiv.org/pdf/1105.4868v1", 
    "title": "Search for Hidden Knowledge in Collective Intelligence dealing   Indeterminacy Ontology of Folksonomy with Linguistic Pragmatics and Quantum   Logic", 
    "arxiv-id": "1105.4868v1", 
    "author": "Massimiliano Dal Mas", 
    "publish": "2011-05-24T19:47:03Z", 
    "summary": "Information retrieval is not only the most frequent application executed on\nthe Web but it is also the base of different types of applications. Considering\ncollective intelligence of groups of individuals as a framework for evaluating\nand incorporating new experiences and information we often cannot retrieve such\nknowledge being tacit. Tacit knowledge underlies many competitive capabilities\nand it is hard to articulate on discrete ontology structure. It is unstructured\nor unorganized, and therefore remains hidden. Developing generic solutions that\ncan find the hidden knowledge is extremely complex. Moreover this will be a\ngreat challenge for the developers of semantic technologies. This work aims to\nexplore ways to make explicit and available the tacit knowledge hidden in the\ncollective intelligence of a collaborative environment within organizations.\nThe environment was defined by folksonomies supported by a faceted semantic\nsearch. Vector space model which incorporates an analogy with the mathematical\napparatus of quantum theory is adopted for the representation and manipulation\nof the meaning of folksonomy. Vector space retrieval has been proven efficiency\nwhen there isn't a data behavioural because it bears ranking algorithms\ninvolving a small number of types of elements and few operations. A solution to\nfind what the user has in mind when posing a query could be based on \"joint\nmeaning\" understood as a joint construal of the creator of the contents and the\nreader of the contents. The joint meaning was proposed to deal with vagueness\non ontology of folksonomy indeterminacy, incompleteness and inconsistencies on\ncollective intelligence. A proof-of concept prototype was built for\ncollaborative environment as evolution of the actual social networks (like\nFacebook, LinkedIn,..) using the information visualization on a RIA application\nwith Semantic Web techniques and technologies."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3202", 
    "link": "http://arxiv.org/pdf/1105.6213v1", 
    "title": "Using Context to Improve the Evaluation of Information Retrieval Systems", 
    "arxiv-id": "1105.6213v1", 
    "author": "Bich-Lien Doan", 
    "publish": "2011-05-31T09:19:16Z", 
    "summary": "The crucial role of the evaluation in the development of the information\nretrieval tools is useful evidence to improve the performance of these tools\nand the quality of results that they return. However, the classic evaluation\napproaches have limitations and shortcomings especially regarding to the user\nconsideration, the measure of the adequacy between the query and the returned\ndocuments and the consideration of characteristics, specifications and\nbehaviors of the search tool. Therefore, we believe that the exploitation of\ncontextual elements could be a very good way to evaluate the search tools. So,\nthis paper presents a new approach that takes into account the context during\nthe evaluation process at three complementary levels. The experiments gives at\nthe end of this article has shown the applicability of the proposed approach to\nreal research tools. The tests were performed with the most popular searching\nengine (i.e. Google, Bing and Yahoo) selected in particular for their high\nselectivity. The obtained results revealed that the ability of these engines to\nrejecting dead links, redundant results and parasites pages depends strongly to\nhow queries are formulated, and to the political of sites offering this\ninformation to present their content. The relevance evaluation of results\nprovided by these engines, using the user's judgments, then using an automatic\nmanner to take into account the query context has also shown a general decline\nin the perceived relevance according to the number of the considered results."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.739", 
    "link": "http://arxiv.org/pdf/1106.0248v1", 
    "title": "Technical Paper Recommendation: A Study in Combining Multiple   Information Sources", 
    "arxiv-id": "1106.0248v1", 
    "author": "C. Nevill-Manning", 
    "publish": "2011-06-01T16:39:29Z", 
    "summary": "The growing need to manage and exploit the proliferation of online data\nsources is opening up new opportunities for bringing people closer to the\nresources they need. For instance, consider a recommendation service through\nwhich researchers can receive daily pointers to journal papers in their fields\nof interest. We survey some of the known approaches to the problem of technical\npaper recommendation and ask how they can be extended to deal with multiple\ninformation sources. More specifically, we focus on a variant of this problem -\nrecommending conference paper submissions to reviewing committee members -\nwhich offers us a testbed to try different approaches. Using WHIRL - an\ninformation integration system - we are able to implement different\nrecommendation algorithms derived from information retrieval principles. We\nalso use a novel autonomous procedure for gathering reviewer interest\ninformation from the Web. We evaluate our approach and compare it to other\nmethods using preference data provided by members of the AAAI-98 conference\nreviewing committee along with data about the actual submissions."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.739", 
    "link": "http://arxiv.org/pdf/1106.1521v3", 
    "title": "A Linear-Time Approximation of the Earth Mover's Distance", 
    "arxiv-id": "1106.1521v3", 
    "author": "Sunju Park", 
    "publish": "2011-06-08T08:53:21Z", 
    "summary": "Color descriptors are one of the important features used in content-based\nimage retrieval. The Dominant Color Descriptor (DCD) represents a few\nperceptually dominant colors in an image through color quantization. For image\nretrieval based on DCD, the earth mover's distance and the optimal color\ncomposition distance are proposed to measure the dissimilarity between two\nimages. Although providing good retrieval results, both methods are too\ntime-consuming to be used in a large image database. To solve the problem, we\npropose a new distance function that calculates an approximate earth mover's\ndistance in linear time. To calculate the dissimilarity in linear time, the\nproposed approach employs the space-filling curve for multidimensional color\nspace. To improve the accuracy, the proposed approach uses multiple curves and\nadjusts the color positions. As a result, our approach achieves\norder-of-magnitude time improvement but incurs small errors. We have performed\nextensive experiments to show the effectiveness and efficiency of the proposed\napproach. The results reveal that our approach achieves almost the same results\nwith the EMD in linear time."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1106.2289v1", 
    "title": "PRESY: A Context Based Query Reformulation Tool for Information   Retrieval on the Web", 
    "arxiv-id": "1106.2289v1", 
    "author": "Bich-Lien Doan", 
    "publish": "2011-06-12T08:41:40Z", 
    "summary": "Problem Statement: The huge number of information on the web as well as the\ngrowth of new inexperienced users creates new challenges for information\nretrieval. It has become increasingly difficult for these users to find\nrelevant documents that satisfy their individual needs. Certainly the current\nsearch engines (such as Google, Bing and Yahoo) offer an efficient way to\nbrowse the web content. However, the result quality is highly based on uses\nqueries which need to be more precise to find relevant documents. This task\nstill complicated for the majority of inept users who cannot express their\nneeds with significant words in the query. For that reason, we believe that a\nreformulation of the initial user's query can be a good alternative to improve\nthe information selectivity. This study proposes a novel approach and presents\na prototype system called PRESY (Profile-based REformulation SYstem) for\ninformation retrieval on the web. Approach: It uses an incremental approach to\ncategorize users by constructing a contextual base. The latter is composed of\ntwo types of context (static and dynamic) obtained using the users' profiles.\nThe architecture proposed was implemented using .Net environment to perform\nqueries reformulating tests. Results: The experiments gives at the end of this\narticle show that the precision of the returned content is effectively\nimproved. The tests were performed with the most popular searching engine (i.e.\nGoogle, Bind and Yahoo) selected in particular for their high selectivity.\nAmong the given results, we found that query reformulation improve the first\nthree results by 10.7% and 11.7% of the next seven returned elements. So as we\ncan see the reformulation of users' initial queries improves the pertinence of\nreturned content."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1106.2946v6", 
    "title": "A Unified Relevance Retrieval Model by Eliteness Hypothesis", 
    "arxiv-id": "1106.2946v6", 
    "author": "Jun Wang", 
    "publish": "2011-06-15T11:50:31Z", 
    "summary": "In this paper, an Eliteness Hypothesis for information retrieval is proposed,\nwhere we define two generative processes to create information items and\nqueries. By assuming the deterministic relationships between the eliteness of\nterms and relevance, we obtain a new theoretical retrieval framework. The\nresulting ranking function is a unified one as it is capable of using available\nrelevance information on both the document and the query, which is otherwise\nunachievable by existing retrieval models. Our preliminary experiment on a\nsimple ranking function has demonstrated the potential of the approach."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1106.5213v1", 
    "title": "Personalised Travel Recommendation based on Location Co-occurrence", 
    "arxiv-id": "1106.5213v1", 
    "author": "Marcel J. T. Reinders", 
    "publish": "2011-06-26T11:55:23Z", 
    "summary": "We propose a new task of recommending touristic locations based on a user's\nvisiting history in a geographically remote region. This can be used to plan a\ntouristic visit to a new city or country, or by travel agencies to provide\npersonalised travel deals.\n  A set of geotags is used to compute a location similarity model between two\ndifferent regions. The similarity between two landmarks is derived from the\nnumber of users that have visited both places, using a Gaussian density\nestimation of the co-occurrence space of location visits to cluster related\ngeotags. The standard deviation of the kernel can be used as a scale parameter\nthat determines the size of the recommended landmarks.\n  A personalised recommendation based on the location similarity model is\nevaluated on city and country scale and is able to outperform a location\nranking based on popularity. Especially when a tourist filter based on visit\nduration is enforced, the prediction can be accurately adapted to the\npreference of the user. An extensive evaluation based on manual annotations\nshows that more strict ranking methods like cosine similarity and a proposed\nRankDiff algorithm provide more serendipitous recommendations and are able to\nlink similar locations on opposite sides of the world."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1107.2727v1", 
    "title": "Proposed Quality Evaluation Framework to Incorporate Quality Aspects in   Web Warehouse Creation", 
    "arxiv-id": "1107.2727v1", 
    "author": "Yasir Mehmood", 
    "publish": "2011-07-14T05:22:38Z", 
    "summary": "Web Warehouse is a read only repository maintained on the web to effectively\nhandle the relevant data. Web warehouse is a system comprised of various\nsubsystems and process. It supports the organizations in decision making.\nQuality of data store in web warehouse can affect the quality of decision made.\nFor a valuable decision making it is required to consider the quality aspects\nin designing and modelling of a web warehouse. Thus data quality is one of the\nmost important issues of the web warehousing system. Quality must be\nincorporated at different stages of the web warehousing system development. It\nis necessary to enhance existing data warehousing system to increase the data\nquality. It results in the storage of high quality data in the repository and\nefficient decision making. In this paper a Quality Evaluation Framework is\nproposed keeping in view the quality dimensions associated with different\nphases of a web warehouse. Further more, the proposed framework is validated\nempirically with the help of quantitative analysis."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1107.3360v1", 
    "title": "Object Oriented Information Computing over WWW", 
    "arxiv-id": "1107.3360v1", 
    "author": "Harmunish Taneja", 
    "publish": "2011-07-18T06:05:12Z", 
    "summary": "Traditional search engines on World Wide Web (WWW) focus essentially on\nrelevance ranking at the page level. But this lead to missing innumerable\nstructured information about real-world objects embedded in static Web pages\nand online Web databases. Page-level information retrieval (IR) can\nunfortunately lead to highly inaccurate relevance ranking in answering\nobject-oriented queries. On the other hand, Object Oriented Information\nComputing (OOIC) is promising and greatly reduces the complexity of the system\nwhile improving reusability and manageability. The most distinguishing\nrequirement of today's complex heterogeneous systems is the need of the\ncomputing system to instantly adapt to vigorously changing conditions. OOIC\nallows reflecting the dynamic characteristics of the applications by\ninstantiating objects dynamically. In this paper, major challenges of OOIC as\nwell as its rudiments are recapped. The review includes the insight to PopRank\nModel and comparison analysis of conventional page rank based IR with OOIC"
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1107.5661v1", 
    "title": "On the Impact of Random Index-Partitioning on Index Compression", 
    "arxiv-id": "1107.5661v1", 
    "author": "K. Vornovitsky", 
    "publish": "2011-07-28T09:37:44Z", 
    "summary": "The performance of processing search queries depends heavily on the stored\nindex size. Accordingly, considerable research efforts have been devoted to the\ndevelopment of efficient compression techniques for inverted indexes. Roughly,\nindex compression relies on two factors: the ordering of the indexed documents,\nwhich strives to position similar documents in proximity, and the encoding of\nthe inverted lists that result from the ordered stream of documents. Large\ncommercial search engines index tens of billions of pages of the ever growing\nWeb. The sheer size of their indexes dictates the distribution of documents\namong thousands of servers in a scheme called local index-partitioning, such\nthat each server indexes only several millions pages. Due to engineering and\nruntime performance considerations, random distribution of documents to servers\nis common. However, random index-partitioning among many servers adversely\nimpacts the resulting index sizes, as it decreases the effectiveness of\ndocument ordering schemes. We study the impact of random index-partitioning on\ndocument ordering schemes. We show that index-partitioning decreases the\naggregated size of the inverted lists logarithmically with the number of\nservers, when documents within each server are randomly reordered. On the other\nhand, the aggregated partitioned index size increases logarithmically with the\nnumber of servers, when state-of-the-art document ordering schemes, such as\nlexical URL sorting and clustering with TSP, are applied. Finally, we justify\nthe common practice of randomly distributing documents to servers, as we\nqualitatively show that despite its ill-effects on the ensuing compression, it\ndecreases key factors in distributed query evaluation time by an order of\nmagnitude as compared with partitioning techniques that compress better."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1108.1956v1", 
    "title": "Factorization-based Lossless Compression of Inverted Indices", 
    "arxiv-id": "1108.1956v1", 
    "author": "Vanja Josifovski", 
    "publish": "2011-08-09T15:25:17Z", 
    "summary": "Many large-scale Web applications that require ranked top-k retrieval such as\nWeb search and online advertising are implemented using inverted indices. An\ninverted index represents a sparse term-document matrix, where non-zero\nelements indicate the strength of term-document association. In this work, we\npresent an approach for lossless compression of inverted indices. Our approach\nmaps terms in a document corpus to a new term space in order to reduce the\nnumber of non-zero elements in the term-document matrix, resulting in a more\ncompact inverted index. We formulate the problem of selecting a new term space\nthat minimizes the resulting index size as a matrix factorization problem, and\nprove that finding the optimal factorization is an NP-hard problem. We develop\na greedy algorithm for finding an approximate solution. A side effect of our\napproach is increasing the number of terms in the index, which may negatively\naffect query evaluation performance. To eliminate such effect, we develop a\nmethodology for modifying query evaluation algorithms by exploiting specific\nproperties of our compression approach. Our experimental evaluation\ndemonstrates that our approach achieves an index size reduction of 20%, while\nmaintaining the same query response times. Higher compression ratios up to 35%\nare achievable, however at the cost of slightly longer query response times.\nFurthermore, combining our approach with other lossless compression techniques,\nnamely variable-byte encoding, leads to index size reduction of up to 50%."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1108.2685v1", 
    "title": "Efficient Query Rewrite for Structured Web Queries", 
    "arxiv-id": "1108.2685v1", 
    "author": "Stelios Paparizos", 
    "publish": "2011-08-12T18:37:11Z", 
    "summary": "Web search engines and specialized online verticals are increasingly\nincorporating results from structured data sources to answer semantically rich\nuser queries. For example, the query \\WebQuery{Samsung 50 inch led tv} can be\nanswered using information from a table of television data. However, the users\nare not domain experts and quite often enter values that do not match precisely\nthe underlying data. Samsung makes 46- or 55- inch led tvs, but not 50-inch\nones. So a literal execution of the above mentioned query will return zero\nresults. For optimal user experience, a search engine would prefer to return at\nleast a minimum number of results as close to the original query as possible.\nFurthermore, due to typical fast retrieval speeds in web-search, a search\nengine query execution is time-bound.\n  In this paper, we address these challenges by proposing algorithms that\nrewrite the user query in a principled manner, surfacing at least the required\nnumber of results while satisfying the low-latency constraint. We formalize\nthese requirements and introduce a general formulation of the problem. We show\nthat under a natural formulation, the problem is NP-Hard to solve optimally,\nand present approximation algorithms that produce good rewrites. We empirically\nvalidate our algorithms on large-scale data obtained from a commercial search\nengine's shopping vertical."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1108.2754v1", 
    "title": "Structured Learning of Two-Level Dynamic Rankings", 
    "arxiv-id": "1108.2754v1", 
    "author": "Pannaga Shivaswamy", 
    "publish": "2011-08-13T03:22:48Z", 
    "summary": "For ambiguous queries, conventional retrieval systems are bound by two\nconflicting goals. On the one hand, they should diversify and strive to present\nresults for as many query intents as possible. On the other hand, they should\nprovide depth for each intent by displaying more than a single result. Since\nboth diversity and depth cannot be achieved simultaneously in the conventional\nstatic retrieval model, we propose a new dynamic ranking approach. Dynamic\nranking models allow users to adapt the ranking through interaction, thus\novercoming the constraints of presenting a one-size-fits-all static ranking. In\nparticular, we propose a new two-level dynamic ranking model for presenting\nsearch results to the user. In this model, a user's interactions with the\nfirst-level ranking are used to infer this user's intent, so that second-level\nrankings can be inserted to provide more results relevant for this intent.\nUnlike for previous dynamic ranking models, we provide an algorithm to\nefficiently compute dynamic rankings with provable approximation guarantees for\na large family of performance measures. We also propose the first principled\nalgorithm for learning dynamic ranking functions from training data. In\naddition to the theoretical results, we provide empirical evidence\ndemonstrating the gains in retrieval quality that our method achieves over\nconventional approaches."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1108.5460v1", 
    "title": "Personalized Web Services for Web Information Extraction", 
    "arxiv-id": "1108.5460v1", 
    "author": "Mahammed Erradi", 
    "publish": "2011-08-27T15:49:28Z", 
    "summary": "The field of information extraction from the Web emerged with the growth of\nthe Web and the multiplication of online data sources. This paper is an\nanalysis of information extraction methods. It presents a service oriented\napproach for web information extraction considering both web data management\nand extraction services. Then we propose an SOA based architecture to enhance\nflexibility and on-the-fly modification of web extraction services. An\nimplementation of the proposed architecture is proposed on the middleware level\nof Java Enterprise Edition (JEE) servers."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1108.5703v1", 
    "title": "Web Pages Clustering: A New Approach", 
    "arxiv-id": "1108.5703v1", 
    "author": "Vinay Hegde", 
    "publish": "2011-08-26T07:02:35Z", 
    "summary": "The rapid growth of web has resulted in vast volume of information.\nInformation availability at a rapid speed to the user is vital. English\nlanguage (or any for that matter) has lot of ambiguity in the usage of words.\nSo there is no guarantee that a keyword based search engine will provide the\nrequired results. This paper introduces the use of dictionary (standardised) to\nobtain the context with which a keyword is used and in turn cluster the results\nbased on this context. These ideas can be merged with a metasearch engine to\nenhance the search efficiency."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1110.0289v1", 
    "title": "Repr\u00e9sentation de donn\u00e9es et m\u00e9tadonn\u00e9es dans une biblioth\u00e8que   virtuelle pour une ad\u00e9quation avec l'usager et les outils de glanage ou   moissonnage scientifique", 
    "arxiv-id": "1110.0289v1", 
    "author": "G\u00e9rald Kembellec", 
    "publish": "2011-10-03T08:07:11Z", 
    "summary": "The vehicles for official knowledge, as well as university libraries, suffer\nfrom an increasingly visible lack of interest. This is due to the advent of\nfully digital practices. By studying the psychological and cognitive models in\ninformation retrieval initiated in the 1980s, it is possible to use these\ntheories and apply them practically to the Information Retrieval System, taking\ninto account the requirements of virtual libraries. New metadata standards\nalong with modern tools that help managing references should help automating\nthe process of scientific research. We offer a practical implementation of the\ngiven theories to test them when they are applied to the information retrieval\nin computer sciences. This case under study will highlight good practices in\ngleaning and harvesting scientific literature."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1110.0704v1", 
    "title": "Hierarchical Composable Optimization of Web Pages", 
    "arxiv-id": "1110.0704v1", 
    "author": "Oren Somekh", 
    "publish": "2011-10-04T14:33:26Z", 
    "summary": "The process of creating modern Web media experiences is challenged by the\nneed to adapt the content and presentation choices to dynamic real-time\nfluctuations of user interest across multiple audiences. We introduce FAME - a\nFramework for Agile Media Experiences - which addresses this scalability\nproblem. FAME allows media creators to define abstract page models that are\nsubsequently transformed into real experiences through algorithmic\nexperimentation. FAME's page models are hierarchically composed of simple\nbuilding blocks, mirroring the structure of most Web pages. They are resolved\ninto concrete page instances by pluggable algorithms which optimize the pages\nfor specific business goals. Our framework allows retrieving dynamic content\nfrom multiple sources, defining the experimentation's degrees of freedom, and\nconstraining the algorithmic choices. It offers an effective separation of\nconcerns in the media creation process, enabling multiple stakeholders with\nprofoundly different skills to apply their crafts and perform their duties\nindependently, composing and reusing each other's work in modular ways."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1110.1112v1", 
    "title": "Modeling Perceived Relevance for Tail Queries without Click-Through Data", 
    "arxiv-id": "1110.1112v1", 
    "author": "Belle Tseng", 
    "publish": "2011-10-05T21:48:53Z", 
    "summary": "Click-through data has been used in various ways in Web search such as\nestimating relevance between documents and queries. Since only search snippets\nare perceived by users before issuing any clicks, the relevance induced by\nclicks are usually called \\emph{perceived relevance} which has proven to be\nquite useful for Web search. While there is plenty of click data for popular\nqueries, very little information is available for unpopular tail ones. These\ntail queries take a large portion of the search volume but search accuracy for\nthese queries is usually unsatisfactory due to data sparseness such as limited\nclick information. In this paper, we study the problem of modeling perceived\nrelevance for queries without click-through data. Instead of relying on users'\nclick data, we carefully design a set of snippet features and use them to\napproximately capture the perceived relevance. We study the effectiveness of\nthis set of snippet features in two settings: (1) predicting perceived\nrelevance and (2) enhancing search engine ranking. Experimental results show\nthat our proposed model is effective to predict the relative perceived\nrelevance of Web search results. Furthermore, our proposed snippet features are\neffective to improve search accuracy for longer tail queries without\nclick-through data."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1110.5722v1", 
    "title": "Annotation of Scientific Summaries for Information Retrieval", 
    "arxiv-id": "1110.5722v1", 
    "author": "Charton Eric", 
    "publish": "2011-10-26T07:26:22Z", 
    "summary": "We present a methodology combining surface NLP and Machine Learning\ntechniques for ranking asbtracts and generating summaries based on annotated\ncorpora. The corpora were annotated with meta-semantic tags indicating the\ncategory of information a sentence is bearing (objective, findings, newthing,\nhypothesis, conclusion, future work, related work). The annotated corpus is fed\ninto an automatic summarizer for query-oriented abstract ranking and multi-\nabstract summarization. To adapt the summarizer to these two tasks, two novel\nweighting functions were devised in order to take into account the distribution\nof the tags in the corpus. Results, although still preliminary, are encouraging\nus to pursue this line of work and find better ways of building IR systems that\ncan take into account semantic annotations in a corpus."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1111.1497v4", 
    "title": "An IR-based Evaluation Framework for Web Search Query Segmentation", 
    "arxiv-id": "1111.1497v4", 
    "author": "Srivatsan Laxman", 
    "publish": "2011-11-07T07:26:27Z", 
    "summary": "This paper presents the first evaluation framework for Web search query\nsegmentation based directly on IR performance. In the past, segmentation\nstrategies were mainly validated against manual annotations. Our work shows\nthat the goodness of a segmentation algorithm as judged through evaluation\nagainst a handful of human annotated segmentations hardly reflects its\neffectiveness in an IR-based setup. In fact, state-of the-art algorithms are\nshown to perform as good as, and sometimes even better than human annotations\n-- a fact masked by previous validations. The proposed framework also provides\nus an objective understanding of the gap between the present best and the best\npossible segmentation algorithm. We draw these conclusions based on an\nextensive evaluation of six segmentation strategies, including three most\nrecent algorithms, vis-a-vis segmentations from three human annotators. The\nevaluation framework also gives insights about which segments should be\nnecessarily detected by an algorithm for achieving the best retrieval results.\nThe meticulously constructed dataset used in our experiments has been made\npublic for use by the research community."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1111.6349v1", 
    "title": "XML Information Retrieval Systems: A Survey", 
    "arxiv-id": "1111.6349v1", 
    "author": "Awny Sayed", 
    "publish": "2011-11-28T05:45:43Z", 
    "summary": "The continuous growth in the XML information repositories has been matched by\nincreasing efforts in development of XML retrieval systems, in large parts\naiming at supporting content-oriented XML retrieval. These systems exploit the\navailable structural information, as market up in XML documents, in order to\nreturn documents components- the so called XML elements-instead of the\ncomplement documents in repose to the user query. In this paper, we provide an\noverview of the different XML information retrieval systems and classify them\naccording to their storage and query evaluation strategies."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1111.6640v1", 
    "title": "A Markov Random Field Topic Space Model for Document Retrieval", 
    "arxiv-id": "1111.6640v1", 
    "author": "Scott Hand", 
    "publish": "2011-11-28T22:33:10Z", 
    "summary": "This paper proposes a novel statistical approach to intelligent document\nretrieval. It seeks to offer a more structured and extensible mathematical\napproach to the term generalization done in the popular Latent Semantic\nAnalysis (LSA) approach to document indexing. A Markov Random Field (MRF) is\npresented that captures relationships between terms and documents as\nprobabilistic dependence assumptions between random variables. From there, it\nuses the MRF-Gibbs equivalence to derive joint probabilities as well as local\nprobabilities for document variables. A parameter learning method is proposed\nthat utilizes rank reduction with singular value decomposition in a matter\nsimilar to LSA to reduce dimensionality of document-term relationships to that\nof a latent topic space. Experimental results confirm the ability of this\napproach to effectively and efficiently retrieve documents from substantial\ndata sets."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.0052v2", 
    "title": "Query Optimization Using Genetic Algorithms in the Vector Space Model", 
    "arxiv-id": "1112.0052v2", 
    "author": "Mohammad Othman Nassar", 
    "publish": "2011-11-30T23:14:20Z", 
    "summary": "In information retrieval research; Genetic Algorithms (GA) can be used to\nfind global solutions in many difficult problems. This study used different\nsimilarity measures (Dice, Inner Product) in the VSM, for each similarity\nmeasure we compared ten different GA approaches based on different fitness\nfunctions, different mutations and different crossover strategies to find the\nbest strategy and fitness function that can be used when the data collection is\nthe Arabic language. Our results shows that the GA approach which uses\none-point crossover operator, point mutation and Inner Product similarity as a\nfitness function is the best IR system in VSM."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.0054v2", 
    "title": "Improving the User Query for the Boolean Model Using Genetic Algorithms", 
    "arxiv-id": "1112.0054v2", 
    "author": "Eman Al Mashagba", 
    "publish": "2011-11-30T23:20:06Z", 
    "summary": "The Use of genetic algorithms in the Information retrieval (IR) area,\nespecially in optimizing a user query in Arabic data collections is presented\nin this paper. Very little research has been carried out on Arabic text\ncollections. Boolean model have been used in this research. To optimize the\nquery using GA we used different fitness functions, different mutation\nstrategies to find which is the best strategy and fitness function that can be\nused with Boolean model when the data collection is the Arabic language. Our\nresults show that the best GA strategy for the Boolean model is the GA (M2,\nPrecision) method."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.2015v1", 
    "title": "A Framework for Picture Extraction on Search Engine Improved and   Meaningful Result", 
    "arxiv-id": "1112.2015v1", 
    "author": "Anamika Sharma", 
    "publish": "2011-12-09T04:26:47Z", 
    "summary": "Searching is an important tool of information gathering, if information is in\nthe form of picture than it play a major role to take quick action and easy to\nmemorize. This is a human tendency to retain more picture than text. The\ncomplexity and the occurrence of variety of query can give variation in result\nand provide the humans to learn something new or get confused. This paper\npresents a development of a framework that will focus on recourse\nidentification for the user so that they can get faster access with accurate &\nconcise results on time and analysis of the change that is evident as the\nscenario changes from text to picture retrieval. This paper also provides a\nglimpse how to get accurate picture information in advance and extended\ntechnologies searching framework. The new challenges and design techniques of\npicture retrieval systems are also suggested in this paper."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.2028v1", 
    "title": "Document Classification Using Expectation Maximization with Semi   Supervised Learning", 
    "arxiv-id": "1112.2028v1", 
    "author": "Swati Vamney", 
    "publish": "2011-12-09T07:09:21Z", 
    "summary": "As the amount of online document increases, the demand for document\nclassification to aid the analysis and management of document is increasing.\nText is cheap, but information, in the form of knowing what classes a document\nbelongs to, is expensive. The main purpose of this paper is to explain the\nexpectation maximization technique of data mining to classify the document and\nto learn how to improve the accuracy while using semi-supervised approach.\nExpectation maximization algorithm is applied with both supervised and\nsemi-supervised approach. It is found that semi-supervised approach is more\naccurate and effective. The main advantage of semi supervised approach is\n\"Dynamically Generation of New Class\". The algorithm first trains a classifier\nusing the labeled document and probabilistically classifies the unlabeled\ndocuments. The car dataset for the evaluation purpose is collected from UCI\nrepository dataset in which some changes have been done from our side."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.2031v1", 
    "title": "Learning Context for Text Categorization", 
    "arxiv-id": "1112.2031v1", 
    "author": "Dr. Parag Kulkarni", 
    "publish": "2011-12-09T07:24:13Z", 
    "summary": "This paper describes our work which is based on discovering context for text\ndocument categorization. The document categorization approach is derived from a\ncombination of a learning paradigm known as relation extraction and an\ntechnique known as context discovery. We demonstrate the effectiveness of our\ncategorization approach using reuters 21578 dataset and synthetic real world\ndata from sports domain. Our experimental results indicate that the learned\ncontext greatly improves the categorization performance as compared to\ntraditional categorization approaches."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.2071v1", 
    "title": "Thematic Analysis and Visualization of Textual Corpus", 
    "arxiv-id": "1112.2071v1", 
    "author": "Mohamed Ben Ahmed", 
    "publish": "2011-12-09T11:04:32Z", 
    "summary": "The semantic analysis of documents is a domain of intense research at\npresent. The works in this domain can take several directions and touch several\nlevels of granularity. In the present work we are exactly interested in the\nthematic analysis of the textual documents. In our approach, we suggest\nstudying the variation of the theme relevance within a text to identify the\nmajor theme and all the minor themes evoked in the text. This allows us at the\nsecond level of analysis to identify the relations of thematic associations in\na textual corpus. Through the identification and the analysis of these\nassociation relations we suggest generating thematic paths allowing users,\nwithin the frame work of information search system, to explore the corpus\naccording to their themes of interest and to discover new knowledge by\nnavigating in the thematic association relations."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.2807v2", 
    "title": "Design and Implementation of a Simple Web Search Engine", 
    "arxiv-id": "1112.2807v2", 
    "author": "Andri Mirzal", 
    "publish": "2011-12-13T06:46:26Z", 
    "summary": "We present a simple web search engine for indexing and searching html\ndocuments using python programming language. Because python is well known for\nits simple syntax and strong support for main operating systems, we hope it\nwill be beneficial for learning information retrieval techniques, especially\nweb search engine technology."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1112.5355v1", 
    "title": "2P-Med: Building a Personalization Platform for Mediation Systems", 
    "arxiv-id": "1112.5355v1", 
    "author": "Laila Benhlima", 
    "publish": "2011-12-22T15:38:17Z", 
    "summary": "Nowadays, with the increasing number of integrated data sources, there is a\nreal trend to personalize mediation systems to improve user satisfaction. To\nmake these systems user sensitive, we propose a personalization platform called\n2P-Med. 2P-Med allows personalizing any mediation system used in any domain\nfollowing a cyclic process. The process includes building and managing adequate\nuser profiles and sources profiles, content and quality matching, source\nselection, adapting the mediator responses to user preferences and handling\nuser feedbacks. In this paper, we describe 2P-Med architecture and highlight\nits main functionalities. We also illustrate the operation of the platform\nthrough personalizing source selection in a travel planning assistant."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1201.0274v1", 
    "title": "Overview of EIREX 2010: Computing", 
    "arxiv-id": "1201.0274v1", 
    "author": "Jorge Morato", 
    "publish": "2011-12-31T15:40:05Z", 
    "summary": "The first Information Retrieval Education through Experimentation track\n(EIREX 2010) was run at the University Carlos III of Madrid, during the 2010\nspring semester. EIREX 2010 is the first in a series of experiments designed to\nfoster new Information Retrieval (IR) education methodologies and resources,\nwith the specific goal of teaching undergraduate IR courses from an\nexperimental perspective. For an introduction to the motivation behind the\nEIREX experiments, see the first sections of [Urbano et al., 2011]. For\ninformation on other editions of EIREX and related data, see the website at\nhttp://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)\nto help students get a view of the Information Retrieval process as they would\nfind it in a real-world scenario, either industrial or academic; b) to make\nstudents realize the importance of laboratory experiments in Computer Science\nand have them initiated in their execution and analysis; c) to create a public\nrepository of resources to teach Information Retrieval courses; d) to seek the\ncollaboration and active participation of other Universities in this endeavor.\nThis overview paper summarizes the results of the EIREX 2010 track, focusing on\nthe creation of the test collection and the analysis to assess its reliability."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1201.2304v1", 
    "title": "Query sensitive comparative summarization of search results using   concept based segmentation", 
    "arxiv-id": "1201.2304v1", 
    "author": "K. Sarukesi", 
    "publish": "2012-01-11T14:18:02Z", 
    "summary": "Query sensitive summarization aims at providing the users with the summary of\nthe contents of single or multiple web pages based on the search query. This\npaper proposes a novel idea of generating a comparative summary from a set of\nURLs from the search result. User selects a set of web page links from the\nsearch result produced by search engine. Comparative summary of these selected\nweb sites is generated. This method makes use of HTML DOM tree structure of\nthese web pages. HTML documents are segmented into set of concept blocks.\nSentence score of each concept block is computed with respect to the query and\nfeature keywords. The important sentences from the concept blocks of different\nweb pages are extracted to compose the comparative summary on the fly. This\nsystem reduces the time and effort required for the user to browse various web\nsites to compare the information. The comparative summary of the contents would\nhelp the users in quick decision making."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1201.3417v1", 
    "title": "Mining Educational Data to Analyze Students' Performance", 
    "arxiv-id": "1201.3417v1", 
    "author": "Saurabh Pal", 
    "publish": "2012-01-17T03:34:50Z", 
    "summary": "The main objective of higher education institutions is to provide quality\neducation to its students. One way to achieve highest level of quality in\nhigher education system is by discovering knowledge for prediction regarding\nenrolment of students in a particular course, alienation of traditional\nclassroom teaching model, detection of unfair means used in online examination,\ndetection of abnormal values in the result sheets of the students, prediction\nabout students' performance and so on. The knowledge is hidden among the\neducational data set and it is extractable through data mining techniques.\nPresent paper is designed to justify the capabilities of data mining techniques\nin context of higher education by offering a data mining model for higher\neducation system in the university. In this research, the classification task\nis used to evaluate student's performance and as there are many approaches that\nare used for data classification, the decision tree method is used here. By\nthis task we extract knowledge that describes students' performance in end\nsemester examination. It helps earlier in identifying the dropouts and students\nwho need special attention and allow the teacher to provide appropriate\nadvising/counseling. Keywords-Educational Data Mining (EDM); Classification;\nKnowledge Discovery in Database (KDD); ID3 Algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1201.3418v1", 
    "title": "Data Mining: A prediction for performance improvement using   classification", 
    "arxiv-id": "1201.3418v1", 
    "author": "Saurabh Pal", 
    "publish": "2012-01-17T03:42:49Z", 
    "summary": "Now-a-days the amount of data stored in educational database increasing\nrapidly. These databases contain hidden information for improvement of\nstudents' performance. The performance in higher education in India is a\nturning point in the academics for all students. This academic performance is\ninfluenced by many factors, therefore it is essential to develop predictive\ndata mining model for students' performance so as to identify the difference\nbetween high learners and slow learners student. In the present investigation,\nan experimental methodology was adopted to generate a database. The raw data\nwas preprocessed in terms of filling up missing values, transforming values in\none form into another and relevant attribute/ variable selection. As a result,\nwe had 300 student records, which were used for by Byes classification\nprediction model construction. Keywords- Data Mining, Educational Data Mining,\nPredictive Model, Classification."
},{
    "category": "cs.IR", 
    "doi": "10.3844/jcssp.2010.470.477", 
    "link": "http://arxiv.org/pdf/1201.5182v1", 
    "title": "Data Mining as a Torch Bearer in Education Sector", 
    "arxiv-id": "1201.5182v1", 
    "author": "Saurabh pal", 
    "publish": "2012-01-25T04:50:58Z", 
    "summary": "Every data has a lot of hidden information. The processing method of data\ndecides what type of information data produce. In India education sector has a\nlot of data that can produce valuable information. This information can be used\nto increase the quality of education. But educational institution does not use\nany knowledge discovery process approach on these data. Information and\ncommunication technology puts its leg into the education sector to capture and\ncompile low cost information. Now a day a new research community, educational\ndata mining (EDM), is growing which is intersection of data mining and\npedagogy. In this paper we present roadmap of research done in EDM in various\nsegment of education sector."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.1881v1", 
    "title": "A personalized web page content filtering model based on segmentation", 
    "arxiv-id": "1202.1881v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-09T03:49:03Z", 
    "summary": "In the view of massive content explosion in World Wide Web through diverse\nsources, it has become mandatory to have content filtering tools. The filtering\nof contents of the web pages holds greater significance in cases of access by\nminor-age people. The traditional web page blocking systems goes by the Boolean\nmethodology of either displaying the full page or blocking it completely. With\nthe increased dynamism in the web pages, it has become a common phenomenon that\ndifferent portions of the web page holds different types of content at\ndifferent time instances. This paper proposes a model to block the contents at\na fine-grained level i.e. instead of completely blocking the page it would be\nefficient to block only those segments which holds the contents to be blocked.\nThe advantages of this method over the traditional methods are fine-graining\nlevel of blocking and automatic identification of portions of the page to be\nblocked. The experiments conducted on the proposed model indicate 88% of\naccuracy in filtering out the segments."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2187v1", 
    "title": "Museum: Multidimensional web page segment evaluation model", 
    "arxiv-id": "1202.2187v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-10T04:49:58Z", 
    "summary": "The evaluation of a web page with respect to a query is a vital task in the\nweb information retrieval domain. This paper proposes the evaluation of a web\npage as a bottom-up process from the segment level to the page level. A model\nfor evaluating the relevancy is proposed incorporating six different\ndimensions. An algorithm for evaluating the segments of a web page, using the\nabove mentioned six dimensions is proposed. The benefits of fine-granining the\nevaluation process to the segment level instead of the page level are explored.\nThe proposed model can be incorporated for various tasks like web page\npersonalization, result re-ranking, mobile device page rendering etc."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2614v1", 
    "title": "Semantic snippet construction for search engine results based on segment   evaluation", 
    "arxiv-id": "1202.2614v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-13T03:57:12Z", 
    "summary": "The result listing from search engines includes a link and a snippet from the\nweb page for each result item. The snippet in the result listing plays a vital\nrole in assisting the user to click on it. This paper proposes a novel approach\nto construct the snippets based on a semantic evaluation of the segments in the\npage. The target segment(s) is/are identified by applying a model to evaluate\nsegments present in the page and selecting the segments with top scores. The\nproposed model makes the user judgment to click on a result item easier since\nthe snippet is constructed semantically after a critical evaluation based on\nmultiple factors. A prototype implementation of the proposed model confirms the\nempirical validation."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2615v1", 
    "title": "Live-marker: A personalized web page content marking tool", 
    "arxiv-id": "1202.2615v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-13T04:02:40Z", 
    "summary": "The tremendous amount of increase in the quantity of information resources\navailable on the web has made the total time that the user spends on a single\npage very minimal. Users revisiting the same page would be able to fetch the\nrequired information much faster if the information that they consumed during\nthe previous visit(s) gets presented to them with a special style. This paper\nproposes a model which empowers the users to mark the content interesting to\nthem, so that it can be identified easily during successive visits. In addition\nto the explicit marking by the users, the model facilitates implicit marking\nbased on the user preferences. The prototype implementation based on proposed\nmodel validates the model's efficiency."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2617v1", 
    "title": "Segmentation Based Approach to Dynamic Page Construction from Search   Engine Results", 
    "arxiv-id": "1202.2617v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-13T04:13:56Z", 
    "summary": "The results rendered by the search engines are mostly a linear snippet list.\nWith the prolific increase in the dynamism of web pages there is a need for\nenhanced result lists from search engines in order to cope-up with the\nexpectations of the users. This paper proposes a model for dynamic construction\nof a resultant page from various results fetched by the search engine, based on\nthe web page segmentation approach. With the incorporation of personalization\nthrough user profile during the candidate segment selection, the enriched\nresultant page is constructed. The benefits of this approach include instant,\none-shot navigation to relevant portions from various result items, in contrast\nto a linear page-by-page visit approach. The experiments conducted on the\nprototype model with various levels of users, quantifies the improvements in\nterms of amount of relevant information fetched."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2619v1", 
    "title": "We.I.Pe: Web Identification of People using e-mail ID", 
    "arxiv-id": "1202.2619v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-13T04:17:30Z", 
    "summary": "With the phenomenal growth of content in the World Wide Web, the diversity of\nuser supplied queries have become vivid. Searching for people on the web has\nbecome an important type of search activity in the web search engines. This\npaper proposes a model named \"We.I.Pe\" to identify people on the World Wide Web\nusing e-mail Id as the primary input. The approach followed in this research\nwork provides the collected information, based on the user supplied e-mail id,\nin an easier to navigate manner. The grouping of collected information based on\nvarious sources makes the result visualization process more effective. The\nproposed model is validated by a prototype implementation. Experiments\nconducted on the prototype implementation provide encouraging results"
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2622v1", 
    "title": "A Model for Web Page Usage Mining Based on Segmentation", 
    "arxiv-id": "1202.2622v1", 
    "author": "G. Aghila", 
    "publish": "2012-02-13T04:21:15Z", 
    "summary": "The web page usage mining plays a vital role in enriching the page's content\nand structure based on the feedbacks received from the user's interactions with\nthe page. This paper proposes a model for micro-managing the tracking\nactivities by fine-tuning the mining from the page level to the segment level.\nThe proposed model enables the web-master to identify the segments which\nreceives more focus from users comparing with others. The segment level\nanalytics of user actions provides an important metric to analyse the factors\nwhich facilitate the increase in traffic for the page. The empirical validation\nof the model is performed through prototype implementation."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.2880v4", 
    "title": "Approximate Recall Confidence Intervals", 
    "arxiv-id": "1202.2880v4", 
    "author": "William Webber", 
    "publish": "2012-02-13T21:54:50Z", 
    "summary": "Recall, the proportion of relevant documents retrieved, is an important\nmeasure of effectiveness in information retrieval, particularly in the legal,\npatent, and medical domains. Where document sets are too large for exhaustive\nrelevance assessment, recall can be estimated by assessing a random sample of\ndocuments; but an indication of the reliability of this estimate is also\nrequired. In this article, we examine several methods for estimating two-tailed\nrecall confidence intervals. We find that the normal approximation in current\nuse provides poor coverage in many circumstances, even when adjusted to correct\nits inappropriate symmetry. Analytic and Bayesian methods based on the ratio of\nbinomials are generally more accurate, but are inaccurate on small populations.\nThe method we recommend derives beta-binomial posteriors on retrieved and\nunretrieved yield, with fixed hyperparameters, and a Monte Carlo estimate of\nthe posterior distribution of recall. We demonstrate that this method gives\nmean coverage at or near the nominal level, across several scenarios, while\nbeing balanced and stable. We offer advice on sampling design, including the\nallocation of assessments to the retrieved and unretrieved segments, and\ncompare the proposed beta-binomial with the officially reported normal\nintervals for recent TREC Legal Track iterations."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1202.6685v1", 
    "title": "Faceted Semantic Search for Personalized Social Search", 
    "arxiv-id": "1202.6685v1", 
    "author": "Massimiliano Dal Mas", 
    "publish": "2012-02-29T20:59:55Z", 
    "summary": "Actual social networks (like Facebook, Twitter, Linkedin, ...) need to deal\nwith vagueness on ontological indeterminacy. In this paper is analyzed the\nprototyping of a faceted semantic search for personalized social search using\nthe \"joint meaning\" in a community environment. User researches in a\n\"collaborative\" environment defined by folksonomies can be supported by the\nmost common features on the faceted semantic search. A solution for the\ncontext-aware personalized search is based on \"joint meaning\" understood as a\njoint construal of the creators of the contents and the user of the contents\nusing the faced taxonomy with the Semantic Web. A proof-of concept prototype\nshows how the proposed methodological approach can also be applied to existing\npresentation components, built with different languages and/or component\ntechnologies."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.0518v1", 
    "title": "Overview of EIREX 2011: Crowdsourcing", 
    "arxiv-id": "1203.0518v1", 
    "author": "Jorge Morato", 
    "publish": "2012-03-02T16:47:48Z", 
    "summary": "The second Information Retrieval Education through EXperimentation track\n(EIREX 2011) was run at the University Carlos III of Madrid, during the 2011\nspring semester. EIREX 2011 is the second in a series of experiments designed\nto foster new Information Retrieval (IR) education methodologies and resources,\nwith the specific goal of teaching undergraduate IR courses from an\nexperimental perspective. For an introduction to the motivation behind the\nEIREX experiments, see the first sections of [Urbano et al., 2011a]. For\ninformation on other editions of EIREX and related data, see the website at\nhttp://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)\nto help students get a view of the Information Retrieval process as they would\nfind it in a real-world scenario, either industrial or academic; b) to make\nstudents realize the importance of laboratory experiments in Computer Science\nand have them initiated in their execution and analysis; c) to create a public\nrepository of resources to teach Information Retrieval courses; d) to seek the\ncollaboration and active participation of other Universities in this endeavor.\nThis overview paper summarizes the results of the EIREX 2011 track, focusing on\nthe creation of the test collection and the analysis to assess its reliability."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.0747v2", 
    "title": "A review of EO image information mining", 
    "arxiv-id": "1203.0747v2", 
    "author": "Igor G. Olaizola", 
    "publish": "2012-03-04T16:33:41Z", 
    "summary": "We analyze the state of the art of content-based retrieval in Earth\nobservation image archives focusing on complete systems showing promise for\noperational implementation. The different paradigms at the basis of the main\nsystem families are introduced. The approaches taken are analyzed, focusing in\nparticular on the phases after primitive feature extraction. The solutions\nenvisaged for the issues related to feature simplification and synthesis,\nindexing, semantic labeling are reviewed. The methodologies for query\nspecification and execution are analyzed."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.2021v1", 
    "title": "A new supervised non-linear mapping", 
    "arxiv-id": "1203.2021v1", 
    "author": "Michael Aupetit", 
    "publish": "2012-03-09T09:15:43Z", 
    "summary": "Supervised mapping methods project multi-dimensional labeled data onto a\n2-dimensional space attempting to preserve both data similarities and topology\nof classes. Supervised mappings are expected to help the user to understand the\nunderlying original class structure and to classify new data visually. Several\nmethods have been designed to achieve supervised mapping, but many of them\nmodify original distances prior to the mapping so that original data\nsimilarities are corrupted and even overlapping classes tend to be separated\nonto the map ignoring their original topology. We propose ClassiMap, an\nalternative method for supervised mapping. Mappings come with distortions which\ncan be split between tears (close points mapped far apart) and false\nneighborhoods (points far apart mapped as neighbors). Some mapping methods\nfavor the former while others favor the latter. ClassiMap switches between such\nmapping methods so that tears tend to appear between classes and false\nneighborhood within classes, better preserving classes' topology. We also\npropose two new objective criteria instead of the usual subjective visual\ninspection to perform fair comparisons of supervised mapping methods. ClassiMap\nappears to be the best supervised mapping method according to these criteria in\nour experiments on synthetic and real datasets."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.2569v1", 
    "title": "When Index Term Probability Violates the Classical Probability Axioms   Quantum Probability can be a Necessary Theory for Information Retrieval", 
    "arxiv-id": "1203.2569v1", 
    "author": "Massimo Melucci", 
    "publish": "2012-03-12T17:57:40Z", 
    "summary": "Probabilistic models require the notion of event space for defining a\nprobability measure. An event space has a probability measure which ensues the\nKolmogorov axioms. However, the probabilities observed from distinct sources,\nsuch as that of relevance of documents, may not admit a single event space thus\ncausing some issues. In this article, some results are introduced for ensuring\nwhether the observed prob- abilities of relevance of documents admit a single\nevent space. More- over, an alternative framework of probability is introduced,\nthus chal- lenging the use of classical probability for ranking documents. Some\nreflections on the convenience of extending the classical probabilis- tic\nretrieval toward a more general framework which encompasses the issues are\nmade."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.4487v2", 
    "title": "Recommender systems in industrial contexts", 
    "arxiv-id": "1203.4487v2", 
    "author": "Frank Meyer", 
    "publish": "2012-03-20T16:14:42Z", 
    "summary": "This thesis consists of four parts: - An analysis of the core functions and\nthe prerequisites for recommender systems in an industrial context: we identify\nfour core functions for recommendation systems: Help do Decide, Help to\nCompare, Help to Explore, Help to Discover. The implementation of these\nfunctions has implications for the choices at the heart of algorithmic\nrecommender systems. - A state of the art, which deals with the main techniques\nused in automated recommendation system: the two most commonly used algorithmic\nmethods, the K-Nearest-Neighbor methods (KNN) and the fast factorization\nmethods are detailed. The state of the art presents also purely content-based\nmethods, hybridization techniques, and the classical performance metrics used\nto evaluate the recommender systems. This state of the art then gives an\noverview of several systems, both from academia and industry (Amazon, Google\n...). - An analysis of the performances and implications of a recommendation\nsystem developed during this thesis: this system, Reperio, is a hybrid\nrecommender engine using KNN methods. We study the performance of the KNN\nmethods, including the impact of similarity functions used. Then we study the\nperformance of the KNN method in critical uses cases in cold start situation. -\nA methodology for analyzing the performance of recommender systems in\nindustrial context: this methodology assesses the added value of algorithmic\nstrategies and recommendation systems according to its core functions."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1203.5415v1", 
    "title": "Incremental Collaborative Filtering Considering Temporal Effects", 
    "arxiv-id": "1203.5415v1", 
    "author": "Jingzheng Wu", 
    "publish": "2012-03-24T14:14:00Z", 
    "summary": "Recommender systems require their recommendation algorithms to be accurate,\nscalable and should handle very sparse training data which keep changing over\ntime. Inspired by ant colony optimization, we propose a novel collaborative\nfiltering scheme: Ant Collaborative Filtering that enjoys those favorable\ncharacteristics above mentioned. With the mechanism of pheromone transmission\nbetween users and items, our method can pinpoint most relative users and items\neven in face of the sparsity problem. By virtue of the evaporation of existing\npheromone, we capture the evolution of user preference over time. Meanwhile,\nthe computation complexity is comparatively small and the incremental update\ncan be done online. We design three experiments on three typical recommender\nsystems, namely movie recommendation, book recommendation and music\nrecommendation, which cover both explicit and implicit rating data. The results\nshow that the proposed algorithm is well suited for real-world recommendation\nscenarios which have a high throughput and are time sensitive."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.0591v1", 
    "title": "Multi-Faceted Ranking of News Articles using Post-Read Actions", 
    "arxiv-id": "1205.0591v1", 
    "author": "Xuanhui Wang", 
    "publish": "2012-05-03T00:59:39Z", 
    "summary": "Personalized article recommendation is important to improve user engagement\non news sites. Existing work quantifies engagement primarily through click\nrates. We argue that quality of recommendations can be improved by\nincorporating different types of \"post-read\" engagement signals like sharing,\ncommenting, printing and e-mailing article links. More specifically, we propose\na multi-faceted ranking problem for recommending news articles where each facet\ncorresponds to a ranking problem to maximize actions of a post-read action\ntype. The key technical challenge is to estimate the rates of post-read action\ntypes by mitigating the impact of enormous data sparsity, we do so through\nseveral variations of factor models. To exploit correlations among post-read\naction types we also introduce a novel variant called locally augmented tensor\n(LAT) model. Through data obtained from a major news site in the US, we show\nthat factor models significantly outperform a few baseline IR models and the\nLAT model significantly outperforms several other variations of factor models.\nOur findings show that it is possible to incorporate post-read signals that are\ncommonly available on online news sites to improve quality of recommendations."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.0919v1", 
    "title": "ViQIE: A New Approach for Visual Query Interpretation and Extraction", 
    "arxiv-id": "1205.0919v1", 
    "author": "Mohamed Nazih Omri", 
    "publish": "2012-05-04T11:08:31Z", 
    "summary": "Web services are accessed via query interfaces which hide databases\ncontaining thousands of relevant information. User's side, distant database is\na black box which accepts query and returns results, there is no way to access\ndatabase schema which reflect data and query meanings. Hence, web services are\nvery autonomous. Users view this autonomy as a major drawback because they need\noften to combine query capabilities of many web services at the same time. In\nthis work, we will present a new approach which allows users to benefit of\nquery capabilities of many web services while respecting autonomy of each\nservice. This solution is a new contribution in Information Retrieval research\naxe and has proven good performances on two standard datasets."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.1602v1", 
    "title": "Indexing of Arabic documents automatically based on lexical analysis", 
    "arxiv-id": "1205.1602v1", 
    "author": "Izzat Alsmadi", 
    "publish": "2012-05-08T06:52:15Z", 
    "summary": "The continuous information explosion through the Internet and all information\nsources makes it necessary to perform all information processing activities\nautomatically in quick and reliable manners. In this paper, we proposed and\nimplemented a method to automatically create and Index for books written in\nArabic language. The process depends largely on text summarization and\nabstraction processes to collect main topics and statements in the book. The\nprocess is developed in terms of accuracy and performance and results showed\nthat this process can effectively replace the effort of manually indexing books\nand document, a process that can be very useful in all information processing\nand retrieval applications."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.1779v1", 
    "title": "A Common Evaluation Setting for Just.Ask, Open Ephyra and Aranea QA   systems", 
    "arxiv-id": "1205.1779v1", 
    "author": "Ricardo Pires", 
    "publish": "2012-05-08T19:19:32Z", 
    "summary": "Question Answering (QA) is not a new research field in Natural Language\nProcessing (NLP). However in recent years, QA has been a subject of growing\nstudy. Nowadays, most of the QA systems have a similar pipelined architecture\nand each system use a set of unique techniques to accomplish its state of the\nart results. However, many things are not clear in the QA processing. It is not\nclear the extend of the impact of tasks performed in earlier stages in\nfollowing stages of the pipelining process. It is not clear, if techniques used\nin a QA system can be used in another QA system to improve its results. And\nfinally, it is not clear in what setting should be these systems tested in\norder to properly analyze their results."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.2891v1", 
    "title": "Effective performance of information retrieval on web by using web   crawling", 
    "arxiv-id": "1205.2891v1", 
    "author": "P. Premchand", 
    "publish": "2012-05-13T17:56:52Z", 
    "summary": "World Wide Web consists of more than 50 billion pages online. It is highly\ndynamic i.e. the web continuously introduces new capabilities and attracts many\npeople. Due to this explosion in size, the effective information retrieval\nsystem or search engine can be used to access the information. In this paper we\nhave proposed the EPOW (Effective Performance of WebCrawler) architecture. It\nis a software agent whose main objective is to minimize the overload of a user\nlocating needed information. We have designed the web crawler by considering\nthe parallelization policy. Since our EPOW crawler has a highly optimized\nsystem it can download a large number of pages per second while being robust\nagainst crashes. We have also proposed to use the data structure concepts for\nimplementation of scheduler & circular Queue to improve the performance of our\nweb crawler."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.3031v1", 
    "title": "The model of information retrieval based on the theory of hypercomplex   numerical systems", 
    "arxiv-id": "1205.3031v1", 
    "author": "Yu. E. Boyarinova", 
    "publish": "2012-05-14T14:01:56Z", 
    "summary": "The paper provided a description of a new model of information retrieval,\nwhich is an extension of vector-space model and is based on the principles of\nthe theory of hypercomplex numerical systems. The model allows to some extent\nrealize the idea of fuzzy search and allows you to apply in practice the model\nof information retrieval practical developments in the field of hypercomplex\nnumerical systems."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.5569v3", 
    "title": "A Theory of Information Matching", 
    "arxiv-id": "1205.5569v3", 
    "author": "Tamas Jambor", 
    "publish": "2012-05-24T20:56:16Z", 
    "summary": "In this work, we propose a theory for information matching. It is motivated\nby the observation that retrieval is about the relevance matching between two\nsets of properties (features), namely, the information need representation and\ninformation item representation. However, many probabilistic retrieval models\nrely on fixing one representation and optimizing the other (e.g. fixing the\nsingle information need and tuning the document) but not both. Therefore, it is\ndifficult to use the available related information on both the document and the\nquery at the same time in calculating the probability of relevance. In this\npaper, we address the problem by hypothesizing the relevance as a logical\nrelationship between the two sets of properties; the relationship is defined on\ntwo separate mappings between these properties. By using the hypothesis we\ndevelop a unified probabilistic relevance model which is capable of using all\nthe available information. We validate the proposed theory by formulating and\ndeveloping probabilistic relevance ranking functions for both ad-hoc text\nretrieval and collaborative filtering. Our derivation in text retrieval\nillustrates the use of the theory in the situation where no relevance\ninformation is available. In collaborative filtering, we show that the\nresulting recommender model unifies the user and item information into a\nrelevance ranking function without applying any dimensionality reduction\ntechniques or computing explicit similarity between two different users (or\nitems), in contrast to the state-of-the-art recommender models."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.5632v1", 
    "title": "Quantum contextuality in classical information retrieval", 
    "arxiv-id": "1205.5632v1", 
    "author": "Roman Zapatrin", 
    "publish": "2012-05-25T08:43:06Z", 
    "summary": "Document ranking based on probabilistic evaluations of relevance is known to\nexhibit non-classical correlations, which may be explained by admitting a\ncomplex structure of the event space, namely, by assuming the events to emerge\nfrom multiple sample spaces. The structure of event space formed by overlapping\nsample spaces is known in quantum mechanics, they may exhibit some\ncounter-intuitive features, called quantum contextuality. In this Note I\nobserve that from the structural point of view quantum contextuality looks\nsimilar to personalization of information retrieval scenarios. Along these\nlines, Knowledge Revision is treated as operationalistic measurement and a way\nto quantify the rate of personalization of Information Retrieval scenarios is\nsuggested."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1205.5923v1", 
    "title": "Conversion database of the shapes into XML data for shape matching", 
    "arxiv-id": "1205.5923v1", 
    "author": "Mohamed Bahaj", 
    "publish": "2012-05-26T22:45:50Z", 
    "summary": "We present a new approach to the matching of 2D shapes using XML language and\ndynamic programming. Given a 2D shape, we extract its contour and which is\nrepresented by set of points. The contour is divided into curves using corner\ndetection. After, each curve is described by local and global features; these\nfeatures are coded in a string of symbols and stored in a XML file. Finally,\nusing the dynamic programming, we find the optimal alignment between sequences\nof symbols. Results are presented and compared with existing methods using\nMATLAB for KIMIA-25 database and MPEG7 databases."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.0905v1", 
    "title": "A Fuzzy Approach for Pertinent Information Extraction from Web Resources", 
    "arxiv-id": "1206.0905v1", 
    "author": "Habib Youssef", 
    "publish": "2012-06-05T12:36:09Z", 
    "summary": "Recent work in machine learning for information extraction has focused on two\ndistinct sub-problems: the conventional problem of filling template slots from\nnatural language text, and the problem of wrapper induction, learning simple\nextraction procedures (\"wrappers\") for highly structured text such as Web\npages. For suitable regular domains, existing wrapper induction algorithms can\nefficiently learn wrappers that are simple and highly accurate, but the\nregularity bias of these algorithms makes them unsuitable for most conventional\ninformation extraction tasks. This paper describes a new approach for wrapping\nsemistructured Web pages. The wrapper is capable of learning how to extract\nrelevant information from Web resources on the basis of user supplied examples.\nIt is based on inductive learning techniques as well as fuzzy logic rules.\nExperimental results show that our approach achieves noticeably better\nprecision and recall coefficient performance measures than SoftMealy, which is\none of the most recently reported wrappers capable of wrapping semi-structured\nWeb pages with missing attributes, multiple attributes, variant attribute\npermutations, exceptions, and typos."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.0968v1", 
    "title": "Pertinent Information retrieval based on Possibilistic Bayesian network   : origin and possibilistic perspective", 
    "arxiv-id": "1206.0968v1", 
    "author": "Bachir Elayeb", 
    "publish": "2012-06-05T15:45:06Z", 
    "summary": "In this paper we present a synthesis of work performed on tow information\nretrieval models: Bayesian network information retrieval model witch encode\n(in) dependence relation between terms and possibilistic network information\nretrieval model witch make use of necessity and possibility measures to\nrepresent the fuzziness of pertinence measure. It is known that the use of a\ngeneral Bayesian network methodology as the basis for an IR system is difficult\nto tackle. The problem mainly appears because of the large number of variables\ninvolved and the computational efforts needed to both determine the\nrelationships between variables and perform the inference processes. To resolve\nthese problems, many models have been proposed such as BNR model. Generally,\nBayesian network models doesn't consider the fuzziness of natural language in\nthe relevance measure of a document to a given query and possibilistic models\ndoesn't undertake the dependence relations between terms used to index\ndocuments. As a first solution we propose a hybridization of these two models\nin one that will undertake both the relationship between terms and the\nintrinsic fuzziness of natural language. We believe that the translation of\nBayesian network model from the probabilistic framework to possibilistic one\nwill allow a performance improvement of BNRM."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1042v2", 
    "title": "Relevance Feedback for Goal's Extraction from Fuzzy Semantic Networks", 
    "arxiv-id": "1206.1042v2", 
    "author": "Mohamed Nazih Omri", 
    "publish": "2012-06-05T12:49:44Z", 
    "summary": "In this paper we present a short survey of fuzzy and Semantic approaches to\nKnowledge Extraction. The goal of such approaches is to define flexible\nKnowledge Extraction Systems able to deal with the inherent vagueness and\nuncertainty of the Extraction process. It has long been recognised that\ninteractivity improves the effectiveness of Knowledge Extraction systems.\nNovice user's queries is the most natural and interactive medium of\ncommunication and recent progress in recognition is making it possible to build\nsystems that interact with the user. However, given the typical novice user's\nqueries submitted to Knowledge Extraction systems, it is easy to imagine that\nthe effects of goal recognition errors in novice user's queries must be\nseverely destructive on the system's effectiveness. The experimental work\nreported in this paper shows that the use of classical Knowledge Extraction\ntechniques for novice user's query processing is robust to considerably high\nlevels of goal recognition errors. Moreover, both standard relevance feedback\nand pseudo relevance feedback can be effectively employed to improve the\neffectiveness of novice user's query processing."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1492v3", 
    "title": "Ordinary Search Engine Users Carrying Out Complex Search Tasks", 
    "arxiv-id": "1206.1492v3", 
    "author": "Dirk Lewandowski", 
    "publish": "2012-06-07T13:45:05Z", 
    "summary": "Web search engines have become the dominant tools for finding information on\nthe Internet. Due to their popularity, users apply them to a wide range of\nsearch needs, from simple look-ups to rather complex information tasks. This\npaper presents the results of a study to investigate the characteristics of\nthese complex information needs in the context of Web search engines. The aim\nof the study is to find out more about (1) what makes complex search tasks\ndistinct from simple tasks and if it is possible to find simple measures for\ndescribing their complexity, (2) if search success for a task can be predicted\nby means of unique measures, and (3) if successful searchers show a different\nbehavior than unsuccessful ones. The study includes 60 people who carried out a\nset of 12 search tasks with current commercial search engines. Their behavior\nwas logged with the Search-Logger tool. The results confirm that complex tasks\nshow significantly different characteristics than simple tasks. Yet it seems to\nbe difficult to distinguish successful from unsuccessful search behaviors. Good\nsearchers can be differentiated from bad searchers by means of measurable\nparameters. The implications of these findings for search engine vendors are\ndiscussed."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1615v1", 
    "title": "Objects and Goals Extraction from Semantic Networks : Applications of   Fuzzy SetS Theory", 
    "arxiv-id": "1206.1615v1", 
    "author": "Mohamed Nazih Omri", 
    "publish": "2012-06-07T21:04:15Z", 
    "summary": "In this paper we present a short survey of fuzzy and Semantic approaches to\nKnowledge Extraction. The goal of such approaches is to define flexible\nKnowledge Extraction Systems able to deal with the inherent vagueness and\nuncertainty of the Extraction process. In this survey we address if and how\nsome approaches met their goal."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1624v1", 
    "title": "Measure of Similarity between Fuzzy Concepts for Optimization of Fuzzy   Semantic Nets", 
    "arxiv-id": "1206.1624v1", 
    "author": "Noureddine Chouigui", 
    "publish": "2012-06-07T21:36:08Z", 
    "summary": "This paper presents a method to measure the similarity between different\nfuzzy concepts in order to optimize Semantic networks. The problem approached\nis the minimization of the time of research and identification of user's\nObjects and Goals. Indeed, it concerns to determine to each instant the\ntotality of Objects (respectively Goals) among which one can identify rapidly\nthe most satisfactory for the user's Object and Goal. Alone Objects and most\nsimilar Goals to Objects and researched Goals of the viewpoint of attribute\nvalues will be processed, what will avoid the analysis of all Objects and\nsystem Goals far of needs of the user."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1754v2", 
    "title": "Internet Advertising: An Interplay among Advertisers, Online Publishers,   Ad Exchanges and Web Users", 
    "arxiv-id": "1206.1754v2", 
    "author": "Jun Wang", 
    "publish": "2012-06-08T13:28:30Z", 
    "summary": "Internet advertising is a fast growing business which has proved to be\nsignificantly important in digital economics. It is vitally important for both\nweb search engines and online content providers and publishers because web\nadvertising provides them with major sources of revenue. Its presence is\nincreasingly important for the whole media industry due to the influence of the\nWeb. For advertisers, it is a smarter alternative to traditional marketing\nmedia such as TVs and newspapers. As the web evolves and data collection\ncontinues, the design of methods for more targeted, interactive, and friendly\nadvertising may have a major impact on the way our digital economy evolves, and\nto aid societal development.\n  Towards this goal mathematically well-grounded Computational Advertising\nmethods are becoming necessary and will continue to develop as a fundamental\ntool towards the Web. As a vibrant new discipline, Internet advertising\nrequires effort from different research domains including Information\nRetrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,\nand even Psychology to predict and understand user behaviours. In this paper,\nwe provide a comprehensive survey on Internet advertising, discussing and\nclassifying the research issues, identifying the recent technologies, and\nsuggesting its future directions. To have a comprehensive picture, we first\nstart with a brief history, introduction, and classification of the industry\nand present a schematic view of the new advertising ecosystem. We then\nintroduce four major participants, namely advertisers, online publishers, ad\nexchanges and web users; and through analysing and discussing the major\nresearch problems and existing solutions from their perspectives respectively,\nwe discover and aggregate the fundamental problems that characterise the\nnewly-formed research field and capture its potential future prospects."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.1852v1", 
    "title": "Optimization of Fuzzy Semantic Networks Based on Galois Lattice and   Bayesian Formalism", 
    "arxiv-id": "1206.1852v1", 
    "author": "Mohamed Nazih Omri", 
    "publish": "2012-06-07T21:53:36Z", 
    "summary": "This paper presents a method of optimization, based on both Bayesian Analysis\ntechnical and Galois Lattice of Fuzzy Semantic Network. The technical System we\nuse learns by interpreting an unknown word using the links created between this\nnew word and known words. The main link is provided by the context of the\nquery. When novice's query is confused with an unknown verb (goal) applied to a\nknown noun denoting either an object in the ideal user's Network or an object\nin the user's Network, the system infer that this new verb corresponds to one\nof the known goal. With the learning of new words in natural language as the\ninterpretation, which was produced in agreement with the user, the system\nimproves its representation scheme at each experiment with a new user and, in\naddition, takes advantage of previous discussions with users. The semantic Net\nof user objects thus obtained by learning is not always optimal because some\nrelationships between couple of user objects can be generalized and others\nsuppressed according to values of forces that characterize them. Indeed, to\nsimplify the obtained Net, we propose to proceed to an Inductive Bayesian\nAnalysis, on the Net obtained from Galois lattice. The objective of this\nanalysis can be seen as an operation of filtering of the obtained descriptive\ngraph."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.2528v1", 
    "title": "Ordinary Search Engine Users assessing Difficulty, Effort, and Outcome   for Simple and Complex Search Tasks", 
    "arxiv-id": "1206.2528v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2012-06-12T13:43:08Z", 
    "summary": "Search engines are the preferred tools for finding information on the Web.\nThey are advancing to be the common helpers to answer any of our search needs.\nWe use them to carry out simple look-up tasks and also to work on rather time\nconsuming and more complex search tasks. Yet, we do not know very much about\nthe user performance while carrying out those tasks -- especially not for\nordinary users. The aim of this study was to get more insight into whether Web\nusers manage to assess difficulty, time effort, query effort, and task outcome\nof search tasks, and if their judging performance relates to task complexity.\nOur study was conducted with a systematically selected sample of 56 people with\na wide demographic background. They carried out a set of 12 search tasks with\ncommercial Web search engines in a laboratory environment. The results confirm\nthat it is hard for normal Web users to judge the difficulty and effort to\ncarry out complex search tasks. The judgments are more reliable for simple\ntasks than for complex ones. Task complexity is an indicator for judging\nperformance."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.3078v1", 
    "title": "Mining Educational Data Using Classification to Decrease Dropout Rate of   Students", 
    "arxiv-id": "1206.3078v1", 
    "author": "Saurabh Pal", 
    "publish": "2012-06-14T12:10:23Z", 
    "summary": "In the last two decades, number of Higher Education Institutions (HEI) grows\nrapidly in India. Since most of the institutions are opened in private mode\ntherefore, a cut throat competition rises among these institutions while\nattracting the student to got admission. This is the reason for institutions to\nfocus on the strength of students not on the quality of education. This paper\npresents a data mining application to generate predictive models for\nengineering student's dropout management. Given new records of incoming\nstudents, the predictive model can produce short accurate prediction list\nidentifying students who tend to need the support from the student dropout\nprogram most. The results show that the machine learning algorithm is able to\nestablish effective predictive model from the existing student dropout data."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.3320v1", 
    "title": "A two-step Recommendation Algorithm via Iterative Local Least Squares", 
    "arxiv-id": "1206.3320v1", 
    "author": "Zi-Ke Zhang", 
    "publish": "2012-06-14T20:23:24Z", 
    "summary": "Recommender systems can change our life a lot and help us select suitable and\nfavorite items much more conveniently and easily. As a consequence, various\nkinds of algorithms have been proposed in last few years to improve the\nperformance. However, all of them face one critical problem: data sparsity. In\nthis paper, we proposed a two-step recommendation algorithm via iterative local\nleast squares (ILLS). Firstly, we obtain the ratings matrix which is\nconstructed via users' behavioral records, and it is normally very sparse.\nSecondly, we preprocess the \"ratings\" matrix through ProbS which can convert\nthe sparse data to a dense one. Then we use ILLS to estimate those missing\nvalues. Finally, the recommendation list is generated. Experimental results on\nthe three datasets: MovieLens, Netflix, RYM, suggest that the proposed method\ncan enhance the algorithmic accuracy of AUC. Especially, it performs much\nbetter in dense datasets. Furthermore, since this methods can improve those\nmissing value more accurately via iteration which might show light in\ndiscovering those inactive users' purchasing intention and eventually solving\ncold-start problem."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.4802v1", 
    "title": "Better Than Their Reputation? On the Reliability of Relevance   Assessments with Students", 
    "arxiv-id": "1206.4802v1", 
    "author": "Philipp Schaer", 
    "publish": "2012-06-21T08:06:15Z", 
    "summary": "During the last three years we conducted several information retrieval\nevaluation series with more than 180 LIS students who made relevance\nassessments on the outcomes of three specific retrieval services. In this study\nwe do not focus on the retrieval performance of our system but on the relevance\nassessments and the inter-assessor reliability. To quantify the agreement we\napply Fleiss' Kappa and Krippendorff's Alpha. When we compare these two\nstatistical measures on average Kappa values were 0.37 and Alpha values 0.15.\nWe use the two agreement measures to drop too unreliable assessments from our\ndata set. When computing the differences between the unfiltered and the\nfiltered data set we see a root mean square error between 0.02 and 0.12. We see\nthis as a clear indicator that disagreement affects the reliability of\nretrieval evaluations. We suggest not to work with unfiltered results or to\nclearly document the disagreement rates."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.4883v1", 
    "title": "Multilingual Medical Documents Classification Based on MesH Domain   Ontology", 
    "arxiv-id": "1206.4883v1", 
    "author": "Amel Belaggoun", 
    "publish": "2012-06-21T13:56:50Z", 
    "summary": "This article deals with the semantic Web and ontologies. It addresses the\nissue of the classification of multilingual Web documents, based on domain\nontology. The objective is being able, using a model, to classify documents in\ndifferent languages. We will try to solve this problematic using two different\napproaches. The two approaches will have two elementary stages: the creation of\nthe model using machine learning algorithms on a labeled corpus, then the\nclassification of documents after detecting their languages and mapping their\nterms into the concepts of the language of reference (English). But each one\nwill deal with the multilingualism with a different approach. One supposes the\nontology is monolingual, whereas the other considers it multilingual. To show\nthe feasibility and the importance of our work, we implemented it on a domain\nthat attracts nowadays a lot of attention from the data mining community: the\nbiomedical domain. The selected documents are from the biomedical benchmark\ncorpus Ohsumed, and the associated ontology is the thesaurus MeSH (Medical\nSubject Headings). The main idea in our work is a new document representation,\nthe masterpiece of all good classification, based on concept. The experimental\nresults show that the recommended ideas are promising."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.5582v1", 
    "title": "A Survey on Web Service Discovery Approaches", 
    "arxiv-id": "1206.5582v1", 
    "author": "Archana Chougule", 
    "publish": "2012-06-25T06:00:56Z", 
    "summary": "Web services are playing an important role in e-business and e-commerce\napplications. As web service applications are interoperable and can work on any\nplatform, large scale distributed systems can be developed easily using web\nservices. Finding most suitable web service from vast collection of web\nservices is very crucial for successful execution of applications. Traditional\nweb service discovery approach is a keyword based search using UDDI. Various\nother approaches for discovering web services are also available. Some of the\ndiscovery approaches are syntax based while other are semantic based. Having\nsystem for service discovery which can work automatically is also the concern\nof service discovery approaches. As these approaches are different, one\nsolution may be better than another depending on requirements. Selecting a\nspecific service discovery system is a hard task. In this paper, we give an\noverview of different approaches for web service discovery described in\nliterature. We present a survey of how these approaches differ from each other."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1206.5584v1", 
    "title": "Web-page Prediction for Domain Specific Web-search using Boolean Bit   Mask", 
    "arxiv-id": "1206.5584v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2012-06-25T06:07:20Z", 
    "summary": "Search Engine is a Web-page retrieval tool. Nowadays Web searchers utilize\ntheir time using an efficient search engine. To improve the performance of the\nsearch engine, we are introducing a unique mechanism which will give Web\nsearchers more prominent search results. In this paper, we are going to discuss\na domain specific Web search prototype which will generate the predicted\nWeb-page list for user given search string using Boolean bit mask."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1208.1011v1", 
    "title": "Credibility in Web Search Engines", 
    "arxiv-id": "1208.1011v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2012-08-05T14:03:22Z", 
    "summary": "Web search engines apply a variety of ranking signals to achieve user\nsatisfaction, i.e., results pages that provide the best-possible results to the\nuser. While these ranking signals implicitly consider credibility (e.g., by\nmeasuring popularity), explicit measures of credibility are not applied. In\nthis chapter, credibility in Web search engines is discussed in a broad\ncontext: credibility as a measure for including documents in a search engine's\nindex, credibility as a ranking signal, credibility in the context of universal\nsearch results, and the possibility of using credibility as an explicit measure\nfor ranking purposes. It is found that while search engines-at least to a\ncertain extent-show credible results to their users, there is no fully\nintegrated credibility framework for Web search engines."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijist.2012.2104", 
    "link": "http://arxiv.org/pdf/1208.1886v1", 
    "title": "Semantic Web Techniques for Yellow Page Service Providers", 
    "arxiv-id": "1208.1886v1", 
    "author": "Ramani Srinivasan", 
    "publish": "2012-08-09T12:26:48Z", 
    "summary": "Use of web pages providing unstructured information poses variety of problems\nto the user, such as use of arbitrary formats, unsuitability for machine\nprocessing and likely incompleteness of information. Structured data alleviates\nthese problems but we require more. Very often yellow page systems are\nimplemented using a centralized database. In some cases, human intermediaries\naccessible over the phone network examine a centralized database and use their\nreasoning ability to deal with the user's need for information. Scaling up such\nsystems is difficult. This paper explores an alternative - a highly distributed\nsystem design meeting a variety of needs - considerably reducing efforts\nrequired at a central organization, enabling large numbers of vendors to enter\ninformation about their own products and services, enabling end-users to\ncontribute information such as their own ratings, using an ontology to describe\neach domain of application in a flexible manner for uses foreseen and\nunforeseen, enabling distributed search and mash-ups, use of vendor independent\nstandards, using reasoning to find the best matches to a given query,\ngeo-spatial reasoning and a simple, interactive, mobile application/interface.\nWe give importance to geo-spatial information and mobile applications because\nof the very wide-spread use of mobile phones and their inherent ability to\nprovide some information about the current location of the user. We have\ncreated a prototype using the Jena Toolkit and geo-spatial extensions to\nSPARQL. We have tested this prototype by asking a group of typical users to use\nit and to provide structured feedback. We have summarized this feedback in the\npaper. We believe that the technology can be applied in many contexts in\naddition to yellow page systems."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijaia.2012.3415", 
    "link": "http://arxiv.org/pdf/1208.1926v1", 
    "title": "Role of Ranking Algorithms for Information Retrieval", 
    "arxiv-id": "1208.1926v1", 
    "author": "Bhawani Shankar Burdak", 
    "publish": "2012-08-09T14:43:51Z", 
    "summary": "As the use of web is increasing more day by day, the web users get easily\nlost in the web's rich hyper structure. The main aim of the owner of the\nwebsite is to give the relevant information according their needs to the users.\nWe explained the Web mining is used to categorize users and pages by analyzing\nuser's behavior, the content of pages and then describe Web Structure mining.\nThis paper includes different Page Ranking algorithms and compares those\nalgorithms used for Information Retrieval. Different Page Rank based algorithms\nlike Page Rank (PR), WPR (Weighted Page Rank), HITS (Hyperlink Induced Topic\nSelection), Distance Rank and EigenRumor algorithms are discussed and compared.\nSimulation Interface has been designed for PageRank algorithm and Weighted\nPageRank algorithm but PageRank is the only ranking algorithm on which Google\nsearch engine works."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijci.2012.1401", 
    "link": "http://arxiv.org/pdf/1208.2782v1", 
    "title": "Multidimensional Web Page Evaluation Model Using Segmentation And   Annotations", 
    "arxiv-id": "1208.2782v1", 
    "author": "G. Aghila", 
    "publish": "2012-08-14T04:40:30Z", 
    "summary": "The evaluation of web pages against a query is the pivot around which the\nInformation Retrieval domain revolves around. The context sensitive, semantic\nevaluation of web pages is a non-trivial problem which needs to be addressed\nimmediately. This research work proposes a model to evaluate the web pages by\ncumulating the segment scores which are computed by multidimensional evaluation\nmethodology. The model proposed is hybrid since it utilizes both the structural\nsemantics and content semantics in the evaluation process. The score of the web\npage is computed in a bottom-up process by evaluating individual segment's\nscore through a multi-dimensional approach. The model incorporates an approach\nfor segment level annotation. The proposed model is prototyped for evaluation;\nexperiments conducted on the prototype confirm the model's efficiency in\nsemantic evaluation of pages."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1208.3774v1", 
    "title": "Graphical Query Builder in Opportunistic Sensor Networks to discover   Sensor Information", 
    "arxiv-id": "1208.3774v1", 
    "author": "Mohammad Saiful Islam Mamun", 
    "publish": "2012-08-18T18:41:07Z", 
    "summary": "A lot of sensor network applications are data-driven. We believe that query\nis the most preferred way to discover sensor services. Normally users are\nunaware of available sensors. Thus users need to pose different types of query\nover the sensor network to get the desired information. Even users may need to\ninput more complicated queries with higher levels of aggregations, and requires\nmore complex interactions with the system. As the users have no prior knowledge\nof the sensor data or services our aim is to develop a visual query interface\nwhere users can feed more user friendly queries and machine can understand\nthose. In this paper work, we have developed an Interactive visual query\ninterface for the users. To accomplish this we have considered several use\ncases and we have derived graphical representation of query from their text\nbased format for those use case scenario. We have facilitated the user by\nextracting class, subclass and properties from Ontology. To do so we have\nparsed OWL file in the user interface and based upon the parsed information\nusers build visual query. Later on we have translated the visual query\nlanguages into SPARQL query, a machine understandable format which helps the\nmachine to communicate with the underlying technology."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1208.3952v1", 
    "title": "Dealing with Sparse Document and Topic Representations: Lab Report for   CHiC 2012", 
    "arxiv-id": "1208.3952v1", 
    "author": "Thomas L\u00fcke", 
    "publish": "2012-08-20T09:18:37Z", 
    "summary": "We will report on the participation of GESIS at the first CHiC workshop\n(Cultural Heritage in CLEF). Being held for the first time, no prior experience\nwith the new data set, a document dump of Europeana with ca. 23 million\ndocuments, exists. The most prominent issues that arose from pretests with this\ntest collection were the very unspecific topics and sparse document\nrepresentations. Only half of the topics (26/50) contained a description and\nthe titles were usually short with just around two words. Therefore we focused\non three different term suggestion and query expansion mechanisms to surpass\nthe sparse topical description. We used two methods that build on concept\nextraction from Wikipedia and on a method that applied co-occurrence statistics\non the available Europeana corpus. In the following paper we will present the\napproaches and preliminary results from their assessments."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1209.0126v1", 
    "title": "Evaluation of some Information Retrieval models for Gujarati Ad hoc   Monolingual Tasks", 
    "arxiv-id": "1209.0126v1", 
    "author": "Pareek Jyoti", 
    "publish": "2012-09-01T19:45:01Z", 
    "summary": "This paper describes the work towards Gujarati Ad hoc Monolingual Retrieval\ntask for widely used Information Retrieval (IR) models. We present an indexing\nbaseline for the Gujarati Language represented by Mean Average Precision (MAP)\nvalues. Our objective is to obtain a relative picture of a better IR model for\nGujarati Language. Results show that Classical IR models like Term Frequency\nInverse Document Frequency (TF_IDF) performs better when compared to few recent\nprobabilistic IR models. The experiments helped to identify the outperforming\nIR models for Gujarati Language."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1209.2274v1", 
    "title": "PCA-Based Relevance Feedback in Document Image Retrieval", 
    "arxiv-id": "1209.2274v1", 
    "author": "Fariborz Mahmoudi", 
    "publish": "2012-09-11T10:02:47Z", 
    "summary": "Research has been devoted in the past few years to relevance feedback as an\neffective solution to improve performance of information retrieval systems.\nRelevance feedback refers to an interactive process that helps to improve the\nretrieval performance. In this paper we propose the use of relevance feedback\nto improve document image retrieval System (DIRS) performance. This paper\ncompares a variety of strategies for positive and negative feedback. In\naddition, feature subspace is extracted and updated during the feedback process\nusing a Principal Component Analysis (PCA) technique and based on user's\nfeedback. That is, in addition to reducing the dimensionality of feature\nspaces, a proper subspace for each type of features is obtained in the feedback\nprocess to further improve the retrieval accuracy. Experiments show that using\nrelevance Feedback in DIR achieves better performance than common DIR."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1209.4479v1", 
    "title": "Beyond Cumulated Gain and Average Precision: Including Willingness and   Expectation in the User Model", 
    "arxiv-id": "1209.4479v1", 
    "author": "Mounia Lalmas", 
    "publish": "2012-09-20T09:59:53Z", 
    "summary": "In this paper, we define a new metric family based on two concepts: The\ndefinition of the stopping criterion and the notion of satisfaction, where the\nformer depends on the willingness and expectation of a user exploring search\nresults. Both concepts have been discussed so far in the IR literature, but we\nargue in this paper that defining a proper single valued metric depends on\nmerging them into a single conceptual framework."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1209.5448v1", 
    "title": "A New Compression Based Index Structure for Efficient Information   Retrieval", 
    "arxiv-id": "1209.5448v1", 
    "author": "Md. Mofizul Islam", 
    "publish": "2012-09-24T22:27:17Z", 
    "summary": "Finding desired information from large data set is a difficult problem.\nInformation retrieval is concerned with the structure, analysis, organization,\nstorage, searching, and retrieval of information. Index is the main constituent\nof an IR system. Now a day exponential growth of information makes the index\nstructure large enough affecting the IR system's quality. So compressing the\nIndex structure is our main contribution in this paper. We compressed the\ndocument number in inverted file entries using a new coding technique based on\nrun-length encoding. Our coding mechanism uses a specified code which acts over\nrun-length coding. We experimented and found that our coding mechanism on an\naverage compresses 67.34% percent more than the other techniques."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1209.6492v1", 
    "title": "Information Retrieval on the web and its evaluation", 
    "arxiv-id": "1209.6492v1", 
    "author": "Deepak Garg", 
    "publish": "2012-09-28T11:50:15Z", 
    "summary": "Internet is one of the main sources of information for millions of people.\nOne can find information related to practically all matters on internet.\nMoreover if we want to retrieve information about some particular topic we may\nfind thousands of Web Pages related to that topic. But our main concern is to\nfind relevant Web Pages from among that collection. So in this paper I have\ndiscussed that how information is retrieved from the web and the efforts\nrequired for retrieving this information in terms of system and users efforts."
},{
    "category": "cs.IR", 
    "doi": "10.5120/1913-2551", 
    "link": "http://arxiv.org/pdf/1210.1626v1", 
    "title": "Discovering and Leveraging the Most Valuable Links for Ranking", 
    "arxiv-id": "1210.1626v1", 
    "author": "Hengshuai Yao", 
    "publish": "2012-10-05T02:05:54Z", 
    "summary": "On the Web, visits of a page are often introduced by one or more valuable\nlinking sources. Indeed, good back links are valuable resources for Web pages\nand sites. We propose to discovering and leveraging the best backlinks of pages\nfor ranking. Similar to PageRank, MaxRank scores are updated {recursively}. In\nparticular, with probability $\\lambda$, the MaxRank of a document is updated\nfrom the backlink source with the maximum score; with probability $1-\\lambda$,\nthe MaxRank of a document is updated from a random backlink source. MaxRank has\nan interesting relation to PageRank. When $\\lambda=0$, MaxRank reduces to\nPageRank; when $\\lambda=1$, MaxRank only looks at the best backlink it thinks.\nEmpirical results on Wikipedia shows that the global authorities are very\ninfluential; Overall large $\\lambda$s (but smaller than 1) perform best: the\nconvergence is dramatically faster than PageRank, but the performance is still\ncomparable. We study the influence of these sources and propose a few measures\nsuch as the times of being the best backlink for others, and related properties\nof the proposed algorithm. The introduction of best backlink sources provides\nnew insights for link analysis. Besides ranking, our method can be used to\ndiscover the most valuable linking sources for a page or Website, which is\nuseful for both search engines and site owners."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1210.6113v1", 
    "title": "Using the DOM Tree for Content Extraction", 
    "arxiv-id": "1210.6113v1", 
    "author": "David Insa", 
    "publish": "2012-10-23T02:55:06Z", 
    "summary": "The main information of a webpage is usually mixed between menus,\nadvertisements, panels, and other not necessarily related information; and it\nis often difficult to automatically isolate this information. This is precisely\nthe objective of content extraction, a research area of widely interest due to\nits many applications. Content extraction is useful not only for the final\nhuman user, but it is also frequently used as a preprocessing stage of\ndifferent systems that need to extract the main content in a web document to\navoid the treatment and processing of other useless information. Other\ninteresting application where content extraction is particularly used is\ndisplaying webpages in small screens such as mobile phones or PDAs. In this\nwork we present a new technique for content extraction that uses the DOM tree\nof the webpage to analyze the hierarchical relations of the elements in the\nwebpage. Thanks to this information, the technique achieves a considerable\nrecall and precision. Using the DOM structure for content extraction gives us\nthe benefits of other approaches based on the syntax of the webpage (such as\ncharacters, words and tags), but it also gives us a very precise information\nregarding the related components in a block, thus, producing very cohesive\nblocks."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1211.0320v1", 
    "title": "TrackMeNot-so-good-after-all", 
    "arxiv-id": "1211.0320v1", 
    "author": "Nikhil Patwardhan", 
    "publish": "2012-11-01T22:16:57Z", 
    "summary": "TrackMeNot is a Firefox plugin with laudable intentions - protecting your\nprivacy. By issuing a customizable stream of random search queries on its\nusers' behalf, TrackMeNot surmises that enough search noise will prevent its\nusers' true query profiles from being discerned. However, we find that\nclustering queries by semantic relatedness allows us to disentangle a\nnontrivial subset of true user queries from TrackMeNot issued noise."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1211.1107v1", 
    "title": "An effective web document clustering for information retrieval", 
    "arxiv-id": "1211.1107v1", 
    "author": "S. K. Sahay", 
    "publish": "2012-11-06T04:39:30Z", 
    "summary": "The size of web has increased exponentially over the past few years with\nthousands of documents related to a subject available to the user. With this\nmuch amount of information available, it is not possible to take the full\nadvantage of the World Wide Web without having a proper framework to search\nthrough the available data. This requisite organization can be done in many\nways. In this paper we introduce a combine approach to cluster the web pages\nwhich first finds the frequent sets and then clusters the documents. These\nfrequent sets are generated by using Frequent Pattern growth technique. Then by\napplying Fuzzy C- Means algorithm on it, we found clusters having documents\nwhich are highly related and have similar features. We used Gensim package to\nimplement our approach because of its simplicity and robust nature. We have\ncompared our results with the combine approach of (Frequent Pattern growth,\nK-means) and (Frequent Pattern growth, Cosine_Similarity). Experimental results\nshow that our approach is more efficient then the above two combine approach\nand can handles more efficiently the serious limitation of traditional Fuzzy\nC-Means algorithm, which is sensitiveto initial centroid and the number of\nclusters to be formed."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1211.1861v1", 
    "title": "Automating Legal Research through Data Mining", 
    "arxiv-id": "1211.1861v1", 
    "author": "Mohamed Firdhous", 
    "publish": "2012-11-08T14:19:49Z", 
    "summary": "The term legal research generally refers to the process of identifying and\nretrieving appropriate information necessary to support legal decision making\nfrom past case records. At present, the process is mostly manual, but some\ntraditional technologies such as keyword searching are commonly used to speed\nthe process up. But a keyword search is not a comprehensive search to cater to\nthe requirements of legal research as the search result includes too many false\nhits in terms of irrelevant case records. Hence the present generic tools\ncannot be used to automate legal research.\n  This paper presents a framework which was developed by combining several Text\nMining techniques to automate the process overcoming the difficulties in the\nexisting methods. Further, the research also identifies the possible\nenhancements that could be done to enhance the effectiveness of the framework."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1211.2854v1", 
    "title": "Using ontology for resume annotation", 
    "arxiv-id": "1211.2854v1", 
    "author": "Wahiba Ben Abdessalem Karaa Nouha Mhimdi", 
    "publish": "2012-11-12T23:47:47Z", 
    "summary": "Employers collect a large number of resumes from job portals, or from the\ncompany's own website. These documents are used for an automated selection of\ncandidates satisfying the requirements and therefore reducing recruitment\ncosts. Various approaches for process documents have already been developed for\nrecruitment. In this paper we present an approach based on semantic annotation\nof resumes for e-recruitment process. The most important task consists on\nmodelling the semantic content of these documents using ontology. The ontology\nis built taking into account the most significant components of resumes\ninspired from the structure of EUROPASS CV. This ontology is thereafter used to\nannotate automatically the resumes."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1211.6159v1", 
    "title": "A semantic association page rank algorithm for web search engines", 
    "arxiv-id": "1211.6159v1", 
    "author": "Manuel Rojas", 
    "publish": "2012-11-26T23:16:02Z", 
    "summary": "The majority of Semantic Web search engines retrieve information by focusing\non the use of concepts and relations restricted to the query provided by the\nuser. By trying to guess the implicit meaning between these concepts and\nrelations, probabilities are calculated to give the pages a score for ranking.\nIn this study, I propose a relation-based page rank algorithm to be used as a\nSemantic Web search engine. Relevance is measured as the probability of finding\nthe connections made by the user at the time of the query, as well as the\ninformation contained in the base knowledge of the Semantic Web environment. By\nthe use of \"virtual links\" between the concepts in a page, which are obtained\nfrom the knowledge base, we can connect concepts and components of a page and\nincrease the probability score for a better ranking. By creating these\nconnections, this study also looks to eliminate the possibility of getting\nresults equal to zero, and to provide a tie-breaker solution when two or more\npages obtain the same score."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1212.2065v1", 
    "title": "A Survey on Information Retrieval, Text Categorization, and Web Crawling", 
    "arxiv-id": "1212.2065v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-12-10T13:57:59Z", 
    "summary": "This paper is a survey discussing Information Retrieval concepts, methods,\nand applications. It goes deep into the document and query modelling involved\nin IR systems, in addition to pre-processing operations such as removing stop\nwords and searching by synonym techniques. The paper also tackles text\ncategorization along with its application in neural networks and machine\nlearning. Finally, the architecture of web crawlers is to be discussed shedding\nthe light on how internet spiders index web documents and how they allow users\nto search for items on the web."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1212.2150v1", 
    "title": "Collaborative Competitive filtering II: Optimal Recommendation and   Collaborative Games", 
    "arxiv-id": "1212.2150v1", 
    "author": "Shuang-Hong Yang", 
    "publish": "2012-12-10T17:57:07Z", 
    "summary": "Recommender systems have emerged as a new weapon to help online firms to\nrealize many of their strategic goals (e.g., to improve sales, revenue,\ncustomer experience etc.). However, many existing techniques commonly approach\nthese goals by seeking to recover preference (e.g., estimating ratings) in a\nmatrix completion framework. This paper aims to bridge this significant gap\nbetween the clearly-defined strategic objectives and the not-so-well-justified\nproxy.\n  We show it is advantageous to think of a recommender system as an analogy to\na monopoly economic market with the system as the sole seller, users as the\nbuyers and items as the goods. This new perspective motivates a game-theoretic\nformulation for recommendation that enables us to identify the optimal\nrecommendation policy by explicit optimizing certain strategic goals. In this\npaper, we revisit and extend our prior work, the Collaborative-Competitive\nFiltering preference model, towards a game-theoretic framework. The proposed\nframework consists of two components. First, a conditional preference model\nthat characterizes how a user would respond to a recommendation action; Second,\nknowing in advance how the user would respond, how a recommender system should\nact (i.e., recommend) strategically to maximize its goals. We show how\nobjectives such as click-through rate, sales revenue and consumption diversity\ncan be optimized explicitly in this framework. Experiments are conducted on a\ncommercial recommender system and demonstrate promising results."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.98.6", 
    "link": "http://arxiv.org/pdf/1212.2478v1", 
    "title": "Preference-based Graphic Models for Collaborative Filtering", 
    "arxiv-id": "1212.2478v1", 
    "author": "ChengXiang Zhai", 
    "publish": "2012-10-19T15:06:09Z", 
    "summary": "Collaborative filtering is a very useful general technique for exploiting the\npreference patterns of a group of users to predict the utility of items to a\nparticular user. Previous research has studied several probabilistic graphic\nmodels for collaborative filtering with promising results. However, while these\nmodels have succeeded in capturing the similarity among users and items in one\nway or the other, none of them has considered the fact that users with similar\ninterests in items can have very different rating patterns; some users tend to\nassign a higher rating to all items than other users. In this paper, we propose\nand study of two new graphic models that address the distinction between user\npreferences and ratings. In one model, called the decoupled model, we introduce\ntwo different variables to decouple a users preferences FROM his ratings. IN\nthe other, called the preference model, we model the orderings OF items\npreferred BY a USER, rather than the USERs numerical ratings of items.\nEmpirical study over two datasets of movie ratings shows that appropriate\nmodeling of the distinction between user preferences and ratings improves the\nperformance substantially and consistently. Specifically, the proposed\ndecoupled model outperforms all five existing approaches that we compare with\nsignificantly, but the preference model is not very successful. These results\nsuggest that explicit modeling of the underlying user preferences is very\nimportant for collaborative filtering, but we can not afford ignoring the\nrating information completely."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICMCS.2012.6320318", 
    "link": "http://arxiv.org/pdf/1212.2587v1", 
    "title": "An ontology-based approach for semantics ranking of the web search   engines results", 
    "arxiv-id": "1212.2587v1", 
    "author": "Bich-Li\u00ean Doan", 
    "publish": "2012-12-11T18:58:13Z", 
    "summary": "This work falls in the areas of information retrieval and semantic web, and\naims to improve the evaluation of web search tools. Indeed, the huge number of\ninformation on the web as well as the growth of new inexperienced users creates\nnew challenges for information retrieval; certainly the current search engines\n(such as Google, Bing and Yahoo) offer an efficient way to browse the web\ncontent. However, this type of tool does not take into account the semantic\ndriven by the query terms and document words. This paper proposes a new\nsemantic based approach for the evaluation of information retrieval systems;\nthe goal is to increase the selectivity of search tools and to improve how\nthese tools are evaluated. The test of the proposed approach for the evaluation\nof search engines has proved its applicability to real search tools. The\nresults showed that semantic evaluation is a promising way to improve the\nperformance and behavior of search engines as well as the relevance of the\nresults that they return."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICMCS.2012.6320318", 
    "link": "http://arxiv.org/pdf/1212.3906v1", 
    "title": "Simple Search Engine Model: Adaptive Properties", 
    "arxiv-id": "1212.3906v1", 
    "author": "Mahyuddin K. M. Nasution", 
    "publish": "2012-12-17T07:13:50Z", 
    "summary": "In this paper we study the relationship between query and search engine by\nexploring the adaptive properties based on a simple search engine. We used set\ntheory and utilized the words and terms for defining singleton space of event\nin a search engine model, and then provided the inclusion between one singleton\nto another."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICMCS.2012.6320318", 
    "link": "http://arxiv.org/pdf/1212.3964v1", 
    "title": "Advanced Bloom Filter Based Algorithms for Efficient Approximate Data   De-Duplication in Streams", 
    "arxiv-id": "1212.3964v1", 
    "author": "Souvik Bhattacherjee", 
    "publish": "2012-12-17T11:47:09Z", 
    "summary": "Applications involving telecommunication call data records, web pages, online\ntransactions, medical records, stock markets, climate warning systems, etc.,\nnecessitate efficient management and processing of such massively exponential\namount of data from diverse sources. De-duplication or Intelligent Compression\nin streaming scenarios for approximate identification and elimination of\nduplicates from such unbounded data stream is a greater challenge given the\nreal-time nature of data arrival. Stable Bloom Filters (SBF) addresses this\nproblem to a certain extent. .\n  In this work, we present several novel algorithms for the problem of\napproximate detection of duplicates in data streams. We propose the Reservoir\nSampling based Bloom Filter (RSBF) combining the working principle of reservoir\nsampling and Bloom Filters. We also present variants of the novel Biased\nSampling based Bloom Filter (BSBF) based on biased sampling concepts. We also\npropose a randomized load balanced variant of the sampling Bloom Filter\napproach to efficiently tackle the duplicate detection. In this work, we thus\nprovide a generic framework for de-duplication using Bloom Filters. Using\ndetailed theoretical analysis we prove analytical bounds on the false positive\nrate, false negative rate and convergence rate of the proposed structures. We\nexhibit that our models clearly outperform the existing methods. We also\ndemonstrate empirical analysis of the structures using real-world datasets (3\nmillion records) and also with synthetic datasets (1 billion records) capturing\nvarious input distributions."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35326-0_44", 
    "link": "http://arxiv.org/pdf/1212.5331v2", 
    "title": "Adapting Voting Techniques for Online Forum Thread Retrieval", 
    "arxiv-id": "1212.5331v2", 
    "author": "Naomie Salim", 
    "publish": "2012-12-21T04:22:13Z", 
    "summary": "Online forums or message boards are rich knowledge-based communities. In\nthese communities, thread retrieval is an essential tool facilitating\ninformation access. However, the issue on thread search is how to combine\nevidence from text units(messages) to estimate thread relevance. In this paper,\nwe first rank a list of messages, then we score threads by aggregating their\nranked messages' scores. To aggregate the message scores, we adopt several\nvoting techniques that have been applied in ranking aggregates tasks such as\nblog distillation and expert finding. The experimental result shows that many\nvoting techniques should be preferred over a baseline that treats a thread as a\nconcatenation of its message texts."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35326-0_44", 
    "link": "http://arxiv.org/pdf/1212.5442v1", 
    "title": "\u00c9tude compar\u00e9e de quatre logiciels de gestion de r\u00e9f\u00e9rences   bibliographiques libres ou gratuits", 
    "arxiv-id": "1212.5442v1", 
    "author": "Claire Scopsi", 
    "publish": "2012-12-21T14:00:23Z", 
    "summary": "This article is the result of the analysis of various bibliographic reference\nmanagement tools, especially those that are free. The use of editorial tools by\nbibliographic editors has evolved rapidly since 2007. But, until recently, free\nsoftware has fallen short when it comes to ergonomics or use. The functional\nand technical panorama offered by free software is the result of the comparison\nof JabRef, Mendeley Desktop, BibDesk and Zotero software undertaken in January\n2012 by two research professors affiliated with the Institut national\nfran\\c{c}ais des techniques de la documentation (INTD)."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35452-6_31", 
    "link": "http://arxiv.org/pdf/1212.5590v1", 
    "title": "Online Forum Thread Retrieval using Pseudo Cluster Selection and Voting   Techniques", 
    "arxiv-id": "1212.5590v1", 
    "author": "Naomie Salim", 
    "publish": "2012-12-21T07:21:06Z", 
    "summary": "Online forums facilitate knowledge seeking and sharing on the Web. However,\nthe shared knowledge is not fully utilized due to information overload. Thread\nretrieval is one method to overcome information overload. In this paper, we\npropose a model that combines two existing approaches: the Pseudo Cluster\nSelection and the Voting Techniques. In both, a retrieval system first scores a\nlist of messages and then ranks threads by aggregating their scored messages.\nThey differ on what and how to aggregate. The pseudo cluster selection focuses\non input, while voting techniques focus on the aggregation method. Our combined\nmodels focus on the input and the aggregation methods. The result shows that\nsome combined models are statistically superior to baseline methods."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35452-6_31", 
    "link": "http://arxiv.org/pdf/1212.5633v1", 
    "title": "Design, implementation and experiment of a YeSQL Web Crawler", 
    "arxiv-id": "1212.5633v1", 
    "author": "Fran\u00e7oise Para", 
    "publish": "2012-12-21T23:20:24Z", 
    "summary": "We describe a novel, \"focusable\", scalable, distributed web crawler based on\nGNU/Linux and PostgreSQL that we designed to be easily extendible and which we\nhave released under a GNU public licence. We also report a first use case\nrelated to an analysis of Twitter's streams about the french 2012 presidential\nelections and the URL's it contains."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35452-6_31", 
    "link": "http://arxiv.org/pdf/1212.5650v1", 
    "title": "Learning the Gain Values and Discount Factors of DCG", 
    "arxiv-id": "1212.5650v1", 
    "author": "Yong Yu", 
    "publish": "2012-12-22T03:33:12Z", 
    "summary": "Evaluation metrics are an essential part of a ranking system, and in the past\nmany evaluation metrics have been proposed in information retrieval and Web\nsearch. Discounted Cumulated Gains (DCG) has emerged as one of the evaluation\nmetrics widely adopted for evaluating the performance of ranking functions used\nin Web search. However, the two sets of parameters, gain values and discount\nfactors, used in DCG are determined in a rather ad-hoc way. In this paper we\nfirst show that DCG is generally not coherent, meaning that comparing the\nperformance of ranking functions using DCG very much depends on the particular\ngain values and discount factors used. We then propose a novel methodology that\ncan learn the gain values and discount factors from user preferences over\nrankings. Numerical simulations illustrate the effectiveness of our proposed\nmethods. Please contact the authors for the full version of this work."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35452-6_31", 
    "link": "http://arxiv.org/pdf/1212.6193v1", 
    "title": "Learning Joint Query Interpretation and Response Ranking", 
    "arxiv-id": "1212.6193v1", 
    "author": "Soumen Chakrabarti", 
    "publish": "2012-12-26T15:28:27Z", 
    "summary": "Thanks to information extraction and semantic Web efforts, search on\nunstructured text is increasingly refined using semantic annotations and\nstructured knowledge bases. However, most users cannot become familiar with the\nschema of knowledge bases and ask structured queries. Interpreting free-format\nqueries into a more structured representation is of much current interest. The\ndominant paradigm is to segment or partition query tokens by purpose\n(references to types, entities, attribute names, attribute values, relations)\nand then launch the interpreted query on structured knowledge bases. Given that\nstructured knowledge extraction is never complete, here we use a data\nrepresentation that retains the unstructured text corpus, along with structured\nannotations (mentions of entities and relationships) on it. We propose two new,\nnatural formulations for joint query interpretation and response ranking that\nexploit bidirectional flow of information between the knowledge base and the\ncorpus.One, inspired by probabilistic language models, computes expected\nresponse scores over the uncertainties of query interpretation. The other is\nbased on max-margin discriminative learning, with latent variables representing\nthose uncertainties. In the context of typed entity search, both formulations\nbridge a considerable part of the accuracy gap between a generic query that\ndoes not constrain the type at all, and the upper bound where the \"perfect\"\ntarget entity type of each query is provided by humans. Our formulations are\nalso superior to a two-stage approach of first choosing a target type using\nrecent query type prediction techniques, and then launching a type-restricted\nentity search query."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-642-35452-6_31", 
    "link": "http://arxiv.org/pdf/1302.1178v1", 
    "title": "Overview of EIREX 2012: Social Media", 
    "arxiv-id": "1302.1178v1", 
    "author": "Jorge Morato", 
    "publish": "2013-02-05T20:20:30Z", 
    "summary": "The third Information Retrieval Education through EXperimentation track\n(EIREX 2012) was run at the University Carlos III of Madrid, during the 2012\nspring semester. EIREX 2012 is the third in a series of experiments designed to\nfoster new Information Retrieval (IR) education methodologies and resources,\nwith the specific goal of teaching undergraduate IR courses from an\nexperimental perspective. For an introduction to the motivation behind the\nEIREX experiments, see the first sections of [Urbano et al., 2011a]. For\ninformation on other editions of EIREX and related data, see the website at\nhttp://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)\nto help students get a view of the Information Retrieval process as they would\nfind it in a real-world scenario, either industrial or academic; b) to make\nstudents realize the importance of laboratory experiments in Computer Science\nand have them initiated in their execution and analysis; c) to create a public\nrepository of resources to teach Information Retrieval courses; d) to seek the\ncollaboration and active participation of other Universities in this endeavor.\nThis overview paper summarizes the results of the EIREX 2012 track, focusing on\nthe creation of the test collection and the analysis to assess its reliability."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.1335v1", 
    "title": "Ontology Guided Information Extraction from Unstructured Text", 
    "arxiv-id": "1302.1335v1", 
    "author": "S Rajagopalan", 
    "publish": "2013-02-06T12:19:43Z", 
    "summary": "In this paper, we describe an approach to populate an existing ontology with\ninstance information present in the natural language text provided as input. An\nontology is defined as an explicit conceptualization of a shared domain. This\napproach starts with a list of relevant domain ontologies created by human\nexperts, and techniques for identifying the most appropriate ontology to be\nextended with information from a given text. Then we demonstrate heuristics to\nextract information from the unstructured text and for adding it as structured\ninformation to the selected ontology. This identification of the relevant\nontology is critical, as it is used in identifying relevant information in the\ntext. We extract information in the form of semantic triples from the text,\nguided by the concepts in the ontology. We then convert the extracted\ninformation about the semantic class instances into Resource Description\nFramework (RDF3) and append it to the existing domain ontology. This enables us\nto perform more precise semantic queries over the semantic triple store thus\ncreated. We have achieved 95% accuracy of information extraction in our\nimplementation."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.1596v3", 
    "title": "Tag-based Semantic Website Recommendation for Turkish Language", 
    "arxiv-id": "1302.1596v3", 
    "author": "Onur Y\u0131lmaz", 
    "publish": "2013-02-06T22:02:31Z", 
    "summary": "With the dramatic increase in the number of websites on the internet, tagging\nhas become popular for finding related, personal and important documents. When\nthe potentially increasing internet markets are analyzed, Turkey, in which most\nof the people use Turkish language on the internet, found to be exponentially\nincreasing. In this paper, a tag-based website recommendation method is\npresented, where similarity measures are combined with semantic relationships\nof tags. In order to evaluate the system, an experiment with 25 people from\nTurkey is undertaken and participants are firstly asked to provide websites and\ntags in Turkish and then they are asked to evaluate recommended websites."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.2318v1", 
    "title": "On Search Engine Evaluation Metrics", 
    "arxiv-id": "1302.2318v1", 
    "author": "Pavel Sirotkin", 
    "publish": "2013-02-10T10:50:56Z", 
    "summary": "The search engine evaluation research has quite a lot metrics available to\nit. Only recently, the question of the significance of individual metrics\nstarted being raised, as these metrics' correlations to real-world user\nexperiences or performance have generally not been well-studied. The first part\nof this thesis provides an overview of previous literature on the evaluation of\nsearch engine evaluation metrics themselves, as well as critiques of and\ncomments on individual studies and approaches. The second part introduces a\nmeta-evaluation metric, the Preference Identification Ratio (PIR), that\nquantifies the capacity of an evaluation metric to capture users' preferences.\nAlso, a framework for simultaneously evaluating many metrics while varying\ntheir parameters and evaluation standards is introduced. Both PIR and the\nmeta-evaluation framework are tested in a study which shows some interesting\npreliminary results; in particular, the unquestioning adherence to metrics or\ntheir ad hoc parameters seems to be disadvantageous. Instead, evaluation\nmethods should themselves be rigorously evaluated with regard to goals set for\na particular study."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.4916v1", 
    "title": "Stacking from Tags: Clustering Bookmarks around a Theme", 
    "arxiv-id": "1302.4916v1", 
    "author": "Raquel Mart\u00ednez", 
    "publish": "2013-02-20T14:38:38Z", 
    "summary": "Since very recently, users on the social bookmarking service Delicious can\nstack web pages in addition to tagging them. Stacking enables users to group\nweb pages around specific themes with the aim of recommending to others.\nHowever, users still stack a small subset of what they tag, and thus many web\npages remain unstacked. This paper presents early research towards\nautomatically clustering web pages from tags to find stacks and extend\nrecommendations."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.6340v1", 
    "title": "A Fuzzy Logic based Method for Efficient Retrieval of Vague and   Uncertain Spatial Expressions in Text Exploiting the Granulation of the   Spatial Event Queries", 
    "arxiv-id": "1302.6340v1", 
    "author": "Raja. K", 
    "publish": "2013-02-26T06:51:33Z", 
    "summary": "The arrangement of things in n-dimensional space is specified as Spatial.\nSpatial data consists of values that denote the location and shape of objects\nand areas on the earths surface. Spatial information includes facts such as\nlocation of features, the relationship of geographic features and measurements\nof geographic features. The spatial cognition is a primal area of study in\nvarious other fields such as Robotics, Psychology, Geosciences, Geography,\nPolitical Sciences, Geographic Economy, Environmental, Mining and Petroleum\nEngineering, Natural Resources, Epidemiology, Demography etc., Any text\ndocument which contains physical location specifications such as place names,\ngeographic coordinates, landmarks, country names etc., are supposed to contain\nthe spatial information. The spatial information may also be represented using\nvague or fuzzy descriptions involving linguistic terms such as near to, far\nfrom, to the east of, very close. Given a query involving events, the aim of\nthis ongoing research work is to extract the relevant information from multiple\ntext documents, resolve the uncertainty and vagueness and translate them in to\nlocations in a map. The input to the system would be a text Corpus and a\nSpatial Query event. The output of the system is a map showing the most\npossible, disambiguated location of the event queried. The author proposes\nFuzzy Logic Techniques for resolving the uncertainty in the spatial\nexpressions."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.6580v1", 
    "title": "Finding the Right Set of Users: Generalized Constraints for Group   Recommendations", 
    "arxiv-id": "1302.6580v1", 
    "author": "Evaggelia Pitoura", 
    "publish": "2013-02-26T20:55:42Z", 
    "summary": "Recently, group recommendations have attracted considerable attention. Rather\nthan recommending items to individual users, group recommenders recommend items\nto groups of users. In this position paper, we introduce the problem of forming\nan appropriate group of users to recommend an item when constraints apply to\nthe members of the group. We present a formal model of the problem and an\nalgorithm for its solution. Finally, we identify several directions for future\nwork."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1302.7131v1", 
    "title": "Presence Factor-Oriented Blog Summarization", 
    "arxiv-id": "1302.7131v1", 
    "author": "Ashutosh Dixit", 
    "publish": "2013-02-28T09:59:54Z", 
    "summary": "The research that has been carried out on blogs focused on blog posts only,\nignoring the title of the blog page. Also, in summarization only a set of\nrepresentative sentences are extracted. Some analysis has been done and it has\nbeen found that the blog post contains the content that is likely to be related\nto the topic of the blog post. Thus, proposed system of summarization makes use\nof title contained in a blog page. The approach makes use of the Presence\nfactor that indicates the presence of each term of the title in each sentence\nof the blog post. This is a key feature because it considers those sentences as\nmore relevant for summarization that contain each of the term present in the\ntitle. The system has been implemented and evaluated experimentally. The system\nhas shown promising results."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.0407v1", 
    "title": "IRS for Computer Character Sequences Filtration: a new software tool and   algorithm to support the IRS at tokenization process", 
    "arxiv-id": "1303.0407v1", 
    "author": "Qasem Abu Al-Haija", 
    "publish": "2013-03-02T17:43:38Z", 
    "summary": "Tokenization is the task of chopping it up into pieces, called tokens,\nperhaps at the same time throwing away certain characters, such as punctuation.\nA token is an instance of token a sequence of characters in some particular\ndocument that are grouped together as a useful semantic unit for processing.\nNew software tool and algorithm to support the IRS at tokenization process are\npresented. Our proposed tool will filter out the three computer character\nSequences: IP-Addresses, Web URLs, Date, and Email Addresses. Our tool will use\nthe pattern matching algorithms and filtration methods. After this process, the\nIRS can start a new tokenization process on the new retrieved text which will\nbe free of these sequences."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.0481v2", 
    "title": "Situation-Aware Approach to Improve Context-based Recommender System", 
    "arxiv-id": "1303.0481v2", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-03-03T09:32:44Z", 
    "summary": "In this paper, we introduce a novel situation aware approach to improve a\ncontext based recommender system. To build situation aware user profiles, we\nrely on evidence issued from retrieval situations. A retrieval situation refers\nto the social spatio temporal context of the user when he interacts with the\nrecommender system. A situation is represented as a combination of social\nspatio temporal concepts inferred from ontological knowledge given social\ngroup, location and time information. User's interests are inferred from past\nuser's interaction with the recommender system related to the identified\nsituations. They are represented using concepts issued from a domain ontology.\nWe also propose a method to dynamically adapt the system to the user's\ninterest's evolution."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.0485v2", 
    "title": "Optimizing an Utility Function for Exploration / Exploitation Trade-off   in Context-Aware Recommender System", 
    "arxiv-id": "1303.0485v2", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-03-03T10:23:38Z", 
    "summary": "In this paper, we develop a dynamic exploration/ exploitation (exr/exp)\nstrategy for contextual recommender systems (CRS). Specifically, our methods\ncan adaptively balance the two aspects of exr/exp by automatically learning the\noptimal tradeoff. This consists of optimizing a utility function represented by\na linearized form of the probability distributions of the rewards of the\nclicked and the non-clicked documents already recommended. Within an offline\nsimulation framework we apply our algorithms to a CRS and conduct an evaluation\nwith real event log data. The experimental results and detailed analysis\ndemonstrate that our algorithms outperform existing algorithms in terms of\nclick-through-rate (CTR)."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.0566v1", 
    "title": "Arabic documents classification using fuzzy R.B.F. classifier with   sliding window", 
    "arxiv-id": "1303.0566v1", 
    "author": "A. Ennaji", 
    "publish": "2013-03-03T20:50:12Z", 
    "summary": "In this paper, we propose a system for contextual and semantic Arabic\ndocuments classification by improving the standard fuzzy model. Indeed,\npromoting neighborhood semantic terms that seems absent in this model by using\na radial basis modeling. In order to identify the relevant documents to the\nquery. This approach calculates the similarity between related terms by\ndetermining the relevance of each relative to documents (NEAR operator), based\non a kernel function. The use of sliding window improves the process of\nclassification. The results obtained on a arabic dataset of press show very\ngood performance compared with the literature."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.0667v1", 
    "title": "Query Expansion Using Term Distribution and Term Association", 
    "arxiv-id": "1303.0667v1", 
    "author": "Kalyankumar Datta", 
    "publish": "2013-03-04T10:34:41Z", 
    "summary": "Good term selection is an important issue for an automatic query expansion\n(AQE) technique. AQE techniques that select expansion terms from the target\ncorpus usually do so in one of two ways. Distribution based term selection\ncompares the distribution of a term in the (pseudo) relevant documents with\nthat in the whole corpus / random distribution. Two well-known\ndistribution-based methods are based on Kullback-Leibler Divergence (KLD) and\nBose-Einstein statistics (Bo1). Association based term selection, on the other\nhand, uses information about how a candidate term co-occurs with the original\nquery terms. Local Context Analysis (LCA) and Relevance-based Language Model\n(RM3) are examples of association-based methods. Our goal in this study is to\ninvestigate how these two classes of methods may be combined to improve\nretrieval effectiveness. We propose the following combination-based approach.\nCandidate expansion terms are first obtained using a distribution based method.\nThis set is then refined based on the strength of the association of terms with\nthe original query terms. We test our methods on 11 TREC collections. The\nproposed combinations generally yield better results than each individual\nmethod, as well as other state-of-the-art AQE approaches. En route to our\nprimary goal, we also propose some modifications to LCA and Bo1 which lead to\nimproved performance."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.1695v1", 
    "title": "Effect of Query Formation on Web Search Engine Results", 
    "arxiv-id": "1303.1695v1", 
    "author": "Ila Pant Bisht", 
    "publish": "2013-03-07T14:15:19Z", 
    "summary": "Query in a search engine is generally based on natural language. A query can\nbe expressed in more than one way without changing its meaning as it depends on\nthinking of human being at a particular moment. Aim of the searcher is to get\nmost relevant results immaterial of how the query has been expressed. In the\npresent paper, we have examined the results of search engine for change in\ncoverage and similarity of first few results when a query is entered in two\nsemantically same but in different formats. Searching has been made through\nGoogle search engine. Fifteen pairs of queries have been chosen for the study.\nThe t-test has been used for the purpose and the results have been checked on\nthe basis of total documents found, similarity of first five and first ten\ndocuments found in the results of a query entered in two different formats. It\nhas been found that the total coverage is same but first few results are\nsignificantly different."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.2156v1", 
    "title": "The Powerful Model Adpredictor for Search Engine Switching Detection   Challenge", 
    "arxiv-id": "1303.2156v1", 
    "author": "Daniel Zeng", 
    "publish": "2013-03-09T02:01:04Z", 
    "summary": "The purpose of the Switching Detection Challenge in the 2013 WSCD workshop\nwas to predict users' search engine swithcing actions given records about\nsearch sessions and logs.Our solution adopted the powerful prediction model\nAdpredictor and utilized the method of feature engineering. We successfully\napplied the click through rate (CTR) prediction model Adpredicitor into our\nsolution framework, and then the discovery of effective features and the\nmultiple classification of different switching type make our model outperforms\nmany competitors. We achieved an AUC score of 0.84255 on the private\nleaderboard and ranked the 5th among all the competitors in the competition."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.2277v1", 
    "title": "Is Learning to Rank Worth It? A Statistical Analysis of Learning to Rank   Methods", 
    "arxiv-id": "1303.2277v1", 
    "author": "Marcos Andr\u00e9 Gon\u00e7alves", 
    "publish": "2013-03-09T23:28:16Z", 
    "summary": "The Learning to Rank (L2R) research field has experienced a fast paced growth\nover the last few years, with a wide variety of benchmark datasets and\nbaselines available for experimentation. We here investigate the main\nassumption behind this field, which is that, the use of sophisticated L2R\nalgorithms and models, produce significant gains over more traditional and\nsimple information retrieval approaches. Our experimental results surprisingly\nindicate that many L2R algorithms, when put up against the best individual\nfeatures of each dataset, may not produce statistically significant\ndifferences, even if the absolute gains may seem large. We also find that most\nof the reported baselines are statistically tied, with no clear winner."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.2308v2", 
    "title": "Improving adaptation of ubiquitous recommander systems by using   reinforcement learning and collaborative filtering", 
    "arxiv-id": "1303.2308v2", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-03-10T11:02:24Z", 
    "summary": "The wide development of mobile applications provides a considerable amount of\ndata of all types (images, texts, sounds, videos, etc.). Thus, two main issues\nhave to be considered: assist users in finding information and reduce search\nand navigation time. In this sense, context-based recommender systems (CBRS)\npropose the user the adequate information depending on her/his situation. Our\nwork consists in applying machine learning techniques and reasoning process in\norder to bring a solution to some of the problems concerning the acceptance of\nrecommender systems by users, namely avoiding the intervention of experts,\nreducing cold start problem, speeding learning process and adapting to the\nuser's interest. To achieve this goal, we propose a fundamental modification in\nterms of how we model the learning of the CBRS. Inspired by models of human\nreasoning developed in robotic, we combine reinforcement learning and\ncase-based reasoning to define a contextual recommendation process based on\ndifferent context dimensions (cognitive, social, temporal, geographic). This\npaper describes an ongoing work on the implementation of a CBRS based on a\nhybrid Q-learning (HyQL) algorithm which combines Q-learning, collaborative\nfiltering and case-based reasoning techniques. It also presents preliminary\nresults by comparing HyQL and the standard Q-Learning w.r.t. solving the cold\nstart problem."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.2438v2", 
    "title": "A Taxonomy of Hyperlink Hiding Techniques", 
    "arxiv-id": "1303.2438v2", 
    "author": "Chi-Jie Meng", 
    "publish": "2013-03-11T06:49:38Z", 
    "summary": "Hidden links are designed solely for search engines rather than visitors. To\nget high search engine rankings, link hiding techniques are usually used for\nthe profitability of black industries, such as illicit game servers, false\nmedical services, illegal gambling, and less attractive high-profit industry,\netc. This paper investigates hyperlink hiding techniques on the Web, and gives\na detailed taxonomy. We believe the taxonomy can help develop appropriate\ncountermeasures. Study on 5,583,451 Chinese sites' home pages indicate that\nlink hidden techniques are very prevalent on the Web. We also tried to explore\nthe attitude of Google towards link hiding spam by analyzing the PageRank\nvalues of relative links. The results show that more should be done to punish\nthe hidden link spam."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.3164v1", 
    "title": "Features and Aggregators for Web-scale Entity Search", 
    "arxiv-id": "1303.3164v1", 
    "author": "Soumen Chakrabarti", 
    "publish": "2013-03-13T14:06:49Z", 
    "summary": "We focus on two research issues in entity search: scoring a document or\nsnippet that potentially supports a candidate entity, and aggregating scores\nfrom different snippets into an entity score. Proximity scoring has been\nstudied in IR outside the scope of entity search. However, aggregation has been\nhardwired except in a few cases where probabilistic language models are used.\nWe instead explore simple, robust, discriminative ranking algorithms, with\ninformative snippet features and broad families of aggregation functions. Our\nfirst contribution is a study of proximity-cognizant snippet features. In\ncontrast with prior work which uses hardwired \"proximity kernels\" that\nimplement a fixed decay with distance, we present a \"universal\" feature\nencoding which jointly expresses the perplexity (informativeness) of a query\nterm match and the proximity of the match to the entity mention. Our second\ncontribution is a study of aggregation functions. Rather than train the ranking\nalgorithm on snippets and then aggregate scores, we directly train on entities\nsuch that the ranking algorithm takes into account the aggregation function\nbeing used. Our third contribution is an extensive Web-scale evaluation of the\nabove algorithms on two data sets having quite different properties and\nbehavior. The first one is the W3C dataset used in TREC-scale enterprise\nsearch, with pre-annotated entity mentions. The second is a Web-scale\nopen-domain entity search dataset consisting of 500 million Web pages, which\ncontain about 8 billion token spans annotated automatically with two million\nentities from 200,000 entity types in Wikipedia. On the TREC dataset, the\nperformance of our system is comparable to the currently prevalent systems. On\nthe much larger and noisier Web dataset, our system delivers significantly\nbetter performance than all other systems, with 8% MAP improvement over the\nclosest competitor."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.3964v1", 
    "title": "Simple Search Engine Model: Selective Properties", 
    "arxiv-id": "1303.3964v1", 
    "author": "Mahyuddin K. M. Nasution", 
    "publish": "2013-03-16T10:21:33Z", 
    "summary": "In this paper we study the relationship between query and search engine by\nexploring the selective properties based on a simple search engine. We used the\nset theory and utilized the words and terms for defining singleton and\ndoubleton in the event spaces and then provided their implementation for\nproving the existence of the shadow of micro-cluster."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.4087v1", 
    "title": "An improved semantic similarity measure for document clustering based on   topic maps", 
    "arxiv-id": "1303.4087v1", 
    "author": "Mohammad Shahid Shaikh", 
    "publish": "2013-03-17T18:28:02Z", 
    "summary": "A major computational burden, while performing document clustering, is the\ncalculation of similarity measure between a pair of documents. Similarity\nmeasure is a function that assigns a real number between 0 and 1 to a pair of\ndocuments, depending upon the degree of similarity between them. A value of\nzero means that the documents are completely dissimilar whereas a value of one\nindicates that the documents are practically identical. Traditionally,\nvector-based models have been used for computing the document similarity. The\nvector-based models represent several features present in documents. These\napproaches to similarity measures, in general, cannot account for the semantics\nof the document. Documents written in human languages contain contexts and the\nwords used to describe these contexts are generally semantically related.\nMotivated by this fact, many researchers have proposed seman-tic-based\nsimilarity measures by utilizing text annotation through external thesauruses\nlike WordNet (a lexical database). In this paper, we define a semantic\nsimilarity measure based on documents represented in topic maps. Topic maps are\nrapidly becoming an industrial standard for knowledge representation with a\nfocus for later search and extraction. The documents are transformed into a\ntopic map based coded knowledge and the similarity between a pair of documents\nis represented as a correlation between the common patterns (sub-trees). The\nexperimental studies on the text mining datasets reveal that this new\nsimilarity measure is more effective as compared to commonly used similarity\nmeasures in text clustering."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1303.5250v1", 
    "title": "Iterative Expectation for Multi Period Information Retrieval", 
    "arxiv-id": "1303.5250v1", 
    "author": "Jun Wang", 
    "publish": "2013-03-21T13:11:24Z", 
    "summary": "Many Information Retrieval (IR) models make use of offline statistical\ntechniques to score documents for ranking over a single period, rather than use\nan online, dynamic system that is responsive to users over time. In this paper,\nwe explicitly formulate a general Multi Period Information Retrieval problem,\nwhere we consider retrieval as a stochastic yet controllable process. The\nranking action during the process continuously controls the retrieval system's\ndynamics, and an optimal ranking policy is found in order to maximise the\noverall users' satisfaction over the multiple periods as much as possible. Our\nderivations show interesting properties about how the posterior probability of\nthe documents relevancy evolves from users feedbacks through clicks, and\nprovides a plug-in framework for incorporating different click models. Based on\nthe Multi-Armed Bandit theory, we propose a simple implementation of our\nframework using a dynamic ranking rule that takes rank bias and exploration of\ndocuments into account. We use TREC data to learn a suitable exploration\nparameter for our model, and then analyse its performance and a number of\nvariants using a search log data set; the experiments suggest an ability to\nexplore document relevance dynamically over time using user feedback in a way\nthat can handle rank bias."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwest.2013.4102", 
    "link": "http://arxiv.org/pdf/1308.2354v1", 
    "title": "RAProp: Ranking Tweets by Exploiting the Tweet/User/Web Ecosystem and   Inter-Tweet Agreement", 
    "arxiv-id": "1308.2354v1", 
    "author": "Subbarao Kambhampati", 
    "publish": "2013-08-11T00:56:59Z", 
    "summary": "The increasing popularity of Twitter renders improved trustworthiness and\nrelevance assessment of tweets much more important for search. However, given\nthe limitations on the size of tweets, it is hard to extract measures for\nranking from the tweets' content alone. We present a novel ranking method,\ncalled RAProp, which combines two orthogonal measures of relevance and\ntrustworthiness of a tweet. The first, called Feature Score, measures the\ntrustworthiness of the source of the tweet. This is done by extracting features\nfrom a 3-layer twitter ecosystem, consisting of users, tweets and the pages\nreferred to in the tweets. The second measure, called agreement analysis,\nestimates the trustworthiness of the content of the tweet, by analyzing how and\nwhether the content is independently corroborated by other tweets. We view the\ncandidate result set of tweets as the vertices of a graph, with the edges\nmeasuring the estimated agreement between each pair of tweets. The feature\nscore is propagated over this agreement graph to compute the top-k tweets that\nhave both trustworthy sources and independent corroboration. The evaluation of\nour method on 16 million tweets from the TREC 2011 Microblog Dataset shows that\nfor top-30 precision we achieve 53% higher than current best performing method\non the Dataset and over 300% over current Twitter Search. We also present a\ndetailed internal empirical evaluation of RAProp in comparison to several\nalternative approaches proposed by us."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1308.3060v1", 
    "title": "Information filtering in sparse online systems: recommendation via   semi-local diffusion", 
    "arxiv-id": "1308.3060v1", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2013-08-14T08:29:41Z", 
    "summary": "With the rapid growth of the Internet and overwhelming amount of information\nand choices that people are confronted with, recommender systems have been\ndeveloped to effectively support users' decision-making process in the online\nsystems. However, many recommendation algorithms suffer from the data sparsity\nproblem, i.e. the user-object bipartite networks are so sparse that algorithms\ncannot accurately recommend objects for users. This data sparsity problem makes\nmany well-known recommendation algorithms perform poorly. To solve the problem,\nwe propose a recommendation algorithm based on the semi-local diffusion process\non a user-object bipartite network. The numerical simulation on two sparse\ndatasets, Amazon and Bookcross, show that our method significantly outperforms\nthe state-of-the-art methods especially for those small-degree users. Two\npersonalized semi-local diffusion methods are proposed which further improve\nthe recommendation accuracy. Finally, our work indicates that sparse online\nsystems are essentially different from the dense online systems, all the\nalgorithms and conclusions based on dense data should be rechecked again in\nsparse data."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1308.4839v1", 
    "title": "Diversification Based Static Index Pruning - Application to Temporal   Collections", 
    "arxiv-id": "1308.4839v1", 
    "author": "St\u00e9phane Gan\u00e7arski", 
    "publish": "2013-08-22T12:09:54Z", 
    "summary": "Nowadays, web archives preserve the history of large portions of the web. As\nmedias are shifting from printed to digital editions, accessing these huge\ninformation sources is drawing increasingly more attention from national and\ninternational institutions, as well as from the research community. These\ncollections are intrinsically big, leading to index files that do not fit into\nthe memory and an increase query response time. Decreasing the index size is a\ndirect way to decrease this query response time.\n  Static index pruning methods reduce the size of indexes by removing a part of\nthe postings. In the context of web archives, it is necessary to remove\npostings while preserving the temporal diversity of the archive. None of the\nexisting pruning approaches take (temporal) diversification into account.\n  In this paper, we propose a diversification-based static index pruning\nmethod. It differs from the existing pruning approaches by integrating\ndiversification within the pruning context. We aim at pruning the index while\npreserving retrieval effectiveness and diversity by pruning while maximizing a\ngiven IR evaluation metric like DCG. We show how to apply this approach in the\ncontext of web archives. Finally, we show on two collections that search\neffectiveness in temporal collections after pruning can be improved using our\napproach rather than diversity oblivious approaches."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.3132v2", 
    "title": "Combination of Multiple Bipartite Ranking for Web Content Quality   Evaluation", 
    "arxiv-id": "1309.3132v2", 
    "author": "Dexian Zhang", 
    "publish": "2013-09-12T12:15:51Z", 
    "summary": "Web content quality estimation is crucial to various web content processing\napplications. Our previous work applied Bagging + C4.5 to achive the best\nresults on the ECML/PKDD Discovery Challenge 2010, which is the comibination of\nmany point-wise rankinig models. In this paper, we combine multiple pair-wise\nbipartite ranking learner to solve the multi-partite ranking problems for the\nweb quality estimation. In encoding stage, we present the ternary encoding and\nthe binary coding extending each rank value to $L - 1$ (L is the number of the\ndifferent ranking value). For the decoding, we discuss the combination of\nmultiple ranking results from multiple bipartite ranking models with the\npredefined weighting and the adaptive weighting. The experiments on ECML/PKDD\n2010 Discovery Challenge datasets show that \\textit{binary coding} +\n\\textit{predefined weighting} yields the highest performance in all four\ncombinations and furthermore it is better than the best results reported in\nECML/PKDD 2010 Discovery Challenge competition."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.3421v6", 
    "title": "Indexing by Latent Dirichlet Allocation and Ensemble Model", 
    "arxiv-id": "1309.3421v6", 
    "author": "In-Chan Choi", 
    "publish": "2013-09-13T10:37:47Z", 
    "summary": "The contribution of this paper is two-fold. First, we present Indexing by\nLatent Dirichlet Allocation (LDI), an automatic document indexing method. The\nprobability distributions in LDI utilize those in Latent Dirichlet Allocation\n(LDA), a generative topic model that has been previously used in applications\nfor document retrieval tasks. However, the ad hoc applications, or their\nvariants with smoothing techniques as prompted by previous studies in LDA-based\nlanguage modeling, result in unsatisfactory performance as the document\nrepresentations do not accurately reflect concept space. To improve\nperformance, we introduce a new definition of document probability vectors in\nthe context of LDA and present a novel scheme for automatic document indexing\nbased on LDA. Second, we propose an Ensemble Model (EnM) for document\nretrieval. The EnM combines basis indexing models by assigning different\nweights and attempts to uncover the optimal weights to maximize the Mean\nAverage Precision (MAP). To solve the optimization problem, we propose an\nalgorithm, EnM.B, which is derived based on the boosting method. The results of\nour computational experiments on benchmark data sets indicate that both the\nproposed approaches are viable options for document retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.4345v1", 
    "title": "Music Files Search System", 
    "arxiv-id": "1309.4345v1", 
    "author": "M. Pintal", 
    "publish": "2013-09-17T15:15:08Z", 
    "summary": "This paper introduces a project of advanced system of music retrieval from\nthe Internet. The system uses combination of text search (by author, title and\nother information about the music file included in id3 tag description or\nsimilar for other file types) with more intuitive and novel method of melody\nsearch using query by humming. The patterns for storing text and melody\ninformation as well as improved clustering algorithm for the pattern space were\nproposed. The search engine is planned to optimise the query due to the data\ninput by user, thanks to the structure of text and melody index database. The\nsystem is planned to be a plug-in for popular digital music players or an\nindependent player. An advanced system of recommendation based on information\ngathered from user's profile and search history is an integral part of the\nsystem. The recommendation mechanism uses scrobbling methods and is responsible\nfor making suggestions of songs unknown to the user but similar to his\npreferred music styles and positioning search results."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.4938v1", 
    "title": "Improving Query Expansion Using WordNet", 
    "arxiv-id": "1309.4938v1", 
    "author": "Kalyankumar Datta", 
    "publish": "2013-09-19T11:38:51Z", 
    "summary": "This study proposes a new way of using WordNet for Query Expansion (QE). We\nchoose candidate expansion terms, as usual, from a set of pseudo relevant\ndocuments; however, the usefulness of these terms is measured based on their\ndefinitions provided in a hand-crafted lexical resource like WordNet.\nExperiments with a number of standard TREC collections show that this method\noutperforms existing WordNet based methods. It also compares favorably with\nestablished QE methods such as KLD and RM3. Leveraging earlier work in which a\ncombination of QE methods was found to outperform each individual method (as\nwell as other well-known QE methods), we next propose a combination-based QE\nmethod that takes into account three different aspects of a candidate expansion\nterm's usefulness: (i) its distribution in the pseudo relevant documents and in\nthe target corpus, (ii) its statistical association with query terms, and (iii)\nits semantic relation with the query, as determined by the overlap between the\nWordNet definitions of the term and query terms. This combination of diverse\nsources of information appears to work well on a number of test collections,\nviz., TREC123, TREC5, TREC678, TREC robust new and TREC910 collections, and\nyields significant improvements over competing methods on most of these\ncollections."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.6908v1", 
    "title": "A Collaborative Filtering Based Approach for Recommending Elective   Courses", 
    "arxiv-id": "1309.6908v1", 
    "author": "Anuj Sharma", 
    "publish": "2013-09-26T14:15:07Z", 
    "summary": "In management education programmes today, students face a difficult time in\nchoosing electives as the number of electives available are many. As the range\nand diversity of different elective courses available for selection have\nincreased, course recommendation systems that help students in making choices\nabout courses have become more relevant. In this paper we extend the concept of\ncollaborative filtering approach to develop a course recommendation system. The\nproposed approach provides student an accurate prediction of the grade they may\nget if they choose a particular course, which will be helpful when they decide\non selecting elective courses, as grade is an important parameter for a student\nwhile deciding on an elective course. We experimentally evaluate the\ncollaborative filtering approach on a real life data set and show that the\nproposed system is effective in terms of accuracy."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1309.7517v1", 
    "title": "Improving tag recommendation by folding in more consistency", 
    "arxiv-id": "1309.7517v1", 
    "author": "Hubert Naacke", 
    "publish": "2013-09-29T01:43:40Z", 
    "summary": "Tag recommendation is a major aspect of collaborative tagging systems. It\naims to recommend tags to a user for tagging an item. In this paper we present\na part of our work in progress which is a novel improvement of recommendations\nby re-ranking the output of a tag recommender. We mine association rules\nbetween candidates tags in order to determine a more consistent list of tags to\nrecommend.\n  Our method is an add-on one which leads to better recommendations as we show\nin this paper. It is easily parallelizable and morever it may be applied to a\nlot of tag recommenders. The experiments we did on five datasets with two kinds\nof tag recommender demonstrated the efficiency of our method."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1311.0339v1", 
    "title": "A Novel Term Weighing Scheme Towards Efficient Crawl of Textual   Databases", 
    "arxiv-id": "1311.0339v1", 
    "author": "Komal Kumar Bhatia", 
    "publish": "2013-11-02T04:45:36Z", 
    "summary": "The Hidden Web is the vast repository of informational databases available\nonly through search form interfaces, accessible by therein typing a set of\nkeywords in the search forms. Typically, a Hidden Web crawler is employed to\nautonomously discover and download pages from the Hidden Web. Traditional\nhidden web crawlers do not provide the search engines with an optimal search\nexperience because of the excessive number of search requests posed through the\nform interface so as to exhaustively crawl and retrieve the contents of the\ntarget hidden web database. Here in our work, we provide a framework to\ninvestigate the problem of optimal search and curtail it by proposing an\neffective query term selection approach based on the frequency & distribution\nof terms in the document database. The paper focuses on developing a\nterm-weighing scheme called VarDF (acronym for variable document frequency)\nthat can ease the identification of optimal terms to be used as queries on the\ninterface for maximizing the achieved coverage of the crawler which in turn\nwill facilitate the search engine to have a diversified and expanded index. We\nexperimentally evaluate the effectiveness of our approach on a manually created\ndatabase of documents in the area of Information Retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0079354", 
    "link": "http://arxiv.org/pdf/1311.2103v1", 
    "title": "Idea of a new Personality-Type based Recommendation Engine", 
    "arxiv-id": "1311.2103v1", 
    "author": "Animesh Pandey", 
    "publish": "2013-11-08T23:17:47Z", 
    "summary": "Myers-Briggs Type Indicator (MBTI) types depict the psychological preferences\nby which a person perceives the world and make decisions. There are 4 principal\nfunctions through which the people see the world: sensation, intuition,\nfeeling, and thinking. These functions along with the Introverted\\Extroverted\nnature of the person, there are 16 personalities types, the humans are divided\ninto. Here an idea is presented where a user can get recommendations for books,\nweb media content, music and movies on the basis of the users' MBTI type. Only\nthings like books and other media content has been chosen because the\npreferences in such things are mostly subjective. Apart from the recommended\ncontent that is generally generated on the basis of the previous purchases,\nsearches can be enhanced by using the MBTI. A minimalist survey was designed\nfor collecting the data. This has a more than 100 features that show the\npreference of a personality type. Those include preferences in book genres,\nmusic genres, movie genres and even video games genres. After analyzing the\ndata that is collected from the survey, some inferences were drawn from it\nwhich can be used to design a new recommendation engine for recommending the\ncontent that coincides with the personality of the user."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.4151v1", 
    "title": "Lattice-cell : Hybrid approach for text categorization", 
    "arxiv-id": "1311.4151v1", 
    "author": "Baghdad Atmani", 
    "publish": "2013-11-17T12:02:12Z", 
    "summary": "In this paper, we propose a new text categorization framework based on\nConcepts Lattice and cellular automata. In this framework, concept structure\nare modeled by a Cellular Automaton for Symbolic Induction (CASI). Our\nobjective is to reduce time categorization caused by the Concept Lattice. We\nexamine, by experiments the performance of the proposed approach and compare it\nwith other algorithms such as Naive Bayes and k nearest neighbors. The results\nshow performance improvement while reducing time categorization."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.4420v1", 
    "title": "CAVDM: Cellular Automata Based Video Cloud Mining Framework for   Information Retrieval", 
    "arxiv-id": "1311.4420v1", 
    "author": "SSSN Usha Devi N", 
    "publish": "2013-11-18T15:12:42Z", 
    "summary": "Cloud Mining technique can be applied to various documents. Acquisition and\nstorage of video data is an easy task but retrieval of information from video\ndata is a challenging task. So video Cloud Mining plays an important role in\nefficient video data management for information retrieval. This paper proposes\na Cellular Automata based framework for video Cloud Mining to extract the\ninformation from video data. This includes developing the technique for shot\ndetection then key frame analysis is considered to compare the frames of each\nshot to each others to define the relationship between shots. Cellular automata\nbased hierarchical clustering technique is adopted to make a group of similar\nshots to detect the particular event on some requirement as per user demand."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.4644v1", 
    "title": "A Qualitative Representation and Similarity Measurement Method in   Geographic Information Retrieval", 
    "arxiv-id": "1311.4644v1", 
    "author": "Yu Liu", 
    "publish": "2013-11-19T08:04:28Z", 
    "summary": "The modern geographic information retrieval technology is based on\nquantitative models and methods. The semantic information in web documents and\nqueries cannot be effectively represented, leading to information lost or\nmisunderstanding so that the results are either unreliable or inconsistent. A\nnew qualitative approach is thus proposed for supporting geographic information\nretrieval based on qualitative representation, semantic matching, and\nqualitative reasoning. A qualitative representation model and the corresponding\nsimilarity measurement method are defined. Information in documents and user\nqueries are represented using propositional logic, which considers the thematic\nand geographic semantics synthetically. Thematic information is represented as\nthematic propositions on the base of domain ontology. Similarly, spatial\ninformation is represented as geo-spatial propositions with the support of\ngeographic knowledge base. Then the similarity is divided into thematic\nsimilarity and spatial similarity. The former is calculated by the weighted\ndistance of proposition keywords in the domain ontology, and the latter\nsimilarity is further divided into conceptual similarity and spatial\nsimilarity. Represented by propositions and information units, the similarity\nmeasurement can take evidence theory and fuzzy logic to combine all sub\nsimilarities to get the final similarity between documents and queries. This\nnovel retrieval method is mainly used to retrieve the qualitative geographic\ninformation to support the semantic matching and results ranking. It does not\ndeal with geometric computation and is consistent with human commonsense\ncognition, and thus can improve the efficiency of geographic information\nretrieval technology."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.5765v1", 
    "title": "Text Classification and Distributional features techniques in Datamining   and Warehousing", 
    "arxiv-id": "1311.5765v1", 
    "author": "M Raghavendra rao", 
    "publish": "2013-11-22T14:36:52Z", 
    "summary": "Text Categorization is traditionally done by using the term frequency and\ninverse document frequency.This type of method is not very good because, some\nwords which are not so important may appear in the document .The term frequency\nof unimportant words may increase and document may be classified in the wrong\ncategory.For reducing the error of classifying of documents in wrong category.\nThe Distributional features are introduced. In the Distribuional Features, the\nDistribution of the words in the whole document is analyzed. Whole Document is\nvery closely analyzed for different measures like FirstAppearence, Last\nAppearance, Centriod, Count, etc.The measures are calculated and they are used\nin tf*idf equation and result is used in k- nearest neighbor and K-means\nalgorithm for classifying the documents."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.6227v1", 
    "title": "Experience of Developing a Meta-Semantic Search Engine", 
    "arxiv-id": "1311.6227v1", 
    "author": "Adarsha Palwe", 
    "publish": "2013-11-25T08:27:35Z", 
    "summary": "Thinking of todays web search scenario which is mainly keyword based, leads\nto the need of effective and meaningful search provided by Semantic Web.\nExisting search engines are vulnerable to provide relevant answers to users\nquery due to their dependency on simple data available in web pages. On other\nhand, semantic search engines provide efficient and relevant results as the\nsemantic web manages information with well defined meaning using ontology. A\nMeta-Search engine is a search tool that forwards users query to several\nexisting search engines and provides combined results by using their own page\nranking algorithm. SemanTelli is a meta semantic search engine that fetches\nresults from different semantic search engines such as Hakia, DuckDuckGo,\nSenseBot through intelligent agents. This paper proposes enhancement of\nSemanTelli with improved snippet analysis based page ranking algorithm and\nsupport for image and news search."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.6240v1", 
    "title": "A Decision Tree Approach to Classify Web Services using Quality   Parameters", 
    "arxiv-id": "1311.6240v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2013-11-25T09:12:15Z", 
    "summary": "With the increase in the number of web services, many web services are\navailable on internet providing the same functionality, making it difficult to\nchoose the best one, fulfilling users all requirements. This problem can be\nsolved by considering the quality of web services to distinguish functionally\nsimilar web services. Nine different quality parameters are considered. Web\nservices can be classified and ranked using decision tree approach since they\ndo not require long training period and can be easily interpreted. Various\ndecision tree and rules approaches available are applied and tested to find the\noptimal decision method to correctly classify functionally similar web services\nconsidering their quality parameters."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.6243v1", 
    "title": "Web-page Indexing based on the Prioritize Ontology Terms", 
    "arxiv-id": "1311.6243v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2013-11-25T09:28:58Z", 
    "summary": "In this world, globalization has become a basic and most popular human trend.\nTo globalize information, people are going to publish the documents in the\ninternet. As a result, information volume of internet has become huge. To\nhandle that huge volume of information, Web searcher uses search engines. The\nWebpage indexing mechanism of a search engine plays a big role to retrieve Web\nsearch results in a faster way from the huge volume of Web resources. Web\nresearchers have introduced various types of Web-page indexing mechanism to\nretrieve Webpages from Webpage repository. In this paper, we have illustrated a\nnew approach of design and development of Webpage indexing. The proposed\nWebpage indexing mechanism has applied on domain specific Webpages and we have\nidentified the Webpage domain based on an Ontology. In our approach, first we\nprioritize the Ontology terms that exist in the Webpage content then apply our\nown indexing mechanism to index that Webpage. The main advantage of storing an\nindex is to optimize the speed and performance while finding relevant documents\nfrom the domain specific search engine storage area for a user given search\nquery."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.6245v1", 
    "title": "A Model Approach to Build Basic Ontology", 
    "arxiv-id": "1311.6245v1", 
    "author": "Sajeeda Shikalgar", 
    "publish": "2013-11-25T09:35:14Z", 
    "summary": "As todays world grows with the technology on the other hand it seems to be\nsmall with the World Wide Web. With the use of Internet more and more\ninformation can be search from the web. When Users fires a query they want\nrelevancy in obtained results. In general, search engines perform the ranking\nof web pages in an offline mode, which is after the web pages have been\nretrieved and stored in the database. But most of the time this method does not\nprovide relevant results as most of the search engines were using some ranking\nalgorithms like page Rank, HITS, SALSA and Hilltop. Where these algorithms does\nnot always provides the results based on the semantic web. So a concept of\nOntology is been introduced in search engines to get more meaningful and\nrelevant results with respect to the users query.Ontologies are used to capture\nknowledge about some domain of interest. Ontology describes the concepts in the\ndomain and also the relationships that hold between those concepts. Different\nontology languages provide different facilities. The most recent development in\nstandard ontology languages is OWL (Ontology Web Language) from the World Wide\nWeb Consortium. OWL makes it possible to describe concept to its full extent\nand enables the search engines to provide accurate results to the user."
},{
    "category": "cs.IR", 
    "doi": "10.5121/csit.2013.3817", 
    "link": "http://arxiv.org/pdf/1311.7204v1", 
    "title": "A Hybrid Web Recommendation System based on the Improved Association   Rule Mining Algorithm", 
    "arxiv-id": "1311.7204v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2013-11-28T04:22:55Z", 
    "summary": "As the growing interest of web recommendation systems those are applied to\ndeliver customized data for their users, we started working on this system.\nGenerally the recommendation systems are divided into two major categories such\nas collaborative recommendation system and content based recommendation system.\nIn case of collaborative recommen-dation systems, these try to seek out users\nwho share same tastes that of given user as well as recommends the websites\naccording to the liking given user. Whereas the content based recommendation\nsystems tries to recommend web sites similar to those web sites the user has\nliked. In the recent research we found that the efficient technique based on\nasso-ciation rule mining algorithm is proposed in order to solve the problem of\nweb page recommendation. Major problem of the same is that the web pages are\ngiven equal importance. Here the importance of pages changes according to the\nfre-quency of visiting the web page as well as amount of time user spends on\nthat page. Also recommendation of newly added web pages or the pages those are\nnot yet visited by users are not included in the recommendation set. To\nover-come this problem, we have used the web usage log in the adaptive\nassociation rule based web mining where the asso-ciation rules were applied to\npersonalization. This algorithm was purely based on the Apriori data mining\nalgorithm in order to generate the association rules. However this method also\nsuffers from some unavoidable drawbacks. In this paper we are presenting and\ninvestigating the new approach based on weighted Association Rule Mining\nAlgorithm and text mining. This is improved algorithm which adds semantic\nknowledge to the results, has more efficiency and hence gives better quality\nand performances as compared to existing approaches."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11864-7648", 
    "link": "http://arxiv.org/pdf/1311.7388v1", 
    "title": "Web Mining Techniques in E-Commerce Applications", 
    "arxiv-id": "1311.7388v1", 
    "author": "Sultan Aljahdali", 
    "publish": "2013-11-28T17:57:20Z", 
    "summary": "Today web is the best medium of communication in modern business. Many\ncompanies are redefining their business strategies to improve the business\noutput. Business over internet provides the opportunity to customers and\npartners where their products and specific business can be found. Nowadays\nonline business breaks the barrier of time and space as compared to the\nphysical office. Big companies around the world are realizing that e-commerce\nis not just buying and selling over Internet, rather it improves the efficiency\nto compete with other giants in the market. For this purpose data mining\nsometimes called as knowledge discovery is used. Web mining is data mining\ntechnique that is applied to the WWW. There are vast quantities of information\navailable over the Internet."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11864-7648", 
    "link": "http://arxiv.org/pdf/1402.0728v2", 
    "title": "Forgetting the Words but Remembering the Meaning: Modeling Forgetting in   a Verbal and Semantic Tag Recommender", 
    "arxiv-id": "1402.0728v2", 
    "author": "Tobias Ley", 
    "publish": "2014-02-04T13:31:10Z", 
    "summary": "We assume that recommender systems are more successful, when they are based\non a thorough understanding of how people process information. In the current\npaper we test this assumption in the context of social tagging systems.\nCognitive research on how people assign tags has shown that they draw on two\ninterconnected levels of knowledge in their memory: on a conceptual level of\nsemantic fields or topics, and on a lexical level that turns patterns on the\nsemantic level into words. Another strand of tagging research reveals a strong\nimpact of time dependent forgetting on users' tag choices, such that recently\nused tags have a higher probability being reused than \"older\" tags. In this\npaper, we align both strands by implementing a computational theory of human\nmemory that integrates the two-level conception and the process of forgetting\nin form of a tag recommender and test it in three large-scale social tagging\ndatasets (drawn from BibSonomy, CiteULike and Flickr).\n  As expected, our results reveal a selective effect of time: forgetting is\nmuch more pronounced on the lexical level of tags. Second, an extensive\nevaluation based on this observation shows that a tag recommender\ninterconnecting both levels and integrating time dependent forgetting on the\nlexical level results in high accuracy predictions and outperforms other\nwell-established algorithms, such as Collaborative Filtering, Pairwise\nInteraction Tensor Factorization, FolkRank and two alternative time dependent\napproaches. We conclude that tag recommenders can benefit from going beyond the\nmanifest level of word co-occurrences, and from including forgetting processes\non the lexical level."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11864-7648", 
    "link": "http://arxiv.org/pdf/1402.1270v1", 
    "title": "Vers une interface pour l enrichissement des requetes en arabe dans un   systeme de recherche d information", 
    "arxiv-id": "1402.1270v1", 
    "author": "Abderrahim Mohammed El Amine", 
    "publish": "2014-02-06T08:29:19Z", 
    "summary": "This presentation focuses on the automatic expansion of Arabic request using\nmorphological analyzer and Arabic Wordnet. The expanded request is sent to\nGoogle."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1402.2073v1", 
    "title": "Mining Images in Biomedical Publications: Detection and Analysis of Gel   Diagrams", 
    "arxiv-id": "1402.2073v1", 
    "author": "Michael Krauthammer", 
    "publish": "2014-02-10T09:16:13Z", 
    "summary": "Authors of biomedical publications use gel images to report experimental\nresults such as protein-protein interactions or protein expressions under\ndifferent conditions. Gel images offer a concise way to communicate such\nfindings, not all of which need to be explicitly discussed in the article text.\nThis fact together with the abundance of gel images and their shared common\npatterns makes them prime candidates for automated image mining and parsing. We\nintroduce an approach for the detection of gel images, and present a workflow\nto analyze them. We are able to detect gel segments and panels at high\naccuracy, and present preliminary results for the identification of gene names\nin these images. While we cannot provide a complete solution at this point, we\npresent evidence that this kind of image mining is feasible."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1402.2145v2", 
    "title": "Using content features to enhance performance of user-based   collaborative filtering performance of user-based collaborative filtering", 
    "arxiv-id": "1402.2145v2", 
    "author": "Mansoor Zolghadri Jahromi", 
    "publish": "2014-02-10T13:52:33Z", 
    "summary": "Content-based and collaborative filtering methods are the most successful\nsolutions in recommender systems. Content based method is based on items\nattributes. This method checks the features of users favourite items and then\nproposes the items which have the most similar characteristics with those\nitems. Collaborative filtering method is based on the determination of similar\nitems or similar users, which are called item-based and user-based\ncollaborative filtering, respectively.In this paper we propose a hybrid method\nthat integrates collaborative filtering and content-based methods. The proposed\nmethod can be viewed as user-based Collaborative filtering technique. However\nto find users with similar taste with active user, we used content features of\nthe item under investigation to put more emphasis on users rating for similar\nitems. In other words two users are similar if their ratings are similar on\nitems that have similar context. This is achieved by assigning a weight to each\nrating when calculating the similarity of two users.We used movielens data set\nto access the performance of the proposed method in comparison with basic\nuser-based collaborative filtering and other popular methods."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1402.5774v1", 
    "title": "Information Filtering via Balanced Diffusion on Bipartite Networks", 
    "arxiv-id": "1402.5774v1", 
    "author": "Tao Zhou", 
    "publish": "2014-02-24T10:26:39Z", 
    "summary": "Recent decade has witnessed the increasing popularity of recommender systems,\nwhich help users acquire relevant commodities and services from overwhelming\nresources on Internet. Some simple physical diffusion processes have been used\nto design effective recommendation algorithms for user-object bipartite\nnetworks, typically mass diffusion (MD) and heat conduction (HC) algorithms\nwhich have different advantages respectively on accuracy and diversity. In this\npaper, we investigate the effect of weight assignment in the hybrid of MD and\nHC, and find that a new hybrid algorithm of MD and HC with balanced weights\nwill achieve the optimal recommendation results, we name it balanced diffusion\n(BD) algorithm. Numerical experiments on three benchmark data sets, MovieLens,\nNetflix and RateYourMusic (RYM), show that the performance of BD algorithm\noutperforms the existing diffusion-based methods on the three important\nrecommendation metrics, accuracy, diversity and novelty. Specifically, it can\nnot only provide accurately recommendation results, but also yield higher\ndiversity and novelty in recommendations by accurately recommending unpopular\nobjects."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1402.6132v1", 
    "title": "Uncovering the information core in recommender systems", 
    "arxiv-id": "1402.6132v1", 
    "author": "Tao Zhou", 
    "publish": "2014-02-25T11:08:02Z", 
    "summary": "With the rapid growth of the Internet and overwhelming amount of information\nthat people are confronted with, recommender systems have been developed to\neffiectively support users' decision-making process in online systems. So far,\nmuch attention has been paid to designing new recommendation algorithms and\nimproving existent ones. However, few works considered the different\ncontributions from different users to the performance of a recommender system.\nSuch studies can help us improve the recommendation efficiency by excluding\nirrelevant users. In this paper, we argue that in each online system there\nexists a group of core users who carry most of the information for\nrecommendation. With them, the recommender systems can already generate\nsatisfactory recommendation. Our core user extraction method enables the\nrecommender systems to achieve 90% of the accuracy by taking only 20% of the\ndata into account."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1402.7200v1", 
    "title": "Mathematical Model of Semantic Look - An Efficient Context Driven Search   Engine", 
    "arxiv-id": "1402.7200v1", 
    "author": "L M Patnaik", 
    "publish": "2014-02-28T10:55:24Z", 
    "summary": "The WorldWideWeb (WWW) is a huge conservatory of web pages. Search Engines\nare key applications that fetch web pages for the user query. In the current\ngeneration web architecture, search engines treat keywords provided by the user\nas isolated keywords without considering the context of the user query. This\nresults in a lot of unrelated pages or links being displayed to the user.\nSemantic Web is based on the current web with a revised framework to display a\nmore precise result set as response to a user query. The current web pages need\nto be annotated by finding relevant meta data to be added to each of them, so\nthat they become useful to Semantic Web search engines. Semantic Look explores\nthe context of user query by processing the Semantic information recorded in\nthe web pages. It is compared with an existing algorithm called OntoLook and it\nis shown that Semantic Look is a better optimized search engine by being more\nthan twice as fast as OntoLook."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1404.0091v1", 
    "title": "Interestingness a Unifying Paradigm Bipolar Function Composition", 
    "arxiv-id": "1404.0091v1", 
    "author": "Iaakov Exman", 
    "publish": "2014-04-01T00:53:37Z", 
    "summary": "Interestingness is an important criterion by which we judge knowledge\ndiscovery. But, interestingness has escaped all attempts to capture its\nintuitive meaning into a concise and comprehensive form. A unifying paradigm is\nformulated by function composition. We claim that composition is bipolar, i.e.\ncomposition of exactly two functions, whose two semantic poles are relevance\nand unexpectedness. The paradigm generality is demonstrated by case studies of\nnew interestingness functions, examples of known functions that fit the\nframework, and counter-examples for which the paradigm points out to the\nlacking pole."
},{
    "category": "cs.IR", 
    "doi": "10.1186/2041-1480-5-10", 
    "link": "http://arxiv.org/pdf/1404.1653v2", 
    "title": "Multi-Linear Interactive Matrix Factorization", 
    "arxiv-id": "1404.1653v2", 
    "author": "Zi-Ke Zhang", 
    "publish": "2014-04-07T04:39:14Z", 
    "summary": "Recommender systems, which can significantly help users find their interested\nitems from the information era, has attracted an increasing attention from both\nthe scientific and application society. One of the widest applied\nrecommendation methods is the Matrix Factorization (MF). However, most of MF\nbased approaches focus on the user-item rating matrix, but ignoring the\ningredients which may have significant influence on users' preferences on\nitems. In this paper, we propose a multi-linear interactive MF algorithm\n(MLIMF) to model the interactions between the users and each event associated\nwith their final decisions. Our model considers not only the user-item rating\ninformation but also the pairwise interactions based on some empirically\nsupported factors. In addition, we compared the proposed model with three\ntypical other methods: user-based collaborative filtering (UCF), item-based\ncollaborative filtering (ICF) and regularized MF (RMF). Experimental results on\ntwo real-world datasets, \\emph{MovieLens} 1M and \\emph{MovieLens} 100k, show\nthat our method performs much better than other three methods in the accuracy\nof recommendation. This work may shed some light on the in-depth understanding\nof modeling user online behaviors and the consequent decisions."
},{
    "category": "cs.IR", 
    "doi": "10.1109/JSTSP.2014.2317286", 
    "link": "http://arxiv.org/pdf/1404.2342v1", 
    "title": "Social Collaborative Retrieval", 
    "arxiv-id": "1404.2342v1", 
    "author": "Alfred Hero", 
    "publish": "2014-04-09T01:18:05Z", 
    "summary": "Socially-based recommendation systems have recently attracted significant\ninterest, and a number of studies have shown that social information can\ndramatically improve a system's predictions of user interests. Meanwhile, there\nare now many potential applications that involve aspects of both recommendation\nand information retrieval, and the task of collaborative retrieval---a\ncombination of these two traditional problems---has recently been introduced.\nSuccessful collaborative retrieval requires overcoming severe data sparsity,\nmaking additional sources of information, such as social graphs, particularly\nvaluable. In this paper we propose a new model for collaborative retrieval, and\nshow that our algorithm outperforms current state-of-the-art approaches by\nincorporating information from social networks. We also provide empirical\nanalyses of the ways in which cultural interests propagate along a social graph\nusing a real-world music dataset."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1404.3435v1", 
    "title": "Web Search of New Linearized Medical Drug Leads", 
    "arxiv-id": "1404.3435v1", 
    "author": "Iaakov Exman", 
    "publish": "2014-04-13T22:01:48Z", 
    "summary": "The Web is a potentially huge source of medical drug leads. But despite the\nsignificant amount of multi- dimensional information about drugs, currently\ncommercial search engines accept only linear keyword strings as inputs. This\nwork uses linearized fragments of molecular structures as knowledge\nrepresentation units to serve as inputs to search engines. It is shown that\nquite arbitrary fragments are surprisingly free of ambiguity, obtaining\nrelatively small result sets, which are both manageable and rich in novel\npotential drug leads."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1404.7467v2", 
    "title": "Coupled Matrix Factorization within Non-IID Context", 
    "arxiv-id": "1404.7467v2", 
    "author": "Longbing Cao", 
    "publish": "2014-04-08T02:10:16Z", 
    "summary": "Recommender systems research has experienced different stages such as from\nuser preference understanding to content analysis. Typical recommendation\nalgorithms were built on the following bases: (1) assuming users and items are\nIID, namely independent and identically distributed, and (2) focusing on\nspecific aspects such as user preferences or contents. In reality, complex\nrecommendation tasks involve and request (1) personalized outcomes to tailor\nheterogeneous subjective preferences; and (2) explicit and implicit objective\ncoupling relationships between users, items, and ratings to be considered as\nintrinsic forces driving preferences. This inevitably involves the non-IID\ncomplexity and the need of combining subjective preference with objective\ncouplings hidden in recommendation applications. In this paper, we propose a\nnovel generic coupled matrix factorization (CMF) model by incorporating non-IID\ncoupling relations between users and items. Such couplings integrate the\nintra-coupled interactions within an attribute and inter-coupled interactions\namong different attributes. Experimental results on two open data sets\ndemonstrate that the user/item couplings can be effectively applied in RS and\nCMF outperforms the benchmark methods."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.0296v1", 
    "title": "Using Mobile Agents for Information Retrival in B2B Systems", 
    "arxiv-id": "1406.0296v1", 
    "author": "Ovidiu Andrei Schipor", 
    "publish": "2014-06-02T09:00:01Z", 
    "summary": "This paper presents an architecture of an information retrieval system that\nuse the advantages offered by mobile agents to collect information from\ndifferent sources and bring the result to the calling user. Mobile agent\ntechnology will be used for determine the traceability of a product and also\nfor searching information about a specific entity."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.1065v2", 
    "title": "Uniform definition of comparable and searchable information on the web", 
    "arxiv-id": "1406.1065v2", 
    "author": "Wolfgang Orthuber", 
    "publish": "2014-06-03T17:39:07Z", 
    "summary": "Basically information means selection within a domain (value or definition\nset) of possibilities. For objectifiable, comparable and precise information\nthe domain should be the same for all. Therefore the global (online) definition\nof the domain is proposed here. It is advantageous to define an ordered domain,\nbecause this allows using numbers for addressing the elements and because\nnature is ordered in many respects. The original data can be ordered in\nmultiple independent ways. We can define a domain with multiple independent\nnumeric dimensions to reflect this. Because we want to search information in\nthe domain, for quantification of similarity we define a distance function or\nmetric. Therefore we propose \"Domain Spaces\" (DSs) which are online defined\nnestable metric spaces. Their elements are called \"Domain Vectors\" (DVs) and\nhave the simple form:\n  URL (of common DS definition) plus sequence of numbers\n  At this the sequence must be given so that the mapping of numbers to the DS\ndimensions is clear. By help of appropriate software DVs can be represented\ne.g. as words and numbers. Compared to words, however, DVs have (as original\ninformation) important objectifiable advantages (clear definition, objectivity,\ninformation content, range, resolution, efficiency, searchability). Using DSs\nusers can define which information they make searchable and how it is\nsearchable. DSs can be also used to make quantitative (numeric) data as uniform\nDVs interoperable, comparable and searchable. The approach is demonstrated in\nan online database with search engine (http://NumericSearch.com). The search\nprocedure is called \"Numeric Search\". It consists of two systematic steps: 1.\nSelection of the appropriate DS e.g. by conventional word based search within\nthe DS definitions. 2. Range and/or similarity search of DVs in the selected\nDS."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.1583v1", 
    "title": "Fuzzy clustering of web documents using equivalence relations and fuzzy   hierarchical clustering", 
    "arxiv-id": "1406.1583v1", 
    "author": "Monika Rani", 
    "publish": "2014-06-06T05:12:38Z", 
    "summary": "The conventional clustering algorithms have difficulties in handling the\nchallenges posed by the collection of natural data which is often vague and\nuncertain. Fuzzy clustering methods have the potential to manage such\nsituations efficiently. Fuzzy clustering method is offered to construct\nclusters with uncertain boundaries and allows that one object belongs to one or\nmore clusters with some membership degree. In this paper, an algorithm and\nexperimental results are presented for fuzzy clustering of web documents using\nequivalence relations and fuzzy hierarchical clustering."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.1855v1", 
    "title": "Text Mining System for Non-Expert Miners", 
    "arxiv-id": "1406.1855v1", 
    "author": "S. Sasirekha", 
    "publish": "2014-06-07T03:04:02Z", 
    "summary": "Service oriented architecture integrated with text mining allows services to\nextract information in a well defined manner. In this paper, it is proposed to\ndesign a knowledge extracting system for the Ocean Information Data System.\nDeployed ARGO floating sensors of INCOIS (Indian National Council for Ocean\nInformation Systems) organization reflects the characteristics of ocean. This\nis forwarded to the OIDS (Ocean Information Data System). For the data received\nfrom OIDS, pre-processing techniques are applied. Pre-processing involves the\nheader retrieval and data separation. Header information is used to identify\nthe region of sensor, whereas data is used in the analysis process of Ocean\nInformation System. Analyzed data is segmented based on the region, by the\nheader value. Mining technique and composition principle is applied on the\nsegments for further analysis. Index Terms-- Service oriented architecture;\nText Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.1875v1", 
    "title": "Bullseye: Structured Passage Retrieval and Document Highlighting for   Scholarly Search", 
    "arxiv-id": "1406.1875v1", 
    "author": "Matthew Lease", 
    "publish": "2014-06-07T09:01:44Z", 
    "summary": "We present the Bullseye system for scholarly search. Given a collection of\nresearch papers, Bullseye: 1) identifies relevant passages using any\non-the-shelf algorithm; 2) automatically detects document structure and\nrestricts retrieved passages to user-specifed sections; and 3) highlights those\npassages for each PDF document retrieved. We evaluate Bullseye with regard to\nthree aspects: system effectiveness, user effectiveness, and user effort. In a\nsystem-blind evaluation, users were asked to compare passage retrieval using\nBullseye vs. a baseline which ignores document structure, in regard to four\ntypes of graded assessments. Results show modest improvement in system\neffectiveness while both user effectiveness and user effort show substantial\nimprovement. Users also report very strong demand for passage highlighting in\nscholarly search across both systems considered."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0003705401080115", 
    "link": "http://arxiv.org/pdf/1406.3188v1", 
    "title": "Assessing the Quality of Web Content", 
    "arxiv-id": "1406.3188v1", 
    "author": "Michael Granitzer", 
    "publish": "2014-06-12T10:40:15Z", 
    "summary": "This paper describes our approach towards the ECML/PKDD Discovery Challenge\n2010. The challenge consists of three tasks: (1) a Web genre and facet\nclassification task for English hosts, (2) an English quality task, and (3) a\nmultilingual quality task (German and French). In our approach, we create an\nensemble of three classifiers to predict unseen Web hosts whereas each\nclassifier is trained on a different feature set. Our final NDCG on the whole\ntest set is 0:575 for Task 1, 0:852 for Task 2, and 0:81 (French) and 0:77\n(German) for Task 3, which ranks second place in the ECML/PKDD Discovery\nChallenge 2010."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.3277v1", 
    "title": "A Semantic VSM-Based Recommender System", 
    "arxiv-id": "1406.3277v1", 
    "author": "Mehran Yazdi", 
    "publish": "2014-06-12T16:02:16Z", 
    "summary": "Online forums enable users to discuss together around various topics. One of\nthe serious problems of these environments is high volume of discussions and\nthus information overload problem. Unfortunately without considering the users\ninterests, traditional Information Retrieval (IR) techniques are not able to\nsolve the problem. Therefore, employment of a Recommender System (RS) that\ncould suggest favorite's topics of users according to their tastes could\nincreases the dynamism of forum and prevent the users from duplicate posts. In\naddition, consideration of semantics can be useful for increasing the\nperformance of IR based RS. Our goal is study of impact of ontology and data\nmining techniques on improving of content-based RS. For this purpose, at first,\nthree type of ontologies will be constructed from the domain corpus with\nutilization of text mining, Natural Language Processing (NLP) and Wordnet and\nthen they will be used as an input in two kind of RS: one, fully ontology-based\nand one with enriching the user profile vector with ontology in vector space\nmodel (VSM) (proposed method). Afterward the results will be compared with the\nsimple VSM based RS. Given results show that the proposed RS presents the\nhighest performance."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.3882v1", 
    "title": "Eclipse Hashing: Alexandrov Compactification and Hashing with   Hyperspheres for Fast Similarity Search", 
    "arxiv-id": "1406.3882v1", 
    "author": "Makiko Konoshima", 
    "publish": "2014-06-16T01:38:50Z", 
    "summary": "The similarity searches that use high-dimensional feature vectors consisting\nof a vast amount of data have a wide range of application. One way of\nconducting a fast similarity search is to transform the feature vectors into\nbinary vectors and perform the similarity search by using the Hamming distance.\nSuch a transformation is a hashing method, and the choice of hashing function\nis important. Hashing methods using hyperplanes or hyperspheres are proposed.\nOne study reported here is inspired by Spherical LSH, and we use hypersperes to\nhash the feature vectors. Our method, called Eclipse-hashing, performs a\ncompactification of R^n by using the inverse stereographic projection, which is\na kind of Alexandrov compactification. By using Eclipse-hashing, one can obtain\nthe hypersphere-hash function without explicitly using hyperspheres. Hence, the\nnumber of nonlinear operations is reduced and the processing time of hashing\nbecomes shorter. Furthermore, we also show that as a result of improving the\napproximation accuracy, Eclipse-hashing is more accurate than\nhyperplane-hashing."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.5616v1", 
    "title": "An Effective Approach for Web Document Classification using the Concept   of Association Analysis of Data Mining", 
    "arxiv-id": "1406.5616v1", 
    "author": "S. K. Sahay", 
    "publish": "2014-06-21T14:33:09Z", 
    "summary": "Exponential growth of the web increased the importance of web document\nclassification and data mining. To get the exact information, in the form of\nknowing what classes a web document belongs to, is expensive. Automatic\nclassification of web document is of great use to search engines which provides\nthis information at a low cost. In this paper, we propose an approach for\nclassifying the web document using the frequent item word sets generated by the\nFrequent Pattern (FP) Growth which is an association analysis technique of data\nmining. These set of associated words act as feature set. The final\nclassification obtained after Na\\\"ive Bayes classifier used on the feature set.\nFor the experimental work, we use Gensim package, as it is simple and robust.\nResults show that our approach can be effectively classifying the web document."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.5617v1", 
    "title": "Web Document Clustering and Ranking using Tf-Idf based Apriori Approach", 
    "arxiv-id": "1406.5617v1", 
    "author": "S. K. Sahay", 
    "publish": "2014-06-21T14:38:21Z", 
    "summary": "The dynamic web has increased exponentially over the past few years with more\nthan thousands of documents related to a subject available to the user now.\nMost of the web documents are unstructured and not in an organized manner and\nhence user facing more difficult to find relevant documents. A more useful and\nefficient mechanism is combining clustering with ranking, where clustering can\ngroup the similar documents in one place and ranking can be applied to each\ncluster for viewing the top documents at the beginning.. Besides the particular\nclustering algorithm, the different term weighting functions applied to the\nselected features to represent web document is a main aspect in clustering\ntask. Keeping this approach in mind, here we proposed a new mechanism called\nTf-Idf based Apriori for clustering the web documents. We then rank the\ndocuments in each cluster using Tf-Idf and similarity factor of documents based\non the user query. This approach will helps the user to get all his relevant\ndocuments in one place and can restrict his search to some top documents of his\nchoice. For experimental purpose, we have taken the Classic3 and Classic4\ndatasets of Cornell University having more than 10,000 documents and use gensim\ntoolkit to carry out our work. We have compared our approach with traditional\napriori algorithm and found that our approach is giving better results for\nhigher minimum support. Our ranking mechanism is also giving a good F-measure\nof 78%."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.5690v1", 
    "title": "WebParF: A Web partitioning framework for Parallel Crawlers", 
    "arxiv-id": "1406.5690v1", 
    "author": "Pikakshi Manchanda", 
    "publish": "2014-06-22T09:33:21Z", 
    "summary": "With the ever proliferating size and scale of the WWW [1] efficient ways of\nexploring content are of increasing importance. How can we efficiently retrieve\ninformation from it through crawling? And in this era of tera and multi-core\nprocessors, we ought to think of multi-threaded processes as a serving\nsolution. So, even better how can we improve the crawling performance by using\nparallel crawlers that work independently? The paper devotes to the fundamental\ndevelopment in the field of parallel crawlers [4] highlighting the advantages\nand challenges arising from its design. The paper also focuses on the aspect of\nURL distribution among the various parallel crawling processes or threads and\nordering the URLs within each distributed set of URLs. How to distribute URLs\nfrom the URL frontier to the various concurrently executing crawling process\nthreads is an orthogonal problem. The paper provides a solution to the problem\nby designing a framework WebParF that partitions the URL frontier into a\nseveral URL queues while considering the various design issues."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.6449v1", 
    "title": "The Links Have It: Infobox Generation by Summarization over Linked   Entities", 
    "arxiv-id": "1406.6449v1", 
    "author": "Wei Wang", 
    "publish": "2014-06-25T03:33:17Z", 
    "summary": "Online encyclopedia such as Wikipedia has become one of the best sources of\nknowledge. Much effort has been devoted to expanding and enriching the\nstructured data by automatic information extraction from unstructured text in\nWikipedia. Although remarkable progresses have been made, their effectiveness\nand efficiency is still limited as they try to tackle an extremely difficult\nnatural language understanding problems and heavily relies on supervised\nlearning approaches which require large amount effort to label the training\ndata. In this paper, instead of performing information extraction over\nunstructured natural language text directly, we focus on a rich set of\nsemi-structured data in Wikipedia articles: linked entities. The idea of this\npaper is the following: If we can summarize the relationship between the entity\nand its linked entities, we immediately harvest some of the most important\ninformation about the entity. To this end, we propose a novel rank aggregation\napproach to remove noise, an effective clustering and labeling algorithm to\nextract knowledge."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.7093v1", 
    "title": "Using multi-categorization semantic analysis and personalization for   semantic search", 
    "arxiv-id": "1406.7093v1", 
    "author": "Moyi Shi", 
    "publish": "2014-06-27T07:43:38Z", 
    "summary": "Semantic search technology has received more attention in the last years.\nCompared with the keyword based search, semantic search is used to excavate the\nlatent semantics information and help users find the information items that\nthey want indeed. In this paper, we present a novel approach for semantic\nsearch which combines Multi-Categorization Semantic Analysis with\npersonalization technology. The MCSA approach can classify documents into\nmultiple categories, which is distinct from the existing approaches of\nclassifying documents into a single category. Then, the search history and\npersonal information for users are significantly considered in analysing and\nmatching the original search result by Term Vector DataBase. A series of\npersonalization algorithms are proposed to match personal information and\nsearch history. At last, the related experiments are made to validate the\neffectiveness and efficiency of our method. The experimental results show that\nour method based on MCSA and personalization outperforms some existing methods\nwith the higher search accuracy and the lower extra time cost."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1406.7727v1", 
    "title": "Recommending Items in Social Tagging Systems Using Tag and Time   Information", 
    "arxiv-id": "1406.7727v1", 
    "author": "Denis Parra", 
    "publish": "2014-06-30T13:29:33Z", 
    "summary": "In this work we present a novel item recommendation approach that aims at\nimproving Collaborative Filtering (CF) in social tagging systems using the\ninformation about tags and time. Our algorithm follows a two-step approach,\nwhere in the first step a potentially interesting candidate item-set is found\nusing user-based CF and in the second step this candidate item-set is ranked\nusing item-based CF. Within this ranking step we integrate the information of\ntag usage and time using the Base-Level Learning (BLL) equation coming from\nhuman memory theory that is used to determine the reuse-probability of words\nand tags using a power-law forgetting function.\n  As the results of our extensive evaluation conducted on data-sets gathered\nfrom three social tagging systems (BibSonomy, CiteULike and MovieLens) show,\nthe usage of tag-based and time information via the BLL equation also helps to\nimprove the ranking and recommendation process of items and thus, can be used\nto realize an effective item recommender that outperforms two alternative\nalgorithms which also exploit time and tag-based information."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1408.0719v1", 
    "title": "Personalized PageRank with Node-dependent Restart", 
    "arxiv-id": "1408.0719v1", 
    "author": "Marina Sokol", 
    "publish": "2014-08-04T15:56:27Z", 
    "summary": "Personalized PageRank is an algorithm to classify the improtance of web pages\non a user-dependent basis. We introduce two generalizations of Personalized\nPageRank with node-dependent restart. The first generalization is based on the\nproportion of visits to nodes before the restart, whereas the second\ngeneralization is based on the probability of visited node just before the\nrestart. In the original case of constant restart probability, the two measures\ncoincide. We discuss interesting particular cases of restart probabilities and\nrestart distributions. We show that the both generalizations of Personalized\nPageRank have an elegant expression connecting the so-called direct and reverse\nPersonalized PageRanks that yield a symmetry property of these Personalized\nPageRanks."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1408.3587v1", 
    "title": "A Family of Rank Similarity Measures based on Maximized Effectiveness   Difference", 
    "arxiv-id": "1408.3587v1", 
    "author": "Clarke L. A. Clarke", 
    "publish": "2014-08-15T17:05:52Z", 
    "summary": "Rank similarity measures provide a method for quantifying differences between\nsearch engine results without the need for relevance judgments. For example,\nthe providers of a search service might use such measures to estimate the\nimpact of a proposed algorithmic change across a large number of queries -\nperhaps millions - identifying those queries where the impact is greatest. In\nthis paper, we propose and validate a family of rank similarity measures, each\nderived from an associated effectiveness measure. Each member of the family is\nbased on the maximization of effectiveness difference under this associated\nmeasure. Computing this maximized effectiveness difference (MED) requires the\nsolution of an optimization problem that varies in difficulty, depending on the\nassociated measure. We present solutions for several standard effectiveness\nmeasures, including nDCG, MAP, and ERR. Through an experimental validation, we\nshow that MED reveals meaningful differences between retrieval runs.\nMathematically, MED is a metric, regardless of the associated measure. Prior\nwork has established a number of other desiderata for rank similarity in the\ncontext of search, and we demonstrate that MED satisfies these requirements.\nUnlike previous proposals, MED allows us to directly translate assumptions\nabout user behavior from any established effectiveness measure to create a\ncorresponding rank similarity measure. In addition, MED cleanly accommodates\npartial relevance judgments, and if complete relevance information is\navailable, it reduces to a simple difference between effectiveness values."
},{
    "category": "cs.IR", 
    "doi": "10.7763/IJCTE.2013.V5.704", 
    "link": "http://arxiv.org/pdf/1408.4519v1", 
    "title": "Searchers Seeking: What Happens When you Frustrate Searchers?", 
    "arxiv-id": "1408.4519v1", 
    "author": "Gareth Renaud", 
    "publish": "2014-08-20T04:00:12Z", 
    "summary": "People searching for information occasionally experience difficulties finding\nwhat they want on the Web. This might happen if they cannot quite come up with\nthe right search terms. What do searchers do when this happens? Intuitively one\nimagines that they will try a number of associated search terms to zero in on\ntheir intended search target. Certainly the provision of spelling suggestions\nand related search terms assume that frustrated searchers will use these to\nimplement this strategy. Is this assumption correct? What do people really do?\n  We ran an experiment where we asked people to find some relevant links, but\nwe prevented them from using the most obvious search terms, which we termed\ntaboo words. To make the experiment more interesting we also provided the\ntraditional forms of assistance: spelling suggestions and related search\nsuggestions. We assigned participants using a magic square to get no\nassistance, one kind of assistance, or both. Forty eight people participated in\nthe experiment.\n  What emerged from the analysis was that when people are frustrated in their\nsearching attempts, a minority soldier on, attempting to find other terms, but\nthe majority will stick with their original query term and simply progress from\npage to page in a vain attempt to find something relevant. This confirms\nfindings by other researchers about the difficulties of query re-formulation.\nOur finding will serve to inform the developers of user interfaces to search\nengines, since it would be helpful if we could find a better way of supporting\nfrustrated searchers."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICoIA.2013.6650270", 
    "link": "http://arxiv.org/pdf/1408.6930v1", 
    "title": "Mobile recommender systems: An overview of technologies and challenges", 
    "arxiv-id": "1408.6930v1", 
    "author": "Christos K. Georgiadis", 
    "publish": "2014-08-29T06:27:18Z", 
    "summary": "The use of mobile devices in combination with the rapid growth of the\ninternet has generated an information overload problem. Recommender systems is\na necessity to decide which of the data are relevant to the user. However in\nmobile devices there are different factors who are crucial to information\nretrieval, such as the location, the screen size and the processor speed. This\npaper gives an overview of the technologies related to mobile recommender\nsystems and a more detailed description of the challenged faced."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICoIA.2013.6650270", 
    "link": "http://arxiv.org/pdf/1409.0104v1", 
    "title": "Marginalizing over the PageRank Damping Factor", 
    "arxiv-id": "1409.0104v1", 
    "author": "Christian Bauckhage", 
    "publish": "2014-08-30T11:39:32Z", 
    "summary": "In this note, we show how to marginalize over the damping parameter of the\nPageRank equation so as to obtain a parameter-free version known as TotalRank.\nOur discussion is meant as a reference and intended to provide a guided tour\ntowards an interesting result that has applications in information retrieval\nand classification."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICoIA.2013.6650270", 
    "link": "http://arxiv.org/pdf/1409.0491v1", 
    "title": "Facets and Typed Relations as Tools for Reasoning Processes in   Information Retrieval", 
    "arxiv-id": "1409.0491v1", 
    "author": "Winfried G\u00f6dert", 
    "publish": "2014-09-01T17:20:13Z", 
    "summary": "Faceted arrangement of entities and typed relations for representing\ndifferent associations between the entities are established tools in knowledge\nrepresentation. In this paper, a proposal is being discussed combining both\ntools to draw inferences along relational paths. This approach may yield new\nbenefit for information retrieval processes, especially when modeled for\nheterogeneous environments in the Semantic Web. Faceted arrangement can be used\nas a se-lection tool for the semantic knowledge modeled within the knowledge\nrepre-sentation. Typed relations between the entities of different facets can\nbe used as restrictions for selecting them across the facets."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICAdLT.2013.6568453", 
    "link": "http://arxiv.org/pdf/1409.0921v1", 
    "title": "A Generalized Framework for Ontology-Based Information Retrieval   Application to a public-transportation system", 
    "arxiv-id": "1409.0921v1", 
    "author": "Mourad Abed", 
    "publish": "2014-09-02T23:41:58Z", 
    "summary": "In this paper we present a generic framework for ontology-based information\nretrieval. We focus on the recognition of semantic information extracted from\ndata sources and the mapping of this knowledge into ontology. In order to\nachieve more scalability, we propose an approach for semantic indexing based on\nentity retrieval model. In addition, we have used ontology of public\ntransportation domain in order to validate these proposals. Finally, we\nevaluated our system using ontology mapping and real world data sources.\nExperiments show that our framework can provide meaningful search results."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICAdLT.2013.6568453", 
    "link": "http://arxiv.org/pdf/1409.1357v1", 
    "title": "Recommending Scientific Literature: Comparing Use-Cases and Algorithms", 
    "arxiv-id": "1409.1357v1", 
    "author": "Michael Granitzer", 
    "publish": "2014-09-04T08:30:36Z", 
    "summary": "An important aspect of a researcher's activities is to find relevant and\nrelated publications. The task of a recommender system for scientific\npublications is to provide a list of papers that match these criteria. Based on\nthe collection of publications managed by Mendeley, four data sets have been\nassembled that reflect different aspects of relatedness. Each of these\nrelatedness scenarios reflect a user's search strategy. These scenarios are\npublic groups, venues, author publications and user libraries. The first three\nof these data sets are being made publicly available for other researchers to\ncompare algorithms against. Three recommender systems have been implemented: a\ncollaborative filtering system; a content-based filtering system; and a hybrid\nof these two systems. Results from testing demonstrate that collaborative\nfiltering slightly outperforms the content-based approach, but fails in some\nscenarios. The hybrid system, that combines the two recommendation methods,\nprovides the best performance, achieving a precision of up to 70%. This\nsuggests that both techniques contribute complementary information in the\ncontext of recommending scientific literature and different approaches suite\nfor different information needs."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICAdLT.2013.6568453", 
    "link": "http://arxiv.org/pdf/1409.2530v1", 
    "title": "Augmenting recommendation systems using a model of semantically-related   terms extracted from user behavior", 
    "arxiv-id": "1409.2530v1", 
    "author": "Trey Grainger", 
    "publish": "2014-09-08T21:10:19Z", 
    "summary": "Common difficulties like the cold-start problem and a lack of sufficient\ninformation about users due to their limited interactions have been major\nchallenges for most recommender systems (RS). To overcome these challenges and\nmany similar ones that result in low accuracy (precision and recall)\nrecommendations, we propose a novel system that extracts semantically-related\nsearch keywords based on the aggregate behavioral data of many users. These\nsemantically-related search keywords can be used to substantially increase the\namount of knowledge about a specific user's interests based upon even a few\nsearches and thus improve the accuracy of the RS. The proposed system is\ncapable of mining aggregate user search logs to discover semantic relationships\nbetween key phrases in a manner that is language agnostic, human\nunderstandable, and virtually noise-free. These semantically related keywords\nare obtained by looking at the links between queries of similar users which, we\nbelieve, represent a largely untapped source for discovering latent semantic\nrelationships between search terms."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.163.2", 
    "link": "http://arxiv.org/pdf/1409.2590v1", 
    "title": "Automatic Detection of Webpages that Share the Same Web Template", 
    "arxiv-id": "1409.2590v1", 
    "author": "Salvador Tamarit", 
    "publish": "2014-09-09T04:12:36Z", 
    "summary": "Template extraction is the process of isolating the template of a given\nwebpage. It is widely used in several disciplines, including webpages\ndevelopment, content extraction, block detection, and webpages indexing. One of\nthe main goals of template extraction is identifying a set of webpages with the\nsame template without having to load and analyze too many webpages prior to\nidentifying the template. This work introduces a new technique to automatically\ndiscover a reduced set of webpages in a website that implement the template.\nThis set is computed with an hyperlink analysis that computes a very small set\nwith a high level of confidence."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.163.2", 
    "link": "http://arxiv.org/pdf/1409.5443v2", 
    "title": "Exploratory Analysis of a Terabyte Scale Web Corpus", 
    "arxiv-id": "1409.5443v2", 
    "author": "Eleftherios Kayafas", 
    "publish": "2014-09-18T20:00:52Z", 
    "summary": "In this paper we present a preliminary analysis over the largest publicly\naccessible web dataset: the Common Crawl Corpus. We measure nine web\ncharacteristics from two levels of granularity using MapReduce and we comment\non the initial observations over a fraction of it. To the best of our knowledge\ntwo of the characteristics, the language distribution and the HTML version of\npages have not been analyzed in previous work, while the specific dataset has\nbeen only analyzed on page level."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.163.2", 
    "link": "http://arxiv.org/pdf/1409.6182v2", 
    "title": "A Benchmark Suite for Template Detection and Content Extraction", 
    "arxiv-id": "1409.6182v2", 
    "author": "Salvador Tamarit", 
    "publish": "2014-09-22T14:21:33Z", 
    "summary": "Template detection and content extraction are two of the main areas of\ninformation retrieval applied to the Web. They perform different analyses over\nthe structure and content of webpages to extract some part of the document.\nHowever, their objective is different. While template detection identifies the\ntemplate of a webpage (usually comparing with other webpages of the same\nwebsite), content extraction identifies the main content of the webpage\ndiscarding the other part. Therefore, they are somehow complementary, because\nthe main content is not part of the template. It has been measured that\ntemplates represent between 40% and 50% of data on the Web. Therefore,\nidentifying templates is essential for indexing tasks because templates usually\ncontain irrelevant information such as advertisements, menus and banners.\nProcessing and storing this information is likely to lead to a waste of\nresources (storage space, bandwidth, etc.). Similarly, identifying the main\ncontent is essential for many information retrieval tasks. In this paper, we\npresent a benchmark suite to test different approaches for template detection\nand content extraction. The suite is public, and it contains real heterogeneous\nwebpages that have been labelled so that different techniques can be suitable\n(and automatically) compared."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.163.2", 
    "link": "http://arxiv.org/pdf/1409.6512v2", 
    "title": "Learning to Match for Multi-criteria Document Relevance", 
    "arxiv-id": "1409.6512v2", 
    "author": "Sadok Ben Yahia", 
    "publish": "2014-09-23T12:38:33Z", 
    "summary": "In light of the tremendous amount of data produced by social media, a large\nbody of research have revisited the relevance estimation of the users'\ngenerated content. Most of the studies have stressed the multidimensional\nnature of relevance and proved the effectiveness of combining the different\ncriteria that it embodies. Traditional relevance estimates combination methods\nare often based on linear combination schemes. However, despite being\neffective, those aggregation mechanisms are not effective in real-life\napplications since they heavily rely on the non-realistic independence property\nof the relevance dimensions. In this paper, we propose to tackle this issue\nthrough the design of a novel fuzzy-based document ranking model. We also\npropose an automated methodology to capture the importance of relevance\ndimensions, as well as information about their interaction. This model, based\non the Choquet Integral, allows to optimize the aggregated documents relevance\nscores using any target information retrieval relevance metric. Experiments\nwithin the TREC Microblog task and a social personalized information retrieval\ntask highlighted that our model significantly outperforms a wide range of\nstate-of-the-art aggregation operators, as well as a representative learning to\nrank methods."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.163.2", 
    "link": "http://arxiv.org/pdf/1409.7729v1", 
    "title": "Context-Based Information Retrieval in Risky Environment", 
    "arxiv-id": "1409.7729v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2014-07-30T06:40:13Z", 
    "summary": "Context-Based Information Retrieval is recently modelled as an exploration/\nexploitation trade-off (exr/exp) problem, where the system has to choose\nbetween maximizing its expected rewards dealing with its current knowledge\n(exploitation) and learning more about the unknown user's preferences to\nimprove its knowledge (exploration). This problem has been addressed by the\nreinforcement learning community but they do not consider the risk level of the\ncurrent user's situation, where it may be dangerous to explore the\nnon-top-ranked documents the user may not desire in his/her current situation\nif the risk level is high. We introduce in this paper an algorithm named\nCBIR-R-greedy that considers the risk level of the user's situation to\nadaptively balance between exr and exp."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1148170.1148197", 
    "link": "http://arxiv.org/pdf/1409.8518v1", 
    "title": "ProbFuse: A Probabilistic Approach to Data Fusion", 
    "arxiv-id": "1409.8518v1", 
    "author": "John Dunnion", 
    "publish": "2014-09-30T12:27:28Z", 
    "summary": "Data fusion is the combination of the results of independent searches on a\ndocument collection into one single output result set. It has been shown in the\npast that this can greatly improve retrieval effectiveness over that of the\nindividual results.\n  This paper presents probFuse, a probabilistic approach to data fusion.\nProbFuse assumes that the performance of the individual input systems on a\nnumber of training queries is indicative of their future performance. The fused\nresult set is based on probabilities of relevance calculated during this\ntraining process. Retrieval experiments using data from the TREC ad hoc\ncollection demonstrate that probFuse achieves results superior to that of the\npopular CombMNZ fusion algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1148170.1148197", 
    "link": "http://arxiv.org/pdf/1501.00744v1", 
    "title": "Identifying Relevant Document Facets for Keyword-Based Search Queries", 
    "arxiv-id": "1501.00744v1", 
    "author": "Lanbo Zhang", 
    "publish": "2015-01-05T01:49:11Z", 
    "summary": "As structured documents with rich metadata (such as products, movies, etc.)\nbecome increasingly prevalent, searching those documents has become an\nimportant IR problem. Although advanced search interfaces are widely available,\nmost users still prefer to use keyword-based queries to search those documents.\nQuery keywords often imply some hidden restrictions on the desired documents,\nwhich can be represented as document facet-value pairs. To achieve high\nretrieval performance, it's important to be able to identify the relevant\nfacet-value pairs hidden in a query. In this paper, we study the problem of\nidentifying document facet-value pairs that are relevant to a keyword-based\nsearch query. We propose a machine learning approach and a set of useful\nfeatures, and evaluate our approach using a movie data set from INEX."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10115-014-0779-2", 
    "link": "http://arxiv.org/pdf/1501.01996v1", 
    "title": "A probabilistic model to resolve diversity-accuracy challenge of   recommendation systems", 
    "arxiv-id": "1501.01996v1", 
    "author": "Mahdi Jalili", 
    "publish": "2015-01-08T22:42:39Z", 
    "summary": "Recommendation systems have wide-spread applications in both academia and\nindustry. Traditionally, performance of recommendation systems has been\nmeasured by their precision. By introducing novelty and diversity as key\nqualities in recommender systems, recently increasing attention has been\nfocused on this topic. Precision and novelty of recommendation are not in the\nsame direction, and practical systems should make a trade-off between these two\nquantities. Thus, it is an important feature of a recommender system to make it\npossible to adjust diversity and accuracy of the recommendations by tuning the\nmodel. In this paper, we introduce a probabilistic structure to resolve the\ndiversity-accuracy dilemma in recommender systems. We propose a hybrid model\nwith adjustable level of diversity and precision such that one can perform this\nby tuning a single parameter. The proposed recommendation model consists of two\nmodels: one for maximization of the accuracy and the other one for\nspecification of the recommendation list to tastes of users. Our experiments on\ntwo real datasets show the functionality of the model in resolving\naccuracy-diversity dilemma and outperformance of the model over other classic\nmodels. The proposed method could be extensively applied to real commercial\nsystems due to its low computational complexity and significant performance."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.173.2", 
    "link": "http://arxiv.org/pdf/1501.02031v1", 
    "title": "Web Template Extraction Based on Hyperlink Analysis", 
    "arxiv-id": "1501.02031v1", 
    "author": "Salvador Tamarit", 
    "publish": "2015-01-09T03:59:36Z", 
    "summary": "Web templates are one of the main development resources for website\nengineers. Templates allow them to increase productivity by plugin content into\nalready formatted and prepared pagelets. For the final user templates are also\nuseful, because they provide uniformity and a common look and feel for all\nwebpages. However, from the point of view of crawlers and indexers, templates\nare an important problem, because templates usually contain irrelevant\ninformation such as advertisements, menus, and banners. Processing and storing\nthis information is likely to lead to a waste of resources (storage space,\nbandwidth, etc.). It has been measured that templates represent between 40% and\n50% of data on the Web. Therefore, identifying templates is essential for\nindexing tasks. In this work we propose a novel method for automatic template\nextraction that is based on similarity analysis between the DOM trees of a\ncollection of webpages that are detected using menus information. Our\nimplementation and experiments demonstrate the usefulness of the technique."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.173.2", 
    "link": "http://arxiv.org/pdf/1501.04509v1", 
    "title": "Responding to Retrieval: A Proposal to Use Retrieval Information for   Better Presentation of Website Content", 
    "arxiv-id": "1501.04509v1", 
    "author": "Anil Nelakanti", 
    "publish": "2015-01-19T14:59:10Z", 
    "summary": "Retrieval and content management are assumed to be mutually exclusive. In\nthis paper we suggest that they need not be so. In the usual information\nretrieval scenario, some information about queries leading to a website (due to\n`hits' or `visits') is available to the server administrator of the concerned\nwebsite. This information can used to better present the content on the\nwebsite. Further, we suggest that some more information can be shared by the\nretrieval system with the content provider. This will enable the content\nprovider (any website) to have a more dynamic presentation of the content that\nis in tune with the query trends, without violating the privacy of the querying\nuser. The result will be a better synchronization between retrieval systems and\ncontent providers, with the purpose of improving the user's web search\nexperience. This will also give the content provider a say in this process,\ngiven that the content provider is the one who knows much more about the\ncontent than the retrieval system. It also means that the content presentation\nmay change in response to a query. In the end, the user will be able to find\nthe relevant content more easily and quickly."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.05132v1", 
    "title": "Learning to Rank Academic Experts in the DBLP Dataset", 
    "arxiv-id": "1501.05132v1", 
    "author": "P\u00e1vel Calado", 
    "publish": "2015-01-21T11:25:33Z", 
    "summary": "Expert finding is an information retrieval task that is concerned with the\nsearch for the most knowledgeable people with respect to a specific topic, and\nthe search is based on documents that describe people's activities. The task\ninvolves taking a user query as input and returning a list of people who are\nsorted by their level of expertise with respect to the user query. Despite\nrecent interest in the area, the current state-of-the-art techniques lack in\nprincipled approaches for optimally combining different sources of evidence.\nThis article proposes two frameworks for combining multiple estimators of\nexpertise. These estimators are derived from textual contents, from\ngraph-structure of the citation patterns for the community of experts, and from\nprofile information about the experts. More specifically, this article explores\nthe use of supervised learning to rank methods, as well as rank aggregation\napproaches, for combing all of the estimators of expertise. Several supervised\nlearning algorithms, which are representative of the pointwise, pairwise and\nlistwise approaches, were tested, and various state-of-the-art data fusion\ntechniques were also explored for the rank aggregation framework. Experiments\nthat were performed on a dataset of academic publications from the Computer\nScience domain attest the adequacy of the proposed approaches."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.05140v1", 
    "title": "Using Rank Aggregation for Expert Search in Academic Digital Libraries", 
    "arxiv-id": "1501.05140v1", 
    "author": "P\u00e1vel Calado", 
    "publish": "2015-01-21T11:41:57Z", 
    "summary": "The task of expert finding has been getting increasing attention in\ninformation retrieval literature. However, the current state-of-the-art is\nstill lacking in principled approaches for combining different sources of\nevidence. This paper explores the usage of unsupervised rank aggregation\nmethods as a principled approach for combining multiple estimators of\nexpertise, derived from the textual contents, from the graph-structure of the\ncitation patterns for the community of experts, and from profile information\nabout the experts. We specifically experimented two unsupervised rank\naggregation approaches well known in the information retrieval literature,\nnamely CombSUM and CombMNZ. Experiments made over a dataset of academic\npublications for the area of Computer Science attest for the adequacy of these\nmethods."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.06370v1", 
    "title": "Augmented Test Collections: A Step in the Right Direction", 
    "arxiv-id": "1501.06370v1", 
    "author": "Robert Villa", 
    "publish": "2015-01-26T13:08:43Z", 
    "summary": "In this position paper we argue that certain aspects of relevance assessment\nin the evaluation of IR systems are oversimplified and that human assessments\nrepresented by qrels should be augmented to take account of contextual factors\nand the subjectivity of the task at hand. We propose enhancing test collections\nused in evaluation with information related to human assessors and their\ninterpretation of the task. Such augmented collections would provide a more\nrealistic and user-focused evaluation, enabling us to better understand the\nevaluation process, the performance of systems and user interactions. A first\nstep is to conduct user studies to examine in more detail what people actually\ndo when we ask them to judge the relevance of a document."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.06380v1", 
    "title": "Document Distance for the Automated Expansion of Relevance Judgements   for Information Retrieval Evaluation", 
    "arxiv-id": "1501.06380v1", 
    "author": "David Martinez", 
    "publish": "2015-01-26T13:21:09Z", 
    "summary": "This paper reports the use of a document distance-based approach to\nautomatically expand the number of available relevance judgements when these\nare limited and reduced to only positive judgements. This may happen, for\nexample, when the only available judgements are extracted from a list of\nreferences in a published review paper. We compare the results on two document\nsets: OHSUMED, based on medical research publications, and TREC-8, based on\nnews feeds. We show that evaluations based on these expanded relevance\njudgements are more reliable than those using only the initially available\njudgements, especially when the number of available judgements is very limited."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.06412v1", 
    "title": "The Anatomy of Relevance: Topical, Snippet and Perceived Relevance in   Search Result Evaluation", 
    "arxiv-id": "1501.06412v1", 
    "author": "Maarten de Rijke", 
    "publish": "2015-01-26T14:30:17Z", 
    "summary": "Currently, the quality of a search engine is often determined using so-called\ntopical relevance, i.e., the match between the user intent (expressed as a\nquery) and the content of the document. In this work we want to draw attention\nto two aspects of retrieval system performance affected by the presentation of\nresults: result attractiveness (\"perceived relevance\") and immediate usefulness\nof the snippets (\"snippet relevance\"). Perceived relevance may influence\ndiscoverability of good topical documents and seemingly better rankings may in\nfact be less useful to the user if good-looking snippets lead to irrelevant\ndocuments or vice-versa. And result items on a search engine result page (SERP)\nwith high snippet relevance may add towards the total utility gained by the\nuser even without the need to click those items.\n  We start by motivating the need to collect different aspects of relevance\n(topical, perceived and snippet relevances) and how these aspects can improve\nevaluation measures. We then discuss possible ways to collect these relevance\naspects using crowdsourcing and the challenges arising from that."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.06715v1", 
    "title": "Time Aware Knowledge Extraction for Microblog Summarization on Twitter", 
    "arxiv-id": "1501.06715v1", 
    "author": "Mimmo Parente", 
    "publish": "2015-01-27T09:49:54Z", 
    "summary": "Microblogging services like Twitter and Facebook collect millions of user\ngenerated content every moment about trending news, occurring events, and so\non. Nevertheless, it is really a nightmare to find information of interest\nthrough the huge amount of available posts that are often noise and redundant.\nIn general, social media analytics services have caught increasing attention\nfrom both side research and industry. Specifically, the dynamic context of\nmicroblogging requires to manage not only meaning of information but also the\nevolution of knowledge over the timeline. This work defines Time Aware\nKnowledge Extraction (briefly TAKE) methodology that relies on temporal\nextension of Fuzzy Formal Concept Analysis. In particular, a microblog\nsummarization algorithm has been defined filtering the concepts organized by\nTAKE in a time-dependent hierarchy. The algorithm addresses topic-based\nsummarization on Twitter. Besides considering the timing of the concepts,\nanother distinguish feature of the proposed microblog summarization framework\nis the possibility to have more or less detailed summary, according to the\nuser's needs, with good levels of quality and completeness as highlighted in\nthe experimental results."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1501.07716v1", 
    "title": "Attention Please! A Hybrid Resource Recommender Mimicking   Attention-Interpretation Dynamics", 
    "arxiv-id": "1501.07716v1", 
    "author": "Elisabeth Lex", 
    "publish": "2015-01-30T09:55:24Z", 
    "summary": "Classic resource recommenders like Collaborative Filtering (CF) treat users\nas being just another entity, neglecting non-linear user-resource dynamics\nshaping attention and interpretation. In this paper, we propose a novel hybrid\nrecommendation strategy that refines CF by capturing these dynamics. The\nevaluation results reveal that our approach substantially improves CF and,\ndepending on the dataset, successfully competes with a computationally much\nmore expensive Matrix Factorization variant."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1007.3208v1", 
    "title": "Link Graph Analysis for Adult Images Classification", 
    "arxiv-id": "1007.3208v1", 
    "author": "Dmitry Kotlyarov", 
    "publish": "2010-07-19T16:50:30Z", 
    "summary": "In order to protect an image search engine's users from undesirable results\nadult images' classifier should be built. The information about links from\nwebsites to images is employed to create such a classifier. These links are\nrepresented as a bipartite website-image graph. Each vertex is equipped with\nscores of adultness and decentness. The scores for image vertexes are\ninitialized with zero, those for website vertexes are initialized according to\na text-based website classifier. An iterative algorithm that propagates scores\nwithin a website-image graph is described. The scores obtained are used to\nclassify images by choosing an appropriate threshold. The experiments on\nInternet-scale data have shown that the algorithm under consideration increases\nclassification recall by 17% in comparison with a simple algorithm which\nclassifies an image as adult if it is connected with at least one adult site\n(at the same precision level)."
},{
    "category": "cs.IR", 
    "doi": "10.1111/exsy.12062", 
    "link": "http://arxiv.org/pdf/1007.4324v1", 
    "title": "Clustering Unstructured Data (Flat Files) - An Implementation in Text   Mining Tool", 
    "arxiv-id": "1007.4324v1", 
    "author": "Anis Noor Ali", 
    "publish": "2010-07-25T14:38:32Z", 
    "summary": "With the advancement of technology and reduced storage costs, individuals and\norganizations are tending towards the usage of electronic media for storing\ntextual information and documents. It is time consuming for readers to retrieve\nrelevant information from unstructured document collection. It is easier and\nless time consuming to find documents from a large collection when the\ncollection is ordered or classified by group or category. The problem of\nfinding best such grouping is still there. This paper discusses the\nimplementation of k-Means clustering algorithm for clustering unstructured text\ndocuments that we implemented, beginning with the representation of\nunstructured text and reaching the resulting set of clusters. Based on the\nanalysis of resulting clusters for a sample set of documents, we have also\nproposed a technique to represent documents that can further improve the\nclustering result."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijaia.2010.1305", 
    "link": "http://arxiv.org/pdf/1007.5137v1", 
    "title": "Comparison Of Modified Dual Ternary Indexing And Multi-Key Hashing   Algorithms For Music Information Retrieval", 
    "arxiv-id": "1007.5137v1", 
    "author": "Geetha T V", 
    "publish": "2010-07-29T07:47:04Z", 
    "summary": "In this work we have compared two indexing algorithms that have been used to\nindex and retrieve Carnatic music songs. We have compared a modified algorithm\nof the Dual ternary indexing algorithm for music indexing and retrieval with\nthe multi-key hashing indexing algorithm proposed by us. The modification in\nthe dual ternary algorithm was essential to handle variable length query phrase\nand to accommodate features specific to Carnatic music. The dual ternary\nindexing algorithm is adapted for Carnatic music by segmenting using the\nsegmentation technique for Carnatic music. The dual ternary algorithm is\ncompared with the multi-key hashing algorithm designed by us for indexing and\nretrieval in which features like MFCC, spectral flux, melody string and\nspectral centroid are used as features for indexing data into a hash table. The\nway in which collision resolution was handled by this hash table is different\nthan the normal hash table approaches. It was observed that multi-key hashing\nbased retrieval had a lesser time complexity than dual-ternary based indexing\nThe algorithms were also compared for their precision and recall in which\nmulti-key hashing had a better recall than modified dual ternary indexing for\nthe sample data considered."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijaia.2010.1305", 
    "link": "http://arxiv.org/pdf/1104.2982v1", 
    "title": "Multi-representation d'une ontologie : OWL, bases de donnees, syst\u00e8mes   de types et d'objets", 
    "arxiv-id": "1104.2982v1", 
    "author": "Thierry Despeyroux", 
    "publish": "2011-04-15T08:31:04Z", 
    "summary": "Due to the emergence of the semantic Web and the increasing need to formalize\nhuman knowledge, ontologie engineering is now an important activity. But is\nthis activity very different from other ones like software engineering, for\nexample ? In this paper, we investigate analogies between ontologies on one\nhand, types, objects and data bases on the other one, taking into account the\nnotion of evolution of an ontology. We represent a unique ontology using\ndifferent paradigms, and observe that the distance between these different\nconcepts is small. We deduce from this constatation that ontologies and more\nspecifically ontology description languages can take advantage of beeing\nfertilizated with some other computer science domains and inherit important\ncharacteristics as modularity, for example."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijaia.2010.1305", 
    "link": "http://arxiv.org/pdf/1104.3213v1", 
    "title": "Query Expansion Based on Clustered Results", 
    "arxiv-id": "1104.3213v1", 
    "author": "Yi Chen", 
    "publish": "2011-04-16T08:45:55Z", 
    "summary": "Query expansion is a functionality of search engines that suggests a set of\nrelated queries for a user-issued keyword query. Typical corpus-driven keyword\nquery expansion approaches return popular words in the results as expanded\nqueries. Using these approaches, the expanded queries may correspond to a\nsubset of possible query semantics, and thus miss relevant results. To handle\nambiguous queries and exploratory queries, whose result relevance is difficult\nto judge, we propose a new framework for keyword query expansion: we start with\nclustering the results according to user specified granularity, and then\ngenerate expanded queries, such that one expanded query is generated for each\ncluster whose result set should ideally be the corresponding cluster. We\nformalize this problem and show its APX-hardness. Then we propose two efficient\nalgorithms named iterative single-keyword refinement and partial elimination\nbased convergence, respectively, which effectively generate a set of expanded\nqueries from clustered results that provide a classification of the original\nquery results. We believe our study of generating an optimal query based on the\nground truth of the query results not only has applications in query expansion,\nbut has significance for studying keyword search quality in general."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcses.2011.2301", 
    "link": "http://arxiv.org/pdf/1109.0166v1", 
    "title": "Discovering the Impact of Knowledge in Recommender Systems: A   Comparative Study", 
    "arxiv-id": "1109.0166v1", 
    "author": "Mohd Shahizan Othman", 
    "publish": "2011-09-01T12:08:19Z", 
    "summary": "Recommender systems engage user profiles and appropriate filtering techniques\nto assist users in finding more relevant information over the large volume of\ninformation. User profiles play an important role in the success of\nrecommendation process since they model and represent the actual user needs.\nHowever, a comprehensive literature review of recommender systems has\ndemonstrated no concrete study on the role and impact of knowledge in user\nprofiling and filtering approache. In this paper, we review the most prominent\nrecommender systems in the literature and examine the impression of knowledge\nextracted from different sources. We then come up with this finding that\nsemantic information from the user context has substantial impact on the\nperformance of knowledge based recommender systems. Finally, some new clues for\nimprovement the knowledge-based profiles have been proposed."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcses.2011.2301", 
    "link": "http://arxiv.org/pdf/1109.0420v1", 
    "title": "Meta-song evaluation for chord recognition", 
    "arxiv-id": "1109.0420v1", 
    "author": "Tijl De Bie", 
    "publish": "2011-09-02T12:04:07Z", 
    "summary": "We present a new approach to evaluate chord recognition systems on songs\nwhich do not have full annotations. The principle is to use online chord\ndatabases to generate high accurate \"pseudo annotations\" for these songs and\ncompute \"pseudo accuracies\" of test systems. Statistical models that model the\nrelationship between \"pseudo accuracy\" and real performance are then applied to\nestimate test systems' performance. The approach goes beyond the existing\nevaluation metrics, allowing us to carry out extensive analysis on chord\nrecognition systems, such as their generalizations to different genres. In the\nexperiments we applied this method to evaluate three state-of-the-art chord\nrecognition systems, of which the results verified its reliability."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcses.2011.2301", 
    "link": "http://arxiv.org/pdf/1109.0530v1", 
    "title": "Orthogonal Query Expansion", 
    "arxiv-id": "1109.0530v1", 
    "author": "Alejandro Lopez-Ortiz", 
    "publish": "2011-09-02T19:50:20Z", 
    "summary": "Over the last fifteen years, web searching has seen tremendous improvements.\nStarting from a nearly random collection of matching pages in 1995, today,\nsearch engines tend to satisfy the user's informational need on well-formulated\nqueries. One of the main remaining challenges is to satisfy the users' needs\nwhen they provide a poorly formulated query. When the pages matching the user's\noriginal keywords are judged to be unsatisfactory, query expansion techniques\nare used to alter the result set. These techniques find keywords that are\nsimilar to the keywords given by the user, which are then appended to the\noriginal query leading to a perturbation of the result set. However, when the\noriginal query is sufficiently ill-posed, the user's informational need is best\nmet using entirely different keywords, and a small perturbation of the original\nresult set is bound to fail.\n  We propose a novel approach that is not based on the keywords of the original\nquery. We intentionally seek out orthogonal queries, which are related queries\nthat have low similarity to the user's query. The result sets of orthogonal\nqueries intersect with the result set of the original query on a small number\nof pages. An orthogonal query can access the user's informational need while\nconsisting of entirely different terms than the original query. We illustrate\nthe effectiveness of our approach by proposing a query expansion method derived\nfrom these observations that improves upon results obtained using the Yahoo\nBOSS infrastructure."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcses.2011.2301", 
    "link": "http://arxiv.org/pdf/1109.0732v2", 
    "title": "Multilingual ontology matching based on Wiktionary data accessible via   SPARQL endpoint", 
    "arxiv-id": "1109.0732v2", 
    "author": "Andrew Krizhanovsky", 
    "publish": "2011-09-04T17:08:23Z", 
    "summary": "Interoperability is a feature required by the Semantic Web. It is provided by\nthe ontology matching methods and algorithms. But now ontologies are presented\nnot only in English, but in other languages as well. It is important to use an\nautomatic translation for obtaining correct matching pairs in multilingual\nontology matching. The translation into many languages could be based on the\nGoogle Translate API, the Wiktionary database, etc. From the point of view of\nthe balance of presence of many languages, of manually crafted translations, of\na huge size of a dictionary, the most promising resource is the Wiktionary. It\nis a collaborative project working on the same principles as the Wikipedia. The\nparser of the Wiktionary was developed and the machine-readable dictionary was\ndesigned. The data of the machine-readable Wiktionary are stored in a\nrelational database, but with the help of D2R server the database is presented\nas an RDF store. Thus, it is possible to get lexicographic information\n(definitions, translations, synonyms) from web service using SPARQL requests.\nIn the case study, the problem entity is a task of multilingual ontology\nmatching based on Wiktionary data accessible via SPARQL endpoint. Ontology\nmatching results obtained using Wiktionary were compared with results based on\nGoogle Translate API."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.21423", 
    "link": "http://arxiv.org/pdf/1109.0916v1", 
    "title": "Ranking of Wikipedia articles in search engines revisited: Fair ranking   for reasonable quality?", 
    "arxiv-id": "1109.0916v1", 
    "author": "Ulrike Spree", 
    "publish": "2011-09-05T14:38:21Z", 
    "summary": "This paper aims to review the fiercely discussed question of whether the\nranking of Wikipedia articles in search engines is justified by the quality of\nthe articles. After an overview of current research on information quality in\nWikipedia, a summary of the extended discussion on the quality of encyclopedic\nentries in general is given. On this basis, a heuristic method for evaluating\nWikipedia entries is developed and applied to Wikipedia articles that scored\nhighly in a search engine retrieval effectiveness test and compared with the\nrelevance judgment of jurors. In all search engines tested, Wikipedia results\nare unanimously judged better by the jurors than other results on the\ncorresponding results position. Relevance judgments often roughly correspond\nwith the results from the heuristic evaluation. Cases in which high relevance\njudgments are not in accordance with the comparatively low score from the\nheuristic evaluation are interpreted as an indicator of a high degree of trust\nin Wikipedia. One of the systemic shortcomings of Wikipedia lies in its\nnecessarily incoherent user model. A further tuning of the suggested criteria\ncatalogue, for instance the different weighing of the supplied criteria, could\nserve as a starting point for a user model differentiated evaluation of\nWikipedia articles. Approved methods of quality evaluation of reference works\nare applied to Wikipedia articles and integrated with the question of search\nengine evaluation."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.21423", 
    "link": "http://arxiv.org/pdf/1109.1088v1", 
    "title": "A Framework for Business Intelligence Application using Ontological   Classification", 
    "arxiv-id": "1109.1088v1", 
    "author": "V. Prasanna Venkatesan", 
    "publish": "2011-09-06T07:07:26Z", 
    "summary": "Every business needs knowledge about their competitors to survive better. One\nof the information repositories is web. Retrieving Specific information from\nthe web is challenging. An Ontological model is developed to capture specific\ninformation by using web semantics. From the Ontology model, the relations\nbetween the data are mined using decision tree. From all these a new framework\nis developed for Business Intelligence."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.21423", 
    "link": "http://arxiv.org/pdf/1109.1989v1", 
    "title": "Efficient Personalized Web Mining: Utilizing The Most Utilized Data", 
    "arxiv-id": "1109.1989v1", 
    "author": "Dhinaharan Nagamalai", 
    "publish": "2011-09-09T13:00:18Z", 
    "summary": "Looking into the growth of information in the web it is a very tedious\nprocess of getting the exact information the user is looking for. Many search\nengines generate user profile related data listing. This paper involves one\nsuch process where the rating is given to the link that the user is clicking\non. Rather than avoiding the uninterested links both interested links and the\nuninterested links are listed. But sorted according to the weightings given to\neach link by the number of visit made by the particular user and the amount of\ntime spent on the particular link."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3309", 
    "link": "http://arxiv.org/pdf/1109.1991v1", 
    "title": "Effective Personalized Web Mining by Utilizing The Most Utilized Data", 
    "arxiv-id": "1109.1991v1", 
    "author": "Dhinaharan Nagamalai", 
    "publish": "2011-09-09T13:01:49Z", 
    "summary": "Looking into the growth of information in the web it is a very tedious\nprocess of getting the exact information the user is looking for. Many search\nengines generate user profile related data listing. This paper involves one\nsuch process where the rating is given to the link that the user is clicking\non. Rather than avoiding the uninterested links both interested links and the\nuninterested links are listed. But sorted according to the weightings given to\neach link by the number of visit made by the particular user and the amount of\ntime spent on the particular link."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3309", 
    "link": "http://arxiv.org/pdf/1109.2321v1", 
    "title": "Visualizing Domain Ontology using Enhanced Anaphora Resolution Algorithm", 
    "arxiv-id": "1109.2321v1", 
    "author": "A. Kannan", 
    "publish": "2011-09-11T15:08:11Z", 
    "summary": "Enormous explosion in the number of the World Wide Web pages occur every day\nand since the efficiency of most of the information processing systems is found\nto be less, the potential of the Internet applications is often underutilized.\nEfficient utilization of the web can be exploited when similar web pages are\nrigorously, exhaustively organized and clustered based on some domain knowledge\n(semantic-based) .Ontology which is a formal representation of domain knowledge\naids in such efficient utilization. The performance of almost all the\nsemantic-based clustering techniques depends on the constructed ontology,\ndescribing the domain knowledge . The proposed methodology provides an enhanced\npronominal anaphora resolution, one of the key aspects of semantic analysis in\nNatural Language Processing for obtaining cross references within a web page\nproviding better ontology construction. The experimental data sets exhibits\nbetter efficiency of the proposed method compared to earlier traditional\nalgorithms."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3309", 
    "link": "http://arxiv.org/pdf/1109.3138v1", 
    "title": "Folksodriven Structure Network", 
    "arxiv-id": "1109.3138v1", 
    "author": "Massimiliano Dal Mas", 
    "publish": "2011-09-14T17:06:21Z", 
    "summary": "Nowadays folksonomy is used as a system derived from user-generated\nelectronic tags or keywords that annotate and describe online content. But it\nis not a classification system as an ontology. To consider it as a\nclassification system it would be necessary to share a representation of\ncontexts by all the users. This paper is proposing the use of folksonomies and\nnetwork theory to devise a new concept: a \"Folksodriven Structure Network\" to\nrepresent folksonomies. This paper proposed and analyzed the network structure\nof Folksodriven tags thought as folsksonomy tags suggestions for the user on a\ndataset built on chosen websites. It is observed that the Folksodriven Network\nhas relative low path lengths checking it with classic networking measures\n(clustering coefficient). Experiment result shows it can facilitate\nserendipitous discovery of content among users. Neat examples and clear\nformulas can show how a \"Folksodriven Structure Network\" can be used to tackle\nontology mapping challenges."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3309", 
    "link": "http://arxiv.org/pdf/1109.4257v1", 
    "title": "Offering A Product Recommendation System in E-commerce", 
    "arxiv-id": "1109.4257v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2011-09-20T10:13:02Z", 
    "summary": "This paper proposes a number of explicit and implicit ratings in product\nrecommendation system for Business-to-customer e-commerce purposes. The system\nrecommends the products to a new user. It depends on the purchase pattern of\nprevious users whose purchase pattern is close to that of a user who asks for a\nrecommendation. The system is based on weighted cosine similarity measure to\nfind out the closest user profile among the profiles of all users in database.\nIt also implements Association rule mining rule in recommending the products.\nAlso, this product recommendation system takes into consideration the time of\ntransaction of purchasing the items, thus eliminating sequence recognition\nproblem. Experimental result shows for implicit rating, the proposed method\ngives acceptable performance in recommending the products. It also shows\nintroduction of association rule improves the performance measure of\nrecommendation system."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdms.2011.3309", 
    "link": "http://arxiv.org/pdf/1109.5053v1", 
    "title": "A New Approach to Design Graph Based Search Engine for Multiple Domains   Using Different Ontologies", 
    "arxiv-id": "1109.5053v1", 
    "author": "Sukanta Sinha", 
    "publish": "2011-09-23T12:27:25Z", 
    "summary": "Search Engine has become a major tool for searching any information from the\nWorld Wide Web (WWW). While searching the huge digital library available in the\nWWW, every effort is made to retrieve the most relevant results. But in WWW\nmajority of the Web pages are in HTML format and there are no such tags which\ntells the crawler to find any specific domain. To find more relevant result we\nuse Ontology for that particular domain. If we are working with multiple\ndomains then we use multiple ontologies. Now in order to design a domain\nspecific search engine for multiple domains, crawler must crawl through the\ndomain specific Web pages in the WWW according to the predefined ontologies."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2012.2190598", 
    "link": "http://arxiv.org/pdf/1109.5433v2", 
    "title": "Optimal Precoding Design and Power Allocation for Decentralized   Detection of Deterministic Signals", 
    "arxiv-id": "1109.5433v2", 
    "author": "Shaoqian Li", 
    "publish": "2011-09-26T03:09:28Z", 
    "summary": "We consider a decentralized detection problem in a power-constrained wireless\nsensor networks (WSNs), in which a number of sensor nodes collaborate to detect\nthe presence of a deterministic vector signal. The signal to be detected is\nassumed known \\emph{a priori}. Given a constraint on the total amount of\ntransmit power, we investigate the optimal linear precoding design for each\nsensor node. More specifically, in order to achieve the best detection\nperformance, shall sensor nodes transmit their raw data to the fusion center\n(FC), or transmit compressed versions of their original data? The optimal power\nallocation among sensors is studied as well. Also, assuming a fixed total\ntransmit power, we examine how the detection performance behaves with the\nnumber of sensors in the network. A new concept \"detection outage\" is proposed\nto quantify the reliability of the overall detection system. Finally,\ndecentralized detection with unknown signals is studied. Numerical results are\nconducted to corroborate our theoretical analysis and to illustrate the\nperformance of the proposed algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2012.2190598", 
    "link": "http://arxiv.org/pdf/1109.6206v1", 
    "title": "A Framework for Prefetching Relevant Web Pages using Predictive   Prefetching Engine (PPE)", 
    "arxiv-id": "1109.6206v1", 
    "author": "Amit Goel", 
    "publish": "2011-09-28T13:51:56Z", 
    "summary": "This paper presents a framework for increasing the relevancy of the web pages\nretrieved by the search engine. The approach introduces a Predictive\nPrefetching Engine (PPE) which makes use of various data mining algorithms on\nthe log maintained by the search engine. The underlying premise of the approach\nis that in the case of cluster accesses, the next pages requested by users of\nthe Web server are typically based on the current and previous pages requested.\nBased on same, rules are drawn which then lead the path for prefetching the\ndesired pages. To carry out the desired task of prefetching the more relevant\npages, agents have been introduced."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2012.2190598", 
    "link": "http://arxiv.org/pdf/1109.6726v1", 
    "title": "A Fuzzy Co-Clustering approach for Clickstream Data Pattern", 
    "arxiv-id": "1109.6726v1", 
    "author": "K. Thangavel", 
    "publish": "2011-09-30T06:45:41Z", 
    "summary": "Web Usage mining is a very important tool to extract the hidden business\nintelligence data from large databases. The extracted information provides the\norganizations with the ability to produce results more effectively to improve\ntheir businesses and increasing of sales. Co-clustering is a powerful\nbipartition technique which identifies group of users associated to group of\nweb pages. These associations are quantified to reveal the users' interest in\nthe different web pages' clusters. In this paper, Fuzzy Co-Clustering algorithm\nis proposed for clickstream data to identify the subset of users of similar\nnavigational behavior /interest over a subset of web pages of a website.\nTargeting the users group for various promotional activities is an important\naspect of marketing practices. Experiments are conducted on real dataset to\nprove the efficiency of proposed algorithm. The results and findings of this\nalgorithm could be used to enhance the marketing strategy for directing\nmarketing, advertisements for web based businesses and so on."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2012.2190598", 
    "link": "http://arxiv.org/pdf/1204.0182v1", 
    "title": "Hybrid Information Retrieval Model For Web Images", 
    "arxiv-id": "1204.0182v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-04-01T09:12:40Z", 
    "summary": "The Bing Bang of the Internet in the early 90's increased dramatically the\nnumber of images being distributed and shared over the web. As a result, image\ninformation retrieval systems were developed to index and retrieve image files\nspread over the Internet. Most of these systems are keyword-based which search\nfor images based on their textual metadata; and thus, they are imprecise as it\nis vague to describe an image with a human language. Besides, there exist the\ncontent-based image retrieval systems which search for images based on their\nvisual information. However, content-based type systems are still immature and\nnot that effective as they suffer from low retrieval recall/precision rate.\nThis paper proposes a new hybrid image information retrieval model for indexing\nand retrieving web images published in HTML documents. The distinguishing mark\nof the proposed model is that it is based on both graphical content and textual\nmetadata. The graphical content is denoted by color features and color\nhistogram of the image; while textual metadata are denoted by the terms that\nsurround the image in the HTML document, more particularly, the terms that\nappear in the tags p, h1, and h2, in addition to the terms that appear in the\nimage's alt attribute, filename, and class-label. Moreover, this paper presents\na new term weighting scheme called VTF-IDF short for Variable Term\nFrequency-Inverse Document Frequency which unlike traditional schemes, it\nexploits the HTML tag structure and assigns an extra bonus weight for terms\nthat appear within certain particular HTML tags that are correlated to the\nsemantics of the image. Experiments conducted to evaluate the proposed IR model\nshowed a high retrieval precision rate that outpaced other current models."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2012.2190598", 
    "link": "http://arxiv.org/pdf/1204.0186v1", 
    "title": "Semantic-Sensitive Web Information Retrieval Model for HTML Documents", 
    "arxiv-id": "1204.0186v1", 
    "author": "Paul Semaan", 
    "publish": "2012-04-01T09:50:42Z", 
    "summary": "With the advent of the Internet, a new era of digital information exchange\nhas begun. Currently, the Internet encompasses more than five billion online\nsites and this number is exponentially increasing every day. Fundamentally,\nInformation Retrieval (IR) is the science and practice of storing documents and\nretrieving information from within these documents. Mathematically, IR systems\nare at the core based on a feature vector model coupled with a term weighting\nscheme that weights terms in a document according to their significance with\nrespect to the context in which they appear. Practically, Vector Space Model\n(VSM), Term Frequency (TF), and Inverse Term Frequency (IDF) are among other\nlong-established techniques employed in mainstream IR systems. However, present\nIR models only target generic-type text documents, in that, they do not\nconsider specific formats of files such as HTML web documents. This paper\nproposes a new semantic-sensitive web information retrieval model for HTML\ndocuments. It consists of a vector model called SWVM and a weighting scheme\ncalled BTF-IDF, particularly designed to support the indexing and retrieval of\nHTML web documents. The chief advantage of the proposed model is that it\nassigns extra weights for terms that appear in certain pre-specified HTML tags\nthat are correlated to the semantics of the document. Additionally, the model\nis semantic-sensitive as it generates synonyms for every term being indexed and\nlater weights them appropriately to increase the likelihood of retrieving\ndocuments with similar context but different vocabulary terms. Experiments\nconducted, revealed a momentous enhancement in the precision of web IR systems\nand a radical increase in the number of relevant documents being retrieved. As\nfurther research, the proposed model is to be upgraded so as to support the\nindexing and retrieval of web images in multimedia-rich web documents."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.0309v1", 
    "title": "A Model for Personalized Keyword Extraction from Web Pages using   Segmentation", 
    "arxiv-id": "1204.0309v1", 
    "author": "G. Aghila", 
    "publish": "2012-04-02T04:00:16Z", 
    "summary": "The World Wide Web caters to the needs of billions of users in heterogeneous\ngroups. Each user accessing the World Wide Web might have his / her own\nspecific interest and would expect the web to respond to the specific\nrequirements. The process of making the web to react in a customized manner is\nachieved through personalization. This paper proposes a novel model for\nextracting keywords from a web page with personalization being incorporated\ninto it. The keyword extraction problem is approached with the help of web page\nsegmentation which facilitates in making the problem simpler and solving it\neffectively. The proposed model is implemented as a prototype and the\nexperiments conducted on it empirically validate the model's efficiency."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.1162v1", 
    "title": "Performance of the Google Desktop, Arabic Google Desktop and Peer to   Peer Application in Arabic Language", 
    "arxiv-id": "1204.1162v1", 
    "author": "Mazen El-Sayed", 
    "publish": "2012-04-05T09:38:23Z", 
    "summary": "The Arabic language is a complex language; it is different from Western\nlanguages especially at the morphological and spelling variations. Indeed, the\nperformance of information retrieval systems in the Arabic language is still a\nproblem. For this reason, we are interested in studying the performance of the\nmost famous search engine, which is a Google Desktop, while searching in Arabic\nlanguage documents. Then, we propose an update to the Google Desktop to take\ninto consideration in search the Arabic words that have the same root. After\nthat, we evaluate the performance of the Google Desktop in this context. Also,\nwe are interested in evaluation the performance of peer-to-peer application in\ntwo ways. The first one uses a simple indexation that indexes Arabic documents\nwithout taking in consideration the root of words. The second way takes in\nconsideration the roots in the indexation of Arabic documents. This evaluation\nis done by using a corpus of ten thousand documents and one hundred different\nqueries."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.1406v1", 
    "title": "An Effective Information Retrieval for Ambiguous Query", 
    "arxiv-id": "1204.1406v1", 
    "author": "S. K. Sahay", 
    "publish": "2012-04-06T04:37:38Z", 
    "summary": "Search engine returns thousands of web pages for a single user query, in\nwhich most of them are not relevant. In this context, effective information\nretrieval from the expanding web is a challenging task, in particular, if the\nquery is ambiguous. The major question arises here is that how to get the\nrelevant pages for an ambiguous query. We propose an approach for the effective\nresult of an ambiguous query by forming community vector based on association\nconcept of data minning using vector space model and the freedictionary. We\ndevelop clusters by computing the similarity between community vectors and\ndocument vectors formed from the extracted web pages by the search engine. We\nuse Gensim package to implement the algorithm because of its simplicity and\nrobust nature. Analysis shows that our approach is an effective way to form\nclusters for an ambiguous query."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.1528v1", 
    "title": "Extracting Geospatial Preferences Using Relational Neighbors", 
    "arxiv-id": "1204.1528v1", 
    "author": "Jord\u00e3o Ara\u00fajo", 
    "publish": "2012-04-06T18:15:55Z", 
    "summary": "With the increasing popularity of location-based social media applications\nand devices that automatically tag generated content with locations, large\nrepositories of collaborative geo-referenced data are appearing on-line.\nEfficiently extracting user preferences from these data to determine what\ninformation to recommend is challenging because of the sheer volume of data as\nwell as the frequency of updates. Traditional recommender systems focus on the\ninterplay between users and items, but ignore contextual parameters such as\nlocation. In this paper we take a geospatial approach to determine locational\npreferences and similarities between users. We propose to capture the\ngeographic context of user preferences for items using a relational graph,\nthrough which we are able to derive many new and state-of-the-art\nrecommendation algorithms, including combinations of them, requiring changes\nonly in the definition of the edge weights. Furthermore, we discuss several\nsolutions for cold-start scenarios. Finally, we conduct experiments using two\nreal-world datasets and provide empirical evidence that many of the proposed\nalgorithms outperform existing location-aware recommender algorithms."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.2032v3", 
    "title": "Multi-Output Recommender: Items, Groups and Friends, and Their Mutual   Contributing Effects", 
    "arxiv-id": "1204.2032v3", 
    "author": "Li Chen", 
    "publish": "2012-04-10T02:53:03Z", 
    "summary": "Due to the development of social media technology, it becomes easier for\nusers to gather together to form groups. Take the Last.fm for example, users\ncan join groups they may be interested where they can share their loved songs\nand discuss topics about songs and singers. However, the number of groups grows\nover time, users need effective groups recommendations in order to meet more\nlike-minded users."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.2245v1", 
    "title": "Development of a Conceptual Structure for a Domain-Specific Corpus", 
    "arxiv-id": "1204.2245v1", 
    "author": "Adel Elsayed", 
    "publish": "2012-04-10T18:47:49Z", 
    "summary": "The corpus reported in this paper was developed for the evaluation of a\ndomain-specific Text to Knowledge Mapping (TKM) prototype. The TKM prototype\noperates on the basis of both a combinatory categorical grammar (CCG)\nlinguistic model and a knowledge model that consists of three layers: ontology,\nqualitative and quantitative layers. In the course of this evaluation it was\nnecessary to populate these initial models with lexical items and semantic\nrelations. Both elements, the lexicon and semantic relations, are meant to\nreflect the domain of the prototype; hence both had to be extracted from the\ncorpus. While dealing with the lexicon was straight forward, the identification\nand extraction of appropriate semantic relations was much more involved. It was\nnecessary, therefore, to manually develop a conceptual structure for the domain\nwhich was then used to formulate a domain-specific framework of semantic\nrelations. The conceptual structure was developed using the Cmap tool of IHMC.\nThe framework of semantic relations- that has resulted from this study\nconsisted of 55 relations, out of which 42 have inverse relations."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.3362v2", 
    "title": "Event based classification of Web 2.0 text streams", 
    "arxiv-id": "1204.3362v2", 
    "author": "Christian Wolff", 
    "publish": "2012-04-16T04:50:13Z", 
    "summary": "Web 2.0 applications like Twitter or Facebook create a continuous stream of\ninformation. This demands new ways of analysis in order to offer insight into\nthis stream right at the moment of the creation of the information, because\nlots of this data is only relevant within a short period of time. To address\nthis problem real time search engines have recently received increased\nattention. They take into account the continuous flow of information\ndifferently than traditional web search by incorporating temporal and social\nfeatures, that describe the context of the information during its creation.\nStandard approaches where data first get stored and then is processed from a\nperistent storage suffer from latency. We want to address the fluent and rapid\nnature of text stream by providing an event based approach that analyses\ndirectly the stream of information. In a first step we want to define the\ndifference between real time search and traditional search to clarify the\ndemands in modern text filtering. In a second step we want to show how event\nbased features can be used to support the tasks of real time search engines.\nUsing the example of Twitter we present in this paper a way how to combine an\nevent based approach with text mining and information filtering concepts in\norder to classify incoming information based on stream features. We calculate\nstream dependant features and feed them into a neural network in order to\nclassify the text streams. We show the separative capabilities of event based\nfeatures as the foundation for a real time search engine."
},{
    "category": "cs.IR", 
    "doi": "10.5120/5682-7720", 
    "link": "http://arxiv.org/pdf/1204.5373v1", 
    "title": "TopSig: Topology Preserving Document Signatures", 
    "arxiv-id": "1204.5373v1", 
    "author": "Christopher M. De Vries", 
    "publish": "2012-04-24T13:36:39Z", 
    "summary": "Performance comparisons between File Signatures and Inverted Files for text\nretrieval have previously shown several significant shortcomings of file\nsignatures relative to inverted files. The inverted file approach underpins\nmost state-of-the-art search engine algorithms, such as Language and\nProbabilistic models. It has been widely accepted that traditional file\nsignatures are inferior alternatives to inverted files. This paper describes\nTopSig, a new approach to the construction of file signatures. Many advances in\nsemantic hashing and dimensionality reduction have been made in recent times,\nbut these were not so far linked to general purpose, signature file based,\nsearch engines. This paper introduces a different signature file approach that\nbuilds upon and extends these recent advances. We are able to demonstrate\nsignificant improvements in the performance of signature file based indexing\nand retrieval, performance that is comparable to that of state of the art\ninverted file based systems, including Language models and BM25. These findings\nsuggest that file signatures offer a viable alternative to inverted files in\nsuitable settings and from the theoretical perspective it positions the file\nsignatures model in the class of Vector Space retrieval models."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.0246v4", 
    "title": "Web Data Extraction, Applications and Techniques: A Survey", 
    "arxiv-id": "1207.0246v4", 
    "author": "Robert Baumgartner", 
    "publish": "2012-07-01T21:14:39Z", 
    "summary": "Web Data Extraction is an important problem that has been studied by means of\ndifferent scientific tools and in a broad range of applications. Many\napproaches to extracting data from the Web have been designed to solve specific\nproblems and operate in ad-hoc domains. Other approaches, instead, heavily\nreuse techniques and algorithms developed in the field of Information\nExtraction.\n  This survey aims at providing a structured and comprehensive overview of the\nliterature in the field of Web Data Extraction. We provided a simple\nclassification framework in which existing Web Data Extraction applications are\ngrouped into two main classes, namely applications at the Enterprise level and\nat the Social Web level. At the Enterprise level, Web Data Extraction\ntechniques emerge as a key tool to perform data analysis in Business and\nCompetitive Intelligence systems as well as for business process\nre-engineering. At the Social Web level, Web Data Extraction techniques allow\nto gather a large amount of structured data continuously generated and\ndisseminated by Web 2.0, Social Media and Online Social Network users and this\noffers unprecedented opportunities to analyze human behavior at a very large\nscale. We discuss also the potential of cross-fertilization, i.e., on the\npossibility of re-using Web Data Extraction techniques originally designed to\nwork in a given domain, in other domains."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.0446v1", 
    "title": "Medical Documents Classification Based on the Domain Ontology MeSH", 
    "arxiv-id": "1207.0446v1", 
    "author": "Taibi Malika", 
    "publish": "2012-07-02T17:00:51Z", 
    "summary": "This paper addresses the problem of classifying web documents using domain\nontology. Our goal is to provide a method for improving the classification of\nmedical documents by exploiting the MeSH thesaurus (Medical Subject Headings)\nwhich will allow us to generate a new representation based on concepts. This\napproach was tested with two well-known data mining algorithms C4.5 and KNN,\nand a comparison was made with the usual representation using stems. The\nenrichment of vectors using the concepts and the hyperonyms drawn from the\ndomain ontology has significantly boosted their representation, something\nessential for good classification. The results of our experiments on the\nbenchmark biomedical collection Ohsumed confirm the importance of the approach\nby a very significant improvement in the performance of the ontology-based\nclassification compared to the classical representation (Stems) by 30%."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.2615v3", 
    "title": "Broccoli: Semantic Full-Text Search at your Fingertips", 
    "arxiv-id": "1207.2615v3", 
    "author": "Elmar Haussmann", 
    "publish": "2012-07-11T12:29:35Z", 
    "summary": "We present Broccoli, a fast and easy-to-use search engine for what we call\nsemantic full-text search. Semantic full-text search combines the capabilities\nof standard full-text search and ontology search. The search operates on four\nkinds of objects: ordinary words (e.g., edible), classes (e.g., plants),\ninstances (e.g., Broccoli), and relations (e.g., occurs-with or native-to).\nQueries are trees, where nodes are arbitrary bags of these objects, and arcs\nare relations. The user interface guides the user in incrementally constructing\nsuch trees by instant (search-as-you-type) suggestions of words, classes,\ninstances, or relations that lead to good hits. Both standard full-text search\nand pure ontology search are included as special cases. In this paper, we\ndescribe the query language of Broccoli, the main idea behind a new kind of\nindex that enables fast processing of queries from that language as well as\nfast query suggestion, the natural language processing required, and the user\ninterface. We evaluated query times and result quality on the full version of\nthe English Wikipedia (40 GB XML dump) combined with the YAGO ontology (26\nmillion facts). We have implemented a fully functional prototype based on our\nideas and provide a web application to reproduce our quality experiments. Both\nare accessible via http://broccoli.informatik.uni-freiburg.de/repro-corr/ ."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.3583v1", 
    "title": "Information Retrieval Model: A Social Network Extraction Perspective", 
    "arxiv-id": "1207.3583v1", 
    "author": "Shahrul Azman Noah", 
    "publish": "2012-07-16T06:07:47Z", 
    "summary": "Future Information Retrieval, especially in connection with the internet,\nwill incorporate the content descriptions that are generated with social\nnetwork extraction technologies and preferably incorporate the probability\ntheory for assigning the semantic. Although there is an increasing interest\nabout social network extraction, but a little of them has a significant impact\nto infomation retrieval. Therefore this paper proposes a model of information\nretrieval from the social network extraction."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.3628v1", 
    "title": "Identify Web-page Content meaning using Knowledge based System for Dual   Meaning Words", 
    "arxiv-id": "1207.3628v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2012-07-16T11:20:23Z", 
    "summary": "Meaning of Web-page content plays a big role while produced a search result\nfrom a search engine. Most of the cases Web-page meaning stored in title or\nmeta-tag area but those meanings do not always match with Web-page content. To\novercome this situation we need to go through the Web-page content to identify\nthe Web-page meaning. In such cases, where Webpage content holds dual meaning\nwords that time it is really difficult to identify the meaning of the Web-page.\nIn this paper, we are introducing a new design and development mechanism of\nidentifying the Web-page content meaning which holds dual meaning words in\ntheir Web-page content."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1207.5745v1", 
    "title": "Semantic Information Retrieval Using Ontology In University Domain", 
    "arxiv-id": "1207.5745v1", 
    "author": "S. Swamynathan", 
    "publish": "2012-07-24T16:51:43Z", 
    "summary": "Today's conventional search engines hardly do provide the essential content\nrelevant to the user's search query. This is because the context and semantics\nof the request made by the user is not analyzed to the full extent. So here the\nneed for a semantic web search arises. SWS is upcoming in the area of web\nsearch which combines Natural Language Processing and Artificial Intelligence.\nThe objective of the work done here is to design, develop and implement a\nsemantic search engine- SIEU(Semantic Information Extraction in University\nDomain) confined to the university domain. SIEU uses ontology as a knowledge\nbase for the information retrieval process. It is not just a mere keyword\nsearch. It is one layer above what Google or any other search engines retrieve\nby analyzing just the keywords. Here the query is analyzed both syntactically\nand semantically. The developed system retrieves the web results more relevant\nto the user query through keyword expansion. The results obtained here will be\naccurate enough to satisfy the request made by the user. The level of accuracy\nwill be enhanced since the query is analyzed semantically. The system will be\nof great use to the developers and researchers who work on web. The Google\nresults are re-ranked and optimized for providing the relevant links. For\nranking an algorithm has been applied which fetches more apt results for the\nuser query."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1301.3885v1", 
    "title": "Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and   Model-Based Approach", 
    "arxiv-id": "1301.3885v1", 
    "author": "C. Lee Giles", 
    "publish": "2013-01-16T15:52:09Z", 
    "summary": "The growth of Internet commerce has stimulated the use of collaborative\nfiltering (CF) algorithms as recommender systems. Such systems leverage\nknowledge about the known preferences of multiple users to recommend items of\ninterest to other users. CF methods have been harnessed to make recommendations\nabout such items as web pages, movies, books, and toys. Researchers have\nproposed and evaluated many approaches for generating recommendations. We\ndescribe and evaluate a new method called emph{personality diagnosis (PD)}.\nGiven a user's preferences for some items, we compute the probability that he\nor she is of the same \"personality type\" as other users, and, in turn, the\nprobability that he or she will like new items. PD retains some of the\nadvantages of traditional similarity-weighting techniques in that all data is\nbrought to bear on each prediction and new data can be added easily and\nincrementally. Additionally, PD has a meaningful probabilistic interpretation,\nwhich may be leveraged to justify, explain, and augment results. We report\nempirical results on the EachMovie database of movie ratings, and on user\nprofile data collected from the CiteSeer digital library of Computer Science\nresearch papers. The probabilistic framework naturally supports a variety of\ndescriptive measurements - in particular, we consider the applicability of a\nvalue of information (VOI) computation."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1304.1924v1", 
    "title": "Automatic Detection of Search Tactic in Individual Information Seeking:   A Hidden Markov Model Approach", 
    "arxiv-id": "1304.1924v1", 
    "author": "Daqing He", 
    "publish": "2013-04-06T19:13:41Z", 
    "summary": "Information seeking process is an important topic in information seeking\nbehavior research. Both qualitative and empirical methods have been adopted in\nanalyzing information seeking processes, with major focus on uncovering the\nlatent search tactics behind user behaviors. Most of the existing works require\ndefining search tactics in advance and coding data manually. Among the few\nworks that can recognize search tactics automatically, they missed making sense\nof those tactics. In this paper, we proposed using an automatic technique, i.e.\nthe Hidden Markov Model (HMM), to explicitly model the search tactics. HMM\nresults show that the identified search tactics of individual information\nseeking behaviors are consistent with Marchioninis Information seeking process\nmodel. With the advantages of showing the connections between search tactics\nand search actions and the transitions among search tactics, we argue that HMM\nis a useful tool to investigate information seeking process, or at least it\nprovides a feasible way to analyze large scale dataset."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.knosys.2014.07.007", 
    "link": "http://arxiv.org/pdf/1304.2514v1", 
    "title": "Automatic Structuring Of Semantic Web Services An Approach", 
    "arxiv-id": "1304.2514v1", 
    "author": "J. M. Nandhini", 
    "publish": "2013-04-09T10:23:05Z", 
    "summary": "Ontologies have become the effective modeling for various applications and\nsignificantly in the semantic web. The difficulty of extracting information\nfrom the web, which was created mainly for visualising information, has driven\nthe birth of the semantic web, which will contain much more resources than the\nweb and will attach machine-readable semantic information to these resources.\nOntological bootstrapping on a set of predefined sources, such as web services,\nmust address the problem of multiple, largely unrelated concepts. The web\nservices consist of basically two components, Web Services Description Language\n(WSDL) descriptors and free text descriptors. The WSDL descriptor is evaluated\nusing two methods, namely Term Frequency/Inverse Document Frequency (TF/IDF)\nand web context generation. The proposed bootstrapping ontological process\nintegrates TF/IDF and web context generation and applies validation using the\nfree text descriptor service, so that, it offers more accurate definition of\nontologies. This paper uses ranking adaption model which predicts the rank for\na collection of web service documents which leads to the automatic\nconstruction, enrichment and adaptation of ontologies."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijwsc.2013.4101", 
    "link": "http://arxiv.org/pdf/1304.3268v1", 
    "title": "Web Services Discovery and Recommendation Based on Information   Extraction and Symbolic Reputation", 
    "arxiv-id": "1304.3268v1", 
    "author": "Zahi Jarir", 
    "publish": "2013-04-11T12:21:36Z", 
    "summary": "This paper shows that the problem of web services representation is crucial\nand analyzes the various factors that influence on it. It presents the\ntraditional representation of web services considering traditional textual\ndescriptions based on the information contained in WSDL files. Unfortunately,\ntextual web services descriptions are dirty and need significant cleaning to\nkeep only useful information. To deal with this problem, we introduce rules\nbased text tagging method, which allows filtering web service description to\nkeep only significant information. A new representation based on such filtered\ndata is then introduced. Many web services have empty descriptions. Also, we\nconsider web services representations based on the WSDL file structure (types,\nattributes, etc.). Alternatively, we introduce a new representation called\nsymbolic reputation, which is computed from relationships between web services.\nThe impact of the use of these representations on web service discovery and\nrecommendation is studied and discussed in the experimentation using real world\nweb services."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1304.3563v1", 
    "title": "Data, text and web mining for business intelligence: a survey", 
    "arxiv-id": "1304.3563v1", 
    "author": "Abdul-Aziz Rashid Al-Azmi", 
    "publish": "2013-04-12T08:04:31Z", 
    "summary": "The Information and Communication Technologies revolution brought a digital\nworld with huge amounts of data available. Enterprises use mining technologies\nto search vast amounts of data for vital insight and knowledge. Mining tools\nsuch as data mining, text mining, and web mining are used to find hidden\nknowledge in large databases or the Internet."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1304.3845v2", 
    "title": "The Impact of Situation Clustering in Contextual-Bandit Algorithm for   Context-Aware Recommender Systems", 
    "arxiv-id": "1304.3845v2", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-04-13T20:35:56Z", 
    "summary": "Most existing approaches in Context-Aware Recommender Systems (CRS) focus on\nrecommending relevant items to users taking into account contextual\ninformation, such as time, location, or social aspects. However, few of them\nhave considered the problem of user's content dynamicity. We introduce in this\npaper an algorithm that tackles the user's content dynamicity by modeling the\nCRS as a contextual bandit algorithm and by including a situation clustering\nalgorithm to improve the precision of the CRS. Within a deliberately designed\noffline simulation framework, we conduct evaluations with real online event log\ndata. The experimental results and detailed analysis reveal several important\ndiscoveries in context aware recommender system."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1304.6181v1", 
    "title": "Evaluating Web Content Quality via Multi-scale Features", 
    "arxiv-id": "1304.6181v1", 
    "author": "De-Xian Zhang", 
    "publish": "2013-04-23T06:42:55Z", 
    "summary": "Web content quality measurement is crucial to various web content processing\napplications. This paper will explore multi-scale features which may affect the\nquality of a host, and develop automatic statistical methods to evaluate the\nWeb content quality. The extracted properties include statistical content\nfeatures, page and host level link features and TFIDF features. The experiments\non ECML/PKDD 2010 Discovery Challenge data set show that the algorithm is\neffective and feasible for the quality tasks of multiple languages, and the\nmulti-scale features have different identification ability and provide good\ncomplement to each other for most tasks."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.0540v2", 
    "title": "Privacy Preserving Recommendation System Based on Groups", 
    "arxiv-id": "1305.0540v2", 
    "author": "Sanjeev Kulkarni", 
    "publish": "2013-05-02T19:17:08Z", 
    "summary": "Recommendation systems have received considerable attention in the recent\ndecades. Yet with the development of information technology and social media,\nthe risk in revealing private data to service providers has been a growing\nconcern to more and more users. Trade-offs between quality and privacy in\nrecommendation systems naturally arise. In this paper, we present a privacy\npreserving recommendation framework based on groups. The main idea is to use\ngroups as a natural middleware to preserve users' privacy. A distributed\npreference exchange algorithm is proposed to ensure the anonymity of data,\nwherein the effective size of the anonymity set asymptotically approaches the\ngroup size with time. We construct a hybrid collaborative filtering model based\non Markov random walks to provide recommendations and predictions to group\nmembers. Experimental results on the MovieLens and Epinions datasets show that\nour proposed methods outperform the baseline methods, L+ and ItemRank, two\nstate-of-the-art personalized recommendation algorithms, for both\nrecommendation precision and hit rate despite the absence of personal\npreference information."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.0939v1", 
    "title": "Intelligent Agent Based Semantic Web in Cloud Computing Environment", 
    "arxiv-id": "1305.0939v1", 
    "author": "Adarsha Palwe", 
    "publish": "2013-05-04T17:22:12Z", 
    "summary": "Considering today's web scenario, there is a need of effective and meaningful\nsearch over the web which is provided by Semantic Web. Existing search engines\nare keyword based. They are vulnerable in answering intelligent queries from\nthe user due to the dependence of their results on information available in web\npages. While semantic search engines provides efficient and relevant results as\nthe semantic web is an extension of the current web in which information is\ngiven well defined meaning. MetaCrawler is a search tool that uses several\nexisting search engines and provides combined results by using their own page\nranking algorithm. This paper proposes development of a meta-semantic-search\nengine called SemanTelli which works within cloud. SemanTelli fetches results\nfrom different semantic search engines such as Hakia, DuckDuckGo, SenseBot with\nthe help of intelligent agents that eliminate the limitations of existing\nsearch engines."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.1114v1", 
    "title": "Towards User Profile Modelling in Recommender System", 
    "arxiv-id": "1305.1114v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-05-06T08:42:19Z", 
    "summary": "The notion of profile appeared in the 1970s decade, which was mainly due to\nthe need to create custom applications that could be adapted to the user. In\nthis paper, we treat the different aspects of the user's profile, defining it,\nprofile, its features and its indicators of interest, and then we describe the\ndifferent approaches of modelling and acquiring the user's interests."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.1372v1", 
    "title": "Cold-start recommendation through granular association rules", 
    "arxiv-id": "1305.1372v1", 
    "author": "William Zhu", 
    "publish": "2013-05-07T01:08:27Z", 
    "summary": "Recommender systems are popular in e-commerce as they suggest items of\ninterest to users. Researchers have addressed the cold-start problem where\neither the user or the item is new. However, the situation with both new user\nand new item has seldom been considered. In this paper, we propose a cold-start\nrecommendation approach to this situation based on granular association rules.\nSpecifically, we provide a means for describing users and items through\ninformation granules, a means for generating association rules between users\nand items, and a means for recommending items to users using these rules.\nExperiments are undertaken on a publicly available dataset MovieLens. Results\nindicate that rule sets perform similarly on the training and the testing sets,\nand the appropriate setting of granule is essential to the application of\ngranular association rules."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.1429v1", 
    "title": "Speech User Interface for Information Retrieval", 
    "arxiv-id": "1305.1429v1", 
    "author": "Anjali Mahajan", 
    "publish": "2013-05-07T07:28:58Z", 
    "summary": "Along with the rapid development of information technology, the amount of\ninformation generated at a given time far exceeds human's ability to organize,\nsearch, and manipulate without the help of automatic systems. Now a days so\nmany tools and techniques are available for storage and retrieval of\ninformation. User uses interface to interact with these techniques, mostly text\nuser interface (TUI) or graphical user interface (GUI). Here, I am trying to\nintroduce a new interface i.e. speech for information retrieval. The goal of\nthis project is to develop a speech interface that can search and read the\nrequired information from the database effectively, efficiently and more\nfriendly. This tool will be highly useful to blind people, they will able to\ndemand the information to the computer by giving voice command/s (keyword)\nthrough microphone and listen the required information using speaker or\nheadphones."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.1745v1", 
    "title": "Mobile Recommender Systems Methods: An Overview", 
    "arxiv-id": "1305.1745v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-05-08T08:38:04Z", 
    "summary": "The information that mobiles can access becomes very wide nowadays, and the\nuser is faced with a dilemma: there is an unlimited pool of information\navailable to him but he is unable to find the exact information he is looking\nfor. This is why the current research aims to design Recommender Systems (RS)\nable to continually send information that matches the user's interests in order\nto reduce his navigation time. In this paper, we treat the different approaches\nto recommend."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.1787v1", 
    "title": "Evolution of the user's content: An Overview of the state of the art", 
    "arxiv-id": "1305.1787v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-05-08T11:52:39Z", 
    "summary": "The evolution of the user's content still remains a problem for an accurate\nrecommendation.This is why the current research aims to design Recommender\nSystems (RS) able to continually adapt information that matches the user's\ninterests. This paper aims to explain this problematic point in outlining the\nproposals that have been made in research with their advantages and\ndisadvantages."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijdkp.2013.3201", 
    "link": "http://arxiv.org/pdf/1305.2686v1", 
    "title": "Using Exclusive Web Crawlers to Store Better Results in Search Engines'   Database", 
    "arxiv-id": "1305.2686v1", 
    "author": "Amir Seyed Danesh", 
    "publish": "2013-05-13T07:06:30Z", 
    "summary": "Crawler-based search engines are the mostly used search engines among web and\nInternet users, involve web crawling, storing in database, ranking, indexing\nand displaying to the user. But it is noteworthy that because of increasing\nchanges in web sites search engines suffer high time and transfers costs which\nare consumed to investigate the existence of each page in database while\ncrawling, updating database and even investigating its existence in any\ncrawling operations. \"Exclusive Web Crawler\" proposes guidelines for crawling\nfeatures, links, media and other elements and to store crawling results in a\ncertain table in its database on the web. With doing this, search engines store\neach site's tables in their databases and implement their ranking results on\nthem. Thus, accuracy of data in every table (and its being up-to-date) is\nensured and no 404 result is shown in search results since, in fact, this data\ncrawler crawls data entered by webmaster and the database stores whatever he\nwants to display."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.2755v1", 
    "title": "Clustering Web Search Results For Effective Arabic Language Browsing", 
    "arxiv-id": "1305.2755v1", 
    "author": "Abdelmonaime Lachkar", 
    "publish": "2013-05-13T12:28:34Z", 
    "summary": "The process of browsing Search Results is one of the major problems with\ntraditional Web search engines for English, European, and any other languages\ngenerally, and for Arabic Language particularly. This process is absolutely\ntime consuming and the browsing style seems to be unattractive. Organizing Web\nsearch results into clusters facilitates users quick browsing through search\nresults. Traditional clustering techniques (data-centric clustering algorithms)\nare inadequate since they don't generate clusters with highly readable names or\ncluster labels. To solve this problem, Description-centric algorithms such as\nSuffix Tree Clustering (STC) algorithm have been introduced and used\nsuccessfully and extensively with different adapted versions for English,\nEuropean, and Chinese Languages. However, till the day of writing this paper,\nin our knowledge, STC algorithm has been never applied for Arabic Web Snippets\nSearch Results Clustering.In this paper, we propose first, to study how STC can\nbe applied for Arabic Language? We then illustrate by example that is\nimpossible to apply STC after Arabic Snippets pre-processing (stem or root\nextraction) because the Merging process yields many redundant clusters.\nSecondly, to overcome this problem, we propose to integrate STC in a new scheme\ntaking into a count the Arabic language properties in order to get the web more\nand more adapted to Arabic users. The proposed approach automatically clusters\nthe web search results into high quality, and high significant clusters labels.\nThe obtained clusters not only are coherent, but also can convey the contents\nto the users concisely and accurately. Therefore the Arabic users can decide at\na glance whether the contents of a cluster are of interest....."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.2831v1", 
    "title": "Test Model for Text Categorization and Text Summarization", 
    "arxiv-id": "1305.2831v1", 
    "author": "Urmila Shrawankar", 
    "publish": "2013-05-10T08:06:15Z", 
    "summary": "Text Categorization is the task of automatically sorting a set of documents\ninto categories from a predefined set and Text Summarization is a brief and\naccurate representation of input text such that the output covers the most\nimportant concepts of the source in a condensed manner. Document Summarization\nis an emerging technique for understanding the main purpose of any kind of\ndocuments. This paper presents a model that uses text categorization and text\nsummarization for searching a document based on user query."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.4801v1", 
    "title": "Mining top-k granular association rules for recommendation", 
    "arxiv-id": "1305.4801v1", 
    "author": "William Zhu", 
    "publish": "2013-05-21T12:44:30Z", 
    "summary": "Recommender systems are important for e-commerce companies as well as\nresearchers. Recently, granular association rules have been proposed for\ncold-start recommendation. However, existing approaches reserve only globally\nstrong rules; therefore some users may receive no recommendation at all. In\nthis paper, we propose to mine the top-k granular association rules for each\nuser. First we define three measures of granular association rules. These are\nthe source coverage which measures the user granule size, the target coverage\nwhich measures the item granule size, and the confidence which measures the\nstrength of the association. With the confidence measure, rules can be ranked\naccording to their strength. Then we propose algorithms for training the\nrecommender and suggesting items to each user. Experimental are undertaken on a\npublicly available data set MovieLens. Results indicate that the appropriate\nsetting of granule can avoid over-fitting and at the same time, help obtaining\nhigh recommending accuracy."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.4820v1", 
    "title": "Nouvelle approche de recommandation personnalisee dans les folksonomies   basee sur le profil des utilisateurs", 
    "arxiv-id": "1305.4820v1", 
    "author": "Engelbert Mephu Nguifo", 
    "publish": "2013-05-21T13:59:51Z", 
    "summary": "In folksonomies, users use to share objects (movies, books, bookmarks, etc.)\nby annotating them with a set of tags of their own choice. With the rise of the\nWeb 2.0 age, users become the core of the system since they are both the\ncontributors and the creators of the information. Yet, each user has its own\nprofile and its own ideas making thereby the strength as well as the weakness\nof folksonomies. Indeed, it would be helpful to take account of users' profile\nwhen suggesting a list of tags and resources or even a list of friends, in\norder to make a personal recommandation, instead of suggesting the more used\ntags and resources in the folksonomy. In this paper, we consider users' profile\nas a new dimension of a folksonomy classically composed of three dimensions\n<users, tags, ressources> and we propose an approach to group users with\nequivalent profiles and equivalent interests as quadratic concepts. Then, we\nuse such structures to propose our personalized recommendation system of users,\ntags and resources according to each user's profile. Carried out experiments on\ntwo real-world datasets, i.e., MovieLens and BookCrossing highlight encouraging\nresults in terms of precision as well as a good social evaluation."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.5330v1", 
    "title": "A toy model of information retrieval system based on quantum probability", 
    "arxiv-id": "1305.5330v1", 
    "author": "Roman Zapatrin", 
    "publish": "2013-05-23T06:49:20Z", 
    "summary": "Recent numerical results show that non-Bayesian knowledge revision may be\nhelpful in search engine training and optimization. In order to demonstrate how\nbasic assumption about about the physical nature (and hence the observed\nstatistics) of retrieved documents can affect the performance of search engines\nwe suggest an idealized toy model with minimal number of parameters."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1305.5959v2", 
    "title": "ArcLink: Optimization Techniques to Build and Retrieve the Temporal Web   Graph", 
    "arxiv-id": "1305.5959v2", 
    "author": "Michael L. Nelson", 
    "publish": "2013-05-25T19:18:05Z", 
    "summary": "Archiving the web is socially and culturally critical, but presents problems\nof scale. The Internet Archive's Wayback Machine can replay captured web pages\nas they existed at a certain point in time, but it has limited ability to\nprovide extensive content and structural metadata about the web graph. While\nthe live web has developed a rich ecosystem of APIs to facilitate web\napplications (e.g., APIs from Google and Twitter), the web archiving community\nhas not yet broadly implemented this level of access.\n  We present ArcLink, a proof-of-concept system that complements open source\nWayback Machine installations by optimizing the construction, storage, and\naccess to the temporal web graph. We divide the web graph construction into\nfour stages (filtering, extraction, storage, and access) and explore\noptimization for each stage. ArcLink extends the current Web archive interfaces\nto return content and structural metadata for each URI. We show how this API\ncan be applied to such applications as retrieving inlinks, outlinks,\nanchortext, and PageRank."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1306.2597v1", 
    "title": "Introducing LETOR 4.0 Datasets", 
    "arxiv-id": "1306.2597v1", 
    "author": "Tie-Yan Liu", 
    "publish": "2013-06-09T09:58:00Z", 
    "summary": "LETOR is a package of benchmark data sets for research on LEarning TO Rank,\nwhich contains standard features, relevance judgments, data partitioning,\nevaluation tools, and several baselines. Version 1.0 was released in April\n2007. Version 2.0 was released in Dec. 2007. Version 3.0 was released in Dec.\n2008. This version, 4.0, was released in July 2009. Very different from\nprevious versions (V3.0 is an update based on V2.0 and V2.0 is an update based\non V1.0), LETOR4.0 is a totally new release. It uses the Gov2 web page\ncollection (~25M pages) and two query sets from Million Query track of TREC\n2007 and TREC 2008. We call the two query sets MQ2007 and MQ2008 for short.\nThere are about 1700 queries in MQ2007 with labeled documents and about 800\nqueries in MQ2008 with labeled documents. If you have any questions or\nsuggestions about the datasets, please kindly email us (letor@microsoft.com).\nOur goal is to make the dataset reliable and useful for the community."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijnlc.2013.2202", 
    "link": "http://arxiv.org/pdf/1306.3955v1", 
    "title": "The Number of Terms and Documents for Pseudo-Relevant Feedback for   Ad-hoc Information Retrieval", 
    "arxiv-id": "1306.3955v1", 
    "author": "Abderrahim Mohammed Alaeddine", 
    "publish": "2013-06-17T19:14:12Z", 
    "summary": "In Information Retrieval System (IRS), the Automatic Relevance Feedback (ARF)\nis a query reformulation technique that modifies the initial one without the\nuser intervention. It is applied mainly through the addition of terms coming\nfrom the external resources such as the ontologies and or the results of the\ncurrent research. In this context we are mainly interested in the local\nanalysis technique for the ARF in ad-hoc IRS on Arabic documents. In this\narticle, we have examined the impact of the variation of the two parameters\nimplied in this technique, that is to say, the number of the documents\n{\\guillemotleft}D{\\guillemotright} and the number of terms\n{\\guillemotleft}T{\\guillemotright}, on an Arabic IRS performance. The\nexperimentation, carried out on an Arabic corpus text, enables us to deduce\nthat there are queries which are not easily improvable with the query\nreformulation. In addition, the success of the ARF is due mainly to the\nselection of a sufficient number of documents D and to the extraction of a very\nreduced set of relevant terms T for retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1306.4427v1", 
    "title": "Multidimensional User Data Model for Web Personalization", 
    "arxiv-id": "1306.4427v1", 
    "author": "Surekha Mariam Varghese", 
    "publish": "2013-06-19T05:25:45Z", 
    "summary": "Personalization is being applied to great extend in many systems. This paper\npresents a multi-dimensional user data model and its application in web search.\nOnline and Offline activities of the user are tracked for creating the user\nmodel. The main phases are identification of relevant documents and the\nrepresentation of relevance and similarity of the documents. The concepts\nKeywords, Topics, URLs and clusters are used in the implementation. The\nalgorithms for profiling, grading and clustering the concepts in the user model\nand algorithm for determining the personalized search results by re-ranking the\nresults in a search bank are presented in this paper. Simple experiments for\nevaluation of the model and their results are described."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1306.4606v1", 
    "title": "Keyphrase Cloud Generation of Broadcast News", 
    "arxiv-id": "1306.4606v1", 
    "author": "Jo\u00e3o Paulo da Silva Neto", 
    "publish": "2013-06-19T16:37:23Z", 
    "summary": "This paper describes an enhanced automatic keyphrase extraction method\napplied to Broadcast News. The keyphrase extraction process is used to create a\nconcept level for each news. On top of words resulting from a speech\nrecognition system output and news indexation and it contributes to the\ngeneration of a tag/keyphrase cloud of the top news included in a Multimedia\nMonitoring Solution system for TV and Radio news/programs, running daily, and\nmonitoring 12 TV channels and 4 Radios."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1306.4608v1", 
    "title": "Hourly Traffic Prediction of News Stories", 
    "arxiv-id": "1306.4608v1", 
    "author": "Jaime Carbonell", 
    "publish": "2013-06-19T16:44:16Z", 
    "summary": "The process of predicting news stories popularity from several news sources\nhas become a challenge of great importance for both news producers and readers.\nIn this paper, we investigate methods for automatically predicting the number\nof clicks on a news story during one hour. Our approach is a combination of\nadditive regression and bagging applied over a M5P regression tree using a\nlogarithmic scale (log10). The features included are social-based (social\nnetwork metadata from Facebook), content-based (automatically extracted\nkeyphrases, and stylometric statistics from news titles), and time-based. In\n1st Sapo Data Challenge we obtained 11.99% as mean relative error value which\nput us in the 4th place out of 26 participants."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1307.1024v1", 
    "title": "Overview of Web Content Mining Tools", 
    "arxiv-id": "1307.1024v1", 
    "author": "Mahieddine Djoudi", 
    "publish": "2013-07-02T19:57:29Z", 
    "summary": "Nowadays, the Web has become one of the most widespread platforms for\ninformation change and retrieval. As it becomes easier to publish documents, as\nthe number of users, and thus publishers, increases and as the number of\ndocuments grows, searching for information is turning into a cumbersome and\ntime-consuming operation. Due to heterogeneity and unstructured nature of the\ndata available on the WWW, Web mining uses various data mining techniques to\ndiscover useful knowledge from Web hyperlinks, page content and usage log. The\nmain uses of web content mining are to gather, categorize, organize and provide\nthe best possible information available on the Web to the user requesting the\ninformation. The mining tools are imperative to scanning the many HTML\ndocuments, images, and text. Then, the result is used by the search engines. In\nthis paper, we first introduce the concepts related to web mining; we then\npresent an overview of different Web Content Mining tools. We conclude by\npresenting a comparative table of these tools based on some pertinent criteria."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1307.1179v1", 
    "title": "Future Web Growth and its Consequences for Web Search Architectures", 
    "arxiv-id": "1307.1179v1", 
    "author": "Jinglan Zhang", 
    "publish": "2013-07-04T00:38:41Z", 
    "summary": "Introduction: Before embarking on the design of any computer system it is\nfirst necessary to assess the magnitude of the problem. In the case of a web\nsearch engine this assessment amounts to determining the current size of the\nweb, the growth rate of the web, and the quantity of computing resource\nnecessary to search it, and projecting the historical growth of this into the\nfuture. Method: The over 20 year history of the web makes it possible to make\nshort-term projections on future growth. The longer history of hard disk drives\n(and smart phone memory card) makes it possible to make short-term hardware\nprojections. Analysis: Historical data on Internet uptake and hardware growth\nis extrapolated. Results: It is predicted that within a decade the storage\ncapacity of a single hard drive will exceed the size of the index of the web at\nthat time. Within another decade it will be possible to store the entire\nsearchable text on the same hard drive. Within another decade the entire\nsearchable web (including images) will also fit. Conclusion: This result raises\nquestions about the future architecture of search engines. Several new models\nare proposed. In one model the user's computer is an active part of the\ndistributed search architecture. They search a pre-loaded snapshot (back-file)\nof the web on their local device which frees up the online data centre for\nsearching just the difference between the snapshot and the current time.\nAdvantageously this also makes it possible to search when the user is\ndisconnected from the Internet. In another model all changes to all files are\nbroadcast to all users (forming a star-like network) and no data centre is\nneeded."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1307.1718v2", 
    "title": "Graph-based Approach to Automatic Taxonomy Generation (GraBTax)", 
    "arxiv-id": "1307.1718v2", 
    "author": "C. Lee Giles", 
    "publish": "2013-07-05T21:05:20Z", 
    "summary": "We propose a novel graph-based approach for constructing concept hierarchy\nfrom a large text corpus. Our algorithm, GraBTax, incorporates both statistical\nco-occurrences and lexical similarity in optimizing the structure of the\ntaxonomy. To automatically generate topic-dependent taxonomies from a large\ntext corpus, GraBTax first extracts topical terms and their relationships from\nthe corpus. The algorithm then constructs a weighted graph representing topics\nand their associations. A graph partitioning algorithm is then used to\nrecursively partition the topic graph into a taxonomy. For evaluation, we apply\nGraBTax to articles, primarily computer science, in the CiteSeerX digital\nlibrary and search engine. The quality of the resulting concept hierarchy is\nassessed by both human judges and comparison with Wikipedia categories."
},{
    "category": "cs.IR", 
    "doi": "10.5120/11896-7955", 
    "link": "http://arxiv.org/pdf/1307.2669v1", 
    "title": "Text Categorization via Similarity Search: An Efficient and Effective   Novel Algorithm", 
    "arxiv-id": "1307.2669v1", 
    "author": "Varun Singla", 
    "publish": "2013-07-10T04:41:19Z", 
    "summary": "We present a supervised learning algorithm for text categorization which has\nbrought the team of authors the 2nd place in the text categorization division\nof the 2012 Cybersecurity Data Mining Competition (CDMC'2012) and a 3rd prize\noverall. The algorithm is quite different from existing approaches in that it\nis based on similarity search in the metric space of measure distributions on\nthe dictionary. At the preprocessing stage, given a labeled learning sample of\ntexts, we associate to every class label (document category) a point in the\nspace of question. Unlike it is usual in clustering, this point is not a\ncentroid of the category but rather an outlier, a uniform measure distribution\non a selection of domain-specific words. At the execution stage, an unlabeled\ntext is assigned a text category as defined by the closest labeled neighbour to\nthe point representing the frequency distribution of the words in the text. The\nalgorithm is both effective and efficient, as further confirmed by experiments\non the Reuters 21578 dataset."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.3284v1", 
    "title": "Sequential Selection of Correlated Ads by POMDPs", 
    "arxiv-id": "1307.3284v1", 
    "author": "Jun Wang", 
    "publish": "2013-07-11T22:20:32Z", 
    "summary": "Online advertising has become a key source of revenue for both web search\nengines and online publishers. For them, the ability of allocating right ads to\nright webpages is critical because any mismatched ads would not only harm web\nusers' satisfactions but also lower the ad income. In this paper, we study how\nonline publishers could optimally select ads to maximize their ad incomes over\ntime. The conventional offline, content-based matching between webpages and ads\nis a fine start but cannot solve the problem completely because good matching\ndoes not necessarily lead to good payoff. Moreover, with the limited display\nimpressions, we need to balance the need of selecting ads to learn true ad\npayoffs (exploration) with that of allocating ads to generate high immediate\npayoffs based on the current belief (exploitation). In this paper, we address\nthe problem by employing Partially observable Markov decision processes\n(POMDPs) and discuss how to utilize the correlation of ads to improve the\nefficiency of the exploration and increase ad incomes in a long run. Our\nmathematical derivation shows that the belief states of correlated ads can be\nnaturally updated using a formula similar to collaborative filtering. To test\nour model, a real world ad dataset from a major search engine is collected and\ncategorized. Experimenting over the data, we provide an analyse of the effect\nof the underlying parameters, and demonstrate that our algorithms significantly\noutperform other strong baselines."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.3573v1", 
    "title": "Adaptive Keywords Extraction with Contextual Bandits for Advertising on   Parked Domains", 
    "arxiv-id": "1307.3573v1", 
    "author": "Maurice van der Meer", 
    "publish": "2013-07-12T20:32:42Z", 
    "summary": "Domain name registrars and URL shortener service providers place\nadvertisements on the parked domains (Internet domain names which are not in\nservice) in order to generate profits. As the web contents have been removed,\nit is critical to make sure the displayed ads are directly related to the\nintents of the visitors who have been directed to the parked domains. Because\nof the missing contents in these domains, it is non-trivial to generate the\nkeywords to describe the previous contents and therefore the users intents. In\nthis paper we discuss the adaptive keywords extraction problem and introduce an\nalgorithm based on the BM25F term weighting and linear multi-armed bandits. We\nbuilt a prototype over a production domain registration system and evaluated it\nusing crowdsourcing in multiple iterations. The prototype is compared with\nother popular methods and is shown to be more effective."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.3855v1", 
    "title": "GAPfm: Optimal Top-N Recommendations for Graded Relevance Domains", 
    "arxiv-id": "1307.3855v1", 
    "author": "Alan Hanjalic", 
    "publish": "2013-07-15T08:55:11Z", 
    "summary": "Recommender systems are frequently used in domains in which users express\ntheir preferences in the form of graded judgments, such as ratings. If accurate\ntop-N recommendation lists are to be produced for such graded relevance\ndomains, it is critical to generate a ranked list of recommended items directly\nrather than predicting ratings. Current techniques choose one of two\nsub-optimal approaches: either they optimize for a binary metric such as\nAverage Precision, which discards information on relevance grades, or they\noptimize for Normalized Discounted Cumulative Gain (NDCG), which ignores the\ndependence of an item's contribution on the relevance of more highly ranked\nitems.\n  In this paper, we address the shortcomings of existing approaches by\nproposing the Graded Average Precision factor model (GAPfm), a latent factor\nmodel that is particularly suited to the problem of top-N recommendation in\ndomains with graded relevance data. The model optimizes for Graded Average\nPrecision, a metric that has been proposed recently for assessing the quality\nof ranked results list for graded relevance. GAPfm learns a latent factor model\nby directly optimizing a smoothed approximation of GAP. GAPfm's advantages are\ntwofold: it maintains full information about graded relevance and also\naddresses the limitations of models that optimize NDCG. Experimental results\nshow that GAPfm achieves substantial improvements on the top-N recommendation\ntask, compared to several state-of-the-art approaches. In order to ensure that\nGAPfm is able to scale to very large data sets, we propose a fast learning\nalgorithm that uses an adaptive item selection strategy. A final experiment\nshows that GAPfm is useful not only for generating recommendation lists, but\nalso for ranking a given list of rated items."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.4063v1", 
    "title": "Reading the Correct History? Modeling Temporal Intention in Resource   Sharing", 
    "arxiv-id": "1307.4063v1", 
    "author": "Michael L. Nelson", 
    "publish": "2013-07-15T19:28:10Z", 
    "summary": "The web is trapped in the \"perpetual now\", and when users traverse from page\nto page, they are seeing the state of the web resource (i.e., the page) as it\nexists at the time of the click and not necessarily at the time when the link\nwas made. Thus, a temporal discrepancy can arise between the resource at the\ntime the page author created a link to it and the time when a reader follows\nthe link. This is especially important in the context of social media: the ease\nof sharing links in a tweet or Facebook post allows many people to author web\ncontent, but the space constraints combined with poor awareness by authors\noften prevents sufficient context from being generated to determine the intent\nof the post. If the links are clicked as soon as they are shared, the temporal\ndistance between sharing and clicking is so small that there is little to no\ndifference in content. However, not all clicks occur immediately, and a delay\nof days or even hours can result in reading something other than what the\nauthor intended. We introduce the concept of a user's temporal intention upon\npublishing a link in social media. We investigate the features that could be\nextracted from the post, the linked resource, and the patterns of social\ndissemination to model this user intention. Finally, we analyze the historical\nintegrity of the shared resources in social media across time. In other words,\nhow much is the knowledge of the author's intent beneficial in maintaining the\nconsistency of the story being told through social posts and in enriching the\narchived content coverage and depth of vulnerable resources?"
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.6080v2", 
    "title": "Timely crawling of high-quality ephemeral new content", 
    "arxiv-id": "1307.6080v2", 
    "author": "Pavel Serdyukov", 
    "publish": "2013-07-23T13:52:14Z", 
    "summary": "Nowadays, more and more people use the Web as their primary source of\nup-to-date information. In this context, fast crawling and indexing of newly\ncreated Web pages has become crucial for search engines, especially because\nuser traffic to a significant fraction of these new pages (like news, blog and\nforum posts) grows really quickly right after they appear, but lasts only for\nseveral days.\n  In this paper, we study the problem of timely finding and crawling of such\nephemeral new pages (in terms of user interest). Traditional crawling policies\ndo not give any particular priority to such pages and may thus crawl them not\nquickly enough, and even crawl already obsolete content. We thus propose a new\nmetric, well thought out for this task, which takes into account the decrease\nof user interest for ephemeral pages over time.\n  We show that most ephemeral new pages can be found at a relatively small set\nof content sources and present a procedure for finding such a set. Our idea is\nto periodically recrawl content sources and crawl newly created pages linked\nfrom them, focusing on high-quality (in terms of user interest) content. One of\nthe main difficulties here is to divide resources between these two activities\nin an efficient way. We find the adaptive balance between crawls and recrawls\nby maximizing the proposed metric. Further, we incorporate search engine click\nlogs to give our crawler an insight about the current user demands. Efficiency\nof our approach is finally demonstrated experimentally on real-world data."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.6422v1", 
    "title": "Mesure de la similarit\u00e9 entre termes et labels de concepts   ontologiques", 
    "arxiv-id": "1307.6422v1", 
    "author": "Mauro Gaio", 
    "publish": "2013-07-24T13:59:48Z", 
    "summary": "We propose in this paper a method for measuring the similarity between\nontological concepts and terms. Our metric can take into account not only the\ncommon words of two strings to compare but also other features such as the\nposition of the words in these strings, or the number of deletion, insertion or\nreplacement of words required for the construction of one of the two strings\nfrom each other. The proposed method was then used to determine the ontological\nconcepts which are equivalent to the terms that qualify toponymes. It aims to\nfind the topographical type of the toponyme."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1307.7974v1", 
    "title": "Image Tag Refinement by Regularized Latent Dirichlet Allocation", 
    "arxiv-id": "1307.7974v1", 
    "author": "Shipeng Li", 
    "publish": "2013-07-29T08:12:03Z", 
    "summary": "Tagging is nowadays the most prevalent and practical way to make images\nsearchable. However, in reality many manually-assigned tags are irrelevant to\nimage content and hence are not reliable for applications. A lot of recent\nefforts have been conducted to refine image tags. In this paper, we propose to\ndo tag refinement from the angle of topic modeling and present a novel\ngraphical model, regularized Latent Dirichlet Allocation (rLDA). In the\nproposed approach, tag similarity and tag relevance are jointly estimated in an\niterative manner, so that they can benefit from each other, and the multi-wise\nrelationships among tags are explored. Moreover, both the statistics of tags\nand visual affinities of images in the corpus are explored to help topic\nmodeling. We also analyze the superiority of our approach from the deep\nstructure perspective. The experiments on tag ranking and image retrieval\ndemonstrate the advantages of the proposed method."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1310.0894v1", 
    "title": "Differential Data Analysis for Recommender Systems", 
    "arxiv-id": "1310.0894v1", 
    "author": "Gokay Saldamli", 
    "publish": "2013-10-03T04:47:47Z", 
    "summary": "We present techniques to characterize which data is important to a\nrecommender system and which is not. Important data is data that contributes\nmost to the accuracy of the recommendation algorithm, while less important data\ncontributes less to the accuracy or even decreases it. Characterizing the\nimportance of data has two potential direct benefits: (1) increased privacy and\n(2) reduced data management costs, including storage. For privacy, we enable\nincreased recommendation accuracy for comparable privacy levels using existing\ndata obfuscation techniques. For storage, our results indicate that we can\nachieve large reductions in recommendation data and yet maintain recommendation\naccuracy.\n  Our main technique is called differential data analysis. The name is inspired\nby other sorts of differential analysis, such as differential power analysis\nand differential cryptanalysis, where insight comes through analysis of\nslightly differing inputs. In differential data analysis we chunk the data and\ncompare results in the presence or absence of each chunk. We present results\napplying differential data analysis to two datasets and three different kinds\nof attributes. The first attribute is called user hardship. This is a novel\nattribute, particularly relevant to location datasets, that indicates how\nburdensome a data point was to achieve. The second and third attributes are\nmore standard: timestamp and user rating. For user rating, we confirm previous\nwork concerning the increased importance to the recommender of data\ncorresponding to high and low user ratings."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2396761.2396828", 
    "link": "http://arxiv.org/pdf/1310.1498v1", 
    "title": "Deeper Into the Folksonomy Graph: FolkRank Adaptations and Extensions   for Improved Tag Recommendations", 
    "arxiv-id": "1310.1498v1", 
    "author": "Nathan Griffiths", 
    "publish": "2013-10-05T17:27:42Z", 
    "summary": "The information contained in social tagging systems is often modelled as a\ngraph of connections between users, items and tags. Recommendation algorithms\nsuch as FolkRank, have the potential to leverage complex relationships in the\ndata, corresponding to multiple hops in the graph. We present an in-depth\nanalysis and evaluation of graph models for social tagging data and propose\nnovel adaptations and extensions of FolkRank to improve tag recommendations. We\nhighlight implicit assumptions made by the widely used folksonomy model, and\npropose an alternative and more accurate graph-representation of the data. Our\nextensions of FolkRank address the new item problem by incorporating content\ndata into the algorithm, and significantly improve prediction results on\nunpruned datasets. Our adaptations address issues in the iterative weight\nspreading calculation that potentially hinder FolkRank's ability to leverage\nthe deep graph as an information source. Moreover, we evaluate the benefit of\nconsidering each deeper level of the graph, and present important insights\nregarding the characteristics of social tagging data in general. Our results\nsuggest that the base assumption made by conventional weight propagation\nmethods, that closeness in the graph always implies a positive relationship,\ndoes not hold for the social tagging domain."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijasa.2013.1302", 
    "link": "http://arxiv.org/pdf/1310.2127v1", 
    "title": "BloSEn: Blog Search Engine Based On Post Concept Clustering", 
    "arxiv-id": "1310.2127v1", 
    "author": "G. Aghila", 
    "publish": "2013-10-08T13:16:30Z", 
    "summary": "This paper focuses on building a blog search engine which doesn't focus only\non keyword search but includes extended search capabilities. It also\nincorporates the blog-post concept clustering which is based on the category\nextracted from the blog post semantic content analysis. The proposed approach\nis titled as \"BloSen (Blog Search Engine)\". It involves in extracting the posts\nfrom blogs and parsing them to extract the blog elements and store them as\nfields in a document format. Inverted index is being built on the fields of the\ndocuments. Search is induced on the index and requested query is processed\nbased on the documents so far made from blog posts. It currently focuses on\nBlogger and Wordpress hosted blogs since both these hosting services are the\nmost popular ones in the blogosphere. The proposed BloSen model is experimented\nwith a prototype implementation and the results of the experiments with the\nuser's relevance cumulative metric value of 95.44% confirms the efficiency of\nthe proposed model."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijasa.2013.1302", 
    "link": "http://arxiv.org/pdf/1310.4774v1", 
    "title": "IntelligentWeb Agent for Search Engines", 
    "arxiv-id": "1310.4774v1", 
    "author": "B. B. Meshram", 
    "publish": "2013-10-17T17:07:38Z", 
    "summary": "In this paper we review studies of the growth of the Internet and\ntechnologies that are useful for information search and retrieval on the Web.\nSearch engines are retrieve the efficient information. We collected data on the\nInternet from several different sources, e.g., current as well as projected\nnumber of users, hosts, and Web sites. The trends cited by the sources are\nconsistent and point to exponential growth in the past and in the coming\ndecade. Hence it is not surprising that about 85% of Internet users surveyed\nclaim using search engines and search services to find specific information and\nusers are not satisfied with the performance of the current generation of\nsearch engines; the slow retrieval speed, communication delays, and poor\nquality of retrieved results. Web agents, programs acting autonomously on some\ntask, are already present in the form of spiders, crawler, and robots. Agents\noffer substantial benefits and hazards, and because of this, their development\nmust involve attention to technical details. This paper illustrates the\ndifferent types of agents,crawlers, robots,etc for mining the contents of web\nin a methodical, automated manner, also discusses the use of crawler to gather\nspecific types of information from Web pages, such as harvesting e-mail\naddresses"
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijasa.2013.1302", 
    "link": "http://arxiv.org/pdf/1310.5698v1", 
    "title": "Massive Query Expansion by Exploiting Graph Knowledge Bases", 
    "arxiv-id": "1310.5698v1", 
    "author": "Josep-LLuis Larriba-Pey", 
    "publish": "2013-10-21T14:27:45Z", 
    "summary": "Keyword based search engines have problems with term ambiguity and vocabulary\nmismatch. In this paper, we propose a query expansion technique that enriches\nqueries expressed as keywords and short natural language descriptions. We\npresent a new massive query expansion strategy that enriches queries using a\nknowledge base by identifying the query concepts, and adding relevant synonyms\nand semantically related terms. We propose two approaches: (i) lexical\nexpansion that locates the relevant concepts in the knowledge base; and, (ii)\ntopological expansion that analyzes the network of relations among the\nconcepts, and suggests semantically related terms by path and community\nanalysis of the knowledge graph. We perform our expansions by using two\nversions of the Wikipedia as knowledge base, concluding that the combination of\nboth lexical and topological expansion provides improvements of the system's\nprecision up to more than 27%."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijasa.2013.1302", 
    "link": "http://arxiv.org/pdf/1310.6110v1", 
    "title": "A two-step model and the algorithm for recalling in recommender systems", 
    "arxiv-id": "1310.6110v1", 
    "author": "Tomihisa Kamada", 
    "publish": "2013-10-23T04:41:18Z", 
    "summary": "When a user finds an interesting recommendation in a recommender system, the\nuser may want to recall related items recommended in the past to reconsider or\nto enjoy them again. If the system can pick up such \"recalled\" items at each\nuser's request, it must deepen the user experience.\n  We propose a model and the algorithm for such personalized \"recalling\" in\nconventional recommender systems, which is an application of neural networks\nfor associative memory. In our model, the \"recalled\" items can reflect each\nuser's personality beyond naive similarities between items."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijasa.2013.1302", 
    "link": "http://arxiv.org/pdf/1310.6637v1", 
    "title": "A language independent web data extraction using vision based page   segmentation algorithm", 
    "arxiv-id": "1310.6637v1", 
    "author": "P KiranSree", 
    "publish": "2013-10-24T15:01:13Z", 
    "summary": "Web usage mining is a process of extracting useful information from server\nlogs i.e. users history. Web usage mining is a process of finding out what\nusers are looking for on the internet. Some users might be looking at only\ntextual data, where as some others might be interested in multimedia data. One\nwould retrieve the data by copying it and pasting it to the relevant document.\nBut this is tedious and time consuming as well as difficult when the data to be\nretrieved is plenty. Extracting structured data from a web page is challenging\nproblem due to complicated structured pages. Earlier they were used web page\nprogramming language dependent; the main problem is to analyze the html source\ncode. In earlier they were considered the scripts such as java scripts and\ncascade styles in the html files. When it makes different for existing\nsolutions to infer the regularity of the structure of the Web Pages only by\nanalyzing the tag structures. To overcome this problem we are using a new\nalgorithm called VIPS algorithm i.e. independent language. This approach\nprimary utilizes the visual features on the webpage to implement web data\nextraction."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1310.7428v1", 
    "title": "Musical recommendations and personalization in a social network", 
    "arxiv-id": "1310.7428v1", 
    "author": "Alexandr Dzuba", 
    "publish": "2013-10-28T14:30:52Z", 
    "summary": "This paper presents a set of algorithms used for music recommendations and\npersonalization in a general purpose social network www.ok.ru, the second\nlargest social network in the CIS visited by more then 40 millions users per\nday. In addition to classical recommendation features like \"recommend a\nsequence\" and \"find similar items\" the paper describes novel algorithms for\nconstruction of context aware recommendations, personalization of the service,\nhandling of the cold-start problem, and more. All algorithms described in the\npaper are working on-line and are able to detect and address changes in the\nuser's behavior and needs in the real time.\n  The core component of the algorithms is a taste graph containing information\nabout different entities (users, tracks, artists, etc.) and relations between\nthem (for example, user A likes song B with certainty X, track B created by\nartist C, artist C is similar to artist D with certainty Y and so on). Using\nthe graph it is possible to select tracks a user would most probably like, to\narrange them in a way that they match each other well, to estimate which items\nfrom a fixed list are most relevant for the user, and more.\n  In addition, the paper describes the approach used to estimate algorithms\nefficiency and analyze the impact of different recommendation related features\non the users' behavior and overall activity at the service."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1310.7957v1", 
    "title": "A Random Walk Model for Item Recommendation in Folksonomies", 
    "arxiv-id": "1310.7957v1", 
    "author": "Jing Peng", 
    "publish": "2013-09-27T15:19:54Z", 
    "summary": "Social tagging, as a novel approach to information organization and\ndiscovery, has been widely adopted in many Web2.0 applications. The tags\nprovide a new type of information that can be exploited by recommender systems.\nNevertheless, the sparsity of ternary <user, tag, item> interaction data limits\nthe performance of tag-based collaborative filtering. This paper proposes a\nrandom-walk-based algorithm to deal with the sparsity problem in social tagging\ndata, which captures the potential transitive associations between users and\nitems through their interaction with tags. In particular, two smoothing\nstrategies are presented from both the user-centric and item-centric\nperspectives. Experiments on real-world data sets empirically demonstrate the\nefficacy of the proposed algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1312.0182v1", 
    "title": "Query Segmentation for Relevance Ranking in Web Search", 
    "arxiv-id": "1312.0182v1", 
    "author": "Enhong Chen", 
    "publish": "2013-12-01T07:23:12Z", 
    "summary": "In this paper, we try to answer the question of how to improve the\nstate-of-the-art methods for relevance ranking in web search by query\nsegmentation. Here, by query segmentation it is meant to segment the input\nquery into segments, typically natural language phrases, so that the\nperformance of relevance ranking in search is increased. We propose employing\nthe re-ranking approach in query segmentation, which first employs a generative\nmodel to create top $k$ candidates and then employs a discriminative model to\nre-rank the candidates to obtain the final segmentation result. The method has\nbeen widely utilized for structure prediction in natural language processing,\nbut has not been applied to query segmentation, as far as we know. Furthermore,\nwe propose a new method for using the result of query segmentation in relevance\nranking, which takes both the original query words and the segmented query\nphrases as units of query representation. We investigate whether our method can\nimprove three relevance models, namely BM25, key n-gram model, and dependency\nmodel. Our experimental results on three large scale web search datasets show\nthat our method can indeed significantly improve relevance ranking in all the\nthree cases."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1312.1448v1", 
    "title": "Food Recommendation using Ontology and Heuristics", 
    "arxiv-id": "1312.1448v1", 
    "author": "A. H. EL-Bassiouny", 
    "publish": "2013-12-05T06:50:30Z", 
    "summary": "Recommender systems are needed to find food items of ones interest. We review\nrecommender systems and recommendation methods. We propose a food\npersonalization framework based on adaptive hypermedia. We extend Hermes\nframework with food recommendation functionality. We combine TF-IDF term\nextraction method with cosine similarity measure. Healthy heuristics and\nstandard food database are incorporated into the knowledgebase. Based on the\nperformed evaluation, we conclude that semantic recommender systems in general\noutperform traditional recommenders systems with respect to accuracy,\nprecision, and recall, and that the proposed recommender has a better F-measure\nthan existing semantic recommenders."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1312.1611v1", 
    "title": "Intent Models for Contextualising and Diversifying Query Suggestions", 
    "arxiv-id": "1312.1611v1", 
    "author": "Iadh Ounis", 
    "publish": "2013-12-05T16:47:41Z", 
    "summary": "The query suggestion or auto-completion mechanisms help users to type less\nwhile interacting with a search engine. A basic approach that ranks suggestions\naccording to their frequency in the query logs is suboptimal. Firstly, many\ncandidate queries with the same prefix can be removed as redundant. Secondly,\nthe suggestions can also be personalised based on the user's context. These two\ndirections to improve the aforementioned mechanisms' quality can be in\nopposition: while the latter aims to promote suggestions that address search\nintents that a user is likely to have, the former aims to diversify the\nsuggestions to cover as many intents as possible. We introduce a\ncontextualisation framework that utilises a short-term context using the user's\nbehaviour within the current search session, such as the previous query, the\ndocuments examined, and the candidate query suggestions that the user has\ndiscarded. This short-term context is used to contextualise and diversify the\nranking of query suggestions, by modelling the user's information need as a\nmixture of intent-specific user models. The evaluation is performed offline on\na set of approximately 1.0M test user sessions. Our results suggest that the\nproposed approach significantly improves query suggestions compared to the\nbaseline approach."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1312.1897v1", 
    "title": "Bootstrapped Grouping of Results to Ambiguous Person Name Queries", 
    "arxiv-id": "1312.1897v1", 
    "author": "Felix Naumann", 
    "publish": "2013-12-06T15:50:54Z", 
    "summary": "Some of the main ranking features of today's search engines reflect result\npopularity and are based on ranking models, such as PageRank, implicit feedback\naggregation, and more. While such features yield satisfactory results for a\nwide range of queries, they aggravate the problem of search for ambiguous\nentities: Searching for a person yields satisfactory results only if the person\nwe are looking for is represented by a high-ranked Web page and all required\ninformation are contained in this page. Otherwise, the user has to either\nreformulate/refine the query or manually inspect low-ranked results to find the\nperson in question. A possible approach to solve this problem is to cluster the\nresults, so that each cluster represents one of the persons occurring in the\nanswer set. However clustering search results has proven to be a difficult\nendeavor by itself, where the clusters are typically of moderate quality.\n  A wealth of useful information about persons occurs in Web 2.0 platforms,\nsuch as LinkedIn, Wikipedia, Facebook, etc. Being human-generated, the\ninformation on these platforms is clean, focused, and already disambiguated. We\nshow that when searching for ambiguous person names the information from such\nplatforms can be bootstrapped to group the results according to the individuals\noccurring in them. We have evaluated our methods on a hand-labeled dataset of\naround 5,000 Web pages retrieved from Google queries on 50 ambiguous person\nnames."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2507157.2507192", 
    "link": "http://arxiv.org/pdf/1312.1913v1", 
    "title": "Adapting Binary Information Retrieval Evaluation Metrics for   Segment-based Retrieval Tasks", 
    "arxiv-id": "1312.1913v1", 
    "author": "Gareth J. F. Jones", 
    "publish": "2013-12-06T16:34:14Z", 
    "summary": "This report describes metrics for the evaluation of the effectiveness of\nsegment-based retrieval based on existing binary information retrieval metrics.\nThis metrics are described in the context of a task for the hyperlinking of\nvideo segments. This evaluation approach re-uses existing evaluation measures\nfrom the standard Cranfield evaluation paradigm. Our adaptation approach can in\nprinciple be used with any kind of effectiveness measure that uses binary\nrelevance, and for other segment-baed retrieval tasks. In our video\nhyperlinking setting, we use precision at a cut-off rank n and mean average\nprecision."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1312.2375v1", 
    "title": "Novel text categorization by amalgamation of augmented k-nearest   neighborhood classification and k-medoids clustering", 
    "arxiv-id": "1312.2375v1", 
    "author": "Dr. K Karteeka Pavan", 
    "publish": "2013-12-09T10:36:22Z", 
    "summary": "Machine learning for text classification is the underpinning of document\ncataloging, news filtering, document steering and exemplification. In text\nmining realm, effective feature selection is significant to make the learning\ntask more accurate and competent. One of the traditional lazy text classifier\nk-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity\nbetween all the objects in training and testing sets, there by leads to\nexaggeration of both computational complexity of the algorithm and massive\nconsumption of main memory. To diminish these shortcomings in viewpoint of a\ndata-mining practitioner an amalgamative technique is proposed in this paper\nusing a novel restructured version of kNN called AugmentedkNN(AkNN) and\nk-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the\ninitial training set by imposing attribute feature selection for reduction of\nhigh dimensionality, also it detects and excludes the high-fliers samples in\nthe initial training set and restructures a constrictedtraining set. The kMdd\nclustering algorithm generates the cluster centers (as interior objects) for\neach category and restructures the constricted training set with centroids.\nThis technique is amalgamated with AkNNclassifier that was prearranged with\ntext mining similarity measures. Eventually, significantweights and ranks were\nassigned to each object in the new training set based upon their accessory\ntowards the object in testing set. Experiments conducted on Reuters-21578 a UCI\nbenchmark text mining data set, and comparisons with traditional kNNclassifier\ndesignates the referredmethod yieldspreeminentrecitalin both clustering and\nclassification."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1312.4036v1", 
    "title": "Mind Your Language: Effects of Spoken Query Formulation on Retrieval   Effectiveness", 
    "arxiv-id": "1312.4036v1", 
    "author": "Srikanta Bedathur", 
    "publish": "2013-12-14T12:13:58Z", 
    "summary": "Voice search is becoming a popular mode for interacting with search engines.\nAs a result, research has gone into building better voice transcription\nengines, interfaces, and search engines that better handle inherent verbosity\nof queries. However, when one considers its use by non- native speakers of\nEnglish, another aspect that becomes important is the formulation of the query\nby users. In this paper, we present the results of a preliminary study that we\nconducted with non-native English speakers who formulate queries for given\nretrieval tasks. Our results show that the current search engines are sensitive\nin their rankings to the query formulation, and thus highlights the need for\ndeveloping more robust ranking methods."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1312.4425v1", 
    "title": "An Ontology-based Model for Indexing and Retrieval", 
    "arxiv-id": "1312.4425v1", 
    "author": "Winfried G\u00f6dert", 
    "publish": "2013-12-16T16:49:32Z", 
    "summary": "Starting from an unsolved problem of information retrieval this paper\npresents an ontology-based model for indexing and retrieval. The model combines\nthe methods and experiences of cognitive-to-interpret indexing languages with\nthe strengths and possibilities of formal knowledge representation. The core\ncomponent of the model uses inferences along the paths of typed relations\nbetween the entities of a knowledge representation for enabling the\ndetermination of hit quantities in the context of retrieval processes. The\nentities are arranged in aspect-oriented facets to ensure a consistent\nhierarchical structure. The possible consequences for indexing and retrieval\nare discussed."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1312.5111v1", 
    "title": "Long Time No See: The Probability of Reusing Tags as a Function of   Frequency and Recency", 
    "arxiv-id": "1312.5111v1", 
    "author": "Tobias Ley", 
    "publish": "2013-12-18T12:31:23Z", 
    "summary": "In this paper, we introduce a tag recommendation algorithm that mimics the\nway humans draw on items in their long-term memory. This approach uses the\nfrequency and recency of previous tag assignments to estimate the probability\nof reusing a particular tag. Using three real-world folksonomies gathered from\nbookmarks in BibSonomy, CiteULike and Flickr, we show how adding a\ntime-dependent component outperforms conventional \"most popular tags\"\napproaches and another existing and very effective but less theory-driven,\ntime-dependent recommendation mechanism. By combining our approach with a\nsimple resource-specific frequency analysis, our algorithm outperforms other\nwell-established algorithms, such as FolkRank, Pairwise Interaction Tensor\nFactorization and Collaborative Filtering. We conclude that our approach\nprovides an accurate and computationally efficient model of a user's temporal\ntagging behavior. We show how effective principles for information retrieval\ncan be designed and implemented if human memory processes are taken into\naccount."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1401.0864v1", 
    "title": "Predicting a Business Star in Yelp from Its Reviews Text Alone", 
    "arxiv-id": "1401.0864v1", 
    "author": "Maryam Khademi", 
    "publish": "2014-01-05T03:29:05Z", 
    "summary": "Yelp online reviews are invaluable source of information for users to choose\nwhere to visit or what to eat among numerous available options. But due to\noverwhelming number of reviews, it is almost impossible for users to go through\nall reviews and find the information they are looking for. To provide a\nbusiness overview, one solution is to give the business a 1-5 star(s). This\nrating can be subjective and biased toward users personality. In this paper, we\npredict a business rating based on user-generated reviews texts alone. This not\nonly provides an overview of plentiful long review texts but also cancels out\nsubjectivity. Selecting the restaurant category from Yelp Dataset Challenge, we\nuse a combination of three feature generation methods as well as four machine\nlearning models to find the best prediction result. Our approach is to create\nbag of words from the top frequent words in all raw text reviews, or top\nfrequent words/adjectives from results of Part-of-Speech analysis. Our results\nshow Root Mean Square Error (RMSE) of 0.6 for the combination of Linear\nRegression with either of the top frequent words from raw data or top frequent\nadjectives after Part-of-Speech (POS)."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1401.1456v2", 
    "title": "Using temporal IDF for efficient novelty detection in text streams", 
    "arxiv-id": "1401.1456v2", 
    "author": "Michalis Vazirgiannis", 
    "publish": "2014-01-07T17:43:37Z", 
    "summary": "Novelty detection in text streams is a challenging task that emerges in quite\na few different scenarios, ranging from email thread filtering to RSS news feed\nrecommendation on a smartphone. An efficient novelty detection algorithm can\nsave the user a great deal of time and resources when browsing through relevant\nyet usually previously-seen content. Most of the recent research on detection\nof novel documents in text streams has been building upon either geometric\ndistances or distributional similarities, with the former typically performing\nbetter but being much slower due to the need of comparing an incoming document\nwith all the previously-seen ones. In this paper, we propose a new approach to\nnovelty detection in text streams. We describe a resource-aware mechanism that\nis able to handle massive text streams such as the ones present today thanks to\nthe burst of social media and the emergence of the Web as the main source of\ninformation. We capitalize on the historical Inverse Document Frequency (IDF)\nthat was known for capturing well term specificity and we show that it can be\nused successfully at the document level as a measure of document novelty. This\nenables us to avoid similarity comparisons with previous documents in the text\nstream, thus scaling better and leading to faster execution times. Moreover, as\nthe collection of documents evolves over time, we use a temporal variant of IDF\nnot only to maintain an efficient representation of what has already been seen\nbut also to decay the document frequencies as the time goes by. We evaluate the\nperformance of the proposed approach on a real-world news articles dataset\ncreated for this task. The results show that the proposed method outperforms\nall of the baselines while managing to operate efficiently in terms of time\ncomplexity and memory usage, which are of great importance in a mobile setting\nscenario."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1401.1732v1", 
    "title": "Looking at Vector Space and Language Models for IR using Density   Matrices", 
    "arxiv-id": "1401.1732v1", 
    "author": "Jian-Yun Nie", 
    "publish": "2014-01-08T15:46:35Z", 
    "summary": "In this work, we conduct a joint analysis of both Vector Space and Language\nModels for IR using the mathematical framework of Quantum Theory. We shed light\non how both models allocate the space of density matrices. A density matrix is\nshown to be a general representational tool capable of leveraging capabilities\nof both VSM and LM representations thus paving the way for a new generation of\nretrieval models. We analyze the possible implications suggested by our\nfindings."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsity.2013.1406", 
    "link": "http://arxiv.org/pdf/1401.1766v5", 
    "title": "G-Bean: an ontology-graph based web tool for biomedical literature   retrieval", 
    "arxiv-id": "1401.1766v5", 
    "author": "Philip S. Yu", 
    "publish": "2014-01-08T18:15:31Z", 
    "summary": "Currently, most people use PubMed to search the MEDLINE database, an\nimportant bibliographical information source for life science and biomedical\ninformation. However, PubMed has some drawbacks that make it difficult to find\nrelevant publications pertaining to users' individual intentions, especially\nfor non-expert users. To ameliorate the disadvantages of PubMed, we developed\nG-Bean, a graph based biomedical search engine, to search biomedical articles\nin MEDLINE database more efficiently.G-Bean addresses PubMed's limitations with\nthree innovations: parallel document index creation,ontology-graph based query\nexpansion, and retrieval and re-ranking of documents based on user's search\nintention.Performance evaluation with 106 OHSUMED benchmark queries shows that\nG-Bean returns more relevant results than PubMed does when using these queries\nto search the MEDLINE database. PubMed could not even return any search result\nfor some OHSUMED queries because it failed to form the appropriate Boolean\nquery statement automatically from the natural language query strings. G-Bean\nis available at http://bioinformatics.clemson.edu/G-Bean/index.php.G-Bean\naddresses PubMed's limitations with ontology-graph based query expansion,\nautomatic document indexing, and user search intention discovery. It shows\nsignificant advantages in finding relevant articles from the MEDLINE database\nto meet the information need of the user."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcsa.2013.3604", 
    "link": "http://arxiv.org/pdf/1401.2229v1", 
    "title": "A Survey on optimization approaches to text document clustering", 
    "arxiv-id": "1401.2229v1", 
    "author": "Dr. G. Wiselin Jiji", 
    "publish": "2014-01-10T04:47:05Z", 
    "summary": "Text Document Clustering is one of the fastest growing research areas because\nof availability of huge amount of information in an electronic form. There are\nseveral number of techniques launched for clustering documents in such a way\nthat documents within a cluster have high intra-similarity and low\ninter-similarity to other clusters. Many document clustering algorithms provide\nlocalized search in effectively navigating, summarizing, and organizing\ninformation. A global optimal solution can be obtained by applying high-speed\nand high-quality optimization algorithms. The optimization technique performs a\nglobalized search in the entire solution space. In this paper, a brief survey\non optimization approaches to text document clustering is turned out."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-81-322-1143-3_21", 
    "link": "http://arxiv.org/pdf/1401.2516v1", 
    "title": "Progressive Filtering Using Multiresolution Histograms for Query by   Humming System", 
    "arxiv-id": "1401.2516v1", 
    "author": "Nagappa U. Bhajantri", 
    "publish": "2014-01-11T10:29:34Z", 
    "summary": "The rising availability of digital music stipulates effective categorization\nand retrieval methods. Real world scenarios are characterized by mammoth music\ncollections through pertinent and non-pertinent songs with reference to the\nuser input. The primary goal of the research work is to counter balance the\nperilous impact of non-relevant songs through Progressive Filtering (PF) for\nQuery by Humming (QBH) system. PF is a technique of problem solving through\nreduced space. This paper presents the concept of PF and its efficient design\nbased on Multi-Resolution Histograms (MRH) to accomplish searching in\nmanifolds. Initially the entire music database is searched to obtain high\nrecall rate and narrowed search space. Later steps accomplish slow search in\nthe reduced periphery and achieve additional accuracy.\n  Experimentation on large music database using recursive programming\nsubstantiates the potential of the method. The outcome of proposed strategy\nglimpses that MRH effectively locate the patterns. Distances of MRH at lower\nlevel are the lower bounds of the distances at higher level, which guarantees\nevasion of false dismissals during PF. In due course, proposed method helps to\nstrike a balance between efficiency and effectiveness. The system is scalable\nfor large music retrieval systems and also data driven for performance\noptimization as an added advantage."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-81-322-1143-3_21", 
    "link": "http://arxiv.org/pdf/1401.2545v1", 
    "title": "Design and Development of a User Specific Dynamic E-Magazine", 
    "arxiv-id": "1401.2545v1", 
    "author": "Sanjay Singh", 
    "publish": "2014-01-11T16:28:23Z", 
    "summary": "Internet and electronic media gaining more popularity due to ease and speed,\nthe count of Internet users has increased tremendously. The world is moving\nfaster each day with several events taking place at once and the Internet is\nflooded with information in every field. There are categories of information\nranging from most relevant to user, to the information totally irrelevant or\nless relevant to specific users. In such a scenario getting the information\nwhich is most relevant to the user is indispensable to save time. The\nmotivation of our solution is based on the idea of optimizing the search for\ninformation automatically. This information is delivered to user in the form of\nan interactive GUI. The optimization of the contents or information served to\nhim is based on his social networking profiles and on his reading habits on the\nproposed solution. The aim is to get the user's profile information based on\nhis social networking profile considering that almost every Internet user has\none. This helps us personalize the contents delivered to the user in order to\nproduce what is most relevant to him, in the form of a personalized e-magazine.\nFurther the proposed solution learns user's reading habits for example the news\nhe saves or clicks the most and makes a decision to provide him with the best\ncontents."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-81-322-1143-3_21", 
    "link": "http://arxiv.org/pdf/1401.2684v1", 
    "title": "Improving Quality of Clustering using Cellular Automata for Information   retrieval", 
    "arxiv-id": "1401.2684v1", 
    "author": "Inampudi Ramesh Babu", 
    "publish": "2014-01-13T00:05:34Z", 
    "summary": "Clustering has been widely applied to Information Retrieval (IR) on the\ngrounds of its potential improved effectiveness over inverted file search.\nClustering is a mostly unsupervised procedure and the majority of the\nclustering algorithms depend on certain assumptions in order to define the\nsubgroups present in a data set .A clustering quality measure is a function\nthat, given a data set and its partition into clusters, returns a non-negative\nreal number representing the quality of that clustering. Moreover, they may\nbehave in a different way depending on the features of the data set and their\ninput parameters values. Therefore, in most applications the resulting\nclustering scheme requires some sort of evaluation as regards its validity. The\nquality of clustering can be enhanced by using a Cellular Automata Classifier\nfor information retrieval. In this study we take the view that if cellular\nautomata with clustering is applied to search results (query-specific\nclustering), then it has the potential to increase the retrieval effectiveness\ncompared both to that of static clustering and of conventional inverted file\nsearch. We conducted a number of experiments using ten document collections and\neight hierarchic clustering methods. Our results show that the effectiveness of\nquery-specific clustering with cellular automata is indeed higher and suggest\nthat there is scope for its application to IR."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.3214", 
    "link": "http://arxiv.org/pdf/1401.3883v1", 
    "title": "From \"Identical\" to \"Similar\": Fusing Retrieved Lists Based on   Inter-Document Similarities", 
    "arxiv-id": "1401.3883v1", 
    "author": "Oren Kurland", 
    "publish": "2014-01-16T05:13:26Z", 
    "summary": "Methods for fusing document lists that were retrieved in response to a query\noften utilize the retrieval scores and/or ranks of documents in the lists. We\npresent a novel fusion approach that is based on using, in addition,\ninformation induced from inter-document similarities. Specifically, our methods\nlet similar documents from different lists provide relevance-status support to\neach other. We use a graph-based method to model relevance-status propagation\nbetween documents. The propagation is governed by inter-document-similarities\nand by retrieval scores of documents in the lists. Empirical evaluation\ndemonstrates the effectiveness of our methods in fusing TREC runs. The\nperformance of our most effective methods transcends that of effective fusion\nmethods that utilize only retrieval scores or ranks."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.3327", 
    "link": "http://arxiv.org/pdf/1401.3896v1", 
    "title": "The Opposite of Smoothing: A Language Model Approach to Ranking   Query-Specific Document Clusters", 
    "arxiv-id": "1401.3896v1", 
    "author": "Eyal Krikon", 
    "publish": "2014-01-16T05:18:05Z", 
    "summary": "Exploiting information induced from (query-specific) clustering of\ntop-retrieved documents has long been proposed as a means for improving\nprecision at the very top ranks of the returned results. We present a novel\nlanguage model approach to ranking query-specific clusters by the presumed\npercentage of relevant documents that they contain. While most previous cluster\nranking approaches focus on the cluster as a whole, our model utilizes also\ninformation induced from documents associated with the cluster. Our model\nsubstantially outperforms previous approaches for identifying clusters\ncontaining a high relevant-document percentage. Furthermore, using the model to\nproduce document ranking yields precision-at-top-ranks performance that is\nconsistently better than that of the initial ranking upon which clustering is\nperformed. The performance also favorably compares with that of a\nstate-of-the-art pseudo-feedback-based retrieval method."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.3327", 
    "link": "http://arxiv.org/pdf/1401.6891v1", 
    "title": "Unsupervised Visual and Textual Information Fusion in Multimedia   Retrieval - A Graph-based Point of View", 
    "arxiv-id": "1401.6891v1", 
    "author": "St\u00e9phane Clinchant", 
    "publish": "2014-01-27T15:29:14Z", 
    "summary": "Multimedia collections are more than ever growing in size and diversity.\nEffective multimedia retrieval systems are thus critical to access these\ndatasets from the end-user perspective and in a scalable way. We are interested\nin repositories of image/text multimedia objects and we study multimodal\ninformation fusion techniques in the context of content based multimedia\ninformation retrieval. We focus on graph based methods which have proven to\nprovide state-of-the-art performances. We particularly examine two of such\nmethods : cross-media similarities and random walk based scores. From a\ntheoretical viewpoint, we propose a unifying graph based framework which\nencompasses the two aforementioned approaches. Our proposal allows us to\nhighlight the core features one should consider when using a graph based\ntechnique for the combination of visual and textual information. We compare\ncross-media and random walk based results using three different real-world\ndatasets. From a practical standpoint, our extended empirical analysis allow us\nto provide insights and guidelines about the use of graph based methods for\nmultimodal information fusion in content based multimedia information\nretrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1613/jair.3327", 
    "link": "http://arxiv.org/pdf/1403.0761v1", 
    "title": "The Obvious Solution to Semantic Mapping -- Ask an Expert", 
    "arxiv-id": "1403.0761v1", 
    "author": "Kieran Greer", 
    "publish": "2014-03-04T12:29:40Z", 
    "summary": "The semantic mapping problem is probably the main obstacle to\ncomputer-to-computer communication. If computer A knows that its concept X is\nthe same as computer B's concept Y, then the two machines can communicate. They\nwill in effect be talking the same language. This paper describes a relatively\nstraightforward way of enhancing the semantic descriptions of Web Service\ninterfaces by using online sources of keyword definitions. Method interface\ndescriptions can be enhanced using these standard dictionary definitions.\nBecause the generated metadata is now standardised, this means that any other\ncomputer that has access to the same source, or understands standard language\nconcepts, can now understand the description. This helps to remove a lot of the\nheterogeneity that would otherwise build up though humans creating their own\ndescriptions independently of each other. The description comes in the form of\nan XML script that can be retrieved and read through the Web Service interface\nitself. An additional use for these scripts would be for adding descriptions in\ndifferent languages, which would mean that human users that speak a different\nlanguage would also understand what the service was about."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V8P285", 
    "link": "http://arxiv.org/pdf/1403.1939v1", 
    "title": "Extraction of Core Contents from Web Pages", 
    "arxiv-id": "1403.1939v1", 
    "author": "Sandeep Sirsat", 
    "publish": "2014-03-08T06:49:03Z", 
    "summary": "The information available on web pages mostly contains semi-structured text\ndocuments which are represented either in XML, or HTML, or XHTML format that\nlacks formatted document structure. The document does not discriminate between\nthe text and the schema that represent the text. Also the amount of structure\nused to represent the text depends on the purpose and size of text document. No\nsemantic is applied to semi-structured documents. This requires extracting core\ncontents of text document to analyse words or sentences to generate useful\nknowledge. This paper discusses several techniques and approaches useful for\nextracting core content from semi-structured text documents and their merits\nand demerits"
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V8P285", 
    "link": "http://arxiv.org/pdf/1403.3515v2", 
    "title": "Concept Trees: Building Dynamic Concepts from Semi-Structured Data using   Nature-Inspired Methods", 
    "arxiv-id": "1403.3515v2", 
    "author": "Kieran Greer", 
    "publish": "2014-03-14T09:38:01Z", 
    "summary": "This paper describes a method for creating structure from heterogeneous\nsources, as part of an information database, or more specifically, a 'concept\nbase'. Structures called 'concept trees' can grow from the semi-structured\nsources when consistent sequences of concepts are presented. They might be\nconsidered to be dynamic databases, possibly a variation on the distributed\nAgent-Based or Cellular Automata models, or even related to Markov models.\nSemantic comparison of text is required, but the trees can be built more, from\nautomatic knowledge and statistical feedback. This reduced model might also be\nattractive for security or privacy reasons, as not all of the potential data\ngets saved. The construction process maintains the key requirement of\ngenerality, allowing it to be used as part of a generic framework. The nature\nof the method also means that some level of optimisation or normalisation of\nthe information will occur. This gives comparisons with databases or\nknowledge-bases, but a database system would firstly model its environment or\ndatasets and then populate the database with instance values. The concept base\ndeals with a more uncertain environment and therefore cannot fully model it\nbeforehand. The model itself therefore evolves over time. Similar to databases,\nit also needs a good indexing system, where the construction process provides\nmemory and indexing structures. These allow for more complex concepts to be\nautomatically created, stored and retrieved, possibly as part of a more\ncognitive model. There are also some arguments, or more abstract ideas, for\nmerging physical-world laws into these automatic processes."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V8P285", 
    "link": "http://arxiv.org/pdf/1403.5771v2", 
    "title": "A Novel Method to Calculate Click Through Rate for Sponsored Search", 
    "arxiv-id": "1403.5771v2", 
    "author": "Sanjay Singh", 
    "publish": "2014-03-23T16:35:29Z", 
    "summary": "Sponsored search adopts generalized second price (GSP) auction mechanism\nwhich works on the concept of pay per click which is most commonly used for the\nallocation of slots in the searched page. Two main aspects associated with GSP\nare the bidding amount and the click through rate (CTR). The CTR learning\nalgorithms currently being used works on the basic principle of (#clicks_i/\n#impressions_i) under a fixed window of clicks or impressions or time. CTR are\nprone to fraudulent clicks, resulting in sudden increase of CTR. The current\nalgorithms are unable to find the solutions to stop this, although with the use\nof machine learning algorithms it can be detected that fraudulent clicks are\nbeing generated. In our paper, we have used the concept of relative ranking\nwhich works on the basic principle of (#clicks_i /#clicks_t). In this\nalgorithm, both the numerator and the denominator are linked. As #clicks_t is\nhigher than previous algorithms and is linked to the #clicks_i, the small\nchange in the clicks which occurs in the normal scenario have a very small\nchange in the result but in case of fraudulent clicks the number of clicks\nincreases or decreases rapidly which will add up with the normal clicks to\nincrease the denominator, thereby decreasing the CTR."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V8P285", 
    "link": "http://arxiv.org/pdf/1403.7162v1", 
    "title": "Information Retrieval (IR) through Semantic Web (SW): An Overview", 
    "arxiv-id": "1403.7162v1", 
    "author": "Vishal Jain", 
    "publish": "2014-03-27T18:36:15Z", 
    "summary": "A large amount of data is present on the web. It contains huge number of web\npages and to find suitable information from them is very cumbersome task. There\nis need to organize data in formal manner so that user can easily access and\nuse them. To retrieve information from documents, we have many Information\nRetrieval (IR) techniques. Current IR techniques are not so advanced that they\ncan be able to exploit semantic knowledge within documents and give precise\nresults. IR technology is major factor responsible for handling annotations in\nSemantic Web (SW) languages and in the present paper knowledgeable\nrepresentation languages used for retrieving information are discussed."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V8P285", 
    "link": "http://arxiv.org/pdf/1403.7315v1", 
    "title": "HRank: A Path based Ranking Framework in Heterogeneous Information   Network", 
    "arxiv-id": "1403.7315v1", 
    "author": "Qing Chen", 
    "publish": "2014-03-28T09:31:43Z", 
    "summary": "Recently, there is a surge of interests on heterogeneous information network\nanalysis. As a newly emerging network model, heterogeneous information networks\nhave many unique features (e.g., complex structure and rich semantics) and a\nnumber of interesting data mining tasks have been exploited in this kind of\nnetworks, such as similarity measure, clustering, and classification. Although\nevaluating the importance of objects has been well studied in homogeneous\nnetworks, it is not yet exploited in heterogeneous networks. In this paper, we\nstudy the ranking problem in heterogeneous networks and propose the HRank\nframework to evaluate the importance of multiple types of objects and meta\npaths. Since the importance of objects depends upon the meta paths in\nheterogeneous networks, HRank develops a path based random walk process.\nMoreover, a constrained meta path is proposed to subtly capture the rich\nsemantics in heterogeneous networks. Furthermore, HRank can simultaneously\ndetermine the importance of objects and meta paths through applying the tensor\nanalysis. Extensive experiments on three real datasets show that HRank can\neffectively evaluate the importance of objects and paths together. Moreover,\nthe constrained meta path shows its potential on mining subtle semantics by\nobtaining more accurate ranking results."
},{
    "category": "cs.IR", 
    "doi": "10.14569/IJACSA.2014.050423", 
    "link": "http://arxiv.org/pdf/1405.0190v1", 
    "title": "Towards a Modular Recommender System for Research Papers written in   Albanian", 
    "arxiv-id": "1405.0190v1", 
    "author": "Silvana Greca", 
    "publish": "2014-05-01T15:32:09Z", 
    "summary": "In the recent years there has been an increase in scientific papers\npublications in Albania and its neighboring countries that have large\ncommunities of Albanian speaking researchers. Many of these papers are written\nin Albanian. It is a very time consuming task to find papers related to the\nresearchers' work, because there is no concrete system that facilitates this\nprocess. In this paper we present the design of a modular intelligent search\nsystem for articles written in Albanian. The main part of it is the recommender\nmodule that facilitates searching by providing relevant articles to the users\n(in comparison with a given one). We used a cosine similarity based heuristics\nthat differentiates the importance of term frequencies based on their location\nin the article. We did not notice big differences on the recommendation results\nwhen using different combinations of the importance factors of the keywords,\ntitle, abstract and body. We got similar results when using only the title and\nabstract in comparison with the other combinations. Because we got fairly good\nresults in this initial approach, we believe that similar recommender systems\nfor documents written in Albanian can be build also in contexts not related to\nscientific publishing."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.0580v1", 
    "title": "Web Content Classification: A Survey", 
    "arxiv-id": "1405.0580v1", 
    "author": "Prabhjot Kaur", 
    "publish": "2014-05-03T12:57:50Z", 
    "summary": "As the information contained within the web is increasing day by day,\norganizing this information could be a necessary requirement.The data mining\nprocess is to extract information from a data set and transform it into an\nunderstandable structure for further use. Classification of web page content is\nessential to many tasks in web information retrieval such as maintaining web\ndirectories and focused crawling.The uncontrolled type of nature of web content\npresents additional challenges to web page classification as compared to the\ntraditional text classification, but the interconnected nature of hypertext\nalso provides features that can assist the process. In this paper the web\nclassification is discussed in detail and its importance in field of data\nmining is explored."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.0749v1", 
    "title": "A Brief History of Web Crawlers", 
    "arxiv-id": "1405.0749v1", 
    "author": "Iosif Viorel Onut", 
    "publish": "2014-05-05T00:06:25Z", 
    "summary": "Web crawlers visit internet applications, collect data, and learn about new\nweb pages from visited pages. Web crawlers have a long and interesting history.\nEarly web crawlers collected statistics about the web. In addition to\ncollecting statistics about the web and indexing the applications for search\nengines, modern crawlers can be used to perform accessibility and vulnerability\nchecks on the application. Quick expansion of the web, and the complexity added\nto web applications have made the process of crawling a very challenging one.\nThroughout the history of web crawling many researchers and industrial groups\naddressed different issues and challenges that web crawlers face. Different\nsolutions have been proposed to reduce the time and cost of crawling.\nPerforming an exhaustive crawl is a challenging question. Additionally\ncapturing the model of a modern web application and extracting data from it\nautomatically is another open question. What follows is a brief history of\ndifferent technique and algorithms used from the early days of crawling up to\nthe recent days. We introduce criteria to evaluate the relative performance of\nweb crawlers. Based on these criteria we plot the evolution of web crawlers and\ncompare their performance"
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.0770v1", 
    "title": "Attributes Coupling based Item Enhanced Matrix Factorization Technique   for Recommender Systems", 
    "arxiv-id": "1405.0770v1", 
    "author": "Yang Gao", 
    "publish": "2014-05-05T02:36:52Z", 
    "summary": "Recommender system has attracted lots of attentions since it helps users\nalleviate the information overload problem. Matrix factorization technique is\none of the most widely employed collaborative filtering techniques in the\nresearch of recommender systems due to its effectiveness and efficiency in\ndealing with very large user-item rating matrices. Recently, based on the\nintuition that additional information provides useful insights for matrix\nfactorization techniques, several recommendation algorithms have utilized\nadditional information to improve the performance of matrix factorization\nmethods. However, the majority focus on dealing with the cold start user\nproblem and ignore the cold start item problem. In addition, there are few\nsuitable similarity measures for these content enhanced matrix factorization\napproaches to compute the similarity between categorical items. In this paper,\nwe propose attributes coupling based item enhanced matrix factorization method\nby incorporating item attribute information into matrix factorization technique\nas well as adapting the coupled object similarity to capture the relationship\nbetween items. Item attribute information is formed as an item relationship\nregularization term to regularize the process of matrix factorization.\nSpecifically, the similarity between items is measured by the Coupled Object\nSimilarity considering coupling between items. Experimental results on two real\ndata sets show that our proposed method outperforms state-of-the-art\nrecommendation algorithms and can effectively cope with the cold start item\nproblem when more item attribute information is available."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.1740v1", 
    "title": "Turkish Text Retrieval Experiments Using Lemur Toolkit", 
    "arxiv-id": "1405.1740v1", 
    "author": "Ozgur Yilmazel", 
    "publish": "2014-05-07T20:20:35Z", 
    "summary": "We used Lemur Toolkit, an open source toolkit designed for Information\nRetrieval (IR) research, for our automated indexing and retrieval experiments\non a TREC-like test collection for Turkish. We study and compare three\nretrieval models Lemur supports, especially Language modeling approach to IR,\ncombined with language specific preprocessing techniques. Our experiments show\nthat all retrieval models benefits from language specific preprocessing in\nterms of retrieval quality. Also Language Modeling approach is the best\nperforming retrieval model when language specific preprocessing applied."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.1837v2", 
    "title": "Utilizing Online Social Network and Location-Based Data to Recommend   Products and Categories in Online Marketplaces", 
    "arxiv-id": "1405.1837v2", 
    "author": "Leandro Marinho", 
    "publish": "2014-05-08T08:43:55Z", 
    "summary": "Recent research has unveiled the importance of online social networks for\nimproving the quality of recommender systems and encouraged the research\ncommunity to investigate better ways of exploiting the social information for\nrecommendations. To contribute to this sparse field of research, in this paper\nwe exploit users' interactions along three data sources (marketplace, social\nnetwork and location-based) to assess their performance in a barely studied\ndomain: recommending products and domains of interests (i.e., product\ncategories) to people in an online marketplace environment. To that end we\ndefined sets of content- and network-based user similarity features for each\ndata source and studied them isolated using an user-based Collaborative\nFiltering (CF) approach and in combination via a hybrid recommender algorithm,\nto assess which one provides the best recommendation performance.\nInterestingly, in our experiments conducted on a rich dataset collected from\nSecondLife, a popular online virtual world, we found that recommenders relying\non user similarity features obtained from the social network data clearly\nyielded the best results in terms of accuracy in case of predicting products,\nwhereas the features obtained from the marketplace and location-based data\nsources also obtained very good results in case of predicting categories. This\nfinding indicates that all three types of data sources are important and should\nbe taken into account depending on the level of specialization of the\nrecommendation task."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.1842v1", 
    "title": "SocRecM: A Scalable Social Recommender Engine for Online Marketplaces", 
    "arxiv-id": "1405.1842v1", 
    "author": "Christoph Trattner", 
    "publish": "2014-05-08T09:00:25Z", 
    "summary": "In this paper, we present work-in-progress on SocRecM, a novel social\nrecommendation framework for online marketplaces. We demonstrate that SocRecM\nis not only easy to integrate with existing Web technologies through a RESTful,\nscalable and easy-to-extend service-based architecture but also reveal the\nextent to which various social features and recommendation approaches are\nuseful in an online social marketplace environment."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.2210v1", 
    "title": "Evaluating the retrieval effectiveness of Web search engines using a   representative query sample", 
    "arxiv-id": "1405.2210v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2014-05-09T12:06:55Z", 
    "summary": "Search engine retrieval effectiveness studies are usually small-scale, using\nonly limited query samples. Furthermore, queries are selected by the\nresearchers. We address these issues by taking a random representative sample\nof 1,000 informational and 1,000 navigational queries from a major German\nsearch engine and comparing Google's and Bing's results based on this sample.\nJurors were found through crowdsourcing, data was collected using specialised\nsoftware, the Relevance Assessment Tool (RAT). We found that while Google\noutperforms Bing in both query types, the difference in the performance for\ninformational queries was rather low. However, for navigational queries, Google\nfound the correct answer in 95.3 per cent of cases whereas Bing only found the\ncorrect answer 76.6 per cent of the time. We conclude that search engine\nperformance on navigational queries is of great importance, as users in this\ncase can clearly identify queries that have returned correct results. So,\nperformance on this query type may contribute to explaining user satisfaction\nwith search engines."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V10P117", 
    "link": "http://arxiv.org/pdf/1405.3353v1", 
    "title": "Which one is better: presentation-based or content-based math search?", 
    "arxiv-id": "1405.3353v1", 
    "author": "Akiko Aizawa", 
    "publish": "2014-05-14T03:44:32Z", 
    "summary": "Mathematical content is a valuable information source and retrieving this\ncontent has become an important issue. This paper compares two searching\nstrategies for math expressions: presentation-based and content-based\napproaches. Presentation-based search uses state-of-the-art math search system\nwhile content-based search uses semantic enrichment of math expressions to\nconvert math expressions into their content forms and searching is done using\nthese content-based expressions. By considering the meaning of math\nexpressions, the quality of search system is improved over presentation-based\nsystems."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0004178900540063", 
    "link": "http://arxiv.org/pdf/1405.3557v1", 
    "title": "The Interestingness Tool for Search in the Web", 
    "arxiv-id": "1405.3557v1", 
    "author": "Ran Shaltiel", 
    "publish": "2014-05-14T16:03:14Z", 
    "summary": "Interestingness,as the composition of Relevance and Unexpectedness, has been\ntested by means of Web search cases studies and led to promising results. But\nfor thorough investigation and routine practical application one needs a\nflexible and robust tool. This work describes such an Interestingness based\nsearch tool, its software architecture and actual implementation. One of its\nflexibility traits is the choice of Interestingness functions: it may work with\nMatch-Mismatch and Tf-Idf, among other functions. The tool has been\nexperimentally verified by application to various domains of interest. It has\nbeen validated by comparison of results with those of commercial search engines\nand results from differing Interestingness functions."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0004178900540063", 
    "link": "http://arxiv.org/pdf/1405.5509v1", 
    "title": "Web Log Data Analysis by Enhanced Fuzzy C Means Clustering", 
    "arxiv-id": "1405.5509v1", 
    "author": "Antony Selvadoss Thanamani", 
    "publish": "2014-05-21T18:26:06Z", 
    "summary": "World Wide Web is a huge repository of information and there is a tremendous\nincrease in the volume of information daily. The number of users are also\nincreasing day by day. To reduce users browsing time lot of research is taken\nplace. Web Usage Mining is a type of web mining in which mining techniques are\napplied in log data to extract the behaviour of users. Clustering plays an\nimportant role in a broad range of applications like Web analysis, CRM,\nmarketing, medical diagnostics, computational biology, and many others.\nClustering is the grouping of similar instances or objects. The key factor for\nclustering is some sort of measure that can determine whether two objects are\nsimilar or dissimilar . In this paper a novel clustering method to partition\nuser sessions into accurate clusters is discussed. The accuracy and various\nperformance measures of the proposed algorithm shows that the proposed method\nis a better method for web log mining."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0004178900540063", 
    "link": "http://arxiv.org/pdf/1405.6287v1", 
    "title": "\u00c9tude des dimensions sp\u00e9cifiques du contexte dans un syst\u00e8me de   filtrage d'informations", 
    "arxiv-id": "1405.6287v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2014-05-24T10:16:11Z", 
    "summary": "In the context of business information systems, e-commerce and access to\nknowledge, the relevance of the information provided to use is a key fact to\nthe success of information systems. Therefore the quality of access is\ndetermined by access to the right information at the right time, at the right\nplace. In this context, it is important to consider the users needs when access\nto information and his contextual situation in order to provide relevant\ninformation, tailored to their needs and context use. In what follows we\ndescribe the prelude to a project that tries to combine all of these needs to\nimprove information systems."
},{
    "category": "cs.IR", 
    "doi": "10.5220/0004178900540063", 
    "link": "http://arxiv.org/pdf/1405.7544v1", 
    "title": "Cold-start Problems in Recommendation Systems via Contextual-bandit   Algorithms", 
    "arxiv-id": "1405.7544v1", 
    "author": "Philippe Preux", 
    "publish": "2014-05-29T13:05:39Z", 
    "summary": "In this paper, we study a cold-start problem in recommendation systems where\nwe have completely new users entered the systems. There is not any interaction\nor feedback of the new users with the systems previoustly, thus no ratings are\navailable. Trivial approaches are to select ramdom items or the most popular\nones to recommend to the new users. However, these methods perform poorly in\nmany case. In this research, we provide a new look of this cold-start problem\nin recommendation systems. In fact, we cast this cold-start problem as a\ncontextual-bandit problem. No additional information on new users and new items\nis needed. We consider all the past ratings of previous users as contextual\ninformation to be integrated into the recommendation framework. To solve this\ntype of the cold-start problems, we propose a new efficient method which is\nbased on the LinUCB algorithm for contextual-bandit problems. The experiments\nwere conducted on three different publicly-available data sets, namely\nMovielens, Netflix and Yahoo!Music. The new proposed methods were also compared\nwith other state-of-the-art techniques. Experiments showed that our new method\nsignificantly improves upon all these methods."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.1133v1", 
    "title": "A Synonym Based Approach of Data Mining in Search Engine Optimization", 
    "arxiv-id": "1407.1133v1", 
    "author": "Tarun Bhalla", 
    "publish": "2014-07-04T06:38:02Z", 
    "summary": "In todays era with the rapid growth of information on the web, makes users\nturn to search engines as a replacement of traditional media. This makes\nsorting of particular information through billions of webpages and displaying\nthe relevant data makes the task tough for the search engine. Remedy for this\nis SEO i.e. having a website optimized in such a way that it will display the\nrelevant webpages based on ranking. This is the main reason that makes search\nengine optimization a prominent position in online world. This paper present a\nsynonym based data mining approach for SEO that makes the task of improving the\nranking of the website much easier way and user will get answer to their query\neasily through any of search engine available in market."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.1539v1", 
    "title": "A Framework for Specific Term Recommendation Systems", 
    "arxiv-id": "1407.1539v1", 
    "author": "Philipp Mayr", 
    "publish": "2014-07-06T20:14:08Z", 
    "summary": "In this paper we present the IRSA framework that enables the automatic\ncreation of search term suggestion or recommendation systems (TS). Such TS are\nused to operationalize interactive query expansion and help users in refining\ntheir information need in the query formulation phase. Our recent research has\nshown TS to be more effective when specific to a certain domain. The presented\ntechnical framework allows owners of Digital Libraries to create their own\nspecific TS constructed via OAI-harvested metadata with very little effort."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.5732v1", 
    "title": "A Comparative Study of Hidden Web Crawlers", 
    "arxiv-id": "1407.5732v1", 
    "author": "Komal Kumar Bhatia", 
    "publish": "2014-07-22T05:25:53Z", 
    "summary": "A large amount of data on the WWW remains inaccessible to crawlers of Web\nsearch engines because it can only be exposed on demand as users fill out and\nsubmit forms. The Hidden web refers to the collection of Web data which can be\naccessed by the crawler only through an interaction with the Web-based search\nform and not simply by traversing hyperlinks. Research on Hidden Web has\nemerged almost a decade ago with the main line being exploring ways to access\nthe content in online databases that are usually hidden behind search forms.\nThe efforts in the area mainly focus on designing hidden Web crawlers that\nfocus on learning forms and filling them with meaningful values. The paper\ngives an insight into the various Hidden Web crawlers developed for the purpose\ngiving a mention to the advantages and shortcoming of the techniques employed\nin each."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.6100v1", 
    "title": "A framework for contextual information retrieval from the WWW", 
    "arxiv-id": "1407.6100v1", 
    "author": "Stephen G. MacDonell", 
    "publish": "2014-07-23T03:34:58Z", 
    "summary": "Search engines are the most commonly used type of tool for finding relevant\ninformation on the Internet. However, today's search engines are far from\nperfect. Typical search queries are short, often one or two words, and can be\nambiguous therefore returning inappropriate results. Contextual information\nretrieval (CIR) is a critical technique for these search engines to facilitate\nqueries and return relevant information. Despite its importance, little\nprogress has been made in CIR due to the difficulty of capturing and\nrepresenting contextual information about users. Numerous contextual\ninformation retrieval approaches exist today, but to the best of our knowledge\nnone of them offer a similar service to the one proposed in this paper.\n  This paper proposes an alternative framework for contextual information\nretrieval from the WWW. The framework aims to improve query results (or make\nsearch results more relevant) by constructing a contextual profile based on a\nuser's behaviour, their preferences, and a shared knowledge base, and using\nthis information in the search engine framework to find and return relevant\ninformation."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.6101v1", 
    "title": "Improving web search using contextual retrieval", 
    "arxiv-id": "1407.6101v1", 
    "author": "Stephen G. MacDonell", 
    "publish": "2014-07-23T03:38:12Z", 
    "summary": "Contextual retrieval is a critical technique for today's search engines in\nterms of facilitating queries and returning relevant information. This paper\nreports on the development and evaluation of a system designed to tackle some\nof the challenges associated with contextual information retrieval from the\nWorld Wide Web (WWW). The developed system has been designed with a view to\ncapturing both implicit and explicit user data which is used to develop a\npersonal contextual profile. Such profiles can be shared across multiple users\nto create a shared contextual knowledge base. These are used to refine search\nqueries and improve both the search results for a user as well as their search\nexperience. An empirical study has been undertaken to evaluate the system\nagainst a number of hypotheses. In this paper, results related to one are\npresented that support the claim that users can find information more readily\nusing the contextual search system."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22312803/IJCTT-V12P140", 
    "link": "http://arxiv.org/pdf/1407.6952v1", 
    "title": "Search Space Engine Optimize Search Using FCC_STF Algorithm in Fuzzy   Co-Clustering", 
    "arxiv-id": "1407.6952v1", 
    "author": "Anu Malviya", 
    "publish": "2014-07-24T16:10:40Z", 
    "summary": "Fuzzy co-clustering can be improved if we handle two main problem first is\noutlier and second curse of dimensionality .outlier problem can be reduce by\nimplementing page replacement algorithm like FIFO, LRU or priority algorithm in\na set of frame of web pages efficiently through a search engine. The web page\nwhich has zero priority (outlier) can be represented in separate slot of frame.\nWhereas curse of dimensionality problem can be improved by implementing FCC_STF\nalgorithm for web pages obtain by search engine that reduce the outlier problem\nfirst. The algorithm FCCM and FUZZY CO-DOK are compared with FCC_STF algorithm\nwith merit and demerits on the bases of different fuzzifier used. FCC_STF\nalgorithm in which fuzzifier fused into one entity who have shown high\nperformance by experiment result of values (A1,B1,Vcj,A2,B2) seem to less\nsensitive to local maxima and obtain optimization search space in 2-D for web\npages by plotting graph between J(fcc_stf) and Vcj."
},{
    "category": "cs.IR", 
    "doi": "10.1209/0295-5075/107/18001", 
    "link": "http://arxiv.org/pdf/1407.7049v1", 
    "title": "Ultra accurate collaborative information filtering via directed user   similarity", 
    "arxiv-id": "1407.7049v1", 
    "author": "Jian-Guo Liu", 
    "publish": "2014-07-24T08:22:17Z", 
    "summary": "A key challenge of the collaborative filtering (CF) information filtering is\nhow to obtain the reliable and accurate results with the help of peers'\nrecommendation. Since the similarities from small-degree users to large-degree\nusers would be larger than the ones opposite direction, the large-degree users'\nselections are recommended extensively by the traditional second-order CF\nalgorithms. By considering the users' similarity direction and the second-order\ncorrelations to depress the influence of mainstream preferences, we present the\ndirected second-order CF (HDCF) algorithm specifically to address the challenge\nof accuracy and diversity of the CF algorithm. The numerical results for two\nbenchmark data sets, MovieLens and Netflix, show that the accuracy of the new\nalgorithm outperforms the state-of-the-art CF algorithms. Comparing with the CF\nalgorithm based on random-walks proposed in the Ref.7, the average ranking\nscore could reach 0.0767 and 0.0402, which is enhanced by 27.3\\% and 19.1\\% for\nMovieLens and Netflix respectively. In addition, the diversity, precision and\nrecall are also enhanced greatly. Without relying on any context-specific\ninformation, tuning the similarity direction of CF algorithms could obtain\naccurate and diverse recommendations. This work suggests that the user\nsimilarity direction is an important factor to improve the personalized\nrecommendation performance."
},{
    "category": "cs.IR", 
    "doi": "10.7321/jscse.v3.n3.65", 
    "link": "http://arxiv.org/pdf/1407.7989v2", 
    "title": "Multi-agents Architecture for Semantic Retrieving Video in Distributed   Environment", 
    "arxiv-id": "1407.7989v2", 
    "author": "Omar El Beqqali", 
    "publish": "2014-07-30T10:23:18Z", 
    "summary": "This paper presents an integrated multi-agents architecture for indexing and\nretrieving video information.The focus of our work is to elaborate an\nextensible approach that gathers a priori almost of the mandatory tools which\npalliate to the major intertwining problems raised in the whole process of the\nvideo lifecycle (classification, indexing and retrieval). In fact, effective\nand optimal retrieval video information needs a collaborative approach based on\nmultimodal aspects. Clearly, it must to take into account the distributed\naspect of the data sources, the adaptation of the contents, semantic\nannotation, personalized request and active feedback which constitute the\nbackbone of a vigorous system which improve its performances in a smart way"
},{
    "category": "cs.IR", 
    "doi": "10.7321/jscse.v3.n3.65", 
    "link": "http://arxiv.org/pdf/1410.0993v1", 
    "title": "Document Clustering Based On Max-Correntropy Non-Negative Matrix   Factorization", 
    "arxiv-id": "1410.0993v1", 
    "author": "Honggang Zhang", 
    "publish": "2014-10-03T23:09:09Z", 
    "summary": "Nonnegative matrix factorization (NMF) has been successfully applied to many\nareas for classification and clustering. Commonly-used NMF algorithms mainly\ntarget on minimizing the $l_2$ distance or Kullback-Leibler (KL) divergence,\nwhich may not be suitable for nonlinear case. In this paper, we propose a new\ndecomposition method by maximizing the correntropy between the original and the\nproduct of two low-rank matrices for document clustering. This method also\nallows us to learn the new basis vectors of the semantic feature space from the\ndata. To our knowledge, we haven't seen any work has been done by maximizing\ncorrentropy in NMF to cluster high dimensional document data. Our experiment\nresults show the supremacy of our proposed method over other variants of NMF\nalgorithm on Reuters21578 and TDT2 databasets."
},{
    "category": "cs.IR", 
    "doi": "10.7321/jscse.v3.n3.65", 
    "link": "http://arxiv.org/pdf/1410.2085v1", 
    "title": "Low cost page quality factors to detect web spam", 
    "arxiv-id": "1410.2085v1", 
    "author": "Dr. Rizwan Beg", 
    "publish": "2014-10-08T12:46:34Z", 
    "summary": "Web spam is a big challenge for quality of search engine results. It is very\nimportant for search engines to detect web spam accurately. In this paper we\npresent 32 low cost quality factors to classify spam and ham pages on real time\nbasis. These features can be divided in to three categories: (i) URL features,\n(ii) Content features, and (iii) Link features. We developed a classifier using\nResilient Back-propagation learning algorithm of neural network and obtained\ngood accuracy. This classifier can be applied to search engine results on real\ntime because calculation of these features require very little CPU resources."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.2634v1", 
    "title": "Extending Probabilistic Data Fusion Using Sliding Windows", 
    "arxiv-id": "1410.2634v1", 
    "author": "John Dunnion", 
    "publish": "2014-10-09T21:28:40Z", 
    "summary": "Recent developments in the field of data fusion have seen a focus on\ntechniques that use training queries to estimate the probability that various\ndocuments are relevant to a given query and use that information to assign\nscores to those documents on which they are subsequently ranked. This paper\nintroduces SlideFuse, which builds on these techniques, introducing a sliding\nwindow in order to compensate for situations where little relevance information\nis available to aid in the estimation of probabilities.\n  SlideFuse is shown to perform favourably in comparison with CombMNZ, ProbFuse\nand SegFuse. CombMNZ is the standard baseline technique against which data\nfusion algorithms are compared whereas ProbFuse and SegFuse represent the\nstate-of-the-art for probabilistic data fusion methods."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.4500v1", 
    "title": "On the Feasibility and Implications of Self-Contained Search Engines in   the Browser", 
    "arxiv-id": "1410.4500v1", 
    "author": "Jimmy Lin", 
    "publish": "2014-10-16T17:17:51Z", 
    "summary": "JavaScript engines inside modern browsers are capable of running\nsophisticated multi-player games, rendering impressive 3D scenes, and\nsupporting complex, interactive visualizations. Can this processing power be\nharnessed for information retrieval? This paper explores the feasibility of\nbuilding a JavaScript search engine that runs completely self-contained on the\nclient side within the browser---this includes building the inverted index,\ngathering terms statistics for scoring, and performing query evaluation. The\ndesign takes advantage of the IndexDB API, which is implemented by the LevelDB\nkey-value store inside Google's Chrome browser. Experiments show that although\nthe performance of the JavaScript prototype falls far short of the open-source\nLucene search engine, it is sufficiently responsive for interactive\napplications. This feasibility demonstration opens the door to interesting\napplications in offline and private search across multiple platforms as well as\nhybrid split-execution architectures whereby clients and servers\ncollaboratively perform query evaluation. One possible future scenario is the\nrise of an online search marketplace in which commercial search engine\ncompanies and individual users participate as rational economic actors,\nbalancing privacy, resource usage, latency, and other factors based on\ncustomizable utility profiles."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.4616v1", 
    "title": "Accurate Local Estimation of Geo-Coordinates for Social Media Posts", 
    "arxiv-id": "1410.4616v1", 
    "author": "Aldo Dagnino", 
    "publish": "2014-10-17T01:54:04Z", 
    "summary": "Associating geo-coordinates with the content of social media posts can\nenhance many existing applications and services and enable a host of new ones.\nUnfortunately, a majority of social media posts are not tagged with\ngeo-coordinates. Even when location data is available, it may be inaccurate,\nvery broad or sometimes fictitious. Contemporary location estimation approaches\nbased on analyzing the content of these posts can identify only broad areas\nsuch as a city, which limits their usefulness. To address these shortcomings,\nthis paper proposes a methodology to narrowly estimate the geo-coordinates of\nsocial media posts with high accuracy. The methodology relies solely on the\ncontent of these posts and prior knowledge of the wide geographical region from\nwhere the posts originate. An ensemble of language models, which are smoothed\nover non-overlapping sub-regions of a wider region, lie at the heart of the\nmethodology. Experimental evaluation using a corpus of over half a million\ntweets from New York City shows that the approach, on an average, estimates\nlocations of tweets to within just 2.15km of their actual positions."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.5777v1", 
    "title": "Penerapan teknik web scraping pada mesin pencari artikel ilmiah", 
    "arxiv-id": "1410.5777v1", 
    "author": "Suryayusra", 
    "publish": "2014-10-18T11:20:07Z", 
    "summary": "Search engines are a combination of hardware and computer software supplied\nby a particular company through the website which has been determined. Search\nengines collect information from the web through bots or web crawlers that\ncrawls the web periodically. The process of retrieval of information from\nexisting websites is called \"web scraping.\" Web scraping is a technique of\nextracting information from websites. Web scraping is closely related to Web\nindexing, as for how to develop a web scraping technique that is by first\nstudying the program makers HTML document from the website will be taken to the\ninformation in the HTML tag flanking the aim is for information collected after\nthe program makers learn navigation techniques on the website information will\nbe taken to a web application mimicked the scraping that we will create. It\nshould also be noted that the implementation of this writing only scraping\ninvolves a free search engine such as: portal garuda, Indonesian scientific\njournal databases (ISJD), google scholar."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.7654v1", 
    "title": "XML Information Retrieval:An overview", 
    "arxiv-id": "1410.7654v1", 
    "author": "Raviraja Holla M", 
    "publish": "2014-10-27T06:39:32Z", 
    "summary": "Locating and distilling the valuable relevant information continued to be the\nmajor challenges of Information Retrieval (IR) Systems owing to the explosive\ngrowth of online web information. These challenges can be considered the XML\nInformation Retrieval challenges as XML has become a de facto standard over the\nWeb. The research on XML IR starts with the classical IR strategies customized\nto XML IR. Later novel IR strategies specific to XML IR are evolved. Meanwhile\nliteratures reveal development of the rapid and intelligent IR systems. Despite\ntheir success in their specified constrained domains, they have additional\nlimitations in the complex information space. The effectiveness of IR systems\nis thus unsolved in satisfying the most. This article attemptsan overview of\nearlier efforts and the gaps in XML IR."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1410.8068v1", 
    "title": "Health Information Search Behavior on the Web: A Pilot Study", 
    "arxiv-id": "1410.8068v1", 
    "author": "Si-Chi Chin", 
    "publish": "2014-10-27T23:38:59Z", 
    "summary": "Searching health information on web has become an integral part of today's\nworld, and many people turn to the Web for healthcare information and\nhealthcare assessment. Our pilot study investigates users' preferences for the\ntype of search results (image, news, video, etc.), and investigates users'\nability to accurately interpret online health information for the purpose of\nself diagnosis. The preliminary results reveal that blog and news articles are\nmost sought by users when searching online information and there exist\nchallenges in the use of online health information for self-diagnosis."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.3761v1", 
    "title": "A Hybrid Approach to Finding Relevant Social Media Content for Complex   Domain Specific Information Needs", 
    "arxiv-id": "1411.3761v1", 
    "author": "Gary A. Smith", 
    "publish": "2014-11-13T23:05:41Z", 
    "summary": "While contemporary semantic search systems offer to improve classical\nkeyword-based search, they are not always adequate for complex domain specific\ninformation needs. The domain of prescription drug abuse, for example, requires\nknowledge of both ontological concepts and 'intelligible constructs' not\ntypically modeled in ontologies. These intelligible constructs convey essential\ninformation that include notions of intensity, frequency, interval, dosage and\nsentiments, which could be important to the holistic needs of the information\nseeker. We present a hybrid approach to domain specific information retrieval\n(or knowledge-aware search) that integrates ontology-driven query\ninterpretation with synonym-based query expansion and domain specific rules, to\nfacilitate search in social media. Our framework is based on a context-free\ngrammar (CFG) that defines the query language of constructs interpretable by\nthe search system. The grammar provides two levels of semantic interpretation:\n1) a top-level CFG that facilitates retrieval of diverse textual patterns,\nwhich belong to broad templates and 2) a low-level CFG that enables\ninterpretation of certain specific expressions that belong to such patterns.\nThese low-level expressions occur as concepts from four different categories of\ndata: 1) ontological concepts, 2) concepts in lexicons (such as emotions and\nsentiments), 3) concepts in lexicons with only partial ontology representation,\ncalled lexico-ontology concepts (such as side effects and routes of\nadministration (ROA)), and 4) domain specific expressions (such as date, time,\ninterval, frequency and dosage) derived solely through rules. Our approach is\nembodied in a novel Semantic Web platform called PREDOSE developed for\nprescription drug abuse epidemiology.\n  Keywords: Knowledge-Aware Search, Ontology, Semantic Search, Background\nKnowledge, Context-Free Grammar"
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.3843v1", 
    "title": "Quantum emulation of query extension in information retrieval", 
    "arxiv-id": "1411.3843v1", 
    "author": "Rom\u00e0n Zapatrin", 
    "publish": "2014-11-14T09:43:24Z", 
    "summary": "An operationalistic scheme, called Melucci metaphor, is suggested\nrepresenting Information Retrieval as physical measurements with beam of\nparticles playing the role of the flow of retrieved documents. The\npossibilities of query expansion by extra term are studied from this\nperspective, when the particles-`docuscles' are assumed to be of classical or\nquantum nature. It is shown that in both cases the choice of an extra term\nbased on Bayesian belief revision is still valid on the qualitative level for\nboosting the relevance of the retrieved documents."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.4366v1", 
    "title": "PDD Crawler: A focused web crawler using link and content analysis for   relevance prediction", 
    "arxiv-id": "1411.4366v1", 
    "author": "Latesh malik", 
    "publish": "2014-11-17T05:33:51Z", 
    "summary": "Majority of the computer or mobile phone enthusiasts make use of the web for\nsearching activity. Web search engines are used for the searching; The results\nthat the search engines get are provided to it by a software module known as\nthe Web Crawler. The size of this web is increasing round-the-clock. The\nprincipal problem is to search this huge database for specific information. To\nstate whether a web page is relevant to a search topic is a dilemma. This paper\nproposes a crawler called as PDD crawler which will follow both a link based as\nwell as a content based approach. This crawler follows a completely new\ncrawling strategy to compute the relevance of the page. It analyses the content\nof the page based on the information contained in various tags within the HTML\nsource code and then computes the total weight of the page. The page with the\nhighest weight, thus has the maximum content and highest relevance."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.4972v1", 
    "title": "Towards an objective ranking in online reputation systems: the effect of   the rating projection", 
    "arxiv-id": "1411.4972v1", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2014-11-15T15:22:08Z", 
    "summary": "Online reputation systems are commonly used by e-commerce providers nowadays.\nIn order to generate an objective ranking of online items' quality according to\nusers' ratings, many sophisticated algorithms have been proposed in the\nliterature. In this paper, instead of proposing new algorithms we focus on a\nmore fundamental problem: the rating projection. The basic idea is that even\nthough the rating values given by users are linearly separated, the real\npreference of users to items between different values gave is nonlinear. We\nthus design an approach to project the original ratings of users to more\nrepresentative values. This approach can be regarded as a data pretreatment\nmethod. Simulation in both artificial and real networks shows that the\nperformance of the ranking algorithms can be improved when the projected\nratings are used."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.6496v1", 
    "title": "Automatic Summarization of Soccer Highlights Using Audio-visual   Descriptors", 
    "arxiv-id": "1411.6496v1", 
    "author": "Francesc Tarres", 
    "publish": "2014-11-24T15:56:43Z", 
    "summary": "Automatic summarization generation of sports video content has been object of\ngreat interest for many years. Although semantic descriptions techniques have\nbeen proposed, many of the approaches still rely on low-level video descriptors\nthat render quite limited results due to the complexity of the problem and to\nthe low capability of the descriptors to represent semantic content. In this\npaper, a new approach for automatic highlights summarization generation of\nsoccer videos using audio-visual descriptors is presented. The approach is\nbased on the segmentation of the video sequence into shots that will be further\nanalyzed to determine its relevance and interest. Of special interest in the\napproach is the use of the audio information that provides additional\nrobustness to the overall performance of the summarization system. For every\nvideo shot a set of low and mid level audio-visual descriptors are computed and\nlately adequately combined in order to obtain different relevance measures\nbased on empirical knowledge rules. The final summary is generated by selecting\nthose shots with highest interest according to the specifications of the user\nand the results of relevance measures. A variety of results are presented with\nreal soccer video sequences that prove the validity of the approach."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1411.6773v1", 
    "title": "Efficient Fuzzy Search Engine with B-Tree Search Mechanism", 
    "arxiv-id": "1411.6773v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2014-11-25T09:03:22Z", 
    "summary": "Search engines play a vital role in day to day life on internet. People use\nsearch engines to find content on internet. Cloud computing is the computing\nconcept in which data is stored and accessed with the help of a third party\nserver called as cloud. Data is not stored locally on our machines and the\nsoftwares and information are provided to user if user demands for it. Search\nqueries are the most important part in searching data on internet. A search\nquery consists of one or more than one keywords. A search query is searched\nfrom the database for exact match, and the traditional searchable schemes do\nnot tolerate minor typos and format inconsistencies, which happen quite\nfrequently. This drawback makes the existing techniques unsuitable and they\noffer very low efficiency. In this paper, we will for the first time formulate\nthe problem of effective fuzzy search by introducing tree search methodologies.\nWe will explore the benefits of B trees in search mechanism and use them to\nhave an efficient keyword search. We have taken into consideration the security\nanalysis strictly so as to get a secure and privacy-preserving system."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-540-78646-7_33", 
    "link": "http://arxiv.org/pdf/1412.1888v1", 
    "title": "Document clustering using graph based document representation with   constraints", 
    "arxiv-id": "1412.1888v1", 
    "author": "Mohammad Shahid Shaikh", 
    "publish": "2014-12-05T03:47:34Z", 
    "summary": "Document clustering is an unsupervised approach in which a large collection\nof documents (corpus) is subdivided into smaller, meaningful, identifiable, and\nverifiable sub-groups (clusters). Meaningful representation of documents and\nimplicitly identifying the patterns, on which this separation is performed, is\nthe challenging part of document clustering. We have proposed a document\nclustering technique using graph based document representation with\nconstraints. A graph data structure can easily capture the non-linear\nrelationships of nodes, document contains various feature terms that can be\nnon-linearly connected hence a graph can easily represents this information.\nConstrains, are explicit conditions for document clustering where background\nknowledge is use to set the direction for Linking or Not-Linking a set of\ndocuments for a target clusters, thus guiding the clustering process. We deemed\nclustering is an ill-define problem, there can be many clustering results.\nBackground knowledge can be used to drive the clustering algorithm in the right\ndirection. We have proposed three different types of constraints, Instance\nlevel, corpus level and cluster level constraints. A new algorithm Constrained\nHAC is also proposed which will incorporate Instance level constraints as prior\nknowledge; it will guide the clustering process leading to better results.\nExtensive set of experiments have been performed on both synthetic and standard\ndocument clustering datasets, results are compared on standard clustering\nmeasures like: purity, entropy and F-measure. Results clearly establish that\nour proposed approach leads to improvement in cluster quality."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.3103v1", 
    "title": "Sequential Hypothesis Tests for Adaptive Locality Sensitive Hashing", 
    "arxiv-id": "1412.3103v1", 
    "author": "Srinivasan Parthasarathy", 
    "publish": "2014-12-09T18:31:17Z", 
    "summary": "All pairs similarity search is a problem where a set of data objects is given\nand the task is to find all pairs of objects that have similarity above a\ncertain threshold for a given similarity measure-of-interest. When the number\nof points or dimensionality is high, standard solutions fail to scale\ngracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and\nits Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some\nextent and provides substantial speedup over traditional index based\napproaches. BayesLSH is used for pruning the candidate space and computation of\napproximate similarity, whereas BayesLSHLite can only prune the candidates, but\nsimilarity needs to be computed exactly on the original data. Thus where ever\nthe explicit data representation is available and exact similarity computation\nis not too expensive, BayesLSHLite can be used to aggressively prune candidates\nand provide substantial speedup without losing too much on quality. However,\nthe loss in quality is higher in the BayesLSH variant, where explicit data\nrepresentation is not available, rather only a hash sketch is available and\nsimilarity has to be estimated approximately. In this work we revisit the LSH\nproblem from a Frequentist setting and formulate sequential tests for composite\nhypothesis (similarity greater than or less than threshold) that can be\nleveraged by such LSH algorithms for adaptively pruning candidates\naggressively. We propose a vanilla sequential probability ration test (SPRT)\napproach based on this idea and two novel variants. We extend these variants to\nthe case where approximate similarity needs to be computed using fixed-width\nsequential confidence interval generation technique."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.3898v1", 
    "title": "ILCR: Item-based Latent Factors for Sparse Collaborative Retrieval", 
    "arxiv-id": "1412.3898v1", 
    "author": "Zike Zhang", 
    "publish": "2014-12-12T06:32:47Z", 
    "summary": "Interactions between search and recommendation have recently attracted\nsignificant attention, and several studies have shown that many potential\napplications involve with a joint problem of producing recommendations to users\nwith respect to a given query, termed $Collaborative$ $Retrieval$ (CR).\nSuccessful algorithms designed for CR should be potentially flexible at dealing\nwith the sparsity challenges since the setup of collaborative retrieval\nassociates with a given $query$ $\\times$ $user$ $\\times$ $item$ tensor instead\nof traditional $user$ $\\times$ $item$ matrix. Recently, several works are\nproposed to study CR task from users' perspective. In this paper, we aim to\nsufficiently explore the sophisticated relationship of each $query$ $\\times$\n$user$ $\\times$ $item$ triple from items' perspective. By integrating\nitem-based collaborative information for this joint task, we present an\nalternative factorized model that could better evaluate the ranks of those\nitems with sparse information for the given query-user pair. In addition, we\nsuggest to employ a recently proposed scalable ranking learning algorithm,\nnamely BPR, to optimize the state-of-the-art approach, $Latent$ $Collaborative$\n$Retrieval$ model, instead of the original learning algorithm. The experimental\nresults on two real-world datasets, (i.e. \\emph{Last.fm}, \\emph{Yelp}),\ndemonstrate the efficiency and effectiveness of our proposed approach."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.6629v3", 
    "title": "Semantic Modelling with Long-Short-Term Memory for Information Retrieval", 
    "arxiv-id": "1412.6629v3", 
    "author": "R. Ward", 
    "publish": "2014-12-20T07:56:29Z", 
    "summary": "In this paper we address the following problem in web document and\ninformation retrieval (IR): How can we use long-term context information to\ngain better IR performance? Unlike common IR methods that use bag of words\nrepresentation for queries and documents, we treat them as a sequence of words\nand use long short term memory (LSTM) to capture contextual dependencies. To\nthe best of our knowledge, this is the first time that LSTM is applied to\ninformation retrieval tasks. Unlike training traditional LSTMs, the training\nstrategy is different due to the special nature of information retrieval\nproblem. Experimental evaluation on an IR task derived from the Bing web search\ndemonstrates the ability of the proposed method in addressing both lexical\nmismatch and long-term context modelling issues, thereby, significantly\noutperforming existing state of the art methods for web document retrieval\ntask."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.7610v1", 
    "title": "Hete-CF: Social-Based Collaborative Filtering Recommendation using   Heterogeneous Relations", 
    "arxiv-id": "1412.7610v1", 
    "author": "Zhe Wang", 
    "publish": "2014-12-24T06:21:42Z", 
    "summary": "Collaborative filtering algorithms haven been widely used in recommender\nsystems. However, they often suffer from the data sparsity and cold start\nproblems. With the increasing popularity of social media, these problems may be\nsolved by using social-based recommendation. Social-based recommendation, as an\nemerging research area, uses social information to help mitigate the data\nsparsity and cold start problems, and it has been demonstrated that the\nsocial-based recommendation algorithms can efficiently improve the\nrecommendation performance. However, few of the existing algorithms have\nconsidered using multiple types of relations within one social network. In this\npaper, we investigate the social-based recommendation algorithms on\nheterogeneous social networks and proposed Hete-CF, a Social Collaborative\nFiltering algorithm using heterogeneous relations. Distinct from the exiting\nmethods, Hete-CF can effectively utilize multiple types of relations in a\nheterogeneous social network. In addition, Hete-CF is a general approach and\ncan be used in arbitrary social networks, including event based social\nnetworks, location based social networks, and any other types of heterogeneous\ninformation networks associated with social information. The experimental\nresults on two real-world data sets, DBLP (a typical heterogeneous information\nnetwork) and Meetup (a typical event based social network) show the\neffectiveness and efficiency of our algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.8099v1", 
    "title": "Extraction of Web Usage Profiles using Simulated Annealing Based   Biclustering Approach", 
    "arxiv-id": "1412.8099v1", 
    "author": "K. Thangavel", 
    "publish": "2014-12-01T10:06:25Z", 
    "summary": "In this paper, the Simulated Annealing (SA) based biclustering approach is\nproposed in which SA is used as an optimization tool for biclustering of web\nusage data to identify the optimal user profile from the given web usage data.\nExtracted biclusters are consists of correlated users whose usage behaviors are\nsimilar across the subset of web pages of a web site where as these users are\nuncorrelated for remaining pages of a web site. These results are very useful\nin web personalization so that it communicates better with its users and for\nmaking customized prediction. Also useful for providing customized web service\ntoo. Experiment was conducted on the real web usage dataset called CTI dataset.\nResults show that proposed SA based biclustering approach can extract highly\ncorrelated user groups from the preprocessed web usage data."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.8118v1", 
    "title": "Hierarchical Bayesian Models with Factorization for Content-Based   Recommendation", 
    "arxiv-id": "1412.8118v1", 
    "author": "Yi Zhang", 
    "publish": "2014-12-28T06:07:48Z", 
    "summary": "Most existing content-based filtering approaches learn user profiles\nindependently without capturing the similarity among users. Bayesian\nhierarchical models \\cite{Zhang:Efficient} learn user profiles jointly and have\nthe advantage of being able to borrow discriminative information from other\nusers through a Bayesian prior. However, the standard Bayesian hierarchical\nmodels assume all user profiles are generated from the same prior. Considering\nthe diversity of user interests, this assumption could be improved by\nintroducing more flexibility. Besides, most existing content-based filtering\napproaches implicitly assume that each user profile corresponds to exactly one\nuser interest and fail to capture a user's multiple interests (information\nneeds).\n  In this paper, we present a flexible Bayesian hierarchical modeling approach\nto model both commonality and diversity among users as well as individual\nusers' multiple interests. We propose two models each with different\nassumptions, and the proposed models are called Discriminative Factored Prior\nModels (DFPM). In our models, each user profile is modeled as a discriminative\nclassifier with a factored model as its prior, and different factors contribute\nin different levels to each user profile. Compared with existing content-based\nfiltering models, DFPM are interesting because they can 1) borrow\ndiscriminative criteria of other users while learning a particular user profile\nthrough the factored prior; 2) trade off well between diversity and commonality\namong users; and 3) handle the challenging classification situation where each\nclass contains multiple concepts. The experimental results on a dataset\ncollected from real users on digg.com show that our models significantly\noutperform the baseline models of L-2 regularized logistic regression and\ntraditional Bayesian hierarchical model with logistic regression."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.8125v1", 
    "title": "Learning from Labeled Features for Document Filtering", 
    "arxiv-id": "1412.8125v1", 
    "author": "Qianli Xing", 
    "publish": "2014-12-28T07:32:09Z", 
    "summary": "Existing document filtering systems learn user profiles based on user\nrelevance feedback on documents. In some cases, users may have prior knowledge\nabout what features are important. For example, a Spanish speaker may only want\nnews written in Spanish, and thus a relevant document should contain the\nfeature \"Language: Spanish\"; a researcher focusing on HIV knows an article with\nthe medical subject \"Subject: AIDS\" is very likely to be relevant to him/her.\n  Semi-structured documents with rich metadata are increasingly prevalent on\nthe Internet. Motivated by the well-adopted faceted search interface in\ne-commerce, we study the exploitation of user prior knowledge on faceted\nfeatures for semi-structured document filtering. We envision two faceted\nfeedback mechanisms, and propose a novel user profile learning algorithm that\ncan incorporate user feedback on features. To evaluate the proposed work, we\nuse two data sets from the TREC filtering track, and conduct a user study on\nAmazon Mechanical Turk. Our experiment results show that user feedback on\nfaceted features is useful for filtering. The proposed user profile learning\nalgorithm can effectively learn from user feedback on both documents and\nfeatures, and performs better than several existing methods."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1412.8281v1", 
    "title": "Interactive Retrieval Based on Wikipedia Concepts", 
    "arxiv-id": "1412.8281v1", 
    "author": "Lanbo Zhang", 
    "publish": "2014-12-29T08:45:59Z", 
    "summary": "This paper presents a new user feedback mechanism based on Wikipedia concepts\nfor interactive retrieval. In this mechanism, the system presents to the user a\ngroup of Wikipedia concepts, and the user can choose those relevant to refine\nhis/her query. To realize this mechanism, we propose methods to address two\nproblems: 1) how to select a small number of possibly relevant Wikipedia\nconcepts to show the user, and 2) how to re-rank retrieved documents given the\nuser-identified Wikipedia concepts. Our methods are evaluated on three TREC\ndata sets. The experiment results show that our methods can dramatically\nimprove retrieval performances."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1502.00527v1", 
    "title": "Context Models For Web Search Personalization", 
    "arxiv-id": "1502.00527v1", 
    "author": "Maksims Volkovs", 
    "publish": "2015-02-02T15:50:39Z", 
    "summary": "We present our solution to the Yandex Personalized Web Search Challenge. The\naim of this challenge was to use the historical search logs to personalize\ntop-N document rankings for a set of test users. We used over 100 features\nextracted from user- and query-depended contexts to train neural net and\ntree-based learning-to-rank and regression models. Our final submission, which\nwas a blend of several different models, achieved an NDCG@10 of 0.80476 and\nplaced 4'th amongst the 194 teams winning 3'rd prize."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2736277.2741665", 
    "link": "http://arxiv.org/pdf/1502.00804v2", 
    "title": "A Polya Urn Document Language Model for Improved Information Retrieval", 
    "arxiv-id": "1502.00804v2", 
    "author": "Yuanhua Lv", 
    "publish": "2015-02-03T10:41:12Z", 
    "summary": "The multinomial language model has been one of the most effective models of\nretrieval for over a decade. However, the multinomial distribution does not\nmodel one important linguistic phenomenon relating to term-dependency, that is\nthe tendency of a term to repeat itself within a document (i.e. word\nburstiness). In this article, we model document generation as a random process\nwith reinforcement (a multivariate Polya process) and develop a Dirichlet\ncompound multinomial language model that captures word burstiness directly.\n  We show that the new reinforced language model can be computed as efficiently\nas current retrieval models, and with experiments on an extensive set of TREC\ncollections, we show that it significantly outperforms the state-of-the-art\nlanguage model for a number of standard effectiveness metrics. Experiments also\nshow that the tuning parameter in the proposed model is more robust than in the\nmultinomial language model. Furthermore, we develop a constraint for the\nverbosity hypothesis and show that the proposed model adheres to the\nconstraint. Finally, we show that the new language model essentially introduces\na measure closely related to idf which gives theoretical justification for\ncombining the term and document event spaces in tf-idf type schemes."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2735629", 
    "link": "http://arxiv.org/pdf/1502.01916v1", 
    "title": "A General SIMD-based Approach to Accelerating Compression Algorithms", 
    "arxiv-id": "1502.01916v1", 
    "author": "Ji-Rong Wen", 
    "publish": "2015-02-06T15:11:47Z", 
    "summary": "Compression algorithms are important for data oriented tasks, especially in\nthe era of Big Data. Modern processors equipped with powerful SIMD instruction\nsets, provide us an opportunity for achieving better compression performance.\nPrevious research has shown that SIMD-based optimizations can multiply decoding\nspeeds. Following these pioneering studies, we propose a general approach to\naccelerate compression algorithms. By instantiating the approach, we have\ndeveloped several novel integer compression algorithms, called Group-Simple,\nGroup-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding\nvectorized versions. We evaluate the proposed algorithms on two public TREC\ndatasets, a Wikipedia dataset and a Twitter dataset. With competitive\ncompression ratios and encoding speeds, our SIMD-based algorithms outperform\nstate-of-the-art non-vectorized algorithms with respect to decoding speeds."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2735629", 
    "link": "http://arxiv.org/pdf/1502.01965v1", 
    "title": "How can heat maps of indexing vocabularies be utilized for information   seeking purposes?", 
    "arxiv-id": "1502.01965v1", 
    "author": "Karima Haddou ou Moussa", 
    "publish": "2015-02-06T17:38:48Z", 
    "summary": "The ability to browse an information space in a structured way by exploiting\nsimilarities and dissimilarities between information objects is crucial for\nknowledge discovery. Knowledge maps use visualizations to gain insights into\nthe structure of large-scale information spaces, but are still far away from\nbeing applicable for searching. The paper proposes a use case for enhancing\nsearch term recommendations by heat map visualizations of co-word\nrelation-ships taken from indexing vocabulary. By contrasting areas of\ndifferent \"heat\" the user is enabled to indicate mainstream areas of the field\nin question more easily."
},{
    "category": "cs.IR", 
    "doi": "10.3233/FI-2015-1166", 
    "link": "http://arxiv.org/pdf/1502.04032v1", 
    "title": "On Projection Based Operators in Lp space for Exact Similarity Search", 
    "arxiv-id": "1502.04032v1", 
    "author": "Catarina Moreira", 
    "publish": "2015-02-12T10:56:10Z", 
    "summary": "We investigate exact indexing for high dimensional Lp norms based on the\n1-Lipschitz property and projection operators. The orthogonal projection that\nsatisfies the 1-Lipschitz property for the Lp norm is described. The adaptive\nprojection defined by the first principal component is introduced."
},{
    "category": "cs.IR", 
    "doi": "10.3233/FI-2015-1166", 
    "link": "http://arxiv.org/pdf/1502.04331v1", 
    "title": "Two-Stage Document Length Normalization for Information Retrieval", 
    "arxiv-id": "1502.04331v1", 
    "author": "Seung-Hoon Na", 
    "publish": "2015-02-15T16:49:09Z", 
    "summary": "The standard approach for term frequency normalization is based only on the\ndocument length. However, it does not distinguish the verbosity from the scope,\nthese being the two main factors determining the document length. Because the\nverbosity and scope have largely different effects on the increase in term\nfrequency, the standard approach can easily suffer from insufficient or\nexcessive penalization depending on the specific type of long document. To\novercome these problems, this paper proposes two-stage normalization by\nperforming verbosity and scope normalization separately, and by employing\ndifferent penalization functions. In verbosity normalization, each document is\npre-normalized by dividing the term frequency by the verbosity of the document.\nIn scope normalization, an existing retrieval model is applied in a\nstraightforward manner to the pre-normalized document, finally leading us to\nformulate our proposed verbosity normalized (VN) retrieval model. Experimental\nresults carried out on standard TREC collections demonstrate that the VN model\nleads to marginal but statistically significant improvements over standard\nretrieval models."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICOSC.2015.7050845", 
    "link": "http://arxiv.org/pdf/1502.04348v1", 
    "title": "Semantic Modeling of Analytic-based Relationships with Direct   Qualification", 
    "arxiv-id": "1502.04348v1", 
    "author": "Matthew Paulini", 
    "publish": "2015-02-15T19:01:14Z", 
    "summary": "Successfully modeling state and analytics-based semantic relationships of\ndocuments enhances representation, importance, relevancy, provenience, and\npriority of the document. These attributes are the core elements that form the\nmachine-based knowledge representation for documents. However, modeling\ndocument relationships that can change over time can be inelegant, limited,\ncomplex or overly burdensome for semantic technologies. In this paper, we\npresent Direct Qualification (DQ), an approach for modeling any semantically\nreferenced document, concept, or named graph with results from associated\napplied analytics. The proposed approach supplements the traditional\nsubject-object relationships by providing a third leg to the relationship; the\nqualification of how and why the relationship exists. To illustrate, we show a\nprototype of an event-based system with a realistic use case for applying DQ to\nrelevancy analytics of PageRank and Hyperlink-Induced Topic Search (HITS)."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICOSC.2015.7050845", 
    "link": "http://arxiv.org/pdf/1502.04696v1", 
    "title": "Enhancing Information Awareness Through Directed Qualification of   Semantic Relevancy Scoring Operations", 
    "arxiv-id": "1502.04696v1", 
    "author": "Matthew Paulini", 
    "publish": "2015-02-15T19:13:26Z", 
    "summary": "Successfully managing analytics-based semantic relationships and their\nprovenance enables determinations of document importance and priority,\nfurthering capabilities for machine-based relevancy scoring operations.\nSemantic technologies are well suited for modeling explicit and fully qualified\nrelationships but struggle with modeling relationships that are qualified in\nnature, or resultant from applied analytics. Our work seeks to implement the\nautonomous Directed Qualification of analytic-based relationships by pairing\nthe Prov-O Ontology (W3C Recommendation) with a relevancy ontology supporting\nanalytics terminology. This work results in the capability for any semantically\nreferenced document, concept, or named graph to be associated with the results\nof applied analytics as Direct Qualification (DQ) modeled relational nodes.\nThis new capability will enable role, identity, or any other content-based\nmeasures of relevancy and analytics-based metrics for semantically described\ndocuments."
},{
    "category": "cs.IR", 
    "doi": "10.1633/JISTaP.2013.1.3.3", 
    "link": "http://arxiv.org/pdf/1502.04823v1", 
    "title": "Topic Level Disambiguation for Weak Queries", 
    "arxiv-id": "1502.04823v1", 
    "author": "Elin Jacob", 
    "publish": "2015-02-17T08:14:51Z", 
    "summary": "Despite limited success, information retrieval (IR) systems today are not\nintelligent or reliable. IR systems return poor search results when users\nformulate their information needs into incomplete or ambiguous queries (i.e.,\nweak queries). Therefore, one of the main challenges in modern IR research is\nto provide consistent results across all queries by improving the performance\non weak queries. However, existing IR approaches such as query expansion are\nnot overly effective because they make little effort to analyze and exploit the\nmeanings of the queries. Furthermore, word sense disambiguation approaches,\nwhich rely on textual context, are ineffective against weak queries that are\ntypically short. Motivated by the demand for a robust IR system that can\nconsistently provide highly accurate results, the proposed study implemented a\nnovel topic detection that leveraged both the language model and structural\nknowledge of Wikipedia and systematically evaluated the effect of query\ndisambiguation and topic-based retrieval approaches on TREC collections. The\nresults not only confirm the effectiveness of the proposed topic detection and\ntopic-based retrieval approaches but also demonstrate that query disambiguation\ndoes not improve IR as expected."
},{
    "category": "cs.IR", 
    "doi": "10.1633/JISTaP.2013.1.3.3", 
    "link": "http://arxiv.org/pdf/1502.05131v1", 
    "title": "Affective Music Information Retrieval", 
    "arxiv-id": "1502.05131v1", 
    "author": "Hsin-Min Wang", 
    "publish": "2015-02-18T06:29:45Z", 
    "summary": "Much of the appeal of music lies in its power to convey emotions/moods and to\nevoke them in listeners. In consequence, the past decade witnessed a growing\ninterest in modeling emotions from musical signals in the music information\nretrieval (MIR) community. In this article, we present a novel generative\napproach to music emotion modeling, with a specific focus on the\nvalence-arousal (VA) dimension model of emotion. The presented generative\nmodel, called \\emph{acoustic emotion Gaussians} (AEG), better accounts for the\nsubjectivity of emotion perception by the use of probability distributions.\nSpecifically, it learns from the emotion annotations of multiple subjects a\nGaussian mixture model in the VA space with prior constraints on the\ncorresponding acoustic features of the training music pieces. Such a\ncomputational framework is technically sound, capable of learning in an online\nfashion, and thus applicable to a variety of applications, including\nuser-independent (general) and user-dependent (personalized) emotion\nrecognition and emotion-based music retrieval. We report evaluations of the\naforementioned applications of AEG on a larger-scale emotion-annotated corpora,\nAMG1608, to demonstrate the effectiveness of AEG and to showcase how\nevaluations are conducted for research on emotion-based MIR. Directions of\nfuture work are also discussed."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1502.05168v1", 
    "title": "Query Expansion Strategy based on Pseudo Relevance Feedback and Term   Weight Scheme for Monolingual Retrieval", 
    "arxiv-id": "1502.05168v1", 
    "author": "Namita Srivastava", 
    "publish": "2015-02-18T09:55:37Z", 
    "summary": "Query Expansion using Pseudo Relevance Feedback is a useful and a popular\ntechnique for reformulating the query. In our proposed query expansion method,\nwe assume that relevant information can be found within a document near the\ncentral idea. The document is normally divided into sections, paragraphs and\nlines. The proposed method tries to extract keywords that are closer to the\ncentral theme of the document. The expansion terms are obtained by\nequi-frequency partition of the documents obtained from pseudo relevance\nfeedback and by using tf-idf scores. The idf factor is calculated for number of\npartitions in documents. The group of words for query expansion is selected\nusing the following approaches: the highest score, average score and a group of\nwords that has maximum number of keywords. As each query behaved differently\nfor different methods, the effect of these methods in selecting the words for\nquery expansion is investigated. From this initial study, we extend the\nexperiment to develop a rule-based statistical model that automatically selects\nthe best group of words incorporating the tf-idf scoring and the 3 approaches\nexplained here, in the future. The experiments were performed on FIRE 2011\nAdhoc Hindi and English test collections on 50 queries each, using Terrier as\nretrieval engine."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1502.05535v1", 
    "title": "Evolutionary algorithm based adaptive navigation in information   retrieval interfaces", 
    "arxiv-id": "1502.05535v1", 
    "author": "Taras Filatov", 
    "publish": "2015-02-19T11:47:28Z", 
    "summary": "In computer interfaces in general, especially in information retrieval tasks,\nit is important to be able to quickly find and retrieve information. State of\nthe art approach, used, for example, in search engines, is not effective as it\nintroduces losses of meanings due to context to keywords back and forth\ntranslation. Authors argue it increases the time and reduces the accuracy of\ninformation retrieval compared to what it could be in the system that employs\nmodern information retrieval and text mining methods while presenting results\nin an adaptive human- computer interface where system effectively learns what\noperator needs through iterative interaction. In current work, a combination of\nadaptive navigational interface and real time collaborative feedback analysis\nfor documents relevance weighting is proposed as an viable alternative to\nprevailing \"telegraphic\" approach in information retrieval systems. Adaptive\nnavigation is provided through a dynamic links panel controlled by an\nevolutionary algorithm. Documents relevance is initially established with\nstandard information retrieval techniques and is further refined in real time\nthrough interaction of users with the system. Introduced concepts of\nmultidimensional Knowledge Map and Weighted Point of Interest allow finding\nrelevant documents and users with common interests through a trivial\ncalculation. Browsing search approach, the ability of the algorithm to adapt\nnavigation to users interests, collaborative refinement and the self-organising\nfeatures of the system are the main factors making such architecture effective\nin various fields where non-structured knowledge shall be represented to the\nusers."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1502.07015v1", 
    "title": "A framework to discover potential ideas of new product development from   crowdsourcing application", 
    "arxiv-id": "1502.07015v1", 
    "author": "Joonsoo Bae", 
    "publish": "2015-02-25T00:13:44Z", 
    "summary": "In this paper, we study idea mining from crowdsourcing applications which\nencourage a group of people, who are usually undefined and very large sized, to\ngenerate ideas for new product development (NPD). In order to isolate the\nrelatively small number of potential ones among ideas from crowd, decision\nmakers not only have to identify the key textual information representing the\nideas, but they also need to consider online opinions of people who gave\ncomments and votes on the ideas. Due to the extremely large size of text data\ngenerated by people on the Internet, identifying textual information has been\ncarried out in manual ways, and has been considered very time consuming and\ncostly. To overcome the ineffectiveness, this paper introduces a novel\nframework that can help decision makers discover ideas having the potential to\nbe used in an NPD process. To achieve this, a semi-automatic text mining\ntechnique that retrieves useful text patterns from ideas posted on\ncrowdsourcing application is proposed. Then, we provide an online learning\nalgorithm to evaluate whether the idea is potential or not. Finally to verify\nthe effectiveness of our algorithm, we conducted experiments on the data, which\nare collected from an existing crowd sourcing website."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1502.07938v1", 
    "title": "Document Clustering using K-Means and K-Medoids", 
    "arxiv-id": "1502.07938v1", 
    "author": "Monica Jha", 
    "publish": "2015-02-27T15:44:46Z", 
    "summary": "With the huge upsurge of information in day-to-days life, it has become\ndifficult to assemble relevant information in nick of time. But people, always\nare in dearth of time, they need everything quick. Hence clustering was\nintroduced to gather the relevant information in a cluster. There are several\nalgorithms for clustering information out of which in this paper, we accomplish\nK-means and K-Medoids clustering algorithm and a comparison is carried out to\nfind which algorithm is best for clustering. On the best clusters formed,\ndocument summarization is executed based on sentence weight to focus on key\npoint of the whole document, which makes it easier for people to ascertain the\ninformation they want and thus read only those documents which is relevant in\ntheir point of view."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.01647v1", 
    "title": "Decentralized Recommender Systems", 
    "arxiv-id": "1503.01647v1", 
    "author": "Thomas S. Huang", 
    "publish": "2015-03-05T14:34:02Z", 
    "summary": "This paper proposes a decentralized recommender system by formulating the\npopular collaborative filleting (CF) model into a decentralized matrix\ncompletion form over a set of users. In such a way, data storages and\ncomputations are fully distributed. Each user could exchange limited\ninformation with its local neighborhood, and thus it avoids the centralized\nfusion. Advantages of the proposed system include a protection on user privacy,\nas well as better scalability and robustness. We compare our proposed algorithm\nwith several state-of-the-art algorithms on the FlickerUserFavor dataset, and\ndemonstrate that the decentralized algorithm can gain a competitive performance\nto others."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.03168v1", 
    "title": "Experimental Estimation of Number of Clusters Based on Cluster Quality", 
    "arxiv-id": "1503.03168v1", 
    "author": "Kalyani Desikan", 
    "publish": "2015-03-10T10:34:06Z", 
    "summary": "Text Clustering is a text mining technique which divides the given set of\ntext documents into significant clusters. It is used for organizing a huge\nnumber of text documents into a well-organized form. In the majority of the\nclustering algorithms, the number of clusters must be specified apriori, which\nis a drawback of these algorithms. The aim of this paper is to show\nexperimentally how to determine the number of clusters based on cluster\nquality. Since partitional clustering algorithms are well-suited for clustering\nlarge document datasets, we have confined our analysis to a partitional\nclustering algorithm."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.03607v1", 
    "title": "A divisive hierarchical clustering-based method for indexing image   information", 
    "arxiv-id": "1503.03607v1", 
    "author": "Najva Izadpanah", 
    "publish": "2015-03-12T06:51:06Z", 
    "summary": "In most practical applications of image retrieval, high-dimensional feature\nvectors are required, but current multi-dimensional indexing structures lose\ntheir efficiency with growth of dimensions. Our goal is to propose a divisive\nhierarchical clustering-based multi-dimensional indexing structure which is\nefficient in high-dimensional feature spaces. A projection pursuit method has\nbeen used for finding a component of the data, which data's projections onto it\nmaximizes the approximation of negentropy for preparing essential information\nin order to partitioning of the data space. Various tests and experimental\nresults on high-dimensional datasets indicate the performance of proposed\nmethod in comparison with others."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.03957v1", 
    "title": "Implementation of an efficient Fuzzy Logic based Information Retrieval   System", 
    "arxiv-id": "1503.03957v1", 
    "author": "Narina Thakur", 
    "publish": "2015-03-13T05:21:02Z", 
    "summary": "This paper exemplifies the implementation of an efficient Information\nRetrieval (IR) System to compute the similarity between a dataset and a query\nusing Fuzzy Logic. TREC dataset has been used for the same purpose. The dataset\nis parsed to generate keywords index which is used for the similarity\ncomparison with the user query. Each query is assigned a score value based on\nits fuzzy similarity with the index keywords. The relevant documents are\nretrieved based on the score value. The performance and accuracy of the\nproposed fuzzy similarity model is compared with Cosine similarity model using\nPrecision-Recall curves. The results prove the dominance of Fuzzy Similarity\nbased IR system."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.03961v1", 
    "title": "Knowledge-based Query Expansion in Real-Time Microblog Search", 
    "arxiv-id": "1503.03961v1", 
    "author": "Jianwu Yang", 
    "publish": "2015-03-13T05:54:04Z", 
    "summary": "Since the length of microblog texts, such as tweets, is strictly limited to\n140 characters, traditional Information Retrieval techniques suffer from the\nvocabulary mismatch problem severely and cannot yield good performance in the\ncontext of microblogosphere. To address this critical challenge, in this paper,\nwe propose a new language modeling approach for microblog retrieval by\ninferring various types of context information. In particular, we expand the\nquery using knowledge terms derived from Freebase so that the expanded one can\nbetter reflect users' search intent. Besides, in order to further satisfy\nusers' real-time information need, we incorporate temporal evidences into the\nexpansion method, which can boost recent tweets in the retrieval results with\nrespect to a given topic. Experimental results on two official TREC Twitter\ncorpora demonstrate the significant superiority of our approach over baseline\nmethods."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.04424v1", 
    "title": "Bridging Social Media via Distant Supervision", 
    "arxiv-id": "1503.04424v1", 
    "author": "Fabrizio Sebastiani", 
    "publish": "2015-03-15T13:22:03Z", 
    "summary": "Microblog classification has received a lot of attention in recent years.\nDifferent classification tasks have been investigated, most of them focusing on\nclassifying microblogs into a small number of classes (five or less) using a\ntraining set of manually annotated tweets. Unfortunately, labelling data is\ntedious and expensive, and finding tweets that cover all the classes of\ninterest is not always straightforward, especially when some of the classes do\nnot frequently arise in practice. In this paper we study an approach to tweet\nclassification based on distant supervision, whereby we automatically transfer\nlabels from one social medium to another for a single-label multi-class\nclassification task. In particular, we apply YouTube video classes to tweets\nlinking to these videos. This provides for free a virtually unlimited number of\nlabelled instances that can be used as training data. The classification\nexperiments we have run show that training a tweet classifier via these\nautomatically labelled data achieves substantially better performance than\ntraining the same classifier with a limited amount of manually labelled data;\nthis is advantageous, given that the automatically labelled data come at no\ncost. Further investigation of our approach shows its robustness when applied\nwith different numbers of classes and across different languages."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.05781v1", 
    "title": "Memantic: A Medical Knowledge Discovery Engine", 
    "arxiv-id": "1503.05781v1", 
    "author": "Alexei Yavlinsky", 
    "publish": "2015-03-19T14:31:28Z", 
    "summary": "We present a system that constructs and maintains an up-to-date co-occurrence\nnetwork of medical concepts based on continuously mining the latest biomedical\nliterature. Users can explore this network visually via a concise online\ninterface to quickly discover important and novel relationships between medical\nentities. This enables users to rapidly gain contextual understanding of their\nmedical topics of interest, and we believe this constitutes a significant user\nexperience improvement over contemporary search engines operating in the\nbiomedical literature domain."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.06562v1", 
    "title": "An Item-Based Collaborative Filtering using Dimensionality Reduction   Techniques on Mahout Framework", 
    "arxiv-id": "1503.06562v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2015-03-23T09:09:07Z", 
    "summary": "Collaborative Filtering is the most widely used prediction technique in\nRecommendation System. Most of the current CF recommender systems maintains\nsingle criteria user rating in user item matrix. However, recent studies\nindicate that recommender system depending on multi criteria can improve\nprediction and accuracy levels of recommendation by considering the user\npreferences in multi aspects of items. This gives birth to Multi Criteria\nCollaborative Filtering. In MC CF users provide the rating on multiple aspects\nof an item in new dimensions,thereby increasing the size of rating matrix,\nsparsity and scalability problem. Appropriate dimensionality reduction\ntechniques are thus needed to take care of these challenges to reduce the\ndimension of user item rating matrix to improve the prediction accuracy and\nefficiency of CF recommender system. The process of dimensionality reduction\nmaps the high dimensional input space into lower dimensional space. Thus, the\nobjective of this paper is to propose an efficient MC CF algorithm using\ndimensionality reduction technique to improve the recommendation quality and\nprediction accuracy. Dimensionality reduction techniques such as Singular Value\nDecomposition and Principal Component Analysis are used to solve the\nscalability and alleviate the sparsity problems in overall rating. The proposed\nMC CF approach will be implemented using Apache Mahout, which allows processing\nof massive dataset stored in distributed/non-distributed file system."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.06598v1", 
    "title": "Identifying Web Tables - Supporting a Neglected Type of Content on the   Web", 
    "arxiv-id": "1503.06598v1", 
    "author": "S\u00f6ren Auer", 
    "publish": "2015-03-23T11:05:26Z", 
    "summary": "The abundance of the data in the Internet facilitates the improvement of\nextraction and processing tools. The trend in the open data publishing\nencourages the adoption of structured formats like CSV and RDF. However, there\nis still a plethora of unstructured data on the Web which we assume contain\nsemantics. For this reason, we propose an approach to derive semantics from web\ntables which are still the most popular publishing tool on the Web. The paper\nalso discusses methods and services of unstructured data extraction and\nprocessing as well as machine learning techniques to enhance such a workflow.\nThe eventual result is a framework to process, publish and visualize linked\nopen data. The software enables tables extraction from various open data\nsources in the HTML format and an automatic export to the RDF format making the\ndata linked. The paper also gives the evaluation of machine learning techniques\nin conjunction with string similarity functions to be applied in a tables\nrecognition task."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.06609v1", 
    "title": "Web Search Result Clustering based on Cuckoo Search and Consensus   Clustering", 
    "arxiv-id": "1503.06609v1", 
    "author": "Kishwar Sadaf", 
    "publish": "2015-03-23T11:52:49Z", 
    "summary": "Clustering of web search result document has emerged as a promising tool for\nimproving retrieval performance of an Information Retrieval (IR) system. Search\nresults often plagued by problems like synonymy, polysemy, high volume etc.\nClustering other than resolving these problems also provides the user the\neasiness to locate his/her desired information. In this paper, a method, called\nWSRDC-CSCC, is introduced to cluster web search result using cuckoo search\nmeta-heuristic method and Consensus clustering. Cuckoo search provides a solid\nfoundation for consensus clustering. As a local clustering function, k-means\ntechnique is used. The final number of cluster is not depended on this k.\nConsensus clustering finds the natural grouping of the objects. The proposed\nalgorithm is compared to another clustering method which is based on cuckoo\nsearch and Bayesian Information Criterion. The experimental results show that\nproposed algorithm finds the actual number of clusters with great value of\nprecision, recall and F-measure as compared to the other method"
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.07387v4", 
    "title": "Vectorized VByte Decoding", 
    "arxiv-id": "1503.07387v4", 
    "author": "Daniel Lemire", 
    "publish": "2015-02-20T14:52:06Z", 
    "summary": "We consider the ubiquitous technique of VByte compression, which represents\neach integer as a variable length sequence of bytes. The low 7 bits of each\nbyte encode a portion of the integer, and the high bit of each byte is reserved\nas a continuation flag. This flag is set to 1 for all bytes except the last,\nand the decoding of each integer is complete when a byte with a high bit of 0\nis encountered. VByte decoding can be a performance bottleneck especially when\nthe unpredictable lengths of the encoded integers cause frequent branch\nmispredictions. Previous attempts to accelerate VByte decoding using SIMD\nvector instructions have been disappointing, prodding search engines such as\nGoogle to use more complicated but faster-to-decode formats for\nperformance-critical code. Our decoder (Masked VByte) is 2 to 4 times faster\nthan a conventional scalar VByte decoder, making the format once again\ncompetitive with regard to speed."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.07474v1", 
    "title": "User Profiling Trends, Techniques and Applications", 
    "arxiv-id": "1503.07474v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2015-03-25T17:52:21Z", 
    "summary": "The Personalization of information has taken recommender systems at a very\nhigh level. With personalization these systems can generate user specific\nrecommendations accurately and efficiently. User profiling helps\npersonalization, where information retrieval is done to personalize a scenario\nwhich maintains a separate user profile for individual user. The main objective\nof this paper is to explore this field of personalization in context of user\nprofiling, to help researchers make aware of the user profiling. Various\ntrends, techniques and Applications have been discussed in paper which will\nfulfill this motto."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.07475v1", 
    "title": "Role of Matrix Factorization Model in Collaborative Filtering Algorithm:   A Survey", 
    "arxiv-id": "1503.07475v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2015-03-25T17:54:43Z", 
    "summary": "Recommendation Systems apply Information Retrieval techniques to select the\nonline information relevant to a given user. Collaborative Filtering is\ncurrently most widely used approach to build Recommendation System. CF\ntechniques uses the user behavior in form of user item ratings as their\ninformation source for prediction. There are major challenges like sparsity of\nrating matrix and growing nature of data which is faced by CF algorithms. These\nchallenges are been well taken care by Matrix Factorization. In this paper we\nattempt to present an overview on the role of different MF model to address the\nchallenges of CF algorithms, which can be served as a roadmap for research in\nthis area."
},{
    "category": "cs.IR", 
    "doi": "10.5120/18394-9646", 
    "link": "http://arxiv.org/pdf/1503.08463v1", 
    "title": "A Novel Modified Apriori Approach for Web Document Clustering", 
    "arxiv-id": "1503.08463v1", 
    "author": "Sanjay Kumar Sahay", 
    "publish": "2015-03-29T17:40:18Z", 
    "summary": "The traditional apriori algorithm can be used for clustering the web\ndocuments based on the association technique of data mining. But this algorithm\nhas several limitations due to repeated database scans and its weak association\nrule analysis. In modern world of large databases, efficiency of traditional\napriori algorithm would reduce manifolds. In this paper, we proposed a new\nmodified apriori approach by cutting down the repeated database scans and\nimproving association analysis of traditional apriori algorithm to cluster the\nweb documents. Further we improve those clusters by applying Fuzzy C-Means\n(FCM), K-Means and Vector Space Model (VSM) techniques separately. For\nexperimental purpose, we use Classic3 and Classic4 datasets of Cornell\nUniversity having more than 10,000 documents and run both traditional apriori\nand our modified apriori approach on it. Experimental results show that our\napproach outperforms the traditional apriori algorithm in terms of database\nscan and improvement on association of analysis. We found out that FCM is\nbetter than K-Means and VSM in terms of F-measure of clusters of different\nsizes."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICHPCA.2014.7045347", 
    "link": "http://arxiv.org/pdf/1504.00191v1", 
    "title": "Automated Document Indexing via Intelligent Hierarchical Clustering: A   Novel Approach", 
    "arxiv-id": "1504.00191v1", 
    "author": "Sanjay Kumar Sahay", 
    "publish": "2015-04-01T12:08:36Z", 
    "summary": "With the rising quantity of textual data available in electronic format, the\nneed to organize it become a highly challenging task. In the present paper, we\nexplore a document organization framework that exploits an intelligent\nhierarchical clustering algorithm to generate an index over a set of documents.\nThe framework has been designed to be scalable and accurate even with large\ncorpora. The advantage of the proposed algorithm lies in the need for minimal\ninputs, with much of the hierarchy attributes being decided in an automated\nmanner using statistical methods. The use of topic modeling in a pre-processing\nstage ensures robustness to a range of variations in the input data. For\nexperimental work 20-Newsgroups dataset has been used. The F- measure of the\nproposed approach has been compared with the traditional K-Means and K-Medoids\nclustering algorithms. Test results demonstrate the applicability, efficiency\nand effectiveness of our proposed approach. After extensive experimentation, we\nconclude that the framework shows promise for further research and specialized\ncommercial applications."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICHPCA.2014.7045347", 
    "link": "http://arxiv.org/pdf/1504.00305v1", 
    "title": "Study the effectiveness of genetic algorithm for documentary subject   search", 
    "arxiv-id": "1504.00305v1", 
    "author": "B. V. Palyukh", 
    "publish": "2015-04-01T17:24:52Z", 
    "summary": "This article presents results of experimental studies the effectiveness of\nthe genetic algorithm that was applied to effective queries creation and\nrelevant document selection. Studies were carried out to the comparative\nanalysis of the semantic relevance and quality ranking of the documents found\non the Internet in various ways. Analysis of the results shows that the\ngreatest effect of presented technology is achieved by finding new documents\nfor skilled users in the initial stages of the study of the topic.\nAdditionally, the number of unique and relevant results is significantly\nincreased."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICHPCA.2014.7045347", 
    "link": "http://arxiv.org/pdf/1504.01183v1", 
    "title": "Document Clustering using K-Medoids", 
    "arxiv-id": "1504.01183v1", 
    "author": "Monica Jha", 
    "publish": "2015-04-06T01:19:25Z", 
    "summary": "People are always in search of matters for which they are prone to use\ninternet, but again it has huge assemblage of data due to which it becomes\ndifficult for the reader to get the most accurate data. To make it easier for\npeople to gather accurate data, similar information has to be clustered at one\nplace. There are many algorithms used for clustering of relevant information in\none platform. In this paper, K-Medoids clustering algorithm has been employed\nfor formation of clusters which is further used for document summarization."
},{
    "category": "cs.IR", 
    "doi": "10.1109/ICHPCA.2014.7045347", 
    "link": "http://arxiv.org/pdf/1504.01433v1", 
    "title": "Automated System for Improving RSS Feeds Data Quality", 
    "arxiv-id": "1504.01433v1", 
    "author": "Joan Hurtado", 
    "publish": "2015-04-06T22:55:38Z", 
    "summary": "Nowadays, the majority of RSS feeds provide incomplete information about\ntheir news items. The lack of information leads to engagement loss in users. We\npresent a new automated system for improving the RSS feeds' data quality. RSS\nfeeds provide a list of the latest news items ordered by date. Therefore, it\nmakes it easy for a web crawler to precisely locate the item and extract its\nraw content. Then it identifies where the main content is located and extracts:\nmain text corpus, relevant keywords, bigrams, best image and predicts the\ncategory of the item. The output of the system is an enhanced RSS feed. The\nproposed system showed an average item data quality improvement from 39.98% to\n95.62%."
},{
    "category": "cs.IR", 
    "doi": "10.15439/978-83-60810-57-6", 
    "link": "http://arxiv.org/pdf/1504.02362v1", 
    "title": "Approaches to the Intelligent Subject Search", 
    "arxiv-id": "1504.02362v1", 
    "author": "A. N. Sotnikov", 
    "publish": "2015-04-09T16:22:32Z", 
    "summary": "This article presents main results of the pilot study of approaches to the\nsubject information search based on automated semantic processing of mass\nscientific and technical data. The authors focus on technology of building and\nqualification of search queries with the following filtering and ranking of\nsearch data. Software architecture, specific features of subject search and\nresearch results application are considered."
},{
    "category": "cs.IR", 
    "doi": "10.15439/978-83-60810-57-6", 
    "link": "http://arxiv.org/pdf/1504.03713v1", 
    "title": "Detecting Sponsored Recommendations", 
    "arxiv-id": "1504.03713v1", 
    "author": "Sanjay Shakkottai", 
    "publish": "2015-04-14T20:38:30Z", 
    "summary": "With a vast number of items, web-pages, and news to choose from, online\nservices and the customers both benefit tremendously from personalized\nrecommender systems. Such systems however provide great opportunities for\ntargeted advertisements, by displaying ads alongside genuine recommendations.\nWe consider a biased recommendation system where such ads are displayed without\nany tags (disguised as genuine recommendations), rendering them\nindistinguishable to a single user. We ask whether it is possible for a small\nsubset of collaborating users to detect such a bias. We propose an algorithm\nthat can detect such a bias through statistical analysis on the collaborating\nusers' feedback. The algorithm requires only binary information indicating\nwhether a user was satisfied with each of the recommended item or not. This\nmakes the algorithm widely appealing to real world issues such as\nidentification of search engine bias and pharmaceutical lobbying. We prove that\nthe proposed algorithm detects the bias with high probability for a broad class\nof recommendation systems when sufficient number of users provide feedback on\nsufficient number of recommendations. We provide extensive simulations with\nreal data sets and practical recommender systems, which confirm the trade offs\nin the theoretical guarantees."
},{
    "category": "cs.IR", 
    "doi": "10.15439/978-83-60810-57-6", 
    "link": "http://arxiv.org/pdf/1504.04596v2", 
    "title": "Structural Learning of Diverse Ranking", 
    "arxiv-id": "1504.04596v2", 
    "author": "Xueqi Cheng", 
    "publish": "2015-04-17T18:23:16Z", 
    "summary": "Relevance and diversity are both crucial criteria for an effective search\nsystem. In this paper, we propose a unified learning framework for\nsimultaneously optimizing both relevance and diversity. Specifically, the\nproblem is formalized as a structural learning framework optimizing\nDiversity-Correlated Evaluation Measures (DCEM), such as ERR-IA, a-NDCG and\nNRBP. Within this framework, the discriminant function is defined to be a\nbi-criteria objective maximizing the sum of the relevance scores and\ndissimilarities (or diversity) among the documents. Relevance and diversity\nfeatures are utilized to define the relevance scores and dissimilarities,\nrespectively. Compared with traditional methods, the advantages of our approach\nlie in that: (1) Directly optimizing DCEM as the loss function is more\nfundamental for the task; (2) Our framework does not rely on explicit diversity\ninformation such as subtopics, thus is more adaptive to real application; (3)\nThe representation of diversity as the feature-based scoring function is more\nflexible to incorporate rich diversity-based features into the learning\nframework. Extensive experiments on the public TREC datasets show that our\napproach significantly outperforms state-of-the-art diversification approaches,\nwhich validate the above advantages."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2911493", 
    "link": "http://arxiv.org/pdf/1504.04818v2", 
    "title": "Composite Correlation Quantization for Efficient Multimodal Retrieval", 
    "arxiv-id": "1504.04818v2", 
    "author": "Philip S. Yu", 
    "publish": "2015-04-19T10:16:50Z", 
    "summary": "Efficient similarity retrieval from large-scale multimodal database is\npervasive in modern search engines and social networks. To support queries\nacross content modalities, the system should enable cross-modal correlation and\ncomputation-efficient indexing. While hashing methods have shown great\npotential in achieving this goal, current attempts generally fail to learn\nisomorphic hash codes in a seamless scheme, that is, they embed multiple\nmodalities in a continuous isomorphic space and separately threshold embeddings\ninto binary codes, which incurs substantial loss of retrieval accuracy. In this\npaper, we approach seamless multimodal hashing by proposing a novel Composite\nCorrelation Quantization (CCQ) model. Specifically, CCQ jointly finds\ncorrelation-maximal mappings that transform different modalities into\nisomorphic latent space, and learns composite quantizers that convert the\nisomorphic latent features into compact binary codes. An optimization framework\nis devised to preserve both intra-modal similarity and inter-modal correlation\nthrough minimizing both reconstruction and quantization errors, which can be\ntrained from both paired and partially paired data in linear time. A\ncomprehensive set of experiments clearly show the superior effectiveness and\nefficiency of CCQ against the state of the art hashing methods for both\nunimodal and cross-modal retrieval."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2911493", 
    "link": "http://arxiv.org/pdf/1504.04945v1", 
    "title": "Topic-focused Dynamic Information Filtering in Social Media", 
    "arxiv-id": "1504.04945v1", 
    "author": "Xueqi Cheng", 
    "publish": "2015-04-20T06:12:06Z", 
    "summary": "With the quick development of online social media such as twitter or sina\nweibo in china, many users usually track hot topics to satisfy their desired\ninformation need. For a hot topic, new opinions or ideas will be continuously\nproduced in the form of online data stream. In this scenario, how to\neffectively filter and display information for a certain topic dynamically,\nwill be a critical problem. We call the problem as Topic-focused Dynamic\nInformation Filtering (denoted as TDIF for short) in social media. In this\npaper, we start open discussions on such application problems. We first analyze\nthe properties of the TDIF problem, which usually contains several typical\nrequirements: relevance, diversity, recency and confidence. Recency means that\nusers want to follow the recent opinions or news. Additionally, the confidence\nof information must be taken into consideration. How to balance these factors\nproperly in online data stream is very important and challenging. We propose a\ndynamic preservation strategy on the basis of an existing feature-based utility\nfunction, to solve the TDIF problem. Additionally, we propose new dynamic\ndiversity measures, to get a more reasonable evaluation for such application\nproblems. Extensive exploratory experiments have been conducted on TREC public\ntwitter dataset, and the experimental results validate the effectiveness of our\napproach."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-16354-3_18", 
    "link": "http://arxiv.org/pdf/1504.06961v1", 
    "title": "WHOSE - A Tool for Whole-Session Analysis in IIR", 
    "arxiv-id": "1504.06961v1", 
    "author": "Dagmar Kern", 
    "publish": "2015-04-27T08:06:21Z", 
    "summary": "One of the main challenges in Interactive Information Retrieval (IIR)\nevaluation is the development and application of re-usable tools that allow\nresearchers to analyze search behavior of real users in different environments\nand different domains, but with comparable results. Furthermore, IIR recently\nfocuses more on the analysis of whole sessions, which includes all user\ninteractions that are carried out within a session but also across several\nsessions by the same user. Some frameworks have already been proposed for the\nevaluation of controlled experiments in IIR, but yet no framework is available\nfor interactive evaluation of search behavior from real-world information\nretrieval (IR) systems with real users. In this paper we present a framework\nfor whole-session evaluation that can also utilize these uncontrolled data\nsets. The logging component can easily be integrated into real-world IR systems\nfor generating and analyzing new log data. Furthermore, due to a supplementary\nmapping it is also possible to analyze existing log data. For every IR system\ndifferent actions and filters can be defined. This allows system operators and\nresearchers to use the framework for the analysis of user search behavior in\ntheir IR systems and to compare it with others. Using a graphical user\ninterface they have the possibility to interactively explore the data set from\na broad overview down to individual sessions."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2014.09.001", 
    "link": "http://arxiv.org/pdf/1504.07350v1", 
    "title": "Geo-Temporal Distribution of Tag Terms for Event-Related Image Retrieval", 
    "arxiv-id": "1504.07350v1", 
    "author": "Heri Ramampiaro", 
    "publish": "2015-04-28T05:38:34Z", 
    "summary": "Media sharing applications, such as Flickr and Panoramio, contain a large\namount of pictures related to real life events. For this reason, the\ndevelopment of effective methods to retrieve these pictures is important, but\nstill a challenging task. Recognizing this importance, and to improve the\nretrieval effectiveness of tag-based event retrieval systems, we propose a new\nmethod to extract a set of geographical tag features from raw geo-spatial\nprofiles of user tags. The main idea is to use these features to select the\nbest expansion terms in a machine learning-based query expansion approach.\nSpecifically, we apply rigorous statistical exploratory analysis of spatial\npoint patterns to extract the geo-spatial features. We use the features both to\nsummarize the spatial characteristics of the spatial distribution of a single\nterm, and to determine the similarity between the spatial profiles of two terms\n-- i.e., term-to-term spatial similarity. To further improve our approach, we\ninvestigate the effect of combining our geo-spatial features with temporal\nfeatures on choosing the expansion terms. To evaluate our method, we perform\nseveral experiments, including well-known feature analyses. Such analyses show\nhow much our proposed geo-spatial features contribute to improve the overall\nretrieval performance. The results from our experiments demonstrate the\neffectiveness and viability of our method."
},{
    "category": "cs.IR", 
    "doi": "10.13140/2.1.4381.5367", 
    "link": "http://arxiv.org/pdf/1504.08175v1", 
    "title": "Evaluation of recommender systems in streaming environments", 
    "arxiv-id": "1504.08175v1", 
    "author": "Jo\u00e3o Gama", 
    "publish": "2015-04-30T11:41:49Z", 
    "summary": "Evaluation of recommender systems is typically done with finite datasets.\nThis means that conventional evaluation methodologies are only applicable in\noffline experiments, where data and models are stationary. However, in real\nworld systems, user feedback is continuously generated, at unpredictable rates.\nGiven this setting, one important issue is how to evaluate algorithms in such a\nstreaming data environment. In this paper we propose a prequential evaluation\nprotocol for recommender systems, suitable for streaming data environments, but\nalso applicable in stationary settings. Using this protocol we are able to\nmonitor the evolution of algorithms' accuracy over time. Furthermore, we are\nable to perform reliable comparative assessments of algorithms by computing\nsignificance tests over a sliding window. We argue that besides being suitable\nfor streaming data, prequential evaluation allows the detection of phenomena\nthat would otherwise remain unnoticed in the evaluation of both offline and\nonline recommender systems."
},{
    "category": "cs.IR", 
    "doi": "10.13140/2.1.4381.5367", 
    "link": "http://arxiv.org/pdf/1505.00168v1", 
    "title": "Comparison Clustering using Cosine and Fuzzy set based Similarity   Measures of Text Documents", 
    "arxiv-id": "1505.00168v1", 
    "author": "Nayan Jyoti Kalita", 
    "publish": "2015-05-01T12:21:24Z", 
    "summary": "Keeping in consideration the high demand for clustering, this paper focuses\non understanding and implementing K-means clustering using two different\nsimilarity measures. We have tried to cluster the documents using two different\nmeasures rather than clustering it with Euclidean distance. Also a comparison\nis drawn based on accuracy of clustering between fuzzy and cosine similarity\nmeasure. The start time and end time parameters for formation of clusters are\nused in deciding optimum similarity measure."
},{
    "category": "cs.IR", 
    "doi": "10.13140/2.1.4381.5367", 
    "link": "http://arxiv.org/pdf/1505.00862v1", 
    "title": "Classifying and Ranking Microblogging Hashtags with News Categories", 
    "arxiv-id": "1505.00862v1", 
    "author": "Yao Meng", 
    "publish": "2015-05-05T02:02:23Z", 
    "summary": "In microblogging, hashtags are used to be topical markers, and they are\nadopted by users that contribute similar content or express a related idea.\nHowever, hashtags are created in a free style and there is no domain category\ninformation about them, which make users hard to get access to organized\nhashtag presentation. In this paper, we propose an approach that classifies\nhashtags with news categories, and then carry out a domain-sensitive popularity\nranking to get hot hashtags in each domain. The proposed approach first trains\na domain classification model with news content and news category information,\nthen detects microblogs related to a hashtag to be its representative text,\nbased on which we can classify this hashtag with a domain. Finally, we\ncalculate the domain-sensitive popularity of each hashtag with multiple\nfactors, to get most hotly discussed hashtags in each domain. Preliminary\nexperimental results on a dataset from Sina Weibo, one of the largest Chinese\nmicroblogging websites, show usefulness of the proposed approach on describing\nhashtags."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.01306v1", 
    "title": "Understanding Graph Structure of Wikipedia for Query Expansion", 
    "arxiv-id": "1505.01306v1", 
    "author": "Arnau Prat-P\u00e9rez", 
    "publish": "2015-05-06T10:14:06Z", 
    "summary": "Knowledge bases are very good sources for knowledge extraction, the ability\nto create knowledge from structured and unstructured sources and use it to\nimprove automatic processes as query expansion. However, extracting knowledge\nfrom unstructured sources is still an open challenge. In this respect,\nunderstanding the structure of knowledge bases can provide significant benefits\nfor the effectiveness of such purpose. In particular, Wikipedia has become a\nvery popular knowledge base in the last years because it is a general\nencyclopedia that has a large amount of information and thus, covers a large\namount of different topics. In this piece of work, we analyze how articles and\ncategories of Wikipedia relate to each other and how these relationships can\nsupport a query expansion technique. In particular, we show that the structures\nin the form of dense cycles with a minimum amount of categories tend to\nidentify the most relevant information."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.01606v1", 
    "title": "A comparative study of approaches in user-centered health information   retrieval", 
    "arxiv-id": "1505.01606v1", 
    "author": "Prasenjit Majumder", 
    "publish": "2015-05-07T07:32:33Z", 
    "summary": "In this paper, we survey various user-centered or context-based biomedical\nhealth information retrieval systems. We present and discuss the performance of\nsystems submitted in CLEF eHealth 2014 Task 3 for this purpose. We classify and\nfocus on comparing the two most prevalent retrieval models in biomedical\ninformation retrieval namely: Language Model (LM) and Vector Space Model (VSM).\nWe also report on the effectiveness of using external medical resources and\nontologies like MeSH, Metamap, UMLS, etc. We observed that the L.M. based\nretrieval systems outperform VSM based systems on various fronts. From the\nresults we conclude that the state-of-art system scores for MAP was 0.4146,\nP@10 was 0.7560 and NDCG@10 was 0.7445, respectively. All of these score were\nreported by systems built on language modelling approaches."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.02798v1", 
    "title": "Math Search for the Masses: Multimodal Search Interfaces and   Appearance-Based Retrieval", 
    "arxiv-id": "1505.02798v1", 
    "author": "Awelemdy Orakwue", 
    "publish": "2015-05-11T20:39:48Z", 
    "summary": "We summarize math search engines and search interfaces produced by the\nDocument and Pattern Recognition Lab in recent years, and in particular the min\nmath search interface and the Tangent search engine. Source code for both\nsystems are publicly available. \"The Masses\" refers to our emphasis on creating\nsystems for mathematical non-experts, who may be looking to define unfamiliar\nnotation, or browse documents based on the visual appearance of formulae rather\nthan their mathematical semantics."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.03014v1", 
    "title": "Frappe: Understanding the Usage and Perception of Mobile App   Recommendations In-The-Wild", 
    "arxiv-id": "1505.03014v1", 
    "author": "Nuria Oliver", 
    "publish": "2015-05-12T14:11:58Z", 
    "summary": "This paper describes a real world deployment of a context-aware mobile app\nrecommender system (RS) called Frappe. Utilizing a hybrid-approach, we\nconducted a large-scale app market deployment with 1000 Android users combined\nwith a small-scale local user study involving 33 users. The resulting usage\nlogs and subjective feedback enabled us to gather key insights into (1)\ncontext-dependent app usage and (2) the perceptions and experiences of\nend-users while interacting with context-aware mobile app recommendations.\nWhile Frappe performs very well based on usage-centric evaluation metrics\ninsights from the small-scale study reveal some negative user experiences. Our\nresults point to a number of actionable lessons learned specifically related to\ndesigning, deploying and evaluating mobile context-aware RS in-the-wild with\nreal users."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.03090v1", 
    "title": "Efficient Similarity Indexing and Searching in High Dimensions", 
    "arxiv-id": "1505.03090v1", 
    "author": "Yu Zhong", 
    "publish": "2015-05-12T17:17:28Z", 
    "summary": "Efficient indexing and searching of high dimensional data has been an area of\nactive research due to the growing exploitation of high dimensional data and\nthe vulnerability of traditional search methods to the curse of dimensionality.\nThis paper presents a new approach for fast and effective searching and\nindexing of high dimensional features using random partitions of the feature\nspace. Experiments on both handwritten digits and 3-D shape descriptors have\nshown the proposed algorithm to be highly effective and efficient in indexing\nand searching real data sets of several hundred dimensions. We also compare its\nperformance to that of the state-of-the-art locality sensitive hashing\nalgorithm."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2764947.2764953", 
    "link": "http://arxiv.org/pdf/1505.03934v1", 
    "title": "Textual Spatial Cosine Similarity", 
    "arxiv-id": "1505.03934v1", 
    "author": "Giancarlo Crocetti", 
    "publish": "2015-05-15T01:08:57Z", 
    "summary": "When dealing with document similarity many methods exist today, like cosine\nsimilarity. More complex methods are also available based on the semantic\nanalysis of textual information, which are computationally expensive and rarely\nused in the real time feeding of content as in enterprise-wide search\nenvironments. To address these real-time constraints, we developed a new\nmeasure of document similarity called Textual Spatial Cosine Similarity, which\nis able to detect similitude at the semantic level using word placement\ninformation contained in the document. We will see in this paper that two\ndegenerate cases exist for this model, which coincide with Cosine Similarity on\none side and with a paraphrasing detection model to the other."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.03984v1", 
    "title": "Location Prediction of Social Images via Generative Model", 
    "arxiv-id": "1505.03984v1", 
    "author": "Xueqiang Lv", 
    "publish": "2015-05-15T08:19:24Z", 
    "summary": "The vast amount of geo-tagged social images has attracted great attention in\nresearch of predicting location using the plentiful content of images, such as\nvisual content and textual description. Most of the existing researches use the\ntext-based or vision-based method to predict location. There still exists a\nproblem: how to effectively exploit the correlation between different types of\ncontent as well as their geographical distributions for location prediction. In\nthis paper, we propose to predict image location by learning the latent\nrelation between geographical location and multiple types of image content. In\nparticularly, we propose a geographical topic model GTMI (geographical topic\nmodel of social image) to integrate multiple types of image content as well as\nthe geographical distributions, In GTMI, image topic is modeled on both text\nvocabulary and visual feature. Each region has its own distribution over topics\nand hence has its own language model and vision pattern. The location of a new\nimage is estimated based on the joint probability of image content and\nsimilarity measure on topic distribution between images. Experiment results\ndemonstrate the performance of location prediction based on GTMI."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.05187v1", 
    "title": "Techniques for Deep Query Understanding", 
    "arxiv-id": "1505.05187v1", 
    "author": "Dhaval Patel", 
    "publish": "2015-05-19T21:02:44Z", 
    "summary": "Query Understanding concerns about inferring the precise intent of search by\nthe user with his formulated query, which is challenging because the queries\nare often very short and ambiguous. The report discusses the various kind of\nqueries that can be put to a Search Engine and illustrates the Role of Query\nUnderstanding for return of relevant results. With different advances in\ntechniques for deep understanding of queries as well as documents, the Search\nTechnology has witnessed three major era. A lot of interesting real world\nexamples have been used to illustrate the role of Query Understanding in each\nof them. The Query Understanding Module is responsible to correct the mistakes\ndone by user in the query, to guide him in formulation of query with precise\nintent, and to precisely infer the intent of the user query. The report\ndescribes the complete architecture to handle aforementioned three tasks, and\nthen discusses basic as well as recent advanced techniques for each of the\ncomponent, through appropriate papers from reputed conferences and journals."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.05821v2", 
    "title": "Optimizing the Information Retrieval Trade-off in Data Visualization   Using $\u03b1$-Divergence", 
    "arxiv-id": "1505.05821v2", 
    "author": "Erkki Oja", 
    "publish": "2015-05-21T18:19:28Z", 
    "summary": "Data visualization is one of the major applications of nonlinear\ndimensionality reduction. From the information retrieval perspective, the\nquality of a visualization can be evaluated by considering the extent that the\nneighborhood relation of each data point is maintained while the number of\nunrelated points that are retrieved is minimized. This property can be\nquantified as a trade-off between the mean precision and mean recall of the\nvisualization. While there have been some approaches to formulate the\nvisualization objective directly as a weighted sum of the precision and recall,\nthere is no systematic way to determine the optimal trade-off between these two\nnor a clear interpretation of the optimal value. In this paper, we investigate\nthe properties of $\\alpha$-divergence for information visualization, focusing\nour attention on a particular range of $\\alpha$ values. We show that the\nminimization of the new cost function corresponds to maximizing a geometric\nmean between precision and recall, parameterized by $\\alpha$. Contrary to some\nearlier methods, no hand-tuning is needed, but we can rigorously estimate the\noptimal value of $\\alpha$ for a given input data. For this, we provide a\nstatistical framework using a novel distribution called Exponential Divergence\nwith Augmentation (EDA). By the extensive set of experiments, we show that the\noptimal value of $\\alpha$, obtained by EDA corresponds to the optimal trade-off\nbetween the precision and recall for a given data distribution."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.06792v2", 
    "title": "Seeing the Forest through the Trees: Adaptive Local Exploration of Large   Graphs", 
    "arxiv-id": "1505.06792v2", 
    "author": "Duen Horng Chau", 
    "publish": "2015-05-26T02:47:12Z", 
    "summary": "Visualization is a powerful paradigm for exploratory data analysis.\nVisualizing large graphs, however, often results in a meaningless hairball. In\nthis paper, we propose a different approach that helps the user adaptively\nexplore large million-node graphs from a local perspective. For nodes that the\nuser investigates, we propose to only show the neighbors with the most\nsubjectively interesting neighborhoods. We contribute novel ideas to measure\nthis interestingness in terms of how surprising a neighborhood is given the\nbackground distribution, as well as how well it fits the nodes the user chose\nto explore. We introduce FACETS, a fast and scalable method for visually\nexploring large graphs. By implementing our above ideas, it allows users to\nlook into the forest through its trees. Empirical evaluation shows that our\nmethod works very well in practice, providing rankings of nodes that match\ninterests of users. Moreover, as it scales linearly, FACETS is suited for the\nexploration of very large graphs."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.07900v1", 
    "title": "A Faster Algorithm to Build New Users Similarity List in   Neighbourhood-based Collaborative Filtering", 
    "arxiv-id": "1505.07900v1", 
    "author": "Hong Shen", 
    "publish": "2015-05-29T01:21:00Z", 
    "summary": "Neighbourhood-based Collaborative Filtering (CF) has been applied in the\nindustry for several decades, because of the easy implementation and high\nrecommendation accuracy. As the core of neighbourhood-based CF, the task of\ndynamically maintaining users' similarity list is challenged by cold-start\nproblem and scalability problem. Recently, several methods are presented on\nsolving the two problems. However, these methods applied an $O(n^2)$ algorithm\nto compute the similarity list in a special case, where the new users, with\nenough recommendation data, have the same rating list. To address the problem\nof large computational cost caused by the special case, we design a faster\n($O(\\frac{1}{125}n^2)$) algorithm, TwinSearch Algorithm, to avoid computing and\nsorting the similarity list for the new users repeatedly to save the\ncomputational resources. Both theoretical and experimental results show that\nthe TwinSearch Algorithm achieves better running time than the traditional\nmethod."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1505.08155v1", 
    "title": "Performance Evaluation and Optimization of Math-Similarity Search", 
    "arxiv-id": "1505.08155v1", 
    "author": "Abdou Youssef", 
    "publish": "2015-05-29T19:17:03Z", 
    "summary": "Similarity search in math is to find mathematical expressions that are\nsimilar to a user's query. We conceptualized the similarity factors between\nmathematical expressions, and proposed an approach to math similarity search\n(MSS) by defining metrics based on those similarity factors [11]. Our\npreliminary implementation indicated the advantage of MSS compared to\nnon-similarity based search. In order to more effectively and efficiently\nsearch similar math expressions, MSS is further optimized. This paper focuses\non performance evaluation and optimization of MSS. Our results show that the\nproposed optimization process significantly improved the performance of MSS\nwith respect to both relevance ranking and recall."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2671188.2749308", 
    "link": "http://arxiv.org/pdf/1506.00717v1", 
    "title": "Assessing Efficiency-Effectiveness Tradeoffs in Multi-Stage Retrieval   Systems Without Using Relevance Judgments", 
    "arxiv-id": "1506.00717v1", 
    "author": "Alistair Moffat", 
    "publish": "2015-06-02T01:08:37Z", 
    "summary": "Large-scale retrieval systems are often implemented as a cascading sequence\nof phases -- a first filtering step, in which a large set of candidate\ndocuments are extracted using a simple technique such as Boolean matching\nand/or static document scores; and then one or more ranking steps, in which the\npool of documents retrieved by the filter is scored more precisely using dozens\nor perhaps hundreds of different features. The documents returned to the user\nare then taken from the head of the final ranked list. Here we examine methods\nfor measuring the quality of filtering and preliminary ranking stages, and show\nhow to use these measurements to tune the overall performance of the system.\nStandard top-weighted metrics used for overall system evaluation are not\nappropriate for assessing filtering stages, since the output is a set of\ndocuments, rather than an ordered sequence of documents. Instead, we use an\napproach in which a quality score is computed based on the discrepancy between\nfiltered and full evaluation. Unlike previous approaches, our methods do not\nrequire relevance judgments, and thus can be used with virtually any query set.\nWe show that this quality score directly correlates with actual differences in\nmeasured effectiveness when relevance judgments are available. Since the\nquality score does not require relevance judgments, it can be used to identify\nqueries that perform particularly poorly for a given filter. Using these\nmethods, we explore a wide range of filtering options using thousands of\nqueries, categorize the relative merits of the different approaches, and\nidentify useful parameter combinations."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1506.00904v1", 
    "title": "Where to Go on Your Next Trip? Optimizing Travel Destinations Based on   User Preferences", 
    "arxiv-id": "1506.00904v1", 
    "author": "Djoerd Hiemstra", 
    "publish": "2015-06-02T14:40:37Z", 
    "summary": "Recommendation based on user preferences is a common task for e-commerce\nwebsites. New recommendation algorithms are often evaluated by offline\ncomparison to baseline algorithms such as recommending random or the most\npopular items. Here, we investigate how these algorithms themselves perform and\ncompare to the operational production system in large scale online experiments\nin a real-world application. Specifically, we focus on recommending travel\ndestinations at Booking.com, a major online travel site, to users searching for\ntheir preferred vacation activities. To build ranking models we use\nmulti-criteria rating data provided by previous users after their stay at a\ndestination. We implement three methods and compare them to the current\nbaseline in Booking.com: random, most popular, and Naive Bayes. Our general\nconclusion is that, in an online A/B test with live users, our Naive-Bayes\nbased ranker increased user engagement significantly over the current online\nsystem."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1506.01743v2", 
    "title": "Socially Driven News Recommendation", 
    "arxiv-id": "1506.01743v2", 
    "author": "Magdalini Eirinaki", 
    "publish": "2015-06-04T22:32:40Z", 
    "summary": "The participatory Web has enabled the ubiquitous and pervasive access of\ninformation, accompanied by an increase of speed and reach in information\nsharing. Data dissemination services such as news aggregators are expected to\nprovide up-to-date, real-time information to the end users. News aggregators\nare in essence recommendation systems that filter and rank news stories in\norder to select the few that will appear on the users front screen at any time.\nOne of the main challenges in such systems is to address the recency and\nlatency problems, that is, to identify as soon as possible how important a news\nstory is. In this work we propose an integrated framework that aims at\npredicting the importance of news items upon their publication with a focus on\nrecent and highly popular news, employing resampling strategies, and at\ntranslating the result into concrete news rankings. We perform an extensive\nexperimental evaluation using real-life datasets of the proposed framework as\nboth a stand-alone system and when applied to news recommendations from Google\nNews. Additionally, we propose and evaluate a combinatorial solution to the\naugmentation of official media recommendations with social information. Results\nshow that the proposed approach complements and enhances the news rankings\ngenerated by state-of-the-art systems."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1506.04094v1", 
    "title": "The WDAqua ITN: Answering Questions using Web Data", 
    "arxiv-id": "1506.04094v1", 
    "author": "Soren Auer", 
    "publish": "2015-06-10T10:56:33Z", 
    "summary": "WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded under\nEU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims\nat advancing the state of the art by intertwining training, research and\ninnovation efforts, centered around one service: data-driven question\nanswering. Question answering is immediately useful to a wide audience of end\nusers, and we will demonstrate this in settings including e-commerce, public\nsector information, publishing and smart cities. Question answering also covers\nweb science and data science broadly, leading to transferrable research results\nand to transferrable skills of the researchers who have finished our training\nprogramme. To ensure that our research improves question answering overall,\nevery individual research project connects at least two of these steps.\nIntersectional secondments (within a consortium covering academia, research\ninstitutes and industrial research as well as network-wide workshops, R and D\nchallenges and innovation projects further balance ground-breaking research and\nthe needs of society and industry. Training-wise these offers equip early stage\nresearchers with the expertise and transferable technical and non-technical\nskills that will allow them to pursue a successful career as an academic,\ndecision maker, practitioner or entrepreneur."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1506.05752v3", 
    "title": "Detecting Abnormal Profiles in Collaborative Filtering Recommender   Systems", 
    "arxiv-id": "1506.05752v3", 
    "author": "Zhihai Yang", 
    "publish": "2015-06-18T17:26:14Z", 
    "summary": "Personalization collaborative filtering recommender systems (CFRSs) are the\ncrucial components of popular e-commerce services. In practice, CFRSs are also\nparticularly vulnerable to \"shilling\" attacks or \"profile injection\" attacks\ndue to their openness. The attackers can carefully inject chosen attack\nprofiles into CFRSs in order to bias the recommendation results to their\nbenefits. To reduce this risk, various detection techniques have been proposed\nto detect such attacks, which use diverse features extracted from user\nprofiles. However, relying on limited features to improve the detection\nperformance is difficult seemingly, since the existing features can not fully\ncharacterize the attack profiles and genuine profiles. In this paper, we\npropose a novel detection method to make recommender systems resistant to the\n\"shilling\" attacks or \"profile injection\" attacks. The existing features can be\nbriefly summarized as two aspects including rating behavior based and item\ndistribution based. We firstly formulate the problem as finding a mapping model\nbetween rating behavior and item distribution by exploiting the least-squares\napproximate solution. Based on the trained model, we design a detector by\nemploying a regressor to detect such attacks. Extensive experiments on both the\nMovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness\nof our proposed detection method. Experimental results were included to\nvalidate the outperformance of our approach in comparison with benchmarked\nmethod including KNN."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.02002v1", 
    "title": "Topical Discovery of Web Content", 
    "arxiv-id": "1507.02002v1", 
    "author": "Giancarlo Crocetti", 
    "publish": "2015-07-08T01:06:55Z", 
    "summary": "This work describes the theory and the implementation of a new software tool,\nthe \"Web Topical Discovery System\" (WTDS), which provides an approach to the\nautomatic discovery and selection of new web pages relevant to specific\nanalytical needs. We will see how it is possible to specify the research\ncontext with search keywords related to the area of interest and consider the\nimportant problem of removing extraneous data from a web page containing an\narticle in order to reduce, to a minimum, false positives represented by a\nmatch on a keyword that is showing up on the latest news box of the same page.\nThe removal of duplicates, the analysis of richness of information contained in\nthe article and lexical diversity are all taken into consideration in order to\nprovide the optimum set of recommendations to the end user or system."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.03928v1", 
    "title": "Pseudo-Query Reformulation", 
    "arxiv-id": "1507.03928v1", 
    "author": "Fernando Diaz", 
    "publish": "2015-07-14T17:06:51Z", 
    "summary": "Automatic query reformulation refers to rewriting a user's original query in\norder to improve the ranking of retrieval results compared to the original\nquery. We present a general framework for automatic query reformulation based\non discrete optimization. Our approach, referred to as pseudo-query\nreformulation, treats automatic query reformulation as a search problem over\nthe graph of unweighted queries linked by minimal transformations (e.g. term\nadditions, deletions). This framework allows us to test existing performance\nprediction methods as heuristics for the graph search process. We demonstrate\nthe effectiveness of the approach on several publicly available datasets."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.05214v1", 
    "title": "On the Application of Link Analysis Algorithms for Ranking Bipartite   Graphs", 
    "arxiv-id": "1507.05214v1", 
    "author": "Antonia Korba", 
    "publish": "2015-07-18T18:36:08Z", 
    "summary": "Recently bipartite graphs have been widely used to represent the relationship\ntwo sets of items for information retrieval applications. The Web offers a wide\nrange of data which can be represented by bipartite graphs, such us movies and\nreviewers in recomender systems, queries and URLs in search engines, users and\nposts in social networks. The size and the dynamic nature of such graphs\ngenerate the need for more efficient ranking methods.\n  In this thesis, at first we present the fundamental mathematical backround\nthat we use subsequently and we describe the basic principles of the\nPerron-Frobebius theory for non negative matrices as well as the the basic\nprinciples of the Markov chain theory. Then, we propose a novel algorithm named\nBipartiteRank, which is suitable to rank scenarios, that can be represented as\na bipartite graph. This algorithm is based on the random surfer model and\ninherits the basic mathematical characteristics of PageRank. What makes it\ndifferent, is the fact that it introduces an alternative type of teleportation,\nbased on the block structure of the bipartite graph in order to achieve more\nefficient ranking. Finally, we support this opinion with mathematical arguments\nand then we confirm it experimentally through a series of tests on real data."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.06235v1", 
    "title": "The Tangent Search Engine: Improved Similarity Metrics and Scalability   for Math Formula Search", 
    "arxiv-id": "1507.06235v1", 
    "author": "Frank Tompa", 
    "publish": "2015-07-22T16:02:00Z", 
    "summary": "With the ever-increasing quantity and variety of data worldwide, the Web has\nbecome a rich repository of mathematical formulae. This necessitates the\ncreation of robust and scalable systems for Mathematical Information Retrieval,\nwhere users search for mathematical information using individual formulae\n(query-by-expression) or a combination of keywords and formulae. Often, the\npages that best satisfy users' information needs contain expressions that only\napproximately match the query formulae. For users trying to locate or re-find a\nspecific expression, browse for similar formulae, or who are mathematical\nnon-experts, the similarity of formulae depends more on the relative positions\nof symbols than on deep mathematical semantics.\n  We propose the Maximum Subtree Similarity (MSS) metric for\nquery-by-expression that produces intuitive rankings of formulae based on their\nappearance, as represented by the types and relative positions of symbols.\nBecause it is too expensive to apply the metric against all formulae in large\ncollections, we first retrieve expressions using an inverted index over tuples\nthat encode relationships between pairs of symbols, ranking hits using the Dice\ncoefficient. The top-k formulae are then re-ranked using MSS. Our approach\nobtains state-of-the-art performance on the NTCIR-11 Wikipedia formula\nretrieval benchmark and is efficient in terms of both index space and overall\nretrieval time. Retrieval systems for other graphical forms, including chemical\ndiagrams, flowcharts, figures, and tables, may also benefit from adopting our\napproach."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.07382v1", 
    "title": "Application of Kullback-Leibler divergence for short-term user interest   detection", 
    "arxiv-id": "1507.07382v1", 
    "author": "Artem Noskov", 
    "publish": "2015-07-27T12:05:58Z", 
    "summary": "Classical approaches in recommender systems such as collaborative filtering\nare concentrated mainly on static user preference extraction. This approach\nworks well as an example for music recommendations when a user behavior tends\nto be stable over long period of time, however the most common situation in\ne-commerce is different which requires reactive algorithms based on a\nshort-term user activity analysis. This paper introduces a small mathematical\nframework for short-term user interest detection formulated in terms of item\nproperties and its application for recommender systems enhancing. The framework\nis based on the fundamental concept of information theory --- Kullback-Leibler\ndivergence."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.08198v1", 
    "title": "Non-Compositional Term Dependence for Information Retrieval", 
    "arxiv-id": "1507.08198v1", 
    "author": "Niels Dalum Hansen", 
    "publish": "2015-07-29T16:08:48Z", 
    "summary": "Modelling term dependence in IR aims to identify co-occurring terms that are\ntoo heavily dependent on each other to be treated as a bag of words, and to\nadapt the indexing and ranking accordingly. Dependent terms are predominantly\nidentified using lexical frequency statistics, assuming that (a) if terms\nco-occur often enough in some corpus, they are semantically dependent; (b) the\nmore often they co-occur, the more semantically dependent they are. This\nassumption is not always correct: the frequency of co-occurring terms can be\nseparate from the strength of their semantic dependence. E.g. \"red tape\" might\nbe overall less frequent than \"tape measure\" in some corpus, but this does not\nmean that \"red\"+\"tape\" are less dependent than \"tape\"+\"measure\". This is\nespecially the case for non-compositional phrases, i.e. phrases whose meaning\ncannot be composed from the individual meanings of their terms (such as the\nphrase \"red tape\" meaning bureaucracy). Motivated by this lack of distinction\nbetween the frequency and strength of term dependence in IR, we present a\nprincipled approach for handling term dependence in queries, using both lexical\nfrequency and semantic evidence. We focus on non-compositional phrases,\nextending a recent unsupervised model for their detection [21] to IR. Our\napproach, integrated into ranking using Markov Random Fields [31], yields\neffectiveness gains over competitive TREC baselines, showing that there is\nstill room for improvement in the very well-studied area of term dependence in\nIR."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.08234v1", 
    "title": "Entropy and Graph Based Modelling of Document Coherence using Discourse   Entities: An Application", 
    "arxiv-id": "1507.08234v1", 
    "author": "Birger Larsen", 
    "publish": "2015-07-29T17:40:19Z", 
    "summary": "We present two novel models of document coherence and their application to\ninformation retrieval (IR). Both models approximate document coherence using\ndiscourse entities, e.g. the subject or object of a sentence. Our first model\nviews text as a Markov process generating sequences of discourse entities\n(entity n-grams); we use the entropy of these entity n-grams to approximate the\nrate at which new information appears in text, reasoning that as more new words\nappear, the topic increasingly drifts and text coherence decreases. Our second\nmodel extends the work of Guinaudeau & Strube [28] that represents text as a\ngraph of discourse entities, linked by different relations, such as their\ndistance or adjacency in text. We use several graph topology metrics to\napproximate different aspects of the discourse flow that can indicate\ncoherence, such as the average clustering or betweenness of discourse entities\nin text. Experiments with several instantiations of these models show that: (i)\nour models perform on a par with two other well-known models of text coherence\neven without any parameter tuning, and (ii) reranking retrieval results\naccording to their coherence scores gives notable performance gains, confirming\na relation between document coherence and relevance. This work contributes two\nnovel models of document coherence, the application of which to IR complements\nrecent work in the integration of document cohesiveness or comprehensibility to\nranking [5, 56]."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2766462.2776777", 
    "link": "http://arxiv.org/pdf/1507.08439v1", 
    "title": "Metadata Embeddings for User and Item Cold-start Recommendations", 
    "arxiv-id": "1507.08439v1", 
    "author": "Maciej Kula", 
    "publish": "2015-07-30T10:08:14Z", 
    "summary": "I present a hybrid matrix factorisation model representing users and items as\nlinear combinations of their content features' latent factors. The model\noutperforms both collaborative and content-based models in cold-start or sparse\ninteraction data scenarios (using both user and item metadata), and performs at\nleast as well as a pure collaborative matrix factorisation model where\ninteraction data is abundant. Additionally, feature embeddings produced by the\nmodel encode semantic information in a way reminiscent of word embedding\napproaches, making them useful for a range of related tasks such as tag\nrecommendations."
},{
    "category": "cs.IR", 
    "doi": "10.2298/CSIS160229042W", 
    "link": "http://arxiv.org/pdf/1507.08586v3", 
    "title": "Generalized Ensemble Model for Document Ranking in Information Retrieval", 
    "arxiv-id": "1507.08586v3", 
    "author": "Hongfang Liu", 
    "publish": "2015-07-30T17:09:28Z", 
    "summary": "A generalized ensemble model (gEnM) for document ranking is proposed in this\npaper. The gEnM linearly combines basis document retrieval models and tries to\nretrieve relevant documents at high positions. In order to obtain the optimal\nlinear combination of multiple document retrieval models or rankers, an\noptimization program is formulated by directly maximizing the mean average\nprecision. Both supervised and unsupervised learning algorithms are presented\nto solve this program. For the supervised scheme, two approaches are considered\nbased on the data setting, namely batch and online setting. In the batch\nsetting, we propose a revised Newton's algorithm, gEnM.BAT, by approximating\nthe derivative and Hessian matrix. In the online setting, we advocate a\nstochastic gradient descent (SGD) based algorithm---gEnM.ON. As for the\nunsupervised scheme, an unsupervised ensemble model (UnsEnM) by iteratively\nco-learning from each constituent ranker is presented. Experimental study on\nbenchmark data sets verifies the effectiveness of the proposed algorithms.\nTherefore, with appropriate algorithms, the gEnM is a viable option in diverse\npractical information retrieval applications."
},{
    "category": "cs.IR", 
    "doi": "10.2298/CSIS160229042W", 
    "link": "http://arxiv.org/pdf/1508.00027v2", 
    "title": "Analysis of Financial News with NewsStream", 
    "arxiv-id": "1508.00027v2", 
    "author": "Igor Mozetic", 
    "publish": "2015-07-31T20:44:04Z", 
    "summary": "Unstructured data, such as news and blogs, can provide valuable insights into\nthe financial world. We present the NewsStream portal, an intuitive and\neasy-to-use tool for news analytics, which supports interactive querying and\nvisualizations of the documents at different levels of detail. It relies on a\nscalable architecture for real-time processing of a continuous stream of\ntextual data, which incorporates data acquisition, cleaning, natural-language\npreprocessing and semantic annotation components. It has been running for over\ntwo years and collected over 18 million news articles and blog posts. The\nNewsStream portal can be used to answer the questions when, how often, in what\ncontext, and with what sentiment was a financial entity or term mentioned in a\ncontinuous stream of news and blogs, and therefore providing a complement to\nnews aggregators. We illustrate some features of our system in three use cases:\nrelations between the rating agencies and the PIIGS countries, reflection of\nfinancial news on credit default swap (CDS) prices, the emergence of the\nBitcoin digital currency, and visualizing how the world is connected through\nnews."
},{
    "category": "cs.IR", 
    "doi": "10.2298/CSIS160229042W", 
    "link": "http://arxiv.org/pdf/1508.01177v1", 
    "title": "The Continuous Cold Start Problem in e-Commerce Recommender Systems", 
    "arxiv-id": "1508.01177v1", 
    "author": "Melanie JI M\u00fcller", 
    "publish": "2015-08-05T19:03:49Z", 
    "summary": "Many e-commerce websites use recommender systems to recommend items to users.\nWhen a user or item is new, the system may fail because not enough information\nis available on this user or item. Various solutions to this `cold-start\nproblem' have been proposed in the literature. However, many real-life\ne-commerce applications suffer from an aggravated, recurring version of\ncold-start even for known users or items, since many users visit the website\nrarely, change their interests over time, or exhibit different personas. This\npaper exposes the `Continuous Cold Start' (CoCoS) problem and its consequences\nfor content- and context-based recommendation from the viewpoint of typical\ne-commerce applications, illustrated with examples from a major travel\nrecommendation website, Booking.com."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijfcst.2015.5402", 
    "link": "http://arxiv.org/pdf/1508.01696v1", 
    "title": "A Location-Based Movie Recommender System Using Collaborative Filtering", 
    "arxiv-id": "1508.01696v1", 
    "author": "Kasra Madadipouya", 
    "publish": "2015-08-07T14:03:41Z", 
    "summary": "Available recommender systems mostly provide recommendations based on the\nusers preferences by utilizing traditional methods such as collaborative\nfiltering which only relies on the similarities between users and items.\nHowever, collaborative filtering might lead to provide poor recommendation\nbecause it does not rely on other useful available data such as users locations\nand hence the accuracy of the recommendations could be very low and\ninefficient. This could be very obvious in the systems that locations would\naffect users preferences highly such as movie recommender systems. In this\npaper a new location-based movie recommender system based on the collaborative\nfiltering is introduced for enhancing the accuracy and the quality of\nrecommendations. In this approach, users locations have been utilized and take\nin consideration in the entire processing of the recommendations and peer\nselections. The potential of the proposed approach in providing novel and\nbetter quality recommendations have been discussed through experiments in real\ndatasets."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810355.2810359", 
    "link": "http://arxiv.org/pdf/1508.01929v1", 
    "title": "Combining Text and Formula Queries in Math Information Retrieval:   Evaluation of Query Results Merging Strategies", 
    "arxiv-id": "1508.01929v1", 
    "author": "Michal R\u016f\u017ei\u010dka", 
    "publish": "2015-08-08T17:18:40Z", 
    "summary": "Specific to Math Information Retrieval is combining text with mathematical\nformulae both in documents and in queries. Rigorous evaluation of query\nexpansion and merging strategies combining math and standard textual keyword\nterms in a query are given. It is shown that techniques similar to those known\nfrom textual query processing may be applied in math information retrieval as\nwell, and lead to a cutting edge performance. Striping and merging partial\nresults from subqueries is one technique that improves results measured by\ninformation retrieval evaluation metrics like Bpref."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V26P204", 
    "link": "http://arxiv.org/pdf/1508.02127v1", 
    "title": "A novel design of hidden web crawler using ontology", 
    "arxiv-id": "1508.02127v1", 
    "author": "Ashutosh Dixit", 
    "publish": "2015-08-10T04:56:08Z", 
    "summary": "Deep Web is content hidden behind HTML forms. Since it represents a large\nportion of the structured, unstructured and dynamic data on the Web, accessing\nDeep-Web content has been a long challenge for the database community. This\npaper describes a crawler for accessing Deep-Web using Ontologies. Performance\nevaluation of the proposed work showed that this new approach has promising\nresults."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V26P204", 
    "link": "http://arxiv.org/pdf/1508.02552v1", 
    "title": "Web Search Result Clustering based on Heuristic Search and k-means", 
    "arxiv-id": "1508.02552v1", 
    "author": "Kishwar Sadaf", 
    "publish": "2015-08-11T11:00:54Z", 
    "summary": "Giving user a simple and well organized web search result has been a topic of\nactive information Retrieval (IR) research. Irrespective of how small or\nambiguous a query is, a user always wants the desired result on the first\ndisplay of an IR system. Clustering of an IR system result can render a way,\nwhich fulfills the actual information need of a user. In this paper, an\napproach to cluster an IR system result is presented.The approach is a\ncombination of heuristics and k-means technique using cosine similarity. Our\nheuristic approach detects the initial value of k for creating initial\ncentroids. This eliminates the problem of external specification of the value\nk, which may lead to unwanted result if wrongly specified. The centroids\ncreated in this way are more specific and meaningful in the context of web\nsearch result. Another advantage of the proposed method is the removal of the\nobjective means function of k-means which makes cluster sizes same. The end\nresult of the proposed approach consists of different clusters of documents\nhaving different sizes."
},{
    "category": "cs.IR", 
    "doi": "10.14445/22315381/IJETT-V26P204", 
    "link": "http://arxiv.org/pdf/1508.03298v1", 
    "title": "Enabling Complex Wikipedia Queries - Technical Report", 
    "arxiv-id": "1508.03298v1", 
    "author": "Bracha Shapira", 
    "publish": "2015-08-13T18:35:06Z", 
    "summary": "In this technical report we present a database schema used to store Wikipedia\nso it can be easily used in query-intensive applications. In addition to\nstoring the information in a way that makes it highly accessible, our schema\nenables users to easily formulate complex queries using information such as the\nanchor-text of links and their location in the page, the titles and number of\nredirect pages for each page and the paragraph structure of entity pages. We\nhave successfully used the schema in domains such as recommender systems,\ninformation retrieval and sentiment analysis. In order to assist other\nresearchers, we now make the schema and its content available online."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.188.6", 
    "link": "http://arxiv.org/pdf/1508.03902v1", 
    "title": "Domain-specific queries and Web search personalization: some   investigations", 
    "arxiv-id": "1508.03902v1", 
    "author": "Rocco De Nicola", 
    "publish": "2015-08-17T01:51:11Z", 
    "summary": "Major search engines deploy personalized Web results to enhance users'\nexperience, by showing them data supposed to be relevant to their interests.\nEven if this process may bring benefits to users while browsing, it also raises\nconcerns on the selection of the search results. In particular, users may be\nunknowingly trapped by search engines in protective information bubbles, called\n\"filter bubbles\", which can have the undesired effect of separating users from\ninformation that does not fit their preferences. This paper moves from early\nresults on quantification of personalization over Google search query results.\nInspired by previous works, we have carried out some experiments consisting of\nsearch queries performed by a battery of Google accounts with differently\nprepared profiles. Matching query results, we quantify the level of\npersonalization, according to topics of the queries and the profile of the\naccounts. This work reports initial results and it is a first step a for more\nextensive investigation to measure Web search personalization."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.188.6", 
    "link": "http://arxiv.org/pdf/1509.01649v1", 
    "title": "Using of Neuro-Indexes", 
    "arxiv-id": "1509.01649v1", 
    "author": "Valerii Garnaga", 
    "publish": "2015-09-05T00:52:43Z", 
    "summary": "The article describes a new data structure called neuro-index. It is an\nalternative to well-known file indexes. The neuro-index is fundamentally\ndifferent because it stores weight coefficients in neural network. It is not a\nreference type like \"keyword-position in a file\"."
},{
    "category": "cs.IR", 
    "doi": "10.4204/EPTCS.188.6", 
    "link": "http://arxiv.org/pdf/1509.02010v2", 
    "title": "LocLinkVis: A Geographic Information Retrieval-Based System for   Large-Scale Exploratory Search", 
    "arxiv-id": "1509.02010v2", 
    "author": "Rosa Merino Claros", 
    "publish": "2015-09-07T12:36:19Z", 
    "summary": "In this paper we present LocLinkVis (Locate-Link-Visualize); a system which\nsupports exploratory information access to a document collection based on\ngeo-referencing and visualization. It uses a gazetteer which contains\nrepresentations of places ranging from countries to buildings, and that is used\nto recognize toponyms, disambiguate them into places, and to visualize the\nresulting spatial footprints."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-18818-8_33", 
    "link": "http://arxiv.org/pdf/1509.04525v1", 
    "title": "Ranking Entities in the Age of Two Webs, an Application to Semantic   Snippets", 
    "arxiv-id": "1509.04525v1", 
    "author": "Harald Kosch", 
    "publish": "2015-09-15T12:45:41Z", 
    "summary": "The advances of the Linked Open Data (LOD) initiative are giving rise to a\nmore structured Web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)\nconnecting many other datasets. They also made possible new Web services for\nentity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for\nnew applications that can benefit from a combination of the Web of documents\nand the Web of data. To ease the emergence of these new applications, we\npropose a query-biased algorithm (LDRANK) for the ranking of web of data\nresources with associated textual data. Our algorithm combines link analysis\nwith dimensionality reduction. We use crowdsourcing for building a publicly\navailable and reusable dataset for the evaluation of query-biased ranking of\nWeb of data resources detected in Web pages. We show that, on this dataset,\nLDRANK outperforms the state of the art. Finally, we use this algorithm for the\nconstruction of semantic snippets of which we evaluate the usefulness with a\ncrowdsourcing-based approach."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-18818-8_33", 
    "link": "http://arxiv.org/pdf/1509.05567v1", 
    "title": "Exploring Query Categorisation for Query Expansion: A Study", 
    "arxiv-id": "1509.05567v1", 
    "author": "Samar Bhattacharya", 
    "publish": "2015-09-18T10:04:09Z", 
    "summary": "The vocabulary mismatch problem is one of the important challenges facing\ntraditional keyword-based Information Retrieval Systems. The aim of query\nexpansion (QE) is to reduce this query-document mismatch by adding related or\nsynonymous words or phrases to the query.\n  Several existing query expansion algorithms have proved their merit, but they\nare not uniformly beneficial for all kinds of queries. Our long-term goal is to\nformulate methods for applying QE techniques tailored to individual queries,\nrather than applying the same general QE method to all queries. As an initial\nstep, we have proposed a taxonomy of query classes (from a QE perspective) in\nthis report. We have discussed the properties of each query class with\nexamples. We have also discussed some QE strategies that might be effective for\neach query category.\n  In future work, we intend to test the proposed techniques using standard\ndatasets, and to explore automatic query categorisation methods."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-18818-8_33", 
    "link": "http://arxiv.org/pdf/1509.06553v2", 
    "title": "Diverse Yet Efficient Retrieval using Hash Functions", 
    "arxiv-id": "1509.06553v2", 
    "author": "C. V Jawahar", 
    "publish": "2015-09-22T11:38:20Z", 
    "summary": "Typical retrieval systems have three requirements: a) Accurate retrieval\ni.e., the method should have high precision, b) Diverse retrieval, i.e., the\nobtained set of points should be diverse, c) Retrieval time should be small.\nHowever, most of the existing methods address only one or two of the above\nmentioned requirements. In this work, we present a method based on randomized\nlocality sensitive hashing which tries to address all of the above requirements\nsimultaneously. While earlier hashing approaches considered approximate\nretrieval to be acceptable only for the sake of efficiency, we argue that one\ncan further exploit approximate retrieval to provide impressive trade-offs\nbetween accuracy and diversity. We extend our method to the problem of\nmulti-label prediction, where the goal is to output a diverse and accurate set\nof labels for a given document in real-time. Moreover, we introduce a new\nnotion to simultaneously evaluate a method's performance for both the precision\nand diversity measures. Finally, we present empirical results on several\ndifferent retrieval tasks and show that our method retrieves diverse and\naccurate images/labels while ensuring $100x$-speed-up over the existing diverse\nretrieval approaches."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-18818-8_33", 
    "link": "http://arxiv.org/pdf/1509.06847v1", 
    "title": "Design and Implementation of Domain based Semantic Hidden Web Crawler", 
    "arxiv-id": "1509.06847v1", 
    "author": "Ashutosh Dixit", 
    "publish": "2015-09-23T05:26:44Z", 
    "summary": "Web is a wide term which mainly consists of surface web and hidden web. One\ncan easily access the surface web using traditional web crawlers, but they are\nnot able to crawl the hidden portion of the web. These traditional crawlers\nretrieve contents from web pages, which are linked by hyperlinks ignoring the\ninformation hidden behind form pages, which cannot be extracted using simple\nhyperlink structure. Thus, they ignore large amount of data hidden behind\nsearch forms. This paper emphasizes on the extraction of hidden data behind\nhtml search forms. The proposed technique makes use of semantic mapping to fill\nthe html search form using domain specific database. Using semantics to fill\nvarious fields of a form leads to more accurate and qualitative data\nextraction."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-18818-8_33", 
    "link": "http://arxiv.org/pdf/1510.00585v1", 
    "title": "A Complex Network Approach for Collaborative Recommendation", 
    "arxiv-id": "1510.00585v1", 
    "author": "Bibhas Adhikari", 
    "publish": "2015-10-02T13:05:42Z", 
    "summary": "Collaborative filtering (CF) is the most widely used and successful approach\nfor personalized service recommendations. Among the collaborative\nrecommendation approaches, neighborhood based approaches enjoy a huge amount of\npopularity, due to their simplicity, justifiability, efficiency and stability.\nNeighborhood based collaborative filtering approach finds K nearest neighbors\nto an active user or K most similar rated items to the target item for\nrecommendation. Traditional similarity measures use ratings of co-rated items\nto find similarity between a pair of users. Therefore, traditional similarity\nmeasures cannot compute effective neighbors in sparse dataset. In this paper,\nwe propose a two-phase approach, which generates user-user and item-item\nnetworks using traditional similarity measures in the first phase. In the\nsecond phase, two hybrid approaches HB1, HB2, which utilize structural\nsimilarity of both the network for finding K nearest neighbors and K most\nsimilar items to a target items are introduced. To show effectiveness of the\nmeasures, we compared performances of neighborhood based CFs using\nstate-of-the-art similarity measures with our proposed structural similarity\nmeasures based CFs. Recommendation results on a set of real data show that\nproposed measures based CFs outperform existing measures based CFs in various\nevaluation metrics."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1510.01991v1", 
    "title": "HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest   Neighbor Search", 
    "arxiv-id": "1510.01991v1", 
    "author": "Steven C. H. Hoi", 
    "publish": "2015-10-07T15:39:39Z", 
    "summary": "Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale\ndata processing and analytics, particularly for analyzing multimedia contents\nwhich are often of high dimensionality. Instead of using exact NN search,\nextensive research efforts have been focusing on approximate NN search\nalgorithms. In this work, we present \"HDIdx\", an efficient high-dimensional\nindexing library for fast approximate NN search, which is open-source and\nwritten in Python. It offers a family of state-of-the-art algorithms that\nconvert input high-dimensional vectors into compact binary codes, making them\nvery efficient and scalable for NN search with very low space complexity."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1510.03025v1", 
    "title": "Mining Interesting Trivia for Entities from Wikipedia", 
    "arxiv-id": "1510.03025v1", 
    "author": "Abhay Prakash", 
    "publish": "2015-10-11T09:05:23Z", 
    "summary": "Trivia is any fact about an entity, which is interesting due to any of the\nfollowing characteristics - unusualness, uniqueness, unexpectedness or\nweirdness. Such interesting facts are provided in 'Did You Know?' section at\nmany places. Although trivia are facts of little importance to be known, but we\nhave presented their usage in user engagement purpose. Such fun facts generally\nspark intrigue and draws user to engage more with the entity, thereby promoting\nrepeated engagement. The thesis has cited some case studies, which show the\nsignificant impact of using trivia for increasing user engagement or for wide\npublicity of the product/service.\n  In this thesis, we propose a novel approach for mining entity trivia from\ntheir Wikipedia pages. Given an entity, our system extracts relevant sentences\nfrom its Wikipedia page and produces a list of sentences ranked based on their\ninterestingness as trivia. At the heart of our system lies an interestingness\nranker which learns the notion of interestingness, through a rich set of\ndomain-independent linguistic and entity based features. Our ranking model is\ntrained by leveraging existing user-generated trivia data available on the Web\ninstead of creating new labeled data for movie domain. For other domains like\nsports, celebrities, countries etc. labeled data would have to be created as\ndescribed in thesis. We evaluated our system on movies domain and celebrity\ndomain, and observed that the system performs significantly better than the\ndefined baselines. A thorough qualitative analysis of the results revealed that\nour engineered rich set of features indeed help in surfacing interesting trivia\nin the top ranks."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1510.03299v2", 
    "title": "Further Theoretical Study of Distribution Separation Method for   Information Retrieval", 
    "arxiv-id": "1510.03299v2", 
    "author": "Bin Hu", 
    "publish": "2015-10-12T14:27:18Z", 
    "summary": "Recently, a Distribution Separation Method (DSM) is proposed for relevant\nfeedback in information retrieval, which aims to approximate the true relevance\ndistribution by separating a seed irrelevance distribution from the mixture\none. While DSM achieved a promising empirical performance, theoretical analysis\nof DSM is still need further study and comparison with other relative retrieval\nmodel. In this article, we first generalize DSM's theoretical property, by\nproving that its minimum correlation assumption is equivalent to the maximum\n(original and symmetrized) KL-Divergence assumption. Second, we also\nanalytically show that the EM algorithm in a well-known Mixture Model is\nessentially a distribution separation process and can be simplified using the\nlinear separation algorithm in DSM. Some empirical results are also presented\nto support our theoretical analysis."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1510.07197v1", 
    "title": "Comparative Document Analysis for Large Text Corpora", 
    "arxiv-id": "1510.07197v1", 
    "author": "Jiawei Han", 
    "publish": "2015-10-25T01:22:05Z", 
    "summary": "This paper presents a novel research problem on joint discovery of\ncommonalities and differences between two individual documents (or document\nsets), called Comparative Document Analysis (CDA). Given any pair of documents\nfrom a document collection, CDA aims to automatically identify sets of quality\nphrases to summarize the commonalities of both documents and highlight the\ndistinctions of each with respect to the other informatively and concisely. Our\nsolution uses a general graph-based framework to derive novel measures on\nphrase semantic commonality and pairwise distinction}, and guides the selection\nof sets of phrases by solving two joint optimization problems. We develop an\niterative algorithm to integrate the maximization of phrase commonality or\ndistinction measure with the learning of phrase-document semantic relevance in\na mutually enhancing way. Experiments on text corpora from two different\ndomains---scientific publications and news---demonstrate the effectiveness and\nrobustness of the proposed method on comparing individual documents. Our case\nstudy on comparing news articles published at different dates shows the power\nof the proposed method on comparing document sets."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1511.03780v1", 
    "title": "A User's Guide to CARSKit", 
    "arxiv-id": "1511.03780v1", 
    "author": "Yong Zheng", 
    "publish": "2015-11-12T05:24:35Z", 
    "summary": "Context-aware recommender systems extend traditional recommenders by adapting\ntheir suggestions to users' contextual situations. CARSKit is a Java-based\nopen-source library specifically designed for the context-aware recommendation,\nwhere the state-of-the-art context-aware recommendation algorithms have been\nimplemented. This report provides the basic user's guide to CARSKit, including\nhow to prepare the data set, how to configure the experimental settings, and\nhow to evaluate the algorithms, as well as interpreting the outputs."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.neucom.2015.11.104", 
    "link": "http://arxiv.org/pdf/1511.04674v1", 
    "title": "Using Text Mining To Analyze Real Estate Classifieds", 
    "arxiv-id": "1511.04674v1", 
    "author": "Sherief Abdallah", 
    "publish": "2015-11-15T09:00:56Z", 
    "summary": "Many brokers have adapted their operation to exploit the potential of the\nweb. Despite the importance of the real estate classifieds, there has been\nlittle work in analyzing such data. In this paper we propose a two-stage\nregression model that exploits the textual data in real estate classifieds. We\nshow how our model can be used to predict the price of a real estate\nclassified. We also show how our model can be used to highlight keywords that\naffect the price positively or negatively. To assess our contributions, we\nanalyze four real world data sets, which we gathered from three different\nproperty websites. The analysis shows that our model (which exploits textual\nfeatures) achieves significantly lower root mean squared error across the\ndifferent data sets and against variety of regression models."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.04717v1", 
    "title": "Applying Semantic Web Technologies for Improving the Visibility of   Tourism Data", 
    "arxiv-id": "1511.04717v1", 
    "author": "Antoine Doucet", 
    "publish": "2015-11-15T15:55:21Z", 
    "summary": "Tourism industry is an extremely information-intensive, complex and dynamic\nactivity. It can benefit from semantic Web technologies, due to the significant\nheterogeneity of information sources and the high volume of on-line data. The\nmanagement of semantically diverse annotated tourism data is facilitated by\nontologies that provide methods and standards, which allow flexibility and more\nintelligent access to on-line data. This paper provides a description of some\nof the early results of the Tourinflux project which aims to apply semantic Web\ntechnologies to support tourist actors in effectively finding and publishing\ninformation on the Web."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05659v1", 
    "title": "Learning Discriminative Representations for Semantic Cross Media   Retrieval", 
    "arxiv-id": "1511.05659v1", 
    "author": "Mingwen Wang", 
    "publish": "2015-11-18T05:20:32Z", 
    "summary": "Heterogeneous gap among different modalities emerges as one of the critical\nissues in modern AI problems. Unlike traditional uni-modal cases, where raw\nfeatures are extracted and directly measured, the heterogeneous nature of cross\nmodal tasks requires the intrinsic semantic representation to be compared in a\nunified framework. This paper studies the learning of different representations\nthat can be retrieved across different modality contents. A novel approach for\nmining cross-modal representations is proposed by incorporating explicit linear\nsemantic projecting in Hilbert space. The insight is that the discriminative\nstructures of different modality data can be linearly represented in\nappropriate high dimension Hilbert spaces, where linear operations can be used\nto approximate nonlinear decisions in the original spaces. As a result, an\nefficient linear semantic down mapping is jointly learned for multimodal data,\nleading to a common space where they can be compared. The mechanism of \"feature\nup-lifting and down-projecting\" works seamlessly as a whole, which accomplishes\ncrossmodal retrieval tasks very well. The proposed method, named as shared\ndiscriminative semantic representation learning (\\textbf{SDSRL}), is tested on\ntwo public multimodal dataset for both within- and inter- modal retrieval. The\nexperiments demonstrate that it outperforms several state-of-the-art methods in\nmost scenarios."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05798v1", 
    "title": "Problems with the use of Web search engines to find results in foreign   languages", 
    "arxiv-id": "1511.05798v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:26:58Z", 
    "summary": "Purpose - To test the ability of major search engines, Google, Yahoo, MSN,\nand Ask, to distinguish between German and English-language documents\n  Design/methodology/approach - 50 queries, using words common in German and in\nEnglish, were posed to the engines. The advanced search option of language\nrestriction was used, once in German and once in English. The first 20 results\nper engine in each language were investigated.\n  Findings - While none of the search engines faces problems in providing\nresults in the language of the interface that is used, both Google and MSN face\nproblems when the results are restricted to a foreign language.\n  Research limitations/implications - Search engines were only tested in German\nand in English. We have only anecdotal evidence that the problems are the same\nwith other languages.\n  Practical implications - Searchers should not use the language restriction in\nGoogle and MSN when searching for foreign-language documents. Instead,\nsearchers should use Yahoo or Ask. If searching for foreign language documents\nin Google or MSN, the interface in the target language/country should be used.\n  Value of paper - Demonstrates a problem with search engines that has not been\npreviously investigated."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05800v1", 
    "title": "The Retrieval Effectiveness of Web Search Engines: Considering Results   Descriptions", 
    "arxiv-id": "1511.05800v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:29:50Z", 
    "summary": "Purpose: To compare five major Web search engines (Google, Yahoo, MSN,\nAsk.com, and Seekport) for their retrieval effectiveness, taking into account\nnot only the results but also the results descriptions.\n  Design/Methodology/Approach: The study uses real-life queries. Results are\nmade anonymous and are randomised. Results are judged by the persons posing the\noriginal queries.\n  Findings: The two major search engines, Google and Yahoo, perform best, and\nthere are no significant differences between them. Google delivers\nsignificantly more relevant result descriptions than any other search engine.\nThis could be one reason for users perceiving this engine as superior.\n  Research Limitations: The study is based on a user model where the user takes\ninto account a certain amount of results rather systematically. This may not be\nthe case in real life.\n  Practical Implications: Implies that search engines should focus on relevant\ndescriptions. Searchers are advised to use other search engines in addition to\nGoogle.\n  Originality/Value: This is the first major study comparing results and\ndescriptions systematically and proposes new retrieval measures to take into\naccount results descriptions"
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05802v1", 
    "title": "What Users See - Structures in Search Engine Results Pages", 
    "arxiv-id": "1511.05802v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:33:27Z", 
    "summary": "This paper investigates the composition of search engine results pages. We\ndefine what elements the most popular web search engines use on their results\npages (e.g., organic results, advertisements, shortcuts) and to which degree\nthey are used for popular vs. rare queries. Therefore, we send 500 queries of\nboth types to the major search engines Google, Yahoo, Live.com and Ask. We\ncount how often the different elements are used by the individual engines. In\ntotal, our study is based on 42,758 elements. Findings include that search\nengines use quite different approaches to results pages composition and\ntherefore, the user gets to see quite different results sets depending on the\nsearch engine and search query used. Organic results still play the major role\nin the results pages, but different shortcuts are of some importance, too.\nRegarding the frequency of certain host within the results sets, we find that\nall search engines show Wikipedia results quite often, while other hosts shown\ndepend on the search engine used. Both Google and Yahoo prefer results from\ntheir own offerings (such as YouTube or Yahoo Answers). Since we used the .com\ninterfaces of the search engines, results may not be valid for other\ncountry-specific interfaces."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05810v1", 
    "title": "The Influence of Commercial Intent of Search Results on Their Perceived   Relevance", 
    "arxiv-id": "1511.05810v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:43:22Z", 
    "summary": "We carried out a retrieval effectiveness test on the three major web search\nengines (i.e., Google, Microsoft and Yahoo). In addition to relevance\njudgments, we classified the results according to their commercial intent and\nwhether or not they carried any advertising. We found that all search engines\nprovide a large number of results with a commercial intent. Google provides\nsignificantly more commercial results than the other search engines do.\nHowever, the commercial intent of a result did not influence jurors in their\nrelevance judgments."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05812v1", 
    "title": "The retrieval effectiveness of search engines on navigational queries", 
    "arxiv-id": "1511.05812v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:45:37Z", 
    "summary": "Purpose - To test major Web search engines on their performance on\nnavigational queries, i.e. searches for homepages. Design/methodology/approach\n- 100 real user queries are posed to six search engines (Google, Yahoo, MSN,\nAsk, Seekport, and Exalead). Users described the desired pages, and the results\nposition of these is recorded. Measured success N and mean reciprocal rank are\ncalculated. Findings - Performance of the major search engines Google, Yahoo,\nand MSN is best, with around 90 percent of queries answered correctly. Ask and\nExalead perform worse but receive good scores as well. Research\nlimitations/implications - All queries were in German, and the German-language\ninterfaces of the search engines were used. Therefore, the results are only\nvalid for German queries. Practical implications - When designing a search\nengine to compete with the major search engines, care should be taken on the\nperformance on navigational queries. Users can be influenced easily in their\nquality ratings of search engines based on this performance. Originality/value\n- This study systematically compares the major search engines on navigational\nqueries and compares the findings with studies on the retrieval effectiveness\nof the engines on informational queries. Paper type - research paper"
},{
    "category": "cs.IR", 
    "doi": "10.1145/2810133.2810137", 
    "link": "http://arxiv.org/pdf/1511.05817v1", 
    "title": "A Framework for Evaluating the Retrieval Effectiveness of Search Engines", 
    "arxiv-id": "1511.05817v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:54:10Z", 
    "summary": "This chapter presents a theoretical framework for evaluating next generation\nsearch engines. We focus on search engines whose results presentation is\nenriched with additional information and does not merely present the usual list\nof 10 blue links, that is, of ten links to results, accompanied by a short\ndescription. While Web search is used as an example here, the framework can\neasily be applied to search engines in any other area. The framework not only\naddresses the results presentation, but also takes into account an extension of\nthe general design of retrieval effectiveness tests. The chapter examines the\nways in which this design might influence the results of such studies and how a\nreliable test is best designed."
},{
    "category": "cs.IR", 
    "doi": "10.5210/fm.v17i6.3960", 
    "link": "http://arxiv.org/pdf/1511.05819v1", 
    "title": "The relationship between internet user type and user performance when   carrying out simple vs. complex search tasks", 
    "arxiv-id": "1511.05819v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2015-11-18T14:57:04Z", 
    "summary": "It is widely known that people become better at an activity if they perform\nthis activity long and often. Yet, the question is whether being active in\nrelated areas like communicating online, writing blog articles or commenting on\ncommunity forums have an impact on a persons ability to perform Web searches,\nis still unanswered. Web searching has become a key task conducted online; in\nthis paper we present our findings on whether the user type, which categorises\na persons online activities, has an impact on her or his search capabilities.\nWe show (1) the characteristics of different user types when carrying out\nsimple search tasks; (2) their characteristics when carrying out complex search\ntasks; and, (3) the significantly different user type characteristics between\nsimple and complex search tasks. The results are based on an experiment with 56\nordinary Web users in a laboratory environment. The Search-Logger study\nframework was used to analyze and measure user behavior when carrying out a set\nof 12 predefined search tasks. Our findings include the fact that depending on\ntask type (simple or complex) significant differences can be observed between\nusers of different types."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1511.07237v1", 
    "title": "Predicting Relevance based on Assessor Disagreement: Analysis and   Practical Applications for Search Evaluation", 
    "arxiv-id": "1511.07237v1", 
    "author": "Chris Develder", 
    "publish": "2015-11-23T14:31:43Z", 
    "summary": "Evaluation of search engines relies on assessments of search results for\nselected test queries, from which we would ideally like to draw conclusions in\nterms of relevance of the results for general (e.g., future, unknown) users. In\npractice however, most evaluation scenarios only allow us to conclusively\ndetermine the relevance towards the particular assessor that provided the\njudgments. A factor that cannot be ignored when extending conclusions made from\nassessors towards users, is the possible disagreement on relevance, assuming\nthat a single gold truth label does not exist. This paper presents and analyzes\nthe Predicted Relevance Model (PRM), which allows predicting a particular\nresult's relevance for a random user, based on an observed assessment and\nknowledge on the average disagreement between assessors. With the PRM, existing\nevaluation metrics designed to measure binary assessor relevance, can be\ntransformed into more robust and effectively graded measures that evaluate\nrelevance towards a random user. It also leads to a principled way of\nquantifying multiple graded or categorical relevance levels for use as gains in\nestablished graded relevance measures, such as normalized discounted cumulative\ngain (nDCG), which nowadays often use heuristic and data-independent gain\nvalues. Given a set of test topics with graded relevance judgments, the PRM\nallows evaluating systems on different scenarios, such as their capability of\nretrieving top results, or how well they are able to filter out non-relevant\nones. Its use in actual evaluation scenarios is illustrated on several\ninformation retrieval test collections."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1511.08996v1", 
    "title": "Entity Suggestion by Example using a Conceptual Taxonomy", 
    "arxiv-id": "1511.08996v1", 
    "author": "Wei Wang", 
    "publish": "2015-11-29T11:18:10Z", 
    "summary": "Entity suggestion by example (ESbE) refers to a type of entity acquisition\nquery in which a user provides a set of example entities as the query and\nobtains in return some entities that best complete the concept underlying the\ngiven query. Such entity acquisition queries can be useful in many applications\nsuch as related-entity recommendation and query expansion. A number of ESbE\nquery processing solutions exist in the literature. However, they mostly build\nonly on the idea of entity co-occurrences either in text or web lists, without\ntaking advantage of the existence of many web-scale conceptual taxonomies that\nconsist of hierarchical isA relationships between entity-concept pairs. This\npaper provides a query processing method based on the relevance models between\nentity sets and concepts. These relevance models can be used to obtain the\nfine-grained concepts implied by the query entity set, and the entities that\nbelong to a given concept, thereby providing the entity suggestions. Extensive\nevaluations with real data sets show that the accuracy of the queries processed\nwith this new method is significantly higher than that of existing solutions."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1511.09009v1", 
    "title": "Long Concept Query on Conceptual Taxonomies", 
    "arxiv-id": "1511.09009v1", 
    "author": "Wei Wang", 
    "publish": "2015-11-29T13:20:01Z", 
    "summary": "This paper studies the problem of finding typical entities when the concept\nis given as a query. For a short concept such as university, this is a\nwell-studied problem of retrieving knowledge base such as Microsoft's Probase\nand Google's isA database pre-materializing entities found for the concept in\nHearst patterns of the web corpus. However, we find most real-life queries are\nlong concept queries (LCQs), such as top American private university, which\ncannot and should not be pre-materialized. Our goal is an online construction\nof entity retrieval for LCQs. We argue a naive baseline of rewriting LCQs into\nan intersection of an expanded set of composing short concepts leads to highly\nprecise results with extremely low recall. Instead, we propose to augment the\nconcept list, by identifying related concepts of the query concept. However, as\nsuch increase of recall often invites false positives and decreases precision\nin return, we propose the following two techniques: First, we identify concepts\nwith different relatedness to generate linear orderings and pairwise ordering\nconstraints. Second, we rank entities trying to avoid conflicts with these\nconstraints, to prune out lowly ranked one (likely false positives). With these\nnovel techniques, our approach significantly outperforms state-of-the-arts."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1511.09290v1", 
    "title": "\"Piaf\" vs \"Adele\": classifying encyclopedic queries using automatically   labeled training data", 
    "arxiv-id": "1511.09290v1", 
    "author": "Lu\u00eds Sarmento", 
    "publish": "2015-11-30T13:08:31Z", 
    "summary": "Encyclopedic queries express the intent of obtaining information typically\navailable in encyclopedias, such as biographical, geographical or historical\nfacts. In this paper, we train a classifier for detecting the encyclopedic\nintent of web queries. For training such a classifier, we automatically label\ntraining data from raw query logs. We use click-through data to select positive\nexamples of encyclopedic queries as those queries that mostly lead to Wikipedia\narticles. We investigated a large set of features that can be generated to\ndescribe the input query. These features include both term-specific patterns as\nwell as query projections on knowledge bases items (e.g. Freebase). Results\nshow that using these feature sets it is possible to achieve an F1 score above\n87%, competing with a Google-based baseline, which uses a much wider set of\nsignals to boost the ranking of Wikipedia for potential encyclopedic queries.\nThe results also show that both query projections on Wikipedia article titles\nand Freebase entity match represent the most relevant groups of features. When\nthe training set contains frequent positive examples (i.e rare queries are\nexcluded) results tend to improve."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.00198v2", 
    "title": "Efficient filtering of adult content using textual information", 
    "arxiv-id": "1512.00198v2", 
    "author": "Sylvain Peyronnet", 
    "publish": "2015-12-01T09:57:03Z", 
    "summary": "Nowadays adult content represents a non negligible proportion of the Web\ncontent. It is of the utmost importance to protect children from this content.\nSearch engines, as an entry point for Web navigation are ideally placed to deal\nwith this issue.\n  In this paper, we propose a method that builds a safe index i.e.\nadult-content free for search engines. This method is based on a filter that\nuses only textual information from the web page and the associated URL."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.03165v1", 
    "title": "Semantic Arabic Information Retrieval Framework", 
    "arxiv-id": "1512.03165v1", 
    "author": "Eissa M. Alshari", 
    "publish": "2015-12-10T08:10:49Z", 
    "summary": "The continuous increasing in the amount of the published and stored\ninformation requires a special Information Retrieval (IR) frameworks to search\nand get information accurately and speedily. Currently, keywords-based\ntechniques are commonly used in information retrieval. However, a major\ndrawback of the keywords approach is its inability of handling the polysemy and\nsynonymy phenomenon of the natural language. For instance, the meanings of\nwords and understanding of concepts differ in different communities. Same word\nuse for different concepts (polysemy) or use different words for the same\nconcept (synonymy). Most of information retrieval frameworks have a weakness to\ndeal with the semantics of the words in term of (indexing, Boolean model,\nLatent Semantic Analysis (LSA) , Latent semantic Index (LSI) and semantic\nranking, etc.). Traditional Arabic Information Retrieval (AIR) models\nperformance insufficient with semantic queries, which deal with not only the\nkeywords but also with the context of these keywords. Therefore, there is a\nneed for a semantic information retrieval model with a semantic index structure\nand ranking algorithm based on semantic index."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.03167v1", 
    "title": "Semantic Boolean Arabic Information Retrieval", 
    "arxiv-id": "1512.03167v1", 
    "author": "Hatem Abdulkader", 
    "publish": "2015-12-10T08:19:16Z", 
    "summary": "Arabic language is one of the most widely spoken languages. This language has\na complex morphological structure and is considered as one of the most prolific\nlanguages in terms of article linguistic. Therefore, Arabic Information\nRetrieval (AIR) models need specific techniques to deal with this complex\nmorphological structure. This paper aims to develop an integrate AIR\nframeworks. It lists and analysis the different Information Retrieval (IR)\nmethods and techniques such as query processing, stemming and indexing which\nare used in AIR systems. We conclude that AIR frameworks have a weakness to\ndeal with semantic in term of indexing, Boolean model, Latent Semantic Analysis\n(LSA), Latent Semantic Index (LSI) and semantic ranking. Therefore, semantic\nBoolean IR framework is proposed in this paper. This model is implemented and\nthe precision, recall and run time are measured and compared with the\ntraditional IR model."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.04038v1", 
    "title": "An Uncertainty-Aware Approach for Exploratory Microblog Retrieval", 
    "arxiv-id": "1512.04038v1", 
    "author": "Shimei Pan", 
    "publish": "2015-12-13T11:56:09Z", 
    "summary": "Although there has been a great deal of interest in analyzing customer\nopinions and breaking news in microblogs, progress has been hampered by the\nlack of an effective mechanism to discover and retrieve data of interest from\nmicroblogs. To address this problem, we have developed an uncertainty-aware\nvisual analytics approach to retrieve salient posts, users, and hashtags. We\nextend an existing ranking technique to compute a multifaceted retrieval\nresult: the mutual reinforcement rank of a graph node, the uncertainty of each\nrank, and the propagation of uncertainty among different graph nodes. To\nillustrate the three facets, we have also designed a composite visualization\nwith three visual components: a graph visualization, an uncertainty glyph, and\na flow map. The graph visualization with glyphs, the flow map, and the\nuncertainty analysis together enable analysts to effectively find the most\nuncertain results and interactively refine them. We have applied our approach\nto several Twitter datasets. Qualitative evaluation and two real-world case\nstudies demonstrate the promise of our approach for retrieving high-quality\nmicroblog data."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.05437v1", 
    "title": "A Method of Passage-Based Document Retrieval in Question Answering   System", 
    "arxiv-id": "1512.05437v1", 
    "author": "Chol-Jun Hwang", 
    "publish": "2015-12-17T01:48:37Z", 
    "summary": "We propose a method for using the scoring values of passages to effectively\nretrieve documents in a Question Answering system.\n  For this, we suggest evaluation function that considers proximity between\neach question terms in passage. And using this evaluation function , we extract\na documents which involves scoring values in the highest collection, as a\nsuitable document for question.\n  The proposed method is very effective in document retrieval of Korean\nquestion answering system."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.06303v1", 
    "title": "Predicting the Sentiment Polarity and Rating of Yelp Reviews", 
    "arxiv-id": "1512.06303v1", 
    "author": "Andrew Elkouri", 
    "publish": "2015-12-20T01:12:38Z", 
    "summary": "Online reviews of businesses have become increasingly important in recent\nyears, as customers and even competitors use them to judge the quality of a\nbusiness. Yelp is one of the most popular websites for users to write such\nreviews, and it would be useful for them to be able to predict the sentiment or\neven the star rating of a review. In this paper, we develop two classifiers to\nperform positive/negative classification and 5-star classification. We use\nNaive Bayes, Support Vector Machines, and Logistic Regression as models, and\nachieved the best accuracy with Logistic Regression: 92.90% for\npositive/negative classification, and 63.92% for 5-star classification. These\nresults demonstrate the quality of the Logistic Regression model using only the\ntext of the review, yet there is a promising opportunity for improvement with\nmore data, more features, and perhaps different models."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1512.07051v1", 
    "title": "The Impact of Technical Domain Expertise on Search Behavior and Task   Outcome", 
    "arxiv-id": "1512.07051v1", 
    "author": "Nikita Spirin", 
    "publish": "2015-12-22T12:31:51Z", 
    "summary": "Domain expertise is regarded as one of the key factors impacting search\nsuccess: experts are known to write more effective queries, to select the right\nresults on the result page, and to find answers satisfying their information\nneeds. Search transaction logs play the crucial role in the result ranking. Yet\ndespite the variety in expertise levels of users, all prior interactions are\ntreated alike, suggesting that weighting in expertise can improve the ranking\nfor informational tasks. The main aim of this paper is to investigate the\nimpact of high levels of technical domain expertise on both search behavior and\ntask outcome. We conduct an online user study with searchers proficient in\nprogramming languages. We focus on Java and Javascript, yet we believe that our\nstudy and results are applicable for other expertise-sensitive search tasks.\nThe main findings are three-fold: First, we constructed expertise tests that\neffectively measure technical domain expertise and correlate well with the\nself-reported expertise. Second, we showed that there is a clear position bias,\nbut technical domain experts were less affected by position bias. Third, we\nfound that general expertise helped finding the correct answers, but the domain\nexperts were more successful as they managed to detect better answers. Our work\nis using explicit tests to determine user expertise levels, which is an\nimportant step toward fully automatic detection of expertise levels based on\ninteraction behavior. A deeper understanding of the impact of expertise on\nsearch behavior and task outcome can enable more effective use of expert\nbehavior in search logs - essentially make everyone search as an expert."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1601.00643v1", 
    "title": "Hybrid Approach for Single Text Document Summarization using Statistical   and Sentiment Features", 
    "arxiv-id": "1601.00643v1", 
    "author": "Aditi Sharan", 
    "publish": "2016-01-03T05:55:56Z", 
    "summary": "Summarization is a way to represent same information in concise way with\nequal sense. This can be categorized in two type Abstractive and Extractive\ntype. Our work is focused around Extractive summarization. A generic approach\nto extractive summarization is to consider sentence as an entity, score each\nsentence based on some indicative features to ascertain the quality of sentence\nfor inclusion in summary. Sort the sentences on the score and consider top n\nsentences for summarization. Mostly statistical features have been used for\nscoring the sentences. We are proposing a hybrid model for a single text\ndocument summarization. This hybrid model is an extraction based approach,\nwhich is combination of Statistical and semantic technique. The hybrid model\ndepends on the linear combination of statistical measures : sentence position,\nTF-IDF, Aggregate similarity, centroid, and semantic measure. Our idea to\ninclude sentiment analysis for salient sentence extraction is derived from the\nconcept that emotion plays an important role in communication to effectively\nconvey any message hence, it can play a vital role in text document\nsummarization. For comparison we have generated five system summaries Proposed\nWork, MEAD system, Microsoft system, OPINOSIS system, and Human generated\nsummary, and evaluation is done using ROUGE score."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1601.00855v1", 
    "title": "TimeMachine: Entity-centric Search and Visualization of News Archives", 
    "arxiv-id": "1601.00855v1", 
    "author": "Eug\u00e9nio Oliveira", 
    "publish": "2016-01-05T15:18:10Z", 
    "summary": "We present a dynamic web tool that allows interactive search and\nvisualization of large news archives using an entity-centric approach. Users\nare able to search entities using keyword phrases expressing news stories or\nevents and the system retrieves the most relevant entities to the user query\nbased on automatically extracted and indexed entity profiles. From the\ncomputational journalism perspective, TimeMachine allows users to explore media\ncontent through time using automatic identification of entity names, jobs,\nquotations and relations between entities from co-occurrences networks\nextracted from the news articles. TimeMachine demo is available at\nhttp://maquinadotempo.sapo.pt/"
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1601.01611v1", 
    "title": "Automatic Construction of Evaluation Sets and Evaluation of Document   Similarity Models in Large Scholarly Retrieval Systems", 
    "arxiv-id": "1601.01611v1", 
    "author": "Michael J. Kurtz", 
    "publish": "2016-01-07T17:27:31Z", 
    "summary": "Retrieval systems for scholarly literature offer the ability for the\nscientific community to search, explore and download scholarly articles across\nvarious scientific disciplines. Mostly used by the experts in the particular\nfield, these systems contain user community logs including information on user\nspecific downloaded articles. In this paper we present a novel approach for\nautomatically evaluating document similarity models in large collections of\nscholarly publications. Unlike typical evaluation settings that use test\ncollections consisting of query documents and human annotated relevance\njudgments, we use download logs to automatically generate pseudo-relevant set\nof similar document pairs. More specifically we show that consecutively\ndownloaded document pairs, extracted from a scholarly information retrieval\n(IR) system, could be utilized as a test collection for evaluating document\nsimilarity models. Another novel aspect of our approach lies in the method that\nwe employ for evaluating the performance of the model by comparing the\ndistribution of consecutively downloaded document pairs and random document\npairs in log space. Across two families of similarity models, that represent\ndocuments in the term vector and topic spaces, we show that our evaluation\napproach achieves very high correlation with traditional performance metrics\nsuch as Mean Average Precision (MAP), while being more efficient to compute."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1601.03354v1", 
    "title": "Identifier Namespaces in Mathematical Notation", 
    "arxiv-id": "1601.03354v1", 
    "author": "Alexey Grigorev", 
    "publish": "2016-01-13T19:17:00Z", 
    "summary": "In this thesis, we look at the problem of assigning each identifier of a\ndocument to a namespace. At the moment, there does not exist a special dataset\nwhere all identifiers are grouped to namespaces, and therefore we need to\ncreate such a dataset ourselves.\n  To do that, we need to find groups of documents that use identifiers in the\nsame way. This can be done with cluster analysis methods. We argue that\ndocuments can be represented by the identifiers they contain, and this approach\nis similar to representing textual information in the Vector Space Model.\nBecause of this, we can apply traditional document clustering techniques for\nnamespace discovery.\n  Because the problem is new, there is no gold standard dataset, and it is hard\nto evaluate the performance of our method. To overcome it, we first use Java\nsource code as a dataset for our experiments, since it contains the namespace\ninformation. We verify that our method can partially recover namespaces from\nsource code using only information about identifiers.\n  The algorithms are evaluated on the English Wikipedia, and the proposed\nmethod can extract namespaces on a variety of topics. After extraction, the\nnamespaces are organized into a hierarchical structure by using existing\nclassification schemes such as MSC, PACS and ACM. We also apply it to the\nRussian Wikipedia, and the results are consistent across the languages.\n  To our knowledge, the problem of introducing namespaces to mathematics has\nnot been studied before, and prior to our work there has been no dataset where\nidentifiers are grouped into namespaces. Thus, our result is not only a good\nstart, but also a good indicator that automatic namespace discovery is\npossible."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9275-x", 
    "link": "http://arxiv.org/pdf/1601.03541v2", 
    "title": "Question Answering on Linked Data: Challenges and Future Directions", 
    "arxiv-id": "1601.03541v2", 
    "author": "Christoph Lange", 
    "publish": "2016-01-14T10:21:06Z", 
    "summary": "Question Answering (QA) systems are becoming the inspiring model for the\nfuture of search engines. While recently, underlying datasets for QA systems\nhave been promoted from unstructured datasets to structured datasets with\nhighly semantic-enriched metadata, but still question answering systems involve\nserious challenges which cause to be far beyond desired expectations. In this\npaper, we raise the challenges for building a Question Answering (QA) system\nespecially with the focus of employing structured data (i.e. knowledge graph).\nThis paper provide an exhaustive insight of the known challenges, so far. Thus,\nit helps researchers to easily spot open rooms for the future research agenda."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2808194.2809457", 
    "link": "http://arxiv.org/pdf/1601.04605v1", 
    "title": "Dynamic Information Retrieval: Theoretical Framework and Application", 
    "arxiv-id": "1601.04605v1", 
    "author": "Jun Wang", 
    "publish": "2016-01-18T17:01:34Z", 
    "summary": "Theoretical frameworks like the Probability Ranking Principle and its more\nrecent Interactive Information Retrieval variant have guided the development of\nranking and retrieval algorithms for decades, yet they are not capable of\nhelping us model problems in Dynamic Information Retrieval which exhibit the\nfollowing three properties; an observable user signal, retrieval over multiple\nstages and an overall search intent. In this paper a new theoretical framework\nfor retrieval in these scenarios is proposed. We derive a general dynamic\nutility function for optimizing over these types of tasks, that takes into\naccount the utility of each stage and the probability of observing user\nfeedback. We apply our framework to experiments over TREC data in the dynamic\nmulti page search scenario as a practical demonstration of its effectiveness\nand to frame the discussion of its use, its limitations and to compare it\nagainst the existing frameworks."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1601.04615v2", 
    "title": "A Term-Based Methodology for Query Reformulation Understanding", 
    "arxiv-id": "1601.04615v2", 
    "author": "Jun Wang", 
    "publish": "2016-01-18T17:17:41Z", 
    "summary": "Key to any research involving session search is the understanding of how a\nuser's queries evolve throughout the session. When a user creates a query\nreformulation, he or she is consciously retaining terms from their original\nquery, removing others and adding new terms. By measuring the similarity\nbetween queries we can make inferences on the user's information need and how\nsuccessful their new query is likely to be. By identifying the origins of added\nterms we can infer the user's motivations and gain an understanding of their\ninteractions.\n  In this paper we present a novel term-based methodology for understanding and\ninterpreting query reformulation actions. We use TREC Session Track data to\ndemonstrate how our technique is able to learn from query logs and we make use\nof click data to test user interaction behavior when reformulating queries. We\nidentify and evaluate a range of term-based query reformulation strategies and\nshow that our methods provide valuable insight into understanding query\nreformulation in session search."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1601.04745v1", 
    "title": "A Theoretical Analysis of Two-Stage Recommendation for Cold-Start   Collaborative Filtering", 
    "arxiv-id": "1601.04745v1", 
    "author": "Jun Wang", 
    "publish": "2016-01-18T22:31:06Z", 
    "summary": "In this paper, we present a theoretical framework for tackling the cold-start\ncollaborative filtering problem, where unknown targets (items or users) keep\ncoming to the system, and there is a limited number of resources (users or\nitems) that can be allocated and related to them. The solution requires a\ntrade-off between exploitation and exploration as with the limited\nrecommendation opportunities, we need to, on one hand, allocate the most\nrelevant resources right away, but, on the other hand, it is also necessary to\nallocate resources that are useful for learning the target's properties in\norder to recommend more relevant ones in the future. In this paper, we study a\nsimple two-stage recommendation combining a sequential and a batch solution\ntogether. We first model the problem with the partially observable Markov\ndecision process (POMDP) and provide an exact solution. Then, through an\nin-depth analysis over the POMDP value iteration solution, we identify that an\nexact solution can be abstracted as selecting resources that are not only\nhighly relevant to the target according to the initial-stage information, but\nalso highly correlated, either positively or negatively, with other potential\nresources for the next stage. With this finding, we propose an approximate\nsolution to ease the intractability of the exact solution. Our initial results\non synthetic data and the Movie Lens 100K dataset confirm the performance gains\nof our theoretical development and analysis."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1601.07754v1", 
    "title": "Deep Learning Based Semantic Video Indexing and Retrieval", 
    "arxiv-id": "1601.07754v1", 
    "author": "Sergey Podlesnyy", 
    "publish": "2016-01-28T13:43:30Z", 
    "summary": "We share the implementation details and testing results for video retrieval\nsystem based exclusively on features extracted by convolutional neural\nnetworks. We show that deep learned features might serve as universal signature\nfor semantic content of video useful in many search and retrieval tasks. We\nfurther show that graph-based storage structure for video index allows to\nefficiently retrieving the content with complicated spatial and temporal search\nqueries."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1602.01137v1", 
    "title": "A Dual Embedding Space Model for Document Ranking", 
    "arxiv-id": "1602.01137v1", 
    "author": "Rich Caruana", 
    "publish": "2016-02-02T22:23:18Z", 
    "summary": "A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\n  We postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1602.01665v1", 
    "title": "Improved Query Topic Models via Pseudo-Relevant P\u00f3lya Document Models", 
    "arxiv-id": "1602.01665v1", 
    "author": "Ronan Cummins", 
    "publish": "2016-02-04T13:19:31Z", 
    "summary": "Query-expansion via pseudo-relevance feedback is a popular method of\novercoming the problem of vocabulary mismatch and of increasing average\nretrieval effectiveness. In this paper, we develop a new method that estimates\na query topic model from a set of pseudo-relevant documents using a new\nlanguage modelling framework.\n  We assume that documents are generated via a mixture of multivariate Polya\ndistributions, and we show that by identifying the topical terms in each\ndocument, we can appropriately select terms that are likely to belong to the\nquery topic model. The results of experiments on several TREC collections show\nthat the new approach compares favourably to current state-of-the-art expansion\nmethods."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1602.01792v3", 
    "title": "Random Forest DBSCAN for USPTO Inventor Name Disambiguation", 
    "arxiv-id": "1602.01792v3", 
    "author": "C. Lee Giles", 
    "publish": "2016-02-04T19:00:30Z", 
    "summary": "Name disambiguation and the subsequent name conflation are essential for the\ncorrect processing of person name queries in a digital library or other\ndatabase. It distinguishes each unique person from all other records in the\ndatabase. We study inventor name disambiguation for a patent database using\nmethods and features from earlier work on author name disambiguation and\npropose a feature set appropriate for a patent database. A random forest was\nselected for the pairwise linking classifier since they outperform Naive Bayes,\nLogistic Regression, Support Vector Machines (SVM), Conditional Inference Tree,\nand Decision Trees. Blocking size, very important for scaling, was selected\nbased on experiments that determined feature importance and accuracy. The\nDBSCAN algorithm is used for clustering records, using a distance function\nderived from random forest classifier. For additional scalability clustering\nwas parallelized. Tests on the USPTO patent database show that our method\nsuccessfully disambiguated 12 million inventor mentions within 6.5 hours.\nEvaluation on datasets from USPTO PatentsView inventor name disambiguation\ncompetition shows our algorithm outperforms all algorithms in the competition."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1602.02255v2", 
    "title": "Deep Cross-Modal Hashing", 
    "arxiv-id": "1602.02255v2", 
    "author": "Wu-Jun Li", 
    "publish": "2016-02-06T13:43:24Z", 
    "summary": "Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10791-015-9251-5", 
    "link": "http://arxiv.org/pdf/1602.02506v1", 
    "title": "Wikipedia Tools for Google Spreadsheets", 
    "arxiv-id": "1602.02506v1", 
    "author": "Thomas Steiner", 
    "publish": "2016-02-08T09:40:43Z", 
    "summary": "In this paper, we introduce the Wikipedia Tools for Google Spreadsheets.\nGoogle Spreadsheets is part of a free, Web-based software office suite offered\nby Google within its Google Docs service. It allows users to create and edit\nspreadsheets online, while collaborating with other users in realtime.\nWikipedia is a free-access, free-content Internet encyclopedia, whose content\nand data is available, among other means, through an API. With the Wikipedia\nTools for Google Spreadsheets, we have created a toolkit that facilitates\nworking with Wikipedia data from within a spreadsheet context. We make these\ntools available as open-source on GitHub\n[https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released\nunder the permissive Apache 2.0 license."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1602.03101v1", 
    "title": "Barbara Made the News: Mining the Behavior of Crowds for Time-Aware   Learning to Rank", 
    "arxiv-id": "1602.03101v1", 
    "author": "Jamie Callan", 
    "publish": "2016-02-09T18:01:57Z", 
    "summary": "In Twitter, and other microblogging services, the generation of new content\nby the crowd is often biased towards immediacy: what is happening now. Prompted\nby the propagation of commentary and information through multiple mediums,\nusers on the Web interact with and produce new posts about newsworthy topics\nand give rise to trending topics. This paper proposes to leverage on the\nbehavioral dynamics of users to estimate the most relevant time periods for a\ntopic. Our hypothesis stems from the fact that when a real-world event occurs\nit usually has peak times on the Web: a higher volume of tweets, new visits and\nedits to related Wikipedia articles, and news published about the event. In\nthis paper, we propose a novel time-aware ranking model that leverages on\nmultiple sources of crowd signals. Our approach builds on two major novelties.\nFirst, a unifying approach that given query q, mines and represents temporal\nevidence from multiple sources of crowd signals. This allows us to predict the\ntemporal relevance of documents for query q. Second, a principled retrieval\nmodel that integrates temporal signals in a learning to rank framework, to rank\nresults according to the predicted temporal relevance. Evaluation on the TREC\n2013 and 2014 Microblog track datasets demonstrates that the proposed model\nachieves a relative improvement of 13.2% over lexical retrieval models and 6.2%\nover a learning to rank baseline."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1602.05157v2", 
    "title": "A Ranking Algorithm for Re-finding", 
    "arxiv-id": "1602.05157v2", 
    "author": "Gangli Liu", 
    "publish": "2016-02-16T20:01:25Z", 
    "summary": "Re-finding files from a personal computer is a frequent demand to users. When\nencountered a difficult re-finding task, people may not recall the attributes\nused by conventional re-finding methods, such as a file's path, file name,\nkeywords etc., the re-finding would fail.\n  We proposed a method to support difficult re-finding tasks. By asking the\nuser a list of questions about the target, such as a document's pages, author\nnumbers, accumulated reading time, last reading location etc. Then use the\nuser's answers to filter out the target.\n  After the user answered a list of questions about the target file, we\nevaluate the user's familiar degree about the target file based on the answers.\nWe devise a ranking algorithm which sorts the candidates by comparing the\nuser's familiarity degree about the target and the candidates.\n  We also propose a method to generate re-finding tasks artificially based on\nthe user's own document corpus."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1602.07799v1", 
    "title": "A Study on the usage of Data Structures in Information Retrieval", 
    "arxiv-id": "1602.07799v1", 
    "author": "G. Maheeja", 
    "publish": "2016-02-25T05:12:55Z", 
    "summary": "This paper tries to throw light in the usage of data structures in the field\nof information retrieval. Information retrieval is an area of study which is\ngaining momentum as the need and urge for sharing and exploring information is\ngrowing day by day. Data structures have been the area of research for a long\nperiod in the arena of computer science. The need to have efficient data\nstructures has become even more important as the data grows in an exponential\nnature."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1603.04953v1", 
    "title": "Mitigation Procedures to Rank Experts through Information Retrieval   Measures", 
    "arxiv-id": "1603.04953v1", 
    "author": "Matthieu Vergne", 
    "publish": "2016-03-16T03:58:38Z", 
    "summary": "In order to find experts, different approaches build rankings of people,\nassuming that they are ranked by level of expertise, and use typical\nInformation Retrieval (IR) measures to evaluate their effectiveness. However,\nwe figured out that expert rankings (i) tend to be partially ordered, (ii)\nincomplete, and (iii) consequently provide more an order rather than absolute\nranks, which is not what usual IR measures exploit. To improve this state of\nthe art, we propose to revise the formalism used in IR to design proper\nmeasures for comparing expert rankings. In this report, we investigate a first\nstep by providing mitigation procedures for the three issues, and we analyse IR\nmeasures with the help of these procedures to identify interesting revisions\nand remaining limitations. From this analysis, we see that most of the measures\ncan be exploited for this more generic context because of our mitigation\nprocedures. Moreover, measures based on precision and recall, usually unable to\nconsider the order of the ranked items, are of first interest if we represent a\nranking as a set of ordered pairs. Cumulative measures, on the other hand, are\nspecifically designed for considering the order but suffer from a higher\ncomplexity, motivating the use of precision/recall measures with the right\nrepresentation."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1603.05572v5", 
    "title": "Supervised Matrix Factorization for Cross-Modality Hashing", 
    "arxiv-id": "1603.05572v5", 
    "author": "Gang Hua", 
    "publish": "2016-03-17T16:51:44Z", 
    "summary": "Matrix factorization has been recently utilized for the task of multi-modal\nhashing for cross-modality visual search, where basis functions are learned to\nmap data from different modalities to the same Hamming embedding. In this\npaper, we propose a novel cross-modality hashing algorithm termed Supervised\nMatrix Factorization Hashing (SMFH) which tackles the multi-modal hashing\nproblem with a collective non-matrix factorization across the different\nmodalities. In particular, SMFH employs a well-designed binary code learning\nalgorithm to preserve the similarities among multi-modal original features\nthrough a graph regularization. At the same time, semantic labels, when\navailable, are incorporated into the learning procedure. We conjecture that all\nthese would facilitate to preserve the most relevant information during the\nbinary quantization process, and hence improve the retrieval accuracy. We\ndemonstrate the superior performance of SMFH on three cross-modality visual\nsearch benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with\nquantitative comparison to various state-of-the-art methods"
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1603.09522v1", 
    "title": "Image Retrieval with a Bayesian Model of Relevance Feedback", 
    "arxiv-id": "1603.09522v1", 
    "author": "John Shawe-Taylor", 
    "publish": "2016-03-31T10:50:50Z", 
    "summary": "A content-based image retrieval system based on multinomial relevance\nfeedback is proposed. The system relies on an interactive search paradigm where\nat each round a user is presented with k images and selects the one closest to\ntheir ideal target. Two approaches, one based on the Dirichlet distribution and\none based the Beta distribution, are used to model the problem motivating an\nalgorithm that trades exploration and exploitation in presenting the images in\neach round. Experimental results show that the new approach compares favourably\nwith previous work."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2835776.2835825", 
    "link": "http://arxiv.org/pdf/1604.00942v1", 
    "title": "High Enough? Explaining and Predicting Traveler Satisfaction Using   Airline Review", 
    "arxiv-id": "1604.00942v1", 
    "author": "Elisabeth Lex", 
    "publish": "2016-04-04T16:44:00Z", 
    "summary": "Air travel is one of the most frequently used means of transportation in our\nevery-day life. Thus, it is not surprising that an increasing number of\ntravelers share their experiences with airlines and airports in form of online\nreviews on the Web. In this work, we thrive to explain and uncover the features\nof airline reviews that contribute most to traveler satisfaction. To that end,\nwe examine reviews crawled from the Skytrax air travel review portal. Skytrax\nprovides four review categories to review airports, lounges, airlines and\nseats. Each review category consists of several five-star ratings as well as\nfree-text review content. In this paper, we conducted a comprehensive feature\nstudy and we find that not only five-star rating information such as airport\nqueuing time and lounge comfort highly correlate with traveler satisfaction but\nalso textual features in the form of the inferred review text sentiment. Based\non our findings, we created classifiers to predict traveler satisfaction using\nthe best performing rating features. Our results reveal that given our\nmethodology, traveler satisfaction can be predicted with high accuracy.\nAdditionally, we find that training a model on the sentiment of the review text\nprovides a competitive alternative when no five star rating information is\navailable. We believe that our work is of interest for researchers in the area\nof modeling and predicting user satisfaction based on available review data on\nthe Web."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.01070v2", 
    "title": "Science Concierge: A fast content-based recommendation system for   scientific publications", 
    "arxiv-id": "1604.01070v2", 
    "author": "Konrad Kording", 
    "publish": "2016-04-04T21:35:16Z", 
    "summary": "Finding relevant publications is important for scientists who have to cope\nwith exponentially increasing numbers of scholarly material. Algorithms can\nhelp with this task as they help for music, movie, and product recommendations.\nHowever, we know little about the performance of these algorithms with\nscholarly material. Here, we develop an algorithm, and an accompanying Python\nlibrary, that implements a recommendation system based on the content of\narticles. Design principles are to adapt to new content, provide near-real time\nsuggestions, and be open source. We tested the library on 15K posters from the\nSociety of Neuroscience Conference 2015. Human curated topics are used to cross\nvalidate parameters in the algorithm and produce a similarity metric that\nmaximally correlates with human judgments. We show that our algorithm\nsignificantly outperformed suggestions based on keywords. The work presented\nhere promises to make the exploration of scholarly material faster and more\naccurate."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.02071v1", 
    "title": "Interpretable recommendations via overlapping co-clusters", 
    "arxiv-id": "1604.02071v1", 
    "author": "Michail Vlachos", 
    "publish": "2016-04-07T16:40:53Z", 
    "summary": "There is an increasing need to provide not only accurate but also\ninterpretable recommendations, in order to enhance transparency and trust in\nthe recommendation process. This is particularly important in a\nbusiness-to-business setting, where recommendations are generated for\nexperienced sales staff and not directly for the end-client. In this paper, we\nconsider the problem of generating interpretable recommendations based on the\npurchase history of clients, or more general, based on positive or one-class\nratings only. We present an algorithm that generates recommendations by\nidentifying overlapping co-clusters consisting of clients and products. Our\nalgorithm uses matrix factorization techniques to identify co-clusters, and\nrecommends a client-product pair because of its membership in one or more\nclient-product co-clusters. The algorithm exhibits linear complexity in the\nnumber of co-clusters and input examples, and can therefore be applied to very\nlarge datasets. We show, both on a real client-product dataset from our\ninstitution, as well as on publicly available datasets, that the recommendation\naccuracy of our algorithm is better than or equivalent to standard\ninterpretable and non-interpretable recommendation techniques, such as standard\none-class nearest neighbor and matrix factorization techniques. Most\nimportantly, our approach is capable of offering textually and visually\ninterpretable recommendations."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.04007v1", 
    "title": "Balancing Between Over-Weighting and Under-Weighting in Supervised Term   Weighting", 
    "arxiv-id": "1604.04007v1", 
    "author": "Xiaodong Gu", 
    "publish": "2016-04-14T01:29:52Z", 
    "summary": "Supervised term weighting could improve the performance of text\ncategorization. A way proven to be effective is to give more weight to terms\nwith more imbalanced distributions across categories. This paper shows that\nsupervised term weighting should not just assign large weights to imbalanced\nterms, but should also control the trade-off between over-weighting and\nunder-weighting. Over-weighting, a new concept proposed in this paper, is\ncaused by the improper handling of singular terms and too large ratios between\nterm weights. To prevent over-weighting, we present three regularization\ntechniques: add-one smoothing, sublinear scaling and bias term. Add-one\nsmoothing is used to handle singular terms. Sublinear scaling and bias term\nshrink the ratios between term weights. However, if sublinear functions scale\ndown term weights too much, or the bias term is too large, under-weighting\nwould occur and harm the performance. It is therefore critical to balance\nbetween over-weighting and under-weighting. Inspired by this insight, we also\npropose a new supervised term weighting scheme, regularized entropy (re). Our\nre employs entropy to measure term distribution, and introduces the bias term\nto control over-weighting and under-weighting. Empirical evaluations on topical\nand sentiment classification datasets indicate that sublinear scaling and bias\nterm greatly influence the performance of supervised term weighting, and our re\nenjoys the best results in comparison with existing schemes."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.05492v3", 
    "title": "Count-Min Tree Sketch: Approximate counting for NLP", 
    "arxiv-id": "1604.05492v3", 
    "author": "Abdul Mouhamadsultane", 
    "publish": "2016-04-19T09:51:34Z", 
    "summary": "The Count-Min Sketch is a widely adopted structure for approximate event\ncounting in large scale processing. In a previous work we improved the original\nversion of the Count-Min-Sketch (CMS) with conservative update using\napproximate counters instead of linear counters. These structures are\ncomputationaly efficient and improve the average relative error (ARE) of a CMS\nat constant memory footprint. These improvements are well suited for NLP tasks,\nin which one is interested by the low-frequency items. However, if Log counters\nallow to improve ARE, they produce a residual error due to the approximation.\nIn this paper, we propose the Count-Min Tree Sketch (Copyright 2016 eXenSa. All\nrights reserved) variant with pyramidal counters, which are focused toward\ntaking advantage of the Zipfian distribution of text data."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.05754v1", 
    "title": "Document Retrieval using Predication Similarity", 
    "arxiv-id": "1604.05754v1", 
    "author": "Kalpa Gunaratna", 
    "publish": "2016-04-19T21:23:23Z", 
    "summary": "Document retrieval has been an important research problem over many years in\nthe information retrieval community. State-of-the-art techniques utilize\nvarious methods in matching documents to a given document including keywords,\nphrases, and annotations. In this paper, we propose a new approach for document\nretrieval that utilizes predications (subject-predicate-object triples)\nextracted from the documents. We represent documents as sets of predications.\nWe measure the similarity between predications to compute the similarity\nbetween documents. Our approach utilizes the hierarchical information available\nin ontologies in computing concept-concept similarity, making the approach\nflexible. Predication-based document similarity is more precise and forms the\nbasis for a semantically aware document retrieval system. We show that the\napproach is competitive with an existing state-of-the-art related document\nretrieval technique in the biomedical domain."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.06270v1", 
    "title": "Incorporating Semantic Knowledge into Latent Matching Model in Search", 
    "arxiv-id": "1604.06270v1", 
    "author": "Bin Wang", 
    "publish": "2016-04-21T12:17:42Z", 
    "summary": "The relevance between a query and a document in search can be represented as\nmatching degree between the two objects. Latent space models have been proven\nto be effective for the task, which are often trained with click-through data.\nOne technical challenge with the approach is that it is hard to train a model\nfor tail queries and tail documents for which there are not enough clicks. In\nthis paper, we propose to address the challenge by learning a latent matching\nmodel, using not only click-through data but also semantic knowledge. The\nsemantic knowledge can be categories of queries and documents as well as\nsynonyms of words, manually or automatically created. Specifically, we\nincorporate semantic knowledge into the objective function by including\nregularization terms. We develop two methods to solve the learning task on the\nbasis of coordinate descent and gradient descent respectively, which can be\nemployed in different settings. Experimental results on two datasets from an\napp search engine demonstrate that our model can make effective use of semantic\nknowledge, and thus can significantly enhance the accuracies of latent matching\nmodels, particularly for tail queries."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.07044v1", 
    "title": "Analyzing User Preference for Social Image Recommendation", 
    "arxiv-id": "1604.07044v1", 
    "author": "Thomas Huang", 
    "publish": "2016-04-24T15:54:02Z", 
    "summary": "With the incredibly growing amount of multimedia data shared on the social\nmedia platforms, recommender systems have become an important necessity to ease\nusers' burden on the information overload. In such a scenario, extensive amount\nof heterogeneous information such as tags, image content, in addition to the\nuser-to-item preferences, is extremely valuable for making effective\nrecommendations. In this paper, we explore a novel hybrid algorithm termed {\\em\nSTM}, for image recommendation. STM jointly considers the problem of image\ncontent analysis with the users' preferences on the basis of sparse\nrepresentation. STM is able to tackle the challenges of highly sparse user\nfeedbacks and cold-start problmes in the social network scenario. In addition,\nour model is based on the classical probabilistic matrix factorization and can\nbe easily extended to incorporate other useful information such as the social\nrelationships. We evaluate our approach with a newly collected 0.3 million\nsocial image data set from Flickr. The experimental results demonstrate that\nsparse topic modeling of the image content leads to more effective\nrecommendations, , with a significant performance gain over the\nstate-of-the-art alternatives."
},{
    "category": "cs.IR", 
    "doi": "10.1371/journal.pone.0158423", 
    "link": "http://arxiv.org/pdf/1604.07521v1", 
    "title": "Feedback-based Approach to Introduce Freshness in Recommendations", 
    "arxiv-id": "1604.07521v1", 
    "author": "Saikiran Thunuguntla", 
    "publish": "2016-04-26T05:08:13Z", 
    "summary": "Recommender systems usually face the problem of serving the same\nrecommendations across multiple sessions regardless of whether the user is\ninterested in them or not, thereby reducing their effectiveness. To add\nfreshness to the recommended products, we introduce a feedback loop where the\nset of recommended products in the current session depend on the user's\ninteraction with the previously recommended sets. We also describe ways of\naddressing freshness when there is little or even no direct user interaction.\nWe define a metric to quantify freshness by reducing the problem to measuring\ntemporal diversity."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1604.08640v1", 
    "title": "Hilbert Exclusion: Improved Metric Search through Finite Isometric   Embeddings", 
    "arxiv-id": "1604.08640v1", 
    "author": "Fausto Rabitti", 
    "publish": "2016-04-28T22:30:54Z", 
    "summary": "Most research into similarity search in metric spaces relies upon the\ntriangle inequality property. This property allows the space to be arranged\naccording to relative distances to avoid searching some subspaces. We show that\nmany common metric spaces, notably including those using Euclidean and\nJensen-Shannon distances, also have a stronger property, sometimes called the\nfour-point property: in essence, these spaces allow an isometric embedding of\nany four points in three-dimensional Euclidean space, as well as any three\npoints in two-dimensional Euclidean space. In fact, we show that any space\nwhich is isometrically embeddable in Hilbert space has the stronger property.\nThis property gives stronger geometric guarantees, and one in particular, which\nwe name the Hilbert Exclusion property, allows any indexing mechanism which\nuses hyperplane partitioning to perform better. One outcome of this observation\nis that a number of state-of-the-art indexing mechanisms over high dimensional\nspaces can be easily extended to give a significant increase in performance;\nfurthermore, the improvement given is greater in higher dimensions. This\ntherefore leads to a significant improvement in the cost of metric search in\nthese spaces."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1605.00184v2", 
    "title": "A Supervised Learning Algorithm for Binary Domain Classification of Web   Queries using SERPs", 
    "arxiv-id": "1605.00184v2", 
    "author": "Michael Nelson", 
    "publish": "2016-04-30T23:02:43Z", 
    "summary": "General purpose Search Engines (SEs) crawl all domains (e.g., Sports, News,\nEntertainment) of the Web, but sometimes the informational need of a query is\nrestricted to a particular domain (e.g., Medical). We leverage the work of SEs\nas part of our effort to route domain specific queries to local Digital\nLibraries (DLs). SEs are often used even if they are not the \"best\" source for\ncertain types of queries. Rather than tell users to \"use this DL for this kind\nof query\", we intend to automatically detect when a query could be better\nserved by a local DL (such as a private, access-controlled DL that is not\ncrawlable via SEs). This is not an easy task because Web queries are short,\nambiguous, and there is lack of quality labeled training data (or it is\nexpensive to create). To detect queries that should be routed to local,\nspecialized DLs, we first send the queries to Google and then examine the\nfeatures in the resulting Search Engine Result Pages (SERPs), and then classify\nthe query as belonging to either the scholar or non-scholar domain. Using\n400,000 AOL queries for the non-scholar domain and 400,000 queries from the\nNASA Technical Report Server (NTRS) for the scholar domain, our classifier\nachieved a precision of 0.809 and F-measure of 0.805."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1605.07980v1", 
    "title": "Structural Analysis of User Choices for Mobile App Recommendation", 
    "arxiv-id": "1605.07980v1", 
    "author": "Martin Ester", 
    "publish": "2016-05-25T17:47:30Z", 
    "summary": "Advances in smartphone technology have promoted the rapid development of\nmobile apps. However, the availability of a huge number of mobile apps in\napplication stores has imposed the challenge of finding the right apps to meet\nthe user needs. Indeed, there is a critical demand for personalized app\nrecommendations. Along this line, there are opportunities and challenges posed\nby two unique characteristics of mobile apps. First, app markets have organized\napps in a hierarchical taxonomy. Second, apps with similar functionalities are\ncompeting with each other. While there are a variety of approaches for mobile\napp recommendations, these approaches do not have a focus on dealing with these\nopportunities and challenges. To this end, in this paper, we provide a\nsystematic study for addressing these challenges. Specifically, we develop a\nStructural User Choice Model (SUCM) to learn fine-grained user preferences by\nexploiting the hierarchical taxonomy of apps as well as the competitive\nrelationships among apps. Moreover, we design an efficient learning algorithm\nto estimate the parameters for the SUCM model. Finally, we perform extensive\nexperiments on a large app adoption data set collected from Google Play. The\nresults show that SUCM consistently outperforms state-of-the-art top-N\nrecommendation methods by a significant margin."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1605.09362v2", 
    "title": "Document Retrieval on Repetitive String Collections", 
    "arxiv-id": "1605.09362v2", 
    "author": "Jouni Sir\u00e9n", 
    "publish": "2016-05-30T19:40:18Z", 
    "summary": "Most of the fastest-growing string collections today are repetitive, that is,\nmost of the constituent documents are similar to many others. As these\ncollections keep growing, a key approach to handling them is to exploit their\nrepetitiveness, which can reduce their space usage by orders of magnitude. We\nstudy the problem of indexing repetitive string collections in order to perform\nefficient document retrieval operations on them. Document retrieval problems\nare routinely solved by search engines on large natural language collections,\nbut the techniques are less developed on generic string collections. The case\nof repetitive string collections is even less understood, and there are very\nfew existing solutions. We develop two novel ideas, {\\em interleaved LCPs} and\n{\\em precomputed document lists}, that yield highly compressed indexes solving\nthe problem of document listing (find all the documents where a string\nappears), top-$k$ document retrieval (find the $k$ documents where a string\nappears most often), and document counting (count the number of documents where\na string appears). We also show that a classical data structure supporting the\nlatter query becomes highly compressible on repetitive data. Finally, we show\nhow the tools we developed can be combined to solve ranked conjunctive and\ndisjunctive multi-term queries under the simple tf-idf model of relevance. We\nthoroughly evaluate the resulting techniques in various real-life\nrepetitiveness scenarios, and recommend the best choices for each case."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.00615v2", 
    "title": "Low-dimensional Query Projection based on Divergence Minimization   Feedback Model for Ad-hoc Retrieval", 
    "arxiv-id": "1606.00615v2", 
    "author": "Azadeh Shakery", 
    "publish": "2016-06-02T10:44:52Z", 
    "summary": "Low-dimensional word vectors have long been used in a wide range of\napplications in natural language processing. In this paper we shed light on\nestimating query vectors in ad-hoc retrieval where a limited information is\navailable in the original query. Pseudo-relevance feedback (PRF) is a\nwell-known technique for updating query language models and expanding the\nqueries with a number of relevant terms. We formulate the query updating in\nlow-dimensional spaces first with rotating the query vector and then with\nscaling. These consequential steps are embedded in a query-specific projection\nmatrix capturing both angle and scaling. In this paper we propose a new but not\nthe most effective technique necessarily for PRF in language modeling, based on\nthe query projection algorithm. We learn an embedded coefficient matrix for\neach query, whose aim is to improve the vector representation of the query by\ntransforming it to a more reliable space, and then update the query language\nmodel. The proposed embedded coefficient divergence minimization model (ECDMM)\ntakes top-ranked documents retrieved by the query and obtains a couple of\npositive and negative sample sets; these samples are used for learning the\ncoefficient matrix which will be used for projecting the query vector and\nupdating the query language model using a softmax function. Experimental\nresults on several TREC and CLEF data sets in several languages demonstrate\neffectiveness of ECDMM. The experimental results reveal that the new\nformulation for the query works as well as state-of-the-art PRF techniques and\noutperforms state-of-the-art PRF techniques in a TREC collection in terms of\nMAP,P@5, and P@10 significantly."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.03066v1", 
    "title": "The Effects of Latency Penalties in Evaluating Push Notification Systems", 
    "arxiv-id": "1606.03066v1", 
    "author": "Charles L. A. Clarke", 
    "publish": "2016-06-09T18:58:51Z", 
    "summary": "We examine the effects of different latency penalties in the evaluation of\npush notification systems, as operationalized in the TREC 2015 Microblog track\nevaluation. The purpose of this study is to inform the design of metrics for\nthe TREC 2016 Real-Time Summarization track, which is largely modeled after the\nTREC 2015 evaluation design."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.04223v1", 
    "title": "Learning Term Weights for Ad-hoc Retrieval", 
    "arxiv-id": "1606.04223v1", 
    "author": "B. Piwowarski", 
    "publish": "2016-06-14T07:19:08Z", 
    "summary": "Most Information Retrieval models compute the relevance score of a document\nfor a given query by summing term weights specific to a document or a query.\nHeuristic approaches, like TF-IDF, or probabilistic models, like BM25, are used\nto specify how a term weight is computed. In this paper, we propose to leverage\nlearning-to-rank principles to learn how to compute a term weight for a given\ndocument based on the term occurrence pattern."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.04648v1", 
    "title": "A Study of MatchPyramid Models on Ad-hoc Retrieval", 
    "arxiv-id": "1606.04648v1", 
    "author": "Xueqi Cheng", 
    "publish": "2016-06-15T05:39:30Z", 
    "summary": "Deep neural networks have been successfully applied to many text matching\ntasks, such as paraphrase identification, question answering, and machine\ntranslation. Although ad-hoc retrieval can also be formalized as a text\nmatching task, few deep models have been tested on it. In this paper, we study\na state-of-the-art deep matching model, namely MatchPyramid, on the ad-hoc\nretrieval task. The MatchPyramid model employs a convolutional neural network\nover the interactions between query and document to produce the matching score.\nWe conducted extensive experiments to study the impact of different pooling\nsizes, interaction functions and kernel sizes on the retrieval performance.\nFinally, we show that the MatchPyramid models can significantly outperform\nseveral recently introduced deep matching models on the retrieval task, but\nstill cannot compete with the traditional retrieval models, such as BM25 and\nlanguage models."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.05978v1", 
    "title": "M3A: Model, MetaModel, and Anomaly Detection in Web Searches", 
    "arxiv-id": "1606.05978v1", 
    "author": "Christos Faloutsos", 
    "publish": "2016-06-20T05:46:47Z", 
    "summary": "'Alice' is submitting one web search per five minutes, for three hours in a\nrow - is it normal? How to detect abnormal search behaviors, among Alice and\nother users? Is there any distinct pattern in Alice's (or other users') search\nbehavior? We studied what is probably the largest, publicly available, query\nlog that contains more than 30 million queries from 0.6 million users. In this\npaper, we present a novel, user-and group-level framework, M3A: Model,\nMetaModel and Anomaly detection. For each user, we discover and explain a\nsurprising, bi-modal pattern of the inter-arrival time (IAT) of landed queries\n(queries with user click-through). Specifically, the model Camel-Log is\nproposed to describe such an IAT distribution; we then notice the correlations\namong its parameters at the group level. Thus, we further propose the metamodel\nMeta-Click, to capture and explain the two-dimensional, heavy-tail distribution\nof the parameters. Combining Camel-Log and Meta-Click, the proposed M3A has the\nfollowing strong points: (1) the accurate modeling of marginal IAT\ndistribution, (2) quantitative interpretations, and (3) anomaly detection."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.06081v1", 
    "title": "How Relevant is the Long Tail? A Relevance Assessment Study on Million   Short", 
    "arxiv-id": "1606.06081v1", 
    "author": "Dirk Lewandowski", 
    "publish": "2016-06-20T12:18:35Z", 
    "summary": "Users of web search engines are known to mostly focus on the top ranked\nresults of the search engine result page. While many studies support this well\nknown information seeking pattern only few studies concentrate on the question\nwhat users are missing by neglecting lower ranked results. To learn more about\nthe relevance distributions in the so-called long tail we conducted a relevance\nassessment study with the Million Short long-tail web search engine. While we\nsee a clear difference in the content between the head and the tail of the\nsearch engine result list we see no statistical significant differences in the\nbinary relevance judgments and weak significant differences when using graded\nrelevance. The tail contains different but still valuable results. We argue\nthat the long tail can be a rich source for the diversification of web search\nengine result lists but it needs more evaluation to clearly describe the\ndifferences."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.06816v1", 
    "title": "Learning Optimal Card Ranking from Query Reformulation", 
    "arxiv-id": "1606.06816v1", 
    "author": "Suju Rajan", 
    "publish": "2016-06-22T04:52:18Z", 
    "summary": "Mobile search has recently been shown to be the major contributor to the\ngrowing search market. The key difference between mobile search and desktop\nsearch is that information presentation is limited to the screen space of the\nmobile device. Thus, major search engines have adopted a new type of search\nresult presentation, known as \\textit{information cards}, in which each card\npresents summarized results from one domain/vertical, for a given query, to\naugment the standard blue-links search results. While it has been widely\nacknowledged that information cards are particularly suited to mobile user\nexperience, it is also challenging to optimize such result sets. Typically,\nuser engagement metrics like query reformulation are based on whole ranked list\nof cards for each query and most traditional learning to rank algorithms\nrequire per-item relevance labels. In this paper, we investigate the\npossibility of interpreting query reformulation into effective relevance labels\nfor query-card pairs. We inherit the concept of conventional learning-to-rank,\nand propose pointwise, pairwise and listwise interpretations for query\nreformulation. In addition, we propose a learning-to-label strategy that learns\nthe contribution of each card, with respect to a query, where such\ncontributions can be used as labels for training card ranking models. We\nutilize a state-of-the-art ranking model and demonstrate the effectiveness of\nproposed mechanisms on a large-scale mobile data from a major search engine,\nshowing that models trained from labels derived from user engagement can\nsignificantly outperform ones trained from human judgment labels."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.07188v1", 
    "title": "Selective Term Proximity Scoring Via BP-ANN", 
    "arxiv-id": "1606.07188v1", 
    "author": "Xiaoguang Liu", 
    "publish": "2016-06-23T05:02:24Z", 
    "summary": "When two terms occur together in a document, the probability of a close\nrelationship between them and the document itself is greater if they are in\nnearby positions. However, ranking functions including term proximity (TP)\nrequire larger indexes than traditional document-level indexing, which slows\ndown query processing. Previous studies also show that this technique is not\neffective for all types of queries. Here we propose a document ranking model\nwhich decides for which queries it would be beneficial to use a proximity-based\nranking, based on a collection of features of the query. We use a machine\nlearning approach in determining whether utilizing TP will be beneficial.\nExperiments show that the proposed model returns improved rankings while also\nreducing the overhead incurred as a result of using TP statistics."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3001583", 
    "link": "http://arxiv.org/pdf/1606.07351v1", 
    "title": "A Relevant Content Filtering Based Framework For Data Stream   Summarization", 
    "arxiv-id": "1606.07351v1", 
    "author": "Arvind Agarwal", 
    "publish": "2016-06-23T15:49:08Z", 
    "summary": "Social media platforms are a rich source of information these days, however,\nof all the available information, only a small fraction is of users' interest.\nTo help users catch up with the latest topics of their interests from the large\namount of information available in social media, we present a relevant content\nfiltering based framework for data stream summarization. More specifically,\ngiven the topic or event of interest, this framework can dynamically discover\nand filter out relevant information from irrelevant information in the stream\nof text provided by social media platforms. It then further captures the most\nrepresentative and up-to-date information to generate a sequential summary or\nevent story line along with the evolution of the topic or event. Our framework\ndoes not depend on any labeled data, it instead uses the weak supervision\nprovided by the user, which matches the real scenarios of users searching for\ninformation about an ongoing event. We experimented on two real events traced\nby a Twitter dataset from TREC 2011. The results verified the effectiveness of\nrelevant content filtering and sequential summary generation of the proposed\nframework. It also shows its robustness of using the most easy-to-obtain weak\nsupervision, i.e., trending topic or hashtag. Thus, this framework can be\neasily integrated into social media platforms such as Twitter to generate\nsequential summaries for the events of interest. We also make the manually\ngenerated gold-standard sequential summaries of the two test events publicly\navailable for future use in the community."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4053.4641", 
    "link": "http://arxiv.org/pdf/1606.07604v1", 
    "title": "Kategorisasi dokumen web secara otomatis berdasarkan folksonomy   menggunakan multinomial naive Bayes classifier", 
    "arxiv-id": "1606.07604v1", 
    "author": "Hendy Irawan", 
    "publish": "2016-06-24T08:36:21Z", 
    "summary": "Folksonomy is a non-hierarchical document categorizing system, that treats\nevery category in a flat manner, dan every category is entered freely by anyone\nwho submitted a document in these categories. Categorization is done\nautomatically at the time a document is submitted, by entering the list of\ncategories that best fit the document. del.icio.us (http://del.icio.us) site is\none of the most popular social bookmarking sites that uses folksonomy.\n  Usage of folksonomy, although very easy, also has its weaknesses, such as use\nof different tags for the same concept, use of the same tag for different\nconcepts, no quality control, etc. We try to provide a solution for some of\nthese problems by analyzing Web documents' contents and categorizing them\nautomatically using multinomial naive Bayes algorithm.\n  Bayes classifier works by using a set of evidences and a set of classes. By\ntraining the system using sample data, we can determine the probability of an\nevidence given a particular class. Bayes classifier also uses prior probability\nof a class, which can be calculated from sample data. From these analysis, when\ngiven a new document which is formed by a set of evidences (words), the\nprobabilities of each class given that document (posterior probabilities) can\nbe determined.\n  This system is implemented using PHP 5, Apache, and MySQL. The conclusion\nfrom building this system is that the Bayes method can be used to automatically\ncategorize documents and also as an assistive tool for manual categorization.\n  -----\n  Folksonomy merupakan metode kategorisasi dokumen yang tidak hierarkis,\nmenyamaratakan kedudukan setiap kategori, dan judul kategori ditentukan secara\nbebas oleh siapa saja yang memasukkan sebuah dokumen di dalam kategori-kategori\ntersebut."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4053.4641", 
    "link": "http://arxiv.org/pdf/1606.07608v1", 
    "title": "Using Word Embeddings for Automatic Query Expansion", 
    "arxiv-id": "1606.07608v1", 
    "author": "Utpal Garain", 
    "publish": "2016-06-24T08:41:57Z", 
    "summary": "In this paper a framework for Automatic Query Expansion (AQE) is proposed\nusing distributed neural language model word2vec. Using semantic and contextual\nrelation in a distributed and unsupervised framework, word2vec learns a low\ndimensional embedding for each vocabulary entry. Using such a framework, we\ndevise a query expansion technique, where related terms to a query are obtained\nby K-nearest neighbor approach. We explore the performance of the AQE methods,\nwith and without feedback query expansion, and a variant of simple K-nearest\nneighbor in the proposed framework. Experiments on standard TREC ad-hoc data\n(Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with\nquery set 451-550) shows significant improvement over standard term-overlapping\nbased retrieval methods. However the proposed method fails to achieve\ncomparable performance with statistical co-occurrence based feedback method\nsuch as RM3. We have also found that the word2vec based query expansion methods\nperform similarly with and without any feedback information."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4053.4641", 
    "link": "http://arxiv.org/pdf/1606.07660v2", 
    "title": "Deep Learning Relevance: Creating Relevant Information (as Opposed to   Retrieving it)", 
    "arxiv-id": "1606.07660v2", 
    "author": "Jakob Grue Simonsen", 
    "publish": "2016-06-24T12:41:50Z", 
    "summary": "What if Information Retrieval (IR) systems did not just retrieve relevant\ninformation that is stored in their indices, but could also \"understand\" it and\nsynthesise it into a single document? We present a preliminary study that makes\na first step towards answering this question. Given a query, we train a\nRecurrent Neural Network (RNN) on existing relevant information to that query.\nWe then use the RNN to \"deep learn\" a single, synthetic, and we assume,\nrelevant document for that query. We design a crowdsourcing experiment to\nassess how relevant the \"deep learned\" document is, compared to existing\nrelevant documents. Users are shown a query and four wordclouds (of three\nexisting relevant documents and our deep learned synthetic document). The\nsynthetic document is ranked on average most relevant of all."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1606.07674v2", 
    "title": "Neural Autoregressive Collaborative Filtering for Implicit Feedback", 
    "arxiv-id": "1606.07674v2", 
    "author": "Hanning Zhou", 
    "publish": "2016-06-24T13:10:50Z", 
    "summary": "This paper proposes implicit CF-NADE, a neural autoregressive model for\ncollaborative filtering tasks using implicit feedback ( e.g. click, watch,\nbrowse behaviors). We first convert a users implicit feedback into a like\nvector and a confidence vector, and then model the probability of the like\nvector, weighted by the confidence vector. The training objective of implicit\nCF-NADE is to maximize a weighted negative log-likelihood. We test the\nperformance of implicit CF-NADE on a dataset collected from a popular digital\nTV streaming service. More specifically, in the experiments, we describe how to\nconvert watch counts into implicit relative rating, and feed into implicit\nCF-NADE. Then we compare the performance of implicit CF-NADE model with the\npopular implicit matrix factorization approach. Experimental results show that\nimplicit CF-NADE significantly outperforms the baseline."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1606.07828v1", 
    "title": "Modelling User Preferences using Word Embeddings for Context-Aware Venue   Recommendation", 
    "arxiv-id": "1606.07828v1", 
    "author": "Iadh Ounis", 
    "publish": "2016-06-24T20:15:28Z", 
    "summary": "Venue recommendation aims to assist users by making personalised suggestions\nof venues to visit, building upon data available from location-based social\nnetworks (LBSNs) such as Foursquare. A particular challenge for this task is\ncontext-aware venue recommendation (CAVR), which additionally takes the\nsurrounding context of the user (e.g. the user's location and the time of day)\ninto account in order to provide more relevant venue suggestions. To address\nthe challenges of CAVR, we describe two approaches that exploit word embedding\ntechniques to infer the vector-space representations of venues, users' existing\npreferences, and users' contextual preferences. Our evaluation upon the test\ncollection of the TREC 2015 Contextual Suggestion track demonstrates that we\ncan significantly enhance the effectiveness of a state-of-the-art venue\nrecommendation approach, as well as produce context-aware recommendations that\nare at least as effective as the top TREC 2015 systems."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1606.07869v1", 
    "title": "Representing Documents and Queries as Sets of Word Embedded Vectors for   Information Retrieval", 
    "arxiv-id": "1606.07869v1", 
    "author": "Gareth J. F. Jones", 
    "publish": "2016-06-25T04:35:47Z", 
    "summary": "A major difficulty in applying word vector embeddings in IR is in devising an\neffective and efficient strategy for obtaining representations of compound\nunits of text, such as whole documents, (in comparison to the atomic words),\nfor the purpose of indexing and scoring documents. Instead of striving for a\nsuitable method for obtaining a single vector representation of a large\ndocument of text, we rather aim for developing a similarity metric that makes\nuse of the similarities between the individual embedded word vectors in a\ndocument and a query. More specifically, we represent a document and a query as\nsets of word vectors, and use a standard notion of similarity measure between\nthese sets, computed as a function of the similarities between each constituent\nword pair from these sets. We then make use of this similarity measure in\ncombination with standard IR based similarities for document ranking. The\nresults of our initial experimental investigations shows that our proposed\nmethod improves MAP by up to $5.77\\%$, in comparison to standard text-based\nlanguage model similarity, on the TREC ad-hoc dataset."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1606.08406v1", 
    "title": "The Apps You Use Bring The Blogs to Follow", 
    "arxiv-id": "1606.08406v1", 
    "author": "Beitao Li", 
    "publish": "2016-06-27T18:45:56Z", 
    "summary": "We tackle the blog recommendation problem in Tumblr for mobile users in this\npaper. Blog recommendation is challenging since most mobile users would suffer\nfrom the cold start when there are only a limited number of blogs followed by\nthe user. Specifically to address this problem in the mobile domain, we take\ninto account mobile apps, which typically provide rich information from the\nusers. Based on the assumption that the user interests can be reflected from\ntheir app usage patterns, we propose to exploit the app usage data for\nimproving blog recommendation. Building on the state-of-the-art recommendation\nframework, Factorization Machines (FM), we implement app-based FM that\nintegrates app usage data with the user-blog follow relations. In this approach\nthe blog recommendation is generated not only based on the blogs that the user\nfollowed before, but also the apps that the user has often used. We demonstrate\nin a series of experiments that app-based FM can outperform other alternative\napproaches to a significant extent. Our experimental results also show that\nexploiting app usage information is particularly effective for improving blog\nrecommendation quality for cold start users."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1607.00223v1", 
    "title": "Memory Based Collaborative Filtering with Lucene", 
    "arxiv-id": "1607.00223v1", 
    "author": "Claudio Gennaro", 
    "publish": "2016-07-01T12:45:43Z", 
    "summary": "Memory Based Collaborative Filtering is a widely used approach to provide\nrecommendations. It exploits similarities between ratings across a population\nof users by forming a weighted vote to predict unobserved ratings. Bespoke\nsolutions are frequently adopted to deal with the problem of high quality\nrecommendations on large data sets. A disadvantage of this approach, however,\nis the loss of generality and flexibility of the general collaborative\nfiltering systems. In this paper, we have developed a methodology that allows\none to build a scalable and effective collaborative filtering system on top of\na conventional full-text search engine such as Apache Lucene."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1607.00647v1", 
    "title": "A Survey of Point-of-interest Recommendation in Location-based Social   Networks", 
    "arxiv-id": "1607.00647v1", 
    "author": "Michael R. Lyu", 
    "publish": "2016-07-03T14:42:18Z", 
    "summary": "Point-of-interest (POI) recommendation that suggests new places for users to\nvisit arises with the popularity of location-based social networks (LBSNs). Due\nto the importance of POI recommendation in LBSNs, it has attracted much\nacademic and industrial interest. In this paper, we offer a systematic review\nof this field, summarizing the contributions of individual efforts and\nexploring their relations. We discuss the new properties and challenges in POI\nrecommendation, compared with traditional recommendation problems, e.g., movie\nrecommendation. Then, we present a comprehensive review in three aspects:\ninfluential factors for POI recommendation, methodologies employed for POI\nrecommendation, and different tasks in POI recommendation. Specifically, we\npropose three taxonomies to classify POI recommendation systems. First, we\ncategorize the systems by the influential factors check-in characteristics,\nincluding the geographical information, social relationship, temporal\ninfluence, and content indications. Second, we categorize the systems by the\nmethodology, including systems modeled by fused methods and joint methods.\nThird, we categorize the systems as general POI recommendation and successive\nPOI recommendation by subtle differences in the recommendation task whether to\nbe bias to the recent check-in. For each category, we summarize the\ncontributions and system features, and highlight the representative work.\nMoreover, we discuss the available data sets and the popular metrics. Finally,\nwe point out the possible future directions in this area and conclude this\nsurvey."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1607.02641v1", 
    "title": "Randomised Relevance Model", 
    "arxiv-id": "1607.02641v1", 
    "author": "Victor Lavrenko", 
    "publish": "2016-07-09T18:10:06Z", 
    "summary": "Relevance Models are well-known retrieval models and capable of producing\ncompetitive results. However, because they use query expansion they can be very\nslow. We address this slowness by incorporating two variants of locality\nsensitive hashing (LSH) into the query expansion process. Results on two\ndocument collections suggest that we can obtain large reductions in the amount\nof work, with a small reduction in effectiveness. Our approach is shown to be\nadditive when pruning query terms."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988453", 
    "link": "http://arxiv.org/pdf/1607.02754v1", 
    "title": "Hybrid Recommender System Based on Personal Behavior Mining", 
    "arxiv-id": "1607.02754v1", 
    "author": "Kun Chen", 
    "publish": "2016-07-10T15:32:03Z", 
    "summary": "Recommender systems are mostly well known for their applications in\ne-commerce sites and are mostly static models. Classical personalized\nrecommender algorithm includes item-based collaborative filtering method\napplied in Amazon, matrix factorization based collaborative filtering algorithm\nfrom Netflix, etc. In this article, we hope to combine traditional model with\nbehavior pattern extraction method. We use desensitized mobile transaction\nrecord provided by T-mall, Alibaba to build a hybrid dynamic recommender\nsystem. The sequential pattern mining aims to find frequent sequential pattern\nin sequence database and is applied in this hybrid model to predict customers'\npayment behavior thus contributing to the accuracy of the model."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2917761", 
    "link": "http://arxiv.org/pdf/1607.03296v1", 
    "title": "Implicit Negative Feedback in Clinical Information Retrieval", 
    "arxiv-id": "1607.03296v1", 
    "author": "Carsten Eickhoff", 
    "publish": "2016-07-12T10:15:15Z", 
    "summary": "In this paper, we reflect on ways to improve the quality of bio-medical\ninformation retrieval by drawing implicit negative feedback from negated\ninformation in noisy natural language search queries. We begin by studying the\nextent to which negations occur in clinical texts and quantify their\ndetrimental effect on retrieval performance. Subsequently, we present a number\nof query reformulation and ranking approaches that remedy these shortcomings by\nresolving natural language negations. Our experimental results are based on\ndata collected in the course of the TREC Clinical Decision Support Track and\nshow consistent improvements compared to state-of-the-art methods. Using our\nnovel algorithms, we are able to reduce the negative impact of negations on\nearly precision by up to 65%."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4167.0649", 
    "link": "http://arxiv.org/pdf/1607.05088v3", 
    "title": "Towards Personality-Aware Recommendation", 
    "arxiv-id": "1607.05088v3", 
    "author": "Giorgio Roffo", 
    "publish": "2016-07-18T14:08:20Z", 
    "summary": "In the last decade new ways of shopping online have increased the possibility\nof buying products and services more easily and faster than ever. In this new\ncontext, personality is a key determinant in the decision making of the\nconsumer when shopping. The two main reasons are: firstly, a person's buying\nchoices are influenced by psychological factors like impulsiveness, and\nsecondly, some consumers may be more susceptible to making impulse purchases\nthan others. To the best of our knowledge, the impact of personality factors on\nadvertisements has been largely neglected at the level of recommender systems.\nThis work proposes a highly innovative research which uses a personality\nperspective to determine the unique associations among the consumer's buying\ntendency and advert recommendations. As a matter of fact, the lack of a\npublicly available benchmark for computational advertising do not allow both\nthe exploration of this intriguing research direction and the evaluation of\nstate-of-the-art algorithms. We present the ADS Dataset, a publicly available\nbenchmark for computational advertising enriched with Big-Five users'\npersonality factors and 1,200 personal users' pictures. The proposed benchmark\nallows two main tasks: rating prediction over 300 real advertisements (i.e.,\nRich Media Ads, Image Ads, Text Ads) and click-through rate prediction.\nMoreover, this work carries out experiments, reviews various evaluation\ncriteria used in the literature, and provides a library for each one of them\nwithin one integrated toolbox."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4167.0649", 
    "link": "http://arxiv.org/pdf/1607.05746v3", 
    "title": "Bayesian Non-Exhaustive Classification A Case Study: Online Name   Disambiguation using Temporal Record Streams", 
    "arxiv-id": "1607.05746v3", 
    "author": "Mohammad Al Hasan", 
    "publish": "2016-07-19T20:17:10Z", 
    "summary": "The name entity disambiguation task aims to partition the records of multiple\nreal-life persons so that each partition contains records pertaining to a\nunique person. Most of the existing solutions for this task operate in a batch\nmode, where all records to be disambiguated are initially available to the\nalgorithm. However, more realistic settings require that the name\ndisambiguation task be performed in an online fashion, in addition to, being\nable to identify records of new ambiguous entities having no preexisting\nrecords. In this work, we propose a Bayesian non-exhaustive classification\nframework for solving online name disambiguation task. Our proposed method uses\na Dirichlet process prior with a Normal * Normal * Inverse Wishart data model\nwhich enables identification of new ambiguous entities who have no records in\nthe training data. For online classification, we use one sweep Gibbs sampler\nwhich is very efficient and effective. As a case study we consider\nbibliographic data in a temporal stream format and disambiguate authors by\npartitioning their papers into homogeneous groups. Our experimental results\ndemonstrate that the proposed method is better than existing methods for\nperforming online name disambiguation task."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4167.0649", 
    "link": "http://arxiv.org/pdf/1607.05806v1", 
    "title": "A Local-Global LDA Model for Discovering Geographical Topics from Social   Media", 
    "arxiv-id": "1607.05806v1", 
    "author": "Yaohui Jin", 
    "publish": "2016-07-20T02:48:15Z", 
    "summary": "Micro-blogging services can track users' geo-locations when users check-in\ntheir places or use geo-tagging which implicitly reveals locations. This \"geo\ntracking\" can help to find topics triggered by some events in certain regions.\nHowever, discovering such topics is very challenging because of the large\namount of noisy messages (e.g. daily conversations). This paper proposes a\nmethod to model geographical topics, which can filter out irrelevant words by\ndifferent weights in the local and global contexts. Our method is based on the\nLatent Dirichlet Allocation (LDA) model but each word is generated from either\na local or a global topic distribution by its generation probabilities. We\nevaluated our model with data collected from Weibo, which is currently the most\npopular micro-blogging service for Chinese. The evaluation results demonstrate\nthat our method outperforms other baseline methods in several metrics such as\nmodel perplexity, two kinds of entropies and KL-divergence of discovered\ntopics."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.4167.0649", 
    "link": "http://arxiv.org/pdf/1607.07504v1", 
    "title": "Verso folio: Diversified Ranking for Large Graphs with Context-Aware   Considerations", 
    "arxiv-id": "1607.07504v1", 
    "author": "George Tsatsanifos", 
    "publish": "2016-07-25T23:30:26Z", 
    "summary": "This work is pertaining to the diversified ranking of web-resources and\ninterconnected documents that rely on a network-like structure, e.g. web-pages.\nA practical example of this would be a query for the k most relevant web-pages\nthat are also in the same time as dissimilar with each other as possible.\nRelevance and dissimilarity are quantified using an aggregation of network\ndistance and context similarity. For example, for a specific configuration of\nthe problem, we might be interested in web-pages that are similar with the\nquery in terms of their textual description but distant from each other in\nterms of the web-graph, e.g. many clicks away. In retrospect, a dearth of work\ncan be found in the literature addressing this problem taking the network\nstructure formed by the document links into consideration.\n  In this work, we propose a hill-climbing approach that is seeded with a\ndocument collection which is generated using greedy heuristics to diversify\ninitially. More importantly, we tackle the problem in the context of web-pages\nwhere there is an underlying network structure connecting the available\ndocuments and resources. This is a significant difference to the majority of\nworks that tackle the problem in terms of either content definitions, or the\ngraph structure of the data, but never addressing both aspects simultaneously.\nTo the best of our knowledge, this is the very first effort that can be found\nto combine both aspects of this important problem in an elegant fashion by also\nallowing a great degree of flexibility on how to configure the trade-offs of\n(i) document relevance over result-items' dissimilarity, and (ii) network\ndistance over content relevance or dissimilarity. Last but not least, we\npresent an extensive evaluation of our methods that demonstrate the\neffectiveness and efficiency thereof."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.2488.7288", 
    "link": "http://arxiv.org/pdf/1607.07904v1", 
    "title": "Beyond Movie Recommendations: Solving the Continuous Cold Start Problem   in E-commerceRecommendations", 
    "arxiv-id": "1607.07904v1", 
    "author": "Djoerd Hiemstra", 
    "publish": "2016-07-26T21:32:14Z", 
    "summary": "Many e-commerce websites use recommender systems or personalized rankers to\npersonalize search results based on their previous interactions. However, a\nlarge fraction of users has no prior inter-actions, making it impossible to use\ncollaborative filtering or rely on user history for personalization. Even the\nmost active users mayvisit only a few times a year and may have volatile needs\nor different personas, making their personal history a sparse and noisy signal\nat best. This paper investigates how, when we cannot rely on the user history,\nthe large scale availability of other user interactions still allows us to\nbuild meaningful profiles from the contextual data and whether such contextual\nprofiles are useful to customize the ranking, exemplified by data from a major\nonline travel agentBooking.com.Our main findings are threefold: First, we\ncharacterize the Continuous Cold Start Problem(CoCoS) from the viewpoint of\ntypical e-commerce applications. Second, as explicit situational con-text is\nnot available in typical real world applications, implicit cues from\ntransaction logs used at scale can capture essential features of situational\ncontext. Third, contextual user profiles can be created offline, resulting in a\nset of smaller models compared to a single huge non-contextual model, making\ncontextual ranking available with negligible CPU and memory footprint. Finally\nwe conclude that, in an online A/B test on live users, our contextual ranker\nin-creased user engagement substantially over a non-contextual base-line, with\nclick-through-rate (CTR) increased by 20%. This clearly demonstrates the value\nof contextual user profiles in a real world application."
},{
    "category": "cs.IR", 
    "doi": "10.13140/RG.2.1.2488.7288", 
    "link": "http://arxiv.org/pdf/1608.00147v1", 
    "title": "Attention Span For Personalisation", 
    "arxiv-id": "1608.00147v1", 
    "author": "Joan Figuerola Hurtado", 
    "publish": "2016-07-30T16:53:05Z", 
    "summary": "A click on an item is arguably the most widely used feature in recommender\nsystems. However, a click is one out of 174 events a browser can trigger. This\npaper presents a framework to effectively collect and store data from event\nstreams. A set of mining methods is provided to extract user engagement\nfeatures such as: attention span, scrolling depth and visible impressions. In\nthis work, we present an experiment where recommendations based on attention\nspan drove 340% higher click-through-rate than clicks."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.00276v2", 
    "title": "Exploring Deep Space: Learning Personalized Ranking in a Semantic Space", 
    "arxiv-id": "1608.00276v2", 
    "author": "Arjen P. de Vries", 
    "publish": "2016-07-31T22:38:46Z", 
    "summary": "Recommender systems leverage both content and user interactions to generate\nrecommendations that fit users' preferences. The recent surge of interest in\ndeep learning presents new opportunities for exploiting these two sources of\ninformation. To recommend items we propose to first learn a user-independent\nhigh-dimensional semantic space in which items are positioned according to\ntheir substitutability, and then learn a user-specific transformation function\nto transform this space into a ranking according to the user's past\npreferences. An advantage of the proposed architecture is that it can be used\nto effectively recommend items using either content that describes the items or\nuser-item ratings. We show that this approach significantly outperforms\nstate-of-the-art recommender systems on the MovieLens 1M dataset."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.00758v1", 
    "title": "Exploiting the Bipartite Structure of Entity Grids for Document   Coherence and Retrieval", 
    "arxiv-id": "1608.00758v1", 
    "author": "Birger Larsen", 
    "publish": "2016-08-02T10:24:11Z", 
    "summary": "Document coherence describes how much sense text makes in terms of its\nlogical organisation and discourse flow. Even though coherence is a relatively\ndifficult notion to quantify precisely, it can be approximated automatically.\nThis type of coherence modelling is not only interesting in itself, but also\nuseful for a number of other text processing tasks, including Information\nRetrieval (IR), where adjusting the ranking of documents according to both\ntheir relevance and their coherence has been shown to increase retrieval\neffectiveness [34,37].\n  The state of the art in unsupervised coherence modelling represents documents\nas bipartite graphs of sentences and discourse entities, and then projects\nthese bipartite graphs into one-mode undirected graphs. However, one-mode\nprojections may incur significant loss of the information present in the\noriginal bipartite structure. To address this we present three novel graph\nmetrics that compute document coherence on the original bipartite graph of\nsentences and entities. Evaluation on standard settings shows that: (i) one of\nour coherence metrics beats the state of the art in terms of coherence\naccuracy; and (ii) all three of our coherence metrics improve retrieval\neffectiveness because, as closer analysis reveals, they capture aspects of\ndocument quality that go undetected by both keyword-based standard ranking and\nby spam filtering. This work contributes document coherence metrics that are\ntheoretically principled, parameter-free, and useful to IR."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.00788v1", 
    "title": "An Improved Multileaving Algorithm for Online Ranker Evaluation", 
    "arxiv-id": "1608.00788v1", 
    "author": "Christina Lioma", 
    "publish": "2016-08-02T12:29:36Z", 
    "summary": "Online ranker evaluation is a key challenge in information retrieval. An\nimportant task in the online evaluation of rankers is using implicit user\nfeedback for inferring preferences between rankers. Interleaving methods have\nbeen found to be efficient and sensitive, i.e. they can quickly detect even\nsmall differences in quality. It has recently been shown that multileaving\nmethods exhibit similar sensitivity but can be more efficient than interleaving\nmethods. This paper presents empirical results demonstrating that existing\nmultileaving methods either do not scale well with the number of rankers, or,\nmore problematically, can produce results which substantially differ from\nevaluation measures like NDCG. The latter problem is caused by the fact that\nthey do not correctly account for the similarities that can occur between\nrankers being multileaved. We propose a new multileaving method for handling\nthis problem and demonstrate that it substantially outperforms existing\nmethods, in some cases reducing errors by as much as 50%."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.01068v1", 
    "title": "Ranking Entity Based on Both of Word Frequency and Word Sematic Features", 
    "arxiv-id": "1608.01068v1", 
    "author": "Zhi-Wei Yan", 
    "publish": "2016-08-03T03:49:54Z", 
    "summary": "Entity search is a new application meeting either precise or vague\nrequirements from the search engines users. Baidu Cup 2016 Challenge just\nprovided such a chance to tackle the problem of the entity search. We achieved\nthe first place with the average MAP scores on 4 tasks including movie, tvShow,\ncelebrity and restaurant. In this paper, we propose a series of similarity\nfeatures based on both of the word frequency features and the word semantic\nfeatures and describe our ranking architecture and experiment details."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.01573v1", 
    "title": "Local Term Weight Models from Power Transformations: Development of   BM25IR: A Best Match Model based on Inverse Regression", 
    "arxiv-id": "1608.01573v1", 
    "author": "Edel Garcia", 
    "publish": "2016-08-04T15:14:04Z", 
    "summary": "In this article we show how power transformations can be used as a common\nframework for the derivation of local term weights. We found that under some\nparametric conditions, BM25 and inverse regression produce equivalent results.\nAs a special case of inverse regression, we show that the largest increment in\nterm weight occurs when a term is mentioned for the second time. A model based\non inverse regression (BM25IR) is presented. Simulations suggest that BM25IR\nworks fairly well for different BM25 parametric conditions and document\nlengths."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.02761v2", 
    "title": "SpEnD: Linked Data SPARQL Endpoints Discovery Using Search Engines", 
    "arxiv-id": "1608.02761v2", 
    "author": "Andreas Kamilaris", 
    "publish": "2016-08-09T11:05:20Z", 
    "summary": "In this study, a novel metacrawling method is proposed for discovering and\nmonitoring linked data sources on the Web. We implemented the method in a\nprototype system, named SPARQL Endpoints Discovery (SpEnD). SpEnD starts with a\n\"search keyword\" discovery process for finding relevant keywords for the linked\ndata domain and specifically SPARQL endpoints. Then, these search keywords are\nutilized to find linked data sources via popular search engines (Google, Bing,\nYahoo, Yandex). By using this method, most of the currently listed SPARQL\nendpoints in existing endpoint repositories, as well as a significant number of\nnew SPARQL endpoints, have been discovered. Finally, we have developed a new\nSPARQL endpoint crawler (SpEC) for crawling and link analysis."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.03905v1", 
    "title": "Using Centroids of Word Embeddings and Word Mover's Distance for   Biomedical Document Retrieval in Question Answering", 
    "arxiv-id": "1608.03905v1", 
    "author": "Ion Androutsopoulos", 
    "publish": "2016-08-12T20:33:54Z", 
    "summary": "We propose a document retrieval method for question answering that represents\ndocuments and questions as weighted centroids of word embeddings and reranks\nthe retrieved documents with a relaxation of Word Mover's Distance. Using\nbiomedical questions and documents from BIOASQ, we show that our method is\ncompetitive with PUBMED. With a top-k approximation, our method is fast, and\neasily portable to other domains and languages."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.04185v3", 
    "title": "Learning to Rank Questions for Community Question Answering with Ranking   SVM", 
    "arxiv-id": "1608.04185v3", 
    "author": "Minh-Le Nguyen", 
    "publish": "2016-08-15T06:16:18Z", 
    "summary": "This paper presents our method to retrieve relevant queries given a new\nquestion in the context of Discovery Challenge: Learning to Re-Ranking\nQuestions for Community Question Answering competition. In order to do that, a\nset of learning to rank methods was investigated to select an appropriate\nmethod. The selected method was optimized on training data by using a search\nstrategy. After optimizing, the method was applied to development and test set.\nResults from the competition indicate that the performance of our method\noutperforms almost participants and show that Ranking SVM is efficient for\nretrieving relevant queries in community question answering."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.05346v1", 
    "title": "Diversified Top-k Similarity Search in Large Attributed Networks", 
    "arxiv-id": "1608.05346v1", 
    "author": "Hong Shen", 
    "publish": "2016-08-18T17:45:45Z", 
    "summary": "Given a large network and a query node, finding its top-k similar nodes is a\nprimitive operation in many graph-based applications. Recently enhancing search\nresults with diversification have received much attention. In this paper, we\nexplore an novel problem of searching for top-k diversified similar nodes in\nattributed networks, with the motivation that modeling diversification in an\nattributed network should consider both the emergence of network links and the\nattribute features of nodes such as user profile information. We formulate this\npractical problem as two optimization problems: the Attributed Coverage\nDiversification (ACD) problem and the r-Dissimilar Attributed Coverage\nDiversification (r-DACD) problem. Based on the submodularity and the\nmonotonicity of ACD, we propose an efficient greedy algorithm achieving a tight\napproximation guarantee of 1-1/e. Unlike the expension based methods only\nconsidering nodes' neighborhood, ACD generalize the definition of\ndiversification to nodes' own features. To capture diversification in\ntopological structure of networks, the r-DACD problem introduce a dissimilarity\nconstraint. We refer to this problem as the Dissimilarity Constrained\nNon-monotone Submodular Maximization (DCNSM) problem. We prove that there is no\nconstant-factor approximation for DCNSM, and also present an efficient greedy\nalgorithms achieving $1/\\rho$ approximation, where $\\rho\\le\\Delta$, $\\Delta$ is\nthe maximum degree of its dissimilarity based graph. To the best of our\nknowledge, it is the first approximation algorithm for the Submodular\nMaximization problem with a distance constraint. The experimental results on\nreal-world attributed network datasets demonstrate the effectiveness of our\nmethods, and confirm that adding dissimilarity constraint can significantly\nenhance the performance of diversification."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2988450.2988457", 
    "link": "http://arxiv.org/pdf/1608.06298v1", 
    "title": "Infusing Collaborative Recommenders with Distributed Representations", 
    "arxiv-id": "1608.06298v1", 
    "author": "Jonathan Gemmell", 
    "publish": "2016-08-22T20:05:01Z", 
    "summary": "Recommender systems assist users in navigating complex information spaces and\nfocus their attention on the content most relevant to their needs. Often these\nsystems rely on user activity or descriptions of the content. Social annotation\nsystems, in which users collaboratively assign tags to items, provide another\nmeans to capture information about users and items. Each of these data sources\nprovides unique benefits, capturing different relationships.\n  In this paper, we propose leveraging multiple sources of data: ratings data\nas users report their affinity toward an item, tagging data as users assign\nannotations to items, and item data collected from an online database. Taken\ntogether, these datasets provide the opportunity to learn rich distributed\nrepresentations by exploiting recent advances in neural network architectures.\nWe first produce representations that subjectively capture interesting\nrelationships among the data. We then empirically evaluate the utility of the\nrepresentations to predict a user's rating on an item and show that it\noutperforms more traditional representations. Finally, we demonstrate that\ntraditional representations can be combined with representations trained\nthrough a neural network to achieve even better results."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2926730", 
    "link": "http://arxiv.org/pdf/1608.06876v1", 
    "title": "Sedano: A News Stream Processor for Business", 
    "arxiv-id": "1608.06876v1", 
    "author": "Roberto Santoro", 
    "publish": "2016-08-24T15:52:19Z", 
    "summary": "We present Sedano, a system for processing and indexing a continuous stream\nof business-related news. Sedano defines pipelines whose stages analyze and\nenrich news items (e.g., newspaper articles and press releases). News data\ncoming from several content sources are stored, processed and then indexed in\norder to be consumed by Atoka, our business intelligence product. Atoka users\ncan retrieve news about specific companies, filtering according to various\nfacets. Sedano features both an entity-linking phase, which finds mentions of\ncompanies in news, and a classification phase, which classifies news according\nto a set of business events. Its flexible architecture allows Sedano to be\ndeployed on commodity machines while being scalable and fault-tolerant."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2926730", 
    "link": "http://arxiv.org/pdf/1608.07102v4", 
    "title": "Multi-behavioral Sequential Prediction with Recurrent Log-bilinear Model", 
    "arxiv-id": "1608.07102v4", 
    "author": "Liang Wang", 
    "publish": "2016-08-25T12:01:18Z", 
    "summary": "With the rapid growth of Internet applications, sequential prediction in\ncollaborative filtering has become an emerging and crucial task. Given the\nbehavioral history of a specific user, predicting his or her next choice plays\na key role in improving various online services. Meanwhile, there are more and\nmore scenarios with multiple types of behaviors, while existing works mainly\nstudy sequences with a single type of behavior. As a widely used approach,\nMarkov chain based models are based on a strong independence assumption. As two\nclassical neural network methods for modeling sequences, recurrent neural\nnetworks cannot well model short-term contexts, and the log-bilinear model is\nnot suitable for long-term contexts. In this paper, we propose a Recurrent\nLog-BiLinear (RLBL) model. It can model multiple types of behaviors in\nhistorical sequences with behavior-specific transition matrices. RLBL applies a\nrecurrent structure for modeling long-term contexts. It models several items in\neach hidden layer and employs position-specific transition matrices for\nmodeling short-term contexts. Moreover, considering continuous time difference\nin behavioral history is a key factor for dynamic prediction, we further extend\nRLBL and replace position-specific transition matrices with time-specific\ntransition matrices, and accordingly propose a Time-Aware Recurrent\nLog-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL\nmodel and TA-RLBL model yield significant improvements over the competitive\ncompared methods on three datasets, i.e., Movielens-1M dataset, Global\nTerrorism Database and Tmall dataset with different numbers of behavior types."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2926730", 
    "link": "http://arxiv.org/pdf/1608.07952v3", 
    "title": "Topical Generalization for Presentation of User Profiles", 
    "arxiv-id": "1608.07952v3", 
    "author": "Emil de Valk", 
    "publish": "2016-08-29T08:47:12Z", 
    "summary": "Fine-grained user profile generation approaches have made it increasingly\nfeasible to display on a profile page in which topics a user has expertise or\ninterest. Earlier work on topical user profiling has been directed at enhancing\nsearch and personalization functionality, but making such profiles useful for\nhuman consumption presents new challenges. With this work, we have taken a\nfirst step toward a semantic layout mode for topical user profiles. We have\ndeveloped a topical generalization approach which finds coherent groups of\ntopics and adds labels to them, based on their association with broader topics\nin the Wikipedia category graph. A nested layout mode, employing topical\ngeneralization, is compared with a simpler flat layout mode in our user study.\nThe results indicate that users favor the nested structure over flat profiles,\nbut tend to overlook the specific topics on the lower level. We propose a third\nlayout mode to address this issue."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2926730", 
    "link": "http://arxiv.org/pdf/1608.08646v1", 
    "title": "LiRa: A New Likelihood-Based Similarity Score for Collaborative   Filtering", 
    "arxiv-id": "1608.08646v1", 
    "author": "Weimin Ouyang", 
    "publish": "2016-08-30T20:21:31Z", 
    "summary": "Recommender system data presents unique challenges to the data mining,\nmachine learning, and algorithms communities. The high missing data rate, in\ncombination with the large scale and high dimensionality that is typical of\nrecommender systems data, requires new tools and methods for efficient data\nanalysis. Here, we address the challenge of evaluating similarity between two\nusers in a recommender system, where for each user only a small set of ratings\nis available. We present a new similarity score, that we call LiRa, based on a\nstatistical model of user similarity, for large-scale, discrete valued data\nwith many missing values. We show that this score, based on a ratio of\nlikelihoods, is more effective at identifying similar users than traditional\nsimilarity scores in user-based collaborative filtering, such as the Pearson\ncorrelation coefficient. We argue that our approach has significant potential\nto improve both accuracy and scalability in collaborative filtering."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.00552v1", 
    "title": "Incorporating Clicks, Attention and Satisfaction into a Search Engine   Result Page Evaluation Model", 
    "arxiv-id": "1609.00552v1", 
    "author": "Maarten de Rijke", 
    "publish": "2016-09-02T11:37:01Z", 
    "summary": "Modern search engine result pages often provide immediate value to users and\norganize information in such a way that it is easy to navigate. The core\nranking function contributes to this and so do result snippets, smart\norganization of result blocks and extensive use of one-box answers or side\npanels. While they are useful to the user and help search engines to stand out,\nsuch features present two big challenges for evaluation. First, the presence of\nsuch elements on a search engine result page (SERP) may lead to the absence of\nclicks, which is, however, not related to dissatisfaction, so-called \"good\nabandonments.\" Second, the non-linear layout and visual difference of SERP\nitems may lead to non-trivial patterns of user attention, which is not captured\nby existing evaluation metrics.\n  In this paper we propose a model of user behavior on a SERP that jointly\ncaptures click behavior, user attention and satisfaction, the CAS model, and\ndemonstrate that it gives more accurate predictions of user actions and\nself-reported satisfaction than existing models based on clicks alone. We use\nthe CAS model to build a novel evaluation metric that can be applied to\nnon-linear SERP layouts and that can account for the utility that users obtain\ndirectly on a SERP. We demonstrate that this metric shows better agreement with\nuser-reported satisfaction than conventional evaluation metrics."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.00969v1", 
    "title": "Adaptive Distributional Extensions to DFR Ranking", 
    "arxiv-id": "1609.00969v1", 
    "author": "Christina Lioma", 
    "publish": "2016-09-04T17:55:39Z", 
    "summary": "Divergence From Randomness (DFR) ranking models assume that informative terms\nare distributed in a corpus differently than non-informative terms. Different\nstatistical models (e.g. Poisson, geometric) are used to model the distribution\nof non-informative terms, producing different DFR models. An informative term\nis then detected by measuring the divergence of its distribution from the\ndistribution of non-informative terms. However, there is little empirical\nevidence that the distributions of non-informative terms used in DFR actually\nfit current datasets. Practically this risks providing a poor separation\nbetween informative and non-informative terms, thus compromising the\ndiscriminative power of the ranking model. We present a novel extension to DFR,\nwhich first detects the best-fitting distribution of non-informative terms in a\ncollection, and then adapts the ranking computation to this best-fitting\ndistribution. We call this model Adaptive Distributional Ranking (ADR) because\nit adapts the ranking to the statistics of the specific dataset being processed\neach time. Experiments on TREC data show ADR to outperform DFR models (and\ntheir extensions) and be comparable in performance to a query likelihood\nlanguage model (LM)."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.01331v1", 
    "title": "Joint Audio-Video Fingerprint Media Retrieval Using Rate-Coverage   Optimization", 
    "arxiv-id": "1609.01331v1", 
    "author": "Zhihai He", 
    "publish": "2016-09-05T21:25:44Z", 
    "summary": "In this work, we propose a joint audio-video fingerprint Automatic Content\nRecognition (ACR) technology for media retrieval. The problem is focused on how\nto balance the query accuracy and the size of fingerprint, and how to allocate\nthe bits of the fingerprint to video frames and audio frames to achieve the\nbest query accuracy. By constructing a novel concept called Coverage, which is\nhighly correlated to the query accuracy, we are able to form a rate-coverage\nmodel to translate the original problem into an optimization problem that can\nbe resolved by dynamic programming. To the best of our knowledge, this is the\nfirst work that uses joint audio-video fingerprint ACR technology for media\nretrieval with a theoretical problem formulation. Experimental results indicate\nthat compared to reference algorithms, the proposed method has up to 25% query\naccuracy improvement while using 60% overall bit-rates, and 25% bit-rate\nreduction while achieving 85% accuracy, and it significantly outperforms the\nsolution with single audio or video source fingerprint."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.01415v1", 
    "title": "A Framework for Scientific Paper Retrieval and Recommender Systems", 
    "arxiv-id": "1609.01415v1", 
    "author": "Natalie Pang", 
    "publish": "2016-09-06T07:36:00Z", 
    "summary": "Information retrieval (IR) and recommender systems (RS) have been employed\nfor addressing search tasks executed during literature review and the overall\nscholarly communication lifecycle. Majority of the studies have concentrated on\nalgorithm design for improving the accuracy and usefulness of these systems.\nContextual elements related to the scholarly tasks have been largely ignored.\nIn this paper, we propose a framework called the Scientific Paper Recommender\nand Retrieval Framework (SPRRF) that combines aspects of user role modeling and\nuser-interface features with IR/RS components. The framework is based on eight\nemergent themes identified from participants feedback in a user evaluation\nstudy conducted with a prototype assistive system. 119 researchers participated\nin the study for evaluating the prototype system that provides recommendations\nfor two literature review and one manuscript writing tasks. This holistic\nframework is meant to guide future studies in this area."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.02451v2", 
    "title": "A Flexible Recommendation System for Cable TV", 
    "arxiv-id": "1609.02451v2", 
    "author": "Francisco M. Couto", 
    "publish": "2016-09-08T14:57:25Z", 
    "summary": "Recommendation systems are being explored by Cable TV operators to improve\nuser satisfaction with services, such as Live TV and Video on Demand (VOD)\nservices. More recently, Catch-up TV has been introduced, allowing users to\nwatch recent broadcast content whenever they want to. These services give users\na large set of options from which they can choose from, creating an information\noverflow problem. Thus, recommendation systems arise as essential tools to\nsolve this problem by helping users in their selection, which increases not\nonly user satisfaction but also user engagement and content consumption. In\nthis paper we present a learning to rank approach that uses contextual\ninformation and implicit feedback to improve recommendation systems for a Cable\nTV operator that provides Live and Catch-up TV services. We compare our\napproach with existing state-of-the-art algorithms and show that our approach\nis superior in accuracy, while maintaining high scores of diversity and\nserendipity."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.02453v2", 
    "title": "A Large-Scale Characterization of User Behaviour in Cable TV", 
    "arxiv-id": "1609.02453v2", 
    "author": "Francisco M. Couto", 
    "publish": "2016-09-08T14:59:49Z", 
    "summary": "Nowadays, Cable TV operators provide their users multiple ways to watch TV\ncontent, such as Live TV and Video on Demand (VOD) services. In the last years,\nCatch-up TV has been introduced, allowing users to watch recent broadcast\ncontent whenever they want to. Understanding how the users interact with such\nservices is important to develop solutions that may increase user satisfaction\n, user engagement and user consumption. In this paper, we characterize, for the\nfirst time, how users interact with a large European Cable TV operator that\nprovides Live TV, Catch-up TV and VOD services. We analyzed many\ncharacteristics, such as the service usage, user engagement, program type,\nprogram genres and time periods. This characterization will help us to have a\ndeeper understanding on how users interact with these different services, that\nmay be used to enhance the recommendation systems of Cable TV providers."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.02687v1", 
    "title": "Extraction of Layout Entities and Sub-layout Query-based Retrieval of   Document Images", 
    "arxiv-id": "1609.02687v1", 
    "author": "Gaurav Harit", 
    "publish": "2016-09-09T08:21:13Z", 
    "summary": "Layouts and sub-layouts constitute an important clue while searching a\ndocument on the basis of its structure, or when textual content is\nunknown/irrelevant. A sub-layout specifies the arrangement of document entities\nwithin a smaller portion of the document. We propose an efficient graph-based\nmatching algorithm, integrated with hash-based indexing, to prune a possibly\nlarge search space. A user can specify a combination of sub-layouts of interest\nusing sketch-based queries. The system supports partial matching for\nunspecified layout entities. We handle cases of segmentation pre-processing\nerrors (for text/non-text blocks) with a symmetry maximization-based strategy,\nand accounting for multiple domain-specific plausible segmentation hypotheses.\nWe show promising results of our system on a database of unstructured entities,\ncontaining 4776 newspaper images."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983829", 
    "link": "http://arxiv.org/pdf/1609.03067v1", 
    "title": "Quantifying the informativeness for biomedical text summarization: An   itemset mining method", 
    "arxiv-id": "1609.03067v1", 
    "author": "Nasser Ghadiri", 
    "publish": "2016-09-10T16:09:28Z", 
    "summary": "Automatic text summarization helps the clinicians and researchers in the\nbiomedical domain to access the intended information efficiently from the large\nvolume of scientific literature and other textual resources. In this paper, we\npropose a summarization method that utilizes domain knowledge and an itemset\nmining approach to generate a conceptual model from a text document. The\ninformativeness of sentences is quantified according to the extent that each\nsentence covers the main subtopics of text. To address the concept-level\nanalysis of text, we map the original document to biomedical concepts using the\nUnified Medical Language System (UMLS). Then, the essential subtopics of text\nare discovered using a data mining technique, namely itemset mining, and the\nconceptual model is constructed. The employed itemset mining algorithm supplies\na set of frequent itemsets containing correlated and recurrent concepts of the\ninput document. The final summary is created by selecting the most related and\ninformative sentences. We evaluated the competency of our itemset-based\nsummarizer using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE)\nmetrics, by performing a set of experiments. The proposed method was compared\nwith the SUMMA, SweSum, AutoSummarize, term-based version of the itemset-based\nsummarizer, and two baselines. The results show that the itemset-based\nsummarizer significantly outperforms the compared competitors and the baseline\nmethods."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2914586.2914611", 
    "link": "http://arxiv.org/pdf/1609.03176v1", 
    "title": "E3 : Keyphrase based News Event Exploration Engine", 
    "arxiv-id": "1609.03176v1", 
    "author": "Dhaval Patel", 
    "publish": "2016-09-11T15:59:35Z", 
    "summary": "This paper presents a novel system E3 for extracting keyphrases from news\ncontent for the purpose of offering the news audience a broad overview of news\nevents, with especially high content volume. Given an input query, E3 extracts\nkeyphrases and enrich them by tagging, ranking and finding role for frequently\nassociated keyphrases. Also, E3 finds the novelty and activeness of keyphrases\nusing news publication date, to identify the most interesting and informative\nkeyphrases."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983728", 
    "link": "http://arxiv.org/pdf/1609.04281v1", 
    "title": "Document Filtering for Long-tail Entities", 
    "arxiv-id": "1609.04281v1", 
    "author": "Maarten de Rijke", 
    "publish": "2016-09-14T14:09:20Z", 
    "summary": "Filtering relevant documents with respect to entities is an essential task in\nthe context of knowledge base construction and maintenance. It entails\nprocessing a time-ordered stream of documents that might be relevant to an\nentity in order to select only those that contain vital information.\nState-of-the-art approaches to document filtering for popular entities are\nentity-dependent: they rely on and are also trained on the specifics of\ndifferentiating features for each specific entity. Moreover, these approaches\ntend to use so-called extrinsic information such as Wikipedia page views and\nrelated entities which is typically only available only for popular head\nentities. Entity-dependent approaches based on such signals are therefore\nill-suited as filtering methods for long-tail entities. In this paper we\npropose a document filtering method for long-tail entities that is\nentity-independent and thus also generalizes to unseen or rarely seen entities.\nIt is based on intrinsic features, i.e., features that are derived from the\ndocuments in which the entities are mentioned. We propose a set of features\nthat capture informativeness, entity-saliency, and timeliness. In particular,\nwe introduce features based on entity aspect similarities, relation patterns,\nand temporal expressions and combine these with standard features for document\nfiltering. Experiments following the TREC KBA 2014 setup on a publicly\navailable dataset show that our model is able to improve the filtering\nperformance for long-tail entities over several baselines. Results of applying\nthe model to unseen entities are promising, indicating that the model is able\nto learn the general characteristics of a vital document. The overall\nperformance across all entities---i.e., not just long-tail entities---improves\nupon the state-of-the-art without depending on any entity-specific training\ndata."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983728", 
    "link": "http://arxiv.org/pdf/1609.04556v1", 
    "title": "Resource Selection for Federated Search on the Web", 
    "arxiv-id": "1609.04556v1", 
    "author": "Djoerd Hiemstra", 
    "publish": "2016-09-15T09:49:27Z", 
    "summary": "A publicly available dataset for federated search reflecting a real web\nenvironment has long been absent, making it difficult for researchers to test\nthe validity of their federated search algorithms for the web setting. We\npresent several experiments and analyses on resource selection on the web using\na recently released test collection containing the results from more than a\nhundred real search engines, ranging from large general web search engines such\nas Google, Bing and Yahoo to small domain-specific engines. First, we\nexperiment with estimating the size of uncooperative search engines on the web\nusing query based sampling and propose a new method using the ClueWeb09\ndataset. We find the size estimates to be highly effective in resource\nselection. Second, we show that an optimized federated search system based on\nsmaller web search engines can be an alternative to a system using large web\nsearch engines. Third, we provide an empirical comparison of several popular\nresource selection methods and find that these methods are not readily suitable\nfor resource selection on the web. Challenges include the sparse resource\ndescriptions and extremely skewed sizes of collections."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1609.06568v1", 
    "title": "On the Mathematical Relationship between Expected n-call@k and the   Relevance vs. Diversity Trade-off", 
    "arxiv-id": "1609.06568v1", 
    "author": "Shengbo Guo", 
    "publish": "2016-09-21T14:11:27Z", 
    "summary": "It has been previously noted that optimization of the n-call@k relevance\nobjective (i.e., a set-based objective that is 1 if at least n documents in a\nset of k are relevant, otherwise 0) encourages more result set diversification\nfor smaller n, but this statement has never been formally quantified. In this\nwork, we explicitly derive the mathematical relationship between expected\nn-call@k and the relevance vs. diversity trade-off --- through fortuitous\ncancellations in the resulting combinatorial optimization, we show the\ntrade-off is a simple and intuitive function of n (notably independent of the\nresult set size k e n), where diversification increases as n approaches 1."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1609.09152v1", 
    "title": "Fusing Similarity Models with Markov Chains for Sparse Sequential   Recommendation", 
    "arxiv-id": "1609.09152v1", 
    "author": "Julian McAuley", 
    "publish": "2016-09-28T23:09:24Z", 
    "summary": "Predicting personalized sequential behavior is a key task for recommender\nsystems. In order to predict user actions such as the next product to purchase,\nmovie to watch, or place to visit, it is essential to take into account both\nlong-term user preferences and sequential patterns (i.e., short-term dynamics).\nMatrix Factorization and Markov Chain methods have emerged as two separate but\npowerful paradigms for modeling the two respectively. Combining these ideas has\nled to unified methods that accommodate long- and short-term dynamics\nsimultaneously by modeling pairwise user-item and item-item interactions.\n  In spite of the success of such methods for tackling dense data, they are\nchallenged by sparsity issues, which are prevalent in real-world datasets. In\nrecent years, similarity-based methods have been proposed for\n(sequentially-unaware) item recommendation with promising results on sparse\ndatasets. In this paper, we propose to fuse such methods with Markov Chains to\nmake personalized sequential recommendations. We evaluate our method, Fossil,\non a variety of large, real-world datasets. We show quantitatively that Fossil\noutperforms alternative algorithms, especially on sparse datasets, and\nqualitatively that it captures personalized dynamics and is able to make\nmeaningful recommendations."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.00735v1", 
    "title": "MatLM: a Matrix Formulation for Probabilistic Language Models", 
    "arxiv-id": "1610.00735v1", 
    "author": "Hongfang Liu", 
    "publish": "2016-10-03T20:36:56Z", 
    "summary": "Probabilistic language models are widely used in Information Retrieval (IR)\nto rank documents by the probability that they generate the query. However, the\nimplementation of the probabilistic representations with programming languages\nthat favor matrix calculations is challenging. In this paper, we utilize matrix\nrepresentations to reformulate the probabilistic language models. The matrix\nrepresentation is a superstructure for the probabilistic language models to\norganize the calculated probabilities and a potential formalism for\nstandardization of language models and for further mathematical analysis. It\nfacilitates implementations by matrix friendly programming languages. In this\npaper, we consider the matrix formulation of conventional language model with\nDirichlet smoothing, and two language models based on Latent Dirichlet\nAllocation (LDA), i.e., LBDM and LDI. We release a Java software\npackage--MatLM--implementing the proposed models. Code is available at:\nhttps://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.01327v2", 
    "title": "A Study of Factuality, Objectivity and Relevance: Three Desiderata in   Large-Scale Information Retrieval?", 
    "arxiv-id": "1610.01327v2", 
    "author": "Yong Huang", 
    "publish": "2016-10-05T09:24:49Z", 
    "summary": "Much of the information processed by Information Retrieval (IR) systems is\nunreliable, biased, and generally untrustworthy [1], [2], [3]. Yet, factuality\n& objectivity detection is not a standard component of IR systems, even though\nit has been possible in Natural Language Processing (NLP) in the last decade.\nMotivated by this, we ask if and how factuality & objectivity detection may\nbenefit IR. We answer this in two parts. First, we use state-of-the-art NLP to\ncompute the probability of document factuality & objectivity in two TREC\ncollections, and analyse its relation to document relevance. We find that\nfactuality is strongly and positively correlated to document relevance, but\nobjectivity is not. Second, we study the impact of factuality & objectivity to\nretrieval effectiveness by treating them as query independent features that we\ncombine with a competitive language modelling baseline. Experiments with 450\nTREC queries show that factuality improves precision >10% over strong\nbaselines, especially for uncurated data used in web search; objectivity gives\nmixed results. An overall clear trend is that document factuality & objectivity\nis much more beneficial to IR when searching uncurated (e.g. web) documents vs.\ncurated (e.g. state documentation and newswire articles). To our knowledge,\nthis is the first study of factuality & objectivity for back-end IR,\ncontributing novel findings about the relation between relevance and\nfactuality/objectivity, and statistically significant gains to retrieval\neffectiveness in the competitive web search task."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.01366v1", 
    "title": "A cumulative approach to quantification for sentiment analysis", 
    "arxiv-id": "1610.01366v1", 
    "author": "Giuseppe Marcone", 
    "publish": "2016-10-05T11:30:28Z", 
    "summary": "We estimate sentiment categories proportions for retrieval within large\nretrieval sets. In general, estimates are produced by counting the\nclassification outcomes and then by adjusting such category sizes taking into\naccount misclassification error matrix. However, both the accuracy of the\nclassifier and the precision of the retrieval produce a large number of errors\nthat makes difficult the application of an aggregative approach to sentiment\nanalysis as a reliable and efficient estimation of proportions for sentiment\ncategories.\n  The challenge for real time analytics during retrieval is thus to overcome\nmisclassification errors, and more importantly, to apply sentiment\nclassification or any other similar post-processing analytics at retrieval\ntime. We present a non-aggregative approach that can be applied to very large\nretrieval sets of queries."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.01901v1", 
    "title": "Discriminative Information Retrieval for Knowledge Discovery", 
    "arxiv-id": "1610.01901v1", 
    "author": "Benjamin Van Durme", 
    "publish": "2016-10-06T15:04:10Z", 
    "summary": "We propose a framework for discriminative Information Retrieval (IR) atop\nlinguistic features, trained to improve the recall of tasks such as answer\ncandidate passage retrieval, the initial step in text-based Question Answering\n(QA). We formalize this as an instance of linear feature-based IR (Metzler and\nCroft, 2007), illustrating how a variety of knowledge discovery tasks are\ncaptured under this approach, leading to a 44% improvement in recall for\ncandidate triage for QA."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.02502v1", 
    "title": "Dynamic Trade-Off Prediction in Multi-Stage Retrieval Systems", 
    "arxiv-id": "1610.02502v1", 
    "author": "Jimmy Lin", 
    "publish": "2016-10-08T08:59:03Z", 
    "summary": "Modern multi-stage retrieval systems are comprised of a candidate generation\nstage followed by one or more reranking stages. In such an architecture, the\nquality of the final ranked list may not be sensitive to the quality of initial\ncandidate pool, especially in terms of early precision. This provides several\nopportunities to increase retrieval efficiency without significantly\nsacrificing effectiveness. In this paper, we explore a new approach to\ndynamically predicting two different parameters in the candidate generation\nstage which can directly affect the overall efficiency and effectiveness of the\nentire system. Previous work exploring this tradeoff has focused on global\nparameter settings that apply to all queries, even though optimal settings vary\nacross queries. In contrast, we propose a technique which makes a parameter\nprediction that maximizes efficiency within a effectiveness envelope on a per\nquery basis, using only static pre-retrieval features. The query-specific\ntradeoff point between effectiveness and efficiency is decided using a\nclassifier cascade that weighs possible efficiency gains against effectiveness\nlosses over a range of possible parameter cutoffs to make the prediction. The\ninteresting twist in our new approach is to train classifiers without requiring\nexplicit relevance judgments. We show that our framework is generalizable by\napplying it to two different retrieval parameters - selecting k in common top-k\nquery retrieval algorithms, and setting a quality threshold, $\\rho$, for\nscore-at-a-time approximate query evaluation algorithms. Experimental results\nshow that substantial efficiency gains are achievable depending on the dynamic\nparameter choice. In addition, our framework provides a versatile tool that can\nbe used to estimate the effectiveness-efficiency tradeoffs that are possible\nbefore selecting and tuning algorithms to make machine learned predictions."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.05819v1", 
    "title": "Finding Representative Points in Multivariate Data Using PCA", 
    "arxiv-id": "1610.05819v1", 
    "author": "Matt Schmill", 
    "publish": "2016-10-18T22:35:49Z", 
    "summary": "The idea of representation has been used in various fields of study from data\nanalysis to political science. In this paper, we define representativeness and\ndescribe a method to isolate data points that can represent the entire data\nset. Also, we show how the minimum set of representative data points can be\ngenerated. We use data from GLOBE (a project to study the effects on Land\nChange based on a set of parameters that include temperature, forest cover,\nhuman population, atmospheric parameters and many other variables) to test &\nvalidate the algorithm. Principal Component Analysis (PCA) is used to reduce\nthe dimensions of the multivariate data set, so that the representative points\ncan be generated efficiently and its Representativeness has been compared\nagainst Random Sampling of points from the data set."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.07328v1", 
    "title": "SSH (Sketch, Shingle, & Hash) for Indexing Massive-Scale Time Series", 
    "arxiv-id": "1610.07328v1", 
    "author": "Anshumali Shrivastava", 
    "publish": "2016-10-24T08:33:44Z", 
    "summary": "Similarity search on time series is a frequent operation in large-scale\ndata-driven applications. Sophisticated similarity measures are standard for\ntime series matching, as they are usually misaligned. Dynamic Time Warping or\nDTW is the most widely used similarity measure for time series because it\ncombines alignment and matching at the same time. However, the alignment makes\nDTW slow. To speed up the expensive similarity search with DTW, branch and\nbound based pruning strategies are adopted. However, branch and bound based\npruning are only useful for very short queries (low dimensional time series),\nand the bounds are quite weak for longer queries. Due to the loose bounds\nbranch and bound pruning strategy boils down to a brute-force search.\n  To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), an\nefficient and approximate hashing scheme which is much faster than the\nstate-of-the-art branch and bound searching technique: the UCR suite. SSH uses\na novel combination of sketching, shingling and hashing techniques to produce\n(probabilistic) indexes which align (near perfectly) with DTW similarity\nmeasure. The generated indexes are then used to create hash buckets for\nsub-linear search. Our results show that SSH is very effective for longer time\nsequence and prunes around 95% candidates, leading to the massive speedup in\nsearch with DTW. Empirical results on two large-scale benchmark time series\ndata show that our proposed method can be around 20 times faster than the\nstate-of-the-art package (UCR suite) without any significant loss in accuracy."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.08136v1", 
    "title": "Learning to Match Using Local and Distributed Representations of Text   for Web Search", 
    "arxiv-id": "1610.08136v1", 
    "author": "Nick Craswell", 
    "publish": "2016-10-26T01:10:05Z", 
    "summary": "Models such as latent semantic analysis and those based on neural embeddings\nlearn distributed representations of text, and match the query against the\ndocument in the latent semantic space. In traditional information retrieval\nmodels, on the other hand, terms have discrete or local representations, and\nthe relevance of a document is determined by the exact matches of query terms\nin the body text. We hypothesize that matching with distributed representations\ncomplements matching with traditional local representations, and that a\ncombination of the two is favorable. We propose a novel document ranking model\ncomposed of two separate deep neural networks, one that matches the query and\nthe document using a local representation, and another that matches the query\nand the document using learned distributed representations. The two networks\nare jointly trained as part of a single neural network. We show that this\ncombination or `duet' performs significantly better than either neural network\nindividually on a Web page ranking task, and also significantly outperforms\ntraditional baselines and other recently proposed models based on neural\nnetworks."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2348283.2348497", 
    "link": "http://arxiv.org/pdf/1610.08442v1", 
    "title": "Inferring individual attributes from search engine queries and auxiliary   information", 
    "arxiv-id": "1610.08442v1", 
    "author": "Elad Yom-Tov", 
    "publish": "2016-10-26T18:08:16Z", 
    "summary": "Internet data has surfaced as a primary source for investigation of different\naspects of human behavior. A crucial step in such studies is finding a suitable\ncohort (i.e., a set of users) that shares a common trait of interest to\nresearchers. However, direct identification of users sharing this trait is\noften impossible, as the data available to researchers is usually anonymized to\npreserve user privacy. To facilitate research on specific topics of interest,\nespecially in medicine, we introduce an algorithm for identifying a trait of\ninterest in anonymous users. We illustrate how a small set of labeled examples,\ntogether with statistical information about the entire population, can be\naggregated to obtain labels on unseen examples. We validate our approach using\nlabeled data from the political domain.\n  We provide two applications of the proposed algorithm to the medical domain.\nIn the first, we demonstrate how to identify users whose search patterns\nindicate they might be suffering from certain types of cancer. In the second,\nwe detail an algorithm to predict the distribution of diseases given their\nincidence in a subset of the population at study, making it possible to predict\ndisease spread from partial epidemiological data."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983815", 
    "link": "http://arxiv.org/pdf/1610.10001v1", 
    "title": "Off the Beaten Path: Let's Replace Term-Based Retrieval with k-NN Search", 
    "arxiv-id": "1610.10001v1", 
    "author": "Eric Nyberg", 
    "publish": "2016-10-31T16:27:08Z", 
    "summary": "Retrieval pipelines commonly rely on a term-based search to obtain candidate\nrecords, which are subsequently re-ranked. Some candidates are missed by this\napproach, e.g., due to a vocabulary mismatch. We address this issue by\nreplacing the term-based search with a generic k-NN retrieval algorithm, where\na similarity function can take into account subtle term associations. While an\nexact brute-force k-NN search using this similarity function is slow, we\ndemonstrate that an approximate algorithm can be nearly two orders of magnitude\nfaster at the expense of only a small loss in accuracy. A retrieval pipeline\nusing an approximate k-NN search can be more effective and efficient than the\nterm-based pipeline. This opens up new possibilities for designing effective\nretrieval pipelines. Our software (including data-generating code) and\nderivative data based on the Stack Overflow collection is available online."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983815", 
    "link": "http://arxiv.org/pdf/1611.00558v1", 
    "title": "Online bagging for recommendation with incremental matrix factorization", 
    "arxiv-id": "1611.00558v1", 
    "author": "Jo\u00e3o Gama", 
    "publish": "2016-11-02T11:52:49Z", 
    "summary": "Online recommender systems often deal with continuous, potentially fast and\nunbounded flows of data. Ensemble methods for recommender systems have been\nused in the past in batch algorithms, however they have never been studied with\nincremental algorithms, that are capable of processing those data streams on\nthe fly. We propose online bagging, using an incremental matrix factorization\nalgorithm for positive-only data streams. Using prequential evaluation, we show\nthat bagging is able to improve accuracy more than 35% over the baseline with\nsmall computational overhead."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983815", 
    "link": "http://arxiv.org/pdf/1611.01228v1", 
    "title": "URL ordering policies for distributed crawlers: a review", 
    "arxiv-id": "1611.01228v1", 
    "author": "Ashutosh Dixit", 
    "publish": "2015-12-30T10:11:56Z", 
    "summary": "With the increase in size of web, the information is also spreading at large\nscale. Search Engines are the medium to access this information. Crawler is the\nmodule of search engine which is responsible for download the web pages. In\norder to download the fresh information and get the database rich, crawler\nshould crawl the web in some order. This is called as ordering of URLs. URL\nordering should be done in efficient and effective manner in order to crawl the\nweb in proficient manner. In this paper, a survey is done on some existing\nmethods of URL ordering and at the end of this paper comparison is also carried\nout among them."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2983323.2983815", 
    "link": "http://arxiv.org/pdf/1611.01974v2", 
    "title": "Item-to-item recommendation based on Contextual Fisher Information", 
    "arxiv-id": "1611.01974v2", 
    "author": "Andr\u00e1s Bencz\u00far", 
    "publish": "2016-11-07T10:28:06Z", 
    "summary": "Web recommendation services bear great importance in e-commerce, as they aid\nthe user in navigating through the items that are most relevant to her needs.\nIn a typical Web site, long history of previous activities or purchases by the\nuser is rarely available. Hence in most cases, recommenders propose items that\nare similar to the most recent ones viewed in the current user session. The\ncorresponding task is called session based item-to-item recommendation. For\nfrequent items, it is easy to present item-to-item recommendations by \"people\nwho viewed this, also viewed\" lists. However, most of the items belong to the\nlong tail, where previous actions are sparsely available. Another difficulty is\nthe so-called cold start problem, when the item has recently appeared and had\nno time yet to accumulate sufficient number of transactions. In order to\nrecommend a next item in a session in sparse or cold start situations, we also\nhave to incorporate item similarity models. In this paper we describe a\nprobabilistic similarity model based on Random Fields to approximate\nitem-to-item transition probabilities. We give a generative model for the item\ninteractions based on arbitrary distance measures over the items including\nexplicit, implicit ratings and external metadata. The model may change in time\nto fit better recent events and recommend the next item based on the updated\nFisher Information. Our new model outperforms both simple similarity baseline\nmethods and recent item-to-item recommenders, under several different\nperformance metrics and publicly available data sets. We reach significant\ngains in particular for recommending a new item following a rare item."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2695664.2695761", 
    "link": "http://arxiv.org/pdf/1611.03350v1", 
    "title": "On the Impact of Entity Linking in Microblog Real-Time Filtering", 
    "arxiv-id": "1611.03350v1", 
    "author": "Diego Marcheggiani", 
    "publish": "2016-11-10T15:40:14Z", 
    "summary": "Microblogging is a model of content sharing in which the temporal locality of\nposts with respect to important events, either of foreseeable or unforeseeable\nnature, makes applica- tions of real-time filtering of great practical\ninterest. We propose the use of Entity Linking (EL) in order to improve the\nretrieval effectiveness, by enriching the representation of microblog posts and\nfiltering queries. EL is the process of recognizing in an unstructured text the\nmention of relevant entities described in a knowledge base. EL of short pieces\nof text is a difficult task, but it is also a scenario in which the information\nEL adds to the text can have a substantial impact on the retrieval process. We\nimplement a start-of-the-art filtering method, based on the best systems from\nthe TREC Microblog track realtime adhoc retrieval and filtering tasks , and\nextend it with a Wikipedia-based EL method. Results show that the use of EL\nsignificantly improves over non-EL based versions of the filtering methods."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2695664.2695761", 
    "link": "http://arxiv.org/pdf/1611.06668v1", 
    "title": "A Visual and Textual Recurrent Neural Network for Sequential Prediction", 
    "arxiv-id": "1611.06668v1", 
    "author": "Liang Wang", 
    "publish": "2016-11-21T07:00:51Z", 
    "summary": "Sequential prediction is a fundamental task for Web applications. Due to the\ninsufficiency of user feedbacks, sequential prediction usually suffers from the\ncold start problem. There are two kinds of popular approaches based on matrix\nfactorization (MF) and Markov chains (MC) for item prediction. MF methods\nfactorize the user-item matrix to learn general tastes of users. MC methods\npredict the next behavior based on recent behaviors. However, they have\nlimitations. MF methods can merge additional information to address cold start\nbut could not capture dynamic properties of user's interest, and MC based\nsequential methods have difficulty in addressing cold start and has a strong\nMarkov assumption that the next state only depends on the last state. In this\nwork, to deal with the cold start problem of sequential prediction, we propose\na RNN model adopting visual and textual content of items, which is named as\n$\\mathbf{V}$isual and $\\mathbf{T}$extual $\\mathbf{R}$ecurrent $\\mathbf{N}$eural\n$\\mathbf{N}$etwork ($\\mathbf{VT}$-$\\mathbf{RNN}$). We can simultaneously learn\nthe sequential latent vectors that dynamically capture the user's interest, as\nwell as content-based representations that contribute to address the cold\nstart. Experiments on two real-world datasets show that our proposed VT-RNN\nmodel can effectively generate the personalized ranking list and significantly\nalleviate the cold start problem."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2695664.2695761", 
    "link": "http://arxiv.org/pdf/1611.06792v3", 
    "title": "Neural Information Retrieval: A Literature Review", 
    "arxiv-id": "1611.06792v3", 
    "author": "Matthew Lease", 
    "publish": "2016-11-18T03:44:34Z", 
    "summary": "A recent \"third wave\" of Neural Network (NN) approaches now delivers\nstate-of-the-art performance in many machine learning tasks, spanning speech\nrecognition, computer vision, and natural language processing. Because these\nmodern NNs often comprise multiple interconnected layers, this new NN research\nis often referred to as deep learning. Stemming from this tide of NN work, a\nnumber of researchers have recently begun to investigate NN approaches to\nInformation Retrieval (IR). While deep NNs have yet to achieve the same level\nof success in IR as seen in other areas, the recent surge of interest and work\nin NNs for IR suggest that this state of affairs may be quickly changing. In\nthis work, we survey the current landscape of Neural IR research, paying\nspecial attention to the use of learned representations of queries and\ndocuments (i.e., neural embeddings). We highlight the successes of neural IR\nthus far, catalog obstacles to its wider adoption, and suggest potentially\npromising directions for future research."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2695664.2695761", 
    "link": "http://arxiv.org/pdf/1611.09496v1", 
    "title": "A Graph-based Push Service Platform", 
    "arxiv-id": "1611.09496v1", 
    "author": "Xiuqiang He", 
    "publish": "2016-11-29T05:36:41Z", 
    "summary": "It is well known that learning customers' preference and making\nrecommendations to them from today's information-exploded environment is\ncritical and non-trivial in an on-line system. There are two different modes of\nrecommendation systems, namely pull-mode and push-mode. The majority of the\nrecommendation systems are pull-mode, which recommend items to users only when\nand after users enter Application Market. While push-mode works more actively\nto enhance or re-build connection between Application Market and users. As one\nof the most successful phone manufactures,both the number of users and apps\nincrease dramatically in Huawei Application Store (also named Hispace Store),\nwhich has approximately 0.3 billion registered users and 1.2 million apps until\n2016 and whose number of users is growing with high-speed. For the needs of\nreal scenario, we establish a Push Service Platform (shortly, PSP) to discover\nthe target user group automatically from web-scale user operation log data with\nan additional small set of labelled apps (usually around 10 apps),in Hispace\nStore. As presented in this work,PSP includes distributed storage layer,\napplication layer and evaluation layer. In the application layer, we design a\npractical graph-based algorithm (named A-PARW) for user group discovery, which\nis an approximate version of partially absorbing random walk. Based on I mode\nof A-PARW, the effectiveness of our system is significantly improved, compared\nto the predecessor to presented system, which uses Personalized Pagerank in its\napplication layer."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2695664.2695761", 
    "link": "http://arxiv.org/pdf/1611.10248v1", 
    "title": "Assessing pattern recognition or labeling in streams of temporal data", 
    "arxiv-id": "1611.10248v1", 
    "author": "Pierre-Fran\u00e7ois Marteau", 
    "publish": "2016-11-30T16:08:40Z", 
    "summary": "In the data deluge context, pattern recognition or labeling in streams is\nbecoming quite an essential and pressing task as data flows inside always\nbigger streams. The assessment of such tasks is not so easy when dealing with\ntemporal data, namely patterns that have a duration (a beginning and an end\ntime-stamp). This paper details an approach based on an editing distance to\nfirst align a sequence of labeled temporal segments with a ground truth\nsequence, and then, by back-tracing an optimal alignment path, to provide a\nconfusion matrix at the label level. From this confusion matrix, standard\nevaluation measures can easily be derived as well as other measures such as the\n\"latency\" that can be quite important in (early) pattern detection\napplications."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.00260v1", 
    "title": "Factory of realities: on the emergence of virtual spatiotemporal   structures", 
    "arxiv-id": "1612.00260v1", 
    "author": "Roman Zapatrin", 
    "publish": "2016-11-30T08:41:04Z", 
    "summary": "The ubiquitous nature of modern Information Retrieval and Virtual World give\nrise to new realities. To what extent are these \"realities\" real? Which\n\"physics\" should be applied to quantitatively describe them? In this essay I\ndwell on few examples. The first is Adaptive neural networks, which are not\nnetworks and not neural, but still provide service similar to classical ANNs in\nextended fashion. The second is the emergence of objects looking like\nEinsteinian spacetime, which describe the behavior of an Internet surfer like\ngeodesic motion. The third is the demonstration of nonclassical and even\nstronger-than-quantum probabilities in Information Retrieval, their use.\n  Immense operable datasets provide new operationalistic environments, which\nbecome to greater and greater extent \"realities\". In this essay, I consider the\noverall Information Retrieval process as an objective physical process,\nrepresenting it according to Melucci metaphor in terms of physical-like\nexperiments. Various semantic environments are treated as analogs of various\nrealities. The readers' attention is drawn to topos approach to physical\ntheories, which provides a natural conceptual and technical framework to cope\nwith the new emerging realities."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.03316v1", 
    "title": "Label Visualization and Exploration in IR", 
    "arxiv-id": "1612.03316v1", 
    "author": "Omar Alonso", 
    "publish": "2016-12-10T16:33:06Z", 
    "summary": "There is a renaissance in visual analytics systems for data analysis and\nsharing, in particular, in the current wave of big data applications. We\nintroduce RAVE, a prototype that automates the generation of an interface that\nuses facets and visualization techniques for exploring and analyzing relevance\nassessments data sets collected via crowdsourcing. We present a technical\ndescription of the main components and demonstrate its use."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.04883v1", 
    "title": "A Graph Summarization: A Survey", 
    "arxiv-id": "1612.04883v1", 
    "author": "Danai Koutra", 
    "publish": "2016-12-14T23:39:45Z", 
    "summary": "While advances in computing resources have made processing enormous amounts\nof data possible, human ability to identify patterns in such data has not\nscaled accordingly. Thus, efficient computational methods for condensing and\nsimplifying data are becoming vital for extracting actionable insights. In\nparticular, while data summarization techniques have been studied extensively,\nonly recently has summarizing interconnected data, or graphs, become popular.\nThis survey is a structured, comprehensive overview of the state-of-the-art\nmethods for summarizing graph data. We first broach the motivation behind and\nthe challenges of graph summarization. We then categorize summarization\napproaches by the type of graphs taken as input and further organize each\ncategory by core methodology. Finally, we discuss applications of summarization\non real-world graphs and conclude by describing some open problems in the\nfield."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.06136v1", 
    "title": "Data-Driven Relevance Judgments for Ranking Evaluation", 
    "arxiv-id": "1612.06136v1", 
    "author": "Jo\u00e3o Vinagre", 
    "publish": "2016-12-19T11:48:53Z", 
    "summary": "Ranking evaluation metrics are a fundamental element of design and\nimprovement efforts in information retrieval. We observe that most popular\nmetrics disregard information portrayed in the scores used to derive rankings,\nwhen available. This may pose a numerical scaling problem, causing an under- or\nover-estimation of the evaluation depending on the degree of divergence between\nthe scores of ranked items. The purpose of this work is to propose a principled\nway of quantifying multi-graded relevance judgments of items and enable a more\naccurate penalization of ordering errors in rankings. We propose a data-driven\ngeneration of relevance functions based on the degree of the divergence amongst\na set of items' scores and its application in the evaluation metric Normalized\nDiscounted Cumulative Gain (nDCG). We use synthetic data to demonstrate the\ninterest of our proposal and a combination of data on news items from Google\nNews and their respective popularity in Twitter to show its performance in\ncomparison to the standard nDCG. Results show that our proposal is capable of\nproviding a more fine-grained evaluation of rankings when compared to the\nstandard nDCG, and that the latter frequently under- or over-estimates its\nevaluation scores in light of the divergence of items' scores."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.08391v1", 
    "title": "Audio-based Distributional Semantic Models for Music Auto-tagging and   Similarity Measurement", 
    "arxiv-id": "1612.08391v1", 
    "author": "Alexandros Potamianos", 
    "publish": "2016-12-26T14:35:51Z", 
    "summary": "The recent development of Audio-based Distributional Semantic Models (ADSMs)\nenables the computation of audio and lexical vector representations in a joint\nacoustic-semantic space. In this work, these joint representations are applied\nto the problem of automatic tag generation. The predicted tags together with\ntheir corresponding acoustic representation are exploited for the construction\nof acoustic-semantic clip embeddings. The proposed algorithms are evaluated on\nthe task of similarity measurement between music clips. Acoustic-semantic\nmodels are shown to outperform the state-of-the-art for this task and produce\nhigh quality tags for audio/music clips."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1612.09062v1", 
    "title": "Condensedly: comprehending article contents through condensed texts", 
    "arxiv-id": "1612.09062v1", 
    "author": "Jung-Hsien Chiang", 
    "publish": "2016-12-29T07:56:09Z", 
    "summary": "Summary: Abstracts in biomedical articles can provide a quick overview of the\narticles but detailed information cannot be obtained without reading full-text\ncontents. Full-text articles certainly generate more information and contents;\nhowever, accessing full-text documents is usually time consuming. Condensedly\nis a web-based application, which provides readers an easy and efficient way to\naccess full-text paragraphs using sentences in abstracts as fishing bait to\nretrieve the big fish reside in full-text. Condensedly is based on the\nparagraph ranking algorithm, which evaluates and ranks full-text paragraphs\nbased on their association scores with sentences in abstracts.\n  Availability: http://140.116.247.185/~research/Condensedly"
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1701.00324v1", 
    "title": "Patent Retrieval: A Literature Review", 
    "arxiv-id": "1701.00324v1", 
    "author": "Wlodek Zadrozny", 
    "publish": "2017-01-02T06:32:38Z", 
    "summary": "With the ever increasing number of filed patent applications every year, the\nneed for effective and efficient systems for managing such tremendous amounts\nof data becomes inevitably important. Patent Retrieval (PR) is considered is\nthe pillar of almost all patent analysis tasks. PR is a subfield of Information\nRetrieval (IR) which is concerned with developing techniques and methods that\neffectively and efficiently retrieve relevant patent documents in response to a\ngiven search request. In this paper we present a comprehensive review on PR\nmethods and approaches. It is clear that, recent successes and maturity in IR\napplications such as Web search can not be transferred directly to PR without\ndeliberate domain adaptation and customization. Furthermore, state-of-the-art\nperformance in automatic PR is still around average. These observations\nmotivates the need for interactive search tools which provide cognitive\nassistance to patent professionals with minimal effort. These tools must also\nbe developed in hand with patent professionals considering their practices and\nexpectations. We additionally touch on related tasks to PR such as patent\nvaluation, litigation, licensing, and highlight potential opportunities and\nopen directions for computational scientists in these domains."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1701.01250v1", 
    "title": "A Probabilistic View of Neighborhood-based Recommendation Methods", 
    "arxiv-id": "1701.01250v1", 
    "author": "Qiang Tang", 
    "publish": "2017-01-05T08:53:02Z", 
    "summary": "Probabilistic graphic model is an elegant framework to compactly present\ncomplex real-world observations by modeling uncertainty and logical flow\n(conditionally independent factors). In this paper, we present a probabilistic\nframework of neighborhood-based recommendation methods (PNBM) in which\nsimilarity is regarded as an unobserved factor. Thus, PNBM leads the estimation\nof user preference to maximizing a posterior over similarity. We further\nintroduce a novel multi-layer similarity descriptor which models and learns the\njoint influence of various features under PNBM, and name the new framework\nMPNBM. Empirical results on real-world datasets show that MPNBM allows very\naccurate estimation of user preferences."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1701.01276v1", 
    "title": "Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired   Hashtag Recommendation Approach", 
    "arxiv-id": "1701.01276v1", 
    "author": "Elisabeth Lex", 
    "publish": "2017-01-05T11:07:16Z", 
    "summary": "Hashtags have become a powerful tool in social platforms such as Twitter to\ncategorize and search for content, and to spread short messages across members\nof the social network. In this paper, we study temporal hashtag usage practices\nin Twitter with the aim of designing a cognitive-inspired hashtag\nrecommendation algorithm we call BLLi,s. Our main idea is to incorporate the\neffect of time on (i) individual hashtag reuse (i.e., reusing own hashtags),\nand (ii) social hashtag reuse (i.e., reusing hashtags, which has been\npreviously used by a followee) into a predictive model. For this, we turn to\nthe Base-Level Learning (BLL) equation from the cognitive architecture ACT-R,\nwhich accounts for the time-dependent decay of item exposure in human memory.\nWe validate BLLi,s using two crawled Twitter datasets in two evaluation\nscenarios: firstly, only temporal usage patterns of past hashtag assignments\nare utilized and secondly, these patterns are combined with a content-based\nanalysis of the current tweet. In both scenarios, we find not only that\ntemporal effects play an important role for both individual and social hashtag\nreuse but also that BLLi,s provides significantly better prediction accuracy\nand ranking results than current state-of-the-art hashtag recommendation\nmethods."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1701.01737v1", 
    "title": "Spotting Information biases in Chinese and Western Media", 
    "arxiv-id": "1701.01737v1", 
    "author": "Yumeng Qin", 
    "publish": "2017-01-06T19:04:34Z", 
    "summary": "Newswire and Social Media are the major sources of information in our time.\nWhile the topical demographic of Western Media was subjects of studies in the\npast, less is known about Chinese Media. In this paper, we apply event\ndetection and tracking technology to examine the information overlap and\ndifferences between Chinese and Western - Traditional Media and Social Media.\nOur experiments reveal a biased interest of China towards the West, which\nbecomes particularly apparent when comparing the interest in celebrities."
},{
    "category": "cs.IR", 
    "doi": "10.1142/9781783268320_0008", 
    "link": "http://arxiv.org/pdf/1701.02021v1", 
    "title": "Toward Active Learning in Cross-domain Recommender Systems", 
    "arxiv-id": "1701.02021v1", 
    "author": "Paolo Cremonesi", 
    "publish": "2017-01-08T21:47:45Z", 
    "summary": "One of the main challenges in Recommender Systems (RSs) is the New User\nproblem which happens when the system has to generate personalised\nrecommendations for a new user whom the system has no information about. Active\nLearning tries to solve this problem by acquiring user preference data with the\nmaximum quality, and with the minimum acquisition cost. Although there are\nvariety of works in active learning for RSs research area, almost all of them\nhave focused only on the single-domain recommendation scenario. However,\nseveral real-world RSs operate in the cross-domain scenario, where the system\ngenerates recommendations in the target domain by exploiting user preferences\nin both the target and auxiliary domains. In such a scenario, the performance\nof active learning strategies can be significantly influenced and typical\nactive learning strategies may fail to perform properly. In this paper, we\naddress this limitation, by evaluating active learning strategies in a novel\nevaluation framework, explicitly suited for the cross-domain recommendation\nscenario. We show that having access to the preferences of the users in the\nauxiliary domain may have a huge impact on the performance of active learning\nstrategies w.r.t. the classical, single-domain scenario."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3020165.3022129", 
    "link": "http://arxiv.org/pdf/1701.02050v1", 
    "title": "Personalised Query Suggestion for Intranet Search with Temporal User   Profiling", 
    "arxiv-id": "1701.02050v1", 
    "author": "Dawei Song", 
    "publish": "2017-01-09T01:41:13Z", 
    "summary": "Recent research has shown the usefulness of using collective user interaction\ndata (e.g., query logs) to recommend query modification suggestions for\nIntranet search. However, most of the query suggestion approaches for Intranet\nsearch follow an \"one size fits all\" strategy, whereby different users who\nsubmit an identical query would get the same query suggestion list. This is\nproblematic, as even with the same query, different users may have different\ntopics of interest, which may change over time in response to the user's\ninteraction with the system. We address the problem by proposing a personalised\nquery suggestion framework for Intranet search. For each search session, we\nconstruct two temporal user profiles: a click user profile using the user's\nclicked documents and a query user profile using the user's submitted queries.\nWe then use the two profiles to re-rank the non-personalised query suggestion\nlist returned by a state-of-the-art query suggestion method for Intranet\nsearch. Experimental results on a large-scale query logs collection show that\nour personalised framework significantly improves the quality of suggested\nqueries."
},{
    "category": "cs.IR", 
    "doi": "10.1145/3020165.3022129", 
    "link": "http://arxiv.org/pdf/1701.03939v1", 
    "title": "Semantic Annotation for Microblog Topics Using Wikipedia Temporal   Information", 
    "arxiv-id": "1701.03939v1", 
    "author": "Robert J\u00e4schke", 
    "publish": "2017-01-14T16:11:50Z", 
    "summary": "Trending topics in microblogs such as Twitter are valuable resources to\nunderstand social aspects of real-world events. To enable deep analyses of such\ntrends, semantic annotation is an effective approach; yet the problem of\nannotating microblog trending topics is largely unexplored by the research\ncommunity. In this work, we tackle the problem of mapping trending Twitter\ntopics to entities from Wikipedia. We propose a novel model that complements\ntraditional text-based approaches by rewarding entities that exhibit a high\ntemporal correlation with topics during their burst time period. By exploiting\ntemporal information from the Wikipedia edit history and page view logs, we\nhave improved the annotation performance by 17-28\\%, as compared to the\ncompetitive baselines."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.03942v1", 
    "title": "Can We Find Documents in Web Archives without Knowing their Contents?", 
    "arxiv-id": "1701.03942v1", 
    "author": "Wolfgang Nejdl", 
    "publish": "2017-01-14T16:23:09Z", 
    "summary": "Recent advances of preservation technologies have led to an increasing number\nof Web archive systems and collections. These collections are valuable to\nexplore the past of the Web, but their value can only be uncovered with\neffective access and exploration mechanisms. Ideal search and rank- ing methods\nmust be robust to the high redundancy and the temporal noise of contents, as\nwell as scalable to the huge amount of data archived. Despite several attempts\nin Web archive search, facilitating access to Web archive still remains a\nchallenging problem.\n  In this work, we conduct a first analysis on different ranking strategies\nthat exploit evidences from metadata instead of the full content of documents.\nWe perform a first study to compare the usefulness of non-content evidences to\nWeb archive search, where the evidences are mined from the metadata of file\nheaders, links and URL strings only. Based on these findings, we propose a\nsimple yet surprisingly effective learning model that combines multiple\nevidences to distinguish \"good\" from \"bad\" search results. We conduct empirical\nexperiments quantitatively as well as qualitatively to confirm the validity of\nour proposed method, as a first step towards better ranking in Web archives\ntaking meta- data into account."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.04273v1", 
    "title": "Hierarchical Re-estimation of Topic Models for Measuring Topical   Diversity", 
    "arxiv-id": "1701.04273v1", 
    "author": "Maarten de Rijke", 
    "publish": "2017-01-16T12:59:47Z", 
    "summary": "A high degree of topical diversity is often considered to be an important\ncharacteristic of interesting text documents. A recent proposal for measuring\ntopical diversity identifies three elements for assessing diversity: words,\ntopics, and documents as collections of words. Topic models play a central role\nin this approach. Using standard topic models for measuring diversity of\ndocuments is suboptimal due to generality and impurity. General topics only\ninclude common information from a background corpus and are assigned to most of\nthe documents in the collection. Impure topics contain words that are not\nrelated to the topic; impurity lowers the interpretability of topic models and\nimpure topics are likely to get assigned to documents erroneously. We propose a\nhierarchical re-estimation approach for topic models to combat generality and\nimpurity; the proposed approach operates at three levels: words, topics, and\ndocuments. Our re-estimation approach for measuring documents' topical\ndiversity outperforms the state of the art on PubMed dataset which is commonly\nused for diversity experiments."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.04931v1", 
    "title": "Characterizing Linguistic Attributes for Automatic Classification of   Intent Based Racist/Radicalized Posts on Tumblr Micro-Blogging Website", 
    "arxiv-id": "1701.04931v1", 
    "author": "Ashish Sureka", 
    "publish": "2017-01-18T03:17:20Z", 
    "summary": "Research shows that many like-minded people use popular microblogging\nwebsites for posting hateful speech against various religions and race.\nAutomatic identification of racist and hate promoting posts is required for\nbuilding social media intelligence and security informatics based solutions.\nHowever, just keyword spotting based techniques cannot be used to accurately\nidentify the intent of a post. In this paper, we address the challenge of the\npresence of ambiguity in such posts by identifying the intent of author. We\nconduct our study on Tumblr microblogging website and develop a cascaded\nensemble learning classifier for identifying the posts having racist or\nradicalized intent. We train our model by identifying various semantic,\nsentiment and linguistic features from free-form text. Our experimental results\nshows that the proposed approach is effective and the emotion tone, social\ntendencies, language cues and personality traits of a narrative are\ndiscriminatory features for identifying the racist intent behind a post."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.04934v1", 
    "title": "Investigating the Application of Common-Sense Knowledge-Base for   Identifying Term Obfuscation in Adversarial Communication", 
    "arxiv-id": "1701.04934v1", 
    "author": "Ashish Sureka", 
    "publish": "2017-01-18T03:36:33Z", 
    "summary": "Word obfuscation or substitution means replacing one word with another word\nin a sentence to conceal the textual content or communication. Word obfuscation\nis used in adversarial communication by terrorist or criminals for conveying\ntheir messages without getting red-flagged by security and intelligence\nagencies intercepting or scanning messages (such as emails and telephone\nconversations). ConceptNet is a freely available semantic network represented\nas a directed graph consisting of nodes as concepts and edges as assertions of\ncommon sense about these concepts. We present a solution approach exploiting\nvast amount of semantic knowledge in ConceptNet for addressing the technically\nchallenging problem of word substitution in adversarial communication. We frame\nthe given problem as a textual reasoning and context inference task and utilize\nConceptNet's natural-language-processing tool-kit for determining word\nsubstitution. We use ConceptNet to compute the conceptual similarity between\nany two given terms and define a Mean Average Conceptual Similarity (MACS)\nmetric to identify out-of-context terms. The test-bed to evaluate our proposed\napproach consists of Enron email dataset (having over 600000 emails generated\nby 158 employees of Enron Corporation) and Brown corpus (totaling about a\nmillion words drawn from a wide variety of sources). We implement word\nsubstitution techniques used by previous researches to generate a test dataset.\nWe conduct a series of experiments consisting of word substitution methods used\nin the past to evaluate our approach. Experimental results reveal that the\nproposed approach is effective."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.05149v1", 
    "title": "Comparison of the Efficiency of Different Algorithms on Recommendation   System Design: a Case Study", 
    "arxiv-id": "1701.05149v1", 
    "author": "G\u00fcrkan Alpaslan", 
    "publish": "2017-01-01T17:58:38Z", 
    "summary": "By the growing trend of online shopping and e-commerce websites,\nrecommendation systems have gained more importance in recent years in order to\nincrease the sales ratios of companies. Different algorithms on recommendation\nsystems are used and every one produce different results. Every algorithm on\nthis area have positive and negative attributes. The purpose of the research is\nto test the different algorithms for choosing the best one according as\nstructure of dataset and aims of developers. For this purpose, threshold and\nk-means based collaborative filtering and content-based filtering algorithms\nare utilized on the dataset contains 100*73421 matrix length. What are the\ndifferences and effects of these different algorithms on the same dataset? What\nare the challenges of the algorithms? What criteria are more important in order\nto evaluate a recommendation systems? In the study, we answer these crucial\nproblems with the case study."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.05596v1", 
    "title": "The Parallel Distributed Image Search Engine (ParaDISE)", 
    "arxiv-id": "1701.05596v1", 
    "author": "Henning M\u00fcller", 
    "publish": "2017-01-19T20:51:56Z", 
    "summary": "Image retrieval is a complex task that differs according to the context and\nthe user requirements in any specific field, for example in a medical\nenvironment. Search by text is often not possible or optimal and retrieval by\nthe visual content does not always succeed in modelling high-level concepts\nthat a user is looking for. Modern image retrieval techniques consist of\nmultiple steps and aim to retrieve information from large--scale datasets and\nnot only based on global image appearance but local features and if possible in\na connection between visual features and text or semantics. This paper presents\nthe Parallel Distributed Image Search Engine (ParaDISE), an image retrieval\nsystem that combines visual search with text--based retrieval and that is\navailable as open source and free of charge. The main design concepts of\nParaDISE are flexibility, expandability, scalability and interoperability.\nThese concepts constitute the system, able to be used both in real-world\napplications and as an image retrieval research platform. Apart from the\narchitecture and the implementation of the system, two use cases are described,\nan application of ParaDISE in retrieval of images from the medical literature\nand a visual feature evaluation for medical image retrieval. Future steps\ninclude the creation of an open source community that will contribute and\nexpand this platform based on the existing parts."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.07810v3", 
    "title": "Learning to Effectively Select Topics For Information Retrieval Test   Collections", 
    "arxiv-id": "1701.07810v3", 
    "author": "Matthew Lease", 
    "publish": "2017-01-26T18:44:37Z", 
    "summary": "While test collections are commonly employed to evaluate the effectiveness of\ninformation retrieval (IR) systems, constructing these collections has become\nincreasingly expensive in recent years as collection sizes have grown\never-larger. To address this, we propose a new learning-to-rank topic selection\nmethod which reduces the number of search topics needed for reliable evaluation\nof IR systems. As part of this work, we also revisit the deep vs. shallow\njudging debate: whether it is better to collect many relevance judgments for a\nfew topics or a few judgments for many topics. We consider a number of factors\nimpacting this trade-off: how topics are selected, topic familiarity to judges,\nand how topic generation cost may impact both budget utilization and the\nresultant quality of judgments. Experiments on NIST TREC Robust 2003 and Robust\n2004 test collections show not only our method's ability to reliably evaluate\nIR systems using fewer topics, but also that when topics are intelligently\nselected, deep judging is often more cost-effective than shallow judging in\nachieving the same level of evaluation reliability. Topic familiarity and\nconstruction costs are also seen to notably impact the evaluation cost vs.\nreliability tradeoff and provide further evidence supporting deep judging in\npractice."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1701.08888v1", 
    "title": "Integrating Reviews into Personalized Ranking for Cold Start   Recommendation", 
    "arxiv-id": "1701.08888v1", 
    "author": "Guang-Neng Hu", 
    "publish": "2017-01-31T02:13:57Z", 
    "summary": "Item recommendation task predicts a personalized ranking over a set of items\nfor individual user. One paradigm is the rating-based methods that concentrate\non explicit feedbacks and hence face the difficulties in collecting them.\nMeanwhile, the ranking-based methods are presented with rated items and then\nrank the rated above the unrated. This paradigm uses widely available implicit\nfeedback but it usually ignores some important information: item reviews. Item\nreviews not only justify the preferences of users, but also help alleviate the\ncold-start problem that fails the collaborative filtering. In this paper, we\npropose two novel and simple models to integrate item reviews into matrix\nfactorization based Bayesian personalized ranking (BPR-MF). In each model, we\nmake use of text features extracted from item reviews via word embeddings. On\ntop of text features we uncover the review dimensions that explain the\nvariation in users' feedback and these review factors represent a prior\npreference of a user. Experiments on real-world data sets show the benefits of\nleveraging item reviews on ranking prediction. We also conduct analyses to\nunderstand the proposed models."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2908131.2908165", 
    "link": "http://arxiv.org/pdf/1702.00171v1", 
    "title": "Inferring Conceptual Relationships When Ranking Patients", 
    "arxiv-id": "1702.00171v1", 
    "author": "Iadh Ounis", 
    "publish": "2017-02-01T09:21:20Z", 
    "summary": "Searching patients based on the relevance of their medical records is\nchallenging because of the inherent implicit knowledge within the patients'\nmedical records and queries. Such knowledge is known to the medical\npractitioners but may be hidden from a search system. For example, when\nsearching for the patients with a heart disease, medical practitioners commonly\nknow that patients who are taking the amiodarone medicine are relevant, since\nthis drug is used to combat heart disease. In this article, we argue that\nleveraging such implicit knowledge improves the retrieval effectiveness, since\nit provides new evidence to infer the relevance of patients' medical records\ntowards a query. Specifically, built upon existing conceptual representation\nfor both medical records and queries, we proposed a novel expansion of queries\nthat infers additional conceptual relationships from domain-specific resources\nas well as by extracting informative concepts from the top-ranked patients'\nmedical records. We evaluate the retrieval effectiveness of our proposed\napproach in the context of the TREC 2011 and 2012 Medical Records track. Our\nresults show the effectiveness of our approach to model the implicit knowledge\nin patient search, whereby the retrieval performance is significantly improved\nover both an effective conceptual representation baseline and an existing\nsemantic query expansion baseline. In addition, we provide an analysis of the\ntypes of queries that the proposed approach is likely to be effective."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-27932-9_14", 
    "link": "http://arxiv.org/pdf/1702.00619v1", 
    "title": "Semantic URL Analytics to Support Efficient Annotation of Large Scale   Web Archives", 
    "arxiv-id": "1702.00619v1", 
    "author": "Julian Szymanski", 
    "publish": "2017-02-02T11:09:53Z", 
    "summary": "Long-term Web archives comprise Web documents gathered over longer time\nperiods and can easily reach hundreds of terabytes in size. Semantic\nannotations such as named entities can facilitate intelligent access to the Web\narchive data. However, the annotation of the entire archive content on this\nscale is often infeasible. The most efficient way to access the documents\nwithin Web archives is provided through their URLs, which are typically stored\nin dedicated index files.The URLs of the archived Web documents can contain\nsemantic information and can offer an efficient way to obtain initial semantic\nannotations for the archived documents. In this paper, we analyse the\napplicability of semantic analysis techniques such as named entity extraction\nto the URLs in a Web archive. We evaluate the precision of the named entity\nextraction from the URLs in the Popular German Web dataset and analyse the\nproportion of the archived URLs from 1,444 popular domains in the time interval\nfrom 2000 to 2012 to which these techniques are applicable. Our results\ndemonstrate that named entity recognition can be successfully applied to a\nlarge number of URLs in our Web archive and provide a good starting point to\nefficiently annotate large scale collections of Web documents."
},{
    "category": "cs.IR", 
    "doi": "10.1007/978-3-319-27932-9_14", 
    "link": "http://arxiv.org/pdf/1702.00855v2", 
    "title": "Neural Feature Embedding for User Response Prediction in Real-Time   Bidding (RTB)", 
    "arxiv-id": "1702.00855v2", 
    "author": "Masayuki Arai", 
    "publish": "2017-02-02T22:32:29Z", 
    "summary": "In the area of ad-targeting, predicting user responses is essential for many\napplications such as Real-Time Bidding (RTB). Many of the features available in\nthis domain are sparse categorical features. This presents a challenge\nespecially when the user responses to be predicted are rare, because each\nfeature will only have very few positive examples. Recently, neural embedding\ntechniques such as word2vec which learn distributed representations of words\nusing occurrence statistics in the corpus have been shown to be effective in\nmany Natural Language Processing tasks. In this paper, we use real-world data\nset to show that a similar technique can be used to learn distributed\nrepresentations of features from users' web history, and that such\nrepresentations can be used to improve the accuracy of commonly used models for\npredicting rare user responses."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2872518.2890555", 
    "link": "http://arxiv.org/pdf/1702.01076v1", 
    "title": "Tempas: Temporal Archive Search Based on Tags", 
    "arxiv-id": "1702.01076v1", 
    "author": "Avishek Anand", 
    "publish": "2017-02-03T16:47:46Z", 
    "summary": "Limited search and access patterns over Web archives have been well\ndocumented. One of the key reasons is the lack of understanding of the user\naccess patterns over such collections, which in turn is attributed to the lack\nof effective search interfaces. Current search interfaces for Web archives are\n(a) either purely navigational or (b) have sub-optimal search experience due to\nineffective retrieval models or query modeling. We identify that external\nlongitudinal resources, such as social bookmarking data, are crucial sources to\nidentify important and popular websites in the past. To this extent we present\nTempas, a tag-based temporal search engine for Web archives.\n  Websites are posted at specific times of interest on several external\nplatforms, such as bookmarking sites like Delicious. Attached tags not only act\nas relevant descriptors useful for retrieval, but also encode the time of\nrelevance. With Tempas we tackle the challenge of temporally searching a Web\narchive by indexing tags and time. We allow temporal selections for search\nterms, rank documents based on their popularity and also provide meaningful\nquery recommendations by exploiting tag-tag and tag-document co-occurrence\nstatistics in arbitrary time windows. Finally, Tempas operates as a fairly\nnon-invasive indexing framework. By not dealing with contents from the actual\nWeb archive it constitutes an attractive and low-overhead approach for quick\naccess into Web archives."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2914724", 
    "link": "http://arxiv.org/pdf/1702.01159v1", 
    "title": "On the Applicability of Delicious for Temporal Search on Web Archives", 
    "arxiv-id": "1702.01159v1", 
    "author": "Avishek Anand", 
    "publish": "2017-02-03T21:06:47Z", 
    "summary": "Web archives are large longitudinal collections that store webpages from the\npast, which might be missing on the current live Web. Consequently, temporal\nsearch over such collections is essential for finding prominent missing\nwebpages and tasks like historical analysis. However, this has been challenging\ndue to the lack of popularity information and proper ground truth to evaluate\ntemporal retrieval models. In this paper we investigate the applicability of\nexternal longitudinal resources to identify important and popular websites in\nthe past and analyze the social bookmarking service Delicious for this purpose.\n  The timestamped bookmarks on Delicious provide explicit cues about popular\ntime periods in the past along with relevant descriptors. These are valuable to\nidentify important documents in the past for a given temporal query. Focusing\npurely on recall, we analyzed more than 12,000 queries and find that using\nDelicious yields average recall values from 46% up to 100%, when limiting\nourselves to the best represented queries in the considered dataset. This\nconstitutes an attractive and low-overhead approach for quick access into Web\narchives by not dealing with the actual contents."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2914724", 
    "link": "http://arxiv.org/pdf/1702.01516v1", 
    "title": "Leveraging High-Dimensional Side Information for Top-N Recommendation", 
    "arxiv-id": "1702.01516v1", 
    "author": "Xiang Zhao", 
    "publish": "2017-02-06T07:23:47Z", 
    "summary": "Top-$N$ recommender systems typically utilize side information to address the\nproblem of data sparsity. As nowadays side information is growing towards high\ndimensionality, the performances of existing methods deteriorate in terms of\nboth effectiveness and efficiency, which imposes a severe technical challenge.\nIn order to take advantage of high-dimensional side information, we propose in\nthis paper an embedded feature selection method to facilitate top-$N$\nrecommendation. In particular, we propose to learn feature weights of side\ninformation, where zero-valued features are naturally filtered out. We also\nintroduce non-negativity and sparsity to the feature weights, to facilitate\nfeature selection and encourage low-rank structure. Two optimization problems\nare accordingly put forward, respectively, where the feature selection is\ntightly or loosely coupled with the learning procedure. Augmented Lagrange\nMultiplier and Alternating Direction Method are applied to efficiently solve\nthe problems. Experiment results demonstrate the superior recommendation\nquality of the proposed algorithm to that of the state-of-the-art alternatives."
},{
    "category": "cs.IR", 
    "doi": "10.1145/2911451.2914724", 
    "link": "http://arxiv.org/pdf/1702.01520v1", 
    "title": "Document Visualization using Topic Clouds", 
    "arxiv-id": "1702.01520v1", 
    "author": "Tat-Seng Chua", 
    "publish": "2017-02-06T07:38:03Z", 
    "summary": "Traditionally a document is visualized by a word cloud. Recently, distributed\nrepresentation methods for documents have been developed, which map a document\nto a set of topic embeddings. Visualizing such a representation is useful to\npresent the semantics of a document in higher granularity; it is also\nchallenging, as there are multiple topics, each containing multiple words. We\npropose to visualize a set of topics using Topic Cloud, which is a pie chart\nconsisting of topic slices, where each slice contains important words in this\ntopic. To make important topics/words visually prominent, the sizes of topic\nslices and word fonts are proportional to their importance in the document. A\ntopic cloud can help the user quickly evaluate the quality of derived document\nrepresentations. For NLP practitioners, It can be used to qualitatively compare\nthe topic quality of different document representation algorithms, or to\ninspect how model parameters impact the derived representations."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1702.01713v1", 
    "title": "A dynamic multi-level collaborative filtering method for improved   recommendations", 
    "arxiv-id": "1702.01713v1", 
    "author": "Christos K. Georgiadis", 
    "publish": "2017-02-06T17:19:07Z", 
    "summary": "One of the most used approaches for providing recommendations in various\nonline environments such as e-commerce is collaborative filtering. Although,\nthis is a simple method for recommending items or services, accuracy and\nquality problems still exist. Thus, we propose a dynamic multi-level\ncollaborative filtering method that improves the quality of the\nrecommendations. The proposed method is based on positive and negative\nadjustments and can be used in different domains that utilize collaborative\nfiltering to increase the quality of the user experience. Furthermore, the\neffectiveness of the proposed method is shown by providing an extensive\nexperimental evaluation based on three real datasets and by comparisons to\nalternative methods."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1702.03222v1", 
    "title": "Mining Electronic Health Records: A Survey", 
    "arxiv-id": "1702.03222v1", 
    "author": "Gyorgy Simon", 
    "publish": "2017-02-09T17:33:48Z", 
    "summary": "The continuously increasing cost of the US healthcare system has received\nsignificant attention. Central to the ideas aimed at curbing this trend is the\nuse of technology, in the form of the mandate to implement electronic health\nrecords (EHRs). EHRs consist of patient information such as demographics,\nmedications, laboratory test results, diagnosis codes and procedures. Mining\nEHRs could lead to improvement in patient health management as EHRs contain\ndetailed information related to disease prognosis for large patient\npopulations. In this manuscript, we provide a structured and comprehensive\noverview of data mining techniques for modeling EHR data. We first provide a\ndetailed understanding of the major application areas to which EHR mining has\nbeen applied and then discuss the nature of EHR data and its accompanying\nchallenges. Next, we describe major approaches used for EHR mining, the metrics\nassociated with EHRs, and the various study designs. With this foundation, we\nthen provide a systematic and methodological organization of existing data\nmining techniques used to model EHRs and discuss ideas for future research. We\nconclude this survey with a comprehensive summary of clinical data mining\napplications of EHR data, as illustrated in the online supplement."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1702.04815v1", 
    "title": "Multimodal Content Representation and Similarity Ranking of Movies", 
    "arxiv-id": "1702.04815v1", 
    "author": "Konstantinos Bougiatiotis", 
    "publish": "2017-02-15T23:31:44Z", 
    "summary": "In this paper we examine the existence of correlation between movie\nsimilarity and low level features from respective movie content. In particular,\nwe demonstrate the extraction of multi-modal representation models of movies\nbased on subtitles, audio and metadata mining. We emphasize our research in\ntopic modeling of movies based on their subtitles. In order to demonstrate the\nproposed content representation approach, we have built a small dataset of 160\nwidely known movies. We assert movie similarities, as propagated by the\nsingular modalities and fusion models, in the form of recommendation rankings.\nWe showcase a novel topic model browser for movies that allows for exploration\nof the different aspects of similarities between movies and an information\nretrieval system for movie similarity based on multi-modal content."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1702.05042v1", 
    "title": "Luandri: a Clean Lua Interface to the Indri Search Engine", 
    "arxiv-id": "1702.05042v1", 
    "author": "Nick Craswell", 
    "publish": "2017-02-16T16:30:06Z", 
    "summary": "In recent years, the information retrieval (IR) community has witnessed the\nfirst successful applications of deep neural network models to short-text\nmatching and ad-hoc retrieval. It is exciting to see the research on deep\nneural networks and IR converge on these tasks of shared interest. However, the\ntwo communities have less in common when it comes to the choice of programming\nlanguages. Indri, an indexing framework popularly used by the IR community, is\nwritten in C++, while Torch, a popular machine learning library for deep\nlearning, is written in the light-weight scripting language Lua. To bridge this\ngap, we introduce Luandri (pronounced \"laundry\"), a simple interface for\nexposing the search capabilities of Indri to Torch models implemented in Lua."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1702.07969v1", 
    "title": "Related Pins at Pinterest: The Evolution of a Real-World Recommender   System", 
    "arxiv-id": "1702.07969v1", 
    "author": "Yushi Jing", 
    "publish": "2017-02-26T01:50:27Z", 
    "summary": "Related Pins is the Web-scale recommender system that powers over 40% of user\nengagement on Pinterest. This paper is a longitudinal study of three years of\nits development, exploring the evolution of the system and its components from\nprototypes to present state. Each component was originally built with many\nconstraints on engineering effort and computational resources, so we\nprioritized the simplest and highest-leverage solutions. We show how organic\ngrowth led to a complex system and how we managed this complexity. Many\nchallenges arose while building this system, such as avoiding feedback loops,\nevaluating performance, activating content, and eliminating legacy heuristics.\nFinally, we offer suggestions for tackling these challenges when engineering\nWeb-scale recommender systems."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1703.00397v1", 
    "title": "Combating the Cold Start User Problem in Model Based Collaborative   Filtering", 
    "arxiv-id": "1703.00397v1", 
    "author": "Senjuti Basu Ray", 
    "publish": "2017-02-18T03:06:09Z", 
    "summary": "For tackling the well known cold-start user problem in model-based\nrecommender systems, one approach is to recommend a few items to a cold-start\nuser and use the feedback to learn a profile. The learned profile can then be\nused to make good recommendations to the cold user. In the absence of a good\ninitial profile, the recommendations are like random probes, but if not chosen\njudiciously, both bad recommendations and too many recommendations may turn off\na user. We formalize the cold-start user problem by asking what are the $b$\nbest items we should recommend to a cold-start user, in order to learn her\nprofile most accurately, where $b$, a given budget, is typically a small\nnumber. We formalize the problem as an optimization problem and present\nmultiple non-trivial results, including NP-hardness as well as hardness of\napproximation. We furthermore show that the objective function, i.e., the least\nsquare error of the learned profile w.r.t. the true user profile, is neither\nsubmodular nor supermodular, suggesting efficient approximations are unlikely\nto exist. Finally, we discuss several scalable heuristic approaches for\nidentifying the $b$ best items to recommend to the user and experimentally\nevaluate their performance on 4 real datasets. Our experiments show that our\nproposed accelerated algorithms significantly outperform the prior art in\nrunnning time, while achieving similar error in the learned user profile as\nwell as in the rating predictions."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1703.01093v1", 
    "title": "Employing Spectral Domain Features for Efficient Collaborative Filtering", 
    "arxiv-id": "1703.01093v1", 
    "author": "Doaa M. Shawky", 
    "publish": "2017-03-03T09:53:58Z", 
    "summary": "Collaborative filtering (CF) is a powerful recommender system that generates\na list of recommended items for an active user based on the ratings of similar\nusers. This paper presents a novel approach to CF by first finding the set of\nusers similar to the active user by adopting self-organizing maps (SOM),\nfollowed by k-means clustering. Then, the ratings for each item in the cluster\nclosest to the active user are mapped to the frequency domain using the\nDiscrete Fourier Transform (DFT). The power spectra of the mapped ratings are\ngenerated, and a new similarity measure based on the coherence of these power\nspectra is calculated. The proposed similarity measure is more time efficient\nthan current state-of-the-art measures. Moreover, it can capture the global\nsimilarity between the profiles of users. Experimental results show that the\nproposed approach overcomes the major problems in existing CF algorithms as\nfollows: First, it mitigates the scalability problem by creating clusters of\nsimilar users and applying the time-efficient similarity measure. Second, its\nfrequency-based similarity measure is less sensitive to sparsity problems\nbecause the DFT performs efficiently even with sparse data. Third, it\noutperforms standard similarity measures in terms of accuracy."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1703.02314v1", 
    "title": "Heterogeneous information network model for equipment-standard system", 
    "arxiv-id": "1703.02314v1", 
    "author": "Duan-Bing Chen", 
    "publish": "2017-03-07T10:29:52Z", 
    "summary": "Entity information network is used to describe structural relationships\nbetween entities. Taking advantage of its extension and heterogeneity, entity\ninformation network is more and more widely applied to relationship modeling.\nRecent years, lots of researches about entity information network modeling have\nbeen proposed, while seldom of them concentrate on equipment-standard system\nwith properties of multi-layer, multi-dimension and multi-scale. In order to\nefficiently deal with some complex issues in equipment-standard system such as\nstandard revising, standard controlling, and production designing, a\nheterogeneous information network model for equipment-standard system is\nproposed in this paper. Three types of entities and six types of relationships\nare considered in the proposed model. Correspondingly, several different\nsimilarity-measuring methods are used in the modeling process. The experiments\nshow that the heterogeneous information network model established in this paper\ncan reflect relationships between entities accurately. Meanwhile, the modeling\nprocess has a good performance on time consumption."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1703.02915v1", 
    "title": "Kaggle Competition: Expedia Hotel Recommendations", 
    "arxiv-id": "1703.02915v1", 
    "author": "Anwar Shaikh", 
    "publish": "2017-03-06T21:05:42Z", 
    "summary": "With hundreds, even thousands, of hotels to choose from at every destination,\nit's difficult to know which will suit your personal preferences. Expedia wants\nto take the proverbial rabbit hole out of hotel search by providing\npersonalized hotel recommendations to their users. This is no small task for a\nsite with hundreds of millions of visitors every month! Currently, Expedia uses\nsearch parameters to adjust their hotel recommendations, but there aren't\nenough customer specific data to personalize them for each user. In this\nproject, we have taken up the challenge to contextualize customer data and\npredict the likelihood a user will stay at 100 different hotel groups."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/1703.03112v2", 
    "title": "Dynamic Intention-Aware Recommendation System", 
    "arxiv-id": "1703.03112v2", 
    "author": "Lina Yao", 
    "publish": "2017-03-09T02:51:30Z", 
    "summary": "Recommender systems have been actively and extensively studied over past\ndecades. In the meanwhile, the boom of Big Data is driving fundamental changes\nin the development of recommender systems. In this paper, we propose a dynamic\nintention-aware recommender system to better facilitate users to find desirable\nproducts and services. Compare to prior work, our proposal possesses the\nfollowing advantages: (1) it takes user intentions and demands into account\nthrough intention mining techniques. By unearthing user intentions from the\nhistorical user-item interactions, and various user digital traces harvested\nfrom social media and Internet of Things, it is capable of delivering more\nsatisfactory recommendations by leveraging rich online and offline user data;\n(2) it embraces the benefits of embedding heterogeneous source information and\nshared representations of multiple domains to provide accurate and effective\nrecommendations comprehensively; (3) it recommends products or services\nproactively and timely by capturing the dynamic influences, which can\nsignificantly reduce user involvements and efforts."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/9809121v1", 
    "title": "Using Local Optimality Criteria for Efficient Information Retrieval with   Redundant Information Filters", 
    "arxiv-id": "cs/9809121v1", 
    "author": "Neil C. Rowe", 
    "publish": "1998-09-29T21:55:20Z", 
    "summary": "We consider information retrieval when the data, for instance multimedia, is\ncoputationally expensive to fetch. Our approach uses \"information filters\" to\nconsiderably narrow the universe of possiblities before retrieval. We are\nespecially interested in redundant information filters that save time over more\ngeneral but more costly filters. Efficient retrieval requires that decision\nmust be made about the necessity, order, and concurrent processing of proposed\nfilters (an \"execution plan\"). We develop simple polynomial-time local criteria\nfor optimal execution plans, and show that most forms of concurrency are\nsuboptimal with information filters. Although the general problem of finding an\noptimal execution plan is likely exponential in the number of filters, we show\nexperimentally that our local optimality criteria, used in a polynomial-time\nalgorithm, nearly always find the global optimum with 15 filters or less, a\nsufficient number of filters for most applications. Our methods do not require\nspecial hardware and avoid the high processor idleness that is characteristic\nof massive parallelism solutions to this problem. We apply our ideas to an\nimportant application, information retrieval of cpationed data using\nnatural-language understanding, a problem for which the natural-language\nprocessing can be the bottleneck if not implemented well."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/9902021v1", 
    "title": "Visualization of Retrieved Documents using a Presentation Server", 
    "arxiv-id": "cs/9902021v1", 
    "author": "Sung Hyon Myaeng", 
    "publish": "1999-02-10T09:30:59Z", 
    "summary": "In any search-based digital library (DL) systems dealing with a non-trivial\nnumber of documents, users are often required to go through a long list of\nshort document descriptions in order to identify what they are looking for. To\ntackle the problem, a variety of document organization algorithms and/or\nvisualization techniques have been used to guide users in selecting relevant\ndocuments. Since these techniques require heavy computations, however, we\ndeveloped a presentation server designed to serve as an intermediary between\nretrieval servers and clients equipped with a visualization interface. In\naddition, we designed our own visual interface by which users can view a set of\ndocuments from different perspectives through layers of document maps. We\nfinally ran experiments to show that the visual interface, in conjunction with\nthe presentation server, indeed helps users in selecting relevant documents\nfrom the retrieval results."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/9902028v2", 
    "title": "A Scrollbar-based Visualization for Document Navigation", 
    "arxiv-id": "cs/9902028v2", 
    "author": "Donald Byrd", 
    "publish": "1999-02-24T17:10:46Z", 
    "summary": "We are interested in questions of improving user control in best-match\ntext-retrieval systems, specifically questions as to whether simple\nvisualizations that nonetheless go beyond the minimal ones generally available\ncan significantly help users. Recently, we have been investigating ways to help\nusers decide-given a set of documents retrieved by a query-which documents and\npassages are worth closer examination. We built a document viewer incorporating\na visualization centered around a novel content-displaying scrollbar and color\nterm highlighting, and studied whether the visualization is helpful to\nnon-expert searchers. Participants' reaction to the visualization was very\npositive, while the objective results were inconclusive."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/9907042v1", 
    "title": "Raising Reliability of Web Search Tool Research through Replication and   Chaos Theory", 
    "arxiv-id": "cs/9907042v1", 
    "author": "Scott Nicholson", 
    "publish": "1999-07-27T16:42:18Z", 
    "summary": "Because the World Wide Web is a dynamic collection of information, the Web\nsearch tools (or \"search engines\") that index the Web are dynamic. Traditional\ninformation retrieval evaluation techniques may not provide reliable results\nwhen applied to the Web search tools. This study is the result of ten\nreplications of the classic 1996 Ding and Marchionini Web search tool research.\nIt explores the effects that replication can have on transforming unreliable\nresults from one iteration into replicable and therefore reliable results after\nmultiple iterations."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/9910015v3", 
    "title": "PIPE: Personalizing Recommendations via Partial Evaluation", 
    "arxiv-id": "cs/9910015v3", 
    "author": "Naren Ramakrishnan", 
    "publish": "1999-10-18T15:47:29Z", 
    "summary": "It is shown that personalization of web content can be advantageously viewed\nas a form of partial evaluation --- a technique well known in the programming\nlanguages community. The basic idea is to model a recommendation space as a\nprogram, then partially evaluate this program with respect to user preferences\n(and features) to obtain specialized content. This technique supports both\ncontent-based and collaborative approaches, and is applicable to a range of\napplications that require automatic information integration from multiple web\nsources. The effectiveness of this methodology is illustrated by two example\napplications --- (i) personalizing content for visitors to the Blacksburg\nElectronic Village (http://www.bev.net), and (ii) locating and selecting\nscientific software on the Internet. The scalability of this technique is\ndemonstrated by its ability to interface with online web ontologies that index\nthousands of web pages."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0003043v1", 
    "title": "Automatic Classification of Text Databases through Query Probing", 
    "arxiv-id": "cs/0003043v1", 
    "author": "Mehran Sahami", 
    "publish": "2000-03-09T04:01:22Z", 
    "summary": "Many text databases on the web are \"hidden\" behind search interfaces, and\ntheir documents are only accessible through querying. Search engines typically\nignore the contents of such search-only databases. Recently, Yahoo-like\ndirectories have started to manually organize these databases into categories\nthat users can browse to find these valuable resources. We propose a novel\nstrategy to automate the classification of search-only text databases. Our\ntechnique starts by training a rule-based document classifier, and then uses\nthe classifier's rules to generate probing queries. The queries are sent to the\ntext databases, which are then classified based on the number of matches that\nthey produce for each query. We report some initial exploratory experiments\nthat show that our approach is promising to automatically characterize the\ncontents of text databases accessible on the web."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0004008v1", 
    "title": "How to Evaluate your Question Answering System Every Day and Still Get   Real Work Done", 
    "arxiv-id": "cs/0004008v1", 
    "author": "Inderjeet Mani", 
    "publish": "2000-04-17T19:29:51Z", 
    "summary": "In this paper, we report on Qaviar, an experimental automated evaluation\nsystem for question answering applications. The goal of our research was to\nfind an automatically calculated measure that correlates well with human\njudges' assessment of answer correctness in the context of question answering\ntasks. Qaviar judges the response by computing recall against the stemmed\ncontent words in the human-generated answer key. It counts the answer correct\nif it exceeds agiven recall threshold. We determined that the answer\ncorrectness predicted by Qaviar agreed with the human 93% to 95% of the time.\n41 question-answering systems were ranked by both Qaviar and human assessors,\nand these rankings correlated with a Kendall's Tau measure of 0.920, compared\nto a correlation of 0.956 between human assessors on the same data."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0005028v1", 
    "title": "A method for command identification, using modified collision free   hashing with addition & rotation iterative hash functions (part 1)", 
    "arxiv-id": "cs/0005028v1", 
    "author": "Dimitrios Skraparlis", 
    "publish": "2000-05-27T01:08:07Z", 
    "summary": "This paper proposes a method for identification of a user`s fixed string set\n(which can be a command/instruction set for a terminal or microprocessor). This\nmethod is fast and has very small memory requirements, compared to a\ntraditional full string storage and compare method. The user feeds characters\ninto a microcontroller via a keyboard or another microprocessor sends commands\nand the microcontroller hashes the input in order to identify valid commands,\nensuring no collisions between hashed valid strings, while applying further\ncriteria to narrow collision between random and valid strings. The method\nproposed narrows the possibility of the latter kind of collision, achieving\nsmall code and memory-size utilization and very fast execution. Hashing is\nachieved using additive & rotating hash functions in an iterative form, which\ncan be very easily implemented in simple microcontrollers and microprocessors.\nSuch hash functions are presented and compared according to their efficiency\nfor a given string/command set, using the program found in the appendix."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0007003v1", 
    "title": "Using compression to identify acronyms in text", 
    "arxiv-id": "cs/0007003v1", 
    "author": "Ian H. Witten", 
    "publish": "2000-07-04T02:02:01Z", 
    "summary": "Text mining is about looking for patterns in natural language text, and may\nbe defined as the process of analyzing text to extract information from it for\nparticular purposes. In previous work, we claimed that compression is a key\ntechnology for text mining, and backed this up with a study that showed how\nparticular kinds of lexical tokens---names, dates, locations, etc.---can be\nidentified and located in running text, using compression models to provide the\nleverage necessary to distinguish different token types (Witten et al., 1999)"
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0007041v1", 
    "title": "Relevance as Deduction: A Logical View of Information Retrieval", 
    "arxiv-id": "cs/0007041v1", 
    "author": "Konstantinos Georgatos", 
    "publish": "2000-07-26T19:34:35Z", 
    "summary": "The problem of Information Retrieval is, given a set of documents D and a\nquery q, providing an algorithm for retrieving all documents in D relevant to\nq. However, retrieval should depend and be updated whenever the user is able to\nprovide as an input a preferred set of relevant documents; this process is\nknown as em relevance feedback. Recent work in IR has been paying great\nattention to models which employ a logical approach; the advantage being that\none can have a simple computable characterization of retrieval on the basis of\na pure logical analysis of retrieval. Most of the logical models make use of\nprobabilities or similar belief functions in order to introduce the inductive\ncomponent whereby uncertainty is treated. Their general paradigm is the\nfollowing: em find the nature of conditional $d\\imp q$ and then define a\nprobability on the top of it. We just reverse this point of view; first use the\nnumerical information, frequencies or probabilities, then define your own\nlogical consequence. More generally, we claim that retrieval is a form of\ndeduction. We introduce a simple but powerful logical framework of relevance\nfeedback, derived from the well founded area of nonmonotonic logic. This\ndescription can help us evaluate, describe and compare from a theoretical point\nof view previous approaches based on conditionals or probabilities."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.csi.2016.10.014", 
    "link": "http://arxiv.org/pdf/cs/0011028v1", 
    "title": "Retrieval from Captioned Image Databases Using Natural Language   Processing", 
    "arxiv-id": "cs/0011028v1", 
    "author": "David Elworthy", 
    "publish": "2000-11-20T15:36:09Z", 
    "summary": "It might appear that natural language processing should improve the accuracy\nof information retrieval systems, by making available a more detailed analysis\nof queries and documents. Although past results appear to show that this is not\nso, if the focus is shifted to short phrases rather than full documents, the\nsituation becomes somewhat different. The ANVIL system uses a natural language\ntechnique to obtain high accuracy retrieval of images which have been annotated\nwith a descriptive textual caption. The natural language techniques also allow\nadditional contextual information to be derived from the relation between the\nquery and the caption, which can help users to understand the overall\ncollection of retrieval results. The techniques have been successfully used in\na information retrieval system which forms both a testbed for research and the\nbasis of a commercial system."
},{
    "category": "cs.IR", 
    "doi": "10.1117/12.411898", 
    "link": "http://arxiv.org/pdf/cs/0012021v1", 
    "title": "A Benchmark for Image Retrieval using Distributed Systems over the   Internet: BIRDS-I", 
    "arxiv-id": "cs/0012021v1", 
    "author": "Giordano B. Beretta", 
    "publish": "2000-12-22T23:38:37Z", 
    "summary": "The performance of CBIR algorithms is usually measured on an isolated\nworkstation. In a real-world environment the algorithms would only constitute a\nminor component among the many interacting components. The Internet\ndramati-cally changes many of the usual assumptions about measuring CBIR\nperformance. Any CBIR benchmark should be designed from a networked systems\nstandpoint. These benchmarks typically introduce communication overhead because\nthe real systems they model are distributed applications. We present our\nimplementation of a client/server benchmark called BIRDS-I to measure image\nretrieval performance over the Internet. It has been designed with the trend\ntoward the use of small personalized wireless systems in mind. Web-based CBIR\nimplies the use of heteroge-neous image sets, imposing certain constraints on\nhow the images are organized and the type of performance metrics applicable.\nBIRDS-I only requires controlled human intervention for the compilation of the\nimage collection and none for the generation of ground truth in the measurement\nof retrieval accuracy. Benchmark image collections need to be evolved\nincrementally toward the storage of millions of images and that scaleup can\nonly be achieved through the use of computer-aided compilation. Finally, our\nscoring metric introduces a tightly optimized image-ranking window."
},{
    "category": "cs.HC", 
    "doi": "10.1117/12.411898", 
    "link": "http://arxiv.org/pdf/cs/0101012v1", 
    "title": "Communities of Practice in the Distributed International Environment", 
    "arxiv-id": "cs/0101012v1", 
    "author": "Peter Wright", 
    "publish": "2001-01-16T12:18:12Z", 
    "summary": "Modern commercial organisations are facing pressures which have caused them\nto lose personnel. When they lose people, they also lose their knowledge.\nOrganisations also have to cope with the internationalisation of business\nforcing collaboration and knowledge sharing across time and distance. Knowledge\nManagement (KM) claims to tackle these issues. This paper looks at an area\nwhere KM does not offer sufficient support, that is, the sharing of knowledge\nthat is not easy to articulate.\n  The focus in this paper is on Communities of Practice in commercial\norganisations. We do this by exploring knowledge sharing in Lave and Wenger's\n[1] theory of Communities of Practice and investigating how Communities of\nPractice may translate to a distributed international environment. The paper\nreports on two case studies that explore the functioning of Communities of\nPractice across international boundaries."
},{
    "category": "cs.DB", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0104008v1", 
    "title": "Event Indexing Systems for Efficient Selection and Analysis of HERA Data", 
    "arxiv-id": "cs/0104008v1", 
    "author": "Enrico Tassi", 
    "publish": "2001-04-03T16:27:48Z", 
    "summary": "The design and implementation of two software systems introduced to improve\nthe efficiency of offline analysis of event data taken with the ZEUS Detector\nat the HERA electron-proton collider at DESY are presented. Two different\napproaches were made, one using a set of event directories and the other using\na tag database based on a commercial object-oriented database management\nsystem. These are described and compared. Both systems provide quick direct\naccess to individual collision events in a sequential data store of several\nterabytes, and they both considerably improve the event analysis efficiency. In\nparticular the tag database provides a very flexible selection mechanism and\ncan dramatically reduce the computing time needed to extract small subsamples\nfrom the total event sample. Gains as large as a factor 20 have been obtained."
},{
    "category": "cs.CL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0106039v1", 
    "title": "Iterative Residual Rescaling: An Analysis and Generalization of LSI", 
    "arxiv-id": "cs/0106039v1", 
    "author": "Lillian Lee", 
    "publish": "2001-06-17T20:45:37Z", 
    "summary": "We consider the problem of creating document representations in which\ninter-document similarity measurements correspond to semantic similarity. We\nfirst present a novel subspace-based framework for formalizing this task. Using\nthis framework, we derive a new analysis of Latent Semantic Indexing (LSI),\nshowing a precise relationship between its performance and the uniformity of\nthe underlying distribution of documents over topics. This analysis helps\nexplain the improvements gained by Ando's (2000) Iterative Residual Rescaling\n(IRR) algorithm: IRR can compensate for distributional non-uniformity. A\nfurther benefit of our framework is that it provides a well-motivated,\neffective method for automatically determining the rescaling factor IRR depends\non, leading to further improvements. A series of experiments over various\nsettings and with several evaluation metrics validates our claims."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0108003v1", 
    "title": "The Partial Evaluation Approach to Information Personalization", 
    "arxiv-id": "cs/0108003v1", 
    "author": "Saverio Perugini", 
    "publish": "2001-08-07T20:27:39Z", 
    "summary": "Information personalization refers to the automatic adjustment of information\ncontent, structure, and presentation tailored to an individual user. By\nreducing information overload and customizing information access,\npersonalization systems have emerged as an important segment of the Internet\neconomy. This paper presents a systematic modeling methodology - PIPE\n(`Personalization is Partial Evaluation') - for personalization.\nPersonalization systems are designed and implemented in PIPE by modeling an\ninformation-seeking interaction in a programmatic representation. The\nrepresentation supports the description of information-seeking activities as\npartial information and their subsequent realization by partial evaluation, a\ntechnique for specializing programs. We describe the modeling methodology at a\nconceptual level and outline representational choices. We present two\napplication case studies that use PIPE for personalizing web sites and describe\nhow PIPE suggests a novel evaluation criterion for information system designs.\nFinally, we mention several fundamental implications of adopting the PIPE model\nfor personalization and when it is (and is not) applicable."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0108004v1", 
    "title": "Links tell us about lexical and semantic Web content", 
    "arxiv-id": "cs/0108004v1", 
    "author": "Filippo Menczer", 
    "publish": "2001-08-08T02:16:15Z", 
    "summary": "The latest generation of Web search tools is beginning to exploit hypertext\nlink information to improve ranking\\cite{Brin98,Kleinberg98} and\ncrawling\\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden\nassumption behind such approaches, a correlation between the graph structure of\nthe Web and its content, has not been tested explicitly despite increasing\nresearch on Web topology\\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I\nformalize and quantitatively validate two conjectures drawing connections from\nlink information to lexical and semantic Web content. The clink-content\nconjecture states that a page is similar to the pages that link to it, i.e.,\none can infer the lexical content of a page by looking at the pages that link\nto it. I also show that lexical inferences based on link cues are quite\nheterogeneous across Web communities. The link-cluster conjecture states that\npages about the same topic are clustered together, i.e., one can infer the\nmeaning of a page by looking at its neighbours. These results explain the\nsuccess of the newest search technologies and open the way for more dynamic and\nscalable methods to locate information in a topic or user driven way."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0108018v1", 
    "title": "Bipartite graph partitioning and data clustering", 
    "arxiv-id": "cs/0108018v1", 
    "author": "H. Simon", 
    "publish": "2001-08-27T13:07:44Z", 
    "summary": "Many data types arising from data mining applications can be modeled as\nbipartite graphs, examples include terms and documents in a text corpus,\ncustomers and purchasing items in market basket analysis and reviewers and\nmovies in a movie recommender system. In this paper, we propose a new data\nclustering method based on partitioning the underlying bipartite graph. The\npartition is constructed by minimizing a normalized sum of edge weights between\nunmatched pairs of vertices of the bipartite graph. We show that an approximate\nsolution to the minimization problem can be obtained by computing a partial\nsingular value decomposition (SVD) of the associated edge weight matrix of the\nbipartite graph. We point out the connection of our clustering algorithm to\ncorrespondence analysis used in multivariate analysis. We also briefly discuss\nthe issue of assigning data objects to multiple clusters. In the experimental\nresults, we apply our clustering algorithm to the problem of document\nclustering to illustrate its effectiveness and efficiency."
},{
    "category": "cs.CL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0108023v1", 
    "title": "Information Extraction Using the Structured Language Model", 
    "arxiv-id": "cs/0108023v1", 
    "author": "Milind Mahajan", 
    "publish": "2001-08-29T04:00:24Z", 
    "summary": "The paper presents a data-driven approach to information extraction (viewed\nas template filling) using the structured language model (SLM) as a statistical\nparser. The task of template filling is cast as constrained parsing using the\nSLM. The model is automatically trained from a set of sentences annotated with\nframe/slot labels and spans. Training proceeds in stages: first a constrained\nsyntactic parser is trained such that the parses on training data meet the\nspecified semantic spans, then the non-terminal labels are enriched to contain\nsemantic information and finally a constrained syntactic+semantic parser is\ntrained on the parse trees resulting from the previous stage. Despite the small\namount of training data used, the model is shown to outperform the slot level\naccuracy of a simple semantic grammar authored manually for the MiPad ---\npersonal information management --- task."
},{
    "category": "cs.CL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0109013v1", 
    "title": "Conceptual Analysis of Lexical Taxonomies: The Case of WordNet Top-Level", 
    "arxiv-id": "cs/0109013v1", 
    "author": "Alessandro Oltramari", 
    "publish": "2001-09-11T12:17:04Z", 
    "summary": "In this paper we propose an analysis and an upgrade of WordNet's top-level\nsynset taxonomy. We briefly review WordNet and identify its main semantic\nlimitations. Some principles from a forthcoming OntoClean methodology are\napplied to the ontological analysis of WordNet. A revised top-level taxonomy is\nproposed, which is meant to be more conceptually rigorous, cognitively\ntransparent, and efficiently exploitable in several applications."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0110026v1", 
    "title": "Information retrieval in Current Research Information Systems", 
    "arxiv-id": "cs/0110026v1", 
    "author": "Andrei Lopatenko", 
    "publish": "2001-10-10T15:28:00Z", 
    "summary": "In this paper we describe the requirements for research information systems\nand problems which arise in the development of such system. Here is shown which\nproblems could be solved by using of knowledge markup technologies. Ontology\nfor Research Information System offered. Architecture for collecting research\ndata and providing access to it is described."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0110053v1", 
    "title": "Machine Learning in Automated Text Categorization", 
    "arxiv-id": "cs/0110053v1", 
    "author": "Fabrizio Sebastiani", 
    "publish": "2001-10-26T09:27:48Z", 
    "summary": "The automated categorization (or classification) of texts into predefined\ncategories has witnessed a booming interest in the last ten years, due to the\nincreased availability of documents in digital form and the ensuing need to\norganize them. In the research community the dominant approach to this problem\nis based on machine learning techniques: a general inductive process\nautomatically builds a classifier by learning, from a set of preclassified\ndocuments, the characteristics of the categories. The advantages of this\napproach over the knowledge engineering approach (consisting in the manual\ndefinition of a classifier by domain experts) are a very good effectiveness,\nconsiderable savings in terms of expert manpower, and straightforward\nportability to different domains. This survey discusses the main approaches to\ntext categorization that fall within the machine learning paradigm. We will\ndiscuss in detail issues pertaining to three different problems, namely\ndocument representation, classifier construction, and classifier evaluation."
},{
    "category": "cs.HC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0111007v1", 
    "title": "Explaining Scenarios for Information Personalization", 
    "arxiv-id": "cs/0111007v1", 
    "author": "John M. Carroll", 
    "publish": "2001-11-05T19:02:47Z", 
    "summary": "Personalization customizes information access. The PIPE (\"Personalization is\nPartial Evaluation\") modeling methodology represents interaction with an\ninformation space as a program. The program is then specialized to a user's\nknown interests or information seeking activity by the technique of partial\nevaluation. In this paper, we elaborate PIPE by considering requirements\nanalysis in the personalization lifecycle. We investigate the use of scenarios\nas a means of identifying and analyzing personalization requirements. As our\nfirst result, we show how designing a PIPE representation can be cast as a\nsearch within a space of PIPE models, organized along a partial order. This\nallows us to view the design of a personalization system, itself, as\nspecialized interpretation of an information space. We then exploit the\nunderlying equivalence of explanation-based generalization (EBG) and partial\nevaluation to realize high-level goals and needs identified in scenarios; in\nparticular, we specialize (personalize) an information space based on the\nexplanation of a user scenario in that information space, just as EBG\nspecializes a theory based on the explanation of an example in that theory. In\nthis approach, personalization becomes the transformation of information spaces\nto support the explanation of usage scenarios. An example application is\ndescribed."
},{
    "category": "cs.AI", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0111012v1", 
    "title": "Intelligent Anticipated Exploration of Web Sites", 
    "arxiv-id": "cs/0111012v1", 
    "author": "Giovambattista Ianni", 
    "publish": "2001-11-06T18:00:49Z", 
    "summary": "In this paper we describe a web search agent, called Global Search Agent\n(hereafter GSA for short). GSA integrates and enhances several search\ntechniques in order to achieve significant improvements in the user-perceived\nquality of delivered information as compared to usual web search engines. GSA\nfeatures intelligent merging of relevant documents from different search\nengines, anticipated selective exploration and evaluation of links from the\ncurrent result set, automated derivation of refined queries based on user\nrelevance feedback. System architecture as well as experimental accounts are\nalso illustrated."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0203024v1", 
    "title": "The structure of broad topics on the Web", 
    "arxiv-id": "cs/0203024v1", 
    "author": "David M. Pennock", 
    "publish": "2002-03-20T06:46:21Z", 
    "summary": "The Web graph is a giant social network whose properties have been measured\nand modeled extensively in recent years. Most such studies concentrate on the\ngraph structure alone, and do not consider textual properties of the nodes.\nConsequently, Web communities have been characterized purely in terms of graph\nstructure and not on page content. We propose that a topic taxonomy such as\nYahoo! or the Open Directory provides a useful framework for understanding the\nstructure of content-based clusters and communities. In particular, using a\ntopic taxonomy and an automatic classifier, we can measure the background\ndistribution of broad topics on the Web, and analyze the capability of recent\nrandom walk algorithms to draw samples which follow such distributions. In\naddition, we can measure the probability that a page about one broad topic will\nlink to another broad topic. Extending this experiment, we can measure how\nquickly topic context is lost while walking randomly on the Web graph.\nEstimates of this topic mixing distance may explain why a global PageRank is\nstill meaningful in the context of broad queries. In general, our measurements\nmay prove valuable in the design of community-specific crawlers and link-based\nranking systems."
},{
    "category": "cs.DB", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0204038v1", 
    "title": "Technology For Information Engineering (TIE): A New Way of Storing,   Retrieving and Analyzing Information", 
    "arxiv-id": "cs/0204038v1", 
    "author": "Jerzy Lewak", 
    "publish": "2002-04-16T16:03:18Z", 
    "summary": "The theoretical foundations of a new model and paradigm (called TIE) for data\nstorage and access are introduced. Associations between data elements are\nstored in a single Matrix table, which is usually kept entirely in RAM for\nquick access. The model ties together a very intuitive \"guided\" GUI to the\nMatrix structure, allowing extremely easy complex searches through the data.\nAlthough it is an \"Associative Model\" in that it stores the data associations\nseparately from the data itself, in contrast to other implementations of that\nmodel TIE guides the user to only the available information ensuring that every\nsearch is always fruitful. Very many diverse applications of the technology are\nreviewed."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0204054v1", 
    "title": "Navigating the Small World Web by Textual Cues", 
    "arxiv-id": "cs/0204054v1", 
    "author": "Filippo Menczer", 
    "publish": "2002-04-26T22:45:30Z", 
    "summary": "Can a Web crawler efficiently locate an unknown relevant page? While this\nquestion is receiving much empirical attention due to its considerable\ncommercial value in the search engine community\n[Cho98,Chakrabarti99,Menczer00,Menczer01], theoretical efforts to bound the\nperformance of focused navigation have only exploited the link structure of the\nWeb graph, neglecting other features [Kleinberg01,Adamic01,Kim02]. Here I\ninvestigate the connection between linkage and a content-induced topology of\nWeb pages, suggesting that efficient paths can be discovered by decentralized\nnavigation algorithms based on textual cues."
},{
    "category": "cs.AI", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0205022v1", 
    "title": "The Traits of the Personable", 
    "arxiv-id": "cs/0205022v1", 
    "author": "Naren Ramakrishnan", 
    "publish": "2002-05-14T19:25:07Z", 
    "summary": "Information personalization is fertile ground for application of AI\ntechniques. In this article I relate personalization to the ability to capture\npartial information in an information-seeking interaction. The specific focus\nis on personalizing interactions at web sites. Using ideas from partial\nevaluation and explanation-based generalization, I present a modeling\nmethodology for reasoning about personalization. This approach helps identify\nseven tiers of `personable traits' in web sites."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0205059v2", 
    "title": "A Connection-Centric Survey of Recommender Systems Research", 
    "arxiv-id": "cs/0205059v2", 
    "author": "Edward A. Fox", 
    "publish": "2002-05-22T08:36:32Z", 
    "summary": "Recommender systems attempt to reduce information overload and retain\ncustomers by selecting a subset of items from a universal set based on user\npreferences. While research in recommender systems grew out of information\nretrieval and filtering, the topic has steadily advanced into a legitimate and\nchallenging research area of its own. Recommender systems have traditionally\nbeen studied from a content-based filtering vs. collaborative design\nperspective. Recommendations, however, are not delivered within a vacuum, but\nrather cast within an informal community of users and social context.\nTherefore, ultimately all recommender systems make connections among people and\nthus should be surveyed from such a perspective. This viewpoint is\nunder-emphasized in the recommender systems literature. We therefore take a\nconnection-oriented viewpoint toward recommender systems research. We posit\nthat recommendation has an inherently social element and is ultimately intended\nto connect people either directly as a result of explicit user modeling or\nindirectly through the discovery of relationships implicit in extant data.\nThus, recommender systems are characterized by how they model users to bring\npeople together: explicitly or implicitly. Finally, user modeling and the\nconnection-centric viewpoint raise broadening and social issues--such as\nevaluation, targeting, and privacy and trust--which we also briefly address."
},{
    "category": "cs.DL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0205071v1", 
    "title": "A Scalable Architecture for Harvest-Based Digital Libraries - The   ODU/Southampton Experiments", 
    "arxiv-id": "cs/0205071v1", 
    "author": "Michael L. Nelson", 
    "publish": "2002-05-28T15:32:55Z", 
    "summary": "This paper discusses the requirements of current and emerging applications\nbased on the Open Archives Initiative (OAI) and emphasizes the need for a\ncommon infrastructure to support them. Inspired by HTTP proxy, cache, gateway\nand web service concepts, a design for a scalable and reliable infrastructure\nthat aims at satisfying these requirements is presented. Moreover it is shown\nhow various applications can exploit the services included in the proposed\ninfrastructure. The paper concludes by discussing the current status of several\nprototype implementations."
},{
    "category": "cs.NI", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0205080v2", 
    "title": "Transforming the World Wide Web into a Complexity-Based Semantic Network", 
    "arxiv-id": "cs/0205080v2", 
    "author": "A. Das", 
    "publish": "2002-05-31T18:44:36Z", 
    "summary": "The aim of this paper is to introduce the idea of the Semantic Web to the\nComplexity community and set a basic ground for a project resulting in creation\nof Internet-based semantic network of Complexity-related information providers.\nImplementation of the Semantic Web technology would be of mutual benefit to\nboth the participants and users and will confirm self-referencing power of the\ncommunity to apply the products of its own research to itself. We first explain\nthe logic of the transition and discuss important notions associated with the\nSemantic Web technology. We then present a brief outline of the project\nmilestones."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0206028v2", 
    "title": "Knowledge management for enterprises (Wissensmanagement fuer   Unternehmen)", 
    "arxiv-id": "cs/0206028v2", 
    "author": "Wolfgang Eiden", 
    "publish": "2002-06-19T22:13:41Z", 
    "summary": "Although knowledge is one of the most valuable resource of enterprises and an\nimportant production and competition factor, this intellectual potential is\noften used (or maintained) only inadequate by the enterprises. Therefore, in a\nglobalised and growing market the optimal usage of existing knowledge\nrepresents a key factor for enterprises of the future. Here, knowledge\nmanagement systems should engage facilitating. Because geographically far\ndistributed establishments cause, however, a distributed system, this paper\nshould uncover the spectrum connected with it and present a possible basic\napproach which is based on ontologies and modern, platform independent\ntechnologies. Last but not least this attempt, as well as general questions of\nthe knowledge management, are discussed."
},{
    "category": "cs.CL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0207058v2", 
    "title": "Question Answering over Unstructured Data without Domain Restrictions", 
    "arxiv-id": "cs/0207058v2", 
    "author": "Jochen L. Leidner", 
    "publish": "2002-07-14T21:04:43Z", 
    "summary": "Information needs are naturally represented as questions. Automatic\nNatural-Language Question Answering (NLQA) has only recently become a practical\ntask on a larger scale and without domain constraints.\n  This paper gives a brief introduction to the field, its history and the\nimpact of systematic evaluation competitions.\n  It is then demonstrated that an NLQA system for English can be built and\nevaluated in a very short time using off-the-shelf parsers and thesauri. The\nsystem is based on Robust Minimal Recursion Semantics (RMRS) and is portable\nwith respect to the parser used as a frontend. It applies atomic term\nunification supported by question classification and WordNet lookup for\nsemantic similarity matching of parsed question representation and free text."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0211041v1", 
    "title": "An Approach to Automatic Indexing of Scientific Publications in High   Energy Physics for Database SPIRES HEP", 
    "arxiv-id": "cs/0211041v1", 
    "author": "L. A. Vassilevskaya", 
    "publish": "2002-11-28T17:33:19Z", 
    "summary": "We introduce an approach to automatic indexing of e-prints based on a\npattern-matching technique making extensive use of an Associative Patterns\nDictionary (APD), developed by us. Entries in the APD consist of natural\nlanguage phrases with the same semantic interpretation as a set of keywords\nfrom a controlled vocabulary. The method also allows to recognize within\ne-prints formulae written in TeX notations that might also appear as keywords.\nWe present an automatic indexing system, AUTEX, which we have applied to\nkeyword index e-prints in selected areas in high energy physics (HEP) making\nuse of the DESY-HEPI thesaurus as a controlled vocabulary."
},{
    "category": "cs.LG", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0212011v1", 
    "title": "Mining the Web for Lexical Knowledge to Improve Keyphrase Extraction:   Learning from Labeled and Unlabeled Data", 
    "arxiv-id": "cs/0212011v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-08T18:52:33Z", 
    "summary": "Keyphrases are useful for a variety of purposes, including summarizing,\nindexing, labeling, categorizing, clustering, highlighting, browsing, and\nsearching. The task of automatic keyphrase extraction is to select keyphrases\nfrom within the text of a given document. Automatic keyphrase extraction makes\nit feasible to generate keyphrases for the huge number of documents that do not\nhave manually assigned keyphrases. Good performance on this task has been\nobtained by approaching it as a supervised learning problem. An input document\nis treated as a set of candidate phrases that must be classified as either\nkeyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase,\nthe most important features (attributes) appear to be the frequency and\nlocation of the candidate phrase in the document. Recent work has demonstrated\nthat it is also useful to know the frequency of the candidate phrase as a\nmanually assigned keyphrase for other documents in the same domain as the given\ndocument (e.g., the domain of computer science). Unfortunately, this\nkeyphrase-frequency feature is domain-specific (the learning process must be\nrepeated for each new domain) and training-intensive (good performance requires\na relatively large number of training documents in the given domain, with\nmanually assigned keyphrases). The aim of the work described here is to remove\nthese limitations. In this paper, I introduce new features that are derived by\nmining lexical knowledge from a very large collection of unlabeled data,\nconsisting of approximately 350 million Web pages without manually assigned\nkeyphrases. I present experiments that show that the new features result in\nimproved keyphrase extraction, although they are neither domain-specific nor\ntraining-intensive."
},{
    "category": "cs.LG", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0212012v1", 
    "title": "Unsupervised Learning of Semantic Orientation from a   Hundred-Billion-Word Corpus", 
    "arxiv-id": "cs/0212012v1", 
    "author": "Michael L. Littman", 
    "publish": "2002-12-08T19:06:08Z", 
    "summary": "The evaluative character of a word is called its semantic orientation. A\npositive semantic orientation implies desirability (e.g., \"honest\", \"intrepid\")\nand a negative semantic orientation implies undesirability (e.g., \"disturbing\",\n\"superfluous\"). This paper introduces a simple algorithm for unsupervised\nlearning of semantic orientation from extremely large corpora. The method\ninvolves issuing queries to a Web search engine and using pointwise mutual\ninformation to analyse the results. The algorithm is empirically evaluated\nusing a training corpus of approximately one hundred billion words -- the\nsubset of the Web that is indexed by the chosen search engine. Tested with\n3,596 words (1,614 positive and 1,982 negative), the algorithm attains an\naccuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, and\nverbs. The accuracy is comparable with the results achieved by Hatzivassiloglou\nand McKeown (1997), using a complex four-stage supervised learning algorithm\nthat is restricted to determining the semantic orientation of adjectives."
},{
    "category": "cs.LG", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0212013v1", 
    "title": "Learning to Extract Keyphrases from Text", 
    "arxiv-id": "cs/0212013v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-08T19:27:56Z", 
    "summary": "Many academic journals ask their authors to provide a list of about five to\nfifteen key words, to appear on the first page of each article. Since these key\nwords are often phrases of two or more words, we prefer to call them\nkeyphrases. There is a surprisingly wide variety of tasks for which keyphrases\nare useful, as we discuss in this paper. Recent commercial software, such as\nMicrosoft's Word 97 and Verity's Search 97, includes algorithms that\nautomatically extract keyphrases from documents. In this paper, we approach the\nproblem of automatically extracting keyphrases from text as a supervised\nlearning task. We treat a document as a set of phrases, which the learning\nalgorithm must learn to classify as positive or negative examples of\nkeyphrases. Our first set of experiments applies the C4.5 decision tree\ninduction algorithm to this learning task. The second set of experiments\napplies the GenEx algorithm to the task. We developed the GenEx algorithm\nspecifically for this task. The third set of experiments examines the\nperformance of GenEx on the task of metadata generation, relative to the\nperformance of Microsoft's Word 97. The fourth and final set of experiments\ninvestigates the performance of GenEx on the task of highlighting, relative to\nVerity's Search 97. The experimental results support the claim that a\nspecialized learning algorithm (GenEx) can generate better keyphrases than a\ngeneral-purpose learning algorithm (C4.5) and the non-learning algorithms that\nare used in commercial software (Word 97 and Search 97)."
},{
    "category": "cs.LG", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0212014v1", 
    "title": "Extraction of Keyphrases from Text: Evaluation of Four Algorithms", 
    "arxiv-id": "cs/0212014v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-08T19:40:42Z", 
    "summary": "This report presents an empirical evaluation of four algorithms for\nautomatically extracting keywords and keyphrases from documents. The four\nalgorithms are compared using five different collections of documents. For each\ndocument, we have a target set of keyphrases, which were generated by hand. The\ntarget keyphrases were generated for human readers; they were not tailored for\nany of the four keyphrase extraction algorithms. Each of the algorithms was\nevaluated by the degree to which the algorithm's keyphrases matched the\nmanually generated keyphrases. The four algorithms were (1) the AutoSummarize\nfeature in Microsoft's Word 97, (2) an algorithm based on Eric Brill's\npart-of-speech tagger, (3) the Summarize feature in Verity's Search 97, and (4)\nNRC's Extractor algorithm. For all five document collections, NRC's Extractor\nyields the best match with the manually generated keyphrases."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0212045v1", 
    "title": "Local Community Identification through User Access Patterns", 
    "arxiv-id": "cs/0212045v1", 
    "author": "Virgilio A. F. Almeida", 
    "publish": "2002-12-16T17:56:33Z", 
    "summary": "Community identification algorithms have been used to enhance the quality of\nthe services perceived by its users. Although algorithms for community have a\nwidespread use in the Web, their application to portals or specific subsets of\nthe Web has not been much studied. In this paper, we propose a technique for\nlocal community identification that takes into account user access behavior\nderived from access logs of servers in the Web. The technique takes a departure\nfrom the existing community algorithms since it changes the focus of in terest,\nmoving from authors to users. Our approach does not use relations imposed by\nauthors (e.g. hyperlinks in the case of Web pages). It uses information derived\nfrom user accesses to a service in order to infer relationships. The\ncommunities identified are of great interest to content providers since they\ncan be used to improve quality of their services. We also propose an evaluation\nmethodology for analyzing the results obtained by the algorithm. We present two\ncase studies based on actual data from two services: an online bookstore and an\nonline radio. The case of the online radio is particularly relevant, because it\nemphasizes the contribution of the proposed algorithm to find out communities\nin an environment (i.e., streaming media service) without links, that represent\nthe relations imposed by authors (e.g. hyperlinks in the case of Web pages)."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0302023v1", 
    "title": "Segmentation, Indexing, and Visualization of Extended Instructional   Videos", 
    "arxiv-id": "cs/0302023v1", 
    "author": "John R. Kender", 
    "publish": "2003-02-16T22:08:01Z", 
    "summary": "We present a new method for segmenting, and a new user interface for indexing\nand visualizing, the semantic content of extended instructional videos. Given a\nseries of key frames from the video, we generate a condensed view of the data\nby clustering frames according to media type and visual similarities. Using\nvarious visual filters, key frames are first assigned a media type (board,\nclass, computer, illustration, podium, and sheet). Key frames of media type\nboard and sheet are then clustered based on contents via an algorithm with\nnear-linear cost. A novel user interface, the result of two user studies,\ndisplays related topics using icons linked topologically, allowing users to\nquickly locate semantically related portions of the video. We analyze the\naccuracy of the segmentation tool on 17 instructional videos, each of which is\nfrom 75 to 150 minutes in duration (a total of 40 hours); the classification\naccuracy exceeds 96%."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0302024v2", 
    "title": "Analysis and Interface for Instructional Video", 
    "arxiv-id": "cs/0302024v2", 
    "author": "John R. Kender", 
    "publish": "2003-02-16T22:13:42Z", 
    "summary": "We present a new method for segmenting, and a new user interface for indexing\nand visualizing, the semantic content of extended instructional videos. Using\nvarious visual filters, key frames are first assigned a media type (board,\nclass, computer, illustration, podium, and sheet). Key frames of media type\nboard and sheet are then clustered based on contents via an algorithm with\nnear-linear cost. A novel user interface, the result of two user studies,\ndisplays related topics using icons linked topologically, allowing users to\nquickly locate semantically related portions of the video. We analyze the\naccuracy of the segmentation tool on 17 instructional videos, each of which is\nfrom 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%."
},{
    "category": "cs.DB", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0305056v1", 
    "title": "Configuration Database for BaBar On-line", 
    "arxiv-id": "cs/0305056v1", 
    "author": "A. Salnikov", 
    "publish": "2003-05-29T21:37:47Z", 
    "summary": "The configuration database is one of the vital systems in the BaBar on-line\nsystem. It provides services for the different parts of the data acquisition\nsystem and control system, which require run-time parameters. The original\ndesign and implementation of the configuration database played a significant\nrole in the successful BaBar operations since the beginning of experiment.\nRecent additions to the design of the configuration database provide better\nmeans for the management of data and add new tools to simplify main\nconfiguration tasks. We describe the design of the configuration database, its\nimplementation with the Objectivity/DB object-oriented database, and our\nexperience collected during the years of operation."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0306086v1", 
    "title": "GMA Instrumentation of the Athena Framework using NetLogger", 
    "arxiv-id": "cs/0306086v1", 
    "author": "Brian Tierney", 
    "publish": "2003-06-14T05:40:27Z", 
    "summary": "Grid applications are, by their nature, wide-area distributed applications.\nThis WAN aspect of Grid applications makes the use of conventional monitoring\nand instrumentation tools (such as top, gprof, LSF Monitor, etc) impractical\nfor verification that the application is running correctly and efficiently. To\nbe effective, monitoring data must be \"end-to-end\", meaning that all components\nbetween the Grid application endpoints must be monitored. Instrumented\napplications can generate a large amount of monitoring data, so typically the\ninstrumentation is off by default. For jobs running on a Grid, there needs to\nbe a general mechanism to remotely activate the instrumentation in running\njobs. The NetLogger Toolkit Activation Service provides this mechanism.\n  To demonstrate this, we have instrumented the ATLAS Athena Framework with\nNetLogger to generate monitoring events. We then use a GMA-based activation\nservice to control NetLogger's trigger mechanism. The NetLogger trigger\nmechanism allows one to easily start, stop, or change the logging level of a\nrunning program by modifying a trigger file. We present here details of the\ndesign of the NetLogger implementation of the GMA-based activation service and\nthe instrumentation service for Athena. We also describe how this activation\nservice allows us to non-intrusively collect and visualize the ATLAS Athena\nFramework monitoring data."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0306122v1", 
    "title": "The Best Trail Algorithm for Assisted Navigation of Web Sites", 
    "arxiv-id": "cs/0306122v1", 
    "author": "Mark Levene", 
    "publish": "2003-06-22T17:38:13Z", 
    "summary": "We present an algorithm called the Best Trail Algorithm, which helps solve\nthe hypertext navigation problem by automating the construction of memex-like\ntrails through the corpus. The algorithm performs a probabilistic best-first\nexpansion of a set of navigation trees to find relevant and compact trails. We\ndescribe the implementation of the algorithm, scoring methods for trails,\nfiltering algorithms and a new metric called \\emph{potential gain} which\nmeasures the potential of a page for future navigation opportunities."
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0307011v1", 
    "title": "Supporting Out-of-turn Interactions in a Multimodal Web Interface", 
    "arxiv-id": "cs/0307011v1", 
    "author": "Srinidhi Varadarajan", 
    "publish": "2003-07-04T13:44:04Z", 
    "summary": "Multimodal interfaces are becoming increasingly important with the advent of\nmobile devices, accessibility considerations, and novel software technologies\nthat combine diverse interaction media. This article investigates systems\nsupport for web browsing in a multimodal interface. Specifically, we outline\nthe design and implementation of a software framework that integrates hyperlink\nand speech modes of interaction. Instead of viewing speech as merely an\nalternative interaction medium, the framework uses it to support out-of-turn\ninteraction, providing a flexibility of information access not possible with\nhyperlinks alone. This approach enables the creation of websites that adapt to\nthe needs of users, yet permits the designer fine-grained control over what\ninteractions to support. Design methodology, implementation details, and two\ncase studies are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0309022v2", 
    "title": "Proposed Specification of a Distributed XML-Query Network", 
    "arxiv-id": "cs/0309022v2", 
    "author": "Thomas Severiens", 
    "publish": "2003-09-13T19:24:56Z", 
    "summary": "W3C's XML-Query language offers a powerful instrument for information\nretrieval on XML repositories. This article describes an implementation of this\nretrieval in a real world's scenario. Distributed XML-Query processing reduces\nload on every single attending node to an acceptable level. The network allows\nevery participant to control their computing load themselves. Furthermore\nXML-repositories may stay at the rights holder, so every Data-Provider can\ndecide, whether to process critical queries or not. If Data-Providers keep\nredundant information, this distributed network improves reliability of\ninformation with duplicates removed."
},{
    "category": "cs.HC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0310011v1", 
    "title": "Re-Finding Found Things: An Exploratory Study of How Users Re-Find   Information", 
    "arxiv-id": "cs/0310011v1", 
    "author": "Manuel A. Perez-Quinones", 
    "publish": "2003-10-06T19:36:00Z", 
    "summary": "The problem of how people find information is studied extensively; however,\nthe problem of how people organize, re-use, and re-find information that they\nhave found is not as well understood. Recently, several projects have conducted\nin-situ studies to explore how people re-find and re-use information. Here, we\npresent results and observations from a controlled, laboratory study of\nrefinding information found on the web.\n  Our study was conducted as a collaborative exercise with pairs of\nparticipants. One participant acted as a retriever, helping the other\nparticipant re-find information by telephone. This design allowed us to gain\ninsight into the strategies that users employed to re-find information, and\ninto how domain artifacts and contextual information were used to aid the\nre-finding process. We also introduced the ability for users to add their own\nexplicitly artifacts in the form of making annotations on the web pages they\nviewed.\n  We observe that re-finding often occurs as a two stage, iterative process in\nwhich users first attempt to locate an information source (search), and once\nfound, begin a process to find the specific information being sought (browse).\nOur findings are consistent with research on waypoints; orienteering approaches\nto re-finding; and navigation of electronic spaces. Furthermore, we observed\nthat annotations were utilized extensively, indicating that explicitly added\ncontext by the user can play an important role in re-finding."
},{
    "category": "cs.HC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0310013v1", 
    "title": "WebTeach in practice: the entrance test to the Engineering faculty in   Florence", 
    "arxiv-id": "cs/0310013v1", 
    "author": "Andrea Sterbini", 
    "publish": "2003-10-08T14:07:42Z", 
    "summary": "We present the WebTeach project, formed by a web interface to database for\ntest management, a wiki site for the diffusion of teaching material and student\nforums, and a suite for the generation of multiple-choice mathematical quiz\nwith automatic elaboration of forms. This system has been massively tested for\nthe entrance test to the Engineering Faculty of the University of Florence,\nItaly"
},{
    "category": "cs.IR", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0311029v1", 
    "title": "Staging Transformations for Multimodal Web Interaction Management", 
    "arxiv-id": "cs/0311029v1", 
    "author": "Naren Ramakrishnan", 
    "publish": "2003-11-20T17:27:45Z", 
    "summary": "Multimodal interfaces are becoming increasingly ubiquitous with the advent of\nmobile devices, accessibility considerations, and novel software technologies\nthat combine diverse interaction media. In addition to improving access and\ndelivery capabilities, such interfaces enable flexible and personalized dialogs\nwith websites, much like a conversation between humans. In this paper, we\npresent a software framework for multimodal web interaction management that\nsupports mixed-initiative dialogs between users and websites. A\nmixed-initiative dialog is one where the user and the website take turns\nchanging the flow of interaction. The framework supports the functional\nspecification and realization of such dialogs using staging transformations --\na theory for representing and reasoning about dialogs based on partial input.\nIt supports multiple interaction interfaces, and offers sessioning, caching,\nand co-ordination functions through the use of an interaction manager. Two case\nstudies are presented to illustrate the promise of this approach."
},{
    "category": "cs.CL", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0312008v1", 
    "title": "Embedding Web-based Statistical Translation Models in Cross-Language   Information Retrieval", 
    "arxiv-id": "cs/0312008v1", 
    "author": "Michel Simard", 
    "publish": "2003-12-03T13:09:28Z", 
    "summary": "Although more and more language pairs are covered by machine translation\nservices, there are still many pairs that lack translation resources.\nCross-language information retrieval (CLIR) is an application which needs\ntranslation functionality of a relatively low level of sophistication since\ncurrent models for information retrieval (IR) are still based on a\nbag-of-words. The Web provides a vast resource for the automatic construction\nof parallel corpora which can be used to train statistical translation models\nautomatically. The resulting translation models can be embedded in several ways\nin a retrieval model. In this paper, we will investigate the problem of\nautomatically mining parallel texts from the Web and different ways of\nintegrating the translation models within the retrieval process. Our\nexperiments on standard test collections for CLIR show that the Web-based\ntranslation models can surpass commercial MT systems in CLIR tasks. These\nresults open the perspective of constructing a fully automatic query\ntranslation device for CLIR at a very low cost."
},{
    "category": "cs.HC", 
    "doi": "10.1016/S0010-4655(01)00162-X", 
    "link": "http://arxiv.org/pdf/cs/0312016v1", 
    "title": "Taking the Initiative with Extempore: Exploring Out-of-Turn Interactions   with Websites", 
    "arxiv-id": "cs/0312016v1", 
    "author": "Mary Beth Rosson", 
    "publish": "2003-12-08T16:25:46Z", 
    "summary": "We present the first study to explore the use of out-of-turn interaction in\nwebsites. Out-of-turn interaction is a technique which empowers the user to\nsupply unsolicited information while browsing. This approach helps flexibly\nbridge any mental mismatch between the user and the website, in a manner\nfundamentally different from faceted browsing and site-specific search tools.\nWe built a user interface (Extempore) which accepts out-of-turn input via voice\nor text; and employed it in a US congressional website, to determine if users\nutilize out-of-turn interaction for information-finding tasks, and their\nrationale for doing so. The results indicate that users are adept at discerning\nwhen out-of-turn interaction is necessary in a particular task, and actively\ninterleaved it with browsing. However, users found cascading information across\ninformation-finding subtasks challenging. Therefore, this work not only\nimproves our understanding of out-of-turn interaction, but also suggests\nfurther opportunities to enrich browsing experiences for users."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0308253100", 
    "link": "http://arxiv.org/pdf/cs/0312018v1", 
    "title": "Mapping Subsets of Scholarly Information", 
    "arxiv-id": "cs/0312018v1", 
    "author": "Jae-Hoon Sul", 
    "publish": "2003-12-11T20:07:39Z", 
    "summary": "We illustrate the use of machine learning techniques to analyze, structure,\nmaintain, and evolve a large online corpus of academic literature. An emerging\nfield of research can be identified as part of an existing corpus, permitting\nthe implementation of a more coherent community structure for its\npractitioners."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0308253100", 
    "link": "http://arxiv.org/pdf/cs/0312033v1", 
    "title": "Using sensors in the web crawling process", 
    "arxiv-id": "cs/0312033v1", 
    "author": "Ilya Zemskov", 
    "publish": "2003-12-17T11:30:56Z", 
    "summary": "This paper offers a short description of an Internet information field\nmonitoring system, which places a special module-sensor on the side of the\nWeb-server to detect changes in information resources and subsequently\nreindexes only the resources signalized by the corresponding sensor. Concise\nresults of simulation research and an implementation attempt of the given\n\"sensors\" concept are provided."
},{
    "category": "cs.AI", 
    "doi": "10.1073/pnas.0308253100", 
    "link": "http://arxiv.org/pdf/cs/0312059v1", 
    "title": "Polyhierarchical Classifications Induced by Criteria Polyhierarchies,   and Taxonomy Algebra", 
    "arxiv-id": "cs/0312059v1", 
    "author": "Maria Babikova", 
    "publish": "2003-12-26T05:30:24Z", 
    "summary": "A new approach to the construction of general persistent polyhierarchical\nclassifications is proposed. It is based on implicit description of category\npolyhierarchy by a generating polyhierarchy of classification criteria.\nSimilarly to existing approaches, the classification categories are defined by\nlogical functions encoded by attributive expressions. However, the generating\nhierarchy explicitly predefines domains of criteria applicability, and the\nsemantics of relations between categories is invariant to changes in the\nuniverse composition, extending variety of criteria, and increasing their\ncardinalities. The generating polyhierarchy is an independent, compact,\nportable, and re-usable information structure serving as a template\nclassification. It can be associated with one or more particular sets of\nobjects, included in more general classifications as a standard component, or\nused as a prototype for more comprehensive classifications. The approach\ndramatically simplifies development and unplanned modifications of persistent\nhierarchical classifications compared with tree, DAG, and faceted schemes. It\ncan be efficiently implemented in common DBMS, while considerably reducing\namount of computer resources required for storage, maintenance, and use of\ncomplex polyhierarchies."
},{
    "category": "cs.HC", 
    "doi": "10.1073/pnas.0308253100", 
    "link": "http://arxiv.org/pdf/cs/0402001v1", 
    "title": "Mobile Re-Finding of Web Information Using a Voice Interface", 
    "arxiv-id": "cs/0402001v1", 
    "author": "Manuel A. Perez-Quinones", 
    "publish": "2004-01-31T20:09:39Z", 
    "summary": "Mobile access to information is a considerable problem for many users,\nespecially to information found on the Web. In this paper, we explore how a\nvoice-controlled service, accessible by telephone, could support mobile users'\nneeds for refinding specific information previously found on the Web. We\noutline challenges in creating such a service and describe architectural and\nuser interfaces issues discovered in an exploratory prototype we built called\nWebContext.\n  We also present the results of a study, motivated by our experience with\nWebContext, to explore what people remember about information that they are\ntrying to refind and how they express information refinding requests in a\ncollaborative conversation. As part of the study, we examine how\nend-usercreated Web page annotations can be used to help support mobile\ninformation re-finding. We observed the use of URLs, page titles, and\ndescriptions of page contents to help identify waypoints in the search process.\nFurthermore, we observed that the annotations were utilized extensively,\nindicating that explicitly added context by the user can play an important role\nin re-finding."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0402029v1", 
    "title": "Mapping Topics and Topic Bursts in PNAS", 
    "arxiv-id": "cs/0402029v1", 
    "author": "Katy B\u00f6rner", 
    "publish": "2004-02-14T03:55:53Z", 
    "summary": "Scientific research is highly dynamic. New areas of science continually\nevolve;others gain or lose importance, merge or split. Due to the steady\nincrease in the number of scientific publications it is hard to keep an\noverview of the structure and dynamic development of one's own field of\nscience, much less all scientific domains. However, knowledge of hot topics,\nemergent research frontiers, or change of focus in certain areas is a critical\ncomponent of resource allocation decisions in research labs, governmental\ninstitutions, and corporations. This paper demonstrates the utilization of\nKleinberg's burst detection algorithm, co-word occurrence analysis, and graph\nlayout techniques to generate maps that support the identification of major\nresearch topics and trends. The approach was applied to analyze and map the\ncomplete set of papers published in the Proceedings of the National Academy of\nSciences (PNAS) in the years 1982-2001. Six domain experts examined and\ncommented on the resulting maps in an attempt to reconstruct the evolution of\nmajor research areas covered by PNAS."
},{
    "category": "cs.AI", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0403001v1", 
    "title": "Evolving a Stigmergic Self-Organized Data-Mining", 
    "arxiv-id": "cs/0403001v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-02-28T23:50:45Z", 
    "summary": "Self-organizing complex systems typically are comprised of a large number of\nfrequently similar components or events. Through their process, a pattern at\nthe global-level of a system emerges solely from numerous interactions among\nthe lower-level components of the system. Moreover, the rules specifying\ninteractions among the system's components are executed using only local\ninformation, without reference to the global pattern, which, as in many\nreal-world problems is not easily accessible or possible to be found.\nStigmergy, a kind of indirect communication and learning by the environment\nfound in social insects is a well know example of self-organization, providing\nnot only vital clues in order to understand how the components can interact to\nproduce a complex pattern, as can pinpoint simple biological non-linear rules\nand methods to achieve improved artificial intelligent adaptive categorization\nsystems, critical for Data-Mining. On the present work it is our intention to\nshow that a new type of Data-Mining can be designed based on Stigmergic\nparadigms, taking profit of several natural features of this phenomenon. By\nhybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we\nseek for an entire distributed, adaptive, collective and cooperative\nself-organized Data-Mining. As a real-world, real-time test bed for our\nproposal, World-Wide-Web Mining will be used. Having that purpose in mind, Web\nusage Data was collected from the Monash University's Web site (Australia),\nwith over 7 million hits every week. Results are compared to other recent\nsystems, showing that the system presented is by far promising."
},{
    "category": "cs.NI", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0403035v1", 
    "title": "Web pages search engine based on DNS", 
    "arxiv-id": "cs/0403035v1", 
    "author": "Fang Ming", 
    "publish": "2004-03-23T09:20:26Z", 
    "summary": "Search engine is main access to the largest information source in this world,\nInternet. Now Internet is changing every aspect of our life. Information\nretrieval service may be its most important services. But for common user,\ninternet search service is still far from our expectation, too many unrelated\nsearch results, old information, etc. To solve these problems, a new system,\nsearch engine based on DNS is proposed. The original idea, detailed content and\nimplementation of this system all are introduced in this paper."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0405044v1", 
    "title": "Corpus structure, language models, and ad hoc information retrieval", 
    "arxiv-id": "cs/0405044v1", 
    "author": "Lillian Lee", 
    "publish": "2004-05-12T20:18:51Z", 
    "summary": "Most previous work on the recently developed language-modeling approach to\ninformation retrieval focuses on document-specific characteristics, and\ntherefore does not take into account the structure of the surrounding corpus.\nWe propose a novel algorithmic framework in which information provided by\ndocument-based language models is enhanced by the incorporation of information\ndrawn from clusters of similar documents. Using this framework, we develop a\nsuite of new algorithms. Even the simplest typically outperforms the standard\nlanguage-modeling approach in precision and recall, and our new interpolation\nalgorithm posts statistically significant improvements for both metrics over\nall three corpora tested."
},{
    "category": "cs.DB", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0405069v1", 
    "title": "Mining Frequent Itemsets from Secondary Memory", 
    "arxiv-id": "cs/0405069v1", 
    "author": "Jianfei Zhu", 
    "publish": "2004-05-20T14:33:08Z", 
    "summary": "Mining frequent itemsets is at the core of mining association rules, and is\nby now quite well understood algorithmically. However, most algorithms for\nmining frequent itemsets assume that the main memory is large enough for the\ndata structures used in the mining, and very few efficient algorithms deal with\nthe case when the database is very large or the minimum support is very low.\nMining frequent itemsets from a very large database poses new challenges, as\nastronomical amounts of raw data is ubiquitously being recorded in commerce,\nscience and government. In this paper, we discuss approaches to mining frequent\nitemsets when data structures are too large to fit in main memory. Several\ndivide-and-conquer algorithms are given for mining from disks. Many novel\ntechniques are introduced. Experimental results show that the techniques reduce\nthe required disk accesses by orders of magnitude, and enable truly scalable\ndata mining."
},{
    "category": "cs.NI", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0405099v1", 
    "title": "Web search engine based on DNS", 
    "arxiv-id": "cs/0405099v1", 
    "author": "Fang Ming", 
    "publish": "2004-05-27T02:36:44Z", 
    "summary": "Now no web search engine can cover more than 60 percent of all the pages on\nInternet. The update interval of most pages database is almost one month. This\ncondition hasn't changed for many years. Converge and recency problems have\nbecome the bottleneck problem of current web search engine. To solve these\nproblems, a new system, search engine based on DNS is proposed in this paper.\nThis system adopts the hierarchical distributed architecture like DNS, which is\ndifferent from any current commercial search engine. In theory, this system can\ncover all the web pages on Internet. Its update interval could even be one day.\nThe original idea, detailed content and implementation of this system all are\nintroduced in this paper."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0406032v1", 
    "title": "A Dynamic Clustering-Based Markov Model for Web Usage Mining", 
    "arxiv-id": "cs/0406032v1", 
    "author": "Mark Levene", 
    "publish": "2004-06-17T13:38:17Z", 
    "summary": "Markov models have been widely utilized for modelling user web navigation\nbehaviour. In this work we propose a dynamic clustering-based method to\nincrease a Markov model's accuracy in representing a collection of user web\nnavigation sessions. The method makes use of the state cloning concept to\nduplicate states in a way that separates in-links whose corresponding\nsecond-order probabilities diverge. In addition, the new method incorporates a\nclustering technique which determines an effcient way to assign in-links with\nsimilar second-order probabilities to the same clone. We report on experiments\nconducted with both real and random data and we provide a comparison with the\nN-gram Markov concept. The results show that the number of additional states\ninduced by the dynamic clustering method can be controlled through a threshold\nparameter, and suggest that the method's performance is linear time in the size\nof the model."
},{
    "category": "cs.AI", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0407009v1", 
    "title": "Search Using N-gram Technique Based Statistical Analysis for Knowledge   Extraction in Case Based Reasoning Systems", 
    "arxiv-id": "cs/0407009v1", 
    "author": "Moshe Davis", 
    "publish": "2004-07-02T20:22:18Z", 
    "summary": "Searching techniques for Case Based Reasoning systems involve extensive\nmethods of elimination. In this paper, we look at a new method of arriving at\nthe right solution by performing a series of transformations upon the data.\nThese involve N-gram based comparison and deduction of the input data with the\ncase data, using Morphemes and Phonemes as the deciding parameters. A similar\ntechnique for eliminating possible errors using a noise removal function is\nperformed. The error tracking and elimination is performed through a\nstatistical analysis of obtained data, where the entire data set is analyzed as\nsub-categories of various etymological derivatives. A probability analysis for\nthe closest match is then performed, which yields the final expression. This\nfinal expression is referred to the Case Base. The output is redirected through\nan Expert System based on best possible match. The threshold for the match is\ncustomizable, and could be set by the Knowledge-Architect."
},{
    "category": "cs.DB", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0407035v1", 
    "title": "A Framework for High-Accuracy Privacy-Preserving Mining", 
    "arxiv-id": "cs/0407035v1", 
    "author": "Jayant R. Haritsa", 
    "publish": "2004-07-15T14:30:20Z", 
    "summary": "To preserve client privacy in the data mining process, a variety of\ntechniques based on random perturbation of data records have been proposed\nrecently. In this paper, we present a generalized matrix-theoretic model of\nrandom perturbation, which facilitates a systematic approach to the design of\nperturbation mechanisms for privacy-preserving mining. Specifically, we\ndemonstrate that (a) the prior techniques differ only in their settings for the\nmodel parameters, and (b) through appropriate choice of parameter settings, we\ncan derive new perturbation techniques that provide highly accurate mining\nresults even under strict privacy guarantees. We also propose a novel\nperturbation mechanism wherein the model parameters are themselves\ncharacterized as random variables, and demonstrate that this feature provides\nsignificant improvements in privacy at a very marginal cost in accuracy.\n  While our model is valid for random-perturbation-based privacy-preserving\nmining in general, we specifically evaluate its utility here with regard to\nfrequent-itemset mining on a variety of real datasets. The experimental results\nindicate that our mechanisms incur substantially lower identity and support\nerrors as compared to the prior techniques."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0407053v1", 
    "title": "Design of a Parallel and Distributed Web Search Engine", 
    "arxiv-id": "cs/0407053v1", 
    "author": "Fabrizio Silvestri", 
    "publish": "2004-07-21T07:21:50Z", 
    "summary": "This paper describes the architecture of MOSE (My Own Search Engine), a\nscalable parallel and distributed engine for searching the web. MOSE was\nspecifically designed to efficiently exploit affordable parallel architectures,\nsuch as clusters of workstations. Its modular and scalable architecture can\neasily be tuned to fulfill the bandwidth requirements of the application at\nhand. Both task-parallel and data-parallel approaches are exploited within MOSE\nin order to increase the throughput and efficiently use communication, storing\nand computational resources. We used a collection of html documents as a\nbenchmark, and conducted preliminary experiments on a cluster of three SMP\nLinux PCs."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0408001v1", 
    "title": "Semantic Linking - a Context-Based Approach to Interactivity in   Hypermedia", 
    "arxiv-id": "cs/0408001v1", 
    "author": "Thomas C. Schmidt", 
    "publish": "2004-07-31T14:04:04Z", 
    "summary": "The semantic Web initiates new, high level access schemes to online content\nand applications. One area of superior need for a redefined content exploration\nis given by on-line educational applications and their concepts of\ninteractivity in the framework of open hypermedia systems. In the present paper\nwe discuss aspects and opportunities of gaining interactivity schemes from\nsemantic notions of components. A transition from standard educational\nannotation to semantic statements of hyperlinks is discussed. Further on we\nintroduce the concept of semantic link contexts as an approach to manage a\ncoherent rhetoric of linking. A practical implementation is introduced, as\nwell. Our semantic hyperlink implementation is based on the more general\nMultimedia Information Repository MIR, an open hypermedia system supporting the\nstandards XML, Corba and JNDI."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0408004v1", 
    "title": "Hypermedia Learning Objects System - On the Way to a Semantic   Educational Web", 
    "arxiv-id": "cs/0408004v1", 
    "author": "Thomas C. Schmidt", 
    "publish": "2004-07-31T22:16:37Z", 
    "summary": "While eLearning systems become more and more popular in daily education,\navailable applications lack opportunities to structure, annotate and manage\ntheir contents in a high-level fashion. General efforts to improve these\ndeficits are taken by initiatives to define rich meta data sets and a\nsemanticWeb layer. In the present paper we introduce Hylos, an online learning\nsystem. Hylos is based on a cellular eLearning Object (ELO) information model\nencapsulating meta data conforming to the LOM standard. Content management is\nprovisioned on this semantic meta data level and allows for variable,\ndynamically adaptable access structures. Context aware multifunctional links\npermit a systematic navigation depending on the learners and didactic needs,\nthereby exploring the capabilities of the semantic web. Hylos is built upon the\nmore general Multimedia Information Repository (MIR) and the MIR adaptive\ncontext linking environment (MIRaCLE), its linking extension. MIR is an open\nsystem supporting the standards XML, Corba and JNDI. Hylos benefits from\nmanageable information structures, sophisticated access logic and high-level\nauthoring tools like the ELO editor responsible for the semi-manual creation of\nmeta data and WYSIWYG like content editing."
},{
    "category": "cs.CY", 
    "doi": "10.1073/pnas.0307626100", 
    "link": "http://arxiv.org/pdf/cs/0408005v1", 
    "title": "Educational Content Management - A Cellular Approach", 
    "arxiv-id": "cs/0408005v1", 
    "author": "Thomas C. Schmidt", 
    "publish": "2004-08-01T11:34:32Z", 
    "summary": "In recent times online educational applications more and more are requested\nto provide self-consistent learning offers for students at the university\nlevel. Consequently they need to cope with the wide range of complexity and\ninterrelations university course teaching brings along. An urgent need to\novercome simplistically linked HTMLc ontent pages becomes apparent. In the\npresent paper we discuss a schematic concept of educational content\nconstruction from information cells and introduce its implementation on the\nstorage and runtime layer. Starting from cells content is annotated according\nto didactic needs, structured for dynamic arrangement, dynamically decorated\nwith hyperlinks and, as all works are based on XML, open to any presentation\nlayer. Data can be variably accessed through URIs built on semantic path-names\nand edited via an adaptive authoring toolbox. Our content management approach\nis based on the more general Multimedia Information Repository MIR. and allows\nfor personalisation, as well. MIR is an open system supporting the standards\nXML, Corba and JNDI."
},{
    "category": "cs.IR", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0408063v1", 
    "title": "Analysis and Visualization of Index Words from Audio Transcripts of   Instructional Videos", 
    "arxiv-id": "cs/0408063v1", 
    "author": "John R. Kender", 
    "publish": "2004-08-27T20:45:32Z", 
    "summary": "We introduce new techniques for extracting, analyzing, and visualizing\ntextual contents from instructional videos of low production quality. Using\nAutomatic Speech Recognition, approximate transcripts (H75% Word Error Rate)\nare obtained from the originally highly compressed videos of university\ncourses, each comprising between 10 to 30 lectures. Text material in the form\nof books or papers that accompany the course are then used to filter meaningful\nphrases from the seemingly incoherent transcripts. The resulting index into the\ntranscripts is tied together and visualized in 3 experimental graphs that help\nin understanding the overall course structure and provide a tool for localizing\ncertain topics for indexing. We specifically discuss a Transcript Index Map,\nwhich graphically lays out key phrases for a course, a Textbook Chapter to\nTranscript Match, and finally a Lecture Transcript Similarity graph, which\nclusters semantically similar lectures. We test our methods and tools on 7 full\ncourses with 230 hours of video and 273 transcripts. We are able to extract up\nto 98 unique key terms for a given transcript and up to 347 unique key terms\nfor an entire course. The accuracy of the Textbook Chapter to Transcript Match\nexceeds 70% on average. The methods used can be applied to genres of video in\nwhich there are recurrent thematic words (news, sports, meetings,...)"
},{
    "category": "cs.NI", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0411069v1", 
    "title": "CDN: Content Distribution Network", 
    "arxiv-id": "cs/0411069v1", 
    "author": "Gang Peng", 
    "publish": "2004-11-18T23:16:08Z", 
    "summary": "Internet evolves and operates largely without a central coordination, the\nlack of which was and is critically important to the rapid growth and evolution\nof Internet. However, the lack of management in turn makes it very difficult to\nguarantee proper performance and to deal systematically with performance\nproblems. Meanwhile, the available network bandwidth and server capacity\ncontinue to be overwhelmed by the skyrocketing Internet utilization and the\naccelerating growth of bandwidth intensive content. As a result, Internet\nservice quality perceived by customers is largely unpredictable and\nunsatisfactory. Content Distribution Network (CDN) is an effective approach to\nimprove Internet service quality. CDN replicates the content from the place of\norigin to the replica servers scattered over the Internet and serves a request\nfrom a replica server close to where the request originates. In this paper, we\nfirst give an overview about CDN. We then present the critical issues involved\nin designing and implementing an effective CDN and survey the approaches\nproposed in literature to address these problems. An example of CDN is\ndescribed to show how a real commercial CDN operates. After this, we present a\nscheme that provides fast service location for peer-to-peer systems, a special\ntype of CDN with no infrastructure support. We conclude with a brief projection\nabout CDN."
},{
    "category": "cs.CL", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0411074v1", 
    "title": "Building Chinese Lexicons from Scratch by Unsupervised Short Document   Self-Segmentation", 
    "arxiv-id": "cs/0411074v1", 
    "author": "Daniel Gayo-Avello", 
    "publish": "2004-11-20T00:22:30Z", 
    "summary": "Chinese text segmentation is a well-known and difficult problem. On one side,\nthere is not a simple notion of \"word\" in Chinese language making really hard\nto implement rule-based systems to segment written texts, thus lexicons and\nstatistical information are usually employed to achieve such a task. On the\nother side, any piece of Chinese text usually includes segments present neither\nin the lexicons nor in the training data. Even worse, such unseen sequences can\nbe segmented into a number of totally unrelated words making later processing\nphases difficult. For instance, using a lexicon-based system the sequence\n???(Baluozuo, Barroso, current president-designate of the European Commission)\ncan be segmented into ?(ba, to hope, to wish) and ??(luozuo, an undefined word)\nchanging completely the meaning of the sentence. A new and extremely simple\nalgorithm specially suited to work over short Chinese documents is introduced.\nThis new algorithm performs text \"self-segmentation\" producing results\ncomparable to those achieved by native speakers without using either lexicons\nor any statistical information beyond the obtained from the input text.\nFurthermore, it is really robust for finding new \"words\", especially proper\nnouns, and it is well suited to build lexicons from scratch. Some preliminary\nresults are provided in addition to examples of its employment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0412002v3", 
    "title": "Ranking Pages by Topology and Popularity within Web Sites", 
    "arxiv-id": "cs/0412002v3", 
    "author": "Mark Levene", 
    "publish": "2004-12-01T13:22:47Z", 
    "summary": "We compare two link analysis ranking methods of web pages in a site. The\nfirst, called Site Rank, is an adaptation of PageRank to the granularity of a\nweb site and the second, called Popularity Rank, is based on the frequencies of\nuser clicks on the outlinks in a page that are captured by navigation sessions\nof users through the web site. We ran experiments on artificially created web\nsites of different sizes and on two real data sets, employing the relative\nentropy to compare the distributions of the two ranking methods. For the real\ndata sets we also employ a nonparametric measure, called Spearman's footrule,\nwhich we use to compare the top-ten web pages ranked by the two methods. Our\nmain result is that the distributions of the Popularity Rank and Site Rank are\nsurprisingly close to each other, implying that the topology of a web site is\nvery instrumental in guiding users through the site. Thus, in practice, the\nSite Rank provides a reasonable first order approximation of the aggregate\nbehaviour of users within a web site given by the Popularity Rank."
},{
    "category": "cs.DL", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0501019v1", 
    "title": "Clustering SPIRES with EqRank", 
    "arxiv-id": "cs/0501019v1", 
    "author": "S. E. Trunov", 
    "publish": "2005-01-11T05:00:03Z", 
    "summary": "SPIRES is the largest database of scientific papers in the subject field of\nhigh energy and nuclear physics. It contains information on the citation graph\nof more than half a million of papers (vertexes of the citation graph). We\noutline the EqRank algorithm designed to cluster vertexes of directed graphs,\nand present the results of EqRank application to the SPIRES citation graph. The\nhierarchical clustering of SPIRES yielded by EqRank is used to set up a web\nservice, which is also outlined."
},{
    "category": "cs.MM", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0501044v1", 
    "title": "Augmented Segmentation and Visualization for Presentation Videos", 
    "arxiv-id": "cs/0501044v1", 
    "author": "John R. Kender", 
    "publish": "2005-01-20T17:12:05Z", 
    "summary": "We investigate methods of segmenting, visualizing, and indexing presentation\nvideos by separately considering audio and visual data. The audio track is\nsegmented by speaker, and augmented with key phrases which are extracted using\nan Automatic Speech Recognizer (ASR). The video track is segmented by visual\ndissimilarities and augmented by representative key frames. An interactive user\ninterface combines a visual representation of audio, video, text, and key\nframes, and allows the user to navigate a presentation video. We also explore\nclustering and labeling of speaker data and present preliminary results."
},{
    "category": "cs.IR", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0501077v2", 
    "title": "Ontology-Based Users & Requests Clustering in Customer Service   Management System", 
    "arxiv-id": "cs/0501077v2", 
    "author": "Alexey Kashevnik", 
    "publish": "2005-01-26T14:11:38Z", 
    "summary": "Customer Service Management is one of major business activities to better\nserve company customers through the introduction of reliable processes and\nprocedures. Today this kind of activities is implemented through e-services to\ndirectly involve customers into business processes. Traditionally Customer\nService Management involves application of data mining techniques to discover\nusage patterns from the company knowledge memory. Hence grouping of\ncustomers/requests to clusters is one of major technique to improve the level\nof company customization. The goal of this paper is to present an efficient for\nimplementation approach for clustering users and their requests. The approach\nuses ontology as knowledge representation model to improve the semantic\ninteroperability between units of the company and customers. Some fragments of\nthe approach tested in an industrial company are also presented in the paper."
},{
    "category": "cs.CR", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0502052v1", 
    "title": "Log Analysis Case Study Using LoGS", 
    "arxiv-id": "cs/0502052v1", 
    "author": "Dmitry Mogilevsky", 
    "publish": "2005-02-09T19:49:10Z", 
    "summary": "A very useful technique a network administrator can use to identify\nproblematic network behavior is careful analysis of logs of incoming and\noutgoing network flows. The challenge one faces when attempting to undertake\nthis course of action, though, is that large networks tend to generate an\nextremely large quantity of network traffic in a very short period of time,\nresulting in very large traffic logs which must be analyzed post-generation\nwith an eye for contextual information which may reveal symptoms of problematic\ntraffic. A better technique is to perform real-time log analysis using a\nreal-time context-generating tool such as LoGS."
},{
    "category": "cs.CV", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0503001v1", 
    "title": "Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but   actually it is not)", 
    "arxiv-id": "cs/0503001v1", 
    "author": "Emanuel Diamant", 
    "publish": "2005-03-01T05:17:33Z", 
    "summary": "Pattern recognition is generally assumed as an interaction of two inversely\ndirected image-processing streams: the bottom-up information details gathering\nand localization (segmentation) stream, and the top-down information features\naggregation, association and interpretation (recognition) stream. Inspired by\nrecent evidence from biological vision research and by the insights of\nKolmogorov Complexity theory, we propose a new, just top-down evolving,\nprocedure of initial image segmentation. We claim that traditional top-down\ncognitive reasoning, which is supposed to guide the segmentation process to its\nfinal result, is not at all a part of the image information content evaluation.\nAnd that initial image segmentation is certainly an unsupervised process. We\npresent some illustrative examples, which support our claims."
},{
    "category": "cs.CL", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0503033v1", 
    "title": "An Introduction to the Summarization of Evolving Events: Linear and   Non-linear Evolution", 
    "arxiv-id": "cs/0503033v1", 
    "author": "Vangelis Karkaletsis", 
    "publish": "2005-03-15T15:31:14Z", 
    "summary": "This paper examines the summarization of events that evolve through time. It\ndiscusses different types of evolution taking into account the time in which\nthe incidents of an event are happening and the different sources reporting on\nthe specific event. It proposes an approach for multi-document summarization\nwhich employs ``messages'' for representing the incidents of an event and\ncross-document relations that hold between messages according to certain\nconditions. The paper also outlines the current version of the summarization\nsystem we are implementing to realize this approach."
},{
    "category": "cs.IR", 
    "doi": "10.1109/MMSE.2004.27", 
    "link": "http://arxiv.org/pdf/cs/0504036v1", 
    "title": "Scientific impact quantity and quality: Analysis of two sources of   bibliographic data", 
    "arxiv-id": "cs/0504036v1", 
    "author": "Richard K. Belew", 
    "publish": "2005-04-11T13:52:55Z", 
    "summary": "Attempts to understand the consequence of any individual scientist's activity\nwithin the long-term trajectory of science is one of the most difficult\nquestions within the philosophy of science. Because scientific publications\nplay such as central role in the modern enterprise of science, bibliometric\ntechniques which measure the ``impact'' of an individual publication as a\nfunction of the number of citations it receives from subsequent authors have\nprovided some of the most useful empirical data on this question. Until\nrecently, Thompson/ISI has provided the only source of large-scale ``inverted''\nbibliographic data of the sort required for impact analysis. In the end of\n2004, Google introduced a new service, GoogleScholar, making much of this same\ndata available. Here we analyze 203 publications, collectively cited by more\nthan 4000 other publications. We show surprisingly good agreement between data\ncitation counts provided by the two services. Data quality across the systems\nis analyzed, and potentially useful complementarities between are considered.\nThe additional robustness offered by multiple sources of such data promises to\nincrease the utility of these measurements as open citation protocols and open\naccess increase their impact on electronic scientific publication practices."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0504061v1", 
    "title": "Summarization from Medical Documents: A Survey", 
    "arxiv-id": "cs/0504061v1", 
    "author": "Panagiotis Stamatopoulos", 
    "publish": "2005-04-13T20:02:25Z", 
    "summary": "Objective:\n  The aim of this paper is to survey the recent work in medical documents\nsummarization.\n  Background:\n  During the last decade, documents summarization got increasing attention by\nthe AI research community. More recently it also attracted the interest of the\nmedical research community as well, due to the enormous growth of information\nthat is available to the physicians and researchers in medicine, through the\nlarge and growing number of published journals, conference proceedings, medical\nsites and portals on the World Wide Web, electronic medical records, etc.\n  Methodology:\n  This survey gives first a general background on documents summarization,\npresenting the factors that summarization depends upon, discussing evaluation\nissues and describing briefly the various types of summarization techniques. It\nthen examines the characteristics of the medical domain through the different\ntypes of medical documents. Finally, it presents and discusses the\nsummarization techniques used so far in the medical domain, referring to the\ncorresponding systems and their characteristics.\n  Discussion and conclusions:\n  The paper discusses thoroughly the promising paths for future research in\nmedical documents summarization. It mainly focuses on the issue of scaling to\nlarge collections of documents in various languages and from different media,\non personalization issues, on portability to new sub-domains, and on the\nintegration of summarization technology in practical applications"
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0504063v1", 
    "title": "Selection in Scale-Free Small World", 
    "arxiv-id": "cs/0504063v1", 
    "author": "A. Lorincz", 
    "publish": "2005-04-14T07:57:01Z", 
    "summary": "In this paper we compare the performance characteristics of our selection\nbased learning algorithm for Web crawlers with the characteristics of the\nreinforcement learning algorithm. The task of the crawlers is to find new\ninformation on the Web. The selection algorithm, called weblog update, modifies\nthe starting URL lists of our crawlers based on the found URLs containing new\ninformation. The reinforcement learning algorithm modifies the URL orderings of\nthe crawlers based on the received reinforcements for submitted documents. We\nperformed simulations based on data collected from the Web. The collected\nportion of the Web is typical and exhibits scale-free small world (SFSW)\nstructure. We have found that on this SFSW, the weblog update algorithm\nperforms better than the reinforcement learning algorithm. It finds the new\ninformation faster than the reinforcement learning algorithm and has better new\ninformation/all submitted documents ratio. We believe that the advantages of\nthe selection algorithm over reinforcement learning algorithm is due to the\nsmall world property of the Web."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0505008v1", 
    "title": "Data Mining on Crash Simulation Data", 
    "arxiv-id": "cs/0505008v1", 
    "author": "C. -A. Thole", 
    "publish": "2005-05-02T15:27:45Z", 
    "summary": "The work presented in this paper is part of the cooperative research project\nAUTO-OPT carried out by twelve partners from the automotive industries. One\nmajor work package concerns the application of data mining methods in the area\nof automotive design. Suitable methods for data preparation and data analysis\nare developed. The objective of the work is the re-use of data stored in the\ncrash-simulation department at BMW in order to gain deeper insight into the\ninterrelations between the geometric variations of the car during its design\nand its performance in crash testing. In this paper a method for data analysis\nof finite element models and results from crash simulation is proposed and\napplication to recent data from the industrial partner BMW is demonstrated. All\nnecessary steps from data pre-processing to re-integration into the working\nenvironment of the engineer are covered."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0505051v1", 
    "title": "Sub-Optimum Signal Linear Detector Using Wavelets and Support Vector   Machines", 
    "arxiv-id": "cs/0505051v1", 
    "author": "Diego Andina", 
    "publish": "2005-05-20T14:54:40Z", 
    "summary": "The problem of known signal detection in Additive White Gaussian Noise is\nconsidered. In previous work, a new detection scheme was introduced by the\nauthors, and it was demonstrated that optimum performance cannot be reached in\na real implementation. In this paper we analyse Support Vector Machines (SVM)\nas an alternative, evaluating the results in terms of Probability of detection\ncurves for a fixed Probability of false alarm."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0505052v1", 
    "title": "Upgrading Pulse Detection with Time Shift Properties Using Wavelets and   Support Vector Machines", 
    "arxiv-id": "cs/0505052v1", 
    "author": "Juan Seijas", 
    "publish": "2005-05-20T15:01:20Z", 
    "summary": "Current approaches in pulse detection use domain transformations so as to\nconcentrate frequency related information that can be distinguishable from\nnoise. In real cases we do not know when the pulse will begin, so we need a\ntime search process in which time windows are scheduled and analysed. Each\nwindow can contain the pulsed signal (either complete or incomplete) and / or\nnoise. In this paper a simple search process will be introduced, allowing the\nalgorithm to process more information, upgrading the capabilities in terms of\nprobability of detection (Pd) and probability of false alarm (Pfa)."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0505053v1", 
    "title": "Wavelet Time Shift Properties Integration with Support Vector Machines", 
    "arxiv-id": "cs/0505053v1", 
    "author": "Juan Seijas", 
    "publish": "2005-05-20T15:06:40Z", 
    "summary": "This paper presents a short evaluation about the integration of information\nderived from wavelet non-linear-time-invariant (non-LTI) projection properties\nusing Support Vector Machines (SVM). These properties may give additional\ninformation for a classifier trying to detect known patterns hidden by noise.\nIn the experiments we present a simple electromagnetic pulsed signal\nrecognition scheme, where some improvement is achieved with respect to previous\nwork. SVMs are used as a tool for information integration, exploiting some\nunique properties not easily found in neural networks."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0506017v1", 
    "title": "Treillis de concepts et ontologies pour l'interrogation d'un annuaire de   sources de donn\u00e9es biologiques (BioRegistry)", 
    "arxiv-id": "cs/0506017v1", 
    "author": "Amedeo Napoli", 
    "publish": "2005-06-06T12:49:53Z", 
    "summary": "Bioinformatic data sources available on the web are multiple and\nheterogenous. The lack of documentation and the difficulty of interaction with\nthese data sources require users competence in both informatics and biological\nfields for an optimal use of sources contents that remain rather under\nexploited. In this paper we present an approach based on formal concept\nanalysis to classify and search relevant bioinformatic data sources for a given\nquery. It consists in building the concept lattice from the binary relation\nbetween bioinformatic data sources and their associated metadata. The concept\nbuilt from a given query is then merged into the concept lattice. The result is\ngiven by the extraction of the set of sources belonging to the extents of the\nquery concept subsumers in the resulting concept lattice. The sources ranking\nis given by the concept specificity order in the concept lattice. An\nimprovement of the approach consists in automatic query refinement thanks to\ndomain ontologies. Two forms of refinement are possible by generalisation and\nby specialisation.\n  -----\n  Les sources de donn\\'{e}es biologiques disponibles sur le web sont multiples\net h\\'{e}t\\'{e}rog\\`{e}nes. L'utilisation optimale de ces ressources\nn\\'{e}cessite aujourd'hui de la part des utilisateurs des comp\\'{e}tences \\`{a}\nla fois en informatique et en biologie, du fait du manque de documentation et\ndes difficult\\'{e}s d'interaction avec les sources de donn\\'{e}es. De fait, les\ncontenus de ces ressources restent souvent sous-exploit\\'{e}s. Nous\npr\\'{e}sentons ici une approche bas\\'{e}e sur l'analyse de concepts formels,\npour organiser et rechercher des sources de donn\\'{e}es biologiques pertinentes\npour une requ\\^{e}te donn\\'{e}e. Le travail consiste \\`{a} construire un\ntreillis de concepts \\`{a} partir des m\\'{e}ta-donn\\'{e}es associ\\'{e}es aux\nsources. Le concept construit \\`{a} partir d'une requ\\^{e}te donn\\'{e}e est\nalors int\\'{e}gr\\'{e} au treillis. La r\\'{e}ponse \\`{a} la requ\\^{e}te est\nensuite fournie par l'extraction des sources de donn\\'{e}es appartenant aux\nextensions des concepts subsumant le concept requ\\^{e}te dans le treillis. Les\nsources ainsi retourn\\'{e}es peuvent \\^{e}tre tri\\'{e}es selon l'ordre de\nsp\\'{e}cificit\\'{e} des concepts dans le treillis. Une proc\\'{e}dure de\nraffinement de requ\\^{e}te, bas\\'{e}e sur des ontologies du domaine, permet\nd'am\\'{e}liorer le rappel par g\\'{e}n\\'{e}ralisation ou par sp\\'{e}cialisation"
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0506078v1", 
    "title": "Dynamical Neural Network: Information and Topology", 
    "arxiv-id": "cs/0506078v1", 
    "author": "Francisco B. Rodriguez", 
    "publish": "2005-06-20T14:54:41Z", 
    "summary": "A neural network works as an associative memory device if it has large\nstorage capacity and the quality of the retrieval is good enough. The learning\nand attractor abilities of the network both can be measured by the mutual\ninformation (MI), between patterns and retrieval states. This paper deals with\na search for an optimal topology, of a Hebb network, in the sense of the\nmaximal MI. We use small-world topology. The connectivity $\\gamma$ ranges from\nan extremely diluted to the fully connected network; the randomness $\\omega$\nranges from purely local to completely random neighbors. It is found that,\nwhile stability implies an optimal $MI(\\gamma,\\omega)$ at\n$\\gamma_{opt}(\\omega)\\to 0$, for the dynamics, the optimal topology holds at\ncertain $\\gamma_{opt}>0$ whenever $0\\leq\\omega<0.3$."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0508092v1", 
    "title": "Summarizing Reports on Evolving Events; Part I: Linear Evolution", 
    "arxiv-id": "cs/0508092v1", 
    "author": "Panagiotis Stamatopoulos", 
    "publish": "2005-08-22T12:56:32Z", 
    "summary": "We present an approach for summarization from multiple documents which report\non events that evolve through time, taking into account the different document\nsources. We distinguish the evolution of an event into linear and non-linear.\nAccording to our approach, each document is represented by a collection of\nmessages which are then used in order to instantiate the cross-document\nrelations that determine the summary content. The paper presents the\nsummarization system that implements this approach through a case study on\nlinear evolution."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0509020v1", 
    "title": "Transitive Text Mining for Information Extraction and Hypothesis   Generation", 
    "arxiv-id": "cs/0509020v1", 
    "author": "Guenter Grohmann", 
    "publish": "2005-09-07T12:16:22Z", 
    "summary": "Transitive text mining - also named Swanson Linking (SL) after its primary\nand principal researcher - tries to establish meaningful links between\nliterature sets which are virtually disjoint in the sense that each does not\nmention the main concept of the other. If successful, SL may give rise to the\ndevelopment of new hypotheses. In this communication we describe our approach\nto transitive text mining which employs co-occurrence analysis of the medical\nsubject headings (MeSH), the descriptors assigned to papers indexed in PubMed.\nIn addition, we will outline the current state of our web-based information\nsystem which will enable our users to perform literature-driven hypothesis\nbuilding on their own."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0509040v1", 
    "title": "Authoring case based training by document data extraction", 
    "arxiv-id": "cs/0509040v1", 
    "author": "Frank Puppe", 
    "publish": "2005-09-14T13:20:21Z", 
    "summary": "In this paper, we propose an scalable approach to modeling based upon word\nprocessing documents, and we describe the tool Phoenix providing the technical\ninfrastructure.\n  For our training environment d3web.Train, we developed a tool to extract case\nknowledge from existing documents, usually dismissal records, extending Phoenix\nto d3web.CaseImporter. Independent authors used this tool to develop training\nsystems, observing a significant decrease of time for setteling-in and a\ndecrease of time necessary for developing a case."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0510020v1", 
    "title": "Sur le statut r\u00e9f\u00e9rentiel des entit\u00e9s nomm\u00e9es", 
    "arxiv-id": "cs/0510020v1", 
    "author": "Thierry Poibeau", 
    "publish": "2005-10-07T17:39:40Z", 
    "summary": "We show in this paper that, on the one hand, named entities can be designated\nusing different denominations and that, on the second hand, names denoting\nnamed entities are polysemous. The analysis cannot be limited to reference\nresolution but should take into account naming strategies, which are mainly\nbased on two linguistic operations: synecdoche and metonymy. Lastly, we present\na model that explicitly represents the different denominations in discourse,\nunifying the way to represent linguistic knowledge and world knowledge."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0510037v1", 
    "title": "Hi\u00e9rarchisation des r\u00e8gles d'association en fouille de textes", 
    "arxiv-id": "cs/0510037v1", 
    "author": "Amedeo Napoli", 
    "publish": "2005-10-14T14:24:26Z", 
    "summary": "Extraction of association rules is widely used as a data mining method.\nHowever, one of the limit of this approach comes from the large number of\nextracted rules and the difficulty for a human expert to deal with the totality\nof these rules. We propose to solve this problem by structuring the set of\nrules into hierarchy. The expert can then therefore explore the rules, access\nfrom one rule to another one more general when we raise up in the hierarchy,\nand in other hand, or a more specific rules. Rules are structured at two\nlevels. The global level aims at building a hierarchy from the set of rules\nextracted. Thus we define a first type of rule-subsomption relying on Galois\nlattices. The second level consists in a local and more detailed analysis of\neach rule. It generate for a given rule a set of generalization rules\nstructured into a local hierarchy. This leads to the definition of a second\ntype of subsomption. This subsomption comes from inductive logic programming\nand integrates a terminological model."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0510054v1", 
    "title": "The Nature of Novelty Detection", 
    "arxiv-id": "cs/0510054v1", 
    "author": "Shaoping Ma", 
    "publish": "2005-10-19T14:56:48Z", 
    "summary": "Sentence level novelty detection aims at reducing redundant sentences from a\nsentence list. In the task, sentences appearing later in the list with no new\nmeanings are eliminated. Aiming at a better accuracy for detecting redundancy,\nthis paper reveals the nature of the novelty detection task currently\noverlooked by the Novelty community $-$ Novelty as a combination of the partial\noverlap (PO, two sentences sharing common facts) and complete overlap (CO, the\nfirst sentence covers all the facts of the second sentence) relations. By\nformalizing novelty detection as a combination of the two relations between\nsentences, new viewpoints toward techniques dealing with Novelty are proposed.\nAmong the methods discussed, the similarity, overlap, pool and language\nmodeling approaches are commonly used. Furthermore, a novel approach, selected\npool method is provided, which is immediate following the nature of the task.\nExperimental results obtained on all the three currently available novelty\ndatasets showed that selected pool is significantly better or no worse than the\ncurrent methods. Knowledge about the nature of the task also affects the\nevaluation methodologies. We propose new evaluation measures for Novelty\naccording to the nature of the task, as well as possible directions for future\nstudy."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0511002v1", 
    "title": "Bibliographic Classification using the ADS Databases", 
    "arxiv-id": "cs/0511002v1", 
    "author": "Stephen S. Murray", 
    "publish": "2005-10-31T22:34:34Z", 
    "summary": "We discuss two techniques used to characterize bibliographic records based on\ntheir similarity to and relationship with the contents of the NASA Astrophysics\nData System (ADS) databases. The first method has been used to classify input\ntext as being relevant to one or more subject areas based on an analysis of the\nfrequency distribution of its individual words. The second method has been used\nto classify existing records as being relevant to one or more databases based\non the distribution of the papers citing them. Both techniques have proven to\nbe valuable tools in assigning new and existing bibliographic records to\ndifferent disciplines within the ADS databases."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0511016v1", 
    "title": "How to make the top ten: Approximating PageRank from in-degree", 
    "arxiv-id": "cs/0511016v1", 
    "author": "Filippo Menczer", 
    "publish": "2005-11-03T23:01:50Z", 
    "summary": "PageRank has become a key element in the success of search engines, allowing\nto rank the most important hits in the top screen of results. One key aspect\nthat distinguishes PageRank from other prestige measures such as in-degree is\nits global nature. From the information provider perspective, this makes it\ndifficult or impossible to predict how their pages will be ranked. Consequently\na market has emerged for the optimization of search engine results. Here we\nstudy the accuracy with which PageRank can be approximated by in-degree, a\nlocal measure made freely available by search engines. Theoretical and\nempirical analyses lead to conclude that given the weak degree correlations in\nthe Web link graph, the approximation can be relatively accurate, giving\nservice and information providers an effective new marketing tool."
},{
    "category": "cs.CR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0512007v1", 
    "title": "Entangled messages", 
    "arxiv-id": "cs/0512007v1", 
    "author": "Arindam Mitra", 
    "publish": "2005-12-01T15:40:19Z", 
    "summary": "It is sometimes necessary to send copies of the same email to different\nparties, but it is impossible to ensure that if one party reads the message the\nother parties will bound to read it. We propose an entanglement based scheme\nwhere if one party reads the message the other party will bound to read it\nsimultaneously."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0512069v1", 
    "title": "Reconstructing Websites for the Lazy Webmaster", 
    "arxiv-id": "cs/0512069v1", 
    "author": "Johan Bollen", 
    "publish": "2005-12-16T16:02:13Z", 
    "summary": "Backup or preservation of websites is often not considered until after a\ncatastrophic event has occurred. In the face of complete website loss, \"lazy\"\nwebmasters or concerned third parties may be able to recover some of their\nwebsite from the Internet Archive. Other pages may also be salvaged from\ncommercial search engine caches. We introduce the concept of \"lazy\npreservation\"- digital preservation performed as a result of the normal\noperations of the Web infrastructure (search engines and caches). We present\nWarrick, a tool to automate the process of website reconstruction from the\nInternet Archive, Google, MSN and Yahoo. Using Warrick, we have reconstructed\n24 websites of varying sizes and composition to demonstrate the feasibility and\nlimitations of website reconstruction from the public Web infrastructure. To\nmeasure Warrick's window of opportunity, we have profiled the time required for\nnew Web resources to enter and leave search engine caches."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0601045v1", 
    "title": "PageRank without hyperlinks: Structural re-ranking using links induced   by language models", 
    "arxiv-id": "cs/0601045v1", 
    "author": "Lillian Lee", 
    "publish": "2006-01-11T21:27:28Z", 
    "summary": "Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web\nsearch, we propose a structural re-ranking approach to ad hoc information\nretrieval: we reorder the documents in an initially retrieved set by exploiting\nasymmetric relationships between them. Specifically, we consider generation\nlinks, which indicate that the language model induced from one document assigns\nhigh probability to the text of another; in doing so, we take care to prevent\nbias against long documents. We study a number of re-ranking criteria based on\nmeasures of centrality in the graphs formed by generation links, and show that\nintegrating centrality into standard language-model-based retrieval is quite\neffective at improving precision at top ranks."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0601046v1", 
    "title": "Better than the real thing? Iterative pseudo-query processing using   cluster-based language models", 
    "arxiv-id": "cs/0601046v1", 
    "author": "Carmel Domshlak", 
    "publish": "2006-01-11T21:47:44Z", 
    "summary": "We present a novel approach to pseudo-feedback-based ad hoc retrieval that\nuses language models induced from both documents and clusters. First, we treat\nthe pseudo-feedback documents produced in response to the original query as a\nset of pseudo-queries that themselves can serve as input to the retrieval\nprocess. Observing that the documents returned in response to the\npseudo-queries can then act as pseudo-queries for subsequent rounds, we arrive\nat a formulation of pseudo-query-based retrieval as an iterative process.\nExperiments show that several concrete instantiations of this idea, when\napplied in conjunction with techniques designed to heighten precision, yield\nperformance results rivaling those of a number of previously-proposed\nalgorithms, including the standard language-modeling approach. The use of\ncluster-based language models is a key contributing factor to our algorithms'\nsuccess."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.artmed.2004.07.017", 
    "link": "http://arxiv.org/pdf/cs/0601047v1", 
    "title": "Automatic Detection of Trends in Dynamical Text: An Evolutionary   Approach", 
    "arxiv-id": "cs/0601047v1", 
    "author": "Juan J. Merelo", 
    "publish": "2006-01-12T20:23:06Z", 
    "summary": "This paper presents an evolutionary algorithm for modeling the arrival dates\nof document streams, which is any time-stamped collection of documents, such as\nnewscasts, e-mails, IRC conversations, scientific journals archives and weblog\npostings. This algorithm assigns frequencies (number of document arrivals per\ntime unit) to time intervals so that it produces an optimal fit to the data.\nThe optimization is a trade off between accurately fitting the data and\navoiding too many frequency changes; this way the analysis is able to find fits\nwhich ignore the noise. Classical dynamic programming algorithms are limited by\nmemory and efficiency requirements, which can be a problem when dealing with\nlong streams. This suggests to explore alternative search methods which allow\nfor some degree of uncertainty to achieve tractability. Experiments have shown\nthat the designed evolutionary algorithm is able to reach the same solution\nquality as those classical dynamic programming algorithms in a shorter time. We\nhave also explored different probabilistic models to optimize the fitting of\nthe date streams, and applied these algorithms to infer whether a new arrival\nincreases or decreases {\\em interest} in the topic the document stream is\nabout."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0602060v1", 
    "title": "eJournal interface can influence usage statistics: implications for   libraries, publishers, and Project COUNTER", 
    "arxiv-id": "cs/0602060v1", 
    "author": "Jason S. Price", 
    "publish": "2006-02-16T20:29:25Z", 
    "summary": "The design of a publisher's electronic interface can have a measurable effect\non electronic journal usage statistics. A study of journal usage from six\nCOUNTER-compliant publishers at thirty-two research institutions in the United\nStates, the United Kingdom and Sweden indicates that the ratio of PDF to HTML\nviews is not consistent across publisher interfaces, even after controlling for\ndifferences in publisher content. The number of fulltext downloads may be\nartificially inflated when publishers require users to view HTML versions\nbefore accessing PDF versions or when linking mechanisms, such as CrossRef,\ndirect users to the full text, rather than the abstract, of each article. These\nresults suggest that usage reports from COUNTER-compliant publishers are not\ndirectly comparable in their current form. One solution may be to modify\npublisher numbers with adjustment factors deemed to be representative of the\nbenefit or disadvantage due to its interface. Standardization of some interface\nand linking protocols may obviate these differences and allow for more accurate\ncross-publisher comparisons."
},{
    "category": "cs.CV", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0602065v1", 
    "title": "Similarity of Objects and the Meaning of Words", 
    "arxiv-id": "cs/0602065v1", 
    "author": "Paul Vitanyi", 
    "publish": "2006-02-17T16:15:07Z", 
    "summary": "We survey the emerging area of compression-based, parameter-free, similarity\ndistance measures useful in data-mining, pattern recognition, learning and\nautomatic semantics extraction. Given a family of distances on a set of\nobjects, a distance is universal up to a certain precision for that family if\nit minorizes every distance in the family between every two objects in the set,\nup to the stated precision (we do not require the universal distance to be an\nelement of the family). We consider similarity distances for two types of\nobjects: literal objects that as such contain all of their meaning, like\ngenomes or books, and names for objects. The latter may have literal\nembodyments like the first type, but may also be abstract like ``red'' or\n``christianity.'' For the first type we consider a family of computable\ndistance measures corresponding to parameters expressing similarity according\nto particular featuresdistances generated by web users corresponding to\nparticular semantic relations between the (names for) the designated objects.\nFor both families we give universal similarity distance measures, incorporating\nall particular distance measures in the family. In the first case the universal\ndistance is based on compression and in the second case it is based on Google\npage counts related to search terms. In both cases experiments on a massive\nscale give evidence of the viability of the approaches. between pairs of\nliteral objects. For the second type we consider similarity"
},{
    "category": "cs.DL", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0603073v1", 
    "title": "VXA: A Virtual Architecture for Durable Compressed Archives", 
    "arxiv-id": "cs/0603073v1", 
    "author": "Bryan Ford", 
    "publish": "2006-03-18T16:31:33Z", 
    "summary": "Data compression algorithms change frequently, and obsolete decoders do not\nalways run on new hardware and operating systems, threatening the long-term\nusability of content archived using those algorithms. Re-encoding content into\nnew formats is cumbersome, and highly undesirable when lossy compression is\ninvolved. Processor architectures, in contrast, have remained comparatively\nstable over recent decades. VXA, an archival storage system designed around\nthis observation, archives executable decoders along with the encoded content\nit stores. VXA decoders run in a specialized virtual machine that implements an\nOS-independent execution environment based on the standard x86 architecture.\nThe VXA virtual machine strictly limits access to host system services, making\ndecoders safe to run even if an archive contains malicious code. VXA's adoption\nof a \"native\" processor architecture instead of type-safe language technology\nallows reuse of existing \"hand-optimized\" decoders in C and assembly language,\nand permits decoders access to performance-enhancing architecture features such\nas vector processing instructions. The performance cost of VXA's virtualization\nis typically less than 15% compared with the same decoders running natively.\nThe storage cost of archived decoders, typically 30-130KB each, can be\namortized across many archived files sharing the same compression method."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0604036v2", 
    "title": "Collaborative thesaurus tagging the Wikipedia way", 
    "arxiv-id": "cs/0604036v2", 
    "author": "Jakob Voss", 
    "publish": "2006-04-10T12:04:29Z", 
    "summary": "This paper explores the system of categories that is used to classify\narticles in Wikipedia. It is compared to collaborative tagging systems like\ndel.icio.us and to hierarchical classification like the Dewey Decimal\nClassification (DDC). Specifics and commonalitiess of these systems of subject\nindexing are exposed. Analysis of structural and statistical properties\n(descriptors per record, records per descriptor, descriptor levels) shows that\nthe category system of Wikimedia is a thesaurus that combines collaborative\ntagging and hierarchical subject indexing in a special way."
},{
    "category": "cs.LG", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0605035v1", 
    "title": "Query Chains: Learning to Rank from Implicit Feedback", 
    "arxiv-id": "cs/0605035v1", 
    "author": "Thorsten Joachims", 
    "publish": "2006-05-08T22:05:24Z", 
    "summary": "This paper presents a novel approach for using clickthrough data to learn\nranked retrieval functions for web search results. We observe that users\nsearching the web often perform a sequence, or chain, of queries with a similar\ninformation need. Using query chains, we generate new types of preference\njudgments from search engine logs, thus taking advantage of user intelligence\nin reformulating queries. To validate our method we perform a controlled user\nstudy comparing generated preference judgments to explicit relevance judgments.\nWe also implemented a real-world search engine to test our approach, using a\nmodified ranking SVM to learn an improved ranking function from preference\ndata. Our results demonstrate significant improvements in the ranking given by\nthe search engine. The learned rankings outperform both a static ranking\nfunction, as well as one trained without considering query chains."
},{
    "category": "cs.LG", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0605036v1", 
    "title": "Evaluating the Robustness of Learning from Implicit Feedback", 
    "arxiv-id": "cs/0605036v1", 
    "author": "Thorsten Joachims", 
    "publish": "2006-05-08T23:38:13Z", 
    "summary": "This paper evaluates the robustness of learning from implicit feedback in web\nsearch. In particular, we create a model of user behavior by drawing upon user\nstudies in laboratory and real-world settings. The model is used to understand\nthe effect of user behavior on the performance of a learning algorithm for\nranked retrieval. We explore a wide range of possible user behaviors and find\nthat learning from implicit feedback can be surprisingly robust. This\ncomplements previous results that demonstrated our algorithm's effectiveness in\na real-world search engine application."
},{
    "category": "cs.IR", 
    "doi": "10.1002/asi.20405", 
    "link": "http://arxiv.org/pdf/cs/0605037v1", 
    "title": "Minimally Invasive Randomization for Collecting Unbiased Preferences   from Clickthrough Logs", 
    "arxiv-id": "cs/0605037v1", 
    "author": "Thorsten Joachims", 
    "publish": "2006-05-09T01:53:22Z", 
    "summary": "Clickthrough data is a particularly inexpensive and plentiful resource to\nobtain implicit relevance feedback for improving and personalizing search\nengines. However, it is well known that the probability of a user clicking on a\nresult is strongly biased toward documents presented higher in the result set\nirrespective of relevance. We introduce a simple method to modify the\npresentation of search results that provably gives relevance judgments that are\nunaffected by presentation bias under reasonable assumptions. We validate this\nproperty of the training data in interactive real world experiments. Finally,\nwe show that using these unbiased relevance judgments learning methods can be\nguaranteed to converge to an ideal ranking given sufficient data."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606069v1", 
    "title": "Inference and Evaluation of the Multinomial Mixture Model for Text   Clustering", 
    "arxiv-id": "cs/0606069v1", 
    "author": "Fran\u00e7ois Yvon", 
    "publish": "2006-06-14T14:44:06Z", 
    "summary": "In this article, we investigate the use of a probabilistic model for\nunsupervised clustering in text collections. Unsupervised clustering has become\na basic module for many intelligent text processing applications, such as\ninformation retrieval, text classification or information extraction. The model\nconsidered in this contribution consists of a mixture of multinomial\ndistributions over the word counts, each component corresponding to a different\ntheme. We present and contrast various estimation procedures, which apply both\nin supervised and unsupervised contexts. In supervised learning, this work\nsuggests a criterion for evaluating the posterior odds of new documents which\nis more statistically sound than the \"naive Bayes\" approach. In an unsupervised\ncontext, we propose measures to set up a systematic evaluation framework and\nstart with examining the Expectation-Maximization (EM) algorithm as the basic\ntool for inference. We discuss the importance of initialization and the\ninfluence of other features such as the smoothing strategy or the size of the\nvocabulary, thereby illustrating the difficulties incurred by the high\ndimensionality of the parameter space. We also propose a heuristic algorithm\nbased on iterative EM with vocabulary reduction to solve this problem. Using\nthe fact that the latent variables can be analytically integrated out, we\nfinally show that Gibbs sampling algorithm is tractable and compares favorably\nto the basic expectation maximization approach."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606073v1", 
    "title": "Comparison of the estimation of the degree of polarization from four or   two intensity images degraded by speckle noise", 
    "arxiv-id": "cs/0606073v1", 
    "author": "Philippe R\u00e9fr\u00e9gier", 
    "publish": "2006-06-15T13:13:30Z", 
    "summary": "Active polarimetric imagery is a powerful tool for accessing the information\npresent in a scene. Indeed, the polarimetric images obtained can reveal\npolarizing properties of the objects that are not avalaible using conventional\nimaging systems. However, when coherent light is used to illuminate the scene,\nthe images are degraded by speckle noise. The polarization properties of a\nscene are characterized by the degree of polarization. In standard polarimetric\nimagery system, four intensity images are needed to estimate this degree . If\nwe assume the uncorrelation of the measurements, this number can be decreased\nto two images using the Orthogonal State Contrast Image (OSCI). However, this\napproach appears too restrictive in some cases. We thus propose in this paper a\nnew statistical parametric method to estimate the degree of polarization\nassuming correlated measurements with only two intensity images. The estimators\nobtained from four images, from the OSCI and from the proposed method, are\ncompared using simulated polarimetric data degraded by speckle noise."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606097v2", 
    "title": "Synonym search in Wikipedia: Synarcher", 
    "arxiv-id": "cs/0606097v2", 
    "author": "A. Krizhanovsky", 
    "publish": "2006-06-22T14:17:26Z", 
    "summary": "The program Synarcher for synonym (and related terms) search in the text\ncorpus of special structure (Wikipedia) was developed. The results of the\nsearch are presented in the form of graph. It is possible to explore the graph\nand search for graph elements interactively. Adapted HITS algorithm for synonym\nsearch, program architecture, and program work evaluation with test examples\nare presented in the paper. The proposed algorithm can be applied to a query\nexpansion by synonyms (in a search engine) and a synonym dictionary forming."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606115v1", 
    "title": "Evaluating Variable Length Markov Chain Models for Analysis of User Web   Navigation Sessions", 
    "arxiv-id": "cs/0606115v1", 
    "author": "Mark Levene", 
    "publish": "2006-06-28T08:26:45Z", 
    "summary": "Markov models have been widely used to represent and analyse user web\nnavigation data. In previous work we have proposed a method to dynamically\nextend the order of a Markov chain model and a complimentary method for\nassessing the predictive power of such a variable length Markov chain. Herein,\nwe review these two methods and propose a novel method for measuring the\nability of a variable length Markov model to summarise user web navigation\nsessions up to a given length. While the summarisation ability of a model is\nimportant to enable the identification of user navigation patterns, the ability\nto make predictions is important in order to foresee the next link choice of a\nuser after following a given trail so as, for example, to personalise a web\nsite. We present an extensive experimental evaluation providing strong evidence\nthat prediction accuracy increases linearly with summarisation ability."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606118v1", 
    "title": "Adapting a general parser to a sublanguage", 
    "arxiv-id": "cs/0606118v1", 
    "author": "Claire N\u00e9dellec", 
    "publish": "2006-06-28T14:43:42Z", 
    "summary": "In this paper, we propose a method to adapt a general parser (Link Parser) to\nsublanguages, focusing on the parsing of texts in biology. Our main proposal is\nthe use of terminology (identication and analysis of terms) in order to reduce\nthe complexity of the text to be parsed. Several other strategies are explored\nand finally combined among which text normalization, lexicon and\nmorpho-guessing module extensions and grammar rules adaptation. We compare the\nparsing results before and after these adaptations."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606119v1", 
    "title": "Lexical Adaptation of Link Grammar to the Biomedical Sublanguage: a   Comparative Evaluation of Three Approaches", 
    "arxiv-id": "cs/0606119v1", 
    "author": "Adeline Nazarenko", 
    "publish": "2006-06-28T14:44:48Z", 
    "summary": "We study the adaptation of Link Grammar Parser to the biomedical sublanguage\nwith a focus on domain terms not found in a general parser lexicon. Using two\nbiomedical corpora, we implement and evaluate three approaches to addressing\nunknown words: automatic lexicon expansion, the use of morphological clues, and\ndisambiguation using a part-of-speech tagger. We evaluate each approach\nseparately for its effect on parsing performance and consider combinations of\nthese approaches. In addition to a 45% increase in parsing efficiency, we find\nthat the best approach, incorporating information from a domain part-of-speech\ntagger, offers a statistically signicant 10% relative decrease in error. The\nadapted parser is available under an open-source license at\nhttp://www.it.utu.fi/biolg."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0606128v1", 
    "title": "Automatic forming lists of semantically related terms based on texts   rating in the corpus with hyperlinks and categories (In Russian)", 
    "arxiv-id": "cs/0606128v1", 
    "author": "A. Krizhanovsky", 
    "publish": "2006-06-30T15:17:36Z", 
    "summary": "HITS adapted algorithm for synonym search, the program architecture, and the\nprogram work evaluation with test examples are presented in the paper.\nSynarcher program for synonym (and related terms) search in the text corpus of\nspecial structure (Wikipedia) was developed. The results of search are\npresented in the form of a graph. It is possible to explore the graph and\nsearch graph elements interactively. The proposed algorithm could be applied to\nthe search request extending and for synonym dictionary forming."
},{
    "category": "cs.OH", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0607081v1", 
    "title": "Syst\u00e8me de repr\u00e9sentation d'aide au besoin dans le domaine   architectural", 
    "arxiv-id": "cs/0607081v1", 
    "author": "Marie-France Ango-Obiang", 
    "publish": "2006-07-18T05:04:01Z", 
    "summary": "The image is a very important mean of communication in the field of\narchitectural who intervenes in the various phases of the design of a project.\nIt can be regarded as a tool of decision-making aid. The study of our research\naims at to see the contribution of the Economic Intelligence in the resolution\nof a decisional problem of the various partners (Architect, Contractor,\nCustomer) in the architectural field, in order to make strategic decisions\nwithin the framework of the realization or design of an architectural work. The\neconomic Intelligence allows the taking into account of the real needs for the\nuser-decision makers, so that their waiting are considered at the first stage\nof a search for information and not in the final stage of the development of\nthe tool in the evaluation of this last."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0608004v1", 
    "title": "Separating the articles of authors with the same name", 
    "arxiv-id": "cs/0608004v1", 
    "author": "Jose M. Soler", 
    "publish": "2006-08-01T18:23:26Z", 
    "summary": "I describe a method to separate the articles of different authors with the\nsame name. It is based on a distance between any two publications, defined in\nterms of the probability that they would have as many coincidences if they were\ndrawn at random from all published documents. Articles with a given author name\nare then clustered according to their distance, so that all articles in a\ncluster belong very likely to the same author. The method has proven very\nuseful in generating groups of papers that are then selected manually. This\nsimplifies considerably citation analysis when the author publication lists are\nnot available."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609003v1", 
    "title": "In Quest of Image Semantics: Are We Looking for It Under the Right   Lamppost?", 
    "arxiv-id": "cs/0609003v1", 
    "author": "Emanuel Diamant", 
    "publish": "2006-09-02T11:50:29Z", 
    "summary": "In the last years we witness a dramatic growth of research focused on\nsemantic image understanding. Indeed, without understanding image content\nsuccessful accomplishment of any image-processing task is simply incredible. Up\nto the recent times, the ultimate need for such understanding has been met by\nthe knowledge that a domain expert or a vision system supervisor have\ncontributed to every image-processing application. The advent of the Internet\nhas drastically changed this situation. Internet sources of visual information\nare diffused and dispersed over the whole Web, so the duty of information\ncontent discovery and evaluation must be relegated now to an image\nunderstanding agent (a machine or a computer program) capable to perform image\ncontent assessment at a remote image location. Development of Content Based\nImage Retrieval (CBIR) techniques was a right move in a right direction,\nlaunched about ten years ago. Unfortunately, very little progress has been made\nsince then. The reason for this can be seen in a rank of long lasting\nmisconceptions that CBIR designers are continuing to adhere to. I hope, my\narguments will help them to change their minds."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609051v1", 
    "title": "Multilingual person name recognition and transliteration", 
    "arxiv-id": "cs/0609051v1", 
    "author": "Jan Zizka", 
    "publish": "2006-09-11T11:08:23Z", 
    "summary": "We present an exploratory tool that extracts person names from multilingual\nnews collections, matches name variants referring to the same person, and\ninfers relationships between people based on the co-occurrence of their names\nin related news. A novel feature is the matching of name variants across\nlanguages and writing systems, including names written with the Greek, Cyrillic\nand Arabic writing system. Due to our highly multilingual setting, we use an\ninternal standard representation for name representation and matching, instead\nof adopting the traditional bilingual approach to transliteration. This work is\npart of the news analysis system NewsExplorer that clusters an average of\n25,000 news articles per day to detect related news within the same and across\ndifferent languages."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609053v1", 
    "title": "Navigating multilingual news collections using automatically extracted   information", 
    "arxiv-id": "cs/0609053v1", 
    "author": "Camelia Ignat", 
    "publish": "2006-09-11T11:54:41Z", 
    "summary": "We are presenting a text analysis tool set that allows analysts in various\nfields to sieve through large collections of multilingual news items quickly\nand to find information that is of relevance to them. For a given document\ncollection, the tool set automatically clusters the texts into groups of\nsimilar articles, extracts names of places, people and organisations, lists the\nuser-defined specialist terms found, links clusters and entities, and generates\nhyperlinks. Through its daily news analysis operating on thousands of articles\nper day, the tool also learns relationships between people and other entities.\nThe fully functional prototype system allows users to explore and navigate\nmultilingual document collections across languages and time."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609059v1", 
    "title": "Automatic annotation of multilingual text collections with a conceptual   thesaurus", 
    "arxiv-id": "cs/0609059v1", 
    "author": "Camelia Ignat", 
    "publish": "2006-09-12T07:24:01Z", 
    "summary": "Automatic annotation of documents with controlled vocabulary terms\n(descriptors) from a conceptual thesaurus is not only useful for document\nindexing and retrieval. The mapping of texts onto the same thesaurus\nfurthermore allows to establish links between similar documents. This is also a\nsubstantial requirement of the Semantic Web. This paper presents an almost\nlanguage-independent system that maps documents written in different languages\nonto the same multilingual conceptual thesaurus, EUROVOC. Conceptual thesauri\ndiffer from Natural Language Thesauri in that they consist of relatively small\ncontrolled lists of words or phrases with a rather abstract meaning. To\nautomatically identify which thesaurus descriptors describe the contents of a\ndocument best, we developed a statistical, associative system that is trained\non texts that have previously been indexed manually. In addition to describing\nthe large number of empirically optimised parameters of the fully functional\napplication, we present the performance of the software according to a human\nevaluation by professional indexers."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609060v1", 
    "title": "Automatic Identification of Document Translations in Large Multilingual   Document Collections", 
    "arxiv-id": "cs/0609060v1", 
    "author": "Camelia Ignat", 
    "publish": "2006-09-12T08:44:53Z", 
    "summary": "Texts and their translations are a rich linguistic resource that can be used\nto train and test statistics-based Machine Translation systems and many other\napplications. In this paper, we present a working system that can identify\ntranslations and other very similar documents among a large number of\ncandidates, by representing the document contents with a vector of thesaurus\nterms from a multilingual thesaurus, and by then measuring the semantic\nsimilarity between the vectors. Tests on different text types have shown that\nthe system can detect translations with over 96% precision in a large search\nspace of 820 documents or more. The system was tuned to ignore\nlanguage-specific similarities and to give similar documents in a second\nlanguage the same similarity score as equivalent documents in the same\nlanguage. The application can also be used to detect cross-lingual document\nplagiarism."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609061v1", 
    "title": "Cross-lingual keyword assignment", 
    "arxiv-id": "cs/0609061v1", 
    "author": "Ralf Steinberger", 
    "publish": "2006-09-12T09:29:57Z", 
    "summary": "This paper presents a language-independent approach to controlled vocabulary\nkeyword assignment using the EUROVOC thesaurus. Due to the multilingual nature\nof EUROVOC, the keywords for a document written in one language can be\ndisplayed in all eleven official European Union languages. The mapping of\ndocuments written in different languages to the same multilingual thesaurus\nfurthermore allows cross-language document comparison. The assignment of the\ncontrolled vocabulary thesaurus descriptors is achieved by applying a\nstatistical method that uses a collection of manually indexed documents to\nidentify, for each thesaurus descriptor, a large number of lemmas that are\nstatistically associated to the descriptor. These associated words are then\nused during the assignment procedure to identify a ranked list of those EUROVOC\nterms that are most likely to be good keywords for a given document. The paper\nalso describes the challenges of this task and discusses the achieved results\nof the fully functional prototype."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609063v1", 
    "title": "Extending an Information Extraction tool set to Central and Eastern   European languages", 
    "arxiv-id": "cs/0609063v1", 
    "author": "Ralf Steinberger", 
    "publish": "2006-09-12T12:29:17Z", 
    "summary": "In a highly multilingual and multicultural environment such as in the\nEuropean Commission with soon over twenty official languages, there is an\nurgent need for text analysis tools that use minimal linguistic knowledge so\nthat they can be adapted to many languages without much human effort. We are\npresenting two such Information Extraction tools that have already been adapted\nto various Western and Eastern European languages: one for the recognition of\ndate expressions in text, and one for the detection of geographical place names\nand the visualisation of the results in geographical maps. An evaluation of the\nperformance has produced very satisfying results."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609064v1", 
    "title": "Exploiting multilingual nomenclatures and language-independent text   features as an interlingua for cross-lingual text analysis applications", 
    "arxiv-id": "cs/0609064v1", 
    "author": "Camelia Ignat", 
    "publish": "2006-09-12T12:43:37Z", 
    "summary": "We are proposing a simple, but efficient basic approach for a number of\nmultilingual and cross-lingual language technology applications that are not\nlimited to the usual two or three languages, but that can be applied with\nrelatively little effort to larger sets of languages. The approach consists of\nusing existing multilingual linguistic resources such as thesauri,\nnomenclatures and gazetteers, as well as exploiting the existence of additional\nmore or less language-independent text items such as dates, currency\nexpressions, numbers, names and cognates. Mapping texts onto the multilingual\nresources and identifying word token links between texts in different languages\nare basic ingredients for applications such as cross-lingual document\nsimilarity calculation, multilingual clustering and categorisation,\ncross-lingual document retrieval, and tools to provide cross-lingual\ninformation access."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609065v1", 
    "title": "Geocoding multilingual texts: Recognition, disambiguation and   visualisation", 
    "arxiv-id": "cs/0609065v1", 
    "author": "Clive Best", 
    "publish": "2006-09-12T12:57:38Z", 
    "summary": "We are presenting a method to recognise geographical references in free text.\nOur tool must work on various languages with a minimum of language-dependent\nresources, except a gazetteer. The main difficulty is to disambiguate these\nplace names by distinguishing places from persons and by selecting the most\nlikely place out of a list of homographic place names world-wide. The system\nuses a number of language-independent clues and heuristics to disambiguate\nplace name homographs. The final aim is to index texts with the countries and\ncities they mention and to automatically visualise this information on\ngeographical maps using various tools."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609066v1", 
    "title": "Building and displaying name relations using automatic unsupervised   analysis of newspaper articles", 
    "arxiv-id": "cs/0609066v1", 
    "author": "Tamara Oellinger", 
    "publish": "2006-09-12T13:09:37Z", 
    "summary": "We present a tool that, from automatically recognised names, tries to infer\ninter-person relations in order to present associated people on maps. Based on\nan in-house Named Entity Recognition tool, applied on clusters of an average of\n15,000 news articles per day, in 15 different languages, we build a knowledge\nbase that allows extracting statistical co-occurrences of persons and\nvisualising them on a per-person page or in various graphs."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609067v1", 
    "title": "A tool set for the quick and efficient exploration of large document   collections", 
    "arxiv-id": "cs/0609067v1", 
    "author": "Tomaz Erjavec", 
    "publish": "2006-09-12T13:30:01Z", 
    "summary": "We are presenting a set of multilingual text analysis tools that can help\nanalysts in any field to explore large document collections quickly in order to\ndetermine whether the documents contain information of interest, and to find\nthe relevant text passages. The automatic tool, which currently exists as a\nfully functional prototype, is expected to be particularly useful when users\nrepeatedly have to sieve through large collections of documents such as those\ndownloaded automatically from the internet. The proposed system takes a whole\ndocument collection as input. It first carries out some automatic analysis\ntasks (named entity recognition, geo-coding, clustering, term extraction),\nannotates the texts with the generated meta-information and stores the\nmeta-information in a database. The system then generates a zoomable and\nhyperlinked geographic map enhanced with information on entities and terms\nfound. When the system is used on a regular basis, it builds up a historical\ndatabase that contains information on which names have been mentioned together\nwith which other names or places, and users can query this database to retrieve\ninformation extracted in the past."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609133v1", 
    "title": "An application-oriented terminology evaluation: the case of back-of-the   book indexes", 
    "arxiv-id": "cs/0609133v1", 
    "author": "Adeline Nazarenko", 
    "publish": "2006-09-24T19:59:20Z", 
    "summary": "This paper addresses the problem of computational terminology evaluation not\nper se but in a specific application context. This paper describes the\nevaluation procedure that has been used to assess the validity of our overall\nindexing approach and the quality of the IndDoc indexing tool. Even if\nuser-oriented extended evaluation is irreplaceable, we argue that early\nevaluations are possible and they are useful for development guidance."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609134v1", 
    "title": "Using NLP to build the hypertextuel network of a back-of-the-book index", 
    "arxiv-id": "cs/0609134v1", 
    "author": "Adeline Nazarenko", 
    "publish": "2006-09-24T20:00:58Z", 
    "summary": "Relying on the idea that back-of-the-book indexes are traditional devices for\nnavigation through large documents, we have developed a method to build a\nhypertextual network that helps the navigation in a document. Building such an\nhypertextual network requires selecting a list of descriptors, identifying the\nrelevant text segments to associate with each descriptor and finally ranking\nthe descriptors and reference segments by relevance order. We propose a\nspecific document segmentation method and a relevance measure for information\nranking. The algorithms are tested on 4 corpora (of different types and\ndomains) without human intervention or any semantic knowledge."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609135v1", 
    "title": "Event-based Information Extraction for the biomedical domain: the   Caderige project", 
    "arxiv-id": "cs/0609135v1", 
    "author": "Davy Weissenbacher", 
    "publish": "2006-09-24T20:03:06Z", 
    "summary": "This paper gives an overview of the Caderige project. This project involves\nteams from different areas (biology, machine learning, natural language\nprocessing) in order to develop high-level analysis tools for extracting\nstructured information from biological bibliographical databases, especially\nMedline. The paper gives an overview of the approach and compares it to the\nstate of the art."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0609137v1", 
    "title": "Ontologies and Information Extraction", 
    "arxiv-id": "cs/0609137v1", 
    "author": "Adeline Nazarenko", 
    "publish": "2006-09-24T20:10:35Z", 
    "summary": "This report argues that, even in the simplest cases, IE is an ontology-driven\nprocess. It is not a mere text filtering method based on simple pattern\nmatching and keywords, because the extracted pieces of texts are interpreted\nwith respect to a predefined partial domain model. This report shows that\ndepending on the nature and the depth of the interpretation to be done for\nextracting the information, more or less knowledge must be involved. This\nreport is mainly illustrated in biology, a domain in which there are critical\nneeds for content-based exploration of the scientific literature and which\nbecomes a major application domain for IE."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0610019v1", 
    "title": "NectaRSS, an RSS feed ranking system that implicitly learns user   preferences", 
    "arxiv-id": "cs/0610019v1", 
    "author": "J. J. Merelo", 
    "publish": "2006-10-04T15:13:55Z", 
    "summary": "In this paper a new RSS feed ranking method called NectaRSS is introduced.\nThe system recommends information to a user based on his/her past choices. User\npreferences are automatically acquired, avoiding explicit feedback, and ranking\nis based on those preferences distilled to a user profile. NectaRSS uses the\nwell-known vector space model for user profiles and new documents, and compares\nthem using information-retrieval techniques, but introduces a novel method for\nuser profile creation and adaptation from users' past choices. The efficiency\nof the proposed method has been tested by embedding it into an intelligent\naggregator (RSS feed reader), which has been used by different and\nheterogeneous users. Besides, this paper proves that the ranking of newsitems\nyielded by NectaRSS improves its quality with user's choices, and its\nsuperiority over other algorithms that use a different information\nrepresentation method."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0610039v1", 
    "title": "The Application of Fuzzy Logic to the Construction of the Ranking   Function of Information Retrieval Systems", 
    "arxiv-id": "cs/0610039v1", 
    "author": "Neil Rubens", 
    "publish": "2006-10-08T05:34:12Z", 
    "summary": "The quality of the ranking function is an important factor that determines\nthe quality of the Information Retrieval system. Each document is assigned a\nscore by the ranking function; the score indicates the likelihood of relevance\nof the document given a query. In the vector space model, the ranking function\nis defined by a mathematic expression. We propose a fuzzy logic (FL) approach\nto defining the ranking function. FL provides a convenient way of converting\nknowledge expressed in a natural language into fuzzy logic rules. The resulting\nranking function could be easily viewed, extended, and verified: * if (tf is\nhigh) and (idf is high) > (relevance is high); * if (overlap is high) >\n(relevance is high). By using above FL rules, we are able to achieve\nperformance approximately equal to the state of the art search engine Apache\nLucene (deltaP10 +0.92%; deltaMAP -0.1%). The fuzzy logic approach allows\ncombining the logic-based model with the vector model. The resulting model\npossesses simplicity and formalism of the logic based model, and the\nflexibility and performance of the vector model."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0610091v4", 
    "title": "On the Behavior of Journal Impact Factor Rank-Order Distribution", 
    "arxiv-id": "cs/0610091v4", 
    "author": "P. Miramontes", 
    "publish": "2006-10-14T17:03:46Z", 
    "summary": "An empirical law for the rank-order behavior of journal impact factors is\nfound. Using an extensive data base on impact factors including journals on\nEducation, Agrosciences, Geosciences, Biosciences and Environ- mental,\nChemical, Computer, Engineering, Material, Mathematical, Medical and Physical\nSciences we have found extremely good fits out- performing other rank-order\nmodels. Some extensions to other areas of knowledge are discussed."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0610118v1", 
    "title": "Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading", 
    "arxiv-id": "cs/0610118v1", 
    "author": "Erkki Sutinen", 
    "publish": "2006-10-19T17:50:57Z", 
    "summary": "Latent Semantic Analysis (LSA) is a widely used Information Retrieval method\nbased on \"bag-of-words\" assumption. However, according to general conception,\nsyntax plays a role in representing meaning of sentences. Thus, enhancing LSA\nwith part-of-speech (POS) information to capture the context of word\noccurrences appears to be theoretically feasible extension. The approach is\ntested empirically on a automatic essay grading system using LSA for document\nsimilarity comparisons. A comparison on several POS-enhanced LSA models is\nreported. Our findings show that the addition of contextual information in the\nform of POS tags can raise the accuracy of the LSA-based scoring models up to\n10.77 per cent."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0610158v1", 
    "title": "Considering users' behaviours in improving the responses of an   information base", 
    "arxiv-id": "cs/0610158v1", 
    "author": "Odile Thiery", 
    "publish": "2006-10-27T16:02:34Z", 
    "summary": "In this paper, our aim is to propose a model that helps in the efficient use\nof an information system by users, within the organization represented by the\nIS, in order to resolve their decisional problems. In other words we want to\naid the user within an organization in obtaining the information that\ncorresponds to his needs (informational needs that result from his decisional\nproblems). This type of information system is what we refer to as economic\nintelligence system because of its support for economic intelligence processes\nof the organisation. Our assumption is that every EI process begins with the\nidentification of the decisional problem which is translated into an\ninformational need. This need is then translated into one or many information\nsearch problems (ISP). We also assumed that an ISP is expressed in terms of the\nuser's expectations and that these expectations determine the activities or the\nbehaviors of the user, when he/she uses an IS. The model we are proposing is\nused for the conception of the IS so that the process of retrieving of\nsolution(s) or the responses given by the system to an ISP is based on these\nbehaviours and correspond to the needs of the user."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0612043v1", 
    "title": "About the Lifespan of Peer to Peer Networks", 
    "arxiv-id": "cs/0612043v1", 
    "author": "P. Vitanyi", 
    "publish": "2006-12-07T16:20:00Z", 
    "summary": "We analyze the ability of peer to peer networks to deliver a complete file\namong the peers. Early on we motivate a broad generalization of network\nbehavior organizing it into one of two successive phases. According to this\nview the network has two main states: first centralized - few sources (roots)\nhold the complete file, and next distributed - peers hold some parts (chunks)\nof the file such that the entire network has the whole file, but no individual\nhas it. In the distributed state we study two scenarios, first, when the peers\nare ``patient'', i.e, do not leave the system until they obtain the complete\nfile; second, peers are ``impatient'' and almost always leave the network\nbefore obtaining the complete file."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0612132v1", 
    "title": "A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus,   and Google Scholar", 
    "arxiv-id": "cs/0612132v1", 
    "author": "Kiduk Yang", 
    "publish": "2006-12-23T14:47:24Z", 
    "summary": "Academic institutions, federal agencies, publishers, editors, authors, and\nlibrarians increasingly rely on citation analysis for making hiring, promotion,\ntenure, funding, and/or reviewer and journal evaluation and selection\ndecisions. The Institute for Scientific Information's (ISI) citation databases\nhave been used for decades as a starting point and often as the only tools for\nlocating citations and/or conducting citation analyses. ISI databases (or Web\nof Science), however, may no longer be adequate as the only or even the main\nsources of citations because new databases and tools that allow citation\nsearching are now available. Whether these new databases and tools complement\nor represent alternatives to Web of Science (WoS) is important to explore.\nUsing a group of 15 library and information science faculty members as a case\nstudy, this paper examines the effects of using Scopus and Google Scholar (GS)\non the citation counts and rankings of scholars as measured by WoS. The paper\ndiscusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap\nand uniqueness, quality and language of the citations, and the implications of\nthe findings for citation analysis. The project involved citation searching for\napproximately 1,100 scholarly works published by the study group and over 200\nworks by a test group (an additional 10 faculty members). Overall, more than\n10,000 citing and purportedly citing documents were examined. WoS data took\nabout 100 hours of collecting and processing time, Scopus consumed 200 hours,\nand GS a grueling 3,000 hours."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0701136v1", 
    "title": "Citation Advantage For OA Self-Archiving Is Independent of Journal   Impact Factor, Article Age, and Number of Co-Authors", 
    "arxiv-id": "cs/0701136v1", 
    "author": "Stevan Harnad", 
    "publish": "2007-01-22T02:14:10Z", 
    "summary": "Eysenbach has suggested that the OA (Green) self-archiving advantage might\njust be an artifact of potential uncontrolled confounding factors such as\narticle age (older articles may be both more cited and more likely to be\nself-archived), number of authors (articles with more authors might be more\ncited and more self-archived), subject matter (the subjects that are cited\nmore, self-archive more), country (same thing), number of authors, citation\ncounts of authors, etc. Chawki Hajjem (doctoral candidate, UQaM) had already\nshown that the OA advantage was present in all cases when articles were\nanalysed separately by age, subject matter or country. He has now done a\nmultiple regression analysis jointly testing (1) article age, (2) journal\nimpact factor, (3) number of authors, and (4) OA self-archiving as separate\nfactors for 442,750 articles in 576 (biomedical) journals across 11 years, and\nhas shown that each of the four factors contributes an independent,\nstatistically significant increment to the citation counts. The\nOA-self-archiving advantage remains a robust, independent factor. Having\nsuccessfully responded to his challenge, we now challenge Eysenbach to\ndemonstrate -- by testing a sufficiently broad and representative sample of\njournals at all levels of the journal quality, visibility and prestige\nhierarchy -- that his finding of a citation advantage for Gold OA (articles\npublished OA on the high-profile website of the only journal he tested (PNAS)\nover Green OA articles in the same journal (self-archived on the author's\nwebsite) was not just an artifact of having tested only one very high-profile\njournal."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0701137v1", 
    "title": "The Open Access Citation Advantage: Quality Advantage Or Quality Bias?", 
    "arxiv-id": "cs/0701137v1", 
    "author": "Stevan Harnad", 
    "publish": "2007-01-22T02:19:16Z", 
    "summary": "Many studies have now reported the positive correlation between Open Access\n(OA) self-archiving and citation counts (\"OA Advantage,\" OAA). But does this\nOAA occur because (QB) authors are more likely to self-selectively self-archive\narticles that are more likely to be cited (self-selection \"Quality Bias\": QB)?\nor because (QA) articles that are self-archived are more likely to be cited\n(\"Quality Advantage\": QA)? The probable answer is both. Three studies [by (i)\nKurtz and co-workers in astrophysics, (ii) Moed in condensed matter physics,\nand (iii) Davis & Fromerth in mathematics] had reported the OAA to be due to QB\n[plus Early Advantage, EA, from self-archiving the preprint before publication,\nin (i) and (ii)] rather than QA. These three fields, however, (1) have less of\na postprint access problem than most other fields and (i) and (ii) also happen\nto be among the minority of fields that (2) make heavy use of prepublication\npreprints. Chawki Hajjem has now analyzed preliminary evidence based on over\n100,000 articles from multiple fields, comparing self-selected self-archiving\nwith mandated self-archiving to estimate the contributions of QB and QA to the\nOAA. Both factors contribute, and the contribution of QA is greater."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0702085v1", 
    "title": "Social Behaviours Applied to P2P Systems: An efficient Algorithm for   Resource Organisation", 
    "arxiv-id": "cs/0702085v1", 
    "author": "V. Nicosia", 
    "publish": "2007-02-14T11:53:14Z", 
    "summary": "P2P systems are a great solution to the problem of distributing resources.\nThe main issue of P2P networks is that searching and retrieving resources\nshared by peers is usually expensive and does not take into account\nsimilarities among peers. In this paper we present preliminary simulations of\nPROSA, a novel algorithm for P2P network structuring, inspired by social\nbehaviours. Peers in PROSA self--organise in social groups of similar peers,\ncalled ``semantic--groups'', depending on the resources they are sharing. Such\na network smoothly evolves to a small--world graph, where queries for resources\nare efficiently and effectively routed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0702126v1", 
    "title": "Efficient Searching and Retrieval of Documents in PROSA", 
    "arxiv-id": "cs/0702126v1", 
    "author": "M. Malgeri", 
    "publish": "2007-02-22T10:22:39Z", 
    "summary": "Retrieving resources in a distributed environment is more difficult than\nfinding data in centralised databases. In the last decade P2P system arise as\nnew and effective distributed architectures for resource sharing, but searching\nin such environments could be difficult and time-consuming. In this paper we\ndiscuss efficiency of resource discovery in PROSA, a self-organising P2P system\nheavily inspired by social networks. All routing choices in PROSA are made\nlocally, looking only at the relevance of the next peer to each query. We show\nthat PROSA is able to effectively answer queries for rare documents, forwarding\nthem through the most convenient path to nodes that much probably share\nmatching resources. This result is heavily related to the small-world structure\nthat naturally emerges in PROSA."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0702127v1", 
    "title": "Exploiting social networks dynamics for P2P resource organisation", 
    "arxiv-id": "cs/0702127v1", 
    "author": "M. Malgeri", 
    "publish": "2007-02-22T10:35:50Z", 
    "summary": "In this paper we present a formal description of PROSA, a P2P resource\nmanagement system heavily inspired by social networks. Social networks have\nbeen deeply studied in the last two decades in order to understand how\ncommunities of people arise and grow. It is a widely known result that networks\nof social relationships usually evolves to small-worlds, i.e. networks where\nnodes are strongly connected to neighbours and separated from all other nodes\nby a small amount of hops. This work shows that algorithms implemented into\nPROSA allow to obtain an efficient small-world P2P network."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0703027v2", 
    "title": "Interroger un corpus par le sens", 
    "arxiv-id": "cs/0703027v2", 
    "author": "Bernard Jacquemin", 
    "publish": "2007-03-06T19:50:37Z", 
    "summary": "In textual knowledge management, statistical methods prevail. Nonetheless,\nsome difficulties cannot be overcome by these methodologies. I propose a\nsymbolic approach using a complete textual analysis to identify which analysis\nlevel can improve the the answers provided by a system. The approach identifies\nword senses and relation between words and generates as many rephrasings as\npossible. Using synonyms and derivative, the system provides new utterances\nwithout changing the original meaning of the sentences. Such a way, an\ninformation can be retrieved whatever the question or answer's wording may be."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0703042v1", 
    "title": "Recommender System for Online Dating Service", 
    "arxiv-id": "cs/0703042v1", 
    "author": "Vaclav Petricek", 
    "publish": "2007-03-09T15:38:27Z", 
    "summary": "Users of online dating sites are facing information overload that requires\nthem to manually construct queries and browse huge amount of matching user\nprofiles. This becomes even more problematic for multimedia profiles. Although\nmatchmaking is frequently cited as a typical application for recommender\nsystems, there is a surprising lack of work published in this area. In this\npaper we describe a recommender system we implemented and perform a\nquantitative comparison of two collaborative filtering (CF) and two global\nalgorithms. Results show that collaborative filtering recommenders\nsignificantly outperform global algorithms that are currently used by dating\nsites. A blind experiment with real users also confirmed that users prefer CF\nbased recommendations to global popularity recommendations. Recommender systems\nshow a great potential for online dating where they could improve the value of\nthe service to users and improve monetization of the service."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0703095v1", 
    "title": "Copula Component Analysis", 
    "arxiv-id": "cs/0703095v1", 
    "author": "Zengqi Sun", 
    "publish": "2007-03-20T14:52:52Z", 
    "summary": "A framework named Copula Component Analysis (CCA) for blind source separation\nis proposed as a generalization of Independent Component Analysis (ICA). It\ndiffers from ICA which assumes independence of sources that the underlying\ncomponents may be dependent with certain structure which is represented by\nCopula. By incorporating dependency structure, much accurate estimation can be\nmade in principle in the case that the assumption of independence is\ninvalidated. A two phrase inference method is introduced for CCA which is based\non the notion of multidimensional ICA."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/cs/0703131v1", 
    "title": "Open Access Scientometrics and the UK Research Assessment Exercise", 
    "arxiv-id": "cs/0703131v1", 
    "author": "Stevan Harnad", 
    "publish": "2007-03-26T15:40:08Z", 
    "summary": "Scientometric predictors of research performance need to be validated by\nshowing that they have a high correlation with the external criterion they are\ntrying to predict. The UK Research Assessment Exercise (RAE), together with the\ngrowing movement toward making the full-texts of research articles freely\navailable on the web -- offer a unique opportunity to test and validate a\nwealth of old and new scientometric predictors, through multiple regression\nanalysis: Publications, journal impact factors, citations, co-citations,\ncitation chronometrics (age, growth, latency to peak, decay rate),\nhub/authority scores, h-index, prior funding, student counts, co-authorship\nscores, endogamy/exogamy, textual proximity, download/co-downloads and their\nchronometrics, etc. can all be tested and validated jointly, discipline by\ndiscipline, against their RAE panel rankings in the forthcoming parallel\npanel-based and metric RAE in 2008. The weights of each predictor can be\ncalibrated to maximize the joint correlation with the rankings. Open Access\nScientometrics will provide powerful new means of navigating, evaluating,\npredicting and analyzing the growing Open Access database, as well as powerful\nincentives for making it grow faster. ~"
},{
    "category": "math.PR", 
    "doi": "10.1016/j.ipm.2006.11.001", 
    "link": "http://arxiv.org/pdf/math/0607507v3", 
    "title": "In-Degree and PageRank of Web pages: Why do they follow similar power   laws?", 
    "arxiv-id": "math/0607507v3", 
    "author": "Y. Volkovich", 
    "publish": "2006-07-20T16:07:10Z", 
    "summary": "The PageRank is a popularity measure designed by Google to rank Web pages.\nExperiments confirm that the PageRank obeys a `power law' with the same\nexponent as the In-Degree. This paper presents a novel mathematical model that\nexplains this phenomenon. The relation between the PageRank and In-Degree is\nmodelled through a stochastic equation, which is inspired by the original\ndefinition of the PageRank, and is analogous to the well-known distributional\nidentity for the busy period in the M/G/1 queue. Further, we employ the theory\nof regular variation and Tauberian theorems to analytically prove that the tail\nbehavior of the PageRank and the In-Degree differ only by a multiplicative\nfactor, for which we derive a closed-form expression. Our analytical results\nare in good agreement with experimental data."
},{
    "category": "nlin.AO", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/nlin/0409013v2", 
    "title": "Epistemic communities: description and hierarchic categorization", 
    "arxiv-id": "nlin/0409013v2", 
    "author": "Paul Bourgine", 
    "publish": "2004-09-06T14:48:40Z", 
    "summary": "Social scientists have shown an increasing interest in understanding the\nstructure of knowledge communities, and particularly the organization of\n\"epistemic communities\", that is groups of agents sharing common knowledge\nconcerns. However, most existing approaches are based only on either social\nrelationships or semantic similarity, while there has been roughly no attempt\nto link social and semantic aspects. In this paper, we introduce a formal\nframework addressing this issue and propose a method based on Galois lattices\n(or concept lattices) for categorizing epistemic communities in an automated\nand hierarchically structured fashion. Suggesting that our process allows us to\nrebuild a whole community structure and taxonomy, and notably fields and\nsubfields gathering a certain proportion of agents, we eventually apply it to\nempirical data to exhibit these alleged structural properties, and successfully\ncompare our results with categories spontaneously given by domain experts."
},{
    "category": "cs.DL", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0704.2902v1", 
    "title": "Recommending Related Papers Based on Digital Library Access Records", 
    "arxiv-id": "0704.2902v1", 
    "author": "Thorsten Joachims", 
    "publish": "2007-04-23T16:51:40Z", 
    "summary": "An important goal for digital libraries is to enable researchers to more\neasily explore related work. While citation data is often used as an indicator\nof relatedness, in this paper we demonstrate that digital access records (e.g.\nhttp-server logs) can be used as indicators as well. In particular, we show\nthat measures based on co-access provide better coverage than co-citation, that\nthey are available much sooner, and that they are more accurate for recent\npapers."
},{
    "category": "cs.DL", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0704.2963v1", 
    "title": "Using Access Data for Paper Recommendations on ArXiv.org", 
    "arxiv-id": "0704.2963v1", 
    "author": "Stefan Pohl", 
    "publish": "2007-04-23T15:52:47Z", 
    "summary": "This thesis investigates in the use of access log data as a source of\ninformation for identifying related scientific papers. This is done for\narXiv.org, the authority for publication of e-prints in several fields of\nphysics.\n  Compared to citation information, access logs have the advantage of being\nimmediately available, without manual or automatic extraction of the citation\ngraph. Because of that, a main focus is on the question, how far user behavior\ncan serve as a replacement for explicit meta-data, which potentially might be\nexpensive or completely unavailable. Therefore, we compare access, content, and\ncitation-based measures of relatedness on different recommendation tasks. As a\nfinal result, an online recommendation system has been built that can help\nscientists to find further relevant literature, without having to search for\nthem actively."
},{
    "category": "cs.IR", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0704.3359v1", 
    "title": "Direct Optimization of Ranking Measures", 
    "arxiv-id": "0704.3359v1", 
    "author": "Alexander Smola", 
    "publish": "2007-04-25T12:36:55Z", 
    "summary": "Web page ranking and collaborative filtering require the optimization of\nsophisticated performance measures. Current Support Vector approaches are\nunable to optimize them directly and focus on pairwise comparisons instead. We\npresent a new approach which allows direct optimization of the relevant loss\nfunctions. This is achieved via structured estimation in Hilbert spaces. It is\nmost related to Max-Margin-Markov networks optimization of multivariate\nperformance measures. Key to our approach is that during training the ranking\nproblem can be viewed as a linear assignment problem, which can be solved by\nthe Hungarian Marriage algorithm. At test time, a sort operation is sufficient,\nas our algorithm assigns a relevance score to every (document, query) pair.\nExperiments show that the our algorithm is fast and that it works very well."
},{
    "category": "cs.CV", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0704.3635v1", 
    "title": "Rough Sets Computations to Impute Missing Data", 
    "arxiv-id": "0704.3635v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-04-26T22:22:45Z", 
    "summary": "Many techniques for handling missing data have been proposed in the\nliterature. Most of these techniques are overly complex. This paper explores an\nimputation technique based on rough set computations. In this paper,\ncharacteristic relations are introduced to describe incompletely specified\ndecision tables.It is shown that the basic rough set idea of lower and upper\napproximations for incompletely specified decision tables may be defined in a\nvariety of different ways. Empirical results obtained using real data are given\nand they provide a valuable and promising insight to the problem of missing\ndata. Missing data were predicted with an accuracy of up to 99%."
},{
    "category": "cs.IR", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0705.0751v1", 
    "title": "Approximate textual retrieval", 
    "arxiv-id": "0705.0751v1", 
    "author": "Pere Constans", 
    "publish": "2007-05-05T17:27:42Z", 
    "summary": "An approximate textual retrieval algorithm for searching sources with high\nlevels of defects is presented. It considers splitting the words in a query\ninto two overlapping segments and subsequently building composite regular\nexpressions from interlacing subsets of the segments. This procedure reduces\nthe probability of missed occurrences due to source defects, yet diminishes the\nretrieval of irrelevant, non-contextual occurrences."
},{
    "category": "cs.IR", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0705.1161v1", 
    "title": "IDF revisited: A simple new derivation within the Robertson-Sp\u00e4rck   Jones probabilistic model", 
    "arxiv-id": "0705.1161v1", 
    "author": "Lillian Lee", 
    "publish": "2007-05-08T20:08:13Z", 
    "summary": "There have been a number of prior attempts to theoretically justify the\neffectiveness of the inverse document frequency (IDF). Those that take as their\nstarting point Robertson and Sparck Jones's probabilistic model are based on\nstrong or complex assumptions. We show that a more intuitively plausible\nassumption suffices. Moreover, the new assumption, while conceptually very\nsimple, provides a solution to an estimation problem that had been deemed\nintractable by Robertson and Walker (1997)."
},{
    "category": "cs.DL", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0705.2106v1", 
    "title": "Scientific citations in Wikipedia", 
    "arxiv-id": "0705.2106v1", 
    "author": "Finn Aarup Nielsen", 
    "publish": "2007-05-15T09:42:30Z", 
    "summary": "The Internet-based encyclopaedia Wikipedia has grown to become one of the\nmost visited web-sites on the Internet. However, critics have questioned the\nquality of entries, and an empirical study has shown Wikipedia to contain\nerrors in a 2005 sample of science entries. Biased coverage and lack of sources\nare among the \"Wikipedia risks\". The present work describes a simple assessment\nof these aspects by examining the outbound links from Wikipedia articles to\narticles in scientific journals with a comparison against journal statistics\nfrom Journal Citation Reports such as impact factors. The results show an\nincreasing use of structured citation markup and good agreement with the\ncitation pattern seen in the scientific literature though with a slight\ntendency to cite articles in high-impact journals such as Nature and Science.\nThese results increase confidence in Wikipedia as an good information organizer\nfor science in general."
},{
    "category": "cs.IR", 
    "doi": "10.1080/08898480590931404", 
    "link": "http://arxiv.org/pdf/0705.4606v1", 
    "title": "Dynamic User-Defined Similarity Searching in Semi-Structured Text   Retrieval", 
    "arxiv-id": "0705.4606v1", 
    "author": "Marco Pellegrini", 
    "publish": "2007-05-31T13:46:39Z", 
    "summary": "Modern text retrieval systems often provide a similarity search utility, that\nallows the user to find efficiently a fixed number k of documents in the data\nset that are most similar to a given query (here a query is either a simple\nsequence of keywords or the identifier of a full document found in previous\nsearches that is considered of interest). We consider the case of a textual\ndatabase made of semi-structured documents. Each field, in turns, is modelled\nwith a specific vector space. The problem is more complex when we also allow\neach such vector space to have an associated user-defined dynamic weight that\ninfluences its contribution to the overall dynamic aggregated and weighted\nsimilarity. This dynamic problem has been tackled in a recent paper by\nSingitham et al. in in VLDB 2004. Their proposed solution, which we take as\nbaseline, is a variant of the cluster-pruning technique that has the potential\nfor scaling to very large corpora of documents, and is far more efficient than\nthe naive exhaustive search. We devise an alternative way of embedding weights\nin the data structure, coupled with a non-trivial application of a clustering\nalgorithm based on the furthest point first heuristic for the metric k-center\nproblem. The validity of our approach is demonstrated experimentally by showing\nsignificant performance improvements over the scheme proposed in Singitham et\nal. in VLDB 2004. We improve significantly tradeoffs between query time and\noutput quality with respect to the baseline method in Singitham et al. in in\nVLDB 2004, and also with respect to a novel method by Chierichetti et al. to\nappear in ACM PODS 2007. We also speed up the pre-processing time by a factor\nat least thirty."
},{
    "category": "cs.OH", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0706.1179v1", 
    "title": "Collaborative product and process model: Multiple Viewpoints approach", 
    "arxiv-id": "0706.1179v1", 
    "author": "Nikolaos Sapidis", 
    "publish": "2007-06-08T13:09:13Z", 
    "summary": "The design and development of complex products invariably involves many\nactors who have different points of view on the problem they are addressing,\nthe product being developed, and the process by which it is being developed.\nThe actors' viewpoints approach was designed to provide an organisational\nframework in which these different perspectives or points of views, and their\nrelationships, could be explicitly gathered and formatted (by actor activity's\nfocus). The approach acknowledges the inevitability of multiple interpretation\nof product information as different views, promotes gathering of actors'\ninterests, and encourages retrieved adequate information while providing\nsupport for integration through PLM and/or SCM collaboration. In this paper, we\npresent our multiple viewpoints approach, and we illustrate it by an industrial\nexample on cyclone vessel product."
},{
    "category": "cs.NI", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0707.1083v1", 
    "title": "Delayed Correlations in Inter-Domain Network Traffic", 
    "arxiv-id": "0707.1083v1", 
    "author": "Mehmed Kantardzic", 
    "publish": "2007-07-07T19:01:51Z", 
    "summary": "To observe the evolution of network traffic correlations we analyze the\neigenvalue spectra and eigenvectors statistics of delayed correlation matrices\nof network traffic counts time series. Delayed correlation matrix D is composed\nof the correlations between one variable in the multivariable time series and\nanother at a time delay \\tau . Inverse participation ratio (IPR) of\neigenvectors of D deviates substantially from the IPR of eigenvectors of the\nequal time correlation matrix C. We relate this finding to the localization and\ndiscuss its importance for network congestion control. The time-lagged\ncorrelation pattern between network time series is preserved over a long time,\nup to 100\\tau, where \\tau=300 sec. The largest eigenvalue \\lambda_{max} of D\nand the corresponding IPR oscillate with two characteristic periods of 3\\tau\nand 6\\tau . The existence of delayed correlations between network time series\nfits well into the long range dependence (LRD) property of the network traffic.\n  The ability to monitor and control the long memory processes is crucial since\nthey impact the network performance. Injecting the random traffic counts\nbetween non-randomly correlated time series, we were able to break the picture\nof periodicity of \\lambda_{max}. In addition, we investigated influence of the\nperiodic injections on both largest eigenvalue and the IPR, and addressed\nrelevance of these indicators for the LRD and self-similarity of the network\ntraffic."
},{
    "category": "cs.DL", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0707.3575v1", 
    "title": "An exploratory study of Google Scholar", 
    "arxiv-id": "0707.3575v1", 
    "author": "Anne-Kathrin Walter", 
    "publish": "2007-07-24T19:19:36Z", 
    "summary": "The paper discusses and analyzes the scientific search service Google Scholar\n(GS). The focus is on an exploratory study which investigates the coverage of\nscientific serials in GS. The study shows deficiencies in the coverage and\nup-to-dateness of the GS index. Furthermore, the study points up which Web\nservers are the most important data providers for this search service and which\ninformation sources are highly represented. We can show that there is a\nrelatively large gap in Google Scholars coverage of German literature as well\nas weaknesses in the accessibility of Open Access content.\n  Keywords: Search engines, Digital libraries, Worldwide Web, Serials,\nElectronic journals"
},{
    "category": "cs.NA", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0708.4149v2", 
    "title": "On the complexity of nonnegative matrix factorization", 
    "arxiv-id": "0708.4149v2", 
    "author": "Stephen A. Vavasis", 
    "publish": "2007-08-30T13:01:33Z", 
    "summary": "Nonnegative matrix factorization (NMF) has become a prominent technique for\nthe analysis of image databases, text databases and other information retrieval\nand clustering applications. In this report, we define an exact version of NMF.\nThen we establish several results about exact NMF: (1) that it is equivalent to\na problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a\npolynomial-time local search heuristic exists."
},{
    "category": "cs.IR", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0710.0169v6", 
    "title": "Evaluation experiments on related terms search in Wikipedia: Information   Content and Adapted HITS (In Russian)", 
    "arxiv-id": "0710.0169v6", 
    "author": "A. A. Krizhanovsky", 
    "publish": "2007-10-01T16:04:52Z", 
    "summary": "The classification of metrics and algorithms search for related terms via\nWordNet, Roget's Thesaurus, and Wikipedia was extended to include adapted HITS\nalgorithm. Evaluation experiments on Information Content and adapted HITS\nalgorithm are described. The test collection of Russian word pairs with\nhuman-assigned similarity judgments is proposed.\n  -----\n  Klassifikacija metrik i algoritmov poiska semanticheski blizkih slov v\ntezaurusah WordNet, Rozhe i jenciklopedii Vikipedija rasshirena adaptirovannym\nHITS algoritmom. S pomow'ju jeksperimentov v Vikipedii oceneny metrika\nInformation Content i adaptirovannyj algoritm HITS. Predlozhen resurs dlja\nocenki semanticheskoj blizosti russkih slov."
},{
    "category": "cs.DB", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0710.1404v1", 
    "title": "Performance Comparison of Persistence Frameworks", 
    "arxiv-id": "0710.1404v1", 
    "author": "Ashwin a K", 
    "publish": "2007-10-07T08:22:53Z", 
    "summary": "One of the essential and most complex components in the software development\nprocess is the database. The complexity increases when the \"orientation\" of the\ninteracting components differs. A persistence framework moves the program data\nin its most natural form to and from a permanent data store, the database. Thus\na persistence framework manages the database and the mapping between the\ndatabase and the objects. This paper compares the performance of two\npersistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking\ndatabase. The performance of both of these tools in single and multi-user\nenvironments are evaluated."
},{
    "category": "cs.DS", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0710.1525v2", 
    "title": "Efficient Optimally Lazy Algorithms for Minimal-Interval Semantics", 
    "arxiv-id": "0710.1525v2", 
    "author": "Paolo Boldi", 
    "publish": "2007-10-08T12:15:48Z", 
    "summary": "Minimal-interval semantics associates with each query over a document a set\nof intervals, called witnesses, that are incomparable with respect to inclusion\n(i.e., they form an antichain): witnesses define the minimal regions of the\ndocument satisfying the query. Minimal-interval semantics makes it easy to\ndefine and compute several sophisticated proximity operators, provides snippets\nfor user presentation, and can be used to rank documents. In this paper we\nprovide algorithms for computing conjunction and disjunction that are linear in\nthe number of intervals and logarithmic in the number of operands; for\nadditional operators, such as ordered conjunction and Brouwerian difference, we\nprovide linear algorithms. In all cases, space is linear in the number of\noperands. More importantly, we define a formal notion of optimal laziness, and\neither prove it, or prove its impossibility, for each algorithm. We cast our\nresults in a general framework of antichains of intervals on total orders,\nmaking our algorithms directly applicable to other domains."
},{
    "category": "cs.LG", 
    "doi": "10.1000/ISBN0-85358-228-9", 
    "link": "http://arxiv.org/pdf/0710.2889v2", 
    "title": "An efficient reduction of ranking to classification", 
    "arxiv-id": "0710.2889v2", 
    "author": "Mehryar Mohri", 
    "publish": "2007-10-15T18:25:15Z", 
    "summary": "This paper describes an efficient reduction of the learning problem of\nranking to binary classification. The reduction guarantees an average pairwise\nmisranking regret of at most that of the binary classifier regret, improving a\nrecent result of Balcan et al which only guarantees a factor of 2. Moreover,\nour reduction applies to a broader class of ranking loss functions, admits a\nsimpler proof, and the expected running time complexity of our algorithm in\nterms of number of calls to a classifier or preference function is improved\nfrom $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked\nelements only are required ($k \\ll n$), as in many applications in information\nextraction or search engines, the time complexity of our algorithm can be\nfurther reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus\npractical for realistic applications where the number of points to rank exceeds\nseveral thousands. Much of our results also extend beyond the bipartite case\npreviously studied.\n  Our rediction is a randomized one. To complement our result, we also derive\nlower bounds on any deterministic reduction from binary (preference)\nclassification to ranking, implying that our use of a randomized reduction is\nessentially necessary for the guarantees we provide."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0710.3502v1", 
    "title": "Using Synchronic and Diachronic Relations for Summarizing Multiple   Documents Describing Evolving Events", 
    "arxiv-id": "0710.3502v1", 
    "author": "C. Halatsis", 
    "publish": "2007-10-18T13:24:26Z", 
    "summary": "In this paper we present a fresh look at the problem of summarizing evolving\nevents from multiple sources. After a discussion concerning the nature of\nevolving events we introduce a distinction between linearly and non-linearly\nevolving events. We present then a general methodology for the automatic\ncreation of summaries from evolving events. At its heart lie the notions of\nSynchronic and Diachronic cross-document Relations (SDRs), whose aim is the\nidentification of similarities and differences between sources, from a\nsynchronical and diachronical perspective. SDRs do not connect documents or\ntextual elements found therein, but structures one might call messages.\nApplying this methodology will yield a set of messages and relations, SDRs,\nconnecting them, that is a graph which we call grid. We will show how such a\ngrid can be considered as the starting point of a Natural Language Generation\nSystem. The methodology is evaluated in two case-studies, one for linearly\nevolving events (descriptions of football matches) and another one for\nnon-linearly evolving events (terrorist incidents involving hostages). In both\ncases we evaluate the results produced by our computational systems."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0711.2615v1", 
    "title": "A Biologically Inspired Classifier", 
    "arxiv-id": "0711.2615v1", 
    "author": "Francesca Di Patti", 
    "publish": "2007-11-16T13:38:15Z", 
    "summary": "We present a method for measuring the distance among records based on the\ncorrelations of data stored in the corresponding database entries. The original\nmethod (F. Bagnoli, A. Berrones and F. Franci. Physica A 332 (2004) 509-518)\nwas formulated in the context of opinion formation. The opinions expressed over\na set of topic originate a ``knowledge network'' among individuals, where two\nindividuals are nearer the more similar their expressed opinions are. Assuming\nthat individuals' opinions are stored in a database, the authors show that it\nis possible to anticipate an opinion using the correlations in the database.\nThis corresponds to approximating the overlap between the tastes of two\nindividuals with the correlations of their expressed opinions.\n  In this paper we extend this model to nonlinear matching functions, inspired\nby biological problems such as microarray (probe-sample pairing). We\ninvestigate numerically the error between the correlation and the overlap\nmatrix for eight sequences of reference with random probes. Results show that\nthis method is particularly robust for detecting similarities in the presence\nof translocations."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0711.2867v1", 
    "title": "Maximizing PageRank via outlinks", 
    "arxiv-id": "0711.2867v1", 
    "author": "Paul Van Dooren", 
    "publish": "2007-11-19T09:43:22Z", 
    "summary": "We analyze linkage strategies for a set I of webpages for which the webmaster\nwants to maximize the sum of Google's PageRank scores. The webmaster can only\nchoose the hyperlinks starting from the webpages of I and has no control on the\nhyperlinks from other webpages. We provide an optimal linkage strategy under\nsome reasonable assumptions."
},{
    "category": "cs.DL", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0711.4142v2", 
    "title": "Content Reuse and Interest Sharing in Tagging Communities", 
    "arxiv-id": "0711.4142v2", 
    "author": "Adriana Iamnitchi", 
    "publish": "2007-11-26T23:05:02Z", 
    "summary": "Tagging communities represent a subclass of a broader class of user-generated\ncontent-sharing online communities. In such communities users introduce and tag\ncontent for later use. Although recent studies advocate and attempt to harness\nsocial knowledge in this context by exploiting collaboration among users,\nlittle research has been done to quantify the current level of user\ncollaboration in these communities. This paper introduces two metrics to\nquantify the level of collaboration: content reuse and shared interest. Using\nthese two metrics, this paper shows that the current level of collaboration in\nCiteULike and Connotea is consistently low, which significantly limits the\npotential of harnessing the social knowledge in communities. This study also\ndiscusses implications of these findings in the context of recommendation and\nreputation systems."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0712.3298v1", 
    "title": "CLAIRLIB Documentation v1.03", 
    "arxiv-id": "0712.3298v1", 
    "author": "Bryan Gibson", 
    "publish": "2007-12-19T22:20:40Z", 
    "summary": "The Clair library is intended to simplify a number of generic tasks in\nNatural Language Processing (NLP), Information Retrieval (IR), and Network\nAnalysis. Its architecture also allows for external software to be plugged in\nwith very little effort. Functionality native to Clairlib includes\nTokenization, Summarization, LexRank, Biased LexRank, Document Clustering,\nDocument Indexing, PageRank, Biased PageRank, Web Graph Analysis, Network\nGeneration, Power Law Distribution Analysis, Network Analysis (clustering\ncoefficient, degree distribution plotting, average shortest path, diameter,\ntriangles, shortest path matrices, connected components), Cosine Similarity,\nRandom Walks on Graphs, Statistics (distributions, tests), Tf, Idf, Community\nFinding."
},{
    "category": "cs.DL", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0801.0386v1", 
    "title": "Spam: It's Not Just for Inboxes and Search Engines! Making Hirsch   h-index Robust to Scientospam", 
    "arxiv-id": "0801.0386v1", 
    "author": "Panayiotis Bozanis", 
    "publish": "2008-01-02T13:06:37Z", 
    "summary": "What is the 'level of excellence' of a scientist and the real impact of\nhis/her work upon the scientific thinking and practising? How can we design a\nfair, an unbiased metric -- and most importantly -- a metric robust to\nmanipulation?"
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0801.1063v1", 
    "title": "Modeling Online Reviews with Multi-grain Topic Models", 
    "arxiv-id": "0801.1063v1", 
    "author": "Ryan McDonald", 
    "publish": "2008-01-07T17:01:34Z", 
    "summary": "In this paper we present a novel framework for extracting the ratable aspects\nof objects from online user reviews. Extracting such aspects is an important\nchallenge in automatically mining product opinions from the web and in\ngenerating opinion-based summaries of user reviews. Our models are based on\nextensions to standard topic modeling methods such as LDA and PLSA to induce\nmulti-grain topics. We argue that multi-grain models are more appropriate for\nour task since standard models tend to produce topics that correspond to global\nproperties of objects (e.g., the brand of a product type) rather than the\naspects of an object that tend to be rated by a user. The models we present not\nonly extract ratable aspects, but also cluster them into coherent topics, e.g.,\n`waitress' and `bartender' are part of the same topic `staff' for restaurants.\nThis differentiates it from much of the previous work which extracts aspects\nthrough term frequency analysis with minimal clustering. We evaluate the\nmulti-grain models both qualitatively and quantitatively to show that they\nimprove significantly upon standard topic models."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0801.1179v2", 
    "title": "Corpus sp{\u00e9}cialis{\u00e9} et ressource de sp{\u00e9}cialit{\u00e9}", 
    "arxiv-id": "0801.1179v2", 
    "author": "Sabine Ploux", 
    "publish": "2008-01-08T08:21:26Z", 
    "summary": "\"Semantic Atlas\" is a mathematic and statistic model to visualise word senses\naccording to relations between words. The model, that has been applied to\nproximity relations from a corpus, has shown its ability to distinguish word\nsenses as the corpus' contributors comprehend them. We propose to use the model\nand a specialised corpus in order to create automatically a specialised\ndictionary relative to the corpus' domain. A morpho-syntactic analysis\nperformed on the corpus makes it possible to create the dictionary from\nsyntactic relations between lexical units. The semantic resource can be used to\nnavigate semantically - and not only lexically - through the corpus, to create\nclassical dictionaries or for diachronic studies of the language."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0801.2378v1", 
    "title": "String algorithms and data structures", 
    "arxiv-id": "0801.2378v1", 
    "author": "Paolo Ferragina", 
    "publish": "2008-01-15T20:54:18Z", 
    "summary": "The string-matching field has grown at a such complicated stage that various\nissues come into play when studying it: data structure and algorithmic design,\ndatabase principles, compression techniques, architectural features, cache and\nprefetching policies. The expertise nowadays required to design good string\ndata structures and algorithms is therefore transversal to many computer\nscience fields and much more study on the orchestration of known, or novel,\ntechniques is needed to make progress in this fascinating topic. This survey is\naimed at illustrating the key ideas which should constitute, in our opinion,\nthe current background of every index designer. We also discuss the positive\nfeatures and drawback of known indexing schemes and algorithms, and devote much\nattention to detail research issues and open problems both on the theoretical\nand the experimental side."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0801.3102v1", 
    "title": "Balancing transparency, efficiency and security in pervasive systems", 
    "arxiv-id": "0801.3102v1", 
    "author": "Ali Hurson", 
    "publish": "2008-01-20T19:15:50Z", 
    "summary": "This chapter will survey pervasive computing with a look at how its\nconstraint for transparency affects issues of resource management and security.\nThe goal of pervasive computing is to render computing transparent, such that\ncomputing resources are ubiquitously offered to the user and services are\nproactively performed for a user without his or her intervention. The task of\nintegrating computing infrastructure into everyday life without making it\nexcessively invasive brings about tradeoffs between flexibility and robustness,\nefficiency and effectiveness, as well as autonomy and reliability. As the\nfeasibility of ubiquitous computing and its real potential for mass\napplications are still a matter of controversy, this chapter will look into the\nunderlying issues of resource management and authentication to discover how\nthese can be handled in a least invasive fashion. The discussion will be closed\nby an overview of the solutions proposed by current pervasive computing\nefforts, both in the area of generic platforms and for dedicated applications\nsuch as pervasive education and healthcare."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0802.1738v2", 
    "title": "Characterising through Erasing: A Theoretical Framework for Representing   Documents Inspired by Quantum Theory", 
    "arxiv-id": "0802.1738v2", 
    "author": "C. J. van Rijsbergen", 
    "publish": "2008-02-12T23:43:20Z", 
    "summary": "The problem of representing text documents within an Information Retrieval\nsystem is formulated as an analogy to the problem of representing the quantum\nstates of a physical system. Lexical measurements of text are proposed as a way\nof representing documents which are akin to physical measurements on quantum\nstates. Consequently, the representation of the text is only known after\nmeasurements have been made, and because the process of measuring may destroy\nparts of the text, the document is characterised through erasure. The\nmathematical foundations of such a quantum representation of text are provided\nin this position paper as a starting point for indexing and retrieval within a\n``quantum like'' Information Retrieval system."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0802.3293v1", 
    "title": "Use of Rapid Probabilistic Argumentation for Ranking on Large Complex   Networks", 
    "arxiv-id": "0802.3293v1", 
    "author": "Haluk Bingol", 
    "publish": "2008-02-22T11:49:16Z", 
    "summary": "We introduce a family of novel ranking algorithms called ERank which run in\nlinear/near linear time and build on explicitly modeling a network as uncertain\nevidence. The model uses Probabilistic Argumentation Systems (PAS) which are a\ncombination of probability theory and propositional logic, and also a special\ncase of Dempster-Shafer Theory of Evidence. ERank rapidly generates approximate\nresults for the NP-complete problem involved enabling the use of the technique\nin large networks. We use a previously introduced PAS model for citation\nnetworks generalizing it for all networks. We propose a statistical test to be\nused for comparing the performances of different ranking algorithms based on a\nclustering validity test. Our experimentation using this test on a real-world\nnetwork shows ERank to have the best performance in comparison to well-known\nalgorithms including PageRank, closeness, and betweenness."
},{
    "category": "cs.CR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0802.3746v1", 
    "title": "Information Hiding Techniques: A Tutorial Review", 
    "arxiv-id": "0802.3746v1", 
    "author": "Sabu M. Thampi", 
    "publish": "2008-02-26T05:22:30Z", 
    "summary": "The purpose of this tutorial is to present an overview of various information\nhiding techniques. A brief history of steganography is provided along with\ntechniques that were used to hide information. Text, image and audio based\ninformation hiding techniques are discussed. This paper also provides a basic\nintroduction to digital watermarking."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0803.0053v1", 
    "title": "Mobile Agents for Content-Based WWW Distributed Image Retrieval", 
    "arxiv-id": "0803.0053v1", 
    "author": "K. Chandra Sekaran", 
    "publish": "2008-03-01T08:27:59Z", 
    "summary": "At present, the de-facto standard for providing contents in the Internet is\nthe World Wide Web. A technology, which is now emerging on the Web, is\nContent-Based Image Retrieval (CBIR). CBIR applies methods and algorithms from\ncomputer science to analyse and index images based on their visual content.\nMobile agents push the flexibility of distributed systems to their limits since\nnot only computations are dynamically distributed but also the code that\nperforms them. The current commercial applet-based methodologies for accessing\nimage database systems offer limited flexibility, scalability and robustness.\nIn this paper the author proposes a new framework for content-based WWW\ndistributed image retrieval based on Java-based mobile agents. The\nimplementation of the framework shows that its performance is comparable to,\nand in some cases outperforms, the current approach."
},{
    "category": "cs.MM", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0803.0405v1", 
    "title": "Multi-dimensional sparse time series: feature extraction", 
    "arxiv-id": "0803.0405v1", 
    "author": "Giulia Menconi", 
    "publish": "2008-03-04T10:27:59Z", 
    "summary": "We show an analysis of multi-dimensional time series via entropy and\nstatistical linguistic techniques. We define three markers encoding the\nbehavior of the series, after it has been translated into a multi-dimensional\nsymbolic sequence. The leading component and the trend of the series with\nrespect to a mobile window analysis result from the entropy analysis and label\nthe dynamical evolution of the series. The diversification formalizes the\ndifferentiation in the use of recurrent patterns, from a Zipf law point of\nview. These markers are the starting point of further analysis such as\nclassification or clustering of large database of multi-dimensional time\nseries, prediction of future behavior and attribution of new data. We also\npresent an application to economic data. We deal with measurements of money\ninvestments of some business companies in advertising market for different\nmedia sources."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0803.1716v1", 
    "title": "Citation Counting, Citation Ranking, and h-Index of Human-Computer   Interaction Researchers: A Comparison between Scopus and Web of Science", 
    "arxiv-id": "0803.1716v1", 
    "author": "Yvonne Rogers", 
    "publish": "2008-03-12T08:09:19Z", 
    "summary": "This study examines the differences between Scopus and Web of Science in the\ncitation counting, citation ranking, and h-index of 22 top human-computer\ninteraction (HCI) researchers from EQUATOR--a large British Interdisciplinary\nResearch Collaboration project. Results indicate that Scopus provides\nsignificantly more coverage of HCI literature than Web of Science, primarily\ndue to coverage of relevant ACM and IEEE peer-reviewed conference proceedings.\nNo significant differences exist between the two databases if citations in\njournals only are compared. Although broader coverage of the literature does\nnot significantly alter the relative citation ranking of individual\nresearchers, Scopus helps distinguish between the researchers in a more nuanced\nfashion than Web of Science in both citation counting and h-index. Scopus also\ngenerates significantly different maps of citation networks of individual\nscholars than those generated by Web of Science. The study also presents a\ncomparison of h-index scores based on Google Scholar with those based on the\nunion of Scopus and Web of Science. The study concludes that Scopus can be used\nas a sole data source for citation-based research and evaluation in HCI,\nespecially if citations in conference proceedings are sought and that h scores\nshould be manually calculated instead of relying on system calculations."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0804.0317v1", 
    "title": "Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in   Extracting Information from Biomedical Text", 
    "arxiv-id": "0804.0317v1", 
    "author": "Kevin R. Nicholas", 
    "publish": "2008-04-02T09:34:13Z", 
    "summary": "A recent study reported development of Muscorian, a generic text processing\ntool for extracting protein-protein interactions from text that achieved\ncomparable performance to biomedical-specific text processing tools. This\nresult was unexpected since potential errors from a series of text analysis\nprocesses is likely to adversely affect the outcome of the entire process. Most\nbiomedical entity relationship extraction tools have used biomedical-specific\nparts-of-speech (POS) tagger as errors in POS tagging and are likely to affect\nsubsequent semantic analysis of the text, such as shallow parsing. This study\naims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to\nexplore whether a comparable performance is obtained when a generic POS tagger,\nMontyTagger, was used in place of MedPost, a tagger trained in biomedical text.\nOur results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS\ntagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger\nwith MedPost did not result in a significant improvement in entity relationship\nextraction from text; precision of 55.6% from MontyTagger versus 56.8% from\nMedPost on directional relationships and 86.1% from MontyTagger compared to\n81.8% from MedPost on nondirectional relationships. This is unexpected as the\npotential for poor POS tagging by MontyTagger is likely to affect the outcome\nof the information extraction. An analysis of POS tagging errors demonstrated\nthat 78.5% of tagging errors are being compensated by shallow parsing. Thus,\ndespite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy\nof 94.6%."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0804.2354v2", 
    "title": "Information filtering based on wiki index database", 
    "arxiv-id": "0804.2354v2", 
    "author": "A. A. Krizhanovsky", 
    "publish": "2008-04-15T11:05:59Z", 
    "summary": "In this paper we present a profile-based approach to information filtering by\nan analysis of the content of text documents. The Wikipedia index database is\ncreated and used to automatically generate the user profile from the user\ndocument collection. The problem-oriented Wikipedia subcorpora are created\n(using knowledge extracted from the user profile) for each topic of user\ninterests. The index databases of these subcorpora are applied to filtering\ninformation flow (e.g., mails, news). Thus, the analyzed texts are classified\ninto several topics explicitly presented in the user profile. The paper\nconcentrates on the indexing part of the approach. The architecture of an\napplication implementing the Wikipedia indexing is described. The indexing\nmethod is evaluated using the Russian and Simple English Wikipedia."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0804.3599v1", 
    "title": "Respect My Authority! HITS Without Hyperlinks, Utilizing Cluster-Based   Language Models", 
    "arxiv-id": "0804.3599v1", 
    "author": "Lillian Lee", 
    "publish": "2008-04-22T20:02:14Z", 
    "summary": "We present an approach to improving the precision of an initial document\nranking wherein we utilize cluster information within a graph-based framework.\nThe main idea is to perform re-ranking based on centrality within bipartite\ngraphs of documents (on one side) and clusters (on the other side), on the\npremise that these are mutually reinforcing entities. Links between entities\nare created via consideration of language models induced from them.\n  We find that our cluster-document graphs give rise to much better retrieval\nperformance than previously proposed document-only graphs do. For example,\nauthority-based re-ranking of documents via a HITS-style cluster-based approach\noutperforms a previously-proposed PageRank-inspired algorithm applied to\nsolely-document graphs. Moreover, we also show that computing authority scores\nfor clusters constitutes an effective method for identifying clusters\ncontaining a large percentage of relevant documents."
},{
    "category": "cs.DM", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0804.3671v1", 
    "title": "Constructions for Clumps Statistics", 
    "arxiv-id": "0804.3671v1", 
    "author": "Pierre Nicodeme", 
    "publish": "2008-04-23T10:38:15Z", 
    "summary": "We consider a component of the word statistics known as clump; starting from\na finite set of words, clumps are maximal overlapping sets of these\noccurrences. This parameter has first been studied by Schbath with the aim of\ncounting the number of occurrences of words in random texts. Later work with\nsimilar probabilistic approach used the Chen-Stein approximation for a compound\nPoisson distribution, where the number of clumps follows a law close to\nPoisson. Presently there is no combinatorial counterpart to this approach, and\nwe fill the gap here. We emphasize the fact that, in contrast with the\nprobabilistic approach which only provides asymptotic results, the\ncombinatorial approach provides exact results that are useful when considering\nshort sequences."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0804.4451v1", 
    "title": "Dependence Structure Estimation via Copula", 
    "arxiv-id": "0804.4451v1", 
    "author": "Zengqi Sun", 
    "publish": "2008-04-28T17:14:53Z", 
    "summary": "We propose a new framework for dependence structure learning via copula.\nCopula is a statistical theory on dependence and measurement of association.\nGraphical models are considered as a type of special case of copula families,\nnamed product copula. In this paper, a nonparametric algorithm for copula\nestimation is presented. Then a Chow-Liu like method based on dependence\nmeasure via copula is proposed to estimate maximum spanning product copula with\nonly bivariate dependence relations. The advantage of the framework is that\nlearning with empirical copula focuses only on dependence relations among\nrandom variables, without knowing the properties of individual variables.\nAnother advantage is that copula is a universal model of dependence and\ntherefore the framework based on it can be generalized to deal with a wide\nrange of complex dependence relations. Experiments on both simulated data and\nreal application data show the effectiveness of the proposed method."
},{
    "category": "cs.IR", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0805.0120v1", 
    "title": "Nonnegative Matrix Factorization via Rank-One Downdate", 
    "arxiv-id": "0805.0120v1", 
    "author": "Stephen Vavasis", 
    "publish": "2008-05-01T17:59:44Z", 
    "summary": "Nonnegative matrix factorization (NMF) was popularized as a tool for data\nmining by Lee and Seung in 1999. NMF attempts to approximate a matrix with\nnonnegative entries by a product of two low-rank matrices, also with\nnonnegative entries. We propose an algorithm called rank-one downdate (R1D) for\ncomputing a NMF that is partly motivated by singular value decomposition. This\nalgorithm computes the dominant singular values and vectors of adaptively\ndetermined submatrices of a matrix. On each iteration, R1D extracts a rank-one\nsubmatrix from the dataset according to an objective function. We establish a\ntheoretical result that maximizing this objective function corresponds to\ncorrectly classifying articles in a nearly separable corpus. We also provide\ncomputational experiments showing the success of this method in identifying\nfeatures in realistic datasets."
},{
    "category": "cs.DL", 
    "doi": "10.1007/s10844-006-0025-9", 
    "link": "http://arxiv.org/pdf/0805.2045v1", 
    "title": "Semantic Analysis of Tag Similarity Measures in Collaborative Tagging   Systems", 
    "arxiv-id": "0805.2045v1", 
    "author": "Gerd Stumme", 
    "publish": "2008-05-14T14:10:02Z", 
    "summary": "Social bookmarking systems allow users to organise collections of resources\non the Web in a collaborative fashion. The increasing popularity of these\nsystems as well as first insights into their emergent semantics have made them\nrelevant to disciplines like knowledge extraction and ontology learning. The\nproblem of devising methods to measure the semantic relatedness between tags\nand characterizing it semantically is still largely open. Here we analyze three\nmeasures of tag relatedness: tag co-occurrence, cosine similarity of\nco-occurrence distributions, and FolkRank, an adaptation of the PageRank\nalgorithm to folksonomies. Each measure is computed on tags from a large-scale\ndataset crawled from the social bookmarking system del.icio.us. To provide a\nsemantic grounding of our findings, a connection to WordNet (a semantic lexicon\nfor the English language) is established by mapping tags into synonym sets of\nWordNet, and applying there well-known metrics of semantic similarity. Our\nresults clearly expose different characteristics of the selected measures of\nrelatedness, making them applicable to different subtasks of knowledge\nextraction such as synonym detection or discovery of concept hierarchies."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.websem.2013.05.001", 
    "link": "http://arxiv.org/pdf/0805.2855v3", 
    "title": "LCSH, SKOS and Linked Data", 
    "arxiv-id": "0805.2855v3", 
    "author": "Dan Krech", 
    "publish": "2008-05-19T13:11:41Z", 
    "summary": "A technique for converting Library of Congress Subject Headings MARCXML to\nSimple Knowledge Organization System (SKOS) RDF is described. Strengths of the\nSKOS vocabulary are highlighted, as well as possible points for extension, and\nthe integration of other semantic web vocabularies such as Dublin Core. An\napplication for making the vocabulary available as linked-data on the Web is\nalso described."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.websem.2013.05.001", 
    "link": "http://arxiv.org/pdf/0805.4508v1", 
    "title": "Modeling Loosely Annotated Images with Imagined Annotations", 
    "arxiv-id": "0805.4508v1", 
    "author": "Yunhao Chen", 
    "publish": "2008-05-29T10:35:29Z", 
    "summary": "In this paper, we present an approach to learning latent semantic analysis\nmodels from loosely annotated images for automatic image annotation and\nindexing. The given annotation in training images is loose due to: (1)\nambiguous correspondences between visual features and annotated keywords; (2)\nincomplete lists of annotated keywords. The second reason motivates us to\nenrich the incomplete annotation in a simple way before learning topic models.\nIn particular, some imagined keywords are poured into the incomplete annotation\nthrough measuring similarity between keywords. Then, both given and imagined\nannotations are used to learning probabilistic topic models for automatically\nannotating new images. We conduct experiments on a typical Corel dataset of\nimages and loose annotations, and compare the proposed method with\nstate-of-the-art discrete annotation methods (using a set of discrete blobs to\nrepresent an image). The proposed method improves word-driven probability\nLatent Semantic Analysis (PLSA-words) up to a comparable performance with the\nbest discrete annotation method, while a merit of PLSA-words is still kept,\ni.e., a wider semantic range."
},{
    "category": "cs.CY", 
    "doi": "10.1016/j.websem.2013.05.001", 
    "link": "http://arxiv.org/pdf/0806.1918v1", 
    "title": "Analysis of Social Voting Patterns on Digg", 
    "arxiv-id": "0806.1918v1", 
    "author": "Aram Galstyan", 
    "publish": "2008-06-11T17:53:45Z", 
    "summary": "The social Web is transforming the way information is created and\ndistributed. Blog authoring tools enable users to publish content, while sites\nsuch as Digg and Del.icio.us are used to distribute content to a wider\naudience. With content fast becoming a commodity, interest in using social\nnetworks to promote and find content has grown, both on the side of content\nproducers (viral marketing) and consumers (recommendation). Here we study the\nrole of social networks in promoting content on Digg, a social news aggregator\nthat allows users to submit links to and vote on news stories. Digg's goal is\nto feature the most interesting stories on its front page, and it aggregates\nopinions of its many users to identify them. Like other social networking\nsites, Digg allows users to designate other users as ``friends'' and see what\nstories they found interesting. We studied the spread of interest in news\nstories submitted to Digg in June 2006. Our results suggest that pattern of the\nspread of interest in a story on the network is indicative of how popular the\nstory will become. Stories that spread mainly outside of the submitter's\nneighborhood go on to be very popular, while stories that spread mainly through\nsubmitter's social neighborhood prove not to be very popular. This effect is\nvisible already in the early stages of voting, and one can make a prediction\nabout the potential audience of a story simply by analyzing where the initial\nvotes come from."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.websem.2013.05.001", 
    "link": "http://arxiv.org/pdf/0806.3765v1", 
    "title": "Cross-concordances: terminology mapping and its effectiveness for   information retrieval", 
    "arxiv-id": "0806.3765v1", 
    "author": "Vivien Petras", 
    "publish": "2008-06-23T20:37:10Z", 
    "summary": "The German Federal Ministry for Education and Research funded a major\nterminology mapping initiative, which found its conclusion in 2007. The task of\nthis terminology mapping initiative was to organize, create and manage\n'cross-concordances' between controlled vocabularies (thesauri, classification\nsystems, subject heading lists) centred around the social sciences but quickly\nextending to other subject areas. 64 crosswalks with more than 500,000\nrelations were established. In the final phase of the project, a major\nevaluation effort to test and measure the effectiveness of the vocabulary\nmappings in an information system environment was conducted. The paper reports\non the cross-concordance work and evaluation results."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0807.0023v2", 
    "title": "Automatic Metadata Generation using Associative Networks", 
    "arxiv-id": "0807.0023v2", 
    "author": "Herbert Van de Sompel", 
    "publish": "2008-06-30T21:23:28Z", 
    "summary": "In spite of its tremendous value, metadata is generally sparse and\nincomplete, thereby hampering the effectiveness of digital information\nservices. Many of the existing mechanisms for the automated creation of\nmetadata rely primarily on content analysis which can be costly and\ninefficient. The automatic metadata generation system proposed in this article\nleverages resource relationships generated from existing metadata as a medium\nfor propagation from metadata-rich to metadata-poor resources. Because of its\nindependence from content analysis, it can be applied to a wide variety of\nresource media types and is shown to be computationally inexpensive. The\nproposed method operates through two distinct phases. Occurrence and\nco-occurrence algorithms first generate an associative network of repository\nresources leveraging existing repository metadata. Second, using the\nassociative network as a substrate, metadata associated with metadata-rich\nresources is propagated to metadata-poor resources by means of a discrete-form\nspreading activation algorithm. This article discusses the general framework\nfor building associative networks, an algorithm for disseminating metadata\nthrough such networks, and the results of an experiment and validation of the\nproposed method using a standard bibliographic dataset."
},{
    "category": "cs.SE", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0807.0070v1", 
    "title": "Quantitative Paradigm of Software Reliability as Content Relevance", 
    "arxiv-id": "0807.0070v1", 
    "author": "Yuri Arkhipkin", 
    "publish": "2008-07-01T05:29:07Z", 
    "summary": "This paper presents a quantitative approach to software reliability and\ncontent relevance definitions validated by the systems' potential reliability\nlaw.Thus it is argued for the unified math nature or quantitative paradigm of\nsoftware reliability and content relevance."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0807.1560v1", 
    "title": "Scientific Paper Summarization Using Citation Summary Networks", 
    "arxiv-id": "0807.1560v1", 
    "author": "Dragomir R. Radev", 
    "publish": "2008-07-10T00:01:20Z", 
    "summary": "Quickly moving to a new area of research is painful for researchers due to\nthe vast amount of scientific literature in each field of study. One possible\nway to overcome this problem is to summarize a scientific topic. In this paper,\nwe propose a model of summarizing a single article, which can be further used\nto summarize an entire topic. Our model is based on analyzing others' viewpoint\nof the target article's contributions and the study of its citation summary\nnetwork using a clustering approach."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0807.3006v1", 
    "title": "The rank convergence of HITS can be slow", 
    "arxiv-id": "0807.3006v1", 
    "author": "Luca Pretto", 
    "publish": "2008-07-18T16:42:57Z", 
    "summary": "We prove that HITS, to \"get right\" h of the top k ranked nodes of an N>=2k\nnode graph, can require h^(Omega(N h/k)) iterations (i.e. a substantial Omega(N\nh log(h)/k) matrix multiplications even with a \"squaring trick\"). Our proof\nrequires no algebraic tools and is entirely self-contained."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0807.3755v1", 
    "title": "Approximating Document Frequency with Term Count Values", 
    "arxiv-id": "0807.3755v1", 
    "author": "Michael L. Nelson", 
    "publish": "2008-07-23T21:44:46Z", 
    "summary": "For bounded datasets such as the TREC Web Track (WT10g) the computation of\nterm frequency (TF) and inverse document frequency (IDF) is not difficult.\nHowever, when the corpus is the entire web, direct IDF calculation is\nimpossible and values must instead be estimated. Most available datasets\nprovide values for term count (TC) meaning the number of times a certain term\noccurs in the entire corpus. Intuitively this value is different from document\nfrequency (DF), the number of documents (e.g., web pages) a certain term occurs\nin. We conduct a comparison study between TC and DF values within the Web as\nCorpus (WaC). We found a very strong correlation with Spearman's rho >0.8\n(p<0.005) which makes us confident in claiming that for such recently created\ncorpora the TC and DF values can be used interchangeably to compute IDF values.\nThese results are useful for the generation of accurate lexical signatures\nbased on the TF-IDF scheme."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0808.0973v1", 
    "title": "Text Modeling using Unsupervised Topic Models and Concept Hierarchies", 
    "arxiv-id": "0808.0973v1", 
    "author": "Mark Steyvers", 
    "publish": "2008-08-07T07:59:29Z", 
    "summary": "Statistical topic models provide a general data-driven framework for\nautomated discovery of high-level knowledge from large collections of text\ndocuments. While topic models can potentially discover a broad range of themes\nin a data set, the interpretability of the learned topics is not always ideal.\nHuman-defined concepts, on the other hand, tend to be semantically richer due\nto careful selection of words to define concepts but they tend not to cover the\nthemes in a data set exhaustively. In this paper, we propose a probabilistic\nframework to combine a hierarchy of human-defined semantic concepts with\nstatistical topic models to seek the best of both worlds. Experimental results\nusing two different sources of concept hierarchies and two collections of text\ndocuments indicate that this combination leads to systematic improvements in\nthe quality of the associated language models as well as enabling new\ntechniques for inferring and visualizing the semantics of a document."
},{
    "category": "cs.IR", 
    "doi": "10.1145/1462198.1462199", 
    "link": "http://arxiv.org/pdf/0808.1753v2", 
    "title": "Index wiki database: design and experiments", 
    "arxiv-id": "0808.1753v2", 
    "author": "A. A. Krizhanovsky", 
    "publish": "2008-08-12T23:47:21Z", 
    "summary": "With the fantastic growth of Internet usage, information search in documents\nof a special type called a \"wiki page\" that is written using a simple markup\nlanguage, has become an important problem. This paper describes the software\narchitectural model for indexing wiki texts in three languages (Russian,\nEnglish, and German) and the interaction between the software components (GATE,\nLemmatizer, and Synarcher). The inverted file index database was designed using\nvisual tool DBDesigner. The rules for parsing Wikipedia texts are illustrated\nby examples. Two index databases of Russian Wikipedia (RW) and Simple English\nWikipedia (SEW) are built and compared. The size of RW is by order of magnitude\nhigher than SEW (number of words, lexemes), though the growth rate of number of\npages in SEW was found to be 14% higher than in Russian, and the rate of\nacquisition of new words in SEW lexicon was 7% higher during a period of five\nmonths (from September 2007 to February 2008). The Zipf's law was tested with\nboth Russian and Simple Wikipedias. The entire source code of the indexing\nsoftware and the generated index databases are freely available under GPL (GNU\nGeneral Public License)."
},{
    "category": "cs.IR", 
    "doi": "10.1073/pnas.1000488107", 
    "link": "http://arxiv.org/pdf/0808.2670v3", 
    "title": "Solving the apparent diversity-accuracy dilemma of recommender systems", 
    "arxiv-id": "0808.2670v3", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2008-08-19T23:17:40Z", 
    "summary": "Recommender systems use data on past user preferences to predict possible\nfuture likes and interests. A key challenge is that while the most useful\nindividual recommendations are to be found among diverse niche objects, the\nmost reliably accurate results are obtained by methods that recommend objects\nbased on user or object similarity. In this paper we introduce a new algorithm\nspecifically to address the challenge of diversity and show how it can be used\nto resolve this apparent dilemma when combined in an elegant hybrid with an\naccuracy-focused algorithm. By tuning the hybrid appropriately we are able to\nobtain, without relying on any semantic or context-specific information,\nsimultaneous gains in both accuracy and diversity of recommendations."
},{
    "category": "physics.data-an", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0808.3726v2", 
    "title": "Highly accurate recommendation algorithm based on high-order   similarities", 
    "arxiv-id": "0808.3726v2", 
    "author": "Yi-Cheng Zhang", 
    "publish": "2008-08-27T15:42:02Z", 
    "summary": "In this Letter, we introduce a modified collaborative filtering (MCF)\nalgorithm, which has remarkably higher accuracy than the standard collaborative\nfiltering. In the MCF, instead of the standard Pearson coefficient, the\nuser-user similarities are obtained by a diffusion process. Furthermore, by\nconsidering the second order similarities, we design an effective algorithm\nthat depresses the influence of mainstream preferences. The corresponding\nalgorithmic accuracy, measured by the ranking score, is further improved by\n24.9% in the optimal case. In addition, two significant criteria of algorithmic\nperformance, diversity and popularity, are also taken into account. Numerical\nresults show that the algorithm based on second order similarity can outperform\nthe MCF simultaneously in all three criteria."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0809.0680v1", 
    "title": "The Prolog Interface to the Unstructured Information Management   Architecture", 
    "arxiv-id": "0809.0680v1", 
    "author": "David Ferrucci", 
    "publish": "2008-09-03T17:38:32Z", 
    "summary": "In this paper we describe the design and implementation of the Prolog\ninterface to the Unstructured Information Management Architecture (UIMA) and\nsome of its applications in natural language processing. The UIMA Prolog\ninterface translates unstructured data and the UIMA Common Analysis Structure\n(CAS) into a Prolog knowledge base, over which, the developers write rules and\nuse resolution theorem proving to search and generate new annotations over the\nunstructured data. These rules can explore all the previous UIMA annotations\n(such as, the syntactic structure, parsing statistics) and external Prolog\nknowledge bases (such as, Prolog WordNet and Extended WordNet) to implement a\nvariety of tasks for the natural language analysis. We also describe\napplications of this logic programming interface in question analysis (such as,\nfocus detection, answer-type and other constraints detection), shallow parsing\n(such as, relations in the syntactic structure), and answer selection."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0809.0723v1", 
    "title": "A Simple Mechanism for Focused Web-harvesting", 
    "arxiv-id": "0809.0723v1", 
    "author": "L. T. Handoko", 
    "publish": "2008-09-03T23:53:29Z", 
    "summary": "The focused web-harvesting is deployed to realize an automated and\ncomprehensive index databases as an alternative way for virtual topical data\nintegration. The web-harvesting has been implemented and extended by not only\nspecifying the targeted URLs, but also predefining human-edited harvesting\nparameters to improve the speed and accuracy. The harvesting parameter set\ncomprises three main components. First, the depth-scale of being harvested\nfinal pages containing desired information counted from the first page at the\ntargeted URLs. Secondly, the focus-point number to determine the exact box\ncontaining relevant information. Lastly, the combination of keywords to\nrecognize encountered hyperlinks of relevant images or full-texts embedded in\nthose final pages. All parameters are accessible and fully customizable for\neach target by the administrators of participating institutions over an\nintegrated web interface. A real implementation to the Indonesian Scientific\nIndex which covers all scientific information across Indonesia is also briefly\nintroduced."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0809.2553v1", 
    "title": "Normalized Information Distance", 
    "arxiv-id": "0809.2553v1", 
    "author": "Ming Li", 
    "publish": "2008-09-15T15:33:11Z", 
    "summary": "The normalized information distance is a universal distance measure for\nobjects of all kinds. It is based on Kolmogorov complexity and thus\nuncomputable, but there are ways to utilize it. First, compression algorithms\ncan be used to approximate the Kolmogorov complexity if the objects have a\nstring representation. Second, for names and abstract concepts, page count\nstatistics from the World Wide Web can be used. These practical realizations of\nthe normalized information distance can then be applied to machine learning\ntasks, expecially clustering, to perform feature-free and parameter-free data\nmining. This chapter discusses the theoretical foundations of the normalized\ninformation distance and both practical realizations. It presents numerous\nexamples of successful real-world applications based on these distance\nmeasures, ranging from bioinformatics to music clustering to machine\ntranslation."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0809.3447v1", 
    "title": "An Exploratory Study of Calendar Use", 
    "arxiv-id": "0809.3447v1", 
    "author": "Alyssa Sams", 
    "publish": "2008-09-19T19:56:15Z", 
    "summary": "In this paper, we report on findings from an ethnographic study of how people\nuse their calendars for personal information management (PIM). Our participants\nwere faculty, staff and students who were not required to use or contribute to\nany specific calendaring solution, but chose to do so anyway. The study was\nconducted in three parts: first, an initial survey provided broad insights into\nhow calendars were used; second, this was followed up with personal interviews\nof a few participants which were transcribed and content-analyzed; and third,\nexamples of calendar artifacts were collected to inform our analysis. Findings\nfrom our study include the use of multiple reminder alarms, the reliance on\npaper calendars even among regular users of electronic calendars, and wide use\nof calendars for reporting and life-archival purposes. We conclude the paper\nwith a discussion of what these imply for designers of interactive calendar\nsystems and future work in PIM research."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.1212v1", 
    "title": "Analyse spectrale des textes: d\u00e9tection automatique des fronti\u00e8res   de langue et de discours", 
    "arxiv-id": "0810.1212v1", 
    "author": "Claudia Henry", 
    "publish": "2008-10-07T15:25:31Z", 
    "summary": "We propose a theoretical framework within which information on the vocabulary\nof a given corpus can be inferred on the basis of statistical information\ngathered on that corpus. Inferences can be made on the categories of the words\nin the vocabulary, and on their syntactical properties within particular\nlanguages. Based on the same statistical data, it is possible to build matrices\nof syntagmatic similarity (bigram transition matrices) or paradigmatic\nsimilarity (probability for any pair of words to share common contexts). When\nclustered with respect to their syntagmatic similarity, words tend to group\ninto sublanguage vocabularies, and when clustered with respect to their\nparadigmatic similarity, into syntactic or semantic classes. Experiments have\nexplored the first of these two possibilities. Their results are interpreted in\nthe frame of a Markov chain modelling of the corpus' generative processe(s): we\nshow that the results of a spectral analysis of the transition matrix can be\ninterpreted as probability distributions of words within clusters. This method\nyields a soft clustering of the vocabulary into sublanguages which contribute\nto the generation of heterogeneous corpora. As an application, we show how\nmultilingual texts can be visually segmented into linguistically homogeneous\nsegments. Our method is specifically useful in the case of related languages\nwhich happened to be mixed in corpora."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.1261v1", 
    "title": "Soft Uncoupling of Markov Chains for Permeable Language Distinction: A   New Algorithm", 
    "arxiv-id": "0810.1261v1", 
    "author": "Claudia Henry", 
    "publish": "2008-10-07T18:09:07Z", 
    "summary": "Without prior knowledge, distinguishing different languages may be a hard\ntask, especially when their borders are permeable. We develop an extension of\nspectral clustering -- a powerful unsupervised classification toolbox -- that\nis shown to resolve accurately the task of soft language distinction. At the\nheart of our approach, we replace the usual hard membership assignment of\nspectral clustering by a soft, probabilistic assignment, which also presents\nthe advantage to bypass a well-known complexity bottleneck of the method.\nFurthermore, our approach relies on a novel, convenient construction of a\nMarkov chain out of a corpus. Extensive experiments with a readily available\nsystem clearly display the potential of the method, which brings a visually\nappealing soft distinction of languages that may define altogether a whole\ncorpus."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.2390v2", 
    "title": "Efficient Pattern Matching on Binary Strings", 
    "arxiv-id": "0810.2390v2", 
    "author": "Thierry Lecroq", 
    "publish": "2008-10-14T08:44:27Z", 
    "summary": "The binary string matching problem consists in finding all the occurrences of\na pattern in a text where both strings are built on a binary alphabet. This is\nan interesting problem in computer science, since binary data are omnipresent\nin telecom and computer network applications. Moreover the problem finds\napplications also in the field of image processing and in pattern matching on\ncompressed texts. Recently it has been shown that adaptations of classical\nexact string matching algorithms are not very efficient on binary data. In this\npaper we present two efficient algorithms for the problem adapted to completely\navoid any reference to bits allowing to process pattern and text byte by byte.\nExperimental results show that the new algorithms outperform existing solutions\nin most cases."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.2764v1", 
    "title": "A Simple Linear Ranking Algorithm Using Query Dependent Intercept   Variables", 
    "arxiv-id": "0810.2764v1", 
    "author": "Nir Ailon", 
    "publish": "2008-10-15T19:03:10Z", 
    "summary": "The LETOR website contains three information retrieval datasets used as a\nbenchmark for testing machine learning ideas for ranking. Algorithms\nparticipating in the challenge are required to assign score values to search\nresults for a collection of queries, and are measured using standard IR ranking\nmeasures (NDCG, precision, MAP) that depend only the relative score-induced\norder of the results. Similarly to many of the ideas proposed in the\nparticipating algorithms, we train a linear classifier. In contrast with other\nparticipating algorithms, we define an additional free variable (intercept, or\nbenchmark) for each query. This allows expressing the fact that results for\ndifferent queries are incomparable for the purpose of determining relevance.\nThe cost of this idea is the addition of relatively few nuisance parameters.\nOur approach is simple, and we used a standard logistic regression library to\ntest it. The results beat the reported participating algorithms. Hence, it\nseems promising to combine our approach with other more complex ideas."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.5057v2", 
    "title": "Combining Advanced Visualization and Automatized Reasoning for   Webometrics: A Test Study", 
    "arxiv-id": "0810.5057v2", 
    "author": "Shadi Al Shehabi", 
    "publish": "2008-10-28T15:43:45Z", 
    "summary": "This paper presents a first attempt at performing a precise and automatic\nidentification of the linking behaviour in a scientific domain through the\nanalysis of the communication of the related academic institutions on the web.\nThe proposed approach is based on the paradigm of multiple viewpoint data\nanalysis (MVDA) than can be fruitfully exploited to highlight relationships\nbetween data, like websites, carrying several kinds of description. It uses the\nMultiSOM clustering and mapping method. The domain that has been chosen for\nthis study is the domain of Computer Science in Germany. The analysis is\nconduced on a set of 438 websites of this domain using all together, thematic,\ngeographic and linking information. It highlights interesting results\nconcerning both global and local linking behaviour."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0810.5428v2", 
    "title": "Relating Web pages to enable information-gathering tasks", 
    "arxiv-id": "0810.5428v2", 
    "author": "Garima Lahoti", 
    "publish": "2008-10-30T07:17:49Z", 
    "summary": "We argue that relationships between Web pages are functions of the user's\nintent. We identify a class of Web tasks - information-gathering - that can be\nfacilitated by a search engine that provides links to pages which are related\nto the page the user is currently viewing. We define three kinds of intentional\nrelationships that correspond to whether the user is a) seeking sources of\ninformation, b) reading pages which provide information, or c) surfing through\npages as part of an extended information-gathering process. We show that these\nthree relationships can be productively mined using a combination of textual\nand link information and provide three scoring mechanisms that correspond to\nthem: {\\em SeekRel}, {\\em FactRel} and {\\em SurfRel}. These scoring mechanisms\nincorporate both textual and link information. We build a set of capacitated\nsubnetworks - each corresponding to a particular keyword - that mirror the\ninterconnection structure of the World Wide Web. The scores are computed by\ncomputing flows on these subnetworks. The capacities of the links are derived\nfrom the {\\em hub} and {\\em authority} values of the nodes they connect,\nfollowing the work of Kleinberg (1998) on assigning authority to pages in\nhyperlinked environments. We evaluated our scoring mechanism by running\nexperiments on four data sets taken from the Web. We present user evaluations\nof the relevance of the top results returned by our scoring mechanisms and\ncompare those to the top results returned by Google's Similar Pages feature,\nand the {\\em Companion} algorithm proposed by Dean and Henzinger (1999)."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.0453v1", 
    "title": "CoZo+ - A Content Zoning Engine for textual documents", 
    "arxiv-id": "0811.0453v1", 
    "author": "Christoph Schommer", 
    "publish": "2008-11-04T09:08:32Z", 
    "summary": "Content zoning can be understood as a segmentation of textual documents into\nzones. This is inspired by [6] who initially proposed an approach for the\nargumentative zoning of textual documents. With the prototypical CoZo+ engine,\nwe focus on content zoning towards an automatic processing of textual streams\nwhile considering only the actors as the zones. We gain information that can be\nused to realize an automatic recognition of content for pre-defined actors. We\nunderstand CoZo+ as a necessary pre-step towards an automatic generation of\nsummaries and to make intellectual ownership of documents detectable."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.0719v1", 
    "title": "Web Usage Analysis: New Science Indicators and Co-usage", 
    "arxiv-id": "0811.0719v1", 
    "author": "Dominique Besagni", 
    "publish": "2008-11-05T13:00:52Z", 
    "summary": "A new type of statistical analysis of the science and technical information\n(STI) in the Web context is produced. We propose a set of indicators about Web\nusers, visualized bibliographic records, and e-commercial transactions. In\naddition, we introduce two Web usage factors. Finally, we give an overview of\nthe co-usage analysis. For these tasks, we introduce a computer based system,\ncalled Miri@d, which produces descriptive statistical information about the Web\nusers' searching behaviour, and what is effectively used from a free access\ndigital bibliographical database. The system is conceived as a server of\nstatistical data which are carried out beforehand, and as an interactive server\nfor online statistical work. The results will be made available to analysts,\nwho can use this descriptive statistical information as raw data for their\nindicator design tasks, and as input for multivariate data analysis, clustering\nanalysis, and mapping. Managers also can exploit the results in order to\nimprove management and decision-making."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.1250v1", 
    "title": "Adaptive Base Class Boost for Multi-class Classification", 
    "arxiv-id": "0811.1250v1", 
    "author": "Ping Li", 
    "publish": "2008-11-08T23:23:08Z", 
    "summary": "We develop the concept of ABC-Boost (Adaptive Base Class Boost) for\nmulti-class classification and present ABC-MART, a concrete implementation of\nABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has\nbeen very successful in large-scale applications. For binary classification,\nABC-MART recovers MART. For multi-class classification, ABC-MART considerably\nimproves MART, as evaluated on several public data sets."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.4186v1", 
    "title": "Search Result Clustering via Randomized Partitioning of Query-Induced   Subgraphs", 
    "arxiv-id": "0811.4186v1", 
    "author": "Aleksandar Bradic", 
    "publish": "2008-11-25T23:11:55Z", 
    "summary": "In this paper, we present an approach to search result clustering, using\npartitioning of underlying link graph. We define the notion of \"query-induced\nsubgraph\" and formulate the problem of search result clustering as a problem of\nefficient partitioning of given subgraph into topic-related clusters. Also, we\npropose a novel algorithm for approximative partitioning of such graph, which\nresults in cluster quality comparable to the one obtained by deterministic\nalgorithms, while operating in more efficient computation time, suitable for\npractical implementations. Finally, we present a practical clustering search\nengine developed as a part of this research and use it to get results about\nreal-world performance of proposed concepts."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.4603v2", 
    "title": "Frozen Footprints", 
    "arxiv-id": "0811.4603v2", 
    "author": "Massimo Franceschet", 
    "publish": "2008-11-27T18:12:28Z", 
    "summary": "Bibliometrics has the ambitious goal of measuring science. To this end, it\nexploits the way science is disseminated trough scientific publications and the\nresulting citation network of scientific papers. We survey the main historical\ncontributions to the field, the most interesting bibliometric indicators, and\nthe most popular bibliometric data sources. Moreover, we discuss distributions\ncommonly used to model bibliometric phenomena and give an overview of methods\nto build bibliometric maps of science."
},{
    "category": "cs.IR", 
    "doi": "10.1016/j.physa.2009.10.027", 
    "link": "http://arxiv.org/pdf/0811.4717v1", 
    "title": "Prospective Study for Semantic Inter-Media Fusion in Content-Based   Medical Image Retrieval", 
    "arxiv-id": "0811.4717v1", 
    "author": "Vladimir Cretu", 
    "publish": "2008-11-28T13:30:23Z", 
    "summary": "One important challenge in modern Content-Based Medical Image Retrieval\n(CBMIR) approaches is represented by the semantic gap, related to the\ncomplexity of the medical knowledge. Among the methods that are able to close\nthis gap in CBMIR, the use of medical thesauri/ontologies has interesting\nperspectives due to the possibility of accessing on-line updated relevant\nwebservices and to extract real-time medical semantic structured information.\nThe CBMIR approach proposed in this paper uses the Unified Medical Language\nSystem's (UMLS) Metathesaurus to perform a semantic indexing and fusion of\nmedical media. This fusion operates before the query processing (retrieval) and\nworks at an UMLS-compliant conceptual indexing level. Our purpose is to study\nvarious techniques related to semantic data alignment, preprocessing, fusion,\nclustering and retrieval, by evaluating the various techniques and highlighting\nfuture research directions. The alignment and the preprocessing are based on\npartial text/image retrieval feedback and on the data structure. We analyze\nvarious probabilistic, fuzzy and evidence-based approaches for the fusion\nprocess and different similarity functions for the retrieval process. All the\nproposed methods are evaluated on the Cross Language Evaluation Forum's (CLEF)\nmedical image retrieval benchmark, by focusing also on a more homogeneous\ncomponent medical image database: the Pathology Education Instructional\nResource (PEIR)."
}]