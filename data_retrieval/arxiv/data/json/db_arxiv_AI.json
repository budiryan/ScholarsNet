[{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9308101v1", 
    "title": "Dynamic Backtracking", 
    "arxiv-id": "cs/9308101v1", 
    "author": "M. L. Ginsberg", 
    "publish": "1993-08-01T00:00:00Z", 
    "summary": "Because of their occasional need to return to shallow points in a search\ntree, existing backtracking methods can sometimes erase meaningful progress\ntoward solving a search problem. In this paper, we present a method by which\nbacktrack points can be moved deeper in the search space, thereby avoiding this\ndifficulty. The technique developed is a variant of dependency-directed\nbacktracking that uses only polynomial space while still providing useful\ncontrol information and retaining the completeness guarantees provided by\nearlier approaches."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9308102v1", 
    "title": "A Market-Oriented Programming Environment and its Application to   Distributed Multicommodity Flow Problems", 
    "arxiv-id": "cs/9308102v1", 
    "author": "M. P. Wellman", 
    "publish": "1993-08-01T00:00:00Z", 
    "summary": "Market price systems constitute a well-understood class of mechanisms that\nunder certain conditions provide effective decentralization of decision making\nwith minimal communication overhead. In a market-oriented programming approach\nto distributed problem solving, we derive the activities and resource\nallocations for a set of computational agents by computing the competitive\nequilibrium of an artificial economy. WALRAS provides basic constructs for\ndefining computational market structures, and protocols for deriving their\ncorresponding price equilibria. In a particular realization of this approach\nfor a form of multicommodity flow problem, we see that careful construction of\nthe decision process according to economic principles can lead to efficient\ndistributed resource allocation, and that the behavior of the system can be\nmeaningfully analyzed in economic terms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9309101v1", 
    "title": "An Empirical Analysis of Search in GSAT", 
    "arxiv-id": "cs/9309101v1", 
    "author": "T. Walsh", 
    "publish": "1993-09-01T00:00:00Z", 
    "summary": "We describe an extensive study of search in GSAT, an approximation procedure\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\nmore complete picture of GSAT's search than previous accounts. We describe in\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\nthere is a very simple scaling with problem size for both the mean number of\nsatisfied clauses and the mean branching rate. Our results allow us to make\ndetailed numerical conjectures about the length of the hill-climbing phase, the\naverage gradient of this phase, and to conjecture that both the average score\nand average branching rate decay exponentially during plateau search. We end by\nshowing how these results can be used to direct future theoretical analysis.\nThis work provides a case study of how computer experiments can be used to\nimprove understanding of the theoretical properties of algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9311101v1", 
    "title": "The Difficulties of Learning Logic Programs with Cut", 
    "arxiv-id": "cs/9311101v1", 
    "author": "U. Trinchero", 
    "publish": "1993-11-01T00:00:00Z", 
    "summary": "As real logic programmers normally use cut (!), an effective learning\nprocedure for logic programs should be able to deal with it. Because the cut\npredicate has only a procedural meaning, clauses containing cut cannot be\nlearned using an extensional evaluation method, as is done in most learning\nsystems. On the other hand, searching a space of possible programs (instead of\na space of independent clauses) is unfeasible. An alternative solution is to\ngenerate first a candidate base program which covers the positive examples, and\nthen make it consistent by inserting cut where appropriate. The problem of\nlearning programs with cut has not been investigated before and this seems to\nbe a natural and reasonable approach. We generalize this scheme and investigate\nthe difficulties that arise. Some of the major shortcomings are actually\ncaused, in general, by the need for intensional evaluation. As a conclusion,\nthe analysis of this paper suggests, on precise and technical grounds, that\nlearning cut is difficult, and current induction techniques should probably be\nrestricted to purely declarative logic languages."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9311102v1", 
    "title": "Software Agents: Completing Patterns and Constructing User Interfaces", 
    "arxiv-id": "cs/9311102v1", 
    "author": "L. A. Hermens", 
    "publish": "1993-11-01T00:00:00Z", 
    "summary": "To support the goal of allowing users to record and retrieve information,\nthis paper describes an interactive note-taking system for pen-based computers\nwith two distinctive features. First, it actively predicts what the user is\ngoing to write. Second, it automatically constructs a custom, button-box user\ninterface on request. The system is an example of a learning-apprentice\nsoftware- agent. A machine learning component characterizes the syntax and\nsemantics of the user's information. A performance system uses this learned\ninformation to generate completion strings and construct a user interface.\nDescription of Online Appendix: People like to record information. Doing this\non paper is initially efficient, but lacks flexibility. Recording information\non a computer is less efficient but more powerful. In our new note taking\nsoftwre, the user records information directly on a computer. Behind the\ninterface, an agent acts for the user. To help, it provides defaults and\nconstructs a custom user interface. The demonstration is a QuickTime movie of\nthe note taking agent in action. The file is a binhexed self-extracting\narchive. Macintosh utilities for binhex are available from\nmac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\ndts/mac/sys.soft/quicktime."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9312101v1", 
    "title": "Decidable Reasoning in Terminological Knowledge Representation Systems", 
    "arxiv-id": "cs/9312101v1", 
    "author": "A. Schaerf", 
    "publish": "1993-12-01T00:00:00Z", 
    "summary": "Terminological knowledge representation systems (TKRSs) are tools for\ndesigning and using knowledge bases that make use of terminological languages\n(or concept languages). We analyze from a theoretical point of view a TKRS\nwhose capabilities go beyond the ones of presently available TKRSs. The new\nfeatures studied, often required in practical applications, can be summarized\nin three main points. First, we consider a highly expressive terminological\nlanguage, called ALCNR, including general complements of concepts, number\nrestrictions and role conjunction. Second, we allow to express inclusion\nstatements between general concepts, and terminological cycles as a particular\ncase. Third, we prove the decidability of a number of desirable TKRS-deduction\nservices (like satisfiability, subsumption and instance checking) through a\nsound, complete and terminating calculus for reasoning in ALCNR-knowledge\nbases. Our calculus extends the general technique of constraint systems. As a\nbyproduct of the proof, we get also the result that inclusion statements in\nALCNR can be simulated by terminological cycles, if descriptive semantics is\nadopted."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9401101v1", 
    "title": "Teleo-Reactive Programs for Agent Control", 
    "arxiv-id": "cs/9401101v1", 
    "author": "N. Nilsson", 
    "publish": "1994-01-01T00:00:00Z", 
    "summary": "A formalism is presented for computing and organizing actions for autonomous\nagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)\nprograms whose execution entails the construction of circuitry for the\ncontinuous computation of the parameters and conditions on which agent action\nis based. In addition to continuous feedback, T-R programs support parameter\nbinding and recursion. A primary difference between T-R programs and many other\ncircuit-based systems is that the circuitry of T-R programs is more compact; it\nis constructed at run time and thus does not have to anticipate all the\ncontingencies that might arise over all possible runs. In addition, T-R\nprograms are intuitive and easy to write and are written in a form that is\ncompatible with automatic planning and learning methods. We briefly describe\nsome experimental applications of T-R programs in the control of simulated and\nactual mobile robots."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9402101v1", 
    "title": "Learning the Past Tense of English Verbs: The Symbolic Pattern   Associator vs. Connectionist Models", 
    "arxiv-id": "cs/9402101v1", 
    "author": "C. X. Ling", 
    "publish": "1994-02-01T00:00:00Z", 
    "summary": "Learning the past tense of English verbs - a seemingly minor aspect of\nlanguage acquisition - has generated heated debates since 1986, and has become\na landmark task for testing the adequacy of cognitive modeling. Several\nartificial neural networks (ANNs) have been implemented, and a challenge for\nbetter symbolic models has been posed. In this paper, we present a\ngeneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree\nlearning algorithm ID3. We conduct extensive head-to-head comparisons on the\ngeneralization ability between ANN models and the SPA under different\nrepresentations. We conclude that the SPA generalizes the past tense of unseen\nverbs better than ANN models by a wide margin, and we offer insights as to why\nthis should be the case. We also discuss a new default strategy for\ndecision-tree learning algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9402102v1", 
    "title": "Substructure Discovery Using Minimum Description Length and Background   Knowledge", 
    "arxiv-id": "cs/9402102v1", 
    "author": "L. B. Holder", 
    "publish": "1994-02-01T00:00:00Z", 
    "summary": "The ability to identify interesting and repetitive substructures is an\nessential component to discovering knowledge in structural data. We describe a\nnew version of our SUBDUE substructure discovery system based on the minimum\ndescription length principle. The SUBDUE system discovers substructures that\ncompress the original data and represent structural concepts in the data. By\nreplacing previously-discovered substructures in the data, multiple passes of\nSUBDUE produce a hierarchical description of the structural regularities in the\ndata. SUBDUE uses a computationally-bounded inexact graph match that identifies\nsimilar, but not identical, instances of a substructure and finds an\napproximate measure of closeness of two substructures when under computational\nconstraints. In addition to the minimum description length principle, other\nbackground knowledge can be used by SUBDUE to guide the search towards more\nappropriate substructures. Experiments in a variety of domains demonstrate\nSUBDUE's ability to find substructures capable of compressing the original data\nand to discover structural concepts important to the domain. Description of\nOnline Appendix: This is a compressed tar file containing the SUBDUE discovery\nsystem, written in C. The program accepts as input databases represented in\ngraph form, and will output discovered substructures with their corresponding\nvalue."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9402103v1", 
    "title": "Bias-Driven Revision of Logical Domain Theories", 
    "arxiv-id": "cs/9402103v1", 
    "author": "A. M. Segre", 
    "publish": "1994-02-01T00:00:00Z", 
    "summary": "The theory revision problem is the problem of how best to go about revising a\ndeficient domain theory using information contained in examples that expose\ninaccuracies. In this paper we present our approach to the theory revision\nproblem for propositional domain theories. The approach described here, called\nPTR, uses probabilities associated with domain theory elements to numerically\ntrack the ``flow'' of proof through the theory. This allows us to measure the\nprecise role of a clause or literal in allowing or preventing a (desired or\nundesired) derivation for a given example. This information is used to\nefficiently locate and repair flawed elements of the theory. PTR is proved to\nconverge to a theory which correctly classifies all examples, and shown\nexperimentally to be fast and accurate even for deep theories."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9403101v1", 
    "title": "Exploring the Decision Forest: An Empirical Investigation of Occam's   Razor in Decision Tree Induction", 
    "arxiv-id": "cs/9403101v1", 
    "author": "M. J. Pazzani", 
    "publish": "1994-03-01T00:00:00Z", 
    "summary": "We report on a series of experiments in which all decision trees consistent\nwith the training data are constructed. These experiments were run to gain an\nunderstanding of the properties of the set of consistent decision trees and the\nfactors that affect the accuracy of individual trees. In particular, we\ninvestigated the relationship between the size of a decision tree consistent\nwith some training data and the accuracy of the tree on test data. The\nexperiments were performed on a massively parallel Maspar computer. The results\nof the experiments on several artificial and two real world problems indicate\nthat, for many of the problems investigated, smaller consistent decision trees\nare on average less accurate than the average accuracy of slightly larger\ntrees."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9406101v1", 
    "title": "A Semantics and Complete Algorithm for Subsumption in the CLASSIC   Description Logic", 
    "arxiv-id": "cs/9406101v1", 
    "author": "P. F. Patel-Schneider", 
    "publish": "1994-06-01T00:00:00Z", 
    "summary": "This paper analyzes the correctness of the subsumption algorithm used in\nCLASSIC, a description logic-based knowledge representation system that is\nbeing used in practical applications. In order to deal efficiently with\nindividuals in CLASSIC descriptions, the developers have had to use an\nalgorithm that is incomplete with respect to the standard, model-theoretic\nsemantics for description logics. We provide a variant semantics for\ndescriptions with respect to which the current implementation is complete, and\nwhich can be independently motivated. The soundness and completeness of the\npolynomial-time subsumption algorithm is established using description graphs,\nwhich are an abstracted version of the implementation structures used in\nCLASSIC, and are of independent interest."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9406102v1", 
    "title": "Applying GSAT to Non-Clausal Formulas", 
    "arxiv-id": "cs/9406102v1", 
    "author": "R. Sebastiani", 
    "publish": "1994-06-01T00:00:00Z", 
    "summary": "In this paper we describe how to modify GSAT so that it can be applied to\nnon-clausal formulas. The idea is to use a particular ``score'' function which\ngives the number of clauses of the CNF conversion of a formula which are false\nunder a given truth assignment. Its value is computed in linear time, without\nconstructing the CNF conversion itself. The proposed methodology applies to\nmost of the variants of GSAT proposed so far."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9408101v1", 
    "title": "Random Worlds and Maximum Entropy", 
    "arxiv-id": "cs/9408101v1", 
    "author": "D. Koller", 
    "publish": "1994-08-01T00:00:00Z", 
    "summary": "Given a knowledge base KB containing first-order and statistical facts, we\nconsider a principled method, called the random-worlds method, for computing a\ndegree of belief that some formula Phi holds given KB. If we are reasoning\nabout a world or system consisting of N individuals, then we can consider all\npossible worlds, or first-order models, with domain {1,...,N} that satisfy KB,\nand compute the fraction of them in which Phi is true. We define the degree of\nbelief to be the asymptotic value of this fraction as N grows large. We show\nthat when the vocabulary underlying Phi and KB uses constants and unary\npredicates only, we can naturally associate an entropy with each world. As N\ngrows larger, there are many more worlds with higher entropy. Therefore, we can\nuse a maximum-entropy computation to compute the degree of belief. This result\nis in a similar spirit to previous work in physics and artificial intelligence,\nbut is far more general. Of equal interest to the result itself are the\nlimitations on its scope. Most importantly, the restriction to unary predicates\nseems necessary. Although the random-worlds method makes sense in general, the\nconnection to maximum entropy seems to disappear in the non-unary case. These\nobservations suggest unexpected limitations to the applicability of\nmaximum-entropy methods."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9408102v1", 
    "title": "Pattern Matching and Discourse Processing in Information Extraction from   Japanese Text", 
    "arxiv-id": "cs/9408102v1", 
    "author": "M. Hara", 
    "publish": "1994-08-01T00:00:00Z", 
    "summary": "Information extraction is the task of automatically picking up information of\ninterest from an unconstrained text. Information of interest is usually\nextracted in two steps. First, sentence level processing locates relevant\npieces of information scattered throughout the text; second, discourse\nprocessing merges coreferential information to generate the output. In the\nfirst step, pieces of information are locally identified without recognizing\nany relationships among them. A key word search or simple pattern search can\nachieve this purpose. The second step requires deeper knowledge in order to\nunderstand relationships among separately identified pieces of information.\nPrevious information extraction systems focused on the first step, partly\nbecause they were not required to link up each piece of information with other\npieces. To link the extracted pieces of information and map them onto a\nstructured output format, complex discourse processing is essential. This paper\nreports on a Japanese information extraction system that merges information\nusing a pattern matcher and discourse processor. Evaluation results show a high\nlevel of system performance which approaches human performance."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9408103v1", 
    "title": "A System for Induction of Oblique Decision Trees", 
    "arxiv-id": "cs/9408103v1", 
    "author": "S. Salzberg", 
    "publish": "1994-08-01T00:00:00Z", 
    "summary": "This article describes a new system for induction of oblique decision trees.\nThis system, OC1, combines deterministic hill-climbing with two forms of\nrandomization to find a good oblique split (in the form of a hyperplane) at\neach node of a decision tree. Oblique decision tree methods are tuned\nespecially for domains in which the attributes are numeric, although they can\nbe adapted to symbolic or mixed symbolic/numeric attributes. We present\nextensive empirical studies, using both real and artificial data, that analyze\nOC1's ability to construct oblique trees that are smaller and more accurate\nthan their axis-parallel counterparts. We also examine the benefits of\nrandomization for the construction of oblique decision trees."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9409101v1", 
    "title": "On Planning while Learning", 
    "arxiv-id": "cs/9409101v1", 
    "author": "M. Tennenholtz", 
    "publish": "1994-09-01T00:00:00Z", 
    "summary": "This paper introduces a framework for Planning while Learning where an agent\nis given a goal to achieve in an environment whose behavior is only partially\nknown to the agent. We discuss the tractability of various plan-design\nprocesses. We show that for a large natural class of Planning while Learning\nsystems, a plan can be presented and verified in a reasonable time. However,\ncoming up algorithmically with a plan, even for simple classes of systems is\napparently intractable. We emphasize the role of off-line plan-design\nprocesses, and show that, in most natural cases, the verification (projection)\npart can be carried out in an efficient algorithmic manner."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9412101v1", 
    "title": "Wrap-Up: a Trainable Discourse Module for Information Extraction", 
    "arxiv-id": "cs/9412101v1", 
    "author": "Lehnert. W", 
    "publish": "1994-12-01T00:00:00Z", 
    "summary": "The vast amounts of on-line text now available have led to renewed interest\nin information extraction (IE) systems that analyze unrestricted text,\nproducing a structured representation of selected information from the text.\nThis paper presents a novel approach that uses machine learning to acquire\nknowledge for some of the higher level IE processing. Wrap-Up is a trainable IE\ndiscourse component that makes intersentential inferences and identifies\nlogical relations among information extracted from the text. Previous\ncorpus-based approaches were limited to lower level processing such as\npart-of-speech tagging, lexical disambiguation, and dictionary construction.\nWrap-Up is fully trainable, and not only automatically decides what classifiers\nare needed, but even derives the feature set for each classifier automatically.\nPerformance equals that of a partially trainable discourse module requiring\nmanual customization for each domain."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9412102v1", 
    "title": "Operations for Learning with Graphical Models", 
    "arxiv-id": "cs/9412102v1", 
    "author": "W. L. Buntine", 
    "publish": "1994-12-01T00:00:00Z", 
    "summary": "This paper is a multidisciplinary review of empirical, statistical learning\nfrom a graphical model perspective. Well-known examples of graphical models\ninclude Bayesian networks, directed graphs representing a Markov chain, and\nundirected networks representing a Markov field. These graphical models are\nextended to model data analysis and empirical learning using the notation of\nplates. Graphical operations for simplifying and manipulating a problem are\nprovided including decomposition, differentiation, and the manipulation of\nprobability models from the exponential family. Two standard algorithm schemas\nfor learning are reviewed in a graphical framework: Gibbs sampling and the\nexpectation maximization algorithm. Using these operations and schemas, some\npopular algorithms can be synthesized from their graphical specification. This\nincludes versions of linear regression, techniques for feed-forward networks,\nand learning Gaussian and discrete Bayesian networks from data. The paper\nconcludes by sketching some implications for data analysis and summarizing how\nsome popular algorithms fall within the framework presented. The main original\ncontributions here are the decomposition techniques and the demonstration that\ngraphical models provide a framework for understanding and developing complex\nlearning algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9412103v1", 
    "title": "Total-Order and Partial-Order Planning: A Comparative Analysis", 
    "arxiv-id": "cs/9412103v1", 
    "author": "M. Drummond", 
    "publish": "1994-12-01T00:00:00Z", 
    "summary": "For many years, the intuitions underlying partial-order planning were largely\ntaken for granted. Only in the past few years has there been renewed interest\nin the fundamental principles underlying this paradigm. In this paper, we\npresent a rigorous comparative analysis of partial-order and total-order\nplanning by focusing on two specific planners that can be directly compared. We\nshow that there are some subtle assumptions that underly the wide-spread\nintuitions regarding the supposed efficiency of partial-order planning. For\ninstance, the superiority of partial-order planning can depend critically upon\nthe search strategy and the structure of the search space. Understanding the\nunderlying assumptions is crucial for constructing efficient planners."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9501101v1", 
    "title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes", 
    "arxiv-id": "cs/9501101v1", 
    "author": "G. Bakiri", 
    "publish": "1995-01-01T00:00:00Z", 
    "summary": "Multiclass learning problems involve finding a definition for an unknown\nfunction f(x) whose range is a discrete set containing k &gt 2 values (i.e., k\n``classes''). The definition is acquired by studying collections of training\nexamples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning\nproblems include direct application of multiclass algorithms such as the\ndecision-tree algorithms C4.5 and CART, application of binary concept learning\nalgorithms to learn individual binary functions for each of the k classes, and\napplication of binary concept learning algorithms with distributed output\nrepresentations. This paper compares these three approaches to a new technique\nin which error-correcting codes are employed as a distributed output\nrepresentation. We show that these output representations improve the\ngeneralization performance of both C4.5 and backpropagation on a wide range of\nmulticlass learning tasks. We also demonstrate that this approach is robust\nwith respect to changes in the size of the training sample, the assignment of\ndistributed representations to particular classes, and the application of\noverfitting avoidance techniques such as decision-tree pruning. Finally, we\nshow that---like the other methods---the error-correcting code technique can\nprovide reliable class probability estimates. Taken together, these results\ndemonstrate that error-correcting output codes provide a general-purpose method\nfor improving the performance of inductive learning programs on multiclass\nproblems."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9501102v1", 
    "title": "A Domain-Independent Algorithm for Plan Adaptation", 
    "arxiv-id": "cs/9501102v1", 
    "author": "D. S. Weld", 
    "publish": "1995-01-01T00:00:00Z", 
    "summary": "The paradigms of transformational planning, case-based planning, and plan\ndebugging all involve a process known as plan adaptation - modifying or\nrepairing an old plan so it solves a new problem. In this paper we provide a\ndomain-independent algorithm for plan adaptation, demonstrate that it is sound,\ncomplete, and systematic, and compare it to other adaptation algorithms in the\nliterature. Our approach is based on a view of planning as searching a graph of\npartial plans. Generative planning starts at the graph's root and moves from\nnode to node using plan-refinement operators. In planning by adaptation, a\nlibrary plan - an arbitrary node in the plan graph - is the starting point for\nthe search, and the plan-adaptation algorithm can apply both the same\nrefinement operators available to a generative planner and can also retract\nconstraints and steps from the plan. Our algorithm's completeness ensures that\nthe adaptation algorithm will eventually search the entire graph and its\nsystematicity ensures that it will do so without redundantly searching any\nparts of the graph."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9501103v1", 
    "title": "Truncating Temporal Differences: On the Efficient Implementation of   TD(lambda) for Reinforcement Learning", 
    "arxiv-id": "cs/9501103v1", 
    "author": "P. Cichosz", 
    "publish": "1995-01-01T00:00:00Z", 
    "summary": "Temporal difference (TD) methods constitute a class of methods for learning\npredictions in multi-step prediction problems, parameterized by a recency\nfactor lambda. Currently the most important application of these methods is to\ntemporal credit assignment in reinforcement learning. Well known reinforcement\nlearning algorithms, such as AHC or Q-learning, may be viewed as instances of\nTD learning. This paper examines the issues of the efficient and general\nimplementation of TD(lambda) for arbitrary lambda, for use with reinforcement\nlearning algorithms optimizing the discounted sum of rewards. The traditional\napproach, based on eligibility traces, is argued to suffer from both\ninefficiency and lack of generality. The TTD (Truncated Temporal Differences)\nprocedure is proposed as an alternative, that indeed only approximates\nTD(lambda), but requires very little computation per action and can be used\nwith arbitrary function representation methods. The idea from which it is\nderived is fairly simple and not new, but probably unexplored so far.\nEncouraging experimental results are presented, suggesting that using lambda\n&gt 0 with the TTD procedure allows one to obtain a significant learning\nspeedup at essentially the same cost as usual TD(0) learning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9503102v1", 
    "title": "Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic   Decision Tree Induction Algorithm", 
    "arxiv-id": "cs/9503102v1", 
    "author": "P. D. Turney", 
    "publish": "1995-03-01T00:00:00Z", 
    "summary": "This paper introduces ICET, a new algorithm for cost-sensitive\nclassification. ICET uses a genetic algorithm to evolve a population of biases\nfor a decision tree induction algorithm. The fitness function of the genetic\nalgorithm is the average cost of classification when using the decision tree,\nincluding both the costs of tests (features, measurements) and the costs of\nclassification errors. ICET is compared here with three other algorithms for\ncost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,\nwhich classifies without regard to cost. The five algorithms are evaluated\nempirically on five real-world medical datasets. Three sets of experiments are\nperformed. The first set examines the baseline performance of the five\nalgorithms on the five datasets and establishes that ICET performs\nsignificantly better than its competitors. The second set tests the robustness\nof ICET under a variety of conditions and shows that ICET maintains its\nadvantage. The third set looks at ICET's search in bias space and discovers a\nway to improve the search."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9504101v1", 
    "title": "Rerepresenting and Restructuring Domain Theories: A Constructive   Induction Approach", 
    "arxiv-id": "cs/9504101v1", 
    "author": "L. A. Rendell", 
    "publish": "1995-04-01T00:00:00Z", 
    "summary": "Theory revision integrates inductive learning and background knowledge by\ncombining training examples with a coarse domain theory to produce a more\naccurate theory. There are two challenges that theory revision and other\ntheory-guided systems face. First, a representation language appropriate for\nthe initial theory may be inappropriate for an improved theory. While the\noriginal representation may concisely express the initial theory, a more\naccurate theory forced to use that same representation may be bulky,\ncumbersome, and difficult to reach. Second, a theory structure suitable for a\ncoarse domain theory may be insufficient for a fine-tuned theory. Systems that\nproduce only small, local changes to a theory have limited value for\naccomplishing complex structural alterations that may be required.\nConsequently, advanced theory-guided learning systems require flexible\nrepresentation and flexible structure. An analysis of various theory revision\nsystems and theory-guided learning systems reveals specific strengths and\nweaknesses in terms of these two desired properties. Designed to capture the\nunderlying qualities of each system, a new system uses theory-guided\nconstructive induction. Experiments in three domains show improvement over\nprevious theory-guided systems. This leads to a study of the behavior,\nlimitations, and potential of theory-guided constructive induction."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9505101v1", 
    "title": "Using Pivot Consistency to Decompose and Solve Functional CSPs", 
    "arxiv-id": "cs/9505101v1", 
    "author": "P. David", 
    "publish": "1995-05-01T00:00:00Z", 
    "summary": "Many studies have been carried out in order to increase the search efficiency\nof constraint satisfaction problems; among them, some make use of structural\nproperties of the constraint network; others take into account semantic\nproperties of the constraints, generally assuming that all the constraints\npossess the given property. In this paper, we propose a new decomposition\nmethod benefiting from both semantic properties of functional constraints (not\nbijective constraints) and structural properties of the network; furthermore,\nnot all the constraints need to be functional. We show that under some\nconditions, the existence of solutions can be guaranteed. We first characterize\na particular subset of the variables, which we name a root set. We then\nintroduce pivot consistency, a new local consistency which is a weak form of\npath consistency and can be achieved in O(n^2d^2) complexity (instead of\nO(n^3d^3) for path consistency), and we present associated properties; in\nparticular, we show that any consistent instantiation of the root set can be\nlinearly extended to a solution, which leads to the presentation of the\naforementioned new method for solving by decomposing functional CSPs."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9505102v1", 
    "title": "Adaptive Load Balancing: A Study in Multi-Agent Learning", 
    "arxiv-id": "cs/9505102v1", 
    "author": "M. Tennenholtz", 
    "publish": "1995-05-01T00:00:00Z", 
    "summary": "We study the process of multi-agent reinforcement learning in the context of\nload balancing in a distributed system, without use of either central\ncoordination or explicit communication. We first define a precise framework in\nwhich to study adaptive load balancing, important features of which are its\nstochastic nature and the purely local information available to individual\nagents. Given this framework, we show illuminating results on the interplay\nbetween basic adaptive behavior parameters and their effect on system\nefficiency. We then investigate the properties of adaptive load balancing in\nheterogeneous populations, and address the issue of exploration vs.\nexploitation in that context. Finally, we show that naive use of communication\nmay not improve, and might even harm system efficiency."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9505103v1", 
    "title": "Provably Bounded-Optimal Agents", 
    "arxiv-id": "cs/9505103v1", 
    "author": "D. Subramanian", 
    "publish": "1995-05-01T00:00:00Z", 
    "summary": "Since its inception, artificial intelligence has relied upon a theoretical\nfoundation centered around perfect rationality as the desired property of\nintelligent systems. We argue, as others have done, that this foundation is\ninadequate because it imposes fundamentally unsatisfiable requirements. As a\nresult, there has arisen a wide gap between theory and practice in AI,\nhindering progress in the field. We propose instead a property called bounded\noptimality. Roughly speaking, an agent is bounded-optimal if its program is a\nsolution to the constrained optimization problem presented by its architecture\nand the task environment. We show how to construct agents with this property\nfor a simple class of machine architectures in a broad class of real-time\nenvironments. We illustrate these results using a simple model of an automated\nmail sorting facility. We also define a weaker property, asymptotic bounded\noptimality (ABO), that generalizes the notion of optimality in classical\ncomplexity theory. We then construct universal ABO programs, i.e., programs\nthat are ABO no matter what real-time constraints are applied. Universal ABO\nprograms can be used as building blocks for more complex systems. We conclude\nwith a discussion of the prospects for bounded optimality as a theoretical\nbasis for AI, and relate it to similar trends in philosophy, economics, and\ngame theory."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9505104v1", 
    "title": "Pac-Learning Recursive Logic Programs: Efficient Algorithms", 
    "arxiv-id": "cs/9505104v1", 
    "author": "W. W. Cohen", 
    "publish": "1995-05-01T00:00:00Z", 
    "summary": "We present algorithms that learn certain classes of function-free recursive\nlogic programs in polynomial time from equivalence queries. In particular, we\nshow that a single k-ary recursive constant-depth determinate clause is\nlearnable. Two-clause programs consisting of one learnable recursive clause and\none constant-depth determinate non-recursive clause are also learnable, if an\nadditional ``basecase'' oracle is assumed. These results immediately imply the\npac-learnability of these classes. Although these classes of learnable\nrecursive programs are very constrained, it is shown in a companion paper that\nthey are maximally general, in that generalizing either class in any natural\nway leads to a computationally difficult learning problem. Thus, taken together\nwith its companion paper, this paper establishes a boundary of efficient\nlearnability for recursive logic programs."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9505105v1", 
    "title": "Pac-learning Recursive Logic Programs: Negative Results", 
    "arxiv-id": "cs/9505105v1", 
    "author": "W. W. Cohen", 
    "publish": "1995-05-01T00:00:00Z", 
    "summary": "In a companion paper it was shown that the class of constant-depth\ndeterminate k-ary recursive clauses is efficiently learnable. In this paper we\npresent negative results showing that any natural generalization of this class\nis hard to learn in Valiant's model of pac-learnability. In particular, we show\nthat the following program classes are cryptographically hard to learn:\nprograms with an unbounded number of constant-depth linear recursive clauses;\nprograms with one constant-depth determinate clause containing an unbounded\nnumber of recursive calls; and programs with one linear recursive clause of\nconstant locality. These results immediately imply the non-learnability of any\nmore general class of programs. We also show that learning a constant-depth\ndeterminate program with either two linear recursive clauses or one linear\nrecursive clause and one non-recursive clause is as hard as learning boolean\nDNF. Together with positive results from the companion paper, these negative\nresults establish a boundary of efficient learnability for recursive\nfunction-free clauses."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9506101v1", 
    "title": "FLECS: Planning with a Flexible Commitment Strategy", 
    "arxiv-id": "cs/9506101v1", 
    "author": "P. Stone", 
    "publish": "1995-06-01T00:00:00Z", 
    "summary": "There has been evidence that least-commitment planners can efficiently handle\nplanning problems that involve difficult goal interactions. This evidence has\nled to the common belief that delayed-commitment is the \"best\" possible\nplanning strategy. However, we recently found evidence that eager-commitment\nplanners can handle a variety of planning problems more efficiently, in\nparticular those with difficult operator choices. Resigned to the futility of\ntrying to find a universally successful planning strategy, we devised a planner\nthat can be used to study which domains and problems are best for which\nplanning strategies. In this article we introduce this new planning algorithm,\nFLECS, which uses a FLExible Commitment Strategy with respect to plan-step\norderings. It is able to use any strategy from delayed-commitment to\neager-commitment. The combination of delayed and eager operator-ordering\ncommitments allows FLECS to take advantage of the benefits of explicitly using\na simulated execution state and reasoning about planning constraints. FLECS can\nvary its commitment strategy across different problems and domains, and also\nduring the course of a single planning problem. FLECS represents a novel\ncontribution to planning in that it explicitly provides the choice of which\ncommitment strategy to use while planning. FLECS provides a framework to\ninvestigate the mapping from planning domains and problems to efficient\nplanning strategies."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9506102v1", 
    "title": "Induction of First-Order Decision Lists: Results on Learning the Past   Tense of English Verbs", 
    "arxiv-id": "cs/9506102v1", 
    "author": "M. E. Califf", 
    "publish": "1995-06-01T00:00:00Z", 
    "summary": "This paper presents a method for inducing logic programs from examples that\nlearns a new class of concepts called first-order decision lists, defined as\nordered lists of clauses each ending in a cut. The method, called FOIDL, is\nbased on FOIL (Quinlan, 1990) but employs intensional background knowledge and\navoids the need for explicit negative examples. It is particularly useful for\nproblems that involve rules with specific exceptions, such as learning the\npast-tense of English verbs, a task widely studied in the context of the\nsymbolic/connectionist debate. FOIDL is able to learn concise, accurate\nprograms for this problem from significantly fewer examples than previous\nmethods (both connectionist and symbolic)."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9507101v1", 
    "title": "Building and Refining Abstract Planning Cases by Change of   Representation Language", 
    "arxiv-id": "cs/9507101v1", 
    "author": "W. Wilke", 
    "publish": "1995-07-01T00:00:00Z", 
    "summary": "ion is one of the most promising approaches to improve the performance of\nproblem solvers. In several domains abstraction by dropping sentences of a\ndomain description -- as used in most hierarchical planners -- has proven\nuseful. In this paper we present examples which illustrate significant\ndrawbacks of abstraction by dropping sentences. To overcome these drawbacks, we\npropose a more general view of abstraction involving the change of\nrepresentation language. We have developed a new abstraction methodology and a\nrelated sound and complete learning algorithm that allows the complete change\nof representation language of planning cases from concrete to abstract.\nHowever, to achieve a powerful change of the representation language, the\nabstract language itself as well as rules which describe admissible ways of\nabstracting states must be provided in the domain model. This new abstraction\napproach is the core of Paris (Plan Abstraction and Refinement in an Integrated\nSystem), a system in which abstract planning cases are automatically learned\nfrom given concrete cases. An empirical study in the domain of process planning\nin mechanical engineering shows significant advantages of the proposed\nreasoning from abstract cases over classical hierarchical planning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9508101v1", 
    "title": "Using Qualitative Hypotheses to Identify Inaccurate Data", 
    "arxiv-id": "cs/9508101v1", 
    "author": "T. Nishida", 
    "publish": "1995-08-01T00:00:00Z", 
    "summary": "Identifying inaccurate data has long been regarded as a significant and\ndifficult problem in AI. In this paper, we present a new method for identifying\ninaccurate data on the basis of qualitative correlations among related data.\nFirst, we introduce the definitions of related data and qualitative\ncorrelations among related data. Then we put forward a new concept called\nsupport coefficient function (SCF). SCF can be used to extract, represent, and\ncalculate qualitative correlations among related data within a dataset. We\npropose an approach to determining dynamic shift intervals of inaccurate data,\nand an approach to calculating possibility of identifying inaccurate data,\nrespectively. Both of the approaches are based on SCF. Finally we present an\nalgorithm for identifying inaccurate data by using qualitative correlations\namong related data as confirmatory or disconfirmatory evidence. We have\ndeveloped a practical system for interpreting infrared spectra by applying the\nmethod, and have fully tested the system against several hundred real spectra.\nThe experimental results show that the method is significantly better than the\nconventional methods used in many similar systems."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9508102v1", 
    "title": "An Integrated Framework for Learning and Reasoning", 
    "arxiv-id": "cs/9508102v1", 
    "author": "T. R. Martinez", 
    "publish": "1995-08-01T00:00:00Z", 
    "summary": "Learning and reasoning are both aspects of what is considered to be\nintelligence. Their studies within AI have been separated historically,\nlearning being the topic of machine learning and neural networks, and reasoning\nfalling under classical (or symbolic) AI. However, learning and reasoning are\nin many ways interdependent. This paper discusses the nature of some of these\ninterdependencies and proposes a general framework called FLARE, that combines\ninductive learning using prior knowledge together with reasoning in a\npropositional setting. Several examples that test the framework are presented,\nincluding classical induction, many important reasoning protocols and two\nsimple expert systems."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9510101v1", 
    "title": "Diffusion of Context and Credit Information in Markovian Models", 
    "arxiv-id": "cs/9510101v1", 
    "author": "P. Frasconi", 
    "publish": "1995-10-01T00:00:00Z", 
    "summary": "This paper studies the problem of ergodicity of transition probability\nmatrices in Markovian models, such as hidden Markov models (HMMs), and how it\nmakes very difficult the task of learning to represent long-term context for\nsequential data. This phenomenon hurts the forward propagation of long-term\ncontext information, as well as learning a hidden state representation to\nrepresent long-term context, which depends on propagating credit information\nbackwards in time. Using results from Markov chain theory, we show that this\nproblem of diffusion of context and credit is reduced when the transition\nprobabilities approach 0 or 1, i.e., the transition probability matrices are\nsparse and the model essentially deterministic. The results found in this paper\napply to learning approaches based on continuous optimization, such as gradient\ndescent and the Baum-Welch algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9510102v1", 
    "title": "Improving Connectionist Energy Minimization", 
    "arxiv-id": "cs/9510102v1", 
    "author": "R. Dechter", 
    "publish": "1995-10-01T00:00:00Z", 
    "summary": "Symmetric networks designed for energy minimization such as Boltzman machines\nand Hopfield nets are frequently investigated for use in optimization,\nconstraint satisfaction and approximation of NP-hard problems. Nevertheless,\nfinding a global solution (i.e., a global minimum for the energy function) is\nnot guaranteed and even a local solution may take an exponential number of\nsteps. We propose an improvement to the standard local activation function used\nfor such networks. The improved algorithm guarantees that a global minimum is\nfound in linear time for tree-like subnetworks. The algorithm, called activate,\nis uniform and does not assume that the network is tree-like. It can identify\ntree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid\nlocal minima along these trees. For acyclic networks, the algorithm is\nguaranteed to converge to a global minimum from any initial state of the system\n(self-stabilization) and remains correct under various types of schedulers. On\nthe negative side, we show that in the presence of cycles, no uniform algorithm\nexists that guarantees optimality even under a sequential asynchronous\nscheduler. An asynchronous scheduler can activate only one unit at a time while\na synchronous scheduler can activate any number of units in a single time step.\nIn addition, no uniform algorithm exists to optimize even acyclic networks when\nthe scheduler is synchronous. Finally, we show how the algorithm can be\nimproved using the cycle-cutset scheme. The general algorithm, called\nactivate-with-cutset, improves over activate and has some performance\nguarantees that are related to the size of the network's cycle-cutset."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9510103v1", 
    "title": "Learning Membership Functions in a Function-Based Object Recognition   System", 
    "arxiv-id": "cs/9510103v1", 
    "author": "L. Stark", 
    "publish": "1995-10-01T00:00:00Z", 
    "summary": "Functionality-based recognition systems recognize objects at the category\nlevel by reasoning about how well the objects support the expected function.\nSuch systems naturally associate a ``measure of goodness'' or ``membership\nvalue'' with a recognized object. This measure of goodness is the result of\ncombining individual measures, or membership values, from potentially many\nprimitive evaluations of different properties of the object's shape. A\nmembership function is used to compute the membership value when evaluating a\nprimitive of a particular physical property of an object. In previous versions\nof a recognition system known as Gruff, the membership function for each of the\nprimitive evaluations was hand-crafted by the system designer. In this paper,\nwe provide a learning component for the Gruff system, called Omlet, that\nautomatically learns membership functions given a set of example objects\nlabeled with their desired category measure. The learning algorithm is\ngenerally applicable to any problem in which low-level membership values are\ncombined through an and-or tree structure to give a final overall membership\nvalue."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9511101v1", 
    "title": "Flexibly Instructable Agents", 
    "arxiv-id": "cs/9511101v1", 
    "author": "J. E. Laird", 
    "publish": "1995-11-01T00:00:00Z", 
    "summary": "This paper presents an approach to learning from situated, interactive\ntutorial instruction within an ongoing agent. Tutorial instruction is a\nflexible (and thus powerful) paradigm for teaching tasks because it allows an\ninstructor to communicate whatever types of knowledge an agent might need in\nwhatever situations might arise. To support this flexibility, however, the\nagent must be able to learn multiple kinds of knowledge from a broad range of\ninstructional interactions. Our approach, called situated explanation, achieves\nsuch learning through a combination of analytic and inductive techniques. It\ncombines a form of explanation-based learning that is situated for each\ninstruction with a full suite of contextually guided responses to incomplete\nexplanations. The approach is implemented in an agent called Instructo-Soar\nthat learns hierarchies of new tasks and other domain knowledge from\ninteractive natural language instructions. Instructo-Soar meets three key\nrequirements of flexible instructability that distinguish it from previous\nsystems: (1) it can take known or unknown commands at any instruction point;\n(2) it can handle instructions that apply to either its current situation or to\na hypothetical situation specified in language (as in, for instance,\nconditional instructions); and (3) it can learn, from instructions, each class\nof knowledge it uses to perform tasks."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512101v1", 
    "title": "OPUS: An Efficient Admissible Algorithm for Unordered Search", 
    "arxiv-id": "cs/9512101v1", 
    "author": "G. I. Webb", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "OPUS is a branch and bound search algorithm that enables efficient admissible\nsearch through spaces for which the order of search operator application is not\nsignificant. The algorithm's search efficiency is demonstrated with respect to\nvery large machine learning search spaces. The use of admissible search is of\npotential value to the machine learning community as it means that the exact\nlearning biases to be employed for complex learning tasks can be precisely\nspecified and manipulated. OPUS also has potential for application in other\nareas of artificial intelligence, notably, truth maintenance."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512102v1", 
    "title": "Vision-Based Road Detection in Automotive Systems: A Real-Time   Expectation-Driven Approach", 
    "arxiv-id": "cs/9512102v1", 
    "author": "S. Berte", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "The main aim of this work is the development of a vision-based road detection\nsystem fast enough to cope with the difficult real-time constraints imposed by\nmoving vehicle applications. The hardware platform, a special-purpose massively\nparallel system, has been chosen to minimize system production and operational\ncosts. This paper presents a novel approach to expectation-driven low-level\nimage segmentation, which can be mapped naturally onto mesh-connected massively\nparallel SIMD architectures capable of handling hierarchical data structures.\nThe input image is assumed to contain a distorted version of a given template;\na multiresolution stretching process is used to reshape the original template\nin accordance with the acquired image content, minimizing a potential function.\nThe distorted template is the process output."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512103v1", 
    "title": "Generalization of Clauses under Implication", 
    "arxiv-id": "cs/9512103v1", 
    "author": "P. Idestam-Almquist", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "In the area of inductive learning, generalization is a main operation, and\nthe usual definition of induction is based on logical implication. Recently\nthere has been a rising interest in clausal representation of knowledge in\nmachine learning. Almost all inductive learning systems that perform\ngeneralization of clauses use the relation theta-subsumption instead of\nimplication. The main reason is that there is a well-known and simple technique\nto compute least general generalizations under theta-subsumption, but not under\nimplication. However generalization under theta-subsumption is inappropriate\nfor learning recursive clauses, which is a crucial problem since recursion is\nthe basic program structure of logic programs. We note that implication between\nclauses is undecidable, and we therefore introduce a stronger form of\nimplication, called T-implication, which is decidable between clauses. We show\nthat for every finite set of clauses there exists a least general\ngeneralization under T-implication. We describe a technique to reduce\ngeneralizations under implication of a clause to generalizations under\ntheta-subsumption of what we call an expansion of the original clause. Moreover\nwe show that for every non-tautological clause there exists a T-complete\nexpansion, which means that every generalization under T-implication of the\nclause is reduced to a generalization under theta-subsumption of the expansion."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512104v1", 
    "title": "Decision-Theoretic Foundations for Causal Reasoning", 
    "arxiv-id": "cs/9512104v1", 
    "author": "R. Shachter", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "We present a definition of cause and effect in terms of decision-theoretic\nprimitives and thereby provide a principled foundation for causal reasoning.\nOur definition departs from the traditional view of causation in that causal\nassertions may vary with the set of decisions available. We argue that this\napproach provides added clarity to the notion of cause. Also in this paper, we\nexamine the encoding of causal relationships in directed acyclic graphs. We\ndescribe a special class of influence diagrams, those in canonical form, and\nshow its relationship to Pearl's representation of cause and effect. Finally,\nwe show how canonical form facilitates counterfactual reasoning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512105v1", 
    "title": "Translating between Horn Representations and their Characteristic Models", 
    "arxiv-id": "cs/9512105v1", 
    "author": "R. Khardon", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "Characteristic models are an alternative, model based, representation for\nHorn expressions. It has been shown that these two representations are\nincomparable and each has its advantages over the other. It is therefore\nnatural to ask what is the cost of translating, back and forth, between these\nrepresentations. Interestingly, the same translation questions arise in\ndatabase theory, where it has applications to the design of relational\ndatabases. This paper studies the computational complexity of these problems.\nOur main result is that the two translation problems are equivalent under\npolynomial reductions, and that they are equivalent to the corresponding\ndecision problem. Namely, translating is equivalent to deciding whether a given\nset of models is the set of characteristic models for a given Horn expression.\nWe also relate these problems to the hypergraph transversal problem, a well\nknown problem which is related to other applications in AI and for which no\npolynomial time algorithm is known. It is shown that in general our translation\nproblems are at least as hard as the hypergraph transversal problem, and in a\nspecial case they are equivalent to it."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512106v1", 
    "title": "Statistical Feature Combination for the Evaluation of Game Positions", 
    "arxiv-id": "cs/9512106v1", 
    "author": "M. Buro", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "This article describes an application of three well-known statistical methods\nin the field of game-tree search: using a large number of classified Othello\npositions, feature weights for evaluation functions with a\ngame-phase-independent meaning are estimated by means of logistic regression,\nFisher's linear discriminant, and the quadratic discriminant function for\nnormally distributed features. Thereafter, the playing strengths are compared\nby means of tournaments between the resulting versions of a world-class Othello\nprogram. In this application, logistic regression - which is used here for the\nfirst time in the context of game playing - leads to better results than the\nother approaches."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9512107v1", 
    "title": "Rule-based Machine Learning Methods for Functional Prediction", 
    "arxiv-id": "cs/9512107v1", 
    "author": "N. Indurkhya", 
    "publish": "1995-12-01T00:00:00Z", 
    "summary": "We describe a machine learning method for predicting the value of a\nreal-valued function, given the values of multiple input variables. The method\ninduces solutions from samples in the form of ordered disjunctive normal form\n(DNF) decision rules. A central objective of the method and representation is\nthe induction of compact, easily interpretable solutions. This rule-based\ndecision model can be extended to search efficiently for similar cases prior to\napproximating function values. Experimental results on real-world data\ndemonstrate that the new techniques are competitive with existing machine\nlearning and statistical methods and can sometimes yield superior regression\nperformance."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9601101v1", 
    "title": "The Design and Experimental Analysis of Algorithms for Temporal   Reasoning", 
    "arxiv-id": "cs/9601101v1", 
    "author": "D. W. Manchak", 
    "publish": "1996-01-01T00:00:00Z", 
    "summary": "Many applications -- from planning and scheduling to problems in molecular\nbiology -- rely heavily on a temporal reasoning component. In this paper, we\ndiscuss the design and empirical analysis of algorithms for a temporal\nreasoning system based on Allen's influential interval-based framework for\nrepresenting temporal information. At the core of the system are algorithms for\ndetermining whether the temporal information is consistent, and, if so, finding\none or more scenarios that are consistent with the temporal information. Two\nimportant algorithms for these tasks are a path consistency algorithm and a\nbacktracking algorithm. For the path consistency algorithm, we develop\ntechniques that can result in up to a ten-fold speedup over an already highly\noptimized implementation. For the backtracking algorithm, we develop variable\nand value ordering heuristics that are shown empirically to dramatically\nimprove the performance of the algorithm. As well, we show that a previously\nsuggested reformulation of the backtracking search problem can reduce the time\nand space requirements of the backtracking search. Taken together, the\ntechniques we develop allow a temporal reasoning component to solve problems\nthat are of practical size."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9602101v1", 
    "title": "Well-Founded Semantics for Extended Logic Programs with Dynamic   Preferences", 
    "arxiv-id": "cs/9602101v1", 
    "author": "G. Brewka", 
    "publish": "1996-02-01T00:00:00Z", 
    "summary": "The paper describes an extension of well-founded semantics for logic programs\nwith two types of negation. In this extension information about preferences\nbetween rules can be expressed in the logical language and derived dynamically.\nThis is achieved by using a reserved predicate symbol and a naming technique.\nConflicts among rules are resolved whenever possible on the basis of derived\npreference information. The well-founded conclusions of prioritized logic\nprograms can be computed in polynomial time. A legal reasoning example\nillustrates the usefulness of the approach."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9602102v1", 
    "title": "Logarithmic-Time Updates and Queries in Probabilistic Networks", 
    "arxiv-id": "cs/9602102v1", 
    "author": "J. Pearl", 
    "publish": "1996-02-01T00:00:00Z", 
    "summary": "Traditional databases commonly support efficient query and update procedures\nthat operate in time which is sublinear in the size of the database. Our goal\nin this paper is to take a first step toward dynamic reasoning in probabilistic\ndatabases with comparable efficiency. We propose a dynamic data structure that\nsupports efficient algorithms for updating and querying singly connected\nBayesian networks. In the conventional algorithm, new evidence is absorbed in\nO(1) time and queries are processed in time O(N), where N is the size of the\nnetwork. We propose an algorithm which, after a preprocessing phase, allows us\nto answer queries in time O(log N) at the expense of O(log N) time per evidence\nabsorption. The usefulness of sub-linear processing time manifests itself in\napplications requiring (near) real-time response over large probabilistic\ndatabases. We briefly discuss a potential application of dynamic probabilistic\nreasoning in computational biology."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9603101v1", 
    "title": "Quantum Computing and Phase Transitions in Combinatorial Search", 
    "arxiv-id": "cs/9603101v1", 
    "author": "T. Hogg", 
    "publish": "1996-03-01T00:00:00Z", 
    "summary": "We introduce an algorithm for combinatorial search on quantum computers that\nis capable of significantly concentrating amplitude into solutions for some NP\nsearch problems, on average. This is done by exploiting the same aspects of\nproblem structure as used by classical backtrack methods to avoid unproductive\nsearch choices. This quantum algorithm is much more likely to find solutions\nthan the simple direct use of quantum parallelism. Furthermore, empirical\nevaluation on small problems shows this quantum algorithm displays the same\nphase transition behavior, and at the same location, as seen in many previously\nstudied classical search methods. Specifically, difficult problem instances are\nconcentrated near the abrupt change from underconstrained to overconstrained\nproblems."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9603102v1", 
    "title": "Mean Field Theory for Sigmoid Belief Networks", 
    "arxiv-id": "cs/9603102v1", 
    "author": "M. I. Jordan", 
    "publish": "1996-03-01T00:00:00Z", 
    "summary": "We develop a mean field theory for sigmoid belief networks based on ideas\nfrom statistical mechanics. Our mean field theory provides a tractable\napproximation to the true probability distribution in these networks; it also\nyields a lower bound on the likelihood of evidence. We demonstrate the utility\nof this framework on a benchmark problem in statistical pattern\nrecognition---the classification of handwritten digits."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9603103v1", 
    "title": "Improved Use of Continuous Attributes in C4.5", 
    "arxiv-id": "cs/9603103v1", 
    "author": "J. R. Quinlan", 
    "publish": "1996-03-01T00:00:00Z", 
    "summary": "A reported weakness of C4.5 in domains with continuous attributes is\naddressed by modifying the formation and evaluation of tests on continuous\nattributes. An MDL-inspired penalty is applied to such tests, eliminating some\nof them from consideration and altering the relative desirability of all tests.\nEmpirical trials show that the modifications lead to smaller decision trees\nwith higher predictive accuracies. Results also confirm that a new version of\nC4.5 incorporating these changes is superior to recent approaches that use\nglobal discretization and that construct small trees with multi-interval\nsplits."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9603104v1", 
    "title": "Active Learning with Statistical Models", 
    "arxiv-id": "cs/9603104v1", 
    "author": "M. I. Jordan", 
    "publish": "1996-03-01T00:00:00Z", 
    "summary": "For many types of machine learning algorithms, one can compute the\nstatistically `optimal' way to select training data. In this paper, we review\nhow optimal data selection techniques have been used with feedforward neural\nnetworks. We then show how the same principles may be used to select data for\ntwo alternative, statistically-based learning architectures: mixtures of\nGaussians and locally weighted regression. While the techniques for neural\nnetworks are computationally expensive and approximate, the techniques for\nmixtures of Gaussians and locally weighted regression are both efficient and\naccurate. Empirically, we observe that the optimality criterion sharply\ndecreases the number of training examples the learner needs in order to achieve\ngood performance."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9604101v1", 
    "title": "A Divergence Critic for Inductive Proof", 
    "arxiv-id": "cs/9604101v1", 
    "author": "T. Walsh", 
    "publish": "1996-04-01T00:00:00Z", 
    "summary": "Inductive theorem provers often diverge. This paper describes a simple\ncritic, a computer program which monitors the construction of inductive proofs\nattempting to identify diverging proof attempts. Divergence is recognized by\nmeans of a ``difference matching'' procedure. The critic then proposes lemmas\nand generalizations which ``ripple'' these differences away so that the proof\ncan go through without divergence. The critic enables the theorem prover Spike\nto prove many theorems completely automatically from the definitions alone."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9604102v1", 
    "title": "Practical Methods for Proving Termination of General Logic Programs", 
    "arxiv-id": "cs/9604102v1", 
    "author": "E. Marchiori", 
    "publish": "1996-04-01T00:00:00Z", 
    "summary": "Termination of logic programs with negated body atoms (here called general\nlogic programs) is an important topic. One reason is that many computational\nmechanisms used to process negated atoms, like Clark's negation as failure and\nChan's constructive negation, are based on termination conditions. This paper\nintroduces a methodology for proving termination of general logic programs\nw.r.t. the Prolog selection rule. The idea is to distinguish parts of the\nprogram depending on whether or not their termination depends on the selection\nrule. To this end, the notions of low-, weakly up-, and up-acceptable program\nare introduced. We use these notions to develop a methodology for proving\ntermination of general logic programs, and show how interesting problems in\nnon-monotonic reasoning can be formalized and implemented by means of\nterminating general logic programs."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9604103v1", 
    "title": "Iterative Optimization and Simplification of Hierarchical Clusterings", 
    "arxiv-id": "cs/9604103v1", 
    "author": "D. Fisher", 
    "publish": "1996-04-01T00:00:00Z", 
    "summary": "Clustering is often used for discovering structure in data. Clustering\nsystems differ in the objective function used to evaluate clustering quality\nand the control strategy used to search the space of clusterings. Ideally, the\nsearch strategy should consistently construct clusterings of high quality, but\nbe computationally inexpensive as well. In general, we cannot have it both\nways, but we can partition the search so that a system inexpensively constructs\na `tentative' clustering for initial examination, followed by iterative\noptimization, which continues to search in background for improved clusterings.\nGiven this motivation, we evaluate an inexpensive strategy for creating initial\nclusterings, coupled with several control strategies for iterative\noptimization, each of which repeatedly modifies an initial clustering in search\nof a better one. One of these methods appears novel as an iterative\noptimization strategy in clustering contexts. Once a clustering has been\nconstructed it is judged by analysts -- often according to task-specific\ncriteria. Several authors have abstracted these criteria and posited a generic\nperformance task akin to pattern completion, where the error rate over\ncompleted patterns is used to `externally' judge clustering utility. Given this\nperformance task, we adapt resampling-based pruning strategies used by\nsupervised learning systems to the task of simplifying hierarchical\nclusterings, thus promising to ease post-clustering analysis. Finally, we\npropose a number of objective functions, based on attribute-selection measures\nfor decision-tree induction, that might perform well on the error rate and\nsimplicity dimensions."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605101v1", 
    "title": "Further Experimental Evidence against the Utility of Occam's Razor", 
    "arxiv-id": "cs/9605101v1", 
    "author": "G. I. Webb", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "This paper presents new experimental evidence against the utility of Occam's\nrazor. A~systematic procedure is presented for post-processing decision trees\nproduced by C4.5. This procedure was derived by rejecting Occam's razor and\ninstead attending to the assumption that similar objects are likely to belong\nto the same class. It increases a decision tree's complexity without altering\nthe performance of that tree on the training data from which it is inferred.\nThe resulting more complex decision trees are demonstrated to have, on average,\nfor a variety of common learning tasks, higher predictive accuracy than the\nless complex original decision trees. This result raises considerable doubt\nabout the utility of Occam's razor as it is commonly applied in modern machine\nlearning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605102v1", 
    "title": "Least Generalizations and Greatest Specializations of Sets of Clauses", 
    "arxiv-id": "cs/9605102v1", 
    "author": "R. deWolf", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "The main operations in Inductive Logic Programming (ILP) are generalization\nand specialization, which only make sense in a generality order. In ILP, the\nthree most important generality orders are subsumption, implication and\nimplication relative to background knowledge. The two languages used most often\nare languages of clauses and languages of only Horn clauses. This gives a total\nof six different ordered languages. In this paper, we give a systematic\ntreatment of the existence or non-existence of least generalizations and\ngreatest specializations of finite sets of clauses in each of these six ordered\nsets. We survey results already obtained by others and also contribute some\nanswers of our own. Our main new results are, firstly, the existence of a\ncomputable least generalization under implication of every finite set of\nclauses containing at least one non-tautologous function-free clause (among\nother, not necessarily function-free clauses). Secondly, we show that such a\nleast generalization need not exist under relative implication, not even if\nboth the set that is to be generalized and the background knowledge are\nfunction-free. Thirdly, we give a complete discussion of existence and\nnon-existence of greatest specializations in each of the six ordered languages."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605103v1", 
    "title": "Reinforcement Learning: A Survey", 
    "arxiv-id": "cs/9605103v1", 
    "author": "A. W. Moore", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "This paper surveys the field of reinforcement learning from a\ncomputer-science perspective. It is written to be accessible to researchers\nfamiliar with machine learning. Both the historical basis of the field and a\nbroad selection of current work are summarized. Reinforcement learning is the\nproblem faced by an agent that learns behavior through trial-and-error\ninteractions with a dynamic environment. The work described here has a\nresemblance to work in psychology, but differs considerably in the details and\nin the use of the word ``reinforcement.'' The paper discusses central issues of\nreinforcement learning, including trading off exploration and exploitation,\nestablishing the foundations of the field via Markov decision theory, learning\nfrom delayed reinforcement, constructing empirical models to accelerate\nlearning, making use of generalization and hierarchy, and coping with hidden\nstate. It concludes with a survey of some implemented systems and an assessment\nof the practical utility of current methods for reinforcement learning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605104v1", 
    "title": "Adaptive Problem-solving for Large-scale Scheduling Problems: A Case   Study", 
    "arxiv-id": "cs/9605104v1", 
    "author": "S. Chien", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "Although most scheduling problems are NP-hard, domain specific techniques\nperform well in practice but are quite expensive to construct. In adaptive\nproblem-solving solving, domain specific knowledge is acquired automatically\nfor a general problem solver with a flexible control architecture. In this\napproach, a learning system explores a space of possible heuristic methods for\none well-suited to the eccentricities of the given domain and problem\ndistribution. In this article, we discuss an application of the approach to\nscheduling satellite communications. Using problem distributions based on\nactual mission requirements, our approach identifies strategies that not only\ndecrease the amount of CPU time required to produce schedules, but also\nincrease the percentage of problems that are solvable within computational\nresource limitations."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605105v1", 
    "title": "A Formal Framework for Speedup Learning from Problems and Solutions", 
    "arxiv-id": "cs/9605105v1", 
    "author": "B. K. Natarajan", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "Speedup learning seeks to improve the computational efficiency of problem\nsolving with experience. In this paper, we develop a formal framework for\nlearning efficient problem solving from random problems and their solutions. We\napply this framework to two different representations of learned knowledge,\nnamely control rules and macro-operators, and prove theorems that identify\nsufficient conditions for learning in each representation. Our proofs are\nconstructive in that they are accompanied with learning algorithms. Our\nframework captures both empirical and explanation-based speedup learning in a\nunified fashion. We illustrate our framework with implementations in two\ndomains: symbolic integration and Eight Puzzle. This work integrates many\nstrands of experimental and theoretical work in machine learning, including\nempirical learning of control rules, macro-operator learning, Explanation-Based\nLearning (EBL), and Probably Approximately Correct (PAC) Learning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9605106v1", 
    "title": "2Planning for Contingencies: A Decision-based Approach", 
    "arxiv-id": "cs/9605106v1", 
    "author": "G. Collins", 
    "publish": "1996-05-01T00:00:00Z", 
    "summary": "A fundamental assumption made by classical AI planners is that there is no\nuncertainty in the world: the planner has full knowledge of the conditions\nunder which the plan will be executed and the outcome of every action is fully\npredictable. These planners cannot therefore construct contingency plans, i.e.,\nplans in which different actions are performed in different circumstances. In\nthis paper we discuss some issues that arise in the representation and\nconstruction of contingency plans and describe Cassandra, a partial-order\ncontingency planner. Cassandra uses explicit decision-steps that enable the\nagent executing the plan to decide which plan branch to follow. The\ndecision-steps in a plan result in subgoals to acquire knowledge, which are\nplanned for in the same way as any other subgoals. Cassandra thus distinguishes\nthe process of gathering information from the process of making decisions. The\nexplicit representation of decisions in Cassandra allows a coherent approach to\nthe problems of contingent planning, and provides a solid base for extensions\nsuch as the use of different decision-making procedures."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9606101v1", 
    "title": "A Principled Approach Towards Symbolic Geometric Constraint Satisfaction", 
    "arxiv-id": "cs/9606101v1", 
    "author": "T. J. Hoar", 
    "publish": "1996-06-01T00:00:00Z", 
    "summary": "An important problem in geometric reasoning is to find the configuration of a\ncollection of geometric bodies so as to satisfy a set of given constraints.\nRecently, it has been suggested that this problem can be solved efficiently by\nsymbolically reasoning about geometry. This approach, called degrees of freedom\nanalysis, employs a set of specialized routines called plan fragments that\nspecify how to change the configuration of a set of bodies to satisfy a new\nconstraint while preserving existing constraints. A potential drawback, which\nlimits the scalability of this approach, is concerned with the difficulty of\nwriting plan fragments. In this paper we address this limitation by showing how\nthese plan fragments can be automatically synthesized using first principles\nabout geometric bodies, actions, and topology."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9606102v1", 
    "title": "On Partially Controlled Multi-Agent Systems", 
    "arxiv-id": "cs/9606102v1", 
    "author": "M. Tennenholtz", 
    "publish": "1996-06-01T00:00:00Z", 
    "summary": "Motivated by the control theoretic distinction between controllable and\nuncontrollable events, we distinguish between two types of agents within a\nmulti-agent system: controllable agents, which are directly controlled by the\nsystem's designer, and uncontrollable agents, which are not under the\ndesigner's direct control. We refer to such systems as partially controlled\nmulti-agent systems, and we investigate how one might influence the behavior of\nthe uncontrolled agents through appropriate design of the controlled agents. In\nparticular, we wish to understand which problems are naturally described in\nthese terms, what methods can be applied to influence the uncontrollable\nagents, the effectiveness of such methods, and whether similar methods work\nacross different domains. Using a game-theoretic framework, this paper studies\nthe design of partially controlled multi-agent systems in two contexts: in one\ncontext, the uncontrollable agents are expected utility maximizers, while in\nthe other they are reinforcement learners. We suggest different techniques for\ncontrolling agents' behavior in each domain, assess their success, and examine\ntheir relationship."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9608103v1", 
    "title": "Spatial Aggregation: Theory and Applications", 
    "arxiv-id": "cs/9608103v1", 
    "author": "F. Zhao", 
    "publish": "1996-08-01T00:00:00Z", 
    "summary": "Visual thinking plays an important role in scientific reasoning. Based on the\nresearch in automating diverse reasoning tasks about dynamical systems,\nnonlinear controllers, kinematic mechanisms, and fluid motion, we have\nidentified a style of visual thinking, imagistic reasoning. Imagistic reasoning\norganizes computations around image-like, analogue representations so that\nperceptual and symbolic operations can be brought to bear to infer structure\nand behavior. Programs incorporating imagistic reasoning have been shown to\nperform at an expert level in domains that defy current analytic or numerical\nmethods. We have developed a computational paradigm, spatial aggregation, to\nunify the description of a class of imagistic problem solvers. A program\nwritten in this paradigm has the following properties. It takes a continuous\nfield and optional objective functions as input, and produces high-level\ndescriptions of structure, behavior, or control actions. It computes a\nmulti-layer of intermediate representations, called spatial aggregates, by\nforming equivalence classes and adjacency relations. It employs a small set of\ngeneric operators such as aggregation, classification, and localization to\nperform bidirectional mapping between the information-rich field and\nsuccessively more abstract spatial aggregates. It uses a data structure, the\nneighborhood graph, as a common interface to modularize computations. To\nillustrate our theory, we describe the computational structure of three\nimplemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the\nspatial aggregation generic operators by mixing and matching a library of\ncommonly used routines."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9608104v1", 
    "title": "A Hierarchy of Tractable Subsets for Computing Stable Models", 
    "arxiv-id": "cs/9608104v1", 
    "author": "R. Ben-Eliyahu", 
    "publish": "1996-08-01T00:00:00Z", 
    "summary": "Finding the stable models of a knowledge base is a significant computational\nproblem in artificial intelligence. This task is at the computational heart of\ntruth maintenance systems, autoepistemic logic, and default logic.\nUnfortunately, it is NP-hard. In this paper we present a hierarchy of classes\nof knowledge bases, Omega_1,Omega_2,..., with the following properties: first,\nOmega_1 is the class of all stratified knowledge bases; second, if a knowledge\nbase Pi is in Omega_k, then Pi has at most k stable models, and all of them may\nbe found in time O(lnk), where l is the length of the knowledge base and n the\nnumber of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find\nthe minimum k such that Pi belongs to Omega_k in time polynomial in the size of\nPi; and, last, where K is the class of all knowledge bases, it is the case that\nunion{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some\nclass in the hierarchy."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9609101v1", 
    "title": "Accelerating Partial-Order Planners: Some Techniques for Effective   Search Control and Pruning", 
    "arxiv-id": "cs/9609101v1", 
    "author": "L. Schubert", 
    "publish": "1996-09-01T00:00:00Z", 
    "summary": "We propose some domain-independent techniques for bringing well-founded\npartial-order planners closer to practicality. The first two techniques are\naimed at improving search control while keeping overhead costs low. One is\nbased on a simple adjustment to the default A* heuristic used by UCPOP to\nselect plans for refinement. The other is based on preferring ``zero\ncommitment'' (forced) plan refinements whenever possible, and using LIFO\nprioritization otherwise. A more radical technique is the use of operator\nparameter domains to prune search. These domains are initially computed from\nthe definitions of the operators and the initial and goal conditions, using a\npolynomial-time algorithm that propagates sets of constants through the\noperator graph, starting in the initial conditions. During planning, parameter\ndomains can be used to prune nonviable operator instances and to remove\nspurious clobbering threats. In experiments based on modifications of UCPOP,\nour improved plan and goal selection strategies gave speedups by factors\nranging from 5 to more than 1000 for a variety of problems that are nontrivial\nfor the unmodified version. Crucially, the hardest problems gave the greatest\nimprovements. The pruning technique based on parameter domains often gave\nspeedups by an order of magnitude or more for difficult problems, both with the\ndefault UCPOP search strategy and with our improved strategy. The Lisp code for\nour techniques and for the test problems is provided in on-line appendices."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9609102v1", 
    "title": "Cue Phrase Classification Using Machine Learning", 
    "arxiv-id": "cs/9609102v1", 
    "author": "D. J. Litman", 
    "publish": "1996-09-01T00:00:00Z", 
    "summary": "Cue phrases may be used in a discourse sense to explicitly signal discourse\nstructure, but also in a sentential sense to convey semantic rather than\nstructural information. Correctly classifying cue phrases as discourse or\nsentential is critical in natural language processing systems that exploit\ndiscourse structure, e.g., for performing tasks such as anaphora resolution and\nplan recognition. This paper explores the use of machine learning for\nclassifying cue phrases as discourse or sentential. Two machine learning\nprograms (Cgrendel and C4.5) are used to induce classification models from sets\nof pre-classified cue phrases and their features in text and speech. Machine\nlearning is shown to be an effective technique for not only automating the\ngeneration of classification models, but also for improving upon previous\nresults. When compared to manually derived classification models already in the\nliterature, the learned models often perform with higher accuracy and contain\nnew linguistic insights into the data. In addition, the ability to\nautomatically construct classification models makes it easier to comparatively\nanalyze the utility of alternative feature representations of the data.\nFinally, the ease of retraining makes the learning approach more scalable and\nflexible than manual methods."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9610101v1", 
    "title": "Mechanisms for Automated Negotiation in State Oriented Domains", 
    "arxiv-id": "cs/9610101v1", 
    "author": "J. S. Rosenschein", 
    "publish": "1996-10-01T00:00:00Z", 
    "summary": "This paper lays part of the groundwork for a domain theory of negotiation,\nthat is, a way of classifying interactions so that it is clear, given a domain,\nwhich negotiation mechanisms and strategies are appropriate. We define State\nOriented Domains, a general category of interaction. Necessary and sufficient\nconditions for cooperation are outlined. We use the notion of worth in an\naltered definition of utility, thus enabling agreements in a wider class of\njoint-goal reachable situations. An approach is offered for conflict\nresolution, and it is shown that even in a conflict situation, partial\ncooperative steps can be taken by interacting agents (that is, agents in\nfundamental conflict might still agree to cooperate up to a certain point). A\nUnified Negotiation Protocol (UNP) is developed that can be used in all types\nof encounters. It is shown that in certain borderline cooperative situations, a\npartial cooperative agreement (i.e., one that does not achieve all agents'\ngoals) might be preferred by all agents, even though there exists a rational\nagreement that would achieve all their goals. Finally, we analyze cases where\nagents have incomplete information on the goals and worth of other agents.\nFirst we consider the case where agents' goals are private information, and we\nanalyze what goal declaration strategies the agents might adopt to increase\ntheir utility. Then, we consider the situation where the agents' goals (and\ntherefore stand-alone costs) are common knowledge, but the worth they attach to\ntheir goals is private information. We introduce two mechanisms, one 'strict',\nthe other 'tolerant', and analyze their affects on the stability and efficiency\nof negotiation outcomes."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9610102v1", 
    "title": "Learning First-Order Definitions of Functions", 
    "arxiv-id": "cs/9610102v1", 
    "author": "J. R. Quinlan", 
    "publish": "1996-10-01T00:00:00Z", 
    "summary": "First-order learning involves finding a clause-form definition of a relation\nfrom examples of the relation and relevant background information. In this\npaper, a particular first-order learning system is modified to customize it for\nfinding definitions of functional relations. This restriction leads to faster\nlearning times and, in some cases, to definitions that have higher predictive\naccuracy. Other first-order learning systems might benefit from similar\nspecialization."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9611101v1", 
    "title": "MUSE CSP: An Extension to the Constraint Satisfaction Problem", 
    "arxiv-id": "cs/9611101v1", 
    "author": "M. P. Harper", 
    "publish": "1996-11-01T00:00:00Z", 
    "summary": "This paper describes an extension to the constraint satisfaction problem\n(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).\nThis extension is especially useful for those problems which segment into\nmultiple sets of partially shared variables. Such problems arise naturally in\nsignal processing applications including computer vision, speech processing,\nand handwriting recognition. For these applications, it is often difficult to\nsegment the data in only one way given the low-level information utilized by\nthe segmentation algorithms. MUSE CSP can be used to compactly represent\nseveral similar instances of the constraint satisfaction problem. If multiple\ninstances of a CSP have some common variables which have the same domains and\nconstraints, then they can be combined into a single instance of a MUSE CSP,\nreducing the work required to apply the constraints. We introduce the concepts\nof MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We\nthen demonstrate how MUSE CSP can be used to compactly represent lexically\nambiguous sentences and the multiple sentence hypotheses that are often\ngenerated by speech recognition algorithms so that grammar constraints can be\nused to provide parses for all syntactically correct sentences. Algorithms for\nMUSE arc and path consistency are provided. Finally, we discuss how to create a\nMUSE CSP from a set of CSPs which are labeled to indicate when the same\nvariable is shared by more than a single CSP."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9612101v1", 
    "title": "Exploiting Causal Independence in Bayesian Network Inference", 
    "arxiv-id": "cs/9612101v1", 
    "author": "D. Poole", 
    "publish": "1996-12-01T00:00:00Z", 
    "summary": "A new method is proposed for exploiting causal independencies in exact\nBayesian network inference. A Bayesian network can be viewed as representing a\nfactorization of a joint probability into the multiplication of a set of\nconditional probabilities. We present a notion of causal independence that\nenables one to further factorize the conditional probabilities into a\ncombination of even smaller factors and consequently obtain a finer-grain\nfactorization of the joint probability. The new formulation of causal\nindependence lets us specify the conditional probability of a variable given\nits parents in terms of an associative and commutative operator, such as\n``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a\nsimple algorithm VE for Bayesian network inference that, given evidence and a\nquery variable, uses the factorization to find the posterior distribution of\nthe query. We show how this algorithm can be extended to exploit causal\nindependence. Empirical studies, based on the CPCS networks for medical\ndiagnosis, show that this method is more efficient than previous methods and\nallows for inference in larger networks than previous algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9612102v1", 
    "title": "Quantitative Results Comparing Three Intelligent Interfaces for   Information Capture: A Case Study Adding Name Information into an Electronic   Personal Organizer", 
    "arxiv-id": "cs/9612102v1", 
    "author": "P. C. Wells", 
    "publish": "1996-12-01T00:00:00Z", 
    "summary": "Efficiently entering information into a computer is key to enjoying the\nbenefits of computing. This paper describes three intelligent user interfaces:\nhandwriting recognition, adaptive menus, and predictive fillin. In the context\nof adding a personUs name and address to an electronic organizer, tests show\nhandwriting recognition is slower than typing on an on-screen, soft keyboard,\nwhile adaptive menus and predictive fillin can be twice as fast. This paper\nalso presents strategies for applying these three interfaces to other\ninformation collection domains."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9612103v1", 
    "title": "Characterizations of Decomposable Dependency Models", 
    "arxiv-id": "cs/9612103v1", 
    "author": "L. M. deCampos", 
    "publish": "1996-12-01T00:00:00Z", 
    "summary": "Decomposable dependency models possess a number of interesting and useful\nproperties. This paper presents new characterizations of decomposable models in\nterms of independence relationships, which are obtained by adding a single\naxiom to the well-known set characterizing dependency models that are\nisomorphic to undirected graphs. We also briefly discuss a potential\napplication of our results to the problem of learning graphical models from\ndata."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9701101v1", 
    "title": "Improved Heterogeneous Distance Functions", 
    "arxiv-id": "cs/9701101v1", 
    "author": "T. R. Martinez", 
    "publish": "1997-01-01T00:00:00Z", 
    "summary": "Instance-based learning techniques typically handle continuous and linear\ninput values well, but often do not handle nominal input attributes\nappropriately. The Value Difference Metric (VDM) was designed to find\nreasonable distance values between nominal attribute values, but it largely\nignores continuous attributes, requiring discretization to map continuous\nvalues into nominal values. This paper proposes three new heterogeneous\ndistance functions, called the Heterogeneous Value Difference Metric (HVDM),\nthe Interpolated Value Difference Metric (IVDM), and the Windowed Value\nDifference Metric (WVDM). These new distance functions are designed to handle\napplications with nominal attributes, continuous attributes, or both. In\nexperiments on 48 applications the new distance metrics achieve higher\nclassification accuracy on average than three previous distance functions on\nthose datasets that have both nominal and continuous attributes."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9701102v1", 
    "title": "SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis   Using Artificial Neural Networks", 
    "arxiv-id": "cs/9701102v1", 
    "author": "V. Weber", 
    "publish": "1997-01-01T00:00:00Z", 
    "summary": "Previous approaches of analyzing spontaneously spoken language often have\nbeen based on encoding syntactic and semantic knowledge manually and\nsymbolically. While there has been some progress using statistical or\nconnectionist language models, many current spoken- language systems still use\na relatively brittle, hand-coded symbolic grammar or symbolic semantic\ncomponent. In contrast, we describe a so-called screening approach for learning\nrobust processing of spontaneously spoken language. A screening approach is a\nflat analysis which uses shallow sequences of category representations for\nanalyzing an utterance at various syntactic, semantic and dialog levels. Rather\nthan using a deeply structured symbolic analysis, we use a flat connectionist\nanalysis. This screening approach aims at supporting speech and language\nprocessing by using (1) data-driven learning and (2) robustness of\nconnectionist networks. In order to test this approach, we have developed the\nSCREEN system which is based on this new robust, learned and flat analysis. In\nthis paper, we focus on a detailed description of SCREEN's architecture, the\nflat syntactic and semantic analysis, the interaction with a speech recognizer,\nand a detailed evaluation analysis of the robustness under the influence of\nnoisy or incomplete input. The main result of this paper is that flat\nrepresentations allow more robust processing of spontaneous spoken language\nthan deeply structured representations. In particular, we show how the\nfault-tolerance and learning capability of connectionist networks can support a\nflat analysis for providing more robust spoken-language processing within an\noverall hybrid symbolic/connectionist framework."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9703101v1", 
    "title": "A Uniform Framework for Concept Definitions in Description Logics", 
    "arxiv-id": "cs/9703101v1", 
    "author": "M. Lenzerini", 
    "publish": "1997-03-01T00:00:00Z", 
    "summary": "Most modern formalisms used in Databases and Artificial Intelligence for\ndescribing an application domain are based on the notions of class (or concept)\nand relationship among classes. One interesting feature of such formalisms is\nthe possibility of defining a class, i.e., providing a set of properties that\nprecisely characterize the instances of the class. Many recent articles point\nout that there are several ways of assigning a meaning to a class definition\ncontaining some sort of recursion. In this paper, we argue that, instead of\nchoosing a single style of semantics, we achieve better results by adopting a\nformalism that allows for different semantics to coexist. We demonstrate the\nfeasibility of our argument, by presenting a knowledge representation\nformalism, the description logic muALCQ, with the above characteristics. In\naddition to the constructs for conjunction, disjunction, negation, quantifiers,\nand qualified number restrictions, muALCQ includes special fixpoint constructs\nto express (suitably interpreted) recursive definitions. These constructs\nenable the usual frame-based descriptions to be combined with definitions of\nrecursive data structures such as directed acyclic graphs, lists, streams, etc.\nWe establish several properties of muALCQ, including the decidability and the\ncomputational complexity of reasoning, by formulating a correspondence with a\nparticular modal logic of programs called the modal mu-calculus."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9704101v1", 
    "title": "Lifeworld Analysis", 
    "arxiv-id": "cs/9704101v1", 
    "author": "I. Horswill", 
    "publish": "1997-04-01T00:00:00Z", 
    "summary": "We argue that the analysis of agent/environment interactions should be\nextended to include the conventions and invariants maintained by agents\nthroughout their activity. We refer to this thicker notion of environment as a\nlifeworld and present a partial set of formal tools for describing structures\nof lifeworlds and the ways in which they computationally simplify activity. As\none specific example, we apply the tools to the analysis of the Toast system\nand show how versions of the system with very different control structures in\nfact implement a common control structure together with different conventions\nfor encoding task state in the positions or states of objects in the\nenvironment."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9705101v1", 
    "title": "Query DAGs: A Practical Paradigm for Implementing Belief-Network   Inference", 
    "arxiv-id": "cs/9705101v1", 
    "author": "G. Provan", 
    "publish": "1997-05-01T00:00:00Z", 
    "summary": "We describe a new paradigm for implementing inference in belief networks,\nwhich consists of two steps: (1) compiling a belief network into an arithmetic\nexpression called a Query DAG (Q-DAG); and (2) answering queries using a simple\nevaluation algorithm. Each node of a Q-DAG represents a numeric operation, a\nnumber, or a symbol for evidence. Each leaf node of a Q-DAG represents the\nanswer to a network query, that is, the probability of some event of interest.\nIt appears that Q-DAGs can be generated using any of the standard algorithms\nfor exact inference in belief networks (we show how they can be generated using\nclustering and conditioning algorithms). The time and space complexity of a\nQ-DAG generation algorithm is no worse than the time complexity of the\ninference algorithm on which it is based. The complexity of a Q-DAG evaluation\nalgorithm is linear in the size of the Q-DAG, and such inference amounts to a\nstandard evaluation of the arithmetic expression it represents. The intended\nvalue of Q-DAGs is in reducing the software and hardware resources required to\nutilize belief networks in on-line, real-world applications. The proposed\nframework also facilitates the development of on-line inference on different\nsoftware and hardware platforms due to the simplicity of the Q-DAG evaluation\nalgorithm. Interestingly enough, Q-DAGs were found to serve other purposes:\nsimple techniques for reducing Q-DAGs tend to subsume relatively complex\noptimization techniques for belief-network inference, such as network-pruning\nand computation-caching."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9705102v1", 
    "title": "Connectionist Theory Refinement: Genetically Searching the Space of   Network Topologies", 
    "arxiv-id": "cs/9705102v1", 
    "author": "J. W. Shavlik", 
    "publish": "1997-05-01T00:00:00Z", 
    "summary": "An algorithm that learns from a set of examples should ideally be able to\nexploit the available resources of (a) abundant computing power and (b)\ndomain-specific knowledge to improve its ability to generalize. Connectionist\ntheory-refinement systems, which use background knowledge to select a neural\nnetwork's topology and initial weights, have proven to be effective at\nexploiting domain-specific knowledge; however, most do not exploit available\ncomputing power. This weakness occurs because they lack the ability to refine\nthe topology of the neural networks they produce, thereby limiting\ngeneralization, especially when given impoverished domain theories. We present\nthe REGENT algorithm which uses (a) domain-specific knowledge to help create an\ninitial population of knowledge-based neural networks and (b) genetic operators\nof crossover and mutation (specifically designed for knowledge-based networks)\nto continually search for better network topologies. Experiments on three\nreal-world domains indicate that our new algorithm is able to significantly\nincrease generalization compared to a standard connectionist theory-refinement\nsystem, as well as our previous algorithm for growing knowledge-based networks."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9706101v1", 
    "title": "Flaw Selection Strategies for Partial-Order Planning", 
    "arxiv-id": "cs/9706101v1", 
    "author": "M. Paolucci", 
    "publish": "1997-06-01T00:00:00Z", 
    "summary": "Several recent studies have compared the relative efficiency of alternative\nflaw selection strategies for partial-order causal link (POCL) planning. We\nreview this literature, and present new experimental results that generalize\nthe earlier work and explain some of the discrepancies in it. In particular, we\ndescribe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by\nJoslin and Pollack (1994), and compare it with other strategies, including\nGerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very\ndifferent, and apparently conflicting claims about the most effective way to\nreduce search-space size in POCL planning. We resolve this conflict, arguing\nthat much of the benefit that Gerevini and Schubert ascribe to the LIFO\ncomponent of their ZLIFO strategy is better attributed to other causes. We show\nthat for many problems, a strategy that combines least-cost flaw selection with\nthe delay of separable threats will be effective in reducing search-space size,\nand will do so without excessive computational overhead. Although such a\nstrategy thus provides a good default, we also show that certain domain\ncharacteristics may reduce its effectiveness."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9706102v1", 
    "title": "A Complete Classification of Tractability in RCC-5", 
    "arxiv-id": "cs/9706102v1", 
    "author": "T. Drakengren", 
    "publish": "1997-06-01T00:00:00Z", 
    "summary": "We investigate the computational properties of the spatial algebra RCC-5\nwhich is a restricted version of the RCC framework for spatial reasoning. The\nsatisfiability problem for RCC-5 is known to be NP-complete but not much is\nknown about its approximately four billion subclasses. We provide a complete\nclassification of satisfiability for all these subclasses into polynomial and\nNP-complete respectively. In the process, we identify all maximal tractable\nsubalgebras which are four in total."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9707101v1", 
    "title": "A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search   Difficulty", 
    "arxiv-id": "cs/9707101v1", 
    "author": "T. Hogg", 
    "publish": "1997-07-01T00:00:00Z", 
    "summary": "The easy-hard-easy pattern in the difficulty of combinatorial search problems\nas constraints are added has been explained as due to a competition between the\ndecrease in number of solutions and increased pruning. We test the generality\nof this explanation by examining one of its predictions: if the number of\nsolutions is held fixed by the choice of problems, then increased pruning\nshould lead to a monotonic decrease in search cost. Instead, we find the\neasy-hard-easy pattern in median search cost even when the number of solutions\nis held constant, for some search methods. This generalizes previous\nobservations of this pattern and shows that the existing theory does not\nexplain the full range of the peak in search cost. In these cases the pattern\nappears to be due to changes in the size of the minimal unsolvable subproblems,\nrather than changing numbers of solutions."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9707102v1", 
    "title": "Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time", 
    "arxiv-id": "cs/9707102v1", 
    "author": "P. Jonsson", 
    "publish": "1997-07-01T00:00:00Z", 
    "summary": "This paper combines two important directions of research in temporal\nresoning: that of finding maximal tractable subclasses of Allen's interval\nalgebra, and that of reasoning with metric temporal information. Eight new\nmaximal tractable subclasses of Allen's interval algebra are presented, some of\nthem subsuming previously reported tractable algebras. The algebras allow for\nmetric temporal constraints on interval starting or ending points, using the\nrecent framework of Horn DLRs. Two of the algebras can express the notion of\nsequentiality between intervals, being the first such algebras admitting both\nqualitative and metric time."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9707103v1", 
    "title": "Defining Relative Likelihood in Partially-Ordered Preferential   Structures", 
    "arxiv-id": "cs/9707103v1", 
    "author": "J. Y. Halpern", 
    "publish": "1997-07-01T00:00:00Z", 
    "summary": "Starting with a likelihood or preference order on worlds, we extend it to a\nlikelihood ordering on sets of worlds in a natural way, and examine the\nresulting logic. Lewis earlier considered such a notion of relative likelihood\nin the context of studying counterfactuals, but he assumed a total preference\norder on worlds. Complications arise when examining partial orders that are not\npresent for total orders. There are subtleties involving the exact approach to\nlifting the order on worlds to an order on sets of worlds. In addition, the\naxiomatization of the logic of relative likelihood in the case of partial\norders gives insight into the connection between relative likelihood and\ndefault reasoning."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9709101v1", 
    "title": "Towards Flexible Teamwork", 
    "arxiv-id": "cs/9709101v1", 
    "author": "M. Tambe", 
    "publish": "1997-09-01T00:00:00Z", 
    "summary": "Many AI researchers are today striving to build agent teams for complex,\ndynamic multi-agent domains, with intended applications in arenas such as\neducation, training, entertainment, information integration, and collective\nrobotics. Unfortunately, uncertainties in these complex, dynamic domains\nobstruct coherent teamwork. In particular, team members often encounter\ndiffering, incomplete, and possibly inconsistent views of their environment.\nFurthermore, team members can unexpectedly fail in fulfilling responsibilities\nor discover unexpected opportunities. Highly flexible coordination and\ncommunication is key in addressing such uncertainties. Simply fitting\nindividual agents with precomputed coordination plans will not do, for their\ninflexibility can cause severe failures in teamwork, and their\ndomain-specificity hinders reusability. Our central hypothesis is that the key\nto such flexibility and reusability is providing agents with general models of\nteamwork. Agents exploit such models to autonomously reason about coordination\nand communication, providing requisite flexibility. Furthermore, the models\nenable reuse across domains, both saving implementation effort and enforcing\nconsistency. This article presents one general, implemented model of teamwork,\ncalled STEAM. The basic building block of teamwork in STEAM is joint intentions\n(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a\n(partial) hierarchy of joint intentions (this hierarchy is seen to parallel\nGrosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members\nmonitor the team's and individual members' performance, reorganizing the team\nas necessary. Finally, decision-theoretic communication selectivity in STEAM\nensures reduction in communication overheads of teamwork, with appropriate\nsensitivity to the environmental conditions. This article describes STEAM's\napplication in three different complex domains, and presents detailed empirical\nresults."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9709102v1", 
    "title": "Identifying Hierarchical Structure in Sequences: A linear-time algorithm", 
    "arxiv-id": "cs/9709102v1", 
    "author": "I. H. Witten", 
    "publish": "1997-09-01T00:00:00Z", 
    "summary": "SEQUITUR is an algorithm that infers a hierarchical structure from a sequence\nof discrete symbols by replacing repeated phrases with a grammatical rule that\ngenerates the phrase, and continuing this process recursively. The result is a\nhierarchical representation of the original sequence, which offers insights\ninto its lexical structure. The algorithm is driven by two constraints that\nreduce the size of the grammar, and produce structure as a by-product. SEQUITUR\nbreaks new ground by operating incrementally. Moreover, the method's simple\nstructure permits a proof that it operates in space and time that is linear in\nthe size of the input. Our implementation can process 50,000 symbols per second\nand has been applied to an extensive range of real world sequences."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9711102v1", 
    "title": "Storing and Indexing Plan Derivations through Explanation-based Analysis   of Retrieval Failures", 
    "arxiv-id": "cs/9711102v1", 
    "author": "S. Kambhampati", 
    "publish": "1997-11-01T00:00:00Z", 
    "summary": "Case-Based Planning (CBP) provides a way of scaling up domain-independent\nplanning to solve large problems in complex domains. It replaces the detailed\nand lengthy search for a solution with the retrieval and adaptation of previous\nplanning experiences. In general, CBP has been demonstrated to improve\nperformance over generative (from-scratch) planning. However, the performance\nimprovements it provides are dependent on adequate judgements as to problem\nsimilarity. In particular, although CBP may substantially reduce planning\neffort overall, it is subject to a mis-retrieval problem. The success of CBP\ndepends on these retrieval errors being relatively rare. This paper describes\nthe design and implementation of a replay framework for the case-based planner\nDERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating\nexplanation-based learning techniques that allow it to explain and learn from\nthe retrieval failures it encounters. These techniques are used to refine\njudgements about case similarity in response to feedback when a wrong decision\nhas been made. The same failure analysis is used in building the case library,\nthrough the addition of repairing cases. Large problems are split and stored as\nsingle goal subproblems. Multi-goal problems are stored only when these smaller\ncases fail to be merged into a full solution. An empirical evaluation of this\napproach demonstrates the advantage of learning from experienced retrieval\nfailure."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9711103v1", 
    "title": "A Model Approximation Scheme for Planning in Partially Observable   Stochastic Domains", 
    "arxiv-id": "cs/9711103v1", 
    "author": "W. Liu", 
    "publish": "1997-11-01T00:00:00Z", 
    "summary": "Partially observable Markov decision processes (POMDPs) are a natural model\nfor planning problems where effects of actions are nondeterministic and the\nstate of the world is not completely observable. It is difficult to solve\nPOMDPs exactly. This paper proposes a new approximation scheme. The basic idea\nis to transform a POMDP into another one where additional information is\nprovided by an oracle. The oracle informs the planning agent that the current\nstate of the world is in a certain region. The transformed POMDP is\nconsequently said to be region observable. It is easier to solve than the\noriginal POMDP. We propose to solve the transformed POMDP and use its optimal\npolicy to construct an approximate policy for the original POMDP. By\ncontrolling the amount of additional information that the oracle provides, it\nis possible to find a proper tradeoff between computational time and\napproximation quality. In terms of algorithmic contributions, we study in\ndetails how to exploit region observability in solving the transformed POMDP.\nTo facilitate the study, we also propose a new exact algorithm for general\nPOMDPs. The algorithm is conceptually simple and yet is significantly more\nefficient than all previous exact algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9711104v1", 
    "title": "Dynamic Non-Bayesian Decision Making", 
    "arxiv-id": "cs/9711104v1", 
    "author": "M. Tennenholtz", 
    "publish": "1997-11-01T00:00:00Z", 
    "summary": "The model of a non-Bayesian agent who faces a repeated game with incomplete\ninformation against Nature is an appropriate tool for modeling general\nagent-environment interactions. In such a model the environment state\n(controlled by Nature) may change arbitrarily, and the feedback/reward function\nis initially unknown. The agent is not Bayesian, that is he does not form a\nprior probability neither on the state selection strategy of Nature, nor on his\nreward function. A policy for the agent is a function which assigns an action\nto every history of observations and actions. Two basic feedback structures are\nconsidered. In one of them -- the perfect monitoring case -- the agent is able\nto observe the previous environment state as part of his feedback, while in the\nother -- the imperfect monitoring case -- all that is available to the agent is\nthe reward obtained. Both of these settings refer to partially observable\nprocesses, where the current environment state is unknown. Our main result\nrefers to the competitive ratio criterion in the perfect monitoring case. We\nprove the existence of an efficient stochastic policy that ensures that the\ncompetitive ratio is obtained at almost all stages with an arbitrarily high\nprobability, where efficiency is measured in terms of rate of convergence. It\nis further shown that such an optimal policy does not exist in the imperfect\nmonitoring case. Moreover, it is proved that in the perfect monitoring case\nthere does not exist a deterministic policy that satisfies our long run\noptimality criterion. In addition, we discuss the maxmin criterion and prove\nthat a deterministic efficient optimal strategy does exist in the imperfect\nmonitoring case under this criterion. Finally we show that our approach to\nlong-run optimality can be viewed as qualitative, which distinguishes it from\nprevious work in this area."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9712101v1", 
    "title": "When Gravity Fails: Local Search Topology", 
    "arxiv-id": "cs/9712101v1", 
    "author": "J. Stutz", 
    "publish": "1997-12-01T00:00:00Z", 
    "summary": "Local search algorithms for combinatorial search problems frequently\nencounter a sequence of states in which it is impossible to improve the value\nof the objective function; moves through these regions, called plateau moves,\ndominate the time spent in local search. We analyze and characterize plateaus\nfor three different classes of randomly generated Boolean Satisfiability\nproblems. We identify several interesting features of plateaus that impact the\nperformance of local search algorithms. We show that local minima tend to be\nsmall but occasionally may be very large. We also show that local minima can be\nescaped without unsatisfying a large number of clauses, but that systematically\nsearching for an escape route may be computationally expensive if the local\nminimum is large. We show that plateaus with exits, called benches, tend to be\nmuch larger than minima, and that some benches have very few exit states which\nlocal search can use to escape. We show that the solutions (i.e., global\nminima) of randomly generated problem instances form clusters, which behave\nsimilarly to local minima. We revisit several enhancements of local search\nalgorithms and explain their performance in light of our results. Finally we\ndiscuss strategies for creating the next generation of local search algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9712102v1", 
    "title": "Bidirectional Heuristic Search Reconsidered", 
    "arxiv-id": "cs/9712102v1", 
    "author": "G. Kainz", 
    "publish": "1997-12-01T00:00:00Z", 
    "summary": "The assessment of bidirectional heuristic search has been incorrect since it\nwas first published more than a quarter of a century ago. For quite a long\ntime, this search strategy did not achieve the expected results, and there was\na major misunderstanding about the reasons behind it. Although there is still\nwide-spread belief that bidirectional heuristic search is afflicted by the\nproblem of search frontiers passing each other, we demonstrate that this\nconjecture is wrong. Based on this finding, we present both a new generic\napproach to bidirectional heuristic search and a new approach to dynamically\nimproving heuristic values that is feasible in bidirectional search only. These\napproaches are put into perspective with both the traditional and more recently\nproposed approaches in order to facilitate a better overall understanding.\nEmpirical results of experiments with our new approaches show that\nbidirectional heuristic search can be performed very efficiently and also with\nlimited memory. These results suggest that bidirectional heuristic search\nappears to be better for solving certain difficult problems than corresponding\nunidirectional search. This provides some evidence for the usefulness of a\nsearch strategy that was long neglected. In summary, we show that bidirectional\nheuristic search is viable and consequently propose that it be reconsidered."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9801101v1", 
    "title": "Incremental Recompilation of Knowledge", 
    "arxiv-id": "cs/9801101v1", 
    "author": "M. Sideri", 
    "publish": "1998-01-01T00:00:00Z", 
    "summary": "Approximating a general formula from above and below by Horn formulas (its\nHorn envelope and Horn core, respectively) was proposed by Selman and Kautz\n(1991, 1996) as a form of ``knowledge compilation,'' supporting rapid\napproximate reasoning; on the negative side, this scheme is static in that it\nsupports no updates, and has certain complexity drawbacks pointed out by\nKavvadias, Papadimitriou and Sideri (1993). On the other hand, the many\nframeworks and schemes proposed in the literature for theory update and\nrevision are plagued by serious complexity-theoretic impediments, even in the\nHorn case, as was pointed out by Eiter and Gottlob (1992), and is further\ndemonstrated in the present paper. More fundamentally, these schemes are not\ninductive, in that they may lose in a single update any positive properties of\nthe represented sets of formulas (small size, Horn structure, etc.). In this\npaper we propose a new scheme, incremental recompilation, which combines Horn\napproximation and model-based updates; this scheme is inductive and very\nefficient, free of the problems facing its constituents. A set of formulas is\nrepresented by an upper and lower Horn approximation. To update, we replace the\nupper Horn formula by the Horn envelope of its minimum-change update, and\nsimilarly the lower one by the Horn core of its update; the key fact which\nenables this scheme is that Horn envelopes and cores are easy to compute when\nthe underlying formula is the result of a minimum-change update of a Horn\nformula by a clause. We conjecture that efficient algorithms are possible for\nmore complex updates."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9801102v1", 
    "title": "Monotonicity and Persistence in Preferential Logics", 
    "arxiv-id": "cs/9801102v1", 
    "author": "J. Engelfriet", 
    "publish": "1998-01-01T00:00:00Z", 
    "summary": "An important characteristic of many logics for Artificial Intelligence is\ntheir nonmonotonicity. This means that adding a formula to the premises can\ninvalidate some of the consequences. There may, however, exist formulae that\ncan always be safely added to the premises without destroying any of the\nconsequences: we say they respect monotonicity. Also, there may be formulae\nthat, when they are a consequence, can not be invalidated when adding any\nformula to the premises: we call them conservative. We study these two classes\nof formulae for preferential logics, and show that they are closely linked to\nthe formulae whose truth-value is preserved along the (preferential) ordering.\nWe will consider some preferential logics for illustration, and prove syntactic\ncharacterization results for them. The results in this paper may improve the\nefficiency of theorem provers for preferential logics."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9803101v1", 
    "title": "Synthesizing Customized Planners from Specifications", 
    "arxiv-id": "cs/9803101v1", 
    "author": "S. Kambhampati", 
    "publish": "1998-03-01T00:00:00Z", 
    "summary": "Existing plan synthesis approaches in artificial intelligence fall into two\ncategories -- domain independent and domain dependent. The domain independent\napproaches are applicable across a variety of domains, but may not be very\nefficient in any one given domain. The domain dependent approaches need to be\n(re)designed for each domain separately, but can be very efficient in the\ndomain for which they are designed. One enticing alternative to these\napproaches is to automatically synthesize domain independent planners given the\nknowledge about the domain and the theory of planning. In this paper, we\ninvestigate the feasibility of using existing automated software synthesis\ntools to support such synthesis. Specifically, we describe an architecture\ncalled CLAY in which the Kestrel Interactive Development System (KIDS) is used\nto derive a domain-customized planner through a semi-automatic combination of a\ndeclarative theory of planning, and the declarative control knowledge specific\nto a given domain, to semi-automatically combine them to derive\ndomain-customized planners. We discuss what it means to write a declarative\ntheory of planning and control knowledge for KIDS, and illustrate our approach\nby generating a class of domain-specific planners using state space\nrefinements. Our experiments show that the synthesized planners can outperform\nclassical refinement planners (implemented as instantiations of UCP,\nKambhampati & Srivastava, 1995), using the same control knowledge. We will\ncontrast the costs and benefits of the synthesis approach with conventional\nmethods for customizing domain independent planners."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9803102v1", 
    "title": "Cached Sufficient Statistics for Efficient Machine Learning with Large   Datasets", 
    "arxiv-id": "cs/9803102v1", 
    "author": "M. S. Lee", 
    "publish": "1998-03-01T00:00:00Z", 
    "summary": "This paper introduces new algorithms and data structures for quick counting\nfor machine learning datasets. We focus on the counting task of constructing\ncontingency tables, but our approach is also applicable to counting the number\nof records in a dataset that match conjunctive queries. Subject to certain\nassumptions, the costs of these operations can be shown to be independent of\nthe number of records in the dataset and loglinear in the number of non-zero\nentries in the contingency table. We provide a very sparse data structure, the\nADtree, to minimize memory use. We provide analytical worst-case bounds for\nthis structure for several models of data distribution. We empirically\ndemonstrate that tractably-sized data structures can be produced for large\nreal-world datasets by (a) using a sparse tree structure that never allocates\nmemory for counts of zero, (b) never allocating memory for counts that can be\ndeduced from other counts, and (c) not bothering to expand the tree fully near\nits leaves. We show how the ADtree can be used to accelerate Bayes net\nstructure finding algorithms, rule learning algorithms, and feature selection\nalgorithms, and we provide a number of empirical results comparing ADtree\nmethods against traditional direct counting approaches. We also discuss the\npossible uses of ADtrees in other machine learning methods, and discuss the\nmerits of ADtrees in comparison with alternative representations such as\nkd-trees, R-trees and Frequent Sets."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9803103v1", 
    "title": "Tractability of Theory Patching", 
    "arxiv-id": "cs/9803103v1", 
    "author": "M. Koppel", 
    "publish": "1998-03-01T00:00:00Z", 
    "summary": "In this paper we consider the problem of `theory patching', in which we are\ngiven a domain theory, some of whose components are indicated to be possibly\nflawed, and a set of labeled training examples for the domain concept. The\ntheory patching problem is to revise only the indicated components of the\ntheory, such that the resulting theory correctly classifies all the training\nexamples. Theory patching is thus a type of theory revision in which revisions\nare made to individual components of the theory. Our concern in this paper is\nto determine for which classes of logical domain theories the theory patching\nproblem is tractable. We consider both propositional and first-order domain\ntheories, and show that the theory patching problem is equivalent to that of\ndetermining what information contained in a theory is `stable' regardless of\nwhat revisions might be performed to the theory. We show that determining\nstability is tractable if the input theory satisfies two conditions: that\nrevisions to each theory component have monotonic effects on the classification\nof examples, and that theory components act independently in the classification\nof examples in the theory. We also show how the concepts introduced can be used\nto determine the soundness and completeness of particular theory patching\nalgorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9805101v1", 
    "title": "Integrative Windowing", 
    "arxiv-id": "cs/9805101v1", 
    "author": "J. Fuernkranz", 
    "publish": "1998-05-01T00:00:00Z", 
    "summary": "In this paper we re-investigate windowing for rule learning algorithms. We\nshow that, contrary to previous results for decision tree learning, windowing\ncan in fact achieve significant run-time gains in noise-free domains and\nexplain the different behavior of rule learning algorithms by the fact that\nthey learn each rule independently. The main contribution of this paper is\nintegrative windowing, a new type of algorithm that further exploits this\nproperty by integrating good rules into the final theory right after they have\nbeen discovered. Thus it avoids re-learning these rules in subsequent\niterations of the windowing process. Experimental evidence in a variety of\nnoise-free domains shows that integrative windowing can in fact achieve\nsubstantial run-time gains. Furthermore, we discuss the problem of noise in\nwindowing and present an algorithm that is able to achieve run-time gains in a\nset of experiments in a simple domain with artificial noise."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9806101v1", 
    "title": "Model-Based Diagnosis using Structured System Descriptions", 
    "arxiv-id": "cs/9806101v1", 
    "author": "A. Darwiche", 
    "publish": "1998-06-01T00:00:00Z", 
    "summary": "This paper presents a comprehensive approach for model-based diagnosis which\nincludes proposals for characterizing and computing preferred diagnoses,\nassuming that the system description is augmented with a system structure (a\ndirected graph explicating the interconnections between system components).\nSpecifically, we first introduce the notion of a consequence, which is a\nsyntactically unconstrained propositional sentence that characterizes all\nconsistency-based diagnoses and show that standard characterizations of\ndiagnoses, such as minimal conflicts, correspond to syntactic variations on a\nconsequence. Second, we propose a new syntactic variation on the consequence\nknown as negation normal form (NNF) and discuss its merits compared to standard\nvariations. Third, we introduce a basic algorithm for computing consequences in\nNNF given a structured system description. We show that if the system structure\ndoes not contain cycles, then there is always a linear-size consequence in NNF\nwhich can be computed in linear time. For arbitrary system structures, we show\na precise connection between the complexity of computing consequences and the\ntopology of the underlying system structure. Finally, we present an algorithm\nthat enumerates the preferred diagnoses characterized by a consequence. The\nalgorithm is shown to take linear time in the size of the consequence if the\npreference criterion satisfies some general conditions."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9806102v1", 
    "title": "A Selective Macro-learning Algorithm and its Application to the NxN   Sliding-Tile Puzzle", 
    "arxiv-id": "cs/9806102v1", 
    "author": "S. Markovitch", 
    "publish": "1998-06-01T00:00:00Z", 
    "summary": "One of the most common mechanisms used for speeding up problem solvers is\nmacro-learning. Macros are sequences of basic operators acquired during problem\nsolving. Macros are used by the problem solver as if they were basic operators.\nThe major problem that macro-learning presents is the vast number of macros\nthat are available for acquisition. Macros increase the branching factor of the\nsearch space and can severely degrade problem-solving efficiency. To make macro\nlearning useful, a program must be selective in acquiring and utilizing macros.\nThis paper describes a general method for selective acquisition of macros.\nSolvable training problems are generated in increasing order of difficulty. The\nonly macros acquired are those that take the problem solver out of a local\nminimum to a better state. The utility of the method is demonstrated in several\ndomains, including the domain of NxN sliding-tile puzzles. After learning on\nsmall puzzles, the system is able to efficiently solve puzzles of any size."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9808101v1", 
    "title": "The Computational Complexity of Probabilistic Planning", 
    "arxiv-id": "cs/9808101v1", 
    "author": "M. Mundhenk", 
    "publish": "1998-08-01T00:00:00Z", 
    "summary": "We examine the computational complexity of testing and finding small plans in\nprobabilistic planning domains with both flat and propositional\nrepresentations. The complexity of plan evaluation and existence varies with\nthe plan type sought; we examine totally ordered plans, acyclic plans, and\nlooping plans, and partially ordered plans under three natural definitions of\nplan value. We show that problems of interest are complete for a variety of\ncomplexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the\nprocess of proving that certain planning problems are complete for NP^PP, we\nintroduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the\nstandard Boolean satisfiability problem to computations involving probabilistic\nquantities; our results suggest that the development of good heuristics for\nE-MAJSAT could be important for the creation of efficient algorithms for a wide\nvariety of problems."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9810016v1", 
    "title": "SYNERGY: A Linear Planner Based on Genetic Programming", 
    "arxiv-id": "cs/9810016v1", 
    "author": "Ion Muslea", 
    "publish": "1998-10-16T22:11:35Z", 
    "summary": "In this paper we describe SYNERGY, which is a highly parallelizable, linear\nplanning system that is based on the genetic programming paradigm. Rather than\nreasoning about the world it is planning for, SYNERGY uses artificial\nselection, recombination and fitness measure to generate linear plans that\nsolve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase\nproblem and a few variants of the robot navigation problem), and the\nexperimental results show that our planner is capable of handling problem\ninstances that are one to two orders of magnitude larger than the ones solved\nby UCPOP. In order to facilitate the search reduction and to enhance the\nexpressive power of SYNERGY, we also propose two major extensions to our\nplanning system: a formalism for using hierarchical planning operators, and a\nframework for planning in dynamic environments."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9811024v1", 
    "title": "The Essence of Constraint Propagation", 
    "arxiv-id": "cs/9811024v1", 
    "author": "Krzysztof R. Apt", 
    "publish": "1998-11-13T13:04:02Z", 
    "summary": "We show that several constraint propagation algorithms (also called (local)\nconsistency, consistency enforcing, Waltz, filtering or narrowing algorithms)\nare instances of algorithms that deal with chaotic iteration. To this end we\npropose a simple abstract framework that allows us to classify and compare\nthese algorithms and to establish in a uniform way their basic properties."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9812010v1", 
    "title": "Towards a computational theory of human daydreaming", 
    "arxiv-id": "cs/9812010v1", 
    "author": "Michael G. Dyer", 
    "publish": "1998-12-10T16:29:07Z", 
    "summary": "This paper examines the phenomenon of daydreaming: spontaneously recalling or\nimagining personal or vicarious experiences in the past or future. The\nfollowing important roles of daydreaming in human cognition are postulated:\nplan preparation and rehearsal, learning from failures and successes, support\nfor processes of creativity, emotion regulation, and motivation.\n  A computational theory of daydreaming and its implementation as the program\nDAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based\non relaxed planning, 2) a dynamic episodic memory of experiences used by the\nscenario generator, 3) a collection of personal goals and control goals which\nguide the scenario generator, 4) an emotion component in which daydreams\ninitiate, and are initiated by, emotional states arising from goal outcomes,\nand 5) domain knowledge of interpersonal relations and common everyday\noccurrences.\n  The role of emotions and control goals in daydreaming is discussed. Four\ncontrol goals commonly used in guiding daydreaming are presented:\nrationalization, failure/success reversal, revenge, and preparation. The role\nof episodic memory in daydreaming is considered, including how daydreamed\ninformation is incorporated into memory and later used. An initial version of\nDAYDREAMER which produces several daydreams (in English) is currently running."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9812017v1", 
    "title": "A reusable iterative optimization software library to solve   combinatorial problems with approximate reasoning", 
    "arxiv-id": "cs/9812017v1", 
    "author": "Wolfgang Slany", 
    "publish": "1998-12-15T21:45:15Z", 
    "summary": "Real world combinatorial optimization problems such as scheduling are\ntypically too complex to solve with exact methods. Additionally, the problems\noften have to observe vaguely specified constraints of different importance,\nthe available data may be uncertain, and compromises between antagonistic\ncriteria may be necessary. We present a combination of approximate reasoning\nbased constraints and iterative optimization based heuristics that help to\nmodel and solve such problems in a framework of C++ software libraries called\nStarFLIP++. While initially developed to schedule continuous caster units in\nsteel plants, we present in this paper results from reusing the library\ncomponents in a shift scheduling system for the workforce of an industrial\nproduction plant."
},{
    "category": "cs.AI", 
    "doi": "10.1088/0957-0233/23/11/114010", 
    "link": "http://arxiv.org/pdf/cs/9903016v1", 
    "title": "Modeling Belief in Dynamic Systems, Part II: Revision and Update", 
    "arxiv-id": "cs/9903016v1", 
    "author": "J. Y. Halpern", 
    "publish": "1999-03-24T00:22:01Z", 
    "summary": "The study of belief change has been an active area in philosophy and AI. In\nrecent years two special cases of belief change, belief revision and belief\nupdate, have been studied in detail. In a companion paper (Friedman & Halpern,\n1997), we introduce a new framework to model belief change. This framework\ncombines temporal and epistemic modalities with a notion of plausibility,\nallowing us to examine the change of beliefs over time. In this paper, we show\nhow belief revision and belief update can be captured in our framework. This\nallows us to compare the assumptions made by each method, and to better\nunderstand the principles underlying them. In particular, it shows that Katsuno\nand Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on\nseveral strong assumptions that may limit its applicability in artificial\nintelligence. Finally, our analysis allow us to identify a notion of minimal\nchange that underlies a broad range of belief change operations including\nrevision and update."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/9906002v1", 
    "title": "The Symbol Grounding Problem", 
    "arxiv-id": "cs/9906002v1", 
    "author": "Stevan Harnad", 
    "publish": "1999-06-01T19:57:24Z", 
    "summary": "How can the semantic interpretation of a formal symbol system be made\nintrinsic to the system, rather than just parasitic on the meanings in our\nheads? How can the meanings of the meaningless symbol tokens, manipulated\nsolely on the basis of their (arbitrary) shapes, be grounded in anything but\nother meaningless symbols? The problem is analogous to trying to learn Chinese\nfrom a Chinese/Chinese dictionary alone. A candidate solution is sketched:\nSymbolic representations must be grounded bottom-up in nonsymbolic\nrepresentations of two kinds: (1) \"iconic representations,\" which are analogs\nof the proximal sensory projections of distal objects and events, and (2)\n\"categorical representations,\" which are learned and innate feature-detectors\nthat pick out the invariant features of object and event categories from their\nsensory projections. Elementary symbols are the names of these object and event\ncategories, assigned on the basis of their (nonsymbolic) categorical\nrepresentations. Higher-order (3) \"symbolic representations,\" grounded in these\nelementary symbols, consist of symbol strings describing category membership\nrelations (e.g., \"An X is a Y that is Z\")."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/9909003v1", 
    "title": "Iterative Deepening Branch and Bound", 
    "arxiv-id": "cs/9909003v1", 
    "author": "R. N. Behera", 
    "publish": "1999-09-03T10:31:46Z", 
    "summary": "In tree search problem the best-first search algorithm needs too much of\nspace . To remove such drawbacks of these algorithms the IDA* was developed\nwhich is both space and time cost efficient. But again IDA* can give an optimal\nsolution for real valued problems like Flow shop scheduling, Travelling\nSalesman and 0/1 Knapsack due to their real valued cost estimates. Thus further\nmodifications are done on it and the Iterative Deepening Branch and Bound\nSearch Algorithms is developed which meets the requirements. We have tried\nusing this algorithm for the Flow Shop Scheduling Problem and have found that\nit is quite effective."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/9910016v1", 
    "title": "Probabilistic Agent Programs", 
    "arxiv-id": "cs/9910016v1", 
    "author": "VS Subrahmanian", 
    "publish": "1999-10-21T09:35:38Z", 
    "summary": "Agents are small programs that autonomously take actions based on changes in\ntheir environment or ``state.'' Over the last few years, there have been an\nincreasing number of efforts to build agents that can interact and/or\ncollaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd\nPick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top\nof legacy code. However, their framework assumes that agent states are\ncompletely determined, and there is no uncertainty in an agent's state. Thus,\ntheir framework allows an agent developer to specify how his agents will react\nwhen the agent is 100% sure about what is true/false in the world state. In\nthis paper, we propose the concept of a \\emph{probabilistic agent program} and\nshow how, given an arbitrary program written in any imperative language, we may\nbuild a declarative ``probabilistic'' agent program on top of it which supports\ndecision making in the presence of uncertainty. We provide two alternative\nsemantics for probabilistic agent programs. We show that the second semantics,\nthough more epistemically appealing, is more complex to compute. We provide\nsound and complete algorithms to compute the semantics of \\emph{positive} agent\nprograms."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/9911012v2", 
    "title": "Cox's Theorem Revisited", 
    "arxiv-id": "cs/9911012v2", 
    "author": "Joseph Y. Halpern", 
    "publish": "1999-11-27T17:57:17Z", 
    "summary": "The assumptions needed to prove Cox's Theorem are discussed and examined.\nVarious sets of assumptions under which a Cox-style theorem can be proved are\nprovided, although all are rather strong and, arguably, not natural."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0002002v1", 
    "title": "Uniform semantic treatment of default and autoepistemic logics", 
    "arxiv-id": "cs/0002002v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2000-02-03T21:44:57Z", 
    "summary": "We revisit the issue of connections between two leading formalisms in\nnonmonotonic reasoning: autoepistemic logic and default logic. For each logic\nwe develop a comprehensive semantic framework based on the notion of a belief\npair. The set of all belief pairs together with the so called knowledge\nordering forms a complete lattice. For each logic, we introduce several\nsemantics by means of fixpoints of operators on the lattice of belief pairs.\nOur results elucidate an underlying isomorphism of the respective semantic\nconstructions. In particular, we show that the interpretation of defaults as\nmodal formulas proposed by Konolige allows us to represent all semantics for\ndefault logic in terms of the corresponding semantics for autoepistemic logic.\nThus, our results conclusively establish that default logic can indeed be\nviewed as a fragment of autoepistemic logic. However, as we also demonstrate,\nthe semantics of Moore and Reiter are given by different operators and occupy\ndifferent locations in their corresponding families of semantics. This result\nexplains the source of the longstanding difficulty to formally relate these two\nsemantics. In the paper, we also discuss approximating skeptical reasoning with\nautoepistemic and default logics and establish constructive principles behind\nsuch approximations."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0002003v1", 
    "title": "On the accuracy and running time of GSAT", 
    "arxiv-id": "cs/0002003v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2000-02-04T12:53:57Z", 
    "summary": "Randomized algorithms for deciding satisfiability were shown to be effective\nin solving problems with thousands of variables. However, these algorithms are\nnot complete. That is, they provide no guarantee that a satisfying assignment,\nif one exists, will be found. Thus, when studying randomized algorithms, there\nare two important characteristics that need to be considered: the running time\nand, even more importantly, the accuracy --- a measure of likelihood that a\nsatisfying assignment will be found, provided one exists. In fact, we argue\nthat without a reference to the accuracy, the notion of the running time for\nrandomized algorithms is not well-defined. In this paper, we introduce a formal\nnotion of accuracy. We use it to define a concept of the running time. We use\nboth notions to study the random walk strategy GSAT algorithm. We investigate\nthe dependence of accuracy on properties of input formulas such as\nclause-to-variable ratio and the number of satisfying assignments. We\ndemonstrate that the running time of GSAT grows exponentially in the number of\nvariables of the input formula for randomly generated 3-CNF formulas and for\nthe formulas encoding 3- and 4-colorability of graphs."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0002009v1", 
    "title": "Syntactic Autonomy: Why There is no Autonomy without Symbols and How   Self-Organization Might Evolve Them", 
    "arxiv-id": "cs/0002009v1", 
    "author": "Luis M. Rocha", 
    "publish": "2000-02-16T18:09:20Z", 
    "summary": "Two different types of agency are discussed based on dynamically coherent and\nincoherent couplings with an environment respectively. I propose that until a\nprivate syntax (syntactic autonomy) is discovered by dynamically coherent\nagents, there are no significant or interesting types of closure or autonomy.\nWhen syntactic autonomy is established, then, because of a process of\ndescription-based selected self-organization, open-ended evolution is enabled.\nAt this stage, agents depend, in addition to dynamics, on localized, symbolic\nmemory, thus adding a level of dynamical incoherence to their interaction with\nthe environment. Furthermore, it is the appearance of syntactic autonomy which\nenables much more interesting types of closures amongst agents which share the\nsame syntax. To investigate how we can study the emergence of syntax from\ndynamical systems, experiments with cellular automata leading to emergent\ncomputation to solve non-trivial tasks are discussed. RNA editing is also\nmentioned as a process that may have been used to obtain a primordial\nbiological code necessary open-ended evolution."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003008v1", 
    "title": "Consistency Management of Normal Logic Program by Top-down Abductive   Proof Procedure", 
    "arxiv-id": "cs/0003008v1", 
    "author": "Ken Satoh", 
    "publish": "2000-03-05T10:29:03Z", 
    "summary": "This paper presents a method of computing a revision of a function-free\nnormal logic program. If an added rule is inconsistent with a program, that is,\nif it leads to a situation such that no stable model exists for a new program,\nthen deletion and addition of rules are performed to avoid inconsistency. We\nspecify a revision by translating a normal logic program into an abductive\nlogic program with abducibles to represent deletion and addition of rules. To\ncompute such deletion and addition, we propose an adaptation of our top-down\nabductive proof procedure to compute a relevant abducibles to an added rule. We\ncompute a minimally revised program, by choosing a minimal set of abducibles\namong all the sets of abducibles computed by a top-down proof procedure."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003012v1", 
    "title": "Defeasible Reasoning in OSCAR", 
    "arxiv-id": "cs/0003012v1", 
    "author": "John L. Pollock", 
    "publish": "2000-03-06T22:23:00Z", 
    "summary": "This is a system description for the OSCAR defeasible reasoner."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003016v1", 
    "title": "Abductive and Consistency-Based Diagnosis Revisited: a Modeling   Perspective", 
    "arxiv-id": "cs/0003016v1", 
    "author": "Daniele Theseider Dupre'", 
    "publish": "2000-03-07T11:39:53Z", 
    "summary": "Diagnostic reasoning has been characterized logically as consistency-based\nreasoning or abductive reasoning. Previous analyses in the literature have\nshown, on the one hand, that choosing the (in general more restrictive)\nabductive definition may be appropriate or not, depending on the content of the\nknowledge base [Console&Torasso91], and, on the other hand, that, depending on\nthe choice of the definition the same knowledge should be expressed in\ndifferent form [Poole94].\n  Since in Model-Based Diagnosis a major problem is finding the right way of\nabstracting the behavior of the system to be modeled, this paper discusses the\nrelation between modeling, and in particular abstraction in the model, and the\nnotion of diagnosis."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003020v2", 
    "title": "ACLP: Integrating Abduction and Constraint Solving", 
    "arxiv-id": "cs/0003020v2", 
    "author": "Antonis Kakas", 
    "publish": "2000-03-07T22:47:13Z", 
    "summary": "ACLP is a system which combines abductive reasoning and constraint solving by\nintegrating the frameworks of Abductive Logic Programming (ALP) and Constraint\nLogic Programming (CLP). It forms a general high-level knowledge representation\nenvironment for abductive problems in Artificial Intelligence and other areas.\nIn ACLP, the task of abduction is supported and enhanced by its non-trivial\nintegration with constraint solving facilitating its application to complex\nproblems. The ACLP system is currently implemented on top of the CLP language\nof ECLiPSe as a meta-interpreter exploiting its underlying constraint solver\nfor finite domains. It has been applied to the problems of planning and\nscheduling in order to test its computational effectiveness compared with the\ndirect use of the (lower level) constraint solving framework of CLP on which it\nis built. These experiments provide evidence that the abductive framework of\nACLP does not compromise significantly the computational efficiency of the\nsolutions. Other experiments show the natural ability of ACLP to accommodate\neasily and in a robust way new or changing requirements of the original\nproblem."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003021v1", 
    "title": "Relevance Sensitive Non-Monotonic Inference on Belief Sequences", 
    "arxiv-id": "cs/0003021v1", 
    "author": "Rohit Parikh", 
    "publish": "2000-03-08T03:03:36Z", 
    "summary": "We present a method for relevance sensitive non-monotonic inference from\nbelief sequences which incorporates insights pertaining to prioritized\ninference and relevance sensitive, inconsistency tolerant belief revision.\n  Our model uses a finite, logically open sequence of propositional formulas as\na representation for beliefs and defines a notion of inference from\nmaxiconsistent subsets of formulas guided by two orderings: a temporal\nsequencing and an ordering based on relevance relations between the conclusion\nand formulas in the sequence. The relevance relations are ternary (using\ncontext as a parameter) as opposed to standard binary axiomatizations. The\ninference operation thus defined easily handles iterated revision by\nmaintaining a revision history, blocks the derivation of inconsistent answers\nfrom a possibly inconsistent sequence and maintains the distinction between\nexplicit and implicit beliefs. In doing so, it provides a finitely presented\nformalism and a plausible model of reasoning for automated agents."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003023v1", 
    "title": "Probabilistic Default Reasoning with Conditional Constraints", 
    "arxiv-id": "cs/0003023v1", 
    "author": "Thomas Lukasiewicz", 
    "publish": "2000-03-08T11:05:45Z", 
    "summary": "We propose a combination of probabilistic reasoning from conditional\nconstraints with approaches to default reasoning from conditional knowledge\nbases. In detail, we generalize the notions of Pearl's entailment in system Z,\nLehmann's lexicographic entailment, and Geffner's conditional entailment to\nconditional constraints. We give some examples that show that the new notions\nof z-, lexicographic, and conditional entailment have similar properties like\ntheir classical counterparts. Moreover, we show that the new notions of z-,\nlexicographic, and conditional entailment are proper generalizations of both\ntheir classical counterparts and the classical notion of logical entailment for\nconditional constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003024v1", 
    "title": "A Compiler for Ordered Logic Programs", 
    "arxiv-id": "cs/0003024v1", 
    "author": "Hans Tompits", 
    "publish": "2000-03-08T10:15:51Z", 
    "summary": "This paper describes a system, called PLP, for compiling ordered logic\nprograms into standard logic programs under the answer set semantics. In an\nordered logic program, rules are named by unique terms, and preferences among\nrules are given by a set of dedicated atoms. An ordered logic program is\ntransformed into a second, regular, extended logic program wherein the\npreferences are respected, in that the answer sets obtained in the transformed\ntheory correspond with the preferred answer sets of the original theory. Since\nthe result of the translation is an extended logic program, existing logic\nprogramming systems can be used as underlying reasoning engine. In particular,\nPLP is conceived as a front-end to the logic programming systems dlv and\nsmodels."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003027v1", 
    "title": "SLDNFA-system", 
    "arxiv-id": "cs/0003027v1", 
    "author": "Bert Van Nuffelen", 
    "publish": "2000-03-08T13:22:44Z", 
    "summary": "The SLDNFA-system results from the LP+ project at the K.U.Leuven, which\ninvestigates logics and proof procedures for these logics for declarative\nknowledge representation. Within this project inductive definition logic\n(ID-logic) is used as representation logic. Different solvers are being\ndeveloped for this logic and one of these is SLDNFA. A prototype of the system\nis available and used for investigating how to solve efficiently problems\nrepresented in ID-logic."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003028v1", 
    "title": "Logic Programs with Compiled Preferences", 
    "arxiv-id": "cs/0003028v1", 
    "author": "Hans Tompits", 
    "publish": "2000-03-08T14:09:56Z", 
    "summary": "We describe an approach for compiling preferences into logic programs under\nthe answer set semantics. An ordered logic program is an extended logic program\nin which rules are named by unique terms, and in which preferences among rules\nare given by a set of dedicated atoms. An ordered logic program is transformed\ninto a second, regular, extended logic program wherein the preferences are\nrespected, in that the answer sets obtained in the transformed theory\ncorrespond with the preferred answer sets of the original theory. Our approach\nallows both the specification of static orderings (as found in most previous\nwork), in which preferences are external to a logic program, as well as\norderings on sets of rules. In large part then, we are interested in describing\na general methodology for uniformly incorporating preference information in a\nlogic program. Since the result of our translation is an extended logic\nprogram, we can make use of existing implementations, such as dlv and smodels.\nTo this end, we have developed a compiler, available on the web, as a front-end\nfor these programming systems."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003029v1", 
    "title": "Fuzzy Approaches to Abductive Inference", 
    "arxiv-id": "cs/0003029v1", 
    "author": "Bernadette Bouchon-Meunier", 
    "publish": "2000-03-08T14:56:58Z", 
    "summary": "This paper proposes two kinds of fuzzy abductive inference in the framework\nof fuzzy rule base. The abductive inference processes described here depend on\nthe semantic of the rule. We distinguish two classes of interpretation of a\nfuzzy rule, certainty generation rules and possible generation rules. In this\npaper we present the architecture of abductive inference in the first class of\ninterpretation. We give two kinds of problem that we can resolve by using the\nproposed models of inference."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003030v1", 
    "title": "Problem solving in ID-logic with aggregates: some experiments", 
    "arxiv-id": "cs/0003030v1", 
    "author": "Marc Denecker", 
    "publish": "2000-03-08T15:39:14Z", 
    "summary": "The goal of the LP+ project at the K.U.Leuven is to design an expressive\nlogic, suitable for declarative knowledge representation, and to develop\nintelligent systems based on Logic Programming technology for solving\ncomputational problems using the declarative specifications. The ID-logic is an\nintegration of typed classical logic and a definition logic. Different\nabductive solvers for this language are being developed. This paper is a report\nof the integration of high order aggregates into ID-logic and the consequences\non the solver SLDNFA."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003031v1", 
    "title": "Optimal Belief Revision", 
    "arxiv-id": "cs/0003031v1", 
    "author": "Robert E. Mercer", 
    "publish": "2000-03-08T15:54:50Z", 
    "summary": "We propose a new approach to belief revision that provides a way to change\nknowledge bases with a minimum of effort. We call this way of revising belief\nstates optimal belief revision. Our revision method gives special attention to\nthe fact that most belief revision processes are directed to a specific\ninformational objective. This approach to belief change is founded on notions\nsuch as optimal context and accessibility. For the sentential model of belief\nstates we provide both a formal description of contexts as sub-theories\ndetermined by three parameters and a method to construct contexts. Next, we\nintroduce an accessibility ordering for belief sets, which we then use for\nselecting the best (optimal) contexts with respect to the processing effort\ninvolved in the revision. Then, for finitely axiomatizable knowledge bases, we\ncharacterize a finite accessibility ranking from which the accessibility\nordering for the entire base is generated and show how to determine the ranking\nof an arbitrary sentence in the language. Finally, we define the adjustment of\nthe accessibility ranking of a revised base of a belief set."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003032v1", 
    "title": "cc-Golog: Towards More Realistic Logic-Based Robot Controllers", 
    "arxiv-id": "cs/0003032v1", 
    "author": "Gerhard Lakemeyer", 
    "publish": "2000-03-08T16:14:08Z", 
    "summary": "High-level robot controllers in realistic domains typically deal with\nprocesses which operate concurrently, change the world continuously, and where\nthe execution of actions is event-driven as in ``charge the batteries as soon\nas the voltage level is low''. While non-logic-based robot control languages\nare well suited to express such scenarios, they fare poorly when it comes to\nprojecting, in a conspicuous way, how the world evolves when actions are\nexecuted. On the other hand, a logic-based control language like \\congolog,\nbased on the situation calculus, is well-suited for the latter. However, it has\nproblems expressing event-driven behavior. In this paper, we show how these\nproblems can be overcome by first extending the situation calculus to support\ncontinuous change and event-driven behavior and then presenting \\ccgolog, a\nvariant of \\congolog which is based on the extended situation calculus. One\nbenefit of \\ccgolog is that it narrows the gap in expressiveness compared to\nnon-logic-based control languages while preserving a semantically well-founded\nprojection mechanism."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003033v1", 
    "title": "Smodels: A System for Answer Set Programming", 
    "arxiv-id": "cs/0003033v1", 
    "author": "Tommi Syrjanen", 
    "publish": "2000-03-08T23:25:51Z", 
    "summary": "The Smodels system implements the stable model semantics for normal logic\nprograms. It handles a subclass of programs which contain no function symbols\nand are domain-restricted but supports extensions including built-in functions\nas well as cardinality and weight constraints. On top of this core engine more\ninvolved systems can be built. As an example, we have implemented total and\npartial stable model computation for disjunctive logic programs. An interesting\napplication method is based on answer set programming, i.e., encoding an\napplication problem as a set of rules so that its solutions are captured by the\nstable models of the rules. Smodels has been applied to a number of areas\nincluding planning, model checking, reachability analysis, product\nconfiguration, dynamic constraint satisfaction, and feature interaction."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003034v2", 
    "title": "E-RES: A System for Reasoning about Actions, Events and Observations", 
    "arxiv-id": "cs/0003034v2", 
    "author": "Francesca Toni", 
    "publish": "2000-03-08T16:18:52Z", 
    "summary": "E-RES is a system that implements the Language E, a logic for reasoning about\nnarratives of action occurrences and observations. E's semantics is\nmodel-theoretic, but this implementation is based on a sound and complete\nreformulation of E in terms of argumentation, and uses general computational\ntechniques of argumentation frameworks. The system derives sceptical\nnon-monotonic consequences of a given reformulated theory which exactly\ncorrespond to consequences entailed by E's model-theory. The computation relies\non a complimentary ability of the system to derive credulous non-monotonic\nconsequences together with a set of supporting assumptions which is sufficient\nfor the (credulous) conclusion to hold. E-RES allows theories to contain\ngeneral action laws, statements about action occurrences, observations and\nstatements of ramifications (or universal laws). It is able to derive\nconsequences both forward and backward in time. This paper gives a short\noverview of the theoretical basis of E-RES and illustrates its use on a variety\nof examples. Currently, E-RES is being extended so that the system can be used\nfor planning."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003037v1", 
    "title": "QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks", 
    "arxiv-id": "cs/0003037v1", 
    "author": "Stefan Woltran", 
    "publish": "2000-03-08T17:18:08Z", 
    "summary": "In this paper, we outline the prototype of an automated inference tool,\ncalled QUIP, which provides a uniform implementation for several nonmonotonic\nreasoning formalisms. The theoretical basis of QUIP is derived from well-known\nresults about the computational complexity of nonmonotonic logics and exploits\na representation of the different reasoning tasks in terms of quantified\nboolean formulae."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003038v1", 
    "title": "A Splitting Set Theorem for Epistemic Specifications", 
    "arxiv-id": "cs/0003038v1", 
    "author": "Richard Watson", 
    "publish": "2000-03-08T20:40:31Z", 
    "summary": "Over the past decade a considerable amount of research has been done to\nexpand logic programming languages to handle incomplete information. One such\nlanguage is the language of epistemic specifications. As is usual with logic\nprogramming languages, the problem of answering queries is intractable in the\ngeneral case. For extended disjunctive logic programs, an idea that has proven\nuseful in simplifying the investigation of answer sets is the use of splitting\nsets. In this paper we will present an extended definition of splitting sets\nthat will be applicable to epistemic specifications. Furthermore, an extension\nof the splitting set theorem will be presented. Also, a characterization of\nstratified epistemic specifications will be given in terms of splitting sets.\nThis characterization leads us to an algorithmic method of computing world\nviews of a subclass of epistemic logic programs."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003039v1", 
    "title": "DES: a Challenge Problem for Nonmonotonic Reasoning Systems", 
    "arxiv-id": "cs/0003039v1", 
    "author": "Ilkka Niemela", 
    "publish": "2000-03-08T21:49:57Z", 
    "summary": "The US Data Encryption Standard, DES for short, is put forward as an\ninteresting benchmark problem for nonmonotonic reasoning systems because (i) it\nprovides a set of test cases of industrial relevance which shares features of\nrandomly generated problems and real-world problems, (ii) the representation of\nDES using normal logic programs with the stable model semantics is simple and\neasy to understand, and (iii) this subclass of logic programs can be seen as an\ninteresting special case for many other formalizations of nonmonotonic\nreasoning. In this paper we present two encodings of DES as logic programs: a\ndirect one out of the standard specifications and an optimized one extending\nthe work of Massacci and Marraro. The computational properties of the encodings\nare studied by using them for DES key search with the Smodels system as the\nimplementation of the stable model semantics. Results indicate that the\nencodings and Smodels are quite competitive: they outperform state-of-the-art\nSAT-checkers working with an optimized encoding of DES into SAT and are\ncomparable with a SAT-checker that is customized and tuned for the optimized\nSAT encoding."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003042v1", 
    "title": "Fages' Theorem and Answer Set Programming", 
    "arxiv-id": "cs/0003042v1", 
    "author": "Vladimir Lifschitz", 
    "publish": "2000-03-09T00:28:21Z", 
    "summary": "We generalize a theorem by Francois Fages that describes the relationship\nbetween the completion semantics and the answer set semantics for logic\nprograms with negation as failure. The study of this relationship is important\nin connection with the emergence of answer set programming. Whenever the two\nsemantics are equivalent, answer sets can be computed by a satisfiability\nsolver, and the use of answer set solvers such as smodels and dlv is\nunnecessary. A logic programming representation of the blocks world due to\nIlkka Niemelae is discussed as an example."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003044v1", 
    "title": "On the tractable counting of theory models and its application to belief   revision and truth maintenance", 
    "arxiv-id": "cs/0003044v1", 
    "author": "Adnan Darwiche", 
    "publish": "2000-03-09T08:58:15Z", 
    "summary": "We introduced decomposable negation normal form (DNNF) recently as a\ntractable form of propositional theories, and provided a number of powerful\nlogical operations that can be performed on it in polynomial time. We also\npresented an algorithm for compiling any conjunctive normal form (CNF) into\nDNNF and provided a structure-based guarantee on its space and time complexity.\nWe present in this paper a linear-time algorithm for converting an ordered\nbinary decision diagram (OBDD) representation of a propositional theory into an\nequivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a\nsubclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the\nprevious complexity guarantees on compiling DNNF continue to hold for this\nstricter subclass, which has stronger properties. In particular, we present a\nnew operation on d-DNNF which allows us to count its models under the\nassertion, retraction and flipping of every literal by traversing the d-DNNF\ntwice. That is, after such traversal, we can test in constant-time: the\nentailment of any literal by the d-DNNF, and the consistency of the d-DNNF\nunder the retraction or flipping of any literal. We demonstrate the\nsignificance of these new operations by showing how they allow us to implement\nlinear-time, complete truth maintenance systems and linear-time, complete\nbelief revision systems for two important classes of propositional theories."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003047v1", 
    "title": "BDD-based reasoning in the fluent calculus - first results", 
    "arxiv-id": "cs/0003047v1", 
    "author": "Hans-Peter Stoerr", 
    "publish": "2000-03-09T17:18:12Z", 
    "summary": "The paper reports on first preliminary results and insights gained in a\nproject aiming at implementing the fluent calculus using methods and techniques\nbased on binary decision diagrams. After reporting on an initial experiment\nshowing promising results we discuss our findings concerning various techniques\nand heuristics used to speed up the reasoning process."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003049v1", 
    "title": "Planning with Incomplete Information", 
    "arxiv-id": "cs/0003049v1", 
    "author": "Francesca Toni", 
    "publish": "2000-03-09T22:30:27Z", 
    "summary": "Planning is a natural domain of application for frameworks of reasoning about\nactions and change. In this paper we study how one such framework, the Language\nE, can form the basis for planning under (possibly) incomplete information. We\ndefine two types of plans: weak and safe plans, and propose a planner, called\nthe E-Planner, which is often able to extend an initial weak plan into a safe\nplan even though the (explicit) information available is incomplete, e.g. for\ncases where the initial state is not completely known. The E-Planner is based\nupon a reformulation of the Language E in argumentation terms and a natural\nproof theory resulting from the reformulation. It uses an extension of this\nproof theory by means of abduction for the generation of plans and adopts\nargumentation-based techniques for extending weak plans into safe plans. We\nprovide representative examples illustrating the behaviour of the E-Planner, in\nparticular for cases where the status of fluents is incompletely known."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003051v1", 
    "title": "Local Diagnosis", 
    "arxiv-id": "cs/0003051v1", 
    "author": "Renata Wassermann", 
    "publish": "2000-03-10T22:54:55Z", 
    "summary": "In an earlier work, we have presented operations of belief change which only\naffect the relevant part of a belief base. In this paper, we propose the\napplication of the same strategy to the problem of model-based diangosis. We\nfirst isolate the subset of the system description which is relevant for a\ngiven observation and then solve the diagnosis problem for this subset."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003052v3", 
    "title": "A Consistency-Based Model for Belief Change: Preliminary Report", 
    "arxiv-id": "cs/0003052v3", 
    "author": "Torsten Schaub", 
    "publish": "2000-03-11T06:29:02Z", 
    "summary": "We present a general, consistency-based framework for belief change.\nInformally, in revising K by A, we begin with A and incorporate as much of K as\nconsistently possible. Formally, a knowledge base K and sentence A are\nexpressed, via renaming propositions in K, in separate languages. Using a\nmaximization process, we assume the languages are the same insofar as\nconsistently possible. Lastly, we express the resultant knowledge base in a\nsingle language. There may be more than one way in which A can be so extended\nby K: in choice revision, one such ``extension'' represents the revised state;\nalternately revision consists of the intersection of all such extensions.\n  The most general formulation of our approach is flexible enough to express\nother approaches to revision and update, the merging of knowledge bases, and\nthe incorporation of static and dynamic integrity constraints. Our framework\ndiffers from work based on ordinal conditional functions, notably with respect\nto iterated revision. We argue that the approach is well-suited for\nimplementation: the choice revision operator gives better complexity results\nthan general revision; the approach can be expressed in terms of a finite\nknowledge base; and the scope of a revision can be restricted to just those\npropositions mentioned in the sentence for revision A."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003059v1", 
    "title": "SATEN: An Object-Oriented Web-Based Revision and Extraction Engine", 
    "arxiv-id": "cs/0003059v1", 
    "author": "Aidan Sims", 
    "publish": "2000-03-14T04:58:18Z", 
    "summary": "SATEN is an object-oriented web-based extraction and belief revision engine.\nIt runs on any computer via a Java 1.1 enabled browser such as Netscape 4.\nSATEN performs belief revision based on the AGM approach. The extraction and\nbelief revision reasoning engines operate on a user specified ranking of\ninformation. One of the features of SATEN is that it can be used to integrate\nmutually inconsistent commensuate rankings into a consistent ranking."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003061v1", 
    "title": "dcs: An Implementation of DATALOG with Constraints", 
    "arxiv-id": "cs/0003061v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2000-03-14T18:06:38Z", 
    "summary": "Answer-set programming (ASP) has emerged recently as a viable programming\nparadigm. We describe here an ASP system, DATALOG with constraints or DC, based\non non-monotonic logic. Informally, DC theories consist of propositional\nclauses (constraints) and of Horn rules. The semantics is a simple and natural\nextension of the semantics of the propositional logic. However, thanks to the\npresence of Horn rules in the system, modeling of transitive closure becomes\nstraightforward. We describe the syntax, use and implementation of DC and\nprovide experimental results."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003077v1", 
    "title": "DATALOG with constraints - an answer-set programming system", 
    "arxiv-id": "cs/0003077v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2000-03-24T19:09:59Z", 
    "summary": "Answer-set programming (ASP) has emerged recently as a viable programming\nparadigm well attuned to search problems in AI, constraint satisfaction and\ncombinatorics. Propositional logic is, arguably, the simplest ASP system with\nan intuitive semantics supporting direct modeling of problem constraints.\nHowever, for some applications, especially those requiring that transitive\nclosure be computed, it requires additional variables and results in large\ntheories. Consequently, it may not be a practical computational tool for such\nproblems. On the other hand, ASP systems based on nonmonotonic logics, such as\nstable logic programming, can handle transitive closure computation efficiently\nand, in general, yield very concise theories as problem representations. Their\nsemantics is, however, more complex. Searching for the middle ground, in this\npaper we introduce a new nonmonotonic logic, DATALOG with constraints or DC.\nInformally, DC theories consist of propositional clauses (constraints) and of\nHorn rules. The semantics is a simple and natural extension of the semantics of\nthe propositional logic. However, thanks to the presence of Horn rules in the\nsystem, modeling of transitive closure becomes straightforward. We describe the\nsyntax and semantics of DC, and study its properties. We discuss an\nimplementation of DC and present results of experimental study of the\neffectiveness of DC, comparing it with CSAT, a satisfiability checker and\nSMODELS implementation of stable logic programming. Our results show that DC is\ncompetitive with the other two approaches, in case of many search problems,\noften yielding much more efficient solutions."
},{
    "category": "cs.AI", 
    "doi": "10.1016/0167-2789(90)90087-6", 
    "link": "http://arxiv.org/pdf/cs/0003080v1", 
    "title": "Some Remarks on Boolean Constraint Propagation", 
    "arxiv-id": "cs/0003080v1", 
    "author": "Krzysztof R. Apt", 
    "publish": "2000-03-28T11:49:37Z", 
    "summary": "We study here the well-known propagation rules for Boolean constraints. First\nwe propose a simple notion of completeness for sets of such rules and establish\na completeness result. Then we show an equivalence in an appropriate sense\nbetween Boolean constraint propagation and unit propagation, a form of\nresolution for propositional logic.\n  Subsequently we characterize one set of such rules by means of the notion of\nhyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify\nthe status of a similar, though different, set of rules introduced in (Simonis\n1989a) and more fully in (Codognet and Diaz 1996)."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0005031v3", 
    "title": "Conditional Plausibility Measures and Bayesian Networks", 
    "arxiv-id": "cs/0005031v3", 
    "author": "Joseph Y. Halpern", 
    "publish": "2000-05-30T19:05:21Z", 
    "summary": "A general notion of algebraic conditional plausibility measures is defined.\nProbability measures, ranking functions, possibility measures, and (under the\nappropriate definitions) sets of probability measures can all be viewed as\ndefining algebraic conditional plausibility measures. It is shown that\nalgebraic conditional plausibility measures can be represented using Bayesian\nnetworks."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0006043v1", 
    "title": "Constraint compiling into rules formalism constraint compiling into   rules formalism for dynamic CSPs computing", 
    "arxiv-id": "cs/0006043v1", 
    "author": "J. Rodriguez", 
    "publish": "2000-06-30T10:25:06Z", 
    "summary": "In this paper we present a rule based formalism for filtering variables\ndomains of constraints. This formalism is well adapted for solving dynamic CSP.\nWe take diagnosis as an instance problem to illustrate the use of these rules.\nA diagnosis problem is seen like finding all the minimal sets of constraints to\nbe relaxed in the constraint network that models the device to be diagnosed"
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0007004v1", 
    "title": "Brainstorm/J: a Java Framework for Intelligent Agents", 
    "arxiv-id": "cs/0007004v1", 
    "author": "Analia Amandi", 
    "publish": "2000-07-04T16:31:40Z", 
    "summary": "Despite the effort of many researchers in the area of multi-agent systems\n(MAS) for designing and programming agents, a few years ago the research\ncommunity began to take into account that common features among different MAS\nexists. Based on these common features, several tools have tackled the problem\nof agent development on specific application domains or specific types of\nagents. As a consequence, their scope is restricted to a subset of the huge\napplication domain of MAS. In this paper we propose a generic infrastructure\nfor programming agents whose name is Brainstorm/J. The infrastructure has been\nimplemented as an object oriented framework. As a consequence, our approach\nsupports a broader scope of MAS applications than previous efforts, being\nflexible and reusable."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0010037v1", 
    "title": "On the relationship between fuzzy logic and four-valued relevance logic", 
    "arxiv-id": "cs/0010037v1", 
    "author": "Umberto Straccia", 
    "publish": "2000-10-31T14:14:26Z", 
    "summary": "In fuzzy propositional logic, to a proposition a partial truth in [0,1] is\nassigned. It is well known that under certain circumstances, fuzzy logic\ncollapses to classical logic. In this paper, we will show that under dual\nconditions, fuzzy logic collapses to four-valued (relevance) logic, where\npropositions have truth-value true, false, unknown, or contradiction. As a\nconsequence, fuzzy entailment may be considered as ``in between'' four-valued\n(relevance) entailment and classical entailment."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0011012v3", 
    "title": "Causes and Explanations: A Structural-Model Approach, Part I: Causes", 
    "arxiv-id": "cs/0011012v3", 
    "author": "Judea Pearl", 
    "publish": "2000-11-07T23:21:38Z", 
    "summary": "We propose a new definition of actual cause, using structural equations to\nmodel counterfactuals. We show that the definition yields a plausible and\nelegant account of causation that handles well examples which have caused\nproblems for other definitions and resolves major difficulties in the\ntraditional account."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0011030v1", 
    "title": "Logic Programming Approaches for Representing and Solving Constraint   Satisfaction Problems: A Comparison", 
    "arxiv-id": "cs/0011030v1", 
    "author": "Marc Denecker", 
    "publish": "2000-11-21T13:56:21Z", 
    "summary": "Many logic programming based approaches can be used to describe and solve\ncombinatorial search problems. On the one hand there is constraint logic\nprogramming which computes a solution as an answer substitution to a query\ncontaining the variables of the constraint satisfaction problem. On the other\nhand there are systems based on stable model semantics, abductive systems, and\nfirst order logic model generators which compute solutions as models of some\ntheory. This paper compares these different approaches from the point of view\nof knowledge representation (how declarative are the programs) and from the\npoint of view of performance (how good are they at solving typical problems)."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0105022v1", 
    "title": "Multi-Channel Parallel Adaptation Theory for Rule Discovery", 
    "arxiv-id": "cs/0105022v1", 
    "author": "Li Min Fu", 
    "publish": "2001-05-11T14:17:42Z", 
    "summary": "In this paper, we introduce a new machine learning theory based on\nmulti-channel parallel adaptation for rule discovery. This theory is\ndistinguished from the familiar parallel-distributed adaptation theory of\nneural networks in terms of channel-based convergence to the target rules. We\nshow how to realize this theory in a learning system named CFRule. CFRule is a\nparallel weight-based model, but it departs from traditional neural computing\nin that its internal knowledge is comprehensible. Furthermore, when the model\nconverges upon training, each channel converges to a target rule. The model\nadaptation rule is derived by multi-level parallel weight optimization based on\ngradient descent. Since, however, gradient descent only guarantees local\noptimization, a multi-channel regression-based optimization strategy is\ndeveloped to effectively deal with this problem. Formally, we prove that the\nCFRule model can explicitly and precisely encode any given rule set. Also, we\nprove a property related to asynchronous parallel convergence, which is a\ncritical element of the multi-channel parallel adaptation theory for rule\nlearning. Thanks to the quantizability nature of the CFRule model, rules can be\nextracted completely and soundly via a threshold-based mechanism. Finally, the\npractical application of the theory is demonstrated in DNA promoter recognition\nand hepatitis prognosis prediction."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0106006v1", 
    "title": "A Constraint-Driven System for Contract Assembly", 
    "arxiv-id": "cs/0106006v1", 
    "author": "Marek Sergot", 
    "publish": "2001-06-07T14:27:30Z", 
    "summary": "We present an approach for modelling the structure and coarse content of\nlegal documents with a view to providing automated support for the drafting of\ncontracts and contract database retrieval. The approach is designed to be\napplicable where contract drafting is based on model-form contracts or on\nexisting examples of a similar type. The main features of the approach are: (1)\nthe representation addresses the structure and the interrelationships between\nthe constituent parts of contracts, but not the text of the document itself;\n(2) the representation of documents is separated from the mechanisms that\nmanipulate it; and (3) the drafting process is subject to a collection of\nexplicitly stated constraints that govern the structure of the documents. We\ndescribe the representation of document instances and of 'generic documents',\nwhich are data structures used to drive the creation of new document instances,\nand we show extracts from a sample session to illustrate the features of a\nprototype system implemented in MacProlog."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0106007v1", 
    "title": "Modelling Contractual Arguments", 
    "arxiv-id": "cs/0106007v1", 
    "author": "Aspassia Daskalopulu", 
    "publish": "2001-06-07T14:40:04Z", 
    "summary": "One influential approach to assessing the \"goodness\" of arguments is offered\nby the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can\nbe compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an\napproach that originates in discourse analysis. In p-d terms an argument is\ngood if it avoids committing a fallacy, whereas in RST terms an argument is\ngood if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for\nproviding only a partially functional account of argument, and similar\ncriticisms have been raised in the Natural Language Generation (NLG)\ncommunity-particularly by Moore & Pollack (1992)- with regards to its account\nof intentionality in text in general. Mann and Thompson themselves note that\nalthough RST can be successfully applied to a wide range of texts from diverse\ndomains, it fails to characterise some types of text, most notably legal\ncontracts. There is ongoing research in the Artificial Intelligence and Law\ncommunity exploring the potential for providing electronic support to contract\nnegotiators, focusing on long-term, complex engineering agreements (see for\nexample Daskalopulu & Sergot 1997). This paper provides a brief introduction to\nRST and illustrates its shortcomings with respect to contractual text. An\nalternative approach for modelling argument structure is presented which not\nonly caters for contractual text, but also overcomes the aforementioned\nlimitations of RST."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0106025v1", 
    "title": "Information Integration and Computational Logic", 
    "arxiv-id": "cs/0106025v1", 
    "author": "Antonis Kakas", 
    "publish": "2001-06-11T20:00:04Z", 
    "summary": "Information Integration is a young and exciting field with enormous research\nand commercial significance in the new world of the Information Society. It\nstands at the crossroad of Databases and Artificial Intelligence requiring\nnovel techniques that bring together different methods from these fields.\nInformation from disparate heterogeneous sources often with no a-priori common\nschema needs to be synthesized in a flexible, transparent and intelligent way\nin order to respond to the demands of a query thus enabling a more informed\ndecision by the user or application program. The field although relatively\nyoung has already found many practical applications particularly for\nintegrating information over the World Wide Web. This paper gives a brief\nintroduction of the field highlighting some of the main current and future\nresearch issues and application areas. It attempts to evaluate the current and\npotential role of Computational Logic in this and suggests some of the problems\nwhere logic-based techniques could be used."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0107002v1", 
    "title": "Enhancing Constraint Propagation with Composition Operators", 
    "arxiv-id": "cs/0107002v1", 
    "author": "Eric Monfroy", 
    "publish": "2001-07-02T08:08:39Z", 
    "summary": "Constraint propagation is a general algorithmic approach for pruning the\nsearch space of a CSP. In a uniform way, K. R. Apt has defined a computation as\nan iteration of reduction functions over a domain. He has also demonstrated the\nneed for integrating static properties of reduction functions (commutativity\nand semi-commutativity) to design specialized algorithms such as AC3 and DAC.\nWe introduce here a set of operators for modeling compositions of reduction\nfunctions. Two of the major goals are to tackle parallel computations, and\ndynamic behaviours (such as slow convergence)."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0109006v1", 
    "title": "On Properties of Update Sequences Based on Causal Rejection", 
    "arxiv-id": "cs/0109006v1", 
    "author": "H. Tompits", 
    "publish": "2001-09-05T09:19:34Z", 
    "summary": "We consider an approach to update nonmonotonic knowledge bases represented as\nextended logic programs under answer set semantics. New information is\nincorporated into the current knowledge base subject to a causal rejection\nprinciple enforcing that, in case of conflicts, more recent rules are preferred\nand older rules are overridden. Such a rejection principle is also exploited in\nother approaches to update logic programs, e.g., in dynamic logic programming\nby Alferes et al. We give a thorough analysis of properties of our approach, to\nget a better understanding of the causal rejection principle. We review\npostulates for update and revision operators from the area of theory change and\nnonmonotonic reasoning, and some new properties are considered as well. We then\nconsider refinements of our semantics which incorporate a notion of minimality\nof change. As well, we investigate the relationship to other approaches,\nshowing that our approach is semantically equivalent to inheritance programs by\nBuccafurri et al. and that it coincides with certain classes of dynamic logic\nprograms, for which we provide characterizations in terms of graph conditions.\nTherefore, most of our results about properties of causal rejection principle\napply to these approaches as well. Finally, we deal with computational\ncomplexity of our approach, and outline how the update semantics and its\nrefinements can be implemented on top of existing logic programming engines."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0111060v1", 
    "title": "Gradient-based Reinforcement Planning in Policy-Search Methods", 
    "arxiv-id": "cs/0111060v1", 
    "author": "Juergen Schmidhuber", 
    "publish": "2001-11-28T13:43:13Z", 
    "summary": "We introduce a learning method called ``gradient-based reinforcement\nplanning'' (GREP). Unlike traditional DP methods that improve their policy\nbackwards in time, GREP is a gradient-based method that plans ahead and\nimproves its policy before it actually acts in the environment. We derive\nformulas for the exact policy gradient that maximizes the expected future\nreward and confirm our ideas with numerical experiments."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0112015v1", 
    "title": "Rational Competitive Analysis", 
    "arxiv-id": "cs/0112015v1", 
    "author": "Moshe Tennenholtz", 
    "publish": "2001-12-13T00:46:10Z", 
    "summary": "Much work in computer science has adopted competitive analysis as a tool for\ndecision making under uncertainty. In this work we extend competitive analysis\nto the context of multi-agent systems. Unlike classical competitive analysis\nwhere the behavior of an agent's environment is taken to be arbitrary, we\nconsider the case where an agent's environment consists of other agents. These\nagents will usually obey some (minimal) rationality constraints. This leads to\nthe definition of rational competitive analysis. We introduce the concept of\nrational competitive analysis, and initiate the study of competitive analysis\nfor multi-agent systems. We also discuss the application of rational\ncompetitive analysis to the context of bidding games, as well as to the\nclassical one-way trading problem."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0201022v2", 
    "title": "A theory of experiment", 
    "arxiv-id": "cs/0201022v2", 
    "author": "Pierre Albarede", 
    "publish": "2002-01-23T19:53:08Z", 
    "summary": "This article aims at clarifying the language and practice of scientific\nexperiment, mainly by hooking observability on calculability."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202021v1", 
    "title": "Nonmonotonic Reasoning, Preferential Models and Cumulative Logics", 
    "arxiv-id": "cs/0202021v1", 
    "author": "Menachem Magidor", 
    "publish": "2002-02-18T10:29:54Z", 
    "summary": "Many systems that exhibit nonmonotonic behavior have been described and\nstudied already in the literature. The general notion of nonmonotonic\nreasoning, though, has almost always been described only negatively, by the\nproperty it does not enjoy, i.e. monotonicity. We study here general patterns\nof nonmonotonic reasoning and try to isolate properties that could help us map\nthe field of nonmonotonic reasoning by reference to positive properties. We\nconcentrate on a number of families of nonmonotonic consequence relations,\ndefined in the style of Gentzen. Both proof-theoretic and semantic points of\nview are developed in parallel. The former point of view was pioneered by D.\nGabbay, while the latter has been advocated by Y. Shoham in. Five such families\nare defined and characterized by representation theorems, relating the two\npoints of view. One of the families of interest, that of preferential\nrelations, turns out to have been studied by E. Adams. The \"preferential\"\nmodels proposed here are a much stronger tool than Adams' probabilistic\nsemantics. The basic language used in this paper is that of propositional\nlogic. The extension of our results to first order predicate calculi and the\nstudy of the computational complexity of the decision problems described in\nthis paper will be treated in another paper."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202022v1", 
    "title": "What does a conditional knowledge base entail?", 
    "arxiv-id": "cs/0202022v1", 
    "author": "Menachem Magidor", 
    "publish": "2002-02-18T12:43:18Z", 
    "summary": "This paper presents a logical approach to nonmonotonic reasoning based on the\nnotion of a nonmonotonic consequence relation. A conditional knowledge base,\nconsisting of a set of conditional assertions of the type \"if ... then ...\",\nrepresents the explicit defeasible knowledge an agent has about the way the\nworld generally behaves. We look for a plausible definition of the set of all\nconditional assertions entailed by a conditional knowledge base. In a previous\npaper, S. Kraus and the authors defined and studied \"preferential\" consequence\nrelations. They noticed that not all preferential relations could be considered\nas reasonable inference procedures. This paper studies a more restricted class\nof consequence relations, \"rational\" relations. It is argued that any\nreasonable nonmonotonic inference procedure should define a rational relation.\nIt is shown that the rational relations are exactly those that may be\nrepresented by a \"ranked\" preferential model, or by a (non-standard)\nprobabilistic model. The rational closure of a conditional knowledge base is\ndefined and shown to provide an attractive answer to the question of the title.\nGlobal properties of this closure operation are proved: it is a cumulative\noperation. It is also computationally tractable. This paper assumes the\nunderlying language is propositional."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202024v1", 
    "title": "A note on Darwiche and Pearl", 
    "arxiv-id": "cs/0202024v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-02-18T15:23:06Z", 
    "summary": "It is shown that Darwiche and Pearl's postulates imply an interesting\nproperty, not noticed by the authors."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202025v1", 
    "title": "Distance Semantics for Belief Revision", 
    "arxiv-id": "cs/0202025v1", 
    "author": "Karl Schlechta", 
    "publish": "2002-02-18T15:36:46Z", 
    "summary": "A vast and interesting family of natural semantics for belief revision is\ndefined. Suppose one is given a distance d between any two models. One may then\ndefine the revision of a theory K by a formula a as the theory defined by the\nset of all those models of a that are closest, by d, to the set of models of K.\nThis family is characterized by a set of rationality postulates that extends\nthe AGM postulates. The new postulates describe properties of iterated\nrevisions."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202026v1", 
    "title": "Preferred History Semantics for Iterated Updates", 
    "arxiv-id": "cs/0202026v1", 
    "author": "Karl Schlechta", 
    "publish": "2002-02-18T15:53:56Z", 
    "summary": "We give a semantics to iterated update by a preference relation on possible\ndevelopments. An iterated update is a sequence of formulas, giving (incomplete)\ninformation about successive states of the world. A development is a sequence\nof models, describing a possible trajectory through time. We assume a principle\nof inertia and prefer those developments, which are compatible with the\ninformation, and avoid unnecessary changes. The logical properties of the\nupdates defined in this way are considered, and a representation result is\nproved."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202031v1", 
    "title": "Nonmonotonic inference operations", 
    "arxiv-id": "cs/0202031v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-02-20T14:19:42Z", 
    "summary": "A. Tarski proposed the study of infinitary consequence operations as the\ncentral topic of mathematical logic. He considered monotonicity to be a\nproperty of all such operations. In this paper, we weaken the monotonicity\nrequirement and consider more general operations, inference operations. These\noperations describe the nonmonotonic logics both humans and machines seem to be\nusing when infering defeasible information from incomplete knowledge. We single\nout a number of interesting families of inference operations. This study of\ninfinitary inference operations is inspired by the results of Kraus, Lehmann\nand Magidor on finitary nonmonotonic operations, but this paper is\nself-contained."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0202033v1", 
    "title": "The logical meaning of Expansion", 
    "arxiv-id": "cs/0202033v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-02-20T14:50:49Z", 
    "summary": "The Expansion property considered by researchers in Social Choice is shown to\ncorrespond to a logical property of nonmonotonic consequence relations that is\nthe {\\em pure}, i.e., not involving connectives, version of a previously known\nweak rationality condition. The assumption that the union of two definable sets\nof models is definable is needed for the soundness part of the result."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0203002v1", 
    "title": "Another perspective on Default Reasoning", 
    "arxiv-id": "cs/0203002v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-03-01T11:06:49Z", 
    "summary": "The lexicographic closure of any given finite set D of normal defaults is\ndefined. A conditional assertion \"if a then b\" is in this lexicographic closure\nif, given the defaults D and the fact a, one would conclude b. The\nlexicographic closure is essentially a rational extension of D, and of its\nrational closure, defined in a previous paper. It provides a logic of normal\ndefaults that is different from the one proposed by R. Reiter and that is rich\nenough not to require the consideration of non-normal defaults. A large number\nof examples are provided to show that the lexicographic closure corresponds to\nthe basic intuitions behind Reiter's logic of defaults."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0203003v1", 
    "title": "Deductive Nonmonotonic Inference Operations: Antitonic Representations", 
    "arxiv-id": "cs/0203003v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-03-01T11:20:59Z", 
    "summary": "We provide a characterization of those nonmonotonic inference operations C\nfor which C(X) may be described as the set of all logical consequences of X\ntogether with some set of additional assumptions S(X) that depends\nanti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset\nof S(X)). The operations represented are exactly characterized in terms of\nproperties most of which have been studied in Freund-Lehmann(cs.AI/0202031).\nSimilar characterizations of right-absorbing and cumulative operations are also\nprovided. For cumulative operations, our results fit in closely with those of\nFreund. We then discuss extending finitary operations to infinitary operations\nin a canonical way and discuss co-compactness properties. Our results provide a\nsatisfactory notion of pseudo-compactness, generalizing to deductive\nnonmonotonic operations the notion of compactness for monotonic operations.\nThey also provide an alternative, more elegant and more general, proof of the\nexistence of an infinitary deductive extension for any finitary deductive\noperation (Theorem 7.9 of Freund-Lehmann)."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0203004v1", 
    "title": "Stereotypical Reasoning: Logical Properties", 
    "arxiv-id": "cs/0203004v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-03-04T08:57:54Z", 
    "summary": "Stereotypical reasoning assumes that the situation at hand is one of a kind\nand that it enjoys the properties generally associated with that kind of\nsituation. It is one of the most basic forms of nonmonotonic reasoning. A\nformal model for stereotypical reasoning is proposed and the logical properties\nof this form of reasoning are studied. Stereotypical reasoning is shown to be\ncumulative under weak assumptions."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0203005v2", 
    "title": "A Framework for Compiling Preferences in Logic Programs", 
    "arxiv-id": "cs/0203005v2", 
    "author": "H. Tompits", 
    "publish": "2002-03-04T13:00:41Z", 
    "summary": "We introduce a methodology and framework for expressing general preference\ninformation in logic programming under the answer set semantics. An ordered\nlogic program is an extended logic program in which rules are named by unique\nterms, and in which preferences among rules are given by a set of atoms of form\ns < t where s and t are names. An ordered logic program is transformed into a\nsecond, regular, extended logic program wherein the preferences are respected,\nin that the answer sets obtained in the transformed program correspond with the\npreferred answer sets of the original program. Our approach allows the\nspecification of dynamic orderings, in which preferences can appear arbitrarily\nwithin a program. Static orderings (in which preferences are external to a\nlogic program) are a trivial restriction of the general dynamic case. First, we\ndevelop a specific approach to reasoning with preferences, wherein the\npreference ordering specifies the order in which rules are to be applied. We\nthen demonstrate the wide range of applicability of our framework by showing\nhow other approaches, among them that of Brewka and Eiter, can be captured\nwithin our framework. Since the result of each of these transformations is an\nextended logic program, we can make use of existing implementations, such as\ndlv and smodels. To this end, we have developed a publicly available compiler\nas a front-end for these programming systems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0203007v1", 
    "title": "Two results for proiritized logic programming", 
    "arxiv-id": "cs/0203007v1", 
    "author": "Yan Zhang", 
    "publish": "2002-03-05T00:28:04Z", 
    "summary": "Prioritized default reasoning has illustrated its rich expressiveness and\nflexibility in knowledge representation and reasoning. However, many important\naspects of prioritized default reasoning have yet to be thoroughly explored. In\nthis paper, we investigate two properties of prioritized logic programs in the\ncontext of answer set semantics. Specifically, we reveal a close relationship\nbetween mutual defeasibility and uniqueness of the answer set for a prioritized\nlogic program. We then explore how the splitting technique for extended logic\nprograms can be extended to prioritized logic programs. We prove splitting\ntheorems that can be used to simplify the evaluation of a prioritized logic\nprogram under certain conditions."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0204032v1", 
    "title": "Belief Revision and Rational Inference", 
    "arxiv-id": "cs/0204032v1", 
    "author": "Daniel Lehmann", 
    "publish": "2002-04-14T09:22:42Z", 
    "summary": "The (extended) AGM postulates for belief revision seem to deal with the\nrevision of a given theory K by an arbitrary formula, but not to constrain the\nrevisions of two different theories by the same formula. A new postulate is\nproposed and compared with other similar postulates that have been proposed in\nthe literature. The AGM revisions that satisfy this new postulate stand in\none-to-one correspondence with the rational, consistency-preserving relations.\nThis correspondence is described explicitly. Two viewpoints on iterative\nrevisions are distinguished and discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0205014v1", 
    "title": "Ultimate approximations in nonmonotonic knowledge representation systems", 
    "arxiv-id": "cs/0205014v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2002-05-11T20:44:16Z", 
    "summary": "We study fixpoints of operators on lattices. To this end we introduce the\nnotion of an approximation of an operator. We order approximations by means of\na precision ordering. We show that each lattice operator O has a unique most\nprecise or ultimate approximation. We demonstrate that fixpoints of this\nultimate approximation provide useful insights into fixpoints of the operator\nO.\n  We apply our theory to logic programming and introduce the ultimate\nKripke-Kleene, well-founded and stable semantics. We show that the ultimate\nKripke-Kleene and well-founded semantics are more precise then their standard\ncounterparts We argue that ultimate semantics for logic programming have\nattractive epistemological properties and that, while in general they are\ncomputationally more complex than the standard semantics, for many classes of\ntheories, their complexity is no worse."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0206003v1", 
    "title": "Handling Defeasibilities in Action Domains", 
    "arxiv-id": "cs/0206003v1", 
    "author": "Yan Zhang", 
    "publish": "2002-06-03T06:20:21Z", 
    "summary": "Representing defeasibility is an important issue in common sense reasoning.\nIn reasoning about action and change, this issue becomes more difficult because\ndomain and action related defeasible information may conflict with general\ninertia rules. Furthermore, different types of defeasible information may also\ninterfere with each other during the reasoning. In this paper, we develop a\nprioritized logic programming approach to handle defeasibilities in reasoning\nabout action. In particular, we propose three action languages {\\cal AT}^{0},\n{\\cal AT}^{1} and {\\cal AT}^{2} which handle three types of defeasibilities in\naction domains named defeasible constraints, defeasible observations and\nactions with defeasible and abnormal effects respectively. Each language with a\nhigher superscript can be viewed as an extension of the language with a lower\nsuperscript. These action languages inherit the simple syntax of {\\cal A}\nlanguage but their semantics is developed in terms of transition systems where\ntransition functions are defined based on prioritized logic programs. By\nillustrating various examples, we show that our approach eventually provides a\npowerful mechanism to handle various defeasibilities in temporal prediction and\npostdiction. We also investigate semantic properties of these three action\nlanguages and characterize classes of action domains that present more\ndesirable solutions in reasoning about action within the underlying action\nlanguages."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0206041v2", 
    "title": "Anticipatory Guidance of Plot", 
    "arxiv-id": "cs/0206041v2", 
    "author": "Magnus Boman", 
    "publish": "2002-06-26T09:17:13Z", 
    "summary": "An anticipatory system for guiding plot development in interactive narratives\nis described. The executable model is a finite automaton that provides the\nimplemented system with a look-ahead. The identification of undesirable future\nstates in the model is used to guide the player, in a transparent manner. In\nthis way, too radical twists of the plot can be avoided. Since the player\nparticipates in the development of the plot, such guidance can have many forms,\ndepending on the environment of the player, on the behavior of the other\nplayers, and on the means of player interaction. We present a design method for\ninteractive narratives which produces designs suitable for the implementation\nof anticipatory mechanisms. Use of the method is illustrated by application to\nour interactive computer game Kaktus."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207021v1", 
    "title": "Abduction, ASP and Open Logic Programs", 
    "arxiv-id": "cs/0207021v1", 
    "author": "Piero A. Bonatti", 
    "publish": "2002-07-07T09:55:00Z", 
    "summary": "Open logic programs and open entailment have been recently proposed as an\nabstract framework for the verification of incomplete specifications based upon\nnormal logic programs and the stable model semantics. There are obvious\nanalogies between open predicates and abducible predicates. However, despite\nsuperficial similarities, there are features of open programs that have no\nimmediate counterpart in the framework of abduction and viceversa. Similarly,\nopen programs cannot be immediately simulated with answer set programming\n(ASP). In this paper we start a thorough investigation of the relationships\nbetween open inference, abduction and ASP. We shall prove that open programs\ngeneralize the other two frameworks. The generalized framework suggests\ninteresting extensions of abduction under the generalized stable model\nsemantics. In some cases, we will be able to reduce open inference to abduction\nand ASP, thereby estimating its computational complexity. At the same time, the\naforementioned reduction opens the way to new applications of abduction and\nASP."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207023v2", 
    "title": "Domain-Dependent Knowledge in Answer Set Planning", 
    "arxiv-id": "cs/0207023v2", 
    "author": "Sheila McIlraith", 
    "publish": "2002-07-08T00:31:54Z", 
    "summary": "In this paper we consider three different kinds of domain-dependent control\nknowledge (temporal, procedural and HTN-based) that are useful in planning. Our\napproach is declarative and relies on the language of logic programming with\nanswer set semantics (AnsProlog*). AnsProlog* is designed to plan without\ncontrol knowledge. We show how temporal, procedural and HTN-based control\nknowledge can be incorporated into AnsProlog* by the modular addition of a\nsmall number of domain-dependent rules, without the need to modify the planner.\nWe formally prove the correctness of our planner, both in the absence and\npresence of the control knowledge. Finally, we perform some initial\nexperimentation that demonstrates the potential reduction in planning time that\ncan be achieved when procedural domain knowledge is used to solve planning\nproblems with large plan length."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207025v1", 
    "title": "\"Minimal defence\": a refinement of the preferred semantics for   argumentation frameworks", 
    "arxiv-id": "cs/0207025v1", 
    "author": "J. Mengin", 
    "publish": "2002-07-08T13:30:16Z", 
    "summary": "Dung's abstract framework for argumentation enables a study of the\ninteractions between arguments based solely on an ``attack'' binary relation on\nthe set of arguments. Various ways to solve conflicts between contradictory\npieces of information have been proposed in the context of argumentation,\nnonmonotonic reasoning or logic programming, and can be captured by appropriate\nsemantics within Dung's framework. A common feature of these semantics is that\none can always maximize in some sense the set of acceptable arguments. We\npropose in this paper to extend Dung's framework in order to allow for the\nrepresentation of what we call ``restricted'' arguments: these arguments should\nonly be used if absolutely necessary, that is, in order to support other\narguments that would otherwise be defeated. We modify Dung's preferred\nsemantics accordingly: a set of arguments becomes acceptable only if it\ncontains a minimum of restricted arguments, for a maximum of unrestricted\narguments."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207029v2", 
    "title": "Two Representations for Iterative Non-prioritized Change", 
    "arxiv-id": "cs/0207029v2", 
    "author": "Alexander Bochman", 
    "publish": "2002-07-09T12:32:45Z", 
    "summary": "We address a general representation problem for belief change, and describe\ntwo interrelated representations for iterative non-prioritized change: a\nlogical representation in terms of persistent epistemic states, and a\nconstructive representation in terms of flocks of bases."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207030v2", 
    "title": "Collective Argumentation", 
    "arxiv-id": "cs/0207030v2", 
    "author": "Alexander Bochman", 
    "publish": "2002-07-09T12:42:24Z", 
    "summary": "An extension of an abstract argumentation framework, called collective\nargumentation, is introduced in which the attack relation is defined directly\namong sets of arguments. The extension turns out to be suitable, in particular,\nfor representing semantics of disjunctive logic programs. Two special kinds of\ncollective argumentation are considered in which the opponents can share their\narguments."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207042v1", 
    "title": "Logic Programming with Ordered Disjunction", 
    "arxiv-id": "cs/0207042v1", 
    "author": "Gerhard Brewka", 
    "publish": "2002-07-11T11:03:34Z", 
    "summary": "Logic programs with ordered disjunction (LPODs) combine ideas underlying\nQualitative Choice Logic (Brewka et al. KR 2002) and answer set programming.\nLogic programming under answer set semantics is extended with a new connective\ncalled ordered disjunction. The new connective allows us to represent\nalternative, ranked options for problem solutions in the heads of rules: A\n\\times B intuitively means: if possible A, but if A is not possible then at\nleast B. The semantics of logic programs with ordered disjunction is based on a\npreference relation on answer sets. LPODs are useful for applications in design\nand configuration and can serve as a basis for qualitative decision making."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207045v1", 
    "title": "Compilation of Propositional Weighted Bases", 
    "arxiv-id": "cs/0207045v1", 
    "author": "Pierre Marquis", 
    "publish": "2002-07-11T16:11:40Z", 
    "summary": "In this paper, we investigate the extent to which knowledge compilation can\nbe used to improve inference from propositional weighted bases. We present a\ngeneral notion of compilation of a weighted base that is parametrized by any\nequivalence--preserving compilation function. Both negative and positive\nresults are presented. On the one hand, complexity results are identified,\nshowing that the inference problem from a compiled weighted base is as\ndifficult as in the general case, when the prime implicates, Horn cover or\nrenamable Horn cover classes are targeted. On the other hand, we show that the\ninference problem becomes tractable whenever DNNF-compilations are used and\nclausal queries are considered. Moreover, we show that the set of all preferred\nmodels of a DNNF-compilation of a weighted base can be computed in time\npolynomial in the output size. Finally, we sketch how our results can be used\nin model-based diagnosis in order to compute the most probable diagnoses of a\nsystem."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207056v1", 
    "title": "Modeling Complex Domains of Actions and Change", 
    "arxiv-id": "cs/0207056v1", 
    "author": "Loizos Michael", 
    "publish": "2002-07-13T12:00:16Z", 
    "summary": "This paper studies the problem of modeling complex domains of actions and\nchange within high-level action description languages. We investigate two main\nissues of concern: (a) can we represent complex domains that capture together\ndifferent problems such as ramifications, non-determinism and concurrency of\nactions, at a high-level, close to the given natural ontology of the problem\ndomain and (b) what features of such a representation can affect, and how, its\ncomputational behaviour. The paper describes the main problems faced in this\nrepresentation task and presents the results of an empirical study, carried out\nthrough a series of controlled experiments, to analyze the computational\nperformance of reasoning in these representations. The experiments compare\ndifferent representations obtained, for example, by changing the basic ontology\nof the domain or by varying the degree of use of indirect effect laws through\ndomain constraints. This study has helped to expose the main sources of\ncomputational difficulty in the reasoning and suggest some methodological\nguidelines for representing complex domains. Although our work has been carried\nout within one particular high-level description language, we believe that the\nresults, especially those that relate to the problems of representation, are\nindependent of the specific modeling language."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207059v1", 
    "title": "Value Based Argumentation Frameworks", 
    "arxiv-id": "cs/0207059v1", 
    "author": "T. Bench-Capon", 
    "publish": "2002-07-15T11:30:16Z", 
    "summary": "This paper introduces the notion of value-based argumentation frameworks, an\nextension of the standard argumentation frameworks proposed by Dung, which are\nable toshow how rational decision is possible in cases where arguments derive\ntheir force from the social values their acceptance would promote."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207060v1", 
    "title": "Preferred well-founded semantics for logic programming by alternating   fixpoints: Preliminary report", 
    "arxiv-id": "cs/0207060v1", 
    "author": "Kewen Wang", 
    "publish": "2002-07-15T13:30:24Z", 
    "summary": "We analyze the problem of defining well-founded semantics for ordered logic\nprograms within a general framework based on alternating fixpoint theory. We\nstart by showing that generalizations of existing answer set approaches to\npreference are too weak in the setting of well-founded semantics. We then\nspecify some informal yet intuitive criteria and propose a semantical framework\nfor preference handling that is more suitable for defining well-founded\nsemantics for ordered logic programs. The suitability of the new approach is\nconvinced by the fact that many attractive properties are satisfied by our\nsemantics. In particular, our semantics is still correct with respect to\nvarious existing answer sets semantics while it successfully overcomes the\nweakness of their generalization to well-founded semantics. Finally, we\nindicate how an existing preferred well-founded semantics can be captured\nwithin our semantical framework."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207065v1", 
    "title": "Embedding Default Logic in Propositional Argumentation Systems", 
    "arxiv-id": "cs/0207065v1", 
    "author": "Juerg Kohlas", 
    "publish": "2002-07-16T15:16:07Z", 
    "summary": "In this paper we present a transformation of finite propositional default\ntheories into so-called propositional argumentation systems. This\ntransformation allows to characterize all notions of Reiter's default logic in\nthe framework of argumentation systems. As a consequence, computing extensions,\nor determining wether a given formula belongs to one extension or all\nextensions can be answered without leaving the field of classical propositional\nlogic. The transformation proposed is linear in the number of defaults."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207067v1", 
    "title": "On the existence and multiplicity of extensions in dialectical   argumentation", 
    "arxiv-id": "cs/0207067v1", 
    "author": "Bart Verheij", 
    "publish": "2002-07-17T12:09:45Z", 
    "summary": "In the present paper, the existence and multiplicity problems of extensions\nare addressed. The focus is on extension of the stable type. The main result of\nthe paper is an elegant characterization of the existence and multiplicity of\nextensions in terms of the notion of dialectical justification, a close cousin\nof the notion of admissibility. The characterization is given in the context of\nthe particular logic for dialectical argumentation DEFLOG. The results are of\ndirect relevance for several well-established models of defeasible reasoning\n(like default logic, logic programming and argumentation frameworks), since\nelsewhere dialectical argumentation has been shown to have close formal\nconnections with these models."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207075v1", 
    "title": "Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic   Logic and Probabilistic Logic under Coherence", 
    "arxiv-id": "cs/0207075v1", 
    "author": "Thomas Lukasiewicz", 
    "publish": "2002-07-22T01:44:25Z", 
    "summary": "Recently, it has been shown that probabilistic entailment under coherence is\nweaker than model-theoretic probabilistic entailment. Moreover, probabilistic\nentailment under coherence is a generalization of default entailment in System\nP. In this paper, we continue this line of research by presenting probabilistic\ngeneralizations of more sophisticated notions of classical default entailment\nthat lie between model-theoretic probabilistic entailment and probabilistic\nentailment under coherence. That is, the new formalisms properly generalize\ntheir counterparts in classical default reasoning, they are weaker than\nmodel-theoretic probabilistic entailment, and they are stronger than\nprobabilistic entailment under coherence. The new formalisms are useful\nespecially for handling probabilistic inconsistencies related to conditioning\non zero events. They can also be applied for probabilistic belief revision.\nMore generally, in the same spirit as a similar previous paper, this paper\nsheds light on exciting new formalisms for probabilistic reasoning beyond the\nwell-known standard ones."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0207083v1", 
    "title": "Evaluating Defaults", 
    "arxiv-id": "cs/0207083v1", 
    "author": "Choh Man Teng", 
    "publish": "2002-07-24T23:05:29Z", 
    "summary": "We seek to find normative criteria of adequacy for nonmonotonic logic similar\nto the criterion of validity for deductive logic. Rather than stipulating that\nthe conclusion of an inference be true in all models in which the premises are\ntrue, we require that the conclusion of a nonmonotonic inference be true in\n``almost all'' models of a certain sort in which the premises are true. This\n``certain sort'' specification picks out the models that are relevant to the\ninference, taking into account factors such as specificity and vagueness, and\nprevious inferences. The frequencies characterizing the relevant models reflect\nknown frequencies in our actual world. The criteria of adequacy for a default\ninference can be extended by thresholding to criteria of adequacy for an\nextension. We show that this avoids the implausibilities that might otherwise\nresult from the chaining of default inferences. The model proportions, when\nconstrued in terms of frequencies, provide a verifiable grounding of default\nrules, and can become the basis for generating default rules from statistics."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0208017v1", 
    "title": "Linking Makinson and Kraus-Lehmann-Magidor preferential entailments", 
    "arxiv-id": "cs/0208017v1", 
    "author": "Yves Moinard", 
    "publish": "2002-08-08T17:08:46Z", 
    "summary": "About ten years ago, various notions of preferential entailment have been\nintroduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM),\none of the main competitor being a more general version defined by Makinson\n(MAK). These two versions have already been compared, but it is time to revisit\nthese comparisons. Here are our three main results: (1) These two notions are\nequivalent, provided that we restrict our attention, as done in KLM, to the\ncases where the entailment respects logical equivalence (on the left and on the\nright). (2) A serious simplification of the description of the fundamental\ncases in which MAK is equivalent to KLM, including a natural passage in both\nways. (3) The two previous results are given for preferential entailments more\ngeneral than considered in some of the original texts, but they apply also to\nthe original definitions and, for this particular case also, the models can be\nsimplified."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0208019v1", 
    "title": "Knowledge Representation", 
    "arxiv-id": "cs/0208019v1", 
    "author": "Mikalai Birukou", 
    "publish": "2002-08-12T22:34:47Z", 
    "summary": "This work analyses main features that should be present in knowledge\nrepresentation. It suggests a model for representation and a way to implement\nthis model in software. Representation takes care of both low-level sensor\ninformation and high-level concepts."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0208034v3", 
    "title": "Causes and Explanations: A Structural-Model Approach. Part II:   Explanations", 
    "arxiv-id": "cs/0208034v3", 
    "author": "Judea Pearl", 
    "publish": "2002-08-20T23:08:49Z", 
    "summary": "We propose new definitions of (causal) explanation, using structural\nequations to model counterfactuals. The definition is based on the notion of\nactual cause, as defined and motivated in a companion paper. Essentially, an\nexplanation is a fact that is not known for certain but, if found to be true,\nwould constitute an actual cause of the fact to be explained, regardless of the\nagent's initial uncertainty. We show that the definition handles well a number\nof problematic examples from the literature."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0209019v1", 
    "title": "Reasoning about Evolving Nonmonotonic Knowledge Bases", 
    "arxiv-id": "cs/0209019v1", 
    "author": "H. Tompits", 
    "publish": "2002-09-16T19:23:19Z", 
    "summary": "Recently, several approaches to updating knowledge bases modeled as extended\nlogic programs have been introduced, ranging from basic methods to incorporate\n(sequences of) sets of rules into a logic program, to more elaborate methods\nwhich use an update policy for specifying how updates must be incorporated. In\nthis paper, we introduce a framework for reasoning about evolving knowledge\nbases, which are represented as extended logic programs and maintained by an\nupdate policy. We first describe a formal model which captures various update\napproaches, and we define a logical language for expressing properties of\nevolving knowledge bases. We then investigate semantical and computational\nproperties of our framework, where we focus on properties of knowledge states\nwith respect to the canonical reasoning task of whether a given formula holds\non a given evolving knowledge base. In particular, we present finitary\ncharacterizations of the evolution for certain classes of framework instances,\nwhich can be exploited for obtaining decidability results. In more detail, we\ncharacterize the complexity of reasoning for some meaningful classes of\nevolving knowledge bases, ranging from polynomial to double exponential space\ncomplexity."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0209022v2", 
    "title": "A Comparison of Different Cognitive Paradigms Using Simple Animats in a   Virtual Laboratory, with Implications to the Notion of Cognition", 
    "arxiv-id": "cs/0209022v2", 
    "author": "Carlos Gershenson", 
    "publish": "2002-09-19T16:35:55Z", 
    "summary": "In this thesis I present a virtual laboratory which implements five different\nmodels for controlling animats: a rule-based system, a behaviour-based system,\na concept-based system, a neural network, and a Braitenberg architecture.\nThrough different experiments, I compare the performance of the models and\nconclude that there is no \"best\" model, since different models are better for\ndifferent things in different contexts.\n  The models I chose, although quite simple, represent different approaches for\nstudying cognition. Using the results as an empirical philosophical aid,\n  I note that there is no \"best\" approach for studying cognition, since\ndifferent approaches have all advantages and disadvantages, because they study\ndifferent aspects of cognition from different contexts. This has implications\nfor current debates on \"proper\" approaches for cognition: all approaches are a\nbit proper, but none will be \"proper enough\". I draw remarks on the notion of\ncognition abstracting from all the approaches used to study it, and propose a\nsimple classification for different types of cognition."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0210004v1", 
    "title": "Revising Partially Ordered Beliefs", 
    "arxiv-id": "cs/0210004v1", 
    "author": "Odile Papini", 
    "publish": "2002-10-03T09:29:54Z", 
    "summary": "This paper deals with the revision of partially ordered beliefs. It proposes\na semantic representation of epistemic states by partial pre-orders on\ninterpretations and a syntactic representation by partially ordered belief\nbases. Two revision operations, the revision stemming from the history of\nobservations and the possibilistic revision, defined when the epistemic state\nis represented by a total pre-order, are generalized, at a semantic level, to\nthe case of a partial pre-order on interpretations, and at a syntactic level,\nto the case of a partially ordered belief base. The equivalence between the two\nrepresentations is shown for the two revision operations."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0211008v4", 
    "title": "Can the whole brain be simpler than its \"parts\"?", 
    "arxiv-id": "cs/0211008v4", 
    "author": "Victor Eliashberg", 
    "publish": "2002-11-09T17:16:18Z", 
    "summary": "This is the first in a series of connected papers discussing the problem of a\ndynamically reconfigurable universal learning neurocomputer that could serve as\na computational model for the whole human brain. The whole series is entitled\n\"The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal\nLearning Neurocomputer.\" (For more information visit the website\nwww.brain0.com.) This introductory paper is concerned with general methodology.\nIts main goal is to explain why it is critically important for both neural\nmodeling and cognitive modeling to pay much attention to the basic requirements\nof the whole brain as a complex computing system. The author argues that it can\nbe easier to develop an adequate computational model for the whole\n\"unprogrammed\" (untrained) human brain than to find adequate formal\nrepresentations of some nontrivial parts of brain's performance. (In the same\nway as, for example, it is easier to describe the behavior of a complex\nanalytical function than the behavior of its real and/or imaginary part.) The\n\"curse of dimensionality\" that plagues purely phenomenological (\"brainless\")\ncognitive theories is a natural penalty for an attempt to represent\ninsufficiently large parts of brain's performance in a state space of\ninsufficiently high dimensionality. A \"partial\" modeler encounters \"Catch 22.\"\nAn attempt to simplify a cognitive problem by artificially reducing its\ndimensionality makes the problem more difficult."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0211027v1", 
    "title": "Adaptive Development of Koncepts in Virtual Animats: Insights into the   Development of Knowledge", 
    "arxiv-id": "cs/0211027v1", 
    "author": "Carlos Gershenson", 
    "publish": "2002-11-21T18:13:31Z", 
    "summary": "As a part of our effort for studying the evolution and development of\ncognition, we present results derived from synthetic experimentations in a\nvirtual laboratory where animats develop koncepts adaptively and ground their\nmeaning through action. We introduce the term \"koncept\" to avoid confusions and\nambiguity derived from the wide use of the word \"concept\". We present the\nmodels which our animats use for abstracting koncepts from perceptions,\nplastically adapt koncepts, and associate koncepts with actions. On a more\nphilosophical vein, we suggest that knowledge is a property of a cognitive\nsystem, not an element, and therefore observer-dependent."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0211038v1", 
    "title": "Dynamic Adjustment of the Motivation Degree in an Action Selection   Mechanism", 
    "arxiv-id": "cs/0211038v1", 
    "author": "Pedro Pablo Gonzalez", 
    "publish": "2002-11-27T10:35:50Z", 
    "summary": "This paper presents a model for dynamic adjustment of the motivation degree,\nusing a reinforcement learning approach, in an action selection mechanism\npreviously developed by the authors. The learning takes place in the\nmodification of a parameter of the model of combination of internal and\nexternal stimuli. Experiments that show the claimed properties are presented,\nusing a VR simulation developed for such purposes. The importance of adaptation\nby learning in action selection is also discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0211039v1", 
    "title": "Action Selection Properties in a Software Simulated Agent", 
    "arxiv-id": "cs/0211039v1", 
    "author": "Jose Negrete Martinez", 
    "publish": "2002-11-27T10:42:31Z", 
    "summary": "This article analyses the properties of the Internal Behaviour network, an\naction selection mechanism previously proposed by the authors, with the aid of\na simulation developed for such ends. A brief review of the Internal Behaviour\nnetwork is followed by the explanation of the implementation of the simulation.\nThen, experiments are presented and discussed analysing the properties of the\naction selection in the proposed model."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0211040v1", 
    "title": "A Model for Combination of External and Internal Stimuli in the Action   Selection of an Autonomous Agent", 
    "arxiv-id": "cs/0211040v1", 
    "author": "Carlos Gershenson Garcia", 
    "publish": "2002-11-27T10:45:50Z", 
    "summary": "This paper proposes a model for combination of external and internal stimuli\nfor the action selection in an autonomous agent, based in an action selection\nmechanism previously proposed by the authors. This combination model includes\nadditive and multiplicative elements, which allows to incorporate new\nproperties, which enhance the action selection. A given parameter a, which is\npart of the proposed model, allows to regulate the degree of dependence of the\nobserved external behaviour from the internal states of the entity."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0212025v1", 
    "title": "Searching for Plannable Domains can Speed up Reinforcement Learning", 
    "arxiv-id": "cs/0212025v1", 
    "author": "Andras Lorincz", 
    "publish": "2002-12-10T22:15:25Z", 
    "summary": "Reinforcement learning (RL) involves sequential decision making in uncertain\nenvironments. The aim of the decision-making agent is to maximize the benefit\nof acting in its environment over an extended period of time. Finding an\noptimal policy in RL may be very slow. To speed up learning, one often used\nsolution is the integration of planning, for example, Sutton's Dyna algorithm,\nor various other methods using macro-actions.\n  Here we suggest to separate plannable, i.e., close to deterministic parts of\nthe world, and focus planning efforts in this domain. A novel reinforcement\nlearning method called plannable RL (pRL) is proposed here. pRL builds a simple\nmodel, which is used to search for macro actions. The simplicity of the model\nmakes planning computationally inexpensive. It is shown that pRL finds an\noptimal policy, and that plannable macro actions found by pRL are near-optimal.\nIn turn, it is unnecessary to try large numbers of macro actions, which enables\nfast learning. The utility of pRL is demonstrated by computer simulations."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0301006v1", 
    "title": "Temporal plannability by variance of the episode length", 
    "arxiv-id": "cs/0301006v1", 
    "author": "Andras Lorincz", 
    "publish": "2003-01-09T12:39:03Z", 
    "summary": "Optimization of decision problems in stochastic environments is usually\nconcerned with maximizing the probability of achieving the goal and minimizing\nthe expected episode length. For interacting agents in time-critical\napplications, learning of the possibility of scheduling of subtasks (events) or\nthe full task is an additional relevant issue. Besides, there exist highly\nstochastic problems where the actual trajectories show great variety from\nepisode to episode, but completing the task takes almost the same amount of\ntime. The identification of sub-problems of this nature may promote e.g.,\nplanning, scheduling and segmenting Markov decision processes. In this work,\nformulae for the average duration as well as the standard deviation of the\nduration of events are derived. The emerging Bellman-type equation is a simple\nextension of Sobel's work (1982). Methods of dynamic programming as well as\nmethods of reinforcement learning can be applied for our extension. Computer\ndemonstration on a toy problem serve to highlight the principle."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0301010v2", 
    "title": "Comparisons and Computation of Well-founded Semantics for Disjunctive   Logic Programs", 
    "arxiv-id": "cs/0301010v2", 
    "author": "Lizhu Zhou", 
    "publish": "2003-01-14T08:14:43Z", 
    "summary": "Much work has been done on extending the well-founded semantics to general\ndisjunctive logic programs and various approaches have been proposed. However,\nthese semantics are different from each other and no consensus is reached about\nwhich semantics is the most intended. In this paper we look at disjunctive\nwell-founded reasoning from different angles. We show that there is an\nintuitive form of the well-founded reasoning in disjunctive logic programming\nwhich can be characterized by slightly modifying some exisitng approaches to\ndefining disjunctive well-founded semantics, including program transformations,\nargumentation, unfounded sets (and resolution-like procedure). We also provide\na bottom-up procedure for this semantics. The significance of our work is not\nonly in clarifying the relationship among different approaches, but also shed\nsome light on what is an intended well-founded semantics for disjunctive logic\nprograms."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0301023v1", 
    "title": "A semantic framework for preference handling in answer set programming", 
    "arxiv-id": "cs/0301023v1", 
    "author": "Kewen Wang", 
    "publish": "2003-01-23T09:09:31Z", 
    "summary": "We provide a semantic framework for preference handling in answer set\nprogramming. To this end, we introduce preference preserving consequence\noperators. The resulting fixpoint characterizations provide us with a uniform\nsemantic framework for characterizing preference handling in existing\napproaches. Although our approach is extensible to other semantics by means of\nan alternating fixpoint theory, we focus here on the elaboration of preferences\nunder answer set semantics. Alternatively, we show how these approaches can be\ncharacterized by the concept of order preservation. These uniform semantic\ncharacterizations provide us with new insights about interrelationships and\nmoreover about ways of implementation."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0302029v1", 
    "title": "Defeasible Logic Programming: An Argumentative Approach", 
    "arxiv-id": "cs/0302029v1", 
    "author": "Guillermo Ricardo Simari", 
    "publish": "2003-02-20T00:48:06Z", 
    "summary": "The work reported here introduces Defeasible Logic Programming (DeLP), a\nformalism that combines results of Logic Programming and Defeasible\nArgumentation. DeLP provides the possibility of representing information in the\nform of weak rules in a declarative manner, and a defeasible argumentation\ninference mechanism for warranting the entailed conclusions.\n  In DeLP an argumentation formalism will be used for deciding between\ncontradictory goals. Queries will be supported by arguments that could be\ndefeated by other arguments. A query q will succeed when there is an argument A\nfor q that is warranted, ie, the argument A that supports q is found undefeated\nby a warrant procedure that implements a dialectical analysis.\n  The defeasible argumentation basis of DeLP allows to build applications that\ndeal with incomplete and contradictory information in dynamic domains. Thus,\nthe resulting approach is suitable for representing agent's knowledge and for\nproviding an argumentation based reasoning mechanism to agents."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0302036v2", 
    "title": "Constraint-based analysis of composite solvers", 
    "arxiv-id": "cs/0302036v2", 
    "author": "Eric Monfroy", 
    "publish": "2003-02-25T14:33:08Z", 
    "summary": "Cooperative constraint solving is an area of constraint programming that\nstudies the interaction between constraint solvers with the aim of discovering\nthe interaction patterns that amplify the positive qualities of individual\nsolvers. Automatisation and formalisation of such studies is an important issue\nof cooperative constraint solving.\n  In this paper we present a constraint-based analysis of composite solvers\nthat integrates reasoning about the individual solvers and the processed data.\nThe idea is to approximate this reasoning by resolution of set constraints on\nthe finite sets representing the predicates that express all the necessary\nproperties. We illustrate application of our analysis to two important\ncooperation patterns: deterministic choice and loop."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0302039v1", 
    "title": "Kalman-filtering using local interactions", 
    "arxiv-id": "cs/0302039v1", 
    "author": "Andras Lorincz", 
    "publish": "2003-02-28T18:32:26Z", 
    "summary": "There is a growing interest in using Kalman-filter models for brain\nmodelling. In turn, it is of considerable importance to represent Kalman-filter\nin connectionist forms with local Hebbian learning rules. To our best\nknowledge, Kalman-filter has not been given such local representation. It seems\nthat the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a\nconnectionist representation is presented, which is derived by means of the\nrecursive prediction error method. We show that this method gives rise to\nattractive local learning rules and can adapt the Kalman-gain."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0303006v1", 
    "title": "On the Notion of Cognition", 
    "arxiv-id": "cs/0303006v1", 
    "author": "Carlos Gershenson", 
    "publish": "2003-03-10T18:20:28Z", 
    "summary": "We discuss philosophical issues concerning the notion of cognition basing\nourselves in experimental results in cognitive sciences, especially in computer\nsimulations of cognitive systems. There have been debates on the \"proper\"\napproach for studying cognition, but we have realized that all approaches can\nbe in theory equivalent. Different approaches model different properties of\ncognitive systems from different perspectives, so we can only learn from all of\nthem. We also integrate ideas from several perspectives for enhancing the\nnotion of cognition, such that it can contain other definitions of cognition as\nspecial cases. This allows us to propose a simple classification of different\ntypes of cognition."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0303009v2", 
    "title": "Unfolding Partiality and Disjunctions in Stable Model Semantics", 
    "arxiv-id": "cs/0303009v2", 
    "author": "J. You", 
    "publish": "2003-03-14T14:29:32Z", 
    "summary": "The paper studies an implementation methodology for partial and disjunctive\nstable models where partiality and disjunctions are unfolded from a logic\nprogram so that an implementation of stable models for normal\n(disjunction-free) programs can be used as the core inference engine. The\nunfolding is done in two separate steps. Firstly, it is shown that partial\nstable models can be captured by total stable models using a simple linear and\nmodular program transformation. Hence, reasoning tasks concerning partial\nstable models can be solved using an implementation of total stable models.\nDisjunctive partial stable models have been lacking implementations which now\nbecome available as the translation handles also the disjunctive case.\nSecondly, it is shown how total stable models of disjunctive programs can be\ndetermined by computing stable models for normal programs. Hence, an\nimplementation of stable models of normal programs can be used as a core engine\nfor implementing disjunctive programs. The feasibility of the approach is\ndemonstrated by constructing a system for computing stable models of\ndisjunctive programs using the smodels system as the core engine. The\nperformance of the resulting system is compared to that of dlv which is a\nstate-of-the-art special purpose system for disjunctive programs."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0303018v1", 
    "title": "Multi-target particle filtering for the probability hypothesis density", 
    "arxiv-id": "cs/0303018v1", 
    "author": "Hedvig Sidenbladh", 
    "publish": "2003-03-20T13:48:04Z", 
    "summary": "When tracking a large number of targets, it is often computationally\nexpensive to represent the full joint distribution over target states. In cases\nwhere the targets move independently, each target can instead be tracked with a\nseparate filter. However, this leads to a model-data association problem.\nAnother approach to solve the problem with computational complexity is to track\nonly the first moment of the joint distribution, the probability hypothesis\ndensity (PHD). The integral of this distribution over any area S is the\nexpected number of targets within S. Since no record of object identity is\nkept, the model-data association problem is avoided.\n  The contribution of this paper is a particle filter implementation of the PHD\nfilter mentioned above. This PHD particle filter is applied to tracking of\nmultiple vehicles in terrain, a non-linear tracking problem. Experiments show\nthat the filter can track a changing number of vehicles robustly, achieving\nnear-real-time performance."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0305001v1", 
    "title": "A Framework for Searching AND/OR Graphs with Cycles", 
    "arxiv-id": "cs/0305001v1", 
    "author": "Samir K. Sadhukhan", 
    "publish": "2003-05-01T04:48:29Z", 
    "summary": "Search in cyclic AND/OR graphs was traditionally known to be an unsolved\nproblem. In the recent past several important studies have been reported in\nthis domain. In this paper, we have taken a fresh look at the problem. First, a\nnew and comprehensive theoretical framework for cyclic AND/OR graphs has been\npresented, which was found missing in the recent literature. Based on this\nframework, two best-first search algorithms, S1 and S2, have been developed. S1\ndoes uninformed search and is a simple modification of the Bottom-up algorithm\nby Martelli and Montanari. S2 performs a heuristically guided search and\nreplicates the modification in Bottom-up's successors, namely HS and AO*. Both\nS1 and S2 solve the problem of searching AND/OR graphs in presence of cycles.\nWe then present a detailed analysis for the correctness and complexity results\nof S1 and S2, using the proposed framework. We have observed through\nexperiments that S1 and S2 output correct results in all cases."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0305019v1", 
    "title": "On rho in a Decision-Theoretic Apparatus of Dempster-Shafer Theory", 
    "arxiv-id": "cs/0305019v1", 
    "author": "Johan Schubert", 
    "publish": "2003-05-16T15:07:09Z", 
    "summary": "Thomas M. Strat has developed a decision-theoretic apparatus for\nDempster-Shafer theory (Decision analysis using belief functions, Intern. J.\nApprox. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility\nintervals are constructed for different choices. The choice with the highest\nexpected utility is preferable to others. However, to find the preferred choice\nwhen the expected utility interval of one choice is included in that of\nanother, it is necessary to interpolate a discerning point in the intervals.\nThis is done by the parameter rho, defined as the probability that the\nambiguity about the utility of every nonsingleton focal element will turn out\nas favorable as possible. If there are several different decision makers, we\nmight sometimes be more interested in having the highest expected utility among\nthe decision makers rather than only trying to maximize our own expected\nutility regardless of choices made by other decision makers. The preference of\neach choice is then determined by the probability of yielding the highest\nexpected utility. This probability is equal to the maximal interval length of\nrho under which an alternative is preferred. We must here take into account not\nonly the choices already made by other decision makers but also the rational\nchoices we can assume to be made by later decision makers. In Strats apparatus,\nan assumption, unwarranted by the evidence at hand, has to be made about the\nvalue of rho. We demonstrate that no such assumption is necessary. It is\nsufficient to assume a uniform probability distribution for rho to be able to\ndiscern the most preferable choice. We discuss when this approach is\njustifiable."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0305044v2", 
    "title": "Updating beliefs with incomplete observations", 
    "arxiv-id": "cs/0305044v2", 
    "author": "Marco Zaffalon", 
    "publish": "2003-05-27T11:05:52Z", 
    "summary": "Currently, there is renewed interest in the problem, raised by Shafer in\n1985, of updating probabilities when observations are incomplete. This is a\nfundamental problem in general, and of particular interest for Bayesian\nnetworks. Recently, Grunwald and Halpern have shown that commonly used updating\nstrategies fail in this case, except under very special assumptions. In this\npaper we propose a new method for updating probabilities with incomplete\nobservations. Our approach is deliberately conservative: we make no assumptions\nabout the so-called incompleteness mechanism that associates complete with\nincomplete observations. We model our ignorance about this mechanism by a\nvacuous lower prevision, a tool from the theory of imprecise probabilities, and\nwe use only coherence arguments to turn prior into posterior probabilities. In\ngeneral, this new approach to updating produces lower and upper posterior\nprobabilities and expectations, as well as partially determinate decisions.\nThis is a logical consequence of the existing ignorance about the\nincompleteness mechanism. We apply the new approach to the problem of\nclassification of new evidence in probabilistic expert systems, where it leads\nto a new, so-called conservative updating rule. In the special case of Bayesian\nnetworks constructed using expert knowledge, we provide an exact algorithm for\nclassification based on our updating rule, which has linear-time complexity for\na class of networks wider than polytrees. This result is then extended to the\nmore general framework of credal networks, where computations are often much\nharder than with Bayesian nets. Using an example, we show that our rule appears\nto provide a solid basis for reliable updating with incomplete observations,\nwhen no strong assumptions about the incompleteness mechanism are justified."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0306124v1", 
    "title": "Updating Probabilities", 
    "arxiv-id": "cs/0306124v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2003-06-23T22:24:05Z", 
    "summary": "As examples such as the Monty Hall puzzle show, applying conditioning to\nupdate a probability distribution on a ``naive space'', which does not take\ninto account the protocol used, can often lead to counterintuitive results.\nHere we examine why. A criterion known as CAR (``coarsening at random'') in the\nstatistical literature characterizes when ``naive'' conditioning in a naive\nspace works. We show that the CAR condition holds rather infrequently, and we\nprovide a procedural characterization of it, by giving a randomized algorithm\nthat generates all and only distributions for which CAR holds. This\nsubstantially extends previous characterizations of CAR. We also consider more\ngeneralized notions of update such as Jeffrey conditioning and minimizing\nrelative entropy (MRE). We give a generalization of the CAR condition that\ncharacterizes when Jeffrey conditioning leads to appropriate answers, and show\nthat there exist some very simple settings in which MRE essentially never gives\nthe right results. This generalizes and interconnects previous results obtained\nin the literature on CAR and MRE."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0306135v1", 
    "title": "Pruning Isomorphic Structural Sub-problems in Configuration", 
    "arxiv-id": "cs/0306135v1", 
    "author": "Nicolas Prcovic", 
    "publish": "2003-06-27T11:25:17Z", 
    "summary": "Configuring consists in simulating the realization of a complex product from\na catalog of component parts, using known relations between types, and picking\nvalues for object attributes. This highly combinatorial problem in the field of\nconstraint programming has been addressed with a variety of approaches since\nthe foundation system R1(McDermott82). An inherent difficulty in solving\nconfiguration problems is the existence of many isomorphisms among\ninterpretations. We describe a formalism independent approach to improve the\ndetection of isomorphisms by configurators, which does not require to adapt the\nproblem model. To achieve this, we exploit the properties of a characteristic\nsubset of configuration problems, called the structural sub-problem, which\ncanonical solutions can be produced or tested at a limited cost. In this paper\nwe present an algorithm for testing the canonicity of configurations, that can\nbe added as a symmetry breaking constraint to any configurator. The cost and\nefficiency of this canonicity test are given."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307010v2", 
    "title": "Probabilistic Reasoning as Information Compression by Multiple   Alignment, Unification and Search: An Introduction and Overview", 
    "arxiv-id": "cs/0307010v2", 
    "author": "J Gerard Wolff", 
    "publish": "2003-07-04T16:34:45Z", 
    "summary": "This article introduces the idea that probabilistic reasoning (PR) may be\nunderstood as \"information compression by multiple alignment, unification and\nsearch\" (ICMAUS). In this context, multiple alignment has a meaning which is\nsimilar to but distinct from its meaning in bio-informatics, while unification\nmeans a simple merging of matching patterns, a meaning which is related to but\nsimpler than the meaning of that term in logic.\n  A software model, SP61, has been developed for the discovery and formation of\n'good' multiple alignments, evaluated in terms of information compression. The\nmodel is described in outline.\n  Using examples from the SP61 model, this article describes in outline how the\nICMAUS framework can model various kinds of PR including: PR in best-match\npattern recognition and information retrieval; one-step 'deductive' and\n'abductive' PR; inheritance of attributes in a class hierarchy; chains of\nreasoning (probabilistic decision networks and decision trees, and PR with\n'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with\ndefault values; modelling the function of a Bayesian network."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307025v1", 
    "title": "Information Compression by Multiple Alignment, Unification and Search as   a Unifying Principle in Computing and Cognition", 
    "arxiv-id": "cs/0307025v1", 
    "author": "J Gerard Wolff", 
    "publish": "2003-07-10T15:32:31Z", 
    "summary": "This article presents an overview of the idea that \"information compression\nby multiple alignment, unification and search\" (ICMAUS) may serve as a unifying\nprinciple in computing (including mathematics and logic) and in such aspects of\nhuman cognition as the analysis and production of natural language, fuzzy\npattern recognition and best-match information retrieval, concept hierarchies\nwith inheritance of attributes, probabilistic reasoning, and unsupervised\ninductive learning. The ICMAUS concepts are described together with an outline\nof the SP61 software model in which the ICMAUS concepts are currently realised.\nA range of examples is presented, illustrated with output from the SP61 model."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307048v2", 
    "title": "Integrating cardinal direction relations and other orientation relations   in Qualitative Spatial Reasoning", 
    "arxiv-id": "cs/0307048v2", 
    "author": "Amar Isli", 
    "publish": "2003-07-21T13:03:19Z", 
    "summary": "We propose a calculus integrating two calculi well-known in Qualitative\nSpatial Reasoning (QSR): Frank's projection-based cardinal direction calculus,\nand a coarser version of Freksa's relative orientation calculus. An original\nconstraint propagation procedure is presented, which implements the interaction\nbetween the two integrated calculi. The importance of taking into account the\ninteraction is shown with a real example providing an inconsistent knowledge\nbase, whose inconsistency (a) cannot be detected by reasoning separately about\neach of the two components of the knowledge, just because, taken separately,\neach is consistent, but (b) is detected by the proposed algorithm, thanks to\nthe interaction knowledge propagated from each of the two compnents to the\nother."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307050v1", 
    "title": "A ternary Relation Algebra of directed lines", 
    "arxiv-id": "cs/0307050v1", 
    "author": "Amar Isli", 
    "publish": "2003-07-21T16:01:11Z", 
    "summary": "We define a ternary Relation Algebra (RA) of relative position relations on\ntwo-dimensional directed lines (d-lines for short). A d-line has two degrees of\nfreedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The\nrepresentation of the RDF of a d-line will be handled by an RA of 2D\norientations, CYC_t, known in the literature. A second algebra, TA_t, which\nwill handle the TDF of a d-line, will be defined. The two algebras, CYC_t and\nTA_t, will constitute, respectively, the translational and the rotational\ncomponents of the RA, PA_t, of relative position relations on d-lines: the PA_t\natoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that\nare compatible. We present in detail the RA PA_t, with its converse table, its\nrotation table and its composition tables. We show that a (polynomial)\nconstraint propagation algorithm, known in the literature, is complete for a\nsubset of PA_t relations including almost all of the atomic relations. We will\ndiscuss the application scope of the RA, which includes incidence geometry, GIS\n(Geographic Information Systems), shape representation, localisation in\n(multi-)robot navigation, and the representation of motion prepositions in NLP\n(Natural Language Processing). We then compare the RA to existing ones, such as\nan algebra for reasoning about rectangles parallel to the axes of an\n(orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval\nalgebra, and an algebra for reasoning about 2D segments."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307056v1", 
    "title": "From Statistical Knowledge Bases to Degrees of Belief", 
    "arxiv-id": "cs/0307056v1", 
    "author": "Daphne Koller", 
    "publish": "2003-07-24T21:32:09Z", 
    "summary": "An intelligent agent will often be uncertain about various properties of its\nenvironment, and when acting in that environment it will frequently need to\nquantify its uncertainty. For example, if the agent wishes to employ the\nexpected-utility paradigm of decision theory to guide its actions, it will need\nto assign degrees of belief (subjective probabilities) to various assertions.\nOf course, these degrees of belief should not be arbitrary, but rather should\nbe based on the information available to the agent. This paper describes one\napproach for inducing degrees of belief from very rich knowledge bases, that\ncan include information about particular individuals, statistical correlations,\nphysical laws, and default rules. We call our approach the random-worlds\nmethod. The method is based on the principle of indifference: it treats all of\nthe worlds the agent considers possible as being equally likely. It is able to\nintegrate qualitative default reasoning with quantitative probabilistic\nreasoning by providing a language in which both types of information can be\neasily expressed. Our results show that a number of desiderata that arise in\ndirect inference (reasoning from statistical information to conclusions about\nindividuals) and default reasoning follow directly {from} the semantics of\nrandom worlds. For example, random worlds captures important patterns of\nreasoning such as specificity, inheritance, indifference to irrelevant\ninformation, and default assumptions of independence. Furthermore, the\nexpressive power of the language used and the intuitive semantics of random\nworlds allow the method to deal with problems that are beyond the scope of many\nother non-deductive reasoning systems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0307063v1", 
    "title": "An Alternative to RDF-Based Languages for the Representation and   Processing of Ontologies in the Semantic Web", 
    "arxiv-id": "cs/0307063v1", 
    "author": "J Gerard Wolff", 
    "publish": "2003-07-29T11:13:41Z", 
    "summary": "This paper describes an approach to the representation and processing of\nontologies in the Semantic Web, based on the ICMAUS theory of computation and\nAI. This approach has strengths that complement those of languages based on the\nResource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main\nbenefits of the ICMAUS approach are simplicity and comprehensibility in the\nrepresentation of ontologies, an ability to cope with errors and uncertainties\nin knowledge, and a versatile reasoning system with capabilities in the kinds\nof probabilistic reasoning that seem to be required in the Semantic Web."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0308002v3", 
    "title": "Quantifying and Visualizing Attribute Interactions", 
    "arxiv-id": "cs/0308002v3", 
    "author": "Ivan Bratko", 
    "publish": "2003-08-01T10:50:07Z", 
    "summary": "Interactions are patterns between several attributes in data that cannot be\ninferred from any subset of these attributes. While mutual information is a\nwell-established approach to evaluating the interactions between two\nattributes, we surveyed its generalizations as to quantify interactions between\nseveral attributes. We have chosen McGill's interaction information, which has\nbeen independently rediscovered a number of times under various names in\nvarious disciplines, because of its many intuitively appealing properties. We\napply interaction information to visually present the most important\ninteractions of the data. Visualization of interactions has provided insight\ninto the structure of data on a number of domains, identifying redundant\nattributes and opportunities for constructing new features, discovering\nunexpected regularities in data, and have helped during construction of\npredictive models; we illustrate the methods on numerous examples. A machine\nlearning method that disregards interactions may get caught in two traps:\nmyopia is caused by learning algorithms assuming independence in spite of\ninteractions, whereas fragmentation arises from assuming an interaction in\nspite of independence."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0309025v1", 
    "title": "Evidential Force Aggregation", 
    "arxiv-id": "cs/0309025v1", 
    "author": "Johan Schubert", 
    "publish": "2003-09-15T07:20:48Z", 
    "summary": "In this paper we develop an evidential force aggregation method intended for\nclassification of evidential intelligence into recognized force structures. We\nassume that the intelligence has already been partitioned into clusters and use\nthe classification method individually in each cluster. The classification is\nbased on a measure of fitness between template and fused intelligence that\nmakes it possible to handle intelligence reports with multiple nonspecific and\nuncertain propositions. With this measure we can aggregate on a level-by-level\nbasis, starting from general intelligence to achieve a complete force structure\nwith recognized units on all hierarchical levels."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0310023v1", 
    "title": "Application of Kullback-Leibler Metric to Speech Recognition", 
    "arxiv-id": "cs/0310023v1", 
    "author": "Pavel Lukin", 
    "publish": "2003-10-13T16:17:51Z", 
    "summary": "Article discusses the application of Kullback-Leibler divergence to the\nrecognition of speech signals and suggests three algorithms implementing this\ndivergence criterion: correlation algorithm, spectral algorithm and filter\nalgorithm. Discussion covers an approach to the problem of speech variability\nand is illustrated with the results of experimental modeling of speech signals.\nThe article gives a number of recommendations on the choice of appropriate\nmodel parameters and provides a comparison to some other methods of speech\nrecognition."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.817", 
    "link": "http://arxiv.org/pdf/cs/0310044v1", 
    "title": "The Algebra of Utility Inference", 
    "arxiv-id": "cs/0310044v1", 
    "author": "Ali E. Abbas", 
    "publish": "2003-10-23T01:13:20Z", 
    "summary": "Richard Cox [1] set the axiomatic foundations of probable inference and the\nalgebra of propositions. He showed that consistency within these axioms\nrequires certain rules for updating belief. In this paper we use the analogy\nbetween probability and utility introduced in [2] to propose an axiomatic\nfoundation for utility inference and the algebra of preferences. We show that\nconsistency within these axioms requires certain rules for updating preference.\nWe discuss a class of utility functions that stems from the axioms of utility\ninference and show that this class is the basic building block for any general\nmultiattribute utility function. We use this class of utility functions\ntogether with the algebra of preferences to construct utility functions\nrepresented by logical operations on the attributes."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0310045v1", 
    "title": "An information theory for preferences", 
    "arxiv-id": "cs/0310045v1", 
    "author": "Ali E. Abbas", 
    "publish": "2003-10-23T01:34:44Z", 
    "summary": "Recent literature in the last Maximum Entropy workshop introduced an analogy\nbetween cumulative probability distributions and normalized utility functions.\nBased on this analogy, a utility density function can de defined as the\nderivative of a normalized utility function. A utility density function is\nnon-negative and integrates to unity. These two properties form the basis of a\ncorrespondence between utility and probability. A natural application of this\nanalogy is a maximum entropy principle to assign maximum entropy utility\nvalues. Maximum entropy utility interprets many of the common utility functions\nbased on the preference information needed for their assignment, and helps\nassign utility values based on partial preference information. This paper\nreviews maximum entropy utility and introduces further results that stem from\nthe duality between probability and utility."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0310047v1", 
    "title": "Abductive Logic Programs with Penalization: Semantics, Complexity and   Implementation", 
    "arxiv-id": "cs/0310047v1", 
    "author": "Nicola Leone", 
    "publish": "2003-10-24T18:03:06Z", 
    "summary": "Abduction, first proposed in the setting of classical logics, has been\nstudied with growing interest in the logic programming area during the last\nyears.\n  In this paper we study abduction with penalization in the logic programming\nframework. This form of abductive reasoning, which has not been previously\nanalyzed in logic programming, turns out to represent several relevant\nproblems, including optimization problems, very naturally. We define a formal\nmodel for abduction with penalization over logic programs, which extends the\nabductive framework proposed by Kakas and Mancarella. We address knowledge\nrepresentation issues, encoding a number of problems in our abductive\nframework. In particular, we consider some relevant problems, taken from\ndifferent domains, ranging from optimization theory to diagnosis and planning;\ntheir encodings turn out to be simple and elegant in our formalism. We\nthoroughly analyze the computational complexity of the main problems arising in\nthe context of abduction with penalization from logic programs. Finally, we\nimplement a system supporting the proposed abductive framework on top of the\nDLV engine. To this end, we design a translation from abduction problems with\npenalties into logic programs with weak constraints. We prove that this\napproach is sound and complete."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0310061v1", 
    "title": "Local-search techniques for propositional logic extended with   cardinality constraints", 
    "arxiv-id": "cs/0310061v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2003-10-31T16:29:02Z", 
    "summary": "We study local-search satisfiability solvers for propositional logic extended\nwith cardinality atoms, that is, expressions that provide explicit ways to\nmodel constraints on cardinalities of sets. Adding cardinality atoms to the\nlanguage of propositional logic facilitates modeling search problems and often\nresults in concise encodings. We propose two ``native'' local-search solvers\nfor theories in the extended language. We also describe techniques to reduce\nthe problem to standard propositional satisfiability and allow us to use\noff-the-shelf SAT solvers. We study these methods experimentally. Our general\nfinding is that native solvers designed specifically for the extended language\nperform better than indirect methods relying on SAT solvers."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0310062v1", 
    "title": "WSAT(cc) - a fast local-search ASP solver", 
    "arxiv-id": "cs/0310062v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2003-10-31T16:46:07Z", 
    "summary": "We describe WSAT(cc), a local-search solver for computing models of theories\nin the language of propositional logic extended by cardinality atoms. WSAT(cc)\nis a processing back-end for the logic PS+, a recently proposed formalism for\nanswer-set programming."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311004v1", 
    "title": "Utility-Probability Duality", 
    "arxiv-id": "cs/0311004v1", 
    "author": "Jim Matheson", 
    "publish": "2003-11-06T07:33:23Z", 
    "summary": "This paper presents duality between probability distributions and utility\nfunctions."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311007v1", 
    "title": "Parametric Connectives in Disjunctive Logic Programming", 
    "arxiv-id": "cs/0311007v1", 
    "author": "Nicola Leone", 
    "publish": "2003-11-07T15:57:07Z", 
    "summary": "Disjunctive Logic Programming (\\DLP) is an advanced formalism for Knowledge\nRepresentation and Reasoning (KRR). \\DLP is very expressive in a precise\nmathematical sense: it allows to express every property of finite structures\nthat is decidable in the complexity class $\\SigmaP{2}$ ($\\NP^{\\NP}$).\nImportantly, the \\DLP encodings are often simple and natural.\n  In this paper, we single out some limitations of \\DLP for KRR, which cannot\nnaturally express problems where the size of the disjunction is not known ``a\npriori'' (like N-Coloring), but it is part of the input. To overcome these\nlimitations, we further enhance the knowledge modelling abilities of \\DLP, by\nextending this language by {\\em Parametric Connectives (OR and AND)}. These\nconnectives allow us to represent compactly the disjunction/conjunction of a\nset of atoms having a given property. We formally define the semantics of the\nnew language, named $DLP^{\\bigvee,\\bigwedge}$ and we show the usefulness of the\nnew constructs on relevant knowledge-based problems. We address implementation\nissues and discuss related works."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311024v1", 
    "title": "Logic-Based Specification Languages for Intelligent Software Agents", 
    "arxiv-id": "cs/0311024v1", 
    "author": "Leon Sterling", 
    "publish": "2003-11-20T10:10:25Z", 
    "summary": "The research field of Agent-Oriented Software Engineering (AOSE) aims to find\nabstractions, languages, methodologies and toolkits for modeling, verifying,\nvalidating and prototyping complex applications conceptualized as Multiagent\nSystems (MASs). A very lively research sub-field studies how formal methods can\nbe used for AOSE. This paper presents a detailed survey of six logic-based\nexecutable agent specification languages that have been chosen for their\npotential to be integrated in our ARPEGGIO project, an open framework for\nspecifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the\nIMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each\nexecutable language, the logic foundations are described and an example of use\nis shown. A comparison of the six languages and a survey of similar approaches\ncomplete the paper, together with considerations of the advantages of using\nlogic-based languages in MAS modeling and prototyping."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311026v1", 
    "title": "Great Expectations. Part I: On the Customizability of Generalized   Expected Utility", 
    "arxiv-id": "cs/0311026v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2003-11-20T17:36:53Z", 
    "summary": "We propose a generalization of expected utility that we call generalized EU\n(GEU), where a decision maker's beliefs are represented by plausibility\nmeasures, and the decision maker's tastes are represented by general (i.e.,not\nnecessarily real-valued) utility functions. We show that every agent,\n``rational'' or not, can be modeled as a GEU maximizer. We then show that we\ncan customize GEU by selectively imposing just the constraints we want. In\nparticular, we show how each of Savage's postulates corresponds to constraints\non GEU."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311027v1", 
    "title": "Great Expectations. Part II: Generalized Expected Utility as a Universal   Decision Rule", 
    "arxiv-id": "cs/0311027v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2003-11-20T17:39:22Z", 
    "summary": "Many different rules for decision making have been introduced in the\nliterature. We show that a notion of generalized expected utility proposed in\nPart I of this paper is a universal decision rule, in the sense that it can\nrepresent essentially all other decision rules."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311045v1", 
    "title": "Unsupervised Grammar Induction in a Framework of Information Compression   by Multiple Alignment, Unification and Search", 
    "arxiv-id": "cs/0311045v1", 
    "author": "J Gerard Wolff", 
    "publish": "2003-11-27T11:18:59Z", 
    "summary": "This paper describes a novel approach to grammar induction that has been\ndeveloped within a framework designed to integrate learning with other aspects\nof computing, AI, mathematics and logic. This framework, called \"information\ncompression by multiple alignment, unification and search\" (ICMAUS), is founded\non principles of Minimum Length Encoding pioneered by Solomonoff and others.\nMost of the paper describes SP70, a computer model of the ICMAUS framework that\nincorporates processes for unsupervised learning of grammars. An example is\npresented to show how the model can infer a plausible grammar from appropriate\ninput. Limitations of the current model and how they may be overcome are\nbriefly discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0311051v1", 
    "title": "Integrating existing cone-shaped and projection-based cardinal direction   relations and a TCSP-like decidable generalisation", 
    "arxiv-id": "cs/0311051v1", 
    "author": "Amar Isli", 
    "publish": "2003-11-28T04:06:56Z", 
    "summary": "We consider the integration of existing cone-shaped and projection-based\ncalculi of cardinal direction relations, well-known in QSR. The more general,\nintegrating language we consider is based on convex constraints of the\nqualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal\ndirection atomic relation, or of the quantitative form $(\\alpha ,\\beta)(x,y)$,\nwith $\\alpha ,\\beta\\in [0,2\\pi)$ and $(\\beta -\\alpha)\\in [0,\\pi ]$: the meaning\nof the quantitative constraint, in particular, is that point $x$ belongs to the\n(convex) cone-shaped area rooted at $y$, and bounded by angles $\\alpha$ and\n$\\beta$. The general form of a constraint is a disjunction of the form\n$[r_1\\vee...\\vee r_{n_1}\\vee (\\alpha_1,\\beta_1)\\vee...\\vee (\\alpha\n_{n_2},\\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\\alpha\n_i,\\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above:\nthe meaning of such a general constraint is that, for some $i=1... n_1$,\n$r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\\alpha_i,\\beta_i)(x,y)$ holds. A\nconjunction of such general constraints is a $\\tcsp$-like CSP, which we will\nrefer to as an $\\scsp$ (Spatial Constraint Satisfaction Problem). An effective\nsolution search algorithm for an $\\scsp$ will be described, which uses (1)\nconstraint propagation, based on a composition operation to be defined, as the\nfiltering method during the search, and (2) the Simplex algorithm, guaranteeing\ncompleteness, at the leaves of the search tree. The approach is particularly\nsuited for large-scale high-level vision, such as, e.g., satellite-like\nsurveillance of a geographic area."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0312020v1", 
    "title": "Modeling Object Oriented Constraint Programs in Z", 
    "arxiv-id": "cs/0312020v1", 
    "author": "Laurent Henocque", 
    "publish": "2003-12-12T10:15:38Z", 
    "summary": "Object oriented constraint programs (OOCPs) emerge as a leading evolution of\nconstraint programming and artificial intelligence, first applied to a range of\nindustrial applications called configuration problems. The rich variety of\ntechnical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP,\nTerminological systems, constraint programs with set variables ...) is a source\nof difficulty. No universally accepted formal language exists for communicating\nabout OOCPs, which makes the comparison of systems difficult. We present here a\nZ based specification of OOCPs which avoids the falltrap of hidden object\nsemantics. The object system is part of the specification, and captures all of\nthe most advanced notions from the object oriented modeling standard UML. The\npaper illustrates these issues and the conciseness and precision of Z by the\nspecification of a working OOCP that solves an historical AI problem : parsing\na context free grammar. Being written in Z, an OOCP specification also supports\nformal proofs. The whole builds the foundation of an adaptative and evolving\nframework for communicating about constrained object models and programs."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0312040v1", 
    "title": "Diagnostic reasoning with A-Prolog", 
    "arxiv-id": "cs/0312040v1", 
    "author": "Michael Gelfond", 
    "publish": "2003-12-18T13:38:49Z", 
    "summary": "In this paper we suggest an architecture for a software agent which operates\na physical device and is capable of making observations and of testing and\nrepairing the device's components. We present simplified definitions of the\nnotions of symptom, candidate diagnosis, and diagnosis which are based on the\ntheory of action language ${\\cal AL}$. The definitions allow one to give a\nsimple account of the agent's behavior in which many of the agent's tasks are\nreduced to computing stable models of logic programs."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0312045v1", 
    "title": "Weight Constraints as Nested Expressions", 
    "arxiv-id": "cs/0312045v1", 
    "author": "Vladimir Lifschitz", 
    "publish": "2003-12-19T16:00:43Z", 
    "summary": "We compare two recent extensions of the answer set (stable model) semantics\nof logic programs. One of them, due to Lifschitz, Tang and Turner, allows the\nbodies and heads of rules to contain nested expressions. The other, due to\nNiemela and Simons, uses weight constraints. We show that there is a simple,\nmodular translation from the language of weight constraints into the language\nof nested expressions that preserves the program's answer sets. Nested\nexpressions can be eliminated from the result of this translation in favor of\nadditional atoms. The translation makes it possible to compute answer sets for\nsome programs with weight constraints using satisfiability solvers, and to\nprove the strong equivalence of programs with weight constraints using the\nlogic of here-and there."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0312053v1", 
    "title": "On the Expressibility of Stable Logic Programming", 
    "arxiv-id": "cs/0312053v1", 
    "author": "Jeffrey B. Remmel", 
    "publish": "2003-12-22T18:34:21Z", 
    "summary": "(We apologize for pidgin LaTeX) Schlipf \\cite{sch91} proved that Stable Logic\nProgramming (SLP) solves all $\\mathit{NP}$ decision problems. We extend\nSchlipf's result to prove that SLP solves all search problems in the class\n$\\mathit{NP}$. Moreover, we do this in a uniform way as defined in \\cite{mt99}.\nSpecifically, we show that there is a single $\\mathrm{DATALOG}^{\\neg}$ program\n$P_{\\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$\nwith non-negative integer coefficients and any input $\\sigma$ of size $n$ over\na fixed alphabet $\\Sigma$, there is an extensional database\n$\\mathit{edb}_{M,p,\\sigma}$ such that there is a one-to-one correspondence\nbetween the stable models of $\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$\nand the accepting computations of the machine $M$ that reach the final state in\nat most $p(n)$ steps. Moreover, $\\mathit{edb}_{M,p,\\sigma}$ can be computed in\npolynomial time from $p$, $\\sigma$ and the description of $M$ and the decoding\nof such accepting computations from its corresponding stable model of\n$\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$ can be computed in linear\ntime. A similar statement holds for Default Logic with respect to\n$\\Sigma_2^\\mathrm{P}$-search problems\\footnote{The proof of this result\ninvolves additional technical complications and will be a subject of another\npublication.}."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0401009v1", 
    "title": "Unifying Computing and Cognition: The SP Theory and its Applications", 
    "arxiv-id": "cs/0401009v1", 
    "author": "J Gerard Wolff", 
    "publish": "2004-01-13T16:16:07Z", 
    "summary": "This book develops the conjecture that all kinds of information processing in\ncomputers and in brains may usefully be understood as \"information compression\nby multiple alignment, unification and search\". This \"SP theory\", which has\nbeen under development since 1987, provides a unified view of such things as\nthe workings of a universal Turing machine, the nature of 'knowledge', the\ninterpretation and production of natural language, pattern recognition and\nbest-match information retrieval, several kinds of probabilistic reasoning,\nplanning and problem solving, unsupervised learning, and a range of concepts in\nmathematics and logic. The theory also provides a basis for the design of an\n'SP' computer with several potential advantages compared with traditional\ndigital computers."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0402033v1", 
    "title": "Recycling Computed Answers in Rewrite Systems for Abduction", 
    "arxiv-id": "cs/0402033v1", 
    "author": "Jia-Huai You", 
    "publish": "2004-02-16T06:15:05Z", 
    "summary": "In rule-based systems, goal-oriented computations correspond naturally to the\npossible ways that an observation may be explained. In some applications, we\nneed to compute explanations for a series of observations with the same domain.\nThe question whether previously computed answers can be recycled arises. A yes\nanswer could result in substantial savings of repeated computations. For\nsystems based on classic logic, the answer is YES. For nonmonotonic systems\nhowever, one tends to believe that the answer should be NO, since recycling is\na form of adding information. In this paper, we show that computed answers can\nalways be recycled, in a nontrivial way, for the class of rewrite procedures\nthat we proposed earlier for logic programs with negation. We present some\nexperimental results on an encoding of the logistics domain."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0402035v1", 
    "title": "Memory As A Monadic Control Construct In Problem-Solving", 
    "arxiv-id": "cs/0402035v1", 
    "author": "Jean-Marie Chauvet", 
    "publish": "2004-02-16T17:02:29Z", 
    "summary": "Recent advances in programming languages study and design have established a\nstandard way of grounding computational systems representation in category\ntheory. These formal results led to a better understanding of issues of control\nand side-effects in functional and imperative languages. This framework can be\nsuccessfully applied to the investigation of the performance of Artificial\nIntelligence (AI) inference and cognitive systems. In this paper, we delineate\na categorical formalisation of memory as a control structure driving\nperformance in inference systems. Abstracting away control mechanisms from\nthree widely used representations of memory in cognitive systems (scripts,\nproduction rules and clusters) we explain how categorical triples capture the\ninteraction between learning and problem-solving."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0402057v2", 
    "title": "Integrating Defeasible Argumentation and Machine Learning Techniques", 
    "arxiv-id": "cs/0402057v2", 
    "author": "Carlos Ivan Ches\u00f1evar", 
    "publish": "2004-02-25T18:02:29Z", 
    "summary": "The field of machine learning (ML) is concerned with the question of how to\nconstruct algorithms that automatically improve with experience. In recent\nyears many successful ML applications have been developed, such as datamining\nprograms, information-filtering systems, etc. Although ML algorithms allow the\ndetection and extraction of interesting patterns of data for several kinds of\nproblems, most of these algorithms are based on quantitative reasoning, as they\nrely on training data in order to infer so-called target functions.\n  In the last years defeasible argumentation has proven to be a sound setting\nto formalize common-sense qualitative reasoning. This approach can be combined\nwith other inference techniques, such as those provided by machine learning\ntheory.\n  In this paper we outline different alternatives for combining defeasible\nargumentation and machine learning techniques. We suggest how different aspects\nof a generic argument-based framework can be integrated with other ML-based\napproaches."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0403002v2", 
    "title": "Epistemic Foundation of Stable Model Semantics", 
    "arxiv-id": "cs/0403002v2", 
    "author": "U. Straccia", 
    "publish": "2004-03-02T15:45:29Z", 
    "summary": "Stable model semantics has become a very popular approach for the management\nof negation in logic programming. This approach relies mainly on the closed\nworld assumption to complete the available knowledge and its formulation has\nits basis in the so-called Gelfond-Lifschitz transformation.\n  The primary goal of this work is to present an alternative and\nepistemic-based characterization of stable model semantics, to the\nGelfond-Lifschitz transformation. In particular, we show that stable model\nsemantics can be defined entirely as an extension of the Kripke-Kleene\nsemantics. Indeed, we show that the closed world assumption can be seen as an\nadditional source of `falsehood' to be added cumulatively to the Kripke-Kleene\nsemantics. Our approach is purely algebraic and can abstract from the\nparticular formalism of choice as it is based on monotone operators (under the\nknowledge order) over bilattices only."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0403006v1", 
    "title": "The role of behavior modifiers in representation development", 
    "arxiv-id": "cs/0403006v1", 
    "author": "Angelica Garcia-Vega", 
    "publish": "2004-03-05T12:53:57Z", 
    "summary": "We address the problem of the development of representations and their\nrelationship to the environment. We study a software agent which develops in a\nnetwork a representation of its simple environment which captures and\nintegrates the relationships between agent and environment through a closure\nmechanism. The inclusion of a variable behavior modifier allows better\nrepresentation development. This can be confirmed with an internal description\nof the closure mechanism, and with an external description of the properties of\nthe representation network."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0404011v1", 
    "title": "Parametric external predicates for the DLV System", 
    "arxiv-id": "cs/0404011v1", 
    "author": "M. C. Santoro", 
    "publish": "2004-04-05T17:15:45Z", 
    "summary": "This document describes syntax, semantics and implementation guidelines in\norder to enrich the DLV system with the possibility to make external C function\ncalls. This feature is realized by the introduction of parametric external\npredicates, whose extension is not specified through a logic program but\nimplicitly computed through external code."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0404012v1", 
    "title": "Toward the Implementation of Functions in the DLV System (Preliminary   Technical Report)", 
    "arxiv-id": "cs/0404012v1", 
    "author": "Nicola Leone", 
    "publish": "2004-04-05T17:23:07Z", 
    "summary": "This document describes the functions as they are treated in the DLV system.\nWe give first the language, then specify the main implementation issues."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0404051v1", 
    "title": "Knowledge And The Action Description Language A", 
    "arxiv-id": "cs/0404051v1", 
    "author": "Stuart R. Taylor", 
    "publish": "2004-04-24T14:16:04Z", 
    "summary": "We introduce Ak, an extension of the action description language A (Gelfond\nand Lifschitz, 1993) to handle actions which affect knowledge. We use sensing\nactions to increase an agent's knowledge of the world and non-deterministic\nactions to remove knowledge. We include complex plans involving conditionals\nand loops in our query language for hypothetical reasoning. We also present a\ntranslation of Ak domain descriptions into epistemic logic programs."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405008v1", 
    "title": "A Comparative Study of Fuzzy Classification Methods on Breast Cancer   Data", 
    "arxiv-id": "cs/0405008v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-04T23:02:53Z", 
    "summary": "In this paper, we examine the performance of four fuzzy rule generation\nmethods on Wisconsin breast cancer data. The first method generates fuzzy if\nthen rules using the mean and the standard deviation of attribute values. The\nsecond approach generates fuzzy if then rules using the histogram of attributes\nvalues. The third procedure generates fuzzy if then rules with certainty of\neach attribute into homogeneous fuzzy sets. In the fourth approach, only\noverlapping areas are partitioned. The first two approaches generate a single\nfuzzy if then rule for each class by specifying the membership function of each\nantecedent fuzzy set using the information about attribute values of training\npatterns. The other two approaches are based on fuzzy grids with homogeneous\nfuzzy partitions of each attribute. The performance of each approach is\nevaluated on breast cancer data sets. Simulation results show that the Modified\ngrid approach has a high classification rate of 99.73 %."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405009v1", 
    "title": "Intelligent Systems: Architectures and Perspectives", 
    "arxiv-id": "cs/0405009v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-04T23:48:39Z", 
    "summary": "The integration of different learning and adaptation techniques to overcome\nindividual limitations and to achieve synergetic effects through the\nhybridization or fusion of these techniques has, in recent years, contributed\nto a large number of new intelligent system designs. Computational intelligence\nis an innovative framework for constructing intelligent hybrid architectures\ninvolving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic\nReasoning (PR) and derivative free optimization techniques such as Evolutionary\nComputation (EC). Most of these hybridization approaches, however, follow an ad\nhoc design methodology, justified by success in certain application domains.\nDue to the lack of a common framework it often remains difficult to compare the\nvarious hybrid systems conceptually and to evaluate their performance\ncomparatively. This chapter introduces the different generic architectures for\nintegrating intelligent systems. The designing aspects and perspectives of\ndifferent hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC\nsystems are presented. Some conclusions are also provided towards the end."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405010v1", 
    "title": "A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria", 
    "arxiv-id": "cs/0405010v1", 
    "author": "Baikunth Nath", 
    "publish": "2004-05-05T00:27:53Z", 
    "summary": "Neuro-fuzzy systems have attracted growing interest of researchers in various\nscientific and engineering areas due to the increasing need of intelligent\nsystems. This paper evaluates the use of two popular soft computing techniques\nand conventional statistical approach based on Box--Jenkins autoregressive\nintegrated moving average (ARIMA) model to predict electricity demand in the\nState of Victoria, Australia. The soft computing methods considered are an\nevolving fuzzy neural network (EFuNN) and an artificial neural network (ANN)\ntrained using scaled conjugate gradient algorithm (CGA) and backpropagation\n(BP) algorithm. The forecast accuracy is compared with the forecasts used by\nVictorian Power Exchange (VPX) and the actual energy demand. To evaluate, we\nconsidered load demand patterns for 10 consecutive months taken every 30 min\nfor training the different prediction models. Test results show that the\nneuro-fuzzy system performed better than neural networks, ARIMA model and the\nVPX forecasts."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405011v1", 
    "title": "Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques", 
    "arxiv-id": "cs/0405011v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-05T00:32:52Z", 
    "summary": "Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)\nhave attracted the growing interest of researchers in various scientific and\nengineering areas due to the growing need of adaptive intelligent systems to\nsolve the real world problems. ANN learns from scratch by adjusting the\ninterconnections between layers. FIS is a popular computing framework based on\nthe concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The\nadvantages of a combination of ANN and FIS are obvious. There are several\napproaches to integrate ANN and FIS and very often it depends on the\napplication. We broadly classify the integration of ANN and FIS into three\ncategories namely concurrent model, cooperative model and fully fused model.\nThis paper starts with a discussion of the features of each model and\ngeneralize the advantages and deficiencies of each model. We further focus the\nreview on the different types of fused neuro-fuzzy systems and citing the\nadvantages and disadvantages of each model."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405012v1", 
    "title": "Is Neural Network a Reliable Forecaster on Earth? A MARS Query!", 
    "arxiv-id": "cs/0405012v1", 
    "author": "Dan Steinberg", 
    "publish": "2004-05-05T00:36:17Z", 
    "summary": "Long-term rainfall prediction is a challenging task especially in the modern\nworld where we are facing the major environmental problem of global warming. In\ngeneral, climate and rainfall are highly non-linear phenomena in nature\nexhibiting what is known as the butterfly effect. While some regions of the\nworld are noticing a systematic decrease in annual rainfall, others notice\nincreases in flooding and severe storms. The global nature of this phenomenon\nis very complicated and requires sophisticated computer modeling and simulation\nto predict accurately. In this paper, we report a performance analysis for\nMultivariate Adaptive Regression Splines (MARS)and artificial neural networks\nfor one month ahead prediction of rainfall. To evaluate the prediction\nefficiency, we made use of 87 years of rainfall data in Kerala state, the\nsouthern part of the Indian peninsula situated at latitude -longitude pairs\n(8o29'N - 76o57' E). We used an artificial neural network trained using the\nscaled conjugate gradient algorithm. The neural network and MARS were trained\nwith 40 years of rainfall data. For performance evaluation, network predicted\noutputs were compared with the actual rainfall data. Simulation results reveal\nthat MARS is a good forecasting tool and performed better than the considered\nneural network."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405013v1", 
    "title": "DCT Based Texture Classification Using Soft Computing Approach", 
    "arxiv-id": "cs/0405013v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-05T00:44:12Z", 
    "summary": "Classification of texture pattern is one of the most important problems in\npattern recognition. In this paper, we present a classification method based on\nthe Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works\non gray level image, the color scheme of each image is transformed into gray\nlevels. For classifying the images using DCT we used two popular soft computing\ntechniques namely neurocomputing and neuro-fuzzy computing. We used a\nfeedforward neural network trained using the backpropagation learning and an\nevolving fuzzy neural network to classify the textures. The soft computing\nmodels were trained using 80% of the texture data and remaining was used for\ntesting and validation purposes. A performance comparison was made among the\nsoft computing models for the texture classification problem. We also analyzed\nthe effects of prolonged training of neural networks. It is observed that the\nproposed neuro-fuzzy model performed better than neural network."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405014v1", 
    "title": "Estimating Genome Reversal Distance by Genetic Algorithm", 
    "arxiv-id": "cs/0405014v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-05T00:57:34Z", 
    "summary": "Sorting by reversals is an important problem in inferring the evolutionary\nrelationship between two genomes. The problem of sorting unsigned permutation\nhas been proven to be NP-hard. The best guaranteed error bounded is the 3/2-\napproximation algorithm. However, the problem of sorting signed permutation can\nbe solved easily. Fast algorithms have been developed both for finding the\nsorting sequence and finding the reversal distance of signed permutation. In\nthis paper, we present a way to view the problem of sorting unsigned\npermutation as signed permutation. And the problem can then be seen as\nsearching an optimal signed permutation in all n2 corresponding signed\npermutations. We use genetic algorithm to conduct the search. Our experimental\nresult shows that the proposed method outperform the 3/2-approximation\nalgorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405016v1", 
    "title": "Intrusion Detection Systems Using Adaptive Regression Splines", 
    "arxiv-id": "cs/0405016v1", 
    "author": "Vitorino Ramos", 
    "publish": "2004-05-05T02:22:16Z", 
    "summary": "Past few years have witnessed a growing recognition of intelligent techniques\nfor the construction of efficient and reliable intrusion detection systems. Due\nto increasing incidents of cyber attacks, building effective intrusion\ndetection systems (IDS) are essential for protecting information systems\nsecurity, and yet it remains an elusive goal and a great challenge. In this\npaper, we report a performance analysis between Multivariate Adaptive\nRegression Splines (MARS), neural networks and support vector machines. The\nMARS procedure builds flexible regression models by fitting separate splines to\ndistinct intervals of the predictor variables. A brief comparison of different\nneural network learning algorithms is also given."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405017v1", 
    "title": "Data Mining Approach for Analyzing Call Center Performance", 
    "arxiv-id": "cs/0405017v1", 
    "author": "Ruiyuan Guo", 
    "publish": "2004-05-05T02:27:43Z", 
    "summary": "The aim of our research was to apply well-known data mining techniques (such\nas linear neural networks, multi-layered perceptrons, probabilistic neural\nnetworks, classification and regression trees, support vector machines and\nfinally a hybrid decision tree neural network approach) to the problem of\npredicting the quality of service in call centers; based on the performance\ndata actually collected in a call center of a large insurance company. Our aim\nwas two-fold. First, to compare the performance of models built using the\nabove-mentioned techniques and, second, to analyze the characteristics of the\ninput sensitivity in order to better understand the relationship between the\nperform-ance evaluation process and the actual performance and in this way help\nimprove the performance of call centers. In this paper we summarize our\nfindings."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405018v1", 
    "title": "Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms", 
    "arxiv-id": "cs/0405018v1", 
    "author": "P. Saratchandran", 
    "publish": "2004-05-05T02:38:25Z", 
    "summary": "The use of intelligent systems for stock market predictions has been widely\nestablished. In this paper, we investigate how the seemingly chaotic behavior\nof stock markets could be well represented using several connectionist\nparadigms and soft computing techniques. To demonstrate the different\ntechniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P\nCNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4\nyear's NIFTY index values. This paper investigates the development of a\nreliable and efficient technique to model the seemingly chaotic behavior of\nstock markets. We considered an artificial neural network trained using\nLevenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno\nneuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper\nbriefly explains how the different connectionist paradigms could be formulated\nusing different learning methods and then investigates whether they can provide\nthe required level of performance, which are sufficiently good and robust so as\nto provide a reliable forecast model for stock market indices. Experiment\nresults reveal that all the connectionist paradigms considered could represent\nthe stock indices behavior very accurately."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405019v1", 
    "title": "Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision   Making Problems", 
    "arxiv-id": "cs/0405019v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-05T02:44:41Z", 
    "summary": "The purpose of this paper is to point to the usefulness of applying a linear\nmathematical formulation of fuzzy multiple criteria objective decision methods\nin organising business activities. In this respect fuzzy parameters of linear\nprogramming are modelled by preference-based membership functions. This paper\nbegins with an introduction and some related research followed by some\nfundamentals of fuzzy set theory and technical concepts of fuzzy multiple\nobjective decision models. Further a real case study of a manufacturing plant\nand the implementation of the proposed technique is presented. Empirical\nresults clearly show the superiority of the fuzzy technique in optimising\nindividual objective functions when compared to non-fuzzy approach.\nFurthermore, for the problem considered, the optimal solution helps to infer\nthat by incorporating fuzziness in a linear programming model either in\nconstraints, or both in objective functions and constraints, provides a similar\n(or even better) level of satisfaction for obtained results compared to\nnon-fuzzy linear programming."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405024v1", 
    "title": "Meta-Learning Evolutionary Artificial Neural Networks", 
    "arxiv-id": "cs/0405024v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-06T13:44:20Z", 
    "summary": "In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial\nNeural Network), an automatic computational framework for the adaptive\noptimization of artificial neural networks wherein the neural network\narchitecture, activation function, connection weights; learning algorithm and\nits parameters are adapted according to the problem. We explored the\nperformance of MLEANN and conventionally designed artificial neural networks\nfor function approximation problems. To evaluate the comparative performance,\nwe used three different well-known chaotic time series. We also present the\nstate of the art popular neural network learning algorithms and some\nexperimentation results related to convergence speed and generalization\nperformance. We explored the performance of backpropagation algorithm;\nconjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt\nalgorithm for the three chaotic time series. Performances of the different\nlearning algorithms were evaluated when the activation functions and\narchitecture were changed. We further present the theoretical background,\nalgorithm, design strategy and further demonstrate how effective and inevitable\nis the proposed MLEANN framework to design a neural network, which is smaller,\nfaster and with a better generalization performance."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.1751362", 
    "link": "http://arxiv.org/pdf/cs/0405025v1", 
    "title": "The Largest Compatible Subset Problem for Phylogenetic Data", 
    "arxiv-id": "cs/0405025v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-06T13:52:23Z", 
    "summary": "The phylogenetic tree construction is to infer the evolutionary relationship\nbetween species from the experimental data. However, the experimental data are\noften imperfect and conflicting each others. Therefore, it is important to\nextract the motif from the imperfect data. The largest compatible subset\nproblem is that, given a set of experimental data, we want to discard the\nminimum such that the remaining is compatible. The largest compatible subset\nproblem can be viewed as the vertex cover problem in the graph theory that has\nbeen proven to be NP-hard. In this paper, we propose a hybrid Evolutionary\nComputing (EC) method for this problem. The proposed method combines the EC\napproach and the algorithmic approach for special structured graphs. As a\nresult, the complexity of the problem is dramatically reduced. Experiments were\nperformed on randomly generated graphs with different edge densities. The\nvertex covers produced by the proposed method were then compared to the vertex\ncovers produced by a 2-approximation algorithm. The experimental results showed\nthat the proposed method consistently outperformed a classical 2- approximation\nalgorithm. Furthermore, a significant improvement was found when the graph\ndensity was small."
},{
    "category": "cs.AI", 
    "doi": "10.1109/FUZZ.2003.1206584", 
    "link": "http://arxiv.org/pdf/cs/0405026v1", 
    "title": "A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems", 
    "arxiv-id": "cs/0405026v1", 
    "author": "Lakhmi Jain", 
    "publish": "2004-05-06T13:58:41Z", 
    "summary": "Decision-making is a process of choosing among alternative courses of action\nfor solving complicated problems where multi-criteria objectives are involved.\nThe past few years have witnessed a growing recognition of Soft Computing\ntechnologies that underlie the conception, design and utilization of\nintelligent systems. Several works have been done where engineers and\nscientists have applied intelligent techniques and heuristics to obtain optimal\ndecisions from imprecise information. In this paper, we present a concurrent\nfuzzy-neural network approach combining unsupervised and supervised learning\ntechniques to develop the Tactical Air Combat Decision Support System (TACDSS).\nExperiment results clearly demonstrate the efficiency of the proposed\ntechnique."
},{
    "category": "cs.AI", 
    "doi": "10.1109/FUZZ.2002.1006749", 
    "link": "http://arxiv.org/pdf/cs/0405028v1", 
    "title": "Analysis of Hybrid Soft and Hard Computing Techniques for Forex   Monitoring Systems", 
    "arxiv-id": "cs/0405028v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-07T00:10:07Z", 
    "summary": "In a universe with a single currency, there would be no foreign exchange\nmarket, no foreign exchange rates, and no foreign exchange. Over the past\ntwenty-five years, the way the market has performed those tasks has changed\nenormously. The need for intelligent monitoring systems has become a necessity\nto keep track of the complex forex market. The vast currency market is a\nforeign concept to the average individual. However, once it is broken down into\nsimple terms, the average individual can begin to understand the foreign\nexchange market and use it as a financial instrument for future investing. In\nthis paper, we attempt to compare the performance of hybrid soft computing and\nhard computing techniques to predict the average monthly forex rates one month\nahead. The soft computing models considered are a neural network trained by the\nscaled conjugate gradient algorithm and a neuro-fuzzy model implementing a\nTakagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive\nRegression Splines (MARS), Classification and Regression Trees (CART) and a\nhybrid CART-MARS technique. We considered the exchange rates of Australian\ndollar with respect to US dollar, Singapore dollar, New Zealand dollar,\nJapanese yen and United Kingdom pounds. The models were trained using 70% of\nthe data and remaining was used for testing and validation purposes. It is\nobserved that the proposed hybrid models could predict the forex rates more\naccurately than all the techniques when applied individually. Empirical results\nalso reveal that the hybrid hard computing approach also improved some of our\nprevious work using a neuro-fuzzy approach."
},{
    "category": "cs.AI", 
    "doi": "10.1109/FUZZ.2002.1006749", 
    "link": "http://arxiv.org/pdf/cs/0405030v1", 
    "title": "Business Intelligence from Web Usage Mining", 
    "arxiv-id": "cs/0405030v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-06T23:54:39Z", 
    "summary": "The rapid e-commerce growth has made both business community and customers\nface a new situation. Due to intense competition on one hand and the customer's\noption to choose from several alternatives business community has realized the\nnecessity of intelligent marketing strategies and relationship management. Web\nusage mining attempts to discover useful knowledge from the secondary data\nobtained from the interactions of the users with the Web. Web usage mining has\nbecome very critical for effective Web site management, creating adaptive Web\nsites, business and support services, personalization, network traffic flow\nanalysis and so on. In this paper, we present the important concepts of Web\nusage mining and its various practical applications. We further present a novel\napproach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture\nof a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy\ninference system to analyze the Web site visitor trends. A hybrid evolutionary\nfuzzy clustering algorithm is proposed in this paper to optimally segregate\nsimilar user interests. The clustered data is then used to analyze the trends\nusing a Takagi-Sugeno fuzzy inference system learned using a combination of\nevolutionary algorithm and neural network learning. Proposed approach is\ncompared with self-organizing maps (to discover patterns) and several function\napproximation techniques like neural networks, linear genetic programming and\nTakagi-Sugeno fuzzy inference system (to analyze the clusters). The results are\ngraphically illustrated and the practical significance is discussed in detail.\nEmpirical results clearly show that the proposed Web usage-mining framework is\nefficient."
},{
    "category": "cs.AI", 
    "doi": "10.1109/FUZZ.2002.1006749", 
    "link": "http://arxiv.org/pdf/cs/0405031v1", 
    "title": "Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic   Approach for Tactical Air Combat Decision Support System", 
    "arxiv-id": "cs/0405031v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-06T23:58:46Z", 
    "summary": "Normally a decision support system is build to solve problem where\nmulti-criteria decisions are involved. The knowledge base is the vital part of\nthe decision support containing the information or data that is used in\ndecision-making process. This is the field where engineers and scientists have\napplied several intelligent techniques and heuristics to obtain optimal\ndecisions from imprecise information. In this paper, we present a hybrid\nneuro-genetic learning approach for the adaptation a Mamdani fuzzy inference\nsystem for the Tactical Air Combat Decision Support System (TACDSS). Some\nsimulation results demonstrating the difference of the learning techniques and\nare also provided."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ISIC.2002.1157784", 
    "link": "http://arxiv.org/pdf/cs/0405032v1", 
    "title": "EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using   Neural Network Learning and Evolutionary Computation", 
    "arxiv-id": "cs/0405032v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-07T00:01:54Z", 
    "summary": "Several adaptation techniques have been investigated to optimize fuzzy\ninference systems. Neural network learning algorithms have been used to\ndetermine the parameters of fuzzy inference system. Such models are often\ncalled as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model\nthere is no guarantee that the neural network learning algorithm converges and\nthe tuning of fuzzy inference system will be successful. Success of\nevolutionary search procedures for optimization of fuzzy inference system is\nwell proven and established in many application areas. In this paper, we will\nexplore how the optimization of fuzzy inference systems could be further\nimproved using a meta-heuristic approach combining neural network learning and\nevolutionary computation. The proposed technique could be considered as a\nmethodology to integrate neural networks, fuzzy inference systems and\nevolutionary search procedures. We present the theoretical frameworks and some\nexperimental results to demonstrate the efficiency of the proposed technique."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405033v1", 
    "title": "Optimization of Evolutionary Neural Networks Using Hybrid Learning   Algorithms", 
    "arxiv-id": "cs/0405033v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-07T00:08:16Z", 
    "summary": "Evolutionary artificial neural networks (EANNs) refer to a special class of\nartificial neural networks (ANNs) in which evolution is another fundamental\nform of adaptation in addition to learning. Evolutionary algorithms are used to\nadapt the connection weights, network architecture and learning algorithms\naccording to the problem environment. Even though evolutionary algorithms are\nwell known as efficient global search algorithms, very often they miss the best\nlocal solutions in the complex solution space. In this paper, we propose a\nhybrid meta-heuristic learning approach combining evolutionary learning and\nlocal search methods (using 1st and 2nd order error information) to improve the\nlearning and faster convergence obtained using a direct evolutionary approach.\nThe proposed technique is tested on three different chaotic time series and the\ntest results are compared with some popular neuro-fuzzy systems and a recently\ndeveloped cutting angle method of global optimization. Empirical results reveal\nthat the proposed technique is efficient in spite of the computational\ncomplexity."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405049v1", 
    "title": "Export Behaviour Modeling Using EvoNF Approach", 
    "arxiv-id": "cs/0405049v1", 
    "author": "Sonja Petrovic-Lazarevic", 
    "publish": "2004-05-16T03:24:55Z", 
    "summary": "The academic literature suggests that the extent of exporting by\nmultinational corporation subsidiaries (MCS) depends on their product\nmanufactured, resources, tax protection, customers and markets, involvement\nstrategy, financial independence and suppliers' relationship with a\nmultinational corporation (MNC). The aim of this paper is to model the complex\nexport pattern behaviour using a Takagi-Sugeno fuzzy inference system in order\nto determine the actual volume of MCS export output (sales exported). The\nproposed fuzzy inference system is optimised by using neural network learning\nand evolutionary computation. Empirical results clearly show that the proposed\napproach could model the export behaviour reasonable well compared to a direct\nneural network approach."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405050v1", 
    "title": "Traffic Accident Analysis Using Decision Trees and Neural Networks", 
    "arxiv-id": "cs/0405050v1", 
    "author": "Marcin Paprzycki", 
    "publish": "2004-05-16T03:33:20Z", 
    "summary": "The costs of fatalities and injuries due to traffic accident have a great\nimpact on society. This paper presents our research to model the severity of\ninjury resulting from traffic accidents using artificial neural networks and\ndecision trees. We have applied them to an actual data set obtained from the\nNational Automotive Sampling System (NASS) General Estimates System (GES).\nExperiment results reveal that in all the cases the decision tree outperforms\nthe neural network. Our research analysis also shows that the three most\nimportant factors in fatal injury are: driver's seat belt usage, light\ncondition of the roadway, and driver's alcohol usage."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405051v1", 
    "title": "Short Term Load Forecasting Models in Czech Republic Using Soft   Computing Paradigms", 
    "arxiv-id": "cs/0405051v1", 
    "author": "Ajith Abraham", 
    "publish": "2004-05-16T03:44:06Z", 
    "summary": "This paper presents a comparative study of six soft computing models namely\nmultilayer perceptron networks, Elman recurrent neural network, radial basis\nfunction network, Hopfield model, fuzzy inference system and hybrid fuzzy\nneural network for the hourly electricity demand forecast of Czech Republic.\nThe soft computing models were trained and tested using the actual hourly load\ndata for seven years. A comparison of the proposed techniques is presented for\npredicting 2 day ahead demands for electricity. Simulation results indicate\nthat hybrid fuzzy neural network and radial basis function networks are the\nbest candidates for the analysis and forecasting of electricity demand."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405052v1", 
    "title": "Decision Support Systems Using Intelligent Paradigms", 
    "arxiv-id": "cs/0405052v1", 
    "author": "Lakhmi Jain", 
    "publish": "2004-05-16T03:50:05Z", 
    "summary": "Decision-making is a process of choosing among alternative courses of action\nfor solving complicated problems where multi-criteria objectives are involved.\nThe past few years have witnessed a growing recognition of Soft Computing (SC)\ntechnologies that underlie the conception, design and utilization of\nintelligent systems. In this paper, we present different SC paradigms involving\nan artificial neural network trained using the scaled conjugate gradient\nalgorithm, two different fuzzy inference methods optimised using neural network\nlearning/evolutionary algorithms and regression trees for developing\nintelligent decision support systems. We demonstrate the efficiency of the\ndifferent algorithms by developing a decision support system for a Tactical Air\nCombat Environment (TACE). Some empirical comparisons between the different\nalgorithms are also provided."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405071v1", 
    "title": "Regression with respect to sensing actions and partial states", 
    "arxiv-id": "cs/0405071v1", 
    "author": "Tran Cao Son", 
    "publish": "2004-05-21T12:43:19Z", 
    "summary": "In this paper, we present a state-based regression function for planning\ndomains where an agent does not have complete information and may have sensing\nactions. We consider binary domains and employ the 0-approximation [Son & Baral\n2001] to define the regression function. In binary domains, the use of\n0-approximation means using 3-valued states. Although planning using this\napproach is incomplete with respect to the full semantics, we adopt it to have\na lower complexity. We prove the soundness and completeness of our regression\nformulation with respect to the definition of progression. More specifically,\nwe show that (i) a plan obtained through regression for a planning problem is\nindeed a progression solution of that planning problem, and that (ii) for each\nplan found through progression, using regression one obtains that plan or an\nequivalent one. We then develop a conditional planner that utilizes our\nregression function. We prove the soundness and completeness of our planning\nalgorithm and present experimental results with respect to several well known\nplanning problems in the literature."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405090v1", 
    "title": "Propositional Defeasible Logic has Linear Complexity", 
    "arxiv-id": "cs/0405090v1", 
    "author": "Michael J. Maher", 
    "publish": "2004-05-24T15:45:59Z", 
    "summary": "Defeasible logic is a rule-based nonmonotonic logic, with both strict and\ndefeasible rules, and a priority relation on rules. We show that inference in\nthe propositional form of the logic can be performed in linear time. This\ncontrasts markedly with most other propositional nonmonotonic logics, in which\ninference is intractable."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405106v1", 
    "title": "Pruning Search Space in Defeasible Argumentation", 
    "arxiv-id": "cs/0405106v1", 
    "author": "Alejandro Javier Garc\u00eda", 
    "publish": "2004-05-27T18:43:39Z", 
    "summary": "Defeasible argumentation has experienced a considerable growth in AI in the\nlast decade. Theoretical results have been combined with development of\npractical applications in AI & Law, Case-Based Reasoning and various\nknowledge-based systems. However, the dialectical process associated with\ninference is computationally expensive. This paper focuses on speeding up this\ninference process by pruning the involved search space. Our approach is\ntwofold. On one hand, we identify distinguished literals for computing defeat.\nOn the other hand, we restrict ourselves to a subset of all possible\nconflicting arguments by introducing dialectical constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0405113v2", 
    "title": "A proposal to design expert system for the calculations in the domain of   QFT", 
    "arxiv-id": "cs/0405113v2", 
    "author": "Andrea Severe", 
    "publish": "2004-05-31T10:50:23Z", 
    "summary": "Main purposes of the paper are followings: 1) To show examples of the\ncalculations in domain of QFT via ``derivative rules'' of an expert system; 2)\nTo consider advantages and disadvantage that technology of the calculations; 3)\nTo reflect about how one would develop new physical theories, what knowledge\nwould be useful in their investigations and how this problem can be connected\nwith designing an expert system."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0406038v1", 
    "title": "A New Approach to Draw Detection by Move Repetition in Computer Chess   Programming", 
    "arxiv-id": "cs/0406038v1", 
    "author": "Djordje Vidanovic", 
    "publish": "2004-06-21T13:42:03Z", 
    "summary": "We will try to tackle both the theoretical and practical aspects of a very\nimportant problem in chess programming as stated in the title of this article -\nthe issue of draw detection by move repetition. The standard approach that has\nso far been employed in most chess programs is based on utilising positional\nmatrices in original and compressed format as well as on the implementation of\nthe so-called bitboard format.\n  The new approach that we will be trying to introduce is based on using\nvariant strings generated by the search algorithm (searcher) during the tree\nexpansion in decision making. We hope to prove that this approach is more\nefficient than the standard treatment of the issue, especially in positions\nwith few pieces (endgames). To illustrate what we have in mind a machine\nlanguage routine that implements our theoretical assumptions is attached. The\nroutine is part of the Axon chess program, developed by the authors. Axon, in\nits current incarnation, plays chess at master strength (ca. 2400-2450 Elo,\nbased on both Axon vs computer programs and Axon vs human masters in over 3000\ngames altogether)."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0407008v1", 
    "title": "Autogenic Training With Natural Language Processing Modules: A Recent   Tool For Certain Neuro Cognitive Studies", 
    "arxiv-id": "cs/0407008v1", 
    "author": "M. N. Karthik", 
    "publish": "2004-07-02T20:15:02Z", 
    "summary": "Learning to respond to voice-text input involves the subject's ability in\nunderstanding the phonetic and text based contents and his/her ability to\ncommunicate based on his/her experience. The neuro-cognitive facility of the\nsubject has to support two important domains in order to make the learning\nprocess complete. In many cases, though the understanding is complete, the\nresponse is partial. This is one valid reason why we need to support the\ninformation from the subject with scalable techniques such as Natural Language\nProcessing (NLP) for abstraction of the contents from the output. This paper\nexplores the feasibility of using NLP modules interlaced with Neural Networks\nto perform the required task in autogenic training related to medical\napplications."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0407037v1", 
    "title": "Generalized Evolutionary Algorithm based on Tsallis Statistics", 
    "arxiv-id": "cs/0407037v1", 
    "author": "Shalabh Bhatnagar", 
    "publish": "2004-07-16T06:08:22Z", 
    "summary": "Generalized evolutionary algorithm based on Tsallis canonical distribution is\nproposed. The algorithm uses Tsallis generalized canonical distribution to\nweigh the configurations for `selection' instead of Gibbs-Boltzmann\ndistribution. Our simulation results show that for an appropriate choice of\nnon-extensive index that is offered by Tsallis statistics, evolutionary\nalgorithms based on this generalization outperform algorithms based on\nGibbs-Boltzmann distribution."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0407040v1", 
    "title": "Decomposition Based Search - A theoretical and experimental evaluation", 
    "arxiv-id": "cs/0407040v1", 
    "author": "M. Milano", 
    "publish": "2004-07-16T13:38:19Z", 
    "summary": "In this paper we present and evaluate a search strategy called Decomposition\nBased Search (DBS) which is based on two steps: subproblem generation and\nsubproblem solution. The generation of subproblems is done through value\nranking and domain splitting. Subdomains are explored so as to generate,\naccording to the heuristic chosen, promising subproblems first.\n  We show that two well known search strategies, Limited Discrepancy Search\n(LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First\nwe present a tuning of DBS that visits the same search nodes as IB, but avoids\nrestarts. Then we compare both theoretically and computationally DBS and LDS\nusing the same heuristic. We prove that DBS has a higher probability of being\nsuccessful than LDS on a comparable number of nodes, under realistic\nassumptions. Experiments on a constraint satisfaction problem and an\noptimization problem show that DBS is indeed very effective if compared to LDS."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0407042v1", 
    "title": "Postponing Branching Decisions", 
    "arxiv-id": "cs/0407042v1", 
    "author": "Michela Milano", 
    "publish": "2004-07-16T14:37:11Z", 
    "summary": "Solution techniques for Constraint Satisfaction and Optimisation Problems\noften make use of backtrack search methods, exploiting variable and value\nordering heuristics. In this paper, we propose and analyse a very simple method\nto apply in case the value ordering heuristic produces ties: postponing the\nbranching decision. To this end, we group together values in a tie, branch on\nthis sub-domain, and defer the decision among them to lower levels of the\nsearch tree. We show theoretically and experimentally that this simple\nmodification can dramatically improve the efficiency of the search strategy.\nAlthough in practise similar methods may have been applied already, to our\nknowledge, no empirical or theoretical study has been proposed in the\nliterature to identify when and to what extent this strategy should be used."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0407044v1", 
    "title": "Reduced cost-based ranking for generating promising subproblems", 
    "arxiv-id": "cs/0407044v1", 
    "author": "W. J. van Hoeve", 
    "publish": "2004-07-16T14:53:21Z", 
    "summary": "In this paper, we propose an effective search procedure that interleaves two\nsteps: subproblem generation and subproblem solution. We mainly focus on the\nfirst part. It consists of a variable domain value ranking based on reduced\ncosts. Exploiting the ranking, we generate, in a Limited Discrepancy Search\ntree, the most promising subproblems first. An interesting result is that\nreduced costs provide a very precise ranking that allows to almost always find\nthe optimal solution in the first generated subproblem, even if its dimension\nis significantly smaller than that of the original problem. Concerning the\nproof of optimality, we exploit a way to increase the lower bound for\nsubproblems at higher discrepancies. We show experimental results on the TSP\nand its time constrained variant to show the effectiveness of the proposed\napproach, but the technique could be generalized for other problems."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0408010v5", 
    "title": "A Simple Proportional Conflict Redistribution Rule", 
    "arxiv-id": "cs/0408010v5", 
    "author": "Jean Dezert", 
    "publish": "2004-08-03T16:08:37Z", 
    "summary": "One proposes a first alternative rule of combination to WAO (Weighted Average\nOperator) proposed recently by Josang, Daniel and Vannoorenberghe, called\nProportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are\nparticular cases of WO (the Weighted Operator) because the conflicting mass is\nredistributed with respect to some weighting factors. In this first PCR rule,\nthe proportionalization is done for each non-empty set with respect to the\nnon-zero sum of its corresponding mass matrix - instead of its mass column\naverage as in WAO, but the results are the same as Ph. Smets has pointed out.\nAlso, we extend WAO (which herein gives no solution) for the degenerate case\nwhen all column sums of all non-empty sets are zero, and then the conflicting\nmass is transferred to the non-empty disjunctive form of all non-empty sets\ntogether; but if this disjunctive form happens to be empty, then one considers\nan open world (i.e. the frame of discernment might contain new hypotheses) and\nthus all conflicting mass is transferred to the empty set. In addition to WAO,\nwe propose a general formula for PCR1 (WAO for non-degenerate cases)."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0408021v2", 
    "title": "An Algorithm for Quasi-Associative and Quasi-Markovian Rules of   Combination in Information Fusion", 
    "arxiv-id": "cs/0408021v2", 
    "author": "Jean Dezert", 
    "publish": "2004-08-08T19:41:23Z", 
    "summary": "In this paper one proposes a simple algorithm of combining the fusion rules,\nthose rules which first use the conjunctive rule and then the transfer of\nconflicting mass to the non-empty sets, in such a way that they gain the\nproperty of associativity and fulfill the Markovian requirement for dynamic\nfusion. Also, a new rule, SDL-improved, is presented."
},{
    "category": "cs.AI", 
    "doi": "10.1109/IJCNN.2002.1007591", 
    "link": "http://arxiv.org/pdf/cs/0408044v1", 
    "title": "FLUX: A Logic Programming Method for Reasoning Agents", 
    "arxiv-id": "cs/0408044v1", 
    "author": "Michael Thielscher", 
    "publish": "2004-08-19T14:47:51Z", 
    "summary": "FLUX is a programming method for the design of agents that reason logically\nabout their actions and sensor information in the presence of incomplete\nknowledge. The core of FLUX is a system of Constraint Handling Rules, which\nenables agents to maintain an internal model of their environment by which they\ncontrol their own behavior. The general action representation formalism of the\nfluent calculus provides the formal semantics for the constraint solver. FLUX\nexhibits excellent computational behavior due to both a carefully restricted\nexpressiveness and the inference paradigm of progression."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0408055v1", 
    "title": "Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection   Scheme in Evolutionary Algorithms", 
    "arxiv-id": "cs/0408055v1", 
    "author": "Shalabh Bhatnagar", 
    "publish": "2004-08-24T11:21:06Z", 
    "summary": "Boltzmann selection is an important selection mechanism in evolutionary\nalgorithms as it has theoretical properties which help in theoretical analysis.\nHowever, Boltzmann selection is not used in practice because a good annealing\nschedule for the `inverse temperature' parameter is lacking. In this paper we\npropose a Cauchy annealing schedule for Boltzmann selection scheme based on a\nhypothesis that selection-strength should increase as evolutionary process goes\non and distance between two selection strengths should decrease for the process\nto converge. To formalize these aspects, we develop formalism for selection\nmechanisms using fitness distributions and give an appropriate measure for\nselection-strength. In this paper, we prove an important result, by which we\nderive an annealing schedule called Cauchy annealing schedule. We demonstrate\nthe novelty of proposed annealing schedule using simulations in the framework\nof genetic algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0408064v3", 
    "title": "Proportional Conflict Redistribution Rules for Information Fusion", 
    "arxiv-id": "cs/0408064v3", 
    "author": "Jean Dezert", 
    "publish": "2004-08-28T03:08:39Z", 
    "summary": "In this paper we propose five versions of a Proportional Conflict\nRedistribution rule (PCR) for information fusion together with several\nexamples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of\nthe rules and also the exactitude of the redistribution of conflicting masses.\nPCR1 restricted from the hyper-power set to the power set and without\ndegenerate cases gives the same result as the Weighted Average Operator (WAO)\nproposed recently by J{\\o}sang, Daniel and Vannoorenberghe but does not satisfy\nthe neutrality property of vacuous belief assignment. That's why improved PCR\nrules are proposed in this paper. PCR4 is an improvement of minC and Dempster's\nrules. The PCR rules redistribute the conflicting mass, after the conjunctive\nrule has been applied, proportionally with some functions depending on the\nmasses assigned to their corresponding columns in the mass matrix. There are\ninfinitely many ways these functions (weighting factors) can be chosen\ndepending on the complexity one wants to deal with in specific applications and\nfusion systems. Any fusion combination rule is at some degree ad-hoc."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0409007v1", 
    "title": "The Generalized Pignistic Transformation", 
    "arxiv-id": "cs/0409007v1", 
    "author": "Milan Daniel", 
    "publish": "2004-09-06T17:47:06Z", 
    "summary": "This paper presents in detail the generalized pignistic transformation (GPT)\nsuccinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a\ntool for decision process. The GPT allows to provide a subjective probability\nmeasure from any generalized basic belief assignment given by any corpus of\nevidence. We mainly focus our presentation on the 3D case and provide the\ncomplete result obtained by the GPT and its validation drawn from the\nprobability theory."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0409040v3", 
    "title": "Unification of Fusion Theories", 
    "arxiv-id": "cs/0409040v3", 
    "author": "Florentin Smarandache", 
    "publish": "2004-09-23T02:02:44Z", 
    "summary": "Since no fusion theory neither rule fully satisfy all needed applications,\nthe author proposes a Unification of Fusion Theories and a combination of\nfusion rules in solving problems/applications. For each particular application,\none selects the most appropriate model, rule(s), and algorithm of\nimplementation. We are working in the unification of the fusion theories and\nrules, which looks like a cooking recipe, better we'd say like a logical chart\nfor a computer programmer, but we don't see another method to comprise/unify\nall things. The unification scenario presented herein, which is now in an\nincipient form, should periodically be updated incorporating new discoveries\nfrom the fusion and engineering research."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0410014v1", 
    "title": "Normal forms for Answer Sets Programming", 
    "arxiv-id": "cs/0410014v1", 
    "author": "Alessandro Provetti", 
    "publish": "2004-10-06T15:01:50Z", 
    "summary": "Normal forms for logic programs under stable/answer set semantics are\nintroduced. We argue that these forms can simplify the study of program\nproperties, mainly consistency. The first normal form, called the {\\em kernel}\nof the program, is useful for studying existence and number of answer sets. A\nkernel program is composed of the atoms which are undefined in the Well-founded\nsemantics, which are those that directly affect the existence of answer sets.\nThe body of rules is composed of negative literals only. Thus, the kernel form\ntends to be significantly more compact than other formulations. Also, it is\npossible to check consistency of kernel programs in terms of colorings of the\nExtended Dependency Graph program representation which we previously developed.\nThe second normal form is called {\\em 3-kernel.} A 3-kernel program is composed\nof the atoms which are undefined in the Well-founded semantics. Rules in\n3-kernel programs have at most two conditions, and each rule either belongs to\na cycle, or defines a connection between cycles. 3-kernel programs may have\npositive conditions. The 3-kernel normal form is very useful for the static\nanalysis of program consistency, i.e., the syntactic characterization of\nexistence of answer sets. This result can be obtained thanks to a novel\ngraph-like representation of programs, called Cycle Graph which presented in\nthe companion article \\cite{Cos04b}."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0410033v2", 
    "title": "An In-Depth Look at Information Fusion Rules & the Unification of Fusion   Theories", 
    "arxiv-id": "cs/0410033v2", 
    "author": "Florentin Smarandache", 
    "publish": "2004-10-14T22:53:46Z", 
    "summary": "This paper may look like a glossary of the fusion rules and we also introduce\nnew ones presenting their formulas and examples: Conjunctive, Disjunctive,\nExclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule,\nDempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache\nclassical and hybrid rules, Murphy's average rule,\nInagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as\nparticular cases: Iganaki's parameterized rule, Weighting Average Operator,\nminC (M. Daniel), and newly Proportional Conflict Redistribution rules\n(Smarandache-Dezert) among which PCR5 is the most exact way of redistribution\nof the conflicting mass to non-empty sets following the path of the conjunctive\nrule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus\nOperator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and\nthree new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to\ninformation fusion (Tchamova-Smarandache). Introducing the degree of union and\ndegree of inclusion with respect to the cardinal of sets not with the fuzzy set\npoint of view, besides that of intersection, many fusion rules can be improved.\nThere are corner cases where each rule might have difficulties working or may\nnot get an expected result."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0410049v1", 
    "title": "Intransitivity and Vagueness", 
    "arxiv-id": "cs/0410049v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2004-10-19T17:31:11Z", 
    "summary": "There are many examples in the literature that suggest that\nindistinguishability is intransitive, despite the fact that the\nindistinguishability relation is typically taken to be an equivalence relation\n(and thus transitive). It is shown that if the uncertainty perception and the\nquestion of when an agent reports that two things are indistinguishable are\nboth carefully modeled, the problems disappear, and indistinguishability can\nindeed be taken to be an equivalence relation. Moreover, this model also\nsuggests a logic of vagueness that seems to solve many of the problems related\nto vagueness discussed in the philosophical literature. In particular, it is\nshown here how the logic can handle the sorites paradox."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0410050v1", 
    "title": "Sleeping Beauty Reconsidered: Conditioning and Reflection in   Asynchronous Systems", 
    "arxiv-id": "cs/0410050v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2004-10-19T17:31:44Z", 
    "summary": "A careful analysis of conditioning in the Sleeping Beauty problem is done,\nusing the formal model for reasoning about knowledge and probability developed\nby Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as\nrevealing problems with conditioning in the presence of imperfect recall, the\nanalysis done here reveals that the problems are not so much due to imperfect\nrecall as to asynchrony. The implications of this analysis for van Fraassen's\nReflection Principle and Savage's Sure-Thing Principle are considered."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0411015v1", 
    "title": "Bounded Input Bounded Predefined Control Bounded Output", 
    "arxiv-id": "cs/0411015v1", 
    "author": "Ziny Flikop", 
    "publish": "2004-11-08T01:52:58Z", 
    "summary": "The paper is an attempt to generalize a methodology, which is similar to the\nbounded-input bounded-output method currently widely used for the system\nstability studies. The presented earlier methodology allows decomposition of\ninput space into bounded subspaces and defining for each subspace its bounding\nsurface. It also defines a corresponding predefined control, which maps any\npoint of a bounded input into a desired bounded output subspace. This\nmethodology was improved by providing a mechanism for the fast defining a\nbounded surface. This paper presents enhanced bounded-input\nbounded-predefined-control bounded-output approach, which provides adaptability\nfeature to the control and allows transferring of a controlled system along a\nsuboptimal trajectory."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CEC.2004.1330837", 
    "link": "http://arxiv.org/pdf/cs/0411034v2", 
    "title": "Generating Conditional Probabilities for Bayesian Networks: Easing the   Knowledge Acquisition Problem", 
    "arxiv-id": "cs/0411034v2", 
    "author": "Balaram Das", 
    "publish": "2004-11-12T00:42:55Z", 
    "summary": "The number of probability distributions required to populate a conditional\nprobability table (CPT) in a Bayesian network, grows exponentially with the\nnumber of parent-nodes associated with that table. If the table is to be\npopulated through knowledge elicited from a domain expert then the sheer\nmagnitude of the task forms a considerable cognitive barrier. In this paper we\ndevise an algorithm to populate the CPT while easing the extent of knowledge\nacquisition. The input to the algorithm consists of a set of weights that\nquantify the relative strengths of the influences of the parent-nodes on the\nchild-node, and a set of probability distributions the number of which grows\nonly linearly with the number of associated parent-nodes. These are elicited\nfrom the domain expert. The set of probabilities are obtained by taking into\nconsideration the heuristics that experts use while arriving at probabilistic\nestimations. The algorithm is used to populate the CPT by computing appropriate\nweighted sums of the elicited distributions. We invoke the methods of\ninformation geometry to demonstrate how these weighted sums capture the\nexpert's judgemental strategy."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542024", 
    "link": "http://arxiv.org/pdf/cs/0411071v1", 
    "title": "Comparing Multi-Target Trackers on Different Force Unit Levels", 
    "arxiv-id": "cs/0411071v1", 
    "author": "Johan Schubert", 
    "publish": "2004-11-19T13:12:40Z", 
    "summary": "Consider the problem of tracking a set of moving targets. Apart from the\ntracking result, it is often important to know where the tracking fails, either\nto steer sensors to that part of the state-space, or to inform a human operator\nabout the status and quality of the obtained information. An intuitive quality\nmeasure is the correlation between two tracking results based on uncorrelated\nobservations. In the case of Bayesian trackers such a correlation measure could\nbe the Kullback-Leibler difference.\n  We focus on a scenario with a large number of military units moving in some\nterrain. The units are observed by several types of sensors and \"meta-sensors\"\nwith force aggregation capabilities. The sensors register units of different\nsize. Two separate multi-target probability hypothesis density (PHD) particle\nfilters are used to track some type of units (e.g., companies) and their\nsub-units (e.g., platoons), respectively, based on observations of units of\nthose sizes. Each observation is used in one filter only.\n  Although the state-space may well be the same in both filters, the posterior\nPHD distributions are not directly comparable -- one unit might correspond to\nthree or four spatially distributed sub-units. Therefore, we introduce a\nmapping function between distributions for different unit size, based on\ndoctrine knowledge of unit configuration.\n  The mapped distributions can now be compared -- locally or globally -- using\nsome measure, which gives the correlation between two PHD distributions in a\nbounded volume of the state-space. To locate areas where the tracking fails, a\ndiscretized quality map of the state-space can be generated by applying the\nmeasure locally to different parts of the space."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0411072v1", 
    "title": "Extremal optimization for sensor report pre-processing", 
    "arxiv-id": "cs/0411072v1", 
    "author": "Pontus Svenson", 
    "publish": "2004-11-19T13:37:40Z", 
    "summary": "We describe the recently introduced extremal optimization algorithm and apply\nit to target detection and association problems arising in pre-processing for\nmulti-target tracking.\n  Here we consider the problem of pre-processing for multiple target tracking\nwhen the number of sensor reports received is very large and arrives in large\nbursts. In this case, it is sometimes necessary to pre-process reports before\nsending them to tracking modules in the fusion system. The pre-processing step\nassociates reports to known tracks (or initializes new tracks for reports on\nobjects that have not been seen before). It could also be used as a pre-process\nstep before clustering, e.g., in order to test how many clusters to use.\n  The pre-processing is done by solving an approximate version of the original\nproblem. In this approximation, not all pair-wise conflicts are calculated. The\napproximation relies on knowing how many such pair-wise conflicts that are\nnecessary to compute. To determine this, results on phase-transitions occurring\nwhen coloring (or clustering) large random instances of a particular graph\nensemble are used."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0412091v1", 
    "title": "The Combination of Paradoxical, Uncertain, and Imprecise Sources of   Information based on DSmT and Neutro-Fuzzy Inference", 
    "arxiv-id": "cs/0412091v1", 
    "author": "Jean Dezert", 
    "publish": "2004-12-19T14:56:11Z", 
    "summary": "The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been, and\nstill remains today, of primal importance for the development of reliable\nmodern information systems involving artificial reasoning. In this chapter, we\npresent a survey of our recent theory of plausible and paradoxical reasoning,\nknown as Dezert-Smarandache Theory (DSmT) in the literature, developed for\ndealing with imprecise, uncertain and paradoxical sources of information. We\nfocus our presentation here rather on the foundations of DSmT, and on the two\nimportant new rules of combination, than on browsing specific applications of\nDSmT available in literature. Several simple examples are given throughout the\npresentation to show the efficiency and the generality of this new approach.\nThe last part of this chapter concerns the presentation of the neutrosophic\nlogic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and\nneutrosophic logic are useful tools in decision making after fusioning the\ninformation using the DSm hybrid rule of combination of masses."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501068v1", 
    "title": "Learning to automatically detect features for mobile robots using   second-order Hidden Markov Models", 
    "arxiv-id": "cs/0501068v1", 
    "author": "Richard Washington", 
    "publish": "2005-01-24T11:05:36Z", 
    "summary": "In this paper, we propose a new method based on Hidden Markov Models to\ninterpret temporal sequences of sensor data from mobile robots to automatically\ndetect features. Hidden Markov Models have been used for a long time in pattern\nrecognition, especially in speech recognition. Their main advantages over other\nmethods (such as neural networks) are their ability to model noisy temporal\nsignals of variable length. We show in this paper that this approach is well\nsuited for interpretation of temporal sequences of mobile-robot sensor data. We\npresent two distinct experiments and results: the first one in an indoor\nenvironment where a mobile robot learns to detect features like open doors or\nT-intersections, the second one in an outdoor environment where a different\nmobile robot has to identify situations like climbing a hill or crossing a\nrock."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501072v1", 
    "title": "Inferring knowledge from a large semantic network", 
    "arxiv-id": "cs/0501072v1", 
    "author": "Thierry Poibeau", 
    "publish": "2005-01-25T16:09:11Z", 
    "summary": "In this paper, we present a rich semantic network based on a differential\nanalysis. We then detail implemented measures that take into account common and\ndifferential features between words. In a last section, we describe some\nindustrial applications."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501084v1", 
    "title": "Towards Automated Integration of Guess and Check Programs in Answer Set   Programming: A Meta-Interpreter and Applications", 
    "arxiv-id": "cs/0501084v1", 
    "author": "Axel Polleres", 
    "publish": "2005-01-28T20:19:12Z", 
    "summary": "Answer set programming (ASP) with disjunction offers a powerful tool for\ndeclaratively representing and solving hard problems. Many NP-complete problems\ncan be encoded in the answer set semantics of logic programs in a very concise\nand intuitive way, where the encoding reflects the typical \"guess and check\"\nnature of NP problems: The property is encoded in a way such that polynomial\nsize certificates for it correspond to stable models of a program. However, the\nproblem-solving capacity of full disjunctive logic programs (DLPs) is beyond\nNP, and captures a class of problems at the second level of the polynomial\nhierarchy. While these problems also have a clear \"guess and check\" structure,\nfinding an encoding in a DLP reflecting this structure may sometimes be a\nnon-obvious task, in particular if the \"check\" itself is a coNP-complete\nproblem; usually, such problems are solved by interleaving separate guess and\ncheck programs, where the check is expressed by inconsistency of the check\nprogram. In this paper, we present general transformations of head-cycle free\n(extended) disjunctive logic programs into stratified and positive (extended)\ndisjunctive logic programs based on meta-interpretation techniques. The answer\nsets of the original and the transformed program are in simple correspondence,\nand, moreover, inconsistency of the original program is indicated by a\ndesignated answer set of the transformed program. Our transformations\nfacilitate the integration of separate \"guess\" and \"check\" programs, which are\noften easy to obtain, automatically into a single disjunctive logic program.\nOur results complement recent results on meta-interpretation in ASP, and extend\nmethods and techniques for a declarative \"guess and check\" problem solving\nparadigm through ASP."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501086v1", 
    "title": "Clever Search: A WordNet Based Wrapper for Internet Search Engines", 
    "arxiv-id": "cs/0501086v1", 
    "author": "Manuela Kunze", 
    "publish": "2005-01-31T16:00:22Z", 
    "summary": "This paper presents an approach to enhance search engines with information\nabout word senses available in WordNet. The approach exploits information about\nthe conceptual relations within the lexical-semantic net. In the wrapper for\nsearch engines presented, WordNet information is used to specify user's request\nor to classify the results of a publicly available web search engine, like\ngoogle, yahoo, etc."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501089v1", 
    "title": "Issues in Exploiting GermaNet as a Resource in Real Applications", 
    "arxiv-id": "cs/0501089v1", 
    "author": "Dietmar Roesner", 
    "publish": "2005-01-31T10:07:52Z", 
    "summary": "This paper reports about experiments with GermaNet as a resource within\ndomain specific document analysis. The main question to be answered is: How is\nthe coverage of GermaNet in a specific domain? We report about results of a\nfield test of GermaNet for analyses of autopsy protocols and present a sketch\nabout the integration of GermaNet inside XDOC. Our remarks will contribute to a\nGermaNet user's wish list."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501093v1", 
    "title": "Transforming Business Rules Into Natural Language Text", 
    "arxiv-id": "cs/0501093v1", 
    "author": "Dietmar Roesner", 
    "publish": "2005-01-31T07:59:14Z", 
    "summary": "The aim of the project presented in this paper is to design a system for an\nNLG architecture, which supports the documentation process of eBusiness models.\nA major task is to enrich the formal description of an eBusiness model with\nadditional information needed in an NLG task."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501094v2", 
    "title": "Corpus based Enrichment of GermaNet Verb Frames", 
    "arxiv-id": "cs/0501094v2", 
    "author": "Dietmar Roesner", 
    "publish": "2005-01-31T08:36:39Z", 
    "summary": "Lexical semantic resources, like WordNet, are often used in real applications\nof natural language document processing. For example, we integrated GermaNet in\nour document suite XDOC of processing of German forensic autopsy protocols. In\naddition to the hypernymy and synonymy relation, we want to adapt GermaNet's\nverb frames for our analysis. In this paper we outline an approach for the\ndomain related enrichment of GermaNet verb frames by corpus based syntactic and\nco-occurred data analyses of real documents."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501095v1", 
    "title": "Context Related Derivation of Word Senses", 
    "arxiv-id": "cs/0501095v1", 
    "author": "Dietmar Roesner", 
    "publish": "2005-01-31T09:25:29Z", 
    "summary": "Real applications of natural language document processing are very often\nconfronted with domain specific lexical gaps during the analysis of documents\nof a new domain. This paper describes an approach for the derivation of domain\nspecific concepts for the extension of an existing ontology. As resources we\nneed an initial ontology and a partially processed corpus of a domain. We\nexploit the specific characteristic of the sublanguage in the corpus. Our\napproach is based on syntactical structures (noun phrases) and compound\nanalyses to extract information required for the extension of GermaNet's\nlexical resources."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0501096v1", 
    "title": "Transforming and Enriching Documents for the Semantic Web", 
    "arxiv-id": "cs/0501096v1", 
    "author": "Sylke Kroetzsch", 
    "publish": "2005-01-31T09:48:46Z", 
    "summary": "We suggest to employ techniques from Natural Language Processing (NLP) and\nKnowledge Representation (KR) to transform existing documents into documents\namenable for the Semantic Web. Semantic Web documents have at least part of\ntheir semantics and pragmatics marked up explicitly in both a machine\nprocessable as well as human readable manner. XML and its related standards\n(XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and\nmethodologies developed for different application scenarios."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0502060v1", 
    "title": "Perspectives for Strong Artificial Life", 
    "arxiv-id": "cs/0502060v1", 
    "author": "J. -Ph Rennard", 
    "publish": "2005-02-13T18:20:48Z", 
    "summary": "This text introduces the twin deadlocks of strong artificial life.\nConceptualization of life is a deadlock both because of the existence of a\ncontinuum between the inert and the living, and because we only know one\ninstance of life. Computationalism is a second deadlock since it remains a\nmatter of faith. Nevertheless, artificial life realizations quickly progress\nand recent constructions embed an always growing set of the intuitive\nproperties of life. This growing gap between theory and realizations should\nsooner or later crystallize in some kind of paradigm shift and then give clues\nto break the twin deadlocks."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0504064v1", 
    "title": "Neural-Network Techniques for Visual Mining Clinical   Electroencephalograms", 
    "arxiv-id": "cs/0504064v1", 
    "author": "Anatoly Brazhnikov", 
    "publish": "2005-04-14T10:27:55Z", 
    "summary": "In this chapter we describe new neural-network techniques developed for\nvisual mining clinical electroencephalograms (EEGs), the weak electrical\npotentials invoked by brain activity. These techniques exploit fruitful ideas\nof Group Method of Data Handling (GMDH). Section 2 briefly describes the\nstandard neural-network techniques which are able to learn well-suited\nclassification modes from data presented by relevant features. Section 3\nintroduces an evolving cascade neural network technique which adds new input\nnodes as well as new neurons to the network while the training error decreases.\nThis algorithm is applied to recognize artifacts in the clinical EEGs. Section\n4 presents the GMDH-type polynomial networks learnt from data. We applied this\ntechnique to distinguish the EEGs recorded from an Alzheimer and a healthy\npatient as well as recognize EEG artifacts. Section 5 describes the new\nneural-network technique developed to induce multi-class concepts from data. We\nused this technique for inducing a 16-class concept from the large-scale\nclinical EEG data. Finally we discuss perspectives of applying the\nneural-network techniques to clinical EEGs."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0504065v1", 
    "title": "Estimating Classification Uncertainty of Bayesian Decision Tree   Technique on Financial Data", 
    "arxiv-id": "cs/0504065v1", 
    "author": "Adolfo Hernandez", 
    "publish": "2005-04-14T10:30:54Z", 
    "summary": "Bayesian averaging over classification models allows the uncertainty of\nclassification outcomes to be evaluated, which is of crucial importance for\nmaking reliable decisions in applications such as financial in which risks have\nto be estimated. The uncertainty of classification is determined by a trade-off\nbetween the amount of data available for training, the diversity of a\nclassifier ensemble and the required performance. The interpretability of\nclassification models can also give useful information for experts responsible\nfor making reliable classifications. For this reason Decision Trees (DTs) seem\nto be attractive classification models. The required diversity of the DT\nensemble can be achieved by using the Bayesian model averaging all possible\nDTs. In practice, the Bayesian approach can be implemented on the base of a\nMarkov Chain Monte Carlo (MCMC) technique of random sampling from the posterior\ndistribution. For sampling large DTs, the MCMC method is extended by Reversible\nJump technique which allows inducing DTs under given priors. For the case when\nthe prior information on the DT size is unavailable, the sweeping technique\ndefining the prior implicitly reveals a better performance. Within this Chapter\nwe explore the classification uncertainty of the Bayesian MCMC techniques on\nsome datasets from the StatLog Repository and real financial data. The\nclassification uncertainty is compared within an Uncertainty Envelope technique\ndealing with the class posterior distribution and a given confidence\nprobability. This technique provides realistic estimates of the classification\nuncertainty which can be easily interpreted in statistical terms with the aim\nof risk evaluation."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0504066v1", 
    "title": "Comparison of the Bayesian and Randomised Decision Tree Ensembles within   an Uncertainty Envelope Technique", 
    "arxiv-id": "cs/0504066v1", 
    "author": "Adolfo Hernandez", 
    "publish": "2005-04-14T10:33:33Z", 
    "summary": "Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of\nclassification outcomes that is of crucial importance for safety critical\napplications. The uncertainty of classification is determined by a trade-off\nbetween the amount of data available for training, the classifier diversity and\nthe required performance. The interpretability of MCSs can also give useful\ninformation for experts responsible for making reliable classifications. For\nthis reason Decision Trees (DTs) seem to be attractive classification models\nfor experts. The required diversity of MCSs exploiting such classification\nmodels can be achieved by using two techniques, the Bayesian model averaging\nand the randomised DT ensemble. Both techniques have revealed promising results\nwhen applied to real-world problems. In this paper we experimentally compare\nthe classification uncertainty of the Bayesian model averaging with a\nrestarting strategy and the randomised DT ensemble on a synthetic dataset and\nsome domain problems commonly used in the machine learning community. To make\nthe Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo\ntechnique. The classification uncertainty is evaluated within an Uncertainty\nEnvelope technique dealing with the class posterior distribution and a given\nconfidence probability. Exploring a full posterior distribution, this technique\nproduces realistic estimates which can be easily interpreted in statistical\nterms. In our experiments we found out that the Bayesian DTs are superior to\nthe randomised DT ensembles within the Uncertainty Envelope technique."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.542027", 
    "link": "http://arxiv.org/pdf/cs/0504071v1", 
    "title": "Proceedings of the Pacific Knowledge Acquisition Workshop 2004", 
    "arxiv-id": "cs/0504071v1", 
    "author": "Wai Kiang Yeap", 
    "publish": "2005-04-14T13:14:53Z", 
    "summary": "Artificial intelligence (AI) research has evolved over the last few decades\nand knowledge acquisition research is at the core of AI research. PKAW-04 is\none of three international knowledge acquisition workshops held in the\nPacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong\nemphasis on incremental knowledge acquisition, machine learning, neural nets\nand active mining.\n  The proceedings contain 19 papers that were selected by the program committee\namong 24 submitted papers. All papers were peer reviewed by at least two\nreviewers. The papers in these proceedings cover the methods and tools as well\nas the applications related to develop expert systems or knowledge based\nsystems."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00500-005-0501-0", 
    "link": "http://arxiv.org/pdf/cs/0505018v1", 
    "title": "Temporal and Spatial Data Mining with Second-Order Hidden Models", 
    "arxiv-id": "cs/0505018v1", 
    "author": "Florence Le Ber", 
    "publish": "2005-05-09T06:54:57Z", 
    "summary": "In the frame of designing a knowledge discovery system, we have developed\nstochastic models based on high-order hidden Markov models. These models are\ncapable to map sequences of data into a Markov chain in which the transitions\nbetween the states depend on the \\texttt{n} previous states according to the\norder of the model. We study the process of achieving information extraction\nfromspatial and temporal data by means of an unsupervised classification. We\nuse therefore a French national database related to the land use of a region,\nnamed Teruti, which describes the land use both in the spatial and temporal\ndomain. Land-use categories (wheat, corn, forest, ...) are logged every year on\neach site regularly spaced in the region. They constitute a temporal sequence\nof images in which we look for spatial and temporal dependencies. The temporal\nsegmentation of the data is done by means of a second-order Hidden Markov Model\n(\\hmmd) that appears to have very good capabilities to locate stationary\nsegments, as shown in our previous work in speech recognition. Thespatial\nclassification is performed by defining a fractal scanning ofthe images with\nthe help of a Hilbert-Peano curve that introduces atotal order on the sites,\npreserving the relation ofneighborhood between the sites. We show that the\n\\hmmd performs aclassification that is meaningful for the agronomists.Spatial\nand temporal classification may be achieved simultaneously by means of a 2\nlevels \\hmmd that measures the \\aposteriori probability to map a temporal\nsequence of images onto a set of hidden classes."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00500-005-0501-0", 
    "link": "http://arxiv.org/pdf/cs/0505081v1", 
    "title": "An ontological approach to the construction of problem-solving models", 
    "arxiv-id": "cs/0505081v1", 
    "author": "Gilles Morel", 
    "publish": "2005-05-30T13:42:02Z", 
    "summary": "Our ongoing work aims at defining an ontology-centered approach for building\nexpertise models for the CommonKADS methodology. This approach (which we have\nnamed \"OntoKADS\") is founded on a core problem-solving ontology which\ndistinguishes between two conceptualization levels: at an object level, a set\nof concepts enable us to define classes of problem-solving situations, and at a\nmeta level, a set of meta-concepts represent modeling primitives. In this\narticle, our presentation of OntoKADS will focus on the core ontology and, in\nparticular, on roles - the primitive situated at the interface between domain\nknowledge and reasoning, and whose ontological status is still much debated. We\nfirst propose a coherent, global, ontological framework which enables us to\naccount for this primitive. We then show how this novel characterization of the\nprimitive allows definition of new rules for the construction of expertise\nmodels."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00500-005-0501-0", 
    "link": "http://arxiv.org/pdf/cs/0506031v1", 
    "title": "A Constrained Object Model for Configuration Based Workflow Composition", 
    "arxiv-id": "cs/0506031v1", 
    "author": "Mathias Kleiner", 
    "publish": "2005-06-09T14:57:53Z", 
    "summary": "Automatic or assisted workflow composition is a field of intense research for\napplications to the world wide web or to business process modeling. Workflow\ncomposition is traditionally addressed in various ways, generally via theorem\nproving techniques. Recent research observed that building a composite workflow\nbears strong relationships with finite model search, and that some workflow\nlanguages can be defined as constrained object metamodels . This lead to\nconsider the viability of applying configuration techniques to this problem,\nwhich was proven feasible. Constrained based configuration expects a\nconstrained object model as input. The purpose of this document is to formally\nspecify the constrained object model involved in ongoing experiments and\nresearch using the Z specification language."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00500-005-0501-0", 
    "link": "http://arxiv.org/pdf/cs/0507010v1", 
    "title": "A Study for the Feature Core of Dynamic Reduct", 
    "arxiv-id": "cs/0507010v1", 
    "author": "Jiayang Wang", 
    "publish": "2005-07-05T13:02:02Z", 
    "summary": "To the reduct problems of decision system, the paper proposes the notion of\ndynamic core according to the dynamic reduct model. It describes various formal\ndefinitions of dynamic core, and discusses some properties about dynamic core.\nAll of these show that dynamic core possesses the essential characters of the\nfeature core."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0507023v1", 
    "title": "Two-dimensional cellular automata and the analysis of correlated time   series", 
    "arxiv-id": "cs/0507023v1", 
    "author": "Valmir C. Barbosa", 
    "publish": "2005-07-08T12:47:38Z", 
    "summary": "Correlated time series are time series that, by virtue of the underlying\nprocess to which they refer, are expected to influence each other strongly. We\nintroduce a novel approach to handle such time series, one that models their\ninteraction as a two-dimensional cellular automaton and therefore allows them\nto be treated as a single entity. We apply our approach to the problems of\nfilling gaps and predicting values in rainfall time series. Computational\nresults show that the new approach compares favorably to Kalman smoothing and\nfiltering."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0507029v1", 
    "title": "ATNoSFERES revisited", 
    "arxiv-id": "cs/0507029v1", 
    "author": "Marc Schoenauer", 
    "publish": "2005-07-11T13:11:25Z", 
    "summary": "ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which\nthe rules are represented as edges of an Augmented Transition Network.\nGenotypes are strings of tokens of a stack-based language, whose execution\nbuilds the labeled graph. The original ATNoSFERES, using a bitstring to\nrepresent the language tokens, has been favorably compared in previous work to\nseveral Michigan style LCSs architectures in the context of Non Markov\nproblems. Several modifications of ATNoSFERES are proposed here: the most\nimportant one conceptually being a representational change: each token is now\nrepresented by an integer, hence the genotype is a string of integers; several\nother modifications of the underlying grammar language are also proposed. The\nresulting ATNoSFERES-II is validated on several standard animat Non Markov\nproblems, on which it outperforms all previously published results in the LCS\nliterature. The reasons for these improvement are carefully analyzed, and some\nassumptions are proposed on the underlying mechanisms in order to explain these\ngood results."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0508132v1", 
    "title": "Planning with Preferences using Logic Programming", 
    "arxiv-id": "cs/0508132v1", 
    "author": "Enrico Pontelli", 
    "publish": "2005-08-31T14:50:22Z", 
    "summary": "We present a declarative language, PP, for the high-level specification of\npreferences between possible solutions (or trajectories) of a planning problem.\nThis novel language allows users to elegantly express non-trivial,\nmulti-dimensional preferences and priorities over such preferences. The\nsemantics of PP allows the identification of most preferred trajectories for a\ngiven goal. We also provide an answer set programming implementation of\nplanning problems with PP preferences."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0509011v1", 
    "title": "Clustering Mixed Numeric and Categorical Data: A Cluster Ensemble   Approach", 
    "arxiv-id": "cs/0509011v1", 
    "author": "Shengchun Deng", 
    "publish": "2005-09-05T02:47:12Z", 
    "summary": "Clustering is a widely used technique in data mining applications for\ndiscovering patterns in underlying data. Most traditional clustering algorithms\nare limited to handling datasets that contain either numeric or categorical\nattributes. However, datasets with mixed types of attributes are common in real\nlife data mining applications. In this paper, we propose a novel\ndivide-and-conquer technique to solve this problem. First, the original mixed\ndataset is divided into two sub-datasets: the pure categorical dataset and the\npure numeric dataset. Next, existing well established clustering algorithms\ndesigned for different types of datasets are employed to produce corresponding\nclusters. Last, the clustering results on the categorical and numeric dataset\nare combined as a categorical dataset, on which the categorical data clustering\nalgorithm is used to get the final clusters. Our contribution in this paper is\nto provide an algorithm framework for the mixed attributes clustering problem,\nin which existing clustering algorithms can be easily integrated, the\ncapabilities of different kinds of clustering algorithms and characteristics of\ndifferent types of datasets could be fully exploited. Comparisons with other\nclustering algorithms on real life datasets illustrate the superiority of our\napproach."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0509033v1", 
    "title": "K-Histograms: An Efficient Clustering Algorithm for Categorical Dataset", 
    "arxiv-id": "cs/0509033v1", 
    "author": "Bin Dong", 
    "publish": "2005-09-13T06:33:08Z", 
    "summary": "Clustering categorical data is an integral part of data mining and has\nattracted much attention recently. In this paper, we present k-histogram, a new\nefficient algorithm for clustering categorical data. The k-histogram algorithm\nextends the k-means algorithm to categorical domain by replacing the means of\nclusters with histograms, and dynamically updates histograms in the clustering\nprocess. Experimental results on real datasets show that k-histogram algorithm\ncan produce better clustering results than k-modes algorithm, the one related\nwith our work most closely."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510050v1", 
    "title": "Integration of the DOLCE top-level ontology into the OntoSpec   methodology", 
    "arxiv-id": "cs/0510050v1", 
    "author": "Gilles Kassel", 
    "publish": "2005-10-18T08:32:38Z", 
    "summary": "This report describes a new version of the OntoSpec methodology for ontology\nbuilding. Defined by the LaRIA Knowledge Engineering Team (University of\nPicardie Jules Verne, Amiens, France), OntoSpec aims at helping builders to\nmodel ontological knowledge (upstream of formal representation). The\nmethodology relies on a set of rigorously-defined modelling primitives and\nprinciples. Its application leads to the elaboration of a semi-informal\nontology, which is independent of knowledge representation languages. We\nrecently enriched the OntoSpec methodology by endowing it with a new resource,\nthe DOLCE top-level ontology defined at the LOA (IST-CNR, Trento, Italy). The\ngoal of this integration is to provide modellers with additional help in\nstructuring application ontologies, while maintaining independence\nvis-\\`{a}-vis formal representation languages. In this report, we first provide\nan overview of the OntoSpec methodology's general principles and then describe\nthe DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a\nspecification of DOLCE in the semi-informal OntoSpec language) is presented in\nan appendix."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510062v1", 
    "title": "Using Interval Particle Filtering for Marker less 3D Human Motion   Capture", 
    "arxiv-id": "cs/0510062v1", 
    "author": "Fran\u00e7ois Charpillet", 
    "publish": "2005-10-21T13:45:15Z", 
    "summary": "In this paper we present a new approach for marker less human motion capture\nfrom conventional camera feeds. The aim of our study is to recover 3D positions\nof key points of the body that can serve for gait analysis. Our approach is\nbased on foreground segmentation, an articulated body model and particle\nfilters. In order to be generic and simple no restrictive dynamic modelling was\nused. A new modified particle filtering algorithm was introduced. It is used\nefficiently to search the model configuration space. This new algorithm which\nwe call Interval Particle Filtering reorganizes the configurations search space\nin an optimal deterministic way and proved to be efficient in tracking natural\nhuman movement. Results for human motion capture from a single camera are\npresented and compared to results obtained from a marker based system. The\nsystem proved to be able to track motion successfully even in partial\nocclusions."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510063v1", 
    "title": "Markerless Human Motion Capture for Gait Analysis", 
    "arxiv-id": "cs/0510063v1", 
    "author": "Fran\u00e7ois Charpillet", 
    "publish": "2005-10-21T13:45:49Z", 
    "summary": "The aim of our study is to detect balance disorders and a tendency towards\nthe falls in the elderly, knowing gait parameters. In this paper we present a\nnew tool for gait analysis based on markerless human motion capture, from\ncamera feeds. The system introduced here, recovers the 3D positions of several\nkey points of the human body while walking. Foreground segmentation, an\narticulated body model and particle filtering are basic elements of our\napproach. No dynamic model is used thus this system can be described as generic\nand simple to implement. A modified particle filtering algorithm, which we call\nInterval Particle Filtering, is used to reorganise and search through the\nmodel's configurations search space in a deterministic optimal way. This\nalgorithm was able to perform human movement tracking with success. Results\nfrom the treatment of a single cam feeds are shown and compared to results\nobtained using a marker based human motion capture system."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510079v2", 
    "title": "Evidence with Uncertain Likelihoods", 
    "arxiv-id": "cs/0510079v2", 
    "author": "Riccardo Pucella", 
    "publish": "2005-10-25T21:15:31Z", 
    "summary": "An agent often has a number of hypotheses, and must choose among them based\non observations, or outcomes of experiments. Each of these observations can be\nviewed as providing evidence for or against various hypotheses. All the\nattempts to formalize this intuition up to now have assumed that associated\nwith each hypothesis h there is a likelihood function \\mu_h, which is a\nprobability measure that intuitively describes how likely each observation is,\nconditional on h being the correct hypothesis. We consider an extension of this\nframework where there is uncertainty as to which of a number of likelihood\nfunctions is appropriate, and discuss how one formal approach to defining\nevidence, which views evidence as a function from priors to posteriors, can be\ngeneralized to accommodate this uncertainty."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510083v1", 
    "title": "Neuronal Spectral Analysis of EEG and Expert Knowledge Integration for   Automatic Classification of Sleep Stages", 
    "arxiv-id": "cs/0510083v1", 
    "author": "Mohamed Dogui", 
    "publish": "2005-10-26T14:47:07Z", 
    "summary": "Being able to analyze and interpret signal coming from electroencephalogram\n(EEG) recording can be of high interest for many applications including medical\ndiagnosis and Brain-Computer Interfaces. Indeed, human experts are today able\nto extract from this signal many hints related to physiological as well as\ncognitive states of the recorded subject and it would be very interesting to\nperform such task automatically but today no completely automatic system\nexists. In previous studies, we have compared human expertise and automatic\nprocessing tools, including artificial neural networks (ANN), to better\nunderstand the competences of each and determine which are the difficult\naspects to integrate in a fully automatic system. In this paper, we bring more\nelements to that study in reporting the main results of a practical experiment\nwhich was carried out in an hospital for sleep pathology study. An EEG\nrecording was studied and labeled by a human expert and an ANN. We describe\nhere the characteristics of the experiment, both human and neuronal procedure\nof analysis, compare their performances and point out the main limitations\nwhich arise from this study."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0510091v1", 
    "title": "An efficient memetic, permutation-based evolutionary algorithm for   real-world train timetabling", 
    "arxiv-id": "cs/0510091v1", 
    "author": "Yann Semet", 
    "publish": "2005-10-31T06:06:57Z", 
    "summary": "Train timetabling is a difficult and very tightly constrained combinatorial\nproblem that deals with the construction of train schedules. We focus on the\nparticular problem of local reconstruction of the schedule following a small\nperturbation, seeking minimisation of the total accumulated delay by adapting\ntimes of departure and arrival for each train and allocation of resources\n(tracks, routing nodes, etc.). We describe a permutation-based evolutionary\nalgorithm that relies on a semi-greedy heuristic to gradually reconstruct the\nschedule by inserting trains one after the other following the permutation.\nThis algorithm can be hybridised with ILOG commercial MIP programming tool\nCPLEX in a coarse-grained manner: the evolutionary part is used to quickly\nobtain a good but suboptimal solution and this intermediate solution is refined\nusing CPLEX. Experimental results are presented on a large real-world case\ninvolving more than one million variables and 2 million constraints. Results\nare surprisingly good as the evolutionary algorithm, alone or hybridised,\nproduces excellent solutions much faster than CPLEX alone."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0511004v1", 
    "title": "Evolutionary Computing", 
    "arxiv-id": "cs/0511004v1", 
    "author": "Marc Schoenauer", 
    "publish": "2005-11-01T19:46:18Z", 
    "summary": "Evolutionary computing (EC) is an exciting development in Computer Science.\nIt amounts to building, applying and studying algorithms based on the Darwinian\nprinciples of natural selection. In this paper we briefly introduce the main\nconcepts behind evolutionary computing. We present the main components all\nevolutionary algorithms (EA), sketch the differences between different types of\nEAs and survey application areas ranging from optimization, modeling and\nsimulation to entertainment."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0511015v2", 
    "title": "Towards a Hierarchical Model of Consciousness, Intelligence, Mind and   Body", 
    "arxiv-id": "cs/0511015v2", 
    "author": "Prashant", 
    "publish": "2005-11-03T16:28:05Z", 
    "summary": "This article is taken out."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0511091v1", 
    "title": "Evolution of Voronoi based Fuzzy Recurrent Controllers", 
    "arxiv-id": "cs/0511091v1", 
    "author": "Marc Schoenauer", 
    "publish": "2005-11-28T07:14:18Z", 
    "summary": "A fuzzy controller is usually designed by formulating the knowledge of a\nhuman expert into a set of linguistic variables and fuzzy rules. Among the most\nsuccessful methods to automate the fuzzy controllers development process are\nevolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi\n(RFV) model, a representation for recurrent fuzzy systems. It is an extension\nof the FV model proposed by Kavka and Schoenauer that extends the application\ndomain to include temporal problems. The FV model is a representation for fuzzy\ncontrollers based on Voronoi diagrams that can represent fuzzy systems with\nsynergistic rules, fulfilling the $\\epsilon$-completeness property and\nproviding a simple way to introduce a priory knowledge. In the proposed\nrepresentation, the temporal relations are embedded by including internal units\nthat provide feedback by connecting outputs to inputs. These internal units act\nas memory elements. In the RFV model, the semantic of the internal units can be\nspecified together with the a priori rules. The geometric interpretation of the\nrules allows the use of geometric variational operators during the evolution.\nThe representation and the algorithms are validated in two problems in the area\nof system identification and evolutionary robotics."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0512045v2", 
    "title": "Branch-and-Prune Search Strategies for Numerical Constraint Solving", 
    "arxiv-id": "cs/0512045v2", 
    "author": "Boi Faltings", 
    "publish": "2005-12-11T19:47:42Z", 
    "summary": "When solving numerical constraints such as nonlinear equations and\ninequalities, solvers often exploit pruning techniques, which remove redundant\nvalue combinations from the domains of variables, at pruning steps. To find the\ncomplete solution set, most of these solvers alternate the pruning steps with\nbranching steps, which split each problem into subproblems. This forms the\nso-called branch-and-prune framework, well known among the approaches for\nsolving numerical constraints. The basic branch-and-prune search strategy that\nuses domain bisections in place of the branching steps is called the bisection\nsearch. In general, the bisection search works well in case (i) the solutions\nare isolated, but it can be improved further in case (ii) there are continuums\nof solutions (this often occurs when inequalities are involved). In this paper,\nwe propose a new branch-and-prune search strategy along with several variants,\nwhich not only allow yielding better branching decisions in the latter case,\nbut also work as well as the bisection search does in the former case. These\nnew search algorithms enable us to employ various pruning techniques in the\nconstruction of inner and outer approximations of the solution set. Our\nexperiments show that these algorithms speed up the solving process often by\none order of magnitude or more when solving problems with continuums of\nsolutions, while keeping the same performance as the bisection search when the\nsolutions are isolated."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0512047v2", 
    "title": "Processing Uncertainty and Indeterminacy in Information Systems success   mapping", 
    "arxiv-id": "cs/0512047v2", 
    "author": "Florentin Smarandache", 
    "publish": "2005-12-13T01:21:58Z", 
    "summary": "IS success is a complex concept, and its evaluation is complicated,\nunstructured and not readily quantifiable. Numerous scientific publications\naddress the issue of success in the IS field as well as in other fields. But,\nlittle efforts have been done for processing indeterminacy and uncertainty in\nsuccess research. This paper shows a formal method for mapping success using\nNeutrosophic Success Map. This is an emerging tool for processing indeterminacy\nand uncertainty in success research. EIS success have been analyzed using this\ntool."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0512099v1", 
    "title": "Mathematical Models in Schema Theory", 
    "arxiv-id": "cs/0512099v1", 
    "author": "Mark Burgin", 
    "publish": "2005-12-27T21:29:16Z", 
    "summary": "In this paper, a mathematical schema theory is developed. This theory has\nthree roots: brain theory schemas, grid automata, and block-shemas. In Section\n2 of this paper, elements of the theory of grid automata necessary for the\nmathematical schema theory are presented. In Section 3, elements of brain\ntheory necessary for the mathematical schema theory are presented. In Section\n4, other types of schemas are considered. In Section 5, the mathematical schema\ntheory is developed. The achieved level of schema representation allows one to\nmodel by mathematical tools virtually any type of schemas considered before,\nincluding schemas in neurophisiology, psychology, computer science, Internet\ntechnology, databases, logic, and mathematics."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0601001v2", 
    "title": "Truecluster: robust scalable clustering with model selection", 
    "arxiv-id": "cs/0601001v2", 
    "author": "Jens Oehlschl\u00e4gel", 
    "publish": "2006-01-02T13:17:09Z", 
    "summary": "Data-based classification is fundamental to most branches of science. While\nrecent years have brought enormous progress in various areas of statistical\ncomputing and clustering, some general challenges in clustering remain: model\nselection, robustness, and scalability to large datasets. We consider the\nimportant problem of deciding on the optimal number of clusters, given an\narbitrary definition of space and clusteriness. We show how to construct a\ncluster information criterion that allows objective model selection. Differing\nfrom other approaches, our truecluster method does not require specific\nassumptions about underlying distributions, dissimilarity definitions or\ncluster models. Truecluster puts arbitrary clustering algorithms into a generic\nunified (sampling-based) statistical framework. It is scalable to big datasets\nand provides robust cluster assignments and case-wise diagnostics. Truecluster\nwill make clustering more objective, allows for automation, and will save time\nand costs. Free R software is available."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0601031v1", 
    "title": "Divide-and-Evolve: a New Memetic Scheme for Domain-Independent Temporal   Planning", 
    "arxiv-id": "cs/0601031v1", 
    "author": "Vincent Vidal", 
    "publish": "2006-01-09T16:57:08Z", 
    "summary": "An original approach, termed Divide-and-Evolve is proposed to hybridize\nEvolutionary Algorithms (EAs) with Operational Research (OR) methods in the\ndomain of Temporal Planning Problems (TPPs). Whereas standard Memetic\nAlgorithms use local search methods to improve the evolutionary solutions, and\nthus fail when the local method stops working on the complete problem, the\nDivide-and-Evolve approach splits the problem at hand into several, hopefully\neasier, sub-problems, and can thus solve globally problems that are intractable\nwhen directly fed into deterministic OR algorithms. But the most prominent\nadvantage of the Divide-and-Evolve approach is that it immediately opens up an\navenue for multi-objective optimization, even though the OR method that is used\nis single-objective. Proof of concept approach on the standard\n(single-objective) Zeno transportation benchmark is given, and a small original\nmulti-objective benchmark is proposed in the same Zeno framework to assess the\nmulti-objective capabilities of the proposed methodology, a breakthrough in\nTemporal Planning."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0601052v1", 
    "title": "Artificial and Biological Intelligence", 
    "arxiv-id": "cs/0601052v1", 
    "author": "Subhash Kak", 
    "publish": "2006-01-13T19:01:42Z", 
    "summary": "This article considers evidence from physical and biological sciences to show\nmachines are deficient compared to biological systems at incorporating\nintelligence. Machines fall short on two counts: firstly, unlike brains,\nmachines do not self-organize in a recursive manner; secondly, machines are\nbased on classical logic, whereas Nature's intelligence may depend on quantum\nmechanics."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0601109v3", 
    "title": "Certainty Closure: Reliable Constraint Reasoning with Incomplete or   Erroneous Data", 
    "arxiv-id": "cs/0601109v3", 
    "author": "Carmen Gervet", 
    "publish": "2006-01-25T20:11:11Z", 
    "summary": "Constraint Programming (CP) has proved an effective paradigm to model and\nsolve difficult combinatorial satisfaction and optimisation problems from\ndisparate domains. Many such problems arising from the commercial world are\npermeated by data uncertainty. Existing CP approaches that accommodate\nuncertainty are less suited to uncertainty arising due to incomplete and\nerroneous data, because they do not build reliable models and solutions\nguaranteed to address the user's genuine problem as she perceives it. Other\nfields such as reliable computation offer combinations of models and associated\nmethods to handle these types of uncertain data, but lack an expressive\nframework characterising the resolution methodology independently of the model.\n  We present a unifying framework that extends the CP formalism in both model\nand solutions, to tackle ill-defined combinatorial problems with incomplete or\nerroneous data. The certainty closure framework brings together modelling and\nsolving methodologies from different fields into the CP paradigm to provide\nreliable and efficient approches for uncertain constraint problems. We\ndemonstrate the applicability of the framework on a case study in network\ndiagnosis. We define resolution forms that give generic templates, and their\nassociated operational semantics, to derive practical solution methods for\nreliable solutions."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0602022v1", 
    "title": "Avoiding the Bloat with Stochastic Grammar-based Genetic Programming", 
    "arxiv-id": "cs/0602022v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2006-02-07T07:48:27Z", 
    "summary": "The application of Genetic Programming to the discovery of empirical laws is\noften impaired by the huge size of the search space, and consequently by the\ncomputer resources needed. In many cases, the extreme demand for memory and CPU\nis due to the massive growth of non-coding segments, the introns. The paper\npresents a new program evolution framework which combines distribution-based\nevolution in the PBIL spirit, with grammar-based genetic programming; the\ninformation is stored as a probability distribution on the gra mmar rules,\nrather than in a population. Experiments on a real-world like problem show that\nthis approach gives a practical solution to the problem of intron growth."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0602031v1", 
    "title": "Classifying Signals with Local Classifiers", 
    "arxiv-id": "cs/0602031v1", 
    "author": "Wit Jakuczun", 
    "publish": "2006-02-08T11:38:44Z", 
    "summary": "This paper deals with the problem of classifying signals. The new method for\nbuilding so called local classifiers and local features is presented. The\nmethod is a combination of the lifting scheme and the support vector machines.\nIts main aim is to produce effective and yet comprehensible classifiers that\nwould help in understanding processes hidden behind classified signals. To\nillustrate the method we present the results obtained on an artificial and a\nreal dataset."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0603025v2", 
    "title": "Open Answer Set Programming with Guarded Programs", 
    "arxiv-id": "cs/0603025v2", 
    "author": "Dirk Vermeir", 
    "publish": "2006-03-07T17:54:59Z", 
    "summary": "Open answer set programming (OASP) is an extension of answer set programming\nwhere one may ground a program with an arbitrary superset of the program's\nconstants. We define a fixed point logic (FPL) extension of Clark's completion\nsuch that open answer sets correspond to models of FPL formulas and identify a\nsyntactic subclass of programs, called (loosely) guarded programs. Whereas\nreasoning with general programs in OASP is undecidable, the FPL translation of\n(loosely) guarded programs falls in the decidable (loosely) guarded fixed point\nlogic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP,\nenabling for the first time, a characterization of an answer set semantics by\nmuLGF formulas. We further extend the open answer set semantics for programs\nwith generalized literals. Such generalized programs (gPs) have interesting\nproperties, e.g., the ability to express infinity axioms. We restrict the\nsyntax of gPs such that both rules and generalized literals are guarded. Via a\ntranslation to guarded fixed point logic, we deduce 2-exptime-completeness of\nsatisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted\nGgPs with exptime-complete satisfiability checking, but still sufficiently\nexpressive to optimally simulate computation tree logic (CTL). We translate\nDatalog lite programs to GgPs, establishing equivalence of GgPs under an open\nanswer set semantics, alternation-free muGF, and Datalog lite."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0603034v1", 
    "title": "Metatheory of actions: beyond consistency", 
    "arxiv-id": "cs/0603034v1", 
    "author": "Ivan Varzinczak", 
    "publish": "2006-03-09T10:07:46Z", 
    "summary": "Consistency check has been the only criterion for theory evaluation in\nlogic-based approaches to reasoning about actions. This work goes beyond that\nand contributes to the metatheory of actions by investigating what other\nproperties a good domain description in reasoning about actions should have. We\nstate some metatheoretical postulates concerning this sore spot. When all\npostulates are satisfied together we have a modular action theory. Besides\nbeing easier to understand and more elaboration tolerant in McCarthy's sense,\nmodular theories have interesting properties. We point out the problems that\narise when the postulates about modularity are violated and propose algorithmic\nchecks that can help the designer of an action theory to overcome them."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0603038v2", 
    "title": "Estimation of linear, non-gaussian causal models in the presence of   confounding latent variables", 
    "arxiv-id": "cs/0603038v2", 
    "author": "Antti J. Kerminen", 
    "publish": "2006-03-09T14:46:18Z", 
    "summary": "The estimation of linear causal models (also known as structural equation\nmodels) from data is a well-known problem which has received much attention in\nthe past. Most previous work has, however, made an explicit or implicit\nassumption of gaussianity, limiting the identifiability of the models. We have\nrecently shown (Shimizu et al, 2005; Hoyer et al, 2006) that for non-gaussian\ndistributions the full causal model can be estimated in the no hidden variables\ncase. In this contribution, we discuss the estimation of the model when\nconfounding latent variables are present. Although in this case uniqueness is\nno longer guaranteed, there is at most a finite set of models which can fit the\ndata. We develop an algorithm for estimating this set, and describe numerical\nsimulations which confirm the theoretical arguments and demonstrate the\npractical viability of the approach. Full Matlab code is provided for all\nsimulations."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0603081v1", 
    "title": "Application of Support Vector Regression to Interpolation of Sparse   Shock Physics Data Sets", 
    "arxiv-id": "cs/0603081v1", 
    "author": "David B. Holtkamp", 
    "publish": "2006-03-20T23:43:45Z", 
    "summary": "Shock physics experiments are often complicated and expensive. As a result,\nresearchers are unable to conduct as many experiments as they would like -\nleading to sparse data sets. In this paper, Support Vector Machines for\nregression are applied to velocimetry data sets for shock damaged and melted\ntin metal. Some success at interpolating between data sets is achieved.\nImplications for future work are discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0603120v1", 
    "title": "Approximation Algorithms for K-Modes Clustering", 
    "arxiv-id": "cs/0603120v1", 
    "author": "Zengyou He", 
    "publish": "2006-03-30T02:02:37Z", 
    "summary": "In this paper, we study clustering with respect to the k-modes objective\nfunction, a natural formulation of clustering for categorical data. One of the\nmain contributions of this paper is to establish the connection between k-modes\nand k-median, i.e., the optimum of k-median is at most twice the optimum of\nk-modes for the same categorical data clustering problem. Based on this\nobservation, we derive a deterministic algorithm that achieves an approximation\nfactor of 2. Furthermore, we prove that the distance measure in k-modes defines\na metric. Hence, we are able to extend existing approximation algorithms for\nmetric k-median to k-modes. Empirical results verify the superiority of our\nmethod."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0604009v1", 
    "title": "Can an Organism Adapt Itself to Unforeseen Circumstances?", 
    "arxiv-id": "cs/0604009v1", 
    "author": "Alexey V. Melkikh", 
    "publish": "2006-04-05T10:29:28Z", 
    "summary": "A model of an organism as an autonomous intelligent system has been proposed.\nThis model was used to analyze learning of an organism in various environmental\nconditions. Processes of learning were divided into two types: strong and weak\nprocesses taking place in the absence and the presence of aprioristic\ninformation about an object respectively. Weak learning is synonymous to\nadaptation when aprioristic programs already available in a system (an\norganism) are started. It was shown that strong learning is impossible for both\nan organism and any autonomous intelligent system. It was shown also that the\nknowledge base of an organism cannot be updated. Therefore, all behavior\nprograms of an organism are congenital. A model of a conditioned reflex as a\nseries of consecutive measurements of environmental parameters has been\nadvanced. Repeated measurements are necessary in this case to reduce the error\nduring decision making."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patrec.2006.01.005", 
    "link": "http://arxiv.org/pdf/cs/0604042v1", 
    "title": "Adaptative combination rule and proportional conflict redistribution   rule for information fusion", 
    "arxiv-id": "cs/0604042v1", 
    "author": "Anne-Laure Jousselme", 
    "publish": "2006-04-11T14:35:15Z", 
    "summary": "This paper presents two new promising rules of combination for the fusion of\nuncertain and potentially highly conflicting sources of evidences in the\nframework of the theory of belief functions in order to palliate the well-know\nlimitations of Dempster's rule and to work beyond the limits of applicability\nof the Dempster-Shafer theory. We present both a new class of adaptive\ncombination rules (ACR) and a new efficient Proportional Conflict\nRedistribution (PCR) rule allowing to deal with highly conflicting sources for\nstatic and dynamic fusion applications."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TED.2007.893191", 
    "link": "http://arxiv.org/pdf/cs/0604070v2", 
    "title": "Retraction and Generalized Extension of Computing with Words", 
    "arxiv-id": "cs/0604070v2", 
    "author": "Guoqing Chen", 
    "publish": "2006-04-19T06:28:55Z", 
    "summary": "Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a\nformal model of computing with values. Motivated by Zadeh's paradigm of\ncomputing with words rather than numbers, Ying proposed a kind of fuzzy\nautomata, whose input alphabet consists of all fuzzy subsets of a set of\nsymbols, as a formal model of computing with all words. In this paper, we\nintroduce a somewhat general formal model of computing with (some special)\nwords. The new features of the model are that the input alphabet only comprises\nsome (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy\ntransition function can be specified arbitrarily. By employing the methodology\nof fuzzy control, we establish a retraction principle from computing with words\nto computing with values for handling crisp inputs and a generalized extension\nprinciple from computing with words to computing with all words for handling\nfuzzy inputs. These principles show that computing with values and computing\nwith all words can be respectively implemented by computing with words. Some\nalgebraic properties of retractions and generalized extensions are addressed as\nwell."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TED.2007.893191", 
    "link": "http://arxiv.org/pdf/cs/0604086v1", 
    "title": "A Knowledge-Based Approach for Selecting Information Sources", 
    "arxiv-id": "cs/0604086v1", 
    "author": "Hans Tompits", 
    "publish": "2006-04-21T16:53:28Z", 
    "summary": "Through the Internet and the World-Wide Web, a vast number of information\nsources has become available, which offer information on various subjects by\ndifferent providers, often in heterogeneous formats. This calls for tools and\nmethods for building an advanced information-processing infrastructure. One\nissue in this area is the selection of suitable information sources in query\nanswering. In this paper, we present a knowledge-based approach to this\nproblem, in the setting where one among a set of information sources\n(prototypically, data repositories) should be selected for evaluating a user\nquery. We use extended logic programs (ELPs) to represent rich descriptions of\nthe information sources, an underlying domain theory, and user queries in a\nformal query language (here, XML-QL, but other languages can be handled as\nwell). Moreover, we use ELPs for declarative query analysis and generation of a\nquery description. Central to our approach are declarative source-selection\nprograms, for which we define syntax and semantics. Due to the structured\nnature of the considered data items, the semantics of such programs must\ncarefully respect implicit context information in source-selection rules, and\nfurthermore combine it with possible user preferences. A prototype\nimplementation of our approach has been realized exploiting the DLV KR system\nand its plp front-end for prioritized ELPs. We describe a representative\nexample involving specific movie databases, and report about experimental\nresults."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TED.2007.893191", 
    "link": "http://arxiv.org/pdf/cs/0605012v2", 
    "title": "Perspective alignment in spatial language", 
    "arxiv-id": "cs/0605012v2", 
    "author": "M. Loetzsch", 
    "publish": "2006-05-04T17:16:02Z", 
    "summary": "It is well known that perspective alignment plays a major role in the\nplanning and interpretation of spatial language. In order to understand the\nrole of perspective alignment and the cognitive processes involved, we have\nmade precise complete cognitive models of situated embodied agents that\nself-organise a communication system for dialoging about the position and\nmovement of real world objects in their immediate surroundings. We show in a\nseries of robotic experiments which cognitive mechanisms are necessary and\nsufficient to achieve successful spatial language and why and how perspective\nalignment can take place, either implicitly or based on explicit marking."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TED.2007.893191", 
    "link": "http://arxiv.org/pdf/cs/0605017v1", 
    "title": "Reasoning and Planning with Sensing Actions, Incomplete Information, and   Static Causal Laws using Answer Set Programming", 
    "arxiv-id": "cs/0605017v1", 
    "author": "Chitta Baral", 
    "publish": "2006-05-04T22:35:12Z", 
    "summary": "We extend the 0-approximation of sensing actions and incomplete information\nin [Son and Baral 2000] to action theories with static causal laws and prove\nits soundness with respect to the possible world semantics. We also show that\nthe conditional planning problem with respect to this approximation is\nNP-complete. We then present an answer set programming based conditional\nplanner, called ASCP, that is capable of generating both conformant plans and\nconditional plans in the presence of sensing actions, incomplete information\nabout the initial state, and static causal laws. We prove the correctness of\nour implementation and argue that our planner is sound and complete with\nrespect to the proposed approximation. Finally, we present experimental results\ncomparing ASCP to other planners."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TED.2007.893191", 
    "link": "http://arxiv.org/pdf/cs/0605055v1", 
    "title": "Approximate Discrete Probability Distribution Representation using a   Multi-Resolution Binary Tree", 
    "arxiv-id": "cs/0605055v1", 
    "author": "Pierre Bessiere", 
    "publish": "2006-05-12T13:32:50Z", 
    "summary": "Computing and storing probabilities is a hard problem as soon as one has to\ndeal with complex distributions over multiple random variables. The problem of\nefficient representation of probability distributions is central in term of\ncomputational efficiency in the field of probabilistic reasoning. The main\nproblem arises when dealing with joint probability distributions over a set of\nrandom variables: they are always represented using huge probability arrays. In\nthis paper, a new method based on binary-tree representation is introduced in\norder to store efficiently very large joint distributions. Our approach\napproximates any multidimensional joint distributions using an adaptive\ndiscretization of the space. We make the assumption that the lower is the\nprobability mass of a particular region of feature space, the larger is the\ndiscretization step. This assumption leads to a very optimized representation\nin term of time and memory. The other advantages of our approach are the\nability to refine dynamically the distribution every time it is needed leading\nto a more accurate representation of the probability distribution and to an\nanytime representation of the distribution."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0605108v2", 
    "title": "Diagnosability of Fuzzy Discrete Event Systems", 
    "arxiv-id": "cs/0605108v2", 
    "author": "Zhujun Fan", 
    "publish": "2006-05-24T15:49:06Z", 
    "summary": "In order to more effectively cope with the real-world problems of vagueness,\n{\\it fuzzy discrete event systems} (FDESs) were proposed recently, and the\nsupervisory control theory of FDESs was developed. In view of the importance of\nfailure diagnosis, in this paper, we present an approach of the failure\ndiagnosis in the framework of FDESs. More specifically: (1) We formalize the\ndefinition of diagnosability for FDESs, in which the observable set and failure\nset of events are {\\it fuzzy}, that is, each event has certain degree to be\nobservable and unobservable, and, also, each event may possess different\npossibility of failure occurring. (2) Through the construction of\nobservability-based diagnosers of FDESs, we investigate its some basic\nproperties. In particular, we present a necessary and sufficient condition for\ndiagnosability of FDESs. (3) Some examples serving to illuminate the\napplications of the diagnosability of FDESs are described. To conclude, some\nrelated issues are raised for further consideration."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0605123v1", 
    "title": "Classification of Ordinal Data", 
    "arxiv-id": "cs/0605123v1", 
    "author": "Jaime S. Cardoso", 
    "publish": "2006-05-26T09:44:44Z", 
    "summary": "Classification of ordinal data is one of the most important tasks of relation\nlearning. In this thesis a novel framework for ordered classes is proposed. The\ntechnique reduces the problem of classifying ordered classes to the standard\ntwo-class problem. The introduced method is then mapped into support vector\nmachines and neural networks. Compared with a well-known approach using\npairwise objects as training samples, the new algorithm has a reduced\ncomplexity and training time. A second novel model, the unimodal model, is also\nintroduced and a parametric version is mapped into neural networks. Several\ncase studies are presented to assert the validity of the proposed models."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0606020v2", 
    "title": "Imagination as Holographic Processor for Text Animation", 
    "arxiv-id": "cs/0606020v2", 
    "author": "Brian Sanders", 
    "publish": "2006-06-05T23:55:37Z", 
    "summary": "Imagination is the critical point in developing of realistic artificial\nintelligence (AI) systems. One way to approach imagination would be simulation\nof its properties and operations. We developed two models: AI-Brain Network\nHierarchy of Languages and Semantical Holographic Calculus as well as\nsimulation system ScriptWriter that emulate the process of imagination through\nan automatic animation of English texts. The purpose of this paper is to\ndemonstrate the model and to present ScriptWriter system\nhttp://nvo.sdsc.edu/NVO/JCSG/get_SRB_mime_file2.cgi//home/tamara.sdsc/test/demo.zip?F=/home/tamara.sdsc/test/demo.zip&M=application/x-gtar\nfor simulation of the imagination."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0606029v1", 
    "title": "Belief Calculus", 
    "arxiv-id": "cs/0606029v1", 
    "author": "Audun Josang", 
    "publish": "2006-06-07T14:32:55Z", 
    "summary": "In Dempster-Shafer belief theory, general beliefs are expressed as belief\nmass distribution functions over frames of discernment. In Subjective Logic\nbeliefs are expressed as belief mass distribution functions over binary frames\nof discernment. Belief representations in Subjective Logic, which are called\nopinions, also contain a base rate parameter which express the a priori belief\nin the absence of evidence. Philosophically, beliefs are quantitative\nrepresentations of evidence as perceived by humans or by other intelligent\nagents. The basic operators of classical probability calculus, such as addition\nand multiplication, can be applied to opinions, thereby making belief calculus\npractical. Through the equivalence between opinions and Beta probability\ndensity functions, this also provides a calculus for Beta probability density\nfunctions. This article explains the basic elements of belief calculus."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0606066v1", 
    "title": "The Cumulative Rule for Belief Fusion", 
    "arxiv-id": "cs/0606066v1", 
    "author": "Audun Josang", 
    "publish": "2006-06-14T11:36:06Z", 
    "summary": "The problem of combining beliefs in the Dempster-Shafer belief theory has\nattracted considerable attention over the last two decades. The classical\nDempster's Rule has often been criticised, and many alternative rules for\nbelief combination have been proposed in the literature. The consensus operator\nfor combining beliefs has nice properties and produces more intuitive results\nthan Dempster's rule, but has the limitation that it can only be applied to\nbelief distribution functions on binary state spaces. In this paper we present\na generalisation of the consensus operator that can be applied to Dirichlet\nbelief functions on state spaces of arbitrary size. This rule, called the\ncumulative rule of belief combination, can be derived from classical\nstatistical theory, and corresponds well with human intuition."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0606081v3", 
    "title": "New Millennium AI and the Convergence of History", 
    "arxiv-id": "cs/0606081v3", 
    "author": "Juergen Schmidhuber", 
    "publish": "2006-06-19T09:13:43Z", 
    "summary": "Artificial Intelligence (AI) has recently become a real formal science: the\nnew millennium brought the first mathematically sound, asymptotically optimal,\nuniversal problem solvers, providing a new, rigorous foundation for the\npreviously largely heuristic field of General AI and embedded agents. At the\nsame time there has been rapid progress in practical methods for learning true\nsequence-processing programs, as opposed to traditional methods limited to\nstationary pattern association. Here we will briefly review some of the new\nresults, and speculate about future developments, pointing out that the time\nintervals between the most notable events in over 40,000 years or 2^9 lifetimes\nof human history have sped up exponentially, apparently converging to zero\nwithin the next few decades. Or is this impression just a by-product of the way\nhumans allocate memory space to past events?"
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607005v2", 
    "title": "Belief Conditioning Rules (BCRs)", 
    "arxiv-id": "cs/0607005v2", 
    "author": "Jean Dezert", 
    "publish": "2006-07-02T14:54:54Z", 
    "summary": "In this paper we propose a new family of Belief Conditioning Rules (BCRs) for\nbelief revision. These rules are not directly related with the fusion of\nseveral sources of evidence but with the revision of a belief assignment\navailable at a given time according to the new truth (i.e. conditioning\nconstraint) one has about the space of solutions of the problem."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607071v1", 
    "title": "Islands for SAT", 
    "arxiv-id": "cs/0607071v1", 
    "author": "P. J. Stuckey", 
    "publish": "2006-07-14T12:44:24Z", 
    "summary": "In this note we introduce the notion of islands for restricting local search.\nWe show how we can construct islands for CNF SAT problems, and how much search\nspace can be eliminated by restricting search to the island."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607084v1", 
    "title": "About Norms and Causes", 
    "arxiv-id": "cs/0607084v1", 
    "author": "Farid Nouioua", 
    "publish": "2006-07-18T07:46:07Z", 
    "summary": "Knowing the norms of a domain is crucial, but there exist no repository of\nnorms. We propose a method to extract them from texts: texts generally do not\ndescribe a norm, but rather how a state-of-affairs differs from it. Answers\nconcerning the cause of the state-of-affairs described often reveal the\nimplicit norm. We apply this idea to the domain of driving, and validate it by\ndesigning algorithms that identify, in a text, the \"basic\" norms to which it\nrefers implicitly."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607086v1", 
    "title": "Representing Knowledge about Norms", 
    "arxiv-id": "cs/0607086v1", 
    "author": "Farid Nouioua", 
    "publish": "2006-07-18T08:15:04Z", 
    "summary": "Norms are essential to extend inference: inferences based on norms are far\nricher than those based on logical implications. In the recent decades, much\neffort has been devoted to reason on a domain, once its norms are represented.\nHow to extract and express those norms has received far less attention.\nExtraction is difficult: as the readers are supposed to know them, the norms of\na domain are seldom made explicit. For one thing, extracting norms requires a\nlanguage to represent them, and this is the topic of this paper. We apply this\nlanguage to represent norms in the domain of driving, and show that it is\nadequate to reason on the causes of accidents, as described by car-crash\nreports."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607143v1", 
    "title": "Target Type Tracking with PCR5 and Dempster's rules: A Comparative   Analysis", 
    "arxiv-id": "cs/0607143v1", 
    "author": "Pavlina Konstantinova", 
    "publish": "2006-07-31T15:32:44Z", 
    "summary": "In this paper we consider and analyze the behavior of two combinational rules\nfor temporal (sequential) attribute data fusion for target type estimation. Our\ncomparative analysis is based on Dempster's fusion rule proposed in\nDempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution\nrule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We\nshow through very simple scenario and Monte-Carlo simulation, how PCR5 allows a\nvery efficient Target Type Tracking and reduces drastically the latency delay\nfor correct Target Type decision with respect to Demspter's rule. For cases\npresenting some short Target Type switches, Demspter's rule is proved to be\nunable to detect the switches and thus to track correctly the Target Type\nchanges. The approach proposed here is totally new, efficient and promising to\nbe incorporated in real-time Generalized Data Association - Multi Target\nTracking systems (GDA-MTT) and provides an important result on the behavior of\nPCR5 with respect to Dempster's rule. The MatLab source code is provided in"
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0607147v2", 
    "title": "Fusion of qualitative beliefs using DSmT", 
    "arxiv-id": "cs/0607147v2", 
    "author": "Jean Dezert", 
    "publish": "2006-07-31T17:16:57Z", 
    "summary": "This paper introduces the notion of qualitative belief assignment to model\nbeliefs of human experts expressed in natural language (with linguistic\nlabels). We show how qualitative beliefs can be efficiently combined using an\nextension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical\nquantitative reasoning to qualitative reasoning. We propose a new arithmetic on\nlinguistic labels which allows a direct extension of classical DSm fusion rule\nor DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed\njointly with a Qualitative Average Operator. We also show how crisp or interval\nmappings can be used to deal indirectly with linguistic labels. A very simple\nexample is provided to illustrate our qualitative fusion rules."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0608002v1", 
    "title": "An Introduction to the DSm Theory for the Combination of Paradoxical,   Uncertain, and Imprecise Sources of Information", 
    "arxiv-id": "cs/0608002v1", 
    "author": "Jean Dezert", 
    "publish": "2006-08-01T15:31:13Z", 
    "summary": "The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been, and\nstill remains today, of primal importance for the development of reliable\nmodern information systems involving artificial reasoning. In this\nintroduction, we present a survey of our recent theory of plausible and\nparadoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the\nliterature, developed for dealing with imprecise, uncertain and paradoxical\nsources of information. We focus our presentation here rather on the\nfoundations of DSmT, and on the two important new rules of combination, than on\nbrowsing specific applications of DSmT available in literature. Several simple\nexamples are given throughout the presentation to show the efficiency and the\ngenerality of this new approach."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0608019v1", 
    "title": "Relation Variables in Qualitative Spatial Reasoning", 
    "arxiv-id": "cs/0608019v1", 
    "author": "Sebastian Brand", 
    "publish": "2006-08-03T03:24:24Z", 
    "summary": "We study an alternative to the prevailing approach to modelling qualitative\nspatial reasoning (QSR) problems as constraint satisfaction problems. In the\nstandard approach, a relation between objects is a constraint whereas in the\nalternative approach it is a variable. The relation-variable approach greatly\nsimplifies integration and implementation of QSR. To substantiate this point,\nwe discuss several QSR algorithms from the literature which in the\nrelation-variable approach reduce to the customary constraint propagation\nalgorithm enforcing generalised arc-consistency."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-74205-0_73", 
    "link": "http://arxiv.org/pdf/cs/0608028v1", 
    "title": "Using Sets of Probability Measures to Represent Uncertainty", 
    "arxiv-id": "cs/0608028v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2006-08-04T20:26:25Z", 
    "summary": "I explore the use of sets of probability measures as a representation of\nuncertainty."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0609111v2", 
    "title": "A State-Based Regression Formulation for Domains with Sensing   Actions<br> and Incomplete Information", 
    "arxiv-id": "cs/0609111v2", 
    "author": "Tran Cao Son", 
    "publish": "2006-09-19T21:33:07Z", 
    "summary": "We present a state-based regression function for planning domains where an\nagent does not have complete information and may have sensing actions. We\nconsider binary domains and employ a three-valued characterization of domains\nwith sensing actions to define the regression function. We prove the soundness\nand completeness of our regression formulation with respect to the definition\nof progression. More specifically, we show that (i) a plan obtained through\nregression for a planning problem is indeed a progression solution of that\nplanning problem, and that (ii) for each plan found through progression, using\nregression one obtains that plan or an equivalent one."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0609132v1", 
    "title": "Semantic Description of Parameters in Web Service Annotations", 
    "arxiv-id": "cs/0609132v1", 
    "author": "Jochen Gruber", 
    "publish": "2006-09-24T17:29:49Z", 
    "summary": "A modification of OWL-S regarding parameter description is proposed. It is\nstrictly based on Description Logic. In addition to class description of\nparameters it also allows the modelling of relations between parameters and the\nprecise description of the size of data to be supplied to a service. In\nparticular, it solves two major issues identified within current proposals for\na Semantic Web Service annotation standard."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0609136v1", 
    "title": "The ALVIS Format for Linguistically Annotated Documents", 
    "arxiv-id": "cs/0609136v1", 
    "author": "Davy Weissenbacher", 
    "publish": "2006-09-24T20:04:01Z", 
    "summary": "The paper describes the ALVIS annotation format designed for the indexing of\nlarge collections of documents in topic-specific search engines. This paper is\nexemplified on the biological domain and on MedLine abstracts, as developing a\nspecialized search engine for biologists is one of the ALVIS case studies. The\nALVIS principle for linguistic annotations is based on existing works and\nstandard propositions. We made the choice of stand-off annotations rather than\ninserted mark-up. Annotations are encoded as XML elements which form the\nlinguistic subsection of the document record."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0609142v1", 
    "title": "Modular self-organization", 
    "arxiv-id": "cs/0609142v1", 
    "author": "Bruno Scherrer", 
    "publish": "2006-09-26T07:52:54Z", 
    "summary": "The aim of this paper is to provide a sound framework for addressing a\ndifficult problem: the automatic construction of an autonomous agent's modular\narchitecture. We combine results from two apparently uncorrelated domains:\nAutonomous planning through Markov Decision Processes and a General Data\nClustering Approach using a kernel-like method. Our fundamental idea is that\nthe former is a good framework for addressing autonomy whereas the latter\nallows to tackle self-organizing problems."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610006v2", 
    "title": "A Typed Hybrid Description Logic Programming Language with Polymorphic   Order-Sorted DL-Typed Unification for Semantic Web Type Systems", 
    "arxiv-id": "cs/0610006v2", 
    "author": "Adrian Paschke", 
    "publish": "2006-10-02T08:57:54Z", 
    "summary": "In this paper we elaborate on a specific application in the context of hybrid\ndescription logic programs (hybrid DLPs), namely description logic Semantic Web\ntype systems (DL-types) which are used for term typing of LP rules based on a\npolymorphic, order-sorted, hybrid DL-typed unification as procedural semantics\nof hybrid DLPs. Using Semantic Web ontologies as type systems facilitates\ninterchange of domain-independent rules over domain boundaries via dynamically\ntyping and mapping of explicitly defined type ontologies."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610015v1", 
    "title": "Why did the accident happen? A norm-based reasoning approach", 
    "arxiv-id": "cs/0610015v1", 
    "author": "Farid Nouioua", 
    "publish": "2006-10-04T11:32:22Z", 
    "summary": "In this paper we describe an architecture of a system that answer the\nquestion : Why did the accident happen? from the textual description of an\naccident. We present briefly the different parts of the architecture and then\nwe describe with more detail the semantic part of the system i.e. the part in\nwhich the norm-based reasoning is performed on the explicit knowlege extracted\nfrom the text."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610023v1", 
    "title": "Une exp\u00e9rience de s\u00e9mantique inf\u00e9rentielle", 
    "arxiv-id": "cs/0610023v1", 
    "author": "Daniel Kayser", 
    "publish": "2006-10-05T05:18:03Z", 
    "summary": "We develop a system which must be able to perform the same inferences that a\nhuman reader of an accident report can do and more particularly to determine\nthe apparent causes of the accident. We describe the general framework in which\nwe are situated, linguistic and semantic levels of the analysis and the\ninference rules used by the system."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610043v1", 
    "title": "Farthest-Point Heuristic based Initialization Methods for K-Modes   Clustering", 
    "arxiv-id": "cs/0610043v1", 
    "author": "Zengyou He", 
    "publish": "2006-10-09T12:12:45Z", 
    "summary": "The k-modes algorithm has become a popular technique in solving categorical\ndata clustering problems in different application domains. However, the\nalgorithm requires random selection of initial points for the clusters.\nDifferent initial points often lead to considerable distinct clustering\nresults. In this paper we present an experimental study on applying a\nfarthest-point heuristic based initialization method to k-modes clustering to\nimprove its performance. Experiments show that new initialization method leads\nto better clustering accuracy than random selection initialization method for\nk-modes clustering."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610060v1", 
    "title": "Comparing Typical Opening Move Choices Made by Humans and Chess Engines", 
    "arxiv-id": "cs/0610060v1", 
    "author": "Judit Bar-Ilan", 
    "publish": "2006-10-11T10:26:40Z", 
    "summary": "The opening book is an important component of a chess engine, and thus\ncomputer chess programmers have been developing automated methods to improve\nthe quality of their books. For chess, which has a very rich opening theory,\nlarge databases of high-quality games can be used as the basis of an opening\nbook, from which statistics relating to move choices from given positions can\nbe collected. In order to find out whether the opening books used by modern\nchess engines in machine versus machine competitions are ``comparable'' to\nthose used by chess players in human versus human competitions, we carried out\nanalysis on 26 test positions using statistics from two opening books one\ncompiled from humans' games and the other from machines' games. Our analysis\nusing several nonparametric measures, shows that, overall, there is a strong\nassociation between humans' and machines' choices of opening moves when using a\nbook to guide their choices."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610111v3", 
    "title": "Local approximate inference algorithms", 
    "arxiv-id": "cs/0610111v3", 
    "author": "Devavrat Shah", 
    "publish": "2006-10-18T21:51:44Z", 
    "summary": "We present a new local approximation algorithm for computing Maximum a\nPosteriori (MAP) and log-partition function for arbitrary exponential family\ndistribution represented by a finite-valued pair-wise Markov random field\n(MRF), say $G$. Our algorithm is based on decomposition of $G$ into {\\em\nappropriately} chosen small components; then computing estimates locally in\neach of these components and then producing a {\\em good} global solution. We\nshow that if the underlying graph $G$ either excludes some finite-sized graph\nas its minor (e.g. Planar graph) or has low doubling dimension (e.g. any graph\nwith {\\em geometry}), then our algorithm will produce solution for both\nquestions within {\\em arbitrary accuracy}. We present a message-passing\nimplementation of our algorithm for MAP computation using self-avoiding walk of\ngraph. In order to evaluate the computational cost of this implementation, we\nderive novel tight bounds on the size of self-avoiding walk tree for arbitrary\ngraph.\n  As a consequence of our algorithmic result, we show that the normalized\nlog-partition function (also known as free-energy) for a class of {\\em regular}\nMRFs will converge to a limit, that is computable to an arbitrary accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610140v1", 
    "title": "Constant for associative patterns ensemble", 
    "arxiv-id": "cs/0610140v1", 
    "author": "Peter Komarov", 
    "publish": "2006-10-24T16:27:09Z", 
    "summary": "Creation procedure of associative patterns ensemble in terms of formal logic\nwith using neural net-work (NN) model is formulated. It is shown that the\nassociative patterns set is created by means of unique procedure of NN work\nwhich having individual parameters of entrance stimulus transformation. It is\nascer-tained that the quantity of the selected associative patterns possesses\nis a constant."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610156v1", 
    "title": "Adaptation Knowledge Discovery from a Case Base", 
    "arxiv-id": "cs/0610156v1", 
    "author": "Laszlo Szathmary", 
    "publish": "2006-10-27T10:08:32Z", 
    "summary": "In case-based reasoning, the adaptation step depends in general on\ndomain-dependent knowledge, which motivates studies on adaptation knowledge\nacquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge\ndiscovery from databases. This system explores the variations within the case\nbase to elicit adaptation knowledge. It has been successfully tested in an\napplication of case-based decision support to breast cancer treatment."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610165v1", 
    "title": "Decentralized Failure Diagnosis of Stochastic Discrete Event Systems", 
    "arxiv-id": "cs/0610165v1", 
    "author": "Zhujun Fan", 
    "publish": "2006-10-30T09:59:31Z", 
    "summary": "Recently, the diagnosability of {\\it stochastic discrete event systems}\n(SDESs) was investigated in the literature, and, the failure diagnosis\nconsidered was {\\it centralized}. In this paper, we propose an approach to {\\it\ndecentralized} failure diagnosis of SDESs, where the stochastic system uses\nmultiple local diagnosers to detect failures and each local diagnoser possesses\nits own information. In a way, the centralized failure diagnosis of SDESs can\nbe viewed as a special case of the decentralized failure diagnosis presented in\nthis paper with only one projection. The main contributions are as follows: (1)\nWe formalize the notion of codiagnosability for stochastic automata, which\nmeans that a failure can be detected by at least one local stochastic diagnoser\nwithin a finite delay. (2) We construct a codiagnoser from a given stochastic\nautomaton with multiple projections, and the codiagnoser associated with the\nlocal diagnosers is used to test codiagnosability condition of SDESs. (3) We\ndeal with a number of basic properties of the codiagnoser. In particular, a\nnecessary and sufficient condition for the codiagnosability of SDESs is\npresented. (4) We give a computing method in detail to check whether\ncodiagnosability is violated. And (5) some examples are described to illustrate\nthe applications of the codiagnosability and its computing method."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0610175v1", 
    "title": "DSmT: A new paradigm shift for information fusion", 
    "arxiv-id": "cs/0610175v1", 
    "author": "Florentin Smarandache", 
    "publish": "2006-10-31T14:50:06Z", 
    "summary": "The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been and\nstill remains of primal importance for the development of reliable information\nfusion systems. In this short survey paper, we present the theory of plausible\nand paradoxical reasoning, known as DSmT (Dezert-Smarandache Theory) in\nliterature, developed for dealing with imprecise, uncertain and potentially\nhighly conflicting sources of information. DSmT is a new paradigm shift for\ninformation fusion and recent publications have shown the interest and the\npotential ability of DSmT to solve fusion problems where Dempster's rule used\nin Dempster-Shafer Theory (DST) provides counter-intuitive results or fails to\nprovide useful result at all. This paper is focused on the foundations of DSmT\nand on its main rules of combination (classic, hybrid and Proportional Conflict\nRedistribution rules). Shafer's model on which is based DST appears as a\nparticular and specific case of DSm hybrid model which can be easily handled by\nDSmT as well. Several simple but illustrative examples are given throughout\nthis paper to show the interest and the generality of this new theory."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0611047v1", 
    "title": "The Reaction RuleML Classification of the Event / Action / State   Processing and Reasoning Space", 
    "arxiv-id": "cs/0611047v1", 
    "author": "Adrian Paschke", 
    "publish": "2006-11-10T22:03:11Z", 
    "summary": "Reaction RuleML is a general, practical, compact and user-friendly\nXML-serialized language for the family of reaction rules. In this white paper\nwe give a review of the history of event / action /state processing and\nreaction rule approaches and systems in different domains, define basic\nconcepts and give a classification of the event, action, state processing and\nreasoning space as well as a discussion of relevant / related work"
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-2(4:2)2006", 
    "link": "http://arxiv.org/pdf/cs/0611085v1", 
    "title": "Fuzzy Logic Classification of Imaging Laser Desorption Fourier Transform   Mass Spectrometry Data", 
    "arxiv-id": "cs/0611085v1", 
    "author": "Jill R. Scott", 
    "publish": "2006-11-17T19:14:47Z", 
    "summary": "A fuzzy logic based classification engine has been developed for classifying\nmass spectra obtained with an imaging internal source Fourier transform mass\nspectrometer (I^2LD-FTMS). Traditionally, an operator uses the relative\nabundance of ions with specific mass-to-charge (m/z) ratios to categorize\nspectra. An operator does this by comparing the spectrum of m/z versus\nabundance of an unknown sample against a library of spectra from known samples.\nAutomated positioning and acquisition allow I^2LD-FTMS to acquire data from\nvery large grids, this would require classification of up to 3600 spectrum per\nhour to keep pace with the acquisition. The tedious job of classifying numerous\nspectra generated in an I^2LD-FTMS imaging application can be replaced by a\nfuzzy rule base if the cues an operator uses can be encapsulated. We present\nthe translation of linguistic rules to a fuzzy classifier for mineral phases in\nbasalt. This paper also describes a method for gathering statistics on ions,\nwhich are not currently used in the rule base, but which may be candidates for\nmaking the rule base more accurate and complete or to form new rule bases based\non data obtained from known samples. A spatial method for classifying spectra\nwith low membership values, based on neighboring sample classifications, is\nalso presented."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0611118v2", 
    "title": "A Neutrosophic Description Logic", 
    "arxiv-id": "cs/0611118v2", 
    "author": "Rajshekhar Sunderraman", 
    "publish": "2006-11-22T20:04:21Z", 
    "summary": "Description Logics (DLs) are appropriate, widely used, logics for managing\nstructured knowledge. They allow reasoning about individuals and concepts, i.e.\nset of individuals with common properties. Typically, DLs are limited to\ndealing with crisp, well defined concepts. That is, concepts for which the\nproblem whether an individual is an instance of it is yes/no question. More\noften than not, the concepts encountered in the real world do not have a\nprecisely defined criteria of membership: we may say that an individual is an\ninstance of a concept only to a certain degree, depending on the individual's\nproperties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In\norder to deal with fuzzy, incomplete, indeterminate and inconsistent concepts,\nwe need to extend the fuzzy DLs, combining the neutrosophic logic with a\nclassical DL. In particular, concepts become neutrosophic (here neutrosophic\nmeans fuzzy, incomplete, indeterminate, and inconsistent), thus reasoning about\nneutrosophic concepts is supported. We'll define its syntax, its semantics, and\ndescribe its properties."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0611135v1", 
    "title": "Genetic Programming for Kernel-based Learning with Co-evolving Subsets   Selection", 
    "arxiv-id": "cs/0611135v1", 
    "author": "Marco Tomassini", 
    "publish": "2006-11-27T14:38:44Z", 
    "summary": "Support Vector Machines (SVMs) are well-established Machine Learning (ML)\nalgorithms. They rely on the fact that i) linear learning can be formalized as\na well-posed optimization problem; ii) non-linear learning can be brought into\nlinear learning thanks to the kernel trick and the mapping of the initial\nsearch space onto a high dimensional feature space. The kernel is designed by\nthe ML expert and it governs the efficiency of the SVM approach. In this paper,\na new approach for the automatic design of kernels by Genetic Programming,\ncalled the Evolutionary Kernel Machine (EKM), is presented. EKM combines a\nwell-founded fitness function inspired from the margin criterion, and a\nco-evolution framework ensuring the computational scalability of the approach.\nEmpirical validation on standard ML benchmark demonstrates that EKM is\ncompetitive using state-of-the-art SVMs with tuned hyper-parameters."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0611138v1", 
    "title": "Functional Brain Imaging with Multi-Objective Multi-Modal Evolutionary   Optimization", 
    "arxiv-id": "cs/0611138v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2006-11-28T00:54:43Z", 
    "summary": "Functional brain imaging is a source of spatio-temporal data mining problems.\nA new framework hybridizing multi-objective and multi-modal optimization is\nproposed to formalize these data mining problems, and addressed through\nEvolutionary Computation (EC). The merits of EC for spatio-temporal data mining\nare demonstrated as the approach facilitates the modelling of the experts'\nrequirements, and flexibly accommodates their changing goals."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0611141v1", 
    "title": "A Generic Global Constraint based on MDDs", 
    "arxiv-id": "cs/0611141v1", 
    "author": "Rasmus Pagh", 
    "publish": "2006-11-28T14:23:23Z", 
    "summary": "The paper suggests the use of Multi-Valued Decision Diagrams (MDDs) as the\nsupporting data structure for a generic global constraint. We give an algorithm\nfor maintaining generalized arc consistency (GAC) on this constraint that\namortizes the cost of the GAC computation over a root-to-terminal path in the\nsearch tree. The technique used is an extension of the GAC algorithm for the\nregular language constraint on finite length input. Our approach adds support\nfor skipped variables, maintains the reduced property of the MDD dynamically\nand provides domain entailment detection. Finally we also show how to adapt the\napproach to constraint types that are closely related to MDDs, such as AOMDDs\nand Case DAGs."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0612056v1", 
    "title": "Conscious Intelligent Systems - Part 1 : I X I", 
    "arxiv-id": "cs/0612056v1", 
    "author": "U. Gayathree", 
    "publish": "2006-12-09T17:18:20Z", 
    "summary": "Did natural consciousness and intelligent systems arise out of a path that\nwas co-evolutionary to evolution? Can we explain human self-consciousness as\nhaving risen out of such an evolutionary path? If so how could it have been?\n  In this first part of a two-part paper (titled IXI), we take a learning\nsystem perspective to the problem of consciousness and intelligent systems, an\napproach that may look unseasonable in this age of fMRI's and high tech\nneuroscience.\n  We posit conscious intelligent systems in natural environments and wonder how\nnatural factors influence their design paths. Such a perspective allows us to\nexplain seamlessly a variety of natural factors, factors ranging from the rise\nand presence of the human mind, man's sense of I, his self-consciousness and\nhis looping thought processes to factors like reproduction, incubation,\nextinction, sleep, the richness of natural behavior, etc. It even allows us to\nspeculate on a possible human evolution scenario and other natural phenomena."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0612057v1", 
    "title": "Conscious Intelligent Systems - Part II - Mind, Thought, Language and   Understanding", 
    "arxiv-id": "cs/0612057v1", 
    "author": "U. Gayathree", 
    "publish": "2006-12-09T17:28:24Z", 
    "summary": "This is the second part of a paper on Conscious Intelligent Systems. We use\nthe understanding gained in the first part (Conscious Intelligent Systems Part\n1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the\npresence of mind affects understanding and intelligent systems; we see that the\npresence of mind necessitates language. The rise of language in turn has\nimportant effects on understanding. We discuss the humanoid question and how\nthe question of self-consciousness (and by association mind/thought/language)\nwould affect humanoids too."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0612068v1", 
    "title": "Interactive Configuration by Regular String Constraints", 
    "arxiv-id": "cs/0612068v1", 
    "author": "Henrik Reif Andersen", 
    "publish": "2006-12-12T16:21:16Z", 
    "summary": "A product configurator which is complete, backtrack free and able to compute\nthe valid domains at any state of the configuration can be constructed by\nbuilding a Binary Decision Diagram (BDD). Despite the fact that the size of the\nBDD is exponential in the number of variables in the worst case, BDDs have\nproved to work very well in practice. Current BDD-based techniques can only\nhandle interactive configuration with small finite domains. In this paper we\nextend the approach to handle string variables constrained by regular\nexpressions. The user is allowed to change the strings by adding letters at the\nend of the string. We show how to make a data structure that can perform fast\nvalid domain computations given some assignment on the set of string variables.\n  We first show how to do this by using one large DFA. Since this approach is\ntoo space consuming to be of practical use, we construct a data structure that\nsimulates the large DFA and in most practical cases are much more space\nefficient. As an example a configuration problem on $n$ string variables with\nonly one solution in which each string variable is assigned to a value of\nlength of $k$ the former structure will use $\\Omega(k^n)$ space whereas the\nlatter only need $O(kn)$. We also show how this framework easily can be\ncombined with the recent BDD techniques to allow both boolean, integer and\nstring variables in the configuration problem."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0612109v2", 
    "title": "Truncating the loop series expansion for Belief Propagation", 
    "arxiv-id": "cs/0612109v2", 
    "author": "H. J. Kappen", 
    "publish": "2006-12-21T17:29:28Z", 
    "summary": "Recently, M. Chertkov and V.Y. Chernyak derived an exact expression for the\npartition sum (normalization constant) corresponding to a graphical model,\nwhich is an expansion around the Belief Propagation solution. By adding\ncorrection terms to the BP free energy, one for each \"generalized loop\" in the\nfactor graph, the exact partition sum is obtained. However, the usually\nenormous number of generalized loops generally prohibits summation over all\ncorrection terms. In this article we introduce Truncated Loop Series BP\n(TLSBP), a particular way of truncating the loop series of M. Chertkov and V.Y.\nChernyak by considering generalized loops as compositions of simple loops. We\nanalyze the performance of TLSBP in different scenarios, including the Ising\nmodel, regular random graphs and on Promedas, a large probabilistic medical\ndiagnostic system. We show that TLSBP often improves upon the accuracy of the\nBP solution, at the expense of increased computation time. We also show that\nthe performance of TLSBP strongly depends on the degree of interaction between\nthe variables. For weak interactions, truncating the series leads to\nsignificant improvements, whereas for strong interactions it can be\nineffective, even if a high number of terms is considered."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S1793005708001100", 
    "link": "http://arxiv.org/pdf/cs/0701013v1", 
    "title": "Attribute Value Weighting in K-Modes Clustering", 
    "arxiv-id": "cs/0701013v1", 
    "author": "Shengchun Deng", 
    "publish": "2007-01-03T09:06:03Z", 
    "summary": "In this paper, the traditional k-modes clustering algorithm is extended by\nweighting attribute value matches in dissimilarity computation. The use of\nattribute value weighting technique makes it possible to generate clusters with\nstronger intra-similarities, and therefore achieve better clustering\nperformance. Experimental results on real life datasets show that these value\nweighting based k-modes algorithms are superior to the standard k-modes\nalgorithm with respect to clustering accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0701184v2", 
    "title": "Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br>   SAT-Based Planning", 
    "arxiv-id": "cs/0701184v2", 
    "author": "Bart Selman", 
    "publish": "2007-01-29T12:47:08Z", 
    "summary": "In Verification and in (optimal) AI Planning, a successful method is to\nformulate the application as boolean satisfiability (SAT), and solve it with\nstate-of-the-art DPLL-based procedures. There is a lack of understanding of why\nthis works so well. Focussing on the Planning context, we identify a form of\nproblem structure concerned with the symmetrical or asymmetrical nature of the\ncost of achieving the individual planning goals. We quantify this sort of\nstructure with a simple numeric parameter called AsymRatio, ranging between 0\nand 1. We run experiments in 10 benchmark domains from the International\nPlanning Competitions since 2000; we show that AsymRatio is a good indicator of\nSAT solver performance in 8 of these domains. We then examine carefully crafted\nsynthetic planning domains that allow control of the amount of structure, and\nthat are clean enough for a rigorous analysis of the combinatorial search\nspace. The domains are parameterized by size, and by the amount of structure.\nThe CNFs we examine are unsatisfiable, encoding one planning step less than the\nlength of the optimal plan. We prove upper and lower bounds on the size of the\nbest possible DPLL refutations, under different settings of the amount of\nstructure, as a function of size. We also identify the best possible sets of\nbranching variables (backdoors). With minimum AsymRatio, we prove exponential\nlower bounds, and identify minimal backdoors of size linear in the number of\nvariables. With maximum AsymRatio, we identify logarithmic DPLL refutations\n(and backdoors), showing a doubly exponential gap between the two structural\nextreme cases. The reasons for this behavior -- the proof arguments --\nilluminate the prototypical patterns of structure causing the empirical\nbehavior observed in the competition benchmarks."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0702028v2", 
    "title": "Uniform and Partially Uniform Redistribution Rules", 
    "arxiv-id": "cs/0702028v2", 
    "author": "Jean Dezert", 
    "publish": "2007-02-05T14:56:49Z", 
    "summary": "This short paper introduces two new fusion rules for combining quantitative\nbasic belief assignments. These rules although very simple have not been\nproposed in literature so far and could serve as useful alternatives because of\ntheir low computation cost with respect to the recent advanced Proportional\nConflict Redistribution rules developed in the DSmT framework."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0702170v1", 
    "title": "Generic Global Constraints based on MDDs", 
    "arxiv-id": "cs/0702170v1", 
    "author": "Rasmus Pagh", 
    "publish": "2007-02-28T15:32:48Z", 
    "summary": "Constraint Programming (CP) has been successfully applied to both constraint\nsatisfaction and constraint optimization problems. A wide variety of\nspecialized global constraints provide critical assistance in achieving a good\nmodel that can take advantage of the structure of the problem in the search for\na solution. However, a key outstanding issue is the representation of 'ad-hoc'\nconstraints that do not have an inherent combinatorial nature, and hence are\nnot modeled well using narrowly specialized global constraints. We attempt to\naddress this issue by considering a hybrid of search and compilation.\nSpecifically we suggest the use of Reduced Ordered Multi-Valued Decision\nDiagrams (ROMDDs) as the supporting data structure for a generic global\nconstraint. We give an algorithm for maintaining generalized arc consistency\n(GAC) on this constraint that amortizes the cost of the GAC computation over a\nroot-to-leaf path in the search tree without requiring asymptotically more\nspace than used for the MDD. Furthermore we present an approach for\nincrementally maintaining the reduced property of the MDD during the search,\nand show how this can be used for providing domain entailment detection.\nFinally we discuss how to apply our approach to other similar data structures\nsuch as AOMDDs and Case DAGs. The technique used can be seen as an extension of\nthe GAC algorithm for the regular language constraint on finite length input."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0703060v1", 
    "title": "Redesigning Decision Matrix Method with an indeterminacy-based inference   process", 
    "arxiv-id": "cs/0703060v1", 
    "author": "Florentin Smarandache", 
    "publish": "2007-03-13T02:18:09Z", 
    "summary": "For academics and practitioners concerned with computers, business and\nmathematics, one central issue is supporting decision makers. In this paper, we\npropose a generalization of Decision Matrix Method (DMM), using Neutrosophic\nlogic. It emerges as an alternative to the existing logics and it represents a\nmathematical model of uncertainty and indeterminacy. This paper proposes the\nNeutrosophic Decision Matrix Method as a more realistic tool for decision\nmaking. In addition, a de-neutrosophication process is included."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0703124v1", 
    "title": "Modelling Complexity in Musical Rhythm", 
    "arxiv-id": "cs/0703124v1", 
    "author": "Chia-Ying Lee", 
    "publish": "2007-03-26T07:37:11Z", 
    "summary": "This paper constructs a tree structure for the music rhythm using the\nL-system. It models the structure as an automata and derives its complexity. It\nalso solves the complexity for the L-system. This complexity can resolve the\nsimilarity between trees. This complexity serves as a measure of psychological\ncomplexity for rhythms. It resolves the music complexity of various\ncompositions including the Mozart effect K488.\n  Keyword: music perception, psychological complexity, rhythm, L-system,\nautomata, temporal associative memory, inverse problem, rewriting rule,\nbracketed string, tree similarity"
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0703130v1", 
    "title": "Space-contained conflict revision, for geographic information", 
    "arxiv-id": "cs/0703130v1", 
    "author": "Robert Jeansoulin", 
    "publish": "2007-03-26T12:18:32Z", 
    "summary": "Using qualitative reasoning with geographic information, contrarily, for\ninstance, with robotics, looks not only fastidious (i.e.: encoding knowledge\nPropositional Logics PL), but appears to be computational complex, and not\ntractable at all, most of the time. However, knowledge fusion or revision, is a\ncommon operation performed when users merge several different data sets in a\nunique decision making process, without much support. Introducing logics would\nbe a great improvement, and we propose in this paper, means for deciding -a\npriori- if one application can benefit from a complete revision, under only the\nassumption of a conjecture that we name the \"containment conjecture\", which\nlimits the size of the minimal conflicts to revise. We demonstrate that this\nconjecture brings us the interesting computational property of performing a\nnot-provable but global, revision, made of many local revisions, at a tractable\nsize. We illustrate this approach on an application."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/cs/0703156v1", 
    "title": "Case Base Mining for Adaptation Knowledge Acquisition", 
    "arxiv-id": "cs/0703156v1", 
    "author": "Laszlo Szathmary", 
    "publish": "2007-03-30T16:16:11Z", 
    "summary": "In case-based reasoning, the adaptation of a source case in order to solve\nthe target problem is at the same time crucial and difficult to implement. The\nreason for this difficulty is that, in general, adaptation strongly depends on\ndomain-dependent knowledge. This fact motivates research on adaptation\nknowledge acquisition (AKA). This paper presents an approach to AKA based on\nthe principles and techniques of knowledge discovery from databases and\ndata-mining. It is implemented in CABAMAKA, a system that explores the\nvariations within the case base to elicit adaptation knowledge. This system has\nbeen successfully tested in an application of case-based reasoning to decision\nsupport in the domain of breast cancer treatment."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/0704.1394v1", 
    "title": "Calculating Valid Domains for BDD-Based Interactive Configuration", 
    "arxiv-id": "0704.1394v1", 
    "author": "Henrik Reif Andersen", 
    "publish": "2007-04-11T10:59:56Z", 
    "summary": "In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/0704.2010v2", 
    "title": "A study of structural properties on profiles HMMs", 
    "arxiv-id": "0704.2010v2", 
    "author": "Gerson Zaverucha", 
    "publish": "2007-04-16T13:10:35Z", 
    "summary": "Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-3(1:6)2007", 
    "link": "http://arxiv.org/pdf/0704.3433v1", 
    "title": "Bayesian approach to rough set", 
    "arxiv-id": "0704.3433v1", 
    "author": "Bodie Crossingham", 
    "publish": "2007-04-25T19:50:59Z", 
    "summary": "This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV."
},{
    "category": "cs.AI", 
    "doi": "10.1155/2008/468693", 
    "link": "http://arxiv.org/pdf/0704.3515v1", 
    "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems   for Face Recognition", 
    "arxiv-id": "0704.3515v1", 
    "author": "C. Maple", 
    "publish": "2007-04-26T11:29:19Z", 
    "summary": "Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise."
},{
    "category": "cs.AI", 
    "doi": "10.1155/2008/468693", 
    "link": "http://arxiv.org/pdf/0704.3905v1", 
    "title": "Ensemble Learning for Free with Evolutionary Algorithms ?", 
    "arxiv-id": "0704.3905v1", 
    "author": "Marco Tomassini", 
    "publish": "2007-04-30T09:29:22Z", 
    "summary": "Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles."
},{
    "category": "cs.AI", 
    "doi": "10.1155/2008/468693", 
    "link": "http://arxiv.org/pdf/0705.0197v1", 
    "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support   Vector Machines and Guassian Mixture Models", 
    "arxiv-id": "0705.0197v1", 
    "author": "Snehashish Chakraverty", 
    "publish": "2007-05-02T03:13:28Z", 
    "summary": "Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates."
},{
    "category": "cs.AI", 
    "doi": "10.1155/2008/468693", 
    "link": "http://arxiv.org/pdf/0705.0693v1", 
    "title": "Learning to Bluff", 
    "arxiv-id": "0705.0693v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-07T19:15:24Z", 
    "summary": "The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.0734v1", 
    "title": "Soft constraint abstraction based on semiring homomorphism", 
    "arxiv-id": "0705.0734v1", 
    "author": "Mingsheng Ying", 
    "publish": "2007-05-05T08:47:31Z", 
    "summary": "The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.0761v3", 
    "title": "Bayesian Approach to Neuro-Rough Models", 
    "arxiv-id": "0705.0761v3", 
    "author": "Bodie Crossingham", 
    "publish": "2007-05-06T22:55:58Z", 
    "summary": "This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.0969v1", 
    "title": "Artificial Neural Networks and Support Vector Machines for Water Demand   Time Series Forecasting", 
    "arxiv-id": "0705.0969v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-07T19:00:28Z", 
    "summary": "Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.1031v1", 
    "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs   with Missing Values", 
    "arxiv-id": "0705.1031v1", 
    "author": "T. Marwala", 
    "publish": "2007-05-08T05:12:01Z", 
    "summary": "An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.1209v1", 
    "title": "Artificial Intelligence for Conflict Management", 
    "arxiv-id": "0705.1209v1", 
    "author": "M. Lagazio", 
    "publish": "2007-05-09T05:53:30Z", 
    "summary": "Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.1244v1", 
    "title": "Evolving Symbolic Controllers", 
    "arxiv-id": "0705.1244v1", 
    "author": "Mich\u00e8le Sebag", 
    "publish": "2007-05-09T09:53:31Z", 
    "summary": "The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.1309v1", 
    "title": "Robust Multi-Cellular Developmental Design", 
    "arxiv-id": "0705.1309v1", 
    "author": "Marc Schoenauer", 
    "publish": "2007-05-09T15:33:34Z", 
    "summary": "This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.2235v1", 
    "title": "Response Prediction of Structural System Subject to Earthquake Motions   using Artificial Neural Network", 
    "arxiv-id": "0705.2235v1", 
    "author": "Thando Tettey", 
    "publish": "2007-05-15T20:29:06Z", 
    "summary": "This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.2236v1", 
    "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy   modelling", 
    "arxiv-id": "0705.2236v1", 
    "author": "Snehashish Chakraverty", 
    "publish": "2007-05-15T20:34:05Z", 
    "summary": "This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.2310v1", 
    "title": "On-Line Condition Monitoring using Computational Intelligence", 
    "arxiv-id": "0705.2310v1", 
    "author": "E. Moloto", 
    "publish": "2007-05-16T09:19:00Z", 
    "summary": "This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.3360v1", 
    "title": "The Road to Quantum Artificial Intelligence", 
    "arxiv-id": "0705.3360v1", 
    "author": "Kyriakos N. Sgarbas", 
    "publish": "2007-05-23T12:31:47Z", 
    "summary": "This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0705.4302v1", 
    "title": "Truecluster matching", 
    "arxiv-id": "0705.4302v1", 
    "author": "Jens Oehlschl\u00e4gel", 
    "publish": "2007-05-29T21:52:17Z", 
    "summary": "Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0706.1137v1", 
    "title": "Automatically Restructuring Practice Guidelines using the GEM DTD", 
    "arxiv-id": "0706.1137v1", 
    "author": "Thierry Poibeau", 
    "publish": "2007-06-08T15:39:49Z", 
    "summary": "This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0706.1290v1", 
    "title": "Temporal Reasoning without Transitive Tables", 
    "arxiv-id": "0706.1290v1", 
    "author": "Sylviane R. Schwer", 
    "publish": "2007-06-09T06:57:05Z", 
    "summary": "Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0706.3639v1", 
    "title": "A Collection of Definitions of Intelligence", 
    "arxiv-id": "0706.3639v1", 
    "author": "Marcus Hutter", 
    "publish": "2007-06-25T13:40:56Z", 
    "summary": "This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0706.4375v1", 
    "title": "A Robust Linguistic Platform for Efficient and Domain specific Web   Content Analysis", 
    "arxiv-id": "0706.4375v1", 
    "author": "Julien Derivi\u00e8re", 
    "publish": "2007-06-29T08:58:02Z", 
    "summary": "Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0707.2506v1", 
    "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In   Decentralized Pomdps", 
    "arxiv-id": "0707.2506v1", 
    "author": "Fran\u00e7ois Charpillet", 
    "publish": "2007-07-17T12:49:30Z", 
    "summary": "We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0707.4289v1", 
    "title": "A Leaf Recognition Algorithm for Plant Classification Using   Probabilistic Neural Network", 
    "arxiv-id": "0707.4289v1", 
    "author": "Qiao-Liang Xiang", 
    "publish": "2007-07-29T12:31:40Z", 
    "summary": "In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0708.4311v1", 
    "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25   Years", 
    "arxiv-id": "0708.4311v1", 
    "author": "Juergen Schmidhuber", 
    "publish": "2007-08-31T11:12:26Z", 
    "summary": "When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0709.0522v1", 
    "title": "Qualitative Belief Conditioning Rules (QBCR)", 
    "arxiv-id": "0709.0522v1", 
    "author": "Jean Dezert", 
    "publish": "2007-09-04T20:03:04Z", 
    "summary": "In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0709.1167v2", 
    "title": "Using RDF to Model the Structure and Process of Systems", 
    "arxiv-id": "0709.1167v2", 
    "author": "Carlos Gershenson", 
    "publish": "2007-09-08T01:18:18Z", 
    "summary": "Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2008.03.029", 
    "link": "http://arxiv.org/pdf/0709.1701v1", 
    "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty", 
    "arxiv-id": "0709.1701v1", 
    "author": "Jean Dezert", 
    "publish": "2007-09-11T19:12:25Z", 
    "summary": "This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done."
},{
    "category": "cs.AI", 
    "doi": "10.2478/s13230-010-0014-0", 
    "link": "http://arxiv.org/pdf/0709.2065v1", 
    "title": "Toward Psycho-robots", 
    "arxiv-id": "0709.2065v1", 
    "author": "Andrei Khrennikov", 
    "publish": "2007-09-13T13:06:34Z", 
    "summary": "We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2007.01.001", 
    "link": "http://arxiv.org/pdf/0709.3974v1", 
    "title": "Fitness landscape of the cellular automata majority problem: View from   the Olympus", 
    "arxiv-id": "0709.3974v1", 
    "author": "Leonardo Vanneschi", 
    "publish": "2007-09-25T15:40:38Z", 
    "summary": "In this paper we study cellular automata (CAs) that perform the computational\nMajority task. This task is a good example of what the phenomenon of emergence\nin complex systems is. We take an interest in the reasons that make this\nparticular fitness landscape a difficult one. The first goal is to study the\nlandscape as such, and thus it is ideally independent from the actual\nheuristics used to search the space. However, a second goal is to understand\nthe features a good search technique for this particular problem space should\npossess. We statistically quantify in various ways the degree of difficulty of\nsearching this landscape. Due to neutrality, investigations based on sampling\ntechniques on the whole landscape are difficult to conduct. So, we go exploring\nthe landscape from the top. Although it has been proved that no CA can perform\nthe task perfectly, several efficient CAs for this task have been found.\nExploiting similarities between these CAs and symmetries in the landscape, we\ndefine the Olympus landscape which is regarded as the ''heavenly home'' of the\nbest local optima known (blok). Then we measure several properties of this\nsubspace. Although it is easier to find relevant CAs in this subspace than in\nthe overall landscape, there are structural reasons that prevent a searcher\nfrom finding overfitted CAs in the Olympus. Finally, we study dynamics and\nperformance of genetic algorithms on the Olympus in order to confirm our\nanalysis and to find efficient CAs for the Majority problem with low\ncomputational cost."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2007.01.001", 
    "link": "http://arxiv.org/pdf/0709.4010v1", 
    "title": "Local search heuristics: Fitness Cloud versus Fitness Landscape", 
    "arxiv-id": "0709.4010v1", 
    "author": "Manuel Clergue", 
    "publish": "2007-09-25T19:15:50Z", 
    "summary": "This paper introduces the concept of fitness cloud as an alternative way to\nvisualize and analyze search spaces than given by the geographic notion of\nfitness landscape. It is argued that the fitness cloud concept overcomes\nseveral deficiencies of the landscape representation. Our analysis is based on\nthe correlation between fitness of solutions and fitnesses of nearest solutions\naccording to some neighboring. We focus on the behavior of local search\nheuristics, such as hill climber, on the well-known NK fitness landscape. In\nboth cases the fitness vs. fitness correlation is shown to be related to the\nepistatic parameter K."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.tcs.2007.01.001", 
    "link": "http://arxiv.org/pdf/0709.4011v1", 
    "title": "Measuring the Evolvability Landscape to study Neutrality", 
    "arxiv-id": "0709.4011v1", 
    "author": "Manuel Clergue", 
    "publish": "2007-09-25T19:17:08Z", 
    "summary": "This theoretical work defines the measure of autocorrelation of evolvability\nin the context of neutral fitness landscape. This measure has been studied on\nthe classical MAX-SAT problem. This work highlight a new characteristic of\nneutral fitness landscapes which allows to design new adapted metaheuristic."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-76298-0_69", 
    "link": "http://arxiv.org/pdf/0709.4015v1", 
    "title": "From Texts to Structured Documents: The Case of Health Practice   Guidelines", 
    "arxiv-id": "0709.4015v1", 
    "author": "Amanda Bouffier", 
    "publish": "2007-09-25T19:26:08Z", 
    "summary": "This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present in this paper the rules developed for this task.\nWe show that the system yields good performance when applied to the analysis of\nFrench practice guidelines."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-76298-0_69", 
    "link": "http://arxiv.org/pdf/0710.0013v1", 
    "title": "Lagrangian Relaxation for MAP Estimation in Graphical Models", 
    "arxiv-id": "0710.0013v1", 
    "author": "Alan S. Willsky", 
    "publish": "2007-09-28T21:29:43Z", 
    "summary": "We develop a general framework for MAP estimation in discrete and Gaussian\ngraphical models using Lagrangian relaxation techniques. The key idea is to\nreformulate an intractable estimation problem as one defined on a more\ntractable graph, but subject to additional constraints. Relaxing these\nconstraints gives a tractable dual problem, one defined by a thin graph, which\nis then optimized by an iterative procedure. When this iterative optimization\nleads to a consistent estimate, one which also satisfies the constraints, then\nit corresponds to an optimal MAP estimate of the original model. Otherwise\nthere is a ``duality gap'', and we obtain a bound on the optimal solution.\nThus, our approach combines convex optimization with dynamic programming\ntechniques applicable for thin graphs. The popular tree-reweighted max-product\n(TRMP) method may be seen as solving a particular class of such relaxations,\nwhere the intractable graph is relaxed to a set of spanning trees. We also\nconsider relaxations to a set of small induced subgraphs, thin subgraphs (e.g.\nloops), and a connected tree obtained by ``unwinding'' cycles. In addition, we\npropose a new class of multiscale relaxations that introduce ``summary''\nvariables. The potential benefits of such generalizations include: reducing or\neliminating the ``duality gap'' in hard problems, reducing the number or\nLagrange multipliers in the dual problem, and accelerating convergence of the\niterative optimization procedure."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJSSCI.2009.024936", 
    "link": "http://arxiv.org/pdf/0710.4231v1", 
    "title": "Analyzing covert social network foundation behind terrorism disaster", 
    "arxiv-id": "0710.4231v1", 
    "author": "Yukio Ohsawa", 
    "publish": "2007-10-23T10:40:55Z", 
    "summary": "This paper addresses a method to analyze the covert social network foundation\nhidden behind the terrorism disaster. It is to solve a node discovery problem,\nwhich means to discover a node, which functions relevantly in a social network,\nbut escaped from monitoring on the presence and mutual relationship of nodes.\nThe method aims at integrating the expert investigator's prior understanding,\ninsight on the terrorists' social network nature derived from the complex graph\ntheory, and computational data processing. The social network responsible for\nthe 9/11 attack in 2001 is used to execute simulation experiment to evaluate\nthe performance of the method."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJSSCI.2009.024936", 
    "link": "http://arxiv.org/pdf/0710.4975v2", 
    "title": "Node discovery problem for a social network", 
    "arxiv-id": "0710.4975v2", 
    "author": "Yoshiharu Maeno", 
    "publish": "2007-10-26T01:32:47Z", 
    "summary": "Methods to solve a node discovery problem for a social network are presented.\nCovert nodes refer to the nodes which are not observable directly. They\ntransmit the influence and affect the resulting collaborative activities among\nthe persons in a social network, but do not appear in the surveillance logs\nwhich record the participants of the collaborative activities. Discovering the\ncovert nodes is identifying the suspicious logs where the covert nodes would\nappear if the covert nodes became overt. The performance of the methods is\ndemonstrated with a test dataset generated from computationally synthesized\nnetworks and a real organization."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJSSCI.2009.024936", 
    "link": "http://arxiv.org/pdf/0711.1466v3", 
    "title": "Predicting relevant empty spots in social interaction", 
    "arxiv-id": "0711.1466v3", 
    "author": "Yukio Ohsawa", 
    "publish": "2007-11-09T13:54:30Z", 
    "summary": "An empty spot refers to an empty hard-to-fill space which can be found in the\nrecords of the social interaction, and is the clue to the persons in the\nunderlying social network who do not appear in the records. This contribution\naddresses a problem to predict relevant empty spots in social interaction.\nHomogeneous and inhomogeneous networks are studied as a model underlying the\nsocial interaction. A heuristic predictor function approach is presented as a\nnew method to address the problem. Simulation experiment is demonstrated over a\nhomogeneous network. A test data in the form of baskets is generated from the\nsimulated communication. Precision to predict the empty spots is calculated to\ndemonstrate the performance of the presented approach."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJSSCI.2009.024936", 
    "link": "http://arxiv.org/pdf/0711.3419v1", 
    "title": "Translating OWL and Semantic Web Rules into Prolog: Moving Toward   Description Logic Programs", 
    "arxiv-id": "0711.3419v1", 
    "author": "Jason Peterson", 
    "publish": "2007-11-21T17:36:50Z", 
    "summary": "To appear in Theory and Practice of Logic Programming (TPLP), 2008.\n  We are researching the interaction between the rule and the ontology layers\nof the Semantic Web, by comparing two options: 1) using OWL and its rule\nextension SWRL to develop an integrated ontology/rule language, and 2) layering\nrules on top of an ontology with RuleML and OWL. Toward this end, we are\ndeveloping the SWORIER system, which enables efficient automated reasoning on\nontologies and rules, by translating all of them into Prolog and adding a set\nof general rules that properly capture the semantics of OWL. We have also\nenabled the user to make dynamic changes on the fly, at run time. This work\naddresses several of the concerns expressed in previous work, such as negation,\ncomplementary classes, disjunctive heads, and cardinality, and it discusses\nalternative approaches for dealing with inconsistencies in the knowledge base.\nIn addition, for efficiency, we implemented techniques called\nextensionalization, avoiding reanalysis, and code minimization."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0712.0836v1", 
    "title": "Evolving localizations in reaction-diffusion cellular automata", 
    "arxiv-id": "0712.0836v1", 
    "author": "Emmanuel Sapin", 
    "publish": "2007-12-05T22:07:04Z", 
    "summary": "We consider hexagonal cellular automata with immediate cell neighbourhood and\nthree cell-states. Every cell calculates its next state depending on the\nintegral representation of states in its neighbourhood, i.e. how many\nneighbours are in each one state. We employ evolutionary algorithms to breed\nlocal transition functions that support mobile localizations (gliders), and\ncharacterize sets of the functions selected in terms of quasi-chemical systems.\nAnalysis of the set of functions evolved allows to speculate that mobile\nlocalizations are likely to emerge in the quasi-chemical systems with limited\ndiffusion of one reagent, a small number of molecules is required for\namplification of travelling localizations, and reactions leading to stationary\nlocalizations involve relatively equal amount of quasi-chemical species.\nTechniques developed can be applied in cascading signals in nature-inspired\nspatially extended computing devices, and phenomenological studies and\nclassification of non-linear discrete systems."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0712.2389v2", 
    "title": "Decomposition During Search for Propagation-Based Constraint Solvers", 
    "arxiv-id": "0712.2389v2", 
    "author": "Sebastian Will", 
    "publish": "2007-12-14T18:08:26Z", 
    "summary": "We describe decomposition during search (DDS), an integration of And/Or tree\nsearch into propagation-based constraint solvers. The presented search\nalgorithm dynamically decomposes sub-problems of a constraint satisfaction\nproblem into independent partial problems, avoiding redundant work.\n  The paper discusses how DDS interacts with key features that make\npropagation-based solvers successful: constraint propagation, especially for\nglobal constraints, and dynamic search heuristics.\n  We have implemented DDS for the Gecode constraint programming library. Two\napplications, solution counting in graph coloring and protein structure\nprediction, exemplify the benefits of DDS in practice."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0712.3329v1", 
    "title": "Universal Intelligence: A Definition of Machine Intelligence", 
    "arxiv-id": "0712.3329v1", 
    "author": "Marcus Hutter", 
    "publish": "2007-12-20T05:50:54Z", 
    "summary": "A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this equation formally captures the concept of machine intelligence in the\nbroadest reasonable sense. We then show how this formal definition is related\nto the theory of universal optimal learning agents. Finally, we survey the many\nother tests and definitions of intelligence that have been proposed for\nmachines."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0712.3825v1", 
    "title": "Tests of Machine Intelligence", 
    "arxiv-id": "0712.3825v1", 
    "author": "Marcus Hutter", 
    "publish": "2007-12-22T01:17:24Z", 
    "summary": "Although the definition and measurement of intelligence is clearly of\nfundamental importance to the field of artificial intelligence, no general\nsurvey of definitions and tests of machine intelligence exists. Indeed few\nresearchers are even aware of alternatives to the Turing test and its many\nderivatives. In this paper we fill this gap by providing a short survey of the\nmany tests of machine intelligence that have been proposed."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0712.4318v1", 
    "title": "Convergence of Expected Utilities with Algorithmic Probability   Distributions", 
    "arxiv-id": "0712.4318v1", 
    "author": "Peter de Blanc", 
    "publish": "2007-12-28T07:50:00Z", 
    "summary": "We consider an agent interacting with an unknown environment. The environment\nis a function which maps natural numbers to natural numbers; the agent's set of\nhypotheses about the environment contains all such functions which are\ncomputable and compatible with a finite set of known input-output pairs, and\nthe agent assigns a positive probability to each such hypothesis. We do not\nrequire that this probability distribution be computable, but it must be\nbounded below by a positive computable function. The agent has a utility\nfunction on outputs from the environment. We show that if this utility function\nis bounded below in absolute value by an unbounded computable function, then\nthe expected utility of any input is undefined. This implies that a computable\nutility function will have convergent expected utilities iff that function is\nbounded."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0801.1275v1", 
    "title": "Le terme et le concept : fondements d'une ontoterminologie", 
    "arxiv-id": "0801.1275v1", 
    "author": "Christophe Roche", 
    "publish": "2008-01-08T20:12:02Z", 
    "summary": "Most definitions of ontology, viewed as a \"specification of a\nconceptualization\", agree on the fact that if an ontology can take different\nforms, it necessarily includes a vocabulary of terms and some specification of\ntheir meaning in relation to the domain's conceptualization. And as domain\nknowledge is mainly conveyed through scientific and technical texts, we can\nhope to extract some useful information from them for building ontology. But is\nit as simple as this? In this article we shall see that the lexical structure,\ni.e. the network of words linked by linguistic relationships, does not\nnecessarily match the domain conceptualization. We have to bear in mind that\nwriting documents is the concern of textual linguistics, of which one of the\nprinciples is the incompleteness of text, whereas building ontology - viewed as\ntask-independent knowledge - is concerned with conceptualization based on\nformal and not natural languages. Nevertheless, the famous Sapir and Whorf\nhypothesis, concerning the interdependence of thought and language, is also\napplicable to formal languages. This means that the way an ontology is built\nand a concept is defined depends directly on the formal language which is used;\nand the results will not be the same. The introduction of the notion of\nontoterminology allows to take into account epistemological principles for\nformal ontology building."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0129183108012376", 
    "link": "http://arxiv.org/pdf/0801.1336v1", 
    "title": "Stream Computing", 
    "arxiv-id": "0801.1336v1", 
    "author": "Subhash Kak", 
    "publish": "2008-01-09T14:59:31Z", 
    "summary": "Stream computing is the use of multiple autonomic and parallel modules\ntogether with integrative processors at a higher level of abstraction to embody\n\"intelligent\" processing. The biological basis of this computing is sketched\nand the matter of learning is examined."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1143997.1144098", 
    "link": "http://arxiv.org/pdf/0802.2429v1", 
    "title": "Anisotropic selection in cellular genetic algorithms", 
    "arxiv-id": "0802.2429v1", 
    "author": "Manuel Clergue", 
    "publish": "2008-02-18T07:30:04Z", 
    "summary": "In this paper we introduce a new selection scheme in cellular genetic\nalgorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows\naccurate control of the selective pressure. First we compare this new scheme\nwith the classical rectangular grid shapes solution according to the selective\npressure: we can obtain the same takeover time with the two techniques although\nthe spreading of the best individual is different. We then give experimental\nresults that show to what extent AS promotes the emergence of niches that\nsupport low coupling and high cohesion. Finally, using a cGA with anisotropic\nselection on a Quadratic Assignment Problem we show the existence of an\nanisotropic optimal value for which the best average performance is observed.\nFurther work will focus on the selective pressure self-adjustment ability\nprovided by this new selection scheme."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1143997.1144098", 
    "link": "http://arxiv.org/pdf/0803.1087v3", 
    "title": "The Future of Scientific Simulations: from Artificial Life to Artificial   Cosmogenesis", 
    "arxiv-id": "0803.1087v3", 
    "author": "Clement Vidal", 
    "publish": "2008-03-07T14:42:02Z", 
    "summary": "This philosophical paper explores the relation between modern scientific\nsimulations and the future of the universe. We argue that a simulation of an\nentire universe will result from future scientific activity. This requires us\nto tackle the challenge of simulating open-ended evolution at all levels in a\nsingle simulation. The simulation should encompass not only biological\nevolution, but also physical evolution (a level below) and cultural evolution\n(a level above). The simulation would allow us to probe what would happen if we\nwould \"replay the tape of the universe\" with the same or different laws and\ninitial conditions. We also distinguish between real-world and artificial-world\nmodelling. Assuming that intelligent life could indeed simulate an entire\nuniverse, this leads to two tentative hypotheses. Some authors have argued that\nwe may already be in a simulation run by an intelligent entity. Or, if such a\nsimulation could be made real, this would lead to the production of a new\nuniverse. This last direction is argued with a careful speculative\nphilosophical approach, emphasizing the imperative to find a solution to the\nheat death problem in cosmology. The reader is invited to consult Annex 1 for\nan overview of the logical structure of this paper. -- Keywords: far future,\nfuture of science, ALife, simulation, realization, cosmology, heat death,\nfine-tuning, physical eschatology, cosmological natural selection, cosmological\nartificial selection, artificial cosmogenesis, selfish biocosm hypothesis,\nmeduso-anthropic principle, developmental singularity hypothesis, role of\nintelligent life."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1143997.1144098", 
    "link": "http://arxiv.org/pdf/0803.1207v3", 
    "title": "Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*", 
    "arxiv-id": "0803.1207v3", 
    "author": "Hang Dinh", 
    "publish": "2008-03-08T02:47:27Z", 
    "summary": "This paper has been withdrawn."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1274000.1274098", 
    "link": "http://arxiv.org/pdf/0803.3192v1", 
    "title": "Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC   applied to Interactive One-Max problem", 
    "arxiv-id": "0803.3192v1", 
    "author": "Laurent Dumercy", 
    "publish": "2008-03-21T16:11:38Z", 
    "summary": "In this paper, we describe a new algorithm that consists in combining an\neye-tracker for minimizing the fatigue of a user during the evaluation process\nof Interactive Evolutionary Computation. The approach is then applied to the\nInteractive One-Max optimization problem."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICSMC.2009.5346826", 
    "link": "http://arxiv.org/pdf/0803.3363v2", 
    "title": "Node discovery in a networked organization", 
    "arxiv-id": "0803.3363v2", 
    "author": "Yoshiharu Maeno", 
    "publish": "2008-03-24T05:53:39Z", 
    "summary": "In this paper, I present a method to solve a node discovery problem in a\nnetworked organization. Covert nodes refer to the nodes which are not\nobservable directly. They affect social interactions, but do not appear in the\nsurveillance logs which record the participants of the social interactions.\nDiscovering the covert nodes is defined as identifying the suspicious logs\nwhere the covert nodes would appear if the covert nodes became overt. A\nmathematical model is developed for the maximal likelihood estimation of the\nnetwork behind the social interactions and for the identification of the\nsuspicious logs. Precision, recall, and F measure characteristics are\ndemonstrated with the dataset generated from a real organization and the\ncomputationally synthesized datasets. The performance is close to the\ntheoretical limit for any covert nodes in the networks of any topologies and\nsizes if the ratio of the number of observation to the number of possible\ncommunication patterns is large."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICSMC.2009.5346826", 
    "link": "http://arxiv.org/pdf/0803.3501v1", 
    "title": "Multiagent Approach for the Representation of Information in a Decision   Support System", 
    "arxiv-id": "0803.3501v1", 
    "author": "Fr\u00e9d\u00e9ric Serin", 
    "publish": "2008-03-25T07:43:36Z", 
    "summary": "In an emergency situation, the actors need an assistance allowing them to\nreact swiftly and efficiently. In this prospect, we present in this paper a\ndecision support system that aims to prepare actors in a crisis situation\nthanks to a decision-making support. The global architecture of this system is\npresented in the first part. Then we focus on a part of this system which is\ndesigned to represent the information of the current situation. This part is\ncomposed of a multiagent system that is made of factual agents. Each agent\ncarries a semantic feature and aims to represent a partial part of a situation.\nThe agents develop thanks to their interactions by comparing their semantic\nfeatures using proximity measures and according to specific ontologies."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJAIP.2010.030531", 
    "link": "http://arxiv.org/pdf/0803.4074v2", 
    "title": "Reflective visualization and verbalization of unconscious preference", 
    "arxiv-id": "0803.4074v2", 
    "author": "Yukio Ohsawa", 
    "publish": "2008-03-28T09:36:58Z", 
    "summary": "A new method is presented, that can help a person become aware of his or her\nunconscious preferences, and convey them to others in the form of verbal\nexplanation. The method combines the concepts of reflection, visualization, and\nverbalization. The method was tested in an experiment where the unconscious\npreferences of the subjects for various artworks were investigated. In the\nexperiment, two lessons were learned. The first is that it helps the subjects\nbecome aware of their unconscious preferences to verbalize weak preferences as\ncompared with strong preferences through discussion over preference diagrams.\nThe second is that it is effective to introduce an adjustable factor into\nvisualization to adapt to the differences in the subjects and to foster their\nmutual understanding."
},{
    "category": "cs.AI", 
    "doi": "10.1504/IJAIP.2010.030531", 
    "link": "http://arxiv.org/pdf/0804.0528v1", 
    "title": "Application of Rough Set Theory to Analysis of Hydrocyclone Operation", 
    "arxiv-id": "0804.0528v1", 
    "author": "M. Irannajad", 
    "publish": "2008-04-03T11:47:55Z", 
    "summary": "This paper describes application of rough set theory, on the analysis of\nhydrocyclone operation. In this manner, using Self Organizing Map (SOM) as\npreprocessing step, best crisp granules of data are obtained. Then, using a\ncombining of SOM and rough set theory (RST)-called SORST-, the dominant rules\non the information table, obtained from laboratory tests, are extracted. Based\non these rules, an approximate estimation on decision attribute is fulfilled.\nFinally, a brief comparison of this method with the SOM-NFIS system (briefly\nSONFIS) is highlighted."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-988-98671-5-7,978-988-98671-2-6", 
    "link": "http://arxiv.org/pdf/0804.0558v1", 
    "title": "Agent-Based Perception of an Environment in an Emergency Situation", 
    "arxiv-id": "0804.0558v1", 
    "author": "Cyrille Bertelle", 
    "publish": "2008-04-03T13:45:43Z", 
    "summary": "We are interested in the problem of multiagent systems development for risk\ndetecting and emergency response in an uncertain and partially perceived\nenvironment. The evaluation of the current situation passes by three stages\ninside the multiagent system. In a first time, the situation is represented in\na dynamic way. The second step, consists to characterise the situation and\nfinally, it is compared with other similar known situations. In this paper, we\npresent an information modelling of an observed environment, that we have\napplied on the RoboCupRescue Simulation System. Information coming from the\nenvironment are formatted according to a taxonomy and using semantic features.\nThe latter are defined thanks to a fine ontology of the domain and are managed\nby factual agents that aim to represent dynamically the current situation."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-988-98671-5-7,978-988-98671-2-6", 
    "link": "http://arxiv.org/pdf/0804.0852v1", 
    "title": "On the Influence of Selection Operators on Performances in Cellular   Genetic Algorithms", 
    "arxiv-id": "0804.0852v1", 
    "author": "Manuel Clergue", 
    "publish": "2008-04-05T13:27:51Z", 
    "summary": "In this paper, we study the influence of the selective pressure on the\nperformance of cellular genetic algorithms. Cellular genetic algorithms are\ngenetic algorithms where the population is embedded on a toroidal grid. This\nstructure makes the propagation of the best so far individual slow down, and\nallows to keep in the population potentially good solutions. We present two\nselective pressure reducing strategies in order to slow down even more the best\nsolution propagation. We experiment these strategies on a hard optimization\nproblem, the quadratic assignment problem, and we show that there is a value\nfor of the control parameter for both which gives the best performance. This\noptimal value does not find explanation on only the selective pressure,\nmeasured either by take over time and diversity evolution. This study makes us\nconclude that we need other tools than the sole selective pressure measures to\nexplain the performances of cellular genetic algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00357-008-9007-7", 
    "link": "http://arxiv.org/pdf/0804.1244v1", 
    "title": "Geometric Data Analysis, From Correspondence Analysis to Structured Data   Analysis (book review)", 
    "arxiv-id": "0804.1244v1", 
    "author": "Fionn Murtagh", 
    "publish": "2008-04-08T11:31:55Z", 
    "summary": "Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From\nCorrespondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,\nxi+475 pp."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.0459v1", 
    "title": "Phase transition in SONFIS&SORST", 
    "arxiv-id": "0805.0459v1", 
    "author": "Hamed Owladeghaffari", 
    "publish": "2008-05-05T04:19:50Z", 
    "summary": "In this study, we introduce general frame of MAny Connected Intelligent\nParticles Systems (MACIPS). Connections and interconnections between particles\nget a complex behavior of such merely simple system (system in\nsystem).Contribution of natural computing, under information granulation\ntheory, are the main topics of this spacious skeleton. Upon this clue, we\norganize two algorithms involved a few prominent intelligent computing and\napproximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy\nInference System and Rough Set Theory (RST). Over this, we show how our\nalgorithms can be taken as a linkage of government-society interaction, where\ngovernment catches various fashions of behavior: solid (absolute) or flexible.\nSo, transition of such society, by changing of connectivity parameters (noise)\nfrom order to disorder is inferred. Add to this, one may find an indirect\nmapping among financial systems and eventual market fluctuations with MACIPS.\nKeywords: phase transition, SONFIS, SORST, many connected intelligent particles\nsystem, society-government interaction"
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.1096v1", 
    "title": "Adaptive Affinity Propagation Clustering", 
    "arxiv-id": "0805.1096v1", 
    "author": "Tao Guo", 
    "publish": "2008-05-08T04:20:20Z", 
    "summary": "Affinity propagation clustering (AP) has two limitations: it is hard to know\nwhat value of parameter 'preference' can yield an optimal clustering solution,\nand oscillations cannot be eliminated automatically if occur. The adaptive AP\nmethod is proposed to overcome these limitations, including adaptive scanning\nof preferences to search space of the number of clusters for finding the\noptimal clustering solution, adaptive adjustment of damping factors to\neliminate oscillations, and adaptive escaping from oscillations when the\ndamping adjustment technique fails. Experimental results on simulated and real\ndata sets show that the adaptive AP is effective and can outperform AP in\nquality of clustering results."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.1288v1", 
    "title": "Assessment of effective parameters on dilution using approximate   reasoning methods in longwall mining method, Iran coal mines", 
    "arxiv-id": "0805.1288v1", 
    "author": "G. H. R. Saeedi", 
    "publish": "2008-05-09T07:08:57Z", 
    "summary": "Approximately more than 90% of all coal production in Iranian underground\nmines is derived directly longwall mining method. Out of seam dilution is one\nof the essential problems in these mines. Therefore the dilution can impose the\nadditional cost of mining and milling. As a result, recognition of the\neffective parameters on the dilution has a remarkable role in industry. In this\nway, this paper has analyzed the influence of 13 parameters (attributed\nvariables) versus the decision attribute (dilution value), so that using two\napproximate reasoning methods, namely Rough Set Theory (RST) and Self\nOrganizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our\ncollected data sets has been extracted. The other benefit of later methods is\nto predict new unknown cases. So, the reduced sets (reducts) by RST have been\nobtained. Therefore the emerged results by utilizing mentioned methods shows\nthat the high sensitive variables are thickness of layer, length of stope, rate\nof advance, number of miners, type of advancing."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.2308v1", 
    "title": "Toward Fuzzy block theory", 
    "arxiv-id": "0805.2308v1", 
    "author": "H. Owladeghaffari", 
    "publish": "2008-05-15T13:43:26Z", 
    "summary": "This study, fundamentals of fuzzy block theory, and its application in\nassessment of stability in underground openings, has surveyed. Using fuzzy\ntopics and inserting them in to key block theory, in two ways, fundamentals of\nfuzzy block theory has been presented. In indirect combining, by coupling of\nadaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could\nextract possible damage parts around a tunnel. In direct solution, some\nprinciples of block theory, by means of different fuzzy facets theory, were\nrewritten."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.2440v1", 
    "title": "Analysis of hydrocyclone performance based on information granulation   theory", 
    "arxiv-id": "0805.2440v1", 
    "author": "Mehdi Irannajad", 
    "publish": "2008-05-16T05:11:24Z", 
    "summary": "This paper describes application of information granulation theory, on the\nanalysis of hydrocyclone perforamance. In this manner, using a combining of\nSelf Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and\nfuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules\nand sub fuzzy granules, within non fuzzy information (initial granulation), is\nrendered in an open-close iteration. Using two criteria, \"simplicity of rules\n\"and \"adaptive threoshold error level\", stability of algorithm is guaranteed.\nValidation of the proposed method, on the data set of the hydrocyclone is\nrendered."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.3518v1", 
    "title": "Logic programming with social features", 
    "arxiv-id": "0805.3518v1", 
    "author": "Gianluca Caminiti", 
    "publish": "2008-05-22T18:17:58Z", 
    "summary": "In everyday life it happens that a person has to reason about what other\npeople think and how they behave, in order to achieve his goals. In other\nwords, an individual may be required to adapt his behaviour by reasoning about\nthe others' mental state. In this paper we focus on a knowledge representation\nlanguage derived from logic programming which both supports the representation\nof mental states of individual communities and provides each with the\ncapability of reasoning about others' mental states and acting accordingly. The\nproposed semantics is shown to be translatable into stable model semantics of\nlogic programs with aggregates."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88425-5_35", 
    "link": "http://arxiv.org/pdf/0805.3747v1", 
    "title": "Constructing Folksonomies from User-specified Relations on Flickr", 
    "arxiv-id": "0805.3747v1", 
    "author": "Kristina Lerman", 
    "publish": "2008-05-24T07:02:24Z", 
    "summary": "Many social Web sites allow users to publish content and annotate with\ndescriptive metadata. In addition to flat tags, some social Web sites have\nrecently began to allow users to organize their content and metadata\nhierarchically. The social photosharing site Flickr, for example, allows users\nto group related photos in sets, and related sets in collections. The social\nbookmarking site Del.icio.us similarly lets users group related tags into\nbundles. Although the sites themselves don't impose any constraints on how\nthese hierarchies are used, individuals generally use them to capture\nrelationships between concepts, most commonly the broader/narrower relations.\nCollective annotation of content with hierarchical relations may lead to an\nemergent classification system, called a folksonomy. While some researchers\nhave explored using tags as evidence for learning folksonomies, we believe that\nhierarchical relations described above offer a high-quality source of evidence\nfor this task.\n  We propose a simple approach to aggregate shallow hierarchies created by many\ndistinct Flickr users into a common folksonomy. Our approach uses statistics to\ndetermine if a particular relation should be retained or discarded. The\nrelations are then woven together into larger hierarchies. Although we have not\ncarried out a detailed quantitative evaluation of the approach, it looks very\npromising since it generates very reasonable, non-trivial hierarchies."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patcog.2008.05.026", 
    "link": "http://arxiv.org/pdf/0805.3799v1", 
    "title": "The Structure of Narrative: the Case of Film Scripts", 
    "arxiv-id": "0805.3799v1", 
    "author": "Stewart McKie", 
    "publish": "2008-05-24T23:35:15Z", 
    "summary": "We analyze the style and structure of story narrative using the case of film\nscripts. The practical importance of this is noted, especially the need to have\nsupport tools for television movie writing. We use the Casablanca film script,\nand scripts from six episodes of CSI (Crime Scene Investigation). For analysis\nof style and structure, we quantify various central perspectives discussed in\nMcKee's book, \"Story: Substance, Structure, Style, and the Principles of\nScreenwriting\". Film scripts offer a useful point of departure for exploration\nof the analysis of more general narratives. Our methodology, using\nCorrespondence Analysis, and hierarchical clustering, is innovative in a range\nof areas that we discuss. In particular this work is groundbreaking in taking\nthe qualitative analysis of McKee and grounding this analysis in a quantitative\nand algorithmic framework."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patcog.2008.05.026", 
    "link": "http://arxiv.org/pdf/0805.3802v1", 
    "title": "Feature Selection for Bayesian Evaluation of Trauma Death Risk", 
    "arxiv-id": "0805.3802v1", 
    "author": "V. Schetinin", 
    "publish": "2008-05-25T00:06:29Z", 
    "summary": "In the last year more than 70,000 people have been brought to the UK\nhospitals with serious injuries. Each time a clinician has to urgently take a\npatient through a screening procedure to make a reliable decision on the trauma\ntreatment. Typically, such procedure comprises around 20 tests; however the\ncondition of a trauma patient remains very difficult to be tested properly.\nWhat happens if these tests are ambiguously interpreted, and information about\nthe severity of the injury will come misleading? The mistake in a decision can\nbe fatal: using a mild treatment can put a patient at risk of dying from\nposttraumatic shock, while using an overtreatment can also cause death. How can\nwe reduce the risk of the death caused by unreliable decisions? It has been\nshown that probabilistic reasoning, based on the Bayesian methodology of\naveraging over decision models, allows clinicians to evaluate the uncertainty\nin decision making. Based on this methodology, in this paper we aim at\nselecting the most important screening tests, keeping a high performance. We\nassume that the probabilistic reasoning within the Bayesian methodology allows\nus to discover new relationships between the screening tests and uncertainty in\ndecisions. In practice, selection of the most informative tests can also reduce\nthe cost of a screening procedure in trauma care centers. In our experiments we\nuse the UK Trauma data to compare the efficiency of the proposed technique in\nterms of the performance. We also compare the uncertainty in decisions in terms\nof entropy."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patcog.2008.05.026", 
    "link": "http://arxiv.org/pdf/0805.3935v1", 
    "title": "Fusion for Evaluation of Image Classification in Uncertain Environments", 
    "arxiv-id": "0805.3935v1", 
    "author": "Arnaud Martin", 
    "publish": "2008-05-26T11:50:25Z", 
    "summary": "We present in this article a new evaluation method for classification and\nsegmentation of textured images in uncertain environments. In uncertain\nenvironments, real classes and boundaries are known with only a partial\ncertainty given by the experts. Most of the time, in many presented papers,\nonly classification or only segmentation are considered and evaluated. Here, we\npropose to take into account both the classification and segmentation results\naccording to the certainty given by the experts. We present the results of this\nmethod on a fusion of classifiers of sonar images for a seabed\ncharacterization."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.patcog.2008.05.026", 
    "link": "http://arxiv.org/pdf/0805.3972v2", 
    "title": "Intuitive visualization of the intelligence for the run-down of   terrorist wire-pullers", 
    "arxiv-id": "0805.3972v2", 
    "author": "Yukio Ohsawa", 
    "publish": "2008-05-26T14:28:28Z", 
    "summary": "The investigation of the terrorist attack is a time-critical task. The\ninvestigators have a limited time window to diagnose the organizational\nbackground of the terrorists, to run down and arrest the wire-pullers, and to\ntake an action to prevent or eradicate the terrorist attack. The intuitive\ninterface to visualize the intelligence data set stimulates the investigators'\nexperience and knowledge, and aids them in decision-making for an immediately\neffective action. This paper presents a computational method to analyze the\nintelligence data set on the collective actions of the perpetrators of the\nattack, and to visualize it into the form of a social network diagram which\npredicts the positions where the wire-pullers conceals themselves."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0805.4560v1", 
    "title": "Rock mechanics modeling based on soft granulation theory", 
    "arxiv-id": "0805.4560v1", 
    "author": "H. Owladeghaffari", 
    "publish": "2008-05-29T14:39:57Z", 
    "summary": "This paper describes application of information granulation theory, on the\ndesign of rock engineering flowcharts. Firstly, an overall flowchart, based on\ninformation granulation theory has been highlighted. Information granulation\ntheory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering\nexperiences (especially in fuzzy shape-incomplete information or superfluous),\nor engineering judgments, in each step of designing procedure, while the\nsuitable instruments modeling are employed. In this manner and to extension of\nsoft modeling instruments, using three combinations of Self Organizing Map\n(SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp\nand fuzzy granules, from monitored data sets are obtained. The main underlined\ncore of our algorithms are balancing of crisp(rough or non-fuzzy) granules and\nsub fuzzy granules, within non fuzzy information (initial granulation) upon the\nopen-close iterations. Using different criteria on balancing best granules\n(information pockets), are obtained. Validations of our proposed methods, on\nthe data set of in-situ permeability in rock masses in Shivashan dam, Iran have\nbeen highlighted."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.0526v1", 
    "title": "An Ontology-based Knowledge Management System for Industry Clusters", 
    "arxiv-id": "0806.0526v1", 
    "author": "Abdelaziz Bouras", 
    "publish": "2008-06-03T12:38:50Z", 
    "summary": "Knowledge-based economy forces companies in the nation to group together as a\ncluster in order to maintain their competitiveness in the world market. The\ncluster development relies on two key success factors which are knowledge\nsharing and collaboration between the actors in the cluster. Thus, our study\ntries to propose knowledge management system to support knowledge management\nactivities within the cluster. To achieve the objectives of this study,\nontology takes a very important role in knowledge management process in various\nways; such as building reusable and faster knowledge-bases, better way for\nrepresenting the knowledge explicitly. However, creating and representing\nontology create difficulties to organization due to the ambiguity and\nunstructured of source of knowledge. Therefore, the objectives of this paper\nare to propose the methodology to create and represent ontology for the\norganization development by using knowledge engineering approach. The\nhandicraft cluster in Thailand is used as a case study to illustrate our\nproposed methodology."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.1280v1", 
    "title": "The Role of Artificial Intelligence Technologies in Crisis Response", 
    "arxiv-id": "0806.1280v1", 
    "author": "Abdel-Badeeh M. Salem", 
    "publish": "2008-06-07T12:46:56Z", 
    "summary": "Crisis response poses many of the most difficult information technology in\ncrisis management. It requires information and communication-intensive efforts,\nutilized for reducing uncertainty, calculating and comparing costs and\nbenefits, and managing resources in a fashion beyond those regularly available\nto handle routine problems. In this paper, we explore the benefits of\nartificial intelligence technologies in crisis response. This paper discusses\nthe role of artificial intelligence technologies; namely, robotics, ontology\nand semantic web, and multi-agent systems in crisis response."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.1640v1", 
    "title": "Toward a combination rule to deal with partial conflict and specificity   in belief functions theory", 
    "arxiv-id": "0806.1640v1", 
    "author": "Christophe Osswald", 
    "publish": "2008-06-10T12:03:43Z", 
    "summary": "We present and discuss a mixed conjunctive and disjunctive rule, a\ngeneralization of conflict repartition rules, and a combination of these two\nrules. In the belief functions theory one of the major problem is the conflict\nrepartition enlightened by the famous Zadeh's example. To date, many\ncombination rules have been proposed in order to solve a solution to this\nproblem. Moreover, it can be important to consider the specificity of the\nresponses of the experts. Since few year some unification rules are proposed.\nWe have shown in our previous works the interest of the proportional conflict\nredistribution rule. We propose here a mixed combination rule following the\nproportional conflict redistribution rule modified by a discounting procedure.\nThis rule generalizes many combination rules."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.1797v1", 
    "title": "A new generalization of the proportional conflict redistribution rule   stable in terms of decision", 
    "arxiv-id": "0806.1797v1", 
    "author": "Christophe Osswald", 
    "publish": "2008-06-11T07:05:48Z", 
    "summary": "In this chapter, we present and discuss a new generalized proportional\nconflict redistribution rule. The Dezert-Smarandache extension of the\nDemster-Shafer theory has relaunched the studies on the combination rules\nespecially for the management of the conflict. Many combination rules have been\nproposed in the last few years. We study here different combination rules and\ncompare them in terms of decision on didactic example and on generated data.\nIndeed, in real applications, we need a reliable decision and it is the final\nresults that matter. This chapter shows that a fine proportional conflict\nredistribution rule must be preferred for the combination in the belief\nfunction theory."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.1802v1", 
    "title": "Une nouvelle r\u00e8gle de combinaison r\u00e9partissant le conflit -   Applications en imagerie Sonar et classification de cibles Radar", 
    "arxiv-id": "0806.1802v1", 
    "author": "Christophe Osswald", 
    "publish": "2008-06-11T07:31:36Z", 
    "summary": "These last years, there were many studies on the problem of the conflict\ncoming from information combination, especially in evidence theory. We can\nsummarise the solutions for manage the conflict into three different\napproaches: first, we can try to suppress or reduce the conflict before the\ncombination step, secondly, we can manage the conflict in order to give no\ninfluence of the conflict in the combination step, and then take into account\nthe conflict in the decision step, thirdly, we can take into account the\nconflict in the combination step. The first approach is certainly the better,\nbut not always feasible. It is difficult to say which approach is the best\nbetween the second and the third. However, the most important is the produced\nresults in applications. We propose here a new combination rule that\ndistributes the conflict proportionally on the element given this conflict. We\ncompare these different combination rules on real data in Sonar imagery and\nRadar target classification."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.1806v1", 
    "title": "Perfect Derived Propagators", 
    "arxiv-id": "0806.1806v1", 
    "author": "Guido Tack", 
    "publish": "2008-06-11T08:03:35Z", 
    "summary": "When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear equations both with and without coefficients? Constraint\nvariants are ubiquitous: implementing them requires considerable (if not\nprohibitive) effort and decreases maintainability, but will deliver better\nperformance.\n  This paper shows how to use variable views, previously introduced for an\nimplementation architecture, to derive perfect propagator variants. A model for\nviews and derived propagators is introduced. Derived propagators are proved to\nbe indeed perfect in that they inherit essential properties such as correctness\nand domain and bounds consistency. Techniques for systematically deriving\npropagators such as transformation, generalization, specialization, and\nchanneling are developed for several variable domains. We evaluate the massive\nimpact of derived propagators. Without derived propagators, Gecode would\nrequire 140000 rather than 40000 lines of code for propagators."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.2140v1", 
    "title": "Defaults and Normality in Causal Structures", 
    "arxiv-id": "0806.2140v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2008-06-12T19:27:57Z", 
    "summary": "A serious defect with the Halpern-Pearl (HP) definition of causality is\nrepaired by combining a theory of causality with a theory of defaults. In\naddition, it is shown that (despite a claim to the contrary) a cause according\nto the HP condition need not be a single conjunct. A definition of causality\nmotivated by Wright's NESS test is shown to always hold for a single conjunct.\nMoreover, conditions that hold for all the examples considered by HP are given\nthat guarantee that causality according to (this version) of the NESS test is\nequivalent to the HP definition."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0806.4511v5", 
    "title": "The model of quantum evolution", 
    "arxiv-id": "0806.4511v5", 
    "author": "Konstantin P. Wishnevsky", 
    "publish": "2008-06-27T12:59:45Z", 
    "summary": "This paper has been withdrawn by the author due to extremely unscientific\nerrors."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.ijrmms.2008.09.001", 
    "link": "http://arxiv.org/pdf/0807.0627v1", 
    "title": "Belief decision support and reject for textured images characterization", 
    "arxiv-id": "0807.0627v1", 
    "author": "Arnaud Martin", 
    "publish": "2008-07-03T19:46:21Z", 
    "summary": "The textured images' classification assumes to consider the images in terms\nof area with the same texture. In uncertain environment, it could be better to\ntake an imprecise decision or to reject the area corresponding to an unlearning\nclass. Moreover, on the areas that are the classification units, we can have\nmore than one texture. These considerations allows us to develop a belief\ndecision model permitting to reject an area as unlearning and to decide on\nunions and intersections of learning classes. The proposed approach finds all\nits justification in an application of seabed characterization from sonar\nimages, which contributes to an illustration."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.0908v2", 
    "title": "The Correspondence Analysis Platform for Uncovering Deep Structure in   Data and Information", 
    "arxiv-id": "0807.0908v2", 
    "author": "Fionn Murtagh", 
    "publish": "2008-07-06T15:22:54Z", 
    "summary": "We study two aspects of information semantics: (i) the collection of all\nrelationships, (ii) tracking and spotting anomaly and change. The first is\nimplemented by endowing all relevant information spaces with a Euclidean metric\nin a common projected space. The second is modelled by an induced ultrametric.\nA very general way to achieve a Euclidean embedding of different information\nspaces based on cross-tabulation counts (and from other input data formats) is\nprovided by Correspondence Analysis. From there, the induced ultrametric that\nwe are particularly interested in takes a sequential - e.g. temporal - ordering\nof the data into account. We employ such a perspective to look at narrative,\n\"the flow of thought and the flow of language\" (Chafe). In application to\npolicy decision making, we show how we can focus analysis in a small number of\ndimensions."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.1906v3", 
    "title": "Extension of Inagaki General Weighted Operators and A New Fusion Rule   Class of Proportional Redistribution of Intersection Masses", 
    "arxiv-id": "0807.1906v3", 
    "author": "Florentin Smarandache", 
    "publish": "2008-07-11T18:30:10Z", 
    "summary": "In this paper we extend Inagaki Weighted Operators fusion rule (WO) in\ninformation fusion by doing redistribution of not only the conflicting mass,\nbut also of masses of non-empty intersections, that we call Double Weighted\nOperators (DWO). Then we propose a new fusion rule Class of Proportional\nRedistribution of Intersection Masses (CPRIM), which generates many interesting\nparticular fusion rules in information fusion. Both formulas are presented for\nany number of sources of information. An application and comparison with other\nfusion rules are given in the last section."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.3483v1", 
    "title": "Implementing general belief function framework with a practical   codification for low complexity", 
    "arxiv-id": "0807.3483v1", 
    "author": "Arnaud Martin", 
    "publish": "2008-07-22T13:50:22Z", 
    "summary": "In this chapter, we propose a new practical codification of the elements of\nthe Venn diagram in order to easily manipulate the focal elements. In order to\nreduce the complexity, the eventual constraints must be integrated in the\ncodification at the beginning. Hence, we only consider a reduced hyper power\nset $D_r^\\Theta$ that can be $2^\\Theta$ or $D^\\Theta$. We describe all the\nsteps of a general belief function framework. The step of decision is\nparticularly studied, indeed, when we can decide on intersections of the\nsingletons of the discernment space no actual decision functions are easily to\nuse. Hence, two approaches are proposed, an extension of previous one and an\napproach based on the specificity of the elements on which to decide. The\nprincipal goal of this chapter is to provide practical codes of a general\nbelief function framework for the researchers and users needing the belief\nfunction theory."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.3669v1", 
    "title": "A new probabilistic transformation of belief mass assignment", 
    "arxiv-id": "0807.3669v1", 
    "author": "Florentin Smarandache", 
    "publish": "2008-07-23T13:49:30Z", 
    "summary": "In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a\nnew probabilistic transformation, called DSmP, in order to build a subjective\nprobability measure from any basic belief assignment defined on any model of\nthe frame of discernment. Several examples are given to show how the DSmP\ntransformation works and we compare it to main existing transformations\nproposed in the literature so far. We show the advantages of DSmP over\nclassical transformations in term of Probabilistic Information Content (PIC).\nThe direct extension of this transformation for dealing with qualitative belief\nassignments is also presented."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.4417v2", 
    "title": "On Introspection, Metacognitive Control and Augmented Data Mining Live   Cycles", 
    "arxiv-id": "0807.4417v2", 
    "author": "Daniel Sonntag", 
    "publish": "2008-07-28T12:05:16Z", 
    "summary": "We discuss metacognitive modelling as an enhancement to cognitive modelling\nand computing. Metacognitive control mechanisms should enable AI systems to\nself-reflect, reason about their actions, and to adapt to new situations. In\nthis respect, we propose implementation details of a knowledge taxonomy and an\naugmented data mining life cycle which supports a live integration of obtained\nmodels."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0807.4680v3", 
    "title": "Hacia una teoria de unificacion para los comportamientos cognitivos", 
    "arxiv-id": "0807.4680v3", 
    "author": "Sergio Miguel", 
    "publish": "2008-07-29T15:11:12Z", 
    "summary": "Each cognitive science tries to understand a set of cognitive behaviors. The\nstructuring of knowledge of this nature's aspect is far from what it can be\nexpected about a science. Until now universal standard consistently describing\nthe set of cognitive behaviors has not been found, and there are many questions\nabout the cognitive behaviors for which only there are opinions of members of\nthe scientific community. This article has three proposals. The first proposal\nis to raise to the scientific community the necessity of unified the cognitive\nbehaviors. The second proposal is claim the application of the Newton's\nreasoning rules about nature of his book, Philosophiae Naturalis Principia\nMathematica, to the cognitive behaviors. The third is to propose a scientific\ntheory, currently developing, that follows the rules established by Newton to\nmake sense of nature, and could be the theory to explain all the cognitive\nbehaviors."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0808.1125v1", 
    "title": "Verified Null-Move Pruning", 
    "arxiv-id": "0808.1125v1", 
    "author": "Nathan S. Netanyahu", 
    "publish": "2008-08-08T12:44:10Z", 
    "summary": "In this article we review standard null-move pruning and introduce our\nextended version of it, which we call verified null-move pruning. In verified\nnull-move pruning, whenever the shallow null-move search indicates a fail-high,\ninstead of cutting off the search from the current node, the search is\ncontinued with reduced depth.\n  Our experiments with verified null-move pruning show that on average, it\nconstructs a smaller search tree with greater tactical strength in comparison\nto standard null-move pruning. Moreover, unlike standard null-move pruning,\nwhich fails badly in zugzwang positions, verified null-move pruning manages to\ndetect most zugzwangs and in such cases conducts a re-search to obtain the\ncorrect result. In addition, verified null-move pruning is very easy to\nimplement, and any standard null-move pruning program can use verified\nnull-move pruning by modifying only a few lines of code."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0808.3109v3", 
    "title": "n-ary Fuzzy Logic and Neutrosophic Logic Operators", 
    "arxiv-id": "0808.3109v3", 
    "author": "V. Christianto", 
    "publish": "2008-08-22T16:10:36Z", 
    "summary": "We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and\nneutrosophic logic binary operators. Then we generalize them to n-ary fuzzy\nlogic and neutrosophic logic operators using the smarandache codification of\nthe Venn diagram and a defined vector neutrosophic law. In such way, new\noperators in neutrosophic logic/set/probability are built."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0271v1", 
    "title": "Randomised Variable Neighbourhood Search for Multi Objective   Optimisation", 
    "arxiv-id": "0809.0271v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-01T15:32:49Z", 
    "summary": "Various local search approaches have recently been applied to machine\nscheduling problems under multiple objectives. Their foremost consideration is\nthe identification of the set of Pareto optimal alternatives. An important\naspect of successfully solving these problems lies in the definition of an\nappropriate neighbourhood structure. Unclear in this context remains, how\ninterdependencies within the fitness landscape affect the resolution of the\nproblem.\n  The paper presents a study of neighbourhood search operators for multiple\nobjective flow shop scheduling. Experiments have been carried out with twelve\ndifferent combinations of criteria. To derive exact conclusions, small problem\ninstances, for which the optimal solutions are known, have been chosen.\nStatistical tests show that no single neighbourhood operator is able to equally\nidentify all Pareto optimal alternatives. Significant improvements however have\nbeen obtained by hybridising the solution algorithm using a randomised variable\nneighbourhood search technique."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0406v1", 
    "title": "Foundations of the Pareto Iterated Local Search Metaheuristic", 
    "arxiv-id": "0809.0406v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-02T11:29:45Z", 
    "summary": "The paper describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives. While the obtained results are encouraging in terms of their\nquality, another positive attribute of the approach is its' simplicity as it\ndoes require the setting of only very few parameters. The implementation of the\nPareto Iterated Local Search metaheuristic is based on the MOOPPS computer\nsystem of local search heuristics for multi-objective scheduling which has been\nawarded the European Academic Software Award 2002 in Ronneby, Sweden\n(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)"
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0410v1", 
    "title": "A Computational Study of Genetic Crossover Operators for Multi-Objective   Vehicle Routing Problem with Soft Time Windows", 
    "arxiv-id": "0809.0410v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-02T11:39:52Z", 
    "summary": "The article describes an investigation of the effectiveness of genetic\nalgorithms for multi-objective combinatorial optimization (MOCO) by presenting\nan application for the vehicle routing problem with soft time windows. The work\nis motivated by the question, if and how the problem structure influences the\neffectiveness of different configurations of the genetic algorithm.\nComputational results are presented for different classes of vehicle routing\nproblems, varying in their coverage with time windows, time window size,\ndistribution and number of customers. The results are compared with a simple,\nbut effective local search approach for multi-objective combinatorial\noptimization problems."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0416v1", 
    "title": "Genetic Algorithms for multiple objective vehicle routing", 
    "arxiv-id": "0809.0416v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-02T12:08:56Z", 
    "summary": "The talk describes a general approach of a genetic algorithm for multiple\nobjective optimization problems. A particular dominance relation between the\nindividuals of the population is used to define a fitness operator, enabling\nthe genetic algorithm to adress even problems with efficient, but\nconvex-dominated alternatives. The algorithm is implemented in a multilingual\ncomputer program, solving vehicle routing problems with time windows under\nmultiple objectives. The graphical user interface of the program shows the\nprogress of the genetic algorithm and the main parameters of the approach can\nbe easily modified. In addition to that, the program provides powerful decision\nsupport to the decision maker. The software has proved it's excellence at the\nfinals of the European Academic Software Award EASA, held at the Keble college/\nUniversity of Oxford/ Great Britain."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0610v1", 
    "title": "A framework for the interactive resolution of multi-objective vehicle   routing problems", 
    "arxiv-id": "0809.0610v1", 
    "author": "Wolf Wenger", 
    "publish": "2008-09-03T12:22:08Z", 
    "summary": "The article presents a framework for the resolution of rich vehicle routing\nproblems which are difficult to address with standard optimization techniques.\nWe use local search on the basis on variable neighborhood search for the\nconstruction of the solutions, but embed the techniques in a flexible framework\nthat allows the consideration of complex side constraints of the problem such\nas time windows, multiple depots, heterogeneous fleets, and, in particular,\nmultiple optimization criteria. In order to identify a compromise alternative\nthat meets the requirements of the decision maker, an interactive procedure is\nintegrated in the resolution of the problem, allowing the modification of the\npreference information articulated by the decision maker. The framework is\nprototypically implemented in a computer system. First results of test runs on\nmultiple depot vehicle routing problems with time windows are reported."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0662v1", 
    "title": "Improving Local Search for Fuzzy Scheduling Problems", 
    "arxiv-id": "0809.0662v1", 
    "author": "Sanja Petrovic", 
    "publish": "2008-09-03T16:16:43Z", 
    "summary": "The integration of fuzzy set theory and fuzzy logic into scheduling is a\nrather new aspect with growing importance for manufacturing applications,\nresulting in various unsolved aspects. In the current paper, we investigate an\nimproved local search technique for fuzzy scheduling problems with fitness\nplateaus, using a multi criteria formulation of the problem. We especially\naddress the problem of changing job priorities over time as studied at the\nSherwood Press Ltd, a Nottingham based printing company, who is a collaborator\non the project."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0755v1", 
    "title": "Bin Packing Under Multiple Objectives - a Heuristic Approximation   Approach", 
    "arxiv-id": "0809.0755v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-04T07:02:29Z", 
    "summary": "The article proposes a heuristic approximation approach to the bin packing\nproblem under multiple objectives. In addition to the traditional objective of\nminimizing the number of bins, the heterogeneousness of the elements in each\nbin is minimized, leading to a biobjective formulation of the problem with a\ntradeoff between the number of bins and their heterogeneousness. An extension\nof the Best-Fit approximation algorithm is presented to solve the problem.\nExperimental investigations have been carried out on benchmark instances of\ndifferent size, ranging from 100 to 1000 items. Encouraging results have been\nobtained, showing the applicability of the heuristic approach to the described\nproblem."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.0757v1", 
    "title": "An application of the Threshold Accepting metaheuristic for curriculum   based course timetabling", 
    "arxiv-id": "0809.0757v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2008-09-04T07:12:02Z", 
    "summary": "The article presents a local search approach for the solution of timetabling\nproblems in general, with a particular implementation for competition track 3\nof the International Timetabling Competition 2007 (ITC 2007). The heuristic\nsearch procedure is based on Threshold Accepting to overcome local optima. A\nstochastic neighborhood is proposed and implemented, randomly removing and\nreassigning events from the current solution.\n  The overall concept has been incrementally obtained from a series of\nexperiments, which we describe in each (sub)section of the paper. In result, we\nsuccessfully derived a potential candidate solution approach for the finals of\ntrack 3 of the ITC 2007."
},{
    "category": "cs.AI", 
    "doi": "10.1093/comjnl/bxn045", 
    "link": "http://arxiv.org/pdf/0809.1077v1", 
    "title": "Variable Neighborhood Search for the University Lecturer-Student   Assignment Problem", 
    "arxiv-id": "0809.1077v1", 
    "author": "Wolf Wenger", 
    "publish": "2008-09-05T17:02:55Z", 
    "summary": "The paper presents a study of local search heuristics in general and variable\nneighborhood search in particular for the resolution of an assignment problem\nstudied in the practical work of universities. Here, students have to be\nassigned to scientific topics which are proposed and supported by members of\nstaff. The problem involves the optimization under given preferences of\nstudents which may be expressed when applying for certain topics.\n  It is possible to observe that variable neighborhood search leads to superior\nresults for the tested problem instances. One instance is taken from an actual\ncase, while others have been generated based on the real world data to support\nthe analysis with a deeper analysis.\n  An extension of the problem has been formulated by integrating a second\nobjective function that simultaneously balances the workload of the members of\nstaff while maximizing utility of the students. The algorithmic approach has\nbeen prototypically implemented in a computer system. One important aspect in\nthis context is the application of the research work to problems of other\nscientific institutions, and therefore the provision of decision support\nfunctionalities."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0809.3204v1", 
    "title": "Extended ASP tableaux and rule redundancy in normal logic programs", 
    "arxiv-id": "0809.3204v1", 
    "author": "Emilia Oikarinen", 
    "publish": "2008-09-18T16:35:20Z", 
    "summary": "We introduce an extended tableau calculus for answer set programming (ASP).\nThe proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP\n2006], with an added extension rule. We investigate the power of Extended ASP\nTableaux both theoretically and empirically. We study the relationship of\nExtended ASP Tableaux with the Extended Resolution proof system defined by\nTseitin for sets of clauses, and separate Extended ASP Tableaux from ASP\nTableaux by giving a polynomial-length proof for a family of normal logic\nprograms P_n for which ASP Tableaux has exponential-length minimal proofs with\nrespect to n. Additionally, Extended ASP Tableaux imply interesting insight\ninto the effect of program simplification on the lengths of proofs in ASP.\nClosely related to Extended ASP Tableaux, we empirically investigate the effect\nof redundant rules on the efficiency of ASP solving.\n  To appear in Theory and Practice of Logic Programming (TPLP)."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0809.4582v1", 
    "title": "Achieving compositionality of the stable model semantics for Smodels   programs", 
    "arxiv-id": "0809.4582v1", 
    "author": "Tomi Janhunen", 
    "publish": "2008-09-26T10:32:32Z", 
    "summary": "In this paper, a Gaifman-Shapiro-style module architecture is tailored to the\ncase of Smodels programs under the stable model semantics. The composition of\nSmodels program modules is suitably limited by module conditions which ensure\nthe compatibility of the module system with stable models. Hence the semantics\nof an entire Smodels program depends directly on stable models assigned to its\nmodules. This result is formalized as a module theorem which truly strengthens\nLifschitz and Turner's splitting-set theorem for the class of Smodels programs.\nTo streamline generalizations in the future, the module theorem is first proved\nfor normal programs and then extended to cover Smodels programs using a\ntranslation from the latter class of programs to the former class. Moreover,\nthe respective notion of module-level equivalence, namely modular equivalence,\nis shown to be a proper congruence relation: it is preserved under\nsubstitutions of modules that are modularly equivalent. Principles for program\ndecomposition are also addressed. The strongly connected components of the\nrespective dependency graph can be exploited in order to extract a module\nstructure when there is no explicit a priori knowledge about the modules of a\nprogram. The paper includes a practical demonstration of tools that have been\ndeveloped for automated (de)composition of Smodels programs.\n  To appear in Theory and Practice of Logic Programming."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0810.0139v1", 
    "title": "Determining the Unithood of Word Sequences using a Probabilistic   Approach", 
    "arxiv-id": "0810.0139v1", 
    "author": "Mohammed Bennamoun", 
    "publish": "2008-10-01T12:49:24Z", 
    "summary": "Most research related to unithood were conducted as part of a larger effort\nfor the determination of termhood. Consequently, novelties are rare in this\nsmall sub-field of term extraction. In addition, existing work were mostly\nempirically motivated and derived. We propose a new probabilistically-derived\nmeasure, independent of any influences of termhood, that provides dedicated\nmeasures to gather linguistic evidence from parsed text and statistical\nevidence from Google search engine for the measurement of unithood. Our\ncomparative study using 1,825 test cases against an existing\nempirically-derived function revealed an improvement in terms of precision,\nrecall and accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0810.0156v1", 
    "title": "Determining the Unithood of Word Sequences using Mutual Information and   Independence Measure", 
    "arxiv-id": "0810.0156v1", 
    "author": "Mohammed Bennamoun", 
    "publish": "2008-10-01T13:00:19Z", 
    "summary": "Most works related to unithood were conducted as part of a larger effort for\nthe determination of termhood. Consequently, the number of independent research\nthat study the notion of unithood and produce dedicated techniques for\nmeasuring unithood is extremely small. We propose a new approach, independent\nof any influences of termhood, that provides dedicated measures to gather\nlinguistic evidence from parsed text and statistical evidence from Google\nsearch engine for the measurement of unithood. Our evaluations revealed a\nprecision and recall of 98.68% and 91.82% respectively with an accuracy at\n95.42% in measuring the unithood of 1005 test cases."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0810.0332v1", 
    "title": "Enhanced Integrated Scoring for Cleaning Dirty Texts", 
    "arxiv-id": "0810.0332v1", 
    "author": "Mohammed Bennamoun", 
    "publish": "2008-10-02T03:42:12Z", 
    "summary": "An increasing number of approaches for ontology engineering from text are\ngearing towards the use of online sources such as company intranet and the\nWorld Wide Web. Despite such rise, not much work can be found in aspects of\npreprocessing and cleaning dirty texts from online sources. This paper presents\nan enhancement of an Integrated Scoring for Spelling error correction,\nAbbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as\npart of a text preprocessing phase in an ontology engineering system. New\nevaluations performed on the enhanced ISSAC using 700 chat records reveal an\nimproved accuracy of 98% as compared to 96.5% and 71% based on the use of only\nbasic ISSAC and of Aspell, respectively."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068408003578", 
    "link": "http://arxiv.org/pdf/0810.1186v2", 
    "title": "On-the-fly Macros", 
    "arxiv-id": "0810.1186v2", 
    "author": "Omer Gimenez", 
    "publish": "2008-10-07T13:10:26Z", 
    "summary": "We present a domain-independent algorithm that computes macros in a novel\nway. Our algorithm computes macros \"on-the-fly\" for a given set of states and\ndoes not require previously learned or inferred information, nor prior domain\nknowledge. The algorithm is used to define new domain-independent tractable\nclasses of classical planning that are proved to include \\emph{Blocksworld-arm}\nand \\emph{Towers of Hanoi}."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CANS.2008.8", 
    "link": "http://arxiv.org/pdf/0810.2046v1", 
    "title": "Modeling of Social Transitions Using Intelligent Systems", 
    "arxiv-id": "0810.2046v1", 
    "author": "Mostafa Sharifzadeh", 
    "publish": "2008-10-11T19:09:22Z", 
    "summary": "In this study, we reproduce two new hybrid intelligent systems, involve three\nprominent intelligent computing and approximate reasoning methods: Self\nOrganizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory\n(RST),called SONFIS and SORST. We show how our algorithms can be construed as a\nlinkage of government-society interactions, where government catches various\nstates of behaviors: solid (absolute) or flexible. So, transition of society,\nby changing of connectivity parameters (noise) from order to disorder is\ninferred."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CANS.2008.8", 
    "link": "http://arxiv.org/pdf/0810.3865v1", 
    "title": "Relationship between Diversity and Perfomance of Multiple Classifiers   for Decision Support", 
    "arxiv-id": "0810.3865v1", 
    "author": "T. Marwala", 
    "publish": "2008-10-21T15:42:16Z", 
    "summary": "The paper presents the investigation and implementation of the relationship\nbetween diversity and the performance of multiple classifiers on classification\naccuracy. The study is critical as to build classifiers that are strong and can\ngeneralize better. The parameters of the neural network within the committee\nwere varied to induce diversity; hence structural diversity is the focus for\nthis study. The hidden nodes and the activation function are the parameters\nthat were varied. The diversity measures that were adopted from ecology such as\nShannon and Simpson were used to quantify diversity. Genetic algorithm is used\nto find the optimal ensemble by using the accuracy as the cost function. The\nresults observed shows that there is a relationship between structural\ndiversity and accuracy. It is observed that the classification accuracy of an\nensemble increases as the diversity increases. There was an increase of 3%-6%\nin the classification accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.1109/CANS.2008.8", 
    "link": "http://arxiv.org/pdf/0811.0131v1", 
    "title": "Balancing Exploration and Exploitation by an Elitist Ant System with   Exponential Pheromone Deposition Rule", 
    "arxiv-id": "0811.0131v1", 
    "author": "Amit Konar", 
    "publish": "2008-11-02T06:07:34Z", 
    "summary": "The paper presents an exponential pheromone deposition rule to modify the\nbasic ant system algorithm which employs constant deposition rule. A stability\nanalysis using differential equation is carried out to find out the values of\nparameters that make the ant system dynamics stable for both kinds of\ndeposition rule. A roadmap of connected cities is chosen as the problem\nenvironment where the shortest route between two given cities is required to be\ndiscovered. Simulations performed with both forms of deposition approach using\nElitist Ant System model reveal that the exponential deposition approach\noutperforms the classical one by a large extent. Exhaustive experiments are\nalso carried out to find out the optimum setting of different controlling\nparameters for exponential deposition approach and an empirical relationship\nbetween the major controlling parameters of the algorithm and some features of\nproblem environment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICIAFS.2008.4783925", 
    "link": "http://arxiv.org/pdf/0811.0134v1", 
    "title": "A Novel Parser Design Algorithm Based on Artificial Ants", 
    "arxiv-id": "0811.0134v1", 
    "author": "Janarthanan Ramadoss", 
    "publish": "2008-11-02T06:22:55Z", 
    "summary": "This article presents a unique design for a parser using the Ant Colony\nOptimization algorithm. The paper implements the intuitive thought process of\nhuman mind through the activities of artificial ants. The scheme presented here\nuses a bottom-up approach and the parsing program can directly use ambiguous or\nredundant grammars. We allocate a node corresponding to each production rule\npresent in the given grammar. Each node is connected to all other nodes\n(representing other production rules), thereby establishing a completely\nconnected graph susceptible to the movement of artificial ants. Each ant tries\nto modify this sentential form by the production rule present in the node and\nupgrades its position until the sentential form reduces to the start symbol S.\nSuccessful ants deposit pheromone on the links that they have traversed\nthrough. Eventually, the optimum path is discovered by the links carrying\nmaximum amount of pheromone concentration. The design is simple, versatile,\nrobust and effective and obviates the calculation of the above mentioned sets\nand precedence relation tables. Further advantages of our scheme lie in i)\nascertaining whether a given string belongs to the language represented by the\ngrammar, and ii) finding out the shortest possible path from the given string\nto the start symbol S in case multiple routes exist."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.0136v1", 
    "title": "Extension of Max-Min Ant System with Exponential Pheromone Deposition   Rule", 
    "arxiv-id": "0811.0136v1", 
    "author": "Amit Konar", 
    "publish": "2008-11-02T06:28:50Z", 
    "summary": "The paper presents an exponential pheromone deposition approach to improve\nthe performance of classical Ant System algorithm which employs uniform\ndeposition rule. A simplified analysis using differential equations is carried\nout to study the stability of basic ant system dynamics with both exponential\nand constant deposition rules. A roadmap of connected cities, where the\nshortest path between two specified cities are to be found out, is taken as a\nplatform to compare Max-Min Ant System model (an improved and popular model of\nAnt System algorithm) with exponential and constant deposition rules. Extensive\nsimulations are performed to find the best parameter settings for non-uniform\ndeposition approach and experiments with these parameter settings revealed that\nthe above approach outstripped the traditional one by a large extent in terms\nof both solution quality and convergence time."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.0340v1", 
    "title": "Document stream clustering: experimenting an incremental algorithm and   AR-based tools for highlighting dynamic trends", 
    "arxiv-id": "0811.0340v1", 
    "author": "Pascal Cuxac", 
    "publish": "2008-11-03T16:56:51Z", 
    "summary": "We address here two major challenges presented by dynamic data mining: 1) the\nstability challenge: we have implemented a rigorous incremental density-based\nclustering algorithm, independent from any initial conditions and ordering of\nthe data-vectors stream, 2) the cognitive challenge: we have implemented a\nstringent selection process of association rules between clusters at time t-1\nand time t for directly generating the main conclusions about the dynamics of a\ndata-stream. We illustrate these points with an application to a two years and\n2600 documents scientific information database."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.0602v1", 
    "title": "Classification dynamique d'un flux documentaire : une \u00e9valuation   statique pr\u00e9alable de l'algorithme GERMEN", 
    "arxiv-id": "0811.0602v1", 
    "author": "Joel Johansson", 
    "publish": "2008-11-04T20:42:52Z", 
    "summary": "Data-stream clustering is an ever-expanding subdomain of knowledge\nextraction. Most of the past and present research effort aims at efficient\nscaling up for the huge data repositories. Our approach focuses on qualitative\nimprovement, mainly for \"weak signals\" detection and precise tracking of\ntopical evolutions in the framework of information watch - though scalability\nis intrinsically guaranteed in a possibly distributed implementation. Our\nGERMEN algorithm exhaustively picks up the whole set of density peaks of the\ndata at time t, by identifying the local perturbations induced by the current\ndocument vector, such as changing cluster borders, or new/vanishing clusters.\nOptimality yields from the uniqueness 1) of the density landscape for any value\nof our zoom parameter, 2) of the cluster allocation operated by our border\npropagation rule. This results in a rigorous independence from the data\npresentation ranking or any initialization parameter. We present here as a\nfirst step the only assessment of a static view resulting from one year of the\nCNRS/INIST Pascal database in the field of geotechnics."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.0942v1", 
    "title": "\u00c9tude longitudinale d'une proc\u00e9dure de mod\u00e9lisation de   connaissances en mati\u00e8re de gestion du territoire agricole", 
    "arxiv-id": "0811.0942v1", 
    "author": "Christian Brassac", 
    "publish": "2008-11-06T13:35:12Z", 
    "summary": "This paper gives an introduction to this issue, and presents the framework\nand the main steps of the Rosa project. Four teams of researchers, agronomists,\ncomputer scientists, psychologists and linguists were involved during five\nyears within this project that aimed at the development of a knowledge based\nsystem. The purpose of the Rosa system is the modelling and the comparison of\nfarm spatial organizations. It relies on a formalization of agronomical\nknowledge and thus induces a joint knowledge building process involving both\nthe agronomists and the computer scientists. The paper describes the steps of\nthe modelling process as well as the filming procedures set up by the\npsychologists and linguists in order to make explicit and to analyze the\nunderlying knowledge building process."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.1319v2", 
    "title": "Modeling Social Annotation: a Bayesian Approach", 
    "arxiv-id": "0811.1319v2", 
    "author": "Kristina Lerman", 
    "publish": "2008-11-09T05:49:53Z", 
    "summary": "Collaborative tagging systems, such as Delicious, CiteULike, and others,\nallow users to annotate resources, e.g., Web pages or scientific papers, with\ndescriptive labels called tags. The social annotations contributed by thousands\nof users, can potentially be used to infer categorical knowledge, classify\ndocuments or recommend new relevant information. Traditional text inference\nmethods do not make best use of social annotation, since they do not take into\naccount variations in individual users' perspectives and vocabulary. In a\nprevious work, we introduced a simple probabilistic model that takes interests\nof individual annotators into account in order to find hidden topics of\nannotated resources. Unfortunately, that approach had one major shortcoming:\nthe number of topics and interests must be specified a priori. To address this\ndrawback, we extend the model to a fully Bayesian framework, which offers a way\nto automatically estimate these numbers. In particular, the model allows the\nnumber of interests and topics to change as suggested by the structure of the\ndata. We evaluate the proposed model in detail on the synthetic and real-world\ndata by comparing its performance to Latent Dirichlet Allocation on the topic\nextraction task. For the latter evaluation, we apply the model to infer topics\nof Web resources from social annotations obtained from Delicious in order to\ndiscover new resources similar to a specified one. Our empirical results\ndemonstrate that the proposed model is a promising method for exploiting social\nknowledge contained in user-generated annotations."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.1618v1", 
    "title": "Airport Gate Assignment: New Model and Implementation", 
    "arxiv-id": "0811.1618v1", 
    "author": "Chendong Li", 
    "publish": "2008-11-11T02:33:30Z", 
    "summary": "Airport gate assignment is of great importance in airport operations. In this\npaper, we study the Airport Gate Assignment Problem (AGAP), propose a new model\nand implement the model with Optimization Programming language (OPL). With the\nobjective to minimize the number of conflicts of any two adjacent aircrafts\nassigned to the same gate, we build a mathematical model with logical\nconstraints and the binary constraints, which can provide an efficient\nevaluation criterion for the Airlines to estimate the current gate assignment.\nTo illustrate the feasibility of the model we construct experiments with the\ndata obtained from Continental Airlines, Houston Gorge Bush Intercontinental\nAirport IAH, which indicate that our model is both energetic and effective.\nMoreover, we interpret experimental results, which further demonstrate that our\nproposed model can provide a powerful tool for airline companies to estimate\nthe efficiency of their current work of gate assignment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0811.1711v1", 
    "title": "Artificial Intelligence Techniques for Steam Generator Modelling", 
    "arxiv-id": "0811.1711v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2008-11-11T14:09:36Z", 
    "summary": "This paper investigates the use of different Artificial Intelligence methods\nto predict the values of several continuous variables from a Steam Generator.\nThe objective was to determine how the different artificial intelligence\nmethods performed in making predictions on the given dataset. The artificial\nintelligence methods evaluated were Neural Networks, Support Vector Machines,\nand Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks\ninvestigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian\nand committee techniques were applied to these neural networks. Each of the AI\nmethods considered was simulated in Matlab. The results of the simulations\nshowed that all the AI methods were capable of predicting the Steam Generator\ndata reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system\nout performed the other methods in terms of accuracy and ease of\nimplementation, while still achieving a fast execution time as well as a\nreasonable training time."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.0885v3", 
    "title": "Elementary epistemological features of machine intelligence", 
    "arxiv-id": "0812.0885v3", 
    "author": "Marko Horvat", 
    "publish": "2008-12-04T09:25:37Z", 
    "summary": "Theoretical analysis of machine intelligence (MI) is useful for defining a\ncommon platform in both theoretical and applied artificial intelligence (AI).\nThe goal of this paper is to set canonical definitions that can assist\npragmatic research in both strong and weak AI. Described epistemological\nfeatures of machine intelligence include relationship between intelligent\nbehavior, intelligent and unintelligent machine characteristics, observable and\nunobservable entities and classification of intelligence. The paper also\nestablishes algebraic definitions of efficiency and accuracy of MI tests as\ntheir quality measure. The last part of the paper addresses the learning\nprocess with respect to the traditional epistemology and the epistemology of MI\ndescribed here. The proposed views on MI positively correlate to the Hegelian\nmonistic epistemology and contribute towards amalgamating idealistic\ndeliberations with the AI theory, particularly in a local frame of reference."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.1462v1", 
    "title": "Logic programs with propositional connectives and aggregates", 
    "arxiv-id": "0812.1462v1", 
    "author": "Paolo Ferraris", 
    "publish": "2008-12-08T11:09:14Z", 
    "summary": "Answer set programming (ASP) is a logic programming paradigm that can be used\nto solve complex combinatorial search problems. Aggregates are an ASP construct\nthat plays an important role in many applications. Defining a satisfactory\nsemantics of aggregates turned out to be a difficult problem, and in this paper\nwe propose a new approach, based on an analogy between aggregates and\npropositional connectives. First, we extend the definition of an answer\nset/stable model to cover arbitrary propositional theories; then we define\naggregates on top of them both as primitive constructs and as abbreviations for\nformulas. Our definition of an aggregate combines expressiveness and\nsimplicity, and it inherits many theorems about programs with nested\nexpressions, such as theorems about strong equivalence and splitting."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.1843v1", 
    "title": "Identification of parameters underlying emotions and a classification of   emotions", 
    "arxiv-id": "0812.1843v1", 
    "author": "N. Arvind Kumar", 
    "publish": "2008-12-10T19:02:31Z", 
    "summary": "The standard classification of emotions involves categorizing the expression\nof emotions. In this paper, parameters underlying some emotions are identified\nand a new classification based on these parameters is suggested."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.2785v1", 
    "title": "Prediction of Platinum Prices Using Dynamically Weighted Mixture of   Experts", 
    "arxiv-id": "0812.2785v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2008-12-15T12:35:42Z", 
    "summary": "Neural networks are powerful tools for classification and regression in\nstatic environments. This paper describes a technique for creating an ensemble\nof neural networks that adapts dynamically to changing conditions. The model\nseparates the input space into four regions and each network is given a weight\nin each region based on its performance on samples from that region. The\nensemble adapts dynamically by constantly adjusting these weights based on the\ncurrent performance of the networks. The data set used is a collection of\nfinancial indicators with the goal of predicting the future platinum price. An\nensemble with no weightings does not improve on the naive estimate of no weekly\nchange; our weighting algorithm gives an average percentage error of 63% for\ntwenty weeks of prediction."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.2991v1", 
    "title": "Analyse et structuration automatique des guides de bonnes pratiques   cliniques : essai d'\u00e9valuation", 
    "arxiv-id": "0812.2991v1", 
    "author": "Catherine Duclos", 
    "publish": "2008-12-16T07:49:20Z", 
    "summary": "Health Practice Guideliens are supposed to unify practices and propose\nrecommendations to physicians. This paper describes GemFrame, a system capable\nof semi-automatically filling an XML template from free texts in the clinical\ndomain. The XML template includes semantic information not explicitly encoded\nin the text (pairs of conditions and ac-tions/recommendations). Therefore,\nthere is a need to compute the exact scope of condi-tions over text sequences\nexpressing the re-quired actions. We present a system developped for this task.\nWe show that it yields good performance when applied to the analysis of French\npractice guidelines. We conclude with a precise evaluation of the tool."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0812.3478v1", 
    "title": "Automatic Construction of Lightweight Domain Ontologies for Chemical   Engineering Risk Management", 
    "arxiv-id": "0812.3478v1", 
    "author": "Moses Tade", 
    "publish": "2008-12-18T08:58:52Z", 
    "summary": "The need for domain ontologies in mission critical applications such as risk\nmanagement and hazard identification is becoming more and more pressing. Most\nresearch on ontology learning conducted in the academia remains unrealistic for\nreal-world applications. One of the main problems is the dependence on\nnon-incremental, rare knowledge and textual resources, and manually-crafted\npatterns and rules. This paper reports work in progress aiming to address such\nundesirable dependencies during ontology construction. Initial experiments\nusing a working prototype of the system revealed promising potentials in\nautomatically constructing high-quality domain ontologies using real-world\ntexts."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0901.0786v3", 
    "title": "Approximate inference on planar graphs using Loop Calculus and Belief   Propagation", 
    "arxiv-id": "0901.0786v3", 
    "author": "M. Chertkov", 
    "publish": "2009-01-07T09:21:47Z", 
    "summary": "We introduce novel results for approximate inference on planar graphical\nmodels using the loop calculus framework. The loop calculus (Chertkov and\nChernyak, 2006) allows to express the exact partition function of a graphical\nmodel as a finite sum of terms that can be evaluated once the belief\npropagation (BP) solution is known. In general, full summation over all\ncorrection terms is intractable. We develop an algorithm for the approach\npresented in (Certkov et al., 2008) which represents an efficient truncation\nscheme on planar graphs and a new representation of the series in terms of\nPfaffians of matrices. We analyze the performance of the algorithm for the\npartition function approximation for models with binary variables and pairwise\ninteractions on grids and other planar graphs. We study in detail both the loop\nseries and the equivalent Pfaffian series and show that the first term of the\nPfaffian series for the general, intractable planar model, can provide very\naccurate approximations. The algorithm outperforms previous truncation schemes\nof the loop series and is competitive with other state-of-the-art methods for\napproximate inference."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ADCOM.2008.4760419", 
    "link": "http://arxiv.org/pdf/0901.1289v1", 
    "title": "N-norm and N-conorm in Neutrosophic Logic and Set, and the Neutrosophic   Topologies", 
    "arxiv-id": "0901.1289v1", 
    "author": "Florentin Smarandache", 
    "publish": "2009-01-09T17:58:39Z", 
    "summary": "In this paper we present the N-norms/N-conorms in neutrosophic logic and set\nas extensions of T-norms/T-conorms in fuzzy logic and set. Also, as an\nextension of the Intuitionistic Fuzzy Topology we present the Neutrosophic\nTopologies."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1143997.1144091", 
    "link": "http://arxiv.org/pdf/0901.3769v1", 
    "title": "Deceptiveness and Neutrality - the ND family of fitness landscapes", 
    "arxiv-id": "0901.3769v1", 
    "author": "Cathy Escazut", 
    "publish": "2009-01-23T20:15:22Z", 
    "summary": "When a considerable number of mutations have no effects on fitness values,\nthe fitness landscape is said neutral. In order to study the interplay between\nneutrality, which exists in many real-world applications, and performances of\nmetaheuristics, it is useful to design landscapes which make it possible to\ntune precisely neutral degree distribution. Even though many neutral landscape\nmodels have already been designed, none of them are general enough to create\nlandscapes with specific neutral degree distributions. We propose three steps\nto design such landscapes: first using an algorithm we construct a landscape\nwhose distribution roughly fits the target one, then we use a simulated\nannealing heuristic to bring closer the two distributions and finally we affect\nfitness values to each neutral network. Then using this new family of fitness\nlandscapes we are able to highlight the interplay between deceptiveness and\nneutrality."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1143997.1144091", 
    "link": "http://arxiv.org/pdf/0901.4004v1", 
    "title": "Mining for adverse drug events with formal concept analysis", 
    "arxiv-id": "0901.4004v1", 
    "author": "C\u00e9dric Bousquet", 
    "publish": "2009-01-26T13:29:40Z", 
    "summary": "The pharmacovigilance databases consist of several case reports involving\ndrugs and adverse events (AEs). Some methods are applied consistently to\nhighlight all signals, i.e. all statistically significant associations between\na drug and an AE. These methods are appropriate for verification of more\ncomplex relationships involving one or several drug(s) and AE(s) (e.g;\nsyndromes or interactions) but do not address the identification of them. We\npropose a method for the extraction of these relationships based on Formal\nConcept Analysis (FCA) associated with disproportionality measures. This method\nidentifies all sets of drugs and AEs which are potential signals, syndromes or\ninteractions. Compared to a previous experience of disproportionality analysis\nwithout FCA, the addition of FCA was more efficient for identifying false\npositives related to concomitant drugs."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0901.4761v1", 
    "title": "A Knowledge Discovery Framework for Learning Task Models from User   Interactions in Intelligent Tutoring Systems", 
    "arxiv-id": "0901.4761v1", 
    "author": "E. Mephu Nguifo", 
    "publish": "2009-01-29T19:58:09Z", 
    "summary": "Domain experts should provide relevant domain knowledge to an Intelligent\nTutoring System (ITS) so that it can guide a learner during problemsolving\nlearning activities. However, for many ill-defined domains, the domain\nknowledge is hard to define explicitly. In previous works, we showed how\nsequential pattern mining can be used to extract a partial problem space from\nlogged user interactions, and how it can support tutoring services during\nproblem-solving exercises. This article describes an extension of this approach\nto extract a problem space that is richer and more adapted for supporting\ntutoring services. We combined sequential pattern mining with (1) dimensional\npattern mining (2) time intervals, (3) the automatic clustering of valued\nactions and (4) closed sequences mining. Some tutoring services have been\nimplemented and an experiment has been conducted in a tutoring system."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0901.4963v1", 
    "title": "How Emotional Mechanism Helps Episodic Learning in a Cognitive Agent", 
    "arxiv-id": "0901.4963v1", 
    "author": "Andre Mayers", 
    "publish": "2009-01-30T19:36:18Z", 
    "summary": "In this paper we propose the CTS (Concious Tutoring System) technology, a\nbiologically plausible cognitive agent based on human brain functions.This\nagent is capable of learning and remembering events and any related information\nsuch as corresponding procedures, stimuli and their emotional valences. Our\nproposed episodic memory and episodic learning mechanism are closer to the\ncurrent multiple-trace theory in neuroscience, because they are inspired by it\n[5] contrary to other mechanisms that are incorporated in cognitive agents.\nThis is because in our model emotions play a role in the encoding and\nremembering of events. This allows the agent to improve its behavior by\nremembering previously selected behaviors which are influenced by its emotional\nmechanism. Moreover, the architecture incorporates a realistic memory\nconsolidation process based on a data mining algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.0798v1", 
    "title": "Alleviating Media Bias Through Intelligent Agent Blogging", 
    "arxiv-id": "0902.0798v1", 
    "author": "Ernesto Diaz-Aviles", 
    "publish": "2009-02-04T21:25:59Z", 
    "summary": "Consumers of mass media must have a comprehensive, balanced and plural\nselection of news to get an unbiased perspective; but achieving this goal can\nbe very challenging, laborious and time consuming. News stories development\nover time, its (in)consistency, and different level of coverage across the\nmedia outlets are challenges that a conscientious reader has to overcome in\norder to alleviate bias.\n  In this paper we present an intelligent agent framework currently\nfacilitating analysis of the main sources of on-line news in El Salvador. We\nshow how prior tools of text analysis and Web 2.0 technologies can be combined\nwith minimal manual intervention to help individuals on their rational decision\nprocess, while holding media outlets accountable for their work."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.0899v1", 
    "title": "Comparative concept similarity over Minspaces: Axiomatisation and   Tableaux Calculus", 
    "arxiv-id": "0902.0899v1", 
    "author": "Camilla Schwind", 
    "publish": "2009-02-05T12:55:42Z", 
    "summary": "We study the logic of comparative concept similarity $\\CSL$ introduced by\nSheremet, Tishkovsky, Wolter and Zakharyaschev to capture a form of qualitative\nsimilarity comparison. In this logic we can formulate assertions of the form \"\nobjects A are more similar to B than to C\". The semantics of this logic is\ndefined by structures equipped by distance functions evaluating the similarity\ndegree of objects. We consider here the particular case of the semantics\ninduced by \\emph{minspaces}, the latter being distance spaces where the minimum\nof a set of distances always exists. It turns out that the semantics over\narbitrary minspaces can be equivalently specified in terms of preferential\nstructures, typical of conditional logics. We first give a direct\naxiomatisation of this logic over Minspaces. We next define a decision\nprocedure in the form of a tableaux calculus. Both the calculus and the\naxiomatisation take advantage of the reformulation of the semantics in terms of\npreferential structures."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.1080v1", 
    "title": "A Model for Managing Collections of Patterns", 
    "arxiv-id": "0902.1080v1", 
    "author": "Fran\u00e7ois Jacquenet", 
    "publish": "2009-02-06T12:50:12Z", 
    "summary": "Data mining algorithms are now able to efficiently deal with huge amount of\ndata. Various kinds of patterns may be discovered and may have some great\nimpact on the general development of knowledge. In many domains, end users may\nwant to have their data mined by data mining tools in order to extract patterns\nthat could impact their business. Nevertheless, those users are often\noverwhelmed by the large quantity of patterns extracted in such a situation.\nMoreover, some privacy issues, or some commercial one may lead the users not to\nbe able to mine the data by themselves. Thus, the users may not have the\npossibility to perform many experiments integrating various constraints in\norder to focus on specific patterns they would like to extract. Post processing\nof patterns may be an answer to that drawback. Thus, in this paper we present a\nframework that could allow end users to manage collections of patterns. We\npropose to use an efficient data structure on which some algebraic operators\nmay be used in order to retrieve or access patterns in pattern bases."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.2206v5", 
    "title": "Feature Hashing for Large Scale Multitask Learning", 
    "arxiv-id": "0902.2206v5", 
    "author": "Alex Smola", 
    "publish": "2009-02-12T20:06:36Z", 
    "summary": "Empirical evidence suggests that hashing is an effective strategy for\ndimensionality reduction and practical nonparametric estimation. In this paper\nwe provide exponential tail bounds for feature hashing and show that the\ninteraction between random subspaces is negligible with high probability. We\ndemonstrate the feasibility of this approach with experimental results for a\nnew use case -- multitask learning with hundreds of thousands of tasks."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.2362v1", 
    "title": "XML Representation of Constraint Networks: Format XCSP 2.1", 
    "arxiv-id": "0902.2362v1", 
    "author": "Christophe Lecoutre", 
    "publish": "2009-02-13T18:24:27Z", 
    "summary": "We propose a new extended format to represent constraint networks using XML.\nThis format allows us to represent constraints defined either in extension or\nin intension. It also allows us to reference global constraints. Any instance\nof the problems CSP (Constraint Satisfaction Problem), QCSP (Quantified CSP)\nand WCSP (Weighted CSP) can be represented using this format."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0902.2871v1", 
    "title": "The Semantics of Kalah Game", 
    "arxiv-id": "0902.2871v1", 
    "author": "Kaninda Musumbu", 
    "publish": "2009-02-17T09:08:35Z", 
    "summary": "The present work consisted in developing a plateau game. There are the\ntraditional ones (monopoly, cluedo, ect.) but those which interest us leave\nless place at the chance (luck) than to the strategy such that the chess game.\nKallah is an old African game, its rules are simple but the strategies to be\nused are very complex to implement. Of course, they are based on a strongly\nmathematical basis as in the film \"Rain-Man\" where one can see that gambling\ncan be payed with strategies based on mathematical theories. The Artificial\nIntelligence gives the possibility \"of thinking\" to a machine and, therefore,\nallows it to make decisions. In our work, we use it to give the means to the\ncomputer choosing its best movement."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0041v1", 
    "title": "Learning DTW Global Constraint for Time Series Classification", 
    "arxiv-id": "0903.0041v1", 
    "author": "Chotirat Ann Ratanamahatana", 
    "publish": "2009-02-28T05:46:31Z", 
    "summary": "1-Nearest Neighbor with the Dynamic Time Warping (DTW) distance is one of the\nmost effective classifiers on time series domain. Since the global constraint\nhas been introduced in speech community, many global constraint models have\nbeen proposed including Sakoe-Chiba (S-C) band, Itakura Parallelogram, and\nRatanamahatana-Keogh (R-K) band. The R-K band is a general global constraint\nmodel that can represent any global constraints with arbitrary shape and size\neffectively. However, we need a good learning algorithm to discover the most\nsuitable set of R-K bands, and the current R-K band learning algorithm still\nsuffers from an 'overfitting' phenomenon. In this paper, we propose two new\nlearning algorithms, i.e., band boundary extraction algorithm and iterative\nlearning algorithm. The band boundary extraction is calculated from the bound\nof all possible warping paths in each class, and the iterative learning is\nadjusted from the original R-K band learning. We also use a Silhouette index, a\nwell-known clustering validation technique, as a heuristic function, and the\nlower bound function, LB_Keogh, to enhance the prediction speed. Twenty\ndatasets, from the Workshop and Challenge on Time Series Classification, held\nin conjunction of the SIGKDD 2007, are used to evaluate our approach."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0211v1", 
    "title": "Range and Roots: Two Common Patterns for Specifying and Propagating   Counting and Occurrence Constraints", 
    "arxiv-id": "0903.0211v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-02T05:58:11Z", 
    "summary": "We propose Range and Roots which are two common patterns useful for\nspecifying a wide range of counting and occurrence constraints. We design\nspecialised propagation algorithms for these two patterns. Counting and\noccurrence constraints specified using these patterns thus directly inherit a\npropagation algorithm. To illustrate the capabilities of the Range and Roots\nconstraints, we specify a number of global constraints taken from the\nliterature. Preliminary experiments demonstrate that propagating counting and\noccurrence constraints using these two patterns leads to a small loss in\nperformance when compared to specialised global constraints and is competitive\nwith alternative decompositions using elementary constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0279v1", 
    "title": "An introduction to DSmT", 
    "arxiv-id": "0903.0279v1", 
    "author": "Florentin Smarandache", 
    "publish": "2009-03-02T12:31:00Z", 
    "summary": "The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been, and\nstill remains today, of primal importance for the development of reliable\nmodern information systems involving artificial reasoning. In this\nintroduction, we present a survey of our recent theory of plausible and\nparadoxical reasoning, known as Dezert-Smarandache Theory (DSmT), developed for\ndealing with imprecise, uncertain and conflicting sources of information. We\nfocus our presentation on the foundations of DSmT and on its most important\nrules of combination, rather than on browsing specific applications of DSmT\navailable in literature. Several simple examples are given throughout this\npresentation to show the efficiency and the generality of this new approach."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0314v4", 
    "title": "Granularity-Adaptive Proof Presentation", 
    "arxiv-id": "0903.0314v4", 
    "author": "Christoph Benzmueller", 
    "publish": "2009-03-02T15:52:15Z", 
    "summary": "When mathematicians present proofs they usually adapt their explanations to\ntheir didactic goals and to the (assumed) knowledge of their addressees. Modern\nautomated theorem provers, in contrast, present proofs usually at a fixed level\nof detail (also called granularity). Often these presentations are neither\nintended nor suitable for human use. A challenge therefore is to develop user-\nand goal-adaptive proof presentation techniques that obey common mathematical\npractice. We present a flexible and adaptive approach to proof presentation\nthat exploits machine learning techniques to extract a model of the specific\ngranularity of proof examples and employs this model for the automated\ngeneration of further proofs at an adapted level of granularity."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0465v1", 
    "title": "Breaking Value Symmetry", 
    "arxiv-id": "0903.0465v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-03T08:36:47Z", 
    "summary": "Symmetry is an important factor in solving many constraint satisfaction\nproblems. One common type of symmetry is when we have symmetric values. In a\nrecent series of papers, we have studied methods to break value symmetries. Our\nresults identify computational limits on eliminating value symmetry. For\ninstance, we prove that pruning all symmetric values is NP-hard in general.\nNevertheless, experiments show that much value symmetry can be broken in\npractice. These results may be useful to researchers in planning, scheduling\nand other areas as value symmetry occurs in many different domains."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0475v1", 
    "title": "Reformulating Global Grammar Constraints", 
    "arxiv-id": "0903.0475v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-03T09:31:41Z", 
    "summary": "An attractive mechanism to specify global constraints in rostering and other\ndomains is via formal languages. For instance, the Regular and Grammar\nconstraints specify constraints in terms of the languages accepted by an\nautomaton and a context-free grammar respectively. Taking advantage of the\nfixed length of the constraint, we give an algorithm to transform a\ncontext-free grammar into an automaton. We then study the use of minimization\ntechniques to reduce the size of such automata and speed up propagation. We\nshow that minimizing such automata after they have been unfolded and domains\ninitially reduced can give automata that are more compact than minimizing\nbefore unfolding and reducing. Experimental results show that such\ntransformations can improve the size of rostering problems that we can 'model\nand run'."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0479v1", 
    "title": "Combining Symmetry Breaking and Global Constraints", 
    "arxiv-id": "0903.0479v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-03T09:52:01Z", 
    "summary": "We propose a new family of constraints which combine together lexicographical\nordering constraints for symmetry breaking with other common global\nconstraints. We give a general purpose propagator for this family of\nconstraints, and show how to improve its complexity by exploiting properties of\nthe included global constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0695v1", 
    "title": "Online Estimation of SAT Solving Runtime", 
    "arxiv-id": "0903.0695v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-04T04:56:07Z", 
    "summary": "We present an online method for estimating the cost of solving SAT problems.\nModern SAT solvers present several challenges to estimate search cost including\nnon-chronological backtracking, learning and restarts. Our method uses a linear\nmodel trained on data gathered at the start of search. We show the\neffectiveness of this method using random and structured problems. We\ndemonstrate that predictions made in early restarts can be used to improve\nlater predictions. We also show that we can use such cost estimations to select\na solver from a portfolio."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0786v2", 
    "title": "On Requirements for Programming Exercises from an E-learning Perspective", 
    "arxiv-id": "0903.0786v2", 
    "author": "Carlos Loria-Saenz", 
    "publish": "2009-03-04T15:29:42Z", 
    "summary": "In this work, we deal with the question of modeling programming exercises for\nnovices pointing to an e-learning scenario. Our purpose is to identify basic\nrequirements, raise some key questions and propose potential answers from a\nconceptual perspective. Presented as a general picture, we hypothetically\nsituate our work in a general context where e-learning instructional material\nneeds to be adapted to form part of an introductory Computer Science (CS)\ne-learning course at the CS1-level. Meant is a potential course which aims at\nimproving novices skills and knowledge on the essentials of programming by\nusing e-learning based approaches in connection (at least conceptually) with a\ngeneral host framework like Activemath (www.activemath.org). Our elaboration\ncovers contextual and, particularly, cognitive elements preparing the terrain\nfor eventual research stages in a derived project, as indicated. We concentrate\nour main efforts on reasoning mechanisms about exercise complexity that can\neventually offer tool support for the task of exercise authoring. We base our\nrequirements analysis on our own perception of the exercise subsystem provided\nby Activemath especially within the domain reasoner area. We enrich the\nanalysis by bringing to the discussion several relevant contextual elements\nfrom the CS1 courses, its definition and implementation. Concerning cognitive\nmodels and exercises, we build upon the principles of Bloom's Taxonomy as a\nrelatively standardized basis and use them as a framework for study and\nanalysis of complexity in basic programming exercises. Our analysis includes\nrequirements for the domain reasoner which are necessary for the exercise\nanalysis. We propose for such a purpose a three-layered conceptual model\nconsidering exercise evaluation, programming and metaprogramming."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-540-88636-5", 
    "link": "http://arxiv.org/pdf/0903.0829v1", 
    "title": "Tagging multimedia stimuli with ontologies", 
    "arxiv-id": "0903.0829v1", 
    "author": "Kresimir Cosic", 
    "publish": "2009-03-04T18:13:41Z", 
    "summary": "Successful management of emotional stimuli is a pivotal issue concerning\nAffective Computing (AC) and the related research. As a subfield of Artificial\nIntelligence, AC is concerned not only with the design of computer systems and\nthe accompanying hardware that can recognize, interpret, and process human\nemotions, but also with the development of systems that can trigger human\nemotional response in an ordered and controlled manner. This requires the\nmaximum attainable precision and efficiency in the extraction of data from\nemotionally annotated databases While these databases do use keywords or tags\nfor description of the semantic content, they do not provide either the\nnecessary flexibility or leverage needed to efficiently extract the pertinent\nemotional content. Therefore, to this extent we propose an introduction of\nontologies as a new paradigm for description of emotionally annotated data. The\nability to select and sequence data based on their semantic attributes is vital\nfor any study involving metadata, semantics and ontological sorting like the\nSemantic Web or the Social Semantic Desktop, and the approach described in the\npaper facilitates reuse in these areas as well."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0903.1150v1", 
    "title": "Stochastic Constraint Programming: A Scenario-Based Approach", 
    "arxiv-id": "0903.1150v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-06T04:12:20Z", 
    "summary": "To model combinatorial decision problems involving uncertainty and\nprobability, we introduce scenario based stochastic constraint programming.\nStochastic constraint programs contain both decision variables, which we can\nset, and stochastic variables, which follow a discrete probability\ndistribution. We provide a semantics for stochastic constraint programs based\non scenario trees. Using this semantics, we can compile stochastic constraint\nprograms down into conventional (non-stochastic) constraint programs. This\nallows us to exploit the full power of existing constraint solvers. We have\nimplemented this framework for decision making under uncertainty in stochastic\nOPL, a language which is based on the OPL constraint modelling language\n[Hentenryck et al., 1999]. To illustrate the potential of this framework, we\nmodel a wide range of problems in areas as diverse as portfolio\ndiversification, agricultural planning and production/inventory management."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0903.1152v1", 
    "title": "Stochastic Constraint Programming", 
    "arxiv-id": "0903.1152v1", 
    "author": "Toby Walsh", 
    "publish": "2009-03-06T04:20:41Z", 
    "summary": "To model combinatorial decision problems involving uncertainty and\nprobability, we introduce stochastic constraint programming. Stochastic\nconstraint programs contain both decision variables (which we can set) and\nstochastic variables (which follow a probability distribution). They combine\ntogether the best features of traditional constraint satisfaction, stochastic\ninteger programming, and stochastic satisfiability. We give a semantics for\nstochastic constraint programs, and propose a number of complete algorithms and\napproximation procedures. Finally, we discuss a number of extensions of\nstochastic constraint programming to relax various assumptions like the\nindependence between stochastic variables, and compare with other approaches\nfor decision making under uncertainty."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0903.3926v1", 
    "title": "Designing a GUI for Proofs - Evaluation of an HCI Experiment", 
    "arxiv-id": "0903.3926v1", 
    "author": "Andreas Meier", 
    "publish": "2009-03-23T18:38:01Z", 
    "summary": "Often user interfaces of theorem proving systems focus on assisting\nparticularly trained and skilled users, i.e., proof experts. As a result, the\nsystems are difficult to use for non-expert users. This paper describes a paper\nand pencil HCI experiment, in which (non-expert) students were asked to make\nsuggestions for a GUI for an interactive system for mathematical proofs. They\nhad to explain the usage of the GUI by applying it to construct a proof sketch\nfor a given theorem. The evaluation of the experiment provides insights for the\ninteraction design for non-expert users and the needs and wants of this user\ngroup."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0903.5054v1", 
    "title": "Flow of Activity in the Ouroboros Model", 
    "arxiv-id": "0903.5054v1", 
    "author": "Knud Thomsen", 
    "publish": "2009-03-29T15:29:17Z", 
    "summary": "The Ouroboros Model is a new conceptual proposal for an algorithmic structure\nfor efficient data processing in living beings as well as for artificial\nagents. Its central feature is a general repetitive loop where one iteration\ncycle sets the stage for the next. Sensory input activates data structures\n(schemata) with similar constituents encountered before, thus expectations are\nkindled. This corresponds to the highlighting of empty slots in the selected\nschema, and these expectations are compared with the actually encountered\ninput. Depending on the outcome of this consumption analysis different next\nsteps like search for further data or a reset, i.e. a new attempt employing\nanother schema, are triggered. Monitoring of the whole process, and in\nparticular of the flow of activation directed by the consumption analysis,\nyields valuable feedback for the optimum allocation of attention and resources\nincluding the selective establishment of useful new memory entries."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0903.5289v1", 
    "title": "Heterogeneous knowledge representation using a finite automaton and   first order logic: a case study in electromyography", 
    "arxiv-id": "0903.5289v1", 
    "author": "Yves Besnard", 
    "publish": "2009-03-30T19:08:10Z", 
    "summary": "In a certain number of situations, human cognitive functioning is difficult\nto represent with classical artificial intelligence structures. Such a\ndifficulty arises in the polyneuropathy diagnosis which is based on the spatial\ndistribution, along the nerve fibres, of lesions, together with the synthesis\nof several partial diagnoses. Faced with this problem while building up an\nexpert system (NEUROP), we developed a heterogeneous knowledge representation\nassociating a finite automaton with first order logic. A number of knowledge\nrepresentation problems raised by the electromyography test features are\nexamined in this study and the expert system architecture allowing such a\nknowledge modeling are laid out."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0904.0029v1", 
    "title": "Learning for Dynamic subsumption", 
    "arxiv-id": "0904.0029v1", 
    "author": "Lakhdar Sais", 
    "publish": "2009-03-31T23:14:05Z", 
    "summary": "In this paper a new dynamic subsumption technique for Boolean CNF formulae is\nproposed. It exploits simple and sufficient conditions to detect during\nconflict analysis, clauses from the original formula that can be reduced by\nsubsumption. During the learnt clause derivation, and at each step of the\nresolution process, we simply check for backward subsumption between the\ncurrent resolvent and clauses from the original formula and encoded in the\nimplication graph. Our approach give rise to a strong and dynamic\nsimplification technique that exploits learning to eliminate literals from the\noriginal clauses. Experimental results show that the integration of our dynamic\nsubsumption approach within the state-of-the-art SAT solvers Minisat and Rsat\nachieves interesting improvements particularly on crafted instances."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0904.2827v5", 
    "title": "Principle of development", 
    "arxiv-id": "0904.2827v5", 
    "author": "Elena S. Vishnevksaya", 
    "publish": "2009-04-18T10:49:05Z", 
    "summary": "Today, science have a powerful tool for the description of reality - the\nnumbers. However, the concept of number was not immediately, lets try to trace\nthe evolution of the concept. The numbers emerged as the need for accurate\nestimates of the amount in order to permit a comparison of some objects. So if\nyou see to it how many times a day a person uses the numbers and compare, it\nbecomes evident that the comparison is used much more frequently. However, the\ncomparison is not possible without two opposite basic standards. Thus, to\nintroduce the concept of comparison, must have two opposing standards, in turn,\nthe operation of comparison is necessary to introduce the concept of number.\nArguably, the scientific description of reality is impossible without the\nconcept of opposites.\n  In this paper analyzes the concept of opposites, as the basis for the\nintroduction of the principle of development."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0904.3701v1", 
    "title": "Semantic Social Network Analysis", 
    "arxiv-id": "0904.3701v1", 
    "author": "Michel Buffa", 
    "publish": "2009-04-23T14:22:05Z", 
    "summary": "Social Network Analysis (SNA) tries to understand and exploit the key\nfeatures of social networks in order to manage their life cycle and predict\ntheir evolution. Increasingly popular web 2.0 sites are forming huge social\nnetwork. Classical methods from social network analysis (SNA) have been applied\nto such online networks. In this paper, we propose leveraging semantic web\ntechnologies to merge and exploit the best features of each domain. We present\nhow to facilitate and enhance the analysis of online social networks,\nexploiting the power of semantic social network analysis."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0904.3953v4", 
    "title": "Guarded resolution for answer set programming", 
    "arxiv-id": "0904.3953v4", 
    "author": "J. B. Remmel", 
    "publish": "2009-04-25T00:28:07Z", 
    "summary": "We describe a variant of resolution rule of proof and show that it is\ncomplete for stable semantics of logic programs. We show applications of this\nresult."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.0192v1", 
    "title": "Fuzzy Mnesors", 
    "arxiv-id": "0905.0192v1", 
    "author": "Gilles Champenois", 
    "publish": "2009-05-02T09:23:33Z", 
    "summary": "A fuzzy mnesor space is a semimodule over the positive real numbers. It can\nbe used as theoretical framework for fuzzy sets. Hence we can prove a great\nnumber of properties for fuzzy sets without refering to the membership\nfunctions."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.0197v2", 
    "title": "An Application of Proof-Theory in Answer Set Programming", 
    "arxiv-id": "0905.0197v2", 
    "author": "J. B. Remmel", 
    "publish": "2009-05-02T10:43:30Z", 
    "summary": "We apply proof-theoretic techniques in answer Set Programming. The main\nresults include: 1. A characterization of continuity properties of\nGelfond-Lifschitz operator for logic program. 2. A propositional\ncharacterization of stable models of logic programs (without referring to loop\nformulas."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.3755v1", 
    "title": "Decompositions of All Different, Global Cardinality and Related   Constraints", 
    "arxiv-id": "0905.3755v1", 
    "author": "Toby Walsh", 
    "publish": "2009-05-22T20:45:30Z", 
    "summary": "We show that some common and important global constraints like ALL-DIFFERENT\nand GCC can be decomposed into simple arithmetic constraints on which we\nachieve bound or range consistency, and in some cases even greater pruning.\nThese decompositions can be easily added to new solvers. They also provide\nother constraints with access to the state of the propagator by sharing of\nvariables. Such sharing can be used to improve propagation between constraints.\nWe report experiments with our decomposition in a pseudo-Boolean solver."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.3763v1", 
    "title": "Scenario-based Stochastic Constraint Programming", 
    "arxiv-id": "0905.3763v1", 
    "author": "Toby Walsh", 
    "publish": "2009-05-22T21:23:40Z", 
    "summary": "To model combinatorial decision problems involving uncertainty and\nprobability, we extend the stochastic constraint programming framework proposed\nin [Walsh, 2002] along a number of important dimensions (e.g. to multiple\nchance constraints and to a range of new objectives). We also provide a new\n(but equivalent) semantics based on scenarios. Using this semantics, we can\ncompile stochastic constraint programs down into conventional (nonstochastic)\nconstraint programs. This allows us to exploit the full power of existing\nconstraint solvers. We have implemented this framework for decision making\nunder uncertainty in stochastic OPL, a language which is based on the OPL\nconstraint modelling language [Hentenryck et al., 1999]. To illustrate the\npotential of this framework, we model a wide range of problems in areas as\ndiverse as finance, agriculture and production."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.3766v1", 
    "title": "Reasoning about soft constraints and conditional preferences: complexity   results and approximation techniques", 
    "arxiv-id": "0905.3766v1", 
    "author": "Toby Walsh", 
    "publish": "2009-05-22T21:55:20Z", 
    "summary": "Many real life optimization problems contain both hard and soft constraints,\nas well as qualitative conditional preferences. However, there is no single\nformalism to specify all three kinds of information. We therefore propose a\nframework, based on both CP-nets and soft constraints, that handles both hard\nand soft constraints as well as conditional preferences efficiently and\nuniformly. We study the complexity of testing the consistency of preference\nstatements, and show how soft constraints can faithfully approximate the\nsemantics of conditional preference statements whilst improving the\ncomputational complexity"
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10601-006-6849-7", 
    "link": "http://arxiv.org/pdf/0905.3769v1", 
    "title": "Multiset Ordering Constraints", 
    "arxiv-id": "0905.3769v1", 
    "author": "Toby Walsh", 
    "publish": "2009-05-22T21:51:13Z", 
    "summary": "We identify a new and important global (or non-binary) constraint. This\nconstraint ensures that the values taken by two vectors of variables, when\nviewed as multisets, are ordered. This constraint is useful for a number of\ndifferent applications including breaking symmetry and fuzzy constraint\nsatisfaction. We propose and implement an efficient linear time algorithm for\nenforcing generalised arc consistency on such a multiset ordering constraint.\nExperimental results on several problem domains show considerable promise."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0905.3830v1", 
    "title": "Tag Clouds for Displaying Semantics: The Case of Filmscripts", 
    "arxiv-id": "0905.3830v1", 
    "author": "K. Englmeier", 
    "publish": "2009-05-23T16:11:18Z", 
    "summary": "We relate tag clouds to other forms of visualization, including planar or\nreduced dimensionality mapping, and Kohonen self-organizing maps. Using a\nmodified tag cloud visualization, we incorporate other information into it,\nincluding text sequence and most pertinent words. Our notion of word pertinence\ngoes beyond just word frequency and instead takes a word in a mathematical\nsense as located at the average of all of its pairwise relationships. We\ncapture semantics through context, taken as all pairwise relationships. Our\ndomain of application is that of filmscript analysis. The analysis of\nfilmscripts, always important for cinema, is experiencing a major gain in\nimportance in the context of television. Our objective in this work is to\nvisualize the semantics of filmscript, and beyond filmscript any other\npartially structured, time-ordered, sequence of text segments. In particular we\ndevelop an innovative approach to plot characterization."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0905.4601v1", 
    "title": "Considerations on Construction Ontologies", 
    "arxiv-id": "0905.4601v1", 
    "author": "Alexandra Emilia Fortis", 
    "publish": "2009-05-28T10:19:02Z", 
    "summary": "The paper proposes an analysis on some existent ontologies, in order to point\nout ways to resolve semantic heterogeneity in information systems. Authors are\nhighlighting the tasks in a Knowledge Acquisiton System and identifying aspects\nrelated to the addition of new information to an intelligent system. A solution\nis proposed, as a combination of ontology reasoning services and natural\nlanguages generation. A multi-agent system will be conceived with an extractor\nagent, a reasoner agent and a competence management agent."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0905.4614v2", 
    "title": "A Logic Programming Approach to Activity Recognition", 
    "arxiv-id": "0905.4614v2", 
    "author": "G. Paliouras", 
    "publish": "2009-05-28T11:44:04Z", 
    "summary": "We have been developing a system for recognising human activity given a\nsymbolic representation of video content. The input of our system is a set of\ntime-stamped short-term activities detected on video frames. The output of our\nsystem is a set of recognised long-term activities, which are pre-defined\ntemporal combinations of short-term activities. The constraints on the\nshort-term activities that, if satisfied, lead to the recognition of a\nlong-term activity, are expressed using a dialect of the Event Calculus. We\nillustrate the expressiveness of the dialect by showing the representation of\nseveral typical complex activities. Furthermore, we present a detailed\nevaluation of the system through experimentation on a benchmark dataset of\nsurveillance videos."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.1673v1", 
    "title": "Knowledge Management in Economic Intelligence with Reasoning on Temporal   Attributes", 
    "arxiv-id": "0906.1673v1", 
    "author": "Victor Odumuyiwa", 
    "publish": "2009-06-09T09:33:16Z", 
    "summary": "People have to make important decisions within a time frame. Hence, it is\nimperative to employ means or strategy to aid effective decision making.\nConsequently, Economic Intelligence (EI) has emerged as a field to aid\nstrategic and timely decision making in an organization. In the course of\nattaining this goal: it is indispensable to be more optimistic towards\nprovision for conservation of intellectual resource invested into the process\nof decision making. This intellectual resource is nothing else but the\nknowledge of the actors as well as that of the various processes for effecting\ndecision making. Knowledge has been recognized as a strategic economic resource\nfor enhancing productivity and a key for innovation in any organization or\ncommunity. Thus, its adequate management with cognizance of its temporal\nproperties is highly indispensable. Temporal properties of knowledge refer to\nthe date and time (known as timestamp) such knowledge is created as well as the\nduration or interval between related knowledge. This paper focuses on the needs\nfor a user-centered knowledge management approach as well as exploitation of\nassociated temporal properties. Our perspective of knowledge is with respect to\ndecision-problems projects in EI. Our hypothesis is that the possibility of\nreasoning about temporal properties in exploitation of knowledge in EI projects\nshould foster timely decision making through generation of useful inferences\nfrom available and reusable knowledge for a new project."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.1694v1", 
    "title": "Toward a Category Theory Design of Ontological Knowledge Bases", 
    "arxiv-id": "0906.1694v1", 
    "author": "Nikolaj Glazunov", 
    "publish": "2009-06-09T11:03:41Z", 
    "summary": "I discuss (ontologies_and_ontological_knowledge_bases /\nformal_methods_and_theories) duality and its category theory extensions as a\nstep toward a solution to Knowledge-Based Systems Theory. In particular I focus\non the example of the design of elements of ontologies and ontological\nknowledge bases of next three electronic courses: Foundations of Research\nActivities, Virtual Modeling of Complex Systems and Introduction to String\nTheory."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.3036v2", 
    "title": "Mnesors for automatic control", 
    "arxiv-id": "0906.3036v2", 
    "author": "Gilles Champenois", 
    "publish": "2009-06-16T22:05:57Z", 
    "summary": "Mnesors are defined as elements of a semimodule over the min-plus integers.\nThis two-sorted structure is able to merge graduation properties of vectors and\nidempotent properties of boolean numbers, which makes it appropriate for hybrid\nsystems. We apply it to the control of an inverted pendulum and design a full\nlogical controller, that is, without the usual algebra of real numbers."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.3149v1", 
    "title": "Semi-Myopic Sensing Plans for Value Optimization", 
    "arxiv-id": "0906.3149v1", 
    "author": "Solomon Eyal Shimony", 
    "publish": "2009-06-17T11:45:40Z", 
    "summary": "We consider the following sequential decision problem. Given a set of items\nof unknown utility, we need to select one of as high a utility as possible\n(``the selection problem''). Measurements (possibly noisy) of item values prior\nto selection are allowed, at a known cost. The goal is to optimize the overall\nsequential decision process of measurements and selection.\n  Value of information (VOI) is a well-known scheme for selecting measurements,\nbut the intractability of the problem typically leads to using myopic VOI\nestimates. In the selection problem, myopic VOI frequently badly underestimates\nthe value of information, leading to inferior sensing plans. We relax the\nstrict myopic assumption into a scheme we term semi-myopic, providing a\nspectrum of methods that can improve the performance of sensing plans. In\nparticular, we propose the efficiently computable method of ``blinkered'' VOI,\nand examine theoretical bounds for special cases. Empirical evaluation of\n``blinkered'' VOI in the selection problem with normally distributed item\nvalues shows that is performs much better than pure myopic VOI."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.4332v2", 
    "title": "Updating Sets of Probabilities", 
    "arxiv-id": "0906.4332v2", 
    "author": "Joseph Y. Halpern", 
    "publish": "2009-06-23T19:34:47Z", 
    "summary": "There are several well-known justifications for conditioning as the\nappropriate method for updating a single probability measure, given an\nobservation. However, there is a significant body of work arguing for sets of\nprobability measures, rather than single measures, as a more realistic model of\nuncertainty. Conditioning still makes sense in this context--we can simply\ncondition each measure in the set individually, then combine the results--and,\nindeed, it seems to be the preferred updating procedure in the literature. But\nhow justified is conditioning in this richer setting? Here we show, by\nconsidering an axiomatic account of conditioning given by van Fraassen, that\nthe single-measure and sets-of-measures cases are very different. We show that\nvan Fraassen's axiomatization for the former case is nowhere near sufficient\nfor updating sets of measures. We give a considerably longer (and not as\ncompelling) list of axioms that together force conditioning in this setting,\nand describe other update methods that are allowed once any of these axioms is\ndropped."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.5038v1", 
    "title": "A Novel Two-Stage Dynamic Decision Support based Optimal Threat   Evaluation and Defensive Resource Scheduling Algorithm for Multi Air-borne   threats", 
    "arxiv-id": "0906.5038v1", 
    "author": "Shoab A. Khan", 
    "publish": "2009-06-27T04:24:59Z", 
    "summary": "This paper presents a novel two-stage flexible dynamic decision support based\noptimal threat evaluation and defensive resource scheduling algorithm for\nmulti-target air-borne threats. The algorithm provides flexibility and\noptimality by swapping between two objective functions, i.e. the preferential\nand subtractive defense strategies as and when required. To further enhance the\nsolution quality, it outlines and divides the critical parameters used in\nThreat Evaluation and Weapon Assignment (TEWA) into three broad categories\n(Triggering, Scheduling and Ranking parameters). Proposed algorithm uses a\nvariant of many-to-many Stable Marriage Algorithm (SMA) to solve Threat\nEvaluation (TE) and Weapon Assignment (WA) problem. In TE stage, Threat Ranking\nand Threat-Asset pairing is done. Stage two is based on a new flexible dynamic\nweapon scheduling algorithm, allowing multiple engagements using\nshoot-look-shoot strategy, to compute near-optimal solution for a range of\nscenarios. Analysis part of this paper presents the strengths and weaknesses of\nthe proposed algorithm over an alternative greedy algorithm as applied to\ndifferent offline scenarios."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0906.5119v1", 
    "title": "General combination rules for qualitative and quantitative beliefs", 
    "arxiv-id": "0906.5119v1", 
    "author": "Florentin Smarandache", 
    "publish": "2009-06-28T08:09:04Z", 
    "summary": "Martin and Osswald \\cite{Martin07} have recently proposed many\ngeneralizations of combination rules on quantitative beliefs in order to manage\nthe conflict and to consider the specificity of the responses of the experts.\nSince the experts express themselves usually in natural language with\nlinguistic labels, Smarandache and Dezert \\cite{Li07} have introduced a\nmathematical framework for dealing directly also with qualitative beliefs. In\nthis paper we recall some element of our previous works and propose the new\ncombination rules, developed for the fusion of both qualitative or quantitative\nbeliefs."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.0067v1", 
    "title": "A Novel Two-Staged Decision Support based Threat Evaluation and Weapon   Assignment Algorithm, Asset-based Dynamic Weapon Scheduling using Artificial   Intelligence Techinques", 
    "arxiv-id": "0907.0067v1", 
    "author": "Shoab A. Khan", 
    "publish": "2009-07-01T06:05:22Z", 
    "summary": "Surveillance control and reporting (SCR) system for air threats play an\nimportant role in the defense of a country. SCR system corresponds to air and\nground situation management/processing along with information fusion,\ncommunication, coordination, simulation and other critical defense oriented\ntasks. Threat Evaluation and Weapon Assignment (TEWA) sits at the core of SCR\nsystem. In such a system, maximal or near maximal utilization of constrained\nresources is of extreme importance. Manual TEWA systems cannot provide\noptimality because of different limitations e.g.surface to air missile (SAM)\ncan fire from a distance of 5Km, but manual TEWA systems are constrained by\nhuman vision range and other constraints. Current TEWA systems usually work on\ntarget-by-target basis using some type of greedy algorithm thus affecting the\noptimality of the solution and failing in multi-target scenario. his paper\nrelates to a novel two-staged flexible dynamic decision support based optimal\nthreat evaluation and weapon assignment algorithm for multi-target air-borne\nthreats."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.0589v2", 
    "title": "Generalized Collective Inference with Symmetric Clique Potentials", 
    "arxiv-id": "0907.0589v2", 
    "author": "Ajit A. Diwan", 
    "publish": "2009-07-03T11:32:47Z", 
    "summary": "Collective graphical models exploit inter-instance associative dependence to\noutput more accurate labelings. However existing models support very limited\nkind of associativity which restricts accuracy gains. This paper makes two\nmajor contributions. First, we propose a general collective inference framework\nthat biases data instances to agree on a set of {\\em properties} of their\nlabelings. Agreement is encouraged through symmetric clique potentials. We show\nthat rich properties leads to bigger gains, and present a systematic inference\nprocedure for a large class of such properties. The procedure performs message\npassing on the cluster graph, where property-aware messages are computed with\ncluster specific algorithms. This provides an inference-only solution for\ndomain adaptation. Our experiments on bibliographic information extraction\nillustrate significant test error reduction over unseen domains. Our second\nmajor contribution consists of algorithms for computing outgoing messages from\nclique clusters with symmetric clique potentials. Our algorithms are exact for\narbitrary symmetric potentials on binary labels and for max-like and\nmajority-like potentials on multiple labels. For majority potentials, we also\nprovide an efficient Lagrangian Relaxation based algorithm that compares\nfavorably with the exact algorithm. We present a 13/15-approximation algorithm\nfor the NP-hard Potts potential, with runtime sub-quadratic in the clique size.\nIn contrast, the best known previous guarantee for graphs with Potts potentials\nis only 1/2. We empirically show that our method for Potts potentials is an\norder of magnitude faster than the best alternatives, and our Lagrangian\nRelaxation based algorithm for majority potentials beats the best applicable\nheuristic -- ICM."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.0939v2", 
    "title": "The Soft Cumulative Constraint", 
    "arxiv-id": "0907.0939v2", 
    "author": "Emmanuel Poder", 
    "publish": "2009-07-06T09:11:42Z", 
    "summary": "This research report presents an extension of Cumulative of Choco constraint\nsolver, which is useful to encode over-constrained cumulative problems. This\nnew global constraint uses sweep and task interval violation-based algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.2775v1", 
    "title": "Modelling Concurrent Behaviors in the Process Specification Language", 
    "arxiv-id": "0907.2775v1", 
    "author": "Dai Tri Man Le", 
    "publish": "2009-07-16T08:20:32Z", 
    "summary": "In this paper, we propose a first-order ontology for generalized stratified\norder structure. We then classify the models of the theory using\nmodel-theoretic techniques. An ontology mapping from this ontology to the core\ntheory of Process Specification Language is also discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.2990v1", 
    "title": "The Single Machine Total Weighted Tardiness Problem - Is it (for   Metaheuristics) a Solved Problem ?", 
    "arxiv-id": "0907.2990v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2009-07-17T06:45:46Z", 
    "summary": "The article presents a study of rather simple local search heuristics for the\nsingle machine total weighted tardiness problem (SMTWTP), namely hillclimbing\nand Variable Neighborhood Search. In particular, we revisit these approaches\nfor the SMTWTP as there appears to be a lack of appropriate/challenging\nbenchmark instances in this case. The obtained results are impressive indeed.\nOnly few instances remain unsolved, and even those are approximated within 1%\nof the optimal/best known solutions. Our experiments support the claim that\nmetaheuristics for the SMTWTP are very likely to lead to good results, and\nthat, before refining search strategies, more work must be done with regard to\nthe proposition of benchmark data. Some recommendations for the construction of\nsuch data sets are derived from our investigations."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.2993v1", 
    "title": "Improvements for multi-objective flow shop scheduling by Pareto Iterated   Local Search", 
    "arxiv-id": "0907.2993v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2009-07-17T06:53:02Z", 
    "summary": "The article describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives and compared to other local search approaches. While the obtained\nresults are encouraging in terms of their quality, another positive attribute\nof the approach is its simplicity as it does require the setting of only very\nfew parameters."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.4100v1", 
    "title": "Beyond Turing Machines", 
    "arxiv-id": "0907.4100v1", 
    "author": "Kurt Ammon", 
    "publish": "2009-07-23T15:45:10Z", 
    "summary": "This paper discusses \"computational\" systems capable of \"computing\" functions\nnot computable by predefined Turing machines if the systems are not isolated\nfrom their environment. Roughly speaking, these systems can change their finite\ndescriptions by interacting with their environment."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.4509v1", 
    "title": "Pattern Recognition Theory of Mind", 
    "arxiv-id": "0907.4509v1", 
    "author": "Gilberto de Paiva", 
    "publish": "2009-07-26T19:10:43Z", 
    "summary": "I propose that pattern recognition, memorization and processing are key\nconcepts that can be a principle set for the theoretical modeling of the mind\nfunction. Most of the questions about the mind functioning can be answered by a\ndescriptive modeling and definitions from these principles. An understandable\nconsciousness definition can be drawn based on the assumption that a pattern\nrecognition system can recognize its own patterns of activity. The principles,\ndescriptive modeling and definitions can be a basis for theoretical and applied\nresearch on cognitive sciences, particularly at artificial intelligence\nstudies."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.4561v1", 
    "title": "Fact Sheet on Semantic Web", 
    "arxiv-id": "0907.4561v1", 
    "author": "York Sure", 
    "publish": "2009-07-27T08:28:35Z", 
    "summary": "The report gives an overview about activities on the topic Semantic Web. It\nhas been released as technical report for the project \"KTweb -- Connecting\nKnowledge Technologies Communities\" in 2003."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.5032v1", 
    "title": "Restart Strategy Selection using Machine Learning Techniques", 
    "arxiv-id": "0907.5032v1", 
    "author": "Toby Walsh", 
    "publish": "2009-07-29T01:21:36Z", 
    "summary": "Restart strategies are an important factor in the performance of\nconflict-driven Davis Putnam style SAT solvers. Selecting a good restart\nstrategy for a problem instance can enhance the performance of a solver.\nInspired by recent success applying machine learning techniques to predict the\nruntime of SAT solvers, we present a method which uses machine learning to\nboost solver performance through a smart selection of the restart strategy.\nBased on easy to compute features, we train both a satisfiability classifier\nand runtime models. We use these models to choose between restart strategies.\nWe present experimental results comparing this technique with the most commonly\nused restart strategies. Our results demonstrate that machine learning is\neffective in improving solver performance."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.5033v1", 
    "title": "Online Search Cost Estimation for SAT Solvers", 
    "arxiv-id": "0907.5033v1", 
    "author": "Toby Walsh", 
    "publish": "2009-07-29T01:30:53Z", 
    "summary": "We present two different methods for estimating the cost of solving SAT\nproblems. The methods focus on the online behaviour of the backtracking solver,\nas well as the structure of the problem. Modern SAT solvers present several\nchallenges to estimate search cost including coping with nonchronological\nbacktracking, learning and restarts. Our first method adapt an existing\nalgorithm for estimating the size of a search tree to deal with these\nchallenges. We then suggest a second method that uses a linear model trained on\ndata gathered online at the start of search. We compare the effectiveness of\nthese two methods using random and structured problems. We also demonstrate\nthat predictions made in early restarts can be used to improve later\npredictions. We conclude by showing that the cost of solving a set of problems\ncan be reduced by selecting a solver from a portfolio based on such cost\nestimations."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.5155v4", 
    "title": "On Classification from Outlier View", 
    "arxiv-id": "0907.5155v4", 
    "author": "C. A. Hsiao", 
    "publish": "2009-07-29T15:47:33Z", 
    "summary": "Classification is the basis of cognition. Unlike other solutions, this study\napproaches it from the view of outliers. We present an expanding algorithm to\ndetect outliers in univariate datasets, together with the underlying\nfoundation. The expanding algorithm runs in a holistic way, making it a rather\nrobust solution. Synthetic and real data experiments show its power.\nFurthermore, an application for multi-class problems leads to the introduction\nof the oscillator algorithm. The corresponding result implies the potential\nwide use of the expanding algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0907.5598v2", 
    "title": "Convergence of Expected Utility for Universal AI", 
    "arxiv-id": "0907.5598v2", 
    "author": "Peter de Blanc", 
    "publish": "2009-07-31T19:16:09Z", 
    "summary": "We consider a sequence of repeated interactions between an agent and an\nenvironment. Uncertainty about the environment is captured by a probability\ndistribution over a space of hypotheses, which includes all computable\nfunctions. Given a utility function, we can evaluate the expected utility of\nany computational policy for interaction with the environment. After making\nsome plausible assumptions (and maybe one not-so-plausible assumption), we show\nthat if the utility function is unbounded, then the expected utility of any\npolicy is undefined."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0908.0089v1", 
    "title": "Knowledge Discovery of Hydrocyclone s Circuit Based on SONFIS and SORST", 
    "arxiv-id": "0908.0089v1", 
    "author": "M. Irannajad", 
    "publish": "2009-08-01T17:28:13Z", 
    "summary": "This study describes application of some approximate reasoning methods to\nanalysis of hydrocyclone performance. In this manner, using a combining of Self\nOrganizing Map (SOM), Neuro-Fuzzy Inference System (NFIS)-SONFIS- and Rough Set\nTheory (RST)-SORST-crisp and fuzzy granules are obtained. Balancing of crisp\ngranules and non-crisp granules can be implemented in close-open iteration.\nUsing different criteria and based on granulation level balance point\n(interval) or a pseudo-balance point is estimated. Validation of the proposed\nmethods, on the data set of the hydrocyclone is rendered."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0908.0100v1", 
    "title": "A Class of DSm Conditional Rules", 
    "arxiv-id": "0908.0100v1", 
    "author": "Mark Alford", 
    "publish": "2009-08-01T22:40:37Z", 
    "summary": "In this paper we introduce two new DSm fusion conditioning rules with\nexample, and as a generalization of them a class of DSm fusion conditioning\nrules, and then extend them to a class of DSm conditioning rules."
},{
    "category": "cs.AI", 
    "doi": "10.1057/ivs.2009.19", 
    "link": "http://arxiv.org/pdf/0908.2050v1", 
    "title": "View-based Propagator Derivation", 
    "arxiv-id": "0908.2050v1", 
    "author": "Guido Tack", 
    "publish": "2009-08-14T12:27:51Z", 
    "summary": "When implementing a propagator for a constraint, one must decide about\nvariants: When implementing min, should one also implement max? Should one\nimplement linear constraints both with unit and non-unit coefficients?\nConstraint variants are ubiquitous: implementing them requires considerable (if\nnot prohibitive) effort and decreases maintainability, but will deliver better\nperformance than resorting to constraint decomposition.\n  This paper shows how to use views to derive perfect propagator variants. A\nmodel for views and derived propagators is introduced. Derived propagators are\nproved to be indeed perfect in that they inherit essential properties such as\ncorrectness and domain and bounds consistency. Techniques for systematically\nderiving propagators such as transformation, generalization, specialization,\nand type conversion are developed. The paper introduces an implementation\narchitecture for views that is independent of the underlying constraint\nprogramming system. A detailed evaluation of views implemented in Gecode shows\nthat derived propagators are efficient and that views often incur no overhead.\nWithout views, Gecode would either require 180 000 rather than 40 000 lines of\npropagator code, or would lack many efficient propagator variants. Compared to\n8 000 lines of code for views, the reduction in code for propagators yields a\n1750% return on investment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICNC.2009.614", 
    "link": "http://arxiv.org/pdf/0908.3394v2", 
    "title": "A Cognitive Mind-map Framework to Foster Trust", 
    "arxiv-id": "0908.3394v2", 
    "author": "Christoph Schommer", 
    "publish": "2009-08-24T10:32:01Z", 
    "summary": "The explorative mind-map is a dynamic framework, that emerges automatically\nfrom the input, it gets. It is unlike a verificative modeling system where\nexisting (human) thoughts are placed and connected together. In this regard,\nexplorative mind-maps change their size continuously, being adaptive with\nconnectionist cells inside; mind-maps process data input incrementally and\noffer lots of possibilities to interact with the user through an appropriate\ncommunication interface. With respect to a cognitive motivated situation like a\nconversation between partners, mind-maps become interesting as they are able to\nprocess stimulating signals whenever they occur. If these signals are close to\nan own understanding of the world, then the conversational partner becomes\nautomatically more trustful than if the signals do not or less match the own\nknowledge scheme. In this (position) paper, we therefore motivate explorative\nmind-maps as a cognitive engine and propose these as a decision support engine\nto foster trust."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICNC.2009.614", 
    "link": "http://arxiv.org/pdf/0908.3999v1", 
    "title": "An improved axiomatic definition of information granulation", 
    "arxiv-id": "0908.3999v1", 
    "author": "Ping Zhu", 
    "publish": "2009-08-27T12:30:49Z", 
    "summary": "To capture the uncertainty of information or knowledge in information\nsystems, various information granulations, also known as knowledge\ngranulations, have been proposed. Recently, several axiomatic definitions of\ninformation granulation have been introduced. In this paper, we try to improve\nthese axiomatic definitions and give a universal construction of information\ngranulation by relating information granulations with a class of functions of\nmultiple variables. We show that the improved axiomatic definition has some\nconcrete information granulations in the literature as instances."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICNC.2009.614", 
    "link": "http://arxiv.org/pdf/0909.0122v1", 
    "title": "Reasoning with Topological and Directional Spatial Information", 
    "arxiv-id": "0909.0122v1", 
    "author": "Anthony G. Cohn", 
    "publish": "2009-09-01T08:31:22Z", 
    "summary": "Current research on qualitative spatial representation and reasoning mainly\nfocuses on one single aspect of space. In real world applications, however,\nmultiple spatial aspects are often involved simultaneously.\n  This paper investigates problems arising in reasoning with combined\ntopological and directional information. We use the RCC8 algebra and the\nRectangle Algebra (RA) for expressing topological and directional information\nrespectively. We give examples to show that the bipath-consistency algorithm\nBIPATH is incomplete for solving even basic RCC8 and RA constraints. If\ntopological constraints are taken from some maximal tractable subclasses of\nRCC8, and directional constraints are taken from a subalgebra, termed DIR49, of\nRA, then we show that BIPATH is able to separate topological constraints from\ndirectional ones. This means, given a set of hybrid topological and directional\nconstraints from the above subclasses of RCC8 and RA, we can transfer the joint\nsatisfaction problem in polynomial time to two independent satisfaction\nproblems in RCC8 and RA. For general RA constraints, we give a method to\ncompute solutions that satisfy all topological constraints and approximately\nsatisfy each RA constraint to any prescribed precision."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.0138v1", 
    "title": "Reasoning about Cardinal Directions between Extended Objects", 
    "arxiv-id": "0909.0138v1", 
    "author": "Mingsheng Ying", 
    "publish": "2009-09-01T09:58:22Z", 
    "summary": "Direction relations between extended spatial objects are important\ncommonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model,\nknown as Cardinal Direction Calculus (CDC), for representing direction\nrelations between connected plane regions. CDC is perhaps the most expressive\nqualitative calculus for directional information, and has attracted increasing\ninterest from areas such as artificial intelligence, geographical information\nscience, and image retrieval. Given a network of CDC constraints, the\nconsistency problem is deciding if the network is realizable by connected\nregions in the real plane. This paper provides a cubic algorithm for checking\nconsistency of basic CDC constraint networks, and proves that reasoning with\nCDC is in general an NP-Complete problem. For a consistent network of basic CDC\nconstraints, our algorithm also returns a 'canonical' solution in cubic time.\nThis cubic algorithm is also adapted to cope with cardinal directions between\npossibly disconnected regions, in which case currently the best algorithm is of\ntime complexity O(n^5)."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.0682v1", 
    "title": "On Planning with Preferences in HTN", 
    "arxiv-id": "0909.0682v1", 
    "author": "Sheila A. McIlraith", 
    "publish": "2009-09-03T15:27:52Z", 
    "summary": "In this paper, we address the problem of generating preferred plans by\ncombining the procedural control knowledge specified by Hierarchical Task\nNetworks (HTNs) with rich qualitative user preferences. The outcome of our work\nis a language for specifyin user preferences, tailored to HTN planning,\ntogether with a provably optimal preference-based planner, HTNPLAN, that is\nimplemented as an extension of SHOP2. To compute preferred plans, we propose an\napproach based on forward-chaining heuristic search. Our heuristic uses an\nadmissible evaluation function measuring the satisfaction of preferences over\npartial plans. Our empirical evaluation demonstrates the effectiveness of our\nHTNPLAN heuristics. We prove our approach sound and optimal with respect to the\nplans it generates by appealing to a situation calculus semantics of our\npreference language and of HTN planning. While our implementation builds on\nSHOP2, the language and techniques proposed here are relevant to a broad range\nof HTN planners."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.0901v1", 
    "title": "Assessing the Impact of Informedness on a Consultant's Profit", 
    "arxiv-id": "0909.0901v1", 
    "author": "Martin Caminada", 
    "publish": "2009-09-04T15:31:37Z", 
    "summary": "We study the notion of informedness in a client-consultant setting. Using a\nsoftware simulator, we examine the extent to which it pays off for consultants\nto provide their clients with advice that is well-informed, or with advice that\nis merely meant to appear to be well-informed. The latter strategy is\nbeneficial in that it costs less resources to keep up-to-date, but carries the\nrisk of a decreased reputation if the clients discover the low level of\ninformedness of the consultant. Our experimental results indicate that under\ndifferent circumstances, different strategies yield the optimal results (net\nprofit) for the consultants."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.1021v1", 
    "title": "A multiagent urban traffic simulation Part I: dealing with the ordinary", 
    "arxiv-id": "0909.1021v1", 
    "author": "Eric Daud\u00e9", 
    "publish": "2009-09-05T12:20:17Z", 
    "summary": "We describe in this article a multiagent urban traffic simulation, as we\nbelieve individual-based modeling is necessary to encompass the complex\ninfluence the actions of an individual vehicle can have on the overall flow of\nvehicles. We first describe how we build a graph description of the network\nfrom purely geometric data, ESRI shapefiles. We then explain how we include\ntraffic related data to this graph. We go on after that with the model of the\nvehicle agents: origin and destination, driving behavior, multiple lanes,\ncrossroads, and interactions with the other vehicles in day-to-day, ?ordinary?\ntraffic. We conclude with the presentation of the resulting simulation of this\nmodel on the Rouen agglomeration."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.1151v1", 
    "title": "n-Opposition theory to structure debates", 
    "arxiv-id": "0909.1151v1", 
    "author": "Antoine Seilles", 
    "publish": "2009-09-07T06:41:06Z", 
    "summary": "2007 was the first international congress on the ?square of oppositions?. A\nfirst attempt to structure debate using n-opposition theory was presented along\nwith the results of a first experiment on the web. Our proposal for this paper\nis to define relations between arguments through a structure of opposition\n(square of oppositions is one structure of opposition). We will be trying to\nanswer the following questions: How to organize debates on the web 2.0? How to\nstructure them in a logical way? What is the role of n-opposition theory, in\nthis context? We present in this paper results of three experiments\n(Betapolitique 2007, ECAP 2008, Intermed 2008)."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.2091v1", 
    "title": "Paired Comparisons-based Interactive Differential Evolution", 
    "arxiv-id": "0909.2091v1", 
    "author": "Denis Pallez", 
    "publish": "2009-09-11T06:30:05Z", 
    "summary": "We propose Interactive Differential Evolution (IDE) based on paired\ncomparisons for reducing user fatigue and evaluate its convergence speed in\ncomparison with Interactive Genetic Algorithms (IGA) and tournament IGA. User\ninterface and convergence performance are two big keys for reducing Interactive\nEvolutionary Computation (IEC) user fatigue. Unlike IGA and conventional IDE,\nusers of the proposed IDE and tournament IGA do not need to compare whole\nindividuals each other but compare pairs of individuals, which largely\ndecreases user fatigue. In this paper, we design a pseudo-IEC user and evaluate\nanother factor, IEC convergence performance, using IEC simulators and show that\nour proposed IDE converges significantly faster than IGA and tournament IGA,\ni.e. our proposed one is superior to others from both user interface and\nconvergence performance points of view."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.2339v1", 
    "title": "Back analysis based on SOM-RST system", 
    "arxiv-id": "0909.2339v1", 
    "author": "H. Aghababaei", 
    "publish": "2009-09-12T14:03:04Z", 
    "summary": "This paper describes application of information granulation theory, on the\nback analysis of Jeffrey mine southeast wall Quebec. In this manner, using a\ncombining of Self Organizing Map (SOM) and rough set theory (RST), crisp and\nrough granules are obtained. Balancing of crisp granules and sub rough granules\nis rendered in close-open iteration. Combining of hard and soft computing,\nnamely finite difference method (FDM) and computational intelligence and taking\nin to account missing information are two main benefits of the proposed method.\nAs a practical example, reverse analysis on the failure of the southeast wall\nJeffrey mine is accomplished."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.2375v1", 
    "title": "Similarity Matching Techniques for Fault Diagnosis in Automotive   Infotainment Electronics", 
    "arxiv-id": "0909.2375v1", 
    "author": "Mashud Kabir", 
    "publish": "2009-09-12T22:06:05Z", 
    "summary": "Fault diagnosis has become a very important area of research during the last\ndecade due to the advancement of mechanical and electrical systems in\nindustries. The automobile is a crucial field where fault diagnosis is given a\nspecial attention. Due to the increasing complexity and newly added features in\nvehicles, a comprehensive study has to be performed in order to achieve an\nappropriate diagnosis model. A diagnosis system is capable of identifying the\nfaults of a system by investigating the observable effects (or symptoms). The\nsystem categorizes the fault into a diagnosis class and identifies a probable\ncause based on the supplied fault symptoms. Fault categorization and\nidentification are done using similarity matching techniques. The development\nof diagnosis classes is done by making use of previous experience, knowledge or\ninformation within an application area. The necessary information used may come\nfrom several sources of knowledge, such as from system analysis. In this paper\nsimilarity matching techniques for fault diagnosis in automotive infotainment\napplications are discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.2376v1", 
    "title": "Performing Hybrid Recommendation in Intermodal Transportation-the   FTMarket System's Recommendation Module", 
    "arxiv-id": "0909.2376v1", 
    "author": "Alexis Lazanas", 
    "publish": "2009-09-12T22:21:12Z", 
    "summary": "Diverse recommendation techniques have been already proposed and encapsulated\ninto several e-business applications, aiming to perform a more accurate\nevaluation of the existing information and accordingly augment the assistance\nprovided to the users involved. This paper reports on the development and\nintegration of a recommendation module in an agent-based transportation\ntransactions management system. The module is built according to a novel hybrid\nrecommendation technique, which combines the advantages of collaborative\nfiltering and knowledge-based approaches. The proposed technique and supporting\nmodule assist customers in considering in detail alternative transportation\ntransactions that satisfy their requests, as well as in evaluating completed\ntransactions. The related services are invoked through a software agent that\nconstructs the appropriate knowledge rules and performs a synthesis of the\nrecommendation policy."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.3273v1", 
    "title": "Decomposition of the NVALUE constraint", 
    "arxiv-id": "0909.3273v1", 
    "author": "Toby Walsh", 
    "publish": "2009-09-17T16:58:28Z", 
    "summary": "We study decompositions of NVALUE, a global constraint that can be used to\nmodel a wide range of problems where values need to be counted. Whilst\ndecomposition typically hinders propagation, we identify one decomposition that\nmaintains a global view as enforcing bound consistency on the decomposition\nachieves bound consistency on the original global NVALUE constraint. Such\ndecompositions offer the prospect for advanced solving techniques like nogood\nlearning and impact based branching heuristics. They may also help SAT and IP\nsolvers take advantage of the propagation of global constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.3276v1", 
    "title": "Symmetries of Symmetry Breaking Constraints", 
    "arxiv-id": "0909.3276v1", 
    "author": "Toby Walsh", 
    "publish": "2009-09-17T17:16:34Z", 
    "summary": "Symmetry is an important feature of many constraint programs. We show that\nany symmetry acting on a set of symmetry breaking constraints can be used to\nbreak symmetry. Different symmetries pick out different solutions in each\nsymmetry class. We use these observations in two methods for eliminating\nsymmetry from a problem. These methods are designed to have many of the\nadvantages of symmetry breaking methods that post static symmetry breaking\nconstraint without some of the disadvantages. In particular, the two methods\nprune the search space using fast and efficient propagation of posted\nconstraints, whilst reducing the conflict between symmetry breaking and\nbranching heuristics. Experimental results show that the two methods perform\nwell on some standard benchmarks."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.4446v1", 
    "title": "Elicitation strategies for fuzzy constraint problems with missing   preferences: algorithms and experimental studies", 
    "arxiv-id": "0909.4446v1", 
    "author": "Toby Walsh", 
    "publish": "2009-09-24T13:54:38Z", 
    "summary": "Fuzzy constraints are a popular approach to handle preferences and\nover-constrained problems in scenarios where one needs to be cautious, such as\nin medical or space applications. We consider here fuzzy constraint problems\nwhere some of the preferences may be missing. This models, for example,\nsettings where agents are distributed and have privacy issues, or where there\nis an ongoing preference elicitation process. In this setting, we study how to\nfind a solution which is optimal irrespective of the missing preferences. In\nthe process of finding such a solution, we may elicit preferences from the user\nif necessary. However, our goal is to ask the user as little as possible. We\ndefine a combined solving and preference elicitation scheme with a large number\nof different instantiations, each corresponding to a concrete algorithm which\nwe compare experimentally. We compute both the number of elicited preferences\nand the \"user effort\", which may be larger, as it contains all the preference\nvalues the user has to compute to be able to respond to the elicitation\nrequests. While the number of elicited preferences is important when the\nconcern is to communicate as little information as possible, the user effort\nmeasures also the hidden work the user has to do to be able to communicate the\nelicited preferences. Our experimental results show that some of our algorithms\nare very good at finding a necessarily optimal solution while asking the user\nfor only a very small fraction of the missing preferences. The user effort is\nalso very small for the best algorithms. Finally, we test these algorithms on\nhard constraint problems with possibly missing constraints, where the aim is to\nfind feasible solutions irrespective of the missing constraints."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.4452v1", 
    "title": "Flow-Based Propagators for the SEQUENCE and Related Global Constraints", 
    "arxiv-id": "0909.4452v1", 
    "author": "Toby Walsh", 
    "publish": "2009-09-24T14:05:14Z", 
    "summary": "We propose new filtering algorithms for the SEQUENCE constraint and some\nextensions of the SEQUENCE constraint based on network flows. We enforce domain\nconsistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the\nsearch tree. This improves upon the best existing domain consistency algorithm\nby a factor of $O(\\log n)$. The flows used in these algorithms are derived from\na linear program. Some of them differ from the flows used to propagate global\nconstraints like GCC since the domains of the variables are encoded as costs on\nthe edges rather than capacities. Such flows are efficient for maintaining\nbounds consistency over large domains and may be useful for other global\nconstraints."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0909.4456v1", 
    "title": "The Weighted CFG Constraint", 
    "arxiv-id": "0909.4456v1", 
    "author": "Toby Walsh", 
    "publish": "2009-09-24T14:17:40Z", 
    "summary": "We introduce the weighted CFG constraint and propose a propagation algorithm\nthat enforces domain consistency in $O(n^3|G|)$ time. We show that this\nalgorithm can be decomposed into a set of primitive arithmetic constraints\nwithout hindering propagation."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0910.1014v2", 
    "title": "Building upon Fast Multipole Methods to Detect and Model Organizations", 
    "arxiv-id": "0910.1014v2", 
    "author": "Antoine Dutot", 
    "publish": "2009-10-06T14:19:56Z", 
    "summary": "Many models in natural and social sciences are comprised of sets of\ninter-acting entities whose intensity of interaction decreases with distance.\nThis often leads to structures of interest in these models composed of dense\npacks of entities. Fast Multipole Methods are a family of methods developed to\nhelp with the calculation of a number of computable models such as described\nabove. We propose a method that builds upon FMM to detect and model the dense\nstructures of these systems."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.artint.2010.05.006", 
    "link": "http://arxiv.org/pdf/0910.1026v1", 
    "title": "A multiagent urban traffic simulation. Part II: dealing with the   extraordinary", 
    "arxiv-id": "0910.1026v1", 
    "author": "Patrice Langlois", 
    "publish": "2009-10-06T14:41:57Z", 
    "summary": "In Probabilistic Risk Management, risk is characterized by two quantities:\nthe magnitude (or severity) of the adverse consequences that can potentially\nresult from the given activity or action, and by the likelihood of occurrence\nof the given adverse consequences. But a risk seldom exists in isolation: chain\nof consequences must be examined, as the outcome of one risk can increase the\nlikelihood of other risks. Systemic theory must complement classic PRM. Indeed\nthese chains are composed of many different elements, all of which may have a\ncritical importance at many different levels. Furthermore, when urban\ncatastrophes are envisioned, space and time constraints are key determinants of\nthe workings and dynamics of these chains of catastrophes: models must include\na correct spatial topology of the studied risk. Finally, literature insists on\nthe importance small events can have on the risk on a greater scale: urban\nrisks management models belong to self-organized criticality theory. We chose\nmultiagent systems to incorporate this property in our model: the behavior of\nan agent can transform the dynamics of important groups of them."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.1", 
    "link": "http://arxiv.org/pdf/0910.1238v1", 
    "title": "A Local Search Modeling for Constrained Optimum Paths Problems (Extended   Abstract)", 
    "arxiv-id": "0910.1238v1", 
    "author": "Pascal Van Hentenryck", 
    "publish": "2009-10-07T12:36:40Z", 
    "summary": "Constrained Optimum Path (COP) problems appear in many real-life\napplications, especially on communication networks. Some of these problems have\nbeen considered and solved by specific techniques which are usually difficult\nto extend. In this paper, we introduce a novel local search modeling for\nsolving some COPs by local search. The modeling features the compositionality,\nmodularity, reuse and strengthens the benefits of Constrained-Based Local\nSearch. We also apply the modeling to the edge-disjoint paths problem (EDP). We\nshow that side constraints can easily be added in the model. Computational\nresults show the significance of the approach."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.3", 
    "link": "http://arxiv.org/pdf/0910.1239v1", 
    "title": "Dynamic Demand-Capacity Balancing for Air Traffic Management Using   Constraint-Based Local Search: First Results", 
    "arxiv-id": "0910.1239v1", 
    "author": "Justin Pearson", 
    "publish": "2009-10-07T12:50:34Z", 
    "summary": "Using constraint-based local search, we effectively model and efficiently\nsolve the problem of balancing the traffic demands on portions of the European\nairspace while ensuring that their capacity constraints are satisfied. The\ntraffic demand of a portion of airspace is the hourly number of flights planned\nto enter it, and its capacity is the upper bound on this number under which\nair-traffic controllers can work. Currently, the only form of demand-capacity\nbalancing we allow is ground holding, that is the changing of the take-off\ntimes of not yet airborne flights. Experiments with projected European flight\nplans of the year 2030 show that already this first form of demand-capacity\nbalancing is feasible without incurring too much total delay and that it can\nlead to a significantly better demand-capacity balance."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.4", 
    "link": "http://arxiv.org/pdf/0910.1244v1", 
    "title": "On Improving Local Search for Unsatisfiability", 
    "arxiv-id": "0910.1244v1", 
    "author": "Steven Prestwich", 
    "publish": "2009-10-07T16:08:44Z", 
    "summary": "Stochastic local search (SLS) has been an active field of research in the\nlast few years, with new techniques and procedures being developed at an\nastonishing rate. SLS has been traditionally associated with satisfiability\nsolving, that is, finding a solution for a given problem instance, as its\nintrinsic nature does not address unsatisfiable problems. Unsatisfiable\ninstances were therefore commonly solved using backtrack search solvers. For\nthis reason, in the late 90s Selman, Kautz and McAllester proposed a challenge\nto use local search instead to prove unsatisfiability. More recently, two SLS\nsolvers - Ranger and Gunsat - have been developed, which are able to prove\nunsatisfiability albeit being SLS solvers. In this paper, we first compare\nRanger with Gunsat and then propose to improve Ranger performance using some of\nGunsat's techniques, namely unit propagation look-ahead and extended\nresolution."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.5", 
    "link": "http://arxiv.org/pdf/0910.1247v1", 
    "title": "Integrating Conflict Driven Clause Learning to Local Search", 
    "arxiv-id": "0910.1247v1", 
    "author": "Lakhdar Sa\u00efs", 
    "publish": "2009-10-07T16:06:29Z", 
    "summary": "This article introduces SatHyS (SAT HYbrid Solver), a novel hybrid approach\nfor propositional satisfiability. It combines local search and conflict driven\nclause learning (CDCL) scheme. Each time the local search part reaches a local\nminimum, the CDCL is launched. For SAT problems it behaves like a tabu list,\nwhereas for UNSAT ones, the CDCL part tries to focus on minimum unsatisfiable\nsub-formula (MUS). Experimental results show good performances on many classes\nof SAT instances from the last SAT competitions."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.6", 
    "link": "http://arxiv.org/pdf/0910.1253v1", 
    "title": "A Constraint-directed Local Search Approach to Nurse Rostering Problems", 
    "arxiv-id": "0910.1253v1", 
    "author": "Rong Qu", 
    "publish": "2009-10-07T13:17:36Z", 
    "summary": "In this paper, we investigate the hybridization of constraint programming and\nlocal search techniques within a large neighbourhood search scheme for solving\nhighly constrained nurse rostering problems. As identified by the research, a\ncrucial part of the large neighbourhood search is the selection of the fragment\n(neighbourhood, i.e. the set of variables), to be relaxed and re-optimized\niteratively. The success of the large neighbourhood search depends on the\nadequacy of this identified neighbourhood with regard to the problematic part\nof the solution assignment and the choice of the neighbourhood size. We\ninvestigate three strategies to choose the fragment of different sizes within\nthe large neighbourhood search scheme. The first two strategies are tailored\nconcerning the problem properties. The third strategy is more general, using\nthe information of the cost from the soft constraint violations and their\npropagation as the indicator to choose the variables added into the fragment.\nThe three strategies are analyzed and compared upon a benchmark nurse rostering\nproblem. Promising results demonstrate the possibility of future work in the\nhybrid approach."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.7", 
    "link": "http://arxiv.org/pdf/0910.1255v1", 
    "title": "Sonet Network Design Problems", 
    "arxiv-id": "0910.1255v1", 
    "author": "Charlotte Truchet", 
    "publish": "2009-10-07T13:22:22Z", 
    "summary": "This paper presents a new method and a constraint-based objective function to\nsolve two problems related to the design of optical telecommunication networks,\nnamely the Synchronous Optical Network Ring Assignment Problem (SRAP) and the\nIntra-ring Synchronous Optical Network Design Problem (IDP). These network\ntopology problems can be represented as a graph partitioning with capacity\nconstraints as shown in previous works. We present here a new objective\nfunction and a new local search algorithm to solve these problems. Experiments\nconducted in Comet allow us to compare our method to previous ones and show\nthat we obtain better results."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.8", 
    "link": "http://arxiv.org/pdf/0910.1264v1", 
    "title": "Parallel local search for solving Constraint Problems on the Cell   Broadband Engine (Preliminary Results)", 
    "arxiv-id": "0910.1264v1", 
    "author": "Philippe Codognet", 
    "publish": "2009-10-07T13:44:11Z", 
    "summary": "We explore the use of the Cell Broadband Engine (Cell/BE for short) for\ncombinatorial optimization applications: we present a parallel version of a\nconstraint-based local search algorithm that has been implemented on a\nmultiprocessor BladeCenter machine with twin Cell/BE processors (total of 16\nSPUs per blade). This algorithm was chosen because it fits very well the\nCell/BE architecture and requires neither shared memory nor communication\nbetween processors, while retaining a compact memory footprint. We study the\nperformance on several large optimization benchmarks and show that this\nachieves mostly linear time speedups, even sometimes super-linear. This is\npossible because the parallel implementation might explore simultaneously\ndifferent parts of the search space and therefore converge faster towards the\nbest sub-space and thus towards a solution. Besides getting speedups, the\nresulting times exhibit a much smaller variance, which benefits applications\nwhere a timely reply is critical."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5.2", 
    "link": "http://arxiv.org/pdf/0910.1266v1", 
    "title": "Toward an automaton Constraint for Local Search", 
    "arxiv-id": "0910.1266v1", 
    "author": "Justin Pearson", 
    "publish": "2009-10-07T13:49:26Z", 
    "summary": "We explore the idea of using finite automata to implement new constraints for\nlocal search (this is already a successful technique in constraint-based global\nsearch). We show how it is possible to maintain incrementally the violations of\na constraint and its decision variables from an automaton that describes a\nground checker for that constraint. We establish the practicality of our\napproach idea on real-life personnel rostering problems, and show that it is\ncompetitive with the approach of [Pralong, 2007]."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0910.1404v1", 
    "title": "Proceedings 6th International Workshop on Local Search Techniques in   Constraint Satisfaction", 
    "arxiv-id": "0910.1404v1", 
    "author": "Christine Solnon", 
    "publish": "2009-10-08T06:27:26Z", 
    "summary": "LSCS is a satellite workshop of the international conference on principles\nand practice of Constraint Programming (CP), since 2004. It is devoted to local\nsearch techniques in constraint satisfaction, and focuses on all aspects of\nlocal search techniques, including: design and implementation of new\nalgorithms, hybrid stochastic-systematic search, reactive search optimization,\nadaptive search, modeling for local-search, global constraints, flexibility and\nrobustness, learning methods, and specific applications."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0910.1433v1", 
    "title": "Tracking object's type changes with fuzzy based fusion rule", 
    "arxiv-id": "0910.1433v1", 
    "author": "Florentin Smarandache", 
    "publish": "2009-10-08T07:53:27Z", 
    "summary": "In this paper the behavior of three combinational rules for\ntemporal/sequential attribute data fusion for target type estimation are\nanalyzed. The comparative analysis is based on: Dempster's fusion rule proposed\nin Dempster-Shafer Theory; Proportional Conflict Redistribution rule no. 5\n(PCR5), proposed in Dezert-Smarandache Theory and one alternative class fusion\nrule, connecting the combination rules for information fusion with particular\nfuzzy operators, focusing on the t-norm based Conjunctive rule as an analog of\nthe ordinary conjunctive rule and t-conorm based Disjunctive rule as an analog\nof the ordinary disjunctive rule. The way how different t-conorms and t-norms\nfunctions within TCN fusion rule influence over target type estimation\nperformance is studied and estimated."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0910.2217v1", 
    "title": "Finite element model selection using Particle Swarm Optimization", 
    "arxiv-id": "0910.2217v1", 
    "author": "Sondipon Adhikari", 
    "publish": "2009-10-12T19:10:58Z", 
    "summary": "This paper proposes the application of particle swarm optimization (PSO) to\nthe problem of finite element model (FEM) selection. This problem arises when a\nchoice of the best model for a system has to be made from set of competing\nmodels, each developed a priori from engineering judgment. PSO is a\npopulation-based stochastic search algorithm inspired by the behaviour of\nbiological entities in nature when they are foraging for resources. Each\npotentially correct model is represented as a particle that exhibits both\nindividualistic and group behaviour. Each particle moves within the model\nsearch space looking for the best solution by updating the parameters values\nthat define it. The most important step in the particle swarm algorithm is the\nmethod of representing models which should take into account the number,\nlocation and variables of parameters to be updated. One example structural\nsystem is used to show the applicability of PSO in finding an optimal FEM. An\noptimal model is defined as the model that has the least number of updated\nparameters and has the smallest parameter variable variation from the mean\nmaterial properties. Two different objective functions are used to compare\nperformance of the PSO algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0910.3485v1", 
    "title": "A Fuzzy Petri Nets Model for Computing With Words", 
    "arxiv-id": "0910.3485v1", 
    "author": "Guoqing Chen", 
    "publish": "2009-10-19T09:09:43Z", 
    "summary": "Motivated by Zadeh's paradigm of computing with words rather than numbers,\nseveral formal models of computing with words have recently been proposed.\nThese models are based on automata and thus are not well-suited for concurrent\ncomputing. In this paper, we incorporate the well-known model of concurrent\ncomputing, Petri nets, together with fuzzy set theory and thereby establish a\nconcurrency model of computing with words--fuzzy Petri nets for computing with\nwords (FPNCWs). The new feature of such fuzzy Petri nets is that the labels of\ntransitions are some special words modeled by fuzzy sets. By employing the\nmethodology of fuzzy reasoning, we give a faithful extension of an FPNCW which\nmakes it possible for computing with more words. The language expressiveness of\nthe two formal models of computing with words, fuzzy automata for computing\nwith words and FPNCWs, is compared as well. A few small examples are provided\nto illustrate the theoretical development."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0911.2405v1", 
    "title": "Emotion: Appraisal-coping model for the \"Cascades\" problem", 
    "arxiv-id": "0911.2405v1", 
    "author": "V\u00e9ronique Jay", 
    "publish": "2009-11-12T15:03:22Z", 
    "summary": "Modelling emotion has become a challenge nowadays. Therefore, several models\nhave been produced in order to express human emotional activity. However, only\na few of them are currently able to express the close relationship existing\nbetween emotion and cognition. An appraisal-coping model is presented here,\nwith the aim to simulate the emotional impact caused by the evaluation of a\nparticular situation (appraisal), along with the consequent cognitive reaction\nintended to face the situation (coping). This model is applied to the\n\"Cascades\" problem, a small arithmetical exercise designed for ten-year-old\npupils. The goal is to create a model corresponding to a child's behaviour when\nsolving the problem using his own strategies."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0911.2501v1", 
    "title": "Emotion : mod\u00e8le d'appraisal-coping pour le probl\u00e8me des Cascades", 
    "arxiv-id": "0911.2501v1", 
    "author": "Evelyne Cl\u00e9ment", 
    "publish": "2009-11-12T23:08:43Z", 
    "summary": "Modeling emotion has become a challenge nowadays. Therefore, several models\nhave been produced in order to express human emotional activity. However, only\na few of them are currently able to express the close relationship existing\nbetween emotion and cognition. An appraisal-coping model is presented here,\nwith the aim to simulate the emotional impact caused by the evaluation of a\nparticular situation (appraisal), along with the consequent cognitive reaction\nintended to face the situation (coping). This model is applied to the\n?Cascades? problem, a small arithmetical exercise designed for ten-year-old\npupils. The goal is to create a model corresponding to a child's behavior when\nsolving the problem using his own strategies."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0911.5394v2", 
    "title": "Covering rough sets based on neighborhoods: An approach without using   neighborhoods", 
    "arxiv-id": "0911.5394v2", 
    "author": "Ping Zhu", 
    "publish": "2009-11-28T11:04:06Z", 
    "summary": "Rough set theory, a mathematical tool to deal with inexact or uncertain\nknowledge in information systems, has originally described the indiscernibility\nof elements by equivalence relations. Covering rough sets are a natural\nextension of classical rough sets by relaxing the partitions arising from\nequivalence relations to coverings. Recently, some topological concepts such as\nneighborhood have been applied to covering rough sets. In this paper, we\nfurther investigate the covering rough sets based on neighborhoods by\napproximation operations. We show that the upper approximation based on\nneighborhoods can be defined equivalently without using neighborhoods. To\nanalyze the coverings themselves, we introduce unary and composition operations\non coverings. A notion of homomorphismis provided to relate two covering\napproximation spaces. We also examine the properties of approximations\npreserved by the operations and homomorphisms, respectively."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0911.5395v2", 
    "title": "An axiomatic approach to the roughness measure of rough sets", 
    "arxiv-id": "0911.5395v2", 
    "author": "Ping Zhu", 
    "publish": "2009-11-28T11:07:59Z", 
    "summary": "In Pawlak's rough set theory, a set is approximated by a pair of lower and\nupper approximations. To measure numerically the roughness of an approximation,\nPawlak introduced a quantitative measure of roughness by using the ratio of the\ncardinalities of the lower and upper approximations. Although the roughness\nmeasure is effective, it has the drawback of not being strictly monotonic with\nrespect to the standard ordering on partitions. Recently, some improvements\nhave been made by taking into account the granularity of partitions. In this\npaper, we approach the roughness measure in an axiomatic way. After\naxiomatically defining roughness measure and partition measure, we provide a\nunified construction of roughness measure, called strong Pawlak roughness\nmeasure, and then explore the properties of this measure. We show that the\nimproved roughness measures in the literature are special instances of our\nstrong Pawlak roughness measure and introduce three more strong Pawlak\nroughness measures as well. The advantage of our axiomatic approach is that\nsome properties of a roughness measure follow immediately as soon as the\nmeasure satisfies the relevant axiomatic definition."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.0132v1", 
    "title": "Opportunistic Adaptation Knowledge Discovery", 
    "arxiv-id": "0912.0132v1", 
    "author": "Jean Lieber", 
    "publish": "2009-12-01T12:08:47Z", 
    "summary": "Adaptation has long been considered as the Achilles' heel of case-based\nreasoning since it requires some domain-specific knowledge that is difficult to\nacquire. In this paper, two strategies are combined in order to reduce the\nknowledge engineering cost induced by the adaptation knowledge (CA) acquisition\ntask: CA is learned from the case base by the means of knowledge discovery\ntechniques, and the CA acquisition sessions are opportunistically triggered,\ni.e., at problem-solving time."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.3228v1", 
    "title": "On Backtracking in Real-time Heuristic Search", 
    "arxiv-id": "0912.3228v1", 
    "author": "Vadim Bulitko", 
    "publish": "2009-12-16T18:59:29Z", 
    "summary": "Real-time heuristic search algorithms are suitable for situated agents that\nneed to make their decisions in constant time. Since the original work by Korf\nnearly two decades ago, numerous extensions have been suggested. One of the\nmost intriguing extensions is the idea of backtracking wherein the agent\ndecides to return to a previously visited state as opposed to moving forward\ngreedily. This idea has been empirically shown to have a significant impact on\nvarious performance measures. The studies have been carried out in particular\nempirical testbeds with specific real-time search algorithms that use\nbacktracking. Consequently, the extent to which the trends observed are\ncharacteristic of backtracking in general is unclear. In this paper, we present\nthe first entirely theoretical study of backtracking in real-time heuristic\nsearch. In particular, we present upper bounds on the solution cost exponential\nand linear in a parameter regulating the amount of backtracking. The results\nhold for a wide class of real-time heuristic search algorithms that includes\nmany existing algorithms as a small subclass."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.3309v1", 
    "title": "New Generalization Bounds for Learning Kernels", 
    "arxiv-id": "0912.3309v1", 
    "author": "Afshin Rostamizadeh", 
    "publish": "2009-12-17T02:29:41Z", 
    "summary": "This paper presents several novel generalization bounds for the problem of\nlearning kernels based on the analysis of the Rademacher complexity of the\ncorresponding hypothesis sets. Our bound for learning kernels with a convex\ncombination of p base kernels has only a log(p) dependency on the number of\nkernels, p, which is considerably more favorable than the previous best bound\ngiven for the same problem. We also give a novel bound for learning with a\nlinear combination of p base kernels with an L_2 regularization whose\ndependency on p is only in p^{1/4}."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.4584v1", 
    "title": "A Necessary and Sufficient Condition for Graph Matching Being Equivalent   to the Maximum Weight Clique Problem", 
    "arxiv-id": "0912.4584v1", 
    "author": "Klaus Obermayer", 
    "publish": "2009-12-23T08:40:51Z", 
    "summary": "This paper formulates a necessary and sufficient condition for a generic\ngraph matching problem to be equivalent to the maximum vertex and edge weight\nclique problem in a derived association graph. The consequences of this results\nare threefold: first, the condition is general enough to cover a broad range of\npractical graph matching problems; second, a proof to establish equivalence\nbetween graph matching and clique search reduces to showing that a given graph\nmatching problem satisfies the proposed condition; and third, the result sets\nthe scene for generic continuous solutions for a broad range of graph matching\nproblems. To illustrate the mathematical framework, we apply it to a number of\ngraph matching problems, including the problem of determining the graph edit\ndistance."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.4598v1", 
    "title": "Elkan's k-Means for Graphs", 
    "arxiv-id": "0912.4598v1", 
    "author": "Klaus Obermayer", 
    "publish": "2009-12-23T10:30:11Z", 
    "summary": "This paper extends k-means algorithms from the Euclidean domain to the domain\nof graphs. To recompute the centroids, we apply subgradient methods for solving\nthe optimization-based formulation of the sample mean of graphs. To accelerate\nthe k-means algorithm for graphs without trading computational time against\nsolution quality, we avoid unnecessary graph distance calculations by\nexploiting the triangle inequality of the underlying distance metric following\nElkan's k-means algorithm proposed in \\cite{Elkan03}. In experiments we show\nthat the accelerated k-means algorithm are faster than the standard k-means\nalgorithm for graphs provided there is a cluster structure in the data."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.4879v1", 
    "title": "Similarit\u00e9 en intension vs en extension : \u00e0 la crois\u00e9e de   l'informatique et du th\u00e9\u00e2tre", 
    "arxiv-id": "0912.4879v1", 
    "author": "Francis Rousseaux", 
    "publish": "2009-12-24T15:28:15Z", 
    "summary": "Traditional staging is based on a formal approach of similarity leaning on\ndramaturgical ontologies and instanciation variations. Inspired by interactive\ndata mining, that suggests different approaches, we give an overview of\ncomputer science and theater researches using computers as partners of the\nactor to escape the a priori specification of roles."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.5511v1", 
    "title": "A general approach to belief change in answer set programming", 
    "arxiv-id": "0912.5511v1", 
    "author": "Stefan Woltran", 
    "publish": "2009-12-30T18:33:43Z", 
    "summary": "We address the problem of belief change in (nonmonotonic) logic programming\nunder answer set semantics. Unlike previous approaches to belief change in\nlogic programming, our formal techniques are analogous to those of\ndistance-based belief revision in propositional logic. In developing our\nresults, we build upon the model theory of logic programs furnished by SE\nmodels. Since SE models provide a formal, monotonic characterisation of logic\nprograms, we can adapt techniques from the area of belief revision to belief\nchange in logic programs. We introduce methods for revising and merging logic\nprograms, respectively. For the former, we study both subset-based revision as\nwell as cardinality-based revision, and we show that they satisfy the majority\nof the AGM postulates for revision. For merging, we consider operators\nfollowing arbitration merging and IC merging, respectively. We also present\nencodings for computing the revision as well as the merging of logic programs\nwithin the same logic programming framework, giving rise to a direct\nimplementation of our approach in terms of off-the-shelf answer set solvers.\nThese encodings reflect in turn the fact that our change operators do not\nincrease the complexity of the base formalism."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/0912.5533v1", 
    "title": "Oriented Straight Line Segment Algebra: Qualitative Spatial Reasoning   about Oriented Objects", 
    "arxiv-id": "0912.5533v1", 
    "author": "Till Mossakowski", 
    "publish": "2009-12-30T20:38:12Z", 
    "summary": "Nearly 15 years ago, a set of qualitative spatial relations between oriented\nstraight line segments (dipoles) was suggested by Schlieder. This work received\nsubstantial interest amongst the qualitative spatial reasoning community.\nHowever, it turned out to be difficult to establish a sound constraint calculus\nbased on these relations. In this paper, we present the results of a new\ninvestigation into dipole constraint calculi which uses algebraic methods to\nderive sound results on the composition of relations and other properties of\ndipole calculi. Our results are based on a condensed semantics of the dipole\nrelations.\n  In contrast to the points that are normally used, dipoles are extended and\nhave an intrinsic direction. Both features are important properties of natural\nobjects. This allows for a straightforward representation of prototypical\nreasoning tasks for spatial agents. As an example, we show how to generate\nsurvey knowledge from local observations in a street network. The example\nillustrates the fast constraint-based reasoning capabilities of the dipole\ncalculus. We integrate our results into two reasoning tools which are publicly\navailable."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.0063v1", 
    "title": "On a Model for Integrated Information", 
    "arxiv-id": "1001.0063v1", 
    "author": "Enrico Nardelli", 
    "publish": "2009-12-31T01:44:12Z", 
    "summary": "In this paper we give a thorough presentation of a model proposed by Tononi\net al. for modeling \\emph{integrated information}, i.e. how much information is\ngenerated in a system transitioning from one state to the next one by the\ncausal interaction of its parts and \\emph{above and beyond} the information\ngiven by the sum of its parts. We also provides a more general formulation of\nsuch a model, independent from the time chosen for the analysis and from the\nuniformity of the probability distribution at the initial time instant.\nFinally, we prove that integrated information is null for disconnected systems."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.0921v1", 
    "title": "Graph Quantization", 
    "arxiv-id": "1001.0921v1", 
    "author": "Klaus Obermayer", 
    "publish": "2010-01-06T15:46:03Z", 
    "summary": "Vector quantization(VQ) is a lossy data compression technique from signal\nprocessing, which is restricted to feature vectors and therefore inapplicable\nfor combinatorial structures. This contribution presents a theoretical\nfoundation of graph quantization (GQ) that extends VQ to the domain of\nattributed graphs. We present the necessary Lloyd-Max conditions for optimality\nof a graph quantizer and consistency results for optimal GQ design based on\nempirical distortion measures and stochastic optimization. These results\nstatistically justify existing clustering algorithms in the domain of graphs.\nThe proposed approach provides a template of how to link structural pattern\nrecognition methods other than GQ to statistical pattern recognition."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.1257v1", 
    "title": "Decisional Processes with Boolean Neural Network: the Emergence of   Mental Schemes", 
    "arxiv-id": "1001.1257v1", 
    "author": "Elena Lensi", 
    "publish": "2010-01-08T12:34:04Z", 
    "summary": "Human decisional processes result from the employment of selected quantities\nof relevant information, generally synthesized from environmental incoming data\nand stored memories. Their main goal is the production of an appropriate and\nadaptive response to a cognitive or behavioral task. Different strategies of\nresponse production can be adopted, among which haphazard trials, formation of\nmental schemes and heuristics. In this paper, we propose a model of Boolean\nneural network that incorporates these strategies by recurring to global\noptimization strategies during the learning session. The model characterizes as\nwell the passage from an unstructured/chaotic attractor neural network typical\nof data-driven processes to a faster one, forward-only and representative of\nschema-driven processes. Moreover, a simplified version of the Iowa Gambling\nTask (IGT) is introduced in order to test the model. Our results match with\nexperimental data and point out some relevant knowledge coming from\npsychological domain."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.1836v1", 
    "title": "Web-Based Expert System for Civil Service Regulations: RCSES", 
    "arxiv-id": "1001.1836v1", 
    "author": "Fouad Mousa", 
    "publish": "2010-01-12T10:07:44Z", 
    "summary": "Internet and expert systems have offered new ways of sharing and distributing\nknowledge, but there is a lack of researches in the area of web based expert\nsystems. This paper introduces a development of a web-based expert system for\nthe regulations of civil service in the Kingdom of Saudi Arabia named as RCSES.\nIt is the first time to develop such system (application of civil service\nregulations) as well the development of it using web based approach. The\nproposed system considers 17 regulations of the civil service system. The\ndifferent phases of developing the RCSES system are presented, as knowledge\nacquiring and selection, ontology and knowledge representations using XML\nformat. XML Rule-based knowledge sources and the inference mechanisms were\nimplemented using ASP.net technique. An interactive tool for entering the\nontology and knowledge base, and the inferencing was built. It gives the\nability to use, modify, update, and extend the existing knowledge base in an\neasy way. The knowledge was validated by experts in the domain of civil service\nregulations, and the proposed RCSES was tested, verified, and validated by\ndifferent technical users and the developers staff. The RCSES system is\ncompared with other related web based expert systems, that comparison proved\nthe goodness, usability, and high performance of RCSES."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.2277v1", 
    "title": "Application of a Fuzzy Programming Technique to Production Planning in   the Textile Industry", 
    "arxiv-id": "1001.2277v1", 
    "author": "J. F. Webb", 
    "publish": "2010-01-13T19:22:26Z", 
    "summary": "Many engineering optimization problems can be considered as linear\nprogramming problems where all or some of the parameters involved are\nlinguistic in nature. These can only be quantified using fuzzy sets. The aim of\nthis paper is to solve a fuzzy linear programming problem in which the\nparameters involved are fuzzy quantities with logistic membership functions. To\nexplore the applicability of the method a numerical example is considered to\ndetermine the monthly production planning quotas and profit of a home textile\ngroup."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1001.2279v1", 
    "title": "The Application of Mamdani Fuzzy Model for Auto Zoom Function of a   Digital Camera", 
    "arxiv-id": "1001.2279v1", 
    "author": "J. F. Webb", 
    "publish": "2010-01-13T19:25:09Z", 
    "summary": "Mamdani Fuzzy Model is an important technique in Computational Intelligence\n(CI) study. This paper presents an implementation of a supervised learning\nmethod based on membership function training in the context of Mamdani fuzzy\nmodels. Specifically, auto zoom function of a digital camera is modelled using\nMamdani technique. The performance of control method is verified through a\nseries of simulation and numerical results are provided as illustrations."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.0102v5", 
    "title": "$\u03b1$-Discounting Multi-Criteria Decision Making ($\u03b1$-D MCDM)", 
    "arxiv-id": "1002.0102v5", 
    "author": "Florentin Smarandache", 
    "publish": "2010-01-31T02:38:07Z", 
    "summary": "In this book we introduce a new procedure called \\alpha-Discounting Method\nfor Multi-Criteria Decision Making (\\alpha-D MCDM), which is as an alternative\nand extension of Saaty Analytical Hierarchy Process (AHP). It works for any\nnumber of preferences that can be transformed into a system of homogeneous\nlinear equations. A degree of consistency (and implicitly a degree of\ninconsistency) of a decision-making problem are defined. \\alpha-D MCDM is\nafterwards generalized to a set of preferences that can be transformed into a\nsystem of linear and or non-linear homogeneous and or non-homogeneous equations\nand or inequalities. The general idea of \\alpha-D MCDM is to assign non-null\npositive parameters \\alpha_1, \\alpha_2, and so on \\alpha_p to the coefficients\nin the right-hand side of each preference that diminish or increase them in\norder to transform the above linear homogeneous system of equations which has\nonly the null-solution, into a system having a particular non-null solution.\nAfter finding the general solution of this system, the principles used to\nassign particular values to all parameters \\alpha is the second important part\nof \\alpha-D, yet to be deeper investigated in the future. In the current book\nwe propose the Fairness Principle, i.e. each coefficient should be discounted\nwith the same percentage (we think this is fair: not making any favoritism or\nunfairness to any coefficient), but the reader can propose other principles.\nFor consistent decision-making problems with pairwise comparisons,\n\\alpha-Discounting Method together with the Fairness Principle give the same\nresult as AHP. But for weak inconsistent decision-making problem,\n\\alpha-Discounting together with the Fairness Principle give a different result\nfrom AHP. Many consistent, weak inconsistent, and strong inconsistent examples\nare given in this book."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.0136v1", 
    "title": "Dominion -- A constraint solver generator", 
    "arxiv-id": "1002.0136v1", 
    "author": "Lars Kotthoff", 
    "publish": "2010-01-31T15:46:56Z", 
    "summary": "This paper proposes a design for a system to generate constraint solvers that\nare specialised for specific problem models. It describes the design in detail\nand gives preliminary experimental results showing the feasibility and\neffectiveness of the approach."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.0177v1", 
    "title": "Logical Evaluation of Consciousness: For Incorporating Consciousness   into Machine Architecture", 
    "arxiv-id": "1002.0177v1", 
    "author": "R. R. Panda", 
    "publish": "2010-02-01T04:07:34Z", 
    "summary": "Machine Consciousness is the study of consciousness in a biological,\nphilosophical, mathematical and physical perspective and designing a model that\ncan fit into a programmable system architecture. Prime objective of the study\nis to make the system architecture behave consciously like a biological model\ndoes. Present work has developed a feasible definition of consciousness, that\ncharacterizes consciousness with four parameters i.e., parasitic, symbiotic,\nself referral and reproduction. Present work has also developed a biologically\ninspired consciousness architecture that has following layers: quantum layer,\ncellular layer, organ layer and behavioral layer and traced the characteristics\nof consciousness at each layer. Finally, the work has estimated physical and\nalgorithmic architecture to devise a system that can behave consciously."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.0449v2", 
    "title": "Some improved results on communication between information systems", 
    "arxiv-id": "1002.0449v2", 
    "author": "Qiaoyan Wen", 
    "publish": "2010-02-02T10:54:30Z", 
    "summary": "To study the communication between information systems, Wang et al. [C. Wang,\nC. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,\nInformation Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and\ntype-2 consistent functions. Some properties of such functions and induced\nrelation mappings have been investigated there. In this paper, we provide an\nimprovement of the aforementioned work by disclosing the symmetric relationship\nbetween type-1 and type-2 consistent functions. We present more properties of\nconsistent functions and induced relation mappings and improve upon several\ndeficient assertions in the original work. In particular, we unify and extend\ntype-1 and type-2 consistent functions into the so-called\nneighborhood-consistent functions. This provides a convenient means for\nstudying the communication between information systems based on various\nneighborhoods."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.0908v1", 
    "title": "Homomorphisms between fuzzy information systems revisited", 
    "arxiv-id": "1002.0908v1", 
    "author": "Qiaoyan Wen", 
    "publish": "2010-02-04T07:09:46Z", 
    "summary": "Recently, Wang et al. discussed the properties of fuzzy information systems\nunder homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms\nbetween fuzzy information systems, Applied Mathematics Letters 22 (2009)\n1045-1050], where homomorphisms are based upon the concepts of consistent\nfunctions and fuzzy relation mappings. In this paper, we classify consistent\nfunctions as predecessor-consistent and successor-consistent, and then proceed\nto present more properties of consistent functions. In addition, we improve\nsome characterizations of fuzzy relation mappings provided by Wang et al."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.1157v1", 
    "title": "Establishment of Relationships between Material Design and Product   Design Domains by Hybrid FEM-ANN Technique", 
    "arxiv-id": "1002.1157v1", 
    "author": "M. Joseph Malvin Raj", 
    "publish": "2010-02-05T09:02:54Z", 
    "summary": "In this paper, research on AI based modeling technique to optimize\ndevelopment of new alloys with necessitated improvements in properties and\nchemical mixture over existing alloys as per functional requirements of product\nis done. The current research work novels AI in lieu of predictions to\nestablish association between material and product customary. Advanced\ncomputational simulation techniques like CFD, FEA interrogations are made\nviable to authenticate product dynamics in context to experimental\ninvestigations. Accordingly, the current research is focused towards binding\nrelationships between material design and product design domains. The input to\nfeed forward back propagation prediction network model constitutes of material\ndesign features. Parameters relevant to product design strategies are furnished\nas target outputs. The outcomes of ANN shows good sign of correlation between\nmaterial and product design domains. The study enriches a new path to\nillustrate material factors at the time of new product development."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.5", 
    "link": "http://arxiv.org/pdf/1002.2202v1", 
    "title": "Modeling of Human Criminal Behavior using Probabilistic Networks", 
    "arxiv-id": "1002.2202v1", 
    "author": "Dr. Ramakanth Kumar . P", 
    "publish": "2010-02-10T20:21:52Z", 
    "summary": "Currently, criminals profile (CP) is obtained from investigators or forensic\npsychologists interpretation, linking crime scene characteristics and an\noffenders behavior to his or her characteristics and psychological profile.\nThis paper seeks an efficient and systematic discovery of nonobvious and\nvaluable patterns between variables from a large database of solved cases via a\nprobabilistic network (PN) modeling approach. The PN structure can be used to\nextract behavioral patterns and to gain insight into what factors influence\nthese behaviors. Thus, when a new case is being investigated and the profile\nvariables are unknown because the offender has yet to be identified, the\nobserved crime scene variables are used to infer the unknown variables based on\ntheir connections in the structure and the corresponding numerical\n(probabilistic) weights. The objective is to produce a more systematic and\nempirical approach to profiling, and to use the resulting PN model as a\ndecision tool."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1389449.1389479", 
    "link": "http://arxiv.org/pdf/1002.2897v1", 
    "title": "Model-Driven Constraint Programming", 
    "arxiv-id": "1002.2897v1", 
    "author": "Ricardo Soto", 
    "publish": "2010-02-15T15:47:29Z", 
    "summary": "Constraint programming can definitely be seen as a model-driven paradigm. The\nusers write programs for modeling problems. These programs are mapped to\nexecutable models to calculate the solutions. This paper focuses on efficient\nmodel management (definition and transformation). From this point of view, we\npropose to revisit the design of constraint-programming systems. A model-driven\narchitecture is introduced to map solving-independent constraint models to\nsolving-dependent decision models. Several important questions are examined,\nsuch as the need for a visual highlevel modeling language, and the quality of\nmetamodeling techniques to implement the transformations. A main result is the\ns-COMMA platform that efficiently implements the chain from modeling to solving\nconstraint problems"
},{
    "category": "cs.AI", 
    "doi": "10.1145/1389449.1389479", 
    "link": "http://arxiv.org/pdf/1002.3023v1", 
    "title": "Rewriting Constraint Models with Metamodels", 
    "arxiv-id": "1002.3023v1", 
    "author": "Ricardo Soto", 
    "publish": "2010-02-16T07:26:48Z", 
    "summary": "An important challenge in constraint programming is to rewrite constraint\nmodels into executable programs calculat- ing the solutions. This phase of\nconstraint processing may require translations between constraint programming\nlan- guages, transformations of constraint representations, model\noptimizations, and tuning of solving strategies. In this paper, we introduce a\npivot metamodel describing the common fea- tures of constraint models including\ndifferent kinds of con- straints, statements like conditionals and loops, and\nother first-class elements like object classes and predicates. This metamodel\nis general enough to cope with the constructions of many languages, from\nobject-oriented modeling languages to logic languages, but it is independent\nfrom them. The rewriting operations manipulate metamodel instances apart from\nlanguages. As a consequence, the rewriting operations apply whatever languages\nare selected and they are able to manage model semantic information. A bridge\nis created between the metamodel space and languages using parsing techniques.\nTools from the software engineering world can be useful to implement this\nframework."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1389449.1389479", 
    "link": "http://arxiv.org/pdf/1002.3078v1", 
    "title": "Using ATL to define advanced and flexible constraint model   transformations", 
    "arxiv-id": "1002.3078v1", 
    "author": "Ricardo Soto", 
    "publish": "2010-02-16T13:09:07Z", 
    "summary": "Transforming constraint models is an important task in re- cent constraint\nprogramming systems. User-understandable models are defined during the modeling\nphase but rewriting or tuning them is manda- tory to get solving-efficient\nmodels. We propose a new architecture al- lowing to define bridges between any\n(modeling or solver) languages and to implement model optimizations. This\narchitecture follows a model- driven approach where the constraint modeling\nprocess is seen as a set of model transformations. Among others, an interesting\nfeature is the def- inition of transformations as concept-oriented rules, i.e.\nbased on types of model elements where the types are organized into a hierarchy\ncalled a metamodel."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1389449.1389479", 
    "link": "http://arxiv.org/pdf/1002.4522v1", 
    "title": "Feature Importance in Bayesian Assessment of Newborn Brain Maturity from   EEG", 
    "arxiv-id": "1002.4522v1", 
    "author": "C. Maple", 
    "publish": "2010-02-24T11:11:52Z", 
    "summary": "The methodology of Bayesian Model Averaging (BMA) is applied for assessment\nof newborn brain maturity from sleep EEG. In theory this methodology provides\nthe most accurate assessments of uncertainty in decisions. However, the\nexisting BMA techniques have been shown providing biased assessments in the\nabsence of some prior information enabling to explore model parameter space in\ndetails within a reasonable time. The lack in details leads to disproportional\nsampling from the posterior distribution. In case of the EEG assessment of\nbrain maturity, BMA results can be biased because of the absence of information\nabout EEG feature importance. In this paper we explore how the posterior\ninformation about EEG features can be used in order to reduce a negative impact\nof disproportional sampling on BMA performance. We use EEG data recorded from\nsleeping newborns to test the efficiency of the proposed BMA technique."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1389449.1389479", 
    "link": "http://arxiv.org/pdf/1003.0590v1", 
    "title": "A new model for solution of complex distributed constrained problems", 
    "arxiv-id": "1003.0590v1", 
    "author": "Eduard Babkin", 
    "publish": "2010-03-02T13:40:43Z", 
    "summary": "In this paper we describe an original computational model for solving\ndifferent types of Distributed Constraint Satisfaction Problems (DCSP). The\nproposed model is called Controller-Agents for Constraints Solving (CACS). This\nmodel is intended to be used which is an emerged field from the integration\nbetween two paradigms of different nature: Multi-Agent Systems (MAS) and the\nConstraint Satisfaction Problem paradigm (CSP) where all constraints are\ntreated in central manner as a black-box. This model allows grouping\nconstraints to form a subset that will be treated together as a local problem\ninside the controller. Using this model allows also handling non-binary\nconstraints easily and directly so that no translating of constraints into\nbinary ones is needed. This paper presents the implementation outlines of a\nprototype of DCSP solver, its usage methodology and overview of the CACS\napplication for timetabling problems."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.0746v1", 
    "title": "Automatically Discovering Hidden Transformation Chaining Constraints", 
    "arxiv-id": "1003.0746v1", 
    "author": "Fr\u00e9d\u00e9ric Jouault", 
    "publish": "2010-03-03T08:04:45Z", 
    "summary": "Model transformations operate on models conforming to precisely defined\nmetamodels. Consequently, it often seems relatively easy to chain them: the\noutput of a transformation may be given as input to a second one if metamodels\nmatch. However, this simple rule has some obvious limitations. For instance, a\ntransformation may only use a subset of a metamodel. Therefore, chaining\ntransformations appropriately requires more information. We present here an\napproach that automatically discovers more detailed information about actual\nchaining constraints by statically analyzing transformations. The objective is\nto provide developers who decide to chain transformations with more data on\nwhich to base their choices. This approach has been successfully applied to the\ncase of a library of endogenous transformations. They all have the same source\nand target metamodel but have some hidden chaining constraints. In such a case,\nthe simple metamodel matching rule given above does not provide any useful\ninformation."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.1493v1", 
    "title": "Integration of Rule Based Expert Systems and Case Based Reasoning in an   Acute Bacterial Meningitis Clinical Decision Support System", 
    "arxiv-id": "1003.1493v1", 
    "author": "Ernesto Ocampo Edye", 
    "publish": "2010-03-07T17:09:49Z", 
    "summary": "This article presents the results of the research carried out on the\ndevelopment of a medical diagnostic system applied to the Acute Bacterial\nMeningitis, using the Case Based Reasoning methodology. The research was\nfocused on the implementation of the adaptation stage, from the integration of\nCase Based Reasoning and Rule Based Expert Systems. In this adaptation stage we\nuse a higher level RBC that stores and allows reutilizing change experiences,\ncombined with a classic rule-based inference engine. In order to take into\naccount the most evident clinical situation, a pre-diagnosis stage is\nimplemented using a rule engine that, given an evident situation, emits the\ncorresponding diagnosis and avoids the complete process."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.1504v1", 
    "title": "Indexer Based Dynamic Web Services Discovery", 
    "arxiv-id": "1003.1504v1", 
    "author": "Malik Sikandar Hayat Khiyal", 
    "publish": "2010-03-07T18:04:28Z", 
    "summary": "Recent advancement in web services plays an important role in business to\nbusiness and business to consumer interaction. Discovery mechanism is not only\nused to find a suitable service but also provides collaboration between service\nproviders and consumers by using standard protocols. A static web service\ndiscovery mechanism is not only time consuming but requires continuous human\ninteraction. This paper proposed an efficient dynamic web services discovery\nmechanism that can locate relevant and updated web services from service\nregistries and repositories with timestamp based on indexing value and\ncategorization for faster and efficient discovery of service. The proposed\nprototype focuses on quality of service issues and introduces concept of local\ncache, categorization of services, indexing mechanism, CSP (Constraint\nSatisfaction Problem) solver, aging and usage of translator. Performance of\nproposed framework is evaluated by implementing the algorithm and correctness\nof our method is shown. The results of proposed framework shows greater\nperformance and accuracy in dynamic discovery mechanism of web services\nresolving the existing issues of flexibility, scalability, based on quality of\nservice, and discovers updated and most relevant services with ease of usage."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.1588v1", 
    "title": "On the Failure of the Finite Model Property in some Fuzzy Description   Logics", 
    "arxiv-id": "1003.1588v1", 
    "author": "Umberto Straccia", 
    "publish": "2010-03-08T10:18:12Z", 
    "summary": "Fuzzy Description Logics (DLs) are a family of logics which allow the\nrepresentation of (and the reasoning with) structured knowledge affected by\nvagueness. Although most of the not very expressive crisp DLs, such as ALC,\nenjoy the Finite Model Property (FMP), this is not the case once we move into\nthe fuzzy case. In this paper we show that if we allow arbitrary knowledge\nbases, then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not\nverify the FMP even if we restrict to witnessed models; in other words, finite\nsatisfiability and witnessed satisfiability are different for arbitrary\nknowledge bases. The aim of this paper is to point out the failure of FMP\nbecause it affects several algorithms published in the literature for reasoning\nunder fuzzy ALC."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.1658v3", 
    "title": "A multivalued knowledge-base model", 
    "arxiv-id": "1003.1658v3", 
    "author": "Agnes Achs", 
    "publish": "2010-03-08T16:00:28Z", 
    "summary": "The basic aim of our study is to give a possible model for handling uncertain\ninformation. This model is worked out in the framework of DATALOG. At first the\nconcept of fuzzy Datalog will be summarized, then its extensions for\nintuitionistic- and interval-valued fuzzy logic is given and the concept of\nbipolar fuzzy Datalog is introduced. Based on these ideas the concept of\nmultivalued knowledge-base will be defined as a quadruple of any background\nknowledge; a deduction mechanism; a connecting algorithm, and a function set of\nthe program, which help us to determine the uncertainty levels of the results.\nAt last a possible evaluation strategy is given."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.2641v1", 
    "title": "Release ZERO.0.1 of package RefereeToolbox", 
    "arxiv-id": "1003.2641v1", 
    "author": "Fr\u00e9d\u00e9ric Dambreville", 
    "publish": "2010-03-12T21:25:10Z", 
    "summary": "RefereeToolbox is a java package implementing combination operators for\nfusing evidences. It is downloadable from:\nhttp://refereefunction.fredericdambreville.com/releases RefereeToolbox is based\non an interpretation of the fusion rules by means of Referee Functions. This\napproach implies a dissociation between the definition of the combination and\nits actual implementation, which is common to all referee-based combinations.\nAs a result, RefereeToolbox is designed with the aim to be generic and\nevolutive."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.5173v1", 
    "title": "LEXSYS: Architecture and Implication for Intelligent Agent systems", 
    "arxiv-id": "1003.5173v1", 
    "author": "Charles A. B. Robert", 
    "publish": "2010-03-26T16:01:52Z", 
    "summary": "LEXSYS, (Legume Expert System) was a project conceived at IITA (International\nInstitute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the\nCOMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was\nmeant for a general framework for characterizing on-farm testing for technology\ndesign for sustainable cereal-based cropping system. LEXSYS is not a true\nexpert system as the name would imply, but simply a user-friendly information\nsystem. This work is an attempt to give a formal representation of the existing\nsystem and then present areas where intelligent agent can be applied."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04425-0_8", 
    "link": "http://arxiv.org/pdf/1003.5305v2", 
    "title": "Rational Value of Information Estimation for Measurement Selection", 
    "arxiv-id": "1003.5305v2", 
    "author": "Solomon Eyal Shimony", 
    "publish": "2010-03-27T14:56:16Z", 
    "summary": "Computing value of information (VOI) is a crucial task in various aspects of\ndecision-making under uncertainty, such as in meta-reasoning for search; in\nselecting measurements to make, prior to choosing a course of action; and in\nmanaging the exploration vs. exploitation tradeoff. Since such applications\ntypically require numerous VOI computations during a single run, it is\nessential that VOI be computed efficiently. We examine the issue of anytime\nestimation of VOI, as frequently it suffices to get a crude estimate of the\nVOI, thus saving considerable computational resources. As a case study, we\nexamine VOI estimation in the measurement selection problem. Empirical\nevaluation of the proposed scheme in this domain shows that computational\nresources can indeed be significantly reduced, at little cost in expected\nrewards achieved in the overall decision problem."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1003.5899v1", 
    "title": "Geometric Algebra Model of Distributed Representations", 
    "arxiv-id": "1003.5899v1", 
    "author": "Agnieszka Patyk", 
    "publish": "2010-03-30T19:03:43Z", 
    "summary": "Formalism based on GA is an alternative to distributed representation models\ndeveloped so far --- Smolensky's tensor product, Holographic Reduced\nRepresentations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced\nby geometric products, interpretable in terms of geometry which seems to be the\nmost natural language for visualization of higher concepts. This paper recalls\nthe main ideas behind the GA model and investigates recognition test results\nusing both inner product and a clipped version of matrix representation. The\ninfluence of accidental blade equality on recognition is also studied. Finally,\nthe efficiency of the GA model is compared to that of previously developed\nmodels."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.1540v1", 
    "title": "Importance of Sources using the Repeated Fusion Method and the   Proportional Conflict Redistribution Rules #5 and #6", 
    "arxiv-id": "1004.1540v1", 
    "author": "Jean Dezert", 
    "publish": "2010-04-09T12:38:17Z", 
    "summary": "We present in this paper some examples of how to compute by hand the PCR5\nfusion rule for three sources, so the reader will better understand its\nmechanism. We also take into consideration the importance of sources, which is\ndifferent from the classical discounting of sources."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.1772v1", 
    "title": "Terrorism Event Classification Using Fuzzy Inference Systems", 
    "arxiv-id": "1004.1772v1", 
    "author": "Dat Tran", 
    "publish": "2010-04-11T08:12:31Z", 
    "summary": "Terrorism has led to many problems in Thai societies, not only property\ndamage but also civilian casualties. Predicting terrorism activities in advance\ncan help prepare and manage risk from sabotage by these activities. This paper\nproposes a framework focusing on event classification in terrorism domain using\nfuzzy inference systems (FISs). Each FIS is a decision-making model combining\nfuzzy logic and approximate reasoning. It is generated in five main parts: the\ninput interface, the fuzzification interface, knowledge base unit, decision\nmaking unit and output defuzzification interface. Adaptive neuro-fuzzy\ninference system (ANFIS) is a FIS model adapted by combining the fuzzy logic\nand neural network. The ANFIS utilizes automatic identification of fuzzy logic\nrules and adjustment of membership function (MF). Moreover, neural network can\ndirectly learn from data set to construct fuzzy logic rules and MF implemented\nin various applications. FIS settings are evaluated based on two comparisons.\nThe first evaluation is the comparison between unstructured and structured\nevents using the same FIS setting. The second comparison is the model settings\nbetween FIS and ANFIS for classifying structured events. The data set consists\nof news articles related to terrorism events in three southern provinces of\nThailand. The experimental results show that the classification performance of\nthe FIS resulting from structured events achieves satisfactory accuracy and is\nbetter than the unstructured events. In addition, the classification of\nstructured events using ANFIS gives higher performance than the events using\nonly FIS in the prediction of terrorism events."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.1794v1", 
    "title": "Probabilistic Semantic Web Mining Using Artificial Neural Analysis", 
    "arxiv-id": "1004.1794v1", 
    "author": "N. Lakshmi Narayana", 
    "publish": "2010-04-11T11:19:32Z", 
    "summary": "Most of the web user's requirements are search or navigation time and getting\ncorrectly matched result. These constrains can be satisfied with some\nadditional modules attached to the existing search engines and web servers.\nThis paper proposes that powerful architecture for search engines with the\ntitle of Probabilistic Semantic Web Mining named from the methods used. With\nthe increase of larger and larger collection of various data resources on the\nWorld Wide Web (WWW), Web Mining has become one of the most important\nrequirements for the web users. Web servers will store various formats of data\nincluding text, image, audio, video etc., but servers can not identify the\ncontents of the data. These search techniques can be improved by adding some\nspecial techniques including semantic web mining and probabilistic analysis to\nget more accurate results. Semantic web mining technique can provide meaningful\nsearch of data resources by eliminating useless information with mining\nprocess. In this technique web servers will maintain Meta information of each\nand every data resources available in that particular web server. This will\nhelp the search engine to retrieve information that is relevant to user given\ninput string. This paper proposing the idea of combing these two techniques\nSemantic web mining and Probabilistic analysis for efficient and accurate\nsearch results of web mining. SPF can be calculated by considering both\nsemantic accuracy and syntactic accuracy of data with the input string. This\nwill be the deciding factor for producing results."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.2008v1", 
    "title": "Matrix Coherence and the Nystrom Method", 
    "arxiv-id": "1004.2008v1", 
    "author": "Afshin Rostamizadeh", 
    "publish": "2010-04-12T17:09:16Z", 
    "summary": "The Nystrom method is an efficient technique to speed up large-scale learning\napplications by generating low-rank approximations. Crucial to the performance\nof this technique is the assumption that a matrix can be well approximated by\nworking exclusively with a subset of its columns. In this work we relate this\nassumption to the concept of matrix coherence and connect matrix coherence to\nthe performance of the Nystrom method. Making use of related work in the\ncompressed sensing and the matrix completion literature, we derive novel\ncoherence-based bounds for the Nystrom method in the low-rank setting. We then\npresent empirical results that corroborate these theoretical bounds. Finally,\nwe present more general empirical results for the full-rank setting that\nconvincingly demonstrate the ability of matrix coherence to measure the degree\nto which information can be extracted from a subset of columns."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.2624v1", 
    "title": "Symmetry within Solutions", 
    "arxiv-id": "1004.2624v1", 
    "author": "Toby Walsh", 
    "publish": "2010-04-15T13:15:06Z", 
    "summary": "We define the concept of an internal symmetry. This is a symmety within a\nsolution of a constraint satisfaction problem. We compare this to solution\nsymmetry, which is a mapping between different solutions of the same problem.\nWe argue that we may be able to exploit both types of symmetry when finding\nsolutions. We illustrate the potential of exploiting internal symmetries on two\nbenchmark domains: Van der Waerden numbers and graceful graphs. By identifying\ninternal symmetries we are able to extend the state of the art in both cases."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.2626v1", 
    "title": "Propagating Conjunctions of AllDifferent Constraints", 
    "arxiv-id": "1004.2626v1", 
    "author": "Toby Walsh", 
    "publish": "2010-04-15T13:37:49Z", 
    "summary": "We study propagation algorithms for the conjunction of two AllDifferent\nconstraints. Solutions of an AllDifferent constraint can be seen as perfect\nmatchings on the variable/value bipartite graph. Therefore, we investigate the\nproblem of finding simultaneous bipartite matchings. We present an extension of\nthe famous Hall theorem which characterizes when simultaneous bipartite\nmatchings exists. Unfortunately, finding such matchings is NP-hard in general.\nHowever, we prove a surprising result that finding a simultaneous matching on a\nconvex bipartite graph takes just polynomial time. Based on this theoretical\nresult, we provide the first polynomial time bound consistency algorithm for\nthe conjunction of two AllDifferent constraints. We identify a pathological\nproblem on which this propagator is exponentially faster compared to existing\npropagators. Our experiments show that this new propagator can offer\nsignificant benefits over existing methods."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-1-84996-108-0_19", 
    "link": "http://arxiv.org/pdf/1004.3260v1", 
    "title": "Decision Support Systems (DSS) in Construction Tendering Processes", 
    "arxiv-id": "1004.3260v1", 
    "author": "Noor Maizura Mohamad Noor", 
    "publish": "2010-04-19T17:56:06Z", 
    "summary": "The successful execution of a construction project is heavily impacted by\nmaking the right decision during tendering processes. Managing tender\nprocedures is very complex and uncertain involving coordination of many tasks\nand individuals with different priorities and objectives. Bias and inconsistent\ndecision are inevitable if the decision-making process is totally depends on\nintuition, subjective judgement or emotion. In making transparent decision and\nhealthy competition tendering, there exists a need for flexible guidance tool\nfor decision support. Aim of this paper is to give a review on current\npractices of Decision Support Systems (DSS) technology in construction\ntendering processes. Current practices of general tendering processes as\napplied to the most countries in different regions such as United States,\nEurope, Middle East and Asia are comprehensively discussed. Applications of\nWeb-based tendering processes is also summarised in terms of its properties.\nBesides that, a summary of Decision Support System (DSS) components is included\nin the next section. Furthermore, prior researches on implementation of DSS\napproaches in tendering processes are discussed in details. Current issues\narise from both of paper-based and Web-based tendering processes are outlined.\nFinally, conclusion is included at the end of this paper."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1004.4342v2", 
    "title": "Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version)", 
    "arxiv-id": "1004.4342v2", 
    "author": "Jo\u00e3o Leite", 
    "publish": "2010-04-25T10:51:35Z", 
    "summary": "The need for integration of ontologies with nonmonotonic rules has been\ngaining importance in a number of areas, such as the Semantic Web. A number of\nresearchers addressed this problem by proposing a unified semantics for hybrid\nknowledge bases composed of both an ontology (expressed in a fragment of\nfirst-order logic) and nonmonotonic rules. These semantics have matured over\nthe years, but only provide solutions for the static case when knowledge does\nnot need to evolve. In this paper we take a first step towards addressing the\ndynamics of hybrid knowledge bases. We focus on knowledge updates and,\nconsidering the state of the art of belief update, ontology update and rule\nupdate, we show that current solutions are only partial and difficult to\ncombine. Then we extend the existing work on ABox updates with rules, provide a\nsemantics for such evolving hybrid knowledge bases and study its basic\nproperties. To the best of our knowledge, this is the first time that an update\noperator is proposed for hybrid knowledge bases."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1004.4734v1", 
    "title": "On the comparison of plans: Proposition of an instability measure for   dynamic machine scheduling", 
    "arxiv-id": "1004.4734v1", 
    "author": "Martin Josef Geiger", 
    "publish": "2010-04-27T08:13:51Z", 
    "summary": "On the basis of an analysis of previous research, we present a generalized\napproach for measuring the difference of plans with an exemplary application to\nmachine scheduling. Our work is motivated by the need for such measures, which\nare used in dynamic scheduling and planning situations. In this context,\nquantitative approaches are needed for the assessment of the robustness and\nstability of schedules. Obviously, any `robustness' or `stability' of plans has\nto be defined w. r. t. the particular situation and the requirements of the\nhuman decision maker. Besides the proposition of an instability measure, we\ntherefore discuss possibilities of obtaining meaningful information from the\ndecision maker for the implementation of the introduced approach."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1004.4801v1", 
    "title": "Ontology-based inference for causal explanation", 
    "arxiv-id": "1004.4801v1", 
    "author": "Yves Moinard", 
    "publish": "2010-04-27T13:42:49Z", 
    "summary": "We define an inference system to capture explanations based on causal\nstatements, using an ontology in the form of an IS-A hierarchy. We first\nintroduce a simple logical language which makes it possible to express that a\nfact causes another fact and that a fact explains another fact. We present a\nset of formal inference patterns from causal statements to explanation\nstatements. We introduce an elementary ontology which gives greater\nexpressiveness to the system while staying close to propositional reasoning. We\nprovide an inference system that captures the patterns discussed, firstly in a\npurely propositional framework, then in a datalog (limited predicate)\nframework."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0089v1", 
    "title": "The Exact Closest String Problem as a Constraint Satisfaction Problem", 
    "arxiv-id": "1005.0089v1", 
    "author": "Lars Kotthoff", 
    "publish": "2010-05-01T16:00:59Z", 
    "summary": "We report (to our knowledge) the first evaluation of Constraint Satisfaction\nas a computational framework for solving closest string problems. We show that\ncareful consideration of symbol occurrences can provide search heuristics that\nprovide several orders of magnitude speedup at and above the optimal distance.\nWe also report (to our knowledge) the first analysis and evaluation -- using\nany technique -- of the computational difficulties involved in the\nidentification of all closest strings for a given input set. We describe\nalgorithms for web-scale distributed solution of closest string problems, both\npurely based on AI backtrack search and also hybrid numeric-AI methods."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0104v1", 
    "title": "Joint Structured Models for Extraction from Overlapping Sources", 
    "arxiv-id": "1005.0104v1", 
    "author": "Sunita Sarawagi", 
    "publish": "2010-05-01T20:55:23Z", 
    "summary": "We consider the problem of jointly training structured models for extraction\nfrom sources whose instances enjoy partial overlap. This has important\napplications like user-driven ad-hoc information extraction on the web. Such\napplications present new challenges in terms of the number of sources and their\narbitrary pattern of overlap not seen by earlier collective training schemes\napplied on two sources. We present an agreement-based learning framework and\nalternatives within it to trade-off tractability, robustness to noise, and\nextent of agreement. We provide a principled scheme to discover low-noise\nagreement sets in unlabeled data across the sources. Through extensive\nexperiments over 58 real datasets, we establish that our method of additively\nrewarding agreement over maximal segments of text provides the best trade-offs,\nand also scores over alternatives such as collective inference, staged\ntraining, and multi-view learning."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0605v1", 
    "title": "An approach to visualize the course of solving of a research task in   humans", 
    "arxiv-id": "1005.0605v1", 
    "author": "Rem G. Khlebopros", 
    "publish": "2010-04-26T11:00:24Z", 
    "summary": "A technique to study the dynamics of solving of a research task is suggested.\nThe research task was based on specially developed software Right- Wrong\nResponder (RWR), with the participants having to reveal the response logic of\nthe program. The participants interacted with the program in the form of a\nsemi-binary dialogue, which implies the feedback responses of only two kinds -\n\"right\" or \"wrong\". The technique has been applied to a small pilot group of\nvolunteer participants. Some of them have successfully solved the task\n(solvers) and some have not (non-solvers). In the beginning of the work, the\nsolvers did more wrong moves than non-solvers, and they did less wrong moves\ncloser to the finish of the work. A phase portrait of the work both in solvers\nand non-solvers showed definite cycles that may correspond to sequences of\npartially true hypotheses that may be formulated by the participants during the\nsolving of the task."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0608v1", 
    "title": "Informal Concepts in Machines", 
    "arxiv-id": "1005.0608v1", 
    "author": "Kurt Ammon", 
    "publish": "2010-05-04T19:00:37Z", 
    "summary": "This paper constructively proves the existence of an effective procedure\ngenerating a computable (total) function that is not contained in any given\neffectively enumerable set of such functions. The proof implies the existence\nof machines that process informal concepts such as computable (total) functions\nbeyond the limits of any given Turing machine or formal system, that is, these\nmachines can, in a certain sense, \"compute\" function values beyond these\nlimits. We call these machines creative. We argue that any \"intelligent\"\nmachine should be capable of processing informal concepts such as computable\n(total) functions, that is, it should be creative. Finally, we introduce\nhypotheses on creative machines which were developed on the basis of\ntheoretical investigations and experiments with computer programs. The\nhypotheses say that machine intelligence is the execution of a self-developing\nprocedure starting from any universal programming language and any input."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0896v1", 
    "title": "A two-step fusion process for multi-criteria decision applied to natural   hazards in mountains", 
    "arxiv-id": "1005.0896v1", 
    "author": "Jean Dezert", 
    "publish": "2010-05-06T06:32:59Z", 
    "summary": "Mountain river torrents and snow avalanches generate human and material\ndamages with dramatic consequences. Knowledge about natural phenomenona is\noften lacking and expertise is required for decision and risk management\npurposes using multi-disciplinary quantitative or qualitative approaches.\nExpertise is considered as a decision process based on imperfect information\ncoming from more or less reliable and conflicting sources. A methodology mixing\nthe Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and\ninformation fusion using Belief Function Theory is described. Fuzzy Sets and\nPossibilities theories allow to transform quantitative and qualitative criteria\ninto a common frame of discernment for decision in Dempster-Shafer Theory (DST\n) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic\nbelief assignments elicitation, conflict identification and management, fusion\nrule choices, results validation but also in specific needs to make a\ndifference between importance and reliability and uncertainty in the fusion\nprocess."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.0917v1", 
    "title": "On Building a Knowledge Base for Stability Theory", 
    "arxiv-id": "1005.0917v1", 
    "author": "Christoph Schwarzweller", 
    "publish": "2010-05-06T07:59:52Z", 
    "summary": "A lot of mathematical knowledge has been formalized and stored in\nrepositories by now: different mathematical theorems and theories have been\ntaken into consideration and included in mathematical repositories.\nApplications more distant from pure mathematics, however --- though based on\nthese theories --- often need more detailed knowledge about the underlying\ntheories. In this paper we present an example Mizar formalization from the area\nof electrical engineering focusing on stability theory which is based on\ncomplex analysis. We discuss what kind of special knowledge is necessary here\nand which amount of this knowledge is included in existing repositories."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.1518v1", 
    "title": "Recognizability of Individual Creative Style Within and Across Domains:   Preliminary Studies", 
    "arxiv-id": "1005.1518v1", 
    "author": "Liane Gabora", 
    "publish": "2010-05-10T12:30:36Z", 
    "summary": "It is hypothesized that creativity arises from the self-mending capacity of\nan internal model of the world, or worldview. The uniquely honed worldview of a\ncreative individual results in a distinctive style that is recognizable within\nand across domains. It is further hypothesized that creativity is domaingeneral\nin the sense that there exist multiple avenues by which the distinctiveness of\none's worldview can be expressed. These hypotheses were tested using art\nstudents and creative writing students. Art students guessed significantly\nabove chance both which painting was done by which of five famous artists, and\nwhich artwork was done by which of their peers. Similarly, creative writing\nstudents guessed significantly above chance both which passage was written by\nwhich of five famous writers, and which passage was written by which of their\npeers. These findings support the hypothesis that creative style is\nrecognizable. Moreover, creative writing students guessed significantly above\nchance which of their peers produced particular works of art, supporting the\nhypothesis that creative style is recognizable not just within but across\ndomains."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.1860v2", 
    "title": "Feature Selection Using Regularization in Approximate Linear Programs   for Markov Decision Processes", 
    "arxiv-id": "1005.1860v2", 
    "author": "Shlomo Zilberstein", 
    "publish": "2010-05-11T15:24:36Z", 
    "summary": "Approximate dynamic programming has been used successfully in a large variety\nof domains, but it relies on a small set of provided approximation features to\ncalculate solutions reliably. Large and rich sets of features can cause\nexisting algorithms to overfit because of a limited number of samples. We\naddress this shortcoming using $L_1$ regularization in approximate linear\nprogramming. Because the proposed method can automatically select the\nappropriate richness of features, its performance does not degrade with an\nincreasing number of features. These results rely on new and stronger sampling\nbounds for regularized approximate linear programs. We also propose a\ncomputationally efficient homotopy method. The empirical evaluation of the\napproach shows that the proposed method performs well on simple MDPs and\nstandard benchmark problems."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.2815v1", 
    "title": "Evolving Genes to Balance a Pole", 
    "arxiv-id": "1005.2815v1", 
    "author": "W. Banzhaf", 
    "publish": "2010-05-17T06:44:44Z", 
    "summary": "We discuss how to use a Genetic Regulatory Network as an evolutionary\nrepresentation to solve a typical GP reinforcement problem, the pole balancing.\nThe network is a modified version of an Artificial Regulatory Network proposed\na few years ago, and the task could be solved only by finding a proper way of\nconnecting inputs and outputs to the network. We show that the representation\nis able to generalize well over the problem domain, and discuss the performance\nof different models of this kind."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.3502v1", 
    "title": "Using machine learning to make constraint solver implementation   decisions", 
    "arxiv-id": "1005.3502v1", 
    "author": "Ian Miguel", 
    "publish": "2010-05-19T17:53:43Z", 
    "summary": "Programs to solve so-called constraint problems are complex pieces of\nsoftware which require many design decisions to be made more or less\narbitrarily by the implementer. These decisions affect the performance of the\nfinished solver significantly. Once a design decision has been made, it cannot\neasily be reversed, although a different decision may be more appropriate for a\nparticular problem.\n  We investigate using machine learning to make these decisions automatically\ndepending on the problem to solve with the alldifferent constraint as an\nexample. Our system is capable of making non-trivial, multi-level decisions\nthat improve over always making a default choice."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.4025v1", 
    "title": "A Soft Computing Model for Physicians' Decision Process", 
    "arxiv-id": "1005.4025v1", 
    "author": "Siddharths Sankar Biswas", 
    "publish": "2010-05-21T17:40:38Z", 
    "summary": "In this paper the author presents a kind of Soft Computing Technique, mainly\nan application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical\nExperts Systems. The choosen problem is on design of a physician's decision\nmodel which can take crisp as well as fuzzy data as input, unlike the\ntraditional models. The author presents a mathematical model based on fuzzy set\ntheory for physician aided evaluation of a complete representation of\ninformation emanating from the initial interview including patient past\nhistory, present symptoms, and signs observed upon physical examination and\nresults of clinical and diagnostic tests."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.4159v3", 
    "title": "The Complexity of Manipulating $k$-Approval Elections", 
    "arxiv-id": "1005.4159v3", 
    "author": "Andrew Lin", 
    "publish": "2010-05-23T00:04:11Z", 
    "summary": "An important problem in computational social choice theory is the complexity\nof undesirable behavior among agents, such as control, manipulation, and\nbribery in election systems. These kinds of voting strategies are often\ntempting at the individual level but disastrous for the agents as a whole.\nCreating election systems where the determination of such strategies is\ndifficult is thus an important goal.\n  An interesting set of elections is that of scoring protocols. Previous work\nin this area has demonstrated the complexity of misuse in cases involving a\nfixed number of candidates, and of specific election systems on unbounded\nnumber of candidates such as Borda. In contrast, we take the first step in\ngeneralizing the results of computational complexity of election misuse to\ncases of infinitely many scoring protocols on an unbounded number of\ncandidates. Interesting families of systems include $k$-approval and $k$-veto\nelections, in which voters distinguish $k$ candidates from the candidate set.\n  Our main result is to partition the problems of these families based on their\ncomplexity. We do so by showing they are polynomial-time computable, NP-hard,\nor polynomial-time equivalent to another problem of interest. We also\ndemonstrate a surprising connection between manipulation in election systems\nand some graph theory problems."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S147106841000027X", 
    "link": "http://arxiv.org/pdf/1005.4272v1", 
    "title": "Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of   Analystical Methodology", 
    "arxiv-id": "1005.4272v1", 
    "author": "R. Jagannathan", 
    "publish": "2010-05-24T07:50:55Z", 
    "summary": "In the last two decades, a number of methods have been proposed for\nforecasting based on fuzzy time series. Most of the fuzzy time series methods\nare presented for forecasting of car road accidents. However, the forecasting\naccuracy rates of the existing methods are not good enough. In this paper, we\ncompared our proposed new method of fuzzy time series forecasting with existing\nmethods. Our method is based on means based partitioning of the historical data\nof car road accidents. The proposed method belongs to the kth order and\ntime-variant methods. The proposed method can get the best forecasting accuracy\nrate for forecasting the car road accidents than the existing methods."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijnsa.2010.2202", 
    "link": "http://arxiv.org/pdf/1005.4496v1", 
    "title": "Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection", 
    "arxiv-id": "1005.4496v1", 
    "author": "Mohammad Zahidur Rahman", 
    "publish": "2010-05-25T07:47:00Z", 
    "summary": "In this paper, a new learning algorithm for adaptive network intrusion\ndetection using naive Bayesian classifier and decision tree is presented, which\nperforms balance detections and keeps false positives at acceptable level for\ndifferent types of network attacks, and eliminates redundant attributes as well\nas contradictory examples from training data that make the detection model\ncomplex. The proposed algorithm also addresses some difficulties of data mining\nsuch as handling continuous attribute, dealing with missing attribute values,\nand reducing noise in training data. Due to the large volumes of security audit\ndata as well as the complex and dynamic properties of intrusion behaviours,\nseveral data miningbased intrusion detection techniques have been applied to\nnetwork-based traffic data and host-based data in the last decades. However,\nthere remain various issues needed to be examined towards current intrusion\ndetection systems (IDS). We tested the performance of our proposed algorithm\nwith existing learning algorithms by employing on the KDD99 benchmark intrusion\ndetection dataset. The experimental results prove that the proposed algorithm\nachieved high detection rates (DR) and significant reduce false positives (FP)\nfor different types of network intrusions using limited computational\nresources."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1005.4592v1", 
    "title": "Automated Reasoning and Presentation Support for Formalizing Mathematics   in Mizar", 
    "arxiv-id": "1005.4592v1", 
    "author": "Geoff Sutcliffe", 
    "publish": "2010-05-25T14:49:03Z", 
    "summary": "This paper presents a combination of several automated reasoning and proof\npresentation tools with the Mizar system for formalization of mathematics. The\ncombination forms an online service called MizAR, similar to the SystemOnTPTP\nservice for first-order automated reasoning. The main differences to\nSystemOnTPTP are the use of the Mizar language that is oriented towards human\nmathematicians (rather than the pure first-order logic used in SystemOnTPTP),\nand setting the service in the context of the large Mizar Mathematical Library\nof previous theorems,definitions, and proofs (rather than the isolated problems\nthat are solved in SystemOnTPTP). These differences poses new challenges and\nnew opportunities for automated reasoning and for proof presentation tools.\nThis paper describes the overall structure of MizAR, and presents the automated\nreasoning systems and proof presentation tools that are combined to make MizAR\na useful mathematical service."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1005.4963v1", 
    "title": "Integrating Structured Metadata with Relational Affinity Propagation", 
    "arxiv-id": "1005.4963v1", 
    "author": "Lise Getoor", 
    "publish": "2010-05-26T23:13:05Z", 
    "summary": "Structured and semi-structured data describing entities, taxonomies and\nontologies appears in many domains. There is a huge interest in integrating\nstructured information from multiple sources; however integrating structured\ndata to infer complex common structures is a difficult task because the\nintegration must aggregate similar structures while avoiding structural\ninconsistencies that may appear when the data is combined. In this work, we\nstudy the integration of structured social metadata: shallow personal\nhierarchies specified by many individual users on the SocialWeb, and focus on\ninferring a collection of integrated, consistent taxonomies. We frame this task\nas an optimization problem with structural constraints. We propose a new\ninference algorithm, which we refer to as Relational Affinity Propagation (RAP)\nthat extends affinity propagation (Frey and Dueck 2007) by introducing\nstructural constraints. We validate the approach on a real-world social media\ndataset, collected from the photosharing website Flickr. Our empirical results\nshow that our proposed approach is able to construct deeper and denser\nstructures compared to an approach using only the standard affinity propagation\nalgorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1005.4989v1", 
    "title": "A Formalization of the Turing Test", 
    "arxiv-id": "1005.4989v1", 
    "author": "Evgeny Chutchev", 
    "publish": "2010-05-27T05:59:56Z", 
    "summary": "The paper offers a mathematical formalization of the Turing test. This\nformalization makes it possible to establish the conditions under which some\nTuring machine will pass the Turing test and the conditions under which every\nTuring machine (or every Turing machine of the special class) will fail the\nTuring test."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1005.5114v1", 
    "title": "Growing a Tree in the Forest: Constructing Folksonomies by Integrating   Structured Metadata", 
    "arxiv-id": "1005.5114v1", 
    "author": "Lise Getoor", 
    "publish": "2010-05-27T16:46:04Z", 
    "summary": "Many social Web sites allow users to annotate the content with descriptive\nmetadata, such as tags, and more recently to organize content hierarchically.\nThese types of structured metadata provide valuable evidence for learning how a\ncommunity organizes knowledge. For instance, we can aggregate many personal\nhierarchies into a common taxonomy, also known as a folksonomy, that will aid\nusers in visualizing and browsing social content, and also to help them in\norganizing their own content. However, learning from social metadata presents\nseveral challenges, since it is sparse, shallow, ambiguous, noisy, and\ninconsistent. We describe an approach to folksonomy learning based on\nrelational clustering, which exploits structured metadata contained in personal\nhierarchies. Our approach clusters similar hierarchies using their structure\nand tag statistics, then incrementally weaves them into a deeper, bushier tree.\nWe study folksonomy learning using social metadata extracted from the\nphoto-sharing site Flickr, and demonstrate that the proposed approach addresses\nthe challenges. Moreover, comparing to previous work, the approach produces\nlarger, more accurate folksonomies, and in addition, scales better."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1005.5270v1", 
    "title": "Symmetries of Symmetry Breaking Constraints", 
    "arxiv-id": "1005.5270v1", 
    "author": "Toby Walsh", 
    "publish": "2010-05-28T11:22:29Z", 
    "summary": "Symmetry is an important feature of many constraint programs. We show that\nany problem symmetry acting on a set of symmetry breaking constraints can be\nused to break symmetry. Different symmetries pick out different solutions in\neach symmetry class. This simple but powerful idea can be used in a number of\ndifferent ways. We describe one application within model restarts, a search\ntechnique designed to reduce the conflict between symmetry breaking and the\nbranching heuristic. In model restarts, we restart search periodically with a\nrandom symmetry of the symmetry breaking constraints. Experimental results show\nthat this symmetry breaking technique is effective in practice on some standard\nbenchmark problems."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1006.0274v1", 
    "title": "Learning Probabilistic Hierarchical Task Networks to Capture User   Preferences", 
    "arxiv-id": "1006.0274v1", 
    "author": "Sungwook Yoon", 
    "publish": "2010-06-02T01:33:11Z", 
    "summary": "We propose automatically learning probabilistic Hierarchical Task Networks\n(pHTNs) in order to capture a user's preferences on plans, by observing only\nthe user's behavior. HTNs are a common choice of representation for a variety\nof purposes in planning, including work on learning in planning. Our\ncontributions are (a) learning structure and (b) representing preferences. In\ncontrast, prior work employing HTNs considers learning method preconditions\n(instead of structure) and representing domain physics or search control\nknowledge (rather than preferences). Initially we will assume that the observed\ndistribution of plans is an accurate representation of user preference, and\nthen generalize to the situation where feasibility constraints frequently\nprevent the execution of preferred plans. In order to learn a distribution on\nplans we adapt an Expectation-Maximization (EM) technique from the discipline\nof (probabilistic) grammar induction, taking the perspective of task reductions\nas productions in a context-free grammar over primitive actions. To account for\nthe difference between the distributions of possible and preferred plans we\nsubsequently modify this core EM technique, in short, by rescaling its input."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1006.0385v1", 
    "title": "Brain-Like Stochastic Search: A Research Challenge and Funding   Opportunity", 
    "arxiv-id": "1006.0385v1", 
    "author": "Paul J. Werbos", 
    "publish": "2010-06-01T18:16:10Z", 
    "summary": "Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of\nutility functions U(u,A), where u is a vector of parameters or task\ndescriptors, maximize or minimize U with respect to u, using networks (Option\nNets) which input A and learn to generate good options u stochastically. This\npaper discusses why this is crucial to brain-like intelligence (an area funded\nby NSF) and to many applications, and discusses various possibilities for\nnetwork design and training. The appendix discusses recent research, relations\nto work on stochastic optimization in operations research, and relations to\nengineering-based approaches to understanding neocortex."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1006.0991v1", 
    "title": "Variational Program Inference", 
    "arxiv-id": "1006.0991v1", 
    "author": "Noam Shazeer", 
    "publish": "2010-06-04T20:55:04Z", 
    "summary": "We introduce a framework for representing a variety of interesting problems\nas inference over the execution of probabilistic model programs. We represent a\n\"solution\" to such a problem as a guide program which runs alongside the model\nprogram and influences the model program's random choices, leading the model\nprogram to sample from a different distribution than from its priors. Ideally\nthe guide program influences the model program to sample from the posteriors\ngiven the evidence. We show how the KL- divergence between the true posterior\ndistribution and the distribution induced by the guided model program can be\nefficiently estimated (up to an additive constant) by sampling multiple\nexecutions of the guided model program. In addition, we show how to use the\nguide program as a proposal distribution in importance sampling to\nstatistically prove lower bounds on the probability of the evidence and on the\nprobability of a hypothesis and the evidence. We can use the quotient of these\ntwo bounds as an estimate of the conditional probability of the hypothesis\ngiven the evidence. We thus turn the inference problem into a heuristic search\nfor better guide programs."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-14128-7_12", 
    "link": "http://arxiv.org/pdf/1006.1080v1", 
    "title": "The Dilated Triple", 
    "arxiv-id": "1006.1080v1", 
    "author": "Joshua Shinavier", 
    "publish": "2010-06-06T05:16:55Z", 
    "summary": "The basic unit of meaning on the Semantic Web is the RDF statement, or\ntriple, which combines a distinct subject, predicate and object to make a\ndefinite assertion about the world. A set of triples constitutes a graph, to\nwhich they give a collective meaning. It is upon this simple foundation that\nthe rich, complex knowledge structures of the Semantic Web are built. Yet the\nvery expressiveness of RDF, by inviting comparison with real-world knowledge,\nhighlights a fundamental shortcoming, in that RDF is limited to statements of\nabsolute fact, independent of the context in which a statement is asserted.\nThis is in stark contrast with the thoroughly context-sensitive nature of human\nthought. The model presented here provides a particularly simple means of\ncontextualizing an RDF triple by associating it with related statements in the\nsame graph. This approach, in combination with a notion of graph similarity, is\nsufficient to select only those statements from an RDF graph which are\nsubjectively most relevant to the context of the requesting process."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.1190v2", 
    "title": "Game Information System", 
    "arxiv-id": "1006.1190v2", 
    "author": "Spits Warnars H. L. H", 
    "publish": "2010-06-07T07:22:30Z", 
    "summary": "In this Information system age many organizations consider information system\nas their weapon to compete or gain competitive advantage or give the best\nservices for non profit organizations. Game Information System as combining\nInformation System and game is breakthrough to achieve organizations'\nperformance. The Game Information System will run the Information System with\ngame and how game can be implemented to run the Information System. Game is not\nonly for fun and entertainment, but will be a challenge to combine fun and\nentertainment with Information System. The Challenge to run the information\nsystem with entertainment, deliver the entertainment with information system\nall at once. Game information system can be implemented in many sectors as like\nthe information system itself but in difference's view. A view of game which\npeople can joy and happy and do their transaction as a fun things."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.1701v1", 
    "title": "Virtual information system on working area", 
    "arxiv-id": "1006.1701v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-09T04:08:07Z", 
    "summary": "In order to get strategic positioning for competition in business\norganization, the information system must be ahead in this information age\nwhere the information as one of the weapons to win the competition and in the\nright hand the information will become a right bullet. The information system\nwith the information technology support isn't enough if just only on internet\nor implemented with internet technology. The growth of information technology\nas tools for helping and making people easy to use must be accompanied by\nwanting to make fun and happy when they make contact with the information\ntechnology itself. Basically human like to play, since childhood human have\nbeen playing, free and happy and when human grow up they can't play as much as\nwhen human was in their childhood. We have to develop the information system\nwhich is not perform information system itself but can help human to explore\ntheir natural instinct for playing, making fun and happiness when they interact\nwith the information system. Virtual information system is the way to present\nplaying and having fun atmosphere on working area."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.1703v1", 
    "title": "Indonesian Earthquake Decision Support System", 
    "arxiv-id": "1006.1703v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-09T04:36:14Z", 
    "summary": "Earthquake DSS is an information technology environment which can be used by\ngovernment to sharpen, make faster and better the earthquake mitigation\ndecision. Earthquake DSS can be delivered as E-government which is not only for\ngovernment itself but in order to guarantee each citizen's rights for\neducation, training and information about earthquake and how to overcome the\nearthquake. Knowledge can be managed for future use and would become mining by\nsaving and maintain all the data and information about earthquake and\nearthquake mitigation in Indonesia. Using Web technology will enhance global\naccess and easy to use. Datawarehouse as unNormalized database for\nmultidimensional analysis will speed the query process and increase reports\nvariation. Link with other Disaster DSS in one national disaster DSS, link with\nother government information system and international will enhance the\nknowledge and sharpen the reports."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.2204v1", 
    "title": "MDPs with Unawareness", 
    "arxiv-id": "1006.2204v1", 
    "author": "Ashutosh Saxena", 
    "publish": "2010-06-11T06:18:27Z", 
    "summary": "Markov decision processes (MDPs) are widely used for modeling decision-making\nproblems in robotics, automated control, and economics. Traditional MDPs assume\nthat the decision maker (DM) knows all states and actions. However, this may\nnot be true in many situations of interest. We define a new framework, MDPs\nwith unawareness (MDPUs) to deal with the possibilities that a DM may not be\naware of all possible actions. We provide a complete characterization of when a\nDM can learn to play near-optimally in an MDPU, and give an algorithm that\nlearns to play near-optimally when it is possible to do so, as efficiently as\npossible. In particular, we characterize when a near-optimal solution can be\nfound in polynomial time."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.2743v1", 
    "title": "Global Optimization for Value Function Approximation", 
    "arxiv-id": "1006.2743v1", 
    "author": "Shlomo Zilberstein", 
    "publish": "2010-06-14T15:38:41Z", 
    "summary": "Existing value function approximation methods have been successfully used in\nmany applications, but they often lack useful a priori error bounds. We propose\na new approximate bilinear programming formulation of value function\napproximation, which employs global optimization. The formulation provides\nstrong a priori guarantees on both robust and expected policy loss by\nminimizing specific norms of the Bellman residual. Solving a bilinear program\noptimally is NP-hard, but this is unavoidable because the Bellman-residual\nminimization itself is NP-hard. We describe and analyze both optimal and\napproximate algorithms for solving bilinear programs. The analysis shows that\nthis algorithm offers a convergent generalization of approximate policy\niteration. We also briefly analyze the behavior of bilinear programming\nalgorithms under incomplete samples. Finally, we demonstrate that the proposed\napproach can consistently minimize the Bellman residual on simple benchmark\nproblems."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.3021v1", 
    "title": "A General Framework for Equivalences in Answer-Set Programming by   Countermodels in the Logic of Here-and-There", 
    "arxiv-id": "1006.3021v1", 
    "author": "Michael Fink", 
    "publish": "2010-06-15T16:04:54Z", 
    "summary": "Different notions of equivalence, such as the prominent notions of strong and\nuniform equivalence, have been studied in Answer-Set Programming, mainly for\nthe purpose of identifying programs that can serve as substitutes without\naltering the semantics, for instance in program optimization. Such semantic\ncomparisons are usually characterized by various selections of models in the\nlogic of Here-and-There (HT). For uniform equivalence however, correct\ncharacterizations in terms of HT-models can only be obtained for finite\ntheories, respectively programs. In this article, we show that a selection of\ncountermodels in HT captures uniform equivalence also for infinite theories.\nThis result is turned into coherent characterizations of the different notions\nof equivalence by countermodels, as well as by a mixture of HT-models and\ncountermodels (so-called equivalence interpretations). Moreover, we generalize\nthe so-called notion of relativized hyperequivalence for programs to\npropositional theories, and apply the same methodology in order to obtain a\nsemantic characterization which is amenable to infinite settings. This allows\nfor a lifting of the results to first-order theories under a very general\nsemantics given in terms of a quantified version of HT. We thus obtain a\ngeneral framework for the study of various notions of equivalence for theories\nunder answer-set semantics. Moreover, we prove an expedient property that\nallows for a simplified treatment of extended signatures, and provide further\nresults for non-ground logic programs. In particular, uniform equivalence\ncoincides under open and ordinary answer-set semantics, and for finite\nnon-ground programs under these semantics, also the usual characterization of\nuniform equivalence in terms of maximal and total HT-models of the grounding is\ncorrect, even for infinite domains, when corresponding ground programs are\ninfinite."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.4544v1", 
    "title": "Human Disease Diagnosis Using a Fuzzy Expert System", 
    "arxiv-id": "1006.4544v1", 
    "author": "Ahsan Raja Chowdhury", 
    "publish": "2010-06-23T15:02:19Z", 
    "summary": "Human disease diagnosis is a complicated process and requires high level of\nexpertise. Any attempt of developing a web-based expert system dealing with\nhuman disease diagnosis has to overcome various difficulties. This paper\ndescribes a project work aiming to develop a web-based fuzzy expert system for\ndiagnosing human diseases. Now a days fuzzy systems are being used successfully\nin an increasing number of application areas; they use linguistic rules to\ndescribe systems. This research project focuses on the research and development\nof a web-based clinical tool designed to improve the quality of the exchange of\nhealth information between health care professionals and patients.\nPractitioners can also use this web-based tool to corroborate diagnosis. The\nproposed system is experimented on various scenarios in order to evaluate it's\nperformance. In all the cases, proposed system exhibits satisfactory results."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.4551v1", 
    "title": "Vagueness of Linguistic variable", 
    "arxiv-id": "1006.4551v1", 
    "author": "Smita Rajpal", 
    "publish": "2010-06-23T15:09:04Z", 
    "summary": "In the area of computer science focusing on creating machines that can engage\non behaviors that humans consider intelligent. The ability to create\nintelligent machines has intrigued humans since ancient times and today with\nthe advent of the computer and 50 years of research into various programming\ntechniques, the dream of smart machines is becoming a reality. Researchers are\ncreating systems which can mimic human thought, understand speech, beat the\nbest human chessplayer, and countless other feats never before possible.\nAbility of the human to estimate the information is most brightly shown in\nusing of natural languages. Using words of a natural language for valuation\nqualitative attributes, for example, the person pawns uncertainty in form of\nvagueness in itself estimations. Vague sets, vague judgments, vague conclusions\ntakes place there and then, where and when the reasonable subject exists and\nalso is interested in something. The vague sets theory has arisen as the answer\nto an illegibility of language the reasonable subject speaks. Language of a\nreasonable subject is generated by vague events which are created by the reason\nand which are operated by the mind. The theory of vague sets represents an\nattempt to find such approximation of vague grouping which would be more\nconvenient, than the classical theory of sets in situations where the natural\nlanguage plays a significant role. Such theory has been offered by known\nAmerican mathematician Gau and Buehrer .In our paper we are describing how\nvagueness of linguistic variables can be solved by using the vague set\ntheory.This paper is mainly designed for one of directions of the eventology\n(the theory of the random vague events), which has arisen within the limits of\nthe probability theory and which pursue the unique purpose to describe\neventologically a movement of reason."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.4561v1", 
    "title": "An Efficient Technique for Similarity Identification between Ontologies", 
    "arxiv-id": "1006.4561v1", 
    "author": "Abad Shah", 
    "publish": "2010-06-23T15:17:01Z", 
    "summary": "Ontologies usually suffer from the semantic heterogeneity when simultaneously\nused in information sharing, merging, integrating and querying processes.\nTherefore, the similarity identification between ontologies being used becomes\na mandatory task for all these processes to handle the problem of semantic\nheterogeneity. In this paper, we propose an efficient technique for similarity\nmeasurement between two ontologies. The proposed technique identifies all\ncandidate pairs of similar concepts without omitting any similar pair. The\nproposed technique can be used in different types of operations on ontologies\nsuch as merging, mapping and aligning. By analyzing its results a reasonable\nimprovement in terms of completeness, correctness and overall quality of the\nresults has been found."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.4563v1", 
    "title": "The State of the Art: Ontology Web-Based Languages: XML Based", 
    "arxiv-id": "1006.4563v1", 
    "author": "Mohammad Mustafa Taye", 
    "publish": "2010-06-23T15:18:33Z", 
    "summary": "Many formal languages have been proposed to express or represent Ontologies,\nincluding RDF, RDFS, DAML+OIL and OWL. Most of these languages are based on XML\nsyntax, but with various terminologies and expressiveness. Therefore, choosing\na language for building an Ontology is the main step. The main point of\nchoosing language to represent Ontology is based mainly on what the Ontology\nwill represent or be used for. That language should have a range of quality\nsupport features such as ease of use, expressive power, compatibility, sharing\nand versioning, internationalisation. This is because different kinds of\nknowledge-based applications need different language features. The main\nobjective of these languages is to add semantics to the existing information on\nthe web. The aims of this paper is to provide a good knowledge of existing\nlanguage and understanding of these languages and how could be used."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.4567v1", 
    "title": "Understanding Semantic Web and Ontologies: Theory and Applications", 
    "arxiv-id": "1006.4567v1", 
    "author": "Mohammad Mustafa Taye", 
    "publish": "2010-06-23T15:20:02Z", 
    "summary": "Semantic Web is actually an extension of the current one in that it\nrepresents information more meaningfully for humans and computers alike. It\nenables the description of contents and services in machine-readable form, and\nenables annotating, discovering, publishing, advertising and composing services\nto be automated. It was developed based on Ontology, which is considered as the\nbackbone of the Semantic Web. In other words, the current Web is transformed\nfrom being machine-readable to machine-understandable. In fact, Ontology is a\nkey technique with which to annotate semantics and provide a common,\ncomprehensible foundation for resources on the Semantic Web. Moreover, Ontology\ncan provide a common vocabulary, a grammar for publishing data, and can supply\na semantic description of data which can be used to preserve the Ontologies and\nkeep them ready for inference. This paper provides basic concepts of web\nservices and the Semantic Web, defines the structure and the main applications\nof ontology, and provides many relevant terms are explained in order to provide\na basic understanding of ontologies."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.5041v1", 
    "title": "GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables", 
    "arxiv-id": "1006.5041v1", 
    "author": "Takashi Washio", 
    "publish": "2010-06-24T13:09:36Z", 
    "summary": "Finding the structure of a graphical model has been received much attention\nin many fields. Recently, it is reported that the non-Gaussianity of data\nenables us to identify the structure of a directed acyclic graph without any\nprior knowledge on the structure. In this paper, we propose a novel\nnon-Gaussianity based algorithm for more general type of models; chain graphs.\nThe algorithm finds an ordering of the disjoint subsets of variables by\niteratively evaluating the independence between the variable subset and the\nresiduals when the remaining variables are regressed on those. However, its\ncomputational cost grows exponentially according to the number of variables.\nTherefore, we further discuss an efficient approximate approach for applying\nthe algorithm to large sized graphs. We illustrate the algorithm with\nartificial and real-world datasets."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.5511v2", 
    "title": "Soft Approximations and uni-int Decision Making", 
    "arxiv-id": "1006.5511v2", 
    "author": "Athar Kharal", 
    "publish": "2010-06-29T06:58:35Z", 
    "summary": "Notions of core, support and inversion of a soft set have been defined and\nstudied. Soft approximations are soft sets developed through core and support,\nand are used for granulating the soft space. Membership structure of a soft set\nhas been probed in and many interesting properties presented. The mathematical\napparatus developed so far in this paper yields a detailed analysis of two\nworks viz. [N. Cagman, S. Enginoglu, Soft set theory and uni-int decision\nmaking, European Jr. of Operational Research (article in press, available\nonline 12 May 2010)] and [N. Cagman, S. Enginoglu, Soft matrix theory and its\ndecision making, Computers and Mathematics with Applications 59 (2010) 3308 -\n3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a\ncore-support expression which is computationally far less expansive than\nuni-int. This also highlights some shortcomings in Cagman's uni-int method and\nthus motivates us to improve the method. We first suggest an improvement in\nuni-int method and then present a new conjecture to solve the optimum choice\nproblem given by Cagman and Enginoglu. Our Example 8.6 presents a case where\nthe optimum choice is intuitively clear yet both uni-int methods (Cagman's and\nour improved one) give wrong answer but the new conjecture solves the problem\ncorrectly."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1006.5657v1", 
    "title": "Reasoning Support for Risk Prediction and Prevention in Independent   Living", 
    "arxiv-id": "1006.5657v1", 
    "author": "R. Bisiani", 
    "publish": "2010-06-29T15:49:54Z", 
    "summary": "In recent years there has been growing interest in solutions for the delivery\nof clinical care for the elderly, due to the large increase in aging\npopulation. Monitoring a patient in his home environment is necessary to ensure\ncontinuity of care in home settings, but, to be useful, this activity must not\nbe too invasive for patients and a burden for caregivers. We prototyped a\nsystem called SINDI (Secure and INDependent lIving), focused on i) collecting a\nlimited amount of data about the person and the environment through Wireless\nSensor Networks (WSN), and ii) inferring from these data enough information to\nsupport caregivers in understanding patients' well being and in predicting\npossible evolutions of their health. Our hierarchical logic-based model of\nhealth combines data from different sources, sensor data, tests results,\ncommon-sense knowledge and patient's clinical profile at the lower level, and\ncorrelation rules between health conditions across upper levels. The logical\nformalization and the reasoning process are based on Answer Set Programming.\nThe expressive power of this logic programming paradigm makes it possible to\nreason about health evolution even when the available information is incomplete\nand potentially incoherent, while declarativity simplifies rules specification\nby caregivers and allows automatic encoding of knowledge. This paper describes\nhow these issues have been targeted in the application scenario of the SINDI\nsystem."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1008.0273v1", 
    "title": "Threat assessment of a possible Vehicle-Born Improvised Explosive Device   using DSmT", 
    "arxiv-id": "1008.0273v1", 
    "author": "Florentin Smarandache", 
    "publish": "2010-08-02T10:18:24Z", 
    "summary": "This paper presents the solution about the threat of a VBIED (Vehicle-Born\nImprovised Explosive Device) obtained with the DSmT (Dezert-Smarandache\nTheory). This problem has been proposed recently to the authors by Simon\nMaskell and John Lavery as a typical illustrative example to try to compare the\ndifferent approaches for dealing with uncertainty for decision-making support.\nThe purpose of this paper is to show in details how a solid justified solution\ncan be obtained from DSmT approach and its fusion rules thanks to a proper\nmodeling of the belief functions involved in this problem."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsit.2010.2310", 
    "link": "http://arxiv.org/pdf/1008.0659v2", 
    "title": "Evaluating and Improving Modern Variable and Revision Ordering   Strategies in CSPs", 
    "arxiv-id": "1008.0659v2", 
    "author": "Kostas Stergiou", 
    "publish": "2010-08-03T21:09:43Z", 
    "summary": "A key factor that can dramatically reduce the search space during constraint\nsolving is the criterion under which the variable to be instantiated next is\nselected. For this purpose numerous heuristics have been proposed. Some of the\nbest of such heuristics exploit information about failures gathered throughout\nsearch and recorded in the form of constraint weights, while others measure the\nimportance of variable assignments in reducing the search space. In this work\nwe experimentally evaluate the most recent and powerful variable ordering\nheuristics, and new variants of them, over a wide range of benchmarks. Results\ndemonstrate that heuristics based on failures are in general more efficient.\nBased on this, we then derive new revision ordering heuristics that exploit\nrecorded failures to efficiently order the propagation list when arc\nconsistency is maintained during search. Interestingly, in addition to reducing\nthe number of constraint checks and list operations, these heuristics are also\nable to cut down the size of the explored search tree."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.0660v1", 
    "title": "Adaptive Branching for Constraint Satisfaction Problems", 
    "arxiv-id": "1008.0660v1", 
    "author": "Kostas Stergiou", 
    "publish": "2010-08-03T21:10:15Z", 
    "summary": "The two standard branching schemes for CSPs are d-way and 2-way branching.\nAlthough it has been shown that in theory the latter can be exponentially more\neffective than the former, there is a lack of empirical evidence showing such\ndifferences. To investigate this, we initially make an experimental comparison\nof the two branching schemes over a wide range of benchmarks. Experimental\nresults verify the theoretical gap between d-way and 2-way branching as we move\nfrom a simple variable ordering heuristic like dom to more sophisticated ones\nlike dom/ddeg. However, perhaps surprisingly, experiments also show that when\nstate-of-the-art variable ordering heuristics like dom/wdeg are used then d-way\ncan be clearly more efficient than 2-way branching in many cases. Motivated by\nthis observation, we develop two generic heuristics that can be applied at\ncertain points during search to decide whether 2-way branching or a restricted\nversion of 2-way branching, which is close to d-way branching, will be\nfollowed. The application of these heuristics results in an adaptive branching\nscheme. Experiments with instantiations of the two generic heuristics confirm\nthat search with adaptive branching outperforms search with a fixed branching\nscheme on a wide range of problems."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.0823v1", 
    "title": "A Homogeneous Reaction Rule Language for Complex Event Processing", 
    "arxiv-id": "1008.0823v1", 
    "author": "Harold Boley", 
    "publish": "2010-08-04T17:05:33Z", 
    "summary": "Event-driven automation of reactive functionalities for complex event\nprocessing is an urgent need in today's distributed service-oriented\narchitectures and Web-based event-driven environments. An important problem to\nbe addressed is how to correctly and efficiently capture and process the\nevent-based behavioral, reactive logic embodied in reaction rules, and\ncombining this with other conditional decision logic embodied, e.g., in\nderivation rules. This paper elaborates a homogeneous integration approach that\ncombines derivation rules, reaction rules and other rule types such as\nintegrity constraints into the general framework of logic programming, the\nindustrial-strength version of declarative programming. We describe syntax and\nsemantics of the language, implement a distributed web-based middleware using\nenterprise service technologies and illustrate its adequacy in terms of\nexpressiveness, efficiency and scalability through examples extracted from\nindustrial use cases. The developed reaction rule language provides expressive\nfeatures such as modular ID-based updates with support for external imports and\nself-updates of the intensional and extensional knowledge bases, transactions\nincluding integrity testing and roll-backs of update transition paths. It also\nsupports distributed complex event processing, event messaging and event\nquerying via efficient and scalable enterprise middleware technologies and\nevent/action reasoning based on an event/action algebra implemented by an\ninterval-based event calculus variant as a logic inference formalism."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.1328v1", 
    "title": "Semantic Oriented Agent based Approach towards Engineering Data   Management, Web Information Retrieval and User System Communication Problems", 
    "arxiv-id": "1008.1328v1", 
    "author": "Detlef Gerhard", 
    "publish": "2010-08-07T12:08:43Z", 
    "summary": "The four intensive problems to the software rose by the software industry\n.i.e., User System Communication / Human Machine Interface, Meta Data\nextraction, Information processing & management and Data representation are\ndiscussed in this research paper. To contribute in the field we have proposed\nand described an intelligent semantic oriented agent based search engine\nincluding the concepts of intelligent graphical user interface, natural\nlanguage based information processing, data management and data reconstruction\nfor the final user end information representation."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.1333v1", 
    "title": "An Agent based Approach towards Metadata Extraction, Modelling and   Information Retrieval over the Web", 
    "arxiv-id": "1008.1333v1", 
    "author": "Detlef Gerhard", 
    "publish": "2010-08-07T12:29:02Z", 
    "summary": "Web development is a challenging research area for its creativity and\ncomplexity. The existing raised key challenge in web technology technologic\ndevelopment is the presentation of data in machine read and process able format\nto take advantage in knowledge based information extraction and maintenance.\nCurrently it is not possible to search and extract optimized results using full\ntext queries because there is no such mechanism exists which can fully extract\nthe semantic from full text queries and then look for particular knowledge\nbased information."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.1484v1", 
    "title": "A note on communicating between information systems based on including   degrees", 
    "arxiv-id": "1008.1484v1", 
    "author": "Qiaoyan Wen", 
    "publish": "2010-08-09T10:54:54Z", 
    "summary": "In order to study the communication between information systems, Gong and\nXiao [Z. Gong and Z. Xiao, Communicating between information systems based on\nincluding degrees, International Journal of General Systems 39 (2010) 189--206]\nproposed the concept of general relation mappings based on including degrees.\nSome properties and the extension for fuzzy information systems of the general\nrelation mappings have been investigated there. In this paper, we point out by\ncounterexamples that several assertions (Lemma 3.1, Lemma 3.2, Theorem 4.1, and\nTheorem 4.3) in the aforementioned work are not true in general."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.1723v1", 
    "title": "Role of Ontology in Semantic Web Development", 
    "arxiv-id": "1008.1723v1", 
    "author": "Detlef Gerhard", 
    "publish": "2010-08-07T12:32:22Z", 
    "summary": "World Wide Web (WWW) is the most popular global information sharing and\ncommunication system consisting of three standards .i.e., Uniform Resource\nIdentifier (URL), Hypertext Transfer Protocol (HTTP) and Hypertext Mark-up\nLanguage (HTML). Information is provided in text, image, audio and video\nformats over the web by using HTML which is considered to be unconventional in\ndefining and formalizing the meaning of the context..."
},{
    "category": "cs.AI", 
    "doi": "10.3233/978-1-60750-606-5-855", 
    "link": "http://arxiv.org/pdf/1008.3314v1", 
    "title": "Maximum entropy models and subjective interestingness: an application to   tiles in binary databases", 
    "arxiv-id": "1008.3314v1", 
    "author": "Tijl De Bie", 
    "publish": "2010-08-19T14:41:55Z", 
    "summary": "Recent research has highlighted the practical benefits of subjective\ninterestingness measures, which quantify the novelty or unexpectedness of a\npattern when contrasted with any prior information of the data miner\n(Silberschatz and Tuzhilin, 1995; Geng and Hamilton, 2006). A key challenge\nhere is the formalization of this prior information in a way that lends itself\nto the definition of an interestingness subjective measure that is both\nmeaningful and practical.\n  In this paper, we outline a general strategy of how this could be achieved,\nbefore working out the details for a use case that is important in its own\nright.\n  Our general strategy is based on considering prior information as constraints\non a probabilistic model representing the uncertainty about the data. More\nspecifically, we represent the prior information by the maximum entropy\n(MaxEnt) distribution subject to these constraints. We briefly outline various\nmeasures that could subsequently be used to contrast patterns with this MaxEnt\nmodel, thus quantifying their subjective interestingness."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.3879v1", 
    "title": "A formalism for causal explanations with an Answer Set Programming   translation", 
    "arxiv-id": "1008.3879v1", 
    "author": "Yves Moinard", 
    "publish": "2010-08-23T18:38:23Z", 
    "summary": "We examine the practicality for a user of using Answer Set Programming (ASP)\nfor representing logical formalisms. Our example is a formalism aiming at\ncapturing causal explanations from causal information. We show the naturalness\nand relative efficiency of this translation job. We are interested in the ease\nfor writing an ASP program. Limitations of the earlier systems made that in\npractice, the ``declarative aspect'' was more theoretical than practical. We\nshow how recent improvements in working ASP systems facilitate the translation."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.4257v1", 
    "title": "Learning from Profession Knowledge: Application on Knitting", 
    "arxiv-id": "1008.4257v1", 
    "author": "Oswaldo Castillo", 
    "publish": "2010-08-25T11:41:28Z", 
    "summary": "Knowledge Management is a global process in companies. It includes all the\nprocesses that allow capitalization, sharing and evolution of the Knowledge\nCapital of the firm, generally recognized as a critical resource of the\norganization. Several approaches have been defined to capitalize knowledge but\nfew of them study how to learn from this knowledge. We present in this paper an\napproach that helps to enhance learning from profession knowledge in an\norganisation. We apply our approach on knitting industry."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.4326v1", 
    "title": "Machine learning for constraint solver design -- A case study for the   alldifferent constraint", 
    "arxiv-id": "1008.4326v1", 
    "author": "Peter Nightingale", 
    "publish": "2010-08-25T18:04:03Z", 
    "summary": "Constraint solvers are complex pieces of software which require many design\ndecisions to be made by the implementer based on limited information. These\ndecisions affect the performance of the finished solver significantly. Once a\ndesign decision has been made, it cannot easily be reversed, although a\ndifferent decision may be more appropriate for a particular problem.\n  We investigate using machine learning to make these decisions automatically\ndepending on the problem to solve. We use the alldifferent constraint as a case\nstudy. Our system is capable of making non-trivial, multi-level decisions that\nimprove over always making a default choice and can be implemented as part of a\ngeneral-purpose constraint solver."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.4328v1", 
    "title": "Distributed solving through model splitting", 
    "arxiv-id": "1008.4328v1", 
    "author": "Neil C. A. Moore", 
    "publish": "2010-08-25T18:07:40Z", 
    "summary": "Constraint problems can be trivially solved in parallel by exploring\ndifferent branches of the search tree concurrently. Previous approaches have\nfocused on implementing this functionality in the solver, more or less\ntransparently to the user. We propose a new approach, which modifies the\nconstraint model of the problem. An existing model is split into new models\nwith added constraints that partition the search space. Optionally, additional\nconstraints are imposed that rule out the search already done. The advantages\nof our approach are that it can be implemented easily, computations can be\nstopped and restarted, moved to different machines and indeed solved on\nmachines which are not able to communicate with each other at all."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.5163v1", 
    "title": "Learning Multi-modal Similarity", 
    "arxiv-id": "1008.5163v1", 
    "author": "Gert Lanckriet", 
    "publish": "2010-08-30T20:51:26Z", 
    "summary": "In many applications involving multi-media data, the definition of similarity\nbetween items is integral to several key tasks, e.g., nearest-neighbor\nretrieval, classification, and recommendation. Data in such regimes typically\nexhibits multiple modalities, such as acoustic and visual content of video.\nIntegrating such heterogeneous data to form a holistic similarity space is\ntherefore a key challenge to be overcome in many real-world applications.\n  We present a novel multiple kernel learning technique for integrating\nheterogeneous data into a single, unified similarity space. Our algorithm\nlearns an optimal ensemble of kernel transfor- mations which conform to\nmeasurements of human perceptual similarity, as expressed by relative\ncomparisons. To cope with the ubiquitous problems of subjectivity and\ninconsistency in multi- media similarity, we develop graph-based techniques to\nfilter similarity measurements, resulting in a simplified and robust training\nprocedure."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.5188v2", 
    "title": "Totally Corrective Boosting for Regularized Risk Minimization", 
    "arxiv-id": "1008.5188v2", 
    "author": "Nick Barnes", 
    "publish": "2010-08-30T23:40:51Z", 
    "summary": "Consideration of the primal and dual problems together leads to important new\ninsights into the characteristics of boosting algorithms. In this work, we\npropose a general framework that can be used to design new boosting algorithms.\nA wide variety of machine learning problems essentially minimize a regularized\nrisk functional. We show that the proposed boosting framework, termed CGBoost,\ncan accommodate various loss functions and different regularizers in a\ntotally-corrective optimization fashion. We show that, by solving the primal\nrather than the dual, a large body of totally-corrective boosting algorithms\ncan actually be efficiently solved and no sophisticated convex optimization\nsolvers are needed. We also demonstrate that some boosting algorithms like\nAdaBoost can be interpreted in our framework--even their optimization is not\ntotally corrective. We empirically show that various boosting algorithms based\non the proposed framework perform similarly on the UCIrvine machine learning\ndatasets [1] that we have used in the experiments."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1008.5189v1", 
    "title": "Improving the Performance of maxRPC", 
    "arxiv-id": "1008.5189v1", 
    "author": "Toby Walsh", 
    "publish": "2010-08-30T23:50:33Z", 
    "summary": "Max Restricted Path Consistency (maxRPC) is a local consistency for binary\nconstraints that can achieve considerably stronger pruning than arc\nconsistency. However, existing maxRRC algorithms suffer from overheads and\nredundancies as they can repeatedly perform many constraint checks without\ntriggering any value deletions. In this paper we propose techniques that can\nboost the performance of maxRPC algorithms. These include the combined use of\ntwo data structures to avoid many redundant constraint checks, and heuristics\nfor the efficient ordering and execution of certain operations. Based on these,\nwe propose two closely related algorithms. The first one which is a maxRPC\nalgorithm with optimal O(end^3) time complexity, displays good performance when\nused stand-alone, but is expensive to apply during search. The second one\napproximates maxRPC and has O(en^2d^4) time complexity, but a restricted\nversion with O(end^4) complexity can be very efficient when used during search.\nBoth algorithms have O(ed) space complexity. Experimental results demonstrate\nthat the resulting methods constantly outperform previous algorithms for\nmaxRPC, often by large margins, and constitute a more than viable alternative\nto arc consistency on many problems."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.0347v1", 
    "title": "Solving the Resource Constrained Project Scheduling Problem with   Generalized Precedences by Lazy Clause Generation", 
    "arxiv-id": "1009.0347v1", 
    "author": "Mark G. Wallace", 
    "publish": "2010-09-02T08:03:47Z", 
    "summary": "The technical report presents a generic exact solution approach for\nminimizing the project duration of the resource-constrained project scheduling\nproblem with generalized precedences (Rcpsp/max). The approach uses lazy clause\ngeneration, i.e., a hybrid of finite domain and Boolean satisfiability solving,\nin order to apply nogood learning and conflict-driven search on the solution\ngeneration. Our experiments show the benefit of lazy clause generation for\nfinding an optimal solutions and proving its optimality in comparison to other\nstate-of-the-art exact and non-exact methods. The method is highly robust: it\nmatched or bettered the best known results on all of the 2340 instances we\nexamined except 3, according to the currently available data on the PSPLib. Of\nthe 631 open instances in this set it closed 573 and improved the bounds of 51\nof the remaining 58 instances."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.0407v1", 
    "title": "Experimental Evaluation of Branching Schemes for the CSP", 
    "arxiv-id": "1009.0407v1", 
    "author": "Kostas Stergiou", 
    "publish": "2010-09-02T12:24:32Z", 
    "summary": "The search strategy of a CP solver is determined by the variable and value\nordering heuristics it employs and by the branching scheme it follows. Although\nthe effects of variable and value ordering heuristics on search effort have\nbeen widely studied, the effects of different branching schemes have received\nless attention. In this paper we study this effect through an experimental\nevaluation that includes standard branching schemes such as 2-way, d-way, and\ndichotomic domain splitting, as well as variations of set branching where\nbranching is performed on sets of values. We also propose and evaluate a\ngeneric approach to set branching where the partition of a domain into sets is\ncreated using the scores assigned to values by a value ordering heuristic, and\na clustering algorithm from machine learning. Experimental results demonstrate\nthat although exponential differences between branching schemes, as predicted\nin theory between 2-way and d-way branching, are not very common, still the\nchoice of branching scheme can make quite a difference on certain classes of\nproblems. Set branching methods are very competitive with 2-way branching and\noutperform it on some problem classes. A statistical analysis of the results\nreveals that our generic clustering-based set branching method is the best\namong the methods compared."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.0451v1", 
    "title": "The Challenge of Believability in Video Games: Definitions, Agents   Models and Imitation Learning", 
    "arxiv-id": "1009.0451v1", 
    "author": "Olivier Marc", 
    "publish": "2010-09-02T15:25:06Z", 
    "summary": "In this paper, we address the problem of creating believable agents (virtual\ncharacters) in video games. We consider only one meaning of believability,\n``giving the feeling of being controlled by a player'', and outline the problem\nof its evaluation. We present several models for agents in games which can\nproduce believable behaviours, both from industry and research. For high level\nof believability, learning and especially imitation learning seems to be the\nway to go. We make a quick overview of different approaches to make video\ngames' agents learn from players. To conclude we propose a two-step method to\ndevelop new models for believable agents. First we must find the criteria for\nbelievability for our application and define an evaluation method. Then the\nmodel and the learning algorithm can be designed."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.0501v1", 
    "title": "Automatable Evaluation Method Oriented toward Behaviour Believability   for Video Games", 
    "arxiv-id": "1009.0501v1", 
    "author": "C\u00e9dric Buche", 
    "publish": "2010-09-02T18:36:44Z", 
    "summary": "Classic evaluation methods of believable agents are time-consuming because\nthey involve many human to judge agents. They are well suited to validate work\non new believable behaviours models. However, during the implementation,\nnumerous experiments can help to improve agents' believability. We propose a\nmethod which aim at assessing how much an agent's behaviour looks like humans'\nbehaviours. By representing behaviours with vectors, we can store data computed\nfor humans and then evaluate as many agents as needed without further need of\nhumans. We present a test experiment which shows that even a simple evaluation\nfollowing our method can reveal differences between quite believable agents and\nhumans. This method seems promising although, as shown in our experiment,\nresults' analysis can be difficult."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.2003v1", 
    "title": "AI 3D Cybug Gaming", 
    "arxiv-id": "1009.2003v1", 
    "author": "Zeeshan Ahmed", 
    "publish": "2010-09-10T12:58:30Z", 
    "summary": "In this short paper I briefly discuss 3D war Game based on artificial\nintelligence concepts called AI WAR. Going in to the details, I present the\nimportance of CAICL language and how this language is used in AI WAR. Moreover\nI also present a designed and implemented 3D War Cybug for AI WAR using CAICL\nand discus the implemented strategy to defeat its enemies during the game life."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.2041v1", 
    "title": "Multi-Agent Only-Knowing Revisited", 
    "arxiv-id": "1009.2041v1", 
    "author": "Gerhard Lakemeyer", 
    "publish": "2010-09-10T16:10:26Z", 
    "summary": "Levesque introduced the notion of only-knowing to precisely capture the\nbeliefs of a knowledge base. He also showed how only-knowing can be used to\nformalize non-monotonic behavior within a monotonic logic. Despite its appeal,\nall attempts to extend only-knowing to the many agent case have undesirable\nproperties. A belief model by Halpern and Lakemeyer, for instance, appeals to\nproof-theoretic constructs in the semantics and needs to axiomatize validity as\npart of the logic. It is also not clear how to generalize their ideas to a\nfirst-order case. In this paper, we propose a new account of multi-agent\nonly-knowing which, for the first time, has a natural possible-world semantics\nfor a quantified language with equality. We then provide, for the propositional\nfragment, a sound and complete axiomatization that faithfully lifts Levesque's\nproof theory to the many agent case. We also discuss comparisons to the earlier\napproach by Halpern and Lakemeyer."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.4586v1", 
    "title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining", 
    "arxiv-id": "1009.4586v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-23T11:42:41Z", 
    "summary": "In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.4982v1", 
    "title": "Optimal Bangla Keyboard Layout using Data Mining Technique", 
    "arxiv-id": "1009.4982v1", 
    "author": "Md. Mahadi Hassan", 
    "publish": "2010-09-25T06:55:27Z", 
    "summary": "This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.5048v1", 
    "title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique", 
    "arxiv-id": "1009.5048v1", 
    "author": "S. M. Kamruzzaman", 
    "publish": "2010-09-26T02:09:41Z", 
    "summary": "Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.5268v1", 
    "title": "General Scaled Support Vector Machines", 
    "arxiv-id": "1009.5268v1", 
    "author": "Forrest Sheng Bao", 
    "publish": "2010-09-27T14:27:49Z", 
    "summary": "Support Vector Machines (SVMs) are popular tools for data mining tasks such\nas classification, regression, and density estimation. However, original SVM\n(C-SVM) only considers local information of data points on or over the margin.\nTherefore, C-SVM loses robustness. To solve this problem, one approach is to\ntranslate (i.e., to move without rotation or change of shape) the hyperplane\naccording to the distribution of the entire data. But existing work can only be\napplied for 1-D case. In this paper, we propose a simple and efficient method\ncalled General Scaled SVM (GS-SVM) to extend the existing approach to\nmulti-dimensional case. Our method translates the hyperplane according to the\ndistribution of data projected on the normal vector of the hyperplane. Compared\nwith C-SVM, GS-SVM has better performance on several data sets."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1009.5290v1", 
    "title": "Measuring Similarity of Graphs and their Nodes by Neighbor Matching", 
    "arxiv-id": "1009.5290v1", 
    "author": "Mladen Nikolic", 
    "publish": "2010-09-27T15:31:54Z", 
    "summary": "The problem of measuring similarity of graphs and their nodes is important in\na range of practical problems. There is a number of proposed measures, some of\nthem being based on iterative calculation of similarity between two graphs and\nthe principle that two nodes are as similar as their neighbors are. In our\nwork, we propose one novel method of that sort, with a refined concept of\nsimilarity of two nodes that involves matching of their neighbors. We prove\nconvergence of the proposed method and show that it has some additional\ndesirable properties that, to our knowledge, the existing methods lack. We\nillustrate the method on two specific problems and empirically compare it to\nother methods."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.0298v1", 
    "title": "Steepest Ascent Hill Climbing For A Mathematical Problem", 
    "arxiv-id": "1010.0298v1", 
    "author": "Mukund Sanglikar", 
    "publish": "2010-10-02T07:27:43Z", 
    "summary": "The paper proposes artificial intelligence technique called hill climbing to\nfind numerical solutions of Diophantine Equations. Such equations are important\nas they have many applications in fields like public key cryptography, integer\nfactorization, algebraic curves, projective curves and data dependency in super\ncomputers. Importantly, it has been proved that there is no general method to\nfind solutions of such equations. This paper is an attempt to find numerical\nsolutions of Diophantine equations using steepest ascent version of Hill\nClimbing. The method, which uses tree representation to depict possible\nsolutions of Diophantine equations, adopts a novel methodology to generate\nsuccessors. The heuristic function used help to make the process of finding\nsolution as a minimization process. The work illustrates the effectiveness of\nthe proposed methodology using a class of Diophantine equations given by a1. x1\np1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The\nexperimental results validate that the procedure proposed is successful in\nfinding solutions of Diophantine Equations with sufficiently large powers and\nlarge number of variables."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.2102v1", 
    "title": "Hierarchical Multiclass Decompositions with Application to Authorship   Determination", 
    "arxiv-id": "1010.2102v1", 
    "author": "Noam Etzion-Rosenberg", 
    "publish": "2010-10-11T13:41:21Z", 
    "summary": "This paper is mainly concerned with the question of how to decompose\nmulticlass classification problems into binary subproblems. We extend known\nJensen-Shannon bounds on the Bayes risk of binary problems to hierarchical\nmulticlass problems and use these bounds to develop a heuristic procedure for\nconstructing hierarchical multiclass decomposition for multinomials. We test\nour method and compare it to the well known \"all-pairs\" decomposition. Our\ntests are performed using a new authorship determination benchmark test of\nmachine learning authors. The new method consistently outperforms the all-pairs\ndecomposition when the number of classes is small and breaks even on larger\nmulticlass problems. Using both methods, the classification accuracy we\nachieve, using an SVM over a feature set consisting of both high frequency\nsingle tokens and high frequency token-pairs, appears to be exceptionally high\ncompared to known results in authorship determination."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.3177v1", 
    "title": "Introduction to the iDian", 
    "arxiv-id": "1010.3177v1", 
    "author": "Xin Rong", 
    "publish": "2010-10-15T14:18:25Z", 
    "summary": "The iDian (previously named as the Operation Agent System) is a framework\ndesigned to enable computer users to operate software in natural language.\nDistinct from current speech-recognition systems, our solution supports\nformat-free combinations of orders, and is open to both developers and\ncustomers. We used a multi-layer structure to build the entire framework,\napproached rule-based natural language processing, and implemented demos\nnarrowing down to Windows, text-editing and a few other applications. This\nessay will firstly give an overview of the entire system, and then scrutinize\nthe functions and structure of the system, and finally discuss the prospective\nde-velopment, esp. on-line interaction functions."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.4385v1", 
    "title": "A Protocol for Self-Synchronized Duty-Cycling in Sensor Networks:   Generic Implementation in Wiselib", 
    "arxiv-id": "1010.4385v1", 
    "author": "Sandor P. Fekete", 
    "publish": "2010-10-21T07:54:11Z", 
    "summary": "In this work we present a protocol for self-synchronized duty-cycling in\nwireless sensor networks with energy harvesting capabilities. The protocol is\nimplemented in Wiselib, a library of generic algorithms for sensor networks.\nSimulations are conducted with the sensor network simulator Shawn. They are\nbased on the specifications of real hardware known as iSense sensor nodes. The\nexperimental results show that the proposed mechanism is able to adapt to\nchanging energy availabilities. Moreover, it is shown that the system is very\nrobust against packet loss."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.4561v2", 
    "title": "New S-norm and T-norm Operators for Active Learning Method", 
    "arxiv-id": "1010.4561v2", 
    "author": "Ali Reza Ghatreh Samani", 
    "publish": "2010-10-21T19:48:22Z", 
    "summary": "Active Learning Method (ALM) is a soft computing method used for modeling and\ncontrol based on fuzzy logic. All operators defined for fuzzy sets must serve\nas either fuzzy S-norm or fuzzy T-norm. Despite being a powerful modeling\nmethod, ALM does not possess operators which serve as S-norms and T-norms which\ndeprive it of a profound analytical expression/form. This paper introduces two\nnew operators based on morphology which satisfy the following conditions:\nFirst, they serve as fuzzy S-norm and T-norm. Second, they satisfy Demorgans\nlaw, so they complement each other perfectly. These operators are investigated\nvia three viewpoints: Mathematics, Geometry and fuzzy logic."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.4609v1", 
    "title": "A Partial Taxonomy of Substitutability and Interchangeability", 
    "arxiv-id": "1010.4609v1", 
    "author": "Eugene C. Freuder", 
    "publish": "2010-10-22T04:00:00Z", 
    "summary": "Substitutability, interchangeability and related concepts in Constraint\nProgramming were introduced approximately twenty years ago and have given rise\nto considerable subsequent research. We survey this work, classify, and relate\nthe different concepts, and indicate directions for future work, in particular\nwith respect to making connections with research into symmetry breaking. This\npaper is a condensed version of a larger work in progress."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.4784v1", 
    "title": "Learning under Concept Drift: an Overview", 
    "arxiv-id": "1010.4784v1", 
    "author": "Indr\u0117 \u017dliobait\u0117", 
    "publish": "2010-10-22T19:31:23Z", 
    "summary": "Concept drift refers to a non stationary learning problem over time. The\ntraining and the application data often mismatch in real life problems. In this\nreport we present a context of concept drift problem 1. We focus on the issues\nrelevant to adaptive training set formation. We present the framework and\nterminology, and formulate a global picture of concept drift learners design.\nWe start with formalizing the framework for the concept drifting data in\nSection 1. In Section 2 we discuss the adaptivity mechanisms of the concept\ndrift learners. In Section 3 we overview the principle mechanisms of concept\ndrift learners. In this chapter we give a general picture of the available\nalgorithms and categorize them based on their properties. Section 5 discusses\nthe related research fields and Section 5 groups and presents major concept\ndrift applications. This report is intended to give a bird's view of concept\ndrift research field, provide a context of the research and position it within\nbroad spectrum of research fields and applications."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.4830v2", 
    "title": "A Unifying Probabilistic Perspective for Spectral Dimensionality   Reduction: Insights and New Models", 
    "arxiv-id": "1010.4830v2", 
    "author": "Neil D. Lawrence", 
    "publish": "2010-10-22T23:16:04Z", 
    "summary": "We introduce a new perspective on spectral dimensionality reduction which\nviews these methods as Gaussian Markov random fields (GRFs). Our unifying\nperspective is based on the maximum entropy principle which is in turn inspired\nby maximum variance unfolding. The resulting model, which we call maximum\nentropy unfolding (MEU) is a nonlinear generalization of principal component\nanalysis. We relate the model to Laplacian eigenmaps and isomap. We show that\nparameter fitting in the locally linear embedding (LLE) is approximate maximum\nlikelihood MEU. We introduce a variant of LLE that performs maximum likelihood\nexactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the\nleading spectral approaches on a robot navigation visualization and a human\nmotion capture data set. Finally the maximum likelihood perspective allows us\nto introduce a new approach to dimensionality reduction based on L1\nregularization of the Gaussian random field via the graphical lasso."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1010.5426v1", 
    "title": "Translation-Invariant Representation for Cumulative Foot Pressure Images", 
    "arxiv-id": "1010.5426v1", 
    "author": "Tieniu Tan", 
    "publish": "2010-10-26T15:16:50Z", 
    "summary": "Human can be distinguished by different limb movements and unique ground\nreaction force. Cumulative foot pressure image is a 2-D cumulative ground\nreaction force during one gait cycle. Although it contains pressure spatial\ndistribution information and pressure temporal distribution information, it\nsuffers from several problems including different shoes and noise, when putting\nit into practice as a new biometric for pedestrian identification. In this\npaper, we propose a hierarchical translation-invariant representation for\ncumulative foot pressure images, inspired by the success of Convolutional deep\nbelief network for digital classification. Key contribution in our approach is\ndiscriminative hierarchical sparse coding scheme which helps to learn useful\ndiscriminative high-level visual features. Based on the feature representation\nof cumulative foot pressure images, we develop a pedestrian recognition system\nwhich is invariant to three different shoes and slight local shape change.\nExperiments are conducted on a proposed open dataset that contains more than\n2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the\neffectiveness of the proposed method and the potential of cumulative foot\npressure images as a biometric."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1011.0098v1", 
    "title": "Qualitative Reasoning about Relative Direction on Adjustable Levels of   Granularity", 
    "arxiv-id": "1011.0098v1", 
    "author": "Reinhard Moratz", 
    "publish": "2010-10-30T19:06:44Z", 
    "summary": "An important issue in Qualitative Spatial Reasoning is the representation of\nrelative direction. In this paper we present simple geometric rules that enable\nreasoning about relative direction between oriented points. This framework, the\nOriented Point Algebra OPRA_m, has a scalable granularity m. We develop a\nsimple algorithm for computing the OPRA_m composition tables and prove its\ncorrectness. Using a composition table, algebraic closure for a set of OPRA\nstatements is sufficient to solve spatial navigation tasks. And it turns out\nthat scalable granularity is useful in these navigation tasks."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1011.0187v1", 
    "title": "A Distributed AI Aided 3D Domino Game", 
    "arxiv-id": "1011.0187v1", 
    "author": "Orhan A. Nooraden", 
    "publish": "2010-10-31T18:14:44Z", 
    "summary": "In the article a turn-based game played on four computers connected via\nnetwork is investigated. There are three computers with natural intelligence\nand one with artificial intelligence. Game table is seen by each player's own\nview point in all players' monitors. Domino pieces are three dimensional. For\ndistributed systems TCP/IP protocol is used. In order to get 3D image,\nMicrosoft XNA technology is applied. Domino 101 game is nondeterministic game\nthat is result of the game depends on the initial random distribution of the\npieces. Number of the distributions is equal to the multiplication of following\ncombinations: . Moreover, in this game that is played by four people, players\nare divided into 2 pairs. Accordingly, we cannot predict how the player uses\nthe dominoes that is according to the dominoes of his/her partner or according\nto his/her own dominoes. The fact that the natural intelligence can be a player\nin any level affects the outcome. These reasons make it difficult to develop an\nAI. In the article four levels of AI are developed. The AI in the first level\nis equivalent to the intelligence of a child who knows the rules of the game\nand recognizes the numbers. The AI in this level plays if it has any domino,\nsuitable to play or says pass. In most of the games which can be played on the\ninternet, the AI does the same. But the AI in the last level is a master\nplayer, and it can develop itself according to its competitors' levels."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1011.0190v1", 
    "title": "Prunnig Algorithm of Generation a Minimal Set of Rule Reducts Based on   Rough Set Theory", 
    "arxiv-id": "1011.0190v1", 
    "author": "Serhat Do\u011fan", 
    "publish": "2010-10-31T18:46:50Z", 
    "summary": "In this paper it is considered rule reduct generation problem, based on Rough\nSet Theory. Rule Reduct Generation (RG) and Modified Rule Generation (MRG)\nalgorithms are well-known. Alternative to these algorithms Pruning Algorithm of\nGeneration A Minimal Set of Rule Reducts, or briefly Pruning Rule Generation\n(PRG) algorithm is developed. PRG algorithm uses tree structured data type. PRG\nalgorithm is compared with RG and MRG algorithms"
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1011.0233v2", 
    "title": "Reasoning about Cardinal Directions between Extended Objects: The   Hardness Result", 
    "arxiv-id": "1011.0233v2", 
    "author": "Sanjiang Li", 
    "publish": "2010-11-01T01:02:39Z", 
    "summary": "The cardinal direction calculus (CDC) proposed by Goyal and Egenhofer is a\nvery expressive qualitative calculus for directional information of extended\nobjects. Early work has shown that consistency checking of complete networks of\nbasic CDC constraints is tractable while reasoning with the CDC in general is\nNP-hard. This paper shows, however, if allowing some constraints unspecified,\nthen consistency checking of possibly incomplete networks of basic CDC\nconstraints is already intractable. This draws a sharp boundary between the\ntractable and intractable subclasses of the CDC. The result is achieved by a\nreduction from the well-known 3-SAT problem."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-15280-1_56", 
    "link": "http://arxiv.org/pdf/1011.0330v3", 
    "title": "Imitation learning of motor primitives and language bootstrapping in   robots", 
    "arxiv-id": "1011.0330v3", 
    "author": "Pierre-Yves Oudeyer", 
    "publish": "2010-11-01T14:26:09Z", 
    "summary": "Imitation learning in robots, also called programing by demonstration, has\nmade important advances in recent years, allowing humans to teach context\ndependant motor skills/tasks to robots. We propose to extend the usual contexts\ninvestigated to also include acoustic linguistic expressions that might denote\na given motor skill, and thus we target joint learning of the motor skills and\ntheir potential acoustic linguistic name. In addition to this, a modification\nof a class of existing algorithms within the imitation learning framework is\nmade so that they can handle the unlabeled demonstration of several tasks/motor\nprimitives without having to inform the imitator of what task is being\ndemonstrated or what the number of tasks are, which is a necessity for language\nlearning, i.e; if one wants to teach naturally an open number of new motor\nskills together with their acoustic names. Finally, a mechanism for detecting\nwhether or not linguistic input is relevant to the task is also proposed, and\nour architecture also allows the robot to find the right framing for a given\nidentified motor primitive. With these additions it becomes possible to build\nan imitator that bridges the gap between imitation learning and language\nlearning by being able to learn linguistic expressions using methods from the\nimitation learning community. In this sense the imitator can learn a word by\nguessing whether a certain speech pattern present in the context means that a\nspecific task is to be executed. The imitator is however not assumed to know\nthat speech is relevant and has to figure this out on its own by looking at the\ndemonstrations: indeed, the architecture allows the robot to transparently also\nlearn tasks which should not be triggered by an acoustic word, but for example\nby the color or position of an object or a gesture made by someone in the\nenvironment. To demonstrate this ability to find the ..."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijaia.2010.1409", 
    "link": "http://arxiv.org/pdf/1011.0628v1", 
    "title": "Significance of Classification Techniques in Prediction of Learning   Disabilities", 
    "arxiv-id": "1011.0628v1", 
    "author": "Julie M. David And Kannan Balakrishnan", 
    "publish": "2010-11-02T14:37:51Z", 
    "summary": "The aim of this study is to show the importance of two classification\ntechniques, viz. decision tree and clustering, in prediction of learning\ndisabilities (LD) of school-age children. LDs affect about 10 percent of all\nchildren enrolled in schools. The problems of children with specific learning\ndisabilities have been a cause of concern to parents and teachers for some\ntime. Decision trees and clustering are powerful and popular tools used for\nclassification and prediction in Data mining. Different rules extracted from\nthe decision tree are used for prediction of learning disabilities. Clustering\nis the assignment of a set of observations into subsets, called clusters, which\nare useful in finding the different signs and symptoms (attributes) present in\nthe LD affected child. In this paper, J48 algorithm is used for constructing\nthe decision tree and K-means algorithm is used for creating the clusters. By\napplying these classification techniques, LD in any child can be identified."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.0950v1", 
    "title": "Detecting Ontological Conflicts in Protocols between Semantic Web   Services", 
    "arxiv-id": "1011.0950v1", 
    "author": "Pallab Dasgupta", 
    "publish": "2010-11-03T17:33:23Z", 
    "summary": "The task of verifying the compatibility between interacting web services has\ntraditionally been limited to checking the compatibility of the interaction\nprotocol in terms of message sequences and the type of data being exchanged.\nSince web services are developed largely in an uncoordinated way, different\nservices often use independently developed ontologies for the same domain\ninstead of adhering to a single ontology as standard. In this work we\ninvestigate the approaches that can be taken by the server to verify the\npossibility to reach a state with semantically inconsistent results during the\nexecution of a protocol with a client, if the client ontology is published.\nOften database is used to store the actual data along with the ontologies\ninstead of storing the actual data as a part of the ontology description. It is\nimportant to observe that at the current state of the database the semantic\nconflict state may not be reached even if the verification done by the server\nindicates the possibility of reaching a conflict state. A relational algebra\nbased decision procedure is also developed to incorporate the current state of\nthe client and the server databases in the overall verification procedure."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.1478v2", 
    "title": "Gradient Computation In Linear-Chain Conditional Random Fields Using The   Entropy Message Passing Algorithm", 
    "arxiv-id": "1011.1478v2", 
    "author": "Miomir S. Stankovic", 
    "publish": "2010-11-05T18:41:03Z", 
    "summary": "The paper proposes a numerically stable recursive algorithm for the exact\ncomputation of the linear-chain conditional random field gradient. It operates\nas a forward algorithm over the log-domain expectation semiring and has the\npurpose of enhancing memory efficiency when applied to long observation\nsequences. Unlike the traditional algorithm based on the forward-backward\nrecursions, the memory complexity of our algorithm does not depend on the\nsequence length. The experiments on real data show that it can be useful for\nthe problems which deal with long sequences."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.1660v1", 
    "title": "Reinforcement Learning Based on Active Learning Method", 
    "arxiv-id": "1011.1660v1", 
    "author": "Ali Akbar Kiaei", 
    "publish": "2010-11-07T17:45:57Z", 
    "summary": "In this paper, a new reinforcement learning approach is proposed which is\nbased on a powerful concept named Active Learning Method (ALM) in modeling. ALM\nexpresses any multi-input-single-output system as a fuzzy combination of some\nsingle-input-singleoutput systems. The proposed method is an actor-critic\nsystem similar to Generalized Approximate Reasoning based Intelligent Control\n(GARIC) structure to adapt the ALM by delayed reinforcement signals. Our system\nuses Temporal Difference (TD) learning to model the behavior of useful actions\nof a control system. The goodness of an action is modeled on Reward-\nPenalty-Plane. IDS planes will be updated according to this plane. It is shown\nthat the system can learn with a predefined fuzzy system or without it (through\nrandom actions)."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.1662v1", 
    "title": "A New Sufficient Condition for 1-Coverage to Imply Connectivity", 
    "arxiv-id": "1011.1662v1", 
    "author": "Ali Akbar Kiaei", 
    "publish": "2010-11-07T17:49:38Z", 
    "summary": "An effective approach for energy conservation in wireless sensor networks is\nscheduling sleep intervals for extraneous nodes while the remaining nodes stay\nactive to provide continuous service. For the sensor network to operate\nsuccessfully the active nodes must maintain both sensing coverage and network\nconnectivity, It proved before if the communication range of nodes is at least\ntwice the sensing range, complete coverage of a convex area implies\nconnectivity among the working set of nodes. In this paper we consider a\nrectangular region A = a *b, such that R a R b s s {\\pounds}, {\\pounds}, where\ns R is the sensing range of nodes. and put a constraint on minimum allowed\ndistance between nodes(s). according to this constraint we present a new lower\nbound for communication range relative to sensing range of sensors(s 2 + 3 *R)\nthat complete coverage of considered area implies connectivity among the\nworking set of nodes; also we present a new distribution method, that satisfy\nour constraint."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.2304v1", 
    "title": "Target tracking in the recommender space: Toward a new recommender   system based on Kalman filtering", 
    "arxiv-id": "1011.2304v1", 
    "author": "Anne Boyer", 
    "publish": "2010-11-10T08:26:56Z", 
    "summary": "In this paper, we propose a new approach for recommender systems based on\ntarget tracking by Kalman filtering. We assume that users and their seen\nresources are vectors in the multidimensional space of the categories of the\nresources. Knowing this space, we propose an algorithm based on a Kalman filter\nto track users and to predict the best prediction of their future position in\nthe recommendation space."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.2512v2", 
    "title": "Extended Active Learning Method", 
    "arxiv-id": "1011.2512v2", 
    "author": "Alireza Ghatreh Samani", 
    "publish": "2010-11-10T21:44:26Z", 
    "summary": "Active Learning Method (ALM) is a soft computing method which is used for\nmodeling and control, based on fuzzy logic. Although ALM has shown that it acts\nwell in dynamic environments, its operators cannot support it very well in\ncomplex situations due to losing data. Thus ALM can find better membership\nfunctions if more appropriate operators be chosen for it. This paper\nsubstituted two new operators instead of ALM original ones; which consequently\nrenewed finding membership functions in a way superior to conventional ALM.\nThis new method is called Extended Active Learning Method (EALM)."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.4362v1", 
    "title": "Should one compute the Temporal Difference fix point or minimize the   Bellman Residual? The unified oblique projection view", 
    "arxiv-id": "1011.4362v1", 
    "author": "Bruno Scherrer", 
    "publish": "2010-11-19T08:20:30Z", 
    "summary": "We investigate projection methods, for evaluating a linear approximation of\nthe value function of a policy in a Markov Decision Process context. We\nconsider two popular approaches, the one-step Temporal Difference fix-point\ncomputation (TD(0)) and the Bellman Residual (BR) minimization. We describe\nexamples, where each method outperforms the other. We highlight a simple\nrelation between the objective function they minimize, and show that while BR\nenjoys a performance guarantee, TD(0) does not in general. We then propose a\nunified view in terms of oblique projections of the Bellman equation, which\nsubstantially simplifies and extends the characterization of (schoknecht,2002)\nand the recent analysis of (Yu & Bertsekas, 2008). Eventually, we describe some\nsimulations that suggest that if the TD(0) solution is usually slightly better\nthan the BR solution, its inherent numerical instability makes it very bad in\nsome cases, and thus worse on average."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijwest.2010.1403", 
    "link": "http://arxiv.org/pdf/1011.5349v1", 
    "title": "Distributed Graph Coloring: An Approach Based on the Calling Behavior of   Japanese Tree Frogs", 
    "arxiv-id": "1011.5349v1", 
    "author": "Christian Blum", 
    "publish": "2010-11-24T11:47:59Z", 
    "summary": "Graph coloring, also known as vertex coloring, considers the problem of\nassigning colors to the nodes of a graph such that adjacent nodes do not share\nthe same color. The optimization version of the problem concerns the\nminimization of the number of used colors. In this paper we deal with the\nproblem of finding valid colorings of graphs in a distributed way, that is, by\nmeans of an algorithm that only uses local information for deciding the color\nof the nodes. Such algorithms prescind from any central control. Due to the\nfact that quite a few practical applications require to find colorings in a\ndistributed way, the interest in distributed algorithms for graph coloring has\nbeen growing during the last decade. As an example consider wireless ad-hoc and\nsensor networks, where tasks such as the assignment of frequencies or the\nassignment of TDMA slots are strongly related to graph coloring.\n  The algorithm proposed in this paper is inspired by the calling behavior of\nJapanese tree frogs. Male frogs use their calls to attract females.\nInterestingly, groups of males that are located nearby each other desynchronize\ntheir calls. This is because female frogs are only able to correctly localize\nthe male frogs when their calls are not too close in time. We experimentally\nshow that our algorithm is very competitive with the current state of the art,\nusing different sets of problem instances and comparing to one of the most\ncompetitive algorithms from the literature."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.3573658", 
    "link": "http://arxiv.org/pdf/1011.5480v1", 
    "title": "Bayesian Modeling of a Human MMORPG Player", 
    "arxiv-id": "1011.5480v1", 
    "author": "Pierre Bessiere", 
    "publish": "2010-11-24T20:07:49Z", 
    "summary": "This paper describes an application of Bayesian programming to the control of\nan autonomous avatar in a multiplayer role-playing game (the example is based\non World of Warcraft). We model a particular task, which consists of choosing\nwhat to do and to select which target in a situation where allies and foes are\npresent. We explain the model in Bayesian programming and show how we could\nlearn the conditional probabilities from data gathered during human-played\nsessions."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.3573658", 
    "link": "http://arxiv.org/pdf/1011.5951v1", 
    "title": "Reinforcement Learning in Partially Observable Markov Decision Processes   using Hybrid Probabilistic Logic Programs", 
    "arxiv-id": "1011.5951v1", 
    "author": "Emad Saad", 
    "publish": "2010-11-27T07:48:08Z", 
    "summary": "We present a probabilistic logic programming framework to reinforcement\nlearning, by integrating reinforce-ment learning, in POMDP environments, with\nnormal hybrid probabilistic logic programs with probabilistic answer set\nseman-tics, that is capable of representing domain-specific knowledge. We\nformally prove the correctness of our approach. We show that the complexity of\nfinding a policy for a reinforcement learning problem in our approach is\nNP-complete. In addition, we show that any reinforcement learning problem can\nbe encoded as a classical logic program with answer set semantics. We also show\nthat a reinforcement learning problem can be encoded as a SAT problem. We\npresent a new high level action description language that allows the factored\nrepresentation of POMDP. Moreover, we modify the original model of POMDP so\nthat it be able to distinguish between knowledge producing actions and actions\nthat change the environment."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.3573658", 
    "link": "http://arxiv.org/pdf/1011.6220v1", 
    "title": "Multimodal Biometric Systems - Study to Improve Accuracy and Performance", 
    "arxiv-id": "1011.6220v1", 
    "author": "K. KailasaRao", 
    "publish": "2010-11-29T13:10:27Z", 
    "summary": "Biometrics is the science and technology of measuring and analyzing\nbiological data of human body, extracting a feature set from the acquired data,\nand comparing this set against to the template set in the database.\nExperimental studies show that Unimodal biometric systems had many\ndisadvantages regarding performance and accuracy. Multimodal biometric systems\nperform better than unimodal biometric systems and are popular even more\ncomplex also. We examine the accuracy and performance of multimodal biometric\nauthentication systems using state of the art Commercial Off- The-Shelf (COTS)\nproducts. Here we discuss fingerprint and face biometric systems, decision and\nfusion techniques used in these systems. We also discuss their advantage over\nunimodal biometric systems."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.3573658", 
    "link": "http://arxiv.org/pdf/1012.0322v1", 
    "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in   Safety-Critical Systems", 
    "arxiv-id": "1012.0322v1", 
    "author": "Adolfo Hernandez", 
    "publish": "2010-12-01T21:08:04Z", 
    "summary": "Uncertainty of decisions in safety-critical engineering applications can be\nestimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC)\ntechnique of averaging over decision models. The use of decision tree (DT)\nmodels assists experts to interpret causal relations and find factors of the\nuncertainty. Bayesian averaging also allows experts to estimate the uncertainty\naccurately when a priori information on the favored structure of DTs is\navailable. Then an expert can select a single DT model, typically the Maximum a\nPosteriori model, for interpretation purposes. Unfortunately, a priori\ninformation on favored structure of DTs is not always available. For this\nreason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also\nsuggest a new procedure of selecting a single DT and describe an application\nscenario. In our experiments on the Short-Term Conflict Alert data our\ntechnique outperforms the existing Bayesian techniques in predictive accuracy\nof the selected single DTs."
},{
    "category": "cs.AI", 
    "doi": "10.1063/1.3573658", 
    "link": "http://arxiv.org/pdf/1012.0830v1", 
    "title": "Using ASP with recent extensions for causal explanations", 
    "arxiv-id": "1012.0830v1", 
    "author": "Yves Moinard", 
    "publish": "2010-12-03T20:07:21Z", 
    "summary": "We examine the practicality for a user of using Answer Set Programming (ASP)\nfor representing logical formalisms. We choose as an example a formalism aiming\nat capturing causal explanations from causal information. We provide an\nimplementation, showing the naturalness and relative efficiency of this\ntranslation job. We are interested in the ease for writing an ASP program, in\naccordance with the claimed ``declarative'' aspect of ASP. Limitations of the\nearlier systems (poor data structure and difficulty in reusing pieces of\nprograms) made that in practice, the ``declarative aspect'' was more\ntheoretical than practical. We show how recent improvements in working ASP\nsystems facilitate a lot the translation, even if a few improvements could\nstill be useful."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1255v3", 
    "title": "URSA: A System for Uniform Reduction to SAT", 
    "arxiv-id": "1012.1255v3", 
    "author": "Predrag Janicic", 
    "publish": "2010-12-06T17:40:33Z", 
    "summary": "There are a huge number of problems, from various areas, being solved by\nreducing them to SAT. However, for many applications, translation into SAT is\nperformed by specialized, problem-specific tools. In this paper we describe a\nnew system for uniform solving of a wide class of problems by reducing them to\nSAT. The system uses a new specification language URSA that combines imperative\nand declarative programming paradigms. The reduction to SAT is defined\nprecisely by the semantics of the specification language. The domain of the\napproach is wide (e.g., many NP-complete problems can be simply specified and\nthen solved by the system) and there are problems easily solvable by the\nproposed system, while they can be hardly solved by using other programming\nlanguages or constraint programming systems. So, the system can be seen not\nonly as a tool for solving problems by reducing them to SAT, but also as a\ngeneral-purpose constraint solving system (for finite domains). In this paper,\nwe also describe an open-source implementation of the described approach. The\nperformed experiments suggest that the system is competitive to\nstate-of-the-art related modelling systems."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1619v1", 
    "title": "Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM", 
    "arxiv-id": "1012.1619v1", 
    "author": "Pablo Lopez-Garcia", 
    "publish": "2010-12-07T21:45:50Z", 
    "summary": "SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in\nthe life sciences, with more than 300,000 concepts and relationships, but is\ndistributed with no associated software tools. In this paper we present MySNOM,\na web-based SNOMED CT browser. MySNOM allows organizations to browse their own\ndistribution of SNOMED CT under a controlled environment, focuses on navigating\nusing the structure of SNOMED CT, and has diagramming capabilities."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1635v1", 
    "title": "A study on the relation between linguistics-oriented and domain-specific   semantics", 
    "arxiv-id": "1012.1635v1", 
    "author": "He Tan", 
    "publish": "2010-12-07T23:03:42Z", 
    "summary": "In this paper we dealt with the comparison and linking between lexical\nresources with domain knowledge provided by ontologies. It is one of the issues\nfor the combination of the Semantic Web Ontologies and Text Mining. We\ninvestigated the relations between the linguistics oriented and domain-specific\nsemantics, by associating the GO biological process concepts to the FrameNet\nsemantic frames. The result shows the gaps between the linguistics-oriented and\ndomain-specific semantics on the classification of events and the grouping of\ntarget words. The result provides valuable information for the improvement of\ndomain ontologies supporting for text mining systems. And also, it will result\nin benefits to language understanding technology."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1643v1", 
    "title": "Process Makna - A Semantic Wiki for Scientific Workflows", 
    "arxiv-id": "1012.1643v1", 
    "author": "Zhili Zhao", 
    "publish": "2010-12-07T23:49:29Z", 
    "summary": "Virtual e-Science infrastructures supporting Web-based scientific workflows\nare an example for knowledge-intensive collaborative and weakly-structured\nprocesses where the interaction with the human scientists during process\nexecution plays a central role. In this paper we propose the lightweight\ndynamic user-friendly interaction with humans during execution of scientific\nworkflows via the low-barrier approach of Semantic Wikis as an intuitive\ninterface for non-technical scientists. Our Process Makna Semantic Wiki system\nis a novel combination of an business process management system adapted for\nscientific workflows with a Corporate Semantic Web Wiki user interface\nsupporting knowledge intensive human interaction tasks during scientific\nworkflow execution."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1646v1", 
    "title": "Use of semantic technologies for the development of a dynamic   trajectories generator in a Semantic Chemistry eLearning platform", 
    "arxiv-id": "1012.1646v1", 
    "author": "Adrian Paschke", 
    "publish": "2010-12-07T23:55:47Z", 
    "summary": "ChemgaPedia is a multimedia, webbased eLearning service platform that\ncurrently contains about 18.000 pages organized in 1.700 chapters covering the\ncomplete bachelor studies in chemistry and related topics of chemistry,\npharmacy, and life sciences. The eLearning encyclopedia contains some 25.000\nmedia objects and the eLearning platform provides services such as virtual and\nremote labs for experiments. With up to 350.000 users per month the platform is\nthe most frequently used scientific educational service in the German spoken\nInternet. In this demo we show the benefit of mapping the static eLearning\ncontents of ChemgaPedia to a Linked Data representation for Semantic Chemistry\nwhich allows for generating dynamic eLearning paths tailored to the semantic\nprofiles of the users."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1654v1", 
    "title": "Using Semantic Wikis for Structured Argument in Medical Domain", 
    "arxiv-id": "1012.1654v1", 
    "author": "Radu Balaj", 
    "publish": "2010-12-08T00:34:17Z", 
    "summary": "This research applies ideas from argumentation theory in the context of\nsemantic wikis, aiming to provide support for structured-large scale\nargumentation between human agents. The implemented prototype is exemplified by\nmodelling the MMR vaccine controversy."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1658v1", 
    "title": "Creating a new Ontology: a Modular Approach", 
    "arxiv-id": "1012.1658v1", 
    "author": "Fons J. Verbeek", 
    "publish": "2010-12-08T00:41:20Z", 
    "summary": "Creating a new Ontology: a Modular Approach"
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1667v1", 
    "title": "A semantic approach for the requirement-driven discovery of web services   in the Life Sciences", 
    "arxiv-id": "1012.1667v1", 
    "author": "Ismael Sanz", 
    "publish": "2010-12-08T01:12:57Z", 
    "summary": "Research in the Life Sciences depends on the integration of large,\ndistributed and heterogeneous data sources and web services. The discovery of\nwhich of these resources are the most appropriate to solve a given task is a\ncomplex research question, since there is a large amount of plausible\ncandidates and there is little, mostly unstructured, metadata to be able to\ndecide among them.We contribute a semi-automatic approach,based on semantic\ntechniques, to assist researchers in the discovery of the most appropriate web\nservices to full a set of given requirements."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1743v1", 
    "title": "Scientific Collaborations: principles of WikiBridge Design", 
    "arxiv-id": "1012.1743v1", 
    "author": "Marinette Savonnet", 
    "publish": "2010-12-08T11:43:37Z", 
    "summary": "Semantic wikis, wikis enhanced with Semantic Web technologies, are\nappropriate systems for community-authored knowledge models. They are\nparticularly suitable for scientific collaboration. This paper details the\ndesign principles ofWikiBridge, a semantic wiki."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1745v1", 
    "title": "Populous: A tool for populating ontology templates", 
    "arxiv-id": "1012.1745v1", 
    "author": "Katy Wolstencroft", 
    "publish": "2010-12-08T11:55:06Z", 
    "summary": "We present Populous, a tool for gathering content with which to populate an\nontology. Domain experts need to add content, that is often repetitive in its\nform, but without having to tackle the underlying ontological representation.\nPopulous presents users with a table based form in which columns are\nconstrained to take values from particular ontologies; the user can select a\nconcept from an ontology via its meaningful label to give a value for a given\nentity attribute. Populated tables are mapped to patterns that can then be used\nto automatically generate the ontology's content. Populous's contribution is in\nthe knowledge gathering stage of ontology development. It separates knowledge\ngathering from the conceptualisation and also separates the user from the\nstandard ontology authoring environments. As a result, Populous can allow\nknowledge to be gathered in a straight-forward manner that can then be used to\ndo mass production of ontology content."
},{
    "category": "cs.AI", 
    "doi": "10.2168/LMCS-8(3:30)2012", 
    "link": "http://arxiv.org/pdf/1012.1899v1", 
    "title": "Querying Biomedical Ontologies in Natural Language using Answer Set", 
    "arxiv-id": "1012.1899v1", 
    "author": "Esra Erdem", 
    "publish": "2010-12-09T00:12:24Z", 
    "summary": "In this work, we develop an intelligent user interface that allows users to\nenter biomedical queries in a natural language, and that presents the answers\n(possibly with explanations if requested) in a natural language. We develop a\nrule layer over biomedical ontologies and databases, and use automated\nreasoners to answer queries considering relevant parts of the rule layer."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.2148v1", 
    "title": "Bisimulations for fuzzy transition systems", 
    "arxiv-id": "1012.2148v1", 
    "author": "Etienne Kerre", 
    "publish": "2010-12-10T00:24:42Z", 
    "summary": "There has been a long history of using fuzzy language equivalence to compare\nthe behavior of fuzzy systems, but the comparison at this level is too coarse.\nRecently, a finer behavioral measure, bisimulation, has been introduced to\nfuzzy finite automata. However, the results obtained are applicable only to\nfinite-state systems. In this paper, we consider bisimulation for general fuzzy\nsystems which may be infinite-state or infinite-event, by modeling them as\nfuzzy transition systems. To help understand and check bisimulation, we\ncharacterize it in three ways by enumerating whole transitions, comparing\nindividual transitions, and using a monotonic function. In addition, we address\ncomposition operations, subsystems, quotients, and homomorphisms of fuzzy\ntransition systems and discuss their properties connected with bisimulation.\nThe results presented here are useful for comparing the behavior of general\nfuzzy systems. In particular, this makes it possible to relate an infinite\nfuzzy system to a finite one, which is easier to analyze, with the same\nbehavior."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.2162v1", 
    "title": "Nondeterministic fuzzy automata", 
    "arxiv-id": "1012.2162v1", 
    "author": "Yoshinori Ezawa", 
    "publish": "2010-12-10T02:42:30Z", 
    "summary": "Fuzzy automata have long been accepted as a generalization of\nnondeterministic finite automata. A closer examination, however, shows that the\nfundamental property---nondeterminism---in nondeterministic finite automata has\nnot been well embodied in the generalization. In this paper, we introduce\nnondeterministic fuzzy automata with or without $\\el$-moves and fuzzy languages\nrecognized by them. Furthermore, we prove that (deterministic) fuzzy automata,\nnondeterministic fuzzy automata, and nondeterministic fuzzy automata with\n$\\el$-moves are all equivalent in the sense that they recognize the same class\nof fuzzy languages."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.2789v1", 
    "title": "Experimental Comparison of Representation Methods and Distance Measures   for Time Series Data", 
    "arxiv-id": "1012.2789v1", 
    "author": "Eamonn Keogh", 
    "publish": "2010-12-09T19:43:53Z", 
    "summary": "The previous decade has brought a remarkable increase of the interest in\napplications that deal with querying and mining of time series data. Many of\nthe research efforts in this context have focused on introducing new\nrepresentation methods for dimensionality reduction or novel similarity\nmeasures for the underlying data. In the vast majority of cases, each\nindividual work introducing a particular method has made specific claims and,\naside from the occasional theoretical justifications, provided quantitative\nexperimental observations. However, for the most part, the comparative aspects\nof these experiments were too narrowly focused on demonstrating the benefits of\nthe proposed methods over some of the previously introduced ones. In order to\nprovide a comprehensive validation, we conducted an extensive experimental\nstudy re-implementing eight different time series representations and nine\nsimilarity measures and their variants, and testing their effectiveness on\nthirty-eight time series data sets from a wide variety of application domains.\nIn this paper, we give an overview of these different techniques and present\nour comparative experimental findings regarding their effectiveness. In\naddition to providing a unified validation of some of the existing\nachievements, our experiments also indicate that, in some cases, certain claims\nin the literature may be unduly optimistic."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.3280v1", 
    "title": "A new Recommender system based on target tracking: a Kalman Filter   approach", 
    "arxiv-id": "1012.3280v1", 
    "author": "Anne Boyer", 
    "publish": "2010-12-15T11:07:09Z", 
    "summary": "In this paper, we propose a new approach for recommender systems based on\ntarget tracking by Kalman filtering. We assume that users and their seen\nresources are vectors in the multidimensional space of the categories of the\nresources. Knowing this space, we propose an algorithm based on a Kalman filter\nto track users and to predict the best prediction of their future position in\nthe recommendation space."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.3312v1", 
    "title": "Dynamic Capitalization and Visualization Strategy in Collaborative   Knowledge Management System for EI Process", 
    "arxiv-id": "1012.3312v1", 
    "author": "Amos David", 
    "publish": "2010-12-15T12:45:56Z", 
    "summary": "Knowledge is attributed to human whose problem-solving behavior is subjective\nand complex. In today's knowledge economy, the need to manage knowledge\nproduced by a community of actors cannot be overemphasized. This is due to the\nfact that actors possess some level of tacit knowledge which is generally\ndifficult to articulate. Problem-solving requires searching and sharing of\nknowledge among a group of actors in a particular context. Knowledge expressed\nwithin the context of a problem resolution must be capitalized for future\nreuse. In this paper, an approach that permits dynamic capitalization of\nrelevant and reliable actors' knowledge in solving decision problem following\nEconomic Intelligence process is proposed. Knowledge annotation method and\ntemporal attributes are used for handling the complexity in the communication\namong actors and in contextualizing expressed knowledge. A prototype is built\nto demonstrate the functionalities of a collaborative Knowledge Management\nsystem based on this approach. It is tested with sample cases and the result\nshowed that dynamic capitalization leads to knowledge validation hence\nincreasing reliability of captured knowledge for reuse. The system can be\nadapted to various domains"
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.3336v1", 
    "title": "Dynamic Knowledge Capitalization through Annotation among Economic   Intelligence Actors in a Collaborative Environment", 
    "arxiv-id": "1012.3336v1", 
    "author": "Victor Odumuyiwa", 
    "publish": "2010-12-15T13:56:05Z", 
    "summary": "The shift from industrial economy to knowledge economy in today's world has\nrevolutionalized strategic planning in organizations as well as their problem\nsolving approaches. The point of focus today is knowledge and service\nproduction with more emphasis been laid on knowledge capital. Many\norganizations are investing on tools that facilitate knowledge sharing among\ntheir employees and they are as well promoting and encouraging collaboration\namong their staff in order to build the organization's knowledge capital with\nthe ultimate goal of creating a lasting competitive advantage for their\norganizations. One of the current leading approaches used for solving\norganization's decision problem is the Economic Intelligence (EI) approach\nwhich involves interactions among various actors called EI actors. These actors\ncollaborate to ensure the overall success of the decision problem solving\nprocess. In the course of the collaboration, the actors express knowledge which\ncould be capitalized for future reuse. In this paper, we propose in the first\nplace, an annotation model for knowledge elicitation among EI actors. Because\nof the need to build a knowledge capital, we also propose a dynamic knowledge\ncapitalisation approach for managing knowledge produced by the actors. Finally,\nthe need to manage the interactions and the interdependencies among\ncollaborating EI actors, led to our third proposition which constitute an\nawareness mechanism for group work management."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.3410v1", 
    "title": "Descriptive-complexity based distance for fuzzy sets", 
    "arxiv-id": "1012.3410v1", 
    "author": "Joel Ratsaby", 
    "publish": "2010-12-15T18:02:27Z", 
    "summary": "A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is\nbased on the descriptive complexity, i.e., the number of bits (on average) that\nare needed to describe an element in the symmetric difference of the two sets.\nThe distance gives the amount of additional information needed to describe any\none of the two sets given the other. We prove its mathematical properties and\nperform pattern clustering on data based on this distance."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.4046v1", 
    "title": "Artificial Intelligence in Reverse Supply Chain Management: The State of   the Art", 
    "arxiv-id": "1012.4046v1", 
    "author": "Fulufhelo V. Nelwamondo", 
    "publish": "2010-12-18T01:12:14Z", 
    "summary": "Product take-back legislation forces manufacturers to bear the costs of\ncollection and disposal of products that have reached the end of their useful\nlives. In order to reduce these costs, manufacturers can consider reuse,\nremanufacturing and/or recycling of components as an alternative to disposal.\nThe implementation of such alternatives usually requires an appropriate reverse\nsupply chain management. With the concepts of reverse supply chain are gaining\npopularity in practice, the use of artificial intelligence approaches in these\nareas is also becoming popular. As a result, the purpose of this paper is to\ngive an overview of the recent publications concerning the application of\nartificial intelligence techniques to reverse supply chain with emphasis on\ncertain types of product returns."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.4776v1", 
    "title": "Automatic Estimation of the Exposure to Lateral Collision in Signalized   Intersections using Video Sensors", 
    "arxiv-id": "1012.4776v1", 
    "author": "Sophie Midenet", 
    "publish": "2010-12-21T19:46:21Z", 
    "summary": "Intersections constitute one of the most dangerous elements in road systems.\nTraffic signals remain the most common way to control traffic at high-volume\nintersections and offer many opportunities to apply intelligent transportation\nsystems to make traffic more efficient and safe. This paper describes an\nautomated method to estimate the temporal exposure of road users crossing the\nconflict zone to lateral collision with road users originating from a different\napproach. This component is part of a larger system relying on video sensors to\nprovide queue lengths and spatial occupancy that are used for real time traffic\ncontrol and monitoring. The method is evaluated on data collected during a real\nworld experiment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.5585v1", 
    "title": "Symmetry Breaking with Polynomial Delay", 
    "arxiv-id": "1012.5585v1", 
    "author": "M. R. C. van Dongen", 
    "publish": "2010-12-27T09:58:16Z", 
    "summary": "A conservative class of constraint satisfaction problems CSPs is a class for\nwhich membership is preserved under arbitrary domain reductions. Many\nwell-known tractable classes of CSPs are conservative. It is well known that\nlexleader constraints may significantly reduce the number of solutions by\nexcluding symmetric solutions of CSPs. We show that adding certain lexleader\nconstraints to any instance of any conservative class of CSPs still allows us\nto find all solutions with a time which is polynomial between successive\nsolutions. The time is polynomial in the total size of the instance and the\nadditional lexleader constraints. It is well known that for complete symmetry\nbreaking one may need an exponential number of lexleader constraints. However,\nin practice, the number of additional lexleader constraints is typically\npolynomial number in the size of the instance. For polynomially many lexleader\nconstraints, we may in general not have complete symmetry breaking but\npolynomially many lexleader constraints may provide practically useful symmetry\nbreaking -- and they sometimes exclude super-exponentially many solutions. We\nprove that for any instance from a conservative class, the time between finding\nsuccessive solutions of the instance with polynomially many additional\nlexleader constraints is polynomial even in the size of the instance without\nlexleaderconstraints."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.5705v1", 
    "title": "Looking for plausibility", 
    "arxiv-id": "1012.5705v1", 
    "author": "Wan Ahmad Tajuddin Wan Abdullah", 
    "publish": "2010-12-28T07:14:32Z", 
    "summary": "In the interpretation of experimental data, one is actually looking for\nplausible explanations. We look for a measure of plausibility, with which we\ncan compare different possible explanations, and which can be combined when\nthere are different sets of data. This is contrasted to the conventional\nmeasure for probabilities as well as to the proposed measure of possibilities.\nWe define what characteristics this measure of plausibility should have.\n  In getting to the conception of this measure, we explore the relation of\nplausibility to abductive reasoning, and to Bayesian probabilities. We also\ncompare with the Dempster-Schaefer theory of evidence, which also has its own\ndefinition for plausibility. Abduction can be associated with biconditionality\nin inference rules, and this provides a platform to relate to the\nCollins-Michalski theory of plausibility. Finally, using a formalism for wiring\nlogic onto Hopfield neural networks, we ask if this is relevant in obtaining\nthis measure."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.5815v2", 
    "title": "SAPFOCS: a metaheuristic based approach to part family formation   problems in group technology", 
    "arxiv-id": "1012.5815v2", 
    "author": "Pranab K Dan", 
    "publish": "2010-12-28T18:57:04Z", 
    "summary": "This article deals with Part family formation problem which is believed to be\nmoderately complicated to be solved in polynomial time in the vicinity of Group\nTechnology (GT). In the past literature researchers investigated that the part\nfamily formation techniques are principally based on production flow analysis\n(PFA) which usually considers operational requirements, sequences and time.\nPart Coding Analysis (PCA) is merely considered in GT which is believed to be\nthe proficient method to identify the part families. PCA classifies parts by\nallotting them to different families based on their resemblances in: (1) design\ncharacteristics such as shape and size, and/or (2) manufacturing\ncharacteristics (machining requirements). A novel approach based on simulated\nannealing namely SAPFOCS is adopted in this study to develop effective part\nfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal design\nmethod is employed to solve the critical issues on the subject of parameters\nselection for the proposed metaheuristic algorithm. The adopted technique is\ntherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9\nand the obtained results are compared with C-Linkage clustering technique. The\nexperimental results reported that the proposed metaheuristic algorithm is\nextremely effective in terms of the quality of the solution obtained and has\noutperformed C-Linkage algorithm in most instances."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.5847v2", 
    "title": "On Elementary Loops of Logic Programs", 
    "arxiv-id": "1012.5847v2", 
    "author": "Yuliya Lierler", 
    "publish": "2010-12-28T21:49:11Z", 
    "summary": "Using the notion of an elementary loop, Gebser and Schaub refined the theorem\non loop formulas due to Lin and Zhao by considering loop formulas of elementary\nloops only. In this article, we reformulate their definition of an elementary\nloop, extend it to disjunctive programs, and study several properties of\nelementary loops, including how maximal elementary loops are related to minimal\nunfounded sets. The results provide useful insights into the stable model\nsemantics in terms of elementary loops. For a nondisjunctive program, using a\ngraph-theoretic characterization of an elementary loop, we show that the\nproblem of recognizing an elementary loop is tractable. On the other hand, we\nshow that the corresponding problem is {\\sf coNP}-complete for a disjunctive\nprogram. Based on the notion of an elementary loop, we present the class of\nHead-Elementary-loop-Free (HEF) programs, which strictly generalizes the class\nof Head-Cycle-Free (HCF) programs due to Ben-Eliyahu and Dechter. Like an HCF\nprogram, an HEF program can be turned into an equivalent nondisjunctive program\nin polynomial time by shifting head atoms into the body."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.5960v1", 
    "title": "Extending Binary Qualitative Direction Calculi with a Granular Distance   Concept: Hidden Feature Attachment", 
    "arxiv-id": "1012.5960v1", 
    "author": "Reinhard Moratz", 
    "publish": "2010-12-29T15:29:33Z", 
    "summary": "In this paper we introduce a method for extending binary qualitative\ndirection calculi with adjustable granularity like OPRAm or the star calculus\nwith a granular distance concept. This method is similar to the concept of\nextending points with an internal reference direction to get oriented points\nwhich are the basic entities in the OPRAm calculus. Even if the spatial objects\nare from a geometrical point of view infinitesimal small points locally\navailable reference measures are attached. In the case of OPRAm, a reference\ndirection is attached. The same principle works also with local reference\ndistances which are called elevations. The principle of attaching references\nfeatures to a point is called hidden feature attachment."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1012.6018v1", 
    "title": "Learning a Representation of a Believable Virtual Character's   Environment with an Imitation Algorithm", 
    "arxiv-id": "1012.6018v1", 
    "author": "Olivier Marc", 
    "publish": "2010-12-29T19:58:44Z", 
    "summary": "In video games, virtual characters' decision systems often use a simplified\nrepresentation of the world. To increase both their autonomy and believability\nwe want those characters to be able to learn this representation from human\nplayers. We propose to use a model called growing neural gas to learn by\nimitation the topology of the environment. The implementation of the model, the\nmodifications and the parameters we used are detailed. Then, the quality of the\nlearned representations and their evolution during the learning are studied\nusing different measures. Improvements for the growing neural gas to give more\ninformation to the character's model are given in the conclusion."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1101.2279v1", 
    "title": "Planning with Partial Preference Models", 
    "arxiv-id": "1101.2279v1", 
    "author": "Subbarao Kambhampati", 
    "publish": "2011-01-12T06:40:42Z", 
    "summary": "Current work in planning with preferences assume that the user's preference\nmodels are completely specified and aim to search for a single solution plan.\nIn many real-world planning scenarios, however, the user probably cannot\nprovide any information about her desired plans, or in some cases can only\nexpress partial preferences. In such situations, the planner has to present not\nonly one but a set of plans to the user, with the hope that some of them are\nsimilar to the plan she prefers. We first propose the usage of different\nmeasures to capture quality of plan sets that are suitable for such scenarios:\ndomain-independent distance measures defined based on plan elements (actions,\nstates, causal links) if no knowledge of the user's preferences is given, and\nthe Integrated Convex Preference measure in case the user's partial preference\nis provided. We then investigate various heuristic approaches to find set of\nplans according to these measures, and present empirical results demonstrating\nthe promise of our approach."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1101.2378v1", 
    "title": "Extracting Features from Ratings: The Role of Factor Models", 
    "arxiv-id": "1101.2378v1", 
    "author": "Wolf-Tilo Balke", 
    "publish": "2011-01-12T14:56:01Z", 
    "summary": "Performing effective preference-based data retrieval requires detailed and\npreferentially meaningful structurized information about the current user as\nwell as the items under consideration. A common problem is that representations\nof items often only consist of mere technical attributes, which do not resemble\nhuman perception. This is particularly true for integral items such as movies\nor songs. It is often claimed that meaningful item features could be extracted\nfrom collaborative rating data, which is becoming available through social\nnetworking services. However, there is only anecdotal evidence supporting this\nclaim; but if it is true, the extracted information could very valuable for\npreference-based data retrieval. In this paper, we propose a methodology to\nsystematically check this common claim. We performed a preliminary\ninvestigation on a large collection of movie ratings and present initial\nevidence."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1101.3465v2", 
    "title": "The \"psychological map of the brain\", as a personal information card   (file), - a project for the student of the 21st century", 
    "arxiv-id": "1101.3465v2", 
    "author": "Emanuel Gluskin", 
    "publish": "2011-01-17T17:30:31Z", 
    "summary": "We suggest a procedure that is relevant both to electronic performance and\nhuman psychology, so that the creative logic and the respect for human nature\nappear in a good agreement. The idea is to create an electronic card containing\nbasic information about a person's psychological behavior in order to make it\npossible to quickly decide about the suitability of one for another. This\n\"psychological electronics\" approach could be tested via student projects."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1101.4356v1", 
    "title": "Meaning Negotiation as Inference", 
    "arxiv-id": "1101.4356v1", 
    "author": "Luca Vigan\u00f2", 
    "publish": "2011-01-23T09:49:55Z", 
    "summary": "Meaning negotiation (MN) is the general process with which agents reach an\nagreement about the meaning of a set of terms. Artificial Intelligence scholars\nhave dealt with the problem of MN by means of argumentations schemes, beliefs\nmerging and information fusion operators, and ontology alignment but the\nproposed approaches depend upon the number of participants. In this paper, we\ngive a general model of MN for an arbitrary number of agents, in which each\nparticipant discusses with the others her viewpoint by exhibiting it in an\nactual set of constraints on the meaning of the negotiated terms. We call this\npresentation of individual viewpoints an angle. The agents do not aim at\nforming a common viewpoint but, instead, at agreeing about an acceptable common\nangle. We analyze separately the process of MN by two agents (\\emph{bilateral}\nor \\emph{pairwise} MN) and by more than two agents (\\emph{multiparty} MN), and\nwe use game theoretic models to understand how the process develops in both\ncases: the models are Bargaining Game for bilateral MN and English Auction for\nmultiparty MN. We formalize the process of reaching such an agreement by giving\na deduction system that comprises of rules that are consistent and adequate for\nrepresenting MN."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.0079v1", 
    "title": "Information-theoretic measures associated with rough set approximations", 
    "arxiv-id": "1102.0079v1", 
    "author": "Qiaoyan Wen", 
    "publish": "2011-02-01T05:38:33Z", 
    "summary": "Although some information-theoretic measures of uncertainty or granularity\nhave been proposed in rough set theory, these measures are only dependent on\nthe underlying partition and the cardinality of the universe, independent of\nthe lower and upper approximations. It seems somewhat unreasonable since the\nbasic idea of rough set theory aims at describing vague concepts by the lower\nand upper approximations. In this paper, we thus define new\ninformation-theoretic entropy and co-entropy functions associated to the\npartition and the approximations to measure the uncertainty and granularity of\nan approximation space. After introducing the novel notions of entropy and\nco-entropy, we then examine their properties. In particular, we discuss the\nrelationship of co-entropies between different universes. The theoretical\ndevelopment is accompanied by illustrative numerical examples."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.0714v1", 
    "title": "An architecture for the evaluation of intelligent systems", 
    "arxiv-id": "1102.0714v1", 
    "author": "Jose Hernandez-Orallo", 
    "publish": "2011-02-03T15:58:18Z", 
    "summary": "One of the main research areas in Artificial Intelligence is the coding of\nagents (programs) which are able to learn by themselves in any situation. This\nmeans that agents must be useful for purposes other than those they were\ncreated for, as, for example, playing chess. In this way we try to get closer\nto the pristine goal of Artificial Intelligence. One of the problems to decide\nwhether an agent is really intelligent or not is the measurement of its\nintelligence, since there is currently no way to measure it in a reliable way.\nThe purpose of this project is to create an interpreter that allows for the\nexecution of several environments, including those which are generated\nrandomly, so that an agent (a person or a program) can interact with them. Once\nthe interaction between the agent and the environment is over, the interpreter\nwill measure the intelligence of the agent according to the actions, states and\nrewards the agent has undergone inside the environment during the test. As a\nresult we will be able to measure agents' intelligence in any possible\nenvironment, and to make comparisons between several agents, in order to\ndetermine which of them is the most intelligent. In order to perform the tests,\nthe interpreter must be able to randomly generate environments that are really\nuseful to measure agents' intelligence, since not any randomly generated\nenvironment will serve that purpose."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.0831v1", 
    "title": "Intelligent Semantic Web Search Engines: A Brief Survey", 
    "arxiv-id": "1102.0831v1", 
    "author": "Dr. T. V. Rajinikanth", 
    "publish": "2011-02-04T03:56:09Z", 
    "summary": "The World Wide Web (WWW) allows the people to share the information (data)\nfrom the large database repositories globally. The amount of information grows\nbillions of databases. We need to search the information will specialize tools\nknown generically search engine. There are many of search engines available\ntoday, retrieving meaningful information is difficult. However to overcome this\nproblem in search engines to retrieve meaningful information intelligently,\nsemantic web technologies are playing a major role. In this paper we present\nsurvey on the search engine generations and the role of search engines in\nintelligent web and semantic search technologies."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.2670v1", 
    "title": "Online Least Squares Estimation with Self-Normalized Processes: An   Application to Bandit Problems", 
    "arxiv-id": "1102.2670v1", 
    "author": "Csaba Szepesvari", 
    "publish": "2011-02-14T04:06:31Z", 
    "summary": "The analysis of online least squares estimation is at the heart of many\nstochastic sequential decision making problems. We employ tools from the\nself-normalized processes to provide a simple and self-contained proof of a\ntail bound of a vector-valued martingale. We use the bound to construct a new\ntighter confidence sets for the least squares estimate.\n  We apply the confidence sets to several online decision problems, such as the\nmulti-armed and the linearly parametrized bandit problems. The confidence sets\nare potentially applicable to other problems such as sleeping bandits,\ngeneralized linear bandits, and other linear control problems.\n  We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of\nAuer et al. (2002) and show that its regret is with high-probability a problem\ndependent constant. In the case of linear bandits (Dani et al., 2008), we\nimprove the problem dependent bound in the dimension and number of time steps.\nFurthermore, as opposed to the previous result, we prove that our bound holds\nfor small sample sizes, and at the same time the worst case bound is improved\nby a logarithmic factor and the constant is improved."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.2984v1", 
    "title": "Hybrid Model for Solving Multi-Objective Problems Using Evolutionary   Algorithm and Tabu Search", 
    "arxiv-id": "1102.2984v1", 
    "author": "Abdelaziz Dammak", 
    "publish": "2011-02-15T07:43:03Z", 
    "summary": "This paper presents a new multi-objective hybrid model that makes cooperation\nbetween the strength of research of neighborhood methods presented by the tabu\nsearch (TS) and the important exploration capacity of evolutionary algorithm.\nThis model was implemented and tested in benchmark functions (ZDT1, ZDT2, and\nZDT3), using a network of computers."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.4924v1", 
    "title": "New Worst-Case Upper Bound for #XSAT", 
    "arxiv-id": "1102.4924v1", 
    "author": "Minghao Yin", 
    "publish": "2011-02-24T08:16:59Z", 
    "summary": "An algorithm running in O(1.1995n) is presented for counting models for exact\nsatisfiability formulae(#XSAT). This is faster than the previously best\nalgorithm which runs in O(1.2190n). In order to improve the efficiency of the\nalgorithm, a new principle, i.e. the common literals principle, is addressed to\nsimplify formulae. This allows us to eliminate more common literals. In\naddition, we firstly inject the resolution principles into solving #XSAT\nproblem, and therefore this further improves the efficiency of the algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.5385v2", 
    "title": "Back and Forth Between Rules and SE-Models (Extended Version)", 
    "arxiv-id": "1102.5385v2", 
    "author": "Jo\u00e3o Leite", 
    "publish": "2011-02-26T03:06:55Z", 
    "summary": "Rules in logic programming encode information about mutual interdependencies\nbetween literals that is not captured by any of the commonly used semantics.\nThis information becomes essential as soon as a program needs to be modified or\nfurther manipulated.\n  We argue that, in these cases, a program should not be viewed solely as the\nset of its models. Instead, it should be viewed and manipulated as the set of\nsets of models of each rule inside it. With this in mind, we investigate and\nhighlight relations between the SE-model semantics and individual rules. We\nidentify a set of representatives of rule equivalence classes induced by\nSE-models, and so pinpoint the exact expressivity of this semantics with\nrespect to a single rule. We also characterise the class of sets of\nSE-interpretations representable by a single rule. Finally, we discuss the\nintroduction of two notions of equivalence, both stronger than strong\nequivalence [1] and weaker than strong update equivalence [2], which seem more\nsuitable whenever the dependency information found in rules is of interest."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1102.5635v1", 
    "title": "Practical inventory routing: A problem definition and an optimization   method", 
    "arxiv-id": "1102.5635v1", 
    "author": "Marc Sevaux", 
    "publish": "2011-02-28T10:42:29Z", 
    "summary": "The global objective of this work is to provide practical optimization\nmethods to companies involved in inventory routing problems, taking into\naccount this new type of data. Also, companies are sometimes not able to deal\nwith changing plans every period and would like to adopt regular structures for\nserving customers."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.0127v1", 
    "title": "Fuzzy Approach to Critical Bus Ranking under Normal and Line Outage   Contingencies", 
    "arxiv-id": "1103.0127v1", 
    "author": "Dr. T. Ananthapadmanabha", 
    "publish": "2011-03-01T10:35:44Z", 
    "summary": "Identification of critical or weak buses for a given operating condition is\nan important task in the load dispatch centre. It has become more vital in view\nof the threat of voltage instability leading to voltage collapse. This paper\npresents a fuzzy approach for ranking critical buses in a power system under\nnormal and network contingencies based on Line Flow index and voltage profiles\nat load buses. The Line Flow index determines the maximum load that is possible\nto be connected to a bus in order to maintain stability before the system\nreaches its bifurcation point. Line Flow index (LF index) along with voltage\nprofiles at the load buses are represented in Fuzzy Set notation. Further they\nare evaluated using fuzzy rules to compute Criticality Index. Based on this\nindex, critical buses are ranked. The bus with highest rank is the weakest bus\nas it can withstand a small amount of load before causing voltage collapse. The\nproposed method is tested on Five Bus Test System."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.0632v1", 
    "title": "An Agent Based Architecture (Using Planning) for Dynamic and Semantic   Web Services Composition in an EBXML Context", 
    "arxiv-id": "1103.0632v1", 
    "author": "Boufaida Zizette", 
    "publish": "2011-03-03T09:44:06Z", 
    "summary": "The process-based semantic composition of Web Services is gaining a\nconsiderable momentum as an approach for the effective integration of\ndistributed, heterogeneous, and autonomous applications. To compose Web\nServices semantically, we need an ontology. There are several ways of inserting\nsemantics in Web Services. One of them consists of using description languages\nlike OWL-S. In this paper, we introduce our work which consists in the\nproposition of a new model and the use of semantic matching technology for\nsemantic and dynamic composition of ebXML business processes."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.0697v1", 
    "title": "A Wiki for Business Rules in Open Vocabulary, Executable English", 
    "arxiv-id": "1103.0697v1", 
    "author": "Adrian Walker", 
    "publish": "2011-03-03T14:31:32Z", 
    "summary": "The problem of business-IT alignment is of widespread economic concern.\n  As one way of addressing the problem, this paper describes an online system\nthat functions as a kind of Wiki -- one that supports the collaborative writing\nand running of business and scientific applications, as rules in open\nvocabulary, executable English, using a browser.\n  Since the rules are in English, they are indexed by Google and other search\nengines. This is useful when looking for rules for a task that one has in mind.\n  The design of the system integrates the semantics of data, with a semantics\nof an inference method, and also with the meanings of English sentences. As\nsuch, the system has functionality that may be useful for the Rules, Logic,\nProof and Trust requirements of the Semantic Web.\n  The system accepts rules, and small numbers of facts, typed or copy-pasted\ndirectly into a browser. One can then run the rules, again using a browser. For\nlarger amounts of data, the system uses information in the rules to\nautomatically generate and run SQL over networked databases. From a few highly\ndeclarative rules, the system typically generates SQL that would be too\ncomplicated to write reliably by hand. However, the system can explain its\nresults in step-by-step hypertexted English, at the business or scientific\nlevel\n  As befits a Wiki, shared use of the system is free."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.1003v1", 
    "title": "Teraflop-scale Incremental Machine Learning", 
    "arxiv-id": "1103.1003v1", 
    "author": "Eray \u00d6zkural", 
    "publish": "2011-03-05T03:41:30Z", 
    "summary": "We propose a long-term memory design for artificial general intelligence\nbased on Solomonoff's incremental machine learning methods. We use R5RS Scheme\nand its standard library with a few omissions as the reference machine. We\nintroduce a Levin Search variant based on Stochastic Context Free Grammar\ntogether with four synergistic update algorithms that use the same grammar as a\nguiding probability distribution of programs. The update algorithms include\nadjusting production probabilities, re-using previous solutions, learning\nprogramming idioms and discovery of frequent subprograms. Experiments with two\ntraining sequences demonstrate that our approach to incremental learning is\neffective."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.1157v2", 
    "title": "GRASP and path-relinking for Coalition Structure Generation", 
    "arxiv-id": "1103.1157v2", 
    "author": "Floriana Esposito", 
    "publish": "2011-03-06T18:54:04Z", 
    "summary": "In Artificial Intelligence with Coalition Structure Generation (CSG) one\nrefers to those cooperative complex problems that require to find an optimal\npartition, maximising a social welfare, of a set of entities involved in a\nsystem into exhaustive and disjoint coalitions. The solution of the CSG problem\nfinds applications in many fields such as Machine Learning (covering machines,\nclustering), Data Mining (decision tree, discretization), Graph Theory, Natural\nLanguage Processing (aggregation), Semantic Web (service composition), and\nBioinformatics. The problem of finding the optimal coalition structure is\nNP-complete. In this paper we present a greedy adaptive search procedure\n(GRASP) with path-relinking to efficiently search the space of coalition\nstructures. Experiments and comparisons to other algorithms prove the validity\nof the proposed method in solving this hard combinatorial problem."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TFUZZ.2011.2117431", 
    "link": "http://arxiv.org/pdf/1103.1205v1", 
    "title": "A Directional Feature with Energy based Offline Signature Verification   Network", 
    "arxiv-id": "1103.1205v1", 
    "author": "Pratibha Singh", 
    "publish": "2011-03-07T07:17:13Z", 
    "summary": "Signature used as a biometric is implemented in various systems as well as\nevery signature signed by each person is distinct at the same time. So, it is\nvery important to have a computerized signature verification system. In offline\nsignature verification system dynamic features are not available obviously, but\none can use a signature as an image and apply image processing techniques to\nmake an effective offline signature verification system. Author proposes a\nintelligent network used directional feature and energy density both as inputs\nto the same network and classifies the signature. Neural network is used as a\nclassifier for this system. The results are compared with both the very basic\nenergy density method and a simple directional feature method of offline\nsignature verification system and this proposed new network is found very\neffective as compared to the above two methods, specially for less number of\ntraining samples, which can be implemented practically."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1869", 
    "link": "http://arxiv.org/pdf/1103.1711v1", 
    "title": "Planning Graph Heuristics for Belief Space Search", 
    "arxiv-id": "1103.1711v1", 
    "author": "D. E. Smith", 
    "publish": "2011-03-09T06:43:55Z", 
    "summary": "Some recent works in conditional planning have proposed reachability\nheuristics to improve planner scalability, but many lack a formal description\nof the properties of their distance estimates. To place previous work in\ncontext and extend work on heuristics for conditional planning, we provide a\nformal basis for distance estimates between belief states. We give a definition\nfor the distance between belief states that relies on aggregating underlying\nstate distance measures. We give several techniques to aggregate state\ndistances and their associated properties. Many existing heuristics exhibit a\nsubset of the properties, but in order to provide a standardized comparison we\npresent several generalizations of planning graph heuristics that are used in a\nsingle planner. We compliment our belief state distance estimate framework by\nalso investigating efficient planning graph data structures that incorporate\nBDDs to compute the most effective heuristics.\n  We developed two planners to serve as test-beds for our investigation. The\nfirst, CAltAlt, is a conformant regression planner that uses A* search. The\nsecond, POND, is a conditional progression planner that uses AO* search. We\nshow the relative effectiveness of our heuristic techniques within these\nplanners. We also compare the performance of these planners with several state\nof the art approaches in conditional planning."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1869", 
    "link": "http://arxiv.org/pdf/1103.2091v1", 
    "title": "An Artificial Immune System Model for Multi-Agents Resource Sharing in   Distributed Environments", 
    "arxiv-id": "1103.2091v1", 
    "author": "M. K. Ghose", 
    "publish": "2011-02-24T09:12:17Z", 
    "summary": "Natural Immune system plays a vital role in the survival of the all living\nbeing. It provides a mechanism to defend itself from external predates making\nit consistent systems, capable of adapting itself for survival incase of\nchanges. The human immune system has motivated scientists and engineers for\nfinding powerful information processing algorithms that has solved complex\nengineering tasks. This paper explores one of the various possibilities for\nsolving problem in a Multiagent scenario wherein multiple robots are deployed\nto achieve a goal collectively. The final goal is dependent on the performance\nof individual robot and its survival without having to lose its energy beyond a\npredetermined threshold value by deploying an evolutionary computational\ntechnique otherwise called the artificial immune system that imitates the\nbiological immune system."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1869", 
    "link": "http://arxiv.org/pdf/1103.2342v1", 
    "title": "SPPAM - Statistical PreProcessing AlgorithM", 
    "arxiv-id": "1103.2342v1", 
    "author": "In\u00eas Dutra", 
    "publish": "2011-03-11T18:58:40Z", 
    "summary": "Most machine learning tools work with a single table where each row is an\ninstance and each column is an attribute. Each cell of the table contains an\nattribute value for an instance. This representation prevents one important\nform of learning, which is, classification based on groups of correlated\nrecords, such as multiple exams of a single patient, internet customer\npreferences, weather forecast or prediction of sea conditions for a given day.\nTo some extent, relational learning methods, such as inductive logic\nprogramming, can capture this correlation through the use of intensional\npredicates added to the background knowledge. In this work, we propose SPPAM,\nan algorithm that aggregates past observations in one single record. We show\nthat applying SPPAM to the original correlated data, before the learning task,\ncan produce classifiers that are better than the ones trained using all\nrecords."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1869", 
    "link": "http://arxiv.org/pdf/1103.2376v1", 
    "title": "Language, Emotions, and Cultures: Emotional Sapir-Whorf Hypothesis", 
    "arxiv-id": "1103.2376v1", 
    "author": "Leonid Perlovsky", 
    "publish": "2011-03-11T21:13:38Z", 
    "summary": "An emotional version of Sapir-Whorf hypothesis suggests that differences in\nlanguage emotionalities influence differences among cultures no less than\nconceptual differences. Conceptual contents of languages and cultures to\nsignificant extent are determined by words and their semantic differences;\nthese could be borrowed among languages and exchanged among cultures. Emotional\ndifferences, as suggested in the paper, are related to grammar and mostly\ncannot be borrowed. Conceptual and emotional mechanisms of languages are\nconsidered here along with their functions in the mind and cultural evolution.\nA fundamental contradiction in human mind is considered: language evolution\nrequires reduced emotionality, but \"too low\" emotionality makes language\n\"irrelevant to life,\" disconnected from sensory-motor experience. Neural\nmechanisms of these processes are suggested as well as their mathematical\nmodels: the knowledge instinct, the language instinct, the dual model\nconnecting language and cognition, dynamic logic, neural modeling fields.\nMathematical results are related to cognitive science, linguistics, and\npsychology. Experimental evidence and theoretical arguments are discussed.\nApproximate equations for evolution of human minds and cultures are obtained.\nTheir solutions identify three types of cultures: \"conceptual\"-pragmatic\ncultures, in which emotionality of language is reduced and differentiation\novertakes synthesis resulting in fast evolution at the price of uncertainty of\nvalues, self doubts, and internal crises; \"traditional-emotional\" cultures\nwhere differentiation lags behind synthesis, resulting in cultural stability at\nthe price of stagnation; and \"multi-cultural\" societies combining fast cultural\nevolution and stability. Unsolved problems and future theoretical and\nexperimental directions are discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10115-012-0525-6", 
    "link": "http://arxiv.org/pdf/1103.3123v2", 
    "title": "Reduced Ordered Binary Decision Diagram with Implied Literals: A New   knowledge Compilation Approach", 
    "arxiv-id": "1103.3123v2", 
    "author": "Shengsheng Wang", 
    "publish": "2011-03-16T08:12:05Z", 
    "summary": "Knowledge compilation is an approach to tackle the computational\nintractability of general reasoning problems. According to this approach,\nknowledge bases are converted off-line into a target compilation language which\nis tractable for on-line querying. Reduced ordered binary decision diagram\n(ROBDD) is one of the most influential target languages. We generalize ROBDD by\nassociating some implied literals in each node and the new language is called\nreduced ordered binary decision diagram with implied literals (ROBDD-L). Then\nwe discuss a kind of subsets of ROBDD-L called ROBDD-i with precisely i implied\nliterals (0 \\leq i \\leq \\infty). In particular, ROBDD-0 is isomorphic to ROBDD;\nROBDD-\\infty requires that each node should be associated by the implied\nliterals as many as possible. We show that ROBDD-i has uniqueness over some\nspecific variables order, and ROBDD-\\infty is the most succinct subset in\nROBDD-L and can meet most of the querying requirements involved in the\nknowledge compilation map. Finally, we propose an ROBDD-i compilation algorithm\nfor any i and a ROBDD-\\infty compilation algorithm. Based on them, we implement\na ROBDD-L package called BDDjLu and then get some conclusions from preliminary\nexperimental results: ROBDD-\\infty is obviously smaller than ROBDD for all\nbenchmarks; ROBDD-\\infty is smaller than the d-DNNF the benchmarks whose\ncompilation results are relatively small; it seems that it is better to\ntransform ROBDDs-\\infty into FBDDs and ROBDDs rather than straight compile the\nbenchmarks."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10115-012-0525-6", 
    "link": "http://arxiv.org/pdf/1103.3223v1", 
    "title": "Using Soft Computer Techniques on Smart Devices for Monitoring Chronic   Diseases: the CHRONIOUS case", 
    "arxiv-id": "1103.3223v1", 
    "author": "Roberto Rosso", 
    "publish": "2011-03-16T16:28:00Z", 
    "summary": "CHRONIOUS is an Open, Ubiquitous and Adaptive Chronic Disease Management\nPlatform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease\n(CKD) and Renal Insufficiency. It consists of several modules: an ontology\nbased literature search engine, a rule based decision support system, remote\nsensors interacting with lifestyle interfaces (PDA, monitor touchscreen) and a\nmachine learning module. All these modules interact each other to allow the\nmonitoring of two types of chronic diseases and to help clinician in taking\ndecision for cure purpose. This paper illustrates how some machine learning\nalgorithms and a rule based decision support system can be used in smart\ndevices, to monitor chronic patient. We will analyse how a set of machine\nlearning algorithms can be used in smart devices to alert the clinician in case\nof a patient health condition worsening trend."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TNET.2012.2222923", 
    "link": "http://arxiv.org/pdf/1103.3240v4", 
    "title": "Decentralized Constraint Satisfaction", 
    "arxiv-id": "1103.3240v4", 
    "author": "D. J. Leith", 
    "publish": "2011-03-02T15:00:09Z", 
    "summary": "We show that several important resource allocation problems in wireless\nnetworks fit within the common framework of Constraint Satisfaction Problems\n(CSPs). Inspired by the requirements of these applications, where variables are\nlocated at distinct network devices that may not be able to communicate but may\ninterfere, we define natural criteria that a CSP solver must possess in order\nto be practical. We term these algorithms decentralized CSP solvers. The best\nknown CSP solvers were designed for centralized problems and do not meet these\ncriteria. We introduce a stochastic decentralized CSP solver and prove that it\nwill find a solution in almost surely finite time, should one exist, also\nshowing it has many practically desirable properties. We benchmark the\nalgorithm's performance on a well-studied class of CSPs, random k-SAT,\nillustrating that the time the algorithm takes to find a satisfying assignment\nis competitive with stochastic centralized solvers on problems with order a\nthousand variables despite its decentralized nature. We demonstrate the\nsolver's practical utility for the problems that motivated its introduction by\nusing it to find a non-interfering channel allocation for a network formed from\ndata from downtown Manhattan."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TNET.2012.2222923", 
    "link": "http://arxiv.org/pdf/1103.3417v1", 
    "title": "Finding Shortest Path for Developed Cognitive Map Using Medial Axis", 
    "arxiv-id": "1103.3417v1", 
    "author": "Suhaib I. Al-Ghazi", 
    "publish": "2011-03-17T14:02:50Z", 
    "summary": "this paper presents an enhancement of the medial axis algorithm to be used\nfor finding the optimal shortest path for developed cognitive map. The\ncognitive map has been developed, based on the architectural blueprint maps.\nThe idea for using the medial-axis is to find main path central pixels; each\ncenter pixel represents the center distance between two side boarder pixels.\nThe need for these pixels in the algorithm comes from the need of building a\nnetwork of nodes for the path, where each node represents a turning in the real\nworld (left, right, critical left, critical right...). The algorithm also\nignores from finding the center pixels paths that are too small for intelligent\nrobot navigation. The Idea of this algorithm is to find the possible shortest\npath between start and end points. The goal of this research is to extract a\nsimple, robust representation of the shape of the cognitive map together with\nthe optimal shortest path between start and end points. The intelligent robot\nwill use this algorithm in order to decrease the time that is needed for\nsweeping the targeted building."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TNET.2012.2222923", 
    "link": "http://arxiv.org/pdf/1103.3420v1", 
    "title": "Extraction of handwritten areas from colored image of bank checks by an   hybrid method", 
    "arxiv-id": "1103.3420v1", 
    "author": "Samia Maddouri", 
    "publish": "2011-03-17T14:13:36Z", 
    "summary": "One of the first step in the realization of an automatic system of check\nrecognition is the extraction of the handwritten area. We propose in this paper\nan hybrid method to extract these areas. This method is based on digit\nrecognition by Fourier descriptors and different steps of colored image\nprocessing . It requires the bank recognition of its code which is located in\nthe check marking band as well as the handwritten color recognition by the\nmethod of difference of histograms. The areas extraction is then carried out by\nthe use of some mathematical morphology tools."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TNET.2012.2222923", 
    "link": "http://arxiv.org/pdf/1103.3687v1", 
    "title": "Cost Based Satisficing Search Considered Harmful", 
    "arxiv-id": "1103.3687v1", 
    "author": "Subbarao Kambhampati", 
    "publish": "2011-03-18T18:57:46Z", 
    "summary": "Recently, several researchers have found that cost-based satisficing search\nwith A* often runs into problems. Although some \"work arounds\" have been\nproposed to ameliorate the problem, there has not been any concerted effort to\npinpoint its origin. In this paper, we argue that the origins can be traced\nback to the wide variance in action costs that is observed in most planning\ndomains. We show that such cost variance misleads A* search, and that this is\nno trifling detail or accidental phenomenon, but a systemic weakness of the\nvery concept of \"cost-based evaluation functions + systematic search +\ncombinatorial graphs\". We show that satisficing search with sized-based\nevaluation functions is largely immune to this problem."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TNET.2012.2222923", 
    "link": "http://arxiv.org/pdf/1103.3745v1", 
    "title": "The AllDifferent Constraint with Precedences", 
    "arxiv-id": "1103.3745v1", 
    "author": "Toby Walsh", 
    "publish": "2011-03-19T03:50:45Z", 
    "summary": "We propose AllDiffPrecedence, a new global constraint that combines together\nan AllDifferent constraint with precedence constraints that strictly order\ngiven pairs of variables. We identify a number of applications for this global\nconstraint including instruction scheduling and symmetry breaking. We give an\nefficient propagation algorithm that enforces bounds consistency on this global\nconstraint. We show how to implement this propagator using a decomposition that\nextends the bounds consistency enforcing decomposition proposed for the\nAllDifferent constraint. Finally, we prove that enforcing domain consistency on\nthis global constraint is NP-hard in general."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068412000439", 
    "link": "http://arxiv.org/pdf/1103.3949v2", 
    "title": "A Goal-Directed Implementation of Query Answering for Hybrid MKNF   Knowledge Bases", 
    "arxiv-id": "1103.3949v2", 
    "author": "Terrance Swift", 
    "publish": "2011-03-21T09:51:36Z", 
    "summary": "Ontologies and rules are usually loosely coupled in knowledge representation\nformalisms. In fact, ontologies use open-world reasoning while the leading\nsemantics for rules use non-monotonic, closed-world reasoning. One exception is\nthe tightly-coupled framework of Minimal Knowledge and Negation as Failure\n(MKNF), which allows statements about individuals to be jointly derived via\nentailment from an ontology and inferences from rules. Nonetheless, the\npractical usefulness of MKNF has not always been clear, although recent work\nhas formalized a general resolution-based method for querying MKNF when rules\nare taken to have the well-founded semantics, and the ontology is modeled by a\ngeneral oracle. That work leaves open what algorithms should be used to relate\nthe entailments of the ontology and the inferences of rules. In this paper we\nprovide such algorithms, and describe the implementation of a query-driven\nsystem, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic)\nrules under the well-founded semantics and a (monotonic) ontology, represented\nby a CDF Type-1 (ALQ) theory. To appear in Theory and Practice of Logic\nProgramming (TPLP)"
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068412000439", 
    "link": "http://arxiv.org/pdf/1103.3954v1", 
    "title": "BoolVar/PB v1.0, a java library for translating pseudo-Boolean   constraints into CNF formulae", 
    "arxiv-id": "1103.3954v1", 
    "author": "Olivier Bailleux", 
    "publish": "2011-03-21T10:14:40Z", 
    "summary": "BoolVar/PB is an open source java library dedicated to the translation of\npseudo-Boolean constraints into CNF formulae. Input constraints can be\ncategorized with tags. Several encoding schemes are implemented in a way that\neach input constraint can be translated using one or several encoders,\naccording to the related tags. The library can be easily extended by adding new\nencoders and / or new output formats."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068412000439", 
    "link": "http://arxiv.org/pdf/1103.5034v1", 
    "title": "On Understanding and Machine Understanding", 
    "arxiv-id": "1103.5034v1", 
    "author": "Tong Chern", 
    "publish": "2011-03-24T03:35:24Z", 
    "summary": "In the present paper, we try to propose a self-similar network theory for the\nbasic understanding. By extending the natural languages to a kind of so called\nidealy sufficient language, we can proceed a few steps to the investigation of\nthe language searching and the language understanding of AI.\n  Image understanding, and the familiarity of the brain to the surrounding\nenvironment are also discussed. Group effects are discussed by addressing the\nessense of the power of influences, and constructing the influence network of a\nsociety. We also give a discussion of inspirations."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068411000317", 
    "link": "http://arxiv.org/pdf/1105.0288v1", 
    "title": "Splitting and Updating Hybrid Knowledge Bases (Extended Version)", 
    "arxiv-id": "1105.0288v1", 
    "author": "Terrance Swift", 
    "publish": "2011-05-02T10:12:59Z", 
    "summary": "Over the years, nonmonotonic rules have proven to be a very expressive and\nuseful knowledge representation paradigm. They have recently been used to\ncomplement the expressive power of Description Logics (DLs), leading to the\nstudy of integrative formal frameworks, generally referred to as hybrid\nknowledge bases, where both DL axioms and rules can be used to represent\nknowledge. The need to use these hybrid knowledge bases in dynamic domains has\ncalled for the development of update operators, which, given the substantially\ndifferent way Description Logics and rules are usually updated, has turned out\nto be an extremely difficult task.\n  In [SL10], a first step towards addressing this problem was taken, and an\nupdate operator for hybrid knowledge bases was proposed. Despite its\nsignificance -- not only for being the first update operator for hybrid\nknowledge bases in the literature, but also because it has some applications -\nthis operator was defined for a restricted class of problems where only the\nABox was allowed to change, which considerably diminished its applicability.\nMany applications that use hybrid knowledge bases in dynamic scenarios require\nboth DL axioms and rules to be updated.\n  In this paper, motivated by real world applications, we introduce an update\noperator for a large class of hybrid knowledge bases where both the DL\ncomponent as well as the rule component are allowed to dynamically change. We\nintroduce splitting sequences and splitting theorem for hybrid knowledge bases,\nuse them to define a modular update semantics, investigate its basic\nproperties, and illustrate its use on a realistic example about cargo imports."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068411000214", 
    "link": "http://arxiv.org/pdf/1105.0650v1", 
    "title": "Transition Systems for Model Generators - A Unifying Approach", 
    "arxiv-id": "1105.0650v1", 
    "author": "Miroslaw Truszczynski", 
    "publish": "2011-05-03T18:29:58Z", 
    "summary": "A fundamental task for propositional logic is to compute models of\npropositional formulas. Programs developed for this task are called\nsatisfiability solvers. We show that transition systems introduced by\nNieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers\ncan be adapted for solvers developed for two other propositional formalisms:\nlogic programming under the answer-set semantics, and the logic PC(ID). We show\nthat in each case the task of computing models can be seen as \"satisfiability\nmodulo answer-set programming,\" where the goal is to find a model of a theory\nthat also is an answer set of a certain program. The unifying perspective we\ndevelop shows, in particular, that solvers CLASP and MINISATID are closely\nrelated despite being developed for different formalisms, one for answer-set\nprogramming and the latter for the logic PC(ID)."
},{
    "category": "cs.AI", 
    "doi": "10.1017/S1471068411000214", 
    "link": "http://arxiv.org/pdf/1105.0974v1", 
    "title": "GANC: Greedy Agglomerative Normalized Cut", 
    "arxiv-id": "1105.0974v1", 
    "author": "Michael Rabbat", 
    "publish": "2011-05-05T04:55:53Z", 
    "summary": "This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00170-010-2802-4", 
    "link": "http://arxiv.org/pdf/1105.1247v1", 
    "title": "Machine-Part cell formation through visual decipherable clustering of   Self Organizing Map", 
    "arxiv-id": "1105.1247v1", 
    "author": "Pranab K. Dan", 
    "publish": "2011-05-06T09:27:49Z", 
    "summary": "Machine-part cell formation is used in cellular manufacturing in order to\nprocess a large variety, quality, lower work in process levels, reducing\nmanufacturing lead-time and customer response time while retaining flexibility\nfor new products. This paper presents a new and novel approach for obtaining\nmachine cells and part families. In the cellular manufacturing the fundamental\nproblem is the formation of part families and machine cells. The present paper\ndeals with the Self Organising Map (SOM) method an unsupervised learning\nalgorithm in Artificial Intelligence, and has been used as a visually\ndecipherable clustering tool of machine-part cell formation. The objective of\nthe paper is to cluster the binary machine-part matrix through visually\ndecipherable cluster of SOM color-coding and labelling via the SOM map nodes in\nsuch a way that the part families are processed in that machine cells. The\nUmatrix, component plane, principal component projection, scatter plot and\nhistogram of SOM have been reported in the present work for the successful\nvisualization of the machine-part cell formation. Computational result with the\nproposed algorithm on a set of group technology problems available in the\nliterature is also presented. The proposed SOM approach produced solutions with\na grouping efficacy that is at least as good as any results earlier reported in\nthe literature and improved the grouping efficacy for 70% of the problems and\nfound immensely useful to both industry practitioners and researchers."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00170-010-2802-4", 
    "link": "http://arxiv.org/pdf/1105.1436v1", 
    "title": "Solving Rubik's Cube Using SAT Solvers", 
    "arxiv-id": "1105.1436v1", 
    "author": "Jingchao Chen", 
    "publish": "2011-05-07T12:07:49Z", 
    "summary": "Rubik's Cube is an easily-understood puzzle, which is originally called the\n\"magic cube\". It is a well-known planning problem, which has been studied for a\nlong time. Yet many simple properties remain unknown. This paper studies\nwhether modern SAT solvers are applicable to this puzzle. To our best\nknowledge, we are the first to translate Rubik's Cube to a SAT problem. To\nreduce the number of variables and clauses needed for the encoding, we replace\na naive approach of 6 Boolean variables to represent each color on each facelet\nwith a new approach of 3 or 2 Boolean variables. In order to be able to solve\nquickly Rubik's Cube, we replace the direct encoding of 18 turns with the layer\nencoding of 18-subtype turns based on 6-type turns. To speed up the solving\nfurther, we encode some properties of two-phase algorithm as an additional\nconstraint, and restrict some move sequences by adding some constraint clauses.\nUsing only efficient encoding cannot solve this puzzle. For this reason, we\nimprove the existing SAT solvers, and develop a new SAT solver based on\nPrecoSAT, though it is suited only for Rubik's Cube. The new SAT solver\nreplaces the lookahead solving strategy with an ALO (\\emph{at-least-one})\nsolving strategy, and decomposes the original problem into sub-problems. Each\nsub-problem is solved by PrecoSAT. The empirical results demonstrate both our\nSAT translation and new solving technique are efficient. Without the efficient\nSAT encoding and the new solving technique, Rubik's Cube will not be able to be\nsolved still by any SAT solver. Using the improved SAT solver, we can find\nalways a solution of length 20 in a reasonable time. Although our solver is\nslower than Kociemba's algorithm using lookup tables, but does not require a\nhuge lookup table."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00170-010-2802-4", 
    "link": "http://arxiv.org/pdf/1105.1929v1", 
    "title": "The Hidden Web, XML and Semantic Web: A Scientific Data Management   Perspective", 
    "arxiv-id": "1105.1929v1", 
    "author": "Pierre Senellart", 
    "publish": "2011-05-10T12:33:41Z", 
    "summary": "The World Wide Web no longer consists just of HTML pages. Our work sheds\nlight on a number of trends on the Internet that go beyond simple Web pages.\nThe hidden Web provides a wealth of data in semi-structured form, accessible\nthrough Web forms and Web services. These services, as well as numerous other\napplications on the Web, commonly use XML, the eXtensible Markup Language. XML\nhas become the lingua franca of the Internet that allows customized markups to\nbe defined for specific domains. On top of XML, the Semantic Web grows as a\ncommon structured data source. In this work, we first explain each of these\ndevelopments in detail. Using real-world examples from scientific domains of\ngreat interest today, we then demonstrate how these new developments can assist\nthe managing, harvesting, and organization of data on the Web. On the way, we\nalso illustrate the current research avenues in these domains. We believe that\nthis effort would help bridge multiple database tracks, thereby attracting\nresearchers with a view to extend database technology."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00170-010-2802-4", 
    "link": "http://arxiv.org/pdf/1105.2902v1", 
    "title": "A Multi-Purpose Scenario-based Simulator for Smart House Environments", 
    "arxiv-id": "1105.2902v1", 
    "author": "Ali Reza Manashty", 
    "publish": "2011-05-14T15:11:00Z", 
    "summary": "Developing smart house systems has been a great challenge for researchers and\nengineers in this area because of the high cost of implementation and\nevaluation process of these systems, while being very time consuming. Testing a\ndesigned smart house before actually building it is considered as an obstacle\ntowards an efficient smart house project. This is because of the variety of\nsensors, home appliances and devices available for a real smart environment. In\nthis paper, we present the design and implementation of a multi-purpose smart\nhouse simulation system for designing and simulating all aspects of a smart\nhouse environment. This simulator provides the ability to design the house plan\nand different virtual sensors and appliances in a two dimensional model of the\nvirtual house environment. This simulator can connect to any external smart\nhouse remote controlling system, providing evaluation capabilities to their\nsystem much easier than before. It also supports detailed adding of new\nemerging sensors and devices to help maintain its compatibility with future\nsimulation needs. Scenarios can also be defined for testing various possible\ncombinations of device states; so different criteria and variables can be\nsimply evaluated without the need of experimenting on a real environment."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00170-010-2802-4", 
    "link": "http://arxiv.org/pdf/1105.3486v1", 
    "title": "Xapagy: a cognitive architecture for narrative reasoning", 
    "arxiv-id": "1105.3486v1", 
    "author": "Ladislau B\u00f6l\u00f6ni", 
    "publish": "2011-05-17T20:28:31Z", 
    "summary": "We introduce the Xapagy cognitive architecture: a software system designed to\nperform narrative reasoning. The architecture has been designed from scratch to\nmodel and mimic the activities performed by humans when witnessing, reading,\nrecalling, narrating and talking about stories."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.533", 
    "link": "http://arxiv.org/pdf/1105.3635v1", 
    "title": "Probabilistic Inference from Arbitrary Uncertainty using Mixtures of   Factorized Generalized Gaussians", 
    "arxiv-id": "1105.3635v1", 
    "author": "A. Ruiz", 
    "publish": "2011-05-18T14:06:49Z", 
    "summary": "This paper presents a general and efficient framework for probabilistic\ninference and learning from arbitrary uncertain information. It exploits the\ncalculation properties of finite mixture models, conjugate families and\nfactorization. Both the joint probability density of the variables and the\nlikelihood function of the (objective or subjective) observation are\napproximated by a special mixture model, in such a way that any desired\nconditional distribution can be directly obtained without numerical\nintegration. We have developed an extended version of the expectation\nmaximization (EM) algorithm to estimate the parameters of mixture models from\nuncertain training examples (indirect observations). As a consequence, any\npiece of exact or uncertain information about both input and output values is\nconsistently handled in the inference and learning stages. This ability,\nextremely useful in certain situations, is not found in most alternative\nmethods. The proposed framework is formally justified from standard\nprobabilistic principles and illustrative examples are provided in the fields\nof nonparametric pattern classification, nonlinear regression and pattern\ncompletion. Finally, experiments on a real application and comparative results\nover standard databases provide empirical evidence of the utility of the method\nin a wide range of applications."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.533", 
    "link": "http://arxiv.org/pdf/1105.3821v1", 
    "title": "Ontological Crises in Artificial Agents' Value Systems", 
    "arxiv-id": "1105.3821v1", 
    "author": "Peter de Blanc", 
    "publish": "2011-05-19T09:32:46Z", 
    "summary": "Decision-theoretic agents predict and evaluate the results of their actions\nusing a model, or ontology, of their environment. An agent's goal, or utility\nfunction, may also be specified in terms of the states of, or entities within,\nits ontology. If the agent may upgrade or replace its ontology, it faces a\ncrisis: the agent's original goal may not be well-defined with respect to its\nnew ontology. This crisis must be resolved before the agent can make plans\ntowards achieving its goals.\n  We discuss in this paper which sorts of agents will undergo ontological\ncrises and why we may want to create such agents. We present some concrete\nexamples, and argue that a well-defined procedure for resolving ontological\ncrises is needed. We point to some possible approaches to solving this problem,\nand evaluate these methods on our examples."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.533", 
    "link": "http://arxiv.org/pdf/1105.3833v1", 
    "title": "Typical models: minimizing false beliefs", 
    "arxiv-id": "1105.3833v1", 
    "author": "Eliezer L. Lozinskii", 
    "publish": "2011-05-19T10:00:39Z", 
    "summary": "A knowledge system S describing a part of real world does in general not\ncontain complete information. Reasoning with incomplete information is prone to\nerrors since any belief derived from S may be false in the present state of the\nworld. A false belief may suggest wrong decisions and lead to harmful actions.\nSo an important goal is to make false beliefs as unlikely as possible. This\nwork introduces the notions of \"typical atoms\" and \"typical models\", and shows\nthat reasoning with typical models minimizes the expected number of false\nbeliefs over all ways of using incomplete information. Various properties of\ntypical models are studied, in particular, correctness and stability of beliefs\nsuggested by typical models, and their connection to oblivious reasoning."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.468", 
    "link": "http://arxiv.org/pdf/1105.5440v1", 
    "title": "The Ariadne's Clew Algorithm", 
    "arxiv-id": "1105.5440v1", 
    "author": "E. Mazer", 
    "publish": "2011-05-27T01:44:34Z", 
    "summary": "We present a new approach to path planning, called the \"Ariadne's clew\nalgorithm\". It is designed to find paths in high-dimensional continuous spaces\nand applies to robots with many degrees of freedom in static, as well as\ndynamic environments - ones where obstacles may move. The Ariadne's clew\nalgorithm comprises two sub-algorithms, called Search and Explore, applied in\nan interleaved manner. Explore builds a representation of the accessible space\nwhile Search looks for the target. Both are posed as optimization problems. We\ndescribe a real implementation of the algorithm to plan paths for a six degrees\nof freedom arm in a dynamic environment where another six degrees of freedom\narm is used as a moving obstacle. Experimental results show that a path is\nfound in about one second without any pre-processing."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.477", 
    "link": "http://arxiv.org/pdf/1105.5441v1", 
    "title": "Computational Aspects of Reordering Plans", 
    "arxiv-id": "1105.5441v1", 
    "author": "C. Backstrom", 
    "publish": "2011-05-27T01:44:57Z", 
    "summary": "This article studies the problem of modifying the action ordering of a plan\nin order to optimise the plan according to various criteria. One of these\ncriteria is to make a plan less constrained and the other is to minimize its\nparallel execution time. Three candidate definitions are proposed for the first\nof these criteria, constituting a sequence of increasing optimality guarantees.\nTwo of these are based on deordering plans, which means that ordering relations\nmay only be removed, not added, while the third one uses reordering, where\narbitrary modifications to the ordering are allowed. It is shown that only the\nweakest one of the three criteria is tractable to achieve, the other two being\nNP-hard and even difficult to approximate. Similarly, optimising the parallel\nexecution time of a plan is studied both for deordering and reordering of\nplans. In the general case, both of these computations are NP-hard. However, it\nis shown that optimal deorderings can be computed in polynomial time for a\nclass of planning languages based on the notions of producers, consumers and\nthreats, which includes most of the commonly used planning languages. Computing\noptimal reorderings can potentially lead to even faster parallel executions,\nbut this problem remains NP-hard and difficult to approximate even under quite\nsevere restrictions."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.509", 
    "link": "http://arxiv.org/pdf/1105.5442v1", 
    "title": "The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic   Inference", 
    "arxiv-id": "1105.5442v1", 
    "author": "S. Markovitch", 
    "publish": "2011-05-27T01:45:23Z", 
    "summary": "It is common to view programs as a combination of logic and control: the\nlogic part defines what the program must do, the control part -- how to do it.\nThe Logic Programming paradigm was developed with the intention of separating\nthe logic from the control. Recently, extensive research has been conducted on\nautomatic generation of control for logic programs. Only a few of these works\nconsidered the issue of automatic generation of control for improving the\nefficiency of logic programs. In this paper we present a novel algorithm for\nautomatic finding of lowest-cost subgoal orderings. The algorithm works using\nthe divide-and-conquer strategy. The given set of subgoals is partitioned into\nsmaller sets, based on co-occurrence of free variables. The subsets are ordered\nrecursively and merged, yielding a provably optimal order. We experimentally\ndemonstrate the utility of the algorithm by testing it in several domains, and\ndiscuss the possibilities of its cooperation with other existing methods."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.512", 
    "link": "http://arxiv.org/pdf/1105.5443v1", 
    "title": "The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem", 
    "arxiv-id": "1105.5443v1", 
    "author": "B. Vandegriend", 
    "publish": "2011-05-27T01:45:52Z", 
    "summary": "Using an improved backtrack algorithm with sophisticated pruning techniques,\nwe revise previous observations correlating a high frequency of hard to solve\nHamiltonian Cycle instances with the Gn,m phase transition between\nHamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500\nvertices are easily solved. When we artificially restrict the degree sequence\nwith a bounded maximum degree, although there is some increase in difficulty,\nthe frequency of hard graphs is still low. When we consider more regular graphs\nbased on a generalization of knight's tours, we observe frequent instances of\nreally hard graphs, but on these the average degree is bounded by a constant.\nWe design a set of graphs with a feature our algorithm is unable to detect and\nso are very hard for our algorithm, but in these we can vary the average degree\nfrom O(1) to O(n). We have so far found no class of graphs correlated with the\nGn,m phase transition which asymptotically produces a high frequency of hard\ninstances."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.514", 
    "link": "http://arxiv.org/pdf/1105.5444v1", 
    "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its   Application to Problems of Ambiguity in Natural Language", 
    "arxiv-id": "1105.5444v1", 
    "author": "P. Resnik", 
    "publish": "2011-05-27T01:46:05Z", 
    "summary": "This article presents a measure of semantic similarity in an IS-A taxonomy\nbased on the notion of shared information content. Experimental evaluation\nagainst a benchmark set of human similarity judgments demonstrates that the\nmeasure performs better than the traditional edge-counting approach. The\narticle presents algorithms that take advantage of taxonomic similarity in\nresolving syntactic and semantic ambiguity, along with experimental results\ndemonstrating their effectiveness."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.516", 
    "link": "http://arxiv.org/pdf/1105.5446v1", 
    "title": "A Temporal Description Logic for Reasoning about Actions and Plans", 
    "arxiv-id": "1105.5446v1", 
    "author": "E. Franconi", 
    "publish": "2011-05-27T01:46:39Z", 
    "summary": "A class of interval-based temporal languages for uniformly representing and\nreasoning about actions and plans is presented. Actions are represented by\ndescribing what is true while the action itself is occurring, and plans are\nconstructed by temporally relating actions and world states. The temporal\nlanguages are members of the family of Description Logics, which are\ncharacterized by high expressivity combined with good computational properties.\nThe subsumption problem for a class of temporal Description Logics is\ninvestigated and sound and complete decision procedures are given. The basic\nlanguage TL-F is considered first: it is the composition of a temporal logic TL\n-- able to express interval temporal networks -- together with the non-temporal\nlogic F -- a Feature Description Logic. It is proven that subsumption in this\nlanguage is an NP-complete problem. Then it is shown how to reason with the\nmore expressive languages TLU-FU and TL-ALCF. The former adds disjunction both\nat the temporal and non-temporal sides of the language, the latter extends the\nnon-temporal side with set-valued features (i.e., roles) and a propositionally\ncomplete language."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.518", 
    "link": "http://arxiv.org/pdf/1105.5447v1", 
    "title": "Adaptive Parallel Iterative Deepening Search", 
    "arxiv-id": "1105.5447v1", 
    "author": "R. C. Varnell", 
    "publish": "2011-05-27T01:47:18Z", 
    "summary": "Many of the artificial intelligence techniques developed to date rely on\nheuristic search through large spaces. Unfortunately, the size of these spaces\nand the corresponding computational effort reduce the applicability of\notherwise novel and effective algorithms. A number of parallel and distributed\napproaches to search have considerably improved the performance of the search\nprocess. Our goal is to develop an architecture that automatically selects\nparallel search strategies for optimal performance on a variety of search\nproblems. In this paper we describe one such architecture realized in the\nEureka system, which combines the benefits of many different approaches to\nparallel heuristic search. Through empirical and theoretical analyses we\nobserve that features of the problem space directly affect the choice of\noptimal parallel search strategy. We then employ machine learning techniques to\nselect the optimal parallel search strategy for a given problem space. When a\nnew search task is input to the system, Eureka uses features describing the\nsearch space and the chosen architecture to automatically select the\nappropriate search strategy. Eureka has been tested on a MIMD parallel\nprocessor, a distributed network of workstations, and a single workstation\nusing multithreading. Results generated from fifteen puzzle problems, robot arm\nmotion problems, artificial search spaces, and planning problems indicate that\nEureka outperforms any of the tested strategies used exclusively for all\nproblem instances and is able to greatly reduce the search time for these\napplications."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.520", 
    "link": "http://arxiv.org/pdf/1105.5448v1", 
    "title": "Order of Magnitude Comparisons of Distance", 
    "arxiv-id": "1105.5448v1", 
    "author": "E. Davis", 
    "publish": "2011-05-27T01:47:48Z", 
    "summary": "Order of magnitude reasoning - reasoning by rough comparisons of the sizes of\nquantities - is often called 'back of the envelope calculation', with the\nimplication that the calculations are quick though approximate. This paper\nexhibits an interesting class of constraint sets in which order of magnitude\nreasoning is demonstrably fast. Specifically, we present a polynomial-time\nalgorithm that can solve a set of constraints of the form 'Points a and b are\nmuch closer together than points c and d.' We prove that this algorithm can be\napplied if `much closer together' is interpreted either as referring to an\ninfinite difference in scale or as referring to a finite difference in scale,\nas long as the difference in scale is greater than the number of variables in\nthe constraint set. We also prove that the first-order theory over such\nconstraints is decidable."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.530", 
    "link": "http://arxiv.org/pdf/1105.5449v1", 
    "title": "AntNet: Distributed Stigmergetic Control for Communications Networks", 
    "arxiv-id": "1105.5449v1", 
    "author": "M. Dorigo", 
    "publish": "2011-05-27T01:48:39Z", 
    "summary": "This paper introduces AntNet, a novel approach to the adaptive learning of\nrouting tables in communications networks. AntNet is a distributed, mobile\nagents based Monte Carlo system that was inspired by recent work on the ant\ncolony metaphor for solving optimization problems. AntNet's agents concurrently\nexplore the network and exchange collected information. The communication among\nthe agents is indirect and asynchronous, mediated by the network itself. This\nform of communication is typical of social insects and is called stigmergy. We\ncompare our algorithm with six state-of-the-art routing algorithms coming from\nthe telecommunications and machine learning fields. The algorithms' performance\nis evaluated over a set of realistic testbeds. We run many experiments over\nreal and artificial IP datagram networks with increasing number of nodes and\nunder several paradigmatic spatial and temporal traffic distributions. Results\nare very encouraging. AntNet showed superior performance under all the\nexperimental conditions with respect to its competitors. We analyze the main\ncharacteristics of the algorithm and try to explain the reasons for its\nsuperiority."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.536", 
    "link": "http://arxiv.org/pdf/1105.5450v1", 
    "title": "A Counter Example to Theorems of Cox and Fine", 
    "arxiv-id": "1105.5450v1", 
    "author": "J. Y. Halpern", 
    "publish": "2011-05-27T01:49:04Z", 
    "summary": "Cox's well-known theorem justifying the use of probability is shown not to\nhold in finite domains. The counterexample also suggests that Cox's assumptions\nare insufficient to prove the result even in infinite domains. The same\ncounterexample is used to disprove a result of Fine on comparative conditional\nprobability."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.544", 
    "link": "http://arxiv.org/pdf/1105.5451v1", 
    "title": "The Automatic Inference of State Invariants in TIM", 
    "arxiv-id": "1105.5451v1", 
    "author": "D. Long", 
    "publish": "2011-05-27T01:49:44Z", 
    "summary": "As planning is applied to larger and richer domains the effort involved in\nconstructing domain descriptions increases and becomes a significant burden on\nthe human application designer. If general planners are to be applied\nsuccessfully to large and complex domains it is necessary to provide the domain\ndesigner with some assistance in building correctly encoded domains. One way of\ndoing this is to provide domain-independent techniques for extracting, from a\ndomain description, knowledge that is implicit in that description and that can\nassist domain designers in debugging domain descriptions. This knowledge can\nalso be exploited to improve the performance of planners: several researchers\nhave explored the potential of state invariants in speeding up the performance\nof domain-independent planners. In this paper we describe a process by which\nstate invariants can be extracted from the automatically inferred type\nstructure of a domain. These techniques are being developed for exploitation by\nSTAN, a Graphplan based planner that employs state analysis techniques to\nenhance its performance."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.548", 
    "link": "http://arxiv.org/pdf/1105.5452v1", 
    "title": "Unifying Class-Based Representation Formalisms", 
    "arxiv-id": "1105.5452v1", 
    "author": "D. Nardi", 
    "publish": "2011-05-27T01:49:59Z", 
    "summary": "The notion of class is ubiquitous in computer science and is central in many\nformalisms for the representation of structured knowledge used both in\nknowledge representation and in databases. In this paper we study the basic\nissues underlying such representation formalisms and single out both their\ncommon characteristics and their distinguishing features. Such investigation\nleads us to propose a unifying framework in which we are able to capture the\nfundamental aspects of several representation languages used in different\ncontexts. The proposed formalism is expressed in the style of description\nlogics, which have been introduced in knowledge representation as a means to\nprovide a semantically well-founded basis for the structural aspects of\nknowledge representation systems. The description logic considered in this\npaper is a subset of first order logic with nice computational characteristics.\nIt is quite expressive and features a novel combination of constructs that has\nnot been studied before. The distinguishing constructs are number restrictions,\nwhich generalize existence and functional dependencies, inverse roles, which\nallow one to refer to the inverse of a relationship, and possibly cyclic\nassertions, which are necessary for capturing real world domains. We are able\nto show that it is precisely such combination of constructs that makes our\nlogic powerful enough to model the essential set of features for defining class\nstructures that are common to frame systems, object-oriented database\nlanguages, and semantic data models. As a consequence of the established\ncorrespondences, several significant extensions of each of the above formalisms\nbecome available. The high expressiveness of the logic we propose and the need\nfor capturing the reasoning in different contexts forces us to distinguish\nbetween unrestricted and finite model reasoning. A notable feature of our\nproposal is that reasoning in both cases is decidable. We argue that, by virtue\nof the high expressive power and of the associated reasoning capabilities on\nboth unrestricted and finite models, our logic provides a common core for\nclass-based representation formalisms."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.554", 
    "link": "http://arxiv.org/pdf/1105.5453v1", 
    "title": "Complexity of Prioritized Default Logics", 
    "arxiv-id": "1105.5453v1", 
    "author": "J. Rintanen", 
    "publish": "2011-05-27T01:50:12Z", 
    "summary": "In default reasoning, usually not all possible ways of resolving conflicts\nbetween default rules are acceptable. Criteria expressing acceptable ways of\nresolving the conflicts may be hardwired in the inference mechanism, for\nexample specificity in inheritance reasoning can be handled this way, or they\nmay be given abstractly as an ordering on the default rules. In this article we\ninvestigate formalizations of the latter approach in Reiter's default logic.\nOur goal is to analyze and compare the computational properties of three such\nformalizations in terms of their computational complexity: the prioritized\ndefault logics of Baader and Hollunder, and Brewka, and a prioritized default\nlogic that is based on lexicographic comparison. The analysis locates the\npropositional variants of these logics on the second and third levels of the\npolynomial hierarchy, and identifies the boundary between tractable and\nintractable inference for restricted classes of prioritized default theories."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.561", 
    "link": "http://arxiv.org/pdf/1105.5454v1", 
    "title": "Squeaky Wheel Optimization", 
    "arxiv-id": "1105.5454v1", 
    "author": "D. E. Joslin", 
    "publish": "2011-05-27T01:51:22Z", 
    "summary": "We describe a general approach to optimization which we term `Squeaky Wheel'\nOptimization (SWO). In SWO, a greedy algorithm is used to construct a solution\nwhich is then analyzed to find the trouble spots, i.e., those elements, that,\nif improved, are likely to improve the objective function score. The results of\nthe analysis are used to generate new priorities that determine the order in\nwhich the greedy algorithm constructs the next solution. This\nConstruct/Analyze/Prioritize cycle continues until some limit is reached, or an\nacceptable solution is found. SWO can be viewed as operating on two search\nspaces: solutions and prioritizations. Successive solutions are only indirectly\nrelated, via the re-prioritization that results from analyzing the prior\nsolution. Similarly, successive prioritizations are generated by constructing\nand analyzing solutions. This `coupled search' has some interesting properties,\nwhich we discuss. We report encouraging experimental results on two domains,\nscheduling problems that arise in fiber-optic cable manufacturing, and graph\ncoloring problems. The fact that these domains are very different supports our\nclaim that SWO is a general technique for optimization."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.567", 
    "link": "http://arxiv.org/pdf/1105.5455v1", 
    "title": "Variational Cumulant Expansions for Intractable Distributions", 
    "arxiv-id": "1105.5455v1", 
    "author": "P. de van Laar", 
    "publish": "2011-05-27T01:51:46Z", 
    "summary": "Intractable distributions present a common difficulty in inference within the\nprobabilistic knowledge representation framework and variational methods have\nrecently been popular in providing an approximate solution. In this article, we\ndescribe a perturbational approach in the form of a cumulant expansion which,\nto lowest order, recovers the standard Kullback-Leibler variational bound.\nHigher-order terms describe corrections on the variational approach without\nincurring much further computational cost. The relationship to other\nperturbational approaches such as TAP is also elucidated. We demonstrate the\nmethod on a particular class of undirected graphical models, Boltzmann\nmachines, for which our simulation results confirm improved accuracy and\nenhanced stability during learning."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.570", 
    "link": "http://arxiv.org/pdf/1105.5457v1", 
    "title": "Efficient Implementation of the Plan Graph in STAN", 
    "arxiv-id": "1105.5457v1", 
    "author": "D. Long", 
    "publish": "2011-05-27T01:52:09Z", 
    "summary": "STAN is a Graphplan-based planner, so-called because it uses a variety of\nSTate ANalysis techniques to enhance its performance. STAN competed in the\nAIPS-98 planning competition where it compared well with the other competitors\nin terms of speed, finding solutions fastest to many of the problems posed.\nAlthough the domain analysis techniques STAN exploits are an important factor\nin its overall performance, we believe that the speed at which STAN solved the\ncompetition problems is largely due to the implementation of its plan graph.\nThe implementation is based on two insights: that many of the graph\nconstruction operations can be implemented as bit-level logical operations on\nbit vectors, and that the graph should not be explicitly constructed beyond the\nfix point. This paper describes the implementation of STAN's plan graph and\nprovides experimental results which demonstrate the circumstances under which\nadvantages can be obtained from using this implementation."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.573", 
    "link": "http://arxiv.org/pdf/1105.5458v1", 
    "title": "Cooperation between Top-Down and Bottom-Up Theorem Provers", 
    "arxiv-id": "1105.5458v1", 
    "author": "D. Fuchs", 
    "publish": "2011-05-27T01:52:28Z", 
    "summary": "Top-down and bottom-up theorem proving approaches each have specific\nadvantages and disadvantages. Bottom-up provers profit from strong redundancy\ncontrol but suffer from the lack of goal-orientation, whereas top-down provers\nare goal-oriented but often have weak calculi when their proof lengths are\nconsidered. In order to integrate both approaches, we try to achieve\ncooperation between a top-down and a bottom-up prover in two different ways:\nThe first technique aims at supporting a bottom-up with a top-down prover. A\ntop-down prover generates subgoal clauses, they are then processed by a\nbottom-up prover. The second technique deals with the use of bottom-up\ngenerated lemmas in a top-down prover. We apply our concept to the areas of\nmodel elimination and superposition. We discuss the ability of our techniques\nto shorten proofs as well as to reorder the search space in an appropriate\nmanner. Furthermore, in order to identify subgoal clauses and lemmas which are\nactually relevant for the proof task, we develop methods for a relevancy-based\nfiltering. Experiments with the provers SETHEO and SPASS performed in the\nproblem library TPTP reveal the high potential of our cooperation approaches."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.574", 
    "link": "http://arxiv.org/pdf/1105.5459v1", 
    "title": "Solving Highly Constrained Search Problems with Quantum Computers", 
    "arxiv-id": "1105.5459v1", 
    "author": "T. Hogg", 
    "publish": "2011-05-27T01:52:46Z", 
    "summary": "A previously developed quantum search algorithm for solving 1-SAT problems in\na single step is generalized to apply to a range of highly constrained k-SAT\nproblems. We identify a bound on the number of clauses in satisfiability\nproblems for which the generalized algorithm can find a solution in a constant\nnumber of steps as the number of variables increases. This performance\ncontrasts with the linear growth in the number of steps required by the best\nclassical algorithms, and the exponential number required by classical and\nquantum methods that ignore the problem structure. In some cases, the algorithm\ncan also guarantee that insoluble problems in fact have no solutions, unlike\npreviously proposed quantum search algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.575", 
    "link": "http://arxiv.org/pdf/1105.5460v1", 
    "title": "Decision-Theoretic Planning: Structural Assumptions and Computational   Leverage", 
    "arxiv-id": "1105.5460v1", 
    "author": "S. Hanks", 
    "publish": "2011-05-27T01:53:02Z", 
    "summary": "Planning under uncertainty is a central problem in the study of automated\nsequential decision making, and has been addressed by researchers in many\ndifferent fields, including AI planning, decision analysis, operations\nresearch, control theory and economics. While the assumptions and perspectives\nadopted in these areas often differ in substantial ways, many planning problems\nof interest to researchers in these fields can be modeled as Markov decision\nprocesses (MDPs) and analyzed using the techniques of decision theory. This\npaper presents an overview and synthesis of MDP-related methods, showing how\nthey provide a unifying framework for modeling many classes of planning\nproblems studied in AI. It also describes structural properties of MDPs that,\nwhen exhibited by particular classes of problems, can be exploited in the\nconstruction of optimal or approximately optimal policies or plans. Planning\nproblems commonly possess structure in the reward and value functions used to\ndescribe performance criteria, in the functions used to describe state\ntransitions and observations, and in the relationships among features used to\ndescribe states, actions, rewards, and observations. Specialized\nrepresentations, and algorithms employing these representations, can achieve\ncomputational leverage by exploiting these various forms of structure. Certain\nAI techniques -- in particular those based on the use of structured,\nintensional representations -- can be viewed in this way. This paper surveys\nseveral types of representations for both classical and decision-theoretic\nplanning problems, and planning algorithms that exploit these representations\nin a number of different ways to ease the computational burden of constructing\npolicies or plans. It focuses primarily on abstraction, aggregation and\ndecomposition techniques based on AI-style representations."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.577", 
    "link": "http://arxiv.org/pdf/1105.5461v1", 
    "title": "Probabilistic Deduction with Conditional Constraints over Basic Events", 
    "arxiv-id": "1105.5461v1", 
    "author": "T. Lukasiewicz", 
    "publish": "2011-05-27T01:53:20Z", 
    "summary": "We study the problem of probabilistic deduction with conditional constraints\nover basic events. We show that globally complete probabilistic deduction with\nconditional constraints over basic events is NP-hard. We then concentrate on\nthe special case of probabilistic deduction in conditional constraint trees. We\nelaborate very efficient techniques for globally complete probabilistic\ndeduction. In detail, for conditional constraint trees with point\nprobabilities, we present a local approach to globally complete probabilistic\ndeduction, which runs in linear time in the size of the conditional constraint\ntrees. For conditional constraint trees with interval probabilities, we show\nthat globally complete probabilistic deduction can be done in a global approach\nby solving nonlinear programs. We show how these nonlinear programs can be\ntransformed into equivalent linear programs, which are solvable in polynomial\ntime in the size of the conditional constraint trees."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.583", 
    "link": "http://arxiv.org/pdf/1105.5462v1", 
    "title": "Variational Probabilistic Inference and the QMR-DT Network", 
    "arxiv-id": "1105.5462v1", 
    "author": "M. I. Jordan", 
    "publish": "2011-05-27T01:53:36Z", 
    "summary": "We describe a variational approximation method for efficient inference in\nlarge-scale probabilistic models. Variational methods are deterministic\nprocedures that provide approximations to marginal and conditional\nprobabilities of interest. They provide alternatives to approximate inference\nmethods based on stochastic sampling or search. We describe a variational\napproach to the problem of diagnostic inference in the `Quick Medical\nReference' (QMR) network. The QMR network is a large-scale probabilistic\ngraphical model built on statistical and expert knowledge. Exact probabilistic\ninference is infeasible in this model for all but a small set of cases. We\nevaluate our variational inference algorithm on a large set of diagnostic test\ncases, comparing the algorithm to a state-of-the-art stochastic sampling\nmethod."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.584", 
    "link": "http://arxiv.org/pdf/1105.5463v1", 
    "title": "Extensible Knowledge Representation: the Case of Description Reasoners", 
    "arxiv-id": "1105.5463v1", 
    "author": "A. Borgida", 
    "publish": "2011-05-27T01:53:50Z", 
    "summary": "This paper offers an approach to extensible knowledge representation and\nreasoning for a family of formalisms known as Description Logics. The approach\nis based on the notion of adding new concept constructors, and includes a\nheuristic methodology for specifying the desired extensions, as well as a\nmodularized software architecture that supports implementing extensions. The\narchitecture detailed here falls in the normalize-compared paradigm, and\nsupports both intentional reasoning (subsumption) involving concepts, and\nextensional reasoning involving individuals after incremental updates to the\nknowledge base. The resulting approach can be used to extend the reasoner with\nspecialized notions that are motivated by specific problems or application\nareas, such as reasoning about dates, plans, etc. In addition, it provides an\nopportunity to implement constructors that are not currently yet sufficiently\nwell understood theoretically, but are needed in practice. Also, for\nconstructors that are provably hard to reason with (e.g., ones whose presence\nwould lead to undecidability), it allows the implementation of incomplete\nreasoners where the incompleteness is tailored to be acceptable for the\napplication at hand."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.591", 
    "link": "http://arxiv.org/pdf/1105.5465v1", 
    "title": "Constructing Conditional Plans by a Theorem-Prover", 
    "arxiv-id": "1105.5465v1", 
    "author": "J. Rintanen", 
    "publish": "2011-05-27T01:54:30Z", 
    "summary": "The research on conditional planning rejects the assumptions that there is no\nuncertainty or incompleteness of knowledge with respect to the state and\nchanges of the system the plans operate on. Without these assumptions the\nsequences of operations that achieve the goals depend on the initial state and\nthe outcomes of nondeterministic changes in the system. This setting raises the\nquestions of how to represent the plans and how to perform plan search. The\nanswers are quite different from those in the simpler classical framework. In\nthis paper, we approach conditional planning from a new viewpoint that is\nmotivated by the use of satisfiability algorithms in classical planning.\nTranslating conditional planning to formulae in the propositional logic is not\nfeasible because of inherent computational limitations. Instead, we translate\nconditional planning to quantified Boolean formulae. We discuss three\nformalizations of conditional planning as quantified Boolean formulae, and\npresent experimental results obtained with a theorem-prover."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.594", 
    "link": "http://arxiv.org/pdf/1105.5466v1", 
    "title": "Issues in Stacked Generalization", 
    "arxiv-id": "1105.5466v1", 
    "author": "I. H. Witten", 
    "publish": "2011-05-27T01:54:47Z", 
    "summary": "Stacked generalization is a general method of using a high-level model to\ncombine lower-level models to achieve greater predictive accuracy. In this\npaper we address two crucial issues which have been considered to be a `black\nart' in classification tasks ever since the introduction of stacked\ngeneralization in 1992 by Wolpert: the type of generalizer that is suitable to\nderive the higher-level model, and the kind of attributes that should be used\nas its input. We find that best results are obtained when the higher-level\nmodel combines the confidence (and not just the predictions) of the lower-level\nones. We demonstrate the effectiveness of stacked generalization for combining\nthree different types of learning algorithms for classification tasks. We also\ncompare the performance of stacked generalization with majority vote and\npublished results of arcing and bagging."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.594", 
    "link": "http://arxiv.org/pdf/1105.5516v3", 
    "title": "Ontology Alignment at the Instance and Schema Level", 
    "arxiv-id": "1105.5516v3", 
    "author": "Pierre Senellart", 
    "publish": "2011-05-27T10:18:08Z", 
    "summary": "We present PARIS, an approach for the automatic alignment of ontologies.\nPARIS aligns not only instances, but also relations and classes. Alignments at\nthe instance-level cross-fertilize with alignments at the schema-level.\nThereby, our system provides a truly holistic solution to the problem of\nontology alignment. The heart of the approach is probabilistic. This allows\nPARIS to run without any parameter tuning. We demonstrate the efficiency of the\nalgorithm and its precision through extensive experiments. In particular, we\nobtain a precision of around 90% in experiments with two of the world's largest\nontologies."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.594", 
    "link": "http://arxiv.org/pdf/1105.5667v1", 
    "title": "Complexity of and Algorithms for Borda Manipulation", 
    "arxiv-id": "1105.5667v1", 
    "author": "Toby Walsh", 
    "publish": "2011-05-27T23:11:40Z", 
    "summary": "We prove that it is NP-hard for a coalition of two manipulators to compute\nhow to manipulate the Borda voting rule. This resolves one of the last open\nproblems in the computational complexity of manipulating common voting rules.\nBecause of this NP-hardness, we treat computing a manipulation as an\napproximation problem where we try to minimize the number of manipulators.\nBased on ideas from bin packing and multiprocessor scheduling, we propose two\nnew approximation methods to compute manipulations of the Borda rule.\nExperiments show that these methods significantly outperform the previous best\nknown %existing approximation method. We are able to find optimal manipulations\nin almost all the randomly generated elections tested. Our results suggest\nthat, whilst computing a manipulation of the Borda rule by a coalition is\nNP-hard, computational complexity may provide only a weak barrier against\nmanipulation in practice."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.693", 
    "link": "http://arxiv.org/pdf/1105.6124v1", 
    "title": "Reasoning on Interval and Point-based Disjunctive Metric Constraints in   Temporal Contexts", 
    "arxiv-id": "1105.6124v1", 
    "author": "F. Barber", 
    "publish": "2011-05-30T22:09:11Z", 
    "summary": "We introduce a temporal model for reasoning on disjunctive metric constraints\non intervals and time points in temporal contexts. This temporal model is\ncomposed of a labeled temporal algebra and its reasoning algorithms. The\nlabeled temporal algebra defines labeled disjunctive metric point-based\nconstraints, where each disjunct in each input disjunctive constraint is\nunivocally associated to a label. Reasoning algorithms manage labeled\nconstraints, associated label lists, and sets of mutually inconsistent\ndisjuncts. These algorithms guarantee consistency and obtain a minimal network.\nAdditionally, constraints can be organized in a hierarchy of alternative\ntemporal contexts. Therefore, we can reason on context-dependent disjunctive\nmetric constraints on intervals and points. Moreover, the model is able to\nrepresent non-binary constraints, such that logical dependencies on disjuncts\nin constraints can be handled. The computational cost of reasoning algorithms\nis exponential in accordance with the underlying problem complexity, although\nsome improvements are proposed."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.693", 
    "link": "http://arxiv.org/pdf/1105.6148v2", 
    "title": "Overcoming Misleads In Logic Programs by Redefining Negation", 
    "arxiv-id": "1105.6148v2", 
    "author": "A. H. Naguib", 
    "publish": "2011-05-31T02:19:21Z", 
    "summary": "Negation as failure and incomplete information in logic programs have been\nstudied by many researchers In order to explains HOW a negated conclusion was\nreached, we introduce and proof a different way for negating facts to\novercoming misleads in logic programs. Negating facts can be achieved by asking\nthe user for constants that do not appear elsewhere in the knowledge base."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.693", 
    "link": "http://arxiv.org/pdf/1106.0171v1", 
    "title": "Proposal of Pattern Recognition as a necessary and sufficient Principle   to Cognitive Science", 
    "arxiv-id": "1106.0171v1", 
    "author": "Gilberto de Paiva", 
    "publish": "2011-05-31T06:40:52Z", 
    "summary": "Despite the prevalence of the Computational Theory of Mind and the\nConnectionist Model, the establishing of the key principles of the Cognitive\nScience are still controversy and inconclusive. This paper proposes the concept\nof Pattern Recognition as Necessary and Sufficient Principle for a general\ncognitive science modeling, in a very ambitious scientific proposal. A formal\nphysical definition of the pattern recognition concept is also proposed to\nsolve many key conceptual gaps on the field."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.601", 
    "link": "http://arxiv.org/pdf/1106.0218v1", 
    "title": "The Good Old Davis-Putnam Procedure Helps Counting Models", 
    "arxiv-id": "1106.0218v1", 
    "author": "E. L. Lozinskii", 
    "publish": "2011-06-01T16:14:46Z", 
    "summary": "As was shown recently, many important AI problems require counting the number\nof models of propositional formulas. The problem of counting models of such\nformulas is, according to present knowledge, computationally intractable in a\nworst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP,\nthat computes the exact number of models of a propositional CNF or DNF formula\nF. Let m and n be the number of clauses and variables of F, respectively, and\nlet p denote the probability that a literal l of F occurs in a clause C of F,\nthen the average running time of CDP is shown to be O(nm^d), where\nd=-1/log(1-p). The practical performance of CDP has been estimated in a series\nof experiments on a wide variety of CNF formulas."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.606", 
    "link": "http://arxiv.org/pdf/1106.0219v1", 
    "title": "Identifying Mislabeled Training Data", 
    "arxiv-id": "1106.0219v1", 
    "author": "M. A. Friedl", 
    "publish": "2011-06-01T16:15:28Z", 
    "summary": "This paper presents a new approach to identifying and eliminating mislabeled\ntraining instances for supervised learning. The goal of this approach is to\nimprove classification accuracies produced by learning algorithms by improving\nthe quality of the training data. Our approach uses a set of learning\nalgorithms to create classifiers that serve as noise filters for the training\ndata. We evaluate single algorithm, majority vote and consensus filters on five\ndatasets that are prone to labeling errors. Our experiments illustrate that\nfiltering significantly improves classification accuracy for noise levels up to\n30 percent. An analytical and empirical evaluation of the precision of our\napproach shows that consensus filters are conservative at throwing away good\ndata at the expense of retaining bad data and that majority filters are better\nat detecting bad data at the expense of throwing away good data. This suggests\nthat for situations in which there is a paucity of data, consensus filters are\npreferable, whereas majority vote filters are preferable for situations with an\nabundance of data."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.612", 
    "link": "http://arxiv.org/pdf/1106.0220v1", 
    "title": "Committee-Based Sample Selection for Probabilistic Classifiers", 
    "arxiv-id": "1106.0220v1", 
    "author": "I. Dagan", 
    "publish": "2011-06-01T16:15:56Z", 
    "summary": "In many real-world learning tasks, it is expensive to acquire a sufficient\nnumber of labeled examples for training. This paper investigates methods for\nreducing annotation cost by `sample selection'. In this approach, during\ntraining the learning program examines many unlabeled examples and selects for\nlabeling only those that are most informative at each stage. This avoids\nredundantly labeling examples that contribute little new information. Our work\nfollows on previous research on Query By Committee, extending the\ncommittee-based paradigm to the context of probabilistic classification. We\ndescribe a family of empirical methods for committee-based sample selection in\nprobabilistic classification models, which evaluate the informativeness of an\nexample by measuring the degree of disagreement between several model variants.\nThese variants (the committee) are drawn randomly from a probability\ndistribution conditioned by the training set labeled so far. The method was\napplied to the real-world natural language processing task of stochastic\npart-of-speech tagging. We find that all variants of the method achieve a\nsignificant reduction in annotation cost, although their computational\nefficiency differs. In particular, the simplest variant, a two member committee\nwith no parameters to tune, gives excellent results. We also show that sample\nselection yields a significant reduction in the size of the model used by the\ntagger."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.637", 
    "link": "http://arxiv.org/pdf/1106.0224v1", 
    "title": "Reasoning about Minimal Belief and Negation as Failure", 
    "arxiv-id": "1106.0224v1", 
    "author": "R. Rosati", 
    "publish": "2011-06-01T16:17:18Z", 
    "summary": "We investigate the problem of reasoning in the propositional fragment of\nMBNF, the logic of minimal belief and negation as failure introduced by\nLifschitz, which can be considered as a unifying framework for several\nnonmonotonic formalisms, including default logic, autoepistemic logic,\ncircumscription, epistemic queries, and logic programming. We characterize the\ncomplexity and provide algorithms for reasoning in propositional MBNF. In\nparticular, we show that entailment in propositional MBNF lies at the third\nlevel of the polynomial hierarchy, hence it is harder than reasoning in all the\nabove mentioned propositional formalisms for nonmonotonic reasoning. We also\nprove the exact correspondence between negation as failure in MBNF and negative\nintrospection in Moore's autoepistemic logic."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.638", 
    "link": "http://arxiv.org/pdf/1106.0225v1", 
    "title": "Randomized Algorithms for the Loop Cutset Problem", 
    "arxiv-id": "1106.0225v1", 
    "author": "D. Geiger", 
    "publish": "2011-06-01T16:17:38Z", 
    "summary": "We show how to find a minimum weight loop cutset in a Bayesian network with\nhigh probability. Finding such a loop cutset is the first step in the method of\nconditioning for inference. Our randomized algorithm for finding a loop cutset\noutputs a minimum loop cutset after O(c 6^k kn) steps with probability at least\n1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is\nthe minimal size of a minimum weight loop cutset, and n is the number of\nvertices. We also show empirically that a variant of this algorithm often finds\na loop cutset that is closer to the minimum weight loop cutset than the ones\nfound by the best deterministic algorithms known."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.649", 
    "link": "http://arxiv.org/pdf/1106.0229v1", 
    "title": "OBDD-based Universal Planning for Synchronized Agents in   Non-Deterministic Domains", 
    "arxiv-id": "1106.0229v1", 
    "author": "M. M. Veloso", 
    "publish": "2011-06-01T16:22:36Z", 
    "summary": "Recently model checking representation and search techniques were shown to be\nefficiently applicable to planning, in particular to non-deterministic\nplanning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDs)\nto encode a planning domain as a non-deterministic finite automaton and then\napply fast algorithms from model checking to search for a solution. OBDDs can\neffectively scale and can provide universal plans for complex planning domains.\nWe are particularly interested in addressing the complexities arising in\nnon-deterministic, multi-agent domains. In this article, we present UMOP, a new\nuniversal OBDD-based planning framework for non-deterministic, multi-agent\ndomains. We introduce a new planning domain description language, NADL, to\nspecify non-deterministic, multi-agent domains. The language contributes the\nexplicit definition of controllable agents and uncontrollable environment\nagents. We describe the syntax and semantics of NADL and show how to build an\nefficient OBDD-based representation of an NADL description. The UMOP planning\nsystem uses NADL and different OBDD-based universal planning algorithms. It\nincludes the previously developed strong and strong cyclic planning algorithms.\nIn addition, we introduce our new optimistic planning algorithm that relaxes\noptimality guarantees and generates plausible universal plans in some domains\nwhere no strong nor strong cyclic solution exists. We present empirical results\napplying UMOP to domains ranging from deterministic and single-agent with no\nenvironment actions to non-deterministic and multi-agent with complex\nenvironment actions. UMOP is shown to be a rich and efficient planning system."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.655", 
    "link": "http://arxiv.org/pdf/1106.0230v1", 
    "title": "Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP   Search Techniques in Graphplan", 
    "arxiv-id": "1106.0230v1", 
    "author": "S. Kambhampati", 
    "publish": "2011-06-01T16:22:50Z", 
    "summary": "This paper reviews the connections between Graphplan's planning-graph and the\ndynamic constraint satisfaction problem and motivates the need for adapting CSP\nsearch techniques to the Graphplan algorithm. It then describes how explanation\nbased learning, dependency directed backtracking, dynamic variable ordering,\nforward checking, sticky values and random-restart search strategies can be\nadapted to Graphplan. Empirical results are provided to demonstrate that these\naugmentations improve Graphplan's performance significantly (up to 1000x\nspeedups) on several benchmark problems. Special attention is paid to the\nexplanation-based learning and dependency directed backtracking techniques as\nthey are empirically found to be most useful in improving the performance of\nGraphplan."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.664", 
    "link": "http://arxiv.org/pdf/1106.0233v1", 
    "title": "Space Efficiency of Propositional Knowledge Representation Formalisms", 
    "arxiv-id": "1106.0233v1", 
    "author": "M. Schaerf", 
    "publish": "2011-06-01T16:24:29Z", 
    "summary": "We investigate the space efficiency of a Propositional Knowledge\nRepresentation (PKR) formalism. Intuitively, the space efficiency of a\nformalism F in representing a certain piece of knowledge A, is the size of the\nshortest formula of F that represents A. In this paper we assume that knowledge\nis either a set of propositional interpretations (models) or a set of\npropositional formulae (theorems). We provide a formal way of talking about the\nrelative ability of PKR formalisms to compactly represent a set of models or a\nset of theorems. We introduce two new compactness measures, the corresponding\nclasses, and show that the relative space efficiency of a PKR formalism in\nrepresenting models/theorems is directly related to such classes. In\nparticular, we consider formalisms for nonmonotonic reasoning, such as\ncircumscription and default logic, as well as belief revision operators and the\nstable model semantics for logic programs with negation. One interesting result\nis that formalisms with the same time complexity do not necessarily belong to\nthe same space efficiency class."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.678", 
    "link": "http://arxiv.org/pdf/1106.0234v1", 
    "title": "Value-Function Approximations for Partially Observable Markov Decision   Processes", 
    "arxiv-id": "1106.0234v1", 
    "author": "M. Hauskrecht", 
    "publish": "2011-06-01T16:24:43Z", 
    "summary": "Partially observable Markov decision processes (POMDPs) provide an elegant\nmathematical framework for modeling complex decision and planning problems in\nstochastic domains in which states of the system are observable only\nindirectly, via a set of imperfect or noisy observations. The modeling\nadvantage of POMDPs, however, comes at a price -- exact methods for solving\nthem are computationally very expensive and thus applicable in practice only to\nvery simple problems. We focus on efficient approximation (heuristic) methods\nthat attempt to alleviate the computational problem and trade off accuracy for\nspeed. We have two objectives here. First, we survey various approximation\nmethods, analyze their properties and relations and provide some new insights\ninto their differences. Second, we present a number of new approximation\nmethods and novel refinements of existing techniques. The theoretical results\nare supported by experiments on a problem from the agent navigation domain."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.689", 
    "link": "http://arxiv.org/pdf/1106.0237v1", 
    "title": "On Deducing Conditional Independence from d-Separation in Causal Graphs   with Feedback (Research Note)", 
    "arxiv-id": "1106.0237v1", 
    "author": "R. M. Neal", 
    "publish": "2011-06-01T16:36:47Z", 
    "summary": "Pearl and Dechter (1996) claimed that the d-separation criterion for\nconditional independence in acyclic causal networks also applies to networks of\ndiscrete variables that have feedback cycles, provided that the variables of\nthe system are uniquely determined by the random disturbances. I show by\nexample that this is not true in general. Some condition stronger than\nuniqueness is needed, such as the existence of a causal dynamics guaranteed to\nlead to the unique solution."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.702", 
    "link": "http://arxiv.org/pdf/1106.0238v1", 
    "title": "What's in an Attribute? Consequences for the Least Common Subsumer", 
    "arxiv-id": "1106.0238v1", 
    "author": "R. Kusters", 
    "publish": "2011-06-01T16:36:59Z", 
    "summary": "Functional relationships between objects, called `attributes', are of\nconsiderable importance in knowledge representation languages, including\nDescription Logics (DLs). A study of the literature indicates that papers have\nmade, often implicitly, different assumptions about the nature of attributes:\nwhether they are always required to have a value, or whether they can be\npartial functions. The work presented here is the first explicit study of this\ndifference for subclasses of the CLASSIC DL, involving the same-as concept\nconstructor. It is shown that although determining subsumption between concept\ndescriptions has the same complexity (though requiring different algorithms),\nthe story is different in the case of determining the least common subsumer\n(lcs). For attributes interpreted as partial functions, the lcs exists and can\nbe computed relatively easily; even in this case our results correct and extend\nthree previous papers about the lcs of DLs. In the case where attributes must\nhave a value, the lcs may not exist, and even if it exists it may be of\nexponential size. Interestingly, it is possible to decide in polynomial time if\nthe lcs exists."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.705", 
    "link": "http://arxiv.org/pdf/1106.0239v1", 
    "title": "The Complexity of Reasoning with Cardinality Restrictions and Nominals   in Expressive Description Logics", 
    "arxiv-id": "1106.0239v1", 
    "author": "S. Tobies", 
    "publish": "2011-06-01T16:37:11Z", 
    "summary": "We study the complexity of the combination of the Description Logics ALCQ and\nALCQI with a terminological formalism based on cardinality restrictions on\nconcepts. These combinations can naturally be embedded into C^2, the two\nvariable fragment of predicate logic with counting quantifiers, which yields\ndecidability in NExpTime. We show that this approach leads to an optimal\nsolution for ALCQI, as ALCQI with cardinality restrictions has the same\ncomplexity as C^2 (NExpTime-complete). In contrast, we show that for ALCQ, the\nproblem can be solved in ExpTime. This result is obtained by a reduction of\nreasoning with cardinality restrictions to reasoning with the (in general\nweaker) terminological formalism of general axioms for ALCQ extended with\nnominals. Using the same reduction, we show that, for the extension of ALCQI\nwith nominals, reasoning with general axioms is a NExpTime-complete problem.\nFinally, we sharpen this result and show that pure concept satisfiability for\nALCQI with nominals is NExpTime-complete. Without nominals, this problem is\nknown to be PSpace-complete."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.711", 
    "link": "http://arxiv.org/pdf/1106.0240v1", 
    "title": "Backbone Fragility and the Local Search Cost Peak", 
    "arxiv-id": "1106.0240v1", 
    "author": "A. Smaill", 
    "publish": "2011-06-01T16:37:25Z", 
    "summary": "The local search algorithm WSat is one of the most successful algorithms for\nsolving the satisfiability (SAT) problem. It is notably effective at solving\nhard Random 3-SAT instances near the so-called `satisfiability threshold', but\nstill shows a peak in search cost near the threshold and large variations in\ncost over different instances. We make a number of significant contributions to\nthe analysis of WSat on high-cost random instances, using the\nrecently-introduced concept of the backbone of a SAT instance. The backbone is\nthe set of literals which are entailed by an instance. We find that the number\nof solutions predicts the cost well for small-backbone instances but is much\nless relevant for the large-backbone instances which appear near the threshold\nand dominate in the overconstrained region. We show a very strong correlation\nbetween search cost and the Hamming distance to the nearest solution early in\nWSat's search. This pattern leads us to introduce a measure of the backbone\nfragility of an instance, which indicates how persistent the backbone is as\nclauses are removed. We propose that high-cost random instances for local\nsearch are those with very large backbones which are also backbone-fragile. We\nsuggest that the decay in cost beyond the satisfiability threshold is due to\nincreasing backbone robustness (the opposite of backbone fragility). Our\nhypothesis makes three correct predictions. First, that the backbone robustness\nof an instance is negatively correlated with the local search cost when other\nfactors are controlled for. Second, that backbone-minimal instances (which are\n3-SAT instances altered so as to be more backbone-fragile) are unusually hard\nfor WSat. Third, that the clauses most often unsatisfied during search are\nthose whose deletion has the most effect on the backbone. In understanding the\npathologies of local search methods, we hope to contribute to the development\nof new and better techniques."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.713", 
    "link": "http://arxiv.org/pdf/1106.0241v1", 
    "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection   in a Spoken Dialogue System for Email", 
    "arxiv-id": "1106.0241v1", 
    "author": "M. A. Walker", 
    "publish": "2011-06-01T16:37:37Z", 
    "summary": "This paper describes a novel method by which a spoken dialogue system can\nlearn to choose an optimal dialogue strategy from its experience interacting\nwith human users. The method is based on a combination of reinforcement\nlearning and performance modeling of spoken dialogue systems. The reinforcement\nlearning component applies Q-learning (Watkins, 1989), while the performance\nmodeling component applies the PARADISE evaluation framework (Walker et al.,\n1997) to learn the performance function (reward) used in reinforcement\nlearning. We illustrate the method with a spoken dialogue system named ELVIS\n(EmaiL Voice Interactive System), that supports access to email over the phone.\nWe conduct a set of experiments for training an optimal dialogue strategy on a\ncorpus of 219 dialogues in which human users interact with ELVIS over the\nphone. We then test that strategy on a corpus of 18 dialogues. We show that\nELVIS can learn to optimize its strategy selection for agent initiative, for\nreading messages, and for summarizing email folders."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.714", 
    "link": "http://arxiv.org/pdf/1106.0242v1", 
    "title": "Nonapproximability Results for Partially Observable Markov Decision   Processes", 
    "arxiv-id": "1106.0242v1", 
    "author": "M. Mundhenk", 
    "publish": "2011-06-01T16:37:53Z", 
    "summary": "We show that for several variations of partially observable Markov decision\nprocesses, polynomial-time algorithms for finding control policies are unlikely\nto or simply don't have guarantees of finding policies within a constant factor\nor a constant summand of optimal. Here \"unlikely\" means \"unless some complexity\nclasses collapse,\" where the collapses considered are P=NP, P=PSPACE, or P=EXP.\nUntil or unless these collapses are shown to hold, any control-policy designer\nmust choose between such performance guarantees and efficient computation."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.715", 
    "link": "http://arxiv.org/pdf/1106.0243v1", 
    "title": "On Reasonable and Forced Goal Orderings and their Use in an   Agenda-Driven Planning Algorithm", 
    "arxiv-id": "1106.0243v1", 
    "author": "J. Koehler", 
    "publish": "2011-06-01T16:38:06Z", 
    "summary": "The paper addresses the problem of computing goal orderings, which is one of\nthe longstanding issues in AI planning. It makes two new contributions. First,\nit formally defines and discusses two different goal orderings, which are\ncalled the reasonable and the forced ordering. Both orderings are defined for\nsimple STRIPS operators as well as for more complex ADL operators supporting\nnegation and conditional effects. The complexity of these orderings is\ninvestigated and their practical relevance is discussed. Secondly, two\ndifferent methods to compute reasonable goal orderings are developed. One of\nthem is based on planning graphs, while the other investigates the set of\nactions directly. Finally, it is shown how the ordering relations, which have\nbeen derived for a given set of goals G, can be used to compute a so-called\ngoal agenda that divides G into an ordered set of subgoals. Any planner can\nthen, in principle, use the goal agenda to plan for increasing sets of\nsubgoals. This can lead to an exponential complexity reduction, as the solution\nto a complex planning problem is found by solving easier subproblems. Since\nonly a polynomial overhead is caused by the goal agenda computation, a\npotential exists to dramatically speed up planning algorithms as we demonstrate\nin the empirical evaluation, where we use this method in the IPP planner."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.720", 
    "link": "http://arxiv.org/pdf/1106.0244v1", 
    "title": "Asimovian Adaptive Agents", 
    "arxiv-id": "1106.0244v1", 
    "author": "D. F. Gordon", 
    "publish": "2011-06-01T16:38:25Z", 
    "summary": "The goal of this research is to develop agents that are adaptive and\npredictable and timely. At first blush, these three requirements seem\ncontradictory. For example, adaptation risks introducing undesirable side\neffects, thereby making agents' behavior less predictable. Furthermore,\nalthough formal verification can assist in ensuring behavioral predictability,\nit is known to be time-consuming. Our solution to the challenge of satisfying\nall three requirements is the following. Agents have finite-state automaton\nplans, which are adapted online via evolutionary learning (perturbation)\noperators. To ensure that critical behavioral constraints are always satisfied,\nagents' plans are first formally verified. They are then reverified after every\nadaptation. If reverification concludes that constraints are violated, the\nplans are repaired. The main objective of this paper is to improve the\nefficiency of reverification after learning, so that agents have a sufficiently\nrapid response time. We present two solutions: positive results that certain\nlearning operators are a priori guaranteed to preserve useful classes of\nbehavioral assurance constraints (which implies that no reverification is\nneeded for these operators), and efficient incremental reverification\nalgorithms for those learning operators that have negative a priori results."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.731", 
    "link": "http://arxiv.org/pdf/1106.0245v1", 
    "title": "A Model of Inductive Bias Learning", 
    "arxiv-id": "1106.0245v1", 
    "author": "J. Baxter", 
    "publish": "2011-06-01T16:38:38Z", 
    "summary": "A major problem in machine learning is that of inductive bias: how to choose\na learner's hypothesis space so that it is large enough to contain a solution\nto the problem being learnt, yet small enough to ensure reliable generalization\nfrom reasonably-sized training sets. Typically such bias is supplied by hand\nthrough the skill and insights of experts. In this paper a model for\nautomatically learning bias is investigated. The central assumption of the\nmodel is that the learner is embedded within an environment of related learning\ntasks. Within such an environment the learner can sample from multiple tasks,\nand hence it can search for a hypothesis space that contains good solutions to\nmany of the problems in the environment. Under certain restrictions on the set\nof all hypothesis spaces available to the learner, we show that a hypothesis\nspace that performs well on a sufficiently large number of training tasks will\nalso perform well when learning novel tasks in the same environment. Explicit\nbounds are also derived demonstrating that learning multiple tasks within an\nenvironment of related tasks can potentially give much better generalization\nthan learning a single task."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.734", 
    "link": "http://arxiv.org/pdf/1106.0246v1", 
    "title": "Mean Field Methods for a Special Class of Belief Networks", 
    "arxiv-id": "1106.0246v1", 
    "author": "S. S. Keerthi", 
    "publish": "2011-06-01T16:38:54Z", 
    "summary": "The chief aim of this paper is to propose mean-field approximations for a\nbroad class of Belief networks, of which sigmoid and noisy-or networks can be\nseen as special cases. The approximations are based on a powerful mean-field\ntheory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach\nis the first order approximation in Plefka's approach, via a variational\nderivation. The application of Plefka's theory to belief networks is not\ncomputationally tractable. To tackle this problem we propose new approximations\nbased on Taylor series. Small scale experiments show that the proposed schemes\nare attractive."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.735", 
    "link": "http://arxiv.org/pdf/1106.0247v1", 
    "title": "On the Compilability and Expressive Power of Propositional Planning   Formalisms", 
    "arxiv-id": "1106.0247v1", 
    "author": "B. Nebel", 
    "publish": "2011-06-01T16:39:07Z", 
    "summary": "The recent approaches of extending the GRAPHPLAN algorithm to handle more\nexpressive planning formalisms raise the question of what the formal meaning of\n\"expressive power\" is. We formalize the intuition that expressive power is a\nmeasure of how concisely planning domains and plans can be expressed in a\nparticular formalism by introducing the notion of \"compilation schemes\" between\nplanning formalisms. Using this notion, we analyze the expressiveness of a\nlarge family of propositional planning formalisms, ranging from basic STRIPS to\na formalism with conditional effects, partial state specifications, and\npropositional formulae in the preconditions. One of the results is that\nconditional effects cannot be compiled away if plan size should grow only\nlinearly but can be compiled away if we allow for polynomial growth of the\nresulting plans. This result confirms that the recently proposed extensions to\nthe GRAPHPLAN algorithm concerning conditional effects are optimal with respect\nto the \"compilability\" framework. Another result is that general propositional\nformulae cannot be compiled into conditional effects if the plan size should be\npreserved linearly. This implies that allowing general propositional formulae\nin preconditions and effect conditions adds another level of difficulty in\ngenerating a plan."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.740", 
    "link": "http://arxiv.org/pdf/1106.0249v1", 
    "title": "Partial-Order Planning with Concurrent Interacting Actions", 
    "arxiv-id": "1106.0249v1", 
    "author": "R. I. Brafman", 
    "publish": "2011-06-01T16:39:53Z", 
    "summary": "In order to generate plans for agents with multiple actuators, agent teams,\nor distributed controllers, we must be able to represent and plan using\nconcurrent actions with interacting effects. This has historically been\nconsidered a challenging task requiring a temporal planner with the ability to\nreason explicitly about time. We show that with simple modifications, the\nSTRIPS action representation language can be used to represent interacting\nactions. Moreover, algorithms for partial-order planning require only small\nmodifications in order to be applied in such multiagent domains. We demonstrate\nthis fact by developing a sound and complete partial-order planner for planning\nwith concurrent interacting actions, POMP, that extends existing partial-order\nplanners in a straightforward way. These results open the way to the use of\npartial-order planners for the centralized control of cooperative multiagent\nsystems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.754", 
    "link": "http://arxiv.org/pdf/1106.0250v1", 
    "title": "Planning by Rewriting", 
    "arxiv-id": "1106.0250v1", 
    "author": "C. A. Knoblock", 
    "publish": "2011-06-01T16:40:10Z", 
    "summary": "Domain-independent planning is a hard combinatorial problem. Taking into\naccount plan quality makes the task even more difficult. This article\nintroduces Planning by Rewriting (PbR), a new paradigm for efficient\nhigh-quality domain-independent planning. PbR exploits declarative\nplan-rewriting rules and efficient local search techniques to transform an\neasy-to-generate, but possibly suboptimal, initial plan into a high-quality\nplan. In addition to addressing the issues of planning efficiency and plan\nquality, this framework offers a new anytime planning algorithm. We have\nimplemented this planner and applied it to several existing domains. The\nexperimental results show that the PbR approach provides significant savings in\nplanning effort while generating high-quality plans."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.761", 
    "link": "http://arxiv.org/pdf/1106.0251v1", 
    "title": "Speeding Up the Convergence of Value Iteration in Partially Observable   Markov Decision Processes", 
    "arxiv-id": "1106.0251v1", 
    "author": "W. Zhang", 
    "publish": "2011-06-01T16:40:25Z", 
    "summary": "Partially observable Markov decision processes (POMDPs) have recently become\npopular among many AI researchers because they serve as a natural model for\nplanning under uncertainty. Value iteration is a well-known algorithm for\nfinding optimal policies for POMDPs. It typically takes a large number of\niterations to converge. This paper proposes a method for accelerating the\nconvergence of value iteration. The method has been evaluated on an array of\nbenchmark problems and was found to be very effective: It enabled value\niteration to converge after only a few iterations on all the test problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.774", 
    "link": "http://arxiv.org/pdf/1106.0252v1", 
    "title": "Conformant Planning via Symbolic Model Checking", 
    "arxiv-id": "1106.0252v1", 
    "author": "M. Roveri", 
    "publish": "2011-06-01T16:40:44Z", 
    "summary": "We tackle the problem of planning in nondeterministic domains, by presenting\na new approach to conformant planning. Conformant planning is the problem of\nfinding a sequence of actions that is guaranteed to achieve the goal despite\nthe nondeterminism of the domain. Our approach is based on the representation\nof the planning domain as a finite state automaton. We use Symbolic Model\nChecking techniques, in particular Binary Decision Diagrams, to compactly\nrepresent and efficiently search the automaton. In this paper we make the\nfollowing contributions. First, we present a general planning algorithm for\nconformant planning, which applies to fully nondeterministic domains, with\nuncertainty in the initial condition and in action effects. The algorithm is\nbased on a breadth-first, backward search, and returns conformant plans of\nminimal length, if a solution to the planning problem exists, otherwise it\nterminates concluding that the problem admits no conformant solution. Second,\nwe provide a symbolic representation of the search space based on Binary\nDecision Diagrams (BDDs), which is the basis for search techniques derived from\nsymbolic model checking. The symbolic representation makes it possible to\nanalyze potentially large sets of states and transitions in a single\ncomputation step, thus providing for an efficient implementation. Third, we\npresent CMBP (Conformant Model Based Planner), an efficient implementation of\nthe data structures and algorithm described above, directly based on BDD\nmanipulations, which allows for a compact representation of the search layers\nand an efficient implementation of the search steps. Finally, we present an\nexperimental comparison of our approach with the state-of-the-art conformant\nplanners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems\nfrom the distribution packages of these systems, plus other problems defined to\nstress a number of specific factors. Our approach appears to be the most\neffective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all\nthe problems where a comparison is possible, CMBP outperforms its competitors,\nsometimes by orders of magnitude."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.764", 
    "link": "http://arxiv.org/pdf/1106.0253v1", 
    "title": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential   Reasoning in Large Bayesian Networks", 
    "arxiv-id": "1106.0253v1", 
    "author": "M. J. Druzdzel", 
    "publish": "2011-06-01T16:40:57Z", 
    "summary": "Stochastic sampling algorithms, while an attractive alternative to exact\nalgorithms in very large Bayesian network models, have been observed to perform\npoorly in evidential reasoning with extremely unlikely evidence. To address\nthis problem, we propose an adaptive importance sampling algorithm, AIS-BN,\nthat shows promising convergence rates even under extreme conditions and seems\nto outperform the existing sampling algorithms consistently. Three sources of\nthis performance improvement are (1) two heuristics for initialization of the\nimportance function that are based on the theoretical properties of importance\nsampling in finite-dimensional integrals and the structural advantages of\nBayesian networks, (2) a smooth learning method for the importance function,\nand (3) a dynamic weighting function for combining samples from different\nstages of the algorithm. We tested the performance of the AIS-BN algorithm\nalong with two state of the art general purpose sampling algorithms, likelihood\nweighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance\nsampling (Shachter and Peot, 1989). We used in our tests three large real\nBayesian network models available to the scientific community: the CPCS network\n(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and\nNathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,\n1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always\nperformed better than the other two algorithms, in the majority of the test\ncases it achieved orders of magnitude improvement in precision of the results.\nImprovement in speed given a desired precision is even more dramatic, although\nwe are unable to report numerical results here, as the other algorithms almost\nnever achieved the precision reached even by the first few iterations of the\nAIS-BN algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.788", 
    "link": "http://arxiv.org/pdf/1106.0254v1", 
    "title": "Conflict-Directed Backjumping Revisited", 
    "arxiv-id": "1106.0254v1", 
    "author": "P. van Beek", 
    "publish": "2011-06-01T16:41:13Z", 
    "summary": "In recent years, many improvements to backtracking algorithms for solving\nconstraint satisfaction problems have been proposed. The techniques for\nimproving backtracking algorithms can be conveniently classified as look-ahead\nschemes and look-back schemes. Unfortunately, look-ahead and look-back schemes\nare not entirely orthogonal as it has been observed empirically that the\nenhancement of look-ahead techniques is sometimes counterproductive to the\neffects of look-back techniques. In this paper, we focus on the relationship\nbetween the two most important look-ahead techniques---using a variable\nordering heuristic and maintaining a level of local consistency during the\nbacktracking search---and the look-back technique of conflict-directed\nbackjumping (CBJ). We show that there exists a \"perfect\" dynamic variable\nordering such that CBJ becomes redundant. We also show theoretically that as\nthe level of local consistency that is maintained in the backtracking search is\nincreased, the less that backjumping will be an improvement. Our theoretical\nresults partially explain why a backtracking algorithm doing more in the\nlook-ahead phase cannot benefit more from the backjumping look-back scheme.\nFinally, we show empirically that adding CBJ to a backtracking algorithm that\nmaintains generalized arc consistency (GAC), an algorithm that we refer to as\nGAC-CBJ, can still provide orders of magnitude speedups. Our empirical results\ncontrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an\nalgorithm that maintains arc consistency."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.790", 
    "link": "http://arxiv.org/pdf/1106.0256v1", 
    "title": "Grounding the Lexical Semantics of Verbs in Visual Perception using   Force Dynamics and Event Logic", 
    "arxiv-id": "1106.0256v1", 
    "author": "J. M. Siskind", 
    "publish": "2011-06-01T16:41:31Z", 
    "summary": "This paper presents an implemented system for recognizing the occurrence of\nevents described by simple spatial-motion verbs in short image sequences. The\nsemantics of these verbs is specified with event-logic expressions that\ndescribe changes in the state of force-dynamic relations between the\nparticipants of the event. An efficient finite representation is introduced for\nthe infinite sets of intervals that occur when describing liquid and\nsemi-liquid events. Additionally, an efficient procedure using this\nrepresentation is presented for inferring occurrences of compound events,\ndescribed with event-logic expressions, from occurrences of primitive events.\nUsing force dynamics and event logic to specify the lexical semantics of events\nallows the system to be more robust than prior systems based on motion profile."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.614", 
    "link": "http://arxiv.org/pdf/1106.0257v1", 
    "title": "Popular Ensemble Methods: An Empirical Study", 
    "arxiv-id": "1106.0257v1", 
    "author": "D. Opitz", 
    "publish": "2011-06-01T16:41:44Z", 
    "summary": "An ensemble consists of a set of individually trained classifiers (such as\nneural networks or decision trees) whose predictions are combined when\nclassifying novel instances. Previous research has shown that an ensemble is\noften more accurate than any of the single classifiers in the ensemble. Bagging\n(Breiman, 1996c) and Boosting (Freund and Shapire, 1996; Shapire, 1990) are two\nrelatively new but popular methods for producing ensembles. In this paper we\nevaluate these methods on 23 data sets using both neural networks and decision\ntrees as our classification algorithm. Our results clearly indicate a number of\nconclusions. First, while Bagging is almost always more accurate than a single\nclassifier, it is sometimes much less accurate than Boosting. On the other\nhand, Boosting can create ensembles that are less accurate than a single\nclassifier -- especially when using neural networks. Analysis indicates that\nthe performance of the Boosting methods is dependent on the characteristics of\nthe data set being examined. In fact, further results show that Boosting\nensembles may overfit noisy data sets, thus decreasing its performance.\nFinally, consistent with previous studies, our work suggests that most of the\ngain in an ensemble's performance comes in the first few classifiers combined;\nhowever, relatively large gains can be seen up to 25 classifiers when Boosting\ndecision trees."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.842", 
    "link": "http://arxiv.org/pdf/1106.0284v1", 
    "title": "An Evolutionary Algorithm with Advanced Goal and Priority Specification   for Multi-objective Optimization", 
    "arxiv-id": "1106.0284v1", 
    "author": "K. C. Tan", 
    "publish": "2011-06-01T19:15:16Z", 
    "summary": "This paper presents an evolutionary algorithm with a new goal-sequence\ndomination scheme for better decision support in multi-objective optimization.\nThe approach allows the inclusion of advanced hard/soft priority and constraint\ninformation on each objective component, and is capable of incorporating\nmultiple specifications with overlapping or non-overlapping objective functions\nvia logical 'OR' and 'AND' connectives to drive the search towards multiple\nregions of trade-off. In addition, we propose a dynamic sharing scheme that is\nsimple and adaptively estimated according to the on-line population\ndistribution without needing any a priori parameter setting. Each feature in\nthe proposed algorithm is examined to show its respective contribution, and the\nperformance of the algorithm is compared with other evolutionary optimization\nmethods. It is shown that the proposed algorithm has performed well in the\ndiversity of evolutionary search and uniform distribution of non-dominated\nindividuals along the final trade-offs, without significant computational\neffort. The algorithm is also applied to the design optimization of a practical\nservo control system for hard disk drives with a single voice-coil-motor\nactuator. Results of the evolutionary designed servo control system show a\nsuperior closed-loop performance compared to classical PID or RPT approaches."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.893", 
    "link": "http://arxiv.org/pdf/1106.0285v1", 
    "title": "The GRT Planning System: Backward Heuristic Construction in Forward   State-Space Planning", 
    "arxiv-id": "1106.0285v1", 
    "author": "I. Vlahavas", 
    "publish": "2011-06-01T19:17:11Z", 
    "summary": "This paper presents GRT, a domain-independent heuristic planning system for\nSTRIPS worlds. GRT solves problems in two phases. In the pre-processing phase,\nit estimates the distance between each fact and the goals of the problem, in a\nbackward direction. Then, in the search phase, these estimates are used in\norder to further estimate the distance between each intermediate state and the\ngoals, guiding so the search process in a forward direction and on a best-first\nbasis. The paper presents the benefits from the adoption of opposite directions\nbetween the preprocessing and the search phases, discusses some difficulties\nthat arise in the pre-processing phase and introduces techniques to cope with\nthem. Moreover, it presents several methods of improving the efficiency of the\nheuristic, by enriching the representation and by reducing the size of the\nproblem. Finally, a method of overcoming local optimal states, based on domain\naxioms, is proposed. According to it, difficult problems are decomposed into\neasier sub-problems that have to be solved sequentially. The performance\nresults from various domains, including those of the recent planning\ncompetitions, show that GRT is among the fastest planners."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.641", 
    "link": "http://arxiv.org/pdf/1106.0664v1", 
    "title": "The Complexity of Reasoning about Spatial Congruence", 
    "arxiv-id": "1106.0664v1", 
    "author": "M. Cristani", 
    "publish": "2011-06-03T14:51:33Z", 
    "summary": "In the recent literature of Artificial Intelligence, an intensive research\neffort has been spent, for various algebras of qualitative relations used in\nthe representation of temporal and spatial knowledge, on the problem of\nclassifying the computational complexity of reasoning problems for subsets of\nalgebras. The main purpose of these researches is to describe a restricted set\nof maximal tractable subalgebras, ideally in an exhaustive fashion with respect\nto the hosting algebras. In this paper we introduce a novel algebra for\nreasoning about Spatial Congruence, show that the satisfiability problem in the\nspatial algebra MC-4 is NP-complete, and present a complete classification of\ntractability in the algebra, based on the individuation of three maximal\ntractable subclasses, one containing the basic relations. The three algebras\nare formed by 14, 10 and 9 relations out of 16 which form the full algebra."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.806", 
    "link": "http://arxiv.org/pdf/1106.0665v1", 
    "title": "Infinite-Horizon Policy-Gradient Estimation", 
    "arxiv-id": "1106.0665v1", 
    "author": "J. Baxter", 
    "publish": "2011-06-03T14:52:01Z", 
    "summary": "Gradient-based approaches to direct policy search in reinforcement learning\nhave received much recent attention as a means to solve problems of partial\nobservability and to avoid some of the problems associated with policy\ndegradation in value-function methods. In this paper we introduce GPOMDP, a\nsimulation-based algorithm for generating a biased estimate of the gradient of\nthe average reward in Partially Observable Markov Decision Processes POMDPs\ncontrolled by parameterized stochastic policies. A similar algorithm was\nproposed by (Kimura et al. 1995). The algorithm's chief advantages are that it\nrequires storage of only twice the number of policy parameters, uses one free\nbeta (which has a natural interpretation in terms of bias-variance trade-off),\nand requires no knowledge of the underlying state. We prove convergence of\nGPOMDP, and show how the correct choice of the parameter beta is related to the\nmixing time of the controlled POMDP. We briefly describe extensions of GPOMDP\nto controlled Markov chains, continuous state, observation and control spaces,\nmultiple-agents, higher-order derivatives, and a version for training\nstochastic policies with internal states. In a companion paper (Baxter et al.,\nthis volume) we show how the gradient estimates generated by GPOMDP can be used\nin both a traditional stochastic gradient algorithm and a conjugate-gradient\nprocedure to find local optima of the average reward."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.807", 
    "link": "http://arxiv.org/pdf/1106.0666v1", 
    "title": "Experiments with Infinite-Horizon, Policy-Gradient Estimation", 
    "arxiv-id": "1106.0666v1", 
    "author": "L. Weaver", 
    "publish": "2011-06-03T14:52:26Z", 
    "summary": "In this paper, we present algorithms that perform gradient ascent of the\naverage reward in a partially observable Markov decision process (POMDP). These\nalgorithms are based on GPOMDP, an algorithm introduced in a companion paper\n(Baxter and Bartlett, this volume), which computes biased estimates of the\nperformance gradient in POMDPs. The algorithm's chief advantages are that it\nuses only one free parameter beta, which has a natural interpretation in terms\nof bias-variance trade-off, it requires no knowledge of the underlying state,\nand it can be applied to infinite state, control and observation spaces. We\nshow how the gradient estimates produced by GPOMDP can be used to perform\ngradient ascent, both with a traditional stochastic-gradient algorithm, and\nwith an algorithm based on conjugate-gradients that utilizes gradient\ninformation to bracket maxima in line searches. Experimental results are\npresented illustrating both the theoretical results of (Baxter and Bartlett,\nthis volume) on a toy problem, and practical aspects of the algorithms on a\nnumber of more realistic problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.813", 
    "link": "http://arxiv.org/pdf/1106.0667v1", 
    "title": "Reasoning within Fuzzy Description Logics", 
    "arxiv-id": "1106.0667v1", 
    "author": "U. Straccia", 
    "publish": "2011-06-03T14:52:49Z", 
    "summary": "Description Logics (DLs) are suitable, well-known, logics for managing\nstructured knowledge. They allow reasoning about individuals and well defined\nconcepts, i.e., set of individuals with common properties. The experience in\nusing DLs in applications has shown that in many cases we would like to extend\ntheir capabilities. In particular, their use in the context of Multimedia\nInformation Retrieval (MIR) leads to the convincement that such DLs should\nallow the treatment of the inherent imprecision in multimedia object content\nrepresentation and retrieval. In this paper we will present a fuzzy extension\nof ALC, combining Zadeh's fuzzy logic with a classical DL. In particular,\nconcepts becomes fuzzy and, thus, reasoning about imprecise concepts is\nsupported. We will define its syntax, its semantics, describe its properties\nand present a constraint propagation calculus for reasoning in it."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.816", 
    "link": "http://arxiv.org/pdf/1106.0668v1", 
    "title": "An Analysis of Reduced Error Pruning", 
    "arxiv-id": "1106.0668v1", 
    "author": "M. Kaariainen", 
    "publish": "2011-06-03T14:53:10Z", 
    "summary": "Top-down induction of decision trees has been observed to suffer from the\ninadequate functioning of the pruning phase. In particular, it is known that\nthe size of the resulting tree grows linearly with the sample size, even though\nthe accuracy of the tree does not improve. Reduced Error Pruning is an\nalgorithm that has been used as a representative technique in attempts to\nexplain the problems of decision tree learning. In this paper we present\nanalyses of Reduced Error Pruning in three different settings. First we study\nthe basic algorithmic properties of the method, properties that hold\nindependent of the input decision tree and pruning examples. Then we examine a\nsituation that intuitively should lead to the subtree under consideration to be\nreplaced by a leaf node, one in which the class label and attribute values of\nthe pruning examples are independent of each other. This analysis is conducted\nunder two different assumptions. The general analysis shows that the pruning\nprobability of a node fitting pure noise is bounded by a function that\ndecreases exponentially as the size of the tree grows. In a specific analysis\nwe assume that the examples are distributed uniformly to the tree. This\nassumption lets us approximate the number of subtrees that are pruned because\nthey do not receive any pruning examples. This paper clarifies the different\nvariants of the Reduced Error Pruning algorithm, brings new insight to its\nalgorithmic properties, analyses the algorithm with less imposed assumptions\nthan before, and includes the previously overlooked empty subtrees to the\nanalysis."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.820", 
    "link": "http://arxiv.org/pdf/1106.0669v1", 
    "title": "GIB: Imperfect Information in a Computationally Challenging Game", 
    "arxiv-id": "1106.0669v1", 
    "author": "M. L. Ginsberg", 
    "publish": "2011-06-03T14:53:55Z", 
    "summary": "This paper investigates the problems arising in the construction of a program\nto play the game of contract bridge. These problems include both the difficulty\nof solving the game's perfect information variant, and techniques needed to\naddress the fact that bridge is not, in fact, a perfect information game. GIB,\nthe program being described, involves five separate technical advances:\npartition search, the practical application of Monte Carlo techniques to\nrealistic problems, a focus on achievable sets to solve problems inherent in\nthe Monte Carlo approach, an extension of alpha-beta pruning from total orders\nto arbitrary distributive lattices, and the use of squeaky wheel optimization\nto find approximately optimal solutions to cardplay problems. GIB is currently\nbelieved to be of approximately expert caliber, and is currently the strongest\ncomputer bridge program in the world."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.834", 
    "link": "http://arxiv.org/pdf/1106.0671v1", 
    "title": "Domain Filtering Consistencies", 
    "arxiv-id": "1106.0671v1", 
    "author": "R. Debruyne", 
    "publish": "2011-06-03T14:54:17Z", 
    "summary": "Enforcing local consistencies is one of the main features of constraint\nreasoning. Which level of local consistency should be used when searching for\nsolutions in a constraint network is a basic question. Arc consistency and\npartial forms of arc consistency have been widely studied, and have been known\nfor sometime through the forward checking or the MAC search algorithms. Until\nrecently, stronger forms of local consistency remained limited to those that\nchange the structure of the constraint graph, and thus, could not be used in\npractice, especially on large networks. This paper focuses on the local\nconsistencies that are stronger than arc consistency, without changing the\nstructure of the network, i.e., only removing inconsistent values from the\ndomains. In the last five years, several such local consistencies have been\nproposed by us or by others. We make an overview of all of them, and highlight\nsome relations between them. We compare them both theoretically and\nexperimentally, considering their pruning efficiency and the time required to\nenforce them."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.839", 
    "link": "http://arxiv.org/pdf/1106.0672v1", 
    "title": "Policy Recognition in the Abstract Hidden Markov Model", 
    "arxiv-id": "1106.0672v1", 
    "author": "G. West", 
    "publish": "2011-06-03T14:54:32Z", 
    "summary": "In this paper, we present a method for recognising an agent's behaviour in\ndynamic, noisy, uncertain domains, and across multiple levels of abstraction.\nWe term this problem on-line plan recognition under uncertainty and view it\ngenerally as probabilistic inference on the stochastic process representing the\nexecution of the agent's plan. Our contributions in this paper are twofold. In\nterms of probabilistic inference, we introduce the Abstract Hidden Markov Model\n(AHMM), a novel type of stochastic processes, provide its dynamic Bayesian\nnetwork (DBN) structure and analyse the properties of this network. We then\ndescribe an application of the Rao-Blackwellised Particle Filter to the AHMM\nwhich allows us to construct an efficient, hybrid inference method for this\nmodel. In terms of plan recognition, we propose a novel plan recognition\nframework based on the AHMM as the plan execution model. The Rao-Blackwellised\nhybrid inference for AHMM can take advantage of the independence properties\ninherent in a model of plan execution, leading to an algorithm for online\nprobabilistic plan recognition that scales well with the number of levels in\nthe plan hierarchy. This illustrates that while stochastic models for plan\nexecution can be complex, they exhibit special structures which, if exploited,\ncan lead to efficient plan recognition algorithms. We demonstrate the\nusefulness of the AHMM framework via a behaviour recognition system in a\ncomplex spatial environment using distributed video surveillance data."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.855", 
    "link": "http://arxiv.org/pdf/1106.0675v1", 
    "title": "The FF Planning System: Fast Plan Generation Through Heuristic Search", 
    "arxiv-id": "1106.0675v1", 
    "author": "B. Nebel", 
    "publish": "2011-06-03T14:55:02Z", 
    "summary": "We describe and evaluate the algorithmic techniques that are used in the FF\nplanning system. Like the HSP system, FF relies on forward state space search,\nusing a heuristic that estimates goal distances by ignoring delete lists.\nUnlike HSP's heuristic, our method does not assume facts to be independent. We\nintroduce a novel search strategy that combines hill-climbing with systematic\nsearch, and we show how other powerful heuristic information can be extracted\nand used to prune the search space. FF was the most successful automatic\nplanner at the recent AIPS-2000 planning competition. We review the results of\nthe competition, give data for other benchmark domains, and investigate the\nreasons for the runtime performance of FF compared to HSP."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.865", 
    "link": "http://arxiv.org/pdf/1106.0678v1", 
    "title": "ATTac-2000: An Adaptive Autonomous Bidding Agent", 
    "arxiv-id": "1106.0678v1", 
    "author": "P. Stone", 
    "publish": "2011-06-03T14:55:42Z", 
    "summary": "The First Trading Agent Competition (TAC) was held from June 22nd to July\n8th, 2000. TAC was designed to create a benchmark problem in the complex domain\nof e-marketplaces and to motivate researchers to apply unique approaches to a\ncommon task. This article describes ATTac-2000, the first-place finisher in\nTAC. ATTac-2000 uses a principled bidding strategy that includes several\nelements of adaptivity. In addition to the success at the competition, isolated\nempirical results are presented indicating the robustness and effectiveness of\nATTac-2000's adaptive strategy."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.872", 
    "link": "http://arxiv.org/pdf/1106.0679v1", 
    "title": "Efficient Methods for Qualitative Spatial Reasoning", 
    "arxiv-id": "1106.0679v1", 
    "author": "J. Renz", 
    "publish": "2011-06-03T14:56:05Z", 
    "summary": "The theoretical properties of qualitative spatial reasoning in the RCC8\nframework have been analyzed extensively. However, no empirical investigation\nhas been made yet. Our experiments show that the adaption of the algorithms\nused for qualitative temporal reasoning can solve large RCC8 instances, even if\nthey are in the phase transition region -- provided that one uses the maximal\ntractable subsets of RCC8 that have been identified by us. In particular, we\ndemonstrate that the orthogonal combination of heuristic methods is successful\nin solving almost all apparently hard instances in the phase transition region\nup to a certain size in reasonable time."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.872", 
    "link": "http://arxiv.org/pdf/1106.1510v1", 
    "title": "Towards OWL-based Knowledge Representation in Petrology", 
    "arxiv-id": "1106.1510v1", 
    "author": "Dmitry Kudryavtsev", 
    "publish": "2011-06-08T07:01:59Z", 
    "summary": "This paper presents our work on development of OWL-driven systems for formal\nrepresentation and reasoning about terminological knowledge and facts in\npetrology. The long-term aim of our project is to provide solid foundations for\na large-scale integration of various kinds of knowledge, including basic terms,\nrock classification algorithms, findings and reports. We describe three steps\nwe have taken towards that goal here. First, we develop a semi-automated\nprocedure for transforming a database of igneous rock samples to texts in a\ncontrolled natural language (CNL), and then a collection of OWL ontologies.\nSecond, we create an OWL ontology of important petrology terms currently\ndescribed in natural language thesauri. We describe a prototype of a tool for\ncollecting definitions from domain experts. Third, we present an approach to\nformalization of current industrial standards for classification of rock\nsamples, which requires linear equations in OWL 2. In conclusion, we discuss a\nrange of opportunities arising from the use of semantic technologies in\npetrology and outline the future work in this area."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.904", 
    "link": "http://arxiv.org/pdf/1106.1796v1", 
    "title": "Accelerating Reinforcement Learning by Composing Solutions of   Automatically Identified Subtasks", 
    "arxiv-id": "1106.1796v1", 
    "author": "C. Drummond", 
    "publish": "2011-06-09T13:11:20Z", 
    "summary": "This paper discusses a system that accelerates reinforcement learning by\nusing transfer from related tasks. Without such transfer, even if two tasks are\nvery similar at some abstract level, an extensive re-learning effort is\nrequired. The system achieves much of its power by transferring parts of\npreviously learned solutions rather than a single complete solution. The system\nexploits strong features in the multi-dimensional function produced by\nreinforcement learning in solving a particular task. These features are stable\nand easy to recognize early in the learning process. They generate a\npartitioning of the state space and thus the function. The partition is\nrepresented as a graph. This is used to index and compose functions stored in a\ncase base to form a close approximation to the solution of the new task.\nExperiments demonstrate that function composition often produces more than an\norder of magnitude increase in learning rate compared to a basic reinforcement\nlearning algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.912", 
    "link": "http://arxiv.org/pdf/1106.1797v1", 
    "title": "Parameter Learning of Logic Programs for Symbolic-Statistical Modeling", 
    "arxiv-id": "1106.1797v1", 
    "author": "Y. Kameya", 
    "publish": "2011-06-09T13:13:03Z", 
    "summary": "We propose a logical/mathematical framework for statistical parameter\nlearning of parameterized logic programs, i.e. definite clause programs\ncontaining probabilistic facts with a parameterized distribution. It extends\nthe traditional least Herbrand model semantics in logic programming to\ndistribution semantics, possible world semantics with a probability\ndistribution which is unconditionally applicable to arbitrary logic programs\nincluding ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM\nalgorithm, the graphical EM algorithm, that runs for a class of parameterized\nlogic programs representing sequential decision processes where each decision\nis exclusive and independent. It runs on a new data structure called support\ngraphs describing the logical relationship between observations and their\nexplanations, and learns parameters by computing inside and outside probability\ngeneralized for logic programs. The complexity analysis shows that when\ncombined with OLDT search for all explanations for observations, the graphical\nEM algorithm, despite its generality, has the same time complexity as existing\nEM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside\nalgorithm for PCFGs, and the one for singly connected Bayesian networks that\nhave been developed independently in each research field. Learning experiments\nwith PCFGs using two corpora of moderate size indicate that the graphical EM\nalgorithm can significantly outperform the Inside-Outside algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.914", 
    "link": "http://arxiv.org/pdf/1106.1799v1", 
    "title": "Finding a Path is Harder than Finding a Tree", 
    "arxiv-id": "1106.1799v1", 
    "author": "C. Meek", 
    "publish": "2011-06-09T13:13:51Z", 
    "summary": "I consider the problem of learning an optimal path graphical model from data\nand show the problem to be NP-hard for the maximum likelihood and minimum\ndescription length approaches and a Bayesian approach. This hardness result\nholds despite the fact that the problem is a restriction of the polynomially\nsolvable problem of finding the optimal tree graphical model."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.918", 
    "link": "http://arxiv.org/pdf/1106.1800v1", 
    "title": "Extensions of Simple Conceptual Graphs: the Complexity of Rules and   Constraints", 
    "arxiv-id": "1106.1800v1", 
    "author": "M. L. Mugnier", 
    "publish": "2011-06-09T13:17:53Z", 
    "summary": "Simple conceptual graphs are considered as the kernel of most knowledge\nrepresentation formalisms built upon Sowa's model. Reasoning in this model can\nbe expressed by a graph homomorphism called projection, whose semantics is\nusually given in terms of positive, conjunctive, existential FOL. We present\nhere a family of extensions of this model, based on rules and constraints,\nkeeping graph homomorphism as the basic operation. We focus on the formal\ndefinitions of the different models obtained, including their operational\nsemantics and relationships with FOL, and we analyze the decidability and\ncomplexity of the associated problems (consistency and deduction). As soon as\nrules are involved in reasonings, these problems are not decidable, but we\nexhibit a condition under which they fall in the polynomial hierarchy. These\nresults extend and complete the ones already published by the authors. Moreover\nwe systematically study the complexity of some particular cases obtained by\nrestricting the form of constraints and/or rules."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.919", 
    "link": "http://arxiv.org/pdf/1106.1802v1", 
    "title": "Fusions of Description Logics and Abstract Description Systems", 
    "arxiv-id": "1106.1802v1", 
    "author": "F. Wolter", 
    "publish": "2011-06-09T13:18:57Z", 
    "summary": "Fusions are a simple way of combining logics. For normal modal logics,\nfusions have been investigated in detail. In particular, it is known that,\nunder certain conditions, decidability transfers from the component logics to\ntheir fusion. Though description logics are closely related to modal logics,\nthey are not necessarily normal. In addition, ABox reasoning in description\nlogics is not covered by the results from modal logics. In this paper, we\nextend the decidability transfer results from normal modal logics to a large\nclass of description logics. To cover different description logics in a uniform\nway, we introduce abstract description systems, which can be seen as a common\ngeneralization of description and modal logics, and show the transfer results\nin this general setting."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.924", 
    "link": "http://arxiv.org/pdf/1106.1803v1", 
    "title": "Improving the Efficiency of Inductive Logic Programming Through the Use   of Query Packs", 
    "arxiv-id": "1106.1803v1", 
    "author": "H. Vandecasteele", 
    "publish": "2011-06-09T13:19:53Z", 
    "summary": "Inductive logic programming, or relational learning, is a powerful paradigm\nfor machine learning or data mining. However, in order for ILP to become\npractically useful, the efficiency of ILP systems must improve substantially.\nTo this end, the notion of a query pack is introduced: it structures sets of\nsimilar queries. Furthermore, a mechanism is described for executing such query\npacks. A complexity analysis shows that considerable efficiency improvements\ncan be achieved through the use of this query pack execution mechanism. This\nclaim is supported by empirical results obtained by incorporating support for\nquery pack execution in two existing learning systems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.935", 
    "link": "http://arxiv.org/pdf/1106.1804v1", 
    "title": "A Critical Assessment of Benchmark Comparison in Planning", 
    "arxiv-id": "1106.1804v1", 
    "author": "A. E. Howe", 
    "publish": "2011-06-09T13:20:39Z", 
    "summary": "Recent trends in planning research have led to empirical comparison becoming\ncommonplace. The field has started to settle into a methodology for such\ncomparisons, which for obvious practical reasons requires running a subset of\nplanners on a subset of problems. In this paper, we characterize the\nmethodology and examine eight implicit assumptions about the problems, planners\nand metrics used in many of these comparisons. The problem assumptions are:\nPR1) the performance of a general purpose planner should not be\npenalized/biased if executed on a sampling of problems and domains, PR2) minor\nsyntactic differences in representation do not affect performance, and PR3)\nproblems should be solvable by STRIPS capable planners unless they require ADL.\nThe planner assumptions are: PL1) the latest version of a planner is the best\none to use, PL2) default parameter settings approximate good performance, and\nPL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)\nperformance degrades similarly for each planner when run on degraded runtime\nenvironments (e.g., machine platform) and M2) the number of plan steps\ndistinguishes performance. We find that most of these assumptions are not\nsupported empirically; in particular, that planners are affected differently by\nthese assumptions. We conclude with a call to the community to devote research\nresources to improving the state of the practice and especially to enhancing\nthe available benchmark problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.953", 
    "link": "http://arxiv.org/pdf/1106.1813v1", 
    "title": "SMOTE: Synthetic Minority Over-sampling Technique", 
    "arxiv-id": "1106.1813v1", 
    "author": "W. P. Kegelmeyer", 
    "publish": "2011-06-09T13:53:42Z", 
    "summary": "An approach to the construction of classifiers from imbalanced datasets is\ndescribed. A dataset is imbalanced if the classification categories are not\napproximately equally represented. Often real-world data sets are predominately\ncomposed of \"normal\" examples with only a small percentage of \"abnormal\" or\n\"interesting\" examples. It is also the case that the cost of misclassifying an\nabnormal (interesting) example as a normal example is often much higher than\nthe cost of the reverse error. Under-sampling of the majority (normal) class\nhas been proposed as a good means of increasing the sensitivity of a classifier\nto the minority class. This paper shows that a combination of our method of\nover-sampling the minority (abnormal) class and under-sampling the majority\n(normal) class can achieve better classifier performance (in ROC space) than\nonly under-sampling the majority class. This paper also shows that a\ncombination of our method of over-sampling the minority class and\nunder-sampling the majority class can achieve better classifier performance (in\nROC space) than varying the loss ratios in Ripper or class priors in Naive\nBayes. Our method of over-sampling the minority class involves creating\nsynthetic minority class examples. Experiments are performed using C4.5, Ripper\nand a Naive Bayes classifier. The method is evaluated using the area under the\nReceiver Operating Characteristic curve (AUC) and the ROC convex hull strategy."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.967", 
    "link": "http://arxiv.org/pdf/1106.1814v1", 
    "title": "When do Numbers Really Matter?", 
    "arxiv-id": "1106.1814v1", 
    "author": "A. Darwiche", 
    "publish": "2011-06-09T13:54:07Z", 
    "summary": "Common wisdom has it that small distinctions in the probabilities\n(parameters) quantifying a belief network do not matter much for the results of\nprobabilistic queries. Yet, one can develop realistic scenarios under which\nsmall variations in network parameters can lead to significant changes in\ncomputed queries. A pending theoretical question is then to analytically\ncharacterize parameter changes that do or do not matter. In this paper, we\nstudy the sensitivity of probabilistic queries to changes in network parameters\nand prove some tight bounds on the impact that such parameters can have on\nqueries. Our analytic results pinpoint some interesting situations under which\nparameter changes do or do not matter. These results are important for\nknowledge engineers as they help them identify influential network parameters.\nThey also help explain some of the previous experimental results and\nobservations with regards to network robustness against parameter changes."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.970", 
    "link": "http://arxiv.org/pdf/1106.1816v1", 
    "title": "Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach", 
    "arxiv-id": "1106.1816v1", 
    "author": "M. Tambe", 
    "publish": "2011-06-09T13:54:54Z", 
    "summary": "Recent years are seeing an increasing need for on-line monitoring of teams of\ncooperating agents, e.g., for visualization, or performance tracking. However,\nin monitoring deployed teams, we often cannot rely on the agents to always\ncommunicate their state to the monitoring system. This paper presents a\nnon-intrusive approach to monitoring by 'overhearing', where the monitored\nteam's state is inferred (via plan-recognition) from team-members' routine\ncommunications, exchanged as part of their coordinated task execution, and\nobserved (overheard) by the monitoring system. Key challenges in this approach\ninclude the demanding run-time requirements of monitoring, the scarceness of\nobservations (increasing monitoring uncertainty), and the need to scale-up\nmonitoring to address potentially large teams. To address these, we present a\nset of complementary novel techniques, exploiting knowledge of the social\nstructures and procedures in the monitored team: (i) an efficient probabilistic\nplan-recognition algorithm, well-suited for processing communications as\nobservations; (ii) an approach to exploiting knowledge of the team's social\nbehavior to predict future observations during execution (reducing monitoring\nuncertainty); and (iii) monitoring algorithms that trade expressivity for\nscalability, representing only certain useful monitoring hypotheses, but\nallowing for any number of agents and their different activities to be\nrepresented in a single coherent entity. We present an empirical evaluation of\nthese techniques, in combination and apart, in monitoring a deployed team of\nagents, running on machines physically distributed across the country, and\nengaged in complex, dynamic task execution. We also compare the performance of\nthese techniques to human expert and novice monitors, and show that the\ntechniques presented are capable of monitoring at human-expert levels, despite\nthe difficulty of the task."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.971", 
    "link": "http://arxiv.org/pdf/1106.1817v1", 
    "title": "Automatically Training a Problematic Dialogue Predictor for a Spoken   Dialogue System", 
    "arxiv-id": "1106.1817v1", 
    "author": "H. Wright Hastie", 
    "publish": "2011-06-09T13:55:26Z", 
    "summary": "Spoken dialogue systems promise efficient and natural access to a large\nvariety of information sources and services from any phone. However, current\nspoken dialogue systems are deficient in their strategies for preventing,\nidentifying and repairing problems that arise in the conversation. This paper\nreports results on automatically training a Problematic Dialogue Predictor to\npredict problematic human-computer dialogues using a corpus of 4692 dialogues\ncollected with the 'How May I Help You' (SM) spoken dialogue system. The\nProblematic Dialogue Predictor can be immediately applied to the system's\ndecision of whether to transfer the call to a human customer care agent, or be\nused as a cue to the system's dialogue manager to modify its behavior to repair\nproblems, and even perhaps, to prevent them. We show that a Problematic\nDialogue Predictor using automatically-obtainable features from the first two\nexchanges in the dialogue can predict problematic dialogues 13.2% more\naccurately than the baseline."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.986", 
    "link": "http://arxiv.org/pdf/1106.1818v1", 
    "title": "Inducing Interpretable Voting Classifiers without Trading Accuracy for   Simplicity: Theoretical Results, Approximation Algorithms", 
    "arxiv-id": "1106.1818v1", 
    "author": "R. Nock", 
    "publish": "2011-06-09T13:56:01Z", 
    "summary": "Recent advances in the study of voting classification algorithms have brought\nempirical and theoretical results clearly showing the discrimination power of\nensemble classifiers. It has been previously argued that the search of this\nclassification power in the design of the algorithms has marginalized the need\nto obtain interpretable classifiers. Therefore, the question of whether one\nmight have to dispense with interpretability in order to keep classification\nstrength is being raised in a growing number of machine learning or data mining\npapers. The purpose of this paper is to study both theoretically and\nempirically the problem. First, we provide numerous results giving insight into\nthe hardness of the simplicity-accuracy tradeoff for voting classifiers. Then\nwe provide an efficient \"top-down and prune\" induction heuristic, WIDC, mainly\nderived from recent results on the weak learning and boosting frameworks. It is\nto our knowledge the first attempt to build a voting classifier as a base\nformula using the weak learning framework (the one which was previously highly\nsuccessful for decision tree induction), and not the strong learning framework\n(as usual for such classifiers with boosting-like approaches). While it uses a\nwell-known induction scheme previously successful in other classes of concept\nrepresentations, thus making it easy to implement and compare, WIDC also relies\non recent or new results we give about particular cases of boosting known as\npartition boosting and ranking loss boosting. Experimental results on\nthirty-one domains, most of which readily available, tend to display the\nability of WIDC to produce small, accurate, and interpretable decision\ncommittees."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.989", 
    "link": "http://arxiv.org/pdf/1106.1819v1", 
    "title": "A Knowledge Compilation Map", 
    "arxiv-id": "1106.1819v1", 
    "author": "P. Marquis", 
    "publish": "2011-06-09T13:56:25Z", 
    "summary": "We propose a perspective on knowledge compilation which calls for analyzing\ndifferent compilation approaches according to two key dimensions: the\nsuccinctness of the target compilation language, and the class of queries and\ntransformations that the language supports in polytime. We then provide a\nknowledge compilation map, which analyzes a large number of existing target\ncompilation languages according to their succinctness and their polytime\ntransformations and queries. We argue that such analysis is necessary for\nplacing new compilation approaches within the context of existing ones. We also\ngo beyond classical, flat target compilation languages based on CNF and DNF,\nand consider a richer, nested class based on directed acyclic graphs (such as\nOBDDs), which we show to include a relatively large number of target\ncompilation languages."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.991", 
    "link": "http://arxiv.org/pdf/1106.1820v1", 
    "title": "Inferring Strategies for Sentence Ordering in Multidocument News   Summarization", 
    "arxiv-id": "1106.1820v1", 
    "author": "N. Elhadad", 
    "publish": "2011-06-09T13:57:02Z", 
    "summary": "The problem of organizing information for multidocument summarization so that\nthe generated summary is coherent has received relatively little attention.\nWhile sentence ordering for single document summarization can be determined\nfrom the ordering of sentences in the input article, this is not the case for\nmultidocument summarization where summary sentences may be drawn from different\ninput articles. In this paper, we propose a methodology for studying the\nproperties of ordering information in the news genre and describe experiments\ndone on a corpus of multiple acceptable orderings we developed for the task.\nBased on these experiments, we implemented a strategy for ordering information\nthat combines constraints from chronological order of events and topical\nrelatedness. Evaluation of our augmented algorithm shows a significant\nimprovement of the ordering over two baseline strategies."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.995", 
    "link": "http://arxiv.org/pdf/1106.1821v1", 
    "title": "Collective Intelligence, Data Routing and Braess' Paradox", 
    "arxiv-id": "1106.1821v1", 
    "author": "D. H. Wolpert", 
    "publish": "2011-06-09T13:57:43Z", 
    "summary": "We consider the problem of designing the the utility functions of the\nutility-maximizing agents in a multi-agent system so that they work\nsynergistically to maximize a global utility. The particular problem domain we\nexplore is the control of network routing by placing agents on all the routers\nin the network. Conventional approaches to this task have the agents all use\nthe Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many\ncases, due to the side-effects of one agent's actions on another agent's\nperformance, having agents use ISPA's is suboptimal as far as global aggregate\ncost is concerned, even when they are only used to route infinitesimally small\namounts of traffic. The utility functions of the individual agents are not\n\"aligned\" with the global utility, intuitively speaking. As a particular\nexample of this we present an instance of Braess' paradox in which adding new\nlinks to a network whose agents all use the ISPA results in a decrease in\noverall throughput. We also demonstrate that load-balancing, in which the\nagents' decisions are collectively made to optimize the global cost incurred by\nall traffic currently being routed, is suboptimal as far as global cost\naveraged across time is concerned. This is also due to 'side-effects', in this\ncase of current routing decision on future traffic. The mathematics of\nCollective Intelligence (COIN) is concerned precisely with the issue of\navoiding such deleterious side-effects in multi-agent systems, both over time\nand space. We present key concepts from that mathematics and use them to derive\nan algorithm whose ideal version should have better performance than that of\nhaving all agents use the ISPA, even in the infinitesimal limit. We present\nexperiments verifying this, and also showing that a machine-learning-based\nversion of this COIN algorithm in which costs are only imprecisely estimated\nvia empirical means (a version potentially applicable in the real world) also\noutperforms the ISPA, despite having access to less information than does the\nISPA. In particular, this COIN algorithm almost always avoids Braess' paradox."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.1822v1", 
    "title": "Efficient Solution Algorithms for Factored MDPs", 
    "arxiv-id": "1106.1822v1", 
    "author": "S. Venkataraman", 
    "publish": "2011-06-09T13:58:37Z", 
    "summary": "This paper addresses the problem of planning under uncertainty in large\nMarkov Decision Processes (MDPs). Factored MDPs represent a complex state space\nusing state variables and the transition model using a dynamic Bayesian\nnetwork. This representation often allows an exponential reduction in the\nrepresentation size of structured MDPs, but the complexity of exact solution\nalgorithms for such MDPs can grow exponentially in the representation size. In\nthis paper, we present two approximate solution algorithms that exploit\nstructure in factored MDPs. Both use an approximate value function represented\nas a linear combination of basis functions, where each basis function involves\nonly a small subset of the domain variables. A key contribution of this paper\nis that it shows how the basic operations of both algorithms can be performed\nefficiently in closed form, by exploiting both additive and context-specific\nstructure in a factored MDP. A central element of our algorithms is a novel\nlinear program decomposition technique, analogous to variable elimination in\nBayesian networks, which reduces an exponentially large LP to a provably\nequivalent, polynomial-sized one. One algorithm uses approximate linear\nprogramming, and the second approximate dynamic programming. Our dynamic\nprogramming algorithm is novel in that it uses an approximation based on\nmax-norm, a technique that more directly minimizes the terms that appear in\nerror bounds for approximate MDP algorithms. We provide experimental results on\nproblems with over 10^40 states, demonstrating a promising indication of the\nscalability of our approach, and compare our algorithm to an existing\nstate-of-the-art approach, showing, in some problems, exponential gains in\ncomputation time."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.1853v3", 
    "title": "Intelligent decision: towards interpreting the Pe Algorithm", 
    "arxiv-id": "1106.1853v3", 
    "author": "Xinchun Tian", 
    "publish": "2011-06-09T16:45:49Z", 
    "summary": "The human intelligence lies in the algorithm, the nature of algorithm lies in\nthe classification, and the classification is equal to outlier detection. A lot\nof algorithms have been proposed to detect outliers, meanwhile a lot of\ndefinitions. Unsatisfying point is that definitions seem vague, which makes the\nsolution an ad hoc one. We analyzed the nature of outliers, and give two clear\ndefinitions. We then develop an efficient RDD algorithm, which converts outlier\nproblem to pattern and degree problem. Furthermore, a collapse mechanism was\nintroduced by IIR algorithm, which can be united seamlessly with the RDD\nalgorithm and serve for the final decision. Both algorithms are originated from\nthe study on general AI. The combined edition is named as Pe algorithm, which\nis the basis of the intelligent decision. Here we introduce longest k-turn\nsubsequence problem and corresponding solution as an example to interpret the\nfunction of Pe algorithm in detecting curve-type outliers. We also give a\ncomparison between IIR algorithm and Pe algorithm, where we can get a better\nunderstanding at both algorithms. A short discussion about intelligence is\nadded to demonstrate the function of the Pe algorithm. Related experimental\nresults indicate its robustness."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.1998v2", 
    "title": "A Linear Time Natural Evolution Strategy for Non-Separable Functions", 
    "arxiv-id": "1106.1998v2", 
    "author": "Juergen Schmidhuber", 
    "publish": "2011-06-10T09:56:00Z", 
    "summary": "We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES\n(R1-NES), which uses a low rank approximation of the search distribution\ncovariance matrix. The algorithm allows computation of the natural gradient\nwith cost linear in the dimensionality of the parameter space, and excels in\nsolving high-dimensional non-separable problems, including the best result to\ndate on the Rosenbrock function (512 dimensions)."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.2647v2", 
    "title": "From Causal Models To Counterfactual Structures", 
    "arxiv-id": "1106.2647v2", 
    "author": "Joseph Y. Halpern", 
    "publish": "2011-06-14T09:34:05Z", 
    "summary": "Galles and Pearl claimed that \"for recursive models, the causal model\nframework does not add any restrictions to counterfactuals, beyond those\nimposed by Lewis's [possible-worlds] framework.\" This claim is examined\ncarefully, with the goal of clarifying the exact relationship between causal\nmodels and Lewis's framework. Recursive models are shown to correspond\nprecisely to a subclass of (possible-world) counterfactual structures. On the\nother hand, a slight generalization of recursive models, models where all\nequations have unique solutions, is shown to be incomparable in expressive\npower to counterfactual structures, despite the fact that the Galles and Pearl\narguments should apply to them as well. The problem with the Galles and Pearl\nargument is identified: an axiom that they viewed as irrelevant, because it\ninvolved disjunction (which was not in their language), is not irrelevant at\nall."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.2652v1", 
    "title": "Actual causation and the art of modeling", 
    "arxiv-id": "1106.2652v1", 
    "author": "Christopher Hitchcock", 
    "publish": "2011-06-14T09:40:55Z", 
    "summary": "We look more carefully at the modeling of causality using structural\nequations. It is clear that the structural equations can have a major impact on\nthe conclusions we draw about causality. In particular, the choice of variables\nand their values can also have a significant impact on causality. These choices\nare, to some extent, subjective. We consider what counts as an appropriate\nchoice. More generally, we consider what makes a model an appropriate model,\nespecially if we want to take defaults into account, as was argued is necessary\nin recent work."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.2692v1", 
    "title": "Generating Schemata of Resolution Proofs", 
    "arxiv-id": "1106.2692v1", 
    "author": "Nicolas Peltier", 
    "publish": "2011-06-14T12:40:07Z", 
    "summary": "Two distinct algorithms are presented to extract (schemata of) resolution\nproofs from closed tableaux for propositional schemata. The first one handles\nthe most efficient version of the tableau calculus but generates very complex\nderivations (denoted by rather elaborate rewrite systems). The second one has\nthe advantage that much simpler systems can be obtained, however the considered\nproof procedure is less efficient."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.3361v1", 
    "title": "Random forest models of the retention constants in the thin layer   chromatography", 
    "arxiv-id": "1106.3361v1", 
    "author": "Witold R. Rudnicki", 
    "publish": "2011-06-16T22:05:21Z", 
    "summary": "In the current study we examine an application of the machine learning\nmethods to model the retention constants in the thin layer chromatography\n(TLC). This problem can be described with hundreds or even thousands of\ndescriptors relevant to various molecular properties, most of them redundant\nand not relevant for the retention constant prediction. Hence we employed\nfeature selection to significantly reduce the number of attributes.\nAdditionally we have tested application of the bagging procedure to the feature\nselection. The random forest regression models were built using selected\nvariables. The resulting models have better correlation with the experimental\ndata than the reference models obtained with linear regression. The\ncross-validation confirms robustness of the models."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.3876v1", 
    "title": "Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion   Applications", 
    "arxiv-id": "1106.3876v1", 
    "author": "Sylvain Gatepaille", 
    "publish": "2011-06-20T12:05:20Z", 
    "summary": "Nowadays ontologies present a growing interest in Data Fusion applications.\nAs a matter of fact, the ontologies are seen as a semantic tool for describing\nand reasoning about sensor data, objects, relations and general domain\ntheories. In addition, uncertainty is perhaps one of the most important\ncharacteristics of the data and information handled by Data Fusion. However,\nthe fundamental nature of ontologies implies that ontologies describe only\nasserted and veracious facts of the world. Different probabilistic, fuzzy and\nevidential approaches already exist to fill this gap; this paper recaps the\nmost popular tools. However none of the tools meets exactly our purposes.\nTherefore, we constructed a Dempster-Shafer ontology that can be imported into\nany specific domain ontology and that enables us to instantiate it in an\nuncertain manner. We also developed a Java application that enables reasoning\nabout these uncertain ontological instances."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.3932v1", 
    "title": "Coincidences and the encounter problem: A formal account", 
    "arxiv-id": "1106.3932v1", 
    "author": "Jean-Louis J. -L. Dessalles", 
    "publish": "2011-06-20T15:05:53Z", 
    "summary": "Individuals have an intuitive perception of what makes a good coincidence.\nThough the sensitivity to coincidences has often been presented as resulting\nfrom an erroneous assessment of probability, it appears to be a genuine\ncompetence, based on non-trivial computations. The model presented here\nsuggests that coincidences occur when subjects perceive complexity drops.\nCo-occurring events are, together, simpler than if considered separately. This\nmodel leads to a possible redefinition of subjective probability."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.4218v1", 
    "title": "Rooting opinions in the minds: a cognitive model and a formal account of   opinions and their dynamics", 
    "arxiv-id": "1106.4218v1", 
    "author": "Rosaria Conte", 
    "publish": "2011-06-21T14:52:09Z", 
    "summary": "The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nlike computer science and complexity, have tried to deal with this issue.\nDespite the flourishing of different models and theories in both fields,\nseveral key questions still remain unanswered. The understanding of how\nopinions change and the way they are affected by social influence are\nchallenging issues requiring a thorough analysis of opinion per se but also of\nthe way in which they travel between agents' minds and are modulated by these\nexchanges. To account for the two-faceted nature of opinions, which are mental\nentities undergoing complex social processes, we outline a preliminary model in\nwhich a cognitive theory of opinions is put forward and it is paired with a\nformal description of them and of their spreading among minds. Furthermore,\ninvestigating social influence also implies the necessity to account for the\nway in which people change their minds, as a consequence of interacting with\nother people, and the need to explain the higher or lower persistence of such\nchanges."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1000", 
    "link": "http://arxiv.org/pdf/1106.4221v1", 
    "title": "Understanding opinions. A cognitive and formal account", 
    "arxiv-id": "1106.4221v1", 
    "author": "Rosaria Conte", 
    "publish": "2011-06-21T15:00:33Z", 
    "summary": "The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nas computer science and complexity, have addressed this challenge. Despite the\nflourishing of different models and theories in both fields, several key\nquestions still remain unanswered. The aim of this paper is to challenge the\ncurrent theories on opinion by putting forward a cognitively grounded model\nwhere opinions are described as specific mental representations whose main\nproperties are put forward. A comparison with reputation will be also\npresented."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1199", 
    "link": "http://arxiv.org/pdf/1106.4557v1", 
    "title": "Learning When Training Data are Costly: The Effect of Class Distribution   on Tree Induction", 
    "arxiv-id": "1106.4557v1", 
    "author": "G. M. Weiss", 
    "publish": "2011-06-22T20:11:46Z", 
    "summary": "For large, real-world inductive learning problems, the number of training\nexamples often must be limited due to the costs associated with procuring,\npreparing, and storing the training examples and/or the computational costs\nassociated with learning from them. In such circumstances, one question of\npractical importance is: if only n training examples can be selected, in what\nproportion should the classes be represented? In this article we help to answer\nthis question by analyzing, for a fixed training-set size, the relationship\nbetween the class distribution of the training data and the performance of\nclassification trees induced from these data. We study twenty-six data sets\nand, for each, determine the best class distribution for learning. The\nnaturally occurring class distribution is shown to generally perform well when\nclassifier performance is evaluated using undifferentiated error rate (0/1\nloss). However, when the area under the ROC curve is used to evaluate\nclassifier performance, a balanced distribution is shown to perform well. Since\nneither of these choices for class distribution always generates the\nbest-performing classifier, we introduce a budget-sensitive progressive\nsampling algorithm for selecting training examples based on the class\nassociated with each example. An empirical analysis of this algorithm shows\nthat the class distribution of the resulting training set yields classifiers\nwith good (nearly-optimal) classification performance."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1129", 
    "link": "http://arxiv.org/pdf/1106.4561v1", 
    "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains", 
    "arxiv-id": "1106.4561v1", 
    "author": "D. Long", 
    "publish": "2011-06-22T20:20:10Z", 
    "summary": "In recent years research in the planning community has moved increasingly\ntoward s application of planners to realistic problems involving both time and\nmany typ es of resources. For example, interest in planning demonstrated by the\nspace res earch community has inspired work in observation scheduling,\nplanetary rover ex ploration and spacecraft control domains. Other temporal and\nresource-intensive domains including logistics planning, plant control and\nmanufacturing have also helped to focus the community on the modelling and\nreasoning issues that must be confronted to make planning technology meet the\nchallenges of application. The International Planning Competitions have acted\nas an important motivating fo rce behind the progress that has been made in\nplanning since 1998. The third com petition (held in 2002) set the planning\ncommunity the challenge of handling tim e and numeric resources. This\nnecessitated the development of a modelling langua ge capable of expressing\ntemporal and numeric properties of planning domains. In this paper we describe\nthe language, PDDL2.1, that was used in the competition. We describe the syntax\nof the language, its formal semantics and the validation of concurrent plans.\nWe observe that PDDL2.1 has considerable modelling power --- exceeding the\ncapabilities of current planning technology --- and presents a number of\nimportant challenges to the research community."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1024", 
    "link": "http://arxiv.org/pdf/1106.4569v1", 
    "title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork   Theories and Models", 
    "arxiv-id": "1106.4569v1", 
    "author": "M. Tambe", 
    "publish": "2011-06-22T20:55:38Z", 
    "summary": "Despite the significant progress in multiagent teamwork, existing research\ndoes not address the optimality of its prescriptions nor the complexity of the\nteamwork problem. Without a characterization of the optimality-complexity\ntradeoffs, it is impossible to determine whether the assumptions and\napproximations made by a particular theory gain enough efficiency to justify\nthe losses in overall performance. To provide a tool for use by multiagent\nresearchers in evaluating this tradeoff, we present a unified framework, the\nCOMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model\ncombines and extends existing multiagent theories, such as decentralized\npartially observable Markov decision processes and economic team theory. In\naddition to their generality of representation, COM-MTDPs also support the\nanalysis of both the optimality of team performance and the computational\ncomplexity of the agents' decision problem. In analyzing complexity, we present\na breakdown of the computational complexity of constructing optimal teams under\nvarious classes of problem domains, along the dimensions of observability and\ncommunication cost. In analyzing optimality, we exploit the COM-MTDP's ability\nto encode existing teamwork theories and models to encode two instantiations of\njoint intentions theory taken from the literature. Furthermore, the COM-MTDP\nmodel provides a basis for the development of novel team coordination\nalgorithms. We derive a domain-independent criterion for optimal communication\nand provide a comparative analysis of the two joint intentions instantiations\nwith respect to this optimal policy. We have implemented a reusable,\ndomain-independent software package based on COM-MTDPs to analyze teamwork\ncoordination strategies, and we demonstrate its use by encoding and evaluating\nthe two joint intentions strategies within an example domain."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1037", 
    "link": "http://arxiv.org/pdf/1106.4573v1", 
    "title": "Towards Adjustable Autonomy for the Real World", 
    "arxiv-id": "1106.4573v1", 
    "author": "M. Tambe", 
    "publish": "2011-06-22T20:58:48Z", 
    "summary": "Adjustable autonomy refers to entities dynamically varying their own\nautonomy, transferring decision-making control to other entities (typically\nagents transferring control to human users) in key situations. Determining\nwhether and when such transfers-of-control should occur is arguably the\nfundamental research problem in adjustable autonomy. Previous work has\ninvestigated various approaches to addressing this problem but has often\nfocused on individual agent-human interactions. Unfortunately, domains\nrequiring collaboration between teams of agents and humans reveal two key\nshortcomings of these previous approaches. First, these approaches use rigid\none-shot transfers of control that can result in unacceptable coordination\nfailures in multiagent settings. Second, they ignore costs (e.g., in terms of\ntime delays or effects on actions) to an agent's team due to such\ntransfers-of-control. To remedy these problems, this article presents a novel\napproach to adjustable autonomy, based on the notion of a transfer-of-control\nstrategy. A transfer-of-control strategy consists of a conditional sequence of\ntwo types of actions: (i) actions to transfer decision-making control (e.g.,\nfrom an agent to a user or vice versa) and (ii) actions to change an agent's\npre-specified coordination constraints with team members, aimed at minimizing\nmiscoordination costs. The goal is for high-quality individual decisions to be\nmade with minimal disruption to the coordination of the team. We present a\nmathematical model of transfer-of-control strategies. The model guides and\ninforms the operationalization of the strategies using Markov Decision\nProcesses, which select an optimal strategy, given an uncertain environment and\ncosts to the individuals and teams. The approach has been carefully evaluated,\nincluding via its use in a real-world, deployed multi-agent system that assists\na research group in its daily activities."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1081", 
    "link": "http://arxiv.org/pdf/1106.4575v1", 
    "title": "An Analysis of Phase Transition in NK Landscapes", 
    "arxiv-id": "1106.4575v1", 
    "author": "Y. Gao", 
    "publish": "2011-06-22T20:59:27Z", 
    "summary": "In this paper, we analyze the decision version of the NK landscape model from\nthe perspective of threshold phenomena and phase transitions under two random\ndistributions, the uniform probability model and the fixed ratio model. For the\nuniform probability model, we prove that the phase transition is easy in the\nsense that there is a polynomial algorithm that can solve a random instance of\nthe problem with the probability asymptotic to 1 as the problem size tends to\ninfinity. For the fixed ratio model, we establish several upper bounds for the\nsolubility threshold, and prove that random instances with parameters above\nthese upper bounds can be solved polynomially. This, together with our\nempirical study for random instances generated below and in the phase\ntransition region, suggests that the phase transition of the fixed ratio model\nis also easy."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1089", 
    "link": "http://arxiv.org/pdf/1106.4576v1", 
    "title": "Expert-Guided Subgroup Discovery: Methodology and Application", 
    "arxiv-id": "1106.4576v1", 
    "author": "N. Lavrac", 
    "publish": "2011-06-22T20:59:50Z", 
    "summary": "This paper presents an approach to expert-guided subgroup discovery. The main\nstep of the subgroup discovery process, the induction of subgroup descriptions,\nis performed by a heuristic beam search algorithm, using a novel parametrized\ndefinition of rule quality which is analyzed in detail. The other important\nsteps of the proposed subgroup discovery process are the detection of\nstatistically significant properties of selected subgroups and subgroup\nvisualization: statistically significant properties are used to enrich the\ndescriptions of induced subgroups, while the visualization shows subgroup\nproperties in the form of distributions of the numbers of examples in the\nsubgroups. The approach is illustrated by the results obtained for a medical\nproblem of early detection of patient risk groups."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1113", 
    "link": "http://arxiv.org/pdf/1106.4578v1", 
    "title": "Propositional Independence - Formula-Variable Independence and   Forgetting", 
    "arxiv-id": "1106.4578v1", 
    "author": "P. Marquis", 
    "publish": "2011-06-22T21:01:16Z", 
    "summary": "Independence -- the study of what is relevant to a given problem of reasoning\n-- has received an increasing attention from the AI community. In this paper,\nwe consider two basic forms of independence, namely, a syntactic one and a\nsemantic one. We show features and drawbacks of them. In particular, while the\nsyntactic form of independence is computationally easy to check, there are\ncases in which things that intuitively are not relevant are not recognized as\nsuch. We also consider the problem of forgetting, i.e., distilling from a\nknowledge base only the part that is relevant to the set of queries constructed\nfrom a subset of the alphabet. While such process is computationally hard, it\nallows for a simplification of subsequent reasoning, and can thus be viewed as\na form of compilation: once the relevant part of a knowledge base has been\nextracted, all reasoning tasks to be performed can be simplified."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1121", 
    "link": "http://arxiv.org/pdf/1106.4863v1", 
    "title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization", 
    "arxiv-id": "1106.4863v1", 
    "author": "B. Kappen", 
    "publish": "2011-06-24T00:56:05Z", 
    "summary": "We present a probabilistic generative model for timing deviations in\nexpressive music performance. The structure of the proposed model is equivalent\nto a switching state space model. The switch variables correspond to discrete\nnote locations as in a musical score. The continuous hidden variables denote\nthe tempo. We formulate two well known music recognition problems, namely tempo\ntracking and automatic transcription (rhythm quantization) as filtering and\nmaximum a posteriori (MAP) state estimation tasks. Exact computation of\nposterior features such as the MAP state is intractable in this model class, so\nwe introduce Monte Carlo methods for integration and optimization. We compare\nMarkov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated\nannealing and iterative improvement) and sequential Monte Carlo methods\n(particle filters). Our simulation results suggest better results with\nsequential methods. The methods can be applied in both online and batch\nscenarios such as tempo tracking and transcription and are thus potentially\nuseful in a number of music applications such as adaptive automatic\naccompaniment, score typesetting and music information retrieval."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1122", 
    "link": "http://arxiv.org/pdf/1106.4864v1", 
    "title": "Exploiting Contextual Independence In Probabilistic Inference", 
    "arxiv-id": "1106.4864v1", 
    "author": "N. L. Zhang", 
    "publish": "2011-06-24T00:56:26Z", 
    "summary": "Bayesian belief networks have grown to prominence because they provide\ncompact representations for many problems for which probabilistic inference is\nappropriate, and there are algorithms to exploit this compactness. The next\nstep is to allow compact representations of the conditional probabilities of a\nvariable given its parents. In this paper we present such a representation that\nexploits contextual independence in terms of parent contexts; which variables\nact as parents may depend on the value of other variables. The internal\nrepresentation is in terms of contextual factors (confactors) that is simply a\npair of a context and a table. The algorithm, contextual variable elimination,\nis based on the standard variable elimination algorithm that eliminates the\nnon-query variables in turn, but when eliminating a variable, the tables that\nneed to be multiplied can depend on the context. This algorithm reduces to\nstandard variable elimination when there is no contextual independence\nstructure to exploit. We show how this can be much more efficient than variable\nelimination when there is structure to exploit. We explain why this new method\ncan exploit more structure than previous methods for structured belief network\ninference and an analogous algorithm that uses trees."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1130", 
    "link": "http://arxiv.org/pdf/1106.4865v1", 
    "title": "Bound Propagation", 
    "arxiv-id": "1106.4865v1", 
    "author": "M. Leisink", 
    "publish": "2011-06-24T00:56:48Z", 
    "summary": "In this article we present an algorithm to compute bounds on the marginals of\na graphical model. For several small clusters of nodes upper and lower bounds\non the marginal values are computed independently of the rest of the network.\nThe range of allowed probability distributions over the surrounding nodes is\nrestricted using earlier computed bounds. As we will show, this can be\nconsidered as a set of constraints in a linear programming problem of which the\nobjective function is the marginal probability of the center nodes. In this way\nknowledge about the maginals of neighbouring clusters is passed to other\nclusters thereby tightening the bounds on their marginals. We show that sharp\nbounds can be obtained for undirected and directed graphs that are used for\npractical applications, but for which exact computations are infeasible."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1134", 
    "link": "http://arxiv.org/pdf/1106.4866v1", 
    "title": "On Polynomial Sized MDP Succinct Policies", 
    "arxiv-id": "1106.4866v1", 
    "author": "P. Liberatore", 
    "publish": "2011-06-24T00:57:19Z", 
    "summary": "Policies of Markov Decision Processes (MDPs) determine the next action to\nexecute from the current state and, possibly, the history (the past states).\nWhen the number of states is large, succinct representations are often used to\ncompactly represent both the MDPs and the policies in a reduced amount of\nspace. In this paper, some problems related to the size of succinctly\nrepresented policies are analyzed. Namely, it is shown that some MDPs have\npolicies that can only be represented in space super-polynomial in the size of\nthe MDP, unless the polynomial hierarchy collapses. This fact motivates the\nstudy of the problem of deciding whether a given MDP has a policy of a given\nsize and reward. Since some algorithms for MDPs work by finding a succinct\nrepresentation of the value function, the problem of deciding the existence of\na succinct representation of a value function of a given size and reward is\nalso considered."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1135", 
    "link": "http://arxiv.org/pdf/1106.4867v1", 
    "title": "Compiling Causal Theories to Successor State Axioms and STRIPS-Like   Systems", 
    "arxiv-id": "1106.4867v1", 
    "author": "F. Lin", 
    "publish": "2011-06-24T00:57:41Z", 
    "summary": "We describe a system for specifying the effects of actions. Unlike those\ncommonly used in AI planning, our system uses an action description language\nthat allows one to specify the effects of actions using domain rules, which are\nstate constraints that can entail new action effects from old ones.\nDeclaratively, an action domain in our language corresponds to a nonmonotonic\ncausal theory in the situation calculus. Procedurally, such an action domain is\ncompiled into a set of logical theories, one for each action in the domain,\nfrom which fully instantiated successor state-like axioms and STRIPS-like\nsystems are then generated. We expect the system to be a useful tool for\nknowledge engineers writing action specifications for classical AI planning\nsystems, GOLOG systems, and other systems where formal specifications of\nactions are needed."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1136", 
    "link": "http://arxiv.org/pdf/1106.4868v1", 
    "title": "VHPOP: Versatile Heuristic Partial Order Planner", 
    "arxiv-id": "1106.4868v1", 
    "author": "H. L. S. Younes", 
    "publish": "2011-06-24T00:58:05Z", 
    "summary": "VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.\nIt draws from the experience gained in the early to mid 1990's on flaw\nselection strategies for POCL planning, and combines this with more recent\ndevelopments in the field of domain independent planning such as distance based\nheuristics and reachability analysis. We present an adaptation of the additive\nheuristic for plan space planning, and modify it to account for possible reuse\nof existing actions in a plan. We also propose a large set of novel flaw\nselection strategies, and show how these can help us solve more problems than\npreviously possible by POCL planners. VHPOP also supports planning with\ndurative actions by incorporating standard techniques for temporal constraint\nreasoning. We demonstrate that the same heuristic techniques used to boost the\nperformance of classical POCL planning can be effective in domains with\ndurative actions as well. The result is a versatile heuristic POCL planner\ncompetitive with established CSP-based and heuristic state space planners."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1141", 
    "link": "http://arxiv.org/pdf/1106.4869v1", 
    "title": "SHOP2: An HTN Planning System", 
    "arxiv-id": "1106.4869v1", 
    "author": "F. Yaman", 
    "publish": "2011-06-24T00:58:42Z", 
    "summary": "The SHOP2 planning system received one of the awards for distinguished\nperformance in the 2002 International Planning Competition. This paper\ndescribes the features of SHOP2 which enabled it to excel in the competition,\nespecially those aspects of SHOP2 that deal with temporal and metric planning\ndomains."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1142", 
    "link": "http://arxiv.org/pdf/1106.4871v1", 
    "title": "An Architectural Approach to Ensuring Consistency in Hierarchical   Execution", 
    "arxiv-id": "1106.4871v1", 
    "author": "R. E. Wray", 
    "publish": "2011-06-24T00:59:16Z", 
    "summary": "Hierarchical task decomposition is a method used in many agent systems to\norganize agent knowledge. This work shows how the combination of a hierarchy\nand persistent assertions of knowledge can lead to difficulty in maintaining\nlogical consistency in asserted knowledge. We explore the problematic\nconsequences of persistent assumptions in the reasoning process and introduce\nnovel potential solutions. Having implemented one of the possible solutions,\nDynamic Hierarchical Justification, its effectiveness is demonstrated with an\nempirical analysis."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1145", 
    "link": "http://arxiv.org/pdf/1106.4872v1", 
    "title": "Wrapper Maintenance: A Machine Learning Approach", 
    "arxiv-id": "1106.4872v1", 
    "author": "S. N. Minton", 
    "publish": "2011-06-24T00:59:47Z", 
    "summary": "The proliferation of online information sources has led to an increased use\nof wrappers for extracting data from Web sources. While most of the previous\nresearch has focused on quick and efficient generation of wrappers, the\ndevelopment of tools for wrapper maintenance has received less attention. This\nis an important research problem because Web sources often change in ways that\nprevent the wrappers from extracting data correctly. We present an efficient\nalgorithm that learns structural information about data from positive examples\nalone. We describe how this information can be used for two wrapper maintenance\napplications: wrapper verification and reinduction. The wrapper verification\nsystem detects when a wrapper is not extracting correct data, usually because\nthe Web source has changed its format. The reinduction algorithm automatically\nrecovers from changes in the Web source by identifying data on Web pages so\nthat a new wrapper may be generated for this source. To validate our approach,\nwe monitored 27 wrappers over a period of a year. The verification algorithm\ncorrectly discovered 35 of the 37 wrapper changes, and made 16 mistakes,\nresulting in precision of 0.73 and recall of 0.95. We validated the reinduction\nalgorithm on ten Web sources. We were able to successfully reinduce the\nwrappers, obtaining precision and recall values of 0.90 and 0.80 on the data\nextraction task."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1145", 
    "link": "http://arxiv.org/pdf/1106.5111v1", 
    "title": "Exploiting Reputation in Distributed Virtual Environments", 
    "arxiv-id": "1106.5111v1", 
    "author": "Rosaria Conte", 
    "publish": "2011-06-25T08:40:48Z", 
    "summary": "The cognitive research on reputation has shown several interesting properties\nthat can improve both the quality of services and the security in distributed\nelectronic environments. In this paper, the impact of reputation on\ndecision-making under scarcity of information will be shown. First, a cognitive\ntheory of reputation will be presented, then a selection of simulation\nexperimental results from different studies will be discussed. Such results\nconcern the benefits of reputation when agents need to find out good sellers in\na virtual market-place under uncertainty and informational cheating."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1145", 
    "link": "http://arxiv.org/pdf/1106.5112v1", 
    "title": "The All Relevant Feature Selection using Random Forest", 
    "arxiv-id": "1106.5112v1", 
    "author": "Witold R. Rudnicki", 
    "publish": "2011-06-25T08:47:23Z", 
    "summary": "In this paper we examine the application of the random forest classifier for\nthe all relevant feature selection problem. To this end we first examine two\nrecently proposed all relevant feature selection algorithms, both being a\nrandom forest wrappers, on a series of synthetic data sets with varying size.\nWe show that reasonable accuracy of predictions can be achieved and that\nheuristic algorithms that were designed to handle the all relevant problem,\nhave performance that is close to that of the reference ideal algorithm. Then,\nwe apply one of the algorithms to four families of semi-synthetic data sets to\nassess how the properties of particular data set influence results of feature\nselection. Finally we test the procedure using a well-known gene expression\ndata set. The relevance of nearly all previously established important genes\nwas confirmed, moreover the relevance of several new ones is discovered."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1146", 
    "link": "http://arxiv.org/pdf/1106.5256v1", 
    "title": "Structure and Complexity in Planning with Unary Operators", 
    "arxiv-id": "1106.5256v1", 
    "author": "C. Domshlak", 
    "publish": "2011-06-26T21:01:50Z", 
    "summary": "Unary operator domains -- i.e., domains in which operators have a single\neffect -- arise naturally in many control problems. In its most general form,\nthe problem of STRIPS planning in unary operator domains is known to be as hard\nas the general STRIPS planning problem -- both are PSPACE-complete. However,\nunary operator domains induce a natural structure, called the domain's causal\ngraph. This graph relates between the preconditions and effect of each domain\noperator. Causal graphs were exploited by Williams and Nayak in order to\nanalyze plan generation for one of the controllers in NASA's Deep-Space One\nspacecraft. There, they utilized the fact that when this graph is acyclic, a\nserialization ordering over any subgoal can be obtained quickly. In this paper\nwe conduct a comprehensive study of the relationship between the structure of a\ndomain's causal graph and the complexity of planning in this domain. On the\npositive side, we show that a non-trivial polynomial time plan generation\nalgorithm exists for domains whose causal graph induces a polytree with a\nconstant bound on its node indegree. On the negative side, we show that even\nplan existence is hard when the graph is a directed-path singly connected DAG.\nMore generally, we show that the number of paths in the causal graph is closely\nrelated to the complexity of planning in the associated domain. Finally we\nrelate our results to the question of complexity of planning with serializable\nsubgoals."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1148", 
    "link": "http://arxiv.org/pdf/1106.5257v1", 
    "title": "Answer Set Planning Under Action Costs", 
    "arxiv-id": "1106.5257v1", 
    "author": "A. Polleres", 
    "publish": "2011-06-26T21:02:44Z", 
    "summary": "Recently, planning based on answer set programming has been proposed as an\napproach towards realizing declarative planning systems. In this paper, we\npresent the language Kc, which extends the declarative planning language K by\naction costs. Kc provides the notion of admissible and optimal plans, which are\nplans whose overall action costs are within a given limit resp. minimum over\nall plans (i.e., cheapest plans). As we demonstrate, this novel language allows\nfor expressing some nontrivial planning tasks in a declarative way.\nFurthermore, it can be utilized for representing planning problems under other\noptimality criteria, such as computing ``shortest'' plans (with the least\nnumber of steps), and refinement combinations of cheapest and fastest plans. We\nstudy complexity aspects of the language Kc and provide a transformation to\nlogic programs, such that planning problems are solved via answer set\nprogramming. Furthermore, we report experimental results on selected problems.\nOur experience is encouraging that answer set planning may be a valuable\napproach to expressive planning systems in which intricate planning problems\ncan be naturally specified and solved."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1154", 
    "link": "http://arxiv.org/pdf/1106.5258v1", 
    "title": "Learning to Coordinate Efficiently: A Model-based Approach", 
    "arxiv-id": "1106.5258v1", 
    "author": "M. Tennenholtz", 
    "publish": "2011-06-26T21:03:18Z", 
    "summary": "In common-interest stochastic games all players receive an identical payoff.\nPlayers participating in such games must learn to coordinate with each other in\norder to receive the highest-possible value. A number of reinforcement learning\nalgorithms have been proposed for this problem, and some have been shown to\nconverge to good solutions in the limit. In this paper we show that using very\nsimple model-based algorithms, much better (i.e., polynomial) convergence rates\ncan be attained. Moreover, our model-based algorithms are guaranteed to\nconverge to the optimal value, unlike many of the existing algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1156", 
    "link": "http://arxiv.org/pdf/1106.5260v1", 
    "title": "SAPA: A Multi-objective Metric Temporal Planner", 
    "arxiv-id": "1106.5260v1", 
    "author": "S. Kambhampati", 
    "publish": "2011-06-26T21:03:40Z", 
    "summary": "SAPA is a domain-independent heuristic forward chaining planner that can\nhandle durative actions, metric resource constraints, and deadline goals. It is\ndesigned to be capable of handling the multi-objective nature of metric\ntemporal planning. Our technical contributions include (i) planning-graph based\nmethods for deriving heuristics that are sensitive to both cost and makespan\n(ii) techniques for adjusting the heuristic estimates to take action\ninteractions and metric resource limitations into account and (iii) a linear\ntime greedy post-processing technique to improve execution flexibility of the\nsolution plans. An implementation of SAPA using many of the techniques\npresented in this paper was one of the best domain independent planners for\ndomains with metric and temporal constraints in the third International\nPlanning Competition, held at AIPS-02. We describe the technical details of\nextracting the heuristics and present an empirical evaluation of the current\nimplementation of SAPA."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1166", 
    "link": "http://arxiv.org/pdf/1106.5261v1", 
    "title": "A New General Method to Generate Random Modal Formulae for Testing   Decision Procedures", 
    "arxiv-id": "1106.5261v1", 
    "author": "R. Sebastiani", 
    "publish": "2011-06-26T21:04:07Z", 
    "summary": "The recent emergence of heavily-optimized modal decision procedures has\nhighlighted the key role of empirical testing in this domain. Unfortunately,\nthe introduction of extensive empirical tests for modal logics is recent, and\nso far none of the proposed test generators is very satisfactory. To cope with\nthis fact, we present a new random generation method that provides benefits\nover previous methods for generating empirical tests. It fixes and much\ngeneralizes one of the best-known methods, the random CNF_[]m test, allowing\nfor generating a much wider variety of problems, covering in principle the\nwhole input space. Our new method produces much more suitable test sets for the\ncurrent generation of modal decision procedures. We analyze the features of the\nnew method by means of an extensive collection of empirical tests."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1168", 
    "link": "http://arxiv.org/pdf/1106.5262v1", 
    "title": "AltAltp: Online Parallelization of Plans with Heuristic State Search", 
    "arxiv-id": "1106.5262v1", 
    "author": "R. Sanchez", 
    "publish": "2011-06-26T21:04:32Z", 
    "summary": "Despite their near dominance, heuristic state search planners still lag\nbehind disjunctive planners in the generation of parallel plans in classical\nplanning. The reason is that directly searching for parallel solutions in state\nspace planners would require the planners to branch on all possible subsets of\nparallel actions, thus increasing the branching factor exponentially. We\npresent a variant of our heuristic state search planner AltAlt, called AltAltp\nwhich generates parallel plans by using greedy online parallelization of\npartial plans. The greedy approach is significantly informed by the use of\nnovel distance heuristics that AltAltp derives from a graphplan-style planning\ngraph for the problem. While this approach is not guaranteed to provide optimal\nparallel plans, empirical results show that AltAltp is capable of generating\ngood quality parallel plans at a fraction of the cost incurred by the\ndisjunctive planners."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1170", 
    "link": "http://arxiv.org/pdf/1106.5263v1", 
    "title": "New Polynomial Classes for Logic-Based Abduction", 
    "arxiv-id": "1106.5263v1", 
    "author": "B. Zanuttini", 
    "publish": "2011-06-26T21:04:51Z", 
    "summary": "We address the problem of propositional logic-based abduction, i.e., the\nproblem of searching for a best explanation for a given propositional\nobservation according to a given propositional knowledge base. We give a\ngeneral algorithm, based on the notion of projection; then we study\nrestrictions over the representations of the knowledge base and of the query,\nand find new polynomial classes of abduction problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1183", 
    "link": "http://arxiv.org/pdf/1106.5265v1", 
    "title": "Planning Through Stochastic Local Search and Temporal Action Graphs in   LPG", 
    "arxiv-id": "1106.5265v1", 
    "author": "I. Serina", 
    "publish": "2011-06-26T21:05:34Z", 
    "summary": "We present some techniques for planning in domains specified with the recent\nstandard language PDDL2.1, supporting 'durative actions' and numerical\nquantities. These techniques are implemented in LPG, a domain-independent\nplanner that took part in the 3rd International Planning Competition (IPC). LPG\nis an incremental, any time system producing multi-criteria quality plans. The\ncore of the system is based on a stochastic local search method and on a\ngraph-based representation called 'Temporal Action Graphs' (TA-graphs). This\npaper focuses on temporal planning, introducing TA-graphs and proposing some\ntechniques to guide the search in LPG using this representation. The\nexperimental results of the 3rd IPC, as well as further results presented in\nthis paper, show that our techniques can be very effective. Often LPG\noutperforms all other fully-automated planners of the 3rd IPC in terms of speed\nto derive a solution, or quality of the solutions that can be produced."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1189", 
    "link": "http://arxiv.org/pdf/1106.5266v1", 
    "title": "TALplanner in IPC-2002: Extensions and Control Rules", 
    "arxiv-id": "1106.5266v1", 
    "author": "M. Magnusson", 
    "publish": "2011-06-26T21:06:29Z", 
    "summary": "TALplanner is a forward-chaining planner that relies on domain knowledge in\nthe shape of temporal logic formulas in order to prune irrelevant parts of the\nsearch space. TALplanner recently participated in the third International\nPlanning Competition, which had a clear emphasis on increasing the complexity\nof the problem domains being used as benchmark tests and the expressivity\nrequired to represent these domains in a planning system. Like many other\nplanners, TALplanner had support for some but not all aspects of this increase\nin expressivity, and a number of changes to the planner were required. After a\nshort introduction to TALplanner, this article describes some of the changes\nthat were made before and during the competition. We also describe the process\nof introducing suitable domain knowledge for several of the competition\ndomains."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1194", 
    "link": "http://arxiv.org/pdf/1106.5268v1", 
    "title": "Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems   On-Board", 
    "arxiv-id": "1106.5268v1", 
    "author": "D. Theseider Dupr\u00e8", 
    "publish": "2011-06-26T21:07:43Z", 
    "summary": "The automatic generation of decision trees based on off-line reasoning on\nmodels of a domain is a reasonable compromise between the advantages of using a\nmodel-based approach in technical domains and the constraints imposed by\nembedded applications. In this paper we extend the approach to deal with\ntemporal information. We introduce a notion of temporal decision tree, which is\ndesigned to make use of relevant information as long as it is acquired, and we\npresent an algorithm for compiling such trees from a model-based reasoning\nsystem."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1195", 
    "link": "http://arxiv.org/pdf/1106.5269v1", 
    "title": "Optimal Schedules for Parallelizing Anytime Algorithms: The Case of   Shared Resources", 
    "arxiv-id": "1106.5269v1", 
    "author": "E. Rivlin", 
    "publish": "2011-06-26T21:08:20Z", 
    "summary": "The performance of anytime algorithms can be improved by simultaneously\nsolving several instances of algorithm-problem pairs. These pairs may include\ndifferent instances of a problem (such as starting from a different initial\nstate), different algorithms (if several alternatives exist), or several runs\nof the same algorithm (for non-deterministic algorithms). In this paper we\npresent a methodology for designing an optimal scheduling policy based on the\nstatistical characteristics of the algorithms involved. We formally analyze the\ncase where the processes share resources (a single-processor model), and\nprovide an algorithm for optimal scheduling. We analyze, theoretically and\nempirically, the behavior of our scheduling algorithm for various distribution\ntypes. Finally, we present empirical results of applying our scheduling\nalgorithm to the Latin Square problem."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1200", 
    "link": "http://arxiv.org/pdf/1106.5270v1", 
    "title": "Decision-Theoretic Bidding Based on Learned Density Models in   Simultaneous, Interacting Auctions", 
    "arxiv-id": "1106.5270v1", 
    "author": "P. Stone", 
    "publish": "2011-06-26T21:08:54Z", 
    "summary": "Auctions are becoming an increasingly popular method for transacting\nbusiness, especially over the Internet. This article presents a general\napproach to building autonomous bidding agents to bid in multiple simultaneous\nauctions for interacting goods. A core component of our approach learns a model\nof the empirical price dynamics based on past data and uses the model to\nanalytically calculate, to the greatest extent possible, optimal bids. We\nintroduce a new and general boosting-based algorithm for conditional density\nestimation problems of this kind, i.e., supervised learning problems in which\nthe goal is to estimate the entire conditional distribution of the real-valued\nlabel. This approach is fully implemented as ATTac-2001, a top-scoring agent in\nthe second Trading Agent Competition (TAC-01). We present experiments\ndemonstrating the effectiveness of our boosting-based price predictor relative\nto several reasonable alternatives."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1144", 
    "link": "http://arxiv.org/pdf/1106.5271v1", 
    "title": "The Metric-FF Planning System: Translating \"Ignoring Delete Lists\" to   Numeric State Variables", 
    "arxiv-id": "1106.5271v1", 
    "author": "J. Hoffmann", 
    "publish": "2011-06-26T21:09:14Z", 
    "summary": "Planning with numeric state variables has been a challenge for many years,\nand was a part of the 3rd International Planning Competition (IPC-3). Currently\none of the most popular and successful algorithmic techniques in STRIPS\nplanning is to guide search by a heuristic function, where the heuristic is\nbased on relaxing the planning task by ignoring the delete lists of the\navailable actions. We present a natural extension of ``ignoring delete lists''\nto numeric state variables, preserving the relevant theoretical properties of\nthe STRIPS relaxation under the condition that the numeric task at hand is\n``monotonic''. We then identify a subset of the numeric IPC-3 competition\nlanguage, ``linear tasks'', where monotonicity can be achieved by\npre-processing. Based on that, we extend the algorithms used in the heuristic\nplanning system FF to linear tasks. The resulting system Metric-FF is,\naccording to the IPC-3 results which we discuss, one of the two currently most\nefficient numeric planners."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1144", 
    "link": "http://arxiv.org/pdf/1106.5312v1", 
    "title": "Manipulation of Nanson's and Baldwin's Rules", 
    "arxiv-id": "1106.5312v1", 
    "author": "Lirong Xia", 
    "publish": "2011-06-27T06:42:04Z", 
    "summary": "Nanson's and Baldwin's voting rules select a winner by successively\neliminating candidates with low Borda scores. We show that these rules have a\nnumber of desirable computational properties. In particular, with unweighted\nvotes, it is NP-hard to manipulate either rule with one manipulator, whilst\nwith weighted votes, it is NP-hard to manipulate either rule with a small\nnumber of candidates and a coalition of manipulators. As only a couple of other\nvoting rules are known to be NP-hard to manipulate with a single manipulator,\nNanson's and Baldwin's rules appear to be particularly resistant to\nmanipulation from a theoretical perspective. We also propose a number of\napproximation methods for manipulating these two rules. Experiments demonstrate\nthat both rules are often difficult to manipulate in practice. These results\nsuggest that elimination style voting rules deserve further study."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1144", 
    "link": "http://arxiv.org/pdf/1106.5427v1", 
    "title": "Theory and Algorithms for Partial Order Based Reduction in Planning", 
    "arxiv-id": "1106.5427v1", 
    "author": "Ruoyun Huang", 
    "publish": "2011-06-27T16:06:27Z", 
    "summary": "Search is a major technique for planning. It amounts to exploring a state\nspace of planning domains typically modeled as a directed graph. However,\nprohibitively large sizes of the search space make search expensive. Developing\nbetter heuristic functions has been the main technique for improving search\nefficiency. Nevertheless, recent studies have shown that improving heuristics\nalone has certain fundamental limits on improving search efficiency. Recently,\na new direction of research called partial order based reduction (POR) has been\nproposed as an alternative to improving heuristics. POR has shown promise in\nspeeding up searches.\n  POR has been extensively studied in model checking research and is a key\nenabling technique for scalability of model checking systems. Although the POR\ntheory has been extensively studied in model checking, it has never been\ndeveloped systematically for planning before. In addition, the conditions for\nPOR in the model checking theory are abstract and not directly applicable in\nplanning. Previous works on POR algorithms for planning did not establish the\nconnection between these algorithms and existing theory in model checking.\n  In this paper, we develop a theory for POR in planning. The new theory we\ndevelop connects the stubborn set theory in model checking and POR methods in\nplanning. We show that previous POR algorithms in planning can be explained by\nthe new theory. Based on the new theory, we propose a new, stronger POR\nalgorithm. Experimental results on various planning domains show further search\ncost reduction using the new algorithm."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1144", 
    "link": "http://arxiv.org/pdf/1106.5890v1", 
    "title": "A Comparison of Lex Bounds for Multiset Variables in Constraint   Programming", 
    "arxiv-id": "1106.5890v1", 
    "author": "Toby Walsh", 
    "publish": "2011-06-29T09:57:43Z", 
    "summary": "Set and multiset variables in constraint programming have typically been\nrepresented using subset bounds. However, this is a weak representation that\nneglects potentially useful information about a set such as its cardinality.\nFor set variables, the length-lex (LL) representation successfully provides\ninformation about the length (cardinality) and position in the lexicographic\nordering. For multiset variables, where elements can be repeated, we consider\nricher representations that take into account additional information. We study\neight different representations in which we maintain bounds according to one of\nthe eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC),\nlength-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC)\norderings. These representations integrate together information about the\ncardinality, variety (number of distinct elements in the multiset), and\nposition in some total ordering. Theoretical and empirical comparisons of\nexpressiveness and compactness of the eight representations suggest that\nlength-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually\ngive tighter bounds after constraint propagation. We implement the eight\nrepresentations and evaluate them against the subset bounds representation with\ncardinality and variety reasoning. Results demonstrate that they offer\nsignificantly better pruning and runtime."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1240", 
    "link": "http://arxiv.org/pdf/1106.5998v1", 
    "title": "The 3rd International Planning Competition: Results and Analysis", 
    "arxiv-id": "1106.5998v1", 
    "author": "D. Long", 
    "publish": "2011-06-29T16:42:59Z", 
    "summary": "This paper reports the outcome of the third in the series of biennial\ninternational planning competitions, held in association with the International\nConference on AI Planning and Scheduling (AIPS) in 2002. In addition to\ndescribing the domains, the planners and the objectives of the competition, the\npaper includes analysis of the results. The results are analysed from several\nperspectives, in order to address the questions of comparative performance\nbetween planners, comparative difficulty of domains, the degree of agreement\nbetween planners about the relative difficulty of individual problem instances\nand the question of how well planners scale relative to one another over\nincreasingly difficult problems. The paper addresses these questions through\nstatistical analysis of the raw results of the competition, in order to\ndetermine which results can be considered to be adequately supported by the\ndata. The paper concludes with a discussion of some challenges for the future\nof the competition series."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1466", 
    "link": "http://arxiv.org/pdf/1106.6022v1", 
    "title": "Use of Markov Chains to Design an Agent Bidding Strategy for Continuous   Double Auctions", 
    "arxiv-id": "1106.6022v1", 
    "author": "S. Park", 
    "publish": "2011-06-29T18:38:48Z", 
    "summary": "As computational agents are developed for increasingly complicated e-commerce\napplications, the complexity of the decisions they face demands advances in\nartificial intelligence techniques. For example, an agent representing a seller\nin an auction should try to maximize the seller's profit by reasoning about a\nvariety of possibly uncertain pieces of information, such as the maximum prices\nvarious buyers might be willing to pay, the possible prices being offered by\ncompeting sellers, the rules by which the auction operates, the dynamic arrival\nand matching of offers to buy and sell, and so on. A naive application of\nmultiagent reasoning techniques would require the seller's agent to explicitly\nmodel all of the other agents through an extended time horizon, rendering the\nproblem intractable for many realistically-sized problems. We have instead\ndevised a new strategy that an agent can use to determine its bid price based\non a more tractable Markov chain model of the auction process. We have\nexperimentally identified the conditions under which our new strategy works\nwell, as well as how well it works in comparison to the optimal performance the\nagent could have achieved had it known the future. Our results show that our\nnew strategy in general performs well, outperforming other tractable heuristic\nstrategies in a majority of experiments, and is particularly effective in a\n'seller?s market', where many buy offers are available."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1026", 
    "link": "http://arxiv.org/pdf/1107.0018v1", 
    "title": "A New Technique for Combining Multiple Classifiers using The   Dempster-Shafer Theory of Evidence", 
    "arxiv-id": "1107.0018v1", 
    "author": "M. Deriche", 
    "publish": "2011-06-30T20:31:52Z", 
    "summary": "This paper presents a new classifier combination technique based on the\nDempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a\npowerful method for combining measures of evidence from different classifiers.\nHowever, since each of the available methods that estimates the evidence of\nclassifiers has its own limitations, we propose here a new implementation which\nadapts to training data so that the overall mean square error is minimized. The\nproposed technique is shown to outperform most available classifier combination\nmethods when tested on three different classification problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1061", 
    "link": "http://arxiv.org/pdf/1107.0019v1", 
    "title": "Searching for Bayesian Network Structures in the Space of Restricted   Acyclic Partially Directed Graphs", 
    "arxiv-id": "1107.0019v1", 
    "author": "L. M. de Campos", 
    "publish": "2011-06-30T20:32:05Z", 
    "summary": "Although many algorithms have been designed to construct Bayesian network\nstructures using different approaches and principles, they all employ only two\nmethods: those based on independence criteria, and those based on a scoring\nfunction and a search procedure (although some methods combine the two). Within\nthe score+search paradigm, the dominant approach uses local search methods in\nthe space of directed acyclic graphs (DAGs), where the usual choices for\ndefining the elementary modifications (local changes) that can be applied are\narc addition, arc deletion, and arc reversal. In this paper, we propose a new\nlocal search method that uses a different search space, and which takes account\nof the concept of equivalence between network structures: restricted acyclic\npartially directed graphs (RPDAGs). In this way, the number of different\nconfigurations of the search space is reduced, thus improving efficiency.\nMoreover, although the final result must necessarily be a local optimum given\nthe nature of the search method, the topology of the new search space, which\navoids making early decisions about the directions of the arcs, may help to\nfind better local optima than those obtained by searching in the DAG space.\nDetailed results of the evaluation of the proposed search method on several\ntest problems, including the well-known Alarm Monitoring System, are also\npresented."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1096", 
    "link": "http://arxiv.org/pdf/1107.0020v1", 
    "title": "Learning to Order BDD Variables in Verification", 
    "arxiv-id": "1107.0020v1", 
    "author": "S. Markovitch", 
    "publish": "2011-06-30T20:32:16Z", 
    "summary": "The size and complexity of software and hardware systems have significantly\nincreased in the past years. As a result, it is harder to guarantee their\ncorrect behavior. One of the most successful methods for automated verification\nof finite-state systems is model checking. Most of the current model-checking\nsystems use binary decision diagrams (BDDs) for the representation of the\ntested model and in the verification process of its properties. Generally, BDDs\nallow a canonical compact representation of a boolean function (given an order\nof its variables). The more compact the BDD is, the better performance one gets\nfrom the verifier. However, finding an optimal order for a BDD is an\nNP-complete problem. Therefore, several heuristic methods based on expert\nknowledge have been developed for variable ordering. We propose an alternative\napproach in which the variable ordering algorithm gains 'ordering experience'\nfrom training models and uses the learned knowledge for finding good orders.\nOur methodology is based on offline learning of pair precedence classifiers\nfrom training models, that is, learning which variable pair permutation is more\nlikely to lead to a good order. For each training model, a number of training\nsequences are evaluated. Every training model variable pair permutation is then\ntagged based on its performance on the evaluated orders. The tagged\npermutations are then passed through a feature extractor and are given as\nexamples to a classifier creation algorithm. Given a model for which an order\nis requested, the ordering algorithm consults each precedence classifier and\nconstructs a pair precedence table which is used to create the order. Our\nalgorithm was integrated with SMV, which is one of the most widely used\nverification systems. Preliminary empirical evaluation of our methodology,\nusing real benchmark models, shows performance that is better than random\nordering and is competitive with existing algorithms that use expert knowledge.\nWe believe that in sub-domains of models (alu, caches, etc.) our system will\nprove even more valuable. This is because it features the ability to learn\nsub-domain knowledge, something that no other ordering algorithm does."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1213", 
    "link": "http://arxiv.org/pdf/1107.0021v1", 
    "title": "Decentralized Supply Chain Formation: A Market Protocol and Competitive   Equilibrium Analysis", 
    "arxiv-id": "1107.0021v1", 
    "author": "M. P. Wellman", 
    "publish": "2011-06-30T20:32:28Z", 
    "summary": "Supply chain formation is the process of determining the structure and terms\nof exchange relationships to enable a multilevel, multiagent production\nactivity. We present a simple model of supply chains, highlighting two\ncharacteristic features: hierarchical subtask decomposition, and resource\ncontention. To decentralize the formation process, we introduce a market price\nsystem over the resources produced along the chain. In a competitive\nequilibrium for this system, agents choose locally optimal allocations with\nrespect to prices, and outcomes are optimal overall. To determine prices, we\ndefine a market protocol based on distributed, progressive auctions, and\nmyopic, non-strategic agent bidding policies. In the presence of resource\ncontention, this protocol produces better solutions than the greedy protocols\ncommon in the artificial intelligence and multiagent systems literature. The\nprotocol often converges to high-value supply chains, and when competitive\nequilibria exist, typically to approximate competitive equilibria. However,\ncomplementarities in agent production technologies can cause the protocol to\nwastefully allocate inputs to agents that do not produce their outputs. A\nsubsequent decommitment phase recovers a significant fraction of the lost\nsurplus."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1234", 
    "link": "http://arxiv.org/pdf/1107.0023v1", 
    "title": "CP-nets: A Tool for Representing and Reasoning withConditional Ceteris   Paribus Preference Statements", 
    "arxiv-id": "1107.0023v1", 
    "author": "D. Poole", 
    "publish": "2011-06-30T20:32:52Z", 
    "summary": "Information about user preferences plays a key role in automated decision\nmaking. In many domains it is desirable to assess such preferences in a\nqualitative rather than quantitative way. In this paper, we propose a\nqualitative graphical representation of preferences that reflects conditional\ndependence and independence of preference statements under a ceteris paribus\n(all else being equal) interpretation. Such a representation is often compact\nand arguably quite natural in many circumstances. We provide a formal semantics\nfor this model, and describe how the structure of the network can be exploited\nin several inference tasks, such as determining whether one outcome dominates\n(is preferred to) another, ordering a set outcomes according to the preference\nrelation, and constructing the best outcome subject to available evidence."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1236", 
    "link": "http://arxiv.org/pdf/1107.0024v1", 
    "title": "Complexity Results and Approximation Strategies for MAP Explanations", 
    "arxiv-id": "1107.0024v1", 
    "author": "J. D. Park", 
    "publish": "2011-06-30T20:33:03Z", 
    "summary": "MAP is the problem of finding a most probable instantiation of a set of\nvariables given evidence. MAP has always been perceived to be significantly\nharder than the related problems of computing the probability of a variable\ninstantiation Pr, or the problem of computing the most probable explanation\n(MPE). This paper investigates the complexity of MAP in Bayesian networks.\nSpecifically, we show that MAP is complete for NP^PP and provide further\nnegative complexity results for algorithms based on variable elimination. We\nalso show that MAP remains hard even when MPE and Pr become easy. For example,\nwe show that MAP is NP-complete when the networks are restricted to polytrees,\nand even then can not be effectively approximated. Given the difficulty of\ncomputing MAP exactly, and the difficulty of approximating MAP while providing\nuseful guarantees on the resulting approximation, we investigate best effort\napproximations. We introduce a generic MAP approximation framework. We provide\ntwo instantiations of the framework; one for networks which are amenable to\nexact inference Pr, and one for networks for which even exact inference is too\nhard. This allows MAP approximation on networks that are too complex to even\nexactly solve the easier problems, Pr and MPE. Experimental results indicate\nthat using these approximation algorithms provides much better solutions than\nstandard techniques, and provide accurate MAP estimates in many cases."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1302", 
    "link": "http://arxiv.org/pdf/1107.0025v1", 
    "title": "Taming Numbers and Durations in the Model Checking Integrated Planning   System", 
    "arxiv-id": "1107.0025v1", 
    "author": "S. Edelkamp", 
    "publish": "2011-06-30T20:33:38Z", 
    "summary": "The Model Checking Integrated Planning System (MIPS) is a temporal least\ncommitment heuristic search planner based on a flexible object-oriented\nworkbench architecture. Its design clearly separates explicit and symbolic\ndirected exploration algorithms from the set of on-line and off-line computed\nestimates and associated data structures. MIPS has shown distinguished\nperformance in the last two international planning competitions. In the last\nevent the description language was extended from pure propositional planning to\ninclude numerical state variables, action durations, and plan quality objective\nfunctions. Plans were no longer sequences of actions but time-stamped\nschedules. As a participant of the fully automated track of the competition,\nMIPS has proven to be a general system; in each track and every benchmark\ndomain it efficiently computed plans of remarkable quality. This article\nintroduces and analyzes the most important algorithmic novelties that were\nnecessary to tackle the new layers of expressiveness in the benchmark problems\nand to achieve a high level of performance. The extensions include critical\npath analysis of sequentially generated plans to generate corresponding optimal\nparallel plans. The linear time algorithm to compute the parallel plan bypasses\nknown NP hardness results for partial ordering by scheduling plans with respect\nto the set of actions and the imposed precedence relations. The efficiency of\nthis algorithm also allows us to improve the exploration guidance: for each\nencountered planning state the corresponding approximate sequential plan is\nscheduled. One major strength of MIPS is its static analysis phase that grounds\nand simplifies parameterized predicates, functions and operators, that infers\nknowledge to minimize the state description length, and that detects domain\nobject symmetries. The latter aspect is analyzed in detail. MIPS has been\ndeveloped to serve as a complete and optimal state space planner, with\nadmissible estimates, exploration engines and branching cuts. In the\ncompetition version, however, certain performance compromises had to be made,\nincluding floating point arithmetic, weighted heuristic search exploration\naccording to an inadmissible estimate and parameterized optimization."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1309", 
    "link": "http://arxiv.org/pdf/1107.0026v1", 
    "title": "IDL-Expressions: A Formalism for Representing and Parsing Finite   Languages in Natural Language Processing", 
    "arxiv-id": "1107.0026v1", 
    "author": "G. Satta", 
    "publish": "2011-06-30T20:33:50Z", 
    "summary": "We propose a formalism for representation of finite languages, referred to as\nthe class of IDL-expressions, which combines concepts that were only considered\nin isolation in existing formalisms. The suggested applications are in natural\nlanguage processing, more specifically in surface natural language generation\nand in machine translation, where a sentence is obtained by first generating a\nlarge set of candidate sentences, represented in a compact way, and then by\nfiltering such a set through a parser. We study several formal properties of\nIDL-expressions and compare this new formalism with more standard ones. We also\npresent a novel parsing algorithm for IDL-expressions and prove a non-trivial\nupper bound on its time complexity."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1311", 
    "link": "http://arxiv.org/pdf/1107.0027v1", 
    "title": "Effective Dimensions of Hierarchical Latent Class Models", 
    "arxiv-id": "1107.0027v1", 
    "author": "N. L. Zhang", 
    "publish": "2011-06-30T20:34:11Z", 
    "summary": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks\nwhere leaf nodes are observed while internal nodes are latent. There are no\ntheoretically well justified model selection criteria for HLC models in\nparticular and Bayesian networks with latent nodes in general. Nonetheless,\nempirical studies suggest that the BIC score is a reasonable criterion to use\nin practice for learning HLC models. Empirical studies also suggest that\nsometimes model selection can be improved if standard model dimension is\nreplaced with effective model dimension in the penalty term of the BIC score.\nEffective dimensions are difficult to compute. In this paper, we prove a\ntheorem that relates the effective dimension of an HLC model to the effective\ndimensions of a number of latent class models. The theorem makes it\ncomputationally feasible to compute the effective dimensions of large HLC\nmodels. The theorem can also be used to compute the effective dimensions of\ngeneral tree models."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1322", 
    "link": "http://arxiv.org/pdf/1107.0030v1", 
    "title": "Coherent Integration of Databases by Abductive Logic Programming", 
    "arxiv-id": "1107.0030v1", 
    "author": "B. Van Nuffelen", 
    "publish": "2011-06-30T20:34:53Z", 
    "summary": "We introduce an abductive method for a coherent integration of independent\ndata-sources. The idea is to compute a list of data-facts that should be\ninserted to the amalgamated database or retracted from it in order to restore\nits consistency. This method is implemented by an abductive solver, called\nAsystem, that applies SLDNFA-resolution on a meta-theory that relates\ndifferent, possibly contradicting, input databases. We also give a pure\nmodel-theoretic analysis of the possible ways to `recover' consistent data from\nan inconsistent database in terms of those models of the database that exhibit\nas minimal inconsistent information as reasonably possible. This allows us to\ncharacterize the `recovered databases' in terms of the `preferred' (i.e., most\nconsistent) models of the theory. The outcome is an abductive-based application\nthat is sound and complete with respect to a corresponding model-based,\npreferential semantics, and -- to the best of our knowledge -- is more\nexpressive (thus more general) than any other implementation of coherent\nintegration of databases."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1327", 
    "link": "http://arxiv.org/pdf/1107.0031v1", 
    "title": "Grounded Semantic Composition for Visual Scenes", 
    "arxiv-id": "1107.0031v1", 
    "author": "D. Roy", 
    "publish": "2011-06-30T20:35:04Z", 
    "summary": "We present a visually-grounded language understanding model based on a study\nof how people verbally describe objects in scenes. The emphasis of the model is\non the combination of individual word meanings to produce meanings for complex\nreferring expressions. The model has been implemented, and it is able to\nunderstand a broad range of spatial referring expressions. We describe our\nimplementation of word level visually-grounded semantics and their embedding in\na compositional parsing framework. The implemented system selects the correct\nreferents in response to natural language expressions for a large percentage of\ntest cases. In an analysis of the system's successes and failures we reveal how\nvisual context influences the semantics of utterances and propose future\nextensions to the model that take such context into account."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1333", 
    "link": "http://arxiv.org/pdf/1107.0034v1", 
    "title": "Price Prediction in a Trading Agent Competition", 
    "arxiv-id": "1107.0034v1", 
    "author": "M. P. Wellman", 
    "publish": "2011-06-30T20:35:25Z", 
    "summary": "The 2002 Trading Agent Competition (TAC) presented a challenging market game\nin the domain of travel shopping. One of the pivotal issues in this domain is\nuncertainty about hotel prices, which have a significant influence on the\nrelative cost of alternative trip schedules. Thus, virtually all participants\nemploy some method for predicting hotel prices. We survey approaches employed\nin the tournament, finding that agents apply an interesting diversity of\ntechniques, taking into account differing sources of evidence bearing on\nprices. Based on data provided by entrants on their agents' actual predictions\nin the TAC-02 finals and semifinals, we analyze the relative efficacy of these\napproaches. The results show that taking into account game-specific information\nabout flight prices is a major distinguishing factor. Machine learning methods\neffectively induce the relationship between flight and hotel prices from game\ndata, and a purely analytical approach based on competitive equilibrium\nanalysis achieves equal accuracy with no historical data. Employing a new\nmeasure of prediction quality, we relate absolute accuracy to bottom-line\nperformance in the game."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1335", 
    "link": "http://arxiv.org/pdf/1107.0035v1", 
    "title": "Compositional Model Repositories via Dynamic Constraint Satisfaction   with Order-of-Magnitude Preferences", 
    "arxiv-id": "1107.0035v1", 
    "author": "Q. Shen", 
    "publish": "2011-06-30T20:35:37Z", 
    "summary": "The predominant knowledge-based approach to automated model construction,\ncompositional modelling, employs a set of models of particular functional\ncomponents. Its inference mechanism takes a scenario describing the constituent\ninteracting components of a system and translates it into a useful mathematical\nmodel. This paper presents a novel compositional modelling approach aimed at\nbuilding model repositories. It furthers the field in two respects. Firstly, it\nexpands the application domain of compositional modelling to systems that can\nnot be easily described in terms of interacting functional components, such as\necological systems. Secondly, it enables the incorporation of user preferences\ninto the model selection process. These features are achieved by casting the\ncompositional modelling problem as an activity-based dynamic preference\nconstraint satisfaction problem, where the dynamic constraints describe the\nrestrictions imposed over the composition of partial models and the preferences\ncorrespond to those of the user of the automated modeller. In addition, the\npreference levels are represented through the use of symbolic values that\ndiffer in orders of magnitude."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1338", 
    "link": "http://arxiv.org/pdf/1107.0037v1", 
    "title": "Competitive Coevolution through Evolutionary Complexification", 
    "arxiv-id": "1107.0037v1", 
    "author": "K. O. Stanley", 
    "publish": "2011-06-30T20:36:55Z", 
    "summary": "Two major goals in machine learning are the discovery and improvement of\nsolutions to complex problems. In this paper, we argue that complexification,\ni.e. the incremental elaboration of solutions through adding new structure,\nachieves both these goals. We demonstrate the power of complexification through\nthe NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves\nincreasingly complex neural network architectures. NEAT is applied to an\nopen-ended coevolutionary robot duel domain where robot controllers compete\nhead to head. Because the robot duel domain supports a wide range of\nstrategies, and because coevolution benefits from an escalating arms race, it\nserves as a suitable testbed for studying complexification. When compared to\nthe evolution of networks with fixed structure, complexifying evolution\ndiscovers significantly more sophisticated strategies. The results suggest that\nin order to discover and improve complex solutions, evolution, and search in\ngeneral, should be allowed to complexify as well as optimize."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1351", 
    "link": "http://arxiv.org/pdf/1107.0038v1", 
    "title": "Dual Modelling of Permutation and Injection Problems", 
    "arxiv-id": "1107.0038v1", 
    "author": "T. Walsh", 
    "publish": "2011-06-30T20:37:09Z", 
    "summary": "When writing a constraint program, we have to choose which variables should\nbe the decision variables, and how to represent the constraints on these\nvariables. In many cases, there is considerable choice for the decision\nvariables. Consider, for example, permutation problems in which we have as many\nvalues as variables, and each variable takes an unique value. In such problems,\nwe can choose between a primal and a dual viewpoint. In the dual viewpoint,\neach dual variable represents one of the primal values, whilst each dual value\nrepresents one of the primal variables. Alternatively, by means of channelling\nconstraints to link the primal and dual variables, we can have a combined model\nwith both sets of variables. In this paper, we perform an extensive theoretical\nand empirical study of such primal, dual and combined models for two classes of\nproblems: permutation problems and injection problems. Our results show that it\noften be advantageous to use multiple viewpoints, and to have constraints which\nchannel between them to maintain consistency. They also illustrate a general\nmethodology for comparing different constraint models."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1353", 
    "link": "http://arxiv.org/pdf/1107.0040v1", 
    "title": "Generalizing Boolean Satisfiability I: Background and Survey of Existing   Work", 
    "arxiv-id": "1107.0040v1", 
    "author": "A. J. Parkes", 
    "publish": "2011-06-30T20:38:04Z", 
    "summary": "This is the first of three planned papers describing ZAP, a satisfiability\nengine that substantially generalizes existing tools while retaining the\nperformance characteristics of modern high-performance solvers. The fundamental\nidea underlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal is to define a representation in which this structure is apparent and can\neasily be exploited to improve computational performance. This paper is a\nsurvey of the work underlying ZAP, and discusses previous attempts to improve\nthe performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting\nthe structure of the problem being solved. We examine existing ideas including\nextensions of the Boolean language to allow cardinality constraints,\npseudo-Boolean representations, symmetry, and a limited form of quantification.\nWhile this paper is intended as a survey, our research results are contained in\nthe two subsequent articles, with the theoretical structure of ZAP described in\nthe second paper in this series, and ZAP's implementation described in the\nthird."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1373", 
    "link": "http://arxiv.org/pdf/1107.0041v1", 
    "title": "PHA*: Finding the Shortest Path with A* in An Unknown Physical   Environment", 
    "arxiv-id": "1107.0041v1", 
    "author": "R. Stern", 
    "publish": "2011-06-30T20:38:33Z", 
    "summary": "We address the problem of finding the shortest path between two points in an\nunknown real physical environment, where a traveling agent must move around in\nthe environment to explore unknown territory. We introduce the Physical-A*\nalgorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes\nthat A* would expand and returns the shortest path between the two points.\nHowever, due to the physical nature of the problem, the complexity of the\nalgorithm is measured by the traveling effort of the moving agent and not by\nthe number of generated nodes, as in standard A*. PHA* is presented as a\ntwo-level algorithm, such that its high level, A*, chooses the next node to be\nexpanded and its low level directs the agent to that node in order to explore\nit. We present a number of variations for both the high-level and low-level\nprocedures and evaluate their performance theoretically and experimentally. We\nshow that the travel cost of our best variation is fairly close to the optimal\ntravel cost, assuming that the mandatory nodes of A* are known in advance. We\nthen generalize our algorithm to the multi-agent case, where a number of\ncooperative agents are designed to solve the problem. Specifically, we provide\nan experimental implementation for such a system. It should be noted that the\nproblem addressed here is not a navigation problem, but rather a problem of\nfinding the shortest path between two points for future usage."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1379", 
    "link": "http://arxiv.org/pdf/1107.0042v1", 
    "title": "Restricted Value Iteration: Theory and Algorithms", 
    "arxiv-id": "1107.0042v1", 
    "author": "W. Zhang", 
    "publish": "2011-06-30T20:38:52Z", 
    "summary": "Value iteration is a popular algorithm for finding near optimal policies for\nPOMDPs. It is inefficient due to the need to account for the entire belief\nspace, which necessitates the solution of large numbers of linear programs. In\nthis paper, we study value iteration restricted to belief subsets. We show\nthat, together with properly chosen belief subsets, restricted value iteration\nyields near-optimal policies and we give a condition for determining whether a\ngiven belief subset would bring about savings in space and time. We also apply\nrestricted value iteration to two interesting classes of POMDPs, namely\ninformative POMDPs and near-discernible POMDPs."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1400", 
    "link": "http://arxiv.org/pdf/1107.0043v1", 
    "title": "A Maximal Tractable Class of Soft Constraints", 
    "arxiv-id": "1107.0043v1", 
    "author": "A. Krokhin", 
    "publish": "2011-06-30T20:39:17Z", 
    "summary": "Many researchers in artificial intelligence are beginning to explore the use\nof soft constraints to express a set of (possibly conflicting) problem\nrequirements. A soft constraint is a function defined on a collection of\nvariables which associates some measure of desirability with each possible\ncombination of values for those variables. However, the crucial question of the\ncomputational complexity of finding the optimal solution to a collection of\nsoft constraints has so far received very little attention. In this paper we\nidentify a class of soft binary constraints for which the problem of finding\nthe optimal solution is tractable. In other words, we show that for any given\nset of such constraints, there exists a polynomial time algorithm to determine\nthe assignment having the best overall combined measure of desirability. This\ntractable class includes many commonly-occurring soft constraints, such as 'as\nnear as possible' or 'as soon as possible after', as well as crisp constraints\nsuch as 'greater than'. Finally, we show that this tractable class is maximal,\nin the sense that adding any other form of soft binary constraint which is not\nin the class gives rise to a class of problems which is NP-hard."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1410", 
    "link": "http://arxiv.org/pdf/1107.0044v1", 
    "title": "Towards Understanding and Harnessing the Potential of Clause Learning", 
    "arxiv-id": "1107.0044v1", 
    "author": "A. Sabharwal", 
    "publish": "2011-06-30T20:39:28Z", 
    "summary": "Efficient implementations of DPLL with the addition of clause learning are\nthe fastest complete Boolean satisfiability solvers and can handle many\nsignificant real-world problems, such as verification, planning and design.\nDespite its importance, little is known of the ultimate strengths and\nlimitations of the technique. This paper presents the first precise\ncharacterization of clause learning as a proof system (CL), and begins the task\nof understanding its power by relating it to the well-studied resolution proof\nsystem. In particular, we show that with a new learning scheme, CL can provide\nexponentially shorter proofs than many proper refinements of general resolution\n(RES) satisfying a natural property. These include regular and Davis-Putnam\nresolution, which are already known to be much stronger than ordinary DPLL. We\nalso show that a slight variant of CL with unlimited restarts is as powerful as\nRES itself. Translating these analytical results to practice, however, presents\na challenge because of the nondeterministic nature of clause learning\nalgorithms. We propose a novel way of exploiting the underlying problem\nstructure, in the form of a high level problem description such as a graph or\nPDDL specification, to guide clause learning algorithms toward faster\nsolutions. We show that this leads to exponential speed-ups on grid and\nrandomized pebbling problems, as well as substantial improvements on certain\nordering formulas."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1411", 
    "link": "http://arxiv.org/pdf/1107.0045v1", 
    "title": "Graduality in Argumentation", 
    "arxiv-id": "1107.0045v1", 
    "author": "M. C. Lagasquie-Schiex", 
    "publish": "2011-06-30T20:39:39Z", 
    "summary": "Argumentation is based on the exchange and valuation of interacting\narguments, followed by the selection of the most acceptable of them (for\nexample, in order to take a decision, to make a choice). Starting from the\nframework proposed by Dung in 1995, our purpose is to introduce 'graduality' in\nthe selection of the best arguments, i.e., to be able to partition the set of\nthe arguments in more than the two usual subsets of 'selected' and\n'non-selected' arguments in order to represent different levels of selection.\nOur basic idea is that an argument is all the more acceptable if it can be\npreferred to its attackers. First, we discuss general principles underlying a\n'gradual' valuation of arguments based on their interactions. Following these\nprinciples, we define several valuation models for an abstract argumentation\nsystem. Then, we introduce 'graduality' in the concept of acceptability of\narguments. We propose new acceptability classes and a refinement of existing\nclasses taking advantage of an available 'gradual' valuation."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1417", 
    "link": "http://arxiv.org/pdf/1107.0046v1", 
    "title": "Explicit Learning Curves for Transduction and Application to Clustering   and Compression Algorithms", 
    "arxiv-id": "1107.0046v1", 
    "author": "R. Meir", 
    "publish": "2011-06-30T20:39:52Z", 
    "summary": "Inductive learning is based on inferring a general rule from a finite data\nset and using it to label new data. In transduction one attempts to solve the\nproblem of using a labeled training set to label a set of unlabeled points,\nwhich are given to the learner prior to learning. Although transduction seems\nat the outset to be an easier task than induction, there have not been many\nprovably useful algorithms for transduction. Moreover, the precise relation\nbetween induction and transduction has not yet been determined. The main\ntheoretical developments related to transduction were presented by Vapnik more\nthan twenty years ago. One of Vapnik's basic results is a rather tight error\nbound for transductive classification based on an exact computation of the\nhypergeometric tail. While tight, this bound is given implicitly via a\ncomputational routine. Our first contribution is a somewhat looser but explicit\ncharacterization of a slightly extended PAC-Bayesian version of Vapnik's\ntransductive bound. This characterization is obtained using concentration\ninequalities for the tail of sums of random variables obtained by sampling\nwithout replacement. We then derive error bounds for compression schemes such\nas (transductive) support vector machines and for transduction algorithms based\non clustering. The main observation used for deriving these new error bounds\nand algorithms is that the unlabeled test points, which in the transductive\nsetting are known in advance, can be used in order to construct useful data\ndependent prior distributions over the hypothesis space."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1427", 
    "link": "http://arxiv.org/pdf/1107.0047v1", 
    "title": "Decentralized Control of Cooperative Systems: Categorization and   Complexity Analysis", 
    "arxiv-id": "1107.0047v1", 
    "author": "S. Zilberstein", 
    "publish": "2011-06-30T20:40:04Z", 
    "summary": "Decentralized control of cooperative systems captures the operation of a\ngroup of decision makers that share a single global objective. The difficulty\nin solving optimally such problems arises when the agents lack full\nobservability of the global state of the system when they operate. The general\nproblem has been shown to be NEXP-complete. In this paper, we identify classes\nof decentralized control problems whose complexity ranges between NEXP and P.\nIn particular, we study problems characterized by independent transitions,\nindependent observations, and goal-oriented objective functions. Two algorithms\nare shown to solve optimally useful classes of goal-oriented decentralized\nprocesses in polynomial time. This paper also studies information sharing among\nthe decision-makers, which can improve their performance. We distinguish\nbetween three ways in which agents can exchange information: indirect\ncommunication, direct communication and sharing state features that are not\ncontrolled by the agents. Our analysis shows that for every class of problems\nwe consider, introducing direct or indirect communication does not change the\nworst-case complexity. The results provide a better understanding of the\ncomplexity of decentralized control problems that arise in practice and\nfacilitate the development of planning algorithms for these problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1437", 
    "link": "http://arxiv.org/pdf/1107.0048v1", 
    "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting   in Categorizable Environments", 
    "arxiv-id": "1107.0048v1", 
    "author": "J. M. Porta", 
    "publish": "2011-06-30T20:40:15Z", 
    "summary": "In this paper, we confront the problem of applying reinforcement learning to\nagents that perceive the environment through many sensors and that can perform\nparallel actions using many actuators as is the case in complex autonomous\nrobots. We argue that reinforcement learning can only be successfully applied\nto this case if strong assumptions are made on the characteristics of the\nenvironment in which the learning is performed, so that the relevant sensor\nreadings and motor commands can be readily identified. The introduction of such\nassumptions leads to strongly-biased learning systems that can eventually lose\nthe generality of traditional reinforcement-learning algorithms. In this line,\nwe observe that, in realistic situations, the reward received by the robot\ndepends only on a reduced subset of all the executed actions and that only a\nreduced subset of the sensor inputs (possibly different in each situation and\nfor each action) are relevant to predict the reward. We formalize this property\nin the so called 'categorizability assumption' and we present an algorithm that\ntakes advantage of the categorizability of the environment, allowing a decrease\nin the learning time with respect to existing reinforcement-learning\nalgorithms. Results of the application of the algorithm to a couple of\nsimulated realistic-robotic problems (landmark-based navigation and the\nsix-legged robot gait generation) are reported to validate our approach and to\ncompare it to existing flat and generalization-based reinforcement-learning\napproaches."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1480", 
    "link": "http://arxiv.org/pdf/1107.0050v1", 
    "title": "Additive Pattern Database Heuristics", 
    "arxiv-id": "1107.0050v1", 
    "author": "R. E. Korf", 
    "publish": "2011-06-30T20:41:12Z", 
    "summary": "We explore a method for computing admissible heuristic evaluation functions\nfor search problems. It utilizes pattern databases, which are precomputed\ntables of the exact cost of solving various subproblems of an existing problem.\nUnlike standard pattern database heuristics, however, we partition our problems\ninto disjoint subproblems, so that the costs of solving the different\nsubproblems can be added together without overestimating the cost of solving\nthe original problem. Previously, we showed how to statically partition the\nsliding-tile puzzles into disjoint groups of tiles to compute an admissible\nheuristic, using the same partition for each state and problem instance. Here\nwe extend the method and show that it applies to other domains as well. We also\npresent another method for additive heuristics which we call dynamically\npartitioned pattern databases. Here we partition the problem into disjoint\nsubproblems for each state of the search dynamically. We discuss the pros and\ncons of each of these methods and apply both methods to three different problem\ndomains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and\nfinding an optimal vertex cover of a graph. We find that in some problem\ndomains, static partitioning is most effective, while in others dynamic\npartitioning is a better choice. In each of these problem domains, either\nstatically partitioned or dynamically partitioned pattern database heuristics\nare the best known heuristics for the problem."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1491", 
    "link": "http://arxiv.org/pdf/1107.0051v1", 
    "title": "On Prediction Using Variable Order Markov Models", 
    "arxiv-id": "1107.0051v1", 
    "author": "G. Yona", 
    "publish": "2011-06-30T20:43:01Z", 
    "summary": "This paper is concerned with algorithms for prediction of discrete sequences\nover a finite alphabet, using variable order Markov models. The class of such\nalgorithms is large and in principle includes any lossless compression\nalgorithm. We focus on six prominent prediction algorithms, including Context\nTree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic\nSuffix Trees (PSTs). We discuss the properties of these algorithms and compare\ntheir performance using real life sequences from three domains: proteins,\nEnglish text and music pieces. The comparison is made with respect to\nprediction quality as measured by the average log-loss. We also compare\nclassification algorithms based on these predictors with respect to a number of\nlarge protein classification tasks. Our results indicate that a \"decomposed\"\nCTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in\nsequence prediction tasks. Somewhat surprisingly, a different algorithm, which\nis a modification of the Lempel-Ziv compression algorithm, significantly\noutperforms all algorithms on the protein classification problems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1492", 
    "link": "http://arxiv.org/pdf/1107.0052v1", 
    "title": "Ordered Landmarks in Planning", 
    "arxiv-id": "1107.0052v1", 
    "author": "L. Sebastia", 
    "publish": "2011-06-30T20:43:14Z", 
    "summary": "Many known planning tasks have inherent constraints concerning the best order\nin which to achieve the goals. A number of research efforts have been made to\ndetect such constraints and to use them for guiding search, in the hope of\nspeeding up the planning process. We go beyond the previous approaches by\nconsidering ordering constraints not only over the (top-level) goals, but also\nover the sub-goals that will necessarily arise during planning. Landmarks are\nfacts that must be true at some point in every valid solution plan. We extend\nKoehler and Hoffmann's definition of reasonable orders between top level goals\nto the more general case of landmarks. We show how landmarks can be found, how\ntheir reasonable orders can be approximated, and how this information can be\nused to decompose a given planning task into several smaller sub-tasks. Our\nmethodology is completely domain- and planner-independent. The implementation\ndemonstrates that the approach can yield significant runtime performance\nimprovements when used as a control loop around state-of-the-art sub-optimal\nplanning systems, as exemplified by FF and LPG."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1496", 
    "link": "http://arxiv.org/pdf/1107.0053v2", 
    "title": "Finding Approximate POMDP solutions Through Belief Compression", 
    "arxiv-id": "1107.0053v2", 
    "author": "S. Thrun", 
    "publish": "2011-06-30T20:44:33Z", 
    "summary": "Standard value function approaches to finding policies for Partially\nObservable Markov Decision Processes (POMDPs) are generally considered to be\nintractable for large models. The intractability of these algorithms is to a\nlarge extent a consequence of computing an exact, optimal policy over the\nentire belief space. However, in real-world POMDP problems, computing the\noptimal policy for the full belief space is often unnecessary for good control\neven for problems with complicated policy classes. The beliefs experienced by\nthe controller often lie near a structured, low-dimensional subspace embedded\nin the high-dimensional belief space. Finding a good approximation to the\noptimal value function for only this subspace can be much easier than computing\nthe full value function. We introduce a new method for solving large-scale\nPOMDPs by reducing the dimensionality of the belief space. We use Exponential\nfamily Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to\nrepresent sparse, high-dimensional belief spaces using small sets of learned\nfeatures of the belief state. We then plan only in terms of the low-dimensional\nbelief features. By planning in this low-dimensional space, we can find\npolicies for POMDP models that are orders of magnitude larger than models that\ncan be handled by conventional techniques. We demonstrate the use of this\nalgorithm on a synthetic problem and on mobile robot navigation tasks."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1334", 
    "link": "http://arxiv.org/pdf/1107.0054v1", 
    "title": "A Comprehensive Trainable Error Model for Sung Music Queries", 
    "arxiv-id": "1107.0054v1", 
    "author": "C. J. Meek", 
    "publish": "2011-06-30T20:44:46Z", 
    "summary": "We propose a model for errors in sung queries, a variant of the hidden Markov\nmodel (HMM). This is a solution to the problem of identifying the degree of\nsimilarity between a (typically error-laden) sung query and a potential target\nin a database of musical works, an important problem in the field of music\ninformation retrieval. Similarity metrics are a critical component of\nquery-by-humming (QBH) applications which search audio and multimedia databases\nfor strong matches to oral queries. Our model comprehensively expresses the\ntypes of error or variation between target and query: cumulative and\nnon-cumulative local errors, transposition, tempo and tempo changes,\ninsertions, deletions and modulation. The model is not only expressive, but\nautomatically trainable, or able to learn and generalize from query examples.\nWe present results of simulations, designed to assess the discriminatory\npotential of the model, and tests with real sung queries, to demonstrate\nrelevance to real-world applications."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.0055v1", 
    "title": "Phase Transitions and Backbones of the Asymmetric Traveling Salesman   Problem", 
    "arxiv-id": "1107.0055v1", 
    "author": "W. Zhang", 
    "publish": "2011-06-30T20:45:03Z", 
    "summary": "In recent years, there has been much interest in phase transitions of\ncombinatorial problems. Phase transitions have been successfully used to\nanalyze combinatorial optimization problems, characterize their typical-case\nfeatures and locate the hardest problem instances. In this paper, we study\nphase transitions of the asymmetric Traveling Salesman Problem (ATSP), an\nNP-hard combinatorial optimization problem that has many real-world\napplications. Using random instances of up to 1,500 cities in which intercity\ndistances are uniformly distributed, we empirically show that many properties\nof the problem, including the optimal tour cost and backbone size, experience\nsharp transitions as the precision of intercity distances increases across a\ncritical value. Our experimental results on the costs of the ATSP tours and\nassignment problem agree with the theoretical result that the asymptotic cost\nof assignment problem is pi ^2 /6 the number of cities goes to infinity. In\naddition, we show that the average computational cost of the well-known\nbranch-and-bound subtour elimination algorithm for the problem also exhibits a\nthrashing behavior, transitioning from easy to difficult as the distance\nprecision increases. These results answer positively an open question regarding\nthe existence of phase transitions in the ATSP, and provide guidance on how\ndifficult ATSP problem instances should be generated."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.0134v2", 
    "title": "The Influence of Global Constraints on Similarity Measures for   Time-Series Databases", 
    "arxiv-id": "1107.0134v2", 
    "author": "Mirjana Ivanovi\u0107", 
    "publish": "2011-07-01T08:05:40Z", 
    "summary": "A time series consists of a series of values or events obtained over repeated\nmeasurements in time. Analysis of time series represents and important tool in\nmany application areas, such as stock market analysis, process and quality\ncontrol, observation of natural phenomena, medical treatments, etc. A vital\ncomponent in many types of time-series analysis is the choice of an appropriate\ndistance/similarity measure. Numerous measures have been proposed to date, with\nthe most successful ones based on dynamic programming. Being of quadratic time\ncomplexity, however, global constraints are often employed to limit the search\nspace in the matrix during the dynamic programming procedure, in order to speed\nup computation. Furthermore, it has been reported that such constrained\nmeasures can also achieve better accuracy. In this paper, we investigate two\nrepresentative time-series distance/similarity measures based on dynamic\nprogramming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS),\nand the effects of global constraints on them. Through extensive experiments on\na large number of time-series data sets, we demonstrate how global constrains\ncan significantly reduce the computation time of DTW and LCS. We also show\nthat, if the constraint parameter is tight enough (less than 10-15% of\ntime-series length), the constrained measure becomes significantly different\nfrom its unconstrained counterpart, in the sense of producing qualitatively\ndifferent 1-nearest neighbor graphs. This observation explains the potential\nfor accuracy gains when using constrained measures, highlighting the need for\ncareful tuning of constraint parameters in order to achieve a good trade-off\nbetween speed and accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.0194v1", 
    "title": "Law of Connectivity in Machine Learning", 
    "arxiv-id": "1107.0194v1", 
    "author": "Jitesh Dundas", 
    "publish": "2011-07-01T11:08:32Z", 
    "summary": "We present in this paper our law that there is always a connection present\nbetween two entities, with a selfconnection being present at least in each\nnode. An entity is an object, physical or imaginary, that is connected by a\npath (or connection) and which is important for achieving the desired result of\nthe scenario. In machine learning, we state that for any scenario, a subject\nentity is always, directly or indirectly, connected and affected by single or\nmultiple independent / dependent entities, and their impact on the subject\nentity is dependent on various factors falling into the categories such as the\nexistenc"
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.0268v2", 
    "title": "Simple Algorithm Portfolio for SAT", 
    "arxiv-id": "1107.0268v2", 
    "author": "Predrag Janicic", 
    "publish": "2011-07-01T16:20:44Z", 
    "summary": "The importance of algorithm portfolio techniques for SAT has long been noted,\nand a number of very successful systems have been devised, including the most\nsuccessful one --- SATzilla. However, all these systems are quite complex (to\nunderstand, reimplement, or modify). In this paper we propose a new algorithm\nportfolio for SAT that is extremely simple, but in the same time so efficient\nthat it outperforms SATzilla. For a new SAT instance to be solved, our\nportfolio finds its k-nearest neighbors from the training set and invokes a\nsolver that performs the best at those instances. The main distinguishing\nfeature of our algorithm portfolio is the locality of the selection procedure\n--- the selection of a SAT solver is based only on few instances similar to the\ninput one."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.1020v1", 
    "title": "A Novel Multicriteria Group Decision Making Approach With Intuitionistic   Fuzzy SIR Method", 
    "arxiv-id": "1107.1020v1", 
    "author": "James N. K. Liu", 
    "publish": "2011-07-06T03:32:21Z", 
    "summary": "The superiority and inferiority ranking (SIR) method is a generation of the\nwell-known PROMETHEE method, which can be more efficient to deal with\nmulti-criterion decision making (MCDM) problem. Intuitionistic fuzzy sets\n(IFSs), as an important extension of fuzzy sets (IFs), include both membership\nfunctions and non-membership functions and can be used to, more precisely\ndescribe uncertain information. In real world, decision situations are usually\nunder uncertain environment and involve multiple individuals who have their own\npoints of view on handing of decision problems. In order to solve uncertainty\ngroup MCDM problem, we propose a novel intuitionistic fuzzy SIR method in this\npaper. This approach uses intuitionistic fuzzy aggregation operators and SIR\nranking methods to handle uncertain information; integrate individual opinions\ninto group opinions; make decisions on multiple-criterion; and finally\nstructure a specific decision map. The proposed approach is illustrated in a\nsimulation of group decision making problem related to supply chain management."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.1686v1", 
    "title": "Proceedings of the Doctoral Consortium and Poster Session of the 5th   International Symposium on Rules (RuleML 2011@IJCAI)", 
    "arxiv-id": "1107.1686v1", 
    "author": "Umberto Straccia", 
    "publish": "2011-07-08T18:00:49Z", 
    "summary": "This volume contains the papers presented at the first edition of the\nDoctoral Consortium of the 5th International Symposium on Rules (RuleML\n2011@IJCAI) held on July 19th, 2011 in Barcelona, as well as the poster session\npapers of the RuleML 2011@IJCAI main conference."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.1950v1", 
    "title": "Knowledge Embedding and Retrieval Strategies in an Informledge System", 
    "arxiv-id": "1107.1950v1", 
    "author": "Meenakshi Malhotra", 
    "publish": "2011-07-11T07:13:43Z", 
    "summary": "Informledge System (ILS) is a knowledge network with autonomous nodes and\nintelligent links that integrate and structure the pieces of knowledge. In this\npaper, we put forward the strategies for knowledge embedding and retrieval in\nan ILS. ILS is a powerful knowledge network system dealing with logical storage\nand connectivity of information units to form knowledge using autonomous nodes\nand multi-lateral links. In ILS, the autonomous nodes known as Knowledge\nNetwork Nodes (KNN)s play vital roles which are not only used in storage,\nparsing and in forming the multi-lateral linkages between knowledge points but\nalso in helping the realization of intelligent retrieval of linked information\nunits in the form of knowledge. Knowledge built in to the ILS forms the shape\nof sphere. The intelligence incorporated into the links of a KNN helps in\nretrieving various knowledge threads from a specific set of KNNs. A developed\nentity of information realized through KNN forms in to the shape of a knowledge\ncone"
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2086v1", 
    "title": "Extend Commitment Protocols with Temporal Regulations: Why and How", 
    "arxiv-id": "1107.2086v1", 
    "author": "Cristina Baroglio", 
    "publish": "2011-07-11T18:48:59Z", 
    "summary": "The proposal of Elisa Marengo's thesis is to extend commitment protocols to\nexplicitly account for temporal regulations. This extension will satisfy two\nneeds: (1) it will allow representing, in a flexible and modular way, temporal\nregulations with a normative force, posed on the interaction, so as to\nrepresent conventions, laws and suchlike; (2) it will allow committing to\ncomplex conditions, which describe not only what will be achieved but to some\nextent also how. These two aspects will be deeply investigated in the proposal\nof a unified framework, which is part of the ongoing work and will be included\nin the thesis."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2087v1", 
    "title": "Rule-Based Semantic Sensing", 
    "arxiv-id": "1107.2087v1", 
    "author": "Alun Preece", 
    "publish": "2011-07-11T18:50:19Z", 
    "summary": "Rule-Based Systems have been in use for decades to solve a variety of\nproblems but not in the sensor informatics domain. Rules aid the aggregation of\nlow-level sensor readings to form a more complete picture of the real world and\nhelp to address 10 identified challenges for sensor network middleware. This\npaper presents the reader with an overview of a system architecture and a pilot\napplication to demonstrate the usefulness of a system integrating rules with\nsensor middleware."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2088v1", 
    "title": "Advancing Multi-Context Systems by Inconsistency Management", 
    "arxiv-id": "1107.2088v1", 
    "author": "Antonius Weinzierl", 
    "publish": "2011-07-11T18:52:29Z", 
    "summary": "Multi-Context Systems are an expressive formalism to model (possibly)\nnon-monotonic information exchange between heterogeneous knowledge bases. Such\ninformation exchange, however, often comes with unforseen side-effects leading\nto violation of constraints, making the system inconsistent, and thus unusable.\nAlthough there are many approaches to assess and repair a single inconsistent\nknowledge base, the heterogeneous nature of Multi-Context Systems poses\nproblems which have not yet been addressed in a satisfying way: How to identify\nand explain a inconsistency that spreads over multiple knowledge bases with\ndifferent logical formalisms (e.g., logic programs and ontologies)? What are\nthe causes of inconsistency if inference/information exchange is non-monotonic\n(e.g., absent information as cause)? How to deal with inconsistency if access\nto knowledge bases is restricted (e.g., companies exchange information, but do\nnot allow arbitrary modifications to their knowledge bases)? Many traditional\napproaches solely aim for a consistent system, but automatic removal of\ninconsistency is not always desireable. Therefore a human operator has to be\nsupported in finding the erroneous parts contributing to the inconsistency. In\nmy thesis those issues will be adressed mainly from a foundational perspective,\nwhile our research project also provides algorithms and prototype\nimplementations."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2089v1", 
    "title": "Rule-based query answering method for a knowledge base of economic   crimes", 
    "arxiv-id": "1107.2089v1", 
    "author": "Jaroslaw Bak", 
    "publish": "2011-07-11T18:53:32Z", 
    "summary": "We present a description of the PhD thesis which aims to propose a rule-based\nquery answering method for relational data. In this approach we use an\nadditional knowledge which is represented as a set of rules and describes the\nsource data at concept (ontological) level. Queries are posed in the terms of\nabstract level. We present two methods. The first one uses hybrid reasoning and\nthe second one exploits only forward chaining. These two methods are\ndemonstrated by the prototypical implementation of the system coupled with the\nJess engine. Tests are performed on the knowledge base of the selected economic\ncrimes: fraudulent disbursement and money laundering."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2090v1", 
    "title": "Semantic-ontological combination of Business Rules and Business   Processes in IT Service Management", 
    "arxiv-id": "1107.2090v1", 
    "author": "Erwin Zinser", 
    "publish": "2011-07-11T18:54:36Z", 
    "summary": "IT Service Management deals with managing a broad range of items related to\ncomplex system environments. As there is both, a close connection to business\ninterests and IT infrastructure, the application of semantic expressions which\nare seamlessly integrated within applications for managing ITSM environments,\ncan help to improve transparency and profitability. This paper focuses on the\nchallenges regarding the integration of semantics and ontologies within ITSM\nenvironments. It will describe the paradigm of relationships and inheritance\nwithin complex service trees and will present an approach of ontologically\nexpressing them. Furthermore, the application of SBVR-based rules as executable\nSQL triggers will be discussed. Finally, the broad range of topics for further\nresearch, derived from the findings, will be presented."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.2997v1", 
    "title": "An Ontology-driven Framework for Supporting Complex Decision Process", 
    "arxiv-id": "1107.2997v1", 
    "author": "James N. K. Liu", 
    "publish": "2011-07-15T07:17:08Z", 
    "summary": "The study proposes a framework of ONTOlogy-based Group Decision Support\nSystem (ONTOGDSS) for decision process which exhibits the complex structure of\ndecision-problem and decision-group. It is capable of reducing the complexity\nof problem structure and group relations. The system allows decision makers to\nparticipate in group decision-making through the web environment, via the\nontology relation. It facilitates the management of decision process as a\nwhole, from criteria generation, alternative evaluation, and opinion\ninteraction to decision aggregation. The embedded ontology structure in\nONTOGDSS provides the important formal description features to facilitate\ndecision analysis and verification. It examines the software architecture, the\nselection methods, the decision path, etc. Finally, the ontology application of\nthis system is illustrated with specific real case to demonstrate its\npotentials towards decision-making development."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.3302v1", 
    "title": "A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems", 
    "arxiv-id": "1107.3302v1", 
    "author": "Ouahiba Chouhal", 
    "publish": "2011-07-17T14:13:34Z", 
    "summary": "Fault diagnosis and failure prognosis are essential techniques in improving\nthe safety of many manufacturing systems. Therefore, on-line fault detection\nand isolation is one of the most important tasks in safety-critical and\nintelligent control systems. Computational intelligence techniques are being\ninvestigated as extension of the traditional fault diagnosis methods. This\npaper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within\nan application study of a manufacturing system. The key issues of finding a\nsuitable structure for detecting and isolating ten realistic actuator faults\nare described. Within this framework, data-processing interactive software of\nsimulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.\n  This software devoted primarily to creation, training and test of a\nclassification Neuro-Fuzzy system of industrial process failures. NEFDIAG can\nbe represented like a special type of fuzzy perceptron, with three layers used\nto classify patterns and failures. The system selected is the workshop of\nSCIMAT clinker, cement factory in Algeria."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.3663v1", 
    "title": "Towards Open-Text Semantic Parsing via Multi-Task Learning of Structured   Embeddings", 
    "arxiv-id": "1107.3663v1", 
    "author": "Yoshua Bengio", 
    "publish": "2011-07-19T09:44:09Z", 
    "summary": "Open-text (or open-domain) semantic parsers are designed to interpret any\nstatement in natural language by inferring a corresponding meaning\nrepresentation (MR). Unfortunately, large scale systems cannot be easily\nmachine-learned due to lack of directly supervised data. We propose here a\nmethod that learns to assign MRs to a wide range of text (using a dictionary of\nmore than 70,000 words, which are mapped to more than 40,000 entities) thanks\nto a training scheme that combines learning from WordNet and ConceptNet with\nlearning from raw text. The model learns structured embeddings of words,\nentities and MRs via a multi-task training process operating on these diverse\nsources of data that integrates all the learnt knowledge into a single system.\nThis work ends up combining methods for knowledge acquisition, semantic\nparsing, and word-sense disambiguation. Experiments on various tasks indicate\nthat our approach is indeed successful and can form a basis for future more\nsophisticated systems."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.3894v2", 
    "title": "Online Anomaly Detection Systems Using Incremental Commute Time", 
    "arxiv-id": "1107.3894v2", 
    "author": "Sanjay Chawla", 
    "publish": "2011-07-20T05:35:40Z", 
    "summary": "Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has\nfound widespread applications in many domains including personalized search,\ncollaborative filtering and making search engines robust against manipulation.\nOur interest is inspired by the use of CTD as a metric for anomaly detection.\nIt has been shown that CTD can be used to simultaneously identify both global\nand local anomalies. Here we propose an accurate and efficient approximation\nfor computing the CTD in an incremental fashion in order to facilitate\nreal-time applications. An online anomaly detection algorithm is designed where\nthe CTD of each new arriving data point to any point in the current graph can\nbe estimated in constant time ensuring a real-time response. Moreover, the\nproposed approach can also be applied in many other applications that utilize\ncommute time distance."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.4035v2", 
    "title": "Towards Completely Lifted Search-based Probabilistic Inference", 
    "arxiv-id": "1107.4035v2", 
    "author": "Jacek Kisynski", 
    "publish": "2011-07-20T17:04:12Z", 
    "summary": "The promise of lifted probabilistic inference is to carry out probabilistic\ninference in a relational probabilistic model without needing to reason about\neach individual separately (grounding out the representation) by treating the\nundistinguished individuals as a block. Current exact methods still need to\nground out in some cases, typically because the representation of the\nintermediate results is not closed under the lifted operations. We set out to\nanswer the question as to whether there is some fundamental reason why lifted\nalgorithms would need to ground out undifferentiated individuals. We have two\nmain results: (1) We completely characterize the cases where grounding is\npolynomial in a population size, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2)\nFor the case of no-argument and single-argument parametrized random variables\nwhere the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is\nexponential. Neither of these cases requires reasoning separately about the\nindividuals that are not explicitly mentioned."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.4161v1", 
    "title": "Local Optima Networks of the Quadratic Assignment Problem", 
    "arxiv-id": "1107.4161v1", 
    "author": "Marco Tomassini", 
    "publish": "2011-07-21T05:07:25Z", 
    "summary": "Using a recently proposed model for combinatorial landscapes, Local Optima\nNetworks (LON), we conduct a thorough analysis of two types of instances of the\nQuadratic Assignment Problem (QAP). This network model is a reduction of the\nlandscape in which the nodes correspond to the local optima, and the edges\naccount for the notion of adjacency between their basins of attraction. The\nmodel was inspired by the notion of 'inherent network' of potential energy\nsurfaces proposed in physical-chemistry. The local optima networks extracted\nfrom the so called uniform and real-like QAP instances, show features clearly\ndistinguishing these two types of instances. Apart from a clear confirmation\nthat the search difficulty increases with the problem dimension, the analysis\nprovides new confirming evidence explaining why the real-like instances are\neasier to solve exactly using heuristic search, while the uniform instances are\neasier to solve approximately. Although the local optima network model is still\nunder development, we argue that it provides a novel view of combinatorial\nlandscapes, opening up the possibilities for new analytical tools and\nunderstanding of problem difficulty in combinatorial optimization."
},{
    "category": "cs.AI", 
    "doi": "10.1613/jair.1389", 
    "link": "http://arxiv.org/pdf/1107.4162v1", 
    "title": "Local Optima Networks of NK Landscapes with Neutrality", 
    "arxiv-id": "1107.4162v1", 
    "author": "Marco Tomassini", 
    "publish": "2011-07-21T05:08:03Z", 
    "summary": "In previous work we have introduced a network-based model that abstracts many\ndetails of the underlying landscape and compresses the landscape information\ninto a weighted, oriented graph which we call the local optima network. The\nvertices of this graph are the local optima of the given fitness landscape,\nwhile the arcs are transition probabilities between local optima basins. Here\nwe extend this formalism to neutral fitness landscapes, which are common in\ndifficult combinatorial search spaces. By using two known neutral variants of\nthe NK family (i.e. NKp and NKq) in which the amount of neutrality can be tuned\nby a parameter, we show that our new definitions of the optima networks and the\nassociated basins are consistent with the previous definitions for the\nnon-neutral case. Moreover, our empirical study and statistical analysis show\nthat the features of neutral landscapes interpolate smoothly between landscapes\nwith maximum neutrality and non-neutral ones. We found some unknown structural\ndifferences between the two studied families of neutral landscapes. But\noverall, the network features studied confirmed that neutrality, in landscapes\nwith percolating neutral networks, may enhance heuristic search. Our current\nmethodology requires the exhaustive enumeration of the underlying search space.\nTherefore, sampling techniques should be developed before this analysis can\nhave practical implications. We argue, however, that the proposed model offers\na new perspective into the problem difficulty of combinatorial optimization\nproblems and may inspire the design of more effective search heuristics."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/1107.4163v1", 
    "title": "Centric selection: a way to tune the exploration/exploitation trade-off", 
    "arxiv-id": "1107.4163v1", 
    "author": "Manuel Clergue", 
    "publish": "2011-07-21T05:08:30Z", 
    "summary": "In this paper, we study the exploration / exploitation trade-off in cellular\ngenetic algorithms. We define a new selection scheme, the centric selection,\nwhich is tunable and allows controlling the selective pressure with a single\nparameter. The equilibrium model is used to study the influence of the centric\nselection on the selective pressure and a new model which takes into account\nproblem dependent statistics and selective pressure in order to deal with the\nexploration / exploitation trade-off is proposed: the punctuated equilibria\nmodel. Performances on the quadratic assignment problem and NK-Landscapes put\nin evidence an optimal exploration / exploitation trade-off on both of the\nclasses of problems. The punctuated equilibria model is used to explain these\nresults."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/1107.4164v1", 
    "title": "NK landscapes difficulty and Negative Slope Coefficient: How Sampling   Influences the Results", 
    "arxiv-id": "1107.4164v1", 
    "author": "Marco Tomassini", 
    "publish": "2011-07-21T05:08:50Z", 
    "summary": "Negative Slope Coefficient is an indicator of problem hardness that has been\nintroduced in 2004 and that has returned promising results on a large set of\nproblems. It is based on the concept of fitness cloud and works by partitioning\nthe cloud into a number of bins representing as many different regions of the\nfitness landscape. The measure is calculated by joining the bins centroids by\nsegments and summing all their negative slopes. In this paper, for the first\ntime, we point out a potential problem of the Negative Slope Coefficient: we\nstudy its value for different instances of the well known NK-landscapes and we\nshow how this indicator is dramatically influenced by the minimum number of\npoints contained into a bin. Successively, we formally justify this behavior of\nthe Negative Slope Coefficient and we discuss pros and cons of this measure."
}]