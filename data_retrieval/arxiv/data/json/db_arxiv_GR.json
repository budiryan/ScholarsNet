[{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/9301112v1", 
    "title": "A note on digitized angles", 
    "arxiv-id": "cs/9301112v1", 
    "author": "Donald E. Knuth", 
    "publish": "1990-04-01T00:00:00Z", 
    "summary": "We study the configurations of pixels that occur when two digitized straight\nlines meet each other."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0206029v1", 
    "title": "Computer-Generated Photorealistic Hair", 
    "arxiv-id": "cs/0206029v1", 
    "author": "Alice J. Lin", 
    "publish": "2002-06-20T06:21:15Z", 
    "summary": "This paper presents an efficient method for generating and rendering\nphotorealistic hair in two dimensional pictures. The method consists of three\nmajor steps. Simulating an artist drawing is used to design the rough hair\nshape. A convolution based filter is then used to generate photorealistic hair\npatches. A refine procedure is finally used to blend the boundaries of the\npatches with surrounding areas. This method can be used to create all types of\nphotorealistic human hair (head hair, facial hair and body hair). It is also\nsuitable for fur and grass generation. Applications of this method include:\nhairstyle designing/editing, damaged hair image restoration, human hair\nanimation, virtual makeover of a human, and landscape creation."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0304011v1", 
    "title": "Embedded Reflection Mapping", 
    "arxiv-id": "cs/0304011v1", 
    "author": "Goncalo Carvalho", 
    "publish": "2003-04-08T14:17:53Z", 
    "summary": "Environment maps are used to simulate reflections off curved objects. We\npresent a technique to reflect a user, or a group of users, in a real\nenvironment, onto a virtual object, in a virtual reality application, using the\nlive video feeds from a set of cameras, in real-time. Our setup can be used in\na variety of environments ranging from outdoor or indoor scenes."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0305057v1", 
    "title": "The Persint visualization program for the ATLAS experiment", 
    "arxiv-id": "cs/0305057v1", 
    "author": "M. Virchaux", 
    "publish": "2003-05-29T22:06:27Z", 
    "summary": "The Persint program is designed for the three-dimensional representation of\nobjects and for the interfacing and access to a variety of independent\napplications, in a fully interactive way. Facilities are provided for the\nspatial navigation and the definition of the visualization properties, in order\nto interactively set the viewing and viewed points, and to obtain the desired\nperspective. In parallel, applications may be launched through the use of\ndedicated interfaces, such as the interactive reconstruction and display of\nphysics events. Recent developments have focalized on the interfacing to the\nXML ATLAS General Detector Description AGDD, making it a widely used tool for\nXML developers. The graphics capabilities of this program were exploited in the\ncontext of the ATLAS 2002 Muon Testbeam where it was used as an online event\ndisplay, integrated in the online software framework and participating in the\ncommissioning and debug of the detector system."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0306012v1", 
    "title": "GraXML - Modular Geometric Modeler", 
    "arxiv-id": "cs/0306012v1", 
    "author": "Julius Hrivnac", 
    "publish": "2003-06-02T09:04:18Z", 
    "summary": "Many entities managed by HEP Software Frameworks represent spatial\n(3-dimensional) real objects. Effective definition, manipulation and\nvisualization of such objects is an indispensable functionality.\n  GraXML is a modular Geometric Modeling toolkit capable of processing\ngeometric data of various kinds (detector geometry, event geometry) from\ndifferent sources and delivering them in ways suitable for further use.\nGeometric data are first modeled in one of the Generic Models. Those Models are\nthen used to populate powerful Geometric Model based on the Java3D technology.\nWhile Java3D has been originally created just to provide visualization of 3D\nobjects, its light weight and high functionality allow an effective reuse as a\ngeneral geometric component. This is possible also thanks to a large overlap\nbetween graphical and general geometric functionality and modular design of\nJava3D itself. Its graphical functionalities also allow a natural visualization\nof all manipulated elements.\n  All these techniques have been developed primarily (or only) for the Java\nenvironment. It is, however, possible to interface them transparently to\nFrameworks built in other languages, like for example C++.\n  The GraXML toolkit has been tested with data from several sources, as for\nexample ATLAS and ALICE detector description and ATLAS event data. Prototypes\nfor other sources, like Geometry Description Markup Language (GDML) exist too\nand interface to any other source is easy to add."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0306031v1", 
    "title": "The FRED Event Display: an Extensible HepRep Client for GLAST", 
    "arxiv-id": "cs/0306031v1", 
    "author": "Riccardo Giannitrapani", 
    "publish": "2003-06-06T12:34:53Z", 
    "summary": "A new graphics client prototype for the HepRep protocol is presented. Based\non modern toolkits and high level languages (C++ and Ruby), Fred is an\nexperiment to test applicability of scripting facilities to the high energy\nphysics event display domain. Its flexible structure, extensibility and the use\nof the HepRep protocol are key features for its use in the astroparticle\nexperiment GLAST."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0306059v1", 
    "title": "The Use of HepRep in GLAST", 
    "arxiv-id": "cs/0306059v1", 
    "author": "M. Frailis", 
    "publish": "2003-06-12T20:37:32Z", 
    "summary": "HepRep is a generic, hierarchical format for description of graphics\nrepresentables that can be augmented by physics information and relational\nproperties. It was developed for high energy physics event display applications\nand is especially suited to client/server or component frameworks. The GLAST\nexperiment, an international effort led by NASA for a gamma-ray telescope to\nlaunch in 2006, chose HepRep to provide a flexible, extensible and maintainable\nframework for their event display without tying their users to any one graphics\napplication. To support HepRep in their GUADI infrastructure, GLAST developed a\nHepRep filler and builder architecture. The architecture hides the details of\nXML and CORBA in a set of base and helper classes allowing physics experts to\nfocus on what data they want to represent. GLAST has two GAUDI services:\nHepRepSvc, which registers HepRep fillers in a global registry and allows the\nHepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be\npublished through a CORBA interface and which allows the client application to\nfeed commands back to GAUDI (such as start next event, or run some GAUDI\nalgorithm). GLAST's HepRep solution gives users a choice of client\napplications, WIRED (written in Java) or FRED (written in C++ and Ruby), and\nleaves them free to move to any future HepRep-compliant event display."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0307065v1", 
    "title": "Application of interactive parallel visualization for commodity-based   clusters using visualization APIs", 
    "arxiv-id": "cs/0307065v1", 
    "author": "John Spiletic", 
    "publish": "2003-07-29T13:40:12Z", 
    "summary": "We present an efficient and inexpensive to develop application for\ninteractive high-performance parallel visualization. We extend popular APIs\nsuch as Open Inventor and VTK to support commodity-based cluster visualization.\nOur implementation follows a standard master/slave concept: the general idea is\nto have a ``Master'' node, which will intercept a sequential graphical user\ninterface (GUI) and broadcast it to the ``Slave'' nodes. The interactions\nbetween the nodes are implemented using MPI. The parallel remote rendering uses\nChromium. This paper is mainly the report of our implementation experiences. We\npresent in detail the proposed model and key aspects of its implementation.\nAlso, we present performance measurements, we benchmark and quantitatively\ndemonstrate the dependence of the visualization speed on the data size and the\nnetwork bandwidth, and we identify the singularities and draw conclusions on\nChromium's sort-first rendering architecture. The most original part of this\nwork is the combined use of Open Inventor and Chromium."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0311034v1", 
    "title": "Visualization of variations in human brain morphology using   differentiating reflection functions", 
    "arxiv-id": "cs/0311034v1", 
    "author": "Gibby Koldenhof", 
    "publish": "2003-11-22T18:17:26Z", 
    "summary": "Conventional visualization media such as MRI prints and computer screens are\ninherently two dimensional, making them incapable of displaying true 3D volume\ndata sets. By applying only transparency or intensity projection, and ignoring\nlight-matter interaction, results will likely fail to give optimal results.\nLittle research has been done on using reflectance functions to visually\nseparate the various segments of a MRI volume. We will explore if applying\nspecific reflectance functions to individual anatomical structures can help in\nbuilding an intuitive 2D image from a 3D dataset. We will test our hypothesis\nby visualizing a statistical analysis of the genetic influences on variations\nin human brain morphology because it inherently contains complex and many\ndifferent types of data making it a good candidate for our approach"
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0404022v1", 
    "title": "An Algorithm for Transforming Color Images into Tactile Graphics", 
    "arxiv-id": "cs/0404022v1", 
    "author": "Artur Rataj", 
    "publish": "2004-04-08T14:07:05Z", 
    "summary": "This paper presents an algorithm that transforms color visual images, like\nphotographs or paintings, into tactile graphics. In the algorithm, the edges of\nobjects are detected and colors of the objects are estimated. Then, the edges\nand the colors are encoded into lines and textures in the output tactile image.\nDesign of the method is substantiated by various qualities of haptic\nrecognizing of images. Also, means of presentation of the tactile images in\nprintouts are discussed. Example translated images are shown."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0405048v1", 
    "title": "Interactive visualization of higher dimensional data in a multiview   environment", 
    "arxiv-id": "cs/0405048v1", 
    "author": "Michael McGuigan", 
    "publish": "2004-05-14T18:18:04Z", 
    "summary": "We develop multiple view visualization of higher dimensional data. Our work\nwas chiefly motivated by the need to extract insight from four dimensional\nQuantum Chromodynamic (QCD) data. We develop visualization where multiple\nviews, generally views of 3D projections or slices of a higher dimensional\ndata, are tightly coupled not only by their specific order but also by a view\nsynchronizing interaction style, and an internally defined interaction\nlanguage. The tight coupling of the different views allows a fast and\nwell-coordinated exploration of the data. In particular, the visualization\nallowed us to easily make consistency checks of the 4D QCD data and to infer\nthe correctness of particle properties calculations. The software developed was\nalso successfully applied in material studies, in particular studies of\nmeteorite properties. Our implementation uses the VTK API. To handle a large\nnumber of views (slices/projections) and to still maintain good resolution, we\nuse IBM T221 display (3840 X 2400 pixels)."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0507012v1", 
    "title": "Lattice Gas Cellular Automata for Computational Fluid Animation", 
    "arxiv-id": "cs/0507012v1", 
    "author": "Paulo S. Rodrigues", 
    "publish": "2005-07-05T19:48:09Z", 
    "summary": "The past two decades showed a rapid growing of physically-based modeling of\nfluids for computer graphics applications. In this area, a common top down\napproach is to model the fluid dynamics by Navier-Stokes equations and apply a\nnumerical techniques such as Finite Differences or Finite Elements for the\nsimulation. In this paper we focus on fluid modeling through Lattice Gas\nCellular Automata (LGCA) for computer graphics applications. LGCA are discrete\nmodels based on point particles that move on a lattice, according to suitable\nand simple rules in order to mimic a fully molecular dynamics. By\nChapman-Enskog expansion, a known multiscale technique in this area, it can be\ndemonstrated that the Navier-Stokes model can be reproduced by the LGCA\ntechnique. Thus, with LGCA we get a fluid model that does not require solution\nof complicated equations. Therefore, we combine the advantage of the low\ncomputational cost of LGCA and its ability to mimic the realistic fluid\ndynamics to develop a new animating framework for computer graphics\napplications. In this work, we discuss the theoretical elements of our proposal\nand show experimental results."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0508002v1", 
    "title": "Methods for Analytical Understanding of Agent-Based Modeling of Complex   Systems", 
    "arxiv-id": "cs/0508002v1", 
    "author": "Paulo S. Rodrigues", 
    "publish": "2005-07-30T12:31:00Z", 
    "summary": "Von Neuman's work on universal machines and the hardware development have\nallowed the simulation of dynamical systems through a large set of interacting\nagents. This is a bottom-up approach which tries to derive global properties of\na complex system through local interaction rules and agent behaviour.\nTraditionally, such systems are modeled and simulated through top-down methods\nbased on differential equations. Agent-Based Modeling has the advantage of\nsimplicity and low computational cost. However, unlike differential equations,\nthere is no standard way to express agent behaviour. Besides, it is not clear\nhow to analytically predict the results obtained by the simulation. In this\npaper we survey some of these methods. For expressing agent behaviour formal\nmethods, like Stochastic Process Algebras have been used. Such approach is\nuseful if the global properties of interest can be expressed as a function of\nstochastic time series. However, if space variables must be considered, we\nshall change the focus. In this case, multiscale techniques, based on\nChapman-Enskog expansion, was used to establish the connection between the\nmicroscopic dynamics and the macroscopic observables. Also, we use data mining\ntechniques,like Principal Component Analysis (PCA), to study agent systems like\nCellular Automata. With the help of these tools we will discuss a simple\nsociety model, a Lattice Gas Automaton for fluid modeling, and knowledge\ndiscovery in CA databases. Besides, we show the capabilities of the NetLogo, a\nsoftware for agent simulation of complex system and show our experience about."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0510087v1", 
    "title": "MathPSfrag: Creating Publication-Quality Labels in Mathematica Plots", 
    "arxiv-id": "cs/0510087v1", 
    "author": "J. Grosse", 
    "publish": "2005-10-31T09:40:00Z", 
    "summary": "This article introduces a Mathematica package providing a graphics export\nfunction that automatically replaces Mathematica expressions in a graphic by\nthe corresponding LaTeX constructs and positions them correctly. It thus\nfacilitates the creation of publication-quality Enscapulated PostScript (EPS)\ngraphics."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0603132v1", 
    "title": "Graphics Turing Test", 
    "arxiv-id": "cs/0603132v1", 
    "author": "Michael McGuigan", 
    "publish": "2006-03-31T19:58:30Z", 
    "summary": "We define a Graphics Turing Test to measure graphics performance in a similar\nmanner to the definition of the traditional Turing Test. To pass the test one\nneeds to reach a computational scale, the Graphics Turing Scale, for which\nComputer Generated Imagery becomes comparatively indistinguishable from real\nimages while also being interactive. We derive an estimate for this\ncomputational scale which, although large, is within reach of todays\nsupercomputers. We consider advantages and disadvantages of various computer\nsystems designed to pass the Graphics Turing Test. Finally we discuss\ncommercial applications from the creation of such a system, in particular\nInteractive Cinema."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0607050v2", 
    "title": "Interactive Hatching and Stippling by Example", 
    "arxiv-id": "cs/0607050v2", 
    "author": "Jo\u00eblle Thollot", 
    "publish": "2006-07-11T19:01:41Z", 
    "summary": "We describe a system that lets a designer interactively draw patterns of\nstrokes in the picture plane, then guide the synthesis of similar patterns over\nnew picture regions. Synthesis is based on an initial user-assisted analysis\nphase in which the system recognizes distinct types of strokes (hatching and\nstippling) and organizes them according to perceptual grouping criteria. The\nsynthesized strokes are produced by combining properties (eg. length,\norientation, parallelism, proximity) of the stroke groups extracted from the\ninput examples. We illustrate our technique with a drawing application that\nallows the control of attributes and scale-dependent reproduction of the\nsynthesized patterns."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0609084v1", 
    "title": "Non-photorealistic image rendering with a labyrinthine tiling", 
    "arxiv-id": "cs/0609084v1", 
    "author": "B. Montrucchio", 
    "publish": "2006-09-15T07:21:11Z", 
    "summary": "The paper describes a new image processing for a non-photorealistic\nrendering. The algorithm is based on a random generation of gray tones and\ncompeting statistical requirements. The gray tone value of each pixel in the\nstarting image is replaced selecting among randomly generated tone values,\naccording to the statistics of nearest-neighbor and next-nearest-neighbor\npixels. Two competing conditions for replacing the tone values - one position\non the local mean value the other on the local variance - produce a peculiar\npattern on the image. This pattern has a labyrinthine tiling aspect. For\ncertain subjects, the pattern enhances the look of the image."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0610088v1", 
    "title": "Vector field visualization with streamlines", 
    "arxiv-id": "cs/0610088v1", 
    "author": "B. Montrucchio", 
    "publish": "2006-10-14T09:25:52Z", 
    "summary": "We have recently developed an algorithm for vector field visualization with\noriented streamlines, able to depict the flow directions everywhere in a dense\nvector field and the sense of the local orientations. The algorithm has useful\napplications in the visualization of the director field in nematic liquid\ncrystals. Here we propose an improvement of the algorithm able to enhance the\nvisualization of the local magnitude of the field. This new approach of the\nalgorithm is compared with the same procedure applied to the Line Integral\nConvolution (LIC) visualization."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/cs/0702026v1", 
    "title": "Shape preservation behavior of spline curves", 
    "arxiv-id": "cs/0702026v1", 
    "author": "Ravi Shankar Gautam", 
    "publish": "2007-02-05T10:11:58Z", 
    "summary": "Shape preservation behavior of a spline consists of criterial conditions for\npreserving convexity, inflection, collinearity, torsion and coplanarity shapes\nof data polgonal arc. We present our results which acts as an improvement in\nthe definitions of and provide geometrical insight into each of the above shape\npreservation criteria. We also investigate the effect of various results from\nthe literature on various shape preservation criteria. These results have not\nbeen earlier refered in the context of shape preservation behaviour of splines.\nWe point out that each curve segment need to satisfy more than one shape\npreservation criteria. We investigate the conflict between different shape\npreservation criteria 1)on each curve segment and 2)of adjacent curve segments.\nWe derive simplified formula for shape preservation criteria for cubic curve\nsegments. We study the shape preservation behavior of cubic Catmull-Rom splines\nand see that, though being very simple spline curve, it indeed satisfy all the\nshape preservation criteria."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0708.0712v1", 
    "title": "Virtual Environments for Training: From Individual Learning to   Collaboration with Humanoids", 
    "arxiv-id": "0708.0712v1", 
    "author": "Bruno Arnaldi", 
    "publish": "2007-08-06T07:42:56Z", 
    "summary": "The next generation of virtual environments for training is oriented towards\ncollaborative aspects. Therefore, we have decided to enhance our platform for\nvirtual training environments, adding collaboration opportunities and\nintegrating humanoids. In this paper we put forward a model of humanoid that\nsuits both virtual humans and representations of real users, according to\ncollaborative training activities. We suggest adaptations to the scenario model\nof our platform making it possible to write collaborative procedures. We\nintroduce a mechanism of action selection made up of a global repartition and\nan individual choice. These models are currently being integrated and validated\nin GVT, a virtual training tool for maintenance of military equipments,\ndeveloped in collaboration with the French company NEXTER-Group."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0712.0121v1", 
    "title": "Efficient Binary and Run Length Morphology and its Application to   Document Image Processing", 
    "arxiv-id": "0712.0121v1", 
    "author": "Thomas M. Breuel", 
    "publish": "2007-12-02T07:25:59Z", 
    "summary": "This paper describes the implementation and evaluation of an open source\nlibrary for mathematical morphology based on packed binary and run-length\ncompressed images for document imaging applications. Abstractions and patterns\nuseful in the implementation of the interval operations are described. A number\nof benchmarks and comparisons to bit-blit based implementations on standard\ndocument images are provided."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0801.1500v1", 
    "title": "Toward the Graphics Turing Scale on a Blue Gene Supercomputer", 
    "arxiv-id": "0801.1500v1", 
    "author": "Michael McGuigan", 
    "publish": "2008-01-09T20:51:02Z", 
    "summary": "We investigate raytracing performance that can be achieved on a class of Blue\nGene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144\nprocessor Blue Gene/L. We measure the computational performance as a function\nof number of processors and problem size to determine the scaling performance\nof the raytracing calculation on the Blue Gene. We find nontrivial scaling\nbehavior at large number of processors. We discuss applications of this\ntechnology to scientific visualization with advanced lighting and high\nresolution. We utilize three racks of a Blue Gene/L in our calculations which\nis less than three percent of the the capacity of the worlds largest Blue Gene\ncomputer."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0801.2175v1", 
    "title": "MathPSfrag 2: Convenient LaTeX Labels in Mathematica", 
    "arxiv-id": "0801.2175v1", 
    "author": "Johannes Gro\u00dfe", 
    "publish": "2008-01-15T18:34:44Z", 
    "summary": "This article introduces the next version of MathPSfrag. MathPSfrag is a\nMathematica package that during export automatically replaces all expressions\nin a plot by corresponding LaTeX commands. The new version can also produce\nLaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover\nfrom these files a preview is generated and shown within Mathematica."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0804.0561v1", 
    "title": "Realistic Haptic Rendering of Interacting Deformable Objects in Virtual   Environments", 
    "arxiv-id": "0804.0561v1", 
    "author": "Claude Andriot", 
    "publish": "2008-04-03T13:49:51Z", 
    "summary": "A new computer haptics algorithm to be used in general interactive\nmanipulations of deformable virtual objects is presented. In multimodal\ninteractive simulations, haptic feedback computation often comes from contact\nforces. Subsequently, the fidelity of haptic rendering depends significantly on\ncontact space modeling. Contact and friction laws between deformable models are\noften simplified in up to date methods. They do not allow a \"realistic\"\nrendering of the subtleties of contact space physical phenomena (such as slip\nand stick effects due to friction or mechanical coupling between contacts). In\nthis paper, we use Signorini's contact law and Coulomb's friction law as a\ncomputer haptics basis. Real-time performance is made possible thanks to a\nlinearization of the behavior in the contact space, formulated as the so-called\nDelassus operator, and iteratively solved by a Gauss-Seidel type algorithm.\nDynamic deformation uses corotational global formulation to obtain the Delassus\noperator in which the mass and stiffness ratio are dissociated from the\nsimulation time step. This last point is crucial to keep stable haptic\nfeedback. This global approach has been packaged, implemented, and tested.\nStable and realistic 6D haptic feedback is demonstrated through a clipping task\nexperiment."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0807.1667v1", 
    "title": "Quasi-Mandelbrot sets for perturbed complex analytic maps: visual   patterns", 
    "arxiv-id": "0807.1667v1", 
    "author": "A. V. Toporensky", 
    "publish": "2008-07-10T14:40:35Z", 
    "summary": "We consider perturbations of the complex quadratic map $ z \\to z^2 +c$ and\ncorresponding changes in their quasi-Mandelbrot sets. Depending on particular\nperturbation, visual forms of quasi-Mandelbrot set changes either sharply (when\nthe perturbation reaches some critical value) or continuously. In the latter\ncase we have a smooth transition from the classical form of the set to some\nforms, constructed from mostly linear structures, as it is typical for\ntwo-dimensional real number dynamics. Two examples of continuous evolution of\nthe quasi-Mandelbrot set are described."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0812.1647v1", 
    "title": "Polyomino-Based Digital Halftoning", 
    "arxiv-id": "0812.1647v1", 
    "author": "Victor Ostromoukhov", 
    "publish": "2008-12-09T10:12:36Z", 
    "summary": "In this work, we present a new method for generating a threshold structure.\nThis kind of structure can be advantageously used in various halftoning\nalgorithms such as clustered-dot or dispersed-dot dithering, error diffusion\nwith threshold modulation, etc. The proposed method is based on rectifiable\npolyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean\nplane with no gaps. Each polyomino contains a fixed number of discrete\nthreshold values. Thanks to its inherent non-periodic nature combined with\noff-line optimization of threshold values, our polyomino-based threshold\nstructure shows blue-noise spectral properties. The halftone images produced\nwith this threshold structure have high visual quality. Although the proposed\nmethod is general, and can be applied on any polyomino tiling, we consider one\nparticular case: tiling with G-hexominoes. We compare our polyomino-based\nthreshold structure with the best known state-of-the-art methods for generation\nthreshold matrices, and conclude considerable improvement achieved with our\nmethod."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0907.4364v2", 
    "title": "Dynamic Deformation of Uniform Elastic Two-Layer Objects", 
    "arxiv-id": "0907.4364v2", 
    "author": "Miao Song", 
    "publish": "2009-07-24T19:13:02Z", 
    "summary": "This thesis presents a two-layer uniform facet elastic object for real-time\nsimulation based on physics modeling method. It describes the elastic object\nprocedural modeling algorithm with particle system from the simplest\none-dimensional object, to more complex two-dimensional and three-dimensional\nobjects.\n  The double-layered elastic object consists of inner and outer elastic mass\nspring surfaces and compressible internal pressure. The density of the inner\nlayer can be set different from the density of the outer layer; the motion of\nthe inner layer can be opposite to the motion of the outer layer. These special\nfeatures, which cannot be achieved by a single layered object, result in\nimproved imitation of a soft body, such as tissue's liquidity non-uniform\ndeformation. The construction of the double-layered elastic object is closer to\nthe real tissue's physical structure.\n  The inertial behavior of the elastic object is well illustrated in\nenvironments with gravity and collisions with walls, ceiling, and floor. The\ncollision detection is defined by elastic collision penalty method and the\nmotion of the object is guided by the Ordinary Differential Equation\ncomputation.\n  Users can interact with the modeled objects, deform them, and observe the\nresponse to their action in real time."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0910.4854v1", 
    "title": "Yet Another Pacman 3D Adventures", 
    "arxiv-id": "0910.4854v1", 
    "author": "Yingying She", 
    "publish": "2009-10-26T10:55:16Z", 
    "summary": "This game is meant to be extension of the overly-beaten pacman-style game\n(code-named \"Yet Another Pacman 3D Adventures\", or YAP3DAD) from the proposed\nideas and other projects with advance visual and computer graphics features,\nincluding a-game-in-a-game approach. The project is an open-source project\npublished on SourceForge.net for possible future development and extension."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0911.0902v1", 
    "title": "Digital Image Watermarking for Arbitrarily Shaped Objects Based On   SA-DWT", 
    "arxiv-id": "0911.0902v1", 
    "author": "F. Fegragui", 
    "publish": "2009-11-04T18:11:30Z", 
    "summary": "Many image watermarking schemes have been proposed in recent years, but they\nusually involve embedding a watermark to the entire image without considering\nonly a particular object in the image, which the image owner may be interested\nin. This paper proposes a watermarking scheme that can embed a watermark to an\narbitrarily shaped object in an image. Before embedding, the image owner\nspecifies an object of arbitrary shape that is of a concern to him. Then the\nobject is transformed into the wavelet domain using in place lifting shape\nadaptive DWT(SADWT) and a watermark is embedded by modifying the wavelet\ncoefficients. In order to make the watermark robust and transparent, the\nwatermark is embedded in the average of wavelet blocks using the visual model\nbased on the human visual system. Wavelet coefficients n least significant bits\n(LSBs) are adjusted in concert with the average. Simulation results shows that\nthe proposed watermarking scheme is perceptually invisible and robust against\nmany attacks such as lossy compression (e.g.JPEG, JPEG2000), scaling, adding\nnoise, filtering, etc."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/0912.3923v1", 
    "title": "Secure Watermarking Scheme for Color Image Using Intensity of Pixel and   LSB Substitution", 
    "arxiv-id": "0912.3923v1", 
    "author": "B. B. Amberker", 
    "publish": "2009-12-19T18:47:39Z", 
    "summary": "In this paper a novel spatial domain LSB based watermarking scheme for color\nImages is proposed. The proposed scheme is of type blind and invisible\nwatermarking. Our scheme introduces the concept of storing variable number of\nbits in each pixel based on the actual color value of pixel. Equal or higher\nthe color value of channels with respect to intensity of pixel stores higher\nnumber of watermark bits. The Red, Green and Blue channel of the color image\nhas been used for watermark embedding. The watermark is embedded into selected\nchannels of pixel. The proposed method supports high watermark embedding\ncapacity, which is equivalent to the size of cover image. The security of\nwatermark is preserved by permuting the watermark bits using secret key. The\nproposed scheme is found robust to various image processing operations such as\nimage compression, blurring, salt and pepper noise, filtering and cropping."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1001.3481v2", 
    "title": "Resolution scalability improvement for JPEG2000 standard color image", 
    "arxiv-id": "1001.3481v2", 
    "author": "A. Arul Lawrence Selvakumar", 
    "publish": "2010-01-20T07:35:21Z", 
    "summary": "Removed by arXiv administration. This article was plagiarised from\nhttp://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other\nlocations."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1001.3496v1", 
    "title": "Spatial Domain Watermarking Scheme for Colored Images Based on   Log-average Luminance", 
    "arxiv-id": "1001.3496v1", 
    "author": "Jamal A. Hussein", 
    "publish": "2010-01-20T08:08:39Z", 
    "summary": "In this paper a new watermarking scheme is presented based on log-average\nluminance. A colored-image is divided into blocks after converting the RGB\ncolored image to YCbCr color space. A monochrome image of 1024 bytes is used as\nthe watermark. To embed the watermark, 16 blocks of size 8X8 are selected and\nused to embed the watermark image into the original image. The selected blocks\nare chosen spirally (beginning form the center of the image) among the blocks\nthat have log-average luminance higher than or equal the log-average luminance\nof the entire image. Each byte of the monochrome watermark is added by updating\na luminance value of a pixel of the image. If the byte of the watermark image\nrepresented white color (255) a value <alpha> is added to the image pixel\nluminance value, if it is black (0) the <alpha> is subtracted from the\nluminance value. To extract the watermark, the selected blocks are chosen as\nthe above, if the difference between the luminance value of the watermarked\nimage pixel and the original image pixel is greater than 0, the watermark pixel\nis supposed to be white, otherwise it supposed to be black. Experimental\nresults show that the proposed scheme is efficient against changing the\nwatermarked image to grayscale, image cropping, and JPEG compression."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1002.4006v1", 
    "title": "Text/Graphics Separation and Skew Correction of Text Regions of Business   Card Images for Mobile Devices", 
    "arxiv-id": "1002.4006v1", 
    "author": "Mita Nasipuri", 
    "publish": "2010-02-21T19:46:12Z", 
    "summary": "Separation of the text regions from background texture and graphics is an\nimportant step of any optical character recognition system for the images\ncontaining both texts and graphics. In this paper, we have presented a novel\ntext/graphics separation technique and a method for skew correction of text\nregions extracted from business card images captured with a cell-phone camera.\nAt first, the background is eliminated at a coarse level based on intensity\nvariance. This makes the foreground components distinct from each other. Then\nthe non-text components are removed using various characteristic features of\ntext and graphics. Finally, the text regions are skew corrected for further\nprocessing. Experimenting with business card images of various resolutions, we\nhave found an optimum performance of 98.25% (recall) with 0.75 MP images, that\ntakes 0.17 seconds processing time and 1.1 MB peak memory on a moderately\npowerful computer (DualCore 1.73 GHz Processor, 1 GB RAM, 1 MB L2 Cache). The\ndeveloped technique is computationally efficient and consumes low memory so as\nto be applicable on mobile devices."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1003.1401v1", 
    "title": "Macro and micro view on steady states in state space", 
    "arxiv-id": "1003.1401v1", 
    "author": "Milan Guzan", 
    "publish": "2010-03-06T16:42:53Z", 
    "summary": "This paper describes visualization of chaotic attractor and elements of the\nsingularities in 3D space. 3D view of these effects enables to create a\ndemonstrative projection about relations of chaos generated by physical\ncircuit, the Chua's circuit. Via macro views on chaotic attractor is obtained\nnot only visual space illustration of representative point motion in state\nspace, but also its relation to planes of singularity elements. Our created\nprogram enables view on chaotic attractor both in 2D and 3D space together with\nplane objects visualization -- elements of singularities."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1003.4036v1", 
    "title": "A Very Simple Approach for 3-D to 2-D Mapping", 
    "arxiv-id": "1003.4036v1", 
    "author": "Sugata Sanyal", 
    "publish": "2010-03-21T23:32:56Z", 
    "summary": "Many times we need to plot 3-D functions e.g., in many scientificc\nexperiments. To plot this 3-D functions on 2-D screen it requires some kind of\nmapping. Though OpenGL, DirectX etc 3-D rendering libraries have made this job\nvery simple, still these libraries come with many complex pre- operations that\nare simply not intended, also to integrate these libraries with any kind of\nsystem is often a tough trial. This article presents a very simple method of\nmapping from 3D to 2D, that is free from any complex pre-operation, also it\nwill work with any graphics system where we have some primitive 2-D graphics\nfunction. Also we discuss the inverse transform and how to do basic computer\ngraphics transformations using our coordinate mapping system."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1004.0766v1", 
    "title": "Text/Graphics Separation for Business Card Images for Mobile Devices", 
    "arxiv-id": "1004.0766v1", 
    "author": "Dipak Kumar Basu", 
    "publish": "2010-04-06T03:54:27Z", 
    "summary": "Separation of the text regions from background texture and graphics is an\nimportant step of any optical character recognition sytem for the images\ncontaing both texts and graphics. In this paper, we have presented a novel\ntext/graphics separation technique for business card images captured with a\ncell-phone camera. At first, the background is eliminated at a coarse level\nbased on intensity variance. This makes the foreground components distinct from\neach other. Then the non-text components are removed using various\ncharacteristic features of text and graphics. Finally, the text regions are\nskew corrected and binarized for further processing. Experimenting with\nbusiness card images of various resolutions, we have found an optimum\nperformance of 98.54% with 0.75 MP images, that takes 0.17 seconds processing\ntime and 1.1 MB peak memory on a moderately powerful computer (DualCore 1.73\nGHz Processor, 1 GB RAM, 1 MB L2 Cache). The developed technique is\ncomputationally efficient and consumes low memory so as to be applicable on\nmobile devices."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1004.4485v1", 
    "title": "Finding and Classifying Critical Points of 2D Vector Fields: A   Cell-Oriented Approach Using Group Theory", 
    "arxiv-id": "1004.4485v1", 
    "author": "Daniel Weiskopf", 
    "publish": "2010-04-26T11:26:33Z", 
    "summary": "We present a novel approach to finding critical points in cell-wise\nbarycentrically or bilinearly interpolated vector fields on surfaces. The\nPoincar\\e index of the critical points is determined by investigating the\nqualitative behavior of 0-level sets of the interpolants of the vector field\ncomponents in parameter space using precomputed combinatorial results, thus\navoiding the computation of the Jacobian of the vector field at the critical\npoints in order to determine its index. The locations of the critical points\nwithin a cell are determined analytically to achieve accurate results. This\napproach leads to a correct treatment of cases with two first-order critical\npoints or one second-order critical point of bilinearly interpolated vector\nfields within one cell, which would be missed by examining the linearized field\nonly. We show that for the considered interpolation schemes determining the\nindex of a critical point can be seen as a coloring problem of cell edges. A\ncomplete classification of all possible colorings in terms of the types and\nnumber of critical points yielded by each coloring is given using computational\ngroup theory. We present an efficient algorithm that makes use of these\nprecomputed classifications in order to find and classify critical points in a\ncell-by-cell fashion. Issues of numerical stability, construction of the\ntopological skeleton, topological simplification, and the statistics of the\ndifferent types of critical points are also discussed."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1005.3163v1", 
    "title": "Virtual Texturing", 
    "arxiv-id": "1005.3163v1", 
    "author": "Andreas Neu", 
    "publish": "2010-05-18T11:54:39Z", 
    "summary": "In this thesis a rendering system and an accompanying tool chain for Virtual\nTexturing is presented. Our tools allow to automatically retexture existing\ngeometry in order to apply unique texturing on each face. Furthermore we\ninvestigate several techniques that try to minimize visual artifacts in the\ncase that only a small amount of pages can be streamed per frame. We analyze\nthe influence of different heuristics that are responsible for the page\nselection. Alongside these results we present a measurement method to allow the\ncomparison of our heuristics."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1009.4602v1", 
    "title": "Geoglyphs of Titicaca as an ancient example of graphic design", 
    "arxiv-id": "1009.4602v1", 
    "author": "Amelia Carolina Sparavigna", 
    "publish": "2010-09-23T13:08:32Z", 
    "summary": "The paper proposes an ancient landscape design as an example of graphic\ndesign for an age and place where no written documents existed. It is created\nby a network of earthworks, which constitute the remains of an extensive\nancient agricultural system. It can be seen by means of the Google satellite\nimagery on the Peruvian region near the Titicaca Lake, as a texture\nsuperimposed to the background landform. In this texture, many drawings\n(geoglyphs) can be observed."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1101.0243v1", 
    "title": "Across Browsers SVG Implementation", 
    "arxiv-id": "1101.0243v1", 
    "author": "Dan Tsymbala", 
    "publish": "2010-12-31T12:30:34Z", 
    "summary": "In this work SVG will be translated into VML or HTML by using Javascript\nbased on Backbase Client Framework. The target of this project is to implement\nSVG to be viewed in Internet Explorer without any plug-in and work together\nwith other Backbase Client Framework languages. The result of this project will\nbe added as an extension to the current Backbase Client Framework."
},{
    "category": "cs.GR", 
    "doi": "10.1103/PhysRevA.61.042308", 
    "link": "http://arxiv.org/pdf/1101.0262v1", 
    "title": "High Speed and Area Efficient 2D DWT Processor based Image Compression\"   Signal & Image Processing", 
    "arxiv-id": "1101.0262v1", 
    "author": "Rajesh Mehra", 
    "publish": "2010-12-31T14:34:06Z", 
    "summary": "This paper presents a high speed and area efficient DWT processor based\ndesign for Image Compression applications. In this proposed design, pipelined\npartially serial architecture has been used to enhance the speed along with\noptimal utilization and resources available on target FPGA. The proposed model\nhas been designed and simulated using Simulink and System Generator blocks,\nsynthesized with Xilinx Synthesis tool (XST) and implemented on Spartan 2 and 3\nbased XC2S100-5tq144 and XC3S500E-4fg320 target device. The results show that\nproposed design can operate at maximum frequency 231 MHz in case of Spartan 3\nby consuming power of 117mW at 28 degree/c junction temperature. The result\ncomparison has shown an improvement of 15% in speed."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.imavis.2010.10.002", 
    "link": "http://arxiv.org/pdf/1101.0395v1", 
    "title": "Improving the Performance of K-Means for Color Quantization", 
    "arxiv-id": "1101.0395v1", 
    "author": "M. Emre Celebi", 
    "publish": "2011-01-02T10:09:11Z", 
    "summary": "Color quantization is an important operation with many applications in\ngraphics and image processing. Most quantization methods are essentially based\non data clustering algorithms. However, despite its popularity as a general\npurpose clustering algorithm, k-means has not received much respect in the\ncolor quantization literature because of its high computational requirements\nand sensitivity to initialization. In this paper, we investigate the\nperformance of k-means as a color quantizer. We implement fast and exact\nvariants of k-means with several initialization schemes and then compare the\nresulting quantizers to some of the most popular quantizers in the literature.\nExperiments on a diverse set of images demonstrate that an efficient\nimplementation of k-means with an appropriate initialization strategy can in\nfact serve as a very effective color quantizer."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.imavis.2010.10.002", 
    "link": "http://arxiv.org/pdf/1101.5490v1", 
    "title": "Ray-Based Reflectance Model for Diffraction", 
    "arxiv-id": "1101.5490v1", 
    "author": "Ramesh Raskar", 
    "publish": "2011-01-28T09:41:43Z", 
    "summary": "We present a novel method of simulating wave effects in graphics using\nray--based renderers with a new function: the Wave BSDF (Bidirectional\nScattering Distribution Function). Reflections from neighboring surface patches\nrepresented by local BSDFs are mutually independent. However, in many surfaces\nwith wavelength-scale microstructures, interference and diffraction requires a\njoint analysis of reflected wavefronts from neighboring patches. We demonstrate\na simple method to compute the BSDF for the entire microstructure, which can be\nused independently for each patch. This allows us to use traditional ray--based\nrendering pipelines to synthesize wave effects of light and sound. We exploit\nthe Wigner Distribution Function (WDF) to create transmissive, reflective, and\nemissive BSDFs for various diffraction phenomena in a physically accurate way.\nIn contrast to previous methods for computing interference, we circumvent the\nneed to explicitly keep track of the phase of the wave by using BSDFs that\ninclude positive as well as negative coefficients. We describe and compare the\ntheory in relation to well understood concepts in rendering and demonstrate a\nstraightforward implementation. In conjunction with standard raytracers, such\nas PBRT, we demonstrate wave effects for a range of scenarios such as\nmulti--bounce diffraction materials, holograms and reflection of high frequency\nsurfaces."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.imavis.2010.10.002", 
    "link": "http://arxiv.org/pdf/1102.0634v1", 
    "title": "Glioblastoma Multiforme Segmentation in MRI Data with a Balloon   Inflation Approach", 
    "arxiv-id": "1102.0634v1", 
    "author": "Christopher Nimsky", 
    "publish": "2011-02-03T10:00:15Z", 
    "summary": "Gliomas are the most common primary brain tumors, evolving from the cerebral\nsupportive cells. For clinical follow-up, the evaluation of the preoperative\ntumor volume is essential. Volumetric assessment of tumor volume with manual\nsegmentation of its outlines is a time-consuming process that can be overcome\nwith the help of computer-assisted segmentation methods. In this paper, a\nsemi-automatic approach for World Health Organization (WHO) grade IV glioma\nsegmentation is introduced that uses balloon inflation forces, and relies on\nthe detection of high-intensity tumor boundaries that are coupled by using\ncontrast agent gadolinium. The presented method is evaluated on 27 magnetic\nresonance imaging (MRI) data sets and the ground truth data of the tumor\nboundaries - for evaluation of the results - are manually extracted by\nneurosurgeons."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.imavis.2010.10.002", 
    "link": "http://arxiv.org/pdf/1102.4992v1", 
    "title": "Mathematics of Human Motion: from Animation towards Simulation (A View   form the Outside)", 
    "arxiv-id": "1102.4992v1", 
    "author": "A. I. Zhmakin", 
    "publish": "2011-02-24T13:54:29Z", 
    "summary": "Simulation of human motion is the subject of study in a number of\ndisciplines: Biomechanics, Robotics, Computer Animation, Control Theory,\nNeurophysiology, Medicine, Ergonomics. Since the author has never visited any\nof these fields, this review is indeed a passer-by's impression. On the other\nhand, he happens to be a human (who occasionally is moving) and, as everybody\nelse, rates himself an expert in Applied Common Sense. Thus the author hopes\nthat this view from the {\\em outside} will be of some interest not only for the\nstrangers like himself, but for those who are {\\em inside} as well.\n  Two flaws of the text that follows are inevitable. First, some essential\nissues that are too familar to the specialists to discuss them may be missing.\nSecond, the author probably failed to provide the uniform \"level-of-detail\" for\nthis wide range of topics."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1107.3013v1", 
    "title": "Linear-Time Poisson-Disk Patterns", 
    "arxiv-id": "1107.3013v1", 
    "author": "David R. Karger", 
    "publish": "2011-07-15T08:58:09Z", 
    "summary": "We present an algorithm for generating Poisson-disc patterns taking O(N) time\nto generate $N$ points. The method is based on a grid of regions which can\ncontain no more than one point in the final pattern, and uses an explicit model\nof point arrival times under a uniform Poisson process."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1112.6032v1", 
    "title": "A self-rendering digital image encoding", 
    "arxiv-id": "1112.6032v1", 
    "author": "Daniel L. Ruderman", 
    "publish": "2011-12-27T23:40:12Z", 
    "summary": "Without careful long-term preservation digital data may be lost to a number\nof factors, including physical media decay, lack of suitable decoding\nequipment, and the absence of software. When raw data can be read but lack\nsuitable annotations as to provenance, the ability to interpret them is more\nstraightforward if they can be assessed through simple visual techniques. In\nthis regard digital images are a special case since their data have a natural\nrepresentation on two-dimensional media surfaces. This paper presents a novel\nbinary image pixel encoding that produces an approximate analog rendering of\nencoded images when the image bits are arranged spatially in an appropriate\nmanner. This simultaneous digital and analog representation acts to inseparably\nannotate bits as image data, which may contribute to the longevity of\nso-encoded images."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1201.0070v1", 
    "title": "Fast B-spline Curve Fitting by L-BFGS", 
    "arxiv-id": "1201.0070v1", 
    "author": "Wenping Wang", 
    "publish": "2011-12-30T06:23:54Z", 
    "summary": "We propose a novel method for fitting planar B-spline curves to unorganized\ndata points. In traditional methods, optimization of control points and foot\npoints are performed in two very time-consuming steps in each iteration: 1)\ncontrol points are updated by setting up and solving a linear system of\nequations; and 2) foot points are computed by projecting each data point onto a\nB-spline curve. Our method uses the L-BFGS optimization method to optimize\ncontrol points and foot points simultaneously and therefore it does not need to\nperform either matrix computation or foot point projection in every iteration.\nAs a result, our method is much faster than existing methods."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1202.5360v1", 
    "title": "Efficient and Effective Volume Visualization with Enhanced Isosurface   Rendering", 
    "arxiv-id": "1202.5360v1", 
    "author": "Jie Tian", 
    "publish": "2012-02-24T01:39:12Z", 
    "summary": "Compared with full volume rendering, isosurface rendering has several well\nrecognized advantages in efficiency and accuracy. However, standard isosurface\nrendering has some limitations in effectiveness. First, it uses a monotone\ncolored approach and can only visualize the geometry features of an isosurface.\nThe lack of the capability to illustrate the material property and the internal\nstructures behind an isosurface has been a big limitation of this method in\napplications. Another limitation of isosurface rendering is the difficulty to\nreveal physically meaningful structures, which are hidden in one or multiple\nisosurfaces. As such, the application requirements of extract and recombine\nstructures of interest can not be implemented effectively with isosurface\nrendering. In this work, we develop an enhanced isosurface rendering technique\nto improve the effectiveness while maintaining the performance efficiency of\nthe standard isosurface rendering. First, an isosurface color enhancement\nmethod is proposed to illustrate the neighborhood density and to reveal some of\nthe internal structures. Second, we extend the structure extraction capability\nof isosurface rendering by enabling explicit scene exploration within a\n3D-view, using surface peeling, voxel-selecting, isosurface segmentation, and\nmulti-surface-structure visualization. Our experiments show that the color\nenhancement not only improves the visual fidelity of the rendering, but also\nreveals the internal structures without significant increase of the\ncomputational cost. Explicit scene exploration is also demonstrated as a\npowerful tool in some application scenarios, such as displaying multiple\nabdominal organs."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1205.5204v1", 
    "title": "Visualizing 2D Flows with Animated Arrow Plots", 
    "arxiv-id": "1205.5204v1", 
    "author": "Dmitry Sokolov", 
    "publish": "2012-05-23T15:29:16Z", 
    "summary": "Flow fields are often represented by a set of static arrows to illustrate\nscientific vulgarization, documentary film, meteorology, etc. This simple\nschematic representation lets an observer intuitively interpret the main\nproperties of a flow: its orientation and velocity magnitude. We propose to\ngenerate dynamic versions of such representations for 2D unsteady flow fields.\nOur algorithm smoothly animates arrows along the flow while controlling their\ndensity in the domain over time. Several strategies have been combined to lower\nthe unavoidable popping artifacts arising when arrows appear and disappear and\nto achieve visually pleasing animations. Disturbing arrow rotations in low\nvelocity regions are also handled by continuously morphing arrow glyphs to\nsemi-transparent discs. To substantiate our method, we provide results for\nsynthetic and real velocity field datasets."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1208.1983v1", 
    "title": "An algorithm for improving the quality of compacted JPEG image by   minimizes the blocking artifacts", 
    "arxiv-id": "1208.1983v1", 
    "author": "Sukhpal Singh", 
    "publish": "2012-08-09T17:47:07Z", 
    "summary": "The Block Transform Coded, JPEG- a lossy image compression format has been\nused to keep storage and bandwidth requirements of digital image at practical\nlevels. However, JPEG compression schemes may exhibit unwanted image artifacts\nto appear - such as the 'blocky' artifact found in smooth/monotone areas of an\nimage, caused by the coarse quantization of DCT coefficients. A number of image\nfiltering approaches have been analyzed in literature incorporating\nvalue-averaging filters in order to smooth out the discontinuities that appear\nacross DCT block boundaries. Although some of these approaches are able to\ndecrease the severity of these unwanted artifacts to some extent, other\napproaches have certain limitations that cause excessive blurring to\nhigh-contrast edges in the image. The image deblocking algorithm presented in\nthis paper aims to filter the blocked boundaries. This is accomplished by\nemploying smoothening, detection of blocked edges and then filtering the\ndifference between the pixels containing the blocked edge. The deblocking\nalgorithm presented has been successful in reducing blocky artifacts in an\nimage and therefore increases the subjective as well as objective quality of\nthe reconstructed image."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1211.1768v1", 
    "title": "Nearest Neighbor Value Interpolation", 
    "arxiv-id": "1211.1768v1", 
    "author": "Cao Hanqiang", 
    "publish": "2012-11-08T06:50:44Z", 
    "summary": "This paper presents the nearest neighbor value (NNV) algorithm for high\nresolution (H.R.) image interpolation. The difference between the proposed\nalgorithm and conventional nearest neighbor algorithm is that the concept\napplied, to estimate the missing pixel value, is guided by the nearest value\nrather than the distance. In other words, the proposed concept selects one\npixel, among four directly surrounding the empty location, whose value is\nalmost equal to the value generated by the conventional bilinear interpolation\nalgorithm. The proposed method demonstrated higher performances in terms of\nH.R. when compared to the conventional interpolation algorithms mentioned."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1211.3297v2", 
    "title": "Gap Processing for Adaptive Maximal Poisson-Disk Sampling", 
    "arxiv-id": "1211.3297v2", 
    "author": "Peter Wonka", 
    "publish": "2012-11-14T13:21:03Z", 
    "summary": "In this paper, we study the generation of maximal Poisson-disk sets with\nvarying radii. First, we present a geometric analysis of gaps in such disk\nsets. This analysis is the basis for maximal and adaptive sampling in Euclidean\nspace and on manifolds. Second, we propose efficient algorithms and data\nstructures to detect gaps and update gaps when disks are inserted, deleted,\nmoved, or have their radius changed. We build on the concepts of the regular\ntriangulation and the power diagram. Third, we will show how our analysis can\nmake a contribution to the state-of-the-art in surface remeshing."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1211.3659v1", 
    "title": "Color scales that are effective in both color and grayscale", 
    "arxiv-id": "1211.3659v1", 
    "author": "Silas Alben", 
    "publish": "2012-11-15T17:08:58Z", 
    "summary": "We consider the problem of finding a color scale which performs well when\nconverted to a grayscale. We assume that each color is converted to a shade of\ngray with the same intensity as the color. We also assume that the color scales\nhave a linear variation of intensity and hue, and find scales which maximize\nthe average chroma (or \"colorfulness\") of the colors. We find two classes of\nsolutions, which traverse the color wheel in opposite directions. The two\nclasses of scales start with hues near cyan and red. The average chroma of the\nscales are 65-77% those of the pure colors."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1211.5669v1", 
    "title": "Analysis-suitable T-splines: characterization, refineability, and   approximation", 
    "arxiv-id": "1211.5669v1", 
    "author": "M. A. Scott", 
    "publish": "2012-11-24T12:56:58Z", 
    "summary": "We establish several fundamental properties of analysis-suitable T-splines\nwhich are important for design and analysis. First, we characterize T-spline\nspaces and prove that the space of smooth bicubic polynomials, defined over the\nextended T-mesh of an analysis-suitable T-spline, is contained in the\ncorresponding analysis-suitable T-spline space. This is accomplished through\nthe theory of perturbed analysis-suitable T-spline spaces and a simple\ntopological dimension formula. Second, we establish the theory of\nanalysis-suitable local refinement and describe the conditions under which two\nanalysis-suitable T-spline spaces are nested. Last, we demonstrate that these\nresults can be used to establish basic approximation results which are critical\nfor analysis."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2011.617173", 
    "link": "http://arxiv.org/pdf/1211.5842v1", 
    "title": "A Novel Algorithm for Real-time Procedural Generation of Building Floor   Plans", 
    "arxiv-id": "1211.5842v1", 
    "author": "Abdallah Shami", 
    "publish": "2012-11-26T02:13:52Z", 
    "summary": "Real-time generation of natural-looking floor plans is vital in games with\ndynamic environments. This paper presents an algorithm to generate suburban\nhouse floor plans in real-time. The algorithm is based on the work presented in\n[1]. However, the corridor placement is redesigned to produce floor plans\nsimilar to real houses. Moreover, an optimization stage is added to find a\ncorridor placement with the minimum used space, an approach that is designed to\nmimic the real-life practices to minimize the wasted spaces in the design. The\nresults show very similar floor plans to the ones designed by an architect."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1302.3917v1", 
    "title": "k-d Darts: Sampling by k-Dimensional Flat Searches", 
    "arxiv-id": "1302.3917v1", 
    "author": "John D. Owens", 
    "publish": "2013-02-16T01:14:33Z", 
    "summary": "We formalize the notion of sampling a function using k-d darts. A k-d dart is\na set of independent, mutually orthogonal, k-dimensional subspaces called k-d\nflats. Each dart has d choose k flats, aligned with the coordinate axes for\nefficiency. We show that k-d darts are useful for exploring a function's\nproperties, such as estimating its integral, or finding an exemplar above a\nthreshold. We describe a recipe for converting an algorithm from point sampling\nto k-d dart sampling, assuming the function can be evaluated along a k-d flat.\n  We demonstrate that k-d darts are more efficient than point-wise samples in\nhigh dimensions, depending on the characteristics of the sampling domain: e.g.\nthe subregion of interest has small volume and evaluating the function along a\nflat is not too expensive. We present three concrete applications using line\ndarts (1-d darts): relaxed maximal Poisson-disk sampling, high-quality\nrasterization of depth-of-field blur, and estimation of the probability of\nfailure from a response surface for uncertainty quantification. In these\napplications, line darts achieve the same fidelity output as point darts in\nless time. We also demonstrate the accuracy of higher dimensional darts for a\nvolume estimation problem. For Poisson-disk sampling, we use significantly less\nmemory, enabling the generation of larger point clouds in higher dimensions."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1303.4110v1", 
    "title": "On Linear Spaces of Polyhedral Meshes", 
    "arxiv-id": "1303.4110v1", 
    "author": "Craig Gotsman", 
    "publish": "2013-03-17T22:00:15Z", 
    "summary": "Polyhedral meshes (PM) - meshes having planar faces - have enjoyed a rise in\npopularity in recent years due to their importance in architectural and\nindustrial design. However, they are also notoriously difficult to generate and\nmanipulate. Previous methods start with a smooth surface and then apply\nelaborate meshing schemes to create polyhedral meshes approximating the\nsurface. In this paper, we describe a reverse approach: given the topology of a\nmesh, we explore the space of possible planar meshes with that topology.\n  Our approach is based on a complete characterization of the maximal linear\nspaces of polyhedral meshes contained in the curved manifold of polyhedral\nmeshes with a given topology. We show that these linear spaces can be described\nas nullspaces of differential operators, much like harmonic functions are\nnullspaces of the Laplacian operator. An analysis of this operator provides\ntools for global and local design of a polyhedral mesh, which fully expose the\ngeometric possibilities and limitations of the given topology."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.0375v1", 
    "title": "A New 3D Geometric Approach to Focus and Context Lens Effect Simulation", 
    "arxiv-id": "1308.0375v1", 
    "author": "Xin Zhao", 
    "publish": "2013-08-01T23:08:03Z", 
    "summary": "We present a novel methodology based on geometric approach to simulate\nmagnification lens effects. Our aim is to promote new applications of powerful\ngeometric modeling techniques in visual computing. Conventional image\nprocessing/visualization methods are computed in two dimensional space (2D). We\nexamine this conventional 2D manipulation from a completely innovative\nperspective of 3D geometric processing. Compared with conventional optical lens\ndesign, 3D geometric method are much more capable of preserving shape features\nand minimizing distortion. We magnify an area of interest to better visualize\nthe interior details, while keeping the rest of area without perceivable\ndistortion. We flatten the mesh back into 2D space for viewing, and further\napplications in the screen space. In both steps, we devise an iterative\ndeformation scheme to minimize distortion around both focus and context region,\nwhile avoiding the noncontinuous transition region between the focus and\ncontext areas. Particularly, our method allows the user to flexibly modify the\nROI shapes to accommodate complex feature. The user can also easily specify a\nspectrum of metrics for different visual effects. Various experimental results\ndemonstrate the effectiveness, robustness, and efficiency of our framework."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.0419v2", 
    "title": "Inverse Procedural Modeling of Facade Layouts", 
    "arxiv-id": "1308.0419v2", 
    "author": "Peter Wonka", 
    "publish": "2013-08-02T07:10:47Z", 
    "summary": "In this paper, we address the following research problem: How can we generate\na meaningful split grammar that explains a given facade layout? To evaluate if\na grammar is meaningful, we propose a cost function based on the description\nlength and minimize this cost using an approximate dynamic programming\nframework. Our evaluation indicates that our framework extracts meaningful\nsplit grammars that are competitive with those of expert users, while some\nusers and all competing automatic solutions are less successful."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.0867v1", 
    "title": "A Survey of Spline-based Volumetric Data Modeling Framework and Its   Applications", 
    "arxiv-id": "1308.0867v1", 
    "author": "Bo Li", 
    "publish": "2013-08-05T01:27:36Z", 
    "summary": "The rapid advances in 3D scanning and acquisition techniques have given rise\nto the explosive increase of volumetric digital models in recent years. This\ndissertation systematically trailblazes a novel volumetric modeling framework\nto represent 3D solids. The need to explore more efficient and robust 3D\nmodeling framework has gained the prominence. Although the traditional surface\nrepresentation (e.g., triangle mesh) has many attractive properties, it is\nincapable of expressing the interior space and materials. Such a serious\ndrawback overshadows many potential modeling and analysis applications.\nConsequently volumetric modeling techniques become the well-known solution to\nthis problem. Nevertheless, many unsolved research issues remain when\ndeveloping an efficient modeling paradigm for existing 3D models: complex\ngeometry (fine details and extreme concaveness), arbitrary topology,\nheterogenous materials, large-scale data storage and processing, etc."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.0869v1", 
    "title": "A Spline-based Volumetric Data Modeling Framework and Its Applications", 
    "arxiv-id": "1308.0869v1", 
    "author": "Bo Li", 
    "publish": "2013-08-05T01:56:59Z", 
    "summary": "In this dissertation, we concentrate on the challenging research issue of\ndeveloping a spline-based modeling framework, which converts the conventional\ndata (e.g., surface meshes) to tensor-product trivariate splines. This\nmethodology can represent both boundary/volumetric geometry and real volumetric\nphysical attributes in a compact and continuous fashion. The regular\ntensor-product structure enables our new developed methods to be embedded into\nthe industry standard seamlessly. These properties make our techniques highly\npreferable in many physically-based applications including mechanical analysis,\nshape deformation and editing, virtual surgery training, etc."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.1279v9", 
    "title": "Barycentric Coordinates as Interpolants", 
    "arxiv-id": "1308.1279v9", 
    "author": "Russell A. Brown", 
    "publish": "2013-08-06T14:15:42Z", 
    "summary": "Barycentric coordinates are frequently used as interpolants to shade computer\ngraphics images. A simple equation transforms barycentric coordinates from\nscreen space into eye space in order to undo the perspective transformation and\npermit accurate interpolative shading of texture maps. This technique is\namenable to computation using a block-normalized integer representation."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.3917v1", 
    "title": "Medial Meshes for Volume Approximation", 
    "arxiv-id": "1308.3917v1", 
    "author": "Wenping Wang", 
    "publish": "2013-08-19T03:34:21Z", 
    "summary": "Volume approximation is an important problem found in many applications of\ncomputer graphics, vision, and image processing. The problem is about computing\nan accurate and compact approximate representation of 3D volumes using some\nsimple primitives. In this study, we propose a new volume representation,\ncalled medial meshes, and present an efficient method for its computation.\nSpecifically, we use the union of a novel type of simple volume primitives,\nwhich are spheres and the convex hulls of two or three spheres, to approximate\na given 3D shape. We compute such a volume approximation based on a new method\nfor medial axis simplification guided by Hausdorff errors. We further\ndemonstrate the superior efficiency and accuracy of our method over existing\nmethods for medial axis simplification."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.5843v1", 
    "title": "Affordable Virtual Reality System Architecture for Representation of   Implicit Object Properties", 
    "arxiv-id": "1308.5843v1", 
    "author": "Dimo Chotrov", 
    "publish": "2013-08-27T12:44:54Z", 
    "summary": "A flexible, scalable and affordable virtual reality software system\narchitecture is proposed. This solution can be easily implemented on different\nhardware configurations: on a single computer or on a computer cluster. The\narchitecture is aimed to be integrated in the workflow for solving engineering\ntasks and oriented towards presenting implicit object properties through\nmultiple sensorial channels (visual, audio and haptic). Implicit properties\nrepresent hidden object features (i.e. magnetization, radiation, humidity,\ntoxicity, etc.) which cannot be perceived by the observer through his or her\nsenses but require specialized equipment in order to expand the sensory ability\nof the observer. Our approach extends the underlying general scene graph\nstructure incorporating additional effects nodes for implicit properties\nrepresentation."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1308.5847v1", 
    "title": "Post-processing of Engineering Analysis Results for Visualization in VR   Systems", 
    "arxiv-id": "1308.5847v1", 
    "author": "Dimo Chotrov", 
    "publish": "2013-08-27T12:47:34Z", 
    "summary": "The applicability of Virtual Reality for evaluating engineering analysis\nresults is beginning to receive increased appreciation in the last years. The\nproblem many engineers are still facing is how to import their model together\nwith the analysis results in a virtual reality environment for exploration and\nresults validation. In this paper we propose an algorithm for transforming\nmodel data and results from finite element analysis (FEA) solving application\nto a format easily interpretable by a virtual reality application. The\nalgorithm includes also steps for reducing the face-count of the resulting mesh\nby eliminating faces from the inner part of the model in the cases when only\nthe surfaces of the model is analyzed. We also describe a possibility for\nsimultaneously assessing multiple analysis results relying on multimodal\nresults presentation by stimulating different senses of the operator."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1309.1917v1", 
    "title": "Zahir: a Object-Oriented Framework for Computer Graphics", 
    "arxiv-id": "1309.1917v1", 
    "author": "Mar\u00eda Cecilia Rivara", 
    "publish": "2013-09-07T23:21:31Z", 
    "summary": "In this article we present Zahir, a framework for experimentation in Computer\nGraphics that provides a group of object-oriented base components that take\ncare of common tasks in rendering techniques and algorithms, specially those of\nNon Photo-realistic Rendering (NPR). These components allow developers to\nimplement rendering techniques and algorithms over static and animated meshes.\nCurrently, Zahir is being used in a Master's Thesis and as support material in\nthe undergraduate Computer Graphics course in University of Chile."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1309.7472v1", 
    "title": "Detection and Characterization of Intrinsic Symmetry", 
    "arxiv-id": "1309.7472v1", 
    "author": "Fatih Porikli", 
    "publish": "2013-09-28T16:11:47Z", 
    "summary": "A comprehensive framework for detection and characterization of overlapping\nintrinsic symmetry over 3D shapes is proposed. To identify prominent symmetric\nregions which overlap in space and vary in form, the proposed framework is\ndecoupled into a Correspondence Space Voting procedure followed by a\nTransformation Space Mapping procedure. In the correspondence space voting\nprocedure, significant symmetries are first detected by identifying surface\npoint pairs on the input shape that exhibit local similarity in terms of their\nintrinsic geometry while simultaneously maintaining an intrinsic distance\nstructure at a global level. Since different point pairs can share a common\npoint, the detected symmetric shape regions can potentially overlap. To this\nend, a global intrinsic distance-based voting technique is employed to ensure\nthe inclusion of only those point pairs that exhibit significant symmetry. In\nthe transformation space mapping procedure, the Functional Map framework is\nemployed to generate the final map of symmetries between point pairs. The\ntransformation space mapping procedure ensures the retrieval of the underlying\ndense correspondence map throughout the 3D shape that follows a particular\nsymmetry. Additionally, the formulation of a novel cost matrix enables the\ninner product to succesfully indicate the complexity of the underlying symmetry\ntransformation. The proposed transformation space mapping procedure is shown to\nresult in the formulation of a semi-metric symmetry space where each point in\nthe space represents a specific symmetry transformation and the distance\nbetween points represents the complexity between the corresponding\ntransformations. Experimental results show that the proposed framework can\nsuccessfully process complex 3D shapes that possess rich symmetries."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.0955v2", 
    "title": "A Dual-Beam Method-of-Images 3D Searchlight BSSRDF", 
    "arxiv-id": "1311.0955v2", 
    "author": "Eugene d'Eon", 
    "publish": "2013-11-05T03:25:38Z", 
    "summary": "We present a novel BSSRDF for rendering translucent materials. Angular\neffects lacking in previous BSSRDF models are incorporated by using a dual-beam\nformulation. We employ a Placzek's Lemma interpretation of the method of images\nand discard diffusion theory. Instead, we derive a plane-parallel\ntransformation of the BSSRDF to form the associated BRDF and optimize the image\nconfiurations such that the BRDF is close to the known analytic solutions for\nthe associated albedo problem. This ensures reciprocity, accurate colors, and\nprovides an automatic level-of-detail transition for translucent objects that\nappear at various distances in an image. Despite optimizing the subsurface\nfluence in a plane-parallel setting, we find that this also leads to fairly\naccurate fluence distributions throughout the volume in the original 3D\nsearchlight problem. Our method-of-images modifications can also improve the\naccuracy of previous BSSRDFs."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.5018v1", 
    "title": "On the impact of explicit or semi-implicit integration methods over the   stability of real-time numerical simulations", 
    "arxiv-id": "1311.5018v1", 
    "author": "Horea Caramizaru", 
    "publish": "2013-11-20T11:30:03Z", 
    "summary": "Physics-based animation of soft or rigid bodies for real-time applications\noften suffers from numerical instabilities. We analyse one of the most common\nsources of unwanted behaviour: the numerical integration strategy. To assess\nthe impact of popular integration methods, we consider a scenario where soft\nand hard constraints are added to a custom designed deformable linear object.\nSince the goal for this class of simulation methods is to attain interactive\nframe-rates, we present the drawbacks of using explicit integration methods\nover inherently stable, implicit integrators. To help numerical solver\ndesigners better understand the impact of an integrator on a certain simulated\nworld, we have conceived a method of benchmarking the efficiency of an\nintegrator with respect to its speed, stability and symplecticity."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.6811v1", 
    "title": "Digitize Your Body and Action in 3-D at Over 10 FPS: Real Time Dense   Voxel Reconstruction and Marker-less Motion Tracking via GPU Acceleration", 
    "arxiv-id": "1311.6811v1", 
    "author": "Yuncai Liu", 
    "publish": "2013-11-26T18:41:48Z", 
    "summary": "In this paper, we present an approach to reconstruct 3-D human motion from\nmulti-cameras and track human skeleton using the reconstructed human 3-D point\n(voxel) cloud. We use an improved and more robust algorithm, probabilistic\nshape from silhouette to reconstruct human voxel. In addition, the annealed\nparticle filter is applied for tracking, where the measurement is computed\nusing the reprojection of reconstructed voxel. We use two different ways to\naccelerate the approach. For the CPU only acceleration, we leverage Intel TBB\nto speed up the hot spot of the computational overhead and reached an\naccelerating ratio of 3.5 on a 4-core CPU. Moreover, we implement an\nintensively paralleled version via GPU acceleration without TBB. Taking account\nall data transfer and computing time, the GPU version is about 400 times faster\nthan the original CPU implementation, leading the approach to run at a\nreal-time speed."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.7430v1", 
    "title": "A local Gaussian filter and adaptive morphology as tools for completing   partially discontinuous curves", 
    "arxiv-id": "1311.7430v1", 
    "author": "E. Zaj\u0105c", 
    "publish": "2013-11-28T21:45:10Z", 
    "summary": "This paper presents a method for extraction and analysis of curve--type\nstructures which consist of disconnected components. Such structures are found\nin electron--microscopy (EM) images of metal nanograins, which are widely used\nin the field of nanosensor technology.\n  The topography of metal nanograins in compound nanomaterials is crucial to\nnanosensor characteristics. The method of completing such templates consists of\nthree steps. In the first step, a local Gaussian filter is used with different\nweights for each neighborhood. In the second step, an adaptive morphology\noperation is applied to detect the endpoints of curve segments and connect\nthem. In the last step, pruning is employed to extract a curve which optimally\nfits the template."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.7462v1", 
    "title": "Continuous Collision Detection for Composite Quadric Models", 
    "arxiv-id": "1311.7462v1", 
    "author": "Feng Sun", 
    "publish": "2013-11-29T03:08:37Z", 
    "summary": "A composite quadric model (CQM) is an object modeled by piecewise linear or\nquadric patches. We study the continuous detection problem of a special type of\nCQM objects which are commonly used in CAD/CAM, that is, the boundary surfaces\nof such a CQM intersect only in straight line segments or conic curve segments.\nWe present a framework for continuous collision detection (CCD) of this special\ntype of CQM (which we also call CQM for brevity) in motion. We derive algebraic\nformulations and compute numerically the first contact time instants and the\ncontact points of two moving CQMs in $\\mathbb R^3$. Since it is difficult to\nprocess CCD of two CQMs in a direct manner because they are composed of\nsemi-algebraic varieties, we break down the problem into subproblems of solving\nCCD of pairs of boundary elements of the CQMs. We present procedures to solve\nCCD of different types of boundary element pairs in different dimensions. Some\nCCD problems are reduced to their equivalents in a lower dimensional setting,\nwhere they can be solved more efficiently."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1311.7535v1", 
    "title": "Compact Part-Based Shape Spaces for Dense Correspondences", 
    "arxiv-id": "1311.7535v1", 
    "author": "Reinhard Klein", 
    "publish": "2013-11-29T12:03:00Z", 
    "summary": "We consider the problem of establishing dense correspondences within a set of\nrelated shapes of strongly varying geometry. For such input, traditional shape\nmatching approaches often produce unsatisfactory results. We propose an\nensemble optimization method that improves given coarse correspondences to\nobtain dense correspondences. Following ideas from minimum description length\napproaches, it maximizes the compactness of the induced shape space to obtain\nhigh-quality correspondences. We make a number of improvements that are\nimportant for computer graphics applications: Our approach handles meshes of\ngeneral topology and handles partial matching between input of varying\ntopology. To this end we introduce a novel part-based generative statistical\nshape model. We develop a novel analysis algorithm that learns such models from\ntraining shapes of varying topology. We also provide a novel synthesis method\nthat can generate new instances with varying part layouts and subject to\ngeneric variational constraints. In practical experiments, we obtain a\nsubstantial improvement in correspondence quality over state-of-the-art\nmethods. As example application, we demonstrate a system that learns shape\nfamilies as assemblies of deformable parts and permits real-time editing with\ncontinuous and discrete variability."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1402.5440v1", 
    "title": "Ergonomic-driven Geometric Exploration and Reshaping", 
    "arxiv-id": "1402.5440v1", 
    "author": "Niloy Mitra", 
    "publish": "2014-02-21T22:31:26Z", 
    "summary": "The paper addresses the following problem: given a set of man-made shapes,\ne.g., chairs, can we quickly rank and explore the set of shapes with respect to\na given avatar pose? Answering this question requires identifying which shapes\nare more suitable for the defined avatar and pose; and moreover, to provide\nfast preview of how to alter the input geometry to better fit the deformed\nshapes to the given avatar pose? The problem naturally links physical\nproportions of human body and its interaction with object shapes in an attempt\nto connect ergonomics with shape geometry. We designed an interaction system\nthat allows users to explore shape collections using the deformation of human\ncharacters while at the same time providing interactive previews of how to\nalter the shapes to better fit the user-specified character. We achieve this by\nfirst mapping ergonomics guidelines into a set of simultaneous multi-part\nconstraints based on target contacts; and then, proposing a novel contact-based\ndeformation model to realize multi-contact constraints. We evaluate our\nframework on various chair models and validate the results via a small user\nstudy."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2522528", 
    "link": "http://arxiv.org/pdf/1404.0981v1", 
    "title": "New Julia and Mandelbrot Sets for Jungck Ishikawa Iterates", 
    "arxiv-id": "1404.0981v1", 
    "author": "Dr. Ashish Negi", 
    "publish": "2014-04-03T15:48:55Z", 
    "summary": "The generation of fractals and study of the dynamics of polynomials is one of\nthe emerging and interesting field of research nowadays. We introduce in this\npaper the dynamics of polynomials z^ n - z + c = 0 for n>=2 and applied Jungck\nIshikawa Iteration to generate new Relative Superior Mandelbrot sets and\nRelative Superior Julia sets. In order to solve this function by Jungck type\niterative schemes, we write it in the form of Sz = Tz, where the function T, S\nare defined as Tz = z^ n + c and Sz = z. Only mathematical explanations are\nderived by applying Jungck Ishikawa Iteration for polynomials in the literature\nbut in this paper we have generated Relative Mandelbrot sets and Relative Julia\nsets."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICICM.2013.25", 
    "link": "http://arxiv.org/pdf/1404.2053v1", 
    "title": "Expression driven Trignometric based Procedural Animation of Quadrupeds", 
    "arxiv-id": "1404.2053v1", 
    "author": "Waheed Mahesar", 
    "publish": "2014-04-08T09:23:05Z", 
    "summary": "This research paper addresses the problem of generating involuntary and\nprecise animation of quadrupeds with automatic rigging system of various\ncharacter types. The technique proposed through this research is based on a two\ntier animation control curve with base simulation being driven through dynamic\nmathematical model using procedural algorithm and the top layer with a custom\nuser controlled animation provided with intuitive Graphical User Interface\n(GUI). The character rig is based on forward and inverse kinematics driven\nthrough trigonometric based motion equations. The User is provided with various\nmanipulators and attributes to control and handle the locomotion gaits of the\ncharacters and choose between various types of simulated motions from walking,\nrunning, trotting, ambling and galloping with complete custom controls to\neasily extend the base simulation as per requirements."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1404.3363v3", 
    "title": "Interactive Isogeometric Volume Visualization with Pixel-Accurate   Geometry", 
    "arxiv-id": "1404.3363v3", 
    "author": "Jon M. Hjelmervik", 
    "publish": "2014-04-13T09:52:15Z", 
    "summary": "A recent development, called isogeometric analysis, provides a unified\napproach for design, analysis and optimization of functional products in\nindustry. Traditional volume rendering methods for inspecting the results from\nthe numerical simulations cannot be applied directly to isogeometric models. We\npresent a novel approach for interactive visualization of isogeometric analysis\nresults, ensuring correct, i.e., pixel-accurate geometry of the volume\nincluding its bounding surfaces. The entire OpenGL pipeline is used in a\nmulti-stage algorithm leveraging techniques from surface rendering,\norder-independent transparency, as well as theory and numerical methods for\nordinary differential equations. We showcase the efficiency of our approach on\ndifferent models relevant to industry, ranging from quality inspection of the\nparametrization of the geometry, to stress analysis in linear elasticity, to\nvisualization of computational fluid dynamics results."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1404.6293v2", 
    "title": "Piko: A Design Framework for Programmable Graphics Pipelines", 
    "arxiv-id": "1404.6293v2", 
    "author": "John D. Owens", 
    "publish": "2014-04-25T00:04:20Z", 
    "summary": "We present Piko, a framework for designing, optimizing, and retargeting\nimplementations of graphics pipelines on multiple architectures. Piko\nprogrammers express a graphics pipeline by organizing the computation within\neach stage into spatial bins and specifying a scheduling preference for these\nbins. Our compiler, Pikoc, compiles this input into an optimized implementation\ntargeted to a massively-parallel GPU or a multicore CPU.\n  Piko manages work granularity in a programmable and flexible manner, allowing\nprogrammers to build load-balanced parallel pipeline implementations, to\nexploit spatial and producer-consumer locality in a pipeline implementation,\nand to explore tradeoffs between these considerations. We demonstrate that Piko\ncan implement a wide range of pipelines, including rasterization, Reyes, ray\ntracing, rasterization/ray tracing hybrid, and deferred rendering. Piko allows\nus to implement efficient graphics pipelines with relative ease and to quickly\nexplore design alternatives by modifying the spatial binning configurations and\nscheduling preferences for individual stages, all while delivering real-time\nperformance that is within a factor six of state-of-the-art rendering systems."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1406.5431v2", 
    "title": "Consistently Orienting Facets in Polygon Meshes by Minimizing the   Dirichlet Energy of Generalized Winding Numbers", 
    "arxiv-id": "1406.5431v2", 
    "author": "Olga Sorkine-Hornung", 
    "publish": "2014-06-20T15:34:06Z", 
    "summary": "Jacobson et al. [JKSH13] hypothesized that the local coherency of the\ngeneralized winding number function could be used to correctly determine\nconsistent facet orientations in polygon meshes. We report on an approach to\nconsistently orienting facets in polygon meshes by minimizing the Dirichlet\nenergy of generalized winding numbers. While the energy can be concisely\nformulated and efficiently computed, we found that this approach is\nfundamentally flawed and is unfortunately not applicable for most handmade\nmeshes shared on popular mesh repositories such as Google 3D Warehouse."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1406.6786v2", 
    "title": "3D Texture Coordinates on Polygon Mesh Sequences", 
    "arxiv-id": "1406.6786v2", 
    "author": "Eric Mootz", 
    "publish": "2014-06-26T07:02:33Z", 
    "summary": "A method for creating 3D texture coordinates for a sequence of polygon meshes\nwith changing topology and vertex motion vectors."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1406.7025v1", 
    "title": "DASS: Detail Aware Sketch-Based Surface Modeling", 
    "arxiv-id": "1406.7025v1", 
    "author": "Emilio Vital Brazil", 
    "publish": "2014-06-26T20:50:34Z", 
    "summary": "We present a sketch-based modeling system suitable for detail editing, based\non a multilevel representation for surfaces. The main advantage of this\nrepresentation allowing for the control of local (details) and global changes\nof the model. We used an adaptive mesh (4-8 mesh) and developed a label theory\nto construct a manifold structure, which is responsible for controlling local\nediting of the model. The overall shape and global modifications are defined by\na variational implicit surface (Hermite RBF). Our system assembles the manifold\nstructures to allow the user to add details without changing the overall shape,\nas well as edit the overall shape while repositioning details coherently."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1406.7338v1", 
    "title": "Order-Independent Texture Synthesis", 
    "arxiv-id": "1406.7338v1", 
    "author": "Marc Levoy", 
    "publish": "2014-06-28T00:47:30Z", 
    "summary": "Search-based texture synthesis algorithms are sensitive to the order in which\ntexture samples are generated; different synthesis orders yield different\ntextures. Unfortunately, most polygon rasterizers and ray tracers do not\nguarantee the order with which surfaces are sampled. To circumvent this\nproblem, textures are synthesized beforehand at some maximum resolution and\nrendered using texture mapping.\n  We describe a search-based texture synthesis algorithm in which samples can\nbe generated in arbitrary order, yet the resulting texture remains identical.\nThe key to our algorithm is a pyramidal representation in which each texture\nsample depends only on a fixed number of neighboring samples at each level of\nthe pyramid. The bottom (coarsest) level of the pyramid consists of a noise\nimage, which is small and predetermined. When a sample is requested by the\nrenderer, all samples on which it depends are generated at once. Using this\napproach, samples can be generated in any order. To make the algorithm\nefficient, we propose storing texture samples and their dependents in a\npyramidal cache. Although the first few samples are expensive to generate,\nthere is substantial reuse, so subsequent samples cost less. Fortunately, most\nrendering algorithms exhibit good coherence, so cache reuse is high."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1408.0677v1", 
    "title": "A Moving Least Squares Based Approach for Contour Visualization of   Multi-Dimensional Data", 
    "arxiv-id": "1408.0677v1", 
    "author": "Kwan-Liu Ma", 
    "publish": "2014-08-04T13:27:17Z", 
    "summary": "Analysis of high dimensional data is a common task. Often, small multiples\nare used to visualize 1 or 2 dimensions at a time, such as in a scatterplot\nmatrix. Associating data points between different views can be difficult\nthough, as the points are not fixed. Other times, dimensional reduction\ntechniques are employed to summarize the whole dataset in one image, but\nindividual dimensions are lost in this view. In this paper, we present a means\nof augmenting a dimensional reduction plot with isocontours to reintroduce the\noriginal dimensions. By applying this to each dimension in the original data,\nwe create multiple views where the points are consistent, which facilitates\ntheir comparison. Our approach employs a combination of a novel, graph-based\nprojection technique with a GPU accelerated implementation of moving least\nsquares to interpolate space between the points. We also present evaluations of\nthis approach both with a case study and with a user study."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1408.1118v1", 
    "title": "Spoke Darts for Efficient High Dimensional Blue Noise Sampling", 
    "arxiv-id": "1408.1118v1", 
    "author": "Li-Yi Wei", 
    "publish": "2014-08-05T21:15:56Z", 
    "summary": "Blue noise refers to sample distributions that are random and well-spaced,\nwith a variety of applications in graphics, geometry, and optimization.\nHowever, prior blue noise sampling algorithms typically suffer from the\ncurse-of-dimensionality, especially when striving to cover a domain maximally.\nThis hampers their applicability for high dimensional domains.\n  We present a blue noise sampling method that can achieve high quality and\nperformance across different dimensions. Our key idea is spoke-dart sampling,\nsampling locally from hyper-annuli centered at prior point samples, using\nlines, planes, or, more generally, hyperplanes. Spoke-dart sampling is more\nefficient at high dimensions than the state-of-the-art alternatives: global\nsampling and advancing front point sampling. Spoke-dart sampling achieves good\nquality as measured by differential domain spectrum and spatial coverage. In\nparticular, it probabilistically guarantees that each coverage gap is small,\nwhereas global sampling can only guarantee that the sum of gaps is not large.\nWe demonstrate advantages of our method through empirical analysis and\napplications across dimensions 8 to 23 in Delaunay graphs, global optimization,\nand motion planning."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1408.3326v1", 
    "title": "Regularized Harmonic Surface Deformation", 
    "arxiv-id": "1408.3326v1", 
    "author": "Tino Weinkauf", 
    "publish": "2014-08-14T16:05:29Z", 
    "summary": "Harmonic surface deformation is a well-known geometric modeling method that\ncreates plausible deformations in an interactive manner. However, this method\nis susceptible to artifacts, in particular close to the deformation handles.\nThese artifacts often correlate with strong gradients of the deformation\nenergy.In this work, we propose a novel formulation of harmonic surface\ndeformation, which incorporates a regularization of the deformation energy. To\ndo so, we build on and extend a recently introduced generic linear\nregularization approach. It can be expressed as a change of norm for the linear\noptimization problem, i.e., the regularization is baked into the optimization.\nThis minimizes the implementation complexity and has only a small impact on\nruntime. Our results show that a moderate use of regularization suppresses many\ndeformation artifacts common to the well-known harmonic surface deformation\nmethod, without introducing new artifacts."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1409.2081v1", 
    "title": "History-free Collision Response for Deformable Surfaces", 
    "arxiv-id": "1409.2081v1", 
    "author": "Juntao Ye", 
    "publish": "2014-09-07T04:34:11Z", 
    "summary": "Continuous collision detection (CCD) and response methods are widely adopted\nin dynamics simulation of deformable models. They are history-based, as their\nsuccess is strictly based on an assumption of a collision-free state at the\nstart of each time interval. On the other hand, in many applications surfaces\nhave normals defined to designate their orientation (i.e. front- and\nback-face), yet CCD methods are totally blind to such orientation\nidentification (thus are orientation-free). We notice that if such information\nis utilized, many penetrations can be untangled. In this paper we present a\nhistory-free method for separation of two penetrating meshes, where at least\none of them has clarified surface orientation. This method first computes all\nedge-face (E-F) intersections with discrete collision detection (DCD), and then\nbuilds a number of penetration stencils. On response, the stencil vertices are\nrelocated into a penetration-free state, via a global displacement minimizer.\nOur method is very effective for handling penetration between two meshes, being\nit an initial configuration or in the middle of physics simulation. The major\nlimitation is that it is not applicable to self-collision within one mesh at\nthe time being."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2430337", 
    "link": "http://arxiv.org/pdf/1409.2235v3", 
    "title": "Tracing Analytic Ray Curves for Light and Sound Propagation in   Non-linear Media", 
    "arxiv-id": "1409.2235v3", 
    "author": "Dinesh Manocha", 
    "publish": "2014-09-08T08:19:21Z", 
    "summary": "The physical world consists of spatially varying media, such as the\natmosphere and the ocean, in which light and sound propagates along non-linear\ntrajectories. This presents a challenge to existing ray-tracing based methods,\nwhich are widely adopted to simulate propagation due to their efficiency and\nflexibility, but assume linear rays. We present a novel algorithm that traces\nanalytic ray curves computed from local media gradients, and utilizes the\nclosed-form solutions of both the intersections of the ray curves with planar\nsurfaces, and the travel distance. By constructing an adaptive unstructured\nmesh, our algorithm is able to model general media profiles that vary in three\ndimensions with complex boundaries consisting of terrains and other scene\nobjects such as buildings. We trace the analytic ray curves using the adaptive\nunstructured mesh, which considerably improves the efficiency over prior\nmethods. We highlight the algorithm's application on simulation of sound and\nvisual propagation in outdoor scenes."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11075-015-9988-3", 
    "link": "http://arxiv.org/pdf/1501.03032v3", 
    "title": "$G^{k,l}$-constrained multi-degree reduction of B\u00e9zier curves", 
    "arxiv-id": "1501.03032v3", 
    "author": "Pawe\u0142 Wo\u017any", 
    "publish": "2015-01-13T15:13:35Z", 
    "summary": "We present a new approach to the problem of $G^{k,l}$-constrained ($k,l \\leq\n3$) multi-degree reduction of B\\'{e}zier curves with respect to the least\nsquares norm. First, to minimize the least squares error, we consider two\nmethods of determining the values of geometric continuity parameters. One of\nthem is based on quadratic and nonlinear programming, while the other uses some\nsimplifying assumptions and solves a system of linear equations. Next, for\nprescribed values of these parameters, we obtain control points of the\nmulti-degree reduced curve, using the properties of constrained dual Bernstein\nbasis polynomials. Assuming that the input and output curves are of degree $n$\nand $m$, respectively, we determine these points with the complexity $O(mn)$,\nwhich is significantly less than the cost of other known methods. Finally, we\ngive several examples to demonstrate the effectiveness of our algorithms."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11075-015-9988-3", 
    "link": "http://arxiv.org/pdf/1501.03605v1", 
    "title": "Feature Lines for Illustrating Medical Surface Models: Mathematical   Background and Survey", 
    "arxiv-id": "1501.03605v1", 
    "author": "Bernhard Preim", 
    "publish": "2015-01-15T09:16:26Z", 
    "summary": "This paper provides a tutorial and survey for a specific kind of illustrative\nvisualization technique: feature lines. We examine different feature line\nmethods. For this, we provide the differential geometry behind these concepts\nand adapt this mathematical field to the discrete differential geometry. All\ndiscrete differential geometry terms are explained for triangulated surface\nmeshes. These utilities serve as basis for the feature line methods. We provide\nthe reader with all knowledge to re-implement every feature line method.\nFurthermore, we summarize the methods and suggest a guideline for which kind of\nsurface which feature line algorithm is best suited. Our work is motivated by,\nbut not restricted to, medical and biological surface models."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11075-015-9988-3", 
    "link": "http://arxiv.org/pdf/1007.2204v1", 
    "title": "What's wrong with Phong - Designers' appraisal of shading in CAD-systems", 
    "arxiv-id": "1007.2204v1", 
    "author": "J\u00f6rg M. Hahn", 
    "publish": "2010-07-13T21:09:11Z", 
    "summary": "The Phong illumination model is still widely used in realtime 3D\nvisualization systems. The aim of this article is to document problems with the\nPhong illumination model that are encountered by an important professional user\ngroup, namely digital designers. This leads to a visual evaluation of Phong\nillumination, which at least in this condensed form seems still to be missing\nin the literature. It is hoped that by explicating these flaws, awareness about\nthe limitations and interdependencies of the model will increase, both among\nfellow users, and among researchers and developers."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11075-015-9988-3", 
    "link": "http://arxiv.org/pdf/1109.1914v2", 
    "title": "Jacobians and Hessians of Mean Value Coordinates for Closed Triangular   Meshes", 
    "arxiv-id": "1109.1914v2", 
    "author": "Tamy Boubekeur", 
    "publish": "2011-09-09T06:49:52Z", 
    "summary": "In this technical note, we present the formulae of the derivatives of the\nMean Value Coordinates based transformations, using an enclosing triangle mesh,\nacting as a cage for the deformation of an interior object."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11075-015-9988-3", 
    "link": "http://arxiv.org/pdf/1109.6073v1", 
    "title": "Evaluation of a Bundling Technique for Parallel Coordinates", 
    "arxiv-id": "1109.6073v1", 
    "author": "Daniel Weiskopf", 
    "publish": "2011-09-28T01:44:43Z", 
    "summary": "We describe a technique for bundled curve representations in\nparallel-coordinates plots and present a controlled user study evaluating their\neffectiveness. Replacing the traditional C^0 polygonal lines by C^1 continuous\npiecewise Bezier curves makes it easier to visually trace data points through\neach coordinate axis. The resulting Bezier curves can then be bundled to\nvisualize data with given cluster structures. Curve bundles are efficient to\ncompute, provide visual separation between data clusters, reduce visual\nclutter, and present a clearer overview of the dataset. A controlled user study\nwith 14 participants confirmed the effectiveness of curve bundling for\nparallel-coordinates visualization: 1) compared to polygonal lines, it is\nequally capable of revealing correlations between neighboring data attributes;\n2) its geometric cues can be effective in displaying cluster information. For\nsome datasets curve bundling allows the color perceptual channel to be applied\nto other data attributes, while for complex cluster patterns, bundling and\ncolor can represent clustering far more clearly than either alone."
},{
    "category": "cs.GR", 
    "doi": "10.1111/j.1467-8659.2010.01828.x", 
    "link": "http://arxiv.org/pdf/1109.6494v1", 
    "title": "A Survey of Ocean Simulation and Rendering Techniques in Computer   Graphics", 
    "arxiv-id": "1109.6494v1", 
    "author": "Jean-Christophe Gonzato", 
    "publish": "2011-09-29T11:50:29Z", 
    "summary": "This paper presents a survey of ocean simulation and rendering methods in\ncomputer graphics. To model and animate the ocean's surface, these methods\nmainly rely on two main approaches: on the one hand, those which approximate\nocean dynamics with parametric, spectral or hybrid models and use empirical\nlaws from oceanographic research. We will see that this type of methods\nessentially allows the simulation of ocean scenes in the deep water domain,\nwithout breaking waves. On the other hand, physically-based methods use\nNavier-Stokes Equations (NSE) to represent breaking waves and more generally\nocean surface near the shore. We also describe ocean rendering methods in\ncomputer graphics, with a special interest in the simulation of phenomena such\nas foam and spray, and light's interaction with the ocean surface."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2012.649621", 
    "link": "http://arxiv.org/pdf/1204.1461v1", 
    "title": "Efficient computational noise in GLSL", 
    "arxiv-id": "1204.1461v1", 
    "author": "Mark Richardson", 
    "publish": "2012-04-06T12:03:45Z", 
    "summary": "We present GLSL implementations of Perlin noise and Perlin simplex noise that\nrun fast enough for practical consideration on current generation GPU hardware.\nThe key benefits are that the functions are purely computational, i.e. they use\nneither textures nor lookup tables, and that they are implemented in GLSL\nversion 1.20, which means they are compatible with all current GLSL-capable\nplatforms, including OpenGL ES 2.0 and WebGL 1.0. Their performance is on par\nwith previously presented GPU implementations of noise, they are very\nconvenient to use, and they scale well with increasing parallelism in present\nand upcoming GPU architectures."
},{
    "category": "cs.GR", 
    "doi": "10.1080/2151237X.2012.649621", 
    "link": "http://arxiv.org/pdf/1204.4734v1", 
    "title": "Numerical Analysis of Diagonal-Preserving, Ripple-Minimizing and   Low-Pass Image Resampling Methods", 
    "arxiv-id": "1204.4734v1", 
    "author": "Chantal Racette", 
    "publish": "2012-04-20T20:05:46Z", 
    "summary": "Image resampling is a necessary component of any operation that changes the\nsize of an image or its geometry.\n  Methods tuned for natural image upsampling (roughly speaking, image\nenlargement) are analyzed and developed with a focus on their ability to\npreserve diagonal features and suppress overshoots. Monotone, locally bounded\nand almost monotone \"direct\" interpolation and filtering methods, as well as\nface split and vertex split surface subdivision methods, alone or in\ncombination, are studied. Key properties are established by way of proofs and\ncounterexamples as well as numerical experiments involving 1D curve and 2D\ndiagonal data resampling.\n  In addition, the Remez minimax method for the computation of low-cost\npolynomial approximations of low-pass filter kernels tuned for natural image\ndownsampling (roughly speaking, image reduction) is refactored for relative\nerror minimization in the presence of roots in the interior of the interval of\napproximation and so that even and odd functions are approximated with like\npolynomials. The accuracy and frequency response of the approximations are\ntabulated and plotted against the original, establishing their rapid\nconvergence."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2516971.2516977", 
    "link": "http://arxiv.org/pdf/1204.6216v2", 
    "title": "Geodesics in Heat", 
    "arxiv-id": "1204.6216v2", 
    "author": "Max Wardetzky", 
    "publish": "2012-04-24T20:26:58Z", 
    "summary": "We introduce the heat method for computing the shortest geodesic distance to\na specified subset (e.g., point or curve) of a given domain. The heat method is\nrobust, efficient, and simple to implement since it is based on solving a pair\nof standard linear elliptic problems. The method represents a significant\nbreakthrough in the practical computation of distance on a wide variety of\ngeometric domains, since the resulting linear systems can be prefactored once\nand subsequently solved in near-linear time. In practice, distance can be\nupdated via the heat method an order of magnitude faster than with\nstate-of-the-art methods while maintaining a comparable level of accuracy. We\nprovide numerical evidence that the method converges to the exact geodesic\ndistance in the limit of refinement; we also explore smoothed approximations of\ndistance suitable for applications where more regularity is required."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2516971.2516977", 
    "link": "http://arxiv.org/pdf/1207.3899v1", 
    "title": "Fast View Frustum Culling of Spatial Object by Analytical Bounding Bin", 
    "arxiv-id": "1207.3899v1", 
    "author": "Yunchol Jong", 
    "publish": "2012-07-17T07:17:27Z", 
    "summary": "It is a common sense to apply the VFC (view frustum culling) of spatial\nobject to bounding cube of the object in 3D graphics. The accuracy of VFC can\nnot be guaranteed even in cube rotated three-dimensionally. In this paper is\nproposed a method which is able to carry out more precise and fast VFC of any\nspatial object in the image domain of cube by an analytic mapping, and is\ndemonstrated the effect of the method for terrain block on global surface."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2516971.2516977", 
    "link": "http://arxiv.org/pdf/1301.6809v2", 
    "title": "Skeletal Representations and Applications", 
    "arxiv-id": "1301.6809v2", 
    "author": "Andrea Tagliasacchi", 
    "publish": "2013-01-29T00:18:00Z", 
    "summary": "When representing a solid object there are alternatives to the use of\ntraditional explicit (surface meshes) or implicit (zero crossing of implicit\nfunctions) methods. Skeletal representations encode shape information in a\nmixed fashion: they are composed of a set of explicit primitives, yet they are\nable to efficiently encode the shape's volume as well as its topology. I will\ndiscuss, in two dimensions, how symmetry can be used to reduce the\ndimensionality of the data (from a 2D solid to a 1D curve), and how this\nrelates to the classical definition of skeletons by Medial Axis Transform.\nWhile the medial axis of a 2D shape is composed of a set of curves, in 3D it\nresults in a set of sheets connected in a complex fashion. Because of this\ncomplexity, medial skeletons are difficult to use in practical applications.\nCurve skeletons address this problem by strictly requiring their geometry to be\none dimensional, resulting in an intuitive yet powerful shape representation.\nIn this report I will define both medial and curve skeletons and discuss their\nmutual relationship. I will also present several algorithms for their\ncomputation and a variety of scenarios where skeletons are employed, with a\nspecial focus on geometry processing and shape analysis."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2516971.2516977", 
    "link": "http://arxiv.org/pdf/1304.0600v1", 
    "title": "Software for creating pictures in the LaTeX environment", 
    "arxiv-id": "1304.0600v1", 
    "author": "Bezhentcev Roman Vadimovich", 
    "publish": "2013-04-02T11:55:33Z", 
    "summary": "To create a text with graphic instructions for output pictures into LATEX\ndocument, we offer software that allows us to build a picture in WIZIWIG mode\nand for setting the text with these graphical instructions."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.2889v1", 
    "title": "Glyph Sorting: Interactive Visualization for Multi-dimensional Data", 
    "arxiv-id": "1304.2889v1", 
    "author": "Min Chen", 
    "publish": "2013-04-10T09:50:13Z", 
    "summary": "Glyph-based visualization is an effective tool for depicting multivariate\ninformation. Since sorting is one of the most common analytical tasks performed\non individual attributes of a multi-dimensional data set, this motivates the\nhypothesis that introducing glyph sorting would significantly enhance the\nusability of glyph-based visualization. In this paper, we present a glyph-based\nconceptual framework as part of a visualization process for interactive sorting\nof multivariate data. We examine several technical aspects of glyph sorting and\nprovide design principles for developing effective, visually sortable glyphs.\nGlyphs that are visually sortable provide two key benefits: 1) performing\ncomparative analysis of multiple attributes between glyphs and 2) to support\nmulti-dimensional visual search. We describe a system that incorporates focus\nand context glyphs to control sorting in a visually intuitive manner and for\nviewing sorted results in an Interactive, Multi-dimensional Glyph (IMG) plot\nthat enables users to perform high-dimensional sorting, analyse and examine\ndata trends in detail. To demonstrate the usability of glyph sorting, we\npresent a case study in rugby event analysis for comparing and analysing trends\nwithin matches. This work is undertaken in conjunction with a national rugby\nteam. From using glyph sorting, analysts have reported the discovery of new\ninsight beyond traditional match analysis."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7842v1", 
    "title": "The Logarithmic Curvature Graphs of Generalised Cornu Spirals", 
    "arxiv-id": "1304.7842v1", 
    "author": "Kenjiro T. Miura", 
    "publish": "2013-04-30T03:10:31Z", 
    "summary": "The Generalized Cornu Spiral (GCS) was first proposed by Ali et al. in 1995\n[9]. Due to the monotonocity of its curvature function, the surface generated\nwith GCS segments has been considered as a high quality surface and it has\npotential applications in surface design [2]. In this paper, the analysis of\nGCS segment is carried out by determining its aesthetic value using the log\ncurvature Graph (LCG) as proposed by Kanaya et al.[10]. The analysis of LCG\nsupports the claim that GCS is indeed a generalized aesthetic curve."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7845v1", 
    "title": "G2 Transition curve using Quartic Bezier Curve", 
    "arxiv-id": "1304.7845v1", 
    "author": "Jamaluddin Md. Ali", 
    "publish": "2013-04-30T03:28:07Z", 
    "summary": "A method to construct transition curves using a family of the quartic Bezier\nspiral is described. The transition curves discussed are S-shape and C-shape of\ncontact, between two separated circles. A spiral is a curve of monotone\nincreasing or monotone decreasing curvature of one sign. Thus, a spiral cannot\nhave an inflection point or curvature extreme. The family of quartic Bezier\nspiral form which is introduced has more degrees of freedom and will give a\nbetter approximation. It is proved that the methods of constructing transition\ncurves can be simplified by the transformation process and the ratio of two\nradii has no restriction, which extends the application area, and it gives a\nfamily of transition curves that allow more flexible curve designs."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7848v1", 
    "title": "Characterization of Planar Cubic Alternative curve", 
    "arxiv-id": "1304.7848v1", 
    "author": "Jamaluddin Md. Ali", 
    "publish": "2013-04-30T03:33:43Z", 
    "summary": "In this paper, we analyze the planar cubic Alternative curve to determine the\nconditions for convex, loops, cusps and inflection points. Thus cubic curve is\nrepresented by linear combination of three control points and basis function\nthat consist of two shape parameters. By using algebraic manipulation, we can\ndetermine the constraint of shape parameters and sufficient conditions are\nderived which ensure that the curve is a strictly convex, loops, cusps and\ninflection point. We conclude the result in a shape diagram of parameters. The\nsimplicity of this form makes characterization more intuitive and efficient to\ncompute."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7852v1", 
    "title": "Variational Formulation of the Log-Aesthetic Surface and Development of   Discrete Surface Filters", 
    "arxiv-id": "1304.7852v1", 
    "author": "R. U. Gobithaasan", 
    "publish": "2013-04-30T03:41:31Z", 
    "summary": "The log-aesthetic curves include the logarithmic (equiangular) spiral,\nclothoid, and involute curves. Although most of them are expressed only by an\nintegral form of the tangent vector, it is possible to interactively generate\nand deform them and they are expected to be utilized for practical use of\nindustrial and graphical design. The discrete log-aesthetic filter based on the\nformulation of the log-aesthetic curve has successfully been introduced not to\nimpose strong constraints on the designer's activity, to let him/her design\nfreely and to embed the properties of the log-aesthetic curves for complicated\nones with both increasing and decreasing curvature. In this paper, in order to\ndefine the log-aesthetic surface and develop surface filters based on its\nformulation, at first we reformulate the log-aesthetic curve with variational\nprinciple. Then we propose several new functionals to be minimized for\nfree-form surfaces and define the log-aesthetic surface. Furthermore we propose\nnew discrete surface filters based on the log-aesthetic surface formulation"
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7868v1", 
    "title": "Normal type-2 Fuzzy Rational B-Spline Curve", 
    "arxiv-id": "1304.7868v1", 
    "author": "R. U. Gobithaasan", 
    "publish": "2013-04-30T04:27:35Z", 
    "summary": "In this paper, we proposed a new form of type-2 fuzzy data points(T2FDPs)\nthat is normal type-2 data points(NT2FDPs). These brand-new forms of data were\ndefined by using the definition of normal type-2 triangular fuzzy\nnumber(NT2TFN). Then, we applied fuzzification(alpha-cut) and type-reduction\nprocesses towards NT2FDPs after they had been redefined based on the situation\nof NT2FDPs. Furthermore, we redefine the defuzzification definition along with\nthe new definitions of fuzzification process and type-reduction method to\nobtain crisp type-2 fuzzy solution data points. For all these processes from\nthe defining the NT2FDPs to defuzzification of NT2FDPs, we demonstrate through\ncurve representation by using the rational B-spline curve function as the\nexample form modeling these NT2FDPs."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7881v1", 
    "title": "Various Types of Aesthetic Curves", 
    "arxiv-id": "1304.7881v1", 
    "author": "R. U. Gobithaasan", 
    "publish": "2013-04-30T05:25:31Z", 
    "summary": "The research on developing planar curves to produce visually pleasing\nproducts (ranges from electric appliances to car body design) and\nindentifying/modifying planar curves for special purposes namely for railway\ndesign, highway design and robot trajectories have been progressing since\n1970s. The pattern of research in this field of study has branched to five\nmajor groups namely curve synthesis, fairing process, improvement in control of\nnatural spiral, construction of new type of planar curves and, natural spiral\nfitting & approximation techniques. The purpose of is this paper is to briefly\nreview recent progresses in Computer Aided Geometric Design (CAGD) focusing on\nthe topics states above."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1304.7883v1", 
    "title": "An Improvised Algorithm to Identify The Beauty of A Planar Curve", 
    "arxiv-id": "1304.7883v1", 
    "author": "Kenjiro T. Miura", 
    "publish": "2013-04-30T05:30:00Z", 
    "summary": "An improvised algorithm is proposed based on the work of Yoshimoto and\nHarada. The improvised algorithm results a graph which is called LDGC or\nLogarithmic Distribution Graph of Curvature. This graph has the capability to\nidentify the beauty of monotonic planar curves with less effort as compared to\nLDDC by Yoshimoto and Harada."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1305.0001v1", 
    "title": "Perfectly normal type-2 fuzzy interpolation B-spline curve", 
    "arxiv-id": "1305.0001v1", 
    "author": "R. U. Gobithaasan", 
    "publish": "2013-04-30T05:12:05Z", 
    "summary": "In this paper, we proposed another new form of type-2 fuzzy data\npoints(T2FDPs) that is perfectly normal type-2 data points(PNT2FDPs). These\nkinds of brand-new data were defined by using the existing type-2 fuzzy set\ntheory(T2FST) and type-2 fuzzy number(T2FN) concept since we dealt with the\nproblem of defining complex uncertainty data. Along with this restructuring, we\nincluded the fuzzification(alpha-cut operation), type-reduction and\ndefuzzification processes against PNT2FDPs. In addition, we used interpolation\nB-soline curve function to demonstrate the PNT2FDPs."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1305.1293v1", 
    "title": "Parallel Chen-Han (PCH) Algorithm for Discrete Geodesics", 
    "arxiv-id": "1305.1293v1", 
    "author": "Ying He", 
    "publish": "2013-05-07T14:09:16Z", 
    "summary": "In many graphics applications, the computation of exact geodesic distance is\nvery important. However, the high computational cost of the existing geodesic\nalgorithms means that they are not practical for large-scale models or\ntime-critical applications. To tackle this challenge, we propose the parallel\nChen-Han (or PCH) algorithm, which extends the classic Chen-Han (CH) discrete\ngeodesic algorithm to the parallel setting. The original CH algorithm and its\nvariant both lack a parallel solution because the windows (a key data structure\nthat carries the shortest distance in the wavefront propagation) are maintained\nin a strict order or a tightly coupled manner, which means that only one window\nis processed at a time. We propose dividing the CH's sequential algorithm into\nfour phases, window selection, window propagation, data organization, and\nevents processing so that there is no data dependence or conflicts in each\nphase and the operations within each phase can be carried out in parallel. The\nproposed PCH algorithm is able to propagate a large number of windows\nsimultaneously and independently. We also adopt a simple yet effective strategy\nto control the total number of windows. We implement the PCH algorithm on\nmodern GPUs (such as Nvidia GTX 580) and analyze the performance in detail. The\nperformance improvement (compared to the sequential algorithms) is highly\nconsistent with GPU double-precision performance (GFLOPS). Extensive\nexperiments on real-world models demonstrate an order of magnitude improvement\nin execution time compared to the state-of-the-art."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1305.1473v1", 
    "title": "On the variety of planar spirals and their applications in computer   aided design", 
    "arxiv-id": "1305.1473v1", 
    "author": "Kenjiro T. Miura", 
    "publish": "2013-05-07T11:35:49Z", 
    "summary": "In this paper we discuss the variety of planar spiral segments and their\napplications in objects in both the real and artificial world. The discussed\ncurves with monotonic curvature function are well-known in geometric modelling\nand computer aided geometric design as fair curves, and they are very\nsignificant in aesthetic shape modelling. Fair curve segments are used for\ntwo-point G1 and G2 Hermite interpolation, as well as for generating aesthetic\nsplines."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1305.1737v1", 
    "title": "MC-curves and aesthetic measurements for pseudospiral curve segments", 
    "arxiv-id": "1305.1737v1", 
    "author": "Kenjiro T. Miura", 
    "publish": "2013-05-08T07:59:52Z", 
    "summary": "This article studies families of curves with monotonic curvature function\n(MC-curves) and their applications in geometric modelling and aesthetic design.\nAesthetic analysis and assessment of the structure and plastic qualities of\npseudospirals, which are curves with monotonic curvature function, are\nconducted for the first time in the field of geometric modelling from the\nposition of technical aesthetics laws. The example of car body surface\nmodelling with the use of aesthetics splines is given."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1305.4583v2", 
    "title": "Parallel Coordinates Guided High Dimensional Transfer Function Design", 
    "arxiv-id": "1305.4583v2", 
    "author": "Xin Zhao", 
    "publish": "2013-05-20T17:27:29Z", 
    "summary": "High-dimensional transfer function design is widely used to provide\nappropriate data classification for direct volume rendering of various\ndatasets. However, its design is a complicated task. Parallel coordinate plot\n(PCP), as a powerful visualization tool, can efficiently display\nhigh-dimensional geometry and accurately analyze multivariate data. In this\npaper, we propose to combine parallel coordinates with dimensional reduction\nmethods to guide high-dimensional transfer function design. Our pipeline has\ntwo major advantages: (1) combine and display extracted high-dimensional\nfeatures in parameter space; and (2) select appropriate high-dimensional\nparameters, with the help of dimensional reduction methods, to obtain\nsophisticated data classification as transfer function for volume rendering. In\norder to efficiently design high-dimensional transfer functions, the\ncombination of both parallel coordinate components and dimension reduction\nresults is necessary to generate final visualization results. We demonstrate\nthe capability of our method for direct volume rendering using various CT and\nMRI datasets."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1306.1959v2", 
    "title": "Pattern Recognition and Revealing using Parallel Coordinates Plot", 
    "arxiv-id": "1306.1959v2", 
    "author": "Bo Li", 
    "publish": "2013-06-08T21:18:43Z", 
    "summary": "Parallel coordinates plot (PCP) is an excellent tool for multivariate\nvisualization and analysis, but it may fail to reveal inherent structures for\ndatasets with a large number of items. In this paper, we propose a suite of\nnovel clustering, dimension ordering and visualization techniques based on PCP,\nto reveal and highlight hidden structures. First, we propose a continuous\nspline based polycurves design to extract and classify different cluster\naspects of the data. Then, we provide an efficient and optimal correlation\nbased sorting technique to reorder coordinates, as a helpful visualization tool\nfor data analysis. Various results generated by our framework visually\nrepresent much structure, trend and correlation information to guide the user,\nand improve the efficacy of analysis, especially for complex and noisy\ndatasets."
},{
    "category": "cs.GR", 
    "doi": "10.1177/1473871613511959", 
    "link": "http://arxiv.org/pdf/1306.3113v2", 
    "title": "Multimaterial Front Tracking", 
    "arxiv-id": "1306.3113v2", 
    "author": "Eitan Grinspun", 
    "publish": "2013-06-13T14:11:28Z", 
    "summary": "We present the first triangle mesh-based technique for tracking the evolution\nof general three-dimensional multimaterial interfaces undergoing complex\ntopology changes induced by deformations and collisions. Our core\nrepresentation is a non-manifold triangle surface mesh with material labels\nassigned to each half-face to distinguish volumetric regions. We advect the\nvertices of the mesh in a Lagrangian manner, and employ a complete set of\ncollision-safe mesh improvement and topological operations that track and\nupdate material labels. In particular, we develop a unified, collision-safe\nstrategy for handling complex topological operations acting on evolving triple-\nand higher-valence junctions, and a flexible method to merge colliding\nmultimaterial meshes. We demonstrate our approach with a number of challenging\ngeometric flows, including passive advection, normal flow, and mean curvature\nflow."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1307.0118v3", 
    "title": "Computing a Compact Spline Representation of the Medial Axis Transform   of a 2D Shape", 
    "arxiv-id": "1307.0118v3", 
    "author": "Wenping Wang", 
    "publish": "2013-06-29T15:32:23Z", 
    "summary": "We present a full pipeline for computing the medial axis transform of an\narbitrary 2D shape. The instability of the medial axis transform is overcome by\na pruning algorithm guided by a user-defined Hausdorff distance threshold. The\nstable medial axis transform is then approximated by spline curves in 3D to\nproduce a smooth and compact representation. These spline curves are computed\nby minimizing the approximation error between the input shape and the shape\nrepresented by the medial axis transform. Our results on various 2D shapes\nsuggest that our method is practical and effective, and yields faithful and\ncompact representations of medial axis transforms of 2D shapes."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1307.0147v2", 
    "title": "4-Dimensional Geometry Lens: A Novel Volumetric Magnification Approach", 
    "arxiv-id": "1307.0147v2", 
    "author": "Hong Qin", 
    "publish": "2013-06-29T20:20:37Z", 
    "summary": "We present a novel methodology that utilizes 4-Dimensional (4D) space\ndeformation to simulate a magnification lens on versatile volume datasets and\ntextured solid models. Compared with other magnification methods (e.g.,\ngeometric optics, mesh editing), 4D differential geometry theory and its\npractices are much more flexible and powerful for preserving shape features\n(i.e., minimizing angle distortion), and easier to adapt to versatile solid\nmodels. The primary advantage of 4D space lies at the following fact: we can\nnow easily magnify the volume of regions of interest (ROIs) from the additional\ndimension, while keeping the rest region unchanged. To achieve this primary\ngoal, we first embed a 3D volumetric input into 4D space and magnify ROIs in\nthe 4th dimension. Then we flatten the 4D shape back into 3D space to\naccommodate other typical applications in the real 3D world. In order to\nenforce distortion minimization, in both steps we devise the high dimensional\ngeometry techniques based on rigorous 4D geometry theory for 3D/4D mapping back\nand forth to amend the distortion. Our system can preserve not only focus\nregion, but also context region and global shape. We demonstrate the\neffectiveness, robustness, and efficacy of our framework with a variety of\nmodels ranging from tetrahedral meshes to volume datasets."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1307.0247v1", 
    "title": "Progressive Blue Surfels", 
    "arxiv-id": "1307.0247v1", 
    "author": "Claudius J\u00e4hn", 
    "publish": "2013-06-30T21:54:33Z", 
    "summary": "In this paper we describe a new technique to generate and use surfels for\nrendering of highly complex, polygonal 3D scenes in real time. The basic idea\nis to approximate complex parts of the scene by rendering a set of points\n(surfels). The points are computed in a preprocessing step and offer two\nimportant properties: They are placed only on the visible surface of the\nscene's geometry and they are distributed and sorted in such a way, that every\nprefix of points is a good visual representation of the approximated part of\nthe scene. An early evaluation of the method shows that it is capable of\nrendering scenes consisting of several billions of triangles with high image\nquality."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1310.0041v1", 
    "title": "Gradient-Domain Processing for Large EM Image Stacks", 
    "arxiv-id": "1310.0041v1", 
    "author": "Joshua Vogelstein", 
    "publish": "2013-09-30T20:31:10Z", 
    "summary": "We propose a new gradient-domain technique for processing registered EM image\nstacks to remove the inter-image discontinuities while preserving intra-image\ndetail. To this end, we process the image stack by first performing anisotropic\ndiffusion to smooth the data along the slice axis and then solving a\nscreened-Poisson equation within each slice to re-introduce the detail. The\nfinal image stack is both continuous across the slice axis (facilitating the\ntracking of information between slices) and maintains sharp details within each\nslice (supporting automatic feature detection). To support this editing, we\ndescribe the implementation of the first multigrid solver designed for\nefficient gradient domain processing of large, out-of-core, voxel grids."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1310.1240v1", 
    "title": "Compression of animated 3D models using HO-SVD", 
    "arxiv-id": "1310.1240v1", 
    "author": "Sebastian Opozda", 
    "publish": "2013-10-04T12:27:46Z", 
    "summary": "This work presents an analysis of Higher Order Singular Value Decomposition\n(HO-SVD) applied to lossy compression of 3D mesh animations. We describe\nstrategies for choosing a number of preserved spatial and temporal components\nafter tensor decomposition. Compression error is measured using three metrics\n(MSE, Hausdorff, MSDM). Results are compared with a method based on Principal\nComponent Analysis (PCA) and presented on a set of animations with typical mesh\ndeformations."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1310.4459v1", 
    "title": "Matching LBO eigenspace of non-rigid shapes via high order statistics", 
    "arxiv-id": "1310.4459v1", 
    "author": "Ron Kimmel", 
    "publish": "2013-10-16T17:39:34Z", 
    "summary": "A fundamental tool in shape analysis is the virtual embedding of the\nRiemannian manifold describing the geometry of a shape into Euclidean space.\nSeveral methods have been proposed to embed isometric shapes in flat domains\nwhile preserving distances measured on the manifold. Recently, attention has\nbeen given to embedding shapes into the eigenspace of the Lapalce-Beltrami\noperator. The Laplace-Beltrami eigenspace preserves the diffusion distance, and\nis invariant under isometric transformations. However, Laplace-Beltrami\neigenfunctions computed independently for different shapes are often\nincompatible with each other. Applications involving multiple shapes, such as\npointwise correspondence, would greatly benefit if their respective\neigenfunctions were somehow matched. Here, we introduce a statistical approach\nfor matching eigenfunctions. We consider the values of the eigenfunctions over\nthe manifold as sampling of random variables, and try to match their\nmultivariate distributions. Comparing distributions is done indirectly, using\nhigh order statistics. We show that the permutation and sign ambiguities of low\norder eigenfunctions, can be inferred by minimizing the difference of their\nthird order moments. The sign ambiguities of antisymmetric eigenfunctions can\nbe resolved by exploiting isometric invariant relations between the gradients\nof the eigenfunctions and the surface normal. We present experiments\ndemonstrating the success of the proposed method applied to feature point\ncorrespondence."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1401.0113v3", 
    "title": "Connectivity-preserving Geometry Images", 
    "arxiv-id": "1401.0113v3", 
    "author": "Hubert Roth", 
    "publish": "2013-12-31T08:50:03Z", 
    "summary": "We propose connectivity-preserving geometry images (CGIMs), which map a\nthree-dimensional mesh onto a rectangular regular array of an image, such that\nthe reconstructed mesh produces no sampling errors, but merely round-off\nerrors. We obtain a V-matrix with respect to the original mesh, whose elements\nare vertices of the mesh, which intrinsically preserves the vertex-set and the\nconnectivity of the original mesh in the sense of allowing round-off errors. We\ngenerate a CGIM array by using the Cartesian coordinates of corresponding\nvertices of the V-matrix. To reconstruct a mesh, we obtain a vertex-set and an\nedge-set by collecting all the elements with different pixels, and all\ndifferent pairwise adjacent elements from the CGIM array respectively. Compared\nwith traditional geometry images, CGIMs achieve minimum reconstruction errors\nwith an efficient parametrization-free algorithm via elementary permutation\ntechniques. We apply CGIMs to lossy compression of meshes, and the experimental\nresults show that CGIMs perform well in reconstruction precision and detail\npreservation."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2014.03.007", 
    "link": "http://arxiv.org/pdf/1401.1488v1", 
    "title": "Forward and Inverse Kinematics Seamless Matching Using Jacobian", 
    "arxiv-id": "1401.1488v1", 
    "author": "Mostafa Karbasi", 
    "publish": "2014-01-07T20:14:55Z", 
    "summary": "In this paper the problem of matching Forward Kinematics (FK) motion of a 3\nDimensional (3D) joint chain to the Inverse Kinematics (IK) movement and vice\nversa has been addressed. The problem lies at the heart of animating a 3D\ncharacter having controller and manipulator based rig for animation within any\n3D modeling and animation software. The seamless matching has been achieved\nthrough the use of pseudo-inverse of Jacobian Matrix. The Jacobian Matrix is\nused to determine the rotation values of each joint of character body part such\nas arms, between the inverse kinematics and forward kinematics motion. Then\nmoving the corresponding kinematic joint system to the desired place,\nautomatically eliminating the jumping or popping effect which would reduce the\ncomplexity of the system."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1403.8105v1", 
    "title": "Flux-Limited Diffusion for Multiple Scattering in Participating Media", 
    "arxiv-id": "1403.8105v1", 
    "author": "Bernd Eberhardt", 
    "publish": "2014-03-31T17:54:34Z", 
    "summary": "For the rendering of multiple scattering effects in participating media,\nmethods based on the diffusion approximation are an extremely efficient\nalternative to Monte Carlo path tracing. However, in sufficiently transparent\nregions, classical diffusion approximation suffers from non-physical radiative\nfluxes which leads to a poor match to correct light transport. In particular,\nthis prevents the application of classical diffusion approximation to\nheterogeneous media, where opaque material is embedded within transparent\nregions. To address this limitation, we introduce flux-limited diffusion, a\ntechnique from the astrophysics domain. This method provides a better\napproximation to light transport than classical diffusion approximation,\nparticularly when applied to heterogeneous media, and hence broadens the\napplicability of diffusion-based techniques. We provide an algorithm for\nflux-limited diffusion, which is validated using the transport theory for a\npoint light source in an infinite homogeneous medium. We further demonstrate\nthat our implementation of flux-limited diffusion produces more accurate\nrenderings of multiple scattering in various heterogeneous datasets than\nclassical diffusion approximation, by comparing both methods to ground truth\nrenderings obtained via volumetric path tracing."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1405.4734v1", 
    "title": "A General Framework for Bilateral and Mean Shift Filtering", 
    "arxiv-id": "1405.4734v1", 
    "author": "Chris Wojtan", 
    "publish": "2014-04-30T22:05:15Z", 
    "summary": "We present a generalization of the bilateral filter that can be applied to\nfeature-preserving smoothing of signals on images, meshes, and other domains\nwithin a single unified framework. Our discretization is competitive with\nstate-of-the-art smoothing techniques in terms of both accuracy and speed, is\neasy to implement, and has parameters that are straightforward to understand.\nUnlike previous bilateral filters developed for meshes and other irregular\ndomains, our construction reduces exactly to the image bilateral on rectangular\ndomains and comes with a rigorous foundation in both the smooth and discrete\nsettings. These guarantees allow us to construct unconditionally convergent\nmean-shift schemes that handle a variety of extremely noisy signals. We also\napply our framework to geometric edge-preserving effects like feature\nenhancement and show how it is related to local histogram techniques."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1405.7457v1", 
    "title": "Incorporating Sharp Features in the General Solid Sweep Framework", 
    "arxiv-id": "1405.7457v1", 
    "author": "Milind Sohoni", 
    "publish": "2014-05-29T04:11:31Z", 
    "summary": "This paper extends a recently proposed robust computational framework for\nconstructing the boundary representation (brep) of the volume swept by a given\nsmooth solid moving along a one parameter family $h$ of rigid motions. Our\nextension allows the input solid to have sharp features, i.e., to be of class\nG0 wherein, the unit outward normal to the solid may be discontinuous. In the\nearlier framework, the solid to be swept was restricted to be G1, and thus this\nis a significant and useful extension of that work. This naturally requires a\nprecise description of the geometry of the surface generated by the sweep of a\nsharp edge supported by two intersecting smooth faces. We uncover the geometry\nalong with the related issues like parametrization, self-intersection and\nsingularities via a novel mathematical analysis. Correct trimming of such a\nsurface is achieved by a delicate analysis of the interplay between the cone of\nnormals at a sharp point and its trajectory under $h$. The overall topology is\nexplicated by a key lifting theorem which allows us to compute the adjacency\nrelations amongst entities in the swept volume by relating them to\ncorresponding adjacencies in the input solid. Moreover, global issues related\nto body-check such as orientation are efficiently resolved. Many examples from\na pilot implementation illustrate the efficiency and effectiveness of our\nframework."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1407.2074v1", 
    "title": "Visualization of Large Volumetric Multi-Channel Microscopy Data Streams   on Standard PCs", 
    "arxiv-id": "1407.2074v1", 
    "author": "Klaus Hinrichs", 
    "publish": "2014-07-08T13:24:17Z", 
    "summary": "Background: Visualization of multi-channel microscopy data plays a vital role\nin biological research. With the ever-increasing resolution of modern\nmicroscopes the data set size of the scanned specimen grows steadily. On\ncommodity hardware this size easily exceeds the available main memory and the\neven more limited GPU memory. Common volume rendering techniques require the\nentire data set to be present in the GPU memory. Existing out-of-core rendering\napproaches for large volume data sets either are limited to single-channel\nvolumes, or require a computer cluster, or have long preprocessing times.\nResults: We introduce a ray-casting technique for rendering large volumetric\nmulti-channel microscopy data streams on commodity hardware. The volumetric\ndata is managed at different levels of detail by an octree structure. In\ncontrast to previous octree-based techniques, the octree is built incrementally\nand therefore supports streamed microscopy data as well as data set sizes\nexceeding the available main memory. Furthermore, our approach allows the user\nto interact with the partially rendered data set at all stages of the octree\nconstruction. After a detailed description of our method, we present\nperformance results for different multi-channel data sets with a size of up to\n24 GB on a standard desktop PC. Conclusions: Our rendering technique allows\nbiologists to visualize their scanned specimen on their standard desktop\ncomputers without high-end hardware requirements. Furthermore, the user can\ninteract with the data set during the initial loading to explore the already\nloaded parts, change rendering parameters like color maps or adjust clipping\nplanes. Thus, the time of biologists being idle is reduced. Also, streamed data\ncan be visualized to detect and stop flawed scans early during the scan\nprocess."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1407.2089v1", 
    "title": "Visualization and Correction of Automated Segmentation, Tracking and   Lineaging from 5-D Stem Cell Image Sequences", 
    "arxiv-id": "1407.2089v1", 
    "author": "Andrew Cohen", 
    "publish": "2014-07-08T14:03:06Z", 
    "summary": "Results: We present an application that enables the quantitative analysis of\nmultichannel 5-D (x, y, z, t, channel) and large montage confocal fluorescence\nmicroscopy images. The image sequences show stem cells together with blood\nvessels, enabling quantification of the dynamic behaviors of stem cells in\nrelation to their vascular niche, with applications in developmental and cancer\nbiology. Our application automatically segments, tracks, and lineages the image\nsequence data and then allows the user to view and edit the results of\nautomated algorithms in a stereoscopic 3-D window while simultaneously viewing\nthe stem cell lineage tree in a 2-D window. Using the GPU to store and render\nthe image sequence data enables a hybrid computational approach. An\ninference-based approach utilizing user-provided edits to automatically correct\nrelated mistakes executes interactively on the system CPU while the GPU handles\n3-D visualization tasks. Conclusions: By exploiting commodity computer gaming\nhardware, we have developed an application that can be run in the laboratory to\nfacilitate rapid iteration through biological experiments. There is a pressing\nneed for visualization and analysis tools for 5-D live cell image data. We\ncombine accurate unsupervised processes with an intuitive visualization of the\nresults. Our validation interface allows for each data set to be corrected to\n100% accuracy, ensuring that downstream data analysis is accurate and\nverifiable. Our tool is the first to combine all of these aspects, leveraging\nthe synergies obtained by utilizing validation information from stereo\nvisualization to improve the low level image processing tasks."
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1407.3145v1", 
    "title": "SketchBio: A Scientist's 3D Interface for Molecular Modeling and   Animation", 
    "arxiv-id": "1407.3145v1", 
    "author": "Russell M. Taylor II", 
    "publish": "2014-07-11T13:14:16Z", 
    "summary": "Background: Because of the difficulties involved in learning and using 3D\nmodeling and rendering software, many scientists hire programmers or animators\nto create models and animations. This both slows the discovery process and\nprovides opportunities for miscommunication. Working with multiple\ncollaborators, we developed a set of design goals for a tool that would enable\nthem to directly construct models and animations. Results: We present\nSketchBio, a tool that incorporates state-of-the-art bimanual interaction and\ndrop shadows to enable rapid construction of molecular structures and\nanimations. It includes three novel features: crystal by example, pose-mode\nphysics, and spring-based layout that accelerate operations common in the\nformation of molecular models. We present design decisions and their\nconsequences, including cases where iterative design was required to produce\neffective approaches. Conclusions: The design decisions, novel features, and\ninclusion of state-of-the-art techniques enabled SketchBio to meet all of its\ndesign goals. These features and decisions can be incorporated into existing\nand new tools to improve their effectiveness"
},{
    "category": "cs.GR", 
    "doi": "10.1111/cgf.12342", 
    "link": "http://arxiv.org/pdf/1410.1130v1", 
    "title": "Real-time animation of human characters with fuzzy controllers", 
    "arxiv-id": "1410.1130v1", 
    "author": "Rik van de Walle", 
    "publish": "2014-10-05T07:26:44Z", 
    "summary": "The production of animation is a resource intensive process in game\ncompanies. Therefore, techniques to synthesize animations have been developed.\nHowever, these procedural techniques offer limited adaptability by animation\nartists. In order to solve this, a fuzzy neural network model of the animation\nis proposed, where the parameters can be tuned either by machine learning\ntechniques that use motion capture data as training data or by the animation\nartist himself. This paper illustrates how this real time procedural animation\nsystem can be developed, taking the human gait on flat terrain and inclined\nsurfaces as example. Currently, the parametric model is capable of synthesizing\nanimations for various limb sizes and step sizes."
},{
    "category": "cs.GR", 
    "doi": "10.13187/md.2014.2.6", 
    "link": "http://arxiv.org/pdf/1410.3018v1", 
    "title": "A mathematical design and evaluation of Bernstein-Bezier curves' shape   features using the laws of technical aesthetics", 
    "arxiv-id": "1410.3018v1", 
    "author": "Rushan Ziatdinov", 
    "publish": "2014-10-11T18:21:05Z", 
    "summary": "We present some notes on the definition of mathematical design as well as on\nthe methods of mathematical modeling which are used in the process of the\nartistic design of the environment and its components. For the first time in\nthe field of geometric modeling, we perform an aesthetic analysis of planar\nBernstein-Bezier curves from the standpoint of the laws of technical\naesthetics. The shape features of the curve segments' geometry were evaluated\nusing the following criteria: conciseness-integrity, expressiveness,\nproportional consistency, compositional balance, structural organization,\nimagery, rationality, dynamism, scale, flexibility and harmony. In the\nnon-Russian literature, Bernstein-Bezier curves using a monotonic curvature\nfunction (i.e., a class A Bezier curve) are considered to be fair (i.e.,\nbeautiful) curves, but their aesthetic analysis has never been performed. The\naesthetic analysis performed by the authors of this work means that this is no\nlonger the case. To confirm the conclusions of the authors' research, a survey\nof the \"aesthetic appropriateness\" of certain Bernstein-Bezier curve segments\nwas conducted among 240 children, aged 14-17. The results of this survey have\nshown themselves to be in full accordance with the authors' results."
},{
    "category": "cs.GR", 
    "doi": "10.13187/md.2014.2.6", 
    "link": "http://arxiv.org/pdf/1410.4603v1", 
    "title": "Efficient Distance Computation Algorithm between Nearly Intersected   Objects Using Dynamic Pivot Point in Virtual Environment Application", 
    "arxiv-id": "1410.4603v1", 
    "author": "Mohd Harun Abdullah", 
    "publish": "2014-10-16T23:08:04Z", 
    "summary": "Finding nearly accurate distance between two or more nearly intersecting\nthree-dimensional (3D) objects is vital especially for collision determination\nsuch as in virtual surgeon simulation and real-time car crash simulation.\nInstead of performing broad phase collision detection, we need to check for\naccuracy of detection by running narrow phase collision detection. One of the\nimportant elements for narrow phase collision detection is to determine the\nprecise distance between two or more nearly intersecting objects or polygons in\norder to prepare the area for potential colliding. Distance computation plays\nimportant roles in determine the exact point of contact between two or more\nnearly intersecting polygons where the preparation for collision detection is\ndetermined at the earlier stage. In this paper, we describes our current works\nof determining the distance between objects using dynamic pivot point that will\nbe used as reference point to reduce the complexity searching for potential\npoint of contacts. By using Axis-Aligned Bounding Box for each polygon, we\ncalculate a dynamic pivot point that will become our reference point to\ndetermine the potential candidates for distance computation. The test our\nfinding distance will be simplified by using our method instead of performing\nunneeded operations. Our method provides faster solution than the previous\nmethod where it helps to determine the point of contact efficiently and faster\nthan the other method."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4403", 
    "link": "http://arxiv.org/pdf/1411.1906v1", 
    "title": "Footprint-Driven Locomotion Composition", 
    "arxiv-id": "1411.1906v1", 
    "author": "Christos-Nikolaos Anagnostopoulos", 
    "publish": "2014-11-07T13:29:06Z", 
    "summary": "One of the most efficient ways of generating goal-directed walking motions is\nsynthesising the final motion based on footprints. Nevertheless, current\nimplementations have not examined the generation of continuous motion based on\nfootprints, where different behaviours can be generated automatically.\nTherefore, in this paper a flexible approach for footprint-driven locomotion\ncomposition is presented. The presented solution is based on the ability to\ngenerate footprint-driven locomotion, with flexible features like jumping,\nrunning, and stair stepping. In addition, the presented system examines the\nability of generating the desired motion of the character based on predefined\nfootprint patterns that determine which behaviour should be performed. Finally,\nit is examined the generation of transition patterns based on the velocity of\nthe root and the number of footsteps required to achieve the target behaviour\nsmoothly and naturally."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4403", 
    "link": "http://arxiv.org/pdf/1411.3632v1", 
    "title": "Mesh2Fab: Reforming Shapes for Material-specific Fabrication", 
    "arxiv-id": "1411.3632v1", 
    "author": "Niloy J. Mitra", 
    "publish": "2014-11-13T17:50:51Z", 
    "summary": "As humans, we regularly associate shape of an object with its built material.\nIn the context of geometric modeling, however, this interrelation between form\nand material is rarely explored. In this work, we propose a novel data-driven\nreforming (i.e., reshaping) algorithm that adapts an input multi-component\nmodel for a target fabrication material. The algorithm adapts both the part\ngeometry and the inter-part topology of the input shape to better align with\nmaterial specific fabrication requirements. As output, we produce the reshaped\nmodel along with respective part dimensions and inter-part junction\nspecifications. We evaluate our algorithm on a range of man-made models and\ndemonstrate non-trivial model reshaping examples focusing only on metal and\nwooden materials. We also appraise the output of our algorithm using a user\nstudy."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4403", 
    "link": "http://arxiv.org/pdf/1411.5993v1", 
    "title": "Reverse Engineering Point Clouds to Fit Tensor Product B-Spline Surfaces   by Blending Local Fits", 
    "arxiv-id": "1411.5993v1", 
    "author": "Elaine Cohen", 
    "publish": "2014-11-21T19:37:27Z", 
    "summary": "Being able to reverse engineer from point cloud data to obtain 3D models is\nimportant in modeling. As our main contribution, we present a new method to\nobtain a tensor product B-spline representation from point cloud data by\nfitting surfaces to appropriately segmented data. By blending multiple local\nfits our method is more efficient than existing techniques, with the ability to\ndeal with more detail by efficiently introducing a high number of knots.\nFurther point cloud data obtained by digitizing 3D data, typically presents\nmany associated complications like noise and missing data. As our second\ncontribution, we propose an end-to-end framework for smoothing, hole filling,\nparameterization, knot selection and B-spline fitting that addresses these\nissues, works robustly with large irregularly shaped data containing holes and\nis straightforward to implement."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4403", 
    "link": "http://arxiv.org/pdf/1412.1330v1", 
    "title": "Ceramics Fragments Digitization by Photogrammetry, Reconstructions and   Applications", 
    "arxiv-id": "1412.1330v1", 
    "author": "Val\u00e9rie Gouranton", 
    "publish": "2014-12-03T14:04:48Z", 
    "summary": "This paper presents an application of photogrammetry on ceramic fragments\nfrom two excavation sites located north-west of France. The restitution by\nphotogrammetry of these different fragments allowed reconstructions of the\npotteries in their original state or at least to get to as close as possible.\nWe used the 3D reconstructions to compute some metrics and to generate a\npresentation support by using a 3D printer. This work is based on affordable\ntools and illustrates how 3D technologies can be quite easily integrated in\narchaeology process with limited financial resources. 1. INTRODUCTION Today,\nphotogrammetry and 3D modelling are an integral part of the methods used in\narcheology and heritage management. They provide answers to scientific needs in\nthe fields of conservation, preservation, restoration and mediation of\narchitectural, archaeological and cultural heritage [2] [6] [7] [9].\nPhotogrammetry on ceramic fragments was one of the first applications\ncontemporary of the development of this technique applied in the archaeological\ncommunity [3]. More recently and due to its democratization, it was applied\nmore generally to artifacts [5]. Finally joined today by the rise of 3D\nprinting [8] [10], it can restore fragmented artifacts [1] [12]. These examples\ntarget one or several particular objects and use different types of equipment\nthat can be expensive. These aspects can put off uninitiated archaeologists. So\nit would be appropriate to see if these techniques could be generalized to a\nwhole class of geometrically simple and common artifacts, such as ceramics.\nFrom these observations, associated to ceramics specialists with fragments of\nbroken ceramics, we aimed at arranging different tools and methods, including\nphotogrammetry, to explore opportunities for a cheap and attainable\nreconstruction methodology and its possible applications. Our first objective\nwas to establish a protocol for scanning fragments with photogrammetry, and for\nreconstruction of original ceramics. We used the digital reconstitutions of the\nceramics we got following our process to calculate some metrics and to design\nand 3D print a display for the remaining fragments of one pottery."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cam.2015.10.005", 
    "link": "http://arxiv.org/pdf/1412.3841v6", 
    "title": "Merging of B\u00e9zier curves with box constraints", 
    "arxiv-id": "1412.3841v6", 
    "author": "Pawe\u0142 Wo\u017any", 
    "publish": "2014-12-11T22:11:44Z", 
    "summary": "In this paper, we present a novel approach to the problem of merging of\nB\\'ezier curves with respect to the $L_2$-norm. We give illustrative examples\nto show that the solution of the conventional merging problem may not be\nsuitable for further modification and applications. As in the case of the\ndegree reduction problem, we apply the so-called restricted area approach --\nproposed recently in (P. Gospodarczyk, Computer-Aided Design 62 (2015),\n143--151) -- to avoid certain defects and make the resulting curve more useful.\nA method of solving the new problem is based on box-constrained quadratic\nprogramming approach."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12650-014-0206-5", 
    "link": "http://arxiv.org/pdf/1412.7780v1", 
    "title": "Interactive Visual Exploration of Halos in Large Scale Cosmology   Simulation", 
    "arxiv-id": "1412.7780v1", 
    "author": "Xuebin Chi", 
    "publish": "2014-12-25T03:20:34Z", 
    "summary": "Halo is one of the most important basic elements in cosmology simulation,\nwhich merges from small clumps to ever larger objects. The processes of the\nbirth and merging of the halos play a fundamental role in studying the\nevolution of large scale cosmological structures. In this paper, a visual\nanalysis system is developed to interactively identify and explore the\nevolution histories of thousands of halos. In this system, an intelligent\nstructure-aware selection method in What You See Is What You Get manner is\ndesigned to efficiently define the interesting region in 3D space with 2D\nhand-drawn lasso input. Then the exact information of halos within this 3D\nregion is identified by data mining in the merger tree files. To avoid visual\nclutter, all the halos are projected in 2D space with a MDS method. Through the\nlinked view of 3D View and 2D graph, Users can interactively explore these\nhalos, including the tracing path and evolution history tree."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12650-014-0206-5", 
    "link": "http://arxiv.org/pdf/1502.02139v1", 
    "title": "Marching Surfaces: Isosurface Approximation using G$^1$ Multi-Sided   Surfaces", 
    "arxiv-id": "1502.02139v1", 
    "author": "Alyn Rockwood", 
    "publish": "2015-02-07T13:13:51Z", 
    "summary": "Marching surfaces is a method for isosurface extraction and approximation\nbased on a $G^1$ multi-sided patch interpolation scheme. Given a 3D grid of\nscalar values, an underlying curve network is formed using second order\ninformation and cubic Hermite splines. Circular arc fitting defines the tangent\nvectors for the Hermite curves at specified isovalues. Once the boundary curve\nnetwork is formed, a loop of curves is determined for each grid cell and then\ninterpolated with multi-sided surface patches, which are $G^1$ continuous at\nthe joins. The data economy of the method and its continuity preserving\nproperties provide an effective compression scheme, ideal for indirect volume\nrendering on mobile devices, or collaborating on the Internet, while enhancing\nvisual fidelity. The use of multi-sided patches enables a more natural way to\napproximate the isosurfaces than using a fixed number of sides or polygons as\nis proposed in the literature. This assertion is supported with comparisons to\nthe traditional Marching Cubes algorithm and other $G^1$ methods."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12650-014-0206-5", 
    "link": "http://arxiv.org/pdf/1502.02961v1", 
    "title": "Avatar-independent scripting for real-time gesture animation", 
    "arxiv-id": "1502.02961v1", 
    "author": "Richard Kennaway", 
    "publish": "2015-02-10T16:03:37Z", 
    "summary": "When animation of a humanoid figure is to be generated at run-time, instead\nof by replaying pre-composed motion clips, some method is required of\nspecifying the avatar's movements in a form from which the required motion data\ncan be automatically generated. This form must be of a more abstract nature\nthan raw motion data: ideally, it should be independent of the particular\navatar's proportions, and both writable by hand and suitable for automatic\ngeneration from higher-level descriptions of the required actions.\n  We describe here the development and implementation of such a scripting\nlanguage for the particular area of sign languages of the deaf, called SiGML\n(Signing Gesture Markup Language), based on the existing HamNoSys notation for\nsign languages.\n  We conclude by suggesting how this work may be extended to more general\nanimation for interactive virtual reality applications."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12650-014-0206-5", 
    "link": "http://arxiv.org/pdf/1502.04232v1", 
    "title": "Sketch-based Shape Retrieval using Pyramid-of-Parts", 
    "arxiv-id": "1502.04232v1", 
    "author": "Hongbo Fu", 
    "publish": "2015-02-14T17:42:02Z", 
    "summary": "We present a multi-scale approach to sketch-based shape retrieval. It is\nbased on a novel multi-scale shape descriptor called Pyramidof- Parts, which\nencodes the features and spatial relationship of the semantic parts of query\nsketches. The same descriptor can also be used to represent 2D projected views\nof 3D shapes, allowing effective matching of query sketches with 3D shapes\nacross multiple scales. Experimental results show that the proposed method\noutperforms the state-of-the-art method, whether the sketch segmentation\ninformation is obtained manually or automatically by considering each stroke as\na semantic part."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5102", 
    "link": "http://arxiv.org/pdf/1502.04268v1", 
    "title": "Relative Squared Distances to a Conic Berserkless 8-Connected Midpoint   Algorithm", 
    "arxiv-id": "1502.04268v1", 
    "author": "Valere Huypens", 
    "publish": "2015-02-15T01:22:16Z", 
    "summary": "The midpoint method or technique is a measurement and as each measurement it\nhas a tolerance, but worst of all it can be invalid, called Out-of-Control or\nOoC. The core of all midpoint methods is the accurate measurement of the\ndifference of the squared distances of two points to the polar of their\nmidpoint with respect to the conic. When this measurement is valid, it also\nmeasures the difference of the squared distances of these points to the conic,\nalthough it may be inaccurate, called Out-of-Accuracy or OoA. The primary\ncondition is the necessary and sufficient condition that a measurement is\nvalid. It is comletely new and it can be checked ultra fast and before the\nactual measurement starts. Modeling an incremental algorithm, shows that the\ncurve must be subdivided into piecewise monotonic sections, the start point\nmust be optimal, and it explains that the 2D-incremental method can find,\nlocally, the global Least Square Distance. Locally means that there are at most\nthree candidate points for a given monotonic direction; therefore the\n2D-midpoint method has, locally, at most three measurements. When all the\npossible measurements are invalid, the midpoint method cannot be applied, and\nin that case the ultra fast OoC-rule selects the candidate point. This\nguarantees, for the first time, a 100% stable, ultra-fast, berserkless midpoint\nalgorithm, which can be easily transformed to hardware."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1502.06419v1", 
    "title": "Analysis of Design Principles and Requirements for Procedural Rigging of   Bipeds and Quadrupeds Characters with Custom Manipulators for Animation", 
    "arxiv-id": "1502.06419v1", 
    "author": "Nadeem Mahmood", 
    "publish": "2015-02-14T11:29:26Z", 
    "summary": "Character rigging is a process of endowing a character with a set of custom\nmanipulators and controls making it easy to animate by the animators. These\ncontrols consist of simple joints, handles, or even separate character\nselection windows.This research paper present an automated rigging system for\nquadruped characters with custom controls and manipulators for animation.The\nfull character rigging mechanism is procedurally driven based on various\nprinciples and requirements used by the riggers and animators. The automation\nis achieved initially by creating widgets according to the character type.\nThese widgets then can be customized by the rigger according to the character\nshape, height and proportion. Then joint locations for each body parts are\ncalculated and widgets are replaced programmatically.Finally a complete and\nfully operational procedurally generated character control rig is created and\nattached with the underlying skeletal joints. The functionality and feasibility\nof the rig was analyzed from various source of actual character motion and a\nrequirements criterion was met. The final rigged character provides an\nefficient and easy to manipulate control rig with no lagging and at high frame\nrate."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1502.06686v1", 
    "title": "Data-Driven Shape Analysis and Processing", 
    "arxiv-id": "1502.06686v1", 
    "author": "Evangelos Kalogerakis", 
    "publish": "2015-02-24T04:30:43Z", 
    "summary": "Data-driven methods play an increasingly important role in discovering\ngeometric, structural, and semantic relationships between 3D shapes in\ncollections, and applying this analysis to support intelligent modeling,\nediting, and visualization of geometric data. In contrast to traditional\napproaches, a key feature of data-driven approaches is that they aggregate\ninformation from a collection of shapes to improve the analysis and processing\nof individual shapes. In addition, they are able to learn models that reason\nabout properties and relationships of shapes without relying on hard-coded\nrules or explicitly programmed instructions. We provide an overview of the main\nconcepts and components of these techniques, and discuss their application to\nshape classification, segmentation, matching, reconstruction, modeling and\nexploration, as well as scene analysis and synthesis, through reviewing the\nliterature and relating the existing works with both qualitative and numerical\ncomparisons. We conclude our report with ideas that can inspire future research\nin data-driven shape analysis and processing."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1503.00202v1", 
    "title": "On Integrating Information Visualization Techniques into Data Mining: A   Review", 
    "arxiv-id": "1503.00202v1", 
    "author": "Keqian Li", 
    "publish": "2015-03-01T01:10:03Z", 
    "summary": "The exploding growth of digital data in the information era and its\nimmeasurable potential value has called for different types of data-driven\ntechniques to exploit its value for further applications. Information\nvisualization and data mining are two research field with such goal. While the\ntwo communities advocates different approaches of problem solving, the vision\nof combining the sophisticated algorithmic techniques from data mining as well\nas the intuitivity and interactivity of information visualization is tempting.\nIn this paper, we attempt to survey recent researches and real world systems\nintegrating the wisdom in two fields towards more effective and efficient data\nanalytics. More specifically, we study the intersection from a data mining\npoint of view, explore how information visualization can be used to complement\nand improve different stages of data mining through established theories for\noptimized visual presentation as well as practical toolsets for rapid\ndevelopment. We organize the survey by identifying three main stages of typical\nprocess of data mining, the preliminary analysis of data, the model\nconstruction, as well as the model evaluation, and study how each stage can\nbenefit from information visualization."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1503.05787v2", 
    "title": "Interactive Illustrative Line Styles and Line Style Transfer Functions   for Flow Visualization", 
    "arxiv-id": "1503.05787v2", 
    "author": "Tobias Isenberg", 
    "publish": "2015-03-19T14:53:55Z", 
    "summary": "We present a flexible illustrative line style model for the visualization of\nstreamline data. Our model partitions view-oriented line strips into parallel\nbands whose basic visual properties can be controlled independently. We thus\nextend previous line stylization techniques specifically for visualization\npurposes by allowing the parametrization of these bands based on the local line\ndata attributes. Moreover, our approach supports emphasis and abstraction by\nintroducing line style transfer functions that map local line attribute values\nto complete line styles. With a flexible GPU implementation of this line style\nmodel we enable the interactive exploration of visual representations of\nstreamlines. We demonstrate the effectiveness of our model by applying it to 3D\nflow field datasets."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1504.02687v1", 
    "title": "3D Density Histograms for Criteria-driven Edge Bundling", 
    "arxiv-id": "1504.02687v1", 
    "author": "Daniel C. Moura", 
    "publish": "2015-04-10T14:13:39Z", 
    "summary": "This paper presents a graph bundling algorithm that agglomerates edges taking\ninto account both spatial proximity as well as user-defined criteria in order\nto reveal patterns that were not perceivable with previous bundling techniques.\nEach edge belongs to a group that may either be an input of the problem or\nfound by clustering one or more edge properties such as origin, destination,\norientation, length or domain-specific properties. Bundling is driven by a\nstack of density maps, with each map capturing both the edge density of a given\ngroup as well as interactions with edges from other groups. Density maps are\nefficiently calculated by smoothing 2D histograms of edge occurrence using\nrepeated averaging filters based on integral images.\n  A CPU implementation of the algorithm is tested on several graphs, and\ndifferent grouping criteria are used to illustrate how the proposed technique\ncan render different visualizations of the same data. Bundling performance is\nmuch higher than on previous approaches, being particularly noticeable on large\ngraphs, with millions of edges being bundled in seconds."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1504.02744v1", 
    "title": "Real-time Tool for Affine Transformations of Two Dimensional IFS   Fractals", 
    "arxiv-id": "1504.02744v1", 
    "author": "Marija Shuminoska", 
    "publish": "2015-04-10T17:42:14Z", 
    "summary": "This work introduces a novel tool for interactive, real-time transformations\nof two dimensional IFS fractals. We assign barycentric coordinates (relative to\nan arbitrary affine basis of $\\mathbb{R}^2$) to the points that constitute the\nimage of a fractal. The tool uses some of the nice properties of the\nbarycentric coordinates, enabling any affine transformation of the basis, done\nby click-and-drag, to be immediately followed by the same affine transformation\nof the IFS fractal attractor. In order to have a better control over the\nfractal, as affine basis we use a kind of minimal simplex that contains the\nattractor. We give theoretical grounds of the tool and then the software\napplication."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5104", 
    "link": "http://arxiv.org/pdf/1504.03151v1", 
    "title": "Massively Parallel Ray Tracing Algorithm Using GPU", 
    "arxiv-id": "1504.03151v1", 
    "author": "Xiang Huang", 
    "publish": "2015-04-13T12:39:50Z", 
    "summary": "Ray tracing is a technique for generating an image by tracing the path of\nlight through pixels in an image plane and simulating the effects of\nhigh-quality global illumination at a heavy computational cost. Because of the\nhigh computation complexity, it can't reach the requirement of real-time\nrendering. The emergence of many-core architectures, makes it possible to\nreduce significantly the running time of ray tracing algorithm by employing the\npowerful ability of floating point computation. In this paper, a new GPU\nimplementation and optimization of the ray tracing to accelerate the rendering\nprocess is presented."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1504.04565v1", 
    "title": "Real-time correction of panoramic images using hyperbolic M\u00f6bius   transformations", 
    "arxiv-id": "1504.04565v1", 
    "author": "Leonardo Sacht", 
    "publish": "2015-04-17T16:45:35Z", 
    "summary": "Wide-angle images gained a huge popularity in the last years due to the\ndevelopment of computational photography and imaging technological advances.\nThey present the information of a scene in a way which is more natural for the\nhuman eye but, on the other hand, they introduce artifacts such as bent lines.\nThese artifacts become more and more unnatural as the field of view increases.\n  In this work, we present a technique aimed to improve the perceptual quality\nof panorama visualization. The main ingredients of our approach are, on one\nhand, considering the viewing sphere as a Riemann sphere, what makes natural\nthe application of M\\\"obius (complex) transformations to the input image, and,\non the other hand, a projection scheme which changes in function of the field\nof view used.\n  We also introduce an implementation of our method, compare it against images\nproduced with other methods and show that the transformations can be done in\nreal-time, which makes our technique very appealing for new settings, as well\nas for existing interactive panorama applications."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.00073v1", 
    "title": "Bijective Deformations in $\\mathbb{R}^n$ via Integral Curve Coordinates", 
    "arxiv-id": "1505.00073v1", 
    "author": "Yotam Gingold", 
    "publish": "2015-05-01T02:47:12Z", 
    "summary": "We introduce Integral Curve Coordinates, which identify each point in a\nbounded domain with a parameter along an integral curve of the gradient of a\nfunction $f$ on that domain; suitable functions have exactly one critical\npoint, a maximum, in the domain, and the gradient of the function on the\nboundary points inward. Because every integral curve intersects the boundary\nexactly once, Integral Curve Coordinates provide a natural bijective mapping\nfrom one domain to another given a bijection of the boundary. Our approach can\nbe applied to shapes in any dimension, provided that the boundary of the shape\n(or cage) is topologically equivalent to an $n$-sphere. We present a simple\nalgorithm for generating a suitable function space for $f$ in any dimension. We\ndemonstrate our approach in 2D and describe a practical (simple and robust)\nalgorithm for tracing integral curves on a (piecewise-linear) triangulated\nregular grid."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.01810v5", 
    "title": "B$\\acute{e}$zier curves based on Lupa\u015f $(p,q)$-analogue of Bernstein   polynomials in CAGD", 
    "arxiv-id": "1505.01810v5", 
    "author": "D. K. Lobiyal", 
    "publish": "2015-05-07T18:36:37Z", 
    "summary": "In this paper, we use the blending functions of Lupa\\c{s} type (rational)\n$(p,q)$-Bernstein operators based on $(p,q)$-integers for construction of\nLupa\\c{s} $(p,q)$-B$\\acute{e}$zier curves (rational curves) and surfaces\n(rational surfaces) with shape parameters. We study the nature of degree\nelevation and degree reduction for Lupa\\c{s} $(p,q)$-B$\\acute{e}$zier Bernstein\nfunctions. Parametric curves are represented using Lupa\\c{s} $(p,q)$-Bernstein\nbasis. We introduce affine de Casteljau algorithm for Lupa\\c{s} type\n$(p,q)$-Bernstein B$\\acute{e}$zier curves. The new curves have some properties\nsimilar to $q$-B$\\acute{e}$zier curves. Moreover, we construct the\ncorresponding tensor product surfaces over the rectangular domain $(u, v) \\in\n[0, 1] \\times [0, 1] $ depending on four parameters. We also study the de\nCasteljau algorithm and degree evaluation properties of the surfaces for these\ngeneralization over the rectangular domain. We get $q$-B$\\acute{e}$zier\nsurfaces for $(u, v) \\in [0, 1] \\times [0, 1] $ when we set the parameter\n$p_1=p_2=1.$ In comparison to $q$-B$\\acute{e}$zier curves and surfaces based on\nLupa\\c{s} $q$-Bernstein polynomials, our generalization gives us more\nflexibility in controlling the shapes of curves and surfaces.\n  We also show that the $(p,q)$-analogue of Lupa\\c{s} Bernstein operator\nsequence $L^{n}_{p_n,q_n}(f,x)$ converges uniformly to $f(x)\\in C[0,1]$ if and\nonly if $0<q_n<p_n\\leq1$ such that $\\lim\\limits_{n\\to\\infty} q_n=1, $\n$\\lim\\limits_{n\\to\\infty} p_n=1$ and $\\lim\\limits_{n\\to\\infty}p_n^n=a,$\n$\\lim\\limits_{n\\to\\infty}q_n^n=b$ with $0<a,b\\leq1.$ On the other hand, for any\n$p>0$ fixed and $p \\neq 1,$ the sequence $L^{n}_{p,q}(f,x)$ converges uniformly\nto $f(x)~ \\in C[0,1]$ if and only if $f(x)=ax+b$ for some $a, b \\in\n\\mathbb{R}.$"
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.03615v1", 
    "title": "A Connectivity-Aware Multi-level Finite-Element System for Solving   Laplace-Beltrami Equations", 
    "arxiv-id": "1505.03615v1", 
    "author": "Michael Kazhdan", 
    "publish": "2015-05-14T04:08:53Z", 
    "summary": "Recent work on octree-based finite-element systems has developed a multigrid\nsolver for Poisson equations on meshes. While the idea of defining a regularly\nindexed function space has been successfully used in a number of applications,\nit has also been noted that the richness of the function space is limited\nbecause the function values can be coupled across locally disconnected regions.\nIn this work, we show how to enrich the function space by introducing functions\nthat resolve the coupling while still preserving the nesting hierarchy that\nsupports multigrid. A spectral analysis reveals the superior quality of the\nresulting Laplace-Beltrami operator and applications to surface flow\ndemonstrate that our new solver more efficiently converges to the correct\nsolution."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.03977v2", 
    "title": "A wide diversity of 3D surfaces Generator using a new implicit function", 
    "arxiv-id": "1505.03977v2", 
    "author": "Omar Bouattane", 
    "publish": "2015-05-15T07:38:46Z", 
    "summary": "We present in this paper a new family of implicit function for synthesizing a\nwide variety of 3D surfaces. The basis of this family consists of the usual\nfunctions that are: the function rectangular pulses, the function saw-tooth\npulses, the function of triangular pulses, the staircase function and the power\nfunction. By combining these common functions, named constituent functions, in\none implicit function and by varying some parameters of this function we can\nsynthesize a wide variety of 3D surfaces with the possibility to set their\ndeformations."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.06022v1", 
    "title": "Implementing a Photorealistic Rendering System using GLSL", 
    "arxiv-id": "1505.06022v1", 
    "author": "Toshiya Hachisuka", 
    "publish": "2015-05-22T10:41:45Z", 
    "summary": "Ray tracing on GPUs is becoming quite common these days. There are many\npublicly available documents on how to implement basic ray tracing on GPUs for\nspheres and implicit surfaces. We even have some general frameworks for ray\ntracing on GPUs. We however hardly find details on how to implement more\ncomplex ray tracing algorithms themselves that are commonly used for\nphotorealistic rendering. This paper explains an implementation of a\nstand-alone rendering system on GPUs which supports the bounding volume\nhierarchy and stochastic progressive photon mapping. The key characteristic of\nthe system is that it uses only GLSL shaders without relying on any platform\ndependent feature. The system can thus run on many platforms that support\nOpenGL, making photorealistic rendering on GPUs widely accessible. This paper\nalso sketches practical ideas for stackless traversal and pseudorandom number\ngeneration which both fit well with the limited system configuration."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11554-015-0502-x", 
    "link": "http://arxiv.org/pdf/1505.07079v2", 
    "title": "A survey on Information Visualization in light of Vision and Cognitive   sciences", 
    "arxiv-id": "1505.07079v2", 
    "author": "Agma Traina", 
    "publish": "2015-05-26T19:03:54Z", 
    "summary": "Information Visualization techniques are built on a context with many factors\nrelated to both vision and cognition, making it difficult to draw a clear\npicture of how data visually turns into comprehension. In the intent of\npromoting a better picture, here, we survey concepts on vision, cognition, and\nInformation Visualization organized in a theorization named Visual Expression\nProcess. Our theorization organizes the basis of visualization techniques with\na reduced level of complexity; still, it is complete enough to foster\ndiscussions related to design and analytical tasks. Our work introduces the\nfollowing contributions: (1) a Theoretical compilation of vision, cognition,\nand Information Visualization; (2) Discussions supported by vast literature;\nand (3) Reflections on visual-cognitive aspects concerning use and design. We\nexpect our contributions will provide further clarification about how users and\ndesigners think about InfoVis, leveraging the potential of systems and\ntechniques."
},{
    "category": "cs.GR", 
    "doi": "10.1057/palgrave.ivs.9500161", 
    "link": "http://arxiv.org/pdf/1505.07804v1", 
    "title": "The Spatial-Perceptual Design Space: a new comprehension for Data   Visualization", 
    "arxiv-id": "1505.07804v1", 
    "author": "Caetano Traina Jr", 
    "publish": "2015-05-28T19:14:44Z", 
    "summary": "We revisit the design space of visualizations aiming at identifying and\nrelating its components. In this sense, we establish a model to examine the\nprocess through which visualizations become expressive for users. This model\nhas leaded us to a taxonomy oriented to the human visual perception, a\nconceptualization that provides natural criteria in order to delineate a novel\nunderstanding for the visualization design space. The new organization of\nconcepts that we introduce is our main contribution: a grammar for the\nvisualization design based on the review of former works and of classical and\nstate-of-the-art techniques. Like so, the paper is presented as a survey whose\nstructure introduces a new conceptualization for the space of techniques\nconcerning visual analysis."
},{
    "category": "cs.GR", 
    "doi": "10.1057/palgrave.ivs.9500161", 
    "link": "http://arxiv.org/pdf/1506.00021v1", 
    "title": "Variance Analysis for Monte Carlo Integration: A   Representation-Theoretic Perspective", 
    "arxiv-id": "1506.00021v1", 
    "author": "Victor Ostromoukhov", 
    "publish": "2015-05-29T20:27:11Z", 
    "summary": "In this report, we revisit the work of Pilleboue et al. [2015], providing a\nrepresentation-theoretic derivation of the closed-form expression for the\nexpected value and variance in homogeneous Monte Carlo integration. We show\nthat the results obtained for the variance estimation of Monte Carlo\nintegration on the torus, the sphere, and Euclidean space can be formulated as\nspecific instances of a more general theory. We review the related\nrepresentation theory and show how it can be used to derive a closed-form\nsolution."
},{
    "category": "cs.GR", 
    "doi": "10.1057/palgrave.ivs.9500161", 
    "link": "http://arxiv.org/pdf/1506.02079v1", 
    "title": "Gradient-Domain Fusion for Color Correction in Large EM Image Stacks", 
    "arxiv-id": "1506.02079v1", 
    "author": "Randal Burns", 
    "publish": "2015-06-05T22:35:31Z", 
    "summary": "We propose a new gradient-domain technique for processing registered EM image\nstacks to remove inter-image discontinuities while preserving intra-image\ndetail. To this end, we process the image stack by first performing anisotropic\nsmoothing along the slice axis and then solving a Poisson equation within each\nslice to re-introduce the detail. The final image stack is continuous across\nthe slice axis and maintains sharp details within each slice. Adapting existing\nout-of-core techniques for solving the linear system, we describe a parallel\nalgorithm with time complexity that is linear in the size of the data and space\ncomplexity that is sub-linear, allowing us to process datasets as large as five\nteravoxels with a 600 MB memory footprint."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.02400v2", 
    "title": "Pushing the Limits of 3D Color Printing: Error Diffusion with   Translucent Materials", 
    "arxiv-id": "1506.02400v2", 
    "author": "Philipp Urban", 
    "publish": "2015-06-08T08:47:52Z", 
    "summary": "Accurate color reproduction is important in many applications of 3D printing,\nfrom design prototypes to 3D color copies or portraits. Although full color is\navailable via other technologies, multi-jet printers have greater potential for\ngraphical 3D printing, in terms of reproducing complex appearance properties.\nHowever, to date these printers cannot produce full color, and doing so poses\nsubstantial technical challenges, from the shear amount of data to the\ntranslucency of the available color materials. In this paper, we propose an\nerror diffusion halftoning approach to achieve full color with multi-jet\nprinters, which operates on multiple isosurfaces or layers within the object.\nWe propose a novel traversal algorithm for voxel surfaces, which allows the\ntransfer of existing error diffusion algorithms from 2D printing. The resulting\nprints faithfully reproduce colors, color gradients and fine-scale details."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.02976v1", 
    "title": "Reviewing Data Visualization: an Analytical Taxonomical Study", 
    "arxiv-id": "1506.02976v1", 
    "author": "Caetano Traina Jr", 
    "publish": "2015-06-09T16:08:19Z", 
    "summary": "This paper presents an analytical taxonomy that can suitably describe, rather\nthan simply classify, techniques for data presentation. Unlike previous works,\nwe do not consider particular aspects of visualization techniques, but their\nmechanisms and foundational vision perception. Instead of just adjusting\nvisualization research to a classification system, our aim is to better\nunderstand its process. For doing so, we depart from elementary concepts to\nreach a model that can describe how visualization techniques work and how they\nconvey meaning."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.04480v2", 
    "title": "A Clustering Based Approach for Realistic and Efficient Data-Driven   Crowd Simulation", 
    "arxiv-id": "1506.04480v2", 
    "author": "Mingbi Zhao", 
    "publish": "2015-06-15T05:29:06Z", 
    "summary": "In this paper, we present a data-driven approach to generate realistic\nsteering behaviors for virtual crowds in crowd simulation. We take advantage of\nboth rule-based models and data-driven models by applying the interaction\npatterns discovered from crowd videos. Unlike existing example-based models in\nwhich current states are matched to states extracted from crowd videos\ndirectly, our approach adopts a hierarchical mechanism to generate the steering\nbehaviors of agents. First, each agent is classified into one of the\ninteraction patterns that are automatically discovered from crowd video before\nsimulation. Then the most matched action is selected from the associated\ninteraction pattern to generate the steering behaviors of the agent. By doing\nso, agents can avoid performing a simple state matching as in the traditional\nexample-based approaches, and can perform a wider variety of steering behaviors\nas well as mimic the cognitive process of pedestrians. Simulation results on\nscenarios with different crowd densities and main motion directions demonstrate\nthat our approach performs better than two state-of-the-art simulation models,\nin terms of prediction accuracy. Besides, our approach is efficient enough to\nrun at interactive rates in real time simulation."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.04806v3", 
    "title": "A Novel Semantics and Feature Preserving Perspective for Content Aware   Image Retargeting", 
    "arxiv-id": "1506.04806v3", 
    "author": "Pier Luigi Dragotti", 
    "publish": "2015-06-16T00:53:09Z", 
    "summary": "There is an increasing requirement for efficient image retargeting techniques\nto adapt the content to various forms of digital media. With rapid growth of\nmobile communications and dynamic web page layouts, one often needs to resize\nthe media content to adapt to the desired display sizes. For various layouts of\nweb pages and typically small sizes of handheld portable devices, the\nimportance in the original image content gets obfuscated after resizing it with\nthe approach of uniform scaling. Thus, there occurs a need for resizing the\nimages in a content aware manner which can automatically discard irrelevant\ninformation from the image and present the salient features with more\nmagnitude. There have been proposed some image retargeting techniques keeping\nin mind the content awareness of the input image. However, these techniques\nfail to prove globally effective for various kinds of images and desired sizes.\nThe major problem is the inefficiency of these algorithms to process these\nimages with minimal visual distortion while also retaining the meaning conveyed\nfrom the image. In this dissertation, we present a novel perspective for\ncontent aware image retargeting, which is well implementable in real time. We\nintroduce a novel method of analysing semantic information within the input\nimage while also maintaining the important and visually significant features.\nWe present the various nuances of our algorithm mathematically and logically,\nand show that the results prove better than the state-of-the-art techniques."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.06098v1", 
    "title": "The 12 prophets dataset", 
    "arxiv-id": "1506.06098v1", 
    "author": "R. Assis", 
    "publish": "2015-06-19T17:57:38Z", 
    "summary": "The \"Ajeijadinho 3D\" project is an initiative supported by the University of\nS\\~ao Paulo (Museum of Science and Dean of Culture and Extension), which\ninvolves the 3D digitization of art works of Brazilian sculptor Antonio\nFrancisco Lisboa, better known as Aleijadinho. The project made use of advanced\nacquisition and processing of 3D meshes for preservation and dissemination of\nthe cultural heritage. The dissemination occurs through a Web portal, so that\nthe population has the opportunity to meet the art works in detail using 3D\nvisualization and interaction. The portal address is\nhttp://www.aleijadinho3d.icmc.usp.br. The 3D acquisitions were conducted over a\nweek at the end of July 2013 in the cities of Ouro Preto, MG, Brazil and\nCongonhas do Campo, MG, Brazil. The scanning was done with a special equipment\nsupplied by company Leica Geosystems, which allowed the work to take place at\ndistances between 10 and 30 meters, defining a non-invasive procedure,\nsimplified logistics, and without the need for preparation or isolation of the\nsites. In Ouro Preto, we digitized the churches of Francisco of Assis, Our Lady\nof Carmo, and Our Lady of Mercy; in Congonhas do Campo we scanned the entire\nSanctuary of Bom Jesus de Matosinhos and his 12 prophets. Once scanned, the art\nworks went through a long process of preparation, which required careful\nhandling of meshes done by experts from the University of S\\~ao Paulo in\npartnership with company Imprimate."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.06636v2", 
    "title": "3D Geometric Analysis of Tubular Objects based on Surface Normal   Accumulation", 
    "arxiv-id": "1506.06636v2", 
    "author": "Jacques-Olivier Lachaud", 
    "publish": "2015-06-22T14:46:58Z", 
    "summary": "This paper proposes a simple and efficient method for the reconstruction and\nextraction of geometric parameters from 3D tubular objects. Our method\nconstructs an image that accumulates surface normal information, then peaks\nwithin this image are located by tracking. Finally, the positions of these are\noptimized to lie precisely on the tubular shape centerline. This method is very\nversatile, and is able to process various input data types like full or partial\nmesh acquired from 3D laser scans, 3D height map or discrete volumetric images.\nThe proposed algorithm is simple to implement, contains few parameters and can\nbe computed in linear time with respect to the number of surface faces. Since\nthe extracted tube centerline is accurate, we are able to decompose the tube\ninto rectilinear parts and torus-like parts. This is done with a new linear\ntime 3D torus detection algorithm, which follows the same principle of a\nprevious work on 2D arc circle recognition. Detailed experiments show the\nversatility, accuracy and robustness of our new method."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.06855v1", 
    "title": "Modeling and Correspondence of Topologically Complex 3D Shapes", 
    "arxiv-id": "1506.06855v1", 
    "author": "Ibraheem Alhashim", 
    "publish": "2015-06-23T04:40:30Z", 
    "summary": "3D shape creation and modeling remains a challenging task especially for\nnovice users. Many methods in the field of computer graphics have been proposed\nto automate the often repetitive and precise operations needed during the\nmodeling of detailed shapes. This report surveys different approaches of shape\nmodeling and correspondence especially for shapes exhibiting topological\ncomplexity. We focus on methods designed to help generate or process shapes\nwith large number of interconnected components often found in man-made shapes.\nWe first discuss a variety of modeling techniques, that leverage existing\nshapes, in easy to use creative modeling systems. We then discuss possible\ncorrespondence strategies for topologically different shapes as it is a\nrequirement for such systems. Finally, we look at different shape\nrepresentations and tools that facilitate the modification of shape topology\nand we focus on those particularly useful in free-form 3D modeling."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.06968v1", 
    "title": "A Survey on Distributed Visualization Techniques over Clusters of   Personal Computers", 
    "arxiv-id": "1506.06968v1", 
    "author": "Agma Traina", 
    "publish": "2015-06-23T12:22:32Z", 
    "summary": "In the last years, Distributed Visualization over Personal Computer (PC)\nclusters has become important for research and industrial communities. They\nhave made large-scale visualizations practical and more accessible. In this\nwork we survey Distributed Visualization techniques aiming at compiling last\ndecade's literature on the use of PC clusters as suitable alternatives to\nhigh-end workstations. We review the topic by defining basic concepts,\nenumerating system requirements and implementation challenges, and presenting\nup-to-date methodologies. Our work fulfills the needs of newcomers and seasoned\nprofessionals as an introductory compilation at the same time that it can help\nexperienced personnel by organizing ideas."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2832905", 
    "link": "http://arxiv.org/pdf/1506.07577v3", 
    "title": "Ebb: A DSL for Physical Simulation on CPUs and GPUs", 
    "arxiv-id": "1506.07577v3", 
    "author": "Pat Hanrahan", 
    "publish": "2015-06-24T22:32:41Z", 
    "summary": "Designing programming environments for physical simulation is challenging\nbecause simulations rely on diverse algorithms and geometric domains. These\nchallenges are compounded when we try to run efficiently on heterogeneous\nparallel architectures. We present Ebb, a domain-specific language (DSL) for\nsimulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs,\nEbb uses a three-layer architecture to separate (1) simulation code, (2)\ndefinition of data structures for geometric domains, and (3) runtimes\nsupporting parallel architectures. Different geometric domains are implemented\nas libraries that use a common, unified, relational data model. By structuring\nthe simulation framework in this way, programmers implementing simulations can\nfocus on the physics and algorithms for each simulation without worrying about\ntheir implementation on parallel computers. Because the geometric domain\nlibraries are all implemented using a common runtime based on relations, new\ngeometric domains can be added as needed, without specifying the details of\nmemory management, mapping to different parallel architectures, or having to\nexpand the runtime's interface.\n  We evaluate Ebb by comparing it to several widely used simulations,\ndemonstrating comparable performance to hand-written GPU code where available,\nand surpassing existing CPU performance optimizations by up to 9$\\times$ when\nno GPU code exists."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1506.07915v1", 
    "title": "Combining Visual Analytics and Content Based Data Retrieval Technology   for Efficient Data Analysis", 
    "arxiv-id": "1506.07915v1", 
    "author": "Caetano Traina", 
    "publish": "2015-06-25T22:47:28Z", 
    "summary": "One of the most useful techniques to help visual data analysis systems is\ninteractive filtering (brushing). However, visualization techniques often\nsuffer from overlap of graphical items and multiple attributes complexity,\nmaking visual selection inefficient. In these situations, the benefits of data\nvisualization are not fully observable because the graphical items do not pop\nup as comprehensive patterns. In this work we propose the use of content-based\ndata retrieval technology combined with visual analytics. The idea is to use\nthe similarity query functionalities provided by metric space systems in order\nto select regions of the data domain according to user-guidance and interests.\nAfter that, the data found in such regions feed multiple visualization\nworkspaces so that the user can inspect the correspondent datasets. Our\nexperiments showed that the methodology can break the visual analysis process\ninto smaller problems (views) and that the views hold the expectations of the\nanalyst according to his/her similarity query selection, improving data\nperception and analytical possibilities. Our contribution introduces a\nprinciple that can be used in all sorts of visualization techniques and\nsystems, this principle can be extended with different kinds of integration\nvisualization-metric-space, and with different metrics, expanding the\npossibilities of visual data analysis in aspects such as semantics and\nscalability."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1506.08459v1", 
    "title": "On the Approximation Theory of Linear Variational Subspace Design", 
    "arxiv-id": "1506.08459v1", 
    "author": "Zhixin Yan", 
    "publish": "2015-06-28T22:08:10Z", 
    "summary": "Solving large-scale optimization on-the-fly is often a difficult task for\nreal-time computer graphics applications. To tackle this challenge, model\nreduction is a well-adopted technique. Despite its usefulness, model reduction\noften requires a handcrafted subspace that spans a domain that hypothetically\nembodies desirable solutions. For many applications, obtaining such subspaces\ncase-by-case either is impossible or requires extensive human labors, hence\ndoes not readily have a scalable solution for growing number of tasks. We\npropose linear variational subspace design for large-scale constrained\nquadratic programming, which can be computed automatically without any human\ninterventions. We provide meaningful approximation error bound that\nsubstantiates the quality of calculated subspace, and demonstrate its empirical\nsuccess in interactive deformable modeling for triangular and tetrahedral\nmeshes."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.02766v1", 
    "title": "A Hybrid Graph-drawing Algorithm for Large, Naturally-clustered,   Disconnected Graphs", 
    "arxiv-id": "1507.02766v1", 
    "author": "Eliezer A. Albacea", 
    "publish": "2015-07-10T02:24:41Z", 
    "summary": "In this paper, we present a hybrid graph-drawing algorithm (GDA) for\nlayouting large, naturally-clustered, disconnected graphs. We called it a\nhybrid algorithm because it is an implementation of a series of already known\ngraph-drawing and graph-theoretic procedures. We remedy in this hybrid the\nproblematic nature of the current force-based GDA which has the inability to\nscale to large, naturally-clustered, and disconnected graphs. These kinds of\ngraph usually model the complex inter-relationships among entities in social,\nbiological, natural, and artificial networks. Obviously, the hybrid runs longer\nthan the current GDAs. By using two extreme cases of graphs as inputs, we\npresent in this paper the derivation of the time complexity of the hybrid which\nwe found to be $O(|\\V|^3)$."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.02800v1", 
    "title": "Meshfree C^2-Weighting for Shape Deformation", 
    "arxiv-id": "1507.02800v1", 
    "author": "Charlie C. L. Wang", 
    "publish": "2015-07-10T08:12:22Z", 
    "summary": "Handle-driven deformation based on linear blending is widely used in many\napplications because of its merits in intuitiveness, efficiency and easiness of\nimplementation. We provide a meshfree method to compute the smooth weights of\nlinear blending for shape deformation. The C2-continuity of weighting is\nguaranteed by the carefully formulated basis functions, with which the\ncomputation of weights is in a closed-form. Criteria to ensure the quality of\ndeformation are preserved by the basis functions after decomposing the shape\ndomain according to the Voronoi diagram of handles. The cost of inserting a new\nhandle is only the time to evaluate the distances from the new handle to all\nsample points in the space of deformation. Moreover, a virtual handle insertion\nalgorithm has been developed to allow users freely placing handles while\npreserving the criteria on weights. Experimental examples for real-time 2D/3D\ndeformations are shown to demonstrate the effectiveness of this method."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.02860v1", 
    "title": "A Closed-Form Formulation of HRBF-Based Surface Reconstruction", 
    "arxiv-id": "1507.02860v1", 
    "author": "Jun Wang", 
    "publish": "2015-07-10T11:41:04Z", 
    "summary": "The Hermite radial basis functions (HRBFs) implicits have been used to\nreconstruct surfaces from scattered Hermite data points. In this work, we\npropose a closed-form formulation to construct HRBF-based implicits by a\nquasi-solution approximating the exact solution. A scheme is developed to\nautomatically adjust the support sizes of basis functions to hold the error\nbound of a quasi-solution. Our method can generate an implicit function from\npositions and normals of scattered points without taking any global operation.\nWorking together with an adaptive sampling algorithm, the HRBF-based implicits\ncan also reconstruct surfaces from point clouds with non-uniformity and noises.\nRobust and efficient reconstruction has been observed in our experimental tests\non real data captured from a variety of scenes."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.03351v1", 
    "title": "On Smooth 3D Frame Field Design", 
    "arxiv-id": "1507.03351v1", 
    "author": "Dmitry Sokolov", 
    "publish": "2015-07-13T08:07:19Z", 
    "summary": "We analyze actual methods that generate smooth frame fields both in 2D and in\n3D. We formalize the 2D problem by representing frames as functions (as it was\ndone in 3D), and show that the derived optimization problem is the one that\nprevious work obtain via \"representation vectors.\" We show (in 2D) why this non\nlinear optimization problem is easier to solve than directly minimizing the\nrotation angle of the field, and observe that the 2D algorithm is able to find\ngood fields.\n  Now, the 2D and the 3D optimization problems are derived from the same\nformulation (based on representing frames by functions). Their energies share\nsome similarities from an optimization point of view (smoothness, local minima,\nbounds of partial derivatives, etc.), so we applied the 2D resolution mechanism\nto the 3D problem. Our evaluation of all existing 3D methods suggests to\ninitialize the field by this new algorithm, but possibly use another method for\nfurther smoothing."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.04110v4", 
    "title": "A de Casteljau Algorithm for Bernstein type Polynomials based on   (p,q)-integers", 
    "arxiv-id": "1507.04110v4", 
    "author": "Adem Kilicman", 
    "publish": "2015-07-15T07:57:26Z", 
    "summary": "In this paper, a de Casteljau algorithm to compute (p,q)-Bernstein Bezier\ncurves based on (p,q)-integers is introduced. We study the nature of degree\nelevation and degree reduction for (p,q)-Bezier Bernstein functions. The new\ncurves have some properties similar to q-Bezier curves. Moreover, we construct\nthe corresponding tensor product surfaces over the rectangular domain (u, v)\n\\in [0, 1] \\times [0, 1] depending on four parameters. We also study the de\nCasteljau algorithm and degree evaluation properties of the surfaces for these\ngeneralization over the rectangular domain. Furthermore, some fundamental\nproperties for (p,q)-Bernstein Bezier curves are discussed. We get q-Bezier\ncurves and surfaces for (u, v) \\in [0, 1] \\times [0, 1] when we set the\nparameter p1 = p2 = 1."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1507.05290v2", 
    "title": "A concise parametrisation of affine transformation", 
    "arxiv-id": "1507.05290v2", 
    "author": "Hiroyuki Ochiai", 
    "publish": "2015-07-19T13:53:47Z", 
    "summary": "Good parametrisations of affine transformations are essential to\ninterpolation, deformation, and analysis of shape, motion, and animation. It\nhas been one of the central research topics in computer graphics. However,\nthere is no single perfect method and each one has both advantages and\ndisadvantages. In this paper, we propose a novel parametrisation of affine\ntransformations, which is a generalisation to or an improvement of existing\nmethods. Our method adds yet another choice to the existing toolbox and shows\nbetter performance in some applications. A C++ implementation is available to\nmake our framework ready to use in various applications."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1508.02826v1", 
    "title": "Inappropriate use of L-BFGS, Illustrated on frame field design", 
    "arxiv-id": "1508.02826v1", 
    "author": "Dmitry Sokolov", 
    "publish": "2015-08-12T07:02:19Z", 
    "summary": "L-BFGS is a hill climbing method that is guarantied to converge only for\nconvex problems. In computer graphics, it is often used as a black box solver\nfor a more general class of non linear problems, including problems having many\nlocal minima. Some works obtain very nice results by solving such difficult\nproblems with L-BFGS. Surprisingly, the method is able to escape local minima:\nour interpretation is that the approximation of the Hessian is smoother than\nthe real Hessian, making it possible to evade the local minima. We analyse the\nbehavior of L-BFGS on the design of 2D frame fields. It involves an energy\nfunction that is infinitly continuous, strongly non linear and having many\nlocal minima. Moreover, the local minima have a clear visual interpretation:\nthey corresponds to differents frame field topologies. We observe that the\nperformances of LBFGS are almost unpredictables: they are very competitive when\nthe field is sampled on the primal graph, but really poor when they are sampled\non the dual graph."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1508.07593v1", 
    "title": "The Prose Storyboard Language: A Tool for Annotating and Directing   Movies", 
    "arxiv-id": "1508.07593v1", 
    "author": "Laurent Boiron", 
    "publish": "2015-08-30T16:12:59Z", 
    "summary": "The prose storyboard language is a formal language for describing movies shot\nby shot, where each shot is described with a unique sentence. The language uses\na simple syntax and limited vocabulary borrowed from working practices in\ntraditional movie-making, and is intended to be readable both by machines and\nhumans. The language is designed to serve as a high-level user interface for\nintelligent cinematography and editing systems."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1509.03335v1", 
    "title": "Decomposing Digital Paintings into Layers via RGB-space Geometry", 
    "arxiv-id": "1509.03335v1", 
    "author": "Yotam Gingold", 
    "publish": "2015-09-10T20:58:36Z", 
    "summary": "In digital painting software, layers organize paintings. However, layers are\nnot explicitly represented, transmitted, or published with the final digital\npainting. We propose a technique to decompose a digital painting into layers.\nIn our decomposition, each layer represents a coat of paint of a single paint\ncolor applied with varying opacity throughout the image. Our decomposition is\nbased on the painting's RGB-space geometry. In RGB-space, a geometric structure\nis revealed due to the linear nature of the standard Porter-Duff \"over\" pixel\ncompositing operation. The vertices of the convex hull of pixels in RGB-space\nsuggest paint colors. Users choose the degree of simplification to perform on\nthe convex hull, as well as a layer order for the colors. We solve a\nconstrained optimization problem to find maximally translucent, spatially\ncoherent opacity for each layer, such that the composition of the layers\nreproduces the original image. We demonstrate the utility of the resulting\ndecompositions for re-editing."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1509.03700v1", 
    "title": "Good Colour Maps: How to Design Them", 
    "arxiv-id": "1509.03700v1", 
    "author": "Peter Kovesi", 
    "publish": "2015-09-12T03:35:20Z", 
    "summary": "Many colour maps provided by vendors have highly uneven perceptual contrast\nover their range. It is not uncommon for colour maps to have perceptual flat\nspots that can hide a feature as large as one tenth of the total data range.\nColour maps may also have perceptual discontinuities that induce the appearance\nof false features. Previous work in the design of perceptually uniform colour\nmaps has mostly failed to recognise that CIELAB space is only designed to be\nperceptually uniform at very low spatial frequencies. The most important factor\nin designing a colour map is to ensure that the magnitude of the incremental\nchange in perceptual lightness of the colours is uniform. The specific\nrequirements for linear, diverging, rainbow and cyclic colour maps are\ndeveloped in detail. To support this work two test images for evaluating colour\nmaps are presented. The use of colour maps in combination with relief shading\nis considered and the conditions under which colour can enhance or disrupt\nrelief shading are identified. Finally, a set of new basis colours for the\nconstruction of ternary images are presented. Unlike the RGB primaries these\nbasis colours produce images whereby the salience of structures are consistent\nirrespective of the assignment of basis colours to data channels."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1509.05330v1", 
    "title": "Elements of Validation of Artificial Lighting through the Software   CODYRUN: Application to a Test Case of the International Commission on   Illumination (CIE)", 
    "arxiv-id": "1509.05330v1", 
    "author": "Harry Boyer", 
    "publish": "2015-09-15T09:32:17Z", 
    "summary": "CODYRUN is a software for computational aeraulic and thermal simulation in\nbuildings developed by the Laboratory of Building Physics and Systems\n(L.P.B.S). Numerical simulation codes of artificial lighting have been\nintroduced to extend the tool capacity. These calculation codes are able to\npredict the amount of light received by any point of a given working plane and\nfrom one or more sources installed on the ceiling of the room. The model used\nfor these calculations is original and semi-detailed (simplified). The test\ncase references of the task-3 TC-33 International Commission on Illumination\n(CIE) were applied to the software to ensure reliability to properly handle\nthis photometric aspect. This allowed having a precise idea about the\nreliability of the results of numerical simulations."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1510.03023v1", 
    "title": "Printed Perforated Lampshades for Continuous Projective Images", 
    "arxiv-id": "1510.03023v1", 
    "author": "Baoquan Chen", 
    "publish": "2015-10-11T08:13:38Z", 
    "summary": "We present a technique for designing 3D-printed perforated lampshades, which\nproject continuous grayscale images onto the surrounding walls. Given the\ngeometry of the lampshade and a target grayscale image, our method computes a\ndistribution of tiny holes over the shell, such that the combined footprints of\nthe light emanating through the holes form the target image on a nearby diffuse\nsurface. Our objective is to approximate the continuous tones and the spatial\ndetail of the target image, to the extent possible within the constraints of\nthe fabrication process.\n  To ensure structural integrity, there are lower bounds on the thickness of\nthe shell, the radii of the holes, and the minimal distances between adjacent\nholes. Thus, the holes are realized as thin tubes distributed over the\nlampshade surface. The amount of light passing through a single tube may be\ncontrolled by the tube's radius and by its direction (tilt angle). The core of\nour technique thus consists of determining a suitable configuration of the\ntubes: their distribution across the relevant portion of the lampshade, as well\nas the parameters (radius, tilt angle) of each tube. This is achieved by\ncomputing a capacity-constrained Voronoi tessellation over a suitably defined\ndensity function, and embedding a tube inside the maximal inscribed circle of\neach tessellation cell. The density function for a particular target image is\nderived from a series of simulated images, each corresponding to a different\nuniform density tube pattern on the lampshade."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1510.03935v1", 
    "title": "Curvature-Metric-Free Surface Remeshing via Principle Component Analysis", 
    "arxiv-id": "1510.03935v1", 
    "author": "Zichun Zhong", 
    "publish": "2015-10-14T00:24:22Z", 
    "summary": "In this paper, we present a surface remeshing method with high approximation\nquality based on Principal Component Analysis. Given a triangular mesh and a\nuser assigned polygon/vertex budget, traditional methods usually require the\nextra curvature metric field for the desired anisotropy to best approximate the\nsurface, even though the estimated curvature metric is known to be imperfect\nand already self-contained in the surface. In our approach, this anisotropic\ncontrol is achieved through the optimal geometry partition without this\nexplicit metric field. The minimization of our proposed partition energy has\nthe following properties: Firstly, on a C2 surface, it is theoretically\nguaranteed to have the optimal aspect ratio and cluster size as specified in\napproximation theory for L1 piecewise linear approximation. Secondly, it\ncaptures sharp features on practical models without any pre-tagging. We develop\nan effective merging-swapping framework to seek the optimal partition and\nconstruct polygonal/triangular mesh afterwards. The effectiveness and\nefficiency of our method are demonstrated through the comparison with other\nstate-of-the-art remeshing methods."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1511.01862v2", 
    "title": "On Intra Prediction for Screen Content Video Coding", 
    "arxiv-id": "1511.01862v2", 
    "author": "Felix Fernandes", 
    "publish": "2015-11-05T19:31:51Z", 
    "summary": "Screen content coding (SCC) is becoming increasingly important in various\napplications, such as desktop sharing, video conferencing, and remote\neducation. When compared to natural camera- captured content, screen content\nhas different characteristics, in particular sharper edges. In this paper, we\npropose a novel intra prediction scheme for screen content video. In the\nproposed scheme, bilinear interpolation in angular intra prediction in HEVC is\nselectively replaced by nearest-neighbor intra prediction to preserve the sharp\nedges in screen content video. We present three different variants of the\nproposed nearest neighbor prediction algorithm: two implicit methods where both\nthe encoder, and the decoder derive whether to perform nearest neighbor\nprediction or not based on either (a) the sum of the absolute difference, or\n(b) the difference between the boundary pixels from which prediction is\nperformed; and another variant where Rate-Distortion-Optimization (RDO) search\nis performed at the encoder to decide whether or not to use the nearest\nneighbor interpolation, and explicitly signaled to the decoder. We also discuss\nthe various underlying trade-offs in terms of the complexity of the three\nvariants. All the three proposed variants provide significant gains over HEVC,\nand simulation results show that average gains of 3.3% BD-bitrate in\nIntra-frame coding are achieved by the RDO variant for screen content video. To\nthe best of our knowledge, this is the first paper that 1) points out current\nHEVC intra prediction scheme with bilinear interpolation does not work\nefficiently for screen content video and 2) uses different filters adaptively\nin the HEVC intra prediction interpolation."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1511.04224v2", 
    "title": "Procedural wood textures", 
    "arxiv-id": "1511.04224v2", 
    "author": "Victoria E. Dye", 
    "publish": "2015-11-13T10:19:41Z", 
    "summary": "Existing bidirectional reflectance distribution function (BRDF) models are\ncapable of capturing the distinctive highlights produced by the fibrous nature\nof wood. However, capturing parameter textures for even a single specimen\nremains a laborious process requiring specialized equipment. In this paper we\ntake a procedural approach to generating parameters for the wood BSDF. We\ncharacterize the elements of trees that are important for the appearance of\nwood, discuss techniques appropriate for representing those features, and\npresent a complete procedural wood shader capable of reproducing the growth\npatterns responsible for the distinctive appearance of highly prized\n``figured'' wood. Our procedural wood shader is random-access, 3D, modular, and\nis fast enough to generate a preview for design."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1511.06594v1", 
    "title": "Bezier curves and surfaces based on modified Bernstein polynomials", 
    "arxiv-id": "1511.06594v1", 
    "author": "Adem Kilicman", 
    "publish": "2015-11-20T13:52:39Z", 
    "summary": "In this paper, we use the blending functions of Bernstein polynomials with\nshifted knots for construction of Bezier curves and surfaces. We study the\nnature of degree elevation and degree reduction for Bezier Bernstein functions\nwith shifted knots.\n  Parametric curves are represented using these modified Bernstein basis and\nthe concept of total positivity is applied to investigate the shape properties\nof the curve. We get Bezier curve defined on [0, 1] when we set the parameter\n\\alpha=\\beta to the value 0. We also present a de Casteljau algorithm to\ncompute Bernstein Bezier curves and surfaces with shifted knots. The new curves\nhave some properties similar to Bezier curves. Furthermore, some fundamental\nproperties for Bernstein Bezier curves and surfaces are discussed."
},{
    "category": "cs.GR", 
    "doi": "10.1109/IV.2010.101", 
    "link": "http://arxiv.org/pdf/1511.07932v1", 
    "title": "Embedding of Hypercube into Cylinder", 
    "arxiv-id": "1511.07932v1", 
    "author": "ZhuoJia Shen", 
    "publish": "2015-11-25T01:41:08Z", 
    "summary": "Task mapping in modern high performance parallel computers can be modeled as\na graph embedding problem, which simulates the mapping as embedding one graph\ninto another and try to find the minimum wirelength for the mapping. Though\nembedding problems have been considered for several regular graphs, such as\nhypercubes into grids, binary trees into grids, et al, it is still an open\nproblem for hypercubes into cylinders. In this paper, we consider the problem\nof embedding hypercubes into cylinders to minimize the wirelength. We obtain\nthe exact wirelength formula of embedding hypercube $Q^r$ into cylinder\n$C_{2^3}\\times P_{2^{r-3}}$ with $r\\ge3$."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1511.08118v1", 
    "title": "SlicerPET: A workflow based software module for PET/CT guided needle   biopsy", 
    "arxiv-id": "1511.08118v1", 
    "author": "Andinet Enquobahrie", 
    "publish": "2015-11-25T17:02:20Z", 
    "summary": "Biopsy is commonly used to confirm cancer diagnosis when radiologically\nindicated. Given the ability of PET to localize malignancies in heterogeneous\ntumors and tumors that do not have a CT correlate, PET/CT guided biopsy may\nimprove the diagnostic yield of biopsies. To facilitate PET/CT guided needle\nbiopsy, we developed a workflow that allows us to bring PET image guidance into\nthe interventional CT suite. In this abstract, we present SlicerPET, a\nuser-friendly workflow based module developed using open source software\nlibraries to guide needle biopsy in the interventional suite."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1512.08826v1", 
    "title": "Improving Style Similarity Metrics of 3D Shapes", 
    "arxiv-id": "1512.08826v1", 
    "author": "Manfred Lau", 
    "publish": "2015-12-30T02:26:46Z", 
    "summary": "The idea of style similarity metrics has been recently developed for various\nmedia types such as 2D clip art and 3D shapes. We explore this style metric\nproblem and improve existing style similarity metrics of 3D shapes in four\nnovel ways. First, we consider the color and texture of 3D shapes which are\nimportant properties that have not been previously considered. Second, we\nexplore the effect of clustering a dataset of 3D models by comparing between\nstyle metrics for a single object type and style metrics that combine clusters\nof object types. Third, we explore the idea of user-guided learning for this\nproblem. Fourth, we introduce an iterative approach that can learn a metric\nfrom a general set of 3D models. We demonstrate these contributions with\nvarious classes of 3D shapes and with applications such as style-based\nsimilarity search and scene composition."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1601.04450v1", 
    "title": "Which tone-mapping operator is the best? A comparative study of   perceptual quality", 
    "arxiv-id": "1601.04450v1", 
    "author": "Xavier Otazu", 
    "publish": "2016-01-18T10:11:26Z", 
    "summary": "Tone-mapping operators (TMO) are designed to generate perceptually similar\nlow-dynamic range images from high-dynamic range ones. We studied the\nperformance of fifteen TMOs in two psychophysical experiments where observers\ncompared the digitally generated tone-mapped images to their corresponding\nphysical scenes. All experiments were performed in a controlled environment and\nthe setups were designed to emphasise different image properties: in the first\nexperiment we evaluated the local relationships among intensity-levels, and in\nthe second one we evaluated global visual appearance among physical scenes and\ntone-mapped images, which were presented side by side. We ranked the TMOs\naccording to how well they reproduce the results obtained in the physical\nscene. Our results show that ranking position clearly depends on the adopted\nevaluation criteria, which implies that, in general, these tone-mapping\nalgorithms consider either local or global image attributes but rarely both. We\nconclude that a more thorough and standardized evaluation criteria are needed\nto study all the characteristics of TMOs, as there is ample room for\nimprovement in future developments."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1601.05824v1", 
    "title": "3D digital reassembling of archaeological ceramic pottery fragments   based on their thickness profile", 
    "arxiv-id": "1601.05824v1", 
    "author": "Christos-Nikolaos Anagnostopoulos", 
    "publish": "2016-01-21T21:46:22Z", 
    "summary": "The reassembly of a broken archaeological ceramic pottery is an open and\ncomplex problem, which remains a scientific process of extreme interest for the\narchaeological community. Usually, the solutions suggested by various research\ngroups and universities depend on various aspects such as the matching process\nof the broken surfaces, the outline of sherds or their colors and geometric\ncharacteris-tics, their axis of symmetry, the corners of their contour, the\ntheme portrayed on the surface, the concentric circular rills that are left\nduring the base construction in the inner pottery side by the fingers of the\npotter artist etc. In this work the reassembly process is based on a different\nand more secure idea, since it is based on the thick-ness profile, which is\nappropriately identified in every fragment. Specifically, our approach is based\non information encapsulated in the inner part of the sherd (i.e. thickness),\nwhich is not -or at least not heavily- affected by the presence of harsh\nenvironmental conditions, but is safely kept within the sherd itself. Our\nmethod is verified in various use case experiments, using cutting edge\ntechnologies such as 3D representations and precise measurements on surfaces\nfrom the acquired 3D models."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1601.07765v1", 
    "title": "SculptStat: Statistical Analysis of Digital Sculpting Workflows", 
    "arxiv-id": "1601.07765v1", 
    "author": "Fabio Pellacini", 
    "publish": "2016-01-28T14:09:12Z", 
    "summary": "Targeted user studies are often employed to measure how well artists can\nperform specific tasks. But these studies cannot properly describe editing\nworkflows as wholes, since they guide the artists both by choosing the tasks\nand by using simplified interfaces. In this paper, we investigate digital\nsculpting workflows used to produce detailed models. In our experiment design,\nartists can choose freely what and how to model. We recover whole-workflow\ntrends with sophisticated statistical analyzes and validate these trends with\ngoodness-of-fits measures. We record brush strokes and mesh snapshots by\ninstrumenting a sculpting program and analyze the distribution of these\nproperties and their spatial and temporal characteristics. We hired expert\nartists that can produce relatively sophisticated models in short time, since\ntheir workflows are representative of best practices. We analyze 13 meshes\ncorresponding to roughly 25 thousand strokes in total. We found that artists\nwork mainly with short strokes, with average stroke length dependent on model\nfeatures rather than the artist itself. Temporally, artists do not work\ncoarse-to-fine but rather in bursts. Spatially, artists focus on some selected\nregions by dedicating different amounts of edits and by applying different\ntechniques. Spatio-temporally, artists return to work on the same area multiple\ntimes without any apparent periodicity. We release the entire dataset and all\ncode used for the analyzes as reference for the community."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1601.07953v1", 
    "title": "Boolean Operations using Generalized Winding Numbers", 
    "arxiv-id": "1601.07953v1", 
    "author": "Alec Jacobson", 
    "publish": "2016-01-29T00:08:24Z", 
    "summary": "The generalized winding number function measures insideness for arbitrary\noriented triangle meshes. Exploiting this, I similarly generalize binary\nboolean operations to act on such meshes. The resulting operations for union,\nintersection, difference, etc. avoid volumetric discretization or\npre-processing."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11548-015-1213-2", 
    "link": "http://arxiv.org/pdf/1602.01224v2", 
    "title": "Smooth surface interpolation using patches with rational offsets", 
    "arxiv-id": "1602.01224v2", 
    "author": "Jan Vr\u0161ek", 
    "publish": "2016-02-03T08:24:21Z", 
    "summary": "We present a new method for the interpolation of given data points and\nassociated normals with surface parametric patches with rational normal fields.\nWe give some arguments why a dual approach is the most convenient for these\nsurfaces, which are traditionally called Pythagorean normal vector (PN)\nsurfaces. Our construction is based on the isotropic model of the dual space to\nwhich the original data are pushed. Then the bicubic Coons patches are\nconstructed in the isotropic space and then pulled back to the standard three\ndimensional space. As a result we obtain the patch construction which is\ncompletely local and produces surfaces with the global G1~continuity."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2440273", 
    "link": "http://arxiv.org/pdf/1602.01913v1", 
    "title": "Effective Clipart Image Vectorization Through Direct Optimization of   Bezigons", 
    "arxiv-id": "1602.01913v1", 
    "author": "Jian Sun", 
    "publish": "2016-02-05T02:50:43Z", 
    "summary": "Bezigons, i.e., closed paths composed of B\\'ezier curves, have been widely\nemployed to describe shapes in image vectorization results. However, most\nexisting vectorization techniques infer the bezigons by simply approximating an\nintermediate vector representation (such as polygons). Consequently, the\nresultant bezigons are sometimes imperfect due to accumulated errors, fitting\nambiguities, and a lack of curve priors, especially for low-resolution images.\nIn this paper, we describe a novel method for vectorizing clipart images. In\ncontrast to previous methods, we directly optimize the bezigons rather than\nusing other intermediate representations; therefore, the resultant bezigons are\nnot only of higher fidelity compared with the original raster image but also\nmore reasonable because they were traced by a proficient expert. To enable such\noptimization, we have overcome several challenges and have devised a\ndifferentiable data energy as well as several curve-based prior terms. To\nimprove the efficiency of the optimization, we also take advantage of the local\ncontrol property of bezigons and adopt an overlapped piecewise optimization\nstrategy. The experimental results show that our method outperforms both the\ncurrent state-of-the-art method and commonly used commercial software in terms\nof bezigon quality."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2440273", 
    "link": "http://arxiv.org/pdf/1603.00713v1", 
    "title": "LevelMerge: Collaborative Game Level Editing by Merging Labeled Graphs", 
    "arxiv-id": "1603.00713v1", 
    "author": "Fabio Pellacini", 
    "publish": "2016-03-02T13:52:41Z", 
    "summary": "Game level editing is the process of constructing a full game level starting\nfrom 3D asset libraries, e.g. 3d models, textures, shaders, scripts. In level\nediting, designers define the look and behavior of the whole level by placing\nobjects, assigning materials and lighting parameters, setting animations and\nphysics properties and customizing the objects AI and behavior by editing\nscripts. The heterogeneity of the task usually translates to a workflow where a\nteam of people, experts on separate aspects, cooperate to edit the game level,\noften working on the same objects (e.g.: a programmer working on the AI of a\ncharacter, while an artist works on its 3D model or its materials). Today this\ncollaboration is established by using version control systems designed for text\ndocuments, such as Git, to manage different versions and share them amongst\nusers. The merge algorithms used in these systems though does not perform well\nin our case since it does not respect the relations between game objects\nnecessary to maintain the semantic of the game level behavior and look. This is\na known problem and commercial systems for game level merging exists, e.g.\nPlasticSCM, but these are only slightly more robust than text-based ones. This\ncauses designers to often merge scenes manually, essentially reapplying others\nedits in the game level editor."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2015.2440273", 
    "link": "http://arxiv.org/pdf/1603.04060v1", 
    "title": "Modelling Developable Ribbons Using Ruling Bending Coordinates", 
    "arxiv-id": "1603.04060v1", 
    "author": "Hujun Bao", 
    "publish": "2016-03-13T18:38:15Z", 
    "summary": "This paper presents a new method for modelling the dynamic behaviour of\ndevelopable ribbons, two dimensional strips with much smaller width than\nlength. Instead of approximating such surface with a general triangle mesh, we\ncharacterize it by a set of creases and bending angles across them. This\nrepresentation allows the developability to be satisfied everywhere while still\nleaves enough degree of freedom to represent salient global deformation. We\nshow how the potential and kinetic energies can be properly discretized in this\nconfiguration space and time integrated in a fully implicit manner. The result\nis a dynamic simulator with several desirable features: We can model\nnon-trivial deformation using much fewer elements than conventional FEM method.\nIt is stable under extreme deformation, external force or large timestep size.\nAnd we can readily handle various user constraints in Euclidean space."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1603.05433v2", 
    "title": "Degree reduction of composite B\u00e9zier curves", 
    "arxiv-id": "1603.05433v2", 
    "author": "Pawe\u0142 Wo\u017any", 
    "publish": "2016-03-17T11:30:01Z", 
    "summary": "This paper deals with the problem of multi-degree reduction of a composite\nB\\'ezier curve with the parametric continuity constraints at the endpoints of\nthe segments. We present a novel method which is based on the idea of using\nconstrained dual Bernstein polynomials to compute the control points of the\nreduced composite curve. In contrast to other methods, ours minimizes the\n$L_2$-error for the whole composite curve instead of minimizing the\n$L_2$-errors for each segment separately. As a result, an additional\noptimization is possible. Examples show that the new method gives much better\nresults than multiple application of the degree reduction of a single B\\'ezier\ncurve."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1603.06821v1", 
    "title": "A Report on Shape Deformation with a Stretching and Bending Energy", 
    "arxiv-id": "1603.06821v1", 
    "author": "Steven J. Gortler", 
    "publish": "2016-03-22T15:14:21Z", 
    "summary": "In this report we describe a mesh editing system that we implemented that\nuses a natural stretching and bending energy defined over smooth surfaces. As\nsuch, this energy behaves uniformly under various mesh resolutions. All of the\nelements of our approach already exist in the literature. We hope that our\ndiscussions of these energies helps to shed light on the behaviors of these\nmethods and provides a unified discussion of these methods."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1603.07011v1", 
    "title": "Graphs Drawing through Fuzzy Clustering", 
    "arxiv-id": "1603.07011v1", 
    "author": "Mandana Ghasemi", 
    "publish": "2016-03-22T22:14:01Z", 
    "summary": "Many problems can be presented in an abstract form through a wide range of\nbinary objects and relations which are defined over problem domain. In these\nproblems, graphical demonstration of defined binary objects and solutions is\nthe most suitable representation approach. In this regard, graph drawing\nproblem discusses the methods for transforming combinatorial graphs to\ngeometrical drawings in order to visualize them. This paper studies the\nforce-directed algorithms and multi-surface techniques for drawing general\nundirected graphs. Particularly, this research describes force-directed\napproach to model the drawing of a general graph as a numerical optimization\nproblem. So, it can use rich knowledge which is presented as an established\nsystem by the numerical optimization. Moreover, this research proposes the\nmulti-surface approach as an efficient tool for overcoming local minimums in\nstandard force-directed algorithms. Next, we introduce a new method for\nmulti-surface approach based on fuzzy clustering algorithms."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1603.08753v1", 
    "title": "Curve Networks for Surface Reconstruction", 
    "arxiv-id": "1603.08753v1", 
    "author": "Peter Wonka", 
    "publish": "2016-03-29T13:02:50Z", 
    "summary": "Man-made objects usually exhibit descriptive curved features (i.e., curve\nnetworks). The curve network of an object conveys its high-level geometric and\ntopological structure. We present a framework for extracting feature curve\nnetworks from unstructured point cloud data. Our framework first generates a\nset of initial curved segments fitting highly curved regions. We then optimize\nthese curved segments to respect both data fitting and structural regularities.\nFinally, the optimized curved segments are extended and connected into curve\nnetworks using a clustering method. To facilitate effectiveness in case of\nsevere missing data and to resolve ambiguities, we develop a user interface for\ncompleting the curve networks. Experiments on various imperfect point cloud\ndata validate the effectiveness of our curve network extraction framework. We\ndemonstrate the usefulness of the extracted curve networks for surface\nreconstruction from incomplete point clouds."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1604.00047v1", 
    "title": "Towards Zero-Waste Furniture Design", 
    "arxiv-id": "1604.00047v1", 
    "author": "Niloy J. Mitra", 
    "publish": "2016-03-31T20:40:14Z", 
    "summary": "In traditional design, shapes are first conceived, and then fabricated. While\nthis decoupling simplifies the design process, it can result in inefficient\nmaterial usage, especially where off-cut pieces are hard to reuse. The\ndesigner, in absence of explicit feedback on material usage remains helpless to\neffectively adapt the design -- even though design variabilities exist. In this\npaper, we investigate {\\em waste minimizing furniture design} wherein based on\nthe current design, the user is presented with design variations that result in\nmore effective usage of materials. Technically, we dynamically analyze material\nspace layout to determine {\\em which} parts to change and {\\em how}, while\nmaintaining original design intent specified in the form of design constraints.\nWe evaluate the approach on simple and complex furniture design scenarios, and\ndemonstrate effective material usage that is difficult, if not impossible, to\nachieve without computational support."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1604.02483v2", 
    "title": "On the Hessian of Shape Matching Energy", 
    "arxiv-id": "1604.02483v2", 
    "author": "Yun Fei", 
    "publish": "2016-04-08T20:59:46Z", 
    "summary": "In this technical report we derive the analytic form of the Hessian matrix\nfor shape matching energy. Shape matching is a useful technique for meshless\ndeformation, which can be easily combined with multiple techniques in real-time\ndynamics. Nevertheless, it has been rarely applied in scenarios where implicit\nintegrators are required, and hence strong viscous damping effect, though\npopular in simulation systems nowadays, is forbidden for shape matching. The\nreason lies in the difficulty to derive the Hessian matrix of the shape\nmatching energy. Computing the Hessian matrix correctly, and stably, is the key\nto more broadly application of shape matching in implicitly-integrated systems."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1604.03220v1", 
    "title": "Algorithms and identities for $(p,q)$-B$\\acute{e}$zier curves via   $(p,q)$-Blossom", 
    "arxiv-id": "1604.03220v1", 
    "author": "D. K. Lobiyal", 
    "publish": "2016-04-09T16:47:06Z", 
    "summary": "In this paper, a new variant of the blossom, the $(p,q)$-blossom, is\nintroduced, by altering the diagonal property of the standard blossom. This\n$(p,q)$-blossom has been adapted for developing identities and algorithms for\n$(p,q)$-Bernstein bases and $(p,q)$-B$\\acute{e}$zier curves. We generate\nseveral new identities including an explicit formula representing the monomials\nin terms of the $(p,q)$-Bernstein basis functions and a $(p,q)$-variant of\nMarsden's identity by applying the $(p,q)$-blossom. We also derive for each\n$(p,q)$-B$\\acute{e}$zier curve of degree $n,$ a collection of $n!$ new, affine\ninvariant, recursive evaluation algorithms. Using two of these new recursive\nevaluation algorithms, we construct a recursive subdivision algorithm for\n$(p,q)$-B$\\acute{e}$zier curves."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1604.07378v1", 
    "title": "Towards Real-time Simulation of Hyperelastic Materials", 
    "arxiv-id": "1604.07378v1", 
    "author": "Ladislav Kavan", 
    "publish": "2016-04-25T19:42:37Z", 
    "summary": "We present a new method for real-time physics-based simulation supporting\nmany different types of hyperelastic materials. Previous methods such as\nPosition Based or Projective Dynamics are fast, but support only limited\nselection of materials; even classical materials such as the Neo-Hookean\nelasticity are not supported. Recently, Xu et al. [2015] introduced new\n\"spline-based materials\" which can be easily controlled by artists to achieve\ndesired animation effects. Simulation of these types of materials currently\nrelies on Newton's method, which is slow, even with only one iteration per\ntimestep. In this paper, we show that Projective Dynamics can be interpreted as\na quasi-Newton method. This insight enables very efficient simulation of a\nlarge class of hyperelastic materials, including the Neo-Hookean, spline-based\nmaterials, and others. The quasi-Newton interpretation also allows us to\nleverage ideas from numerical optimization. In particular, we show that our\nsolver can be further accelerated using L-BFGS updates (Limited-memory\nBroyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more\nthan 10 times faster than one iteration of Newton's method without compromising\nquality. In fact, our result is often more accurate than the result obtained\nwith one iteration of Newton's method. Our method is also easier to implement,\nimplying reduced software development costs."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1604.08848v1", 
    "title": "Augmented Reality Oculus Rift", 
    "arxiv-id": "1604.08848v1", 
    "author": "Vincent Lepetit", 
    "publish": "2016-04-29T14:26:55Z", 
    "summary": "This paper covers the whole process of developing an Augmented Reality\nStereoscopig Render Engine for the Oculus Rift. To capture the real world in\nform of a camera stream, two cameras with fish-eye lenses had to be installed\non the Oculus Rift DK1 hardware. The idea was inspired by Steptoe\n\\cite{steptoe2014presence}. After the introduction, a theoretical part covers\nall the most neccessary elements to achieve an AR System for the Oculus Rift,\nfollowing the implementation part where the code from the AR Stereo Engine is\nexplained in more detail. A short conclusion section shows some results,\nreflects some experiences and in the final chapter some future works will be\ndiscussed. The project can be accessed via the git repository\nhttps://github.com/MaXvanHeLL/ARift.git."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1605.01760v1", 
    "title": "Adaptive Mesh Booleans", 
    "arxiv-id": "1605.01760v1", 
    "author": "Tyson Brochu", 
    "publish": "2016-05-05T21:25:54Z", 
    "summary": "We present a new method for performing Boolean operations on volumes\nrepresented as triangle meshes. In contrast to existing methods which treat\nmeshes as 3D polyhedra and try to partition the faces at their exact\nintersection curves, we treat meshes as adaptive surfaces which can be\narbitrarily refined. Rather than depending on computing precise face\nintersections, our approach refines the input meshes in the intersection\nregions, then discards intersecting triangles and fills the resulting holes\nwith high-quality triangles. The original intersection curves are approximated\nto a user-definable precision, and our method can identify and preserve creases\nand sharp features. Advantages of our approach include the ability to trade\nspeed for accuracy, support for open meshes, and the ability to incorporate\ntolerances to handle cases where large numbers of faces are slightly\ninter-penetrating or near-coincident."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1605.04797v2", 
    "title": "Thingi10K: A Dataset of 10,000 3D-Printing Models", 
    "arxiv-id": "1605.04797v2", 
    "author": "Alec Jacobson", 
    "publish": "2016-05-16T15:09:19Z", 
    "summary": "Empirically validating new 3D-printing related algorithms and implementations\nrequires testing data representative of inputs encountered \\emph{in the wild}.\nAn ideal benchmarking dataset should not only draw from the same distribution\nof shapes people print in terms of class (e.g., toys, mechanisms, jewelry),\nrepresentation type (e.g., triangle soup meshes) and complexity (e.g., number\nof facets), but should also capture problems and artifacts endemic to 3D\nprinting models (e.g., self-intersections, non-manifoldness). We observe that\nthe contextual and geometric characteristics of 3D printing models differ\nsignificantly from those used for computer graphics applications, not to\nmention standard models (e.g., Stanford bunny, Armadillo, Fertility). We\npresent a new dataset of 10,000 models collected from an online 3D printing\nmodel-sharing database. Via analysis of both geometric (e.g., triangle aspect\nratios, manifoldness) and contextual (e.g., licenses, tags, classes)\ncharacteristics, we demonstrate that this dataset represents a more concise\nsummary of real-world models used for 3D printing compared to existing\ndatasets. To facilitate future research endeavors, we also present an online\nquery interface to select subsets of the dataset according to project-specific\ncharacteristics. The complete dataset and per-model statistical data are freely\navailable to the public."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1605.09737v1", 
    "title": "3D Printed Stencils for Texturing Flat Surfaces", 
    "arxiv-id": "1605.09737v1", 
    "author": "Vaibhav Vavilala", 
    "publish": "2016-05-31T17:39:49Z", 
    "summary": "We address the problem of texturing flat surfaces by spray-painting through\n3D printed stencils. We propose a system that (1) decomposes an image into\nalpha-blended layers; (2) computes a stippling given a transparency channel;\n(3) generates a 3D printed stencil given a stippling and (4) simulates the\neffects of spray-painting through the stencil."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1606.09540v1", 
    "title": "SurfCuit: Surface Mounted Circuits on 3D Prints", 
    "arxiv-id": "1606.09540v1", 
    "author": "Ryan Schmidt", 
    "publish": "2016-06-30T15:37:53Z", 
    "summary": "We present, SurfCuit, a novel approach to design and construction of electric\ncircuits on the surface of 3D prints. Our surface mounting technique allows\ndurable construction of circuits on the surface of 3D prints. SurfCuit does not\nrequire tedious circuit casing design or expensive set-ups, thus we can\nexpedite the process of circuit construction for 3D models. Our technique\nallows the user to construct complex circuits for consumer-level desktop fused\ndecomposition modeling (FDM) 3D printers. The key idea behind our technique is\nthat FDM plastic forms a strong bond with metal when it is melted. This\nobservation enables construction of a robust circuit traces using copper tape\nand soldering. We also present an interactive tool to design such circuits on\narbitrary 3D geometry. We demonstrate the effectiveness of our approach through\nvarious actual construction examples."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1607.01102v1", 
    "title": "A Visualization Method of Four Dimensional Polytopes by Oval Display of   Parallel Hyperplane Slices", 
    "arxiv-id": "1607.01102v1", 
    "author": "Akira Kageyama", 
    "publish": "2016-07-05T03:22:40Z", 
    "summary": "A method to visualize polytopes in a four dimensional euclidian space\n$(x,y,z,w)$ is proposed. A polytope is sliced by multiple hyperplanes that are\nparallel each other and separated by uniform intervals. Since the hyperplanes\nare perpendicular to the $w$ axis, the resulting multiple slices appear in the\nthree-dimensional $(x,y,z)$ space and they are shown by the standard computer\ngraphics. The polytope is rotated extrinsically in the four dimensional space\nby means of a simple input method based on keyboard typings. The multiple\nslices are placed on a parabola curve in the three-dimensional world\ncoordinates. The slices in a view window form an oval appearance. Both the\nsimple and the double rotations in the four dimensional space are applied to\nthe polytope. All slices synchronously change their shapes when a rotation is\napplied to the polytope. The compact display in the oval of many slices with\nthe help of quick rotations facilitate a grasp of the four dimensional\nconfiguration of the polytope."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1607.05812v1", 
    "title": "HoloMed: A Low-Cost Gesture-Based Holographic", 
    "arxiv-id": "1607.05812v1", 
    "author": "Esmitt Ram\u00edrez", 
    "publish": "2016-07-20T04:00:44Z", 
    "summary": "During medicine studies, visualization of certain elements is common and\nindispensable in order to get more information about the way they work.\nCurrently, we resort to the use of photographs -which are insufficient due to\nbeing static- or tests in patients, which can be invasive or even risky.\nTherefore, a low-cost approach is proposed by using a 3D visualization. This\npaper presents a holographic system built with low-cost materials for teaching\nobstetrics, where student interaction is performed by using voice and gestures.\nOur solution, which we called HoloMed, is focused on the projection of a\neuthocic normal delivery under a web-based infrastructure which also employs a\nKinect. HoloMed is divided in three (3) essential modules: a gesture analyzer,\na data server, and a holographic projection architecture, which can be executed\nin several interconnected computers using different network protocols. Tests\nused for determining the user's position, illumination factors, and response\ntimes, demonstrate HoloMed's effectiveness as a low-cost system for teaching,\nusing a natural user interface and 3D images."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1608.01102v1", 
    "title": "Efficient Optimal Control of Smoke using Spacetime Multigrid", 
    "arxiv-id": "1608.01102v1", 
    "author": "Dinesh Manocha", 
    "publish": "2016-08-03T08:04:24Z", 
    "summary": "We present a novel algorithm to control the physically-based animation of\nsmoke. Given a set of keyframe smoke shapes, we compute a dense sequence of\ncontrol force fields that can drive the smoke shape to match several keyframes\nat certain time instances. Our approach formulates this control problem as a\nPDE constrained spacetime optimization and computes locally optimal control\nforces as the stationary point of the Karush-Kuhn-Tucker conditions. In order\nto reduce the high complexity of multiple passes of fluid resimulation, we\nutilize the coherence between consecutive fluid simulation passes and update\nour solution using a novel spacetime full approximation scheme (STFAS). We\ndemonstrate the benefits of our approach by computing accurate solutions on 2D\nand 3D benchmarks. In practice, we observe more than an order of magnitude\nimprovement over prior methods."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.amc.2016.08.004", 
    "link": "http://arxiv.org/pdf/1608.01933v1", 
    "title": "Geoplotlib: a Python Toolbox for Visualizing Geographical Data", 
    "arxiv-id": "1608.01933v1", 
    "author": "Jakob Eg Larsen", 
    "publish": "2016-08-05T16:39:27Z", 
    "summary": "We introduce geoplotlib, an open-source python toolbox for visualizing\ngeographical data. geoplotlib supports the development of hardware-accelerated\ninteractive visualizations in pure python, and provides implementations of dot\nmaps, kernel density estimation, spatial graphs, Voronoi tesselation,\nshapefiles and many more common spatial visualizations. We describe geoplotlib\ndesign, functionalities and use cases."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2016.07.004", 
    "link": "http://arxiv.org/pdf/1608.04082v1", 
    "title": "A weighted binary average of point-normal pairs with application to   subdivision schemes", 
    "arxiv-id": "1608.04082v1", 
    "author": "Nira Dyn", 
    "publish": "2016-08-14T09:55:34Z", 
    "summary": "Subdivision is a well-known and established method for generating smooth\ncurves and surfaces from discrete data by repeated refinements. The typical\ninput for such a process is a mesh of vertices. In this work we propose to\nrefine 2D data consisting of vertices of a polygon and a normal at each vertex.\nOur core refinement procedure is based on a circle average, which is a new\nnon-linear weighted average of two points and their corresponding normals. The\nability to locally approximate curves by the circle average is demonstrated.\nWith this ability, the circle average is a candidate for modifying linear\nsubdivision schemes refining points, to schemes refining point-normal pairs.\nThis is done by replacing the weighted binary arithmetic means in a linear\nsubdivision scheme, expressed in terms of repeated binary averages, by circle\naverages with the same weights. Here we investigate the modified\nLane-Riesenfeld algorithm and the 4-point scheme. For the case that the initial\ndata consists of a control polygon only, a naive method for choosing initial\nnormals is proposed. An example demonstrates the superiority of the above two\nmodified schemes, with the naive choice of initial normals over the\ncorresponding linear schemes, when applied to a control polygon with edges of\nsignificantly different lengths."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2016.07.004", 
    "link": "http://arxiv.org/pdf/1608.04366v2", 
    "title": "Infill Optimization for Additive Manufacturing -- Approaching Bone-like   Porous Structures", 
    "arxiv-id": "1608.04366v2", 
    "author": "Ole Sigmund", 
    "publish": "2016-08-15T19:02:30Z", 
    "summary": "Porous structures such as trabecular bone are widely seen in nature. These\nstructures exhibit superior mechanical properties whilst being lightweight. In\nthis paper, we present a method to generate bone-like porous structures as\nlightweight infill for additive manufacturing. Our method builds upon and\nextends voxel-wise topology optimization. In particular, for the purpose of\ngenerating sparse yet stable structures distributed in the interior of a given\nshape, we propose upper bounds on the localized material volume in the\nproximity of each voxel in the design domain. We then aggregate the local\nper-voxel constraints by their p-norm into an equivalent global constraint, in\norder to facilitate an efficient optimization process. Implemented on a\nhigh-resolution topology optimization framework, our results demonstrate\nmechanically optimized, detailed porous structures which mimic those found in\nnature. We further show variants of the optimized structures subject to\ndifferent design specifications, and analyze the optimality and robustness of\nthe obtained structures."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1608.04721v1", 
    "title": "Adaptive Position-Based Fluids: Improving Performance of Fluid   Simulations for Real-Time Applications", 
    "arxiv-id": "1608.04721v1", 
    "author": "Antonio Kr\u00fcger", 
    "publish": "2016-08-16T19:38:16Z", 
    "summary": "The Position Based Fluids (PBF) method is a state-of-the-art approach for\nfluid simulations in the context of real-time applications like games. It uses\nan iterative solver concept that tries to maintain a constant fluid density\n(incompressibility) to realize incompressible fluids like water. However,\nlarger fluid volumes that consist of several hundred thousand particles (e.g.\nfor the simulation of oceans) require many iterations and a lot of simulation\npower. We present a lightweight and easy-to-integrate extension to PBF that\nadaptively adjusts the number of solver iterations on a fine-grained basis.\nUsing a novel adaptive-simulation approach, we are able to achieve significant\nimprovements in performance on our evaluation scenarios while maintaining\nhigh-quality results in terms of visualization quality, which makes it a\nperfect choice for game developers. Furthermore, our method does not weaken the\nadvantages of prior work and seamlessly integrates into other position-based\nmethods for physically-based simulations."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1608.05231v2", 
    "title": "Design and Implementation of a Procedural Content Generation Web   Application for Vertex Shaders", 
    "arxiv-id": "1608.05231v2", 
    "author": "Sergiu M. Dascalu", 
    "publish": "2016-08-18T10:34:54Z", 
    "summary": "We present a web application for the procedural generation of transformations\nof 3D models. We generate the transformations by algorithmically generating the\nvertex shaders of the 3D models. The vertex shaders are created with an\ninteractive genetic algorithm, which displays to the user the visual effect\ncaused by each vertex shader, allows the user to select the visual effect the\nuser likes best, and produces a new generation of vertex shaders using the user\nfeedback as the fitness measure of the genetic algorithm. We use genetic\nprogramming to represent each vertex shader as a computer program. This paper\npresents details of requirements specification, software architecture, high and\nlow-level design, and prototype user interface. We discuss the project's\ncurrent status and development challenges."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1608.05772v1", 
    "title": "A Data-Driven Approach for Mapping Multivariate Data to Color", 
    "arxiv-id": "1608.05772v1", 
    "author": "Klaus Mueller", 
    "publish": "2016-08-20T03:18:57Z", 
    "summary": "A wide variety of color schemes have been devised for mapping scalar data to\ncolor. Some use the data value to index a color scale. Others assign colors to\ndifferent, usually blended disjoint materials, to handle areas where materials\noverlap. A number of methods can map low-dimensional data to color, however,\nthese methods do not scale to higher dimensional data. Likewise, schemes that\ntake a more artistic approach through color mixing and the like also face\nlimits when it comes to the number of variables they can encode. We address the\nchallenge of mapping multivariate data to color and avoid these limitations at\nthe same time. It is a data driven method, which first gauges the similarity of\nthe attributes and then arranges them according to the periphery of a convex 2D\ncolor space, such as HSL. The color of a multivariate data sample is then\nobtained via generalized barycentric coordinate (GBC) interpolation."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1608.05773v1", 
    "title": "Extending Scatterplots to Scalar Fields", 
    "arxiv-id": "1608.05773v1", 
    "author": "Klaus Mueller", 
    "publish": "2016-08-20T03:25:50Z", 
    "summary": "Embedding high-dimensional data into a 2D canvas is a popular strategy for\ntheir visualization."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1608.08570v1", 
    "title": "Interpolations of Smoke and Liquid Simulations", 
    "arxiv-id": "1608.08570v1", 
    "author": "Nils Thuerey", 
    "publish": "2016-08-30T17:31:36Z", 
    "summary": "We present a novel method to interpolate smoke and liquid simulations in\norder to perform data-driven fluid simulations. Our approach calculates a dense\nspace-time deformation using grid-based signed-distance functions of the\ninputs. A key advantage of this implicit Eulerian representation is that it\nallows us to use powerful techniques from the optical flow area. We employ a\nfive-dimensional optical flow solve. In combination with a projection\nalgorithm, and residual iterations, we achieve a robust matching of the inputs.\nOnce the match is computed, arbitrary in between variants can be created very\nefficiently. To concatenate multiple long-range deformations, we propose a\nnovel alignment technique. Our approach has numerous advantages, including\nautomatic matches without user input, volumetric deformations that can be\napplied to details around the surface, and the inherent handling of topology\nchanges. As a result, we can interpolate swirling smoke clouds, and splashing\nliquid simulations. We can even match and interpolate phenomena with\nfundamentally different physics: a drop of liquid, and a blob of heavy smoke."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1609.01317v1", 
    "title": "Volume Raycasting mit OpenCL", 
    "arxiv-id": "1609.01317v1", 
    "author": "Nils Kopal", 
    "publish": "2016-09-05T20:26:24Z", 
    "summary": "This German paper was written entirely at the University of Duisburg-Essen in\n2011 for a 3D modeling masters course in applied computer science. We publish\nthis paper, thus, interested people can acquire a first impression of the topic\n\"volume raycasting\". In addition to writing this paper, we developed a\nfunctioning open-source OpenCL raycaster. A video of this raycaster is\navailable: http://www.youtube.com/watch?v=VMMsQnf4zEY. Additionally, we\narchived and published the complete source code of the raycaster in the Google\nCode Archive: http://code.google.com/p/gputracer/. If this is no longer the\ncase, those who are interested can also write an email to the author, hence, we\ncan provide the source code.\n  This paper provides an introduction and overview of the topic \"volume ray\ncasting with OpenCL\". We show how volume data can be loaded, manipulated, and\nvisualized by modern GPUs in real time. In addition, we present basic\nalgorithms and data structures that are necessary for building such a\nraycaster. Then, we describe how we built a rudimentary raycaster using OpenCL\nand .NET C#. Furthermore, we analyze different gradient operators\n(CentralDifference, Sobel3D and Zucker-Hummel) for surface detection and show\nan evaluation of these with respect to their performance. Finally, we present\noptimization techniques (hitpoint refinement, adaptive sampling, octrees, and\nempty-space-skipping) for improving a raycaster."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1609.02072v1", 
    "title": "Sampling BSSRDFs with non-perpendicular incidence", 
    "arxiv-id": "1609.02072v1", 
    "author": "Etienne Ferrier", 
    "publish": "2016-09-07T17:03:19Z", 
    "summary": "Sub-surface scattering is key to our perception of translucent materials.\nModels based on diffusion theory are used to render such materials in a\nrealistic manner by evaluating an approximation of the material BSSRDF at any\ntwo points of the surface. Under the assumption of perpendicular incidence,\nthis BSSRDF approximation can be tabulated over 2 dimensions to provide fast\nevaluation and importance sampling. However, accounting for non-perpendicular\nincidence with the same approach would require to tabulate over 4 dimensions,\nmaking the model too large for practical applications. In this report, we\npresent a method to efficiently evaluate and importance sample the\nmulti-scattering component of diffusion based BSSRDFs for non-perpendicular\nincidence. Our approach is based on tabulating a compressed angular model of\nPhoton Beam Diffusion. We explain how to generate, evaluate and sample our\nmodel. We show that 1 MiB is enough to store a model of the multi-scattering\nBSSRDF that is within $0.5\\%$ relative error of Photon Beam Diffusion. Finally,\nwe present a method to use our model in a Monte Carlo particle tracer and show\nresults of our implementation in PBRT."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1609.03032v1", 
    "title": "Anti-aliasing for fused filament deposition", 
    "arxiv-id": "1609.03032v1", 
    "author": "Sylvain Lefebvre", 
    "publish": "2016-09-10T11:46:08Z", 
    "summary": "Layered manufacturing inherently suffers from staircase defects along\nsurfaces that are gently slopped with respect to the build direction. Reducing\nthe slice thickness improves the situation but never resolves it completely as\nflat layers remain a poor approximation of the true surface in these regions.\nIn addition, reducing the slice thickness largely increases the print time. In\nthis work we focus on a simple yet effective technique to improve the print\naccuracy for layered manufacturing by filament deposition. Our method works\nwith standard three-axis 3D filament printers (e.g. the typical, widely\navailable 3D printers), using standard extrusion nozzles. It better reproduces\nthe geometry of sloped surfaces without increasing the print time. Our key idea\nis to perform a local anti-aliasing, working at a sub-layer accuracy to produce\nslightly curved deposition paths and reduce approximation errors. This is\ninspired by Computer Graphics anti-aliasing techniques which consider sub-pixel\nprecision to treat aliasing effects. We show that the necessary deviation in\nheight compared to standard slicing is bounded by half the layer thickness.\nTherefore, the height changes remain small and plastic deposition remains\nreliable. We further split and order paths to minimize defects due to the\nextruder nozzle shape, avoiding any change to the existing hardware. We apply\nand analyze our approach on 3D printed examples, showing that our technique\ngreatly improves surface accuracy and silhouette quality while keeping the\nprint time nearly identical."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1609.05344v1", 
    "title": "Optimisations for Real-Time Volumetric Cloudscapes", 
    "arxiv-id": "1609.05344v1", 
    "author": "Daniel Zimmermann", 
    "publish": "2016-09-17T14:43:17Z", 
    "summary": "Volumetric cloudscapes are prohibitively expensive to render in real time\nwithout extensive optimisations. Previous approaches render the clouds to an\noffscreen buffer at one quarter resolution and update a fraction of the pixels\nper frame, drawing the remaining pixels by temporal reprojection. We present an\nalternative approach, reducing the number of raymarching steps and adding a\nrandomly jittered offset to the raymarch. We use an analytical integration\ntechnique to make results consistent with a lower number of raymarching steps.\nTo remove noise from the resulting image we apply a temporal anti-aliasing\nimplementation. The result is a technique producing visually similar results\nwith 1/16 the number of steps."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1609.08313v1", 
    "title": "Unsupervised Co-segmentation of 3D Shapes via Functional Maps", 
    "arxiv-id": "1609.08313v1", 
    "author": "Zhenhua Tian", 
    "publish": "2016-09-27T08:35:14Z", 
    "summary": "We present an unsupervised method for co-segmentation of a set of 3D shapes\nfrom the same class with the aim of segmenting the input shapes into consistent\nsemantic parts and establishing their correspondence across the set. Starting\nfrom meaningful pre-segmentation of all given shapes individually, we construct\nthe correspondence between same candidate parts and obtain the labels via\nfunctional maps. And then, we use these labels to mark the input shapes and\nobtain results of co-segmentation. The core of our algorithm is to seek for an\noptimal correspondence between semantically similar parts through functional\nmaps and mark such shape parts. Experimental results on the benchmark datasets\nshow the efficiency of this method and comparable accuracy to the\nstate-of-the-art algorithms."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.00402v2", 
    "title": "Dynamic Polygon Clouds: Representation and Compression for VR/AR", 
    "arxiv-id": "1610.00402v2", 
    "author": "Antonio Ortega", 
    "publish": "2016-10-03T04:25:18Z", 
    "summary": "We introduce the {\\em polygon cloud}, also known as a polygon set or {\\em\nsoup}, as a compressible representation of 3D geometry (including its\nattributes, such as color texture) intermediate between polygonal meshes and\npoint clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal\nmeshes and dynamic point clouds, can take advantage of temporal redundancy for\ncompression, if certain challenges are addressed. In this paper, we propose\nmethods for compressing both static and dynamic polygon clouds, specifically\ntriangle clouds. We compare triangle clouds to both triangle meshes and point\nclouds in terms of compression, for live captured dynamic colored geometry. We\nfind that triangle clouds can be compressed nearly as well as triangle meshes,\nwhile being far more robust to noise and other structures typically found in\nlive captures, which violate the assumption of a smooth surface manifold, such\nas lines, points, and ragged boundaries. We also find that triangle clouds can\nbe used to compress point clouds with significantly better performance than\npreviously demonstrated point cloud compression methods. In particular, for\nintra-frame coding of geometry, our method improves upon octree-based\nintra-frame coding by a factor of 5-10 in bit rate. Inter-frame coding improves\nthis by another factor of 2-5. Overall, our dynamic triangle cloud compression\nimproves over the previous state-of-the-art in dynamic point cloud compression\nby 33\\% or more."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.02769v1", 
    "title": "Inverse Diffusion Curves using Shape Optimization", 
    "arxiv-id": "1610.02769v1", 
    "author": "Changxi Zheng", 
    "publish": "2016-10-10T03:57:40Z", 
    "summary": "The inverse diffusion curve problem focuses on automatic creation of\ndiffusion curve images that resemble user provided color fields. This problem\nis challenging since the 1D curves have a nonlinear and global impact on\nresulting color fields via a partial differential equation (PDE). We introduce\na new approach complementary to previous methods by optimizing curve geometry.\nIn particular, we propose a novel iterative algorithm based on the theory of\nshape derivatives. The resulting diffusion curves are clean and well-shaped,\nand the final image closely approximates the input. Our method provides a\nuser-controlled parameter to regularize curve complexity, and generalizes to\nhandle input color fields represented in a variety of formats."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.03525v2", 
    "title": "Polynomial method for Procedural Terrain Generation", 
    "arxiv-id": "1610.03525v2", 
    "author": "Yann Thorimbert", 
    "publish": "2016-10-11T20:51:48Z", 
    "summary": "A systematic fractal brownian motion approach is proposed for generating\ncoherent noise, aiming at procedurally generating realistic terrain and\ntextures. Two models are tested and compared to Perlin noise method for\ntwo-dimensional height map generation. A fractal analysis is performed in order\nto compare fractal behaviour of generated data to real terrain coastlines from\nthe point of view of fractal dimension. Performance analysis show that one of\nthe described schemes requires half as many primitive operations than Perlin\nnoise while producing data of equivalent quality."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.04281v1", 
    "title": "Augmented Reality with Hololens: Experiential Architectures Embedded in   the Real World", 
    "arxiv-id": "1610.04281v1", 
    "author": "Tim Ingleby", 
    "publish": "2016-10-13T22:32:08Z", 
    "summary": "Early hands-on experiences with the Microsoft Hololens augmented/mixed\nreality device are reported and discussed, with a general aim of exploring\nbasic 3D visualization. A range of usage cases are tested, including data\nvisualization and immersive data spaces, in-situ visualization of 3D models and\nfull scale architectural form visualization. Ultimately, the Hololens is found\nto provide a remarkable tool for moving from traditional visualization of 3D\nobjects on a 2D screen, to fully experiential 3D visualizations embedded in the\nreal world."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.07368v2", 
    "title": "Simplification of Multi-Scale Geometry using Adaptive Curvature Fields", 
    "arxiv-id": "1610.07368v2", 
    "author": "Michael Goesele", 
    "publish": "2016-10-24T11:34:13Z", 
    "summary": "We present a novel algorithm to compute multi-scale curvature fields on\ntriangle meshes. Our algorithm is based on finding robust mean curvatures using\nthe ball neighborhood, where the radius of a ball corresponds to the scale of\nthe features. The essential problem is to find a good radius for each ball to\nobtain a reliable curvature estimation. We propose an algorithm that finds\nsuitable radii in an automatic way. In particular, our algorithm is applicable\nto meshes produced by image-based reconstruction systems. These meshes often\ncontain geometric features at various scales, for example if certain regions\nhave been captured in greater detail. We also show how such a multi-scale\ncurvature field can be converted to a density field and used to guide\napplications like mesh simplification."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2016.6301", 
    "link": "http://arxiv.org/pdf/1610.09988v1", 
    "title": "Selecting the Best Quadrilateral Mesh for Given Planar Shape", 
    "arxiv-id": "1610.09988v1", 
    "author": "Petra Surynkova", 
    "publish": "2016-10-31T15:56:30Z", 
    "summary": "The problem of mesh matching is addressed in this work. For a given n-sided\nplanar region bounded by one loop of n polylines we are selecting optimal\nquadrilateral mesh from existing catalogue of meshes. The formulation of\nmatching between planar shape and quadrilateral mesh from the catalogue is\nbased on the problem of finding longest common subsequence (LCS). Theoretical\nfoundation of mesh matching method is provided. Suggested method represents a\nviable technique for selecting best mesh for planar region and stepping stone\nfor further parametrization of the region."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cad.2016.10.003", 
    "link": "http://arxiv.org/pdf/1611.01765v1", 
    "title": "A Survey on 3D CAD model quality assurance and testing tools", 
    "arxiv-id": "1611.01765v1", 
    "author": "R. Plumed", 
    "publish": "2016-11-06T12:26:37Z", 
    "summary": "A new taxonomy of issues related to CAD model quality is presented, which\ndistinguishes between explicit and procedural models. For each type of model,\nmorphologic, syntactic, and semantic errors are characterized. The taxonomy was\nvalidated successfully when used to classify quality testing tools, which are\naimed at detecting and repairing data errors that may affect the\nsimplification, interoperability, and reusability of CAD models. The study\nshows that low semantic level errors that hamper simplification are reasonably\ncovered in explicit representations, although many CAD quality testers are\nstill unaffordable for Small and Medium Enterprises, both in terms of cost and\ntraining time. Interoperability has been reasonably solved by standards like\nSTEP AP 203 and AP214, but model reusability is not feasible in explicit\nrepresentations. Procedural representations are promising, as interactive\nmodeling editors automatically prevent most morphologic errors derived from\nunsuitable modeling strategies. Interoperability problems between procedural\nrepresentations are expected to decrease dramatically with STEP AP242. Higher\nsemantic aspects of quality such as assurance of design intent, however, are\nhardly supported by current CAD quality testers."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cad.2016.10.003", 
    "link": "http://arxiv.org/pdf/1611.03079v1", 
    "title": "Fractal Art Generation using GPUs", 
    "arxiv-id": "1611.03079v1", 
    "author": "Bryant. M. Wyatt", 
    "publish": "2016-11-08T23:22:59Z", 
    "summary": "Fractal image generation algorithms exhibit extreme parallelizability. Using\ngeneral purpose graphics processing unit (GPU) programming to implement\nescape-time algorithms for Julia sets of functions,parallel methods generate\nvisually attractive fractal images much faster than traditional methods. Vastly\nimproved speeds are achieved using this method of computation, which allow\nreal-time generation and display of images. A comparison is made between\nsequential and parallel implementations of the algorithm. An application\ncreated by the authors demonstrates using the increased speed to create dynamic\nimaging of fractals where the user may explore paths of parameter values\ncorresponding to a given function's Mandelbrot set. Examples are given of\nartistic and mathematical insights gained by experiencing fractals\ninteractively and from the ability to sample the parameter space quickly and\ncomprehensively."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cad.2016.10.003", 
    "link": "http://arxiv.org/pdf/1611.03677v1", 
    "title": "Primal-Dual Optimization for Fluids", 
    "arxiv-id": "1611.03677v1", 
    "author": "Nils Thuerey", 
    "publish": "2016-11-11T12:20:43Z", 
    "summary": "We apply a novel optimization scheme from the image processing and machine\nlearning areas, a fast Primal-Dual method, to achieve controllable and\nrealistic fluid simulations. While our method is generally applicable to many\nproblems in fluid simulations, we focus on the two topics of fluid guiding and\nseparating solid-wall boundary conditions. Each problem is posed as an\noptimization problem and solved using our method, which contains acceleration\nschemes tailored to each problem. In fluid guiding, we are interested in\npartially guiding fluid motion to exert control while preserving fluid\ncharacteristics. With our method, we achieve explicit control over both\nlarge-scale motions and small-scale details which is valuable for many\napplications, such as level-of-detail adjustment (after running the coarse\nsimulation), spatially varying guiding strength, domain modification, and\nresimulation with different fluid parameters. For the separating solid-wall\nboundary conditions problem, our method effectively eliminates unrealistic\nartifacts of fluid crawling up solid walls and sticking to ceilings, requiring\nfew changes to existing implementations. We demonstrate the fast convergence of\nour Primal-Dual method with a variety of test cases for both model problems."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cad.2016.10.003", 
    "link": "http://arxiv.org/pdf/1611.08947v1", 
    "title": "Navigable videos for presenting scientific data on head-mounted displays", 
    "arxiv-id": "1611.08947v1", 
    "author": "Kwan-Liu Ma", 
    "publish": "2016-11-28T01:03:57Z", 
    "summary": "Immersive, stereoscopic viewing enables scientists to better analyze the\nspatial structures of visualized physical phenomena. However, their findings\ncannot be properly presented in traditional media, which lack these core\nattributes. Creating a presentation tool that captures this environment poses\nunique challenges, namely related to poor viewing accessibility. Immersive\nscientific renderings often require high-end equipment, which can be\nimpractical to obtain. We address these challenges with our authoring tool and\nnavigational interface, which is designed for affordable head-mounted displays.\nWith the authoring tool, scientists can show salient data features as connected\n360{\\deg} video paths, resulting in a \"choose-your-own-adventure\" experience.\nOur navigational interface features bidirectional video playback for added\nviewing control when users traverse the tailor-made content. We evaluate our\nsystem's benefits by authoring case studies on several data sets and conducting\na usability study on the navigational interface's design. In summary, our\napproach provides scientists an immersive medium to visually present their\nresearch to the intended audience--spanning from students to colleagues--on\naffordable virtual reality headsets."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1612.04336v1", 
    "title": "A Qualitative and Quantitative Evaluation of 8 Clear Sky Models", 
    "arxiv-id": "1612.04336v1", 
    "author": "Eric Bruneton", 
    "publish": "2016-12-13T20:03:36Z", 
    "summary": "We provide a qualitative and quantitative evaluation of 8 clear sky models\nused in Computer Graphics. We compare the models with each other as well as\nwith measurements and with a reference model from the physics community. After\na short summary of the physics of the problem, we present the measurements and\nthe reference model, and how we \"invert\" it to get the model parameters. We\nthen give an overview of each CG model, and detail its scope, its algorithmic\ncomplexity, and its results using the same parameters as in the reference\nmodel. We also compare the models with a perceptual study. Our quantitative\nresults confirm that the less simplifications and approximations are used to\nsolve the physical equations, the more accurate are the results. We conclude\nwith a discussion of the advantages and drawbacks of each model, and how to\nfurther improve their accuracy."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1612.05064v1", 
    "title": "Orthogonal Edge Routing for the EditLens", 
    "arxiv-id": "1612.05064v1", 
    "author": "Christian Tominski", 
    "publish": "2016-12-15T13:52:30Z", 
    "summary": "The EditLens is an interactive lens technique that supports the editing of\ngraphs. The user can insert, update, or delete nodes and edges while\nmaintaining an already existing layout of the graph. For the nodes and edges\nthat are affected by an edit operation, the EditLens suggests suitable\nlocations and routes, which the user can accept or adjust. For this purpose,\nthe EditLens requires an efficient routing algorithm that can compute results\nat interactive framerates. Existing algorithms cannot fully satisfy the needs\nof the EditLens. This paper describes a novel algorithm that can compute\northogonal edge routes for incremental edit operations of graphs. Tests\nindicate that, in general, the algorithm is better than alternative solutions."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1612.05395v5", 
    "title": "Charted Metropolis Light Transport", 
    "arxiv-id": "1612.05395v5", 
    "author": "Jacopo Pantaleoni", 
    "publish": "2016-12-16T08:41:06Z", 
    "summary": "In this manuscript, inspired by a simpler reformulation of primary sample\nspace Metropolis light transport, we derive a novel family of general Markov\nchain Monte Carlo algorithms called charted Metropolis-Hastings, that\nintroduces the notion of sampling charts to extend a given sampling domain and\nmaking it easier to sample the desired target distribution and escape from\nlocal maxima through coordinate changes. We further apply the novel algorithms\nto light transport simulation, obtaining a new type of algorithm called charted\nMetropolis light transport, that can be seen as a bridge between primary sample\nspace and path space Metropolis light transport. The new algorithms require to\nprovide only right inverses of the sampling functions, a property that we\nbelieve crucial to make them practical in the context of light transport\nsimulation. We further propose a method to integrate density estimation into\nthis framework through a novel scheme that uses it as an independence sampler."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1612.07353v1", 
    "title": "Data-driven Shoulder Inverse Kinematics", 
    "arxiv-id": "1612.07353v1", 
    "author": "JungHyun Han", 
    "publish": "2016-11-17T02:11:59Z", 
    "summary": "This paper proposes a shoulder inverse kinematics (IK) technique. Shoulder\ncomplex is comprised of the sternum, clavicle, ribs, scapula, humerus, and four\njoints. The shoulder complex shows specific motion pattern, such as Scapulo\nhumeral rhythm. As a result, if a motion of the shoulder isgenerated without\nthe knowledge of kinesiology, it will be seen as un-natural. The proposed\ntechnique generates motion of the shoulder complex about the orientation of the\nupper arm by interpolating the measurement data. The shoulder IK method allows\nnovice animators to generate natural shoulder motions easily. As a result, this\ntechnique improves the quality of character animation."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1612.08731v2", 
    "title": "Quantum Optimal Transport for Tensor Field Processing", 
    "arxiv-id": "1612.08731v2", 
    "author": "Justin Solomon", 
    "publish": "2016-12-20T14:06:20Z", 
    "summary": "This article introduces a new notion of optimal transport (OT) between tensor\nfields, which are measures whose values are positive semidefinite matrices\n(PSD). This \"quantum\"' formulation of OT corresponds to a relaxed version of\nthe classical Kantorovich transport problem, where the fidelity between the\ninput PSD-valued measures is captured using the geometry of the Von-Neumann\nquantum entropy. We propose a quantum-entropic regularization of the resulting\nconvex optimization problem, which can be solved efficiently using an iterative\nscaling algorithm. This method is a generalization of the celebrated Sinkhorn\nalgorithm to the quantum setting of PSD matrices. We extend this formulation\nand the quantum Sinkhorn algorithm to compute barycenters within a collection\nof input tensor fields."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1701.03754v2", 
    "title": "LayerBuilder: Layer Decomposition for Interactive Image and Video Color   Editing", 
    "arxiv-id": "1701.03754v2", 
    "author": "Pat Hanrahan", 
    "publish": "2017-01-13T17:48:44Z", 
    "summary": "Exploring and editing colors in images is a common task in graphic design and\nphotography. However, allowing for interactive recoloring while preserving\nsmooth color blends in the image remains a challenging problem. We present\nLayerBuilder, an algorithm that decomposes an image or video into a linear\ncombination of colored layers to facilitate color-editing applications. These\nlayers provide an interactive and intuitive means for manipulating individual\ncolors. Our approach reduces color layer extraction to a fast iterative linear\nsystem. Layer Builder uses locally linear embedding, which represents pixels as\nlinear combinations of their neighbors, to reduce the number of variables in\nthe linear solve and extract layers that can better preserve color blending\neffects. We demonstrate our algorithm on recoloring a variety of images and\nvideos, and show its overall effectiveness in recoloring quality and time\ncomplexity compared to previous approaches. We also show how this\nrepresentation can benefit other applications, such as automatic recoloring\nsuggestion, texture synthesis, and color-based filtering."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1701.04303v1", 
    "title": "Poisson Vector Graphics (PVG) and Its Closed-Form Solver", 
    "arxiv-id": "1701.04303v1", 
    "author": "Ying He", 
    "publish": "2017-01-16T14:33:15Z", 
    "summary": "This paper presents Poisson vector graphics, an extension of the popular\nfirst-order diffusion curves, for generating smooth-shaded images. Armed with\ntwo new types of primitives, namely Poisson curves and Poisson regions, PVG can\neasily produce photorealistic effects such as specular highlights, core\nshadows, translucency and halos. Within the PVG framework, users specify color\nas the Dirichlet boundary condition of diffusion curves and control tone by\noffsetting the Laplacian, where both controls are simply done by mouse click\nand slider dragging. The separation of color and tone not only follows the\nbasic drawing principle that is widely adopted by professional artists, but\nalso brings three unique features to PVG, i.e., local hue change, ease of\nextrema control, and permit of intersection among geometric primitives, making\nPVG an ideal authoring tool.\n  To render PVG, we develop an efficient method to solve 2D Poisson's equations\nwith piecewise constant Laplacians. In contrast to the conventional finite\nelement method that computes numerical solutions only, our method expresses the\nsolution using harmonic B-spline, whose basis functions can be constructed\nlocally and the control coefficients are obtained by solving a small sparse\nlinear system. Our closed-form solver is numerically stable and it supports\nrandom access evaluation, zooming-in of arbitrary resolution and anti-aliasing.\nAlthough the harmonic B-spline based solutions are approximate, computational\nresults show that the relative mean error is less than 0.3%, which cannot be\ndistinguished by naked eyes."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1701.05754v1", 
    "title": "User-guided free-form asset modelling", 
    "arxiv-id": "1701.05754v1", 
    "author": "Daniel Beale", 
    "publish": "2017-01-20T10:58:20Z", 
    "summary": "In this paper a new system for piecewise primitive surface recovery on point\nclouds is presented, which allows a novice user to sketch areas of interest in\norder to guide the fitting process. The algorithm is demonstrated against a\nbenchmark technique for autonomous surface fitting, and, contrasted against\nexisting literature in user guided surface recovery, with empirical evidence.\nIt is concluded that the system is an improvement to the current documented\nliterature for its visual quality when modelling objects which are composed of\npiecewise primitive shapes, and, in its ability to fill large holes on occluded\nsurfaces using free-form input."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1701.06507v2", 
    "title": "Plausible Shading Decomposition For Layered Photo Retouching", 
    "arxiv-id": "1701.06507v2", 
    "author": "Niloy J. Mitra", 
    "publish": "2017-01-23T17:05:22Z", 
    "summary": "Photographers routinely compose multiple manipulated photos of the same scene\n(layers) into a single image, which is better than any individual photo could\nbe alone. Similarly, 3D artists set up rendering systems to produce layered\nimages to contain only individual aspects of the light transport, which are\ncomposed into the final result in post-production. Regrettably, both approaches\neither take considerable time to capture, or remain limited to synthetic\nscenes. In this paper, we suggest a system to allow decomposing a single image\ninto a plausible shading decomposition (PSD) that approximates effects such as\nshadow, diffuse illumination, albedo, and specular shading. This decomposition\ncan then be manipulated in any off-the-shelf image manipulation software and\nrecomposited back. We perform such a decomposition by learning a convolutional\nneural network trained using synthetic data. We demonstrate the effectiveness\nof our decomposition on synthetic (i.e., rendered) and real data (i.e.,\nphotographs), and use them for common photo manipulation, which are nearly\nimpossible to perform otherwise from single images."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1701.07110v1", 
    "title": "By chance is not enough: Preserving relative density through non uniform   sampling", 
    "arxiv-id": "1701.07110v1", 
    "author": "Giuseppe Santucci", 
    "publish": "2017-01-24T23:35:09Z", 
    "summary": "Dealing with visualizations containing large data set is a challenging issue\nand, in the field of Information Visualization, almost every visual technique\nreveals its drawback when visualizing large number of items. To deal with this\nproblem we introduce a formal environment, modeling in a virtual space the\nimage features we are interested in (e.g, absolute and relative density,\nclusters, etc.) and we define some metrics able to characterize the image\ndecay. Such metrics drive our automatic techniques (i.e., not uniform sampling)\nrescuing the image features and making them visible to the user. In this paper\nwe focus on 2D scatter-plots, devising a novel non uniform data sampling\nstrategy able to preserve in an effective way relative densities."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1702.01530v1", 
    "title": "Three dimension visualization by ray-tracing image synthesis on GPU", 
    "arxiv-id": "1702.01530v1", 
    "author": "S. A. Zori", 
    "publish": "2017-02-06T08:36:10Z", 
    "summary": "This paper presents a realization of the approach to spatial three Dimension\nstereo of visualization of three Dimension images with use parallel Graphics\nprocessing unit (GPU). The experiments of realization of synthesis of images of\na 3D stage by a method of trace of beams on GPU with Compute Unified Device\nArchitecture have shown that 60 % of the time is spent for the decision of a\ncomputing problem approximately, the major part of time (40 %) is spent for\ntransfer of data between the central processing unit and GPU for calculations\nand the organization process of visualization. The study of the influence of\nincrease in the size of the GPU network at the speed of calculations showed\nimportance of the correct task of structure of formation of the parallel\ncomputer network and general mechanism of parallelization.\n  Keywords: Volumetric three Dimension visualization, stereo three Dimension\nvisualization, Ray tracing, parallel computing on GPU, Compute Unified Device\nArchitecture."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1702.01537v1", 
    "title": "Conceptual and algorithmic development of Pseudo 3D Graphics and Video   Content Visualization", 
    "arxiv-id": "1702.01537v1", 
    "author": "Anas M. Al-Oraiqat", 
    "publish": "2017-02-06T09:24:02Z", 
    "summary": "The article presents a general concept of the organization of pseudo three\ndimension visualization of graphics and video content for three dimension\nvisualization systems. The steps of algorithms for solving the problem of\nsynthesis of three dimension stereo images based on two dimension images are\nintroduced. The features of synthesis organization of standard format of three\ndimension stereo frame are presented. Moreover, the performed experimental\nsimulation for generating complete stereoframes and the results of its time\ncomplexity are shown. Keywords:Three dimension visualization, pseudo three\ndimension stereo, a stereo pair, three dimension stereo format, algorithm,\nmodeling, time complexity."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1702.01540v1", 
    "title": "Generalized 3D Voxel Image Synthesis Architecture for Volumetric Spatial   Visualization", 
    "arxiv-id": "1702.01540v1", 
    "author": "Aladdein M. Amro", 
    "publish": "2017-02-06T09:38:20Z", 
    "summary": "A general concept of 3D volumetric visualization systems is described based\non 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete\nvoxel scene (world) basic elements and main steps of the image synthesis\nalgorithm are formulated. An algorithm for solving the problem of the voxelized\nworld 3D image synthesis, intended for the systems of volumetric spatial\nvisualization, is proposed. A computer-based architecture for 3D volumetric\nvisualization of 3D discrete voxel world is presented. On the basis of the\nproposed overall concept of discrete voxel representation, the proposed\narchitecture successfully adapts the ray tracing technique for the synthesis of\n3D volumetric images. Since it is algorithmically simple and effectively\nsupports parallelism, it can efficiently be implemented.\n  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,\ndiscrete voxel world, ray tracing."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1702.03246v1", 
    "title": "Towards Developing an Easy-To-Use Scripting Environment for Animating   Virtual Characters", 
    "arxiv-id": "1702.03246v1", 
    "author": "Christos Mousas", 
    "publish": "2017-02-10T16:37:55Z", 
    "summary": "This paper presents the three scripting commands and main functionalities of\na novel character animation environment called CHASE. CHASE was developed for\nenabling inexperienced programmers, animators, artists, and students to animate\nin meaningful ways virtual reality characters. This is achieved by scripting\nsimple commands within CHASE. The commands identified, which are associated\nwith simple parameters, are responsible for generating a number of predefined\nmotions and actions of a character. Hence, the virtual character is able to\nanimate within a virtual environment and to interact with tasks located within\nit. An additional functionality of CHASE is supplied. It provides the ability\nto generate multiple tasks of a character, such as providing the user the\nability to generate scenario-related animated sequences. However, since\nmultiple characters may require simultaneous animation, the ability to script\nactions of different characters at the same time is also provided."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1702.04852v1", 
    "title": "Visualization and Analysis of Large-Scale, Tree-Based, Adaptive Mesh   Refinement Simulations with Arbitrary Rectilinear Geometry", 
    "arxiv-id": "1702.04852v1", 
    "author": "Philippe P. P\u00e9ba\u00ff", 
    "publish": "2017-02-16T03:49:46Z", 
    "summary": "We present here the first systematic treatment of the problems posed by the\nvisualization and analysis of large-scale, parallel adaptive mesh refinement\n(AMR) simulations on an Eulerian grid. When compared to those obtained by\nconstructing an intermediate unstructured mesh with fully described\nconnectivity, our primary results indicate a gain of at least 80\\% in terms of\nmemory footprint, with a better rendering while retaining similar execution\nspeed. In this article, we describe the key concepts that allow us to obtain\nthese results, together with the methodology that facilitates the design,\nimplementation, and optimization of algorithms operating directly on such\nrefined meshes. This native support for AMR meshes has been contributed to the\nopen source Visualization Toolkit (VTK). This work pertains to a broader\nlong-term vision, with the dual goal to both improve interactivity when\nexploring such data sets in 2 and 3 dimensions, and optimize resource\nutilization."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/1703.00212v1", 
    "title": "Two New Contributions to the Visualization of AMR Grids: I. Interactive   Rendering of Extreme-Scale 2-Dimensional Grids II. Novel Selection Filters in   Arbitrary Dimension", 
    "arxiv-id": "1703.00212v1", 
    "author": "Philippe P. P\u00e9ba\u00ff", 
    "publish": "2017-03-01T10:21:24Z", 
    "summary": "We present here the result of continuation work, performed to further fulfill\nthe vision we outlined in [Harel,Lekien,P\\'eba\\\"y-2017] for the visualization\nand analysis of tree-based adaptive mesh refinement (AMR) simulations, using\nthe hypertree grid paradigm which we proposed.\n  The first filter presented hereafter implements an adaptive approach in order\nto accelerate the rendering of 2-dimensional AMR grids, hereby solving the\nproblem posed by the loss of interactivity that occurs when dealing with large\nand/or deeply refined meshes. Specifically, view parameters are taken into\naccount, in order to: on one hand, avoid creating surface elements that are\noutside of the view area; on the other hand, utilize level-of-detail properties\nto cull those cells that are deemed too small to be visible with respect to the\ngiven view parameters. This adaptive approach often results in a massive\nincrease in rendering performance.\n  In addition, two new selection filters provide data analysis capabilities, by\nmeans of allowing for the extraction of those cells within a hypertree grid\nthat are deemed relevant in some sense, either geometrically or topologically.\nAfter a description of these new algorithms, we illustrate their use within the\nVisualization Toolkit (VTK) in which we implemented them. This note ends with\nsome suggestions for subsequent work."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/9809035v2", 
    "title": "Separation-Sensitive Collision Detection for Convex Objects", 
    "arxiv-id": "cs/9809035v2", 
    "author": "Li Zhang", 
    "publish": "1998-09-18T23:16:06Z", 
    "summary": "We develop a class of new kinetic data structures for collision detection\nbetween moving convex polytopes; the performance of these structures is\nsensitive to the separation of the polytopes during their motion. For two\nconvex polygons in the plane, let $D$ be the maximum diameter of the polygons,\nand let $s$ be the minimum distance between them during their motion. Our\nseparation certificate changes $O(\\log(D/s))$ times when the relative motion of\nthe two polygons is a translation along a straight line or convex curve,\n$O(\\sqrt{D/s})$ for translation along an algebraic trajectory, and $O(D/s)$ for\nalgebraic rigid motion (translation and rotation). Each certificate update is\nperformed in $O(\\log(D/s))$ time. Variants of these data structures are also\nshown that exhibit \\emph{hysteresis}---after a separation certificate fails,\nthe new certificate cannot fail again until the objects have moved by some\nconstant fraction of their current separation. We can then bound the number of\nevents by the combinatorial size of a certain cover of the motion path by\nballs."
},{
    "category": "cs.CY", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/9810004v1", 
    "title": "The Design of EzWindows: A Graphics API for an Introductory Programming   Course", 
    "arxiv-id": "cs/9810004v1", 
    "author": "Peter Valle", 
    "publish": "1998-10-03T12:04:29Z", 
    "summary": "Teaching object-oriented programming in an introductory programming course\nposes considerable challenges to the instructor. An often advocated approach to\nmeeting this challenge is the use of a simple, object-oriented graphics\nlibrary. We have developed a simple, portable graphics library for teaching\nobject-oriented programming using C++. The library, EzWindows, allows beginning\nprogrammers to design and write programs that use the graphical display found\non all modern desktop computers. In addition to providing simple graphical\nobjects such as windows, geometric shapes, and bitmaps, EzWindows provides\nfacilities for introducing event-based programming using the mouse and timers.\nEzWindows has proven to be extremely popular; it is currently in use at over\n200 universities, colleges, and high schools. This paper describes the\nrationale for EzWindows and its high-level design."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/9810021v1", 
    "title": "Computational Geometry Column 32", 
    "arxiv-id": "cs/9810021v1", 
    "author": "Joseph O'Rourke", 
    "publish": "1998-10-22T21:01:36Z", 
    "summary": "The proof of Dey's new k-set bound is illustrated."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/9909018v1", 
    "title": "Geometric compression for progressive transmission", 
    "arxiv-id": "cs/9909018v1", 
    "author": "Pierre-Maris Gandoin", 
    "publish": "1999-09-28T06:56:27Z", 
    "summary": "The compression of geometric structures is a relatively new field of data\ncompression. Since about 1995, several articles have dealt with the coding of\nmeshes, using for most of them the following approach: the vertices of the mesh\nare coded in an order such that it contains partially the topology of the mesh.\nIn the same time, some simple rules attempt to predict the position of the\ncurrent vertex from the positions of its neighbours that have been previously\ncoded. In this article, we describe a compression algorithm whose principle is\ncompletely different: the order of the vertices is used to compress their\ncoordinates, and then the topology of the mesh is reconstructed from the\nvertices. This algorithm, particularly suited for terrain models, achieves\ncompression factors that are slightly greater than those of the currently\navailable algorithms, and moreover, it allows progressive and interactive\ntransmission of the meshes."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/9910017v1", 
    "title": "Finite-resolution hidden surface removal", 
    "arxiv-id": "cs/9910017v1", 
    "author": "Jeff Erickson", 
    "publish": "1999-10-21T21:51:18Z", 
    "summary": "We propose a hybrid image-space/object-space solution to the classical hidden\nsurface removal problem: Given n disjoint triangles in Real^3 and p sample\npoints (``pixels'') in the xy-plane, determine the first triangle directly\nbehind each pixel. Our algorithm constructs the sampled visibility map of the\ntriangles with respect to the pixels, which is the subset of the trapezoids in\na trapezoidal decomposition of the analytic visibility map that contain at\nleast one pixel. The sampled visibility map adapts to local changes in image\ncomplexity, and its complexity is bounded both by the number of pixels and by\nthe complexity of the analytic visibility map. Our algorithm runs in time\nO(n^{1+e} + n^{2/3+e}t^{2/3} + p), where t is the output size and e is any\npositive constant. This is nearly optimal in the worst case and compares\nfavorably with the best output-sensitive algorithms for both ray casting and\nanalytic hidden surface removal. In the special case where the pixels form a\nregular grid, a sweepline variant of our algorithm runs in time O(n^{1+e} +\nn^{2/3+e}t^{2/3} + t log p), which is usually sublinear in the number of\npixels."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/0001017v1", 
    "title": "Bezier Curves Intersection Using Relief Perspective", 
    "arxiv-id": "cs/0001017v1", 
    "author": "Radoslav Hlusek", 
    "publish": "2000-01-21T10:02:49Z", 
    "summary": "Presented paper describes the method for finding the intersection of class\nspace rational Bezier curves. The problem curve/curve intersection belongs\namong basic geometric problems and the aim of this article is to describe the\nnew technique to solve the problem using relief perspective and Bezier\nclipping."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/0101021v1", 
    "title": "A Fast General Methodology for Information-Theoretically Optimal   Encodings of Graphs", 
    "arxiv-id": "cs/0101021v1", 
    "author": "Hsueh-I Lu", 
    "publish": "2001-01-23T00:17:50Z", 
    "summary": "We propose a fast methodology for encoding graphs with\ninformation-theoretically minimum numbers of bits. Specifically, a graph with\nproperty pi is called a pi-graph. If pi satisfies certain properties, then an\nn-node m-edge pi-graph G can be encoded by a binary string X such that (1) G\nand X can be obtained from each other in O(n log n) time, and (2) X has at most\nbeta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so\nthat there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The\nmethodology is applicable to general classes of graphs; this paper focuses on\nplanar graphs. Examples of such pi include all conjunctions over the following\ngroups of properties: (1) G is a planar graph or a plane graph; (2) G is\ndirected or undirected; (3) G is triangulated, triconnected, biconnected,\nmerely connected, or not required to be connected; (4) the nodes of G are\nlabeled with labels from {1, ..., ell_1} for ell_1 <= n; (5) the edges of G are\nlabeled with labels from {1, ..., ell_2} for ell_2 <= m; and (6) each node\n(respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively,\nell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be\nO(1) for the cases of pi being a plane triangulation. These examples are novel\napplications of small cycle separators of planar graphs and are the only\nnontrivial classes of graphs, other than rooted trees, with known\npolynomial-time information-theoretically optimal coding schemes."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TVCG.2016.2622272", 
    "link": "http://arxiv.org/pdf/cs/0101033v1", 
    "title": "Linear-Time Succinct Encodings of Planar Graphs via Canonical Orderings", 
    "arxiv-id": "cs/0101033v1", 
    "author": "Hsueh-I Lu", 
    "publish": "2001-01-27T02:05:33Z", 
    "summary": "Let G be an embedded planar undirected graph that has n vertices, m edges,\nand f faces but has no self-loop or multiple edge. If G is triangulated, we can\nencode it using {4/3}m-1 bits, improving on the best previous bound of about\n1.53m bits. In case exponential time is acceptable, roughly 1.08m bits have\nbeen known to suffice. If G is triconnected, we use at most\n(2.5+2\\log{3})\\min\\{n,f\\}-7 bits, which is at most 2.835m bits and smaller than\nthe best previous bound of 3m bits. Both of our schemes take O(n) time for\nencoding and decoding."
},{
    "category": "cs.CV", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0109116v1", 
    "title": "Digital Color Imaging", 
    "arxiv-id": "cs/0109116v1", 
    "author": "H. Joel Trussell", 
    "publish": "2001-09-26T22:14:40Z", 
    "summary": "This paper surveys current technology and research in the area of digital\ncolor imaging. In order to establish the background and lay down terminology,\nfundamental concepts of color perception and measurement are first presented\nus-ing vector-space notation and terminology. Present-day color recording and\nreproduction systems are reviewed along with the common mathematical models\nused for representing these devices. Algorithms for processing color images for\ndisplay and communication are surveyed, and a forecast of research trends is\nattempted. An extensive bibliography is provided."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0210018v1", 
    "title": "User software for the next generation", 
    "arxiv-id": "cs/0210018v1", 
    "author": "R. L. Mikkelson", 
    "publish": "2002-10-19T01:27:45Z", 
    "summary": "New generations of neutron scattering sources and instrumentation are\nproviding challenges in data handling for user software. Time-of-Flight\ninstruments used at pulsed sources typically produce hundreds or thousands of\nchannels of data for each detector segment. New instruments are being designed\nwith thousands to hundreds of thousands of detector segments. High intensity\nneutron sources make possible parametric studies and texture studies which\nfurther increase data handling requirements. The Integrated Spectral Analysis\nWorkbench (ISAW) software developed at Argonne handles large numbers of spectra\nsimultaneously while providing operations to reduce, sort, combine and export\nthe data. It includes viewers to inspect the data in detail in real time. ISAW\nuses existing software components and packages where feasible and takes\nadvantage of the excellent support for user interface design and network\ncommunication in Java. The included scripting language simplifies repetitive\noperations for analyzing many files related to a given experiment. Recent\nadditions to ISAW include a contour view, a time-slice table view, routines for\nfinding and fitting peaks in data, and support for data from other facilities\nusing the NeXus format. In this paper, I give an overview of features and\nplanned improvements of ISAW. Details of some of the improvements are covered\nin other presentations at this conference."
},{
    "category": "cs.CG", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0212007v1", 
    "title": "Optimized Color Gamuts for Tiled Displays", 
    "arxiv-id": "cs/0212007v1", 
    "author": "David Eppstein", 
    "publish": "2002-12-06T21:59:17Z", 
    "summary": "We consider the problem of finding a large color space that can be generated\nby all units in multi-projector tiled display systems. Viewing the problem\ngeometrically as one of finding a large parallelepiped within the intersection\nof multiple parallelepipeds, and using colorimetric principles to define a\nvolume-based objective function for comparing feasible solutions, we develop an\nalgorithm for finding the optimal gamut in time O(n^3), where n denotes the\nnumber of projectors in the system. We also discuss more efficient quasiconvex\nprogramming algorithms for alternative objective functions based on maximizing\nthe quality of the color space extrema."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0212043v1", 
    "title": "Computing Conformal Structure of Surfaces", 
    "arxiv-id": "cs/0212043v1", 
    "author": "Shing-Tung Yau", 
    "publish": "2002-12-13T05:33:10Z", 
    "summary": "This paper solves the problem of computing conformal structures of general\n2-manifolds represented as triangle meshes. We compute conformal structures in\nthe following way: first compute homology bases from simplicial complex\nstructures, then construct dual cohomology bases and diffuse them to harmonic\n1-forms. Next, we construct bases of holomorphic differentials. We then obtain\nperiod matrices by integrating holomorphic differentials along homology bases.\nWe also study the global conformal mapping between genus zero surfaces and\nspheres, and between general meshes and planes. Our method of computing\nconformal structures can be applied to tackle fundamental problems in computer\naid design and computer graphics, such as geometry classification and\nidentification, and surface global parametrization."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0301002v1", 
    "title": "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated   Rendering", 
    "arxiv-id": "cs/0301002v1", 
    "author": "Mark J. Kilgard", 
    "publish": "2003-01-06T20:57:51Z", 
    "summary": "Twenty-five years ago, Crow published the shadow volume approach for\ndetermining shadowed regions in a scene. A decade ago, Heidmann described a\nhardware-accelerated stencil buffer-based shadow volume algorithm.\n  Unfortunately hardware-accelerated stenciled shadow volume techniques have\nnot been widely adopted by 3D games and applications due in large part to the\nlack of robustness of described techniques. This situation persists despite\nwidely available hardware support. Specifically what has been lacking is a\ntechnique that robustly handles various \"hard\" situations created by near or\nfar plane clipping of shadow volumes.\n  We describe a robust, artifact-free technique for hardware-accelerated\nrendering of stenciled shadow volumes. Assuming existing hardware, we resolve\nthe issues otherwise caused by shadow volume near and far plane clipping\nthrough a combination of (1) placing the conventional far clip plane \"at\ninfinity\", (2) rasterization with infinite shadow volume polygons via\nhomogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth\nclamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves\nexisting depth precision by not requiring the far plane to be placed at\ninfinity. We also propose two-sided stencil testing to improve the efficiency\nof rendering stenciled shadow volumes."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0302013v1", 
    "title": "Cg in Two Pages", 
    "arxiv-id": "cs/0302013v1", 
    "author": "Mark J. Kilgard", 
    "publish": "2003-02-12T05:16:12Z", 
    "summary": "Cg is a language for programming GPUs. This paper describes Cg briefly."
},{
    "category": "cs.SE", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0306042v1", 
    "title": "IGUANA Architecture, Framework and Toolkit for Interactive Graphics", 
    "arxiv-id": "cs/0306042v1", 
    "author": "Lucas Taylor", 
    "publish": "2003-06-10T16:15:41Z", 
    "summary": "IGUANA is a generic interactive visualisation framework based on a C++\ncomponent model. It provides powerful user interface and visualisation\nprimitives in a way that is not tied to any particular physics experiment or\ndetector design. The article describes interactive visualisation tools built\nusing IGUANA for the CMS and D0 experiments, as well as generic GEANT4 and\nGEANT3 applications. It covers features of the graphical user interfaces, 3D\nand 2D graphics, high-quality vector graphics output for print media, various\ntextual, tabular and hierarchical data views, and integration with the\napplication through control panels, a command line and different\nmulti-threading models."
},{
    "category": "cs.HC", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0306087v1", 
    "title": "OO Model of the STAR offline production \"Event Display\" and its   implementation based on Qt-ROOT", 
    "arxiv-id": "cs/0306087v1", 
    "author": "Victor Perevoztchikov", 
    "publish": "2003-06-14T05:42:43Z", 
    "summary": "The paper presents the \"Event Display\" package for the STAR offline\nproduction as a special visualization tool to debug the reconstruction code.\nThis can be achieved if an author of the algorithm / code may build his/her own\ncustom Event Display alone from the base software blocks and re-used some\nwell-designed, easy to learn user-friendly patterns. For STAR offline\nproduction Event Display ROOT with Qt lower level interface was chosen as the\nbase tools."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0310002v2", 
    "title": "The Graphics Card as a Streaming Computer", 
    "arxiv-id": "cs/0310002v2", 
    "author": "Suresh Venkatasubramanian", 
    "publish": "2003-10-05T06:30:56Z", 
    "summary": "Massive data sets have radically changed our understanding of how to design\nefficient algorithms; the streaming paradigm, whether it in terms of number of\npasses of an external memory algorithm, or the single pass and limited memory\nof a stream algorithm, appears to be the dominant method for coping with large\ndata.\n  A very different kind of massive computation has had the same effect at the\nlevel of the CPU. The most prominent example is that of the computations\nperformed by a graphics card. The operations themselves are very simple, and\nrequire very little memory, but require the ability to perform many\ncomputations extremely fast and in parallel to whatever degree possible. What\nhas resulted is a stream processor that is highly optimized for stream\ncomputations. An intriguing side effect of this is the growing use of a\ngraphics card as a general purpose stream processing engine. In an\never-increasing array of applications, researchers are discovering that\nperforming a computation on a graphics card is far faster than performing it on\na CPU, and so are using a GPU as a stream co-processor."
},{
    "category": "cs.DC", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0310008v1", 
    "title": "Poster on MPI application in Computational Fluid Dynamics", 
    "arxiv-id": "cs/0310008v1", 
    "author": "Gianluca Argentini", 
    "publish": "2003-10-06T14:20:00Z", 
    "summary": "Poster-presentation of the paper \"Message Passing Fluids: molecules as\nprocesses in parallel computational fluids\" held at \"EURO PVMMPI 2003\"\nCongress; the paper is on the proceedings \"Recent Advances in Parallel Virtual\nMachine and Message Passing Interface\", 10th European PVM/MPI User's Group\nMeeting, LNCS 2840, Springer-Verlag, Dongarra-Laforenza-Orlando editors, pp.\n550-554."
},{
    "category": "cs.CG", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0310017v1", 
    "title": "Circle and sphere blending with conformal geometric algebra", 
    "arxiv-id": "cs/0310017v1", 
    "author": "Chris Doran", 
    "publish": "2003-10-09T15:15:41Z", 
    "summary": "Blending schemes based on circles provide smooth `fair' interpolations\nbetween series of points. Here we demonstrate a simple, robust set of\nalgorithms for performing circle blends for a range of cases. An arbitrary\nlevel of G-continuity can be achieved by simple alterations to the underlying\nparameterisation. Our method exploits the computational framework provided by\nconformal geometric algebra. This employs a five-dimensional representation of\npoints in space, in contrast to the four-dimensional representation typically\nused in projective geometry. The advantage of the conformal scheme is that\nstraight lines and circles are treated in a single, unified framework. As a\nfurther illustration of the power of the conformal framework, the basic idea is\nextended to the case of sphere blending to interpolate over a surface."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0312006v1", 
    "title": "Benchmarking and Implementation of Probability-Based Simulations on   Programmable Graphics Cards", 
    "arxiv-id": "cs/0312006v1", 
    "author": "J. Spiletic", 
    "publish": "2003-12-02T15:47:19Z", 
    "summary": "The latest Graphics Processing Units (GPUs) are reported to reach up to\n  200 billion floating point operations per second (200 Gflops) and to have\nprice performance of 0.1 cents per M flop. These facts raise great interest in\nthe plausibility of extending the GPUs' use to non-graphics applications, in\nparticular numerical simulations on structured grids (lattice).\n  We review previous work on using GPUs for non-graphics applications,\nimplement probability-based simulations on the GPU, namely the\n  Ising and percolation models, implement vector operation benchmarks for the\nGPU, and finally compare the CPU's and GPU's performance.\n  A general conclusion from the results obtained is that moving computations\nfrom the CPU to the GPU is feasible, yielding good time and price performance,\nfor certain lattice computations.\n  Preliminary results also show that it is feasible to use them in parallel"
},{
    "category": "cs.CG", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0405036v1", 
    "title": "Single-Strip Triangulation of Manifolds with Arbitrary Topology", 
    "arxiv-id": "cs/0405036v1", 
    "author": "David Eppstein", 
    "publish": "2004-05-10T14:31:40Z", 
    "summary": "Triangle strips have been widely used for efficient rendering. It is\nNP-complete to test whether a given triangulated model can be represented as a\nsingle triangle strip, so many heuristics have been proposed to partition\nmodels into few long strips. In this paper, we present a new algorithm for\ncreating a single triangle loop or strip from a triangulated model. Our method\napplies a dual graph matching algorithm to partition the mesh into cycles, and\nthen merges pairs of cycles by splitting adjacent triangles when necessary. New\nvertices are introduced at midpoints of edges and the new triangles thus formed\nare coplanar with their parent triangles, hence the visual fidelity of the\ngeometry is not changed. We prove that the increase in the number of triangles\ndue to this splitting is 50% in the worst case, however for all models we\ntested the increase was less than 2%. We also prove tight bounds on the number\nof triangles needed for a single-strip representation of a model with holes on\nits boundary. Our strips can be used not only for efficient rendering, but also\nfor other applications including the generation of space filling curves on a\nmanifold of any arbitrary topology."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0503054v1", 
    "title": "Analytic Definition of Curves and Surfaces by Parabolic Blending", 
    "arxiv-id": "cs/0503054v1", 
    "author": "A. W. Overhauser", 
    "publish": "2005-03-22T16:59:56Z", 
    "summary": "A procedure for interpolating between specified points of a curve or surface\nis described. The method guarantees slope continuity at all junctions. A\nsurface panel divided into p x q contiguous patches is completely specified by\nthe coordinates of (p+1) x (q+1) points. Each individual patch, however,\ndepends parametrically on the coordinates of 16 points, allowing shape\nflexibility and global conformity."
},{
    "category": "cs.CV", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0504031v1", 
    "title": "Convexity Analysis of Snake Models Based on Hamiltonian Formulation", 
    "arxiv-id": "cs/0504031v1", 
    "author": "Antonio Alberto Fernandes de Oliveira", 
    "publish": "2005-04-08T16:41:52Z", 
    "summary": "This paper presents a convexity analysis for the dynamic snake model based on\nthe Potential Energy functional and the Hamiltonian formulation of the\nclassical mechanics. First we see the snake model as a dynamical system whose\nsingular points are the borders we seek. Next we show that a necessary\ncondition for a singular point to be an attractor is that the energy functional\nis strictly convex in a neighborhood of it, that means, if the singular point\nis a local minimum of the potential energy. As a consequence of this analysis,\na local expression relating the dynamic parameters and the rate of convergence\narises. Such results link the convexity analysis of the potential energy and\nthe dynamic snake model and point forward to the necessity of a physical\nquantity whose convexity analysis is related to the dynamic and which\nincorporate the velocity space. Such a quantity is exactly the (conservative)\nHamiltonian of the system."
},{
    "category": "cs.NI", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0504107v2", 
    "title": "k-core decomposition: a tool for the visualization of large scale   networks", 
    "arxiv-id": "cs/0504107v2", 
    "author": "Alessandro Vespignani", 
    "publish": "2005-04-28T13:53:36Z", 
    "summary": "We use the k-core decomposition to visualize large scale complex networks in\ntwo dimensions. This decomposition, based on a recursive pruning of the least\nconnected vertices, allows to disentangle the hierarchical structure of\nnetworks by progressively focusing on their central cores. By using this\nstrategy we develop a general visualization algorithm that can be used to\ncompare the structural properties of various networks and highlight their\nhierarchical structure. The low computational complexity of the algorithm,\nO(n+e), where 'n' is the size of the network, and 'e' is the number of edges,\nmakes it suitable for the visualization of very large sparse networks. We apply\nthe proposed visualization tool to several real and synthetic graphs, showing\nits utility in finding specific structural fingerprints of computer generated\nand real world networks."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0505043v2", 
    "title": "Estimacao Temporal da Deformacao entre Objectos utilizando uma   Metodologia Fisica", 
    "arxiv-id": "cs/0505043v2", 
    "author": "Raquel R. Pinho", 
    "publish": "2005-05-16T02:20:19Z", 
    "summary": "In this paper, it is presented a methodology to estimate the deformation\ninvolved between two objects attending to its physical properties. This\nmethodology can be used, for example, in Computational Vision or Computer\nGraphics applications, and consists in physically modeling the objects, by\nmeans of the Finite Elements Method, establishing correspondences between some\nof its data points, by using Modal Matching, and finally, determining the\ndisplacement field, that is the intermediate shapes, through the resolution of\nthe Lagrange Dynamic Equilibrium Equation. As in many of the possible\napplications of the methodology to present, it is necessary to quantify the\nexisting deformation, as well as to estimate only the non rigid component of\nthe involved global deformation. The solutions adopted to satisfy such\nintentions will be also presented."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0511032v1", 
    "title": "Spatiotemporal sensistivity and visual attention for efficient rendering   of dynamic environments", 
    "arxiv-id": "cs/0511032v1", 
    "author": "Yang Li Hector Yee", 
    "publish": "2005-11-08T14:40:47Z", 
    "summary": "We present a method to accelerate global illumination computation in dynamic\nenvironments by taking advantage of limitations of the human visual system. A\nmodel of visual attention is used to locate regions of interest in a scene and\nto modulate spatiotemporal sensitivity. The method is applied in the form of a\nspatiotemporal error tolerance map. Perceptual acceleration combined with good\nsampling protocols provide a global illumination solution feasible for use in\nanimation. Results indicate an order of magnitude improvement in computational\nspeed. The method is adaptable and can also be used in image-based rendering,\ngeometry level of detail selection, realistic image synthesis, video telephony\nand video compression."
},{
    "category": "cs.AI", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0512010v1", 
    "title": "A geometry of information, I: Nerves, posets and differential forms", 
    "arxiv-id": "cs/0512010v1", 
    "author": "Timothy Porter", 
    "publish": "2005-12-02T16:18:11Z", 
    "summary": "The main theme of this workshop (Dagstuhl seminar 04351) is `Spatial\nRepresentation: Continuous vs. Discrete'. Spatial representation has two\ncontrasting but interacting aspects (i) representation of spaces' and (ii)\nrepresentation by spaces. In this paper, we will examine two aspects that are\ncommon to both interpretations of the theme, namely nerve constructions and\nrefinement. Representations change, data changes, spaces change. We will\nexamine the possibility of a `differential geometry' of spatial representations\nof both types, and in the sequel give an algebra of differential forms that has\nthe potential to handle the dynamical aspect of such a geometry. We will\ndiscuss briefly a conjectured class of spaces, generalising the Cantor set\nwhich would seem ideal as a test-bed for the set of tools we are developing."
},{
    "category": "cs.DM", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0512070v2", 
    "title": "Incremental and Transitive Discrete Rotations", 
    "arxiv-id": "cs/0512070v2", 
    "author": "Eric Remila", 
    "publish": "2005-12-16T16:12:12Z", 
    "summary": "A discrete rotation algorithm can be apprehended as a parametric application\n$f\\_\\alpha$ from $\\ZZ[i]$ to $\\ZZ[i]$, whose resulting permutation ``looks\nlike'' the map induced by an Euclidean rotation. For this kind of algorithm, to\nbe incremental means to compute successively all the intermediate rotate d\ncopies of an image for angles in-between 0 and a destination angle. The di\nscretized rotation consists in the composition of an Euclidean rotation with a\ndiscretization; the aim of this article is to describe an algorithm whic h\ncomputes incrementally a discretized rotation. The suggested method uses o nly\ninteger arithmetic and does not compute any sine nor any cosine. More pr\necisely, its design relies on the analysis of the discretized rotation as a\nstep function: the precise description of the discontinuities turns to be th e\nkey ingredient that will make the resulting procedure optimally fast and e\nxact. A complete description of the incremental rotation process is provided,\nalso this result may be useful in the specification of a consistent set of\ndefin itions for discrete geometry."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0512098v1", 
    "title": "Mathematical models of the complex surfaces in simulation and   visualization systems", 
    "arxiv-id": "cs/0512098v1", 
    "author": "Dmitry P. Paukov", 
    "publish": "2005-12-26T14:45:32Z", 
    "summary": "Modeling, simulation and visualization of three-dimension complex bodies\nwidely use mathematical model of curves and surfaces. The most important curves\nand surfaces for these purposes are curves and surfaces in Hermite and Bezier\nforms, splines and NURBS. Article is devoted to survey this way to use\ngeometrical data in various computer graphics systems and adjacent fields."
},{
    "category": "cs.AR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0603115v1", 
    "title": "Implementation of float-float operators on graphics hardware", 
    "arxiv-id": "cs/0603115v1", 
    "author": "David Defour", 
    "publish": "2006-03-29T11:48:29Z", 
    "summary": "The Graphic Processing Unit (GPU) has evolved into a powerful and flexible\nprocessor. The latest graphic processors provide fully programmable vertex and\npixel processing units that support vector operations up to single\nfloating-point precision. This computational power is now being used for\ngeneral-purpose computations. However, some applications require higher\nprecision than single precision. This paper describes the emulation of a 44-bit\nfloating-point number format and its corresponding operations. An\nimplementation is presented along with performance and accuracy results."
},{
    "category": "cs.CG", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0606055v1", 
    "title": "Simple Methods For Drawing Rational Surfaces as Four or Six Bezier   Patches", 
    "arxiv-id": "cs/0606055v1", 
    "author": "Jean Gallier", 
    "publish": "2006-06-12T22:02:41Z", 
    "summary": "In this paper, we give several simple methods for drawing a whole rational\nsurface (without base points) as several Bezier patches. The first two methods\napply to surfaces specified by triangular control nets and partition the real\nprojective plane RP2 into four and six triangles respectively. The third method\napplies to surfaces specified by rectangular control nets and partitions the\ntorus RP1 X RP1 into four rectangular regions. In all cases, the new control\nnets are obtained by sign flipping and permutation of indices from the original\ncontrol net. The proofs that these formulae are correct involve very little\ncomputations and instead exploit the geometry of the parameter space (RP2 or\nRP1 X RP1). We illustrate our method on some classical examples. We also\npropose a new method for resolving base points using a simple ``blowing up''\ntechnique involving the computation of ``resolved'' control nets."
},{
    "category": "cs.CC", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0606056v1", 
    "title": "Fast and Simple Methods For Computing Control Points", 
    "arxiv-id": "cs/0606056v1", 
    "author": "Weqing Gu", 
    "publish": "2006-06-13T00:47:34Z", 
    "summary": "The purpose of this paper is to present simple and fast methods for computing\ncontrol points for polynomial curves and polynomial surfaces given explicitly\nin terms of polynomials (written as sums of monomials). We give recurrence\nformulae w.r.t. arbitrary affine frames. As a corollary, it is amusing that we\ncan also give closed-form expressions in the case of the frame (r, s) for\ncurves, and the frame ((1, 0, 0), (0, 1, 0), (0, 0, 1) for surfaces. Our\nmethods have the same low polynomial (time and space) complexity as the other\nbest known algorithms, and are very easy to implement."
},{
    "category": "cs.CG", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0606061v1", 
    "title": "On the Efficiency of Strategies for Subdividing Polynomial Triangular   Surface Patches", 
    "arxiv-id": "cs/0606061v1", 
    "author": "Jean Gallier", 
    "publish": "2006-06-13T15:09:24Z", 
    "summary": "In this paper, we investigate the efficiency of various strategies for\nsubdividing polynomial triangular surface patches. We give a simple algorithm\nperforming a regular subdivision in four calls to the standard de Casteljau\nalgorithm (in its subdivision version). A naive version uses twelve calls. We\nalso show that any method for obtaining a regular subdivision using the\nstandard de Casteljau algorithm requires at least 4 calls. Thus, our method is\noptimal. We give another subdivision algorithm using only three calls to the de\nCasteljau algorithm. Instead of being regular, the subdivision pattern is\ndiamond-like. Finally, we present a ``spider-like'' subdivision scheme\nproducing six subtriangles in four calls to the de Casteljau algorithm."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0606098v1", 
    "title": "Outlier Robust ICP for Minimizing Fractional RMSD", 
    "arxiv-id": "cs/0606098v1", 
    "author": "Carlo Tomasi", 
    "publish": "2006-06-22T15:35:56Z", 
    "summary": "We describe a variation of the iterative closest point (ICP) algorithm for\naligning two point sets under a set of transformations. Our algorithm is\nsuperior to previous algorithms because (1) in determining the optimal\nalignment, it identifies and discards likely outliers in a statistically robust\nmanner, and (2) it is guaranteed to converge to a locally optimal solution. To\nthis end, we formalize a new distance measure, fractional root mean squared\ndistance (frmsd), which incorporates the fraction of inliers into the distance\nfunction. We lay out a specific implementation, but our framework can easily\nincorporate most techniques and heuristics from modern registration algorithms.\nWe experimentally validate our algorithm against previous techniques on 2 and 3\ndimensional data exposed to a variety of outlier types."
},{
    "category": "cs.CV", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/cs/0703088v1", 
    "title": "Plot 94 in ambiance X-Window", 
    "arxiv-id": "cs/0703088v1", 
    "author": "Carlos Alberto Hernandez-Hernandez", 
    "publish": "2007-03-16T00:18:11Z", 
    "summary": "<PLOT > is a collection of routines to draw surfaces, contours and so on. In\nthis work we are presenting a version, that functions over work stations with\nthe operative system UNIX, that count with the graphic ambiance X-WINDOW with\nthe tools XLIB and OSF/MOTIF. This implant was realized for the work stations\nDEC 5000-200, DEC IPX, and DEC ALFA of the CINVESTAV (Center of Investigation\nand Advanced Studies). Also implanted in SILICON GRAPHICS of the CENAC\n(National Center of Calculation of the Polytechnic National Institute"
},{
    "category": "math.NA", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/math/0607597v1", 
    "title": "A Vortex Method for Bi-phasic Fluids Interacting with Rigid Bodies", 
    "arxiv-id": "math/0607597v1", 
    "author": "Marie-Paule Cani", 
    "publish": "2006-07-24T12:25:59Z", 
    "summary": "We present an accurate Lagrangian method based on vortex particles,\nlevel-sets, and immersed boundary methods, for animating the interplay between\ntwo fluids and rigid solids. We show that a vortex method is a good choice for\nsimulating bi-phase flow, such as liquid and gas, with a good level of realism.\nVortex particles are localized at the interfaces between the two fluids and\nwithin the regions of high turbulence. We gain local precision and efficiency\nfrom the stable advection permitted by the vorticity formulation. Moreover, our\nnumerical method straightforwardly solves the two-way coupling problem between\nthe fluids and animated rigid solids. This new approach is validated through\nnumerical comparisons with reference experiments from the computational fluid\ncommunity. We also show that the visually appealing results obtained in the CG\ncommunity can be reproduced with increased efficiency and an easier\nimplementation."
},{
    "category": "cs.GR", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/0706.4224v1", 
    "title": "User driven applications - new design paradigm", 
    "arxiv-id": "0706.4224v1", 
    "author": "Sergey Andreyev", 
    "publish": "2007-06-28T13:19:04Z", 
    "summary": "Programs for complicated engineering and scientific tasks always have to deal\nwith a problem of showing numerous graphical results. The limits of the screen\nspace and often opposite requirements from different users are the cause of the\ninfinite discussions between designers and users, but the source of this\nongoing conflict is not in the level of interface design, but in the basic\nprinciple of current graphical output: user may change some views and details,\nbut in general the output view is absolutely defined and fixed by the\ndeveloper. Author was working for several years on the algorithm that will\nallow eliminating this problem thus allowing stepping from designer-driven\napplications to user-driven. Such type of applications in which user is\ndeciding what, when and how to show on the screen, is the dream of scientists\nand engineers working on the analysis of the most complicated tasks. The new\nparadigm is based on movable and resizable graphics, and such type of graphics\ncan be widely used not only for scientific and engineering applications."
},{
    "category": "cs.HC", 
    "doi": "10.1109/83.597268", 
    "link": "http://arxiv.org/pdf/0707.1618v1", 
    "title": "The Trade-offs with Space Time Cube Representation of Spatiotemporal   Patterns", 
    "arxiv-id": "0707.1618v1", 
    "author": "Josefin Stahl", 
    "publish": "2007-07-11T13:39:34Z", 
    "summary": "Space time cube representation is an information visualization technique\nwhere spatiotemporal data points are mapped into a cube. Fast and correct\nanalysis of such information is important in for instance geospatial and social\nvisualization applications. Information visualization researchers have\npreviously argued that space time cube representation is beneficial in\nrevealing complex spatiotemporal patterns in a dataset to users. The argument\nis based on the fact that both time and spatial information are displayed\nsimultaneously to users, an effect difficult to achieve in other\nrepresentations. However, to our knowledge the actual usefulness of space time\ncube representation in conveying complex spatiotemporal patterns to users has\nnot been empirically validated. To fill this gap we report on a\nbetween-subjects experiment comparing novice users error rates and response\ntimes when answering a set of questions using either space time cube or a\nbaseline 2D representation. For some simple questions the error rates were\nlower when using the baseline representation. For complex questions where the\nparticipants needed an overall understanding of the spatiotemporal structure of\nthe dataset, the space time cube representation resulted in on average twice as\nfast response times with no difference in error rates compared to the baseline.\nThese results provide an empirical foundation for the hypothesis that space\ntime cube representation benefits users when analyzing complex spatiotemporal\npatterns."
},{
    "category": "cs.NI", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0708.0660v1", 
    "title": "Network synchronizability analysis: the theory of subgraphs and   complementary graphs", 
    "arxiv-id": "0708.0660v1", 
    "author": "Guanrong Chen", 
    "publish": "2007-08-05T05:25:45Z", 
    "summary": "In this paper, subgraphs and complementary graphs are used to analyze the\nnetwork synchronizability. Some sharp and attainable bounds are provided for\nthe eigenratio of the network structural matrix, which characterizes the\nnetwork synchronizability, especially when the network's corresponding graph\nhas cycles, chains, bipartite graphs or product graphs as its subgraphs."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0709.0674v1", 
    "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective   Attention, Curiosity & Creativity", 
    "arxiv-id": "0709.0674v1", 
    "author": "Juergen Schmidhuber", 
    "publish": "2007-09-05T15:20:59Z", 
    "summary": "I postulate that human or other intelligent agents function or should\nfunction as follows. They store all sensory observations as they come - the\ndata is holy. At any time, given some agent's current coding capabilities, part\nof the data is compressible by a short and hopefully fast program / description\n/ explanation / world model. In the agent's subjective eyes, such data is more\nregular and more \"beautiful\" than other data. It is well-known that knowledge\nof regularity and repeatability may improve the agent's ability to plan actions\nleading to external rewards. In absence of such rewards, however, known beauty\nis boring. Then \"interestingness\" becomes the first derivative of subjective\nbeauty: as the learning agent improves its compression algorithm, formerly\napparently random data parts become subjectively more regular and beautiful.\nSuch progress in compressibility is measured and maximized by the curiosity\ndrive: create action sequences that extend the observation history and yield\npreviously unknown / unpredictable but quickly learnable algorithmic\nregularity. We discuss how all of the above can be naturally implemented on\ncomputers, through an extension of passive unsupervised learning to the case of\nactive data selection: we reward a general reinforcement learner (with access\nto the adaptive compressor) for actions that improve the subjective\ncompressibility of the growing data. An unusually large breakthrough in\ncompressibility deserves the name \"discovery\". The \"creativity\" of artists,\ndancers, musicians, pure mathematicians can be viewed as a by-product of this\nprinciple. Several qualitative examples support this hypothesis."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0709.3553v1", 
    "title": "Design of moveable and resizable graphics", 
    "arxiv-id": "0709.3553v1", 
    "author": "Sergey Andreyev", 
    "publish": "2007-09-22T00:21:08Z", 
    "summary": "We are communicating with computers on two different levels. On upper level\nwe have a very flexible system of windows: we can move them, resize, overlap or\nput side by side. At any moment we decide what would be the best view and\nreorganize the whole view easily. Then we start any application, go to the\ninner level, and everything changes. Here we are stripped of all the\nflexibility and can work only inside the scenario, developed by the designer of\nthe program. Interface will allow us to change some tiny details, but in\ngeneral everything is fixed: graphics is neither moveable, nor resizable, and\nthe same with controls. Author designed an extremely powerful mechanism of\nturning graphical objects and controls into moveable and resizable. This can\nnot only significantly improve the existing applications, but this will bring\nthe applications to another level. (To estimate the possible difference, try to\nimagine the Windows system without its flexibility and compare it with the\ncurrent one.) This article explains in details the construction and use of\nmoveable and resizable graphical objects."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0712.1549v1", 
    "title": "Dynamic Multilevel Graph Visualization", 
    "arxiv-id": "0712.1549v1", 
    "author": "Todd L. Veldhuizen", 
    "publish": "2007-12-10T17:42:48Z", 
    "summary": "We adapt multilevel, force-directed graph layout techniques to visualizing\ndynamic graphs in which vertices and edges are added and removed in an online\nfashion (i.e., unpredictably). We maintain multiple levels of coarseness using\na dynamic, randomized coarsening algorithm. To ensure the vertices follow\nsmooth trajectories, we employ dynamics simulation techniques, treating the\nvertices as point particles. We simulate fine and coarse levels of the graph\nsimultaneously, coupling the dynamics of adjacent levels. Projection from\ncoarser to finer levels is adaptive, with the projection determined by an\naffine transformation that evolves alongside the graph layouts. The result is a\ndynamic graph visualizer that quickly and smoothly adapts to changes in a\ngraph."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0801.3249v1", 
    "title": "Complex Eigenvalues for Binary Subdivision Schemes", 
    "arxiv-id": "0801.3249v1", 
    "author": "Christian Kuehn", 
    "publish": "2008-01-21T18:27:09Z", 
    "summary": "Convergence properties of binary stationary subdivision schemes for curves\nhave been analyzed using the techniques of z-transforms and eigenanalysis.\nEigenanalysis provides a way to determine derivative continuity at specific\npoints based on the eigenvalues of a finite matrix. None of the well-known\nsubdivision schemes for curves have complex eigenvalues. We prove when a\nconvergent scheme with palindromic mask can have complex eigenvalues and that a\nlower limit for the size of the mask exists in this case. We find a scheme with\ncomplex eigenvalues achieving this lower bound. Furthermore we investigate this\nscheme numerically and explain from a geometric viewpoint why such a scheme has\nnot yet been used in computer-aided geometric design."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0802.3355v1", 
    "title": "PVM-Distributed Implementation of the Radiance Code", 
    "arxiv-id": "0802.3355v1", 
    "author": "Jose E. Fern\u00e1ndez", 
    "publish": "2008-02-22T17:32:17Z", 
    "summary": "The Parallel Virtual Machine (PVM) tool has been used for a distributed\nimplementation of Greg Ward's Radiance code. In order to generate exactly the\nsame primary rays with both the sequential and the parallel codes, the quincunx\nsampling technique used in Radiance for the reduction of the number of primary\nrays by interpolation, must be left untouched in the parallel implementation.\nThe octree of local ambient values used in Radiance for the indirect\nillumination has been shared among all the processors. Both static and dynamic\nimage partitioning techniques which replicate the octree of the complete scene\nin all the processors and have load-balancing, have been developed for one\nframe rendering. Speedups larger than 7.5 have been achieved in a network of 8\nworkstations. For animation sequences, a new dynamic partitioning distribution\ntechnique with superlinear speedups has also been developed."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.physd.2007.12.003", 
    "link": "http://arxiv.org/pdf/0804.3103v1", 
    "title": "Size matters: performance declines if your pixels are too big or too   small", 
    "arxiv-id": "0804.3103v1", 
    "author": "Eamonn O'Neill", 
    "publish": "2008-04-18T21:02:19Z", 
    "summary": "We present a conceptual model that describes the effect of pixel size on\ntarget acquisition. We demonstrate the use of our conceptual model by applying\nit to predict and explain the results of an experiment to evaluate users'\nperformance in a target acquisition task involving three distinct display\nsizes: standard desktop, small and large displays. The results indicate that\nusers are fastest on standard desktop displays, undershoots are the most common\nerror on small displays and overshoots are the most common error on large\ndisplays. We propose heuristics to maintain usability when changing displays.\nFinally, we contribute to the growing body of evidence that amplitude does\naffect performance in a display-based pointing task."
},{
    "category": "cs.CG", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0805.0162v2", 
    "title": "Morphing of Triangular Meshes in Shape Space", 
    "arxiv-id": "0805.0162v2", 
    "author": "Alan Brunton", 
    "publish": "2008-05-01T23:08:30Z", 
    "summary": "We present a novel approach to morph between two isometric poses of the same\nnon-rigid object given as triangular meshes. We model the morphs as linear\ninterpolations in a suitable shape space $\\mathcal{S}$. For triangulated 3D\npolygons, we prove that interpolating linearly in this shape space corresponds\nto the most isometric morph in $\\mathbb{R}^3$. We then extend this shape space\nto arbitrary triangulations in 3D using a heuristic approach and show the\npractical use of the approach using experiments. Furthermore, we discuss a\nmodified shape space that is useful for isometric skeleton morphing. All of the\nnewly presented approaches solve the morphing problem without the need to solve\na minimization problem."
},{
    "category": "cs.AI", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0806.2925v2", 
    "title": "Neural networks in 3D medical scan visualization", 
    "arxiv-id": "0806.2925v2", 
    "author": "Gitta Domik", 
    "publish": "2008-06-18T08:36:15Z", 
    "summary": "For medical volume visualization, one of the most important tasks is to\nreveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the\ncoronary arteries, without obscuring them with less significant parts. These\nvolume datasets contain different materials which are difficult to extract and\nvisualize with 1D transfer functions based solely on the attenuation\ncoefficient. Multi-dimensional transfer functions allow a much more precise\nclassification of data which makes it easier to separate different surfaces\nfrom each other. Unfortunately, setting up multi-dimensional transfer functions\ncan become a fairly complex task, generally accomplished by trial and error.\nThis paper explains neural networks, and then presents an efficient way to\nspeed up visualization process by semi-automatic transfer function generation.\nWe describe how to use neural networks to detect distinctive features shown in\nthe 2D histogram of the volume data and how to use this information for data\nclassification."
},{
    "category": "cs.HC", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0809.0884v1", 
    "title": "On the role of metaphor in information visualization", 
    "arxiv-id": "0809.0884v1", 
    "author": "John S. Risch", 
    "publish": "2008-09-04T19:55:40Z", 
    "summary": "The concept of metaphor, in particular graphical (or visual) metaphor, is\ncentral to the field of information visualization. Information graphics and\ninteractive information visualization systems employ a variety of metaphorical\ndevices to make abstract, complex, voluminous, or otherwise\ndifficult-to-comprehend information understandable in graphical terms. This\npaper explores the use of metaphor in information visualization, advancing the\ntheory previously argued by Johnson, Lakoff, Tversky et al. that many\ninformation graphics are metaphorically understood in terms of cognitively\nentrenched spatial patterns known as image schemas. These patterns serve to\nstructure and constrain abstract reasoning processes via metaphorical\nprojection operations that are grounded in everyday perceptual experiences with\nphenomena such as containment, movement, and force dynamics. Building on\nprevious research, I argue that information graphics promote comprehension of\ntheir target information through the use of graphical patterns that invoke\nthese preexisting schematic structures. I further theorize that the degree of\nstructural alignment of a particular graphic with one or more corresponding\nimage schemas accounts for its perceived degree of intuitiveness. Accordingly,\nimage schema theory can provide a powerful explanatory and predictive framework\nfor visualization research. I review relevant theories of analogy and metaphor,\nand discuss the image schematic properties of several common types of\ninformation graphic. I conclude with the proposal that the inventory of image\nschemas culled from linguistic studies can serve as the basis for an inventory\nof design elements suitable for developing intuitive and effective new\ninformation visualization techniques."
},{
    "category": "cs.GR", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0809.4093v2", 
    "title": "Perspective Drawing of Surfaces with Line Hidden Line Elimination,   Dibujando Superficies En Perspectiva Con Eliminacion De Lineas Ocultas", 
    "arxiv-id": "0809.4093v2", 
    "author": "Georgina G. Pulido", 
    "publish": "2008-09-24T05:50:56Z", 
    "summary": "An efficient computer algorithm is described for the perspective drawing of a\nwide class of surfaces. The class includes surfaces corresponding lo\nsingle-valued, continuous functions which are defined over rectangular domains.\nThe algorithm automatically computes and eliminates hidden lines. The number of\ncomputations in the algorithm grows linearly with the number of sample points\non the surface to be drawn. An analysis of the algorithm is presented, and\nextensions lo certain multi-valued functions are indicated. The algorithm is\nimplemented and tested on .Net 2.0 platform that left interactive use. Running\ntimes are found lo be exceedingly efficient for visualization, where\ninteraction on-line and view-point control, enables effective and rapid\nexamination of a surfaces from many perspectives."
},{
    "category": "cs.GR", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0810.2021v1", 
    "title": "Visualization Optimization : Application to the RoboCup Rescue Domain", 
    "arxiv-id": "0810.2021v1", 
    "author": "Ant\u00f3nio Augusto de Sousa", 
    "publish": "2008-10-13T12:53:57Z", 
    "summary": "In this paper we demonstrate the use of intelligent optimization\nmethodologies on the visualization optimization of virtual / simulated\nenvironments. The problem of automatic selection of an optimized set of views,\nwhich better describes an on-going simulation over a virtual environment is\naddressed in the context of the RoboCup Rescue Simulation domain. A generic\narchitecture for optimization is proposed and described. We outline the\npossible extensions of this architecture and argue on how several problems\nwithin the fields of Interactive Rendering and Visualization can benefit from\nit."
},{
    "category": "cs.CV", 
    "doi": "10.1142/S0218654310001341", 
    "link": "http://arxiv.org/pdf/0810.3418v1", 
    "title": "Detecting the Most Unusual Part of a Digital Image", 
    "arxiv-id": "0810.3418v1", 
    "author": "E. Korutcheva", 
    "publish": "2008-10-19T18:04:51Z", 
    "summary": "The purpose of this paper is to introduce an algorithm that can detect the\nmost unusual part of a digital image. The most unusual part of a given shape is\ndefined as a part of the image that has the maximal distance to all non\nintersecting shapes with the same form.\n  The method can be used to scan image databases with no clear model of the\ninteresting part or large image databases, as for example medical databases."
},{
    "category": "astro-ph", 
    "doi": "10.1071/AS08025", 
    "link": "http://arxiv.org/pdf/0810.4201v2", 
    "title": "Interchanging Interactive 3-d Graphics for Astronomy", 
    "arxiv-id": "0810.4201v2", 
    "author": "N. T. Jones", 
    "publish": "2008-10-23T02:58:55Z", 
    "summary": "We demonstrate how interactive, three-dimensional (3-d) scientific\nvisualizations can be efficiently interchanged between a variety of mediums.\nThrough the use of an appropriate interchange format, and a unified interaction\ninterface, we minimize the effort to produce visualizations appropriate for\nundertaking knowledge discovery at the astronomer's desktop, as part of\nconference presentations, in digital publications or as Web content. We use\nexamples from cosmological visualization to address some of the issues of\ninterchange, and to describe our approach to adapting S2PLOT desktop\nvisualizations to the Web.\n  Supporting demonstrations are available at\nhttp://astronomy.swin.edu.au/s2plot/interchange/"
},{
    "category": "cs.GR", 
    "doi": "10.1071/AS08025", 
    "link": "http://arxiv.org/pdf/0811.2055v2", 
    "title": "GPU-Based Interactive Visualization of Billion Point Cosmological   Simulations", 
    "arxiv-id": "0811.2055v2", 
    "author": "Gerard Lemson", 
    "publish": "2008-11-13T09:34:42Z", 
    "summary": "Despite the recent advances in graphics hardware capabilities, a brute force\napproach is incapable of interactively displaying terabytes of data. We have\nimplemented a system that uses hierarchical level-of-detailing for the results\nof cosmological simulations, in order to display visually accurate results\nwithout loading in the full dataset (containing over 10 billion points). The\nguiding principle of the program is that the user should not be able to\ndistinguish what they are seeing from a full rendering of the original data.\nFurthermore, by using a tree-based system for levels of detail, the size of the\nunderlying data is limited only by the capacity of the IO system containing it."
},{
    "category": "cs.GR", 
    "doi": "10.1071/AS08025", 
    "link": "http://arxiv.org/pdf/0811.4121v1", 
    "title": "String Art: Circle Drawing Using Straight Lines", 
    "arxiv-id": "0811.4121v1", 
    "author": "Sarad AV", 
    "publish": "2008-11-25T17:12:22Z", 
    "summary": "An algorithm to generate the locus of a circle using the intersection points\nof straight lines is proposed. The pixels on the circle are plotted independent\nof one another and the operations involved in finding the locus of the circle\nfrom the intersection of straight lines are parallelizable. Integer only\narithmetic and algorithmic optimizations are used for speedup. The proposed\nalgorithm makes use of an envelope to form a parabolic arc which is consequent\ntransformed into a circle. The use of parabolic arcs for the transformation\nresults in higher pixel errors as the radius of the circle to be drawn\nincreases. At its current state, the algorithm presented may be suitable only\nfor generating circles for string art."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.703968", 
    "link": "http://arxiv.org/pdf/0811.4681v1", 
    "title": "The Good, the Bad, and the Ugly: three different approaches to break   their watermarking system", 
    "arxiv-id": "0811.4681v1", 
    "author": "Fran\u00e7ois Cayre", 
    "publish": "2008-11-28T16:31:12Z", 
    "summary": "The Good is Blondie, a wandering gunman with a strong personal sense of\nhonor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The\nUgly is Tuco, a Mexican bandit who's always only looking out for himself.\nAgainst the backdrop of the BOWS contest, they search for a watermark in gold\nburied in three images. Each knows only a portion of the gold's exact location,\nso for the moment they're dependent on each other. However, none are\nparticularly inclined to share..."
},{
    "category": "cs.DM", 
    "doi": "10.1117/12.703968", 
    "link": "http://arxiv.org/pdf/0812.0754v2", 
    "title": "Strong Spatial Mixing and Approximating Partition Functions of Two-State   Spin Systems without Hard Constrains", 
    "arxiv-id": "0812.0754v2", 
    "author": "Jinshan Zhang", 
    "publish": "2008-12-03T16:56:53Z", 
    "summary": "We prove Gibbs distribution of two-state spin systems(also known as binary\nMarkov random fields) without hard constrains on a tree exhibits strong spatial\nmixing(also known as strong correlation decay), under the assumption that, for\narbitrary `external field', the absolute value of `inverse temperature' is\nsmall, or the `external field' is uniformly large or small. The first condition\non `inverse temperature' is tight if the distribution is restricted to\nferromagnetic or antiferromagnetic Ising models.\n  Thanks to Weitz's self-avoiding tree, we extends the result for sparse on\naverage graphs, which generalizes part of the recent work of Mossel and\nSly\\cite{MS08}, who proved the strong spatial mixing property for ferromagnetic\nIsing model. Our proof yields a different approach, carefully exploiting the\nmonotonicity of local recursion. To our best knowledge, the second condition of\n`external field' for strong spatial mixing in this paper is first considered\nand stated in term of `maximum average degree' and `interaction energy'. As an\napplication, we present an FPTAS for partition functions of two-state spin\nmodels without hard constrains under the above assumptions in a general family\nof graphs including interesting bounded degree graphs."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.703968", 
    "link": "http://arxiv.org/pdf/0812.1119v1", 
    "title": "An analysis of a random algorithm for estimating all the matchings", 
    "arxiv-id": "0812.1119v1", 
    "author": "Jinshan Zhang", 
    "publish": "2008-12-05T12:16:53Z", 
    "summary": "Counting the number of all the matchings on a bipartite graph has been\ntransformed into calculating the permanent of a matrix obtained from the\nextended bipartite graph by Yan Huo, and Rasmussen presents a simple approach\n(RM) to approximate the permanent, which just yields a critical ratio\nO($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple\npromising practical way to compute this #P-complete problem. In this paper, the\nperformance of this method will be shown when it's applied to compute all the\nmatchings based on that transformation. The critical ratio will be proved to be\nvery large with a certain probability, owning an increasing factor larger than\nany polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence,\nRM fails to work well when counting all the matchings via computing the\npermanent of the matrix. In other words, we must carefully utilize the known\nmethods of estimating the permanent to count all the matchings through that\ntransformation."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.703968", 
    "link": "http://arxiv.org/pdf/0903.1448v1", 
    "title": "The Digital Restoration of Da Vinci's Sketches", 
    "arxiv-id": "0903.1448v1", 
    "author": "Amelia Sparavigna", 
    "publish": "2009-03-09T08:06:09Z", 
    "summary": "A sketch, found in one of Leonardo da Vinci's notebooks and covered by the\nwritten notes of this genius, has been recently restored. The restoration\nreveals a possible self-portrait of the artist, drawn when he was young. Here,\nwe discuss the discovery of this self-portrait and the procedure used for\nrestoration. Actually, this is a restoration performed on the digital image of\nthe sketch, a procedure that can easily extended and applied to ancient\ndocuments for studies of art and palaeography."
},{
    "category": "cs.PF", 
    "doi": "10.1117/12.703968", 
    "link": "http://arxiv.org/pdf/0903.2119v1", 
    "title": "Adaptive Mesh Approach for Predicting Algorithm Behavior with   Application to Visibility Culling in Computer Graphics", 
    "arxiv-id": "0903.2119v1", 
    "author": "Martin Ziegler", 
    "publish": "2009-03-12T10:16:13Z", 
    "summary": "We propose a concise approximate description, and a method for efficiently\nobtaining this description, via adaptive random sampling of the performance\n(running time, memory consumption, or any other profileable numerical quantity)\nof a given algorithm on some low-dimensional rectangular grid of inputs. The\nformal correctness is proven under reasonable assumptions on the algorithm\nunder consideration; and the approach's practical benefit is demonstrated by\npredicting for which observer positions and viewing directions an occlusion\nculling algorithm yields a net performance benefit or loss compared to a simple\nbrute force renderer."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-642-04103-7_9", 
    "link": "http://arxiv.org/pdf/0903.3524v1", 
    "title": "Ambient Isotopic Meshing of Implicit Algebraic Surface with   Singularities", 
    "arxiv-id": "0903.3524v1", 
    "author": "Jia Li", 
    "publish": "2009-03-20T13:53:35Z", 
    "summary": "A complete method is proposed to compute a certified, or ambient isotopic,\nmeshing for an implicit algebraic surface with singularities. By certified, we\nmean a meshing with correct topology and any given geometric precision. We\npropose a symbolic-numeric method to compute a certified meshing for the\nsurface inside a box containing singularities and use a modified\nPlantinga-Vegter marching cube method to compute a certified meshing for the\nsurface inside a box without singularities. Nontrivial examples are given to\nshow the effectiveness of the algorithm. To our knowledge, this is the first\nmethod to compute a certified meshing for surfaces with singularities."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-04103-7_9", 
    "link": "http://arxiv.org/pdf/0906.1226v1", 
    "title": "On the Complexity of Smooth Spline Surfaces from Quad Meshes", 
    "arxiv-id": "0906.1226v1", 
    "author": "Jianhua Fan", 
    "publish": "2009-06-05T22:57:04Z", 
    "summary": "This paper derives strong relations that boundary curves of a smooth complex\nof patches have to obey when the patches are computed by local averaging. These\nrelations restrict the choice of reparameterizations for geometric continuity.\nIn particular, when one bicubic tensor-product B-spline patch is associated\nwith each facet of a quadrilateral mesh with n-valent vertices and we do not\nwant segments of the boundary curves forced to be linear, then the relations\ndictate the minimal number and multiplicity of knots: For general data, the\ntensor-product spline patches must have at least two internal double knots per\nedge to be able to model a G^1-conneced complex of C^1 splines. This lower\nbound on the complexity of any construction is proven to be sharp by suitably\ninterpreting an existing surface construction. That is, we have a tight bound\non the complexity of smoothing quad meshes with bicubic tensor-product B-spline\npatches."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-04103-7_9", 
    "link": "http://arxiv.org/pdf/0906.2274v1", 
    "title": "A Neural Network Classifier of Volume Datasets", 
    "arxiv-id": "0906.2274v1", 
    "author": "Andreas Kolb", 
    "publish": "2009-06-12T11:17:05Z", 
    "summary": "Many state-of-the art visualization techniques must be tailored to the\nspecific type of dataset, its modality (CT, MRI, etc.), the recorded object or\nanatomical region (head, spine, abdomen, etc.) and other parameters related to\nthe data acquisition process. While parts of the information (imaging modality\nand acquisition sequence) may be obtained from the meta-data stored with the\nvolume scan, there is important information which is not stored explicitly\n(anatomical region, tracing compound). Also, meta-data might be incomplete,\ninappropriate or simply missing.\n  This paper presents a novel and simple method of determining the type of\ndataset from previously defined categories. 2D histograms based on intensity\nand gradient magnitude of datasets are used as input to a neural network, which\nclassifies it into one of several categories it was trained with. The proposed\nmethod is an important building block for visualization systems to be used\nautonomously by non-experts. The method has been tested on 80 datasets, divided\ninto 3 classes and a \"rest\" class.\n  A significant result is the ability of the system to classify datasets into a\nspecific class after being trained with only one dataset of that class. Other\nadvantages of the method are its easy implementation and its high computational\nperformance."
},{
    "category": "cs.HC", 
    "doi": "10.1007/978-3-642-04103-7_9", 
    "link": "http://arxiv.org/pdf/0906.3224v1", 
    "title": "Personal applications, based on moveable / resizable elements", 
    "arxiv-id": "0906.3224v1", 
    "author": "Sergey Andreyev", 
    "publish": "2009-06-17T16:07:01Z", 
    "summary": "All the modern day applications have the interface, absolutely defined by the\ndevelopers. The use of adaptive interface or dynamic layout allows some\nvariations, but even all of them are predetermined on the design stage, because\nthe best reaction (from designer's view) on any possible users' movement was\nhardcoded. But there is a different world of applications, totally constructed\non moveable / resizable elements; such applications turn the full control to\nthe users. The crucial thing in such programs is that not something but\neverything must become moveable and resizable. This article describes the\nfeatures of such applications and the algorithm behind their design."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-642-04103-7_9", 
    "link": "http://arxiv.org/pdf/0906.4036v3", 
    "title": "Physical Modeling Techniques in Active Contours for Image Segmentation", 
    "arxiv-id": "0906.4036v3", 
    "author": "Shanglian Bao", 
    "publish": "2009-06-22T16:38:30Z", 
    "summary": "Physical modeling method, represented by simulation and visualization of the\nprinciples in physics, is introduced in the shape extraction of the active\ncontours. The objectives of adopting this concept are to address the several\nmajor difficulties in the application of Active Contours. Primarily, a\ntechnique is developed to realize the topological changes of Parametric Active\nContours (Snakes). The key strategy is to imitate the process of a balloon\nexpanding and filling in a closed space with several objects. After removing\nthe touched balloon surfaces, the objects can be identified by surrounded\nremaining balloon surfaces. A burned region swept by Snakes is utilized to\ntrace the contour and to give a criterion for stopping the movement of Snake\ncurve. When the Snakes terminates evolution totally, through ignoring this\ncriterion, it can form a connected area by evolving the Snakes again and\ncontinuing the region burning. The contours extracted from the boundaries of\nthe burned area can represent the child snake of each object respectively.\nSecondly, a novel scheme is designed to solve the problems of leakage of the\ncontour from the large gaps, and the segmentation error in Geometric Active\nContours (GAC). It divides the segmentation procedure into two processing\nstages. By simulating the wave propagating in the isotropic substance at the\nfinal stage, it can significantly enhance the effect of image force in GAC\nbased on Level Set and give the satisfied solutions to the two problems.\nThirdly, to support the physical models for active contours above, we introduce\na general image force field created on a template plane over the image plane.\nThis force is more adaptable to noisy images with complicated geometric shapes."
},{
    "category": "cs.CV", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0907.3604v1", 
    "title": "Image Sampling with Quasicrystals", 
    "arxiv-id": "0907.3604v1", 
    "author": "Neil A. Dodgson", 
    "publish": "2009-07-21T10:08:48Z", 
    "summary": "We investigate the use of quasicrystals in image sampling. Quasicrystals\nproduce space-filling, non-periodic point sets that are uniformly discrete and\nrelatively dense, thereby ensuring the sample sites are evenly spread out\nthroughout the sampled image. Their self-similar structure can be attractive\nfor creating sampling patterns endowed with a decorative symmetry. We present a\nbrief general overview of the algebraic theory of cut-and-project quasicrystals\nbased on the geometry of the golden ratio. To assess the practical utility of\nquasicrystal sampling, we evaluate the visual effects of a variety of\nnon-adaptive image sampling strategies on photorealistic image reconstruction\nand non-photorealistic image rendering used in multiresolution image\nrepresentations. For computer visualization of point sets used in image\nsampling, we introduce a mosaic rendering technique."
},{
    "category": "cs.HC", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0908.4374v1", 
    "title": "Visualization of Mined Pattern and Its Human Aspects", 
    "arxiv-id": "0908.4374v1", 
    "author": "Dr. R. S. Kasana", 
    "publish": "2009-08-30T05:30:40Z", 
    "summary": "Researchers got success in mining the Web usage data effectively and\nefficiently. But representation of the mined patterns is often not in a form\nsuitable for direct human consumption. Hence mechanisms and tools that can\nrepresent mined patterns in easily understandable format are utilized.\nDifferent techniques are used for pattern analysis, one of them is\nvisualization. Visualization can provide valuable assistance for data analysis\nand decision making tasks. In the data visualization process, technical\nrepresentations of web pages are replaced by user attractive text\ninterpretations. Experiments with the real world problems showed that the\nvisualization can significantly increase the quality and usefulness of web log\nmining results. However, how decision makers perceive and interact with a\nvisual representation can strongly influence their understanding of the data as\nwell as the usefulness of the visual presentation. Human factors therefore\ncontribute significantly to the visualization process and should play an\nimportant role in the design and evaluation of visualization tools. This\nelectronic document is a live template. The various components of your paper,\ntitle, text, heads, etc., are already defined on the style sheet, as\nillustrated by the portions given in this document."
},{
    "category": "cs.AR", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0910.0505v2", 
    "title": "Hard Data on Soft Errors: A Large-Scale Assessment of Real-World Error   Rates in GPGPU", 
    "arxiv-id": "0910.0505v2", 
    "author": "Vijay S. Pande", 
    "publish": "2009-10-03T02:04:22Z", 
    "summary": "Graphics processing units (GPUs) are gaining widespread use in computational\nchemistry and other scientific simulation contexts because of their huge\nperformance advantages relative to conventional CPUs. However, the reliability\nof GPUs in error-intolerant applications is largely unproven. In particular, a\nlack of error checking and correcting (ECC) capability in the memory subsystems\nof graphics cards has been cited as a hindrance to the acceptance of GPUs as\nhigh-performance coprocessors, but the impact of this design has not been\npreviously quantified.\n  In this article we present MemtestG80, our software for assessing memory\nerror rates on NVIDIA G80 and GT200-architecture-based graphics cards.\nFurthermore, we present the results of a large-scale assessment of GPU error\nrate, conducted by running MemtestG80 on over 20,000 hosts on the Folding@home\ndistributed computing network. Our control experiments on consumer-grade and\ndedicated-GPGPU hardware in a controlled environment found no errors. However,\nour survey over cards on Folding@home finds that, in their installed\nenvironments, two-thirds of tested GPUs exhibit a detectable, pattern-sensitive\nrate of memory soft errors. We demonstrate that these errors persist after\ncontrolling for overclocking and environmental proxies for temperature, but\ndepend strongly on board architecture."
},{
    "category": "cs.CG", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0910.4084v1", 
    "title": "Complementary Space for Enhanced Uncertainty and Dynamics Visualization", 
    "arxiv-id": "0910.4084v1", 
    "author": "Jose Rivera", 
    "publish": "2009-10-20T22:00:54Z", 
    "summary": "Given a computer model of a physical object, it is often quite difficult to\nvisualize and quantify any global effects on the shape representation caused by\nlocal uncertainty and local errors in the data. This problem is further\namplified when dealing with hierarchical representations containing varying\nlevels of detail and / or shapes undergoing dynamic deformations. In this\npaper, we compute, quantify and visualize the complementary topological and\ngeometrical features of 3D shape models, namely, the tunnels, pockets and\ninternal voids of the object. We find that this approach sheds a unique light\non how a model is affected by local uncertainty, errors or modifications and\nshow how the presence or absence of complementary shape features can be\nessential to an object's structural form and function."
},{
    "category": "cs.GR", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0911.5157v3", 
    "title": "Analyzing Midpoint Subdivision", 
    "arxiv-id": "0911.5157v3", 
    "author": "Qi Chen", 
    "publish": "2009-11-26T22:47:37Z", 
    "summary": "Midpoint subdivision generalizes the Lane-Riesenfeld algorithm for uniform\ntensor product splines and can also be applied to non regular meshes. For\nexample, midpoint subdivision of degree 2 is a specific Doo-Sabin algorithm and\nmidpoint subdivision of degree 3 is a specific Catmull-Clark algorithm. In\n2001, Zorin and Schroeder were able to prove C1-continuity for midpoint\nsubdivision surfaces analytically up to degree 9. Here, we develop general\nanalysis tools to show that the limiting surfaces under midpoint subdivision of\nany degree >= 2 are C1-continuous at their extraordinary points."
},{
    "category": "cs.HC", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0912.2706v2", 
    "title": "On the theory of moveable objects", 
    "arxiv-id": "0912.2706v2", 
    "author": "Sergey Andreyev", 
    "publish": "2009-12-14T19:00:29Z", 
    "summary": "User-driven applications belong to the new type of programs, in which users\nget the full control of WHAT, WHEN, and HOW must appear on the screen. Such\nprograms can exist only if the screen view is organized not according with the\npredetermined scenario, written by the developers, but if any screen object can\nbe moved, resized, and reconfigured by any user at any moment. This article\ndescribes the algorithm, by which an object of an arbitrary shape can be turned\ninto moveable and resizable. It also explains some rules of such design and the\ntechnique, which can be useful in many cases. Both the individual movements of\nobjects and their synchronous movements are analysed. After discussing the\nindividually moveable controls, different types of groups are analysed and the\narbitrary grouping of controls is considered."
},{
    "category": "cs.GR", 
    "doi": "10.3842/SIGMA.2009.075", 
    "link": "http://arxiv.org/pdf/0912.5380v1", 
    "title": "Computing Principal Components Dynamically", 
    "arxiv-id": "0912.5380v1", 
    "author": "Klaus Kriegel", 
    "publish": "2009-12-30T18:07:56Z", 
    "summary": "In this paper we present closed-form solutions for efficiently updating the\nprincipal components of a set of $n$ points, when $m$ points are added or\ndeleted from the point set. For both operations performed on a discrete point\nset in $\\mathbb{R}^d$, we can compute the new principal components in $O(m)$\ntime for fixed $d$. This is a significant improvement over the commonly used\napproach of recomputing the principal components from scratch, which takes\n$O(n+m)$ time. An important application of the above result is the dynamical\ncomputation of bounding boxes based on principal component analysis. PCA\nbounding boxes are very often used in many fields, among others in computer\ngraphics for collision detection and fast rendering. We have implemented and\nevaluated few algorithms for computing dynamically PCA bounding boxes in\n$\\mathbb{R}^3$. In addition, we present closed-form solutions for computing\ndynamically principal components of continuous point sets in $\\mathbb{R}^2$ and\n$\\mathbb{R}^3$. In both cases, discrete and continuous, to compute the new\nprincipal components, no additional data structures or storage are needed."
},{
    "category": "cs.GR", 
    "doi": "10.1145/1557626.1557647", 
    "link": "http://arxiv.org/pdf/0912.5494v1", 
    "title": "Teaching Physical Based Animation via OpenGL Slides", 
    "arxiv-id": "0912.5494v1", 
    "author": "Peter Grogono", 
    "publish": "2009-12-30T17:53:18Z", 
    "summary": "This work expands further our earlier poster presentation and integration of\nthe OpenGL Slides Framework (OGLSF) - to make presentations with real-time\nanimated graphics where each slide is a scene with tidgets - and physical based\nanimation of elastic two-, three-layer softbody objects. The whole project is\nvery interactive, and serves dual purpose - delivering the teaching material in\na classroom setting with real running animated examples as well as releasing\nthe source code to the students to show how the actual working things are made."
},{
    "category": "cs.CG", 
    "doi": "10.1145/1557626.1557647", 
    "link": "http://arxiv.org/pdf/1001.2734v1", 
    "title": "Planar Visibility: Testing and Counting", 
    "arxiv-id": "1001.2734v1", 
    "author": "Pat Morin", 
    "publish": "2010-01-15T19:45:18Z", 
    "summary": "In this paper we consider query versions of visibility testing and visibility\ncounting. Let $S$ be a set of $n$ disjoint line segments in $\\R^2$ and let $s$\nbe an element of $S$. Visibility testing is to preprocess $S$ so that we can\nquickly determine if $s$ is visible from a query point $q$. Visibility counting\ninvolves preprocessing $S$ so that one can quickly estimate the number of\nsegments in $S$ visible from a query point $q$.\n  We present several data structures for the two query problems. The structures\nbuild upon a result by O'Rourke and Suri (1984) who showed that the subset,\n$V_S(s)$, of $\\R^2$ that is weakly visible from a segment $s$ can be\nrepresented as the union of a set, $C_S(s)$, of $O(n^2)$ triangles, even though\nthe complexity of $V_S(s)$ can be $\\Omega(n^4)$. We define a variant of their\ncovering, give efficient output-sensitive algorithms for computing it, and\nprove additional properties needed to obtain approximation bounds. Some of our\nbounds rely on a new combinatorial result that relates the number of segments\nof $S$ visible from a point $p$ to the number of triangles in $\\bigcup_{s\\in S}\nC_S(s)$ that contain $p$."
},{
    "category": "cs.GR", 
    "doi": "10.1145/1557626.1557647", 
    "link": "http://arxiv.org/pdf/1001.3974v2", 
    "title": "Modelacion y Visualizacion Tridimensional Interactiva de Variables   Electricas en Celdas de Electro-Obtencion con Electrodos Bipolares", 
    "arxiv-id": "1001.3974v2", 
    "author": "Lautaro Salazar Silva", 
    "publish": "2010-01-22T12:57:59Z", 
    "summary": "The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This paper presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow."
},{
    "category": "cs.GR", 
    "doi": "10.13140/RG.2.1.1291.4082", 
    "link": "http://arxiv.org/pdf/1001.4002v1", 
    "title": "Aplicacion Grafica para el estudio de un Modelo de Celda Electrolitica   usando Tecnicas de Visualizacion de Campos Vectoriales", 
    "arxiv-id": "1001.4002v1", 
    "author": "C\u00e9sar Mena Labra\u00f1a", 
    "publish": "2010-01-22T18:23:27Z", 
    "summary": "The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This thesis presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow."
},{
    "category": "cs.CV", 
    "doi": "10.13140/RG.2.1.1291.4082", 
    "link": "http://arxiv.org/pdf/1002.4317v1", 
    "title": "CLD-shaped Brushstrokes in Non-Photorealistic Rendering", 
    "arxiv-id": "1002.4317v1", 
    "author": "Roberto Marazzato", 
    "publish": "2010-02-23T12:32:34Z", 
    "summary": "Rendering techniques based on a random grid can be improved by adapting\nbrushstrokes to the shape of different areas of the original picture. In this\npaper, the concept of Coherence Length Diagram is applied to determine the\nadaptive brushstrokes, in order to simulate an impressionist painting. Some\nexamples are provided to instance the proposed algorithm."
},{
    "category": "cs.CV", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1004.5424v1", 
    "title": "Graphic Symbol Recognition using Graph Based Signature and Bayesian   Network Classifier", 
    "arxiv-id": "1004.5424v1", 
    "author": "Jean-Yves Ramel", 
    "publish": "2010-04-30T00:04:39Z", 
    "summary": "We present a new approach for recognition of complex graphic symbols in\ntechnical documents. Graphic symbol recognition is a well known challenge in\nthe field of document image analysis and is at heart of most graphic\nrecognition systems. Our method uses structural approach for symbol\nrepresentation and statistical classifier for symbol recognition. In our system\nwe represent symbols by their graph based signatures: a graphic symbol is\nvectorized and is converted to an attributed relational graph, which is used\nfor computing a feature vector for the symbol. This signature corresponds to\ngeometry and topology of the symbol. We learn a Bayesian network to encode\njoint probability distribution of symbol signatures and use it in a supervised\nlearning scenario for graphic symbol recognition. We have evaluated our method\non synthetically deformed and degraded images of pre-segmented 2D architectural\nand electronic symbols from GREC databases and have obtained encouraging\nrecognition rates."
},{
    "category": "cs.CV", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1004.5427v1", 
    "title": "Employing fuzzy intervals and loop-based methodology for designing   structural signature: an application to symbol recognition", 
    "arxiv-id": "1004.5427v1", 
    "author": "Josep Llad\u00f3s", 
    "publish": "2010-04-30T00:16:22Z", 
    "summary": "Motivation of our work is to present a new methodology for symbol\nrecognition. We support structural methods for representing visual associations\nin graphic documents. The proposed method employs a structural approach for\nsymbol representation and a statistical classifier for recognition. We\nvectorize a graphic symbol, encode its topological and geometrical information\nby an ARG and compute a signature from this structural graph. To address the\nsensitivity of structural representations to deformations and degradations, we\nuse data adapted fuzzy intervals while computing structural signature. The\njoint probability distribution of signatures is encoded by a Bayesian network.\nThis network in fact serves as a mechanism for pruning irrelevant features and\nchoosing a subset of interesting features from structural signatures, for\nunderlying symbol set. Finally we deploy the Bayesian network in supervised\nlearning scenario for recognizing query symbols. We have evaluated the\nrobustness of our method against noise, on synthetically deformed and degraded\nimages of pre-segmented 2D architectural and electronic symbols from GREC\ndatabases and have obtained encouraging recognition rates. A second set of\nexperimentation was carried out for evaluating the performance of our method\nagainst context noise i.e. symbols cropped from complete documents. The results\nsupport the use of our signature by a symbol spotting system."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1005.3190v1", 
    "title": "From granular avalanches to fluid turbulences through oozing pastes. A   mesoscopic physically-based particle model", 
    "arxiv-id": "1005.3190v1", 
    "author": "Annie Luciani", 
    "publish": "2010-05-18T13:10:28Z", 
    "summary": "In this paper, we describe how we can precisely produce complex and various\ndynamic morphological features such as structured and chaotic features which\noccur in sand pilings (piles, avalanches, internal collapses, arches) , in\nflowing fluids (laminar flowing, Kelvin-Helmholtz and Von Karmann eddies), and\nin cohesive pastes (twist-and-turn oozing and packing) using only a single\nunified model, called \"mesoscopic model\". This model is a physically-based\nparticle model whose behavior depends on only four simple, but easy to\nunderstand, physically-based parameters : elasticity, viscosity and their local\nareas of influence. It is fast to compute and easy to understand by\nnon-physicist users."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1005.4405v1", 
    "title": "A physically-based particle model of emergent crowd behaviors", 
    "arxiv-id": "1005.4405v1", 
    "author": "Nicolas Castagn\u00e9", 
    "publish": "2010-05-19T06:37:24Z", 
    "summary": "This paper presents a modeling process in order to produce a realistic\nsimulation of crowds in the ancient Greek agora of Argos. This place was a\nsocial theater in which two kinds of collective phenomena took place:\ninterpersonal interactions (small group discussion and negotiation, etc.) and\nglobal collective phenomena, such as flowing and jamming. In this paper, we\nfocus on the second type of collective human phenomena, called non-deliberative\nemergent crowd phenomena. This is a typical case of collective emergent\nself-organization. When a great number of individuals move within a confined\nenvironment and under a common fate, collective structures appear\nspontaneously: jamming with inner collapses, organized flowing with queues,\ncurls, and vortices, propagation effects, etc. These are particularly relevant\nfeatures to enhance the realism - more precisely the \"truthfulness\" - of models\nof this kind of collective phenomena. We assume that this truthfulness is\nstrongly associated with the concept of emergence: evolutions are not\npredetermined by the individual characters, but emerge from the interaction of\nnumerous characters. The evolutions are not repetitive, and evolve on the basis\nof small changes. This paper demonstrates that the physically-based interacting\nparticles system is an adequate candidate to model emergent crowd effects: it\nassociates a large number of elementary dynamic actors via elementary\nnon-linear dynamic interactions. Our model of the scene is regulated as a\nlarge, dynamically coupled network of second order differential automata. We\ntake advantage of symbolic non-photorealistic and efficient visualization to\nrender the style of the person, rather than the person itself. As an artistic\nrepresentation, NPR reinforces the symbolic acceptance of the scene by the\nobserver, triggering an immediate and intuitive recognition of the scene as a\nplausible scene from ancient Greece."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1005.4563v1", 
    "title": "Physically-based particle simulation and visualization of pastes and   gels", 
    "arxiv-id": "1005.4563v1", 
    "author": "Nicolas Castagn\u00e9", 
    "publish": "2010-05-25T13:14:11Z", 
    "summary": "This paper is focused on the question of simulation and visualiza- tion of 3D\ngel and paste dynamic effects. In a first part, we introduce a 3D physically\nbased particle (or mass-interaction) model, with a small number of masses and\nfew powerful interaction parameters, which is able to generate the dynamic\nfeatures of both gels and pastes. This model proves that the 3D\nmass-interaction method is relevant for the simulation of such phenomena,\nwithout an explicit knowledge of their underly- ing physics. In a second part,\nwe expose an original rendering process, the Flow Structuring Method that\nenhances the dynamic properties of the simulation and offers a realistic\nvisualization. This process ignores all the properties of the underlying\nphysical model. It leads to a reconstruction of the spatial structure of the\ngel (or paste) flow only through an analysis of the output of the simula- tion\nwhich is a set of unorganized points moving in a 3D space. Finally, the paper\npresents realistic renderings obtained by using implicit surfaces and\nray-tracing techniques on the Structured Flow previously obtained."
},{
    "category": "cs.CV", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1006.2368v1", 
    "title": "L2-optimal image interpolation and its applications to medical imaging", 
    "arxiv-id": "1006.2368v1", 
    "author": "Oleg Pianykh", 
    "publish": "2010-06-11T19:05:05Z", 
    "summary": "Digital medical images are always displayed scaled to fit particular view.\nInterpolation is responsible for this scaling, and if not done properly, can\nsignificantly degrade diagnostic image quality. However, theoretically-optimal\ninterpolation algorithms may also be the most time-consuming and impractical.\nWe propose a new approach, adapted to the needs of digital medical imaging, to\ncombine high interpolation speed and superior L2-optimal image quality."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1006.4903v2", 
    "title": "Toric degenerations of Bezier patches", 
    "arxiv-id": "1006.4903v2", 
    "author": "Chungang Zhu", 
    "publish": "2010-06-25T03:11:37Z", 
    "summary": "The control polygon of a Bezier curve is well-defined and has geometric\nsignificance---there is a sequence of weights under which the limiting position\nof the curve is the control polygon. For a Bezier surface patch, there are many\npossible polyhedral control structures, and none are canonical. We propose a\nnot necessarily polyhedral control structure for surface patches, regular\ncontrol surfaces, which are certain C^0 spline surfaces. While not unique,\nregular control surfaces are exactly the possible limiting positions of a\nBezier patch when the weights are allowed to vary."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICDAR.2009.92", 
    "link": "http://arxiv.org/pdf/1008.0208v2", 
    "title": "Parametric polynomial minimal surfaces of arbitrary degree", 
    "arxiv-id": "1008.0208v2", 
    "author": "Guozhao Wang", 
    "publish": "2010-08-01T22:58:04Z", 
    "summary": "Weierstrass representation is a classical parameterization of minimal\nsurfaces. However, two functions should be specified to construct the\nparametric form in Weierestrass representation. In this paper, we propose an\nexplicit parametric form for a class of parametric polynomial minimal surfaces\nof arbitrary degree. It includes the classical Enneper surface for cubic case.\nThe proposed minimal surfaces also have some interesting properties such as\nsymmetry, containing straight lines and self-intersections. According to the\nshape properties, the proposed minimal surface can be classified into four\ncategories with respect to $n=4k-1$ $n=4k+1$, $n=4k$ and $n=4k+2$. The explicit\nparametric form of corresponding conjugate minimal surfaces is given and the\nisometric deformation is also implemented."
},{
    "category": "cs.GR", 
    "doi": "10.4204/EPTCS.31.3", 
    "link": "http://arxiv.org/pdf/1008.1664v1", 
    "title": "L-systems in Geometric Modeling", 
    "arxiv-id": "1008.1664v1", 
    "author": "Faramarz Samavati", 
    "publish": "2010-08-10T08:34:46Z", 
    "summary": "We show that parametric context-sensitive L-systems with affine geometry\ninterpretation provide a succinct description of some of the most fundamental\nalgorithms of geometric modeling of curves. Examples include the\nLane-Riesenfeld algorithm for generating B-splines, the de Casteljau algorithm\nfor generating Bezier curves, and their extensions to rational curves. Our\nresults generalize the previously reported geometric-modeling applications of\nL-systems, which were limited to subdivision curves."
},{
    "category": "math.GT", 
    "doi": "10.4204/EPTCS.31.3", 
    "link": "http://arxiv.org/pdf/1008.2819v1", 
    "title": "A symmetric motion picture of the twist-spun trefoil", 
    "arxiv-id": "1008.2819v1", 
    "author": "Ayumu Inoue", 
    "publish": "2010-08-17T05:04:20Z", 
    "summary": "With the aid of a computer, we provide a motion picture of the twist-spun\ntrefoil which exhibits the periodicity well."
},{
    "category": "physics.geo-ph", 
    "doi": "10.4204/EPTCS.31.3", 
    "link": "http://arxiv.org/pdf/1009.2231v2", 
    "title": "Symbolic landforms created by ancient earthworks near Lake Titicaca", 
    "arxiv-id": "1009.2231v2", 
    "author": "Amelia Carolina Sparavigna", 
    "publish": "2010-09-12T11:56:13Z", 
    "summary": "Interesting landforms created by an ancient network of earthworks are shown,\nusing Google satellite imagery enhanced by an image processing. This network\ncovers a large part of the land near the Titicaca Lake. Satellite images\nclearly display the slopes of hills criss-crossed with terrace walls and the\nsurfaces of the plains covered with raised fields, indicating that this was\nonce a highly productive agricultural place for the south central Andes. Some\nof the landforms are rather remarkable, having a clear symbolic function. Among\nthem, there are structures which seem to represent birds, where ponds are their\neyes."
},{
    "category": "q-bio.BM", 
    "doi": "10.1186/1471-2105-13-S4-S16", 
    "link": "http://arxiv.org/pdf/1009.4674v3", 
    "title": "Intuitive representation of surface properties of biomolecules using   BioBlender", 
    "arxiv-id": "1009.4674v3", 
    "author": "Monica Zopp\u00e8", 
    "publish": "2010-09-23T18:19:43Z", 
    "summary": "In this and the associated article 'BioBlender: Fast and Efficient All Atom\nMorphing of Proteins Using Blender Game Engine', by Zini et al., we present\nBioBlender, a complete instrument for the elaboration of motion (Zini et al.)\nand the visualization (here) of proteins and other macromolecules, using\ninstruments of computer graphics. The availability of protein structures\nenables the study of their surfaces and surface properties such as\nelectrostatic potential (EP) and hydropathy (MLP), based on atomic\ncontribution. Recent advances in 3D animation and rendering software have not\nyet been exploited for the representation of proteins and other biological\nmolecules in an intuitive, animated form. Taking advantage of an open-source,\n3D animation and rendering software, Blender, we developed BioBlender, a\npackage dedicated to biological work: elaboration of proteins' motions with the\nsimultaneous visualization of chemical and physical features. EP and MLP are\ncalculated using physico-chemical programs and custom programs and scripts,\norganized and accessed within BioBlender interface. A new visual code is\nintroduced for MLP visualization: a range of optical features that permits a\nphotorealistic rendering of its spatial distribution on the surface of the\nprotein. EP is represented as animated line particles that flow along field\nlines proportional to the total charge of the protein. Our system permits EP\nand MLP visualization of molecules and, in the case of moving proteins, the\ncontinuous perception of these features, calculated for each intermediate\nconformation. Using real world tactile/sight feelings, the nanoscale world of\nproteins becomes more understandable, familiar to our everyday life, making it\neasier to introduce \"un-seen\" phenomena (concepts) such as hydropathy or\ncharges."
},{
    "category": "cs.GR", 
    "doi": "10.1186/1471-2105-13-S4-S16", 
    "link": "http://arxiv.org/pdf/1009.5183v1", 
    "title": "A Framework for an Ego-centered and Time-aware Visualization of   Relations in Arbitrary Data Repositories", 
    "arxiv-id": "1009.5183v1", 
    "author": "Florian Reitz", 
    "publish": "2010-09-27T08:16:36Z", 
    "summary": "Understanding constellations in large data collections has become a common\ntask. One obstacle a user has to overcome is the internal complexity of these\nrepositories. For example, extracting connected data from a normalized\nrelational database requires knowledge of the table structure which might not\nbe available for the casual user. In this paper we present a visualization\nframework which presents the collection as a set of entities and relations (on\nthe data level). Using rating functions, we divide large relation networks into\nsmall graphs which resemble ego-centered networks. These graphs are connected\nso the user can browse from one to another. To further assist the user, we\npresent two views which embed information on the evolution of the relations\ninto the graphs. Each view emphasizes another aspect of temporal development.\nThe framework can be adapted to any repository by a flexible data interface and\na graph configuration file. We present some first web-based applications\nincluding a visualization of the DBLP data set. We use the DBLP visualization\nto evaluate our approach."
},{
    "category": "cs.CG", 
    "doi": "10.1186/1471-2105-13-S4-S16", 
    "link": "http://arxiv.org/pdf/1010.0552v2", 
    "title": "Inaccessibility-Inside Theorem for Point in Polygon", 
    "arxiv-id": "1010.0552v2", 
    "author": "Luca Nanetti", 
    "publish": "2010-10-04T11:49:35Z", 
    "summary": "The manuscript presents a theoretical proof in conglomeration with new\ndefinitions on Inaccessibility and Inside for a point S related to a simple or\nself intersecting polygon P. The proposed analytical solution depicts a novel\nway of solving the point in polygon problem by employing the properties of\nepigraphs and hypographs, explicitly. Contrary to the ambiguous solutions given\nby the cross over for the simple and self intersecting polygons and the\nsolution of a point being multiply inside a self intersecting polygon given by\nthe winding number rule, the current solution gives unambiguous and singular\nresult for both kinds of polygons. Finally, the current theoretical solution\nproves to be mathematically correct for simple and self intersecting polygons."
},{
    "category": "cs.GR", 
    "doi": "10.1186/1471-2105-13-S4-S16", 
    "link": "http://arxiv.org/pdf/1010.2623v2", 
    "title": "Surface Curvature Effects on Reflectance from Translucent Materials", 
    "arxiv-id": "1010.2623v2", 
    "author": "Konstantin Kolchin", 
    "publish": "2010-10-13T10:35:23Z", 
    "summary": "Most of the physically based techniques for rendering translucent objects use\nthe diffusion theory of light scattering in turbid media. The widely used\ndipole diffusion model (Jensen et al. 2001) applies the diffusion-theory\nformula derived for a planar interface to objects of arbitrary shapes. This\npaper presents first results of our investigation of how surface curvature\naffects the diffuse reflectance from translucent materials."
},{
    "category": "cs.CV", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1011.0093v1", 
    "title": "Fast Color Quantization Using Weighted Sort-Means Clustering", 
    "arxiv-id": "1011.0093v1", 
    "author": "M. Emre Celebi", 
    "publish": "2010-10-30T16:56:17Z", 
    "summary": "Color quantization is an important operation with numerous applications in\ngraphics and image processing. Most quantization methods are essentially based\non data clustering algorithms. However, despite its popularity as a general\npurpose clustering algorithm, k-means has not received much respect in the\ncolor quantization literature because of its high computational requirements\nand sensitivity to initialization. In this paper, a fast color quantization\nmethod based on k-means is presented. The method involves several modifications\nto the conventional (batch) k-means algorithm including data reduction, sample\nweighting, and the use of triangle inequality to speed up the nearest neighbor\nsearch. Experiments on a diverse set of images demonstrate that, with the\nproposed modifications, k-means becomes very competitive with state-of-the-art\ncolor quantization methods in terms of both effectiveness and efficiency."
},{
    "category": "cs.CV", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1011.3189v5", 
    "title": "Warping Peirce Quincuncial Panoramas", 
    "arxiv-id": "1011.3189v5", 
    "author": "Brian K. Vogel", 
    "publish": "2010-11-14T07:53:26Z", 
    "summary": "The Peirce quincuncial projection is a mapping of the surface of a sphere to\nthe interior of a square. It is a conformal map except for four points on the\nequator. These points of non-conformality cause significant artifacts in\nphotographic applications. In this paper, we propose an algorithm and\nuser-interface to mitigate these artifacts. Moreover, in order to facilitate an\ninteractive user-interface, we present a fast algorithm for calculating the\nPeirce quincuncial projection of spherical imagery. We then promote the Peirce\nquincuncial projection as a viable alternative to the more popular\nstereographic projection in some scenarios."
},{
    "category": "cs.GR", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1011.6049v1", 
    "title": "Video Stippling", 
    "arxiv-id": "1011.6049v1", 
    "author": "Frank Nielsen", 
    "publish": "2010-11-28T15:04:34Z", 
    "summary": "In this paper, we consider rendering color videos using a non-photo-realistic\nart form technique commonly called stippling. Stippling is the art of rendering\nimages using point sets, possibly with various attributes like sizes,\nelementary shapes, and colors. Producing nice stippling is attractive not only\nfor the sake of image depiction but also because it yields a compact vectorial\nformat for storing the semantic information of media. Moreover, stippling is by\nconstruction easily tunable to various device resolutions without suffering\nfrom bitmap sampling artifacts when resizing. The underlying core technique for\nstippling images is to compute a centroidal Voronoi tessellation on a\nwell-designed underlying density. This density relates to the image content,\nand is used to compute a weighted Voronoi diagram. By considering videos as\nimage sequences and initializing properly the stippling of one image by the\nresult of its predecessor, one avoids undesirable point flickering artifacts\nand can produce stippled videos that nevertheless still exhibit noticeable\nartifacts. To overcome this, our method improves over the naive scheme by\nconsidering dynamic point creation and deletion according to the current scene\nsemantic complexity, and show how to effectively vectorize video while\nadjusting for both color and contrast characteristics. Furthermore, we explain\nhow to produce high quality stippled ``videos'' (eg., fully dynamic\nspatio-temporal point sets) for media containing various fading effects, like\nquick motions of objects or progressive shot changes. We report on practical\nperformances of our implementation, and present several stippled video results\nrendered on-the-fly using our viewer that allows both spatio-temporal dynamic\nrescaling (eg., upscale vectorially frame rate)."
},{
    "category": "cs.HC", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1012.0467v1", 
    "title": "MT4j - A Cross-platform Multi-touch Development Framework", 
    "arxiv-id": "1012.0467v1", 
    "author": "Jan Zibuschka", 
    "publish": "2010-12-02T16:01:21Z", 
    "summary": "This article describes requirements and challenges of crossplatform\nmulti-touch software engineering, and presents the open source framework\nMulti-Touch for Java (MT4j) as a solution. MT4j is designed for rapid\ndevelopment of graphically rich applications on a variety of contemporary\nhardware, from common PCs and notebooks to large-scale ambient displays, as\nwell as different operating systems. The framework has a special focus on\nmaking multi-touch software development easier and more efficient. Architecture\nand abstractions used by MT4j are described, and implementations of several\ncommon use cases are presented."
},{
    "category": "cs.CG", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1012.3057v2", 
    "title": "Speeding Up the 3D Surface Generator VESTA", 
    "arxiv-id": "1012.3057v2", 
    "author": "B. R. Schlei", 
    "publish": "2010-12-14T15:24:04Z", 
    "summary": "The very recent volume-enclosing surface extraction algorithm, VESTA, is\nrevisited. VESTA is used to determine implicit surfaces that are potentially\ncontained in 3D data sets, such as 3D image data and/or 3D simulation data.\nVESTA surfaces are non-degenerate, i.e., they always enclose a volume that is\nlarger than zero and they never self-intersect, prior to a further processing,\ne.g., towards isosurfaces. In addition to its ability to deal with local cell\nambiguities consistently - and thereby avoiding the accidental generation of\nholes in the final surfaces - the information of the interior and/or exterior\nof enclosed 3D volumes is propagated correctly to each of the final surface\ntiles. Particular emphasis is put here on the speed up of the original\nformulation of VESTA, while applying the algorithm to 2x2x2 voxel\nneighborhoods."
},{
    "category": "cs.GR", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1101.0663v1", 
    "title": "The Role of Computer Graphics in Documentary Film Production", 
    "arxiv-id": "1101.0663v1", 
    "author": "Miao Song", 
    "publish": "2011-01-04T06:53:36Z", 
    "summary": "We discuss a topic on the role of computer graphics in the production of\ndocumentaries, which is often ignored in favor of other topics. Typically,\nexcept for some rare occasions, documentary producers and computer scientists\nor digital artists that do computer graphics are relatively far apart in their\ndomains and rarely intercommunicate to have a joint production; yet it happens,\nand perhaps more so in the present and the future.\n  We attempt to classify the documentaries on the amount and techniques of\ncomputer graphics used for documentaries. We come up with the initial\ncategories such as \"plain\" (no graphics), \"in-between\", \"all-out\" -- nearly\n100% of the documentary consisting of computer-generated imagery. Computer\ngraphics can be used to enhance the scenery, fill in the gaps in the missing\nstoryline pieces, or animate between scenes. It can incorporate stereoscopic\neffects for higher viewer impression as well as interactivity aspects. It can\nalso be used simply in old archived image and film restoration."
},{
    "category": "cs.GR", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1101.1240v1", 
    "title": "Chameleon: A Color-Adaptive Web Browser for Mobile OLED Displays", 
    "arxiv-id": "1101.1240v1", 
    "author": "Lin Zhong", 
    "publish": "2010-12-13T18:45:13Z", 
    "summary": "Displays based on organic light-emitting diode (OLED) technology are\nappearing on many mobile devices. Unlike liquid crystal displays (LCD), OLED\ndisplays consume dramatically different power for showing different colors. In\nparticular, OLED displays are inefficient for showing bright colors. This has\nmade them undesirable for mobile devices because much of the web content is of\nbright colors.\n  To tackle this problem, we present the motivational studies, design, and\nrealization of Chameleon, a color adaptive web browser that renders web pages\nwith power-optimized color schemes under user-supplied constraints. Driven by\nthe findings from our motivational studies, Chameleon provides end users with\nimportant options, offloads tasks that are not absolutely needed in real-time,\nand accomplishes real-time tasks by carefully enhancing the codebase of a\nbrowser engine. According to measure-ments with OLED smartphones, Chameleon is\nable to re-duce average system power consumption for web browsing by 41% and\nreduce display power consumption by 64% without introducing any noticeable\ndelay."
},{
    "category": "math.NA", 
    "doi": "10.1364/JOSAA.26.002434", 
    "link": "http://arxiv.org/pdf/1102.0200v3", 
    "title": "Harmonic Functions for Data Reconstruction on 3D Manifolds", 
    "arxiv-id": "1102.0200v3", 
    "author": "Feng Luo", 
    "publish": "2011-02-01T16:15:13Z", 
    "summary": "In computer graphics, smooth data reconstruction on 2D or 3D manifolds\nusually refers to subdivision problems. Such a method is only valid based on\ndense sample points. The manifold usually needs to be triangulated into meshes\n(or patches) and each node on the mesh will have an initial value. While the\nmesh is refined the algorithm will provide a smooth function on the redefined\nmanifolds. However, when data points are not dense and the original mesh is not\nallowed to be changed, how is the \"continuous and/or smooth\" reconstruction\npossible? This paper will present a new method using harmonic functions to\nsolve the problem. Our method contains the following steps: (1) Partition the\nboundary surfaces of the 3D manifold based on sample points so that each sample\npoint is on the edge of the partition. (2) Use gradually varied interpolation\non the edges so that each point on edge will be assigned a value. In addition,\nall values on the edge are gradually varied. (3) Use discrete harmonic function\nto fit the unknown points, i.e. the points inside each partition patch.\n  The fitted function will be a harmonic or a local harmonic function in each\npartitioned area. The function on edge will be \"near\" continuous (or \"near\"\ngradually varied). If we need a smoothed surface on the manifold, we can apply\nsubdivision algorithms. This paper has also a philosophical advantage over\ntriangulation meshes. People usually use triangulation for data reconstruction.\nThis paper employs harmonic functions, a generalization of triangulation\nbecause linearity is a form of harmonic. Therefore, local harmonic\ninitialization is more sophisticated then triangulation. This paper is a\nconceptual and methodological paper. This paper does not focus on detailed\nmathematical analysis nor fine algorithm design."
},{
    "category": "cs.GR", 
    "doi": "10.4204/EPTCS.48.5", 
    "link": "http://arxiv.org/pdf/1102.2652v1", 
    "title": "Rule-based transformations for geometric modelling", 
    "arxiv-id": "1102.2652v1", 
    "author": "Pascale Le Gall", 
    "publish": "2011-02-14T01:09:24Z", 
    "summary": "The context of this paper is the use of formal methods for topology-based\ngeometric modelling. Topology-based geometric modelling deals with objects of\nvarious dimensions and shapes. Usually, objects are defined by a graph-based\ntopological data structure and by an embedding that associates each topological\nelement (vertex, edge, face, etc.) with relevant data as their geometric shape\n(position, curve, surface, etc.) or application dedicated data (e.g. molecule\nconcentration level in a biological context). We propose to define\ntopology-based geometric objects as labelled graphs. The arc labelling defines\nthe topological structure of the object whose topological consistency is then\nensured by labelling constraints. Nodes have as many labels as there are\ndifferent data kinds in the embedding. Labelling constraints ensure then that\nthe embedding is consistent with the topological structure. Thus,\ntopology-based geometric objects constitute a particular subclass of a category\nof labelled graphs in which nodes have multiple labels."
},{
    "category": "cs.GR", 
    "doi": "10.4204/EPTCS.48.5", 
    "link": "http://arxiv.org/pdf/1102.3328v1", 
    "title": "An Efficient and Integrated Algorithm for Video Enhancement in   Challenging Lighting Conditions", 
    "arxiv-id": "1102.3328v1", 
    "author": "Wei Meng", 
    "publish": "2011-02-16T13:04:18Z", 
    "summary": "We describe a novel integrated algorithm for real-time enhancement of video\nacquired under challenging lighting conditions. Such conditions include low\nlighting, haze, and high dynamic range situations. The algorithm automatically\ndetects the dominate source of impairment, then depending on whether it is low\nlighting, haze or others, a corresponding pre-processing is applied to the\ninput video, followed by the core enhancement algorithm. Temporal and spatial\nredundancies in the video input are utilized to facilitate real-time processing\nand to improve temporal and spatial consistency of the output. The proposed\nalgorithm can be used as an independent module, or be integrated in either a\nvideo encoder or a video decoder for further optimizations."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1071/AS10031", 
    "link": "http://arxiv.org/pdf/1102.5123v1", 
    "title": "Scientific Visualization in Astronomy: Towards the Petascale Astronomy   Era", 
    "arxiv-id": "1102.5123v1", 
    "author": "Christopher J. Fluke", 
    "publish": "2011-02-24T22:54:18Z", 
    "summary": "Astronomy is entering a new era of discovery, coincident with the\nestablishment of new facilities for observation and simulation that will\nroutinely generate petabytes of data. While an increasing reliance on automated\ndata analysis is anticipated, a critical role will remain for\nvisualization-based knowledge discovery. We have investigated scientific\nvisualization applications in astronomy through an examination of the\nliterature published during the last two decades. We identify the two most\nactive fields for progress - visualization of large-N particle data and\nspectral data cubes - discuss open areas of research, and introduce a mapping\nbetween astronomical sources of data and data representations used in general\npurpose visualization tools. We discuss contributions using high performance\ncomputing architectures (e.g: distributed processing and GPUs), collaborative\nastronomy visualization, the use of workflow systems to store metadata about\nvisualization parameters, and the use of advanced interaction devices. We\nexamine a number of issues that may be limiting the spread of scientific\nvisualization research in astronomy and identify six grand challenges for\nscientific visualization research in the Petascale Astronomy Era."
},{
    "category": "cs.HC", 
    "doi": "10.1071/AS10031", 
    "link": "http://arxiv.org/pdf/1103.2063v1", 
    "title": "Augmented reality usage for prototyping speed up", 
    "arxiv-id": "1103.2063v1", 
    "author": "Jaromir Landa", 
    "publish": "2011-03-10T16:00:52Z", 
    "summary": "The first part of the article describes our approach for solution of this\nproblem by means of Augmented Reality. The merging of the real world model and\ndigital objects allows streamline the work with the model and speed up the\nwhole production phase significantly. The main advantage of augmented reality\nis the possibility of direct manipulation with the scene using a portable\ndigital camera. Also adding digital objects into the scene could be done using\nidentification markers placed on the surface of the model. Therefore it is not\nnecessary to work with special input devices and lose the contact with the real\nworld model. Adjustments are done directly on the model. The key problem of\noutlined solution is the ability of identification of an object within the\ncamera picture and its replacement with the digital object. The second part of\nthe article is focused especially on the identification of exact position and\norientation of the marker within the picture. The identification marker is\ngeneralized into the triple of points which represents a general plane in\nspace. There is discussed the space identification of these points and the\ndescription of representation of their position and orientation be means of\ntransformation matrix. This matrix is used for rendering of the graphical\nobjects (e. g. in OpenGL and Direct3D)."
},{
    "category": "cs.GR", 
    "doi": "10.4108/icst.simutools.2011.245524", 
    "link": "http://arxiv.org/pdf/1103.4271v2", 
    "title": "Rendering of 3D Dynamic Virtual Environments", 
    "arxiv-id": "1103.4271v2", 
    "author": "Francesco Pagano", 
    "publish": "2011-03-22T14:16:07Z", 
    "summary": "In this paper we present a framework for the rendering of dynamic 3D virtual\nenvironments which can be integrated in the development of videogames. It\nincludes methods to manage sounds and particle effects, paged static\ngeometries, the support of a physics engine and various input systems. It has\nbeen designed with a modular structure to allow future expansions. We exploited\nsome open-source state-of-the-art components such as OGRE, PhysX,\nParticleUniverse, etc.; all of them have been properly integrated to obtain\npeculiar physical and environmental effects. The stand-alone version of the\napplication is fully compatible with Direct3D and OpenGL APIs and adopts OpenAL\nAPIs to manage audio cards. Concluding, we devised a showcase demo which\nreproduces a dynamic 3D environment, including some particular effects: the\nalternation of day and night infuencing the lighting of the scene, the\nrendering of terrain, water and vegetation, the reproduction of sounds and\natmospheric agents."
},{
    "category": "physics.ed-ph", 
    "doi": "10.4108/icst.simutools.2011.245524", 
    "link": "http://arxiv.org/pdf/1103.5028v2", 
    "title": "User guide to TIM, a ray-tracing program for forbidden ray optics", 
    "arxiv-id": "1103.5028v2", 
    "author": "Johannes Courtial", 
    "publish": "2011-03-25T16:45:22Z", 
    "summary": "This user guide outlines the use of TIM, an interactive ray-tracing program\nwith a number of special powers. TIM can be customised and embedded into\ninternet pages, making it suitable not only for research but also for its\ndissemination."
},{
    "category": "cs.CV", 
    "doi": "10.4108/icst.simutools.2011.245524", 
    "link": "http://arxiv.org/pdf/1105.3617v1", 
    "title": "Face Shape and Reflectance Acquisition using a Multispectral Light Stage", 
    "arxiv-id": "1105.3617v1", 
    "author": "Abhishek Dutta", 
    "publish": "2011-05-18T13:00:44Z", 
    "summary": "In this thesis, we discuss the design and calibration (geometric and\nradiometric) of a novel shape and reflectance acquisition device called the\n\"Multispectral Light Stage\". This device can capture highly detailed facial\ngeometry (down to the level of skin pores detail) and Multispectral reflectance\nmap which can be used to estimate biophysical skin parameters such as the\ndistribution of pigmentation and blood beneath the surface of the skin. We\nextend the analysis of the original spherical gradient photometric stereo\nmethod to study the effects of deformed diffuse lobes on the quality of\nrecovered surface normals. Based on our modified radiance equations, we develop\na minimal image set method to recover high quality photometric normals using\nonly four, instead of six, spherical gradient images. Using the same radiance\nequations, we explore a Quadratic Programming (QP) based algorithm for\ncorrection of surface normals obtained using spherical gradient photometric\nstereo. Based on the proposed minimal image sets method, we present a\nperformance capture sequence that significantly reduces the data capture\nrequirement and post-processing computational cost of existing photometric\nstereo based performance geometry capture methods. Furthermore, we explore the\nuse of images captured in our Light Stage to generate stimuli images for a\npsychology experiment exploring the neural representation of 3D shape and\ntexture of a human face."
},{
    "category": "cs.GR", 
    "doi": "10.4108/icst.simutools.2011.245524", 
    "link": "http://arxiv.org/pdf/1106.2877v1", 
    "title": "Injectivity of 2D Toric B\u00e9zier Patches", 
    "arxiv-id": "1106.2877v1", 
    "author": "Chungang Zhu", 
    "publish": "2011-06-15T05:35:51Z", 
    "summary": "Rational B\\'{e}zier functions are widely used as mapping functions in surface\nreparameterization, finite element analysis, image warping and morphing. The\ninjectivity (one-to-one property) of a mapping function is typically necessary\nfor these applications. Toric B\\'{e}zier patches are generalizations of\nclassical patches (triangular, tensor product) which are defined on the convex\nhull of a set of integer lattice points. We give a geometric condition on the\ncontrol points that we show is equivalent to the injectivity of every 2D toric\nB\\'{e}zier patch with those control points for all possible choices of weights.\nThis condition refines that of Craciun, et al., which only implied injectivity\non the interior of a patch."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-30214-5_23", 
    "link": "http://arxiv.org/pdf/1107.0690v1", 
    "title": "A Framework for Designing 3D Virtual Environments", 
    "arxiv-id": "1107.0690v1", 
    "author": "Francesco Pagano", 
    "publish": "2011-07-04T17:51:00Z", 
    "summary": "The process of design and development of virtual environments can be\nsupported by tools and frameworks, to save time in technical aspects and\nfocusing on the content. In this paper we present an academic framework which\nprovides several levels of abstraction to ease this work. It includes\nstate-of-the-art components we devised or integrated adopting open-source\nsolutions in order to face specific problems. Its architecture is modular and\ncustomizable, the code is open-source."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1107.2715v1", 
    "title": "Stereo pairs in Astrophysics", 
    "arxiv-id": "1107.2715v1", 
    "author": "Alexander Y. Wagner", 
    "publish": "2011-07-14T02:42:12Z", 
    "summary": "Stereoscopic visualization is seldom used in Astrophysical publications and\npresentations compared to other scientific fields, e.g., Biochemistry, where it\nhas been recognized as a valuable tool for decades. We put forth the view that\nstereo pairs can be a useful tool for the Astrophysics community in\ncommunicating a truer representation of astrophysical data. Here, we review the\nmain theoretical aspects of stereoscopy, and present a tutorial to easily\ncreate stereo pairs using Python. We then describe how stereo pairs provide a\nway to incorporate 3D data in 2D publications of standard journals. We\nillustrate the use of stereo pairs with one conceptual and two Astrophysical\nscience examples: an integral field spectroscopy study of a supernova remnant,\nand numerical simulations of a relativistic AGN jet. We also use these examples\nto make the case that stereo pairs are not merely an ostentatious way to\npresent data, but an enhancement in the communication of scientific results in\npublications because they provide the reader with a realistic view of\nmulti-dimensional data, be it of observational or theoretical nature. In\nrecognition of the ongoing 3D expansion in the commercial sector, we advocate\nan increased use of stereo pairs in Astrophysics publications and presentations\nas a first step towards new interactive and multi-dimensional publication\nmethods."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1107.3680v1", 
    "title": "3-Phase Recognition Approach to Pseudo 3D Building Generation from 2D   Floor Plan", 
    "arxiv-id": "1107.3680v1", 
    "author": "Abu Salmaan Auleear", 
    "publish": "2011-07-19T10:50:31Z", 
    "summary": "Nowadays three dimension (3D) architectural visualisation has become a\npowerful tool in the conceptualisation, design and presentation of\narchitectural products in the construction industry, providing realistic\ninteraction and walkthrough on engineering products. Traditional ways of\nimplementing 3D models involves the use of specialised 3D authoring tools along\nwith skilled 3D designers with blueprints of the model and this is a slow and\nlaborious process. The aim of this paper is to automate this process by simply\nanalyzing the blueprint document and generating the 3D scene automatically. For\nthis purpose we have devised a 3-Phase recognition approach to pseudo 3D\nbuilding generation from 2D floor plan and developed a software accordingly.\nOur 3-phased 3D building system has been implemented using C, C++ and OpenCV\nlibrary [24] for the Image Processing module; The Save Module generated an XML\nfile for storing the processed floor plan objects attributes; while the\nIrrlitch [14] game engine was used to implement the Interactive 3D module.\nThough still at its infancy, our proposed system gave commendable results. We\ntested our system on 6 floor plans with complexities ranging from low to high\nand the results seems to be very promising with an average processing time of\naround 3s and a 3D generation in 4s. In addition the system provides an\ninteractive walk-though and allows users to modify components."
},{
    "category": "math.DG", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1108.3529v1", 
    "title": "Fat Triangulations and Differential Geometry", 
    "arxiv-id": "1108.3529v1", 
    "author": "Emil Saucan", 
    "publish": "2011-08-17T17:22:46Z", 
    "summary": "We study the differential geometric consequences of our previous result on\nthe existence of fat triangulations, in conjunction with a result of Cheeger,\nM\\\"{u}ller and Schrader, regarding the convergence of Lipschitz-Killing\ncurvatures of piecewise-flat approximations of smooth Riemannian manifolds. A\nfurther application to the existence of quasiconformal mappings between\nmanifolds, as well as an extension of the triangulation result to the case of\nalmost Riemannian manifolds, are also given. In addition, the notion of fatness\nof triangulations and its relation to metric curvature and to excess is\nexplored. Moreover, applications of the main results, and in particular a\npurely metric approach to Regge calculus, are also investigated."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1110.5360v1", 
    "title": "New Zealand involvement in Radio Astronomical VLBI Image Processing", 
    "arxiv-id": "1110.5360v1", 
    "author": "Sergei Gulyaev", 
    "publish": "2011-10-24T21:39:12Z", 
    "summary": "With the establishment of the AUT University 12m radio telescope at\nWarkworth, New Zealand has now become a part of the international Very Long\nBaseline Interferometry (VLBI) community. A major product of VLBI observations\nare images in the radio domain of astronomical objects such as Active Galactic\nNuclei (AGN). Using large geographical separations between radio antennas, very\nhigh angular resolution can be achieved. Detailed images can be created using\nthe technique of VLBI Earth Rotation Aperture Synthesis. We review the current\nprocess of VLBI radio imaging. In addition we model VLBI configurations using\nthe Warkworth telescope, AuScope (a new array of three 12m antennas in\nAustralia) and the Australian Square Kilometre Array Pathfinder (ASKAP) array\ncurrently under construction in Western Australia, and discuss how the\nconfiguration of these arrays affects the quality of images. Recent imaging\nresults that demonstrate the modeled improvements from inclusion of the AUT and\nfirst ASKAP telescope in the Australian Long Baseline Array (LBA) are\npresented."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1111.3969v2", 
    "title": "The Object Projection Feature Estimation Problem in Unsupervised   Markerless 3D Motion Tracking", 
    "arxiv-id": "1111.3969v2", 
    "author": "Alejandro J. Le\u00f3n", 
    "publish": "2011-11-16T21:26:55Z", 
    "summary": "3D motion tracking is a critical task in many computer vision applications.\nExisting 3D motion tracking techniques require either a great amount of\nknowledge on the target object or specific hardware. These requirements\ndiscourage the wide spread of commercial applications based on 3D motion\ntracking. 3D motion tracking systems that require no knowledge on the target\nobject and run on a single low-budget camera require estimations of the object\nprojection features (namely, area and position). In this paper, we define the\nobject projection feature estimation problem and we present a novel 3D motion\ntracking system that needs no knowledge on the target object and that only\nrequires a single low-budget camera, as installed in most computers and\nsmartphones. Our system estimates, in real time, the three-dimensional position\nof a non-modeled unmarked object that may be non-rigid, non-convex, partially\noccluded, self occluded, or motion blurred, given that it is opaque, evenly\ncolored, and enough contrasting with the background in each frame. Our system\nis also able to determine the most relevant object to track in the screen. Our\n3D motion tracking system does not impose hard constraints, therefore it allows\na market-wide implementation of applications that use 3D motion tracking."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1112.3110v1", 
    "title": "GPU-based Image Analysis on Mobile Devices", 
    "arxiv-id": "1112.3110v1", 
    "author": "Seth Hall", 
    "publish": "2011-12-14T03:46:46Z", 
    "summary": "With the rapid advances in mobile technology many mobile devices are capable\nof capturing high quality images and video with their embedded camera. This\npaper investigates techniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues and\nlimitations of image processing on mobile devices are discussed, and the\nperformance of graphical processing units on a range of devices measured\nthrough a programmable shader implementation of Canny edge detection."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1201.1409v1", 
    "title": "Interactive Character Posing by Sparse Coding", 
    "arxiv-id": "1201.1409v1", 
    "author": "J. H. Lai", 
    "publish": "2012-01-06T13:11:01Z", 
    "summary": "Character posing is of interest in computer animation. It is difficult due to\nits dependence on inverse kinematics (IK) techniques and articulate property of\nhuman characters . To solve the IK problem, classical methods that rely on\nnumerical solutions often suffer from the under-determination problem and can\nnot guarantee naturalness. Existing data-driven methods address this problem by\nlearning from motion capture data. When facing a large variety of poses\nhowever, these methods may not be able to capture the pose styles or be\napplicable in real-time environment. Inspired from the low-rank motion\nde-noising and completion model in \\cite{lai2011motion}, we propose a novel\nmodel for character posing based on sparse coding. Unlike conventional\napproaches, our model directly captures the pose styles in Euclidean space to\nprovide intuitive training error measurements and facilitate pose synthesis. A\npose dictionary is learned in training stage and based on it natural poses are\nsynthesized to satisfy users' constraints . We compare our model with existing\nmodels for tasks of pose de-noising and completion. Experiments show our model\nobtains lower de-noising and completion error. We also provide User\nInterface(UI) examples illustrating that our model is effective for interactive\ncharacter posing."
},{
    "category": "cs.CG", 
    "doi": "10.1007/s10509-011-0801-z", 
    "link": "http://arxiv.org/pdf/1201.5788v1", 
    "title": "A toolkit to describe and interactively display three-manifolds embedded   in four-space", 
    "arxiv-id": "1201.5788v1", 
    "author": "Don V. Black", 
    "publish": "2012-01-25T20:18:39Z", 
    "summary": "A data structure and toolkit are presented here that allow for the\ndescription and manipulation of mathematical models of three-manifolds and\ntheir interactive display from multiple viewpoints via the OpenGL 3D graphics\npackage. The data structure and vector math package can be extended to support\nan arbitrary number of Euclidean spatial dimensions.\n  A model in 4-space is described by its bounding pure simplicial 3-complex. By\nintersecting a 3-flat with this 3-manifold, the algorithm will extract the\nrequested closed pure simplicial 2-complex surface enclosing the desired 3D\nslice. The user can interactively rotate, pan, zoom, and shade arbitrary 3D\nsolid or wire-frame views of the revealed 3D object created by intersection,\nthus exploring both expected and unexpected symmetries or asymmetries in the\nworld of 3-manifolds in 4-space."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1202.1444v2", 
    "title": "Fully Automatic Expression-Invariant Face Correspondence", 
    "arxiv-id": "1202.1444v2", 
    "author": "Flavio Prieto", 
    "publish": "2012-02-07T15:05:05Z", 
    "summary": "We consider the problem of computing accurate point-to-point correspondences\namong a set of human face scans with varying expressions. Our fully automatic\napproach does not require any manually placed markers on the scan. Instead, the\napproach learns the locations of a set of landmarks present in a database and\nuses this knowledge to automatically predict the locations of these landmarks\non a newly available scan. The predicted landmarks are then used to compute\npoint-to-point correspondences between a template model and the newly available\nscan. To accurately fit the expression of the template to the expression of the\nscan, we use as template a blendshape model. Our algorithm was tested on a\ndatabase of human faces of different ethnic groups with strongly varying\nexpressions. Experimental results show that the obtained point-to-point\ncorrespondence is both highly accurate and consistent for most of the tested 3D\nface models."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1202.2868v1", 
    "title": "Visual definition of procedures for automatic virtual scene generation", 
    "arxiv-id": "1202.2868v1", 
    "author": "Drazen Lucanin", 
    "publish": "2012-02-10T16:58:00Z", 
    "summary": "With more and more digital media, especially in the field of virtual reality\nwhere detailed and convincing scenes are much required, procedural scene\ngeneration is a big helping tool for artists. A problem is that defining scene\ndescriptions through these procedures usually requires a knowledge in formal\nlanguage grammars, programming theory and manually editing textual files using\na strict syntax, making it less intuitive to use. Luckily, graphical user\ninterfaces has made a lot of tasks on computers easier to perform and out of\nthe belief that creating computer programs can also be one of them, visual\nprogramming languages (VPLs) have emerged. The goal in VPLs is to shift more\nwork from the programmer to the integrated development environment (IDE),\nmaking programming an user-friendlier task.\n  In this thesis, an approach of using a VPL for defining procedures that\nautomatically generate virtual scenes is presented. The methods required to\nbuild a VPL are presented, including a novel method of generating readable code\nin a structured programming language. Also, the methods for achieving basic\nprinciples of VPLs will be shown -- suitable visual presentation of information\nand guiding the programmer in the right direction using constraints. On the\nother hand, procedural generation methods are presented in the context of\nvisual programming -- adapting the application programming interface (API) of\nthese methods to better serve the user. The main focus will be on the methods\nfor urban modeling, such as building, city layout and details generation with\nrandom number generation used to create non-deterministic scenes."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1203.3574v1", 
    "title": "Artimate: an articulatory animation framework for audiovisual speech   synthesis", 
    "arxiv-id": "1203.3574v1", 
    "author": "Slim Ouni", 
    "publish": "2012-03-15T21:23:45Z", 
    "summary": "We present a modular framework for articulatory animation synthesis using\nspeech motion capture data obtained with electromagnetic articulography (EMA).\nAdapting a skeletal animation approach, the articulatory motion data is applied\nto a three-dimensional (3D) model of the vocal tract, creating a portable\nresource that can be integrated in an audiovisual (AV) speech synthesis\nplatform to provide realistic animation of the tongue and teeth for a virtual\ncharacter. The framework also provides an interface to articulatory animation\nsynthesis, as well as an example application to illustrate its use with a 3D\ngame engine. We rely on cross-platform, open-source software and open standards\nto provide a lightweight, accessible, and portable workflow."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1206.1148v2", 
    "title": "From individual to population: Challenges in Medical Visualization", 
    "arxiv-id": "1206.1148v2", 
    "author": "Anders Ynnerman", 
    "publish": "2012-06-06T08:38:27Z", 
    "summary": "In this paper, we first give a high-level overview of medical visualization\ndevelopment over the past 30 years, focusing on key developments and the trends\nthat they represent. During this discussion, we will refer to a number of key\npapers that we have also arranged on the medical visualization research\ntimeline. Based on the overview and our observations of the field, we then\nidentify and discuss the medical visualization research challenges that we\nforesee for the coming decade."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1206.1428v2", 
    "title": "Visualization in Connectomics", 
    "arxiv-id": "1206.1428v2", 
    "author": "Jos B. T. M. Roerdink", 
    "publish": "2012-06-07T09:17:34Z", 
    "summary": "Connectomics is a field of neuroscience that analyzes neuronal connections. A\nconnectome is a complete map of a neuronal system, comprising all neuronal\nconnections between its structures. The term \"connectome\" is close to the word\n\"genome\" and implies completeness of all neuronal connections, in the same way\nas a genome is a complete listing of all nucleotide sequences. The goal of\nconnectomics is to create a complete representation of the brain's wiring. Such\na representation is believed to increase our understanding of how functional\nbrain states emerge from their underlying anatomical structure. Furthermore, it\ncan provide important information for the cure of neuronal dysfunctions like\nschizophrenia or autism. In this paper, we review the current state-of-the-art\nof visualization and image processing techniques in the field of connectomics\nand describe some remaining challenges."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1206.1968v4", 
    "title": "A novel 2.5D approach for interfacing with web applications", 
    "arxiv-id": "1206.1968v4", 
    "author": "Saurabh Sarkar", 
    "publish": "2012-06-09T20:04:38Z", 
    "summary": "Web applications need better user interface to be interactive and attractive.\nA new approach/concept of dimensional enhancement - 2.5D \"a 2D display of a\nvirtual 3D environment\", which can be implemented in social networking sites\nand further in other system applications."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s00138-013-0579-9", 
    "link": "http://arxiv.org/pdf/1206.3975v1", 
    "title": "The Ultrasound Visualization Pipeline - A Survey", 
    "arxiv-id": "1206.3975v1", 
    "author": "Ivan Viola", 
    "publish": "2012-06-18T16:05:47Z", 
    "summary": "Ultrasound is one of the most frequently used imaging modality in medicine.\nThe high spatial resolution, its interactive nature and non-invasiveness makes\nit the first choice in many examinations. Image interpretation is one of\nultrasound's main challenges. Much training is required to obtain a confident\nskill level in ultrasound-based diagnostics. State-of-the-art graphics\ntechniques is needed to provide meaningful visualizations of ultrasound in\nreal-time. In this paper we present the process-pipeline for ultrasound\nvisualization, including an overview of the tasks performed in the specific\nsteps. To provide an insight into the trends of ultrasound visualization\nresearch, we have selected a set of significant publications and divided them\ninto a technique-based taxonomy covering the topics pre-processing,\nsegmentation, registration, rendering and augmented reality. For the different\ntechnique types we discuss the difference between ultrasound-based techniques\nand techniques for other modalities."
},{
    "category": "cs.CV", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1206.4880v1", 
    "title": "Dynamic Domain Classification for Fractal Image Compression", 
    "arxiv-id": "1206.4880v1", 
    "author": "M. Jayamohan", 
    "publish": "2012-05-20T17:22:49Z", 
    "summary": "Fractal image compression is attractive except for its high encoding time\nrequirements. The image is encoded as a set of contractive affine\ntransformations. The image is partitioned into non-overlapping range blocks,\nand a best matching domain block larger than the range block is identified.\nThere are many attempts on improving the encoding time by reducing the size of\nsearch pool for range-domain matching. But these methods are attempting to\nprepare a static domain pool that remains unchanged throughout the encoding\nprocess. This paper proposes dynamic preparation of separate domain pool for\neach range block. This will result in significant reduction in the encoding\ntime. The domain pool for a particular range block can be selected based upon a\nparametric value. Here we use classification based on local fractal dimension."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1206.6049v5", 
    "title": "Improved visualisation of brain arteriovenous malformations using color   intensity projections with hue cycling", 
    "arxiv-id": "1206.6049v5", 
    "author": "Keith S. Cover", 
    "publish": "2012-06-25T11:05:36Z", 
    "summary": "Color intensity projections (CIP) have been shown to improve the\nvisualisation of greyscale angiography images by combining greyscale images\ninto a single color image. A key property of the combined CIP image is the\nencoding of the arrival time information from greyscale images into the hue of\nthe color in the CIP image. A few minor improvements to the calculation of the\nCIP image are introduced that substantially improve the quality of the\nvisualisation. One improvement is interpolating of the greyscale images in time\nbefore calculation of the CIP image. A second is the use of hue cycling - where\nthe hue of the color is cycled through more than once in an image. The hue\ncycling allows the variation of the hue to be concentrated in structures of\ninterest. If there is a zero time point hue cycling can be applied after zero\ntime and before zero time can be indicated by greyscale. If there is an end\ntime point hue cycling can be applied before the end time and pixels can be set\nto black after the end time. An angiogram of a brain is used to demonstrate the\nsubstantial improvements hue cycling brings to CIP images. A third improvement\nis the use of maximum intensity projection for 2D rendering of a 3D CIP image\nvolume. A fourth improvement allowing interpreters to interactively adjust the\nphase of the hue via standard contrast - brightness controls using lookup\ntables. Other potential applications of CIP are also mentioned."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1208.3794v1", 
    "title": "General Midpoint Subdivision", 
    "arxiv-id": "1208.3794v1", 
    "author": "Hartmut Prautzsch", 
    "publish": "2012-08-18T23:40:24Z", 
    "summary": "In this paper, we introduce two generalizations of midpoint subdivision and\nanalyze the smoothness of the resulting subdivision surfaces at regular and\nextraordinary points.\n  The smoothing operators used in midpoint and mid-edge subdivision connect the\nmidpoints of adjacent faces or of adjacent edges, respectively. An arbitrary\ncombination of these two operators and the refinement operator that splits each\nface with m vertices into m quadrilateral subfaces forms a general midpoint\nsubdivision operator. We analyze the smoothness of the resulting subdivision\nsurfaces by estimating the norm of a special second order difference scheme and\nby using established methods for analyzing midpoint subdivision. The surfaces\nare smooth at their regular points and they are also smooth at extraordinary\npoints for a certain subclass of general midpoint subdivision schemes.\n  Generalizing the smoothing rules of non general midpoint subdivision schemes\naround extraordinary and regular vertices or faces results in a class of\nsubdivision schemes, which includes the Catmull-Clark algorithm with restricted\nparameters. We call these subdivision schemes generalized Catmull-Clark schemes\nand we analyze their smoothness properties."
},{
    "category": "cs.CR", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1208.5124v1", 
    "title": "A Novel Data Hiding Scheme for Binary Images", 
    "arxiv-id": "1208.5124v1", 
    "author": "Pham Van At", 
    "publish": "2012-08-25T10:45:55Z", 
    "summary": "This paper presents a new scheme for hiding a secret message in binary\nimages. Given m*n cover image block, the new scheme can conceal as many as\nlog(m*n +1) bits of data in block, by changing at most one bit in the block.\nThe hiding ability of the new scheme is the same as Chang et al.'s scheme and\nhigher than Tseng et al.'s scheme. Additionally, the security of the new scheme\nis higher than the two above schemes."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1209.0999v1", 
    "title": "Visual Exploration of Simulated and Measured Blood Flow", 
    "arxiv-id": "1209.0999v1", 
    "author": "Thomas Wischgoll", 
    "publish": "2012-09-05T14:47:55Z", 
    "summary": "Morphology of cardiovascular tissue is influenced by the unsteady behavior of\nthe blood flow and vice versa. Therefore, the pathogenesis of several\ncardiovascular diseases is directly affected by the blood-flow dynamics.\nUnderstanding flow behavior is of vital importance to understand the\ncardiovascular system and potentially harbors a considerable value for both\ndiagnosis and risk assessment. The analysis of hemodynamic characteristics\ninvolves qualitative and quantitative inspection of the blood-flow field.\nVisualization plays an important role in the qualitative exploration, as well\nas the definition of relevant quantitative measures and its validation. There\nare two main approaches to obtain information about the blood flow: simulation\nby computational fluid dynamics, and in-vivo measurements. Although research on\nblood flow simulation has been performed for decades, many open problems remain\nconcerning accuracy and patient-specific solutions. Possibilities for real\nmeasurement of blood flow have recently increased considerably by new\ndevelopments in magnetic resonance imaging which enable the acquisition of 3D\nquantitative measurements of blood-flow velocity fields. This chapter presents\nthe visualization challenges for both simulation and real measurements of\nunsteady blood-flow fields."
},{
    "category": "cs.HC", 
    "doi": "10.5121/ijcsit.2012.4208", 
    "link": "http://arxiv.org/pdf/1209.4982v1", 
    "title": "Using multimodal speech production data to evaluate articulatory   animation for audiovisual speech synthesis", 
    "arxiv-id": "1209.4982v1", 
    "author": "Slim Ouni", 
    "publish": "2012-09-22T10:36:11Z", 
    "summary": "The importance of modeling speech articulation for high-quality audiovisual\n(AV) speech synthesis is widely acknowledged. Nevertheless, while\nstate-of-the-art, data-driven approaches to facial animation can make use of\nsophisticated motion capture techniques, the animation of the intraoral\narticulators (viz. the tongue, jaw, and velum) typically makes use of simple\nrules or viseme morphing, in stark contrast to the otherwise high quality of\nfacial modeling. Using appropriate speech production data could significantly\nimprove the quality of articulatory animation for AV synthesis."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1209.6491v3", 
    "title": "Review of Statistical Shape Spaces for 3D Data with Comparative Analysis   for Human Faces", 
    "arxiv-id": "1209.6491v3", 
    "author": "Stefanie Wuhrer", 
    "publish": "2012-09-28T11:48:59Z", 
    "summary": "With systems for acquiring 3D surface data being evermore commonplace, it has\nbecome important to reliably extract specific shapes from the acquired data. In\nthe presence of noise and occlusions, this can be done through the use of\nstatistical shape models, which are learned from databases of clean examples of\nthe shape in question. In this paper, we review, analyze and compare different\nstatistical models: from those that analyze the variation in geometry globally\nto those that analyze the variation in geometry locally. We first review how\ndifferent types of models have been used in the literature, then proceed to\ndefine the models and analyze them theoretically, in terms of both their\nstatistical and computational aspects. We then perform extensive experimental\ncomparison on the task of model fitting, and give intuition about which type of\nmodel is better for a few applications. Due to the wide availability of\ndatabases of high-quality data, we use the human face as the specific shape we\nwish to extract from corrupted data."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1210.0026v1", 
    "title": "Coupled quasi-harmonic bases", 
    "arxiv-id": "1210.0026v1", 
    "author": "R. Kimmel", 
    "publish": "2012-09-28T20:29:37Z", 
    "summary": "The use of Laplacian eigenbases has been shown to be fruitful in many\ncomputer graphics applications. Today, state-of-the-art approaches to shape\nanalysis, synthesis, and correspondence rely on these natural harmonic bases\nthat allow using classical tools from harmonic analysis on manifolds. However,\nmany applications involving multiple shapes are obstacled by the fact that\nLaplacian eigenbases computed independently on different shapes are often\nincompatible with each other. In this paper, we propose the construction of\ncommon approximate eigenbases for multiple shapes using approximate joint\ndiagonalization algorithms. We illustrate the benefits of the proposed approach\non tasks from shape editing, pose transfer, correspondence, and similarity."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1210.1192v3", 
    "title": "Reduction of Blocking Artifacts In JPEG Compressed Image", 
    "arxiv-id": "1210.1192v3", 
    "author": "Sukhpal Singh", 
    "publish": "2012-10-03T18:53:36Z", 
    "summary": "In JPEG (DCT based) compresses image data by representing the original image\nwith a small number of transform coefficients. It exploits the fact that for\ntypical images a large amount of signal energy is concentrated in a small\nnumber of coefficients. The goal of DCT transform coding is to minimize the\nnumber of retained transform coefficients while keeping distortion at an\nacceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides\nan image into blocks of equal size and processes each block independently.\nBlock processing allows the coder to adapt to the local image statistics,\nexploit the correlation present among neighboring image pixels, and to reduce\ncomputational and storage requirements. One of the most degradation of the\nblock transform coding is the blocking artifact. These artifacts appear as a\nregular pattern of visible block boundaries. This degradation is a direct\nresult of the coarse quantization of the coefficients and the independent\nprocessing of the blocks which does not take into account the existing\ncorrelations among adjacent block pixels. In this paper attempt is being made\nto reduce the blocking artifact introduced by the Block DCT Transform in JPEG."
},{
    "category": "physics.flu-dyn", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1210.3325v2", 
    "title": "Vortices within vortices: hierarchical nature of vortex tubes in   turbulence", 
    "arxiv-id": "1210.3325v2", 
    "author": "Gregory L Eyink", 
    "publish": "2012-10-11T19:14:50Z", 
    "summary": "The JHU turbulence database [1] can be used with a state of the art\nvisualisation tool [2] to generate high quality fluid dynamics videos. In this\nwork we investigate the classical idea that smaller structures in turbulent\nflows, while engaged in their own internal dynamics, are advected by the larger\nstructures. They are not advected undistorted, however. We see instead that the\nsmall scale structures are sheared and twisted by the larger scales. This\nilluminates the basic mechanisms of the turbulent cascade."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1211.4500v1", 
    "title": "Dynamic Facial Expression of Emotion Made Easy", 
    "arxiv-id": "1211.4500v1", 
    "author": "Willem-Paul Brinkman", 
    "publish": "2012-11-19T17:10:50Z", 
    "summary": "Facial emotion expression for virtual characters is used in a wide variety of\nareas. Often, the primary reason to use emotion expression is not to study\nemotion expression generation per se, but to use emotion expression in an\napplication or research project. What is then needed is an easy to use and\nflexible, but also validated mechanism to do so. In this report we present such\na mechanism. It enables developers to build virtual characters with dynamic\naffective facial expressions. The mechanism is based on Facial Action Coding.\nIt is easy to implement, and code is available for download. To show the\nvalidity of the expressions generated with the mechanism we tested the\nrecognition accuracy for 6 basic emotions (joy, anger, sadness, surprise,\ndisgust, fear) and 4 blend emotions (enthusiastic, furious, frustrated, and\nevil). Additionally we investigated the effect of VC distance (z-coordinate),\nthe effect of the VC's face morphology (male vs. female), the effect of a\nlateral versus a frontal presentation of the expression, and the effect of\nintensity of the expression. Participants (n=19, Western and Asian subjects)\nrated the intensity of each expression for each condition (within subject\nsetup) in a non forced choice manner. All of the basic emotions were uniquely\nperceived as such. Further, the blends and confusion details of basic emotions\nare compatible with findings in psychology."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cviu.2014.05.005", 
    "link": "http://arxiv.org/pdf/1211.5556v1", 
    "title": "Improving Perceptual Color Difference using Basic Color Terms", 
    "arxiv-id": "1211.5556v1", 
    "author": "Michael Werman", 
    "publish": "2012-11-23T17:13:07Z", 
    "summary": "We suggest a new color distance based on two observations. First, perceptual\ncolor differences were designed to be used to compare very similar colors. They\ndo not capture human perception for medium and large color differences well.\nThresholding was proposed to solve the problem for large color differences,\ni.e. two totally different colors are always the same distance apart. We show\nthat thresholding alone cannot improve medium color differences. We suggest to\nalleviate this problem using basic color terms. Second, when a color distance\nis used for edge detection, many small distances around the just noticeable\ndifference may account for false edges. We suggest to reduce the effect of\nsmall distances."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1117/12.2008552", 
    "link": "http://arxiv.org/pdf/1212.3333v1", 
    "title": "Single-Pass GPU-Raycasting for Structured Adaptive Mesh Refinement Data", 
    "arxiv-id": "1212.3333v1", 
    "author": "Tom Abel", 
    "publish": "2012-12-13T21:00:02Z", 
    "summary": "Structured Adaptive Mesh Refinement (SAMR) is a popular numerical technique\nto study processes with high spatial and temporal dynamic range. It reduces\ncomputational requirements by adapting the lattice on which the underlying\ndifferential equations are solved to most efficiently represent the solution.\nParticularly in astrophysics and cosmology such simulations now can capture\nspatial scales ten orders of magnitude apart and more. The irregular locations\nand extensions of the refined regions in the SAMR scheme and the fact that\ndifferent resolution levels partially overlap, poses a challenge for GPU-based\ndirect volume rendering methods. kD-trees have proven to be advantageous to\nsubdivide the data domain into non-overlapping blocks of equally sized cells,\noptimal for the texture units of current graphics hardware, but previous\nGPU-supported raycasting approaches for SAMR data using this data structure\nrequired a separate rendering pass for each node, preventing the application of\nmany advanced lighting schemes that require simultaneous access to more than\none block of cells. In this paper we present a single-pass GPU-raycasting\nalgorithm for SAMR data that is based on a kD-tree. The tree is efficiently\nencoded by a set of 3D-textures, which allows to adaptively sample complete\nrays entirely on the GPU without any CPU interaction. We discuss two different\ndata storage strategies to access the grid data on the GPU and apply them to\nseveral datasets to prove the benefits of the proposed method."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2008552", 
    "link": "http://arxiv.org/pdf/1212.4490v1", 
    "title": "Sketch-to-Design: Context-based Part Assembly", 
    "arxiv-id": "1212.4490v1", 
    "author": "Baoquan Chen", 
    "publish": "2012-12-18T12:52:26Z", 
    "summary": "Designing 3D objects from scratch is difficult, especially when the user\nintent is fuzzy without a clear target form. In the spirit of\nmodeling-by-example, we facilitate design by providing reference and\ninspiration from existing model contexts. We rethink model design as navigating\nthrough different possible combinations of part assemblies based on a large\ncollection of pre-segmented 3D models. We propose an interactive\nsketch-to-design system, where the user sketches prominent features of parts to\ncombine. The sketched strokes are analyzed individually and in context with the\nother parts to generate relevant shape suggestions via a design gallery\ninterface. As the session progresses and more parts get selected, contextual\ncues becomes increasingly dominant and the system quickly converges to a final\ndesign. As a key enabler, we use pre-learned part-based contextual information\nto allow the user to quickly explore different combinations of parts. Our\nexperiments demonstrate the effectiveness of our approach for efficiently\ndesigning new variations from existing shapes."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1212.6048v1", 
    "title": "Discrete Surface Modeling Based on Google Earth: A Case Study", 
    "arxiv-id": "1212.6048v1", 
    "author": "Nengxiong Xu", 
    "publish": "2012-12-25T13:23:32Z", 
    "summary": "Google Earth (GE) has become a powerful tool for geological, geophysical and\ngeographical modeling; yet GE can be accepted to acquire elevation data of\nterrain. In this paper, we present a real study case of building the discrete\nsurface model (DSM) at Haut-Barr Castle in France based on the elevation data\nof terrain points extracted from GE using the COM API. We first locate the\nposition of Haut-Barr Castle and determine the region of the study area, then\nextract elevation data of terrain at Haut-Barr, and thirdly create a planar\ntriangular mesh that covers the study area and finally generate the desired DSM\nby calculating the elevation of vertices in the planar mesh via interpolating\nwith Universal Kriging (UK) and Inverse Distance Weighting (IDW). The generated\nDSM can reflect the features of the ground surface at Haut-Barr well, and can\nbe used for constructingthe Sealed Engineering Geological Model (SEGM) in\nfurther step."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1212.6923v1", 
    "title": "The Geant4 Visualisation System - a multi-driver graphics system", 
    "arxiv-id": "1212.6923v1", 
    "author": "Joseph Perl", 
    "publish": "2012-12-31T16:41:07Z", 
    "summary": "From the beginning the Geant4 Visualisation System was designed to support\nseveral simultaneous graphics systems written to common abstract interfaces.\nToday it has matured into a powerful diagnostic and presentational tool. It\ncomes with a library of models that may be added to the current scene and which\ninclude the representation of the Geant4 geometry hierarchy, simulated\ntrajectories and user-written hits and digitisations. The workhorse is the\nOpenGL suite of drivers for X, Xm, Qt and Win32. There is an Open Inventor\ndriver. Scenes can be exported in special graphics formats for offline viewing\nin the DAWN, VRML, HepRApp and gMocren browsers. PostScript can be generated\nthrough OpenGL, Open Inventor, DAWN and HepRApp. Geant4's own tracking\nalgorithms are used by the Ray Tracer. Not all drivers support all features but\nall drivers bring added functionality of some sort. This paper describes the\ninterfaces and details the individual drivers."
},{
    "category": "cs.CV", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1302.0439v2", 
    "title": "Correcting Camera Shake by Incremental Sparse Approximation", 
    "arxiv-id": "1302.0439v2", 
    "author": "Alfred O. Hero III", 
    "publish": "2013-02-03T00:46:11Z", 
    "summary": "The problem of deblurring an image when the blur kernel is unknown remains\nchallenging after decades of work. Recently there has been rapid progress on\ncorrecting irregular blur patterns caused by camera shake, but there is still\nmuch room for improvement. We propose a new blind deconvolution method using\nincremental sparse edge approximation to recover images blurred by camera\nshake. We estimate the blur kernel first from only the strongest edges in the\nimage, then gradually refine this estimate by allowing for weaker and weaker\nedges. Our method competes with the benchmark deblurring performance of the\nstate-of-the-art while being significantly faster and easier to generalize."
},{
    "category": "cs.AI", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1302.1547v1", 
    "title": "Perception, Attention, and Resources: A Decision-Theoretic Approach to   Graphics Rendering", 
    "arxiv-id": "1302.1547v1", 
    "author": "Jed Lengyel", 
    "publish": "2013-02-06T15:56:18Z", 
    "summary": "We describe work to control graphics rendering under limited computational\nresources by taking a decision-theoretic perspective on perceptual costs and\ncomputational savings of approximations. The work extends earlier work on the\ncontrol of rendering by introducing methods and models for computing the\nexpected cost associated with degradations of scene components. The expected\ncost is computed by considering the perceptual cost of degradations and a\nprobability distribution over the attentional focus of viewers. We review the\ncritical literature describing findings on visual search and attention, discuss\nthe implications of the findings, and introduce models of expected perceptual\ncost. Finally, we discuss policies that harness information about the expected\ncost of scene components."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1302.2024v1", 
    "title": "User Interface for Volume Rendering in Virtual Reality Environments", 
    "arxiv-id": "1302.2024v1", 
    "author": "Andreas Kolb", 
    "publish": "2013-02-08T13:11:39Z", 
    "summary": "Volume Rendering applications require sophisticated user interaction for the\ndefinition and refinement of transfer functions. Traditional 2D desktop user\ninterface elements have been developed to solve this task, but such concepts do\nnot map well to the interaction devices available in Virtual Reality\nenvironments.\n  In this paper, we propose an intuitive user interface for Volume Rendering\nspecifically designed for Virtual Reality environments. The proposed interface\nallows transfer function design and refinement based on intuitive two-handed\noperation of Wand-like controllers. Additional interaction modes such as\nnavigation and clip plane manipulation are supported as well.\n  The system is implemented using the Sony PlayStation Move controller system.\nThis choice is based on controller device capabilities as well as application\nand environment constraints.\n  Initial results document the potential of our approach."
},{
    "category": "cs.CG", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1302.5683v3", 
    "title": "STEVE - Space-Time-Enclosing Volume Extraction", 
    "arxiv-id": "1302.5683v3", 
    "author": "B. R. Schlei", 
    "publish": "2013-02-22T19:32:31Z", 
    "summary": "The novel STEVE (i.e., Space-Time-Enclosing Volume Extraction) algorithm is\ndescribed here for the very first time. It generates iso-valued hypersurfaces\nthat may be implicitly contained in four-dimensional (4D) data sets, such as\ntemporal sequences of three-dimensional images from time-varying computed\ntomography. Any final hypersurface that will be generated by STEVE is\nguaranteed to be free from accidental rifts, i.e., it always fully encloses a\nregion in the 4D space under consideration. Furthermore, the information of the\ninterior/exterior of the enclosed regions is propagated to each one of the\ntetrahedrons, which are embedded into 4D and which in their union represent the\nfinal, iso-valued hypersurface(s). STEVE is usually executed in a purely\ndata-driven mode, and it uses lesser computational resources than other\ntechniques that also generate simplex-based manifolds of codimension 1."
},{
    "category": "cs.GR", 
    "doi": "10.1109/ICCSNT.2012.6526125", 
    "link": "http://arxiv.org/pdf/1303.2824v1", 
    "title": "Fourth-order flows in surface modelling", 
    "arxiv-id": "1303.2824v1", 
    "author": "Ty Kang", 
    "publish": "2013-03-12T10:20:34Z", 
    "summary": "This short article is a brief account of the usage of fourth-order curvature\nflow in surface modelling."
},{
    "category": "physics.optics", 
    "doi": "10.1117/1.OE.53.2.024108", 
    "link": "http://arxiv.org/pdf/1308.0376v1", 
    "title": "Calculation reduction method for color computer-generated hologram using   color space conversion", 
    "arxiv-id": "1308.0376v1", 
    "author": "Tomoyoshi Ito", 
    "publish": "2013-08-01T23:09:28Z", 
    "summary": "We report a calculation reduction method for color computer-generated\nholograms (CGHs) using color space conversion. Color CGHs are generally\ncalculated on RGB space. In this paper, we calculate color CGHs in other color\nspaces: for example, YCbCr color space. In YCbCr color space, a RGB image is\nconverted to the luminance component (Y), blue-difference chroma (Cb) and\nred-difference chroma (Cr) components. In terms of the human eye, although the\nnegligible difference of the luminance component is well-recognized, the\ndifference of the other components is not. In this method, the luminance\ncomponent is normal sampled and the chroma components are down-sampled. The\ndown-sampling allows us to accelerate the calculation of the color CGHs. We\ncompute diffraction calculations from the components, and then we convert the\ndiffracted results in YCbCr color space to RGB color space."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1308.6804v2", 
    "title": "A Low-Dimensional Representation for Robust Partial Isometric   Correspondences Computation", 
    "arxiv-id": "1308.6804v2", 
    "author": "Tino Weinkauf", 
    "publish": "2013-08-30T17:38:40Z", 
    "summary": "Intrinsic isometric shape matching has become the standard approach for pose\ninvariant correspondence estimation among deformable shapes. Most existing\napproaches assume global consistency, i.e., the metric structure of the whole\nmanifold must not change significantly. While global isometric matching is well\nunderstood, only a few heuristic solutions are known for partial matching.\nPartial matching is particularly important for robustness to topological noise\n(incomplete data and contacts), which is a common problem in real-world 3D\nscanner data. In this paper, we introduce a new approach to partial, intrinsic\nisometric matching. Our method is based on the observation that isometries are\nfully determined by purely local information: a map of a single point and its\ntangent space fixes an isometry for both global and the partial maps. From this\nidea, we develop a new representation for partial isometric maps based on\nequivalence classes of correspondences between pairs of points and their\ntangent spaces. From this, we derive a local propagation algorithm that find\nsuch mappings efficiently. In contrast to previous heuristics based on RANSAC\nor expectation maximization, our method is based on a simple and sound\ntheoretical model and fully deterministic. We apply our approach to register\npartial point clouds and compare it to the state-of-the-art methods, where we\nobtain significant improvements over global methods for real-world data and\nstronger guarantees than previous heuristic partial matching algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1309.0192v2", 
    "title": "Reconstruction and uniqueness of moving obstacles", 
    "arxiv-id": "1309.0192v2", 
    "author": "Kamen M. Lozev", 
    "publish": "2013-09-01T08:48:17Z", 
    "summary": "We study the uniqueness and accuracy of the numerical solution of the problem\nof reconstruction of the shape and trajectory of a reflecting obstacle moving\nin an inhomogeneous medium from travel times, start and end points, and initial\nangles of ultrasonic rays reflecting at the obstacle. The speed of sound in the\ndomain when there is no obstacle present is known and provided as an input\nparameter which together with the other initial data enables the algorithm to\ntrace ray paths and find their reflection points. The reflection points\ndetermine with high-resolution the shape and trajectory of the obstacle. The\nmethod has predictable computational complexity and performance and is very\nefficient when it is parallelized and optimized because only a small portion of\nthe domain is reconstructed."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1309.4413v2", 
    "title": "Mobile augmented reality survey: a bottom-up approach", 
    "arxiv-id": "1309.4413v2", 
    "author": "Dimitris Chatzopoulos", 
    "publish": "2013-09-17T18:13:01Z", 
    "summary": "Augmented Reality (AR) is becoming mobile. Mobile devices have many\nconstraints but also rich new features that traditional desktop computers do\nnot have. There are several survey papers on AR, but none is dedicated to\nMobile Augmented Reality (MAR). Our work serves the purpose of closing this\ngap. The contents are organized with a bottom-up approach. We first present the\nstate-of-the-art in system components including hardware platforms, software\nframeworks and display devices, follows with enabling technologies such as\ntracking and data management. We then survey the latest technologies and\nmethods to improve run-time performance and energy efficiency for practical\nimplementation. On top of these, we further introduce the application fields\nand several typical MAR applications. Finally we conclude the survey with\nseveral challenge problems, which are under exploration and require great\nresearch efforts in the future."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1309.4747v2", 
    "title": "A constructive approach to triangular trigonometric patches", 
    "arxiv-id": "1309.4747v2", 
    "author": "Alexandru Krist\u00e1ly", 
    "publish": "2013-09-18T18:50:23Z", 
    "summary": "We construct a constrained trivariate extension of the univariate normalized\nB-basis of the vector space of trigonometric polynomials of arbitrary (finite)\norder n defined on any compact interval [0,\\alpha], where \\alpha is a fixed\n(shape) parameter in (0,\\pi). Our triangular extension is a normalized linearly\nindependent constrained trivariate trigonometric function system of dimension\n3n(n+1)+1 that spans the same vector space of functions as the constrained\ntrivariate extension of the canonical basis of truncated Fourier series of\norder n over [0,\\alpha]. Although the explicit general basis transformation is\nyet unknown, the coincidence of these vector spaces is proved by means of an\nappropriate equivalence relation. As a possible application of our triangular\nextension, we introduce the notion of (rational) triangular trigonometric\npatches of order n and of singularity free parametrization that could be used\nas control point based modeling tools in CAGD."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1311.5595v1", 
    "title": "On Nonrigid Shape Similarity and Correspondence", 
    "arxiv-id": "1311.5595v1", 
    "author": "Ron Kimmel", 
    "publish": "2013-11-18T22:08:02Z", 
    "summary": "An important operation in geometry processing is finding the correspondences\nbetween pairs of shapes. The Gromov-Hausdorff distance, a measure of\ndissimilarity between metric spaces, has been found to be highly useful for\nnonrigid shape comparison. Here, we explore the applicability of related shape\nsimilarity measures to the problem of shape correspondence, adopting spectral\ntype distances. We propose to evaluate the spectral kernel distance, the\nspectral embedding distance and the novel spectral quasi-conformal distance,\ncomparing the manifolds from different viewpoints. By matching the shapes in\nthe spectral domain, important attributes of surface structure are being\naligned. For the purpose of testing our ideas, we introduce a fully automatic\nframework for finding intrinsic correspondence between two shapes. The proposed\nmethod achieves state-of-the-art results on the Princeton isometric shape\nmatching protocol applied, as usual, to the TOSCA and SCAPE benchmarks."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1311.7194v1", 
    "title": "Real-time High Resolution Fusion of Depth Maps on GPU", 
    "arxiv-id": "1311.7194v1", 
    "author": "Dmitry Trifonov", 
    "publish": "2013-11-28T03:17:03Z", 
    "summary": "A system for live high quality surface reconstruction using a single moving\ndepth camera on a commodity hardware is presented. High accuracy and real-time\nframe rate is achieved by utilizing graphics hardware computing capabilities\nvia OpenCL and by using sparse data structure for volumetric surface\nrepresentation. Depth sensor pose is estimated by combining serial texture\nregistration algorithm with iterative closest points algorithm (ICP) aligning\nobtained depth map to the estimated scene model. Aligned surface is then fused\ninto the scene. Kalman filter is used to improve fusion quality. Truncated\nsigned distance function (TSDF) stored as block-based sparse buffer is used to\nrepresent surface. Use of sparse data structure greatly increases accuracy of\nscanned surfaces and maximum scanning area. Traditional GPU implementation of\nvolumetric rendering and fusion algorithms were modified to exploit sparsity to\nachieve desired performance. Incorporation of texture registration for sensor\npose estimation and Kalman filter for measurement integration improved accuracy\nand robustness of scanning process."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.gmod.2013.11.003", 
    "link": "http://arxiv.org/pdf/1402.1690v1", 
    "title": "Heliostat blocking and shadowing efficiency in the video-game era", 
    "arxiv-id": "1402.1690v1", 
    "author": "F. Ramos", 
    "publish": "2014-02-07T16:54:46Z", 
    "summary": "Blocking and shadowing is one of the key effects in designing and evaluating\na thermal central receiver solar tower plant. Therefore it is convenient to\ndevelop efficient algorithms to compute the area of an heliostat blocked or\nshadowed by the rest of the field. In this paper we explore the possibility of\nusing very efficient clipping algorithms developed for the video game and\nimaging industry to compute the blocking and shadowing efficiency of a solar\nthermal plant layout. We propose an algorithm valid for arbitrary position,\norientation and size of the heliostats. This algorithm turns out to be very\naccurate, free of assumptions and fast. We show the feasibility of the use of\nthis algorithm to the optimization of a solar plant by studying a couple of\nexamples in detail."
},{
    "category": "cs.CG", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.2190v1", 
    "title": "Surfaces Representation with Sharp Features Using Sqrt(3) and Loop   Subdivision Schemes", 
    "arxiv-id": "1402.2190v1", 
    "author": "Yasser M. Abd El-Latif", 
    "publish": "2014-02-10T15:47:13Z", 
    "summary": "This paper presents a hybrid algorithm that combines features form both\nSqrt(3) and Loop Subdivision schemes. The algorithm aims at preserving sharp\nfeatures and trim regions, during the surfaces subdivision, using a set of\nrules. The implementation is nontrivial due to the computational, topological,\nand smoothness constraints, which should be satisfied by the underlying\nsurface. The fundamental innovation, in this research work, is the ability to\npreserve sharp features anywhere on a surface. In addition, the resulting\nrepresentation remains within the multiresolution subdivision framework.\nPreserving the original representation has a core advantage that all the\napplicable operations to the multiresolution subdivision surfaces can\nsubsequently be applied to the edited model. Experimental results, including\nsurfaces coarsening and smoothing, were performed using the proposed algorithm\nfor validation purposes, and the results revealed that the proposed algorithm\noutperforms the other recent state of the art algorithms."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1402.2363v1", 
    "title": "Animation of 3D Human Model Using Markerless Motion Capture Applied To   Sports", 
    "arxiv-id": "1402.2363v1", 
    "author": "Archana Ghotkar", 
    "publish": "2014-02-11T04:05:12Z", 
    "summary": "Markerless motion capture is an active research in 3D virtualization. In\nproposed work we presented a system for markerless motion capture for 3D human\ncharacter animation, paper presents a survey on motion and skeleton tracking\ntechniques which are developed or are under development. The paper proposed a\nmethod to transform the motion of a performer to a 3D human character (model),\nthe 3D human character performs similar movements as that of a performer in\nreal time. In the proposed work, human model data will be captured by Kinect\ncamera, processed data will be applied on 3D human model for animation. 3D\nhuman model is created using open source software (MakeHuman). Anticipated\ndataset for sport activity is considered as input which can be applied to any\nHCI application."
},{
    "category": "cs.CG", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1404.0119v1", 
    "title": "A Computational Framework for Boundary Representation of Solid Sweeps", 
    "arxiv-id": "1404.0119v1", 
    "author": "Milind Sohoni", 
    "publish": "2014-04-01T03:44:22Z", 
    "summary": "This paper proposes a robust algorithmic and computational framework to\naddress the problem of modeling the volume obtained by sweeping a solid along a\ntrajectory of rigid motions. The boundary representation (simply brep) of the\ninput solid naturally induces a brep of the swept volume. We show that it is\nlocally similar to the input brep and this serves as the basis of the\nframework. All the same, it admits several intricacies: (i) geometric, in terms\nof parametrizations and, (ii) topological, in terms of orientations. We provide\na novel analysis for their resolution. More specifically, we prove a\nnon-trivial lifting theorem which allows to locally orient the output using the\norientation of the input. We illustrate the framework by providing many\nexamples from a pilot implementation."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1404.2728v2", 
    "title": "Real-time Decolorization using Dominant Colors", 
    "arxiv-id": "1404.2728v2", 
    "author": "Qian Du", 
    "publish": "2014-04-10T08:25:20Z", 
    "summary": "Decolorization is the process to convert a color image or video to its\ngrayscale version, and it has received great attention in recent years. An\nideal decolorization algorithm should preserve the original color contrast as\nmuch as possible. Meanwhile, it should provide the final decolorized result as\nfast as possible. However, most of the current methods are suffering from\neither unsatisfied color information preservation or high computational cost,\nlimiting their application value. In this paper, a simple but effective\ntechnique is proposed for real-time decolorization. Based on the typical\nrgb2gray() color conversion model, which produces a grayscale image by linearly\ncombining R, G, and B channels, we propose a dominant color hypothesis and a\ncorresponding distance measurement metric to evaluate the quality of grayscale\nconversion. The local optimum scheme provides several \"good\" candidates in a\nconfidence interval, from which the \"best\" result can be extracted.\nExperimental results demonstrate that remarkable simplicity of the proposed\nmethod facilitates the process of high resolution images and videos in\nreal-time using a common CPU."
},{
    "category": "math.NA", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1404.3767v1", 
    "title": "Control point based exact description of higher dimensional   trigonometric and hyperbolic curves and multivariate surfaces", 
    "arxiv-id": "1404.3767v1", 
    "author": "\u00c1goston R\u00f3th", 
    "publish": "2014-04-14T22:12:44Z", 
    "summary": "Using the normalized B-bases of vector spaces of trigonometric and hyperbolic\npolynomials of finite order, we specify control point configurations for the\nexact description of higher dimensional (rational) curves and (hybrid)\nmultivariate surfaces determined by coordinate functions that are exclusively\ngiven either by traditional trigonometric or hyperbolic polynomials in each of\ntheir variables. The usefulness and applicability of theoretical results and\nproposed algorithms are illustrated by many examples that also comprise the\ncontrol point based exact description of several famous curves (like epi- and\nhypocycloids, foliums, torus knots, Bernoulli's lemniscate, hyperbolas),\nsurfaces (such as pure trigonometric or hybrid surfaces of revolution like tori\nand hyperboloids, respectively) and 3-dimensional volumes. The core of the\nproposed modeling methods relies on basis transformation matrices with entries\nthat can be efficiently obtained by order elevation. Providing subdivision\nformulae for curves described by convex combinations of these normalized\nB-basis functions and control points, we also ensure the possible incorporation\nof all proposed techniques into today's CAD systems."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1408.6591v1", 
    "title": "Voronoi Grid-Shell Structures", 
    "arxiv-id": "1408.6591v1", 
    "author": "Paolo Cignoni", 
    "publish": "2014-08-26T14:23:57Z", 
    "summary": "We introduce a framework for the generation of grid-shell structures that is\nbased on Voronoi diagrams and allows us to design tessellations that achieve\nexcellent static performances. We start from an analysis of stress on the input\nsurface and we use the resulting tensor field to induce an anisotropic\nnon-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation\nunder the same metric. The resulting mesh is hex-dominant and made of cells\nwith a variable density, which depends on the amount of stress, and anisotropic\nshape, which depends on the direction of maximum stress. This mesh is further\noptimized taking into account symmetry and regularity of cells to improve\naesthetics. We demonstrate that our grid-shells achieve better static\nperformances with respect to quad-based grid shells, while offering an\ninnovative and aesthetically pleasing look."
},{
    "category": "math.NA", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1409.1714v5", 
    "title": "A level set based method for fixing overhangs in 3D printing", 
    "arxiv-id": "1409.1714v5", 
    "author": "Leonardo Rocchi", 
    "publish": "2014-09-05T10:00:35Z", 
    "summary": "3D printers based on the Fused Decomposition Modeling create objects\nlayer-by-layer dropping fused material. As a consequence, strong overhangs\ncannot be printed because the new-come material does not find a suitable\nsupport over the last deposed layer. In these cases, one can add some support\nstructures (scaffolds) which make the object printable, to be removed at the\nend. In this paper we propose a level set method to create object-dependent\nsupport structures, specifically conceived to reduce both the amount of\nadditional material and the printing time. We also review some open problems\nabout 3D printing which can be of interests for the mathematical community."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2014.4104", 
    "link": "http://arxiv.org/pdf/1409.5024v1", 
    "title": "Comparative Study of Geometric and Image Based Modelling and Rendering   Techniques", 
    "arxiv-id": "1409.5024v1", 
    "author": "Deepak Mishra", 
    "publish": "2014-09-01T00:46:26Z", 
    "summary": "This is a comparative study of the traditional 3D computer graphics technique\nof geometric modelling and image-based rendering techniques that were surveyed\nand implemented.We have discussed the classifications and representative\nmethods of both the techniques. The study has shown that there is a strong\ncontinuum between both the techniques and a hybrid of the two is most suitable\nfor further implementations.This hybridisation study is underway to create\nmodels of real life situations and provide disaster management training."
},{
    "category": "cs.GR", 
    "doi": "10.1214/14-STS477", 
    "link": "http://arxiv.org/pdf/1409.7256v1", 
    "title": "Reactive Programming for Interactive Graphics", 
    "arxiv-id": "1409.7256v1", 
    "author": "Xiaoyue Cheng", 
    "publish": "2014-09-09T10:39:46Z", 
    "summary": "One of the big challenges of developing interactive statistical applications\nis the management of the data pipeline, which controls transformations from\ndata to plot. The user's interactions needs to be propagated through these\nmodules and reflected in the output representation at a fast pace. Each\nindividual module may be easy to develop and manage, but the dependency\nstructure can be quite challenging. The MVC (Model/View/Controller) pattern is\nan attempt to solve the problem by separating the user's interaction from the\nrepresentation of the data. In this paper we discuss the paradigm of reactive\nprogramming in the framework of the MVC architecture and show its applicability\nto interactive graphics. Under this paradigm, developers benefit from the\nseparation of user interaction from the graphical representation, which makes\nit easier for users and developers to extend interactive applications. We show\nthe central role of reactive data objects in an interactive graphics system,\nimplemented as the R package cranvas, which is freely available on GitHub and\nthe main developers include the authors of this paper."
},{
    "category": "cs.GR", 
    "doi": "10.1214/14-STS477", 
    "link": "http://arxiv.org/pdf/1409.7724v1", 
    "title": "Using 3D Printing to Visualize Social Media Big Data", 
    "arxiv-id": "1409.7724v1", 
    "author": "Vijay Gadepally", 
    "publish": "2014-07-23T19:17:11Z", 
    "summary": "Big data volume continues to grow at unprecedented rates. One of the key\nfeatures that makes big data valuable is the promise to find unknown patterns\nor correlations that may be able to improve the quality of processes or\nsystems. Unfortunately, with the exponential growth in data, users often have\ndifficulty in visualizing the often-unstructured, non-homogeneous data coming\nfrom a variety of sources. The recent growth in popularity of 3D printing has\nushered in a revolutionary way to interact with big data. Using a 3D printed\nmockup up a physical or notional environment, one can display data on the\nmockup to show real-time data patterns. In this poster and demonstration, we\ndescribe the process of 3D printing and demonstrate an application of\ndisplaying Twitter data on a 3D mockup of the Massachusetts Institute of\nTechnology (MIT) campus, known as LuminoCity."
},{
    "category": "cs.CV", 
    "doi": "10.1214/14-STS477", 
    "link": "http://arxiv.org/pdf/1501.00108v1", 
    "title": "HSI based colour image equalization using iterative nth root and nth   power", 
    "arxiv-id": "1501.00108v1", 
    "author": "Gholamreza Anbarjafari", 
    "publish": "2014-12-31T10:53:50Z", 
    "summary": "In this paper an equalization technique for colour images is introduced. The\nmethod is based on nth root and nth power equalization approach but with\noptimization of the mean of the image in different colour channels such as RGB\nand HSI. The performance of the proposed method has been measured by the means\nof peak signal to noise ratio. The proposed algorithm has been compared with\nconventional histogram equalization and the visual and quantitative\nexperimental results are showing that the proposed method over perform the\nhistogram equalization."
},{
    "category": "cs.CG", 
    "doi": "10.1214/14-STS477", 
    "link": "http://arxiv.org/pdf/1501.04706v3", 
    "title": "A Novel Implementation of QuickHull Algorithm on the GPU", 
    "arxiv-id": "1501.04706v3", 
    "author": "Kunyang Zhao", 
    "publish": "2015-01-20T03:23:29Z", 
    "summary": "We present a novel GPU-accelerated implementation of the QuickHull algorihtm\nfor calculating convex hulls of planar point sets. We also describe a practical\nsolution to demonstrate how to efficiently implement a typical\nDivide-and-Conquer algorithm on the GPU. We highly utilize the parallel\nprimitives provided by the library Thrust such as the parallel segmented scan\nfor better efficiency and simplicity. To evaluate the performance of our\nimplementation, we carry out four groups of experimental tests using two groups\nof point sets in two modes on the GPU K20c. Experimental results indicate that:\nour implementation can achieve the speedups of up to 10.98x over the\nstate-of-art CPU-based convex hull implementation Qhull [16]. In addition, our\nimplementation can find the convex hull of 20M points in about 0.2 seconds."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s00138-012-0472-y", 
    "link": "http://arxiv.org/pdf/1109.1175v2", 
    "title": "Estimating 3D Human Shapes from Measurements", 
    "arxiv-id": "1109.1175v2", 
    "author": "Chang Shu", 
    "publish": "2011-09-06T13:22:49Z", 
    "summary": "The recent advances in 3-D imaging technologies give rise to databases of\nhuman shapes, from which statistical shape models can be built. These\nstatistical models represent prior knowledge of the human shape and enable us\nto solve shape reconstruction problems from partial information. Generating\nhuman shape from traditional anthropometric measurements is such a problem,\nsince these 1-D measurements encode 3-D shape information. Combined with a\nstatistical shape model, these easy-to-obtain measurements can be leveraged to\ncreate 3D human shapes. However, existing methods limit the creation of the\nshapes to the space spanned by the database and thus require a large amount of\ntraining data. In this paper, we introduce a technique that extrapolates the\nstatistically inferred shape to fit the measurement data using nonlinear\noptimization. This method ensures that the generated shape is both human-like\nand satisfies the measurement conditions. We demonstrate the effectiveness of\nthe method and compare it to existing approaches through extensive experiments,\nusing both synthetic data and real human measurements."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s00138-012-0472-y", 
    "link": "http://arxiv.org/pdf/1109.6288v1", 
    "title": "Using Stereoscopic 3D Technologies for the Diagnosis and Treatment of   Amblyopia in Children", 
    "arxiv-id": "1109.6288v1", 
    "author": "Angelo Gargantini", 
    "publish": "2011-09-28T18:35:51Z", 
    "summary": "The 3D4Amb project aims at developing a system based on the stereoscopic 3D\ntechonlogy, like the NVIDIA 3D Vision, for the diagnosis and treatment of\namblyopia in young children. It exploits the active shutter technology to\nprovide binocular vision, i.e. to show different images to the amblyotic (or\nlazy) and the normal eye. It would allow easy diagnosis of amblyopia and its\ntreatment by means of interactive games or other entertainment activities. It\nshould not suffer from the compliance problems of the classical treatment, it\nis suitable to domestic use, and it could at least partially substitute\nocclusion or patching of the normal eye."
},{
    "category": "stat.CO", 
    "doi": "10.1007/s10044-011-0249-3", 
    "link": "http://arxiv.org/pdf/1207.2378v1", 
    "title": "Parametric and Nonparametric Tests for Speckled Imagery", 
    "arxiv-id": "1207.2378v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-10T14:42:45Z", 
    "summary": "Synthetic aperture radar (SAR) has a pivotal role as a remote imaging method.\nObtained by means of coherent illumination, SAR images are contaminated with\nspeckle noise. The statistical modeling of such contamination is well described\naccording with the multiplicative model and its implied G0 distribution. The\nunderstanding of SAR imagery and scene element identification is an important\nobjective in the field. In particular, reliable image contrast tools are\nsought. Aiming the proposition of new tools for evaluating SAR image contrast,\nwe investigated new methods based on stochastic divergence. We propose several\ndivergence measures specifically tailored for G0 distributed data. We also\nintroduce a nonparametric approach based on the Kolmogorov-Smirnov distance for\nG0 data. We devised and assessed tests based on such measures, and their\nperformances were quantified according to their test sizes and powers. Using\nMonte Carlo simulation, we present a robustness analysis of test statistics and\nof maximum likelihood estimators for several degrees of innovative\ncontamination. It was identified that the proposed tests based on triangular\nand arithmetic-geometric measures outperformed the Kolmogorov-Smirnov\nmethodology."
},{
    "category": "stat.ML", 
    "doi": "10.1109/TGRS.2009.2025498", 
    "link": "http://arxiv.org/pdf/1207.2959v1", 
    "title": "Hypothesis Testing in Speckled Data with Stochastic Distances", 
    "arxiv-id": "1207.2959v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-12T13:45:41Z", 
    "summary": "Images obtained with coherent illumination, as is the case of sonar,\nultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected by\nspeckle noise which reduces the ability to extract information from the data.\nSpecialized techniques are required to deal with such imagery, which has been\nmodeled by the G0 distribution and under which regions with different degrees\nof roughness and mean brightness can be characterized by two parameters; a\nthird parameter, the number of looks, is related to the overall signal-to-noise\nratio. Assessing distances between samples is an important step in image\nanalysis; they provide grounds of the separability and, therefore, of the\nperformance of classification procedures. This work derives and compares eight\nstochastic distances and assesses the performance of hypothesis tests that\nemploy them and maximum likelihood estimation. We conclude that tests based on\nthe triangular distance have the closest empirical size to the theoretical one,\nwhile those based on the arithmetic-geometric distances have the best power.\nSince the power of tests based on the triangular distance is close to optimum,\nwe conclude that the safest choice is using this distance for hypothesis\ntesting, even when compared with classical distances as Kullback-Leibler and\nBhattacharyya."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-31401-8_12", 
    "link": "http://arxiv.org/pdf/1207.3351v1", 
    "title": "Combining Brain-Computer Interfaces and Haptics: Detecting Mental   Workload to Adapt Haptic Assistance", 
    "arxiv-id": "1207.3351v1", 
    "author": "Anatole L\u00e9cuyer", 
    "publish": "2012-07-13T13:13:27Z", 
    "summary": "In this paper we introduce the combined use of Brain-Computer Interfaces\n(BCI) and Haptic interfaces. We propose to adapt haptic guides based on the\nmental activity measured by a BCI system. This novel approach is illustrated\nwithin a proof-of-concept system: haptic guides are toggled during a\npath-following task thanks to a mental workload index provided by a BCI. The\naim of this system is to provide haptic assistance only when the user's brain\nactivity reflects a high mental workload. A user study conducted with 8\nparticipants shows that our proof-of-concept is operational and exploitable.\nResults show that activation of haptic guides occurs in the most difficult part\nof the path-following task. Moreover it allows to increase task performance by\n53% by activating assistance only 59% of the time. Taken together, these\nresults suggest that BCI could be used to determine when the user needs\nassistance during haptic interaction and to enable haptic guides accordingly."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-31401-8_12", 
    "link": "http://arxiv.org/pdf/1207.3921v1", 
    "title": "PlotXY: a high quality plotting system for the Herschel Interactive   Processing Environment (HIPE), and the astronomical community", 
    "arxiv-id": "1207.3921v1", 
    "author": "Emmanuel Caux", 
    "publish": "2012-07-17T09:06:59Z", 
    "summary": "The Herschel Interactive Processing Environment (HIPE) was developed by the\nEuropean Space Agency (ESA) in collaboration with NASA and the Herschel\nInstrument Control Centres to provide the astronomical community a complete\nenvironment to process and analyze the data gathered by the Herschel Space\nObservatory. One of the most important components of HIPE is the plotting\nsystem (named PlotXY) that we present here. With PlotXY it is possible to\nproduce easily high quality publication ready 2D plots. It provides a long list\nof features, with fully configurable components, and interactive zooming. The\nentire code of HIPE is written in Java and is open source released under the\nGNU Lesser General Public License version 3. A new version of PlotXY is being\ndeveloped to be independent from the HIPE code base; it is available to the\nsoftware development community for the inclusion in other projects at the URL\nhttp://code.google.com/p/jplot2d/."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-642-31401-8_12", 
    "link": "http://arxiv.org/pdf/1301.0289v1", 
    "title": "Reconstructing Self Organizing Maps as Spider Graphs for better visual   interpretation of large unstructured datasets", 
    "arxiv-id": "1301.0289v1", 
    "author": "Aaditya Prakash", 
    "publish": "2012-12-24T17:10:28Z", 
    "summary": "Self-Organizing Maps (SOM) are popular unsupervised artificial neural network\nused to reduce dimensions and visualize data. Visual interpretation from\nSelf-Organizing Maps (SOM) has been limited due to grid approach of data\nrepresentation, which makes inter-scenario analysis impossible. The paper\nproposes a new way to structure SOM. This model reconstructs SOM to show\nstrength between variables as the threads of a cobweb and illuminate\ninter-scenario analysis. While Radar Graphs are very crude representation of\nspider web, this model uses more lively and realistic cobweb representation to\ntake into account the difference in strength and length of threads. This model\nallows for visualization of highly unstructured dataset with large number of\ndimensions, common in Bigdata sources."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-642-31401-8_12", 
    "link": "http://arxiv.org/pdf/1301.1378v2", 
    "title": "Apollonian Circumcircles of IFS Fractals", 
    "arxiv-id": "1301.1378v2", 
    "author": "J\u00f3zsef Vass", 
    "publish": "2013-01-08T01:57:09Z", 
    "summary": "Euclidean triangles and IFS fractals seem to be disparate geometrical\nconcepts, unless we consider the Sierpi\\'{n}ski gasket, which is a self-similar\ncollection of triangles. The \"circumcircle\" hints at a direct link, as it can\nbe derived for three-map IFS fractals in general, defined in an Apollonian\nmanner. Following this path, one may discover a broader relationship between\npolygons and IFS fractals."
},{
    "category": "math.DS", 
    "doi": "10.1142/S0218348X14500145", 
    "link": "http://arxiv.org/pdf/1301.1379v2", 
    "title": "On Intersecting IFS Fractals with Lines", 
    "arxiv-id": "1301.1379v2", 
    "author": "J\u00f3zsef Vass", 
    "publish": "2013-01-08T01:57:24Z", 
    "summary": "IFS fractals - the attractors of Iterated Function Systems - have motivated\nplenty of research to date, partly due to their simplicity and applicability in\nvarious fields, such as the modeling of plants in computer graphics, and the\ndesign of fractal antennas. The statement and resolution of the Fractal-Line\nIntersection Problem is imperative for a more efficient treatment of certain\napplications. This paper intends to take further steps towards this resolution,\nbuilding on the literature. For the broad class of hyperdense fractals, a\nverifiable condition guaranteeing intersection with any line passing through\nthe convex hull of a planar IFS fractal is shown, in general R^d for\nhyperplanes. The condition also implies a constructive algorithm for finding\nthe points of intersection. Under certain conditions, an infinite number of\napproximate intersections are guaranteed, if there is at least one.\nQuantification of the intersection is done via an explicit formula for the\ninvariant measure of IFS."
},{
    "category": "cs.GR", 
    "doi": "10.1109/CSIT.2013.6588781", 
    "link": "http://arxiv.org/pdf/1301.3455v1", 
    "title": "3D Geological Modeling and Visualization of Rock Masses Based on Google   Earth: A Case Study", 
    "arxiv-id": "1301.3455v1", 
    "author": "Nengxiong Xu", 
    "publish": "2013-01-15T19:14:06Z", 
    "summary": "Google Earth (GE) has become a powerful tool for geological modeling and\nvisualization. An interesting and useful feature of GE, Google Street View, can\nallow the GE users to view geological structure such as layers of rock masses\nat a field site. In this paper, we introduce a practical solution for building\n3D geological models for rock masses based on the data acquired by use with GE.\nA real study case at Haut-Barr, France is presented to demonstrate our\nsolution. We first locate the position of Haut-Barr in GE, and then determine\nthe shape and scale of the rock masses in the study area, and thirdly acquire\nthe layout of layers of rock masses in the Google Street View, and finally\ncreate the approximate 3D geological models by extruding and intersecting. The\ngenerated 3D geological models can simply reflect the basic structure of the\nrock masses at Haut-Barr, and can be used for visualizing the rock bodies\ninteractively."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1088/1742-6596/454/1/012077", 
    "link": "http://arxiv.org/pdf/1301.4535v1", 
    "title": "Applications and a Three-dimensional Desktop Environment for an   Immersive Virtual Reality System", 
    "arxiv-id": "1301.4535v1", 
    "author": "Youhei Masada", 
    "publish": "2013-01-19T07:01:46Z", 
    "summary": "We developed an application launcher called Multiverse for scientific\nvisualizations in a CAVE-type virtual reality (VR) system. Multiverse can be\nregarded as a type of three-dimensional (3D) desktop environment. In\nMultiverse, a user in a CAVE room can browse multiple visualization\napplications with 3D icons and explore movies that float in the air. Touching\none of the movies causes \"teleportation\" into the application's VR space. After\nanalyzing the simulation data using the application, the user can jump back\ninto Multiverse's VR desktop environment in the CAVE."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2013.08.017", 
    "link": "http://arxiv.org/pdf/1301.4546v3", 
    "title": "An Approach to Exascale Visualization: Interactive Viewing of In-Situ   Visualization", 
    "arxiv-id": "1301.4546v3", 
    "author": "Tomoki Yamada", 
    "publish": "2013-01-19T08:39:58Z", 
    "summary": "In the coming era of exascale supercomputing, in-situ visualization will be a\ncrucial approach for reducing the output data size. A problem of in-situ\nvisualization is that it loses interactivity if a steering method is not\nadopted. In this paper, we propose a new method for the interactive analysis of\nin-situ visualization images produced by a batch simulation job. A key idea is\nto apply numerous (thousands to millions) in-situ visualizations\nsimultaneously. The viewer then analyzes the image database interactively\nduring postprocessing. If each movie can be compressed to 100 MB, one million\nmovies will only require 100 TB, which is smaller than the size of the raw\nnumerical data in exascale supercomputing. We performed a feasibility study\nusing the proposed method. Multiple movie files were produced by a simulation\nand they were analyzed using a specially designed movie player. The user could\nchange the viewing angle, the visualization method, and the parameters\ninteractively by retrieving an appropriate sequence of images from the movie\ndataset."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cpc.2013.08.017", 
    "link": "http://arxiv.org/pdf/1301.6007v1", 
    "title": "Immersive VR Visualizations by VFIVE. Part 1: Development", 
    "arxiv-id": "1301.6007v1", 
    "author": "Nobuaki Ohno", 
    "publish": "2013-01-25T11:03:18Z", 
    "summary": "We have been developing a visualization application for CAVE-type virtual\nreality (VR) systems for more than a decade. This application, VFIVE, is\ncurrently used in several CAVE systems in Japan for routine visualizations. It\nis also used as a base system of further developments of advanced\nvisualizations. The development of VFIVE is summarized."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cpc.2013.08.017", 
    "link": "http://arxiv.org/pdf/1301.6008v1", 
    "title": "Immersive VR Visualizations by VFIVE. Part 2: Applications", 
    "arxiv-id": "1301.6008v1", 
    "author": "Hiroaki Ohtani", 
    "publish": "2013-01-25T11:07:37Z", 
    "summary": "VFIVE is a scientific visualization application for CAVE-type immersive\nvirtual reality systems. The source codes are freely available. VFIVE is used\nas a research tool in various VR systems. It also lays the groundwork for\ndevelopments of new visualization software for CAVEs. In this paper, we pick up\nfive CAVE systems in four different institutions in Japan. Applications of\nVFIVE in each CAVE system are summarized. Special emphases will be placed on\nscientific and technical achievements made possible by VFIVE."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cpc.2013.08.017", 
    "link": "http://arxiv.org/pdf/1304.4974v1", 
    "title": "Fast exact digital differential analyzer for circle generation", 
    "arxiv-id": "1304.4974v1", 
    "author": "Leonid V. Moroz", 
    "publish": "2013-04-17T21:32:37Z", 
    "summary": "In the first part of the paper we present a short review of applications of\ndigital differential analyzers (DDA) to generation of circles showing that they\ncan be treated as one-step numerical schemes. In the second part we present and\ndiscuss a novel fast algorithm based on a two-step numerical scheme (explicit\nmidpoint rule). Although our algorithm is as cheap as the simplest one-step DDA\nalgoritm (and can be represented in terms of shifts and additions), it\ngenerates circles with maximal accuracy, i.e., it is exact up to round-off\nerrors."
},{
    "category": "cs.CY", 
    "doi": "10.1016/j.cpc.2013.08.017", 
    "link": "http://arxiv.org/pdf/1305.2276v1", 
    "title": "The effects of computer assisted and distance learning of geometric   modelling", 
    "arxiv-id": "1305.2276v1", 
    "author": "Ismail Ipek", 
    "publish": "2013-05-10T07:58:06Z", 
    "summary": "The effects of computer-assisted and distance learning of geometric modeling\nand computer aided geometric design are studied. It was shown that computer\nalgebra systems and dynamic geometric environments can be considered as\nexcellent tools for teaching mathematical concepts of mentioned areas, and\ndistance education technologies would be indispensable for consolidation of\nsuccessfully passed topics."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1086/671412", 
    "link": "http://arxiv.org/pdf/1306.3481v1", 
    "title": "Visualizing Astronomical Data with Blender", 
    "arxiv-id": "1306.3481v1", 
    "author": "Brian R. Kent", 
    "publish": "2013-06-14T18:48:09Z", 
    "summary": "Astronomical data take on a multitude of forms -- catalogs, data cubes,\nimages, and simulations. The availability of software for rendering\nhigh-quality three-dimensional graphics lends itself to the paradigm of\nexploring the incredible parameter space afforded by the astronomical sciences.\nThe software program Blender gives astronomers a useful tool for displaying\ndata in a manner used by three-dimensional (3D) graphics specialists and\nanimators. The interface to this popular software package is introduced with\nattention to features of interest in astronomy. An overview of the steps for\ngenerating models, textures, animations, camera work, and renders is outlined.\nAn introduction is presented on the methodology for producing animations and\ngraphics with a variety of astronomical data. Examples from sub-fields of\nastronomy with different kinds of data are shown with resources provided to\nmembers of the astronomical community. An example video showcasing the outlined\nprinciples and features is provided along with scripts and files for sample\nvisualizations."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.gmod.2014.10.002", 
    "link": "http://arxiv.org/pdf/1306.4478v3", 
    "title": "Finite Element Based Tracking of Deforming Surfaces", 
    "arxiv-id": "1306.4478v3", 
    "author": "Chang Shu", 
    "publish": "2013-06-19T10:25:24Z", 
    "summary": "We present an approach to robustly track the geometry of an object that\ndeforms over time from a set of input point clouds captured from a single\nviewpoint. The deformations we consider are caused by applying forces to known\nlocations on the object's surface. Our method combines the use of prior\ninformation on the geometry of the object modeled by a smooth template and the\nuse of a linear finite element method to predict the deformation. This allows\nthe accurate reconstruction of both the observed and the unobserved sides of\nthe object. We present tracking results for noisy low-quality point clouds\nacquired by either a stereo camera or a depth camera, and simulations with\npoint clouds corrupted by different error terms. We show that our method is\nalso applicable to large non-linear deformations."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.gmod.2014.10.002", 
    "link": "http://arxiv.org/pdf/1307.1739v2", 
    "title": "Anatomical Feature-guided Volumeric Registration of Multimodal Prostate   MRI", 
    "arxiv-id": "1307.1739v2", 
    "author": "Arie Kaufman", 
    "publish": "2013-07-06T00:30:40Z", 
    "summary": "Radiological imaging of prostate is becoming more popular among researchers\nand clinicians in searching for diseases, primarily cancer. Scans might be\nacquired at different times, with patient movement between scans, or with\ndifferent equipment, resulting in multiple datasets that need to be registered.\nFor this issue, we introduce a registration method using anatomical\nfeature-guided mutual information. Prostate scans of the same patient taken in\nthree different orientations are first aligned for the accurate detection of\nanatomical features in 3D. Then, our pipeline allows for multiple modalities\nregistration through the use of anatomical features, such as the interior\nurethra of prostate and gland utricle, in a bijective way. The novelty of this\napproach is the application of anatomical features as the pre-specified\ncorresponding landmarks for prostate registration. We evaluate the registration\nresults through both artificial and clinical datasets. Registration accuracy is\nevaluated by performing statistical analysis of local intensity differences or\nspatial differences of anatomical landmarks between various MR datasets.\nEvaluation results demonstrate that our method statistics-significantly\nimproves the quality of registration. Although this strategy is tested for\nMRI-guided brachytherapy, the preliminary results from these experiments\nsuggest that it can be also applied to other settings such as transrectal\nultrasound-guided or CT-guided therapy, where the integration of preoperative\nMRI may have a significant impact upon treatment planning and guidance."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.gmod.2014.10.002", 
    "link": "http://arxiv.org/pdf/1307.2457v1", 
    "title": "Detection of Outer Rotations on 3D-Vector Fields with Iterative   Geometric Correlation and its Efficiency", 
    "arxiv-id": "1307.2457v1", 
    "author": "Eckhard Hitzer", 
    "publish": "2013-06-10T14:18:38Z", 
    "summary": "Correlation is a common technique for the detection of shifts. Its\ngeneralization to the multidimensional geometric correlation in Clifford\nalgebras has been proven a useful tool for color image processing, because it\nadditionally contains information about a rotational misalignment. But so far\nthe exact correction of a three-dimensional outer rotation could only be\nachieved in certain special cases. In this paper we prove that applying the\ngeometric correlation iteratively has the potential to detect the outer\nrotational misalignment for arbitrary three-dimensional vector fields. We\nfurther present the explicit iterative algorithm, analyze its efficiency\ndetecting the rotational misalignment in the color space of a color image. The\nexperiments suggest a method for the acceleration of the algorithm, which is\npractically tested with great success."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11760-014-0691-y", 
    "link": "http://arxiv.org/pdf/1307.3581v2", 
    "title": "Image color transfer to evoke different emotions based on color   combinations", 
    "arxiv-id": "1307.3581v2", 
    "author": "Russell Zaretzki", 
    "publish": "2013-07-12T21:20:49Z", 
    "summary": "In this paper, a color transfer framework to evoke different emotions for\nimages based on color combinations is proposed. The purpose of this color\ntransfer is to change the \"look and feel\" of images, i.e., evoking different\nemotions. Colors are confirmed as the most attractive factor in images. In\naddition, various studies in both art and science areas have concluded that\nother than single color, color combinations are necessary to evoke specific\nemotions. Therefore, we propose a novel framework to transfer color of images\nbased on color combinations, using a predefined color emotion model. The\ncontribution of this new framework is three-fold. First, users do not need to\nprovide reference images as used in traditional color transfer algorithms. In\nmost situations, users may not have enough aesthetic knowledge or path to\nchoose desired reference images. Second, because of the usage of color\ncombinations instead of single color for emotions, a new color transfer\nalgorithm that does not require an image library is proposed. Third, again\nbecause of the usage of color combinations, artifacts that are normally seen in\ntraditional frameworks using single color are avoided. We present encouraging\nresults generated from this new framework and its potential in several possible\napplications including color transfer of photos and paintings."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11760-014-0691-y", 
    "link": "http://arxiv.org/pdf/1307.6360v1", 
    "title": "Electronic Visualisation in Chemistry: From Alchemy to Art", 
    "arxiv-id": "1307.6360v1", 
    "author": "Alice M. Bowen", 
    "publish": "2013-07-24T09:54:22Z", 
    "summary": "Chemists now routinely use software as part of their work. For example,\nvirtual chemistry allows chemical reactions to be simulated. In particular, a\nselection of software is available for the visualisation of complex\n3-dimensional molecular structures. Many of these are very beautiful in their\nown right. As well as being included as illustrations in academic papers, such\nvisualisations are often used on the covers of chemistry journals as\nartistically decorative and attractive motifs. Chemical images have also been\nused as the basis of artworks in exhibitions. This paper explores the\ndevelopment of the relationship of chemistry, art, and IT. It covers some of\nthe increasingly sophisticated software used to generate these projections\n(e.g., UCSF Chimera) and their progressive use as a visual art form."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s11760-014-0691-y", 
    "link": "http://arxiv.org/pdf/1310.2994v2", 
    "title": "Depth-dependent Parallel Visualization with 3D Stylized Dense Tubes", 
    "arxiv-id": "1310.2994v2", 
    "author": "Alexander P. Auchus", 
    "publish": "2013-10-11T00:38:49Z", 
    "summary": "We present a parallel visualization algorithm for the illustrative rendering\nof depth-dependent stylized dense tube data at interactive frame rates. While\nthis computation could be efficiently performed on a GPU device, we target a\nparallel framework to enable it to be efficiently running on an ordinary\nmulti-core CPU platform which is much more available than GPUs for common\nusers. Our approach is to map the depth information in each tube onto each of\nthe visual dimensions of shape, color, texture, value, and size on the basis of\nBertin's semiology theory. The purpose is to enable more legible displays in\nthe dense tube environments. A major contribution of our work is an efficient\nand effective parallel depthordering algorithm that makes use of the message\npassing interface (MPI) with VTK. We evaluated our framework with\nvisualizations of depth-stylized tubes derived from 3D diffusion tensor MRI\ndata by comparing its efficiency with several other alternative parallelization\nplatforms running the same computations. As our results show, the\nparallelization framework we proposed can efficiently render highly dense 3D\ndata sets like the tube data and thus is useful as a complement to parallel\nvisualization environments that rely on GPUs."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2682628", 
    "link": "http://arxiv.org/pdf/1310.4389v2", 
    "title": "ImageSpirit: Verbal Guided Image Parsing", 
    "arxiv-id": "1310.4389v2", 
    "author": "Philip Torr", 
    "publish": "2013-10-16T14:16:31Z", 
    "summary": "Humans describe images in terms of nouns and adjectives while algorithms\noperate on images represented as sets of pixels. Bridging this gap between how\nhumans would like to access images versus their typical representation is the\ngoal of image parsing, which involves assigning object and attribute labels to\npixel. In this paper we propose treating nouns as object labels and adjectives\nas visual attribute labels. This allows us to formulate the image parsing\nproblem as one of jointly estimating per-pixel object and attribute labels from\na set of training images. We propose an efficient (interactive time) solution.\nUsing the extracted labels as handles, our system empowers a user to verbally\nrefine the results. This enables hands-free parsing of an image into pixel-wise\nobject/attribute labels that correspond to human semantics. Verbally selecting\nobjects of interests enables a novel and natural interaction modality that can\npossibly be used to interact with new generation devices (e.g. smart phones,\nGoogle Glass, living room devices). We demonstrate our system on a large number\nof real-world images with varying complexity. To help understand the tradeoffs\ncompared to traditional mouse based interactions, results are reported for both\na large scale quantitative evaluation and a user study."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2682628", 
    "link": "http://arxiv.org/pdf/1312.1824v1", 
    "title": "Introduction to computer animation and its possible educational   applications", 
    "arxiv-id": "1312.1824v1", 
    "author": "Carol Griffiths", 
    "publish": "2013-12-06T10:29:50Z", 
    "summary": "Animation, which is basically a form of pictorial presentation, has become\nthe most prominent feature of technology-based learning environments. It refers\nto simulated motion pictures showing movement of drawn objects. Recently,\neducational computer animation has turned out to be one of the most elegant\ntools for presenting multimedia materials for learners, and its significance in\nhelping to understand and remember information has greatly increased since the\nadvent of powerful graphics-oriented computers. In this book chapter we\nintroduce and discuss the history of computer animation, its well-known\nfundamental principles and some educational applications. It is however still\ndebatable if truly educational computer animations help in learning, as the\nresearch on whether animation aids learners' understanding of dynamic phenomena\nhas come up with positive, negative and neutral results. We have tried to\nprovide as much detailed information on computer animation as we could, and we\nhope that this book chapter will be useful for students who study computer\nscience, computer-assisted education or some other courses connected with\ncontemporary education, as well as researchers who conduct their research in\nthe field of computer animation."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cviu.2014.06.012", 
    "link": "http://arxiv.org/pdf/1312.4967v2", 
    "title": "Estimation of Human Body Shape and Posture Under Clothing", 
    "arxiv-id": "1312.4967v2", 
    "author": "Jochen Lang", 
    "publish": "2013-12-17T21:01:05Z", 
    "summary": "Estimating the body shape and posture of a dressed human subject in motion\nrepresented as a sequence of (possibly incomplete) 3D meshes is important for\nvirtual change rooms and security. To solve this problem, statistical shape\nspaces encoding human body shape and posture variations are commonly used to\nconstrain the search space for the shape estimate. In this work, we propose a\nnovel method that uses a posture-invariant shape space to model body shape\nvariation combined with a skeleton-based deformation to model posture\nvariation. Our method can estimate the body shape and posture of both static\nscans and motion sequences of dressed human body scans. In case of motion\nsequences, our method takes advantage of motion cues to solve for a single body\nshape estimate along with a sequence of posture estimates. We apply our\napproach to both static scans and motion sequences and demonstrate that using\nour method, higher fitting accuracy is achieved than when using a variant of\nthe popular SCAPE model as statistical model."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cviu.2014.06.012", 
    "link": "http://arxiv.org/pdf/1312.6935v1", 
    "title": "Application of polynomial texture mapping in process of digitalization   of cultural heritage", 
    "arxiv-id": "1312.6935v1", 
    "author": "Milica Makragic", 
    "publish": "2013-12-25T06:56:35Z", 
    "summary": "In this paper we present modern texture mapping techniques and several\napplications of polynomial texture mapping in cultural heritage programs. We\nalso consider some well-known and some new methods for mathematical procedure\nthat is involved in generation of polynomial texture maps."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1312.7034v2", 
    "title": "A Topologically-informed Hyperstreamline Seeding Method for Alignment   Tensor Fields", 
    "arxiv-id": "1312.7034v2", 
    "author": "Nasser Mohieddin Abukhdeir", 
    "publish": "2013-12-26T00:22:31Z", 
    "summary": "A topologically-informed method is presented for seeding of hyperstreamlines\nfor visualization of alignment tensor fields. The method is inspired by and\napplied to visualization of nematic liquid crystal (LC) reorientation dynamics\nsimulations. The method distributes hyperstreamlines along domain boundaries\nand edges of a nearest-neighbor graph whose vertices are degenerate regions of\nthe alignment tensor field, which correspond to orientational defects in a\nnematic LC domain. This is accomplished without iteration while conforming to a\nuser-specified spacing between hyperstreamlines and avoids possible failure\nmodes associated with hyperstreamline integration in the vicinity of\ndegeneracies of alignment (orientational defects). It is shown that the\npresented seeding method enables automated hyperstreamline-based visualization\nof a broad range of alignment tensor fields which enhances the ability of\nresearchers to interpret these fields and provides an alternative to using\nglyph-based techniques."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1401.0608v1", 
    "title": "A Framework for Creating a Distributed Rendering Environment on the   Compute Clusters", 
    "arxiv-id": "1401.0608v1", 
    "author": "Othmane Bouhali", 
    "publish": "2014-01-03T08:48:25Z", 
    "summary": "This paper discusses the deployment of existing render farm manager in a\ntypical compute cluster environment such as a university. Usually, both a\nrender farm and a compute cluster use different queue managers and assume total\ncontrol over the physical resources. But, taking out the physical resources\nfrom an existing compute cluster in a university-like environment whose primary\nuse of the cluster is to run numerical simulations may not be possible. It can\npotentially reduce the overall resource utilization in a situation where\ncompute tasks are more than rendering tasks. Moreover, it can increase the\nsystem administration cost. In this paper, a framework has been proposed that\ncreates a dynamic distributed rendering environment on top of the compute\nclusters using existing render farm managers without requiring the physical\nseparation of the resources."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1401.2818v2", 
    "title": "Multilinear Wavelets: A Statistical Shape Space for Human Faces", 
    "arxiv-id": "1401.2818v2", 
    "author": "Stefanie Wuhrer", 
    "publish": "2014-01-13T12:48:39Z", 
    "summary": "We present a statistical model for $3$D human faces in varying expression,\nwhich decomposes the surface of the face using a wavelet transform, and learns\nmany localized, decorrelated multilinear models on the resulting coefficients.\nUsing this model we are able to reconstruct faces from noisy and occluded $3$D\nface scans, and facial motion sequences. Accurate reconstruction of face shape\nis important for applications such as tele-presence and gaming. The localized\nand multi-scale nature of our model allows for recovery of fine-scale detail\nwhile retaining robustness to severe noise and occlusion, and is\ncomputationally efficient and scalable. We validate these properties\nexperimentally on challenging data in the form of static scans and motion\nsequences. We show that in comparison to a global multilinear model, our model\nbetter preserves fine detail and is computationally faster, while in comparison\nto a localized PCA model, our model better handles variation in expression, is\nfaster, and allows us to fix identity parameters for a given subject."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1403.0087v1", 
    "title": "Temporal Image Fusion", 
    "arxiv-id": "1403.0087v1", 
    "author": "Francisco J. Estrada", 
    "publish": "2014-03-01T14:08:22Z", 
    "summary": "This paper introduces temporal image fusion. The proposed technique builds\nupon previous research in exposure fusion and expands it to deal with the\nlimited Temporal Dynamic Range of existing sensors and camera technologies. In\nparticular, temporal image fusion enables the rendering of long-exposure\neffects on full frame-rate video, as well as the generation of arbitrarily long\nexposures from a sequence of images of the same scene taken over time. We\nexplore the problem of temporal under-exposure, and show how it can be\naddressed by selectively enhancing dynamic structure. Finally, we show that the\nuse of temporal image fusion together with content-selective image filters can\nproduce a range of striking visual effects on a given input sequence."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1403.0917v1", 
    "title": "An Extension Of Weiler-Atherton Algorithm To Cope With The   Self-intersecting Polygon", 
    "arxiv-id": "1403.0917v1", 
    "author": "Anurag Chakraborty", 
    "publish": "2014-03-04T19:50:00Z", 
    "summary": "In this paper a new algorithm has been proposed which can fix the problem of\nWeiler Atherton algorithm. The problem of Weiler Atherton algorithm lies in\nclipping self intersecting polygon. Clipping self intersecting polygon is not\nconsidered in Weiler Atherton algorithm and hence it is also a main\ndisadvantage of this algorithm. In our new algorithm a self intersecting\npolygon has been divided into non self intersecting contours and then perform\nthe Weiler Atherton clipping algorithm on those sub polygons. For holes we have\nto store the edges that is not own boundary of hole contour from recently\nclipped polygon. Thus if both contour is hole then we have to store all the\nedges of the recently clipped polygon. Finally the resultant polygon has been\nproduced by eliminating all the stored edges."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1403.6566v2", 
    "title": "Image Retargeting by Content-Aware Synthesis", 
    "arxiv-id": "1403.6566v2", 
    "author": "Xiaopeng Zhang", 
    "publish": "2014-03-26T03:29:25Z", 
    "summary": "Real-world images usually contain vivid contents and rich textural details,\nwhich will complicate the manipulation on them. In this paper, we design a new\nframework based on content-aware synthesis to enhance content-aware image\nretargeting. By detecting the textural regions in an image, the textural image\ncontent can be synthesized rather than simply distorted or cropped. This method\nenables the manipulation of textural & non-textural regions with different\nstrategy since they have different natures. We propose to retarget the textural\nregions by content-aware synthesis and non-textural regions by fast\nmulti-operators. To achieve practical retargeting applications for general\nimages, we develop an automatic and fast texture detection method that can\ndetect multiple disjoint textural regions. We adjust the saliency of the image\naccording to the features of the textural regions. To validate the proposed\nmethod, comparisons with state-of-the-art image targeting techniques and a user\nstudy were conducted. Convincing visual results are shown to demonstrate the\neffectiveness of the proposed method."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1403.7987v1", 
    "title": "Implementation of interaction between soft tissues and foreign bodies   using modified voxel model", 
    "arxiv-id": "1403.7987v1", 
    "author": "Sergei Nikolaev", 
    "publish": "2014-03-31T13:20:53Z", 
    "summary": "Interactive bodies collision detection and elimination is one of the most\npopular task nowadays. Collisions can be detected in different ways. Collision\nsearch using space voxelization is one of the most fast. This paper describes\nimproved voxel model that covers only area of collision interest and quickly\neliminates collisions. This new method can be useful in real time collision\nprocessing of different rigid and soft bodies grids."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1405.1902v1", 
    "title": "Proofs of two Theorems concerning Sparse Spacetime Constraints", 
    "arxiv-id": "1405.1902v1", 
    "author": "Klaus Hildebrandt", 
    "publish": "2014-05-08T12:15:59Z", 
    "summary": "In the SIGGRAPH 2014 paper [SvTSH14] an approach for animating deformable\nobjects using sparse spacetime constraints is introduced. This report contains\nthe proofs of two theorems presented in the paper."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1405.3352v2", 
    "title": "Newton-Type Iterative Solver for Multiple View $L2$ Triangulation", 
    "arxiv-id": "1405.3352v2", 
    "author": "Z. Chen", 
    "publish": "2014-05-14T03:35:56Z", 
    "summary": "In this note, we show that the L2 optimal solutions to most real multiple\nview L2 triangulation problems can be efficiently obtained by two-stage\nNewton-like iterative methods, while the difficulty of such problems mainly\nlies in how to verify the L2 optimality. Such a working two-stage bundle\nadjustment approach features: first, the algorithm is initialized by symmedian\npoint triangulation, a multiple-view generalization of the mid-point method;\nsecond, a symbolic-numeric method is employed to compute derivatives\naccurately; third, globalizing strategy such as line search or trust region is\nsmoothly applied to the underlying iteration which assures algorithm robustness\nin general cases.\n  Numerical comparison with tfml method shows that the local minimizers\nobtained by the two-stage iterative bundle adjustment approach proposed here\nare also the L2 optimal solutions to all the calibrated data sets available\nonline by the Oxford visual geometry group. Extensive numerical experiments\nindicate the bundle adjustment approach solves more than 99% the real\ntriangulation problems optimally. An IEEE 754 double precision C++\nimplementation shows that it takes only about 0.205 second tocompute allthe\n4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybrid\nwith a line search strategy on a computer with a 3.4GHz Intel i7 CPU."
},{
    "category": "math.NA", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1407.1303v3", 
    "title": "A Cylindrical Basis Function for Solving Partial Differential Equations   on Manifolds", 
    "arxiv-id": "1407.1303v3", 
    "author": "Zeyun Yu", 
    "publish": "2014-07-04T19:30:43Z", 
    "summary": "Numerical solutions of partial differential equations (PDEs) on manifolds\ncontinues to generate a lot of interest among scientists in the natural and\napplied sciences. On the other hand, recent developments of 3D scanning and\ncomputer vision technologies have produced a large number of 3D surface models\nrepresented as point clouds. Herein, we develop a simple and efficient method\nfor solving PDEs on closed surfaces represented as point clouds. By projecting\nthe radial vector of standard radial basis function(RBF) kernels onto the local\ntangent plane, we are able to produce a representation of functions that\npermits the replacement of surface differential operators with their Cartesian\nequivalent. We demonstrate, numerically, the efficiency of the method in\ndiscretizing the Laplace Beltrami operator."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1407.2110v1", 
    "title": "Addressing the unmet need for visualizing Conditional Random Fields in   Biological Data", 
    "arxiv-id": "1407.2110v1", 
    "author": "Christopher W. Bartlett", 
    "publish": "2014-07-08T14:34:14Z", 
    "summary": "Background: The biological world is replete with phenomena that appear to be\nideally modeled and analyzed by one archetypal statistical framework - the\nGraphical Probabilistic Model (GPM). The structure of GPMs is a uniquely good\nmatch for biological problems that range from aligning sequences to modeling\nthe genome-to-phenome relationship. The fundamental questions that GPMs address\ninvolve making decisions based on a complex web of interacting factors.\nUnfortunately, while GPMs ideally fit many questions in biology, they are not\nan easy solution to apply. Building a GPM is not a simple task for an end user.\nMoreover, applying GPMs is also impeded by the insidious fact that the complex\nweb of interacting factors inherent to a problem might be easy to define and\nalso intractable to compute upon. Discussion: We propose that the visualization\nsciences can contribute to many domains of the bio-sciences, by developing\ntools to address archetypal representation and user interaction issues in GPMs,\nand in particular a variety of GPM called a Conditional Random Field(CRF). CRFs\nbring additional power, and additional complexity, because the CRF dependency\nnetwork can be conditioned on the query data. Conclusions: In this manuscript\nwe examine the shared features of several biological problems that are amenable\nto modeling with CRFs, highlight the challenges that existing visualization and\nvisual analytics paradigms induce for these data, and document an experimental\nsolution called StickWRLD which, while leaving room for improvement, has been\nsuccessfully applied in several biological research projects."
},{
    "category": "q-bio.BM", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1407.5211v1", 
    "title": "Development & Implementation of a PyMOL 'putty' Representation", 
    "arxiv-id": "1407.5211v1", 
    "author": "Cameron Mura", 
    "publish": "2014-07-19T18:14:20Z", 
    "summary": "The PyMOL molecular graphics program has been modified to introduce a new\n'putty' cartoon representation, akin to the 'sausage'-style representation of\nthe MOLMOL molecular visualization (MolVis) software package. This document\noutlines the development and implementation of the putty representation."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1410.2259v1", 
    "title": "Image compression overview", 
    "arxiv-id": "1410.2259v1", 
    "author": "Martin Prantl", 
    "publish": "2014-09-14T11:10:06Z", 
    "summary": "Compression plays a significant role in a data storage and a transmission. If\nwe speak about a generall data compression, it has to be a lossless one. It\nmeans, we are able to recover the original data 1:1 from the compressed file.\nMultimedia data (images, video, sound...), are a special case. In this area, we\ncan use something called a lossy compression. Our main goal is not to recover\ndata 1:1, but only keep them visually similar. This article is about an image\ncompression, so we will be interested only in image compression. For a human\neye, it is not a huge difference, if we recover RGB color with values\n[150,140,138] instead of original [151,140,137]. The magnitude of a difference\ndetermines the loss rate of the compression. The bigger difference usually\nmeans a smaller file, but also worse image quality and noticable differences\nfrom the original image. We want to cover compression techniques mainly from\nthe last decade. Many of them are variations of existing ones, only some of\nthem uses new principes."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1410.6725v1", 
    "title": "Visualising Large Datasets in TOPCAT v4", 
    "arxiv-id": "1410.6725v1", 
    "author": "Mark Taylor", 
    "publish": "2014-10-24T16:13:11Z", 
    "summary": "TOPCAT is a widely used desktop application for manipulation of astronomical\ncatalogues and other tables, which has long provided fast interactive\nvisualisation features including 1, 2 and 3-d plots, multiple datasets, linked\nviews, color coding, transparency and more. In Version 4 a new plotting library\nhas been written from scratch to deliver new and enhanced visualisation\ncapabilities. This paper describes some of the considerations in the design and\nimplementation, particularly in regard to providing comprehensible interactive\nvisualisation for multi-million point datasets."
},{
    "category": "q-bio.QM", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1412.0488v1", 
    "title": "BigDataViewer: Interactive Visualization and Image Processing for   Terabyte Data Sets", 
    "arxiv-id": "1412.0488v1", 
    "author": "Pavel Tomancak", 
    "publish": "2014-12-01T14:24:44Z", 
    "summary": "The increasingly popular light sheet microscopy techniques generate very\nlarge 3D time-lapse recordings of living biological specimen. The necessity to\nmake large volumetric datasets available for interactive visualization and\nanalysis has been widely recognized. However, existing solutions build on\ndedicated servers to generate virtual slices that are transferred to the client\napplications, practically leading to insufficient frame rates (less than 10\nframes per second) for truly interactive experience. An easily accessible open\nsource solution for interactive arbitrary virtual re-slicing of very large\nvolumes and time series of volumes has yet been missing. We fill this gap with\nBigDataViewer, a Fiji plugin to interactively navigate and visualize large\nimage sequences from both local and remote data sources."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1412.1401v1", 
    "title": "Throat Finding Algorithms based on Throat Types", 
    "arxiv-id": "1412.1401v1", 
    "author": "Kyung-Taek Jun", 
    "publish": "2014-10-28T06:19:50Z", 
    "summary": "The three-dimensional geometry and connectivity of pore space determines the\nflow of single-phase incompressible flow. Herein I report on new throat finding\nalgorithms that contribute to finding the exact flow-relevant geometrical\nproperties of the void space, including high porosity samples of X2B images,\nthree-dimensional synchrotron X-ray computed microtomographic images, and\namounting to over 20% porosity. These new algorithms use the modified medial\naxis that comes from the 3DMA-Rock software package. To find accurate throats,\nwe classify three major throat types: mostly planar and simply connected type,\nnon-planar and simply connected type, and non-planar and non-simply connected\ntype. For each type, we make at least one algorithm to find the throats. Here I\nintroduce an example that has a non-planar and simply connected throat, and my\nsolution indicated by one of my algorithms. My five algorithms each calculate\nthe throat for each path. It selects one of them, which has the smallest inner\narea. New algorithms find accurate throats at least 98% among 12 high porosity\nsamples (over 20%). Also, I introduce a new length calculation in the digitized\nimage. The new calculation uses three mathematical concepts: i)\ndifferentiability, ii) implicit function theorem, iii) line integral. The\nresult can convert the discrete boundary of the XMCT image to the real\nboundary. When the real boundary has an arc shape, the new calculation has less\nthan 1% relative error."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1412.4246v1", 
    "title": "A Canonical Representation of Data-Linear Visualization Algorithms", 
    "arxiv-id": "1412.4246v1", 
    "author": "Thomas Baudel", 
    "publish": "2014-12-13T15:24:38Z", 
    "summary": "We introduce linear-state dataflows, a canonical model for a large set of\nvisualization algorithms that we call data-linear visualizations. Our model\ndefines a fixed dataflow architecture: partitioning and subpartitioning of\ninput data, ordering, graphic primitives, and graphic attributes generation.\nLocal variables and accumulators are specific concepts that extend the\nexpressiveness of the dataflow to support features of visualization algorithms\nthat require state handling. We first show the flexibility of our model: it\nenables the declarative construction of many common algorithms with just a few\nmappings. Furthermore, the model enables easy mixing of visual mappings, such\nas creating treemaps of histograms and 2D plots, plots of histograms...\nFinally, we introduce our model in a more formal way and present some of its\nimportant properties. We have implemented this model in a visualization\nframework built around the concept of linear-state dataflows."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1412.6649v1", 
    "title": "Qualitative shape representation based on the qualitative relative   direction and distance calculus eOPRAm", 
    "arxiv-id": "1412.6649v1", 
    "author": "Reinhard Moratz", 
    "publish": "2014-12-20T12:51:24Z", 
    "summary": "This document serves as a brief technical report, detailing the processes\nused to represent and reconstruct simplified polygons using qualitative spatial\ndescriptions, as defined by the eOPRAm qualitative spatial calculus."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1502.01954v1", 
    "title": "Interactive 3D Face Stylization Using Sculptural Abstraction", 
    "arxiv-id": "1502.01954v1", 
    "author": "Andrew J. Davison", 
    "publish": "2015-02-06T16:59:20Z", 
    "summary": "Sculptors often deviate from geometric accuracy in order to enhance the\nappearance of their sculpture. These subtle stylizations may emphasize anatomy,\ndraw the viewer's focus to characteristic features of the subject, or symbolize\ntextures that might not be accurately reproduced in a particular sculptural\nmedium, while still retaining fidelity to the unique proportions of an\nindividual. In this work we demonstrate an interactive system for enhancing\nface geometry using a class of stylizations based on visual decomposition into\nabstract semantic regions, which we call sculptural abstraction. We propose an\ninteractive two-scale optimization framework for stylization based on\nsculptural abstraction, allowing real-time adjustment of both global and local\nparameters. We demonstrate this system's effectiveness in enhancing physical 3D\nprints of scans from various sources."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1502.07666v1", 
    "title": "Landmark-Guided Elastic Shape Analysis of Human Character Motions", 
    "arxiv-id": "1502.07666v1", 
    "author": "Markus Grasmair", 
    "publish": "2015-02-03T08:45:48Z", 
    "summary": "Motions of virtual characters in movies or video games are typically\ngenerated by recording actors using motion capturing methods. Animations\ngenerated this way often need postprocessing, such as improving the periodicity\nof cyclic animations or generating entirely new motions by interpolation of\nexisting ones. Furthermore, search and classification of recorded motions\nbecomes more and more important as the amount of recorded motion data grows.\n  In this paper, we will apply methods from shape analysis to the processing of\nanimations. More precisely, we will use the by now classical elastic metric\nmodel used in shape matching, and extend it by incorporating additional inexact\nfeature point information, which leads to an improved temporal alignment of\ndifferent animations."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1503.00040v1", 
    "title": "Efficient Upsampling of Natural Images", 
    "arxiv-id": "1503.00040v1", 
    "author": "Fatih Porikli", 
    "publish": "2015-02-28T00:18:39Z", 
    "summary": "We propose a novel method of efficient upsampling of a single natural image.\nCurrent methods for image upsampling tend to produce high-resolution images\nwith either blurry salient edges, or loss of fine textural detail, or spurious\nnoise artifacts.\n  In our method, we mitigate these effects by modeling the input image as a sum\nof edge and detail layers, operating upon these layers separately, and merging\nthe upscaled results in an automatic fashion. We formulate the upsampled output\nimage as the solution to a non-convex energy minimization problem, and propose\nan algorithm to obtain a tractable approximate solution. Our algorithm\ncomprises two main stages. 1) For the edge layer, we use a nonparametric\napproach by constructing a dictionary of patches from a given image, and\nsynthesize edge regions in a higher-resolution version of the image. 2) For the\ndetail layer, we use a global parametric texture enhancement approach to\nsynthesize detail regions across the image.\n  We demonstrate that our method is able to accurately reproduce sharp edges as\nwell as synthesize photorealistic textures, while avoiding common artifacts\nsuch as ringing and haloing. In addition, our method involves no training phase\nor estimation of model parameters, and is easily parallelizable. We demonstrate\nthe utility of our method on a number of challenging standard test photos."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1503.00088v1", 
    "title": "Facial Expression Cloning with Elastic and Muscle Models", 
    "arxiv-id": "1503.00088v1", 
    "author": "Wenjun Zhang", 
    "publish": "2015-02-28T07:23:08Z", 
    "summary": "Expression cloning plays an important role in facial expression synthesis. In\nthis paper, a novel algorithm is proposed for facial expression cloning. The\nproposed algorithm first introduces a new elastic model to balance the global\nand local warping effects, such that the impacts from facial feature diversity\namong people can be minimized, and thus more effective geometric warping\nresults can be achieved. Furthermore, a muscle-distribution-based (MD) model is\nproposed, which utilizes the muscle distribution of the human face and results\nin more accurate facial illumination details. In addition, we also propose a\nnew distance-based metric to automatically select the optimal parameters such\nthat the global and local warping effects in the elastic model can be suitably\nbalanced. Experimental results show that our proposed algorithm outperforms the\nexisting methods."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1503.01804v1", 
    "title": "Frequency Domain TOF: Encoding Object Depth in Modulation Frequency", 
    "arxiv-id": "1503.01804v1", 
    "author": "Ramesh Raskar", 
    "publish": "2015-03-05T22:15:33Z", 
    "summary": "Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of\nflight sensors use phase-based sampling, where the phase delay between emitted\nand received, high-frequency signals encodes distance. In this paper, we\npresent a new time of flight architecture that relies only on frequency---we\nrefer to this technique as frequency-domain time of flight (FD-TOF). Inspired\nby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth\nis high. With the increasing frequency of TOF sensors, new challenges to time\nof flight sensing continue to emerge. At high frequencies, FD-TOF offers\nseveral potential benefits over phase-based time of flight methods."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2014.2363828", 
    "link": "http://arxiv.org/pdf/1503.01903v1", 
    "title": "Partial light field tomographic reconstruction from a fixed-camera focal   stack", 
    "arxiv-id": "1503.01903v1", 
    "author": "C. Guillemot", 
    "publish": "2015-03-06T10:50:40Z", 
    "summary": "This paper describes a novel approach to partially reconstruct\nhigh-resolution 4D light fields from a stack of differently focused photographs\ntaken with a fixed camera. First, a focus map is calculated from this stack\nusing a simple approach combining gradient detection and region expansion with\ngraph-cut. Then, this focus map is converted into a depth map thanks to the\ncalibration of the camera. We proceed after this with the tomographic\nreconstruction of the epipolar images by back-projecting the focused regions of\nthe scene only. We call it masked back-projection. The angles of\nback-projection are calculated from the depth map. Thanks to the high angular\nresolution we achieve by suitably exploiting the image content captured over a\nlarge interval of focus distances, we are able to render puzzling perspective\nshifts although the original photographs were taken from a single fixed camera\nat a fixed position."
},{
    "category": "cs.GR", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1503.06995v1", 
    "title": "Interpolation of a spline developable surface between a curve and two   rulings", 
    "arxiv-id": "1503.06995v1", 
    "author": "L. Fern\u00e1ndez-Jambrina", 
    "publish": "2015-03-24T11:42:43Z", 
    "summary": "In this paper we address the problem of interpolating a spline developable\npatch bounded by a given spline curve and the first and the last rulings of the\ndevelopable surface. In order to complete the boundary of the patch a second\nspline curve is to be given. Up to now this interpolation problem could be\nsolved, but without the possibility of choosing both endpoints for the rulings.\nWe circumvent such difficulty here by resorting to degree elevation of the\ndevelopable surface. This is useful not only to solve this problem, but also\nother problems dealing with triangular developable patches."
},{
    "category": "cs.GR", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1504.00097v1", 
    "title": "Conformal Surface Morphing with Applications on Facial Expressions", 
    "arxiv-id": "1504.00097v1", 
    "author": "Shing-Tung Yau", 
    "publish": "2015-04-01T03:27:02Z", 
    "summary": "Morphing is the process of changing one figure into another. Some numerical\nmethods of 3D surface morphing by deformable modeling and conformal mapping are\nshown in this study. It is well known that there exists a unique Riemann\nconformal mapping from a simply connected surface into a unit disk by the\nRiemann mapping theorem. The dilation and relative orientations of the 3D\nsurfaces can be linked through the M\\\"obius transformation due to the conformal\ncharacteristic of the Riemann mapping. On the other hand, a 3D surface\ndeformable model can be built via various approaches such as mutual\nparameterization from direct interpolation or surface matching using landmarks.\nIn this paper, we take the advantage of the unique representation of 3D\nsurfaces by the mean curvatures and the conformal factors associated with the\nRiemann mapping. By registering the landmarks on the conformal parametric\ndomains, the correspondence of the mean curvatures and the conformal factors\nfor each surfaces can be obtained. As a result, we can construct the 3D\ndeformation field from the surface reconstruction algorithm proposed by Gu and\nYau. Furthermore, by composition of the M\\\"obius transformation and the 3D\ndeformation field, the morphing sequence can be generated from the mean\ncurvatures and the conformal factors on a unified mesh structure by using the\ncubic spline homotopy. Several numerical experiments of the face morphing are\npresented to demonstrate the robustness of our approach."
},{
    "category": "cs.HC", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1504.01049v1", 
    "title": "3D visual analysis of seabed on smartphone", 
    "arxiv-id": "1504.01049v1", 
    "author": "Shengzhong Feng", 
    "publish": "2015-04-04T20:34:11Z", 
    "summary": "We create a 'virtual-seabed' platform to realize the 3D visual analysis of\nseabed on smartphone. The 3D seabed platform is based on a 'section-drilling'\nmodel, implementing visualization and analysis of the integrated data of seabed\non the 3D browser on smartphone. Some 3D visual analysis functions are\ndeveloped. This work presents a thorough and interesting way of presenting\nseabed data on smartphone, which raises many application possibilities. This\nplatform is another practical proof based on our WebVRGIS platform."
},{
    "category": "cs.GR", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1504.01379v2", 
    "title": "Preprint Big City 3D Visual Analysis", 
    "arxiv-id": "1504.01379v2", 
    "author": "Jinxing Hu", 
    "publish": "2015-04-06T15:53:09Z", 
    "summary": "This is the preprint version of our paper on EUROGRAPHICS 2015. A big city\nvisual analysis platform based on Web Virtual Reality Geographical Information\nSystem (WEBVRGIS) is presented. Extensive model editing functions and spatial\nanalysis functions are available, including terrain analysis, spatial analysis,\nsunlight analysis, traffic analysis, population analysis and community\nanalysis."
},{
    "category": "cs.CV", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.00880v3", 
    "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition", 
    "arxiv-id": "1505.00880v3", 
    "author": "Erik Learned-Miller", 
    "publish": "2015-05-05T04:51:19Z", 
    "summary": "A longstanding question in computer vision concerns the representation of 3D\nshapes for recognition: should 3D shapes be represented with descriptors\noperating on their native 3D formats, such as voxel grid or polygon mesh, or\ncan they be effectively represented with view-based descriptors? We address\nthis question in the context of learning to recognize 3D shapes from a\ncollection of their rendered views on 2D images. We first present a standard\nCNN architecture trained to recognize the shapes' rendered views independently\nof each other, and show that a 3D shape can be recognized even from a single\nview at an accuracy far higher than using state-of-the-art 3D shape\ndescriptors. Recognition rates further increase when multiple views of the\nshapes are provided. In addition, we present a novel CNN architecture that\ncombines information from multiple views of a 3D shape into a single and\ncompact shape descriptor offering even better recognition performance. The same\narchitecture can be applied to accurately recognize human hand-drawn sketches\nof shapes. We conclude that a collection of 2D views can be highly informative\nfor 3D shape recognition and is amenable to emerging CNN architectures and\ntheir derivatives."
},{
    "category": "math.NA", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.03111v3", 
    "title": "Control point based exact description of curves and surfaces in extended   Chebyshev spaces", 
    "arxiv-id": "1505.03111v3", 
    "author": "\u00c1goston R\u00f3th", 
    "publish": "2015-05-12T18:24:18Z", 
    "summary": "Extended Chebyshev spaces that also comprise the constants represent large\nfamilies of functions that can be used in real-life modeling or engineering\napplications that also involve important (e.g. transcendental) integral or\nrational curves and surfaces. Concerning computer aided geometric design, the\nunique normalized B-bases of such vector spaces ensure optimal shape preserving\nproperties, important evaluation or subdivision algorithms and useful shape\nparameters. Therefore, we propose global explicit formulas for the entries of\nthose transformation matrices that map these normalized B-bases to the\ntraditional (or ordinary) bases of the underlying vector spaces. Then, we also\ndescribe general and ready to use control point configurations for the exact\nrepresentation of those traditional integral parametric curves and (hybrid)\nsurfaces that are specified by coordinate functions given as (products of\nseparable) linear combinations of ordinary basis functions. The obtained\nresults are also extended to the control point and weight based exact\ndescription of the rational counterpart of these integral parametric curves and\nsurfaces. The universal applicability of our methods is presented through\npolynomial, trigonometric, hyperbolic or mixed extended Chebyshev vector\nspaces."
},{
    "category": "cs.CG", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.05590v1", 
    "title": "Constructing Intrinsic Delaunay Triangulations from the Dual of Geodesic   Voronoi Diagrams", 
    "arxiv-id": "1505.05590v1", 
    "author": "Ying He", 
    "publish": "2015-05-21T02:45:41Z", 
    "summary": "Intrinsic Delaunay triangulation (IDT) is a fundamental data structure in\ncomputational geometry and computer graphics. However, except for some\ntheoretical results, such as existence and uniqueness, little progress has been\nmade towards computing IDT on simplicial surfaces. To date the only way for\nconstructing IDTs is the edge-flipping algorithm, which iteratively flips the\nnon-Delaunay edge to be locally Delaunay. Although the algorithm is\nconceptually simple and guarantees to stop in finite steps, it has no known\ntime complexity. Moreover, the edge-flipping algorithm may produce non-regular\ntriangulations, which contain self-loops and/or faces with only two edges. In\nthis paper, we propose a new method for constructing IDT on manifold triangle\nmeshes. Based on the duality of geodesic Voronoi diagrams, our method can\nguarantee the resultant IDTs are regular. Our method has a theoretical\nworst-case time complexity $O(n^2\\log n)$ for a mesh with $n$ vertices. We\nobserve that most real-world models are far from their Delaunay triangulations,\nthus, the edge-flipping algorithm takes many iterations to fix the non-Delaunay\nedges. In contrast, our method is non-iterative and insensitive to the number\nof non-Delaunay edges. Empirically, it runs in linear time $O(n)$ on real-world\nmodels."
},{
    "category": "cs.CL", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.06289v2", 
    "title": "Text to 3D Scene Generation with Rich Lexical Grounding", 
    "arxiv-id": "1505.06289v2", 
    "author": "Christopher D. Manning", 
    "publish": "2015-05-23T08:32:11Z", 
    "summary": "The ability to map descriptions of scenes to 3D geometric representations has\nmany applications in areas such as art, education, and robotics. However, prior\nwork on the text to 3D scene generation task has used manually specified object\ncategories and language that identifies them. We introduce a dataset of 3D\nscenes annotated with natural language descriptions and learn from this data\nhow to ground textual descriptions to physical objects. Our method successfully\ngrounds a variety of lexical terms to concrete referents, and we show\nquantitatively that our method improves 3D scene generation over previous work\nusing purely rule-based methods. We evaluate the fidelity and plausibility of\n3D scenes generated with our grounding approach through human judgments. To\nease evaluation on this task, we also introduce an automated metric that\nstrongly correlates with human judgments."
},{
    "category": "cs.HC", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.07267v1", 
    "title": "Prototyping Information Visualization in 3D City Models: a Model-based   Approach", 
    "arxiv-id": "1505.07267v1", 
    "author": "Gilles Falquet", 
    "publish": "2015-05-27T11:25:46Z", 
    "summary": "When creating 3D city models, selecting relevant visualization techniques is\na particularly difficult user interface design task. A first obstacle is that\ncurrent geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and\nlimited sets of visualization techniques. Another important obstacle is the\nlack of unified description of information visualization techniques for 3D city\nmodels. If many techniques have been devised for different types of data or\ninformation (wind flows, air quality fields, historic or legal texts, etc.)\nthey are generally described in articles, and not really formalized. In this\npaper we address the problem of visualizing information in (rich) 3D city\nmodels by presenting a model-based approach for the rapid prototyping of\nvisualization techniques. We propose to represent visualization techniques as\nthe composition of graph transformations. We show that these transformations\ncan be specified with SPARQL construction operations over RDF graphs. These\nspecifications can then be used in a prototype generator to produce 3D scenes\nthat contain the 3D city model augmented with data represented using the\ndesired technique."
},{
    "category": "cs.HC", 
    "doi": "10.1631/FITEE.14a0210", 
    "link": "http://arxiv.org/pdf/1505.07940v1", 
    "title": "Continuous Mental Effort Evaluation during 3D Object Manipulation Tasks   based on Brain and Physiological Signals", 
    "arxiv-id": "1505.07940v1", 
    "author": "Fabien Lotte", 
    "publish": "2015-05-29T06:53:08Z", 
    "summary": "Designing 3D User Interfaces (UI) requires adequate evaluation tools to\nensure good usability and user experience. While many evaluation tools are\nalready available and widely used, existing approaches generally cannot provide\ncontinuous and objective measures of usa-bility qualities during interaction\nwithout interrupting the user. In this paper, we propose to use brain (with\nElectroEncephaloGraphy) and physiological (ElectroCardioGraphy, Galvanic Skin\nResponse) signals to continuously assess the mental effort made by the user to\nperform 3D object manipulation tasks. We first show how this mental effort\n(a.k.a., mental workload) can be estimated from such signals, and then measure\nit on 8 participants during an actual 3D object manipulation task with an input\ndevice known as the CubTile. Our results suggest that monitoring workload\nenables us to continuously assess the 3DUI and/or interaction technique\nease-of-use. Overall, this suggests that this new measure could become a useful\naddition to the repertoire of available evaluation tools, enabling a finer\ngrain assessment of the ergonomic qualities of a given 3D user interface."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cam.2016.01.006", 
    "link": "http://arxiv.org/pdf/1506.00967v2", 
    "title": "Geometric elements and classification of quadrics in rational B\u00e9zier   form", 
    "arxiv-id": "1506.00967v2", 
    "author": "M. J. V\u00e1zquez-Gallo", 
    "publish": "2015-06-02T17:35:47Z", 
    "summary": "In this paper we classify and derive closed formulas for geometric elements\nof quadrics in rational B\\'ezier triangular form (such as the center, the conic\nat infinity, the vertex and the axis of paraboloids and the principal planes),\nusing just the control vertices and the weights for the quadric patch. The\nresults are extended also to quadric tensor product patches. Our results rely\non using techniques from projective algebraic geometry to find suitable\nbilinear forms for the quadric in a coordinate-free fashion, considering a\npencil of quadrics that are tangent to the given quadric along a conic. Most of\nthe information about the quadric is encoded in one coefficient, involving the\nweights of the patch, which allows us to tell apart oval from ruled quadrics.\nThis coefficient is also relevant to determine the affine type of the quadric.\nSpheres and quadrics of revolution are characterised within this framework."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TIP.2016.2529506", 
    "link": "http://arxiv.org/pdf/1506.06096v1", 
    "title": "Graph-based compression of dynamic 3D point cloud sequences", 
    "arxiv-id": "1506.06096v1", 
    "author": "Pascal Frossard", 
    "publish": "2015-06-19T17:31:34Z", 
    "summary": "This paper addresses the problem of compression of 3D point cloud sequences\nthat are characterized by moving 3D positions and color attributes. As\ntemporally successive point cloud frames are similar, motion estimation is key\nto effective compression of these sequences. It however remains a challenging\nproblem as the point cloud frames have varying numbers of points without\nexplicit correspondence information. We represent the time-varying geometry of\nthese sequences with a set of graphs, and consider 3D positions and color\nattributes of the points clouds as signals on the vertices of the graphs. We\nthen cast motion estimation as a feature matching problem between successive\ngraphs. The motion is estimated on a sparse set of representative vertices\nusing new spectral graph wavelet descriptors. A dense motion field is\neventually interpolated by solving a graph-based regularization problem. The\nestimated motion is finally used for removing the temporal redundancy in the\npredictive coding of the 3D positions and the color characteristics of the\npoint cloud sequences. Experimental results demonstrate that our method is able\nto accurately estimate the motion between consecutive frames. Moreover, motion\nestimation is shown to bring significant improvement in terms of the overall\ncompression performance of the sequence. To the best of our knowledge, this is\nthe first paper that exploits both the spatial correlation inside each frame\n(through the graph) and the temporal correlation between the frames (through\nthe motion estimation) to compress the color and the geometry of 3D point cloud\nsequences in an efficient way."
},{
    "category": "math.GN", 
    "doi": "10.1109/TIP.2016.2529506", 
    "link": "http://arxiv.org/pdf/1506.06426v1", 
    "title": "A Borsuk-Ulam theorem for digital images", 
    "arxiv-id": "1506.06426v1", 
    "author": "P. Christopher Staecker", 
    "publish": "2015-06-22T00:04:04Z", 
    "summary": "The Borsuk-Ulam theorem states that a continuous function $f:S^n \\to \\R^n$\nhas a point $x\\in S^n$ with $f(x)=f(-x)$. We give an analogue of this theorem\nfor digital images, which are modeled as discrete spaces of adjacent pixels\nequipped with $\\Z^n$-valued functions.\n  In particular, for a concrete two-dimensional rectangular digital image whose\npixels all have an assigned \"brightness\" function, we prove that there must\nexist a pair of opposite boundary points whose brightnesses are approximately\nequal. This theorem applies generally to any integer-valued function on an\nabstract simple graph.\n  We also discuss generalizations to digital images of dimension 3 and higher.\nWe give some partial results for higher dimensional images, and show a counter\nexample which demonstrates that the full results obtained in lower dimensions\ncannot hold generally."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TIP.2016.2529506", 
    "link": "http://arxiv.org/pdf/1506.08956v2", 
    "title": "Lens Factory: Automatic Lens Generation Using Off-the-shelf Components", 
    "arxiv-id": "1506.08956v2", 
    "author": "James Hays", 
    "publish": "2015-06-30T06:39:19Z", 
    "summary": "Custom optics is a necessity for many imaging applications. Unfortunately,\ncustom lens design is costly (thousands to tens of thousands of dollars), time\nconsuming (10-12 weeks typical lead time), and requires specialized optics\ndesign expertise. By using only inexpensive, off-the-shelf lens components the\nLens Factory automatic design system greatly reduces cost and time. Design,\nordering of parts, delivery, and assembly can be completed in a few days, at a\ncost in the low hundreds of dollars. Lens design constraints, such as focal\nlength and field of view, are specified in terms familiar to the graphics\ncommunity so no optics expertise is necessary. Unlike conventional lens design\nsystems, which only use continuous optimization methods, Lens Factory adds a\ndiscrete optimization stage. This stage searches the combinatorial space of\npossible combinations of lens elements to find novel designs, evolving simple\ncanonical lens designs into more complex, better designs. Intelligent pruning\nrules make the combinatorial search feasible. We have designed and built\nseveral high performance optical systems which demonstrate the practicality of\nthe system."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2082315", 
    "link": "http://arxiv.org/pdf/1506.09166v1", 
    "title": "Aging display's effect on interpretation of digital pathology slides", 
    "arxiv-id": "1506.09166v1", 
    "author": "Tom R. L. Kimpe", 
    "publish": "2015-06-30T17:29:43Z", 
    "summary": "It is our conjecture that the variability of colors in a pathology image\neffects the interpretation of pathology cases, whether it is diagnostic\naccuracy, diagnostic confidence, or workflow efficiency. In this paper, digital\npathology images are analyzed to quantify the perceived difference in color\nthat occurs due to display aging, in particular a change in the maximum\nluminance, white point, and color gamut. The digital pathology images studied\ninclude diagnostically important features, such as the conspicuity of nuclei.\nThree different display aging models are applied to images: aging of luminance\n& chrominance, aging of chrominance only, and a stabilized luminance &\nchrominance (i.e., no aging). These display models and images are then used to\ncompare conspicuity of nuclei using CIE deltaE2000, a perceptual color\ndifference metric. The effect of display aging using these display models and\nimages is further analyzed through a human reader study designed to quantify\nthe effects from a clinical perspective. Results from our reader study indicate\nsignificant impact of aged displays on workflow as well as diagnosis as follow.\nAs compared to the originals (no-aging), slides with the effect of aging\nsimulated were significantly more difficult to read (p-value of 0.0005) and\ntook longer to score (p-value of 0.02). Moreover, luminance+chrominance aging\nsignificantly reduced inter-session percent agreement of diagnostic scores\n(p-value of 0.0418)."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2082315", 
    "link": "http://arxiv.org/pdf/1507.08571v2", 
    "title": "Agglomerative clustering and collectiveness measure via exponent   generating function", 
    "arxiv-id": "1507.08571v2", 
    "author": "Jun Zhang", 
    "publish": "2015-07-30T16:30:00Z", 
    "summary": "The key in agglomerative clustering is to define the affinity measure between\ntwo sets. A novel agglomerative clustering method is proposed by utilizing the\npath integral to define the affinity measure. Firstly, the path integral\ndescriptor of an edge, a node and a set is computed by path integral and\nexponent generating function. Then, the affinity measure between two sets is\nobtained by path integral descriptor of sets. Several good properties of the\npath integral descriptor is proposed in this paper. In addition, we give the\nphysical interpretation of the proposed path integral descriptor of a set. The\nproposed path integral descriptor of a set can be regard as the collectiveness\nmeasure of a set, which can be a moving system such as human crowd, sheep herd\nand so on. Self-driven particle (SDP) model is used to test the ability of the\nproposed method in measuring collectiveness."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2082315", 
    "link": "http://arxiv.org/pdf/1508.03590v2", 
    "title": "Light-field Microscopy with a Consumer Light-field Camera", 
    "arxiv-id": "1508.03590v2", 
    "author": "Ivo Ihrke", 
    "publish": "2015-05-04T12:17:09Z", 
    "summary": "We explore the use of inexpensive consumer light- field camera technology for\nthe purpose of light-field mi- croscopy. Our experiments are based on the Lytro\n(first gen- eration) camera. Unfortunately, the optical systems of the Lytro\nand those of microscopes are not compatible, lead- ing to a loss of light-field\ninformation due to angular and spatial vignetting when directly recording\nmicroscopic pic- tures. We therefore consider an adaptation of the Lytro op-\ntical system. We demonstrate that using the Lytro directly as an oc- ular\nreplacement, leads to unacceptable spatial vignetting. However, we also found a\nsetting that allows the use of the Lytro camera in a virtual imaging mode which\nprevents the information loss to a large extent. We analyze the new vir- tual\nimaging mode and use it in two different setups for im- plementing light-field\nmicroscopy using a Lytro camera. As a practical result, we show that the camera\ncan be used for low magnification work, as e.g. common in quality control,\nsurface characterization, etc. We achieve a maximum spa- tial resolution of\nabout 6.25{\\mu}m, albeit at a limited SNR for the side views."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5302", 
    "link": "http://arxiv.org/pdf/1508.03840v1", 
    "title": "3D-Computer Animation for a Yoruba Native Folktale", 
    "arxiv-id": "1508.03840v1", 
    "author": "Biola Oyediran", 
    "publish": "2015-08-16T15:58:11Z", 
    "summary": "Computer graphics has wide range of applications which are implemented into\ncomputer animation, computer modeling among others. Since the invention of\ncomputer graphics researchers have not paid much of attentions toward the\npossibility of converting oral tales otherwise known as folktales into possible\ncartoon animated videos. This paper is based on how to develop cartoons of\nlocal folktales that will be of huge benefits to Nigerians. The activities were\ndivided into 5 stages; analysis, design, development, implementation and\nevaluation which involved various processes and use of various specialized\nsoftware and hardware. After the implementation of this project, the video\ncharacteristics were evaluated using likert scale. Analysis of 30 user\nresponses indicated that 17 users (56.7 percent) rated the image quality as\nexcellent, the video and image synchronization was rated as excellent by 9\nusers (30 percent), the Background noise was rated excellent by 18 users (60\npercent), the Character Impression was rated Excellent by 11 users (36.67\npercent), the general assessment of the storyline was rated excellent by 17\nusers (56.7 percent), the video Impression was rated excellent by 11 users\n(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as\nexcellent."
},{
    "category": "cs.GR", 
    "doi": "10.5121/ijcga.2015.5302", 
    "link": "http://arxiv.org/pdf/1509.01220v1", 
    "title": "Light Efficient Flutter Shutter", 
    "arxiv-id": "1509.01220v1", 
    "author": "Moshe Ben-Ezra", 
    "publish": "2015-08-23T00:37:47Z", 
    "summary": "Flutter shutter is a technique in which the exposure is chopped into segments\nand light is only integrated part of the time. By carefully selecting the\nchopping sequence it is possible to better condition the data for\nreconstruction problems such as motion deblurring, focal sweeping, and\ncompressed sensing. The partial exposure trades better conditioning for less\nenergy. In problems such as motion deblurring the available energy is what\ncaused the problem in the first place (as strong illumination allows short\nexposure thus eliminates motion blur). It is still beneficial because the\nbenefit from the better conditioning outweighs the cost in energy.\n  This documents is focused on light efficient flutter shutter that provides\nbetter conditioning and better energy utilization than conventional flutter\nshutter."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2874358", 
    "link": "http://arxiv.org/pdf/1509.08037v1", 
    "title": "Deformation Lamps: A Projection Technique to Make a Static Object   Dynamic", 
    "arxiv-id": "1509.08037v1", 
    "author": "Shin'ya Nishida", 
    "publish": "2015-09-27T00:07:07Z", 
    "summary": "Light projection is a powerful technique to edit appearances of objects in\nthe real world. Based on pixel-wise modification of light transport, previous\ntechniques have successfully modified static surface properties such as surface\ncolor, dynamic range, gloss and shading. Here, we propose an alternative light\nprojection technique that adds a variety of illusory, yet realistic distortions\nto a wide range of static 2D and 3D projection targets. The key idea of our\ntechnique, named Deformation Lamps, is to project only dynamic luminance\ninformation, which effectively activates the motion (and shape) processing in\nthe visual system, while preserving the color and texture of the original\nobject. Although the projected dynamic luminance information is spatially\ninconsistent with the color and texture of the target object, the observer's\nbrain automatically com- bines these sensory signals in such a way as to\ncorrect the inconsistency across visual attributes. We conducted a\npsychophysical experiment to investigate the characteristics of the\ninconsistency correction, and found that the correction was dependent\ncritically on the retinal magnitude of inconsistency. Another experiment showed\nthat perceived magnitude of image deformation by our techniques was\nunderestimated. The results ruled out the possibility that the effect by our\ntechnique stemmed simply from the physical change of object appearance by light\nprojection. Finally, we discuss how our techniques can make the observers\nperceive a vivid and natural movement, deformation, or oscillation of a variety\nof static objects, including drawn pictures, printed photographs, sculptures\nwith 3D shading, objects with natural textures including human bodies."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2874358", 
    "link": "http://arxiv.org/pdf/1509.08834v1", 
    "title": "Visualization techniques for the developing chicken heart", 
    "arxiv-id": "1509.08834v1", 
    "author": "Cindy Grimm", 
    "publish": "2015-09-29T16:31:57Z", 
    "summary": "We present a geometric surface parameterization algorithm and several\nvisualization techniques adapted to the problem of understanding the 4D\nperistaltic-like motion of the outflow tract (OFT) in an embryonic chick heart.\nWe illustrated the techniques using data from hearts under normal conditions\n(four embryos), and hearts in which blood flow conditions are altered through\nOFT banding (four embryos). The overall goal is to create quantitative measures\nof the temporal heart-shape change both within a single subject and between\nmultiple subjects. These measures will help elucidate how altering hemodynamic\nconditions changes the shape and motion of the OFT walls, which in turn\ninfluence the stresses and strains on the developing heart, causing it to\ndevelop differently. We take advantage of the tubular shape and periodic motion\nof the OFT to produce successively lower dimensional visualizations of the\ncardiac motion (e.g. curvature, volume, and cross-section) over time, and\nquantifications of such visualizations."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2874358", 
    "link": "http://arxiv.org/pdf/1510.01113v2", 
    "title": "RAID: A Relation-Augmented Image Descriptor", 
    "arxiv-id": "1510.01113v2", 
    "author": "Peter Wonka", 
    "publish": "2015-10-05T11:58:12Z", 
    "summary": "As humans, we regularly interpret images based on the relations between image\nregions. For example, a person riding object X, or a plank bridging two\nobjects. Current methods provide limited support to search for images based on\nsuch relations. We present RAID, a relation-augmented image descriptor that\nsupports queries based on inter-region relations. The key idea of our\ndescriptor is to capture the spatial distribution of simple point-to-region\nrelationships to describe more complex relationships between two image regions.\nWe evaluate the proposed descriptor by querying into a large subset of the\nMicrosoft COCO database and successfully extract nontrivial images\ndemonstrating complex inter-region relations, which are easily missed or\nerroneously classified by existing methods."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2874358", 
    "link": "http://arxiv.org/pdf/1510.09069v1", 
    "title": "Simulating the Dynamic Behavior of Shear Thickening Fluids", 
    "arxiv-id": "1510.09069v1", 
    "author": "Eric Brown", 
    "publish": "2015-10-28T23:30:53Z", 
    "summary": "While significant research has been dedicated to the simulation of fluids,\nnot much attention has been given to exploring new interesting behavior that\ncan be generated with the different types of non-Newtonian fluids with\nnon-constant viscosity. Going in this direction, this paper introduces a\ncomputational model for simulating the interesting phenomena observed in\nnon-Newtonian shear thickening fluids, which are fluids where the viscosity\nincreases with increased stress. These fluids have unique and unconventional\nbehavior, and they often appear in real world scenarios such as when sinking in\nquicksand or when experimenting with popular cornstarch and water mixtures.\nWhile interesting behavior of shear thickening fluids can be easily observed in\nthe real world, the most interesting phenomena of these fluids have not been\nsimulated before in computer graphics. The fluid exhibits unique phase changes\nbetween solid and liquid states, great impact resistance in its solid state and\nstrong hysteresis effects. Our proposed approach builds on existing\nnon-Newtonian fluid models in computer graphics and introduces an efficient\nhistory-based stiffness term that is essential to produce the most interesting\nshear thickening phenomena. The history-based stiffness is formulated through\nthe use of fractional derivatives, leveraging the fractional calculus ability\nto depict both the viscoelastic behavior and the history effects of\nhistory-dependent systems. Simulations produced by our method are compared\nagainst real experiments and the results demonstrate that the proposed model\nsuccessfully captures key phenomena observed in shear thickening fluids."
},{
    "category": "cs.CG", 
    "doi": "10.1145/2874358", 
    "link": "http://arxiv.org/pdf/1510.09203v1", 
    "title": "Computational Network Design from Functional Specifications", 
    "arxiv-id": "1510.09203v1", 
    "author": "Peter Wonka", 
    "publish": "2015-10-30T19:09:36Z", 
    "summary": "Connectivity and layout of underlying networks largely determine the behavior\nof many environments. For example, transportation networks determine the flow\nof traffic in cities, or maps determine the difficulty and flow in games.\nDesigning such networks from scratch is challenging as even local network\nchanges can have large global effects. We investigate how to computationally\ncreate networks starting from {\\em only} high-level functional specifications.\nSuch specifications can be in the form of network density, travel time versus\nnetwork length, traffic type, destination locations, etc. We propose an integer\nprogramming-based approach that guarantees that the resultant networks are\nvalid by fulfilling all specified hard constraints, and score favorably in\nterms of the objective function. We evaluate our algorithm in three different\ndesign settings (i.e., street layout, floorplanning, and game level design) and\ndemonstrate, for the first time, that diverse networks can emerge purely from\nhigh-level functional specifications."
},{
    "category": "cs.CV", 
    "doi": "10.1109/3DTV.2015.7169366", 
    "link": "http://arxiv.org/pdf/1511.04902v1", 
    "title": "Graph-based denoising for time-varying point clouds", 
    "arxiv-id": "1511.04902v1", 
    "author": "Pierre Vandergheynst", 
    "publish": "2015-11-16T10:34:25Z", 
    "summary": "Noisy 3D point clouds arise in many applications. They may be due to errors\nwhen constructing a 3D model from images or simply to imprecise depth sensors.\nPoint clouds can be given geometrical structure using graphs created from the\nsimilarity information between points. This paper introduces a technique that\nuses this graph structure and convex optimization methods to denoise 3D point\nclouds. A short discussion presents how those methods naturally generalize to\ntime-varying inputs such as 3D point cloud time series."
},{
    "category": "cs.CV", 
    "doi": "10.1109/3DTV.2015.7169366", 
    "link": "http://arxiv.org/pdf/1511.05904v2", 
    "title": "Dense Human Body Correspondences Using Convolutional Networks", 
    "arxiv-id": "1511.05904v2", 
    "author": "Hao Li", 
    "publish": "2015-11-18T18:36:54Z", 
    "summary": "We propose a deep learning approach for finding dense correspondences between\n3D scans of people. Our method requires only partial geometric information in\nthe form of two depth maps or partial reconstructed surfaces, works for humans\nin arbitrary poses and wearing any clothing, does not require the two people to\nbe scanned from similar viewpoints, and runs in real time. We use a deep\nconvolutional neural network to train a feature descriptor on depth map pixels,\nbut crucially, rather than training the network to solve the shape\ncorrespondence problem directly, we train it to solve a body region\nclassification problem, modified to increase the smoothness of the learned\ndescriptors near region boundaries. This approach ensures that nearby points on\nthe human body are nearby in feature space, and vice versa, rendering the\nfeature descriptor suitable for computing dense correspondences between the\nscans. We validate our method on real and synthetic data for both clothed and\nunclothed humans, and show that our correspondences are more robust than is\npossible with state-of-the-art unsupervised methods, and more accurate than\nthose found using methods that require full watertight 3D geometry."
},{
    "category": "cs.GR", 
    "doi": "10.1109/3DTV.2015.7169366", 
    "link": "http://arxiv.org/pdf/1601.01232v1", 
    "title": "Shape Animation with Combined Captured and Simulated Dynamics", 
    "arxiv-id": "1601.01232v1", 
    "author": "Edmond Boyer", 
    "publish": "2016-01-06T16:30:27Z", 
    "summary": "We present a novel volumetric animation generation framework to create new\ntypes of animations from raw 3D surface or point cloud sequence of captured\nreal performances. The framework considers as input time incoherent 3D\nobservations of a moving shape, and is thus particularly suitable for the\noutput of performance capture platforms. In our system, a suitable virtual\nrepresentation of the actor is built from real captures that allows seamless\ncombination and simulation with virtual external forces and objects, in which\nthe original captured actor can be reshaped, disassembled or reassembled from\nuser-specified virtual physics. Instead of using the dominant surface-based\ngeometric representation of the capture, which is less suitable for volumetric\neffects, our pipeline exploits Centroidal Voronoi tessellation decompositions\nas unified volumetric representation of the real captured actor, which we show\ncan be used seamlessly as a building block for all processing stages, from\ncapture and tracking to virtual physic simulation. The representation makes no\nhuman specific assumption and can be used to capture and re-simulate the actor\nwith props or other moving scenery elements. We demonstrate the potential of\nthis pipeline for virtual reanimation of a real captured event with various\nunprecedented volumetric visual effects, such as volumetric distortion,\nerosion, morphing, gravity pull, or collisions."
},{
    "category": "cs.GR", 
    "doi": "10.1109/3DTV.2015.7169366", 
    "link": "http://arxiv.org/pdf/1601.01754v1", 
    "title": "Anti-commutative Dual Complex Numbers and 2D Rigid Transformation", 
    "arxiv-id": "1601.01754v1", 
    "author": "Hiroyuki Ochiai", 
    "publish": "2016-01-08T02:56:57Z", 
    "summary": "We introduce a new presentation of the two dimensional rigid transformation\nwhich is more concise and efficient than the standard matrix presentation. By\nmodifying the ordinary dual number construction for the complex numbers, we\ndefine the ring of the anti-commutative dual complex numbers, which\nparametrizes two dimensional rotation and translation all together. With this\npresentation, one can easily interpolate or blend two or more rigid\ntransformations at a low computational cost. We developed a library for C++\nwith the MIT-licensed source code and demonstrate its facility by an\ninteractive deformation tool developed for iPad."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-319-22804-4_6", 
    "link": "http://arxiv.org/pdf/1601.03224v1", 
    "title": "Implicit equations of non-degenerate rational Bezier quadric triangles", 
    "arxiv-id": "1601.03224v1", 
    "author": "M. J. Vazquez-Gallo", 
    "publish": "2016-01-13T13:11:42Z", 
    "summary": "In this paper we review the derivation of implicit equations for\nnon-degenerate quadric patches in rational Bezier triangular form. These are\nthe case of Steiner surfaces of degree two. We derive the bilinear forms for\nsuch quadrics in a coordinate-free fashion in terms of their control net and\ntheir list of weights in a suitable form. Our construction relies on projective\ngeometry and is grounded on the pencil of quadrics circumscribed to a\ntetrahedron formed by vertices of the control net and an additional point which\nis required for the Steiner surface to be a non-degenerate quadric."
},{
    "category": "cs.HC", 
    "doi": "10.1109/VAAT.2015.7155405", 
    "link": "http://arxiv.org/pdf/1601.04436v1", 
    "title": "Development of a wheelchair simulator for children with multiple   disabilities", 
    "arxiv-id": "1601.04436v1", 
    "author": "Nancy Rodriguez", 
    "publish": "2016-01-18T09:23:46Z", 
    "summary": "Virtual reality allows to create situations which can be experimented under\nthe control of the user, without risks, in a very flexible way. This allows to\ndevelop skills and to have confidence to work in real conditions with real\nequipment. VR is then widely used as a training and learning tool. More\nrecently, VR has also showed its potential in rehabilitation and therapy fields\nbecause it provides users with the ability of repeat their actions several\ntimes and to progress at their own pace. In this communication, we present our\nwork in the development of a wheelchair simulator designed to allow children\nwith multiple disabilities to familiarize themselves with the wheelchair."
},{
    "category": "cs.GR", 
    "doi": "10.1109/VAAT.2015.7155405", 
    "link": "http://arxiv.org/pdf/1601.04816v1", 
    "title": "Tetrisation of triangular meshes and its application in shape blending", 
    "arxiv-id": "1601.04816v1", 
    "author": "Shizuo Kaji", 
    "publish": "2016-01-19T07:48:57Z", 
    "summary": "The As-Rigid-As-Possible (ARAP) shape deformation framework is a versatile\ntechnique for morphing, surface modelling, and mesh editing. We discuss an\nimprovement of the ARAP framework in a few aspects: 1. Given a triangular mesh\nin 3D space, we introduce a method to associate a tetrahedral structure, which\nencodes the geometry of the original mesh. 2. We use a Lie algebra based method\nto interpolate local transformation, which provides better handling of rotation\nwith large angle. 3. We propose a new error function to compile local\ntransformations into a global piecewise linear map, which is rotation invariant\nand easy to minimise. We implemented a shape blender based on our algorithm and\nits MIT licensed source code is available online."
},{
    "category": "cs.CV", 
    "doi": "10.1109/VAAT.2015.7155405", 
    "link": "http://arxiv.org/pdf/1601.05644v1", 
    "title": "B-spline Shape from Motion & Shading: An Automatic Free-form Surface   Modeling for Face Reconstruction", 
    "arxiv-id": "1601.05644v1", 
    "author": "Chao Xu", 
    "publish": "2016-01-21T14:11:40Z", 
    "summary": "Recently, many methods have been proposed for face reconstruction from\nmultiple images, most of which involve fundamental principles of Shape from\nShading and Structure from motion. However, a majority of the methods just\ngenerate discrete surface model of face. In this paper, B-spline Shape from\nMotion and Shading (BsSfMS) is proposed to reconstruct continuous B-spline\nsurface for multi-view face images, according to an assumption that shading and\nmotion information in the images contain 1st- and 0th-order derivative of\nB-spline face respectively. Face surface is expressed as a B-spline surface\nthat can be reconstructed by optimizing B-spline control points. Therefore,\nnormals and 3D feature points computed from shading and motion of images\nrespectively are used as the 1st- and 0th- order derivative information, to be\njointly applied in optimizing the B-spline face. Additionally, an IMLS\n(iterative multi-least-square) algorithm is proposed to handle the difficult\ncontrol point optimization. Furthermore, synthetic samples and LFW dataset are\nintroduced and conducted to verify the proposed approach, and the experimental\nresults demonstrate the effectiveness with different poses, illuminations,\nexpressions etc., even with wild images."
},{
    "category": "cs.CV", 
    "doi": "10.1109/VAAT.2015.7155405", 
    "link": "http://arxiv.org/pdf/1601.06950v1", 
    "title": "Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction", 
    "arxiv-id": "1601.06950v1", 
    "author": "Michael Goesele", 
    "publish": "2016-01-26T09:57:34Z", 
    "summary": "The ultimate goal of many image-based modeling systems is to render\nphoto-realistic novel views of a scene without visible artifacts. Existing\nevaluation metrics and benchmarks focus mainly on the geometric accuracy of the\nreconstructed model, which is, however, a poor predictor of visual accuracy.\nFurthermore, using only geometric accuracy by itself does not allow evaluating\nsystems that either lack a geometric scene representation or utilize coarse\nproxy geometry. Examples include light field or image-based rendering systems.\nWe propose a unified evaluation approach based on novel view prediction error\nthat is able to analyze the visual quality of any method that can render novel\nviews from input images. One of the key advantages of this approach is that it\ndoes not require ground truth geometry. This dramatically simplifies the\ncreation of test datasets and benchmarks. It also allows us to evaluate the\nquality of an unknown scene during the acquisition and reconstruction process,\nwhich is useful for acquisition planning. We evaluate our approach on a range\nof methods including standard geometry-plus-texture pipelines as well as\nimage-based rendering techniques, compare it to existing geometry-based\nbenchmarks, and demonstrate its utility for a range of use cases."
},{
    "category": "cs.CV", 
    "doi": "10.1109/VAAT.2015.7155405", 
    "link": "http://arxiv.org/pdf/1602.00328v2", 
    "title": "Novel Views of Objects from a Single Image", 
    "arxiv-id": "1602.00328v2", 
    "author": "Tinne Tuytelaars", 
    "publish": "2016-01-31T21:43:13Z", 
    "summary": "Taking an image of an object is at its core a lossy process. The rich\ninformation about the three-dimensional structure of the world is flattened to\nan image plane and decisions such as viewpoint and camera parameters are final\nand not easily revertible. As a consequence, possibilities of changing\nviewpoint are limited. Given a single image depicting an object, novel-view\nsynthesis is the task of generating new images that render the object from a\ndifferent viewpoint than the one given. The main difficulty is to synthesize\nthe parts that are disoccluded; disocclusion occurs when parts of an object are\nhidden by the object itself under a specific viewpoint. In this work, we show\nhow to improve novel-view synthesis by making use of the correlations observed\nin 3D models and applying them to new image instances. We propose a technique\nto use the structural information extracted from a 3D model that matches the\nimage object in terms of viewpoint and shape. For the latter part, we propose\nan efficient 2D-to-3D alignment method that associates precisely the image\nappearance with the 3D model geometry with minimal user interaction. Our\ntechnique is able to simulate plausible viewpoint changes for a variety of\nobject classes within seconds. Additionally, we show that our synthesized\nimages can be used as additional training data that improves the performance of\nstandard object detectors."
},{
    "category": "cs.CV", 
    "doi": "10.1109/3DV.2014.46", 
    "link": "http://arxiv.org/pdf/1602.02023v1", 
    "title": "Efficient Multi-view Performance Capture of Fine-Scale Surface Detail", 
    "arxiv-id": "1602.02023v1", 
    "author": "Christian Theobalt", 
    "publish": "2016-02-05T14:08:47Z", 
    "summary": "We present a new effective way for performance capture of deforming meshes\nwith fine-scale time-varying surface detail from multi-view video. Our method\nbuilds up on coarse 4D surface reconstructions, as obtained with commonly used\ntemplate-based methods. As they only capture models of coarse-to-medium scale\ndetail, fine scale deformation detail is often done in a second pass by using\nstereo constraints, features, or shading-based refinement. In this paper, we\npropose a new effective and stable solution to this second step. Our framework\ncreates an implicit representation of the deformable mesh using a dense\ncollection of 3D Gaussian functions on the surface, and a set of 2D Gaussians\nfor the images. The fine scale deformation of all mesh vertices that maximizes\nphoto-consistency can be efficiently found by densely optimizing a new\nmodel-to-image consistency energy on all vertex positions. A principal\nadvantage is that our problem formulation yields a smooth closed form energy\nwith implicit occlusion handling and analytic derivatives. Error-prone\ncorrespondence finding, or discrete sampling of surface displacement values are\nalso not needed. We show several reconstructions of human subjects wearing\nloose clothing, and we qualitatively and quantitatively show that we robustly\ncapture more detail than related methods."
},{
    "category": "cs.CV", 
    "doi": "10.1109/3DV.2014.46", 
    "link": "http://arxiv.org/pdf/1602.02481v3", 
    "title": "A Large Dataset of Object Scans", 
    "arxiv-id": "1602.02481v3", 
    "author": "Vladlen Koltun", 
    "publish": "2016-02-08T07:20:52Z", 
    "summary": "We have created a dataset of more than ten thousand 3D scans of real objects.\nTo create the dataset, we recruited 70 operators, equipped them with\nconsumer-grade mobile 3D scanning setups, and paid them to scan objects in\ntheir environments. The operators scanned objects of their choosing, outside\nthe laboratory and without direct supervision by computer vision professionals.\nThe result is a large and diverse collection of object scans: from shoes, mugs,\nand toys to grand pianos, construction vehicles, and large outdoor sculptures.\nWe worked with an attorney to ensure that data acquisition did not violate\nprivacy constraints. The acquired data was irrevocably placed in the public\ndomain and is available freely at http://redwood-data.org/3dscan ."
},{
    "category": "cs.CV", 
    "doi": "10.1109/CVPR.2014.537", 
    "link": "http://arxiv.org/pdf/1602.02651v1", 
    "title": "Automatic Face Reenactment", 
    "arxiv-id": "1602.02651v1", 
    "author": "Christian Theobalt", 
    "publish": "2016-02-08T17:05:37Z", 
    "summary": "We propose an image-based, facial reenactment system that replaces the face\nof an actor in an existing target video with the face of a user from a source\nvideo, while preserving the original target performance. Our system is fully\nautomatic and does not require a database of source expressions. Instead, it is\nable to produce convincing reenactment results from a short source video\ncaptured with an off-the-shelf camera, such as a webcam, where the user\nperforms arbitrary facial gestures. Our reenactment pipeline is conceived as\npart image retrieval and part face transfer: The image retrieval is based on\ntemporal clustering of target frames and a novel image matching metric that\ncombines appearance and motion to select candidate frames from the source\nvideo, while the face transfer uses a 2D warping strategy that preserves the\nuser's identity. Our system excels in simplicity as it does not rely on a 3D\nface model, it is robust under head motion and does not require the source and\ntarget performance to be similar. We show convincing reenactment results for\nvideos that we recorded ourselves and for low-quality footage taken from the\nInternet."
},{
    "category": "cs.GR", 
    "doi": "10.1109/CVPR.2014.537", 
    "link": "http://arxiv.org/pdf/1602.03206v2", 
    "title": "Design of false color palettes for grayscale reproduction", 
    "arxiv-id": "1602.03206v2", 
    "author": "Filip A. Sala", 
    "publish": "2016-02-06T17:35:21Z", 
    "summary": "Design of false color palette is quite easy but some effort has to be done to\nachieve good dynamic range, contrast and overall appearance of the palette.\nSuch palettes, for instance, are commonly used in scientific papers for\npresenting the data. However, to lower the cost of the paper most scientists\ndecide to let the data to be printed in grayscale. The same applies to e-book\nreaders based on e-ink where most of them are still grayscale. For majority of\nfalse color palettes reproducing them in grayscale results in ambiguous mapping\nof the colors and may be misleading for the reader. In this article design of\nfalse color palettes suitable for grayscale reproduction is described. Due to\nthe monotonic change of luminance of these palettes grayscale representation is\nvery similar to the data directly presented with a grayscale palette. Some\nsuggestions and examples how to design such palettes are provided."
},{
    "category": "cs.CV", 
    "doi": "10.1109/CVPR.2014.537", 
    "link": "http://arxiv.org/pdf/1602.05256v1", 
    "title": "2D SEM images turn into 3D object models", 
    "arxiv-id": "1602.05256v1", 
    "author": "Wichai Shanklin", 
    "publish": "2016-02-17T00:41:58Z", 
    "summary": "The scanning electron microscopy (SEM) is probably one the most fascinating\nexamination approach that has been used since more than two decades to detailed\ninspection of micro scale objects. Most of the scanning electron microscopes\ncould only produce 2D images that could not assist operational analysis of\nmicroscopic surface properties. Computer vision algorithms combined with very\nadvanced geometry and mathematical approaches turn any SEM into a full 3D\nmeasurement device. This work focuses on a methodical literature review for\nautomatic 3D surface reconstruction of scanning electron microscope images."
},{
    "category": "cs.GR", 
    "doi": "10.1109/CVPR.2014.537", 
    "link": "http://arxiv.org/pdf/1602.06239v1", 
    "title": "On a recursive construction of circular paths and the search for $\u03c0$   on the integer lattice $\\mathbb{Z}^2$", 
    "arxiv-id": "1602.06239v1", 
    "author": "Michelle Rudolph-Lilith", 
    "publish": "2016-02-04T15:54:02Z", 
    "summary": "Digital circles not only play an important role in various technological\nsettings, but also provide a lively playground for more fundamental\nnumber-theoretical questions. In this paper, we present a new recursive\nalgorithm for the construction of digital circles on the integer lattice\n$\\mathbb{Z}^2$, which makes sole use of the signum function. By briefly\nelaborating on the nature of discretization of circular paths, we then find\nthat this algorithm recovers, in a space endowed with $\\ell^1$-norm, the\ndefining constant $\\pi$ of a circle in $\\mathbb{R}^2$."
},{
    "category": "cs.GR", 
    "doi": "10.1109/DICTA.2015.7371249", 
    "link": "http://arxiv.org/pdf/1602.06645v1", 
    "title": "Creating Simplified 3D Models with High Quality Textures", 
    "arxiv-id": "1602.06645v1", 
    "author": "Yang-Wai Chow", 
    "publish": "2016-02-22T04:45:43Z", 
    "summary": "This paper presents an extension to the KinectFusion algorithm which allows\ncreating simplified 3D models with high quality RGB textures. This is achieved\nthrough (i) creating model textures using images from an HD RGB camera that is\ncalibrated with Kinect depth camera, (ii) using a modified scheme to update\nmodel textures in an asymmetrical colour volume that contains a higher number\nof voxels than that of the geometry volume, (iii) simplifying dense polygon\nmesh model using quadric-based mesh decimation algorithm, and (iv) creating and\nmapping 2D textures to every polygon in the output 3D model. The proposed\nmethod is implemented in real-time by means of GPU parallel processing.\nVisualization via ray casting of both geometry and colour volumes provides\nusers with a real-time feedback of the currently scanned 3D model. Experimental\nresults show that the proposed method is capable of keeping the model texture\nquality even for a heavily decimated model and that, when reconstructing small\nobjects, photorealistic RGB textures can still be reconstructed."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2216226", 
    "link": "http://arxiv.org/pdf/1603.00961v1", 
    "title": "Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via   User-Defined Templates", 
    "arxiv-id": "1603.00961v1", 
    "author": "Jan Egger", 
    "publish": "2016-03-03T03:39:59Z", 
    "summary": "Among all types of cancer, gynecological malignancies belong to the 4th most\nfrequent type of cancer among women. Besides chemotherapy and external beam\nradiation, brachytherapy is the standard procedure for the treatment of these\nmalignancies. In the progress of treatment planning, localization of the tumor\nas the target volume and adjacent organs of risks by segmentation is crucial to\naccomplish an optimal radiation distribution to the tumor while simultaneously\npreserving healthy tissue. Segmentation is performed manually and represents a\ntime-consuming task in clinical daily routine. This study focuses on the\nsegmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological\nbrachytherapy. The proposed segmentation method uses an interactive,\ngraph-based segmentation scheme with a user-defined template. The scheme\ncreates a directed two dimensional graph, followed by the minimal cost closed\nset computation on the graph, resulting in an outlining of the rectum. The\ngraphs outline is dynamically adapted to the last calculated cut. Evaluation\nwas performed by comparing manual segmentations of the rectum/sigmoid colon to\nresults achieved with the proposed method. The comparison of the algorithmic to\nmanual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,\nin comparison to 83.97+/-8.08% for the comparison of two manual segmentations\nof the same physician. Utilizing the proposed methodology resulted in a median\ntime of 128 seconds per dataset, compared to 300 seconds needed for pure manual\nsegmentation."
},{
    "category": "cs.MM", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.03482v1", 
    "title": "Predicting Chroma from Luma with Frequency Domain Intra Prediction", 
    "arxiv-id": "1603.03482v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-03-10T22:55:36Z", 
    "summary": "This paper describes a technique for performing intra prediction of the\nchroma planes based on the reconstructed luma plane in the frequency domain.\nThis prediction exploits the fact that while RGB to YUV color conversion has\nthe property that it decorrelates the color planes globally across an image,\nthere is still some correlation locally at the block level. Previous proposals\ncompute a linear model of the spatial relationship between the luma plane (Y)\nand the two chroma planes (U and V). In codecs that use lapped transforms this\nis not possible since transform support extends across the block boundaries and\nthus neighboring blocks are unavailable during intra-prediction. We design a\nfrequency domain intra predictor for chroma that exploits the same local\ncorrelation with lower complexity than the spatial predictor and which works\nwith lapped transforms. We then describe a low-complexity algorithm that\ndirectly uses luma coefficients as a chroma predictor based on gain-shape\nquantization and band partitioning. An experiment is performed that compares\nthese two techniques inside the experimental Daala video codec and shows the\nlower complexity algorithm to be a better chroma predictor."
},{
    "category": "cs.DM", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.04292v1", 
    "title": "A linear algorithm for Brick Wang tiling", 
    "arxiv-id": "1603.04292v1", 
    "author": "Yoshihiro Mizoguchi", 
    "publish": "2016-03-14T14:59:41Z", 
    "summary": "In computer graphics, Wang tiles are used as a tool to generate non periodic\ntextures and patterns. We develop a framework and a method for the tiling\nproblem which is suitable for applications. In particular, we apply our\nmethodology to a special kind of Wang tiles, called Brick Wang tiles,\nintroduced by Derouet-Jourdan et al. in 2015 to model wall patterns. We\ngeneralise their result by providing a linear algorithm to decide and solve the\ntiling problem for arbitrary planar regions with holes."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.06078v2", 
    "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading", 
    "arxiv-id": "1603.06078v2", 
    "author": "Tobias Ritschel", 
    "publish": "2016-03-19T10:29:57Z", 
    "summary": "In computer vision, convolutional neural networks (CNNs) have recently\nachieved new levels of performance for several inverse problems where RGB pixel\nappearance is mapped to attributes such as positions, normals or reflectance.\nIn computer graphics, screen-space shading has recently increased the visual\nquality in interactive image synthesis, where per-pixel attributes such as\npositions, normals or reflectance of a virtual 3D scene are converted into RGB\npixel appearance, enabling effects like ambient occlusion, indirect light,\nscattering, depth-of-field, motion blur, or anti-aliasing. In this paper we\nconsider the diagonal problem: synthesizing appearance from given per-pixel\nattributes using a CNN. The resulting Deep Shading simulates various\nscreen-space effects at competitive quality and speed while not being\nprogrammed by human experts but learned from example images."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.06143v2", 
    "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural   Graphics Programs using Neural Networks", 
    "arxiv-id": "1603.06143v2", 
    "author": "Noah D. Goodman", 
    "publish": "2016-03-19T20:58:47Z", 
    "summary": "Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)\nprovide powerful tools for constraining procedural models in computer graphics,\nbut they require many samples to produce desirable results. In this paper, we\nshow how to create procedural models which learn how to satisfy constraints. We\naugment procedural models with neural networks which control how the model\nmakes random choices based on the output it has generated thus far. We call\nsuch models neurally-guided procedural models. As a pre-computation, we train\nthese models to maximize the likelihood of example outputs generated via SMC.\nThey are then used as efficient SMC importance samplers, generating\nhigh-quality results with very few samples. We evaluate our method on\nL-system-like models with image-based constraints. Given a desired quality\nthreshold, neurally-guided models can generate satisfactory results up to 10x\nfaster than unguided models."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.08551v1", 
    "title": "Genetic cellular neural networks for generating three-dimensional   geometry", 
    "arxiv-id": "1603.08551v1", 
    "author": "Hugo Martay", 
    "publish": "2016-03-28T20:28:09Z", 
    "summary": "There are a number of ways to procedurally generate interesting\nthree-dimensional shapes, and a method where a cellular neural network is\ncombined with a mesh growth algorithm is presented here. The aim is to create a\nshape from a genetic code in such a way that a crude search can find\ninteresting shapes. Identical neural networks are placed at each vertex of a\nmesh which can communicate with neural networks on neighboring vertices. The\noutput of the neural networks determine how the mesh grows, allowing\ninteresting shapes to be produced emergently, mimicking some of the complexity\nof biological organism development. Since the neural networks' parameters can\nbe freely mutated, the approach is amenable for use in a genetic algorithm."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1603.08984v1", 
    "title": "SMASH: Data-driven Reconstruction of Physically Valid Collisions", 
    "arxiv-id": "1603.08984v1", 
    "author": "Niloy J. Mitra", 
    "publish": "2016-03-29T22:18:29Z", 
    "summary": "Collision sequences are commonly used in games and entertainment to add drama\nand excitement. Authoring even two body collisions in real world can be\ndifficult as one has to get timing and the object trajectories to be correctly\nsynchronized. After trial-and-error iterations, when objects can actually be\nmade to collide, then they are difficult to acquire in 3D. In contrast,\nsynthetically generating plausible collisions is difficult as it requires\nadjusting different collision parameters (e.g., object mass ratio, coefficient\nof restitution, etc.) and appropriate initial parameters. We present SMASH to\ndirectly `read off' appropriate collision parameters simply based on input\nvideo recordings. Specifically, we describe how to use laws of rigid body\ncollision to regularize the problem of lifting 2D annotated poses to 3D\nreconstruction of collision sequences. The reconstructed sequences can then be\nmodified and combined to easily author novel and plausible collision sequences.\nWe demonstrate the system on various complex collision sequences."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.01093v3", 
    "title": "BundleFusion: Real-time Globally Consistent 3D Reconstruction using   On-the-fly Surface Re-integration", 
    "arxiv-id": "1604.01093v3", 
    "author": "Christian Theobalt", 
    "publish": "2016-04-05T00:06:39Z", 
    "summary": "Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\nreality and robotic applications. However, scalability brings challenges of\ndrift in pose estimation, introducing significant errors in the accumulated\nmodel. Approaches often require hours of offline processing to globally correct\nmodel errors. Recent online methods demonstrate compelling results, but suffer\nfrom: (1) needing minutes to perform online correction preventing true\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\nresulting in many tracking failures; or (3) supporting only unstructured\npoint-based representations, which limit scan quality and applicability. We\nsystematically address these issues with a novel, real-time, end-to-end\nreconstruction framework. At its core is a robust pose estimation strategy,\noptimizing per frame for a global set of camera poses by considering the\ncomplete history of RGB-D input with an efficient hierarchical approach. We\nremove the heavy reliance on temporal tracking, and continually localize to the\nglobally optimized frames instead. We contribute a parallelizable optimization\nframework, which employs correspondences based on sparse features and dense\ngeometric and photometric matching. Our approach estimates globally optimized\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\nthe 3D model in real-time to ensure global consistency; all within a single\nframework. Our approach outperforms state-of-the-art online systems with\nquality on par to offline methods, but with unprecedented speed and scan\ncompleteness. Our framework leads to a comprehensive online scanning solution\nfor large indoor environments, enabling ease of use and high-quality results."
},{
    "category": "math.NA", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.01910v1", 
    "title": "Nielson-type transfinite triangular interpolants by means of quadratic   energy functional optimizations", 
    "arxiv-id": "1604.01910v1", 
    "author": "\u00c1goston R\u00f3th", 
    "publish": "2016-04-07T08:06:34Z", 
    "summary": "We generalize the transfinite triangular interpolant of (Nielson, 1987) in\norder to generate visually smooth (not necessarily polynomial) local\ninterpolating quasi-optimal triangular spline surfaces. Given as input a\ntriangular mesh stored in a half-edge data structure, at first we produce a\nlocal interpolating network of curves by optimizing quadratic energy\nfunctionals described along the arcs as weighted combinations of squared length\nvariations of first and higher order derivatives, then by optimizing weighted\ncombinations of first and higher order quadratic thin-plate-spline-like\nenergies we locally interpolate each curvilinear face of the previous curve\nnetwork with triangular patches that are usually only $C^0$ continuous along\ntheir common boundaries. In a following step, these local interpolating optimal\ntriangular surface patches are used to construct quasi-optimal continuous\nvector fields of averaged unit normals along the joints, and finally we extend\nthe $G^1$ continuous transfinite triangular interpolation scheme of (Nielson,\n1987) by imposing further optimality constraints concerning the isoparametric\nlines of those groups of three side-vertex interpolants that have to be\nconvexly blended in order to generate the final visually smooth local\ninterpolating quasi-optimal triangular spline surface. While we describe the\nproblem in a general context, we present examples in special polynomial,\ntrigonometric, hyperbolic and algebraic-trigonometric vector spaces of\nfunctions that may be useful both in computer-aided geometric design and in\ncomputer graphics."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.02013v1", 
    "title": "Keyboard Based Control of Four Dimensional Rotations", 
    "arxiv-id": "1604.02013v1", 
    "author": "Akira Kageyama", 
    "publish": "2016-04-06T05:20:04Z", 
    "summary": "Aiming at applications to the scientific visualization of three dimensional\nsimulations with time evolution, a keyboard based control method to specify\nrotations in four dimensions is proposed. It is known that four dimensional\nrotations are generally so-called double rotations, and a double rotation is a\ncombination of simultaneously applied two simple rotations. The proposed method\ncan specify both the simple and double rotations by single key typings of the\nkeyboard. The method is tested in visualizations of a regular pentachoron in\nfour dimensional space by a hyperplane slicing."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.02245v3", 
    "title": "Infrared Colorization Using Deep Convolutional Neural Networks", 
    "arxiv-id": "1604.02245v3", 
    "author": "Hendrik P. A. Lensch", 
    "publish": "2016-04-08T07:10:47Z", 
    "summary": "This paper proposes a method for transferring the RGB color spectrum to\nnear-infrared (NIR) images using deep multi-scale convolutional neural\nnetworks. A direct and integrated transfer between NIR and RGB pixels is\ntrained. The trained model does not require any user guidance or a reference\nimage database in the recall phase to produce images with a natural appearance.\nTo preserve the rich details of the NIR image, its high frequency features are\ntransferred to the estimated RGB image. The presented approach is trained and\nevaluated on a real-world dataset containing a large amount of road scene\nimages in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which\nensures a perfect pixel to pixel registration."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.03755v3", 
    "title": "VConv-DAE: Deep Volumetric Shape Learning Without Object Labels", 
    "arxiv-id": "1604.03755v3", 
    "author": "Mario Fritz", 
    "publish": "2016-04-13T13:14:53Z", 
    "summary": "With the advent of affordable depth sensors, 3D capture becomes more and more\nubiquitous and already has made its way into commercial products. Yet,\ncapturing the geometry or complete shapes of everyday objects using scanning\ndevices (e.g. Kinect) still comes with several challenges that result in noise\nor even incomplete shapes. Recent success in deep learning has shown how to\nlearn complex shape distributions in a data-driven way from large scale 3D CAD\nModel collections and to utilize them for 3D processing on volumetric\nrepresentations and thereby circumventing problems of topology and\ntessellation. Prior work has shown encouraging results on problems ranging from\nshape completion to recognition. We provide an analysis of such approaches and\ndiscover that training as well as the resulting representation are strongly and\nunnecessarily tied to the notion of object labels. Thus, we propose a full\nconvolutional volumetric auto encoder that learns volumetric representation\nfrom noisy data by estimating the voxel occupancy grids. The proposed method\noutperforms prior work on challenging tasks like denoising and shape\ncompletion. We also show that the obtained deep embedding gives competitive\nperformance when used for classification and promising results for shape\ninterpolation."
},{
    "category": "cs.HC", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1604.08239v1", 
    "title": "A Collaborative Untethered Virtual Reality Environment for Interactive   Social Network Visualization", 
    "arxiv-id": "1604.08239v1", 
    "author": "Ken Perlin", 
    "publish": "2016-04-27T20:54:37Z", 
    "summary": "The increasing prevalence of Virtual Reality technologies as a platform for\ngaming and video playback warrants research into how to best apply the current\nstate of the art to challenges in data visualization. Many current VR systems\nare noncollaborative, while data analysis and visualization is often a\nmulti-person process. Our goal in this paper is to address the technical and\nuser experience challenges that arise when creating VR environments for\ncollaborative data visualization. We focus on the integration of multiple\ntracking systems and the new interaction paradigms that this integration can\nenable, along with visual design considerations that apply specifically to\ncollaborative network visualization in virtual reality. We demonstrate a system\nfor collaborative interaction with large 3D layouts of Twitter friend/follow\nnetworks. The system is built by combining a 'Holojam' architecture (multiple\nGearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron\nmotion suits, to offer an untethered, full-room multi-person visualization\nexperience."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.01396v1", 
    "title": "Squares that Look Round: Transforming Spherical Images", 
    "arxiv-id": "1605.01396v1", 
    "author": "Henry Segerman", 
    "publish": "2016-05-04T19:45:32Z", 
    "summary": "We propose M\\\"obius transformations as the natural rotation and scaling tools\nfor editing spherical images. As an application we produce spherical Droste\nimages. We obtain other self-similar visual effects using rational functions,\nelliptic functions, and Schwarz-Christoffel maps."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.01816v3", 
    "title": "Sufficient Conditions for Tuza's Conjecture on Packing and Covering   Triangles", 
    "arxiv-id": "1605.01816v3", 
    "author": "Zhongzheng Tang", 
    "publish": "2016-05-06T04:17:12Z", 
    "summary": "Given a simple graph $G=(V,E)$, a subset of $E$ is called a triangle cover if\nit intersects each triangle of $G$. Let $\\nu_t(G)$ and $\\tau_t(G)$ denote the\nmaximum number of pairwise edge-disjoint triangles in $G$ and the minimum\ncardinality of a triangle cover of $G$, respectively. Tuza conjectured in 1981\nthat $\\tau_t(G)/\\nu_t(G)\\le2$ holds for every graph $G$. In this paper, using a\nhypergraph approach, we design polynomial-time combinatorial algorithms for\nfinding small triangle covers. These algorithms imply new sufficient conditions\nfor Tuza's conjecture on covering and packing triangles. More precisely,\nsuppose that the set $\\mathscr T_G$ of triangles covers all edges in $G$. We\nshow that a triangle cover of $G$ with cardinality at most $2\\nu_t(G)$ can be\nfound in polynomial time if one of the following conditions is satisfied: (i)\n$\\nu_t(G)/|\\mathscr T_G|\\ge\\frac13$, (ii) $\\nu_t(G)/|E|\\ge\\frac14$, (iii)\n$|E|/|\\mathscr T_G|\\ge2$.\n  Keywords: Triangle cover, Triangle packing, Linear 3-uniform hypergraphs,\nCombinatorial algorithms"
},{
    "category": "cs.CG", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.02245v1", 
    "title": "Real-time collision detection method for deformable bodies", 
    "arxiv-id": "1605.02245v1", 
    "author": "Claudio Paglia", 
    "publish": "2016-05-07T20:41:58Z", 
    "summary": "This paper presents a real-time solution for collision detection between\nobjects based on the physics properties. Traditional approaches on collision\ndetection often rely on the geometric relationships that computing the\nintersections between polygons. Such technique is very computationally\nexpensive when applied for deformable objects. As an alternative, we\napproximate the 3D mesh in an spherical surface implicitly. This allows us to\nperform a coarse-level collision detection at extremely fast speed. Then a\ndynamic programming based procedure is applied to identify the collision in\nfine details. Our method demonstrates better prevention to collision tunnelling\nand works more efficiently than the state-of-the-arts."
},{
    "category": "cs.DL", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.06242v1", 
    "title": "Visualization of Publication Impact", 
    "arxiv-id": "1605.06242v1", 
    "author": "Gilles Louppe", 
    "publish": "2016-05-20T08:29:28Z", 
    "summary": "Measuring scholarly impact has been a topic of much interest in recent years.\nWhile many use the citation count as a primary indicator of a publications\nimpact, the quality and impact of those citations will vary. Additionally, it\nis often difficult to see where a paper sits among other papers in the same\nresearch area. Questions we wished to answer through this visualization were:\nis a publication cited less than publications in the field?; is a publication\ncited by high or low impact publications?; and can we visually compare the\nimpact of publications across a result set? In this work we address the above\nquestions through a new visualization of publication impact. Our technique has\nbeen applied to the visualization of citation information in INSPIREHEP\n(http://www.inspirehep.net), the largest high energy physics publication\nrepository."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.07829v2", 
    "title": "As-exact-as-possible repair of unprintable STL files", 
    "arxiv-id": "1605.07829v2", 
    "author": "Marco Attene", 
    "publish": "2016-05-25T11:16:00Z", 
    "summary": "The class of models that can be represented by STL files is larger than the\nclass of models that can be printed using additive manufacturing technologies.\nIn this paper such a gap is formalized while providing an unambiguous\ndescription of all the mathematical entities involved in the modeling-printing\npipeline. Possible defects of an STL file are formally defined and classified,\nand a fully automatic procedure is described to turn \"any\" such file into a\nprintable model. The procedure is as exact as possible, meaning that no visible\ndistortion is introduced unless it is strictly imposed by limitations of the\nprinting device. Thanks to such an unprecedented flexibility and accuracy, this\nalgorithm is expected to significantly simplify the modeling-printing process,\nin particular within the continuously emerging non-professional \"maker\"\ncommunities."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1605.09451v1", 
    "title": "Quantitative Analysis of Saliency Models", 
    "arxiv-id": "1605.09451v1", 
    "author": "Neil Anthony Dodgson", 
    "publish": "2016-05-31T00:33:04Z", 
    "summary": "Previous saliency detection research required the reader to evaluate\nperformance qualitatively, based on renderings of saliency maps on a few\nshapes. This qualitative approach meant it was unclear which saliency models\nwere better, or how well they compared to human perception. This paper provides\na quantitative evaluation framework that addresses this issue. In the first\nquantitative analysis of 3D computational saliency models, we evaluate four\ncomputational saliency models and two baseline models against ground-truth\nsaliency collected in previous work."
},{
    "category": "cs.NA", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1606.02959v3", 
    "title": "Isogeometric computation reuse method for complex objects with   topology-consistent volumetric parameterization", 
    "arxiv-id": "1606.02959v3", 
    "author": "Charlie C. L. Wang", 
    "publish": "2016-06-09T13:27:47Z", 
    "summary": "Volumetric spline parameterization and computational efficiency are two main\nchallenges in isogeometric analysis (IGA). To tackle this problem, we propose a\nframework of computation reuse in IGA on a set of three-dimensional models with\nsimilar semantic features. Given a template domain, B-spline based consistent\nvolumetric parameterization is first constructed for a set of models with\nsimilar semantic features. An efficient quadrature-free method is investigated\nin our framework to compute the entries of stiffness matrix by Bezier\nextraction and polynomial approximation. In our approach, evaluation on the\nstiffness matrix and imposition of the boundary conditions can be pre-computed\nand reused during IGA on a set of CAD models. Examples with complex geometry\nare presented to show the effectiveness of our methods, and efficiency similar\nto the computation in linear finite element analysis can be achieved for IGA\ntaken on a set of models."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1606.05785v1", 
    "title": "Automatic 3D Reconstruction for Symmetric Shapes", 
    "arxiv-id": "1606.05785v1", 
    "author": "Atishay Jain", 
    "publish": "2016-06-18T17:42:27Z", 
    "summary": "Generic 3D reconstruction from a single image is a difficult problem. A lot\nof data loss occurs in the projection. A domain based approach to\nreconstruction where we solve a smaller set of problems for a particular use\ncase lead to greater returns. The project provides a way to automatically\ngenerate full 3-D renditions of actual symmetric images that have some prior\ninformation provided in the pipeline by a recognition algorithm. We provide a\ncritical analysis on how this can be enhanced and improved to provide a general\nreconstruction framework for automatic reconstruction for any symmetric shape."
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1607.04767v1", 
    "title": "Optimized Automatic Code Generation for Geometric Algebra Based   Algorithms with Ray Tracing Application", 
    "arxiv-id": "1607.04767v1", 
    "author": "Ahmad Hosney Awad Eid", 
    "publish": "2016-07-16T16:54:39Z", 
    "summary": "Automatic code generation for low-dimensional geometric algorithms is capable\nof producing efficient low-level software code through a high-level geometric\ndomain specific language. Geometric Algebra (GA) is one of the most suitable\nalgebraic systems for being the base for such code generator. This work\npresents an attempt at realizing such idea in practice. A novel GA-based\ngeometric code generator, called GMac, is proposed. Comparisons to similar\nGA-based code generators are provided. The possibility of fully benefiting from\nthe symbolic power of GA while obtaining good performance and maintainability\nof software implementations is illustrated through a ray tracing application."
},{
    "category": "cs.DM", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1607.06138v2", 
    "title": "Dappled tiling", 
    "arxiv-id": "1607.06138v2", 
    "author": "Hiroyuki Ochiai", 
    "publish": "2016-07-20T22:24:10Z", 
    "summary": "We consider a certain tiling problem of a planar region in which there are no\nlong horizontal or vertical strips consisting of copies of the same tile.\nIntuitively speaking, we would like to create a dappled pattern with two or\nmore kinds of tiles. We give an efficient algorithm to turn any tiling into one\nsatisfying the condition, and discuss its applications in texturing."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1607.07980v1", 
    "title": "How2Sketch: Generating Easy-To-Follow Tutorials for Sketching 3D Objects", 
    "arxiv-id": "1607.07980v1", 
    "author": "Niloy J. Mitra", 
    "publish": "2016-07-27T06:55:22Z", 
    "summary": "Accurately drawing 3D objects is difficult for untrained individuals, as it\nrequires an understanding of perspective and its effects on geometry and\nproportions. Step-by-step tutorials break the complex task of sketching an\nentire object down into easy-to-follow steps that even a novice can follow.\nHowever, creating such tutorials requires expert knowledge and is a\ntime-consuming task. As a result, the availability of tutorials for a given\nobject or viewpoint is limited. How2Sketch addresses this problem by\nautomatically generating easy-to-follow tutorials for arbitrary 3D objects.\nGiven a segmented 3D model and a camera viewpoint,it computes a sequence of\nsteps for constructing a drawing scaffold comprised of geometric primitives,\nwhich helps the user draw the final contours in correct perspective and\nproportion. To make the drawing scaffold easy to construct, the algorithm\nsolves for an ordering among the scaffolding primitives and explicitly makes\nsmall geometric modifications to the size and location of the object parts to\nsimplify relative positioning. Technically, we formulate this scaffold\nconstruction as a single selection problem that simultaneously solves for the\nordering and geometric changes of the primitives. We demonstrate our algorithm\nfor generating tutorials on a variety of man-made objects and evaluate how\neasily the tutorials can be followed with a user study."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1607.08874v1", 
    "title": "3D visualization of astronomy data cubes using immersive displays", 
    "arxiv-id": "1607.08874v1", 
    "author": "Pourang Irani", 
    "publish": "2016-07-29T17:39:31Z", 
    "summary": "We report on an exploratory project aimed at performing immersive 3D\nvisualization of astronomical data, starting with spectral-line radio data\ncubes from galaxies. This work is done as a collaboration between the\nDepartment of Physics and Astronomy and the Department of Computer Science at\nthe University of Manitoba. We are building our prototype using the 3D engine\nUnity, because of its ease of use for integration with advanced displays such\nas a CAVE environment, a zSpace tabletop, or virtual reality headsets. We\naddress general issues regarding 3D visualization, such as: load and convert\nastronomy data, perform volume rendering on the GPU, and produce physically\nmeaningful visualizations using principles of visual literacy. We discuss some\nchallenges to be met when designing a user interface that allows us to take\nadvantage of this new way of exploring data. We hope to lay the foundations for\nan innovative framework useful for all astronomers who use spectral line data\ncubes, and encourage interested parties to join our efforts. This pilot project\naddresses the challenges presented by frontier astronomy experiments, such as\nthe Square Kilometre Array and its precursors."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2080837", 
    "link": "http://arxiv.org/pdf/1608.00921v1", 
    "title": "Registration of Volumetric Prostate Scans using Curvature Flow", 
    "arxiv-id": "1608.00921v1", 
    "author": "Arie Kaufman", 
    "publish": "2016-08-02T18:15:34Z", 
    "summary": "Radiological imaging of the prostate is becoming more popular among\nresearchers and clinicians in searching for diseases, primarily cancer. Scans\nmight be acquired with different equipment or at different times for prognosis\nmonitoring, with patient movement between scans, resulting in multiple datasets\nthat need to be registered. For these cases, we introduce a method for\nvolumetric registration using curvature flow. Multiple prostate datasets are\nmapped to canonical solid spheres, which are in turn aligned and registered\nthrough the use of identified landmarks on or within the gland. Theoretical\nproof and experimental results show that our method produces homeomorphisms\nwith feature constraints. We provide thorough validation of our method by\nregistering prostate scans of the same patient in different orientations, from\ndifferent days and using different modes of MRI. Our method also provides the\nfoundation for a general group-wise registration using a standard reference,\ndefined on the complex plane, for any input. In the present context, this can\nbe used for registering as many scans as needed for a single patient or\ndifferent patients on the basis of age, weight or even malignant and\nnon-malignant attributes to study the differences in general population. Though\nwe present this technique with a specific application to the prostate, it is\ngenerally applicable for volumetric registration problems."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2217003", 
    "link": "http://arxiv.org/pdf/1608.00936v4", 
    "title": "Multimodal Brain Visualization", 
    "arxiv-id": "1608.00936v4", 
    "author": "Arie Kaufman", 
    "publish": "2016-08-02T19:02:40Z", 
    "summary": "Current connectivity diagrams of human brain image data are either overly\ncomplex or overly simplistic. In this work we introduce simple yet accurate\ninteractive visual representations of multiple brain image structures and the\nconnectivity among them. We map cortical surfaces extracted from human brain\nmagnetic resonance imaging (MRI) data onto 2D surfaces that preserve shape\n(angle), extent (area), and spatial (neighborhood) information for 2D (circular\ndisk) and 3D (spherical) mapping, split these surfaces into separate patches,\nand cluster functional and diffusion tractography MRI connections between pairs\nof these patches. The resulting visualizations are easier to compute on and\nmore visually intuitive to interact with than the original data, and facilitate\nsimultaneous exploration of multiple data sets, modalities, and statistical\nmaps."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1608.02829v1", 
    "title": "Semi-Automated SVG Programming via Direct Manipulation", 
    "arxiv-id": "1608.02829v1", 
    "author": "Ravi Chugh", 
    "publish": "2016-08-09T15:17:46Z", 
    "summary": "Direct manipulation interfaces provide intuitive and interactive features to\na broad range of users, but they often exhibit two limitations: the built-in\nfeatures cannot possibly cover all use cases, and the internal representation\nof the content is not readily exposed. We believe that if direct manipulation\ninterfaces were to (a) use general-purpose programs as the representation\nformat, and (b) expose those programs to the user, then experts could customize\nthese systems in powerful new ways and non-experts could enjoy some of the\nbenefits of programmable systems.\n  In recent work, we presented a prototype SVG editor called Sketch-n-Sketch\nthat offered a step towards this vision. In that system, the user wrote a\nprogram in a general-purpose lambda-calculus to generate a graphic design and\ncould then directly manipulate the output to indirectly change design\nparameters (i.e. constant literals) in the program in real-time during the\nmanipulation. Unfortunately, the burden of programming the desired\nrelationships rested entirely on the user.\n  In this paper, we design and implement new features for Sketch-n-Sketch that\nassist in the programming process itself. Like typical direct manipulation\nsystems, our extended Sketch-n-Sketch now provides GUI-based tools for drawing\nshapes, relating shapes to each other, and grouping shapes together. Unlike\ntypical systems, however, each tool carries out the user's intention by\ntransforming their general-purpose program. This novel, semi-automated\nprogramming workflow allows the user to rapidly create high-level, reusable\nabstractions in the program while at the same time retaining direct\nmanipulation capabilities. In future work, our approach may be extended with\nmore graphic design features or realized for other application domains."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1608.03898v1", 
    "title": "Curvature transformation", 
    "arxiv-id": "1608.03898v1", 
    "author": "Dimitris Vartziotis", 
    "publish": "2016-07-22T12:52:09Z", 
    "summary": "A transformation based on mean curvature is introduced which morphs\ntriangulated surfaces into round spheres."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1608.04953v1", 
    "title": "A Perceptual Aesthetics Measure for 3D Shapes", 
    "arxiv-id": "1608.04953v1", 
    "author": "Ligang Liu", 
    "publish": "2016-08-17T13:07:27Z", 
    "summary": "While the problem of image aesthetics has been well explored, the study of 3D\nshape aesthetics has focused on specific manually defined features. In this\npaper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel\ndata and without manually-crafted features by leveraging the strength of deep\nlearning. We collect data from humans on their aesthetics preferences for\nvarious 3D shape classes. We take a deep convolutional 3D shape ranking\napproach to compute a measure that gives an aesthetics score for a 3D shape. We\ndemonstrate our approach with various types of shapes and for applications such\nas aesthetics-based visualization, search, and scene composition."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1608.06368v3", 
    "title": "Segmenting a Surface Mesh into Pants Using Morse Theory", 
    "arxiv-id": "1608.06368v3", 
    "author": "Xin Li", 
    "publish": "2016-08-23T03:14:13Z", 
    "summary": "A pair of pants is a genus zero orientable surface with three boundary\ncomponents. A pants decomposition of a surface is a finite collection of\nunordered pairwise disjoint simple closed curves embedded in the surface that\ndecompose the surface into pants. In this paper we present two Morse theory\nbased algorithms for pants decomposition of a surface mesh. Both algorithms\noperates on a choice of an appropriate Morse function on the surface. The first\nalgorithm uses this Morse function to identify handles that are glued\nsystematically to obtain a pant decomposition. The second algorithm uses the\nReeb graph of the Morse function to obtain a pant decomposition. Both\nalgorithms work for surfaces with or without boundaries. Our preliminary\nimplementation of the two algorithms shows that both algorithms run in much\nless time than an existing state-of-the-art method, and the Reeb graph based\nalgorithm achieves the best time efficiency. Finally, we demonstrate the\nrobustness of our algorithms against noise."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1609.00754v1", 
    "title": "A heuristic extending the Squarified treemapping algorithm", 
    "arxiv-id": "1609.00754v1", 
    "author": "Mario Torre", 
    "publish": "2016-09-02T21:47:47Z", 
    "summary": "A heuristic extending the Squarified Treemap technique for the representation\nof hierarchical information as treemaps is presented. The original technique\ngives high quality treemap views, since items are laid out with rectangles that\napproximate squares, allowing easy comparison and selection operations. New key\nsteps, with a low computational impact, have been introduced to yield treemaps\nwith even better aspect ratios and higher homogeneity among items."
},{
    "category": "physics.med-ph", 
    "doi": "10.1145/2984511.2984575", 
    "link": "http://arxiv.org/pdf/1609.00958v1", 
    "title": "Efficient ray tracing on 3D regular grids for fast generation of   digitally reconstructed radiographs in iterative tomographic reconstruction   techniques", 
    "arxiv-id": "1609.00958v1", 
    "author": "Jonas Dittmann", 
    "publish": "2016-09-04T16:46:30Z", 
    "summary": "Cone beam projection is an essential and particularly time consuming part of\nany iterative tomographic reconstruction algorithm. On current graphics\nhardware especially the amount and pattern of memory accesses is a limiting\nfactor when read-only textures cannot be used. With the final objective of\naccelerating iterative reconstruction techniques, a non-oversampling\nJoseph-like raytracing projection algorithm for three dimensions featuring both\na branchless sampling loop and a cache friendly memory access pattern is\npresented. An interpretation of the employed interpolation scheme is given with\nrespect to the effective beam and voxel models implied. The method is further\ncompared to existing techniques, and the modifications required to implement\nfurther voxel and beam shape models are outlined. Both memory access rates and\ntotal run time are benchmarked on a current consumer grade graphics processing\nunit and explicitly compared to the performance of a classic Digital\nDifferential Analyzer (DDA) algorithm. The presented raytracer achieves memory\naccess rates of 292 GB/s in read-and-write memory and 502 GB/s in read-only\ntexture memory. It outperforms the DDA in terms of total run time by a factor\nof up to five and achives 170 to 300 projections of a $512^{3}$ voxel volume\nper second."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-45886-1_35", 
    "link": "http://arxiv.org/pdf/1609.01499v1", 
    "title": "Depth Estimation Through a Generative Model of Light Field Synthesis", 
    "arxiv-id": "1609.01499v1", 
    "author": "Michael Hirsch", 
    "publish": "2016-09-06T11:43:08Z", 
    "summary": "Light field photography captures rich structural information that may\nfacilitate a number of traditional image processing and computer vision tasks.\nA crucial ingredient in such endeavors is accurate depth recovery. We present a\nnovel framework that allows the recovery of a high quality continuous depth map\nfrom light field data. To this end we propose a generative model of a light\nfield that is fully parametrized by its corresponding depth map. The model\nallows for the integration of powerful regularization techniques such as a\nnon-local means prior, facilitating accurate depth map estimation."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.02974v1", 
    "title": "Learning-Based View Synthesis for Light Field Cameras", 
    "arxiv-id": "1609.02974v1", 
    "author": "Ravi Ramamoorthi", 
    "publish": "2016-09-09T23:33:38Z", 
    "summary": "With the introduction of consumer light field cameras, light field imaging\nhas recently become widespread. However, there is an inherent trade-off between\nthe angular and spatial resolution, and thus, these cameras often sparsely\nsample in either spatial or angular domain. In this paper, we use machine\nlearning to mitigate this trade-off. Specifically, we propose a novel\nlearning-based approach to synthesize new views from a sparse set of input\nviews. We build upon existing view synthesis techniques and break down the\nprocess into disparity and color estimation components. We use two sequential\nconvolutional neural networks to model these two components and train both\nnetworks simultaneously by minimizing the error between the synthesized and\nground truth images. We show the performance of our approach using only four\ncorner sub-aperture views from the light fields captured by the Lytro Illum\ncamera. Experimental results show that our approach synthesizes high-quality\nimages that are superior to the state-of-the-art techniques on a variety of\nchallenging real-world scenes. We believe our method could potentially decrease\nthe required angular resolution of consumer light field cameras, which allows\ntheir spatial resolution to increase."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.02994v1", 
    "title": "Simultaneous independent image display technique on multiple 3D objects", 
    "arxiv-id": "1609.02994v1", 
    "author": "Shinsaku Hiura", 
    "publish": "2016-09-10T02:16:51Z", 
    "summary": "We propose a new system to visualize depth-dependent patterns and images on\nsolid objects with complex geometry using multiple projectors. The system,\ndespite consisting of conventional passive LCD projectors, is able to project\ndifferent images and patterns depending on the spatial location of the object.\nThe technique is based on the simple principle that multiple patterns projected\nfrom multiple projectors interfere constructively with each other when their\npatterns are projected on the same object. Previous techniques based on the\nsame principle can only achieve 1) low resolution volume colorization or 2)\nhigh resolution images but only on a limited number of flat planes. In this\npaper, we discretize a 3D object into a number of 3D points so that high\nresolution images can be projected onto the complex shapes. We also propose a\ndynamic ranges expansion technique as well as an efficient optimization\nprocedure based on epipolar constraints.\n  Such technique can be used to the extend projection mapping to have spatial\ndependency, which is desirable for practical applications. We also demonstrate\nthe system potential as a visual instructor for object placement and\nassembling. Experiments prove the effectiveness of our method."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.05268v1", 
    "title": "High-Dimensional Data Visualization by Interactive Construction of   Low-Dimensional Parallel Coordinate Plots", 
    "arxiv-id": "1609.05268v1", 
    "author": "Jinman Kim", 
    "publish": "2016-09-17T01:45:11Z", 
    "summary": "Parallel coordinate plots (PCPs) are among the most useful techniques for the\nvisualization and exploration of high-dimensional data spaces. They are\nespecially useful for the representation of correlations among the dimensions,\nwhich identify relationships and interdependencies between variables. However,\nwithin these high-dimensional spaces, PCPs face difficulties in displaying the\ncorrelation between combinations of dimensions and generally require additional\ndisplay space as the number of dimensions increases. In this paper, we present\na new technique for high-dimensional data visualization in which a set of\nlow-dimensional PCPs are interactively constructed by sampling user-selected\nsubsets of the high-dimensional data space. In our technique, we first\nconstruct a graph visualization of sets of well-correlated dimensions. Users\nobserve this graph and are able to interactively select the dimensions by\nsampling from its cliques, thereby dynamically specifying the most relevant\nlower dimensional data to be used for the construction of focused PCPs. Our\ninteractive sampling overcomes the shortcomings of the PCPs by enabling the\nvisualization of the most meaningful dimensions (i.e., the most relevant\ninformation) from high-dimensional spaces. We demonstrate the effectiveness of\nour technique through two case studies, where we show that the proposed\ninteractive low-dimensional space constructions were pivotal for visualizing\nthe high-dimensional data and discovering new patterns."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.05283v2", 
    "title": "Converting Basic D3 Charts into Reusable Style Templates", 
    "arxiv-id": "1609.05283v2", 
    "author": "Maneesh Agrawala", 
    "publish": "2016-09-17T05:08:24Z", 
    "summary": "We present a technique for converting a basic D3 chart into a reusable style\ntemplate. Then, given a new data source we can apply the style template to\ngenerate a chart that depicts the new data, but in the style of the template.\nTo construct the style template we first deconstruct the input D3 chart to\nrecover its underlying structure: the data, the marks and the mappings that\ndescribe how the marks encode the data. We then rank the perceptual\neffectiveness of the deconstructed mappings. To apply the resulting style\ntemplate to a new data source we first obtain importance ranks for each new\ndata field. We then adjust the template mappings to depict the source data by\nmatching the most important data fields to the most perceptually effective\nmappings. We show how the style templates can be applied to source data in the\nform of either a data table or another D3 chart. While our implementation\nfocuses on generating templates for basic chart types (e.g. variants of bar\ncharts, line charts, dot plots, scatterplots, etc.), these are the most\ncommonly used chart types today. Users can easily find such basic D3 charts on\nthe Web, turn them into templates, and immediately see how their own data would\nlook in the visual style (e.g. colors, shapes, fonts, etc.) of the templates.\nWe demonstrate the effectiveness of our approach by applying a diverse set of\nstyle templates to a variety of source datasets."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.05328v1", 
    "title": "Hermite interpolation by piecewise polynomial surfaces with polynomial   area element", 
    "arxiv-id": "1609.05328v1", 
    "author": "Jan Vr\u0161ek", 
    "publish": "2016-09-17T12:54:41Z", 
    "summary": "This paper is devoted to the construction of polynomial 2-surfaces which\npossess a polynomial area element. In particular we study these surfaces in the\nEuclidean space $\\mathbb R^3$ (where they are equivalent to the PN surfaces)\nand in the Minkowski space $\\mathbb R^{3,1}$ (where they provide the MOS\nsurfaces). We show generally in real vector spaces of any dimension and any\nmetric that the Gram determinant of a parametric set of subspaces is a perfect\nsquare if and only if the Gram determinant of its orthogonal complement is a\nperfect square. Consequently the polynomial surfaces of a given degree with\npolynomial area element can be constructed from the prescribed normal fields\nsolving a system of linear equations. The degree of the constructed surface\ndepending on the degree and the quality of the prescribed normal field is\ninvestigated and discussed. We use the presented approach to interpolate a\nnetwork of points and associated normals with piecewise polynomial surfaces\nwith polynomial area element and demonstrate our method on a number of examples\n(constructions of quadrilateral as well as triangular patches"
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.06536v1", 
    "title": "Facial Performance Capture with Deep Neural Networks", 
    "arxiv-id": "1609.06536v1", 
    "author": "Jaakko Lehtinen", 
    "publish": "2016-09-21T12:55:59Z", 
    "summary": "We present a deep learning technique for facial performance capture, i.e.,\nthe transfer of video footage into a motion sequence of a 3D mesh representing\nan actor's face. Specifically, we build on a conventional capture pipeline\nbased on computer vision and multi-view video, and use its results to train a\ndeep neural network to produce similar output from a monocular video sequence.\nOnce trained, our network produces high-quality results for unseen inputs with\ngreatly reduced effort compared to the conventional system.\n  In practice, we have found that approximately 10 minutes worth of\nhigh-quality data is sufficient for training a network that can then\nautomatically process as much footage from video to 3D as needed. This yields\nmajor savings in the development of modern narrative-driven video games\ninvolving digital doubles of actors and potentially hours of animated dialogue\nper character."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.07049v1", 
    "title": "Customized Facial Constant Positive Air Pressure (CPAP) Masks", 
    "arxiv-id": "1609.07049v1", 
    "author": "Ron Kimmel", 
    "publish": "2016-09-22T16:11:57Z", 
    "summary": "Sleep apnea is a syndrome that is characterized by sudden breathing halts\nwhile sleeping. One of the common treatments involves wearing a mask that\ndelivers continuous air flow into the nostrils so as to maintain a steady air\npressure. These masks are designed for an average facial model and are often\ndifficult to adjust due to poor fit to the actual patient. The incompatibility\nis characterized by gaps between the mask and the face, which deteriorates the\nimpermeability of the mask and leads to air leakage. We suggest a fully\nautomatic approach for designing a personalized nasal mask interface using a\nfacial depth scan. The interfaces generated by the proposed method accurately\nfit the geometry of the scanned face, and are easy to manufacture. The proposed\nmethod utilizes cheap commodity depth sensors and 3D printing technologies to\nefficiently design and manufacture customized masks for patients suffering from\nsleep apnea."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1609.07738v1", 
    "title": "Fast Blended Transformations for Partial Shape Registration", 
    "arxiv-id": "1609.07738v1", 
    "author": "Ron Kimmel", 
    "publish": "2016-09-25T13:20:58Z", 
    "summary": "Automatic estimation of skinning transformations is a popular way to deform a\nsingle reference shape into a new pose by providing a small number of control\nparameters. We generalize this approach by efficiently enabling the use of\nmultiple exemplar shapes. Using a small set of representative natural poses, we\npropose to express an unseen appearance by a low-dimensional linear subspace,\nspecified by a redundant dictionary of weighted vertex positions. Minimizing a\nnonlinear functional that regulates the example manifold, the suggested\napproach supports local-rigid deformations of articulated objects, as well as\nnearly isometric embeddings of smooth shapes. A real-time non-rigid deformation\nsystem is demonstrated, and a shape completion and partial registration\nframework is introduced. These applications can recover a target pose and\nimplicit inverse kinematics from a small number of examples and just a few\nvertex positions. The result reconstruction is more accurate compared to\nstate-of-the-art reduced deformable models."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.01691v1", 
    "title": "Towards a Drone Cinematographer: Guiding Quadrotor Cameras using Visual   Composition Principles", 
    "arxiv-id": "1610.01691v1", 
    "author": "Pat Hanrahan", 
    "publish": "2016-10-05T23:49:21Z", 
    "summary": "We present a system to capture video footage of human subjects in the real\nworld. Our system leverages a quadrotor camera to automatically capture\nwell-composed video of two subjects. Subjects are tracked in a large-scale\noutdoor environment using RTK GPS and IMU sensors. Then, given the tracked\nstate of our subjects, our system automatically computes static shots based on\nwell-established visual composition principles and canonical shots from\ncinematography literature. To transition between these static shots, we\ncalculate feasible, safe, and visually pleasing transitions using a novel\nreal-time trajectory planning algorithm. We evaluate the performance of our\ntracking system, and experimentally show that RTK GPS significantly outperforms\nconventional GPS in capturing a variety of canonical shots. Lastly, we\ndemonstrate our system guiding a consumer quadrotor camera autonomously\ncapturing footage of two subjects in a variety of use cases. This is the first\nend-to-end system that enables people to leverage the mobility of quadrotors,\nas well as the knowledge of expert filmmakers, to autonomously capture\nhigh-quality footage of people in the real world."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.04141v1", 
    "title": "Scale Stain: Multi-Resolution Feature Enhancement in Pathology   Visualization", 
    "arxiv-id": "1610.04141v1", 
    "author": "Claes Lundstr\u00f6m", 
    "publish": "2016-10-13T15:51:58Z", 
    "summary": "Digital whole-slide images of pathological tissue samples have recently\nbecome feasible for use within routine diagnostic practice. These gigapixel\nsized images enable pathologists to perform reviews using computer workstations\ninstead of microscopes. Existing workstations visualize scanned images by\nproviding a zoomable image space that reproduces the capabilities of the\nmicroscope. This paper presents a novel visualization approach that enables\nfiltering of the scale-space according to color preference. The visualization\nmethod reveals diagnostically important patterns that are otherwise not\nvisible. The paper demonstrates how this approach has been implemented into a\nfully functional prototype that lets the user navigate the visualization\nparameter space in real time. The prototype was evaluated for two common\nclinical tasks with eight pathologists in a within-subjects study. The data\nreveal that task efficiency increased by 15% using the prototype, with\nmaintained accuracy. By analyzing behavioral strategies, it was possible to\nconclude that efficiency gain was caused by a reduction of the panning needed\nto perform systematic search of the images. The prototype system was well\nreceived by the pathologists who did not detect any risks that would hinder use\nin clinical routine."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.04861v2", 
    "title": "Digital Makeup from Internet Images", 
    "arxiv-id": "1610.04861v2", 
    "author": "Ligang Liu", 
    "publish": "2016-10-16T13:47:18Z", 
    "summary": "We present a novel approach of color transfer between images by exploring\ntheir high-level semantic information. First, we set up a database which\nconsists of the collection of downloaded images from the internet, which are\nsegmented automatically by using matting techniques. We then, extract image\nforegrounds from both source and multiple target images. Then by using image\nmatting algorithms, the system extracts the semantic information such as faces,\nlips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source\nimage. And, then the color is transferred between corresponding parts with the\nsame semantic information. Next we get the color transferred result by\nseamlessly compositing different parts together using alpha blending. In the\nfinal step, we present an efficient method of color consistency to optimize the\ncolor of a collection of images showing the common scene. The main advantage of\nour method over existing techniques is that it does not need face matching, as\none could use more than one target images. It is not restricted to head shot\nimages as we can also change the color style in the wild. Moreover, our\nalgorithm does not require to choose the same color style, same pose and image\nsize between source and target images. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different parts in the source image. Comparing with\nother approaches, our algorithm is much better in color blending in the input\ndata."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.04936v1", 
    "title": "Partial Procedural Geometric Model Fitting for Point Clouds", 
    "arxiv-id": "1610.04936v1", 
    "author": "Cheng Wang", 
    "publish": "2016-10-17T00:47:36Z", 
    "summary": "Geometric model fitting is a fundamental task in computer graphics and\ncomputer vision. However, most geometric model fitting methods are unable to\nfit an arbitrary geometric model (e.g. a surface with holes) to incomplete\ndata, due to that the similarity metrics used in these methods are unable to\nmeasure the rigid partial similarity between arbitrary models. This paper hence\nproposes a novel rigid geometric similarity metric, which is able to measure\nboth the full similarity and the partial similarity between arbitrary geometric\nmodels. The proposed metric enables us to perform partial procedural geometric\nmodel fitting (PPGMF). The task of PPGMF is to search a procedural geometric\nmodel space for the model rigidly similar to a query of non-complete point set.\nModels in the procedural model space are generated according to a set of\nparametric modeling rules. A typical query is a point cloud. PPGMF is very\nuseful as it can be used to fit arbitrary geometric models to non-complete\n(incomplete, over-complete or hybrid-complete) point cloud data. For example,\nmost laser scanning data is non-complete due to occlusion. Our PPGMF method\nuses Markov chain Monte Carlo technique to optimize the proposed similarity\nmetric over the model space. To accelerate the optimization process, the method\nalso employs a novel coarse-to-fine model dividing strategy to reject\ndissimilar models in advance. Our method has been demonstrated on a variety of\ngeometric models and non-complete data. Experimental results show that the\nPPGMF method based on the proposed metric is able to fit non-complete data,\nwhile the method based on other metrics is unable. It is also shown that our\nmethod can be accelerated by several times via early rejection."
},{
    "category": "math.NA", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.05351v1", 
    "title": "Spline surfaces with T-junctions", 
    "arxiv-id": "1610.05351v1", 
    "author": "J\u00f6rg Peters", 
    "publish": "2016-10-17T20:46:12Z", 
    "summary": "T-junctions support merging or spreading out feature lines. This paper\ndevelops a new way to create smooth piecewise polynomial free-form spline\nsurfaces that include T-junctions. The construction is based on varying the\nparameterization and therefore does not require the non-local coordination of\nknot intervals. Conversely, a simple example shows that some T-junctions do not\nadmit C 1 hierarchical splines and T-splines in particular. Our recommended G 1\ncap of the T-junction consists of two pieces of degree bi-4 framed by bi-cubic\nsplines. Numerous experiments show good highlight line distribution where\nalternatives, such as Catmull-Clark subdivision, fail and hierarchical splines\ndo not apply."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1610.09992v1", 
    "title": "Deconfliction and Surface Generation from Bathymetry Data Using LR   B-splines", 
    "arxiv-id": "1610.09992v1", 
    "author": "Heidi E. I. Dahl", 
    "publish": "2016-10-31T16:08:42Z", 
    "summary": "A set of bathymetry point clouds acquired by different measurement techniques\nat different times, having different accuracy and varying patterns of points,\nare approximated by an LR B-spline surface. The aim is to represent the sea\nbottom with good accuracy and at the same time reduce the data size\nconsiderably. In this process the point clouds must be cleaned by selecting the\n\"best\" points for surface generation. This cleaning process is called\ndeconfliction, and we use a rough approximation of the combined point clouds as\na reference surface to select a consistent set of points. The reference surface\nis updated with the selected points to create an accurate approximation. LR\nB-splines is the selected surface format due to its suitability for adaptive\nrefinement and approximation, and its ability to represent local detail without\na global increase in the data size of the surface"
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1611.00939v1", 
    "title": "Recent Advances in Transient Imaging: A Computer Graphics and Vision   Perspective", 
    "arxiv-id": "1611.00939v1", 
    "author": "Diego Gutierrez", 
    "publish": "2016-11-03T10:11:10Z", 
    "summary": "Transient imaging has recently made a huge impact in the computer graphics\nand computer vision fields. By capturing, reconstructing, or simulating light\ntransport at extreme temporal resolutions, researchers have proposed novel\ntechniques to show movies of light in motion, see around corners, detect\nobjects in highly-scattering media, or infer material properties from a\ndistance, to name a few. The key idea is to leverage the wealth of information\nin the temporal domain at the pico or nanosecond resolution, information\nusually lost during the capture-time temporal integration. This paper presents\nrecent advances in this field of transient imaging from a graphics and vision\nperspective, including capture techniques, analysis, applications and\nsimulation."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1611.01990v1", 
    "title": "Elliptic operator for shape analysis", 
    "arxiv-id": "1611.01990v1", 
    "author": "Ron Kimmel", 
    "publish": "2016-11-07T11:11:10Z", 
    "summary": "Many shape analysis methods treat the geometry of an object as a metric space\nthat can be captured by the Laplace-Beltrami operator. In this paper, we\npropose to adapt a classical operator from quantum mechanics to the field of\nshape analysis where we suggest to integrate a scalar function through a\nunified elliptical Hamiltonian operator. We study the addition of a potential\nfunction to the Laplacian as a generator for dual spaces in which shape\nprocessing is performed. Then, we evaluate the resulting spectral basis for\ndifferent applications such as mesh compression and shape matching. The\nsuggested operator is shown to produce better functional spaces to operate\nwith, as demonstrated by the proposed framework that outperforms existing\nspectral methods, for example, when applied to shape matching benchmarks."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1611.02147v1", 
    "title": "Error-Bounded and Feature Preserving Surface Remeshing with Minimal   Angle Improvement", 
    "arxiv-id": "1611.02147v1", 
    "author": "Bedrich Benes", 
    "publish": "2016-11-07T16:12:08Z", 
    "summary": "The typical goal of surface remeshing consists in finding a mesh that is (1)\ngeometrically faithful to the original geometry, (2) as coarse as possible to\nobtain a low-complexity representation and (3) free of bad elements that would\nhamper the desired application. In this paper, we design an algorithm to\naddress all three optimization goals simultaneously. The user specifies desired\nbounds on approximation error {\\delta}, minimal interior angle {\\theta} and\nmaximum mesh complexity N (number of vertices). Since such a desired mesh might\nnot even exist, our optimization framework treats only the approximation error\nbound {\\delta} as a hard constraint and the other two criteria as optimization\ngoals. More specifically, we iteratively perform carefully prioritized local\noperators, whenever they do not violate the approximation error bound and\nimprove the mesh otherwise. In this way our optimization framework greedily\nsearches for the coarsest mesh with minimal interior angle above {\\theta} and\napproximation error bounded by {\\delta}. Fast runtime is enabled by a local\napproximation error estimation, while implicit feature preservation is obtained\nby specifically designed vertex relocation operators. Experiments show that our\napproach delivers high-quality meshes with implicitly preserved features and\nbetter balances between geometric fidelity, mesh complexity and element quality\nthan the state-of-the-art."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1611.03666v1", 
    "title": "Oriented bounding boxes using multiresolution contours for fast   interference detection of arbitrary geometry objects", 
    "arxiv-id": "1611.03666v1", 
    "author": "P. C. P. Carvalho", 
    "publish": "2016-11-11T11:50:59Z", 
    "summary": "Interference detection of arbitrary geometric objects is not a trivial task\ndue to the heavy computational load imposed by implementation issues. The\nhierarchically structured bounding boxes help us to quickly isolate the contour\nof segments in interference. In this paper, a new approach is introduced to\ntreat the interference detection problem involving the representation of\narbitrary shaped objects. Our proposed method relies upon searching for the\nbest possible way to represent contours by means of hierarchically structured\nrectangular oriented bounding boxes. This technique handles 2D objects\nboundaries defined by closed B-spline curves with roughness details. Each\noriented box is adapted and fitted to the segments of the contour using second\norder statistical indicators from some elements of the segments of the object\ncontour in a multiresolution framework. Our method is efficient and robust when\nit comes to 2D animations in real time. It can deal with smooth curves and\npolygonal approximations as well results are present to illustrate the\nperformance of the new method."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2980179.2980251", 
    "link": "http://arxiv.org/pdf/1611.08841v1", 
    "title": "Long-Term Image Boundary Extrapolation", 
    "arxiv-id": "1611.08841v1", 
    "author": "Mario Fritz", 
    "publish": "2016-11-27T13:45:14Z", 
    "summary": "Boundary prediction in images and videos has been a very active topic of\nresearch and organizing visual information into boundaries and segments is\nbelieved to be a corner stone of visual perception. While prior work has\nfocused on predicting boundaries for observed frames, our work aims at\npredicting boundaries of future unobserved frames. This requires our model to\nlearn about the fate of boundaries and extrapolate motion patterns. We\nexperiment on established real-world video segmentation dataset, which provides\na testbed for this new task. We show for the first time spatio-temporal\nboundary extrapolation, that in contrast to prior work on RGB extrapolation\nmaintains a crisp result. Furthermore, we show long-term prediction of\nboundaries in situations where the motion is governed by the laws of physics.\nWe argue that our model has with minimalistic model assumptions derived a\nnotion of \"intuitive physics\"."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1611.09472v1", 
    "title": "The Bricklayer Ecosystem - Art, Math, and Code", 
    "arxiv-id": "1611.09472v1", 
    "author": "Cindy Corritore", 
    "publish": "2016-11-29T03:39:56Z", 
    "summary": "This paper describes the Bricklayer Ecosystem - a freely-available online\neducational ecosystem created for people of all ages and coding backgrounds.\nBricklayer is designed in accordance with a \"low-threshold infinite ceiling\"\nphilosophy and has been successfully used to teach coding to primary school\nstudents, middle school students, university freshmen, and in-service secondary\nmath teachers. Bricklayer programs are written in the functional programming\nlanguage SML and, when executed, create 2D and 3D artifacts. These artifacts\ncan be viewed using a variety of third-party tools such as LEGO Digital\nDesigner (LDD), LDraw, Minecraft clients, Brickr, as well as STereoLithography\nviewers."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.00522v1", 
    "title": "A Visual Representation for Editing Face Images", 
    "arxiv-id": "1612.00522v1", 
    "author": "David Forsyth", 
    "publish": "2016-12-02T00:07:38Z", 
    "summary": "We propose a new approach for editing face images, which enables numerous\nexciting applications including face relighting, makeup transfer and face\ndetail editing. Our face edits are based on a visual representation, which\nincludes geometry, face segmentation, albedo, illumination and detail map. To\nrecover our visual representation, we start by estimating geometry using a\nmorphable face model, then decompose the face image to recover the albedo, and\nthen shade the geometry with the albedo and illumination. The residual between\nour shaded geometry and the input image produces our detail map, which carries\nhigh frequency information that is either insufficiently or incorrectly\ncaptured by our shading process. By manipulating the detail map, we can edit\nface images with reality and identity preserved. Our representation allows\nvarious applications. First, it allows a user to directly manipulate various\nillumination. Second, it allows non-parametric makeup transfer with input\nface's distinctive identity features preserved. Third, it allows non-parametric\nmodifications to the face appearance by transferring details. For face\nrelighting and detail editing, we evaluate via a user study and our method\noutperforms other methods. For makeup transfer, we evaluate via an online\nattractiveness evaluation system, and can reliably make people look younger and\nmore attractive. We also show extensive qualitative comparisons to existing\nmethods, and have significant improvements over previous techniques."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.00523v1", 
    "title": "Photorealistic Facial Texture Inference Using Deep Neural Networks", 
    "arxiv-id": "1612.00523v1", 
    "author": "Hao Li", 
    "publish": "2016-12-02T00:14:12Z", 
    "summary": "We present a data-driven inference method that can synthesize a\nphotorealistic texture map of a complete 3D face model given a partial 2D view\nof a person in the wild. After an initial estimation of shape and low-frequency\nalbedo, we compute a high-frequency partial texture map, without the shading\ncomponent, of the visible face area. To extract the fine appearance details\nfrom this incomplete input, we introduce a multi-scale detail analysis\ntechnique based on mid-layer feature correlations extracted from a deep\nconvolutional neural network. We demonstrate that fitting a convex combination\nof feature correlations from a high-resolution face database can yield a\nsemantically plausible facial detail description of the entire face. A complete\nand photorealistic texture map can then be synthesized by iteratively\noptimizing for the reconstructed feature correlations. Using these\nhigh-resolution textures and a commercial rendering framework, we can produce\nhigh-fidelity 3D renderings that are visually comparable to those obtained with\nstate-of-the-art multi-view face capture systems. We demonstrate successful\nface reconstructions from a wide range of low resolution input images,\nincluding those of historical figures. In addition to extensive evaluations, we\nvalidate the realism of our results using a crowdsourced user study."
},{
    "category": "cs.GR", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.01944v2", 
    "title": "Porous Structure Design in Tissue Engineering Using Anisotropic Radial   Basis Function", 
    "arxiv-id": "1612.01944v2", 
    "author": "Zeyun Yu", 
    "publish": "2016-12-06T18:42:44Z", 
    "summary": "Development of additive manufacturing in last decade greatly improves tissue\nengineering. During the manufacturing of porous scaffold, simplified but\nfunctionally equivalent models are getting focused for practically reasons.\nScaffolds can be classified into regular porous scaffolds and irregular porous\nscaffolds. Several methodologies are developed to design these scaffolds. A\nnovel method is proposed in this paper using anisotropic radial basis function\n(ARBF) interpolation. This is method uses geometric models such as volumetric\nmeshes as input and proves to be flexible because geometric models are able to\ncapture the characteristics of complex tissues easily. Moreover, this method is\nstraightforward and easy to implement."
},{
    "category": "cs.CG", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.02261v1", 
    "title": "Sparse Geometric Representation Through Local Shape Probing", 
    "arxiv-id": "1612.02261v1", 
    "author": "Rapha\u00eblle Chaine", 
    "publish": "2016-12-07T14:26:36Z", 
    "summary": "Shape analysis is very often performed by segmenting the shape into smooth\nsurface parts that can be further classified using a set of predefined\nprimitives such as planes, cylinders or spheres. Hence the shape is generally\nassumed to be manifold and smooth or to be an assembly of primitive parts. In\nthis paper we propose an approach which does not make any assumption on the\nshape properties but rather learns its characteristics through a statistical\nanalysis of local shape variations. Armed solely with a local probing operator,\nwe are able to perform a non local analysis of the shape yielding a shape\ndictionary which encodes its structures. Our method relies on a novel\ndescription of shape variations, called Local Probing Field (LPF), which\ndescribes how a generic pattern is transformed onto the shape. By carefully\noptimizing the position and orientation of these descriptors we are able to\ncapture shape similarities and gather them into a geometrically relevant\ndictionary over which the shape decomposes sparsely. Furthermore, this analysis\nalso reveals the local dimensions of the shape. Our shape representation has\nseveral potential applications; here we demonstrate its efficiency for shape\nresampling and point set denoising."
},{
    "category": "cs.CG", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.02509v1", 
    "title": "Geodesics using Waves: Computing Distances using Wave Propagation", 
    "arxiv-id": "1612.02509v1", 
    "author": "Michael Kazhdan", 
    "publish": "2016-12-08T01:57:47Z", 
    "summary": "In this paper, we present a new method for computing approximate geodesic\ndistances. We introduce the wave method for approximating geodesic distances\nfrom a point on a manifold mesh. Our method involves the solution of two linear\nsystems of equations. One system of equations is solved repeatedly to propagate\nthe wave on the entire mesh, and one system is solved once after wave\npropagation is complete in order to compute the approximate geodesic distances\nup to an additive constant. However, these systems need to be pre-factored only\nonce, and can be solved efficiently at each iteration. All of our tests\nrequired approximately between 300 and 400 iterations, which were completed in\na few seconds. Therefore, this method can approximate geodesic distances\nquickly, and the approximation is highly accurate."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.02808v1", 
    "title": "3D Shape Segmentation with Projective Convolutional Networks", 
    "arxiv-id": "1612.02808v1", 
    "author": "Siddhartha Chaudhuri", 
    "publish": "2016-12-08T20:46:32Z", 
    "summary": "This paper introduces a deep architecture for segmenting 3D objects into\ntheir labeled semantic parts. Our architecture combines image-based Fully\nConvolutional Networks (FCNs) and surface-based Conditional Random Fields\n(CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are\nused for efficient view-based reasoning about 3D object parts. Through a\nspecial projection layer, FCN outputs are effectively aggregated across\nmultiple views and scales, then are projected onto the 3D object surfaces.\nFinally, a surface-based CRF combines the projected outputs with geometric\nconsistency cues to yield coherent segmentations. The whole architecture\n(multi-view FCNs and CRF) is trained end-to-end. Our approach significantly\noutperforms the existing state-of-the-art methods in the currently largest\nsegmentation benchmark (ShapeNet). Finally, we demonstrate promising\nsegmentation results on noisy 3D shapes acquired from consumer-grade depth\ncameras."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.04956v1", 
    "title": "Cloud Dictionary: Sparse Coding and Modeling for Point Clouds", 
    "arxiv-id": "1612.04956v1", 
    "author": "Alex Bronstein", 
    "publish": "2016-12-15T07:53:27Z", 
    "summary": "With the development of range sensors such as LIDAR and time-of-flight\ncameras, 3D point cloud scans have become ubiquitous in computer vision\napplications, the most prominent ones being gesture recognition and autonomous\ndriving. Parsimony-based algorithms have shown great success on images and\nvideos where data points are sampled on a regular Cartesian grid. We propose an\nadaptation of these techniques to irregularly sampled signals by using\ncontinuous dictionaries. We present an example application in the form of point\ncloud denoising."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1612.08927v1", 
    "title": "Fast color transfer from multiple images", 
    "arxiv-id": "1612.08927v1", 
    "author": "Ligang Liu", 
    "publish": "2016-12-28T16:50:55Z", 
    "summary": "Color transfer between images uses the statistics information of image\neffectively. We present a novel approach of local color transfer between images\nbased on the simple statistics and locally linear embedding. A sketching\ninterface is proposed for quickly and easily specifying the color\ncorrespondences between target and source image. The user can specify the\ncorrespondences of local region using scribes, which more accurately transfers\nthe target color to the source image while smoothly preserving the boundaries,\nand exhibits more natural output results. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different regions in the source image. Moreover, our\nalgorithm does not require to choose the same color style and image size\nbetween source and target images. We propose the sub-sampling to reduce the\ncomputational load. Comparing with other approaches, our algorithm is much\nbetter in color blending in the input data. Our approach preserves the other\ncolor details in the source image. Various experimental results show that our\napproach specifies the correspondences of local color region in source and\ntarget images. And it expresses the intention of users and generates more\nactual and natural results of visual effect."
},{
    "category": "math.NA", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.01595v1", 
    "title": "Localized Tight Frames and Fast Framelet Transforms on the Simplex", 
    "arxiv-id": "1701.01595v1", 
    "author": "Houying Zhu", 
    "publish": "2017-01-06T11:14:19Z", 
    "summary": "This paper constructs a continuous localized tight frame on a two-dimensional\nsimplex $T^{2}$ using orthogonal polynomials. We then use quadrature rules on\n$T^{2}$ to construct discrete tight framelets. Fast algorithms for discrete\ntight framelet transforms on $T^{2}$ are given, which have the same\ncomputational steps as the fast Fourier transforms on the simplex $T^{2}$."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.02123v1", 
    "title": "Green-Blue Stripe Pattern for Range Sensing from a Single Image", 
    "arxiv-id": "1701.02123v1", 
    "author": "Sang Wook Lee", 
    "publish": "2017-01-09T10:16:11Z", 
    "summary": "In this paper, we present a novel method for rapid high-resolution range\nsensing using green-blue stripe pattern. We use green and blue for designing\nhigh-frequency stripe projection pattern. For accurate and reliable range\nrecovery, we identify the stripe patterns by our color-stripe segmentation and\nunwrapping algorithms. The experimental result for a naked human face shows the\neffectiveness of our method."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.02357v1", 
    "title": "Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware   Semantic Segmentation", 
    "arxiv-id": "1701.02357v1", 
    "author": "Tom Goldstein", 
    "publish": "2017-01-09T21:30:03Z", 
    "summary": "Style transfer is an important task in which the style of a source image is\nmapped onto that of a target image. The method is useful for synthesizing\nderivative works of a particular artist or specific painting. This work\nconsiders targeted style transfer, in which the style of a template image is\nused to alter only part of a target image. For example, an artist may wish to\nalter the style of only one particular object in a target image without\naltering the object's general morphology or surroundings. This is useful, for\nexample, in augmented reality applications (such as the recently released\nPokemon GO), where one wants to alter the appearance of a single real-world\nobject in an image frame to make it appear as a cartoon. Most notably, the\nrendering of real-world objects into cartoon characters has been used in a\nnumber of films and television show, such as the upcoming series Son of Zorn.\nWe present a method for targeted style transfer that simultaneously segments\nand stylizes single objects selected by the user. The method uses a Markov\nrandom field model to smooth and anti-alias outlier pixels near object\nboundaries, so that stylized objects naturally blend into their surroundings."
},{
    "category": "cs.CG", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.03230v1", 
    "title": "Surface Reconstruction with Data-driven Exemplar Priors", 
    "arxiv-id": "1701.03230v1", 
    "author": "Jun Wang", 
    "publish": "2017-01-12T04:51:15Z", 
    "summary": "In this paper, we propose a framework to reconstruct 3D models from raw\nscanned points by learning the prior knowledge of a specific class of objects.\nUnlike previous work that heuristically specifies particular regularities and\ndefines parametric models, our shape priors are learned directly from existing\n3D models under a framework based on affinity propagation. Given a database of\n3D models within the same class of objects, we build a comprehensive library of\n3D local shape priors. We then formulate the problem to select\nas-few-as-possible priors from the library, referred to as exemplar priors.\nThese priors are sufficient to represent the 3D shapes of the whole class of\nobjects from where they are generated. By manipulating these priors, we are\nable to reconstruct geometrically faithful models with the same class of\nobjects from raw point clouds. Our framework can be easily generalized to\nreconstruct various categories of 3D objects that have more geometrically or\ntopologically complex structures. Comprehensive experiments exhibit the power\nof our exemplar priors for gracefully solving several problems in 3D shape\nreconstruction such as preserving sharp features, recovering fine details and\nso on."
},{
    "category": "cs.HC", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.03981v1", 
    "title": "A feasibility study on SSVEP-based interaction with motivating and   immersive virtual and augmented reality", 
    "arxiv-id": "1701.03981v1", 
    "author": "Christa Neuper", 
    "publish": "2017-01-15T01:58:47Z", 
    "summary": "Non-invasive steady-state visual evoked potential (SSVEP) based\nbrain-computer interface (BCI) systems offer high bandwidth compared to other\nBCI types and require only minimal calibration and training. Virtual reality\n(VR) has been already validated as effective, safe, affordable and motivating\nfeedback modality for BCI experiments. Augmented reality (AR) enhances the\nphysical world by superimposing informative, context sensitive, computer\ngenerated content. In the context of BCI, AR can be used as a friendlier and\nmore intuitive real-world user interface, thereby facilitating a more seamless\nand goal directed interaction. This can improve practicality and usability of\nBCI systems and may help to compensate for their low bandwidth. In this\nfeasibility study, three healthy participants had to finish a complex\nnavigation task in immersive VR and AR conditions using an online SSVEP BCI.\nTwo out of three subjects were successful in all conditions. To our knowledge,\nthis is the first work to present an SSVEP BCI that operates using target\nstimuli integrated in immersive VR and AR (head-mounted display and camera).\nThis research direction can benefit patients by introducing more intuitive and\neffective real-world interaction (e.g. smart home control). It may also be\nrelevant for user groups that require or benefit from hands free operation\n(e.g. due to temporary situational disability)."
},{
    "category": "cs.GR", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.04383v1", 
    "title": "Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for   B-Spline Curve Approximation", 
    "arxiv-id": "1701.04383v1", 
    "author": "Bar\u0131\u015f Ko\u00e7er", 
    "publish": "2017-01-16T18:20:32Z", 
    "summary": "In this paper, a new approach to solve the cubic B-spline curve fitting\nproblem is presented based on a meta-heuristic algorithm called \" dolphin\necholocation \". The method minimizes the proximity error value of the selected\nnodes that measured using the least squares method and the Euclidean distance\nmethod of the new curve generated by the reverse engineering. The results of\nthe proposed method are compared with the genetic algorithm. As a result, this\nnew method seems to be successful."
},{
    "category": "cs.LG", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1701.07403v1", 
    "title": "Learning Light Transport the Reinforced Way", 
    "arxiv-id": "1701.07403v1", 
    "author": "Alexander Keller", 
    "publish": "2017-01-25T17:50:19Z", 
    "summary": "We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with non-zero contribution\nis dramatically increased, resulting in much less noisy images within a fixed\ntime budget."
},{
    "category": "cs.MM", 
    "doi": "10.4204/EPTCS.230.4", 
    "link": "http://arxiv.org/pdf/1702.00182v1", 
    "title": "Inkjet printing-based volumetric display projecting multiple full-colour   2D patterns", 
    "arxiv-id": "1702.00182v1", 
    "author": "Tomoyoshi Ito", 
    "publish": "2017-02-01T10:01:44Z", 
    "summary": "In this study, a method to construct a full-colour volumetric display is\npresented using a commercially available inkjet printer. Photoreactive\nluminescence materials are minutely and automatically printed as the volume\nelements, and volumetric displays are constructed with high resolution using\neasy-to-fabricate means that exploit inkjet printing technologies. The results\nexperimentally demonstrate the first prototype of an inkjet printing-based\nvolumetric display composed of multiple layers of transparent films that yield\na full-colour three-dimensional (3D) image. Moreover, we propose a design\nalgorithm with 3D structures that provide multiple different 2D full-colour\npatterns when viewed from different directions and experimentally demonstrates\nprototypes. It is considered that these types of 3D volumetric structures and\ntheir fabrication methods based on widely deployed existing printing\ntechnologies can be utilised as novel information display devices and systems,\nincluding digital signage, media art, entertainment and security."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1702.02878v1", 
    "title": "Bezier developable surfaces", 
    "arxiv-id": "1702.02878v1", 
    "author": "L. Fern\u00e1ndez-Jambrina", 
    "publish": "2017-02-09T16:23:36Z", 
    "summary": "In this paper we address the issue of designing developable surfaces with\nBezier patches. We show that developable surfaces with a polynomial edge of\nregression are the set of developable surfaces which can be constructed with\nAumann's algorithm. We also obtain the set of polynomial developable surfaces\nwhich can be constructed using general polynomial curves. The conclusions can\nbe extended to spline surfaces as well."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1702.07619v1", 
    "title": "Fast and robust curve skeletonization for real-world elongated objects", 
    "arxiv-id": "1702.07619v1", 
    "author": "Henry Medeiros", 
    "publish": "2017-02-24T15:01:22Z", 
    "summary": "We consider the problem of extracting curve skeletons of three-dimensional,\nelongated objects given a noisy surface, which has applications in agricultural\ncontexts such as extracting the branching structure of plants. We describe an\nefficient and robust method based on breadth-first search that can determine\ncurve skeletons in these contexts. Our approach is capable of automatically\ndetecting junction points as well as spurious segments and loops. All of that\nis accomplished with only one user-adjustable parameter. The run time of our\nmethod ranges from hundreds of milliseconds to less than four seconds on large,\nchallenging datasets, which makes it appropriate for situations where real-time\ndecision making is needed. Experiments on synthetic models as well as on data\nfrom real world objects, some of which were collected in challenging field\nconditions, show that our approach compares favorably to classical thinning\nalgorithms as well as to recent contributions to the field."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1702.08680v1", 
    "title": "A Data-driven Approach for Furniture and Indoor Scene Colorization", 
    "arxiv-id": "1702.08680v1", 
    "author": "Han Ma", 
    "publish": "2017-02-28T07:54:21Z", 
    "summary": "We present a data-driven approach that colorizes 3D furniture models and\nindoor scenes by leveraging indoor images on the internet. Our approach is able\nto colorize the furniture automatically according to an example image. The core\nis to learn image-guided mesh segmentation to segment the model into different\nparts according to the image object. Given an indoor scene, the system supports\ncolorization-by-example, and has the ability to recommend the colorization\nscheme that is consistent with a user-desired color theme. The latter is\nrealized by formulating the problem as a Markov random field model that imposes\nuser input as an additional constraint. We contribute to the community a\nhierarchically organized image-model database with correspondences between each\nimage and the corresponding model at the part-level. Our experiments and a user\nstudy show that our system produces perceptually convincing results comparable\nto those generated by interior designers."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1703.00061v1", 
    "title": "SceneSuggest: Context-driven 3D Scene Design", 
    "arxiv-id": "1703.00061v1", 
    "author": "Maneesh Agrawala", 
    "publish": "2017-02-28T21:21:03Z", 
    "summary": "We present SceneSuggest: an interactive 3D scene design system providing\ncontext-driven suggestions for 3D model retrieval and placement. Using a\npoint-and-click metaphor we specify regions in a scene in which to\nautomatically place and orient relevant 3D models. Candidate models are ranked\nusing a set of static support, position, and orientation priors learned from 3D\nscenes. We show that our suggestions enable rapid assembly of indoor scenes. We\nperform a user study comparing suggestions to manual search and selection, as\nwell as to suggestions with no automatic orientation. We find that suggestions\nreduce total modeling time by 32%, that orientation priors reduce time spent\nre-orienting objects by 27%, and that context-driven suggestions reduce the\nnumber of text queries by 50%."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1703.00521v1", 
    "title": "The Signals and Systems Approach to Animation", 
    "arxiv-id": "1703.00521v1", 
    "author": "Chris North", 
    "publish": "2017-03-01T21:40:16Z", 
    "summary": "Animation is ubiquitous in visualization systems, and a common technique for\ncreating these animations is the transition. In the transition approach,\nanimations are created by smoothly interpolating a visual attribute between a\nstart and end value, reaching the end value after a specified duration. This\napproach works well when each transition for an attribute is allowed to finish\nbefore the next is triggered, but performs poorly when a new transition is\ntriggered before the current transition has finished. In particular,\ninterruptions introduce velocity discontinuities, and frequent interruptions\ncan slow down the resulting animation. To solve these problems, we model the\nproblem of animation as a signal processing problem. In our technique,\nanimations are produced by transformations of signals, or functions over time.\nIn particular, an animation is produced by transforming an input signal, a\nfunction from time to target attribute value, into an output signal, a function\nfrom time to displayed attribute value. We show that well-known\nsignal-processing techniques can be applied to produce animations that are free\nfrom velocity discontinuities even when interrupted."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1703.02016v1", 
    "title": "Fast Back-Projection for Non-Line of Sight Reconstruction", 
    "arxiv-id": "1703.02016v1", 
    "author": "Adrian Jarabo", 
    "publish": "2017-03-06T18:33:01Z", 
    "summary": "Recent works have demonstrated non-line of sight (NLOS) reconstruction by\nusing the time-resolved signal frommultiply scattered light. These works\ncombine ultrafast imaging systems with computation, which back-projects the\nrecorded space-time signal to build a probabilistic map of the hidden geometry.\nUnfortunately, this computation is slow, becoming a bottleneck as the imaging\ntechnology improves. In this work, we propose a new back-projection technique\nfor NLOS reconstruction, which is up to a thousand times faster than previous\nwork, with almost no quality loss. We base on the observation that the hidden\ngeometry probability map can be built as the intersection of the three-bounce\nspace-time manifolds defined by the light illuminating the hidden geometry and\nthe visible point receiving the scattered light from such hidden geometry. This\nallows us to pose the reconstruction of the hidden geometry as the voxelization\nof these space-time manifolds, which has lower theoretic complexity and is\neasily implementable in the GPU. We demonstrate the efficiency and quality of\nour technique compared against previous methods in both captured and synthetic\ndata"
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1703.05700v1", 
    "title": "Autocomplete Textures for 3D Printing", 
    "arxiv-id": "1703.05700v1", 
    "author": "Mark D. Gross", 
    "publish": "2017-03-16T16:24:01Z", 
    "summary": "Texture is an essential property of physical objects that affects aesthetics,\nusability, and functionality. However, designing and applying textures to 3D\nobjects with existing tools remains difficult and time-consuming; it requires\nproficient 3D modeling skills. To address this, we investigated an\nauto-completion approach for efficient texture creation that automates the\ntedious, repetitive process of applying texture while allowing flexible\ncustomization. We developed techniques for users to select a target surface,\nsketch and manipulate a texture with 2D drawings, and then generate 3D\nprintable textures onto an arbitrary curved surface. In a controlled experiment\nour tool sped texture creation by 80% over conventional tools, a performance\ngain that is higher with more complex target surfaces. This result confirms\nthat auto-completion is powerful for creating 3D textures."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/1703.06003v1", 
    "title": "Color Orchestra: Ordering Color Palettes for Interpolation and   Prediction", 
    "arxiv-id": "1703.06003v1", 
    "author": "Antoni B. Chan", 
    "publish": "2017-03-17T13:25:49Z", 
    "summary": "Color theme or color palette can deeply influence the quality and the feeling\nof a photograph or a graphical design. Although color palettes may come from\ndifferent sources such as online crowd-sourcing, photographs and graphical\ndesigns, in this paper, we consider color palettes extracted from fine art\ncollections, which we believe to be an abundant source of stylistic and unique\ncolor themes. We aim to capture color styles embedded in these collections by\nmeans of statistical models and to build practical applications upon these\nmodels. As artists often use their personal color themes in their paintings,\nmaking these palettes appear frequently in the dataset, we employed density\nestimation to capture the characteristics of palette data. Via density\nestimation, we carried out various predictions and interpolations on palettes,\nwhich led to promising applications such as photo-style exploration, real-time\ncolor suggestion, and enriched photo recolorization. It was, however,\nchallenging to apply density estimation to palette data as palettes often come\nas unordered sets of colors, which make it difficult to use conventional\nmetrics on them. To this end, we developed a divide-and-conquer sorting\nalgorithm to rearrange the colors in the palettes in a coherent order, which\nallows meaningful interpolation between color palettes. To confirm the\nperformance of our model, we also conducted quantitative experiments on\ndatasets of digitized paintings collected from the Internet and received\nfavorable results."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/9809119v1", 
    "title": "Droems: experimental mathematics, informatics and infinite dimensional   geometry", 
    "arxiv-id": "cs/9809119v1", 
    "author": "Denis V. Juriev", 
    "publish": "1998-09-29T07:06:31Z", 
    "summary": "The article is devoted to a problem of elaboration of the real-time\ninteractive videosystems for accelerated nonverbal cognitive computer and\ntelecommunications. The proposed approach is based on the using of droems\n(dynamically reconstructed objects of experimental mathematics) and\ninterpretational figures as pointers to them. Four paragraphs of the article\nare devoted to (1) an exposition of basic notions of the interpretational\ngeometry, (2) the operator methods in the theory of interactive dynamical\nvideosystems, (3) the general concept of organization of the integrated\ninteractive real-time videocognitive systems, (4) the droems and processes of\ntheir dynamical reconstruction, where the general notions are illustrated by a\nconcrete example related to the infinite dimensional geometry. The exposition\nis presumably heuristic and conceptual (the first and the third paragraphs)\nthough some particular aspects such as content of the second and the fourth\nparagraphs, which allow deeper formalization and detailing in present, are\nexposed on the mathematical level of rigor."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/9810020v1", 
    "title": "Computational Geometry Column 33", 
    "arxiv-id": "cs/9810020v1", 
    "author": "Joseph O'Rourke", 
    "publish": "1998-10-22T20:44:35Z", 
    "summary": "Several recent SIGGRAPH papers on surface simplification are described."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0005005v1", 
    "title": "Connectivity Compression for Irregular Quadrilateral Meshes", 
    "arxiv-id": "cs/0005005v1", 
    "author": "Andrzej Szymczak", 
    "publish": "2000-05-04T18:15:08Z", 
    "summary": "Applications that require Internet access to remote 3D datasets are often\nlimited by the storage costs of 3D models. Several compression methods are\navailable to address these limits for objects represented by triangle meshes.\nMany CAD and VRML models, however, are represented as quadrilateral meshes or\nmixed triangle/quadrilateral meshes, and these models may also require\ncompression. We present an algorithm for encoding the connectivity of such\nquadrilateral meshes, and we demonstrate that by preserving and exploiting the\noriginal quad structure, our approach achieves encodings 30 - 80% smaller than\nan approach based on randomly splitting quads into triangles. We present both a\ncode with a proven worst-case cost of 3 bits per vertex (or 2.75 bits per\nvertex for meshes without valence-two vertices) and entropy-coding results for\ntypical meshes ranging from 0.3 to 0.9 bits per vertex, depending on the\nregularity of the mesh. Our method may be implemented by a rule for a\nparticular splitting of quads into triangles and by using the compression and\ndecompression algorithms introduced in [Rossignac99] and\n[Rossignac&Szymczak99]. We also present extensions to the algorithm to compress\nmeshes with holes and handles and meshes containing triangles and other\npolygons as well as quads."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0203026v1", 
    "title": "Conformal Geometry, Euclidean Space and Geometric Algebra", 
    "arxiv-id": "cs/0203026v1", 
    "author": "Joan Lasenby", 
    "publish": "2002-03-22T14:33:34Z", 
    "summary": "Projective geometry provides the preferred framework for most implementations\nof Euclidean space in graphics applications. Translations and rotations are\nboth linear transformations in projective geometry, which helps when it comes\nto programming complicated geometrical operations. But there is a fundamental\nweakness in this approach - the Euclidean distance between points is not\nhandled in a straightforward manner. Here we discuss a solution to this\nproblem, based on conformal geometry. The language of geometric algebra is best\nsuited to exploiting this geometry, as it handles the interior and exterior\nproducts in a single, unified framework. A number of applications are\ndiscussed, including a compact formula for reflecting a line off a general\nspherical surface."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0207004v1", 
    "title": "Optimally cutting a surface into a disk", 
    "arxiv-id": "cs/0207004v1", 
    "author": "Sariel Har-Peled", 
    "publish": "2002-07-02T22:03:51Z", 
    "summary": "We consider the problem of cutting a set of edges on a polyhedral manifold\nsurface, possibly with boundary, to obtain a single topological disk,\nminimizing either the total number of cut edges or their total length. We show\nthat this problem is NP-hard, even for manifolds without boundary and for\npunctured spheres. We also describe an algorithm with running time n^{O(g+k)},\nwhere n is the combinatorial complexity, g is the genus, and k is the number of\nboundary components of the input surface. Finally, we describe a greedy\nalgorithm that outputs a O(log^2 g)-approximation of the minimum cut graph in\nO(g^2 n log n) time."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0306010v1", 
    "title": "On multiple connectedness of regions visible due to multiple diffuse   reflections", 
    "arxiv-id": "cs/0306010v1", 
    "author": "Dilip Sarkar", 
    "publish": "2003-06-02T11:40:37Z", 
    "summary": "It is known that the region $V(s)$ of a simple polygon $P$, directly visible\n(illuminable) from an internal point $s$, is simply connected. Aronov et al.\n\\cite{addpp981} established that the region $V_1(s)$ of a simple polygon\nvisible from an internal point $s$ due to at most one diffuse reflection on the\nboundary of the polygon $P$, is also simply connected. In this paper we\nestablish that the region $V_2(s)$, visible from $s$ due to at most two diffuse\nreflections may be multiply connected; we demonstrate the construction of an\n$n$-sided simple polygon with a point $s$ inside it so that and the region of\n$P$ visible from $s$ after at most two diffuse reflections is multiple\nconnected."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0401023v1", 
    "title": "Surface Triangulation -- The Metric Approach", 
    "arxiv-id": "cs/0401023v1", 
    "author": "Emil Saucan", 
    "publish": "2004-01-26T21:51:04Z", 
    "summary": "We embark in a program of studying the problem of better approximating\nsurfaces by triangulations(triangular meshes) by considering the approximating\ntriangulations as finite metric spaces and the target smooth surface as their\nHaussdorff-Gromov limit. This allows us to define in a more natural way the\nrelevant elements, constants and invariants s.a. principal directions and\nprincipal values, Gaussian and Mean curvature, etc. By a \"natural way\" we mean\nan intrinsic, discrete, metric definitions as opposed to approximating or\nparaphrasing the differentiable notions. In this way we hope to circumvent\ncomputational errors and, indeed, conceptual ones, that are often inherent to\nthe classical, \"numerical\" approach. In this first study we consider the\nproblem of determining the Gaussian curvature of a polyhedral surface, by using\nthe {\\em embedding curvature} in the sense of Wald (and Menger). We present two\nmodalities of employing these definitions for the computation of Gaussian\ncurvature."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0410044v5", 
    "title": "An Example of Clifford Algebras Calculations with GiNaC", 
    "arxiv-id": "cs/0410044v5", 
    "author": "Vladimir V. Kisil", 
    "publish": "2004-10-18T17:39:51Z", 
    "summary": "This example of Clifford algebras calculations uses GiNaC\n(http://www.ginac.de/) library, which includes a support for generic Clifford\nalgebra starting from version~1.3.0. Both symbolic and numeric calculation are\npossible and can be blended with other functions of GiNaC. This calculations\nwas made for the paper math.CV/0410399.\n  Described features of GiNaC are already available at PyGiNaC\n(http://sourceforge.net/projects/pyginac/) and due to course should propagate\ninto other software like GNU Octave (http://www.octave.org/), gTybalt\n(http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as\ntheir back-end."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0606007v1", 
    "title": "A parent-centered radial layout algorithm for interactive graph   visualization and animation", 
    "arxiv-id": "cs/0606007v1", 
    "author": "Jonathan Schull", 
    "publish": "2006-06-01T16:56:55Z", 
    "summary": "We have developed (1) a graph visualization system that allows users to\nexplore graphs by viewing them as a succession of spanning trees selected\ninteractively, (2) a radial graph layout algorithm, and (3) an animation\nalgorithm that generates meaningful visualizations and smooth transitions\nbetween graphs while minimizing edge crossings during transitions and in static\nlayouts.\n  Our system is similar to the radial layout system of Yee et al. (2001), but\ndiffers primarily in that each node is positioned on a coordinate system\ncentered on its own parent rather than on a single coordinate system for all\nnodes. Our system is thus easy to define recursively and lends itself to\nparallelization. It also guarantees that layouts have many nice properties,\nsuch as: it guarantees certain edges never cross during an animation.\n  We compared the layouts and transitions produced by our algorithms to those\nproduced by Yee et al. Results from several experiments indicate that our\nsystem produces fewer edge crossings during transitions between graph drawings,\nand that the transitions more often involve changes in local scaling rather\nthan structure.\n  These findings suggest the system has promise as an interactive graph\nexploration tool in a variety of settings."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cagd.2017.02.001", 
    "link": "http://arxiv.org/pdf/cs/0608003v2", 
    "title": "On a solution to display non-filled-in quaternionic Julia sets", 
    "arxiv-id": "cs/0608003v2", 
    "author": "Alessandro Rosa", 
    "publish": "2006-08-01T19:25:17Z", 
    "summary": "During early 1980s, the so-called `escape time' method, developed to display\nthe Julia sets for complex dynamical systems, was exported to quaternions in\norder to draw analogous pictures in this wider numerical field. Despite of the\nfine results in the complex plane, where all topological configurations of\nJulia sets have been successfully displayed, the `escape time' method fails to\nrender properly the non-filled-in variety of quaternionic Julia sets. So their\ndigital visualisation remained an open problem for several years. Both the\nsolution for extending this old method to non-filled-in quaternionic Julia sets\nand its implementation into a program are explained here."
},{
    "category": "astro-ph", 
    "doi": "10.1109/PACIFICVIS.2008.4475478", 
    "link": "http://arxiv.org/pdf/0801.2405v2", 
    "title": "Multiple Uncertainties in Time-Variant Cosmological Particle Data", 
    "arxiv-id": "0801.2405v2", 
    "author": "Katrin Heitmann", 
    "publish": "2008-01-15T22:57:41Z", 
    "summary": "Though the mediums for visualization are limited, the potential dimensions of\na dataset are not. In many areas of scientific study, understanding the\ncorrelations between those dimensions and their uncertainties is pivotal to\nmining useful information from a dataset. Obtaining this insight can\nnecessitate visualizing the many relationships among temporal, spatial, and\nother dimensionalities of data and its uncertainties. We utilize multiple views\nfor interactive dataset exploration and selection of important features, and we\napply those techniques to the unique challenges of cosmological particle\ndatasets. We show how interactivity and incorporation of multiple visualization\ntechniques help overcome the problem of limited visualization dimensions and\nallow many types of uncertainty to be seen in correlation with other variables."
},{
    "category": "cs.CG", 
    "doi": "10.1109/PACIFICVIS.2008.4475478", 
    "link": "http://arxiv.org/pdf/0802.1617v1", 
    "title": "Discrete Complex Structure on Surfel Surfaces", 
    "arxiv-id": "0802.1617v1", 
    "author": "Christian Mercat", 
    "publish": "2008-02-12T11:06:38Z", 
    "summary": "This paper defines a theory of conformal parametrization of digital surfaces\nmade of surfels equipped with a normal vector. The main idea is to locally\nproject each surfel to the tangent plane, therefore deforming its aspect-ratio.\nIt is a generalization of the theory known for polyhedral surfaces. The main\ndifference is that the conformal ratios that appear are no longer real in\ngeneral. It yields a generalization of the standard Laplacian on weighted\ngraphs."
},{
    "category": "cs.CV", 
    "doi": "10.1109/PACIFICVIS.2008.4475478", 
    "link": "http://arxiv.org/pdf/0804.1046v1", 
    "title": "Discrete schemes for Gaussian curvature and their convergence", 
    "arxiv-id": "0804.1046v1", 
    "author": "Guoliang Xu", 
    "publish": "2008-04-07T14:47:03Z", 
    "summary": "In this paper, several discrete schemes for Gaussian curvature are surveyed.\nThe convergence property of a modified discrete scheme for the Gaussian\ncurvature is considered. Furthermore, a new discrete scheme for Gaussian\ncurvature is resented. We prove that the new scheme converges at the regular\nvertex with valence not less than 5. By constructing a counterexample, we also\nshow that it is impossible for building a discrete scheme for Gaussian\ncurvature which converges over the regular vertex with valence 4. Finally,\nasymptotic errors of several discrete scheme for Gaussian curvature are\ncompared."
},{
    "category": "cs.CG", 
    "doi": "10.1137/090759112", 
    "link": "http://arxiv.org/pdf/0812.0893v2", 
    "title": "Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge   Crossings", 
    "arxiv-id": "0812.0893v2", 
    "author": "Darren Strash", 
    "publish": "2008-12-04T10:29:00Z", 
    "summary": "We provide linear-time algorithms for geometric graphs with sublinearly many\ncrossings. That is, we provide algorithms running in O(n) time on connected\ngeometric graphs having n vertices and k crossings, where k is smaller than n\nby an iterated logarithmic factor. Specific problems we study include Voronoi\ndiagrams and single-source shortest paths. Our algorithms all run in linear\ntime in the standard comparison-based computational model; hence, we make no\nassumptions about the distribution or bit complexities of edge weights, nor do\nwe utilize unusual bit-level operations on memory words. Instead, our\nalgorithms are based on a planarization method that \"zeroes in\" on edge\ncrossings, together with methods for extending planar separator decompositions\nto geometric graphs with sublinearly many crossings. Incidentally, our\nplanarization algorithm also solves an open computational geometry problem of\nChazelle for triangulating a self-intersecting polygonal chain having n\nsegments and k crossings in linear time, for the case when k is sublinear in n\nby an iterated logarithmic factor."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0901.4643v3", 
    "title": "Visual tool for estimating the fractal dimension of images", 
    "arxiv-id": "0901.4643v3", 
    "author": "T. Esanu", 
    "publish": "2009-01-29T14:41:45Z", 
    "summary": "This work presents a new Visual Basic 6.0 application for estimating the\nfractal dimension of images, based on an optimized version of the box-counting\nalgorithm. Following the attempt to separate the real information from noise,\nwe considered also the family of all band-pass filters with the same band-width\n(specified as parameter). The fractal dimension can be thus represented as a\nfunction of the pixel color code. The program was used for the study of\npaintings cracks, as an additional tool which can help the critic to decide if\nan artistic work is original or not. In its second version, the application was\nextended for working also with csv files and three-dimensional images."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0902.2187v1", 
    "title": "A Standalone Markerless 3D Tracker for Handheld Augmented Reality", 
    "arxiv-id": "0902.2187v1", 
    "author": "Judith Kelner", 
    "publish": "2009-02-12T18:25:13Z", 
    "summary": "This paper presents an implementation of a markerless tracking technique\ntargeted to the Windows Mobile Pocket PC platform. The primary aim of this work\nis to allow the development of standalone augmented reality applications for\nhandheld devices based on natural feature tracking. In order to achieve this\ngoal, a subset of two computer vision libraries was ported to the Pocket PC\nplatform. They were also adapted to use fixed point math, with the purpose of\nimproving the overall performance of the routines. The port of these libraries\nopens up the possibility of having other computer vision tasks being executed\non mobile platforms. A model based tracking approach that relies on edge\ninformation was adopted. Since it does not require a high processing power, it\nis suitable for constrained devices such as handhelds. The OpenGL ES graphics\nlibrary was used to perform computer vision tasks, taking advantage of existing\ngraphics hardware acceleration. An augmented reality application was created\nusing the implemented technique and evaluations were done regarding tracking\nperformance and accuracy"
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0904.2096v1", 
    "title": "A Distributed Software Architecture for Collaborative Teleoperation   based on a VR Platform and Web Application Interoperability", 
    "arxiv-id": "0904.2096v1", 
    "author": "Malik Mallem", 
    "publish": "2009-04-14T11:21:47Z", 
    "summary": "Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a\nreal help to complete complex tasks, such as robot teleoperation and\ncooperative teleassistance. Using appropriate augmentations, the HO can\ninteract faster, safer and easier with the remote real world. In this paper, we\npresent an extension of an existing distributed software and network\narchitecture for collaborative teleoperation based on networked human-scaled\nmixed reality and mobile platform. The first teleoperation system was composed\nby a VR application and a Web application. However the 2 systems cannot be used\ntogether and it is impossible to control a distant robot simultaneously. Our\ngoal is to update the teleoperation system to permit a heterogeneous\ncollaborative teleoperation between the 2 platforms. An important feature of\nthis interface is based on different Mobile platforms to control one or many\nrobots."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0906.3074v1", 
    "title": "Feynman Algorithm Implementation for Comparison with Euler in a Uniform   Elastic Two-Layer 2D and 3D Object Dynamic Deformation Framework in OpenGL   with GUI", 
    "arxiv-id": "0906.3074v1", 
    "author": "Miao Song", 
    "publish": "2009-06-17T05:39:28Z", 
    "summary": "We implement for comparative purposes the Feynman algorithm within a\nC++-based framework for two-layer uniform facet elastic object for real-time\nsoftbody simulation based on physics modeling methods. To facilitate the\ncomparison, we implement initial timing measurements on the same hardware\nagainst that of Euler integrator in the softbody framework by varying different\nalgorithm parameters. Due to a relatively large number of such variations we\nimplement a GLUI-based user-interface to allow for much more finer control over\nthe simulation process at real-time, which was lacking completely in the\nprevious versions of the framework. We show our currents results based on the\nenhanced framework. The two-layered elastic object consists of inner and outer\nelastic mass-spring surfaces and compressible internal pressure. The density of\nthe inner layer can be set differently from the density of the outer layer; the\nmotion of the inner layer can be opposite to the motion of the outer layer.\nThese special features, which cannot be achieved by a single layered object,\nresult in improved imitation of a soft body, such as tissue's liquid\nnon-uniform deformation. The inertial behavior of the elastic object is well\nillustrated in environments with gravity and collisions with walls, ceiling,\nand floor. The collision detection is defined by elastic collision penalty\nmethod and the motion of the object is guided by the Ordinary Differential\nEquation computation. Users can interact with the modeled objects, deform them,\nand observe the response to their action in real-time and we provide an\nextensible framework and its implementation for comparative studies of\ndifferent physical-based modeling and integration algorithm implementations."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0909.3137v1", 
    "title": "Succinct Representation of Well-Spaced Point Clouds", 
    "arxiv-id": "0909.3137v1", 
    "author": "Beno\u00eet Hudson", 
    "publish": "2009-09-17T01:39:59Z", 
    "summary": "A set of n points in low dimensions takes Theta(n w) bits to store on a w-bit\nmachine. Surface reconstruction and mesh refinement impose a requirement on the\ndistribution of the points they process. I show how to use this assumption to\nlossily compress a set of n input points into a representation that takes only\nO(n) bits, independent of the word size. The loss can keep inter-point\ndistances to within 10% relative error while still achieving a factor of three\nspace savings. The representation allows standard quadtree operations, along\nwith computing the restricted Voronoi cell of a point, in time O(w^2 + log n),\nwhich can be improved to time O(log n) if w is in Theta(log n). Thus one can\nuse this compressed representation to perform mesh refinement or surface\nreconstruction in O(n) bits with only a logarithmic slowdown."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/0911.3349v1", 
    "title": "Seeing Science", 
    "arxiv-id": "0911.3349v1", 
    "author": "Alyssa Goodman", 
    "publish": "2009-11-17T17:01:33Z", 
    "summary": "The ability to represent scientific data and concepts visually is becoming\nincreasingly important due to the unprecedented exponential growth of\ncomputational power during the present digital age. The data sets and\nsimulations scientists in all fields can now create are literally thousands of\ntimes as large as those created just 20 years ago. Historically successful\nmethods for data visualization can, and should, be applied to today's huge data\nsets, but new approaches, also enabled by technology, are needed as well.\nIncreasingly, \"modular craftsmanship\" will be applied, as relevant\nfunctionality from the graphically and technically best tools for a job are\ncombined as-needed, without low-level programming."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/1003.1410v2", 
    "title": "Local Space-Time Smoothing for Version Controlled Documents", 
    "arxiv-id": "1003.1410v2", 
    "author": "Guy Lebanon", 
    "publish": "2010-03-06T18:08:12Z", 
    "summary": "Unlike static documents, version controlled documents are continuously edited\nby one or more authors. Such collaborative revision process makes traditional\nmodeling and visualization techniques inappropriate. In this paper we propose a\nnew representation based on local space-time smoothing that captures important\nrevision patterns. We demonstrate the applicability of our framework using\nexperiments on synthetic and real-world data."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cpc.2009.05.015", 
    "link": "http://arxiv.org/pdf/1004.0258v1", 
    "title": "Trends and Techniques in Visual Gaze Analysis", 
    "arxiv-id": "1004.0258v1", 
    "author": "Craig A. Lindley", 
    "publish": "2010-04-01T23:48:23Z", 
    "summary": "Visualizing gaze data is an effective way for the quick interpretation of eye\ntracking results. This paper presents a study investigation benefits and\nlimitations of visual gaze analysis among eye tracking professionals and\nresearchers. The results were used to create a tool for visual gaze analysis\nwithin a Master's project."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1004.2447v1", 
    "title": "Autoplot: A browser for scientific data on the web", 
    "arxiv-id": "1004.2447v1", 
    "author": "R. H. W. Friedel", 
    "publish": "2010-04-14T16:40:41Z", 
    "summary": "Autoplot is software developed for the Virtual Observatories in Heliophysics\nto provide intelligent and automated plotting capabilities for many typical\ndata products that are stored in a variety of file formats or databases.\nAutoplot has proven to be a flexible tool for exploring, accessing, and viewing\ndata resources as typically found on the web, usually in the form of a\ndirectory containing data files with multiple parameters contained in each\nfile. Data from a data source is abstracted into a common internal data model\ncalled QDataSet. Autoplot is built from individually useful components, and can\nbe extended and reused to create specialized data handling and analysis\napplications and is being used in a variety of science visualization and\nanalysis applications. Although originally developed for viewing\nheliophysics-related time series and spectrograms, its flexible and generic\ndata representation model makes it potentially useful for the Earth sciences."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1005.3181v1", 
    "title": "Multi-sensorial interaction with a nano-scale phenomenon : the force   curve", 
    "arxiv-id": "1005.3181v1", 
    "author": "Florence Marchi", 
    "publish": "2010-05-18T13:01:46Z", 
    "summary": "Using Atomic Force Microscopes (AFM) to manipulate nano-objects is an actual\nchallenge for surface scientists. Basic haptic interfacesbetween the AFM and\nexperimentalists have already been implemented. Themulti-sensory renderings\n(seeing, hearing and feeling) studied from acognitive point of view increase\nthe efficiency of the actual interfaces. Toallow the experimentalist to feel\nand touch the nano-world, we add mixedrealities between an AFM and a force\nfeedback device, enriching thus thedirect connection by a modeling engine. We\npresent in this paper the firstresults from a real-time remote-control handling\nof an AFM by our ForceFeedback Gestural Device through the example of the\napproach-retract curve."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1005.3185v1", 
    "title": "Dynamical issues in interactive representation of physical objects", 
    "arxiv-id": "1005.3185v1", 
    "author": "Daniela Urma", 
    "publish": "2010-05-18T13:08:00Z", 
    "summary": "The quality of a simulator equipped with a haptic interface is given by the\ndynamical properties of its components: haptic interface, simulator and control\nsystem. Some application areas of such kind of simulator like musical\nsynthesis, animation or more general, instrumental art have specific\nrequirements as for the \"haptic rendering\" of small movements that go beyond\nthe usual haptic interfaces allow. Object properties variability and different\nsituations of object combination represent important aspects of such type of\napplication which makes that the user can be interested as much in the\nrestitution of certain global properties of an entire object domain as in the\nrestitution of properties that are specific to an isolate object. In the\ntraditional approaches, the usual criteria are founded on the paradigm of\ntransparency and are related to the impedance error introduced by the technical\naspects of the system. As a general aim, rather than to minimize these effects,\nwe look to characterize them by physical metaphors conferring to haptic medium\nthe role of a tool. This positioning leads to firstly analyze the natural human\nobject interaction as a simplified evolutive system and then considers its\nsynthesis in the case of the interactive physical simulation. By means of a\nfrequential method, this approach is presented for some elementary\nconfigurations of the simulator"
},{
    "category": "cs.MS", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1005.3992v2", 
    "title": "Groebner bases in Java with applications in computer graphics", 
    "arxiv-id": "1005.3992v2", 
    "author": "Milan Z. Campara", 
    "publish": "2010-05-19T11:26:36Z", 
    "summary": "In this paper we present a Java implementation of the algorithm that computes\nBuchbereger's and reduced Groebner's basis step by step. The Java application\nenables graphical representation of the intersection of two surfaces in\n3-dimensional space and determines conditions of existence and planarity of the\nintersection."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1005.4564v1", 
    "title": "A basic gesture and motion format for virtual reality multisensory   applications", 
    "arxiv-id": "1005.4564v1", 
    "author": "Jean-Loup Florens", 
    "publish": "2010-05-25T13:16:29Z", 
    "summary": "The question of encoding movements such as those produced by human gestures\nmay become central in the coming years, given the growing importance of\nmovement data exchanges between heterogeneous systems and applications (musical\napplications, 3D motion control, virtual reality interaction, etc.). For the\npast 20 years, various formats have been proposed for encoding movement,\nespecially gestures. Though, these formats, at different degrees, were designed\nin the context of quite specific applications (character animation, motion\ncapture, musical gesture, biomechanical concerns...). The article introduce a\nnew file format, called GMS (for 'Gesture and Motion Signal'), with the aim of\nbeing more low-level and generic, by defining the minimal features a format\ncarrying movement/gesture information needs, rather than by gathering all the\ninformation generally given by the existing formats. The article argues that,\ngiven its growing presence in virtual reality situations, the \"gesture signal\"\nitself must be encoded, and that a specific format is needed. The proposed\nformat features the inner properties of such signals: dimensionality,\nstructural features, types of variables, and spatial and temporal properties.\nThe article first reviews the various situations with multisensory virtual\nobjects in which gesture controls intervene. The proposed format is then\ndeduced, as a mean to encode such versatile and variable \"gestural and animated\nscene\"."
},{
    "category": "nlin.CD", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1006.3661v1", 
    "title": "Fractal Basins and Boundaries in 2D Maps inspired in Discrete Population   Models", 
    "arxiv-id": "1006.3661v1", 
    "author": "Ricardo Lopez-Ruiz", 
    "publish": "2010-06-18T10:40:23Z", 
    "summary": "Two-dimensional maps can model interactions between populations. Despite\ntheir simplicity, these dynamical systems can show some complex situations, as\nmultistability or fractal boundaries between basins that lead to remarkable\npictures. Some of them are shown and explained here for three different 2D\ndiscrete models."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1008.0502v2", 
    "title": "Fully automatic extraction of salient objects from videos in near   real-time", 
    "arxiv-id": "1008.0502v2", 
    "author": "Shigeru Takagi", 
    "publish": "2010-08-03T10:00:07Z", 
    "summary": "Automatic video segmentation plays an important role in a wide range of\ncomputer vision and image processing applications. Recently, various methods\nhave been proposed for this purpose. The problem is that most of these methods\nare far from real-time processing even for low-resolution videos due to the\ncomplex procedures. To this end, we propose a new and quite fast method for\nautomatic video segmentation with the help of 1) efficient optimization of\nMarkov random fields with polynomial time of number of pixels by introducing\ngraph cuts, 2) automatic, computationally efficient but stable derivation of\nsegmentation priors using visual saliency and sequential update mechanism, and\n3) an implementation strategy in the principle of stream processing with\ngraphics processor units (GPUs). Test results indicates that our method\nextracts appropriate regions from videos as precisely as and much faster than\nprevious semi-automatic methods even though any supervisions have not been\nincorporated."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s12145-010-0049-0", 
    "link": "http://arxiv.org/pdf/1008.1188v1", 
    "title": "Data visualization in political and social sciences", 
    "arxiv-id": "1008.1188v1", 
    "author": "Andrei Zinovyev", 
    "publish": "2010-08-06T13:29:11Z", 
    "summary": "The basic objective of data visualization is to provide an efficient\ngraphical display for summarizing and reasoning about quantitative information.\nDuring the last decades, political science has accumulated a large corpus of\nvarious kinds of data such as comprehensive factbooks and atlases,\ncharacterizing all or most of existing states by multiple and objectively\nassessed numerical indicators within certain time lapse. As a consequence,\nthere exists a continuous trend for political science to gradually become a\nmore quantitative scientific field and to use quantitative information in the\nanalysis and reasoning. It is believed that any objective analysis in political\nscience must be multidimensional and combine various sources of quantitative\ninformation; however, human capabilities for perception of large massifs of\nnumerical information are limited. Hence, methods and approaches for\nvisualization of quantitative and qualitative data (and, especially\nmultivariate data) is an extremely important topic. Data visualization\napproaches can be classified into several groups, starting from creating\ninformative charts and diagrams (statistical graphics and infographics) and\nending with advanced statistical methods for visualizing multidimensional\ntables containing both quantitative and qualitative information. In this\narticle we provide a short review of existing methods of data visualization\nmethods with applications in political and social science."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1086/657902", 
    "link": "http://arxiv.org/pdf/1008.2205v3", 
    "title": "Viewpoints: A high-performance high-dimensional exploratory data   analysis tool", 
    "arxiv-id": "1008.2205v3", 
    "author": "M. J. Way", 
    "publish": "2010-08-12T20:00:28Z", 
    "summary": "Scientific data sets continue to increase in both size and complexity. In the\npast, dedicated graphics systems at supercomputing centers were required to\nvisualize large data sets, but as the price of commodity graphics hardware has\ndropped and its capability has increased, it is now possible, in principle, to\nview large complex data sets on a single workstation. To do this in practice,\nan investigator will need software that is written to take advantage of the\nrelevant graphics hardware. The Viewpoints visualization package described\nherein is an example of such software. Viewpoints is an interactive tool for\nexploratory visual analysis of large, high-dimensional (multivariate) data. It\nleverages the capabilities of modern graphics boards (GPUs) to run on a single\nworkstation or laptop. Viewpoints is minimalist: it attempts to do a small set\nof useful things very well (or at least very quickly) in comparison with\nsimilar packages today. Its basic feature set includes linked scatter plots\nwith brushing, dynamic histograms, normalization and outlier detection/removal.\nViewpoints was originally designed for astrophysicists, but it has since been\nused in a variety of fields that range from astronomy, quantum chemistry, fluid\ndynamics, machine learning, bioinformatics, and finance to information\ntechnology server log mining. In this article, we describe the Viewpoints\npackage and show examples of its usage."
},{
    "category": "cs.PL", 
    "doi": "10.1109/PST.2011.5971973", 
    "link": "http://arxiv.org/pdf/1009.5423v2", 
    "title": "The Need to Support of Data Flow Graph Visualization of Forensic Lucid   Programs, Forensic Evidence, and their Evaluation by GIPSY", 
    "arxiv-id": "1009.5423v2", 
    "author": "Mourad Debbabi", 
    "publish": "2010-09-28T01:30:40Z", 
    "summary": "Lucid programs are data-flow programs and can be visually represented as data\nflow graphs (DFGs) and composed visually. Forensic Lucid, a Lucid dialect, is a\nlanguage to specify and reason about cyberforensic cases. It includes the\nencoding of the evidence (representing the context of evaluation) and the crime\nscene modeling in order to validate claims against the model and perform event\nreconstruction, potentially within large swaths of digital evidence. To aid\ninvestigators to model the scene and evaluate it, instead of typing a Forensic\nLucid program, we propose to expand the design and implementation of the Lucid\nDFG programming onto Forensic Lucid case modeling and specification to enhance\nthe usability of the language and the system and its behavior. We briefly\ndiscuss the related work on visual programming an DFG modeling in an attempt to\ndefine and select one approach or a composition of approaches for Forensic\nLucid based on various criteria such as previous implementation, wide use,\nformal backing in terms of semantics and translation. In the end, we solicit\nthe readers' constructive, opinions, feedback, comments, and recommendations\nwithin the context of this short discussion."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cag.2011.12.008", 
    "link": "http://arxiv.org/pdf/1011.1787v4", 
    "title": "Volume-Enclosing Surface Extraction", 
    "arxiv-id": "1011.1787v4", 
    "author": "B. R. Schlei", 
    "publish": "2010-11-01T08:53:59Z", 
    "summary": "In this paper we present a new method, which allows for the construction of\ntriangular isosurfaces from three-dimensional data sets, such as 3D image data\nand/or numerical simulation data that are based on regularly shaped, cubic\nlattices. This novel volume-enclosing surface extraction technique, which has\nbeen named VESTA, can produce up to six different results due to the nature of\nthe discretized 3D space under consideration. VESTA is neither template-based\nnor it is necessarily required to operate on 2x2x2 voxel cell neighborhoods\nonly. The surface tiles are determined with a very fast and robust construction\ntechnique while potential ambiguities are detected and resolved. Here, we\nprovide an in-depth comparison between VESTA and various versions of the\nwell-known and very popular Marching Cubes algorithm for the very first time.\nIn an application section, we demonstrate the extraction of VESTA isosurfaces\nfor various data sets ranging from computer tomographic scan data to simulation\ndata of relativistic hydrodynamic fireball expansions."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cag.2011.12.008", 
    "link": "http://arxiv.org/pdf/1011.3583v1", 
    "title": "Fast GPGPU Data Rearrangement Kernels using CUDA", 
    "arxiv-id": "1011.3583v1", 
    "author": "Babu Narayanan", 
    "publish": "2010-11-16T04:38:53Z", 
    "summary": "Many high performance-computing algorithms are bandwidth limited, hence the\nneed for optimal data rearrangement kernels as well as their easy integration\ninto the rest of the application. In this work, we have built a CUDA library of\nfast kernels for a set of data rearrangement operations. In particular, we have\nbuilt generic kernels for rearranging m dimensional data into n dimensions,\nincluding Permute, Reorder, Interlace/De-interlace, etc. We have also built\nkernels for generic Stencil computations on a two-dimensional data using\ntemplates and functors that allow application developers to rapidly build\ncustomized high performance kernels. All the kernels built achieve or surpass\nbest-known performance in terms of bandwidth utilization."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cag.2011.12.008", 
    "link": "http://arxiv.org/pdf/1012.3359v2", 
    "title": "Curve Reconstruction in Riemannian Manifolds: Ordering Motion Frames", 
    "arxiv-id": "1012.3359v2", 
    "author": "Samaresh Chatterji", 
    "publish": "2010-12-15T15:28:37Z", 
    "summary": "In this article we extend the computational geometric curve reconstruction\napproach to curves in Riemannian manifolds. We prove that the minimal spanning\ntree, given a sufficiently dense sample, correctly reconstructs the smooth arcs\nand further closed and simple curves in Riemannian manifolds. The proof is\nbased on the behaviour of the curve segment inside the tubular neighbourhood of\nthe curve. To take care of the local topological changes of the manifold, the\ntubular neighbourhood is constructed in consideration with the injectivity\nradius of the underlying Riemannian manifold. We also present examples of\nsuccessfully reconstructed curves and show an applications of curve\nreconstruction to ordering motion frames."
},{
    "category": "cs.GR", 
    "doi": "10.1364/AO.50.005042", 
    "link": "http://arxiv.org/pdf/1101.0301v1", 
    "title": "Specular holography", 
    "arxiv-id": "1101.0301v1", 
    "author": "Matthew Brand", 
    "publish": "2010-12-31T21:23:11Z", 
    "summary": "By tooling an spot-illuminated surface to control the flow of specular glints\nunder motion, one can produce holographic view-dependent imagery. This paper\npresents the differential equation that governs the shape of the specular\nsurfaces, and illustrates how solutions can be constructed for different kinds\nof motion, lighting, host surface geometries, and fabrication constraints,\nleading to some novel forms of holography."
},{
    "category": "cs.CG", 
    "doi": "10.1364/AO.50.005042", 
    "link": "http://arxiv.org/pdf/1102.3165v1", 
    "title": "An Approximation Algorithm for Computing Shortest Paths in Weighted 3-d   Domains", 
    "arxiv-id": "1102.3165v1", 
    "author": "Joerg-Rudiger Sack", 
    "publish": "2011-02-15T19:50:45Z", 
    "summary": "We present the first polynomial time approximation algorithm for computing\nshortest paths in weighted three-dimensional domains. Given a polyhedral domain\n$\\D$, consisting of $n$ tetrahedra with positive weights, and a real number\n$\\eps\\in(0,1)$, our algorithm constructs paths in $\\D$ from a fixed source\nvertex to all vertices of $\\D$, whose costs are at most $1+\\eps$ times the\ncosts of (weighted) shortest paths, in\n$O(\\C(\\D)\\frac{n}{\\eps^{2.5}}\\log\\frac{n}{\\eps}\\log^3\\frac{1}{\\eps})$ time,\nwhere $\\C(\\D)$ is a geometric parameter related to the aspect ratios of\ntetrahedra. The efficiency of the proposed algorithm is based on an in-depth\nstudy of the local behavior of geodesic paths and additive Voronoi diagrams in\nweighted three-dimensional domains, which are of independent interest. The\npaper extends the results of Aleksandrov, Maheshwari and Sack [JACM 2005] to\nthree dimensions."
},{
    "category": "cs.IT", 
    "doi": "10.1364/AO.50.005042", 
    "link": "http://arxiv.org/pdf/1107.1525v1", 
    "title": "Accelerating Lossless Data Compression with GPUs", 
    "arxiv-id": "1107.1525v1", 
    "author": "P. Bangalore", 
    "publish": "2011-06-21T22:55:02Z", 
    "summary": "Huffman compression is a statistical, lossless, data compression algorithm\nthat compresses data by assigning variable length codes to symbols, with the\nmore frequently appearing symbols given shorter codes than the less. This work\nis a modification of the Huffman algorithm which permits uncompressed data to\nbe decomposed into indepen- dently compressible and decompressible blocks,\nallowing for concurrent compression and decompression on multiple processors.\nWe create implementations of this modified algorithm on a current NVIDIA GPU\nusing the CUDA API as well as on a current Intel chip and the performance\nresults are compared, showing favorable GPU performance for nearly all tests.\nLastly, we discuss the necessity for high performance data compression in\ntoday's supercomputing ecosystem."
},{
    "category": "physics.data-an", 
    "doi": "10.1063/1.3647201", 
    "link": "http://arxiv.org/pdf/1108.5673v1", 
    "title": "Partial wave analysis at BES III harnessing the power of GPUs", 
    "arxiv-id": "1108.5673v1", 
    "author": "Niklaus Berger", 
    "publish": "2011-08-29T17:56:15Z", 
    "summary": "Partial wave analysis is a core tool in hadron spectroscopy. With the high\nstatistics data available at facilities such as the Beijing Spectrometer III,\nthis procedure becomes computationally very expensive. We have successfully\nimplemented a framework for performing partial wave analysis on graphics\nprocessors. We discuss the implementation, the parallel computing frameworks\nemployed and the performance achieved, with a focus on the recent transition to\nthe OpenCL framework."
},{
    "category": "math.NA", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1110.3649v3", 
    "title": "Algorithms to automatically quantify the geometric similarity of   anatomical surfaces", 
    "arxiv-id": "1110.3649v3", 
    "author": "I. Daubechies", 
    "publish": "2011-10-17T12:23:30Z", 
    "summary": "We describe new approaches for distances between pairs of 2-dimensional\nsurfaces (embedded in 3-dimensional space) that use local structures and global\ninformation contained in inter-structure geometric relationships. We present\nalgorithms to automatically determine these distances as well as geometric\ncorrespondences. This is motivated by the aspiration of students of natural\nscience to understand the continuity of form that unites the diversity of life.\nAt present, scientists using physical traits to study evolutionary\nrelationships among living and extinct animals analyze data extracted from\ncarefully defined anatomical correspondence points (landmarks). Identifying and\nrecording these landmarks is time consuming and can be done accurately only by\ntrained morphologists. This renders these studies inaccessible to\nnon-morphologists, and causes phenomics to lag behind genomics in elucidating\nevolutionary patterns. Unlike other algorithms presented for morphological\ncorrespondences our approach does not require any preliminary marking of\nspecial features or landmarks by the user. It also differs from other seminal\nwork in computational geometry in that our algorithms are polynomial in nature\nand thus faster, making pairwise comparisons feasible for significantly larger\nnumbers of digitized surfaces. We illustrate our approach using three datasets\nrepresenting teeth and different bones of primates and humans, and show that it\nleads to highly accurate results."
},{
    "category": "cs.OS", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1110.4623v1", 
    "title": "Efficient Synchronization Primitives for GPUs", 
    "arxiv-id": "1110.4623v1", 
    "author": "John D. Owens", 
    "publish": "2011-10-20T19:43:58Z", 
    "summary": "In this paper, we revisit the design of synchronization\nprimitives---specifically barriers, mutexes, and semaphores---and how they\napply to the GPU. Previous implementations are insufficient due to the\ndiscrepancies in hardware and programming model of the GPU and CPU. We create\nnew implementations in CUDA and analyze the performance of spinning on the GPU,\nas well as a method of sleeping on the GPU, by running a set of memory-system\nbenchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class\nGPUs from NVIDIA. From our results we define higher-level principles that are\nvalid for generic many-core processors, the most important of which is to limit\nthe number of atomic accesses required for a synchronization operation because\natomic accesses are slower than regular memory accesses. We use the results of\nthe benchmarks to critique existing synchronization algorithms and guide our\nnew implementations, and then define an abstraction of GPUs to classify any GPU\nbased on the behavior of the memory system. We use this abstraction to create\nsuitable implementations of the primitives specifically targeting the GPU, and\nanalyze the performance of these algorithms on Tesla and Fermi. We then predict\nperformance on future GPUs based on characteristics of the abstraction. We also\nexamine the roles of spin waiting and sleep waiting in each primitive and how\ntheir performance varies based on the machine abstraction, then give a set of\nguidelines for when each strategy is useful based on the characteristics of the\nGPU and expected contention."
},{
    "category": "cs.CV", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1110.5015v1", 
    "title": "Spectral descriptors for deformable shapes", 
    "arxiv-id": "1110.5015v1", 
    "author": "Alexander M. Bronstein", 
    "publish": "2011-10-23T04:26:03Z", 
    "summary": "Informative and discriminative feature descriptors play a fundamental role in\ndeformable shape analysis. For example, they have been successfully employed in\ncorrespondence, registration, and retrieval tasks. In the recent years,\nsignificant attention has been devoted to descriptors obtained from the\nspectral decomposition of the Laplace-Beltrami operator associated with the\nshape. Notable examples in this family are the heat kernel signature (HKS) and\nthe wave kernel signature (WKS). Laplacian-based descriptors achieve\nstate-of-the-art performance in numerous shape analysis tasks; they are\ncomputationally efficient, isometry-invariant by construction, and can\ngracefully cope with a variety of transformations. In this paper, we formulate\na generic family of parametric spectral descriptors. We argue that in order to\nbe optimal for a specific task, the descriptor should take into account the\nstatistics of the corpus of shapes to which it is applied (the \"signal\") and\nthose of the class of transformations to which it is made insensitive (the\n\"noise\"). While such statistics are hard to model axiomatically, they can be\nlearned from examples. Following the spirit of the Wiener filter in signal\nprocessing, we show a learning scheme for the construction of optimal spectral\ndescriptors and relate it to Mahalanobis metric learning. The superiority of\nthe proposed approach is demonstrated on the SHREC'10 benchmark."
},{
    "category": "stat.CO", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1111.1400v1", 
    "title": "Student's T Robust Bundle Adjustment Algorithm", 
    "arxiv-id": "1111.1400v1", 
    "author": "Michael Broxton", 
    "publish": "2011-11-06T10:56:13Z", 
    "summary": "Bundle adjustment (BA) is the problem of refining a visual reconstruction to\nproduce better structure and viewing parameter estimates. This problem is often\nformulated as a nonlinear least squares problem, where data arises from\ninterest point matching. Mismatched interest points cause serious problems in\nthis approach, as a single mismatch will affect the entire reconstruction. In\nthis paper, we propose a novel robust Student's t BA algorithm (RST-BA). We\nmodel reprojection errors using the heavy tailed Student's t-distribution, and\nuse an implicit trust region method to compute the maximum a posteriori (MAP)\nestimate of the camera and viewing parameters in this model. The resulting\nalgorithm exploits the sparse structure essential for reconstructing\nmulti-image scenarios, has the same time complexity as standard L2 bundle\nadjustment (L2-BA), and can be implemented with minimal changes to the standard\nleast squares framework. We show that the RST-BA is more accurate than either\nL2-BA or L2-BA with a sigma-edit rule for outlier removal for a range of\nsimulated error generation scenarios. The new method has also been used to\nreconstruct lunar topography using data from the NASA Apollo 15 orbiter, and we\npresent visual and quantitative comparisons of RST-BA and L2-BA methods for\nthis application. In particular, using the RST-BA algorithm we were able to\nreconstruct a DEM from unprocessed data with many outliers and no ground\ncontrol points, which was not possible with the L2-BA method."
},{
    "category": "cs.GR", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1111.6321v1", 
    "title": "Shape and Trajectory Tracking of Moving Obstacles", 
    "arxiv-id": "1111.6321v1", 
    "author": "Kamen Lozev", 
    "publish": "2011-11-28T00:10:06Z", 
    "summary": "This work presents new methods and algorithms for tracking the shape and\ntrajectory of moving reflecting obstacles with broken rays, or rays reflecting\nat an obstacle. While in tomography the focus of the reconstruction method is\nto recover the velocity structure of the domain, the shape and trajectory\nreconstruction procedure directly finds the shape and trajectory of the\nobstacle. The physical signal carrier for this innovative method are ultrasonic\nbeams. When the speed of sound is constant, the rays are straight line segments\nand the shape and trajectory of moving objects will be reconstructed with\nmethods based on the travel time equation and ellipsoid geometry. For variable\nspeed of sound, we start with the eikonal equation and a system of differential\nequations that has its origins in acoustics and seismology. In this case, the\nrays are curves that are not necessarily straight line segments and we develop\nalgorithms for shape and trajectory tracking based on the numerical solution of\nthese equations. We present methods and algorithms for shape and trajectory\ntracking of moving obstacles with reflected rays when the location of the\nreceiver of the reflected ray is not known in advance. The shape and trajectory\ntracking method is very efficient because it is not necessary for the reflected\nsignal to traverse the whole domain or the same path back to the transmitter.\nIt could be received close to the point of reflection or far away from the\ntransmitter. This optimizes the energy spent by transmitters for tracking the\nobject, reduces signal attenuation and improves image resolution. It is a safe\nand secure method. We also present algorithms for tracking the shape and\ntrajectory of absorbing obstacles. The new methods and algorithms for shape and\ntrajectory tracking enable new applications and an application to one-hop\nInternet routing is presented."
},{
    "category": "cs.CG", 
    "doi": "10.1073/pnas.1112822108", 
    "link": "http://arxiv.org/pdf/1201.2936v1", 
    "title": "Finding Convex Hulls Using Quickhull on the GPU", 
    "arxiv-id": "1201.2936v1", 
    "author": "John D. Owens", 
    "publish": "2012-01-13T17:48:47Z", 
    "summary": "We present a convex hull algorithm that is accelerated on commodity graphics\nhardware. We analyze and identify the hurdles of writing a recursive divide and\nconquer algorithm on the GPU and divise a framework for representing this class\nof problems. Our framework transforms the recursive splitting step into a\npermutation step that is well-suited for graphics hardware. Our convex hull\nalgorithm of choice is Quickhull. Our parallel Quickhull implementation (for\nboth 2D and 3D cases) achieves an order of magnitude speedup over standard\ncomputational geometry libraries."
},{
    "category": "physics.ed-ph", 
    "doi": "10.1119/1.2730838", 
    "link": "http://arxiv.org/pdf/1201.3671v1", 
    "title": "Visualizing Flat Spacetime: Viewing Optical versus Special Relativistic   Effects", 
    "arxiv-id": "1201.3671v1", 
    "author": "F. Kuester", 
    "publish": "2012-01-17T23:58:24Z", 
    "summary": "A simple visual representation of Minkowski spacetime appropriate for a\nstudent with a background in geometry and algebra is presented. Minkowski\nspacetime can be modeled with a Euclidean 4-space to yield accurate\nvisualizations as predicted by special relativity theory. The contributions of\nrelativistic aberration as compared to classical pre-relativistic aberration to\nthe geometry are discussed in the context of its visual representation."
},{
    "category": "cs.MM", 
    "doi": "10.5121/ijcga.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.1808v1", 
    "title": "Personalised product design using virtual interactive techniques", 
    "arxiv-id": "1202.1808v1", 
    "author": "Surekha Mariam Varghese", 
    "publish": "2012-02-08T20:26:26Z", 
    "summary": "Use of Virtual Interactive Techniques for personalized product design is\ndescribed in this paper. Usually products are designed and built by considering\ngeneral usage patterns and Prototyping is used to mimic the static or working\nbehaviour of an actual product before manufacturing the product. The user does\nnot have any control on the design of the product. Personalized design\npostpones design to a later stage. It allows for personalized selection of\nindividual components by the user. This is implemented by displaying the\nindividual components over a physical model constructed using Cardboard or\nThermocol in the actual size and shape of the original product. The components\nof the equipment or product such as screen, buttons etc. are then projected\nusing a projector connected to the computer into the physical model. Users can\ninteract with the prototype like the original working equipment and they can\nselect, shape, position the individual components displayed on the interaction\npanel using simple hand gestures. Computer Vision techniques as well as sound\nprocessing techniques are used to detect and recognize the user gestures\ncaptured using a web camera and microphone."
},{
    "category": "cs.IR", 
    "doi": "10.5121/ijcga.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.1841v1", 
    "title": "Semantic Visualization and Navigation in Textual Corpus", 
    "arxiv-id": "1202.1841v1", 
    "author": "Mohamed BenAhmed", 
    "publish": "2012-02-08T21:38:11Z", 
    "summary": "This paper gives a survey of related work on the information visualization\ndomain and study the real integration of the cartography paradigms in actual\ninformation search systems. Based on this study, we propose a semantic\nvisualization and navigation approach which offer to users three search modes:\nprecise search, connotative search and thematic search. The objective is to\npropose to the users of an information search system, new interaction paradigms\nwhich support the semantic aspect of the considered information space and guide\nusers in their searches by assisting them to locate their interest center and\nto improve serendipity."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcga.2012.2101", 
    "link": "http://arxiv.org/pdf/1202.6609v2", 
    "title": "Towards an Integrated Visualization Of Semantically Enriched 3D City   Models: An Ontology of 3D Visualization Techniques", 
    "arxiv-id": "1202.6609v2", 
    "author": "Gilles Falquet", 
    "publish": "2012-02-29T17:15:53Z", 
    "summary": "3D city models - which represent in 3 dimensions the geometric elements of a\ncity - are increasingly used for an intended wide range of applications. Such\nuses are made possible by using semantically enriched 3D city models and by\npresenting such enriched 3D city models in a way that allows decision-making\nprocesses to be carried out from the best choices among sets of objectives, and\nacross issues and scales. In order to help in such a decision-making process we\nhave defined a framework to find the best visualization technique(s) for a set\nof potentially heterogeneous data that have to be visualized within the same 3D\ncity model, in order to perform a given task in a specific context. We have\nchosen an ontology-based approach. This approach and the specification and use\nof the resulting ontology of 3D visualization techniques are described in this\npaper."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1071/AS12025", 
    "link": "http://arxiv.org/pdf/1205.0282v1", 
    "title": "A Distributed GPU-based Framework for real-time 3D Volume Rendering of   Large Astronomical Data Cubes", 
    "arxiv-id": "1205.0282v1", 
    "author": "D. G. Barnes", 
    "publish": "2012-05-01T23:17:42Z", 
    "summary": "We present a framework to interactively volume-render three-dimensional data\ncubes using distributed ray-casting and volume bricking over a cluster of\nworkstations powered by one or more graphics processing units (GPUs) and a\nmulti-core CPU. The main design target for this framework is to provide an\nin-core visualization solution able to provide three-dimensional interactive\nviews of terabyte-sized data cubes. We tested the presented framework using a\ncomputing cluster comprising 64 nodes with a total of 128 GPUs. The framework\nproved to be scalable to render a 204 GB data cube with an average of 30 frames\nper second. Our performance analyses also compare between using NVIDIA Tesla\n1060 and 2050 GPU architectures and the effect of increasing the visualization\noutput resolution on the rendering performance. Although our initial focus, and\nthe examples presented in this work, is volume rendering of spectral data cubes\nfrom radio astronomy, we contend that our approach has applicability to other\ndisciplines where close to real-time volume rendering of terabyte-order 3D data\nsets is a requirement."
},{
    "category": "cs.LG", 
    "doi": "10.1587/transinf.E96.D.1134", 
    "link": "http://arxiv.org/pdf/1206.4634v1", 
    "title": "Artist Agent: A Reinforcement Learning Approach to Automatic Stroke   Generation in Oriental Ink Painting", 
    "arxiv-id": "1206.4634v1", 
    "author": "Masashi Sugiyama", 
    "publish": "2012-06-18T15:14:24Z", 
    "summary": "Oriental ink painting, called Sumi-e, is one of the most appealing painting\nstyles that has attracted artists around the world. Major challenges in\ncomputer-based Sumi-e simulation are to abstract complex scene information and\ndraw smooth and natural brush strokes. To automatically find such strokes, we\npropose to model the brush as a reinforcement learning agent, and learn desired\nbrush-trajectories by maximizing the sum of rewards in the policy search\nframework. We also provide elaborate design of actions, states, and rewards\ntailored for a Sumi-e agent. The effectiveness of our proposed approach is\ndemonstrated through simulated Sumi-e experiments."
},{
    "category": "cs.GR", 
    "doi": "10.1587/transinf.E96.D.1134", 
    "link": "http://arxiv.org/pdf/1206.6850v1", 
    "title": "Visualization of Collaborative Data", 
    "arxiv-id": "1206.6850v1", 
    "author": "Christian R. Shelton", 
    "publish": "2012-06-27T16:24:29Z", 
    "summary": "Collaborative data consist of ratings relating two distinct sets of objects:\nusers and items. Much of the work with such data focuses on filtering:\npredicting unknown ratings for pairs of users and items. In this paper we focus\non the problem of visualizing the information. Given all of the ratings, our\ntask is to embed all of the users and items as points in the same Euclidean\nspace. We would like to place users near items that they have rated (or would\nrate) high, and far away from those they would give a low rating. We pose this\nproblem as a real-valued non-linear Bayesian network and employ Markov chain\nMonte Carlo and expectation maximization to find an embedding. We present a\nmetric by which to judge the quality of a visualization and compare our results\nto local linear embedding and Eigentaste on three real-world datasets."
},{
    "category": "cs.HC", 
    "doi": "10.1587/transinf.E96.D.1134", 
    "link": "http://arxiv.org/pdf/1208.1679v1", 
    "title": "Color Assessment and Transfer for Web Pages", 
    "arxiv-id": "1208.1679v1", 
    "author": "Ou Wu", 
    "publish": "2012-08-07T02:24:36Z", 
    "summary": "Colors play a particularly important role in both designing and accessing Web\npages. A well-designed color scheme improves Web pages' visual aesthetic and\nfacilitates user interactions. As far as we know, existing color assessment\nstudies focus on images; studies on color assessment and editing for Web pages\nare rare. This paper investigates color assessment for Web pages based on\nexisting online color theme-rating data sets and applies this assessment to Web\ncolor edit. This study consists of three parts. First, we study the extraction\nof a Web page's color theme. Second, we construct color assessment models that\nscore the color compatibility of a Web page by leveraging machine learning\ntechniques. Third, we incorporate the learned color assessment model into a new\napplication, namely, color transfer for Web pages. Our study combines\ntechniques from computer graphics, Web mining, computer vision, and machine\nlearning. Experimental results suggest that our constructed color assessment\nmodels are effective, and useful in the color transfer for Web pages, which has\nreceived little attention in both Web mining and computer graphics communities."
},{
    "category": "cs.GR", 
    "doi": "10.1587/transinf.E96.D.1134", 
    "link": "http://arxiv.org/pdf/1209.6560v1", 
    "title": "Sparse Modeling of Intrinsic Correspondences", 
    "arxiv-id": "1209.6560v1", 
    "author": "G. Sapiro", 
    "publish": "2012-09-28T16:05:37Z", 
    "summary": "We present a novel sparse modeling approach to non-rigid shape matching using\nonly the ability to detect repeatable regions. As the input to our algorithm,\nwe are given only two sets of regions in two shapes; no descriptors are\nprovided so the correspondence between the regions is not know, nor we know how\nmany regions correspond in the two shapes. We show that even with such scarce\ninformation, it is possible to establish very accurate correspondence between\nthe shapes by using methods from the field of sparse modeling, being this, the\nfirst non-trivial use of sparse models in shape correspondence. We formulate\nthe problem of permuted sparse coding, in which we solve simultaneously for an\nunknown permutation ordering the regions on two shapes and for an unknown\ncorrespondence in functional representation. We also propose a robust variant\ncapable of handling incomplete matches. Numerically, the problem is solved\nefficiently by alternating the solution of a linear assignment and a sparse\ncoding problem. The proposed methods are evaluated qualitatively and\nquantitatively on standard benchmarks containing both synthetic and scanned\nobjects."
},{
    "category": "math.DS", 
    "doi": "10.1587/transinf.E96.D.1134", 
    "link": "http://arxiv.org/pdf/1210.0228v1", 
    "title": "Invariance And Inner Fractals In Polynomial And Transcendental Fractals", 
    "arxiv-id": "1210.0228v1", 
    "author": "Partha P. Ghosh", 
    "publish": "2012-09-30T18:22:20Z", 
    "summary": "A lot of formal and informal recreational study took place in the fields of\nMeromorphic Maps, since Mandelbrot popularized the map z <- z^2 + c. An\nimmediate generalization of the Mandelbrot z <-z^n + c also known as the\nMultibrot family were also studied. In the current paper, general truncated\npolynomial maps of the form z <- \\sum_{p>=2} a_px^p +c are studied. Two\nfundamental properties of these polynomial maps are hereby presented. One of\nthem is the existence of shape preserving transformations on fractal images,\nand another one is the existence of embedded Multibrot fractals inside a\npolynomial fractal. Any transform expression with transcendental terms also\nshows embedded Multibrot fractals, due to Taylor series expansion possible on\nthe transcendental functions. We present a method by which existence of\nembedded fractals can be predicted. A gallery of images is presented alongside\nto showcase the findings."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-642-33863-2_13", 
    "link": "http://arxiv.org/pdf/1210.0880v1", 
    "title": "Schr\u00f6dinger Diffusion for Shape Analysis with Texture", 
    "arxiv-id": "1210.0880v1", 
    "author": "Ron Kimmel", 
    "publish": "2012-10-02T19:03:04Z", 
    "summary": "In recent years, quantities derived from the heat equation have become\npopular in shape processing and analysis of triangulated surfaces. Such\nmeasures are often robust with respect to different kinds of perturbations,\nincluding near-isometries, topological noise and partialities. Here, we propose\nto exploit the semigroup of a Schr\\\"{o}dinger operator in order to deal with\ntexture data, while maintaining the desirable properties of the heat kernel. We\ndefine a family of Schr\\\"{o}dinger diffusion distances analogous to the ones\nassociated to the heat kernels, and show that they are continuous under\nperturbations of the data. As an application, we introduce a method for\nretrieval of textured shapes through comparison of Schr\\\"{o}dinger diffusion\ndistance histograms with the earth's mover distance, and present some numerical\nexperiments showing superior performance compared to an analogous method that\nignores the texture."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-642-33863-2_13", 
    "link": "http://arxiv.org/pdf/1210.6192v1", 
    "title": "Textural Approach to Palmprint Identification", 
    "arxiv-id": "1210.6192v1", 
    "author": "Kasturika B ray", 
    "publish": "2012-10-23T10:52:31Z", 
    "summary": "Biometrics which use of human physiological characteristics for identifying\nan individual is now a widespread method of identification and authentication.\nBiometric identification is a technology which uses several image processing\ntechniques and describes the general procedure for identification and\nverification using feature extraction, storage and matching from the digitized\nimage of biometric characters such as Finger Print, Face, Iris or Palm Print.\nThe current paper uses palm print biometrics. Here we have presented an\nidentification approach using textural properties of palm print images. The\nelegance of the method is that the conventional edge detection technique is\nextended to suitably describe the texture features. In this technique all the\ncharacteristics of the palm such as principal lines, edges and wrinkles are\nconsidered with equal importance."
},{
    "category": "cs.MM", 
    "doi": "10.1007/978-3-642-33863-2_13", 
    "link": "http://arxiv.org/pdf/1210.8025v1", 
    "title": "Beltrami Representation and its applications to texture map and video   compression", 
    "arxiv-id": "1210.8025v1", 
    "author": "XianFeng Gu", 
    "publish": "2012-10-18T12:17:12Z", 
    "summary": "Surface parameterizations and registrations are important in computer\ngraphics and imaging, where 1-1 correspondences between meshes are computed. In\npractice, surface maps are usually represented and stored as 3D coordinates\neach vertex is mapped to, which often requires lots of storage memory. This\ncauses inconvenience in data transmission and data storage. To tackle this\nproblem, we propose an effective algorithm for compressing surface\nhomeomorphisms using Fourier approximation of the Beltrami representation. The\nBeltrami representation is a complex-valued function defined on triangular\nfaces of the surface mesh with supreme norm strictly less than 1. Under\nsuitable normalization, there is a 1-1 correspondence between the set of\nsurface homeomorphisms and the set of Beltrami representations. Hence, every\nbijective surface map is associated with a unique Beltrami representation.\nConversely, given a Beltrami representation, the corresponding bijective\nsurface map can be exactly reconstructed using the Linear Beltrami Solver\nintroduced in this paper. Using the Beltrami representation, the surface\nhomeomorphism can be easily compressed by Fourier approximation, without\ndistorting the bijectivity of the map. The storage memory can be effectively\nreduced, which is useful for many practical problems in computer graphics and\nimaging. In this paper, we proposed to apply the algorithm to texture map\ncompression and video compression. With our proposed algorithm, the storage\nrequirement for the texture properties of a textured surface can be\nsignificantly reduced. Our algorithm can further be applied to compressing\nmotion vector fields for video compression, which effectively improve the\ncompression ratio."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-33863-2_13", 
    "link": "http://arxiv.org/pdf/1211.0729v7", 
    "title": "RE2L: An Efficient Output-sensitive Algorithm for Computing Boolean   Operation on Circular-arc Polygons", 
    "arxiv-id": "1211.0729v7", 
    "author": "Minyi Guo", 
    "publish": "2012-11-04T22:38:11Z", 
    "summary": "The boundaries of \\textit{conic polygons} consist of conic segments or second\ndegree curves. The conic polygon has two degenerate or special cases: the\nlinear polygon and the circular-arc polygon. The natural problem --- boolean\noperation on linear polygons, has been \\textit{well} studied. Surprisingly,\n(almost) no article \\textit{focuses on} the problem of boolean operation on\ncircular-arc polygons, which actually can also find many applications, implying\nthat if there is a targeted solution for boolean operation on circular-arc\npolygons, which should be favourable for potential users. In this article, we\ndevise a concise data structure, and then develop a targeted algorithm called\nR{\\scriptsize E2L}. Our method is surprisingly simple, easy-to-implement but\nwithout loss of efficiency. Given two circular-arc polygons with $m$ and $n$\nedges respectively, we prove that the proposed method runs in $O(m+n+(l+k)\\log\nl)$ time, using $O(m+n+l+k)$ space, where $k$ is the number of intersections,\nand $l$ is the number of related edges. The experimental results show our\nproposed algorithm is significantly faster than the ones that are by directly\nappealing to the existing algorithms."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-642-33863-2_13", 
    "link": "http://arxiv.org/pdf/1211.2569v1", 
    "title": "Teichm\u00fcller extremal mapping and its applications to landmark matching   registration", 
    "arxiv-id": "1211.2569v1", 
    "author": "Xianfeng Gu", 
    "publish": "2012-11-12T11:16:31Z", 
    "summary": "Registration, which aims to find an optimal 1-1 correspondence between\nshapes, is an important process in different research areas. Conformal mappings\nhave been widely used to obtain a diffeomorphism between shapes that minimizes\nangular distortion. Conformal registrations are beneficial since it preserves\nthe local geometry well. However, when landmark constraints are enforced,\nconformal mappings generally do not exist. This motivates us to look for a\nunique landmark matching quasi-conformal registration, which minimizes the\nconformality distortion. Under suitable condition on the landmark constraints,\na unique diffeomporphism, called the Teichm\\\"uller extremal mapping between two\nsurfaces can be obtained, which minimizes the maximal conformality distortion.\nIn this paper, we propose an efficient iterative algorithm, called the\nQuasi-conformal (QC) iterations, to compute the Teichm\\\"uller mapping. The\nbasic idea is to represent the set of diffeomorphisms using Beltrami\ncoefficients (BCs), and look for an optimal BC associated to the desired\nTeichm\\\"uller mapping. The associated diffeomorphism can be efficiently\nreconstructed from the optimal BC using the Linear Beltrami Solver(LBS). Using\nBCs to represent diffeomorphisms guarantees the diffeomorphic property of the\nregistration. Using our proposed method, the Teichm\\\"uller mapping can be\naccurately and efficiently computed within 10 seconds. The obtained\nregistration is guaranteed to be bijective. The proposed algorithm can also be\nextended to compute Teichm\\\"uller mapping with soft landmark constraints. We\napplied the proposed algorithm to real applications, such as brain landmark\nmatching registration, constrained texture mapping and human face registration.\nExperimental results shows that our method is both effective and efficient in\ncomputing a non-overlap landmark matching registration with least amount of\nconformality distortion."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1093/mnras/sts513", 
    "link": "http://arxiv.org/pdf/1211.4896v1", 
    "title": "Tera-scale Astronomical Data Analysis and Visualization", 
    "arxiv-id": "1211.4896v1", 
    "author": "V. A. Kilborn", 
    "publish": "2012-11-20T23:00:51Z", 
    "summary": "We present a high-performance, graphics processing unit (GPU)-based framework\nfor the efficient analysis and visualization of (nearly) terabyte (TB)-sized\n3-dimensional images. Using a cluster of 96 GPUs, we demonstrate for a 0.5 TB\nimage: (1) volume rendering using an arbitrary transfer function at 7--10\nframes per second; (2) computation of basic global image statistics such as the\nmean intensity and standard deviation in 1.7 s; (3) evaluation of the image\nhistogram in 4 s; and (4) evaluation of the global image median intensity in\njust 45 s. Our measured results correspond to a raw computational throughput\napproaching one teravoxel per second, and are 10--100 times faster than the\nbest possible performance with traditional single-node, multi-core CPU\nimplementations. A scalability analysis shows the framework will scale well to\nimages sized 1 TB and beyond. Other parallel data analysis algorithms can be\nadded to the framework with relative ease, and accordingly, we present our\nframework as a possible solution to the image analysis and visualization\nrequirements of next-generation telescopes, including the forthcoming Square\nKilometre Array pathfinder radiotelescopes."
},{
    "category": "math.DG", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1212.0981v1", 
    "title": "A Conformal Approach for Surface Inpainting", 
    "arxiv-id": "1212.0981v1", 
    "author": "Xianfeng Gu", 
    "publish": "2012-12-05T10:01:23Z", 
    "summary": "We address the problem of surface inpainting, which aims to fill in holes or\nmissing regions on a Riemann surface based on its surface geometry. In\npractical situation, surfaces obtained from range scanners often have holes\nwhere the 3D models are incomplete. In order to analyze the 3D shapes\neffectively, restoring the incomplete shape by filling in the surface holes is\nnecessary. In this paper, we propose a novel conformal approach to inpaint\nsurface holes on a Riemann surface based on its surface geometry. The basic\nidea is to represent the Riemann surface using its conformal factor and mean\ncurvature. According to Riemann surface theory, a Riemann surface can be\nuniquely determined by its conformal factor and mean curvature up to a rigid\nmotion. Given a Riemann surface $S$, its mean curvature $H$ and conformal\nfactor $\\lambda$ can be computed easily through its conformal parameterization.\nConversely, given $\\lambda$ and $H$, a Riemann surface can be uniquely\nreconstructed by solving the Gauss-Codazzi equation on the conformal parameter\ndomain. Hence, the conformal factor and the mean curvature are two geometric\nquantities fully describing the surface. With this $\\lambda$-$H$ representation\nof the surface, the problem of surface inpainting can be reduced to the problem\nof image inpainting of $\\lambda$ and $H$ on the conformal parameter domain.\nOnce $\\lambda$ and $H$ are inpainted, a Riemann surface can be reconstructed\nwhich effectively restores the 3D surface with missing holes. Since the\ninpainting model is based on the geometric quantities $\\lambda$ and $H$, the\nrestored surface follows the surface geometric pattern. We test the proposed\nalgorithm on synthetic data as well as real surface data. Experimental results\nshow that our proposed method is an effective surface inpainting algorithm to\nfill in surface holes on an incomplete 3D models based their surface geometry."
},{
    "category": "cs.CG", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1212.1617v2", 
    "title": "Similarity of Polygonal Curves in the Presence of Outliers", 
    "arxiv-id": "1212.1617v2", 
    "author": "Christian Scheffer", 
    "publish": "2012-12-07T14:22:12Z", 
    "summary": "The Fr\\'{e}chet distance is a well studied and commonly used measure to\ncapture the similarity of polygonal curves. Unfortunately, it exhibits a high\nsensitivity to the presence of outliers. Since the presence of outliers is a\nfrequently occurring phenomenon in practice, a robust variant of Fr\\'{e}chet\ndistance is required which absorbs outliers. We study such a variant here. In\nthis modified variant, our objective is to minimize the length of subcurves of\ntwo polygonal curves that need to be ignored (MinEx problem), or alternately,\nmaximize the length of subcurves that are preserved (MaxIn problem), to achieve\na given Fr\\'{e}chet distance. An exact solution to one problem would imply an\nexact solution to the other problem. However, we show that these problems are\nnot solvable by radicals over $\\mathbb{Q}$ and that the degree of the\npolynomial equations involved is unbounded in general. This motivates the\nsearch for approximate solutions. We present an algorithm, which approximates,\nfor a given input parameter $\\delta$, optimal solutions for the \\MinEx\\ and\n\\MaxIn\\ problems up to an additive approximation error $\\delta$ times the\nlength of the input curves. The resulting running time is upper bounded by\n$\\mathcal{O} \\left(\\frac{n^3}{\\delta} \\log \\left(\\frac{n}{\\delta}\n\\right)\\right)$, where $n$ is the complexity of the input polygonal curves."
},{
    "category": "cs.GR", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1212.2845v1", 
    "title": "Dynamic Simulation of Soft Heterogeneous Objects", 
    "arxiv-id": "1212.2845v1", 
    "author": "Hod Lipson", 
    "publish": "2012-12-12T15:41:08Z", 
    "summary": "This paper describes a 2D and 3D simulation engine that quantitatively models\nthe statics, dynamics, and non-linear deformation of heterogeneous soft bodies\nin a computationally efficient manner. There is a large body of work simulating\ncompliant mechanisms. These normally assume small deformations with homogeneous\nmaterial properties actuated with external forces. There is also a large body\nof research on physically-based deformable objects for applications in computer\ngraphics with the purpose of generating realistic appearances at the expense of\naccuracy. Here we present a simulation framework in which an object may be\ncomposed of any number of interspersed materials with varying properties\n(stiffness, density, etc.) to enable true heterogeneous multi-material\nsimulation. Collisions are handled to prevent self-penetration due to large\ndeformation, which also allows multiple bodies to interact. A volumetric\nactuation method is implemented to impart motion to the structures which opens\nthe door to the design of novel structures and mechanisms. The simulator was\nimplemented efficiently such that objects with thousands of degrees of freedom\ncan be simulated at suitable framerates for user interaction using a single\nthread of a typical desktop computer. The code is written in platform agnostic\nC++ and is fully open source. This research opens the door to the dynamic\nsimulation of freeform 3D multi-material mechanisms and objects in a manner\nsuitable for design automation."
},{
    "category": "cs.MM", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1212.6250v1", 
    "title": "Computer-Assisted Interactive Documentary and Performance Arts in   Illimitable Space", 
    "arxiv-id": "1212.6250v1", 
    "author": "Miao Song", 
    "publish": "2012-12-26T20:49:45Z", 
    "summary": "This major component of the research described in this thesis is 3D computer\ngraphics, specifically the realistic physics-based softbody simulation and\nhaptic responsive environments. Minor components include advanced\nhuman-computer interaction environments, non-linear documentary storytelling,\nand theatre performance. The journey of this research has been unusual because\nit requires a researcher with solid knowledge and background in multiple\ndisciplines; who also has to be creative and sensitive in order to combine the\npossible areas into a new research direction. [...] It focuses on the advanced\ncomputer graphics and emerges from experimental cinematic works and theatrical\nartistic practices. Some development content and installations are completed to\nprove and evaluate the described concepts and to be convincing. [...] To\nsummarize, the resulting work involves not only artistic creativity, but\nsolving or combining technological hurdles in motion tracking, pattern\nrecognition, force feedback control, etc., with the available documentary\nfootage on film, video, or images, and text via a variety of devices [....] and\nprogramming, and installing all the needed interfaces such that it all works in\nreal-time. Thus, the contribution to the knowledge advancement is in solving\nthese interfacing problems and the real-time aspects of the interaction that\nhave uses in film industry, fashion industry, new age interactive theatre,\ncomputer games, and web-based technologies and services for entertainment and\neducation. It also includes building up on this experience to integrate Kinect-\nand haptic-based interaction, artistic scenery rendering, and other forms of\ncontrol. This research work connects all the research disciplines, seemingly\ndisjoint fields of research, such as computer graphics, documentary film,\ninteractive media, and theatre performance together."
},{
    "category": "cs.CY", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1303.3077v2", 
    "title": "Using Mathematica & Matlab for CAGD/CAD research and education", 
    "arxiv-id": "1303.3077v2", 
    "author": "M. A Jamaludin", 
    "publish": "2013-03-13T02:22:41Z", 
    "summary": "In CAGD/CAD research and education, users are involved with development of\nmathematical algorithms and followed by the analysis of the resultant\nalgorithm. This process involves geometric display which can only be carried\nout with high end graphics display. There are many approaches practiced and one\nof the so-called easiest approaches is by using C/C++ programming language and\nOpenGL application program interface, API. There are practitioners uses C/C++\nprogramming language to develop the algorithms and finally utilize AutoCAD for\ngraphics display. On the other hand, high end CAD users manage to use Auto Lisp\nas their programming language in AutoCAD. Nevertheless, these traditional ways\nare definitely time consuming. This paper introduces an alternative method\nwhereby the practitioners may maximize scientific computation programs, SCPs:\nMathematica and MATLAB in the context of CAGD/CAD for research and education."
},{
    "category": "cs.CV", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1308.4908v1", 
    "title": "A Unified Framework for Multi-Sensor HDR Video Reconstruction", 
    "arxiv-id": "1308.4908v1", 
    "author": "Jonas Unger", 
    "publish": "2013-08-22T15:58:01Z", 
    "summary": "One of the most successful approaches to modern high quality HDR-video\ncapture is to use camera setups with multiple sensors imaging the scene through\na common optical system. However, such systems pose several challenges for HDR\nreconstruction algorithms. Previous reconstruction techniques have considered\ndebayering, denoising, resampling (align- ment) and exposure fusion as separate\nproblems. In contrast, in this paper we present a unifying approach, performing\nHDR assembly directly from raw sensor data. Our framework includes a camera\nnoise model adapted to HDR video and an algorithm for spatially adaptive HDR\nreconstruction based on fitting of local polynomial approximations to observed\nsensor data. The method is easy to implement and allows reconstruction to an\narbitrary resolution and output mapping. We present an implementation in CUDA\nand show real-time performance for an experimental 4 Mpixel multi-sensor HDR\nvideo system. We further show that our algorithm has clear advantages over\nexisting methods, both in terms of flexibility and reconstruction quality."
},{
    "category": "physics.optics", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1309.3007v1", 
    "title": "On the Relationship Between Dual Photography and Classical Ghost Imaging", 
    "arxiv-id": "1309.3007v1", 
    "author": "Pradeep Sen", 
    "publish": "2013-09-12T00:15:09Z", 
    "summary": "Classical ghost imaging has received considerable attention in recent years\nbecause of its remarkable ability to image a scene without direct observation\nby a light-detecting imaging device. In this article, we show that this imaging\nprocess is actually a realization of a paradigm known as dual photography,\nwhich has been shown to produce full-color dual (ghost) images of 3D objects\nwith complex materials without using a traditional imaging device.\nSpecifically, we demonstrate mathematically that the cross-correlation based\nmethods used to recover ghost images are equivalent to the light transport\nmeasurement process of dual photography. Because of this, we are able to\nprovide a new explanation for ghost imaging using only classical optics by\nleveraging the principle of reciprocity in classical electromagnetics. This\nobservation also shows how to leverage previous work on light transport\nacquisition and dual photography to improve ghost imaging systems in the\nfuture."
},{
    "category": "cs.GR", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1309.3314v1", 
    "title": "Progressive Compression of 3D Objects with an Adaptive Quantization", 
    "arxiv-id": "1309.3314v1", 
    "author": "Mohamed Salim Bouhlel", 
    "publish": "2013-09-12T21:47:37Z", 
    "summary": "This paper presents a new progressive compression method for triangular\nmeshes. This method, in fact, is based on a schema of irregular\nmulti-resolution analysis and is centered on the optimization of the\nrate-distortion trade-off. The quantization precision is adapted to each vertex\nduring the encoding / decoding process to optimize the rate-distortion\ncompromise. The Optimization of the treated mesh geometry improves the\napproximation quality and the compression ratio at each level of resolution.\nThe experimental results show that the proposed algorithm gives competitive\nresults compared to the previous works dealing with the rate-distortion\ncompromise."
},{
    "category": "cs.CV", 
    "doi": "10.3934/ipi.2013.7.863", 
    "link": "http://arxiv.org/pdf/1311.0119v1", 
    "title": "Structure-preserving color transformations using Laplacian commutativity", 
    "arxiv-id": "1311.0119v1", 
    "author": "Michael M. Bronstein", 
    "publish": "2013-11-01T08:48:36Z", 
    "summary": "Mappings between color spaces are ubiquitous in image processing problems\nsuch as gamut mapping, decolorization, and image optimization for color-blind\npeople. Simple color transformations often result in information loss and\nambiguities (for example, when mapping from RGB to grayscale), and one wishes\nto find an image-specific transformation that would preserve as much as\npossible the structure of the original image in the target color space. In this\npaper, we propose Laplacian colormaps, a generic framework for\nstructure-preserving color transformations between images. We use the image\nLaplacian to capture the structural information, and show that if the color\ntransformation between two images preserves the structure, the respective\nLaplacians have similar eigenvectors, or in other words, are approximately\njointly diagonalizable. Employing the relation between joint diagonalizability\nand commutativity of matrices, we use Laplacians commutativity as a criterion\nof color mapping quality and minimize it w.r.t. the parameters of a color\ntransformation to achieve optimal structure preservation. We show numerous\napplications of our approach, including color-to-gray conversion, gamut\nmapping, multispectral image fusion, and image optimization for color deficient\nviewers."
},{
    "category": "cs.LO", 
    "doi": "10.1109/TVCG.2012.294", 
    "link": "http://arxiv.org/pdf/1311.4376v1", 
    "title": "Understanding Visualization: A Formal Approach using Category Theory and   Semiotics", 
    "arxiv-id": "1311.4376v1", 
    "author": "Nick Rossiter", 
    "publish": "2013-11-18T13:51:27Z", 
    "summary": "This article combines the vocabulary of semiotics and category theory to\nprovide a formal analysis of visualization. It shows how familiar processes of\nvisualization fit the semiotic frameworks of both Saussure and Peirce, and\nextends these structures using the tools of category theory to provide a\ngeneral framework for understanding visualization in practice, including:\nrelationships between systems, data collected from those systems, renderings of\nthose data in the form of representations, the reading of those representations\nto create visualizations, and the use of those visualizations to create\nknowledge and understanding of the system under inspection. The resulting\nframework is validated by demonstrating how familiar information visualization\nconcepts (such as literalness, sensitivity, redundancy, ambiguity,\ngeneralizability, and chart junk) arise naturally from it and can be defined\nformally and precisely. This article generalizes previous work on the formal\ncharacterization of visualization by, inter alia, Ziemkiewicz and Kosara and\nallows us to formally distinguish properties of the visualization process that\nprevious work does not."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.comgeo.2016.05.005", 
    "link": "http://arxiv.org/pdf/1311.4665v1", 
    "title": "Analysis of Farthest Point Sampling for Approximating Geodesics in a   Graph", 
    "arxiv-id": "1311.4665v1", 
    "author": "Stefanie Wuhrer", 
    "publish": "2013-11-19T09:22:18Z", 
    "summary": "A standard way to approximate the distance between any two vertices $p$ and\n$q$ on a mesh is to compute, in the associated graph, a shortest path from $p$\nto $q$ that goes through one of $k$ sources, which are well-chosen vertices.\nPrecomputing the distance between each of the $k$ sources to all vertices of\nthe graph yields an efficient computation of approximate distances between any\ntwo vertices. One standard method for choosing $k$ sources, which has been used\nextensively and successfully for isometry-invariant surface processing, is the\nso-called Farthest Point Sampling (FPS), which starts with a random vertex as\nthe first source, and iteratively selects the farthest vertex from the already\nselected sources.\n  In this paper, we analyze the stretch factor $\\mathcal{F}_{FPS}$ of\napproximate geodesics computed using FPS, which is the maximum, over all pairs\nof distinct vertices, of their approximated distance over their geodesic\ndistance in the graph. We show that $\\mathcal{F}_{FPS}$ can be bounded in terms\nof the minimal value $\\mathcal{F}^*$ of the stretch factor obtained using an\noptimal placement of $k$ sources as $\\mathcal{F}_{FPS}\\leq 2 r_e^2\n\\mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths of\nthe longest and the shortest edges of the graph. This provides some evidence\nexplaining why farthest point sampling has been used successfully for\nisometry-invariant shape processing. Furthermore, we show that it is\nNP-complete to find $k$ sources that minimize the stretch factor."
},{
    "category": "cond-mat.dis-nn", 
    "doi": "10.1103/PhysRevE.90.012118", 
    "link": "http://arxiv.org/pdf/1402.6993v2", 
    "title": "Scaling hypothesis for the Euclidean bipartite matching problem", 
    "arxiv-id": "1402.6993v2", 
    "author": "Gabriele Sicuro", 
    "publish": "2014-02-27T17:52:34Z", 
    "summary": "We propose a simple yet very predictive form, based on a Poisson's equation,\nfor the functional dependence of the cost from the density of points in the\nEuclidean bipartite matching problem. This leads, for quadratic costs, to the\nanalytic prediction of the large $N$ limit of the average cost in dimension\n$d=1,2$ and of the subleading correction in higher dimension. A non-trivial\nscaling exponent, $\\gamma_d=\\frac{d-2}{d}$, which differs from the\nmonopartite's one, is found for the subleading correction. We argue that the\nsame scaling holds true for a generic cost exponent in dimension $d>2$."
},{
    "category": "math.DS", 
    "doi": "10.1098/rspa.2014.0639", 
    "link": "http://arxiv.org/pdf/1404.3109v3", 
    "title": "Automated detection of coherent Lagrangian vortices in two-dimensional   unsteady flows", 
    "arxiv-id": "1404.3109v3", 
    "author": "George Haller", 
    "publish": "2014-04-11T13:58:09Z", 
    "summary": "Coherent boundaries of Lagrangian vortices in fluid flows have recently been\nidentified as closed orbits of line fields associated with the Cauchy-Green\nstrain tensor. Here we develop a fully automated procedure for the detection of\nsuch closed orbits in large-scale velocity data sets. We illustrate the power\nof our method on ocean surface velocities derived from satellite altimetry."
},{
    "category": "cs.DM", 
    "doi": "10.1080/15427951.2014.977407", 
    "link": "http://arxiv.org/pdf/1404.5356v1", 
    "title": "Finding safe strategies for competitive diffusion on trees", 
    "arxiv-id": "1404.5356v1", 
    "author": "Celeste Vautour", 
    "publish": "2014-04-22T00:17:20Z", 
    "summary": "We study the two-player safe game of Competitive Diffusion, a game-theoretic\nmodel for the diffusion of technologies or influence through a social network.\nIn game theory, safe strategies are mixed strategies with a minimal expected\ngain against unknown strategies of the opponents. Safe strategies for\ncompetitive diffusion lead to maximum spread of influence in the presence of\nuncertainty about the other players. We study the safe game on two specific\nclasses of trees, spiders and complete trees, and give tight bounds on the\nminimal expected gain. We then use these results to give an algorithm which\nsuggests a safe strategy for a player on any tree. We test this algorithm on\nrandomly generated trees, and show that it finds strategies that are close to\noptimal."
},{
    "category": "cs.HC", 
    "doi": "10.1080/15427951.2014.977407", 
    "link": "http://arxiv.org/pdf/1409.5758v1", 
    "title": "Effects of Coupling in Human-Virtual Agent Body Interaction", 
    "arxiv-id": "1409.5758v1", 
    "author": "Pierre De Loor", 
    "publish": "2014-09-19T18:49:11Z", 
    "summary": "This paper presents a study of the dynamic coupling between a user and a\nvirtual character during body interaction. Coupling is directly linked with\nother dimensions, such as co-presence, engagement, and believability, and was\nmeasured in an experiment that allowed users to describe their subjective\nfeelings about those dimensions of interest. The experiment was based on a\ntheatrical game involving the imitation of slow upper-body movements and the\nproposal of new movements by the user and virtual agent. The agent's behaviour\nvaried in autonomy: the agent could limit itself to imitating the user's\nmovements only, initiate new movements, or combine both behaviours. After the\ngame, each participant completed a questionnaire regarding their engagement in\nthe interaction, their subjective feeling about the co-presence of the agent,\netc. Based on four main dimensions of interest, we tested several hypotheses\nagainst our experimental results, which are discussed here."
},{
    "category": "cs.GR", 
    "doi": "10.15120/GR-2015-1-FG-GENERAL-42", 
    "link": "http://arxiv.org/pdf/1501.06364v1", 
    "title": "GPU Programming - Speeding Up the 3D Surface Generator VESTA", 
    "arxiv-id": "1501.06364v1", 
    "author": "B. R. Schlei", 
    "publish": "2015-01-26T12:38:21Z", 
    "summary": "The novel \"Volume-Enclosing Surface exTraction Algorithm\" (VESTA) generates\ntriangular isosurfaces from computed tomography volumetric images and/or\nthree-dimensional (3D) simulation data. Here, we present various benchmarks for\nGPU-based code implementations of both VESTA and the current state-of-the-art\nMarching Cubes Algorithm (MCA). One major result of this study is that VESTA\nruns significantly faster than the MCA."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2010.05.005", 
    "link": "http://arxiv.org/pdf/1007.3726v1", 
    "title": "Multi-GPU Accelerated Multi-Spin Monte Carlo Simulations of the 2D Ising   Model", 
    "arxiv-id": "1007.3726v1", 
    "author": "Tobias Preis", 
    "publish": "2010-07-21T19:30:57Z", 
    "summary": "A modern graphics processing unit (GPU) is able to perform massively parallel\nscientific computations at low cost. We extend our implementation of the\ncheckerboard algorithm for the two dimensional Ising model [T. Preis et al., J.\nComp. Phys. 228, 4468 (2009)] in order to overcome the memory limitations of a\nsingle GPU which enables us to simulate significantly larger systems. Using\nmulti-spin coding techniques, we are able to accelerate simulations on a single\nGPU by factors up to 35 compared to an optimized single Central Processor Unit\n(CPU) core implementation which employs multi-spin coding. By combining the\nCompute Unified Device Architecture (CUDA) with the Message Parsing Interface\n(MPI) on the CPU level, a single Ising lattice can be updated by a cluster of\nGPUs in parallel. For large systems, the computation time scales nearly\nlinearly with the number of GPUs used. As proof of concept we reproduce the\ncritical temperature of the 2D Ising model using finite size scaling\ntechniques."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cpc.2010.05.005", 
    "link": "http://arxiv.org/pdf/1104.2580v2", 
    "title": "Hypothesize and Bound: A Computational Focus of Attention Mechanism for   Simultaneous N-D Segmentation, Pose Estimation and Classification Using Shape   Priors", 
    "arxiv-id": "1104.2580v2", 
    "author": "Ren\u00e9 Vidal", 
    "publish": "2011-04-13T18:59:52Z", 
    "summary": "Given the ever increasing bandwidth of the visual information available to\nmany intelligent systems, it is becoming essential to endow them with a sense\nof what is worthwhile their attention and what can be safely disregarded. This\narticle presents a general mathematical framework to efficiently allocate the\navailable computational resources to process the parts of the input that are\nrelevant to solve a given perceptual problem. By this we mean to find the\nhypothesis H (i.e., the state of the world) that maximizes a function L(H),\nrepresenting how well each hypothesis \"explains\" the input. Given the large\nbandwidth of the sensory input, fully evaluating L(H) for each hypothesis H is\ncomputationally infeasible (e.g., because it would imply checking a large\nnumber of pixels). To address this problem we propose a mathematical framework\nwith two key ingredients. The first one is a Bounding Mechanism (BM) to compute\nlower and upper bounds of L(H), for a given computational budget. These bounds\nare much cheaper to compute than L(H) itself, can be refined at any time by\nincreasing the budget allocated to a hypothesis, and are frequently enough to\ndiscard a hypothesis. To compute these bounds, we develop a novel theory of\nshapes and shape priors. The second ingredient is a Focus of Attention\nMechanism (FoAM) to select which hypothesis' bounds should be refined next,\nwith the goal of discarding non-optimal hypotheses with the least amount of\ncomputation. The proposed framework: 1) is very efficient since most hypotheses\nare discarded with minimal computation; 2) is parallelizable; 3) is guaranteed\nto find the globally optimal hypothesis; and 4) its running time depends on the\nproblem at hand, not on the bandwidth of the input. We instantiate the proposed\nframework for the problem of simultaneously estimating the class, pose, and a\nnoiseless version of a 2D shape in a 2D image."
},{
    "category": "cs.LO", 
    "doi": "10.1016/j.cpc.2010.05.005", 
    "link": "http://arxiv.org/pdf/1109.4095v2", 
    "title": "Kara: A System for Visualising and Visual Editing of Interpretations for   Answer-Set Programs", 
    "arxiv-id": "1109.4095v2", 
    "author": "Hans Tompits", 
    "publish": "2011-09-19T17:09:21Z", 
    "summary": "In answer-set programming (ASP), the solutions of a problem are encoded in\ndedicated models, called answer sets, of a logical theory. These answer sets\nare computed from the program that represents the theory by means of an ASP\nsolver and returned to the user as sets of ground first-order literals. As this\ntype of representation is often cumbersome for the user to interpret, tools\nlike ASPVIZ and IDPDraw were developed that allow for visualising answer sets.\nThe tool Kara, introduced in this paper, follows these approaches, using ASP\nitself as a language for defining visualisations of interpretations. Unlike\nexisting tools that position graphic primitives according to static coordinates\nonly, Kara allows for more high-level specifications, supporting graph\nstructures, grids, and relative positioning of graphical elements. Moreover,\ngeneralising the functionality of previous tools, Kara provides modifiable\nvisualisations such that interpretations can be manipulated by graphically\nediting their visualisations. This is realised by resorting to abductive\nreasoning techniques. Kara is part of SeaLion, a forthcoming integrated\ndevelopment environment (IDE) for ASP."
},{
    "category": "cs.CV", 
    "doi": "10.5121/ijcsea.2012.2315", 
    "link": "http://arxiv.org/pdf/1207.2597v1", 
    "title": "Automated Training and Maintenance through Kinect", 
    "arxiv-id": "1207.2597v1", 
    "author": "Sandeep Udayagiri", 
    "publish": "2012-07-11T11:17:28Z", 
    "summary": "In this paper, we have worked on reducing burden on mechanic involving\ncomplex automobile maintenance activities that are performed in centralised\nworkshops. We have presented a system prototype that combines Augmented Reality\nwith Kinect. With the use of Kinect, very high quality sensors are available at\nconsiderably low costs, thus reducing overall expenditure for system design.\nThe system can be operated either in Speech mode or in Gesture mode. The system\ncan be controlled by various audio commands if user opts for Speech mode. The\nsame controlling can also be done by using a set of Gestures in Gesture mode.\n  Gesture recognition is the task performed by Kinect system. This system,\nbundled with RGB and Depth camera, processes the skeletal data by keeping track\nof 20 different body joints. Recognizing Gestures is done by verifying user\nmovements and checking them against predefined condition. Augmented Reality\nmodule captures real-time image data streams from high resolution camera. This\nmodule then generates 3D model that is superimposed on real time data."
},{
    "category": "cs.CG", 
    "doi": "10.5121/ijcsea.2012.2315", 
    "link": "http://arxiv.org/pdf/1301.6336v1", 
    "title": "Approximation of Polyhedral Surface Uniformization", 
    "arxiv-id": "1301.6336v1", 
    "author": "Yaron Lipman", 
    "publish": "2013-01-27T09:35:33Z", 
    "summary": "We present a constructive approach for approximating the conformal map\n(uniformization) of a polyhedral surface to a canonical domain in the plane.\nThe main tool is a characterization of convex spaces of quasiconformal\nsimplicial maps and their approximation properties. As far as we are aware,\nthis is the first algorithm proved to approximate the uniformization of general\npolyhedral surfaces."
},{
    "category": "cs.CG", 
    "doi": "10.1137/130932053", 
    "link": "http://arxiv.org/pdf/1305.2283v1", 
    "title": "Geometric Registration of High-genus Surfaces", 
    "arxiv-id": "1305.2283v1", 
    "author": "Lok Ming Lui", 
    "publish": "2013-05-10T08:48:51Z", 
    "summary": "This paper presents a method to obtain geometric registrations between\nhigh-genus ($g\\geq 1$) surfaces. Surface registration between simple surfaces,\nsuch as simply-connected open surfaces, has been well studied. However, very\nfew works have been carried out for the registration of high-genus surfaces.\nThe high-genus topology of the surface poses great challenge for surface\nregistration. A possible approach is to partition surfaces into\nsimply-connected patches and registration is done patch by patch. Consistent\ncuts are required, which are usually difficult to obtain and prone to error. In\nthis work, we propose an effective way to obtain geometric registration between\nhigh-genus surfaces without introducing consistent cuts. The key idea is to\nconformally parameterize the surface into its universal covering space, which\nis either the Euclidean plane or the hyperbolic disk embedded in\n$\\mathbb{R}^2$. Registration can then be done on the universal covering space\nby minimizing a shape mismatching energy measuring the geometric dissimilarity\nbetween the two surfaces. Our proposed algorithm effectively computes a smooth\nregistration between high-genus surfaces that matches geometric information as\nmuch as possible. The algorithm can also be applied to find a smooth and\nbijective registration minimizing any general energy functionals. Numerical\nexperiments on high-genus surface data show that our proposed method is\neffective for registering high-genus surfaces with geometric matching. We also\napplied the method to register anatomical structures for medical imaging, which\ndemonstrates the usefulness of the proposed algorithm."
},{
    "category": "cs.GR", 
    "doi": "10.1137/130932053", 
    "link": "http://arxiv.org/pdf/1305.3971v1", 
    "title": "Sparse Norm Filtering", 
    "arxiv-id": "1305.3971v1", 
    "author": "Min Wu", 
    "publish": "2013-05-17T03:13:28Z", 
    "summary": "Optimization-based filtering smoothes an image by minimizing a fidelity\nfunction and simultaneously preserves edges by exploiting a sparse norm penalty\nover gradients. It has obtained promising performance in practical problems,\nsuch as detail manipulation, HDR compression and deblurring, and thus has\nreceived increasing attentions in fields of graphics, computer vision and image\nprocessing. This paper derives a new type of image filter called sparse norm\nfilter (SNF) from optimization-based filtering. SNF has a very simple form,\nintroduces a general class of filtering techniques, and explains several\nclassic filters as special implementations of SNF, e.g. the averaging filter\nand the median filter. It has advantages of being halo free, easy to implement,\nand low time and memory costs (comparable to those of the bilateral filter).\nThus, it is more generic than a smoothing operator and can better adapt to\ndifferent tasks. We validate the proposed SNF by a wide variety of applications\nincluding edge-preserving smoothing, outlier tolerant filtering, detail\nmanipulation, HDR compression, non-blind deconvolution, image segmentation, and\ncolorization."
},{
    "category": "cond-mat.mtrl-sci", 
    "doi": "10.1137/130932053", 
    "link": "http://arxiv.org/pdf/1306.1599v1", 
    "title": "New Views of Crystal Symmetry", 
    "arxiv-id": "1306.1599v1", 
    "author": "Eckhard Hitzer", 
    "publish": "2013-06-07T03:25:01Z", 
    "summary": "Already Hermann Grassmann's father Justus (1829, 1830) published two works on\nthe geometrical description of crystals, influenced by the earlier works of\nC.S. Weiss (1780-1856) on three main crystal forces governing crystal\nformation. In his 1840 essay on the derivation of crystal shapes from the\ngeneral law of crystal formation Hermann established the notion of a\nthree-dimensional vectorial system of forces with rational coefficients, that\nrepresent the interior crystal structure, regulate its formation, its shape and\nphysical behavior. In the Ausdehnungslehre 1844 (Paragraph 171) he finally\nwrites: I shall conclude this presentation by one of the most beautiful\napplications which can be made of the science treated, i.e. the application to\ncrystal figures (Scholz, 1996). The geometry of crystals thus certainly\ninfluenced the Ausdehnungslehre. In this paper we see how Grassmann's work\ninfluenced Clifford's creation of geometric algebras, which in turn leads to a\nnew geometric description of crystal symmetry suitable for modern computer\nalgebra graphics."
},{
    "category": "cs.GR", 
    "doi": "10.1137/130932053", 
    "link": "http://arxiv.org/pdf/1306.2081v1", 
    "title": "3D model retrieval using global and local radial distances", 
    "arxiv-id": "1306.2081v1", 
    "author": "Henry Johan", 
    "publish": "2013-06-10T01:38:09Z", 
    "summary": "3D model retrieval techniques can be classified as histogram-based,\nview-based and graph-based approaches. We propose a hybrid shape descriptor\nwhich combines the global and local radial distance features by utilizing the\nhistogram-based and view-based approaches respectively. We define an\narea-weighted global radial distance with respect to the center of the bounding\nsphere of the model and encode its distribution into a 2D histogram as the\nglobal radial distance shape descriptor. We then uniformly divide the bounding\ncube of a 3D model into a set of small cubes and define their centers as local\ncenters. Then, we compute the local radial distance of a point based on the\nnearest local center. By sparsely sampling a set of views and encoding the\nlocal radial distance feature on the rendered views by color coding, we extract\nthe local radial distance shape descriptor. Based on these two shape\ndescriptors, we develop a hybrid radial distance shape descriptor for 3D model\nretrieval. Experiment results show that our hybrid shape descriptor outperforms\nseveral typical histogram-based and view-based approaches."
},{
    "category": "cond-mat.mtrl-sci", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1306.2124v1", 
    "title": "New views of crystal symmetry guided by profound admiration of the   extraordinary works of Grassmann and Clifford", 
    "arxiv-id": "1306.2124v1", 
    "author": "Eckhard Hitzer", 
    "publish": "2013-06-10T07:45:02Z", 
    "summary": "This paper shows how beginning with Justus Grassmann's work, Hermann\nGrassmann was influenced in his mathematical thinking by crystallography. H.\nGrassmann's Ausdehnungslehre in turn had a decisive influence on W.K. Clifford\nin the genesis of geometric algebras. Geometric algebras have been expanded to\nconformal geometric algebras, which provide an ideal framework for modern\ncomputer graphics. Within this framework a new visualization of\nthree-dimensional crystallographic space groups has been created. The complex\nbeauty of this new visualization is shown by a range of images of a diamond\ncell. Mathematical details are given in an appendix."
},{
    "category": "math.RA", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1306.2157v1", 
    "title": "The Orthogonal 2D Planes Split of Quaternions and Steerable Quaternion   Fourier Transformations", 
    "arxiv-id": "1306.2157v1", 
    "author": "Stephen J. Sangwine", 
    "publish": "2013-06-10T10:15:42Z", 
    "summary": "The two-sided quaternionic Fourier transformation (QFT) was introduced in\n\\cite{Ell:1993} for the analysis of 2D linear time-invariant\npartial-differential systems. In further theoretical investigations\n\\cite{10.1007/s00006-007-0037-8, EH:DirUP_QFT} a special split of quaternions\nwas introduced, then called $\\pm$split. In the current \\change{chapter} we\nanalyze this split further, interpret it geometrically as \\change{an}\n\\emph{orthogonal 2D planes split} (OPS), and generalize it to a freely\nsteerable split of $\\H$ into two orthogonal 2D analysis planes. The new general\nform of the OPS split allows us to find new geometric interpretations for the\naction of the QFT on the signal. The second major result of this work is a\nvariety of \\emph{new steerable forms} of the QFT, their geometric\ninterpretation, and for each form\\change{,} OPS split theorems, which allow\nfast and efficient numerical implementation with standard FFT software."
},{
    "category": "physics.ins-det", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1307.0155v1", 
    "title": "Free Instrument for Movement Measure", 
    "arxiv-id": "1307.0155v1", 
    "author": "Jos\u00e9 Garcia Vivas Miranda", 
    "publish": "2013-06-29T22:10:34Z", 
    "summary": "This paper presents the validation of a computational tool that serves to\nobtain continuous measurements of moving objects. The software uses techniques\nof computer vision, pattern recognition and optical flow, to enable tracking of\nobjects in videos, generating data trajectory, velocity, acceleration and\nangular movement. The program was applied to track a ball around a simple\npendulum. The methodology used to validate it, taking as a basis to compare the\nvalues measured by the program, as well as the theoretical values expected\naccording to the model of a simple pendulum. The experiment is appropriate to\nthe method because it was built within the limits of the linear harmonic\noscillator and energy losses due to friction had been minimized, making it the\nmost ideal possible. The results indicate that the tool is sensitive and\naccurate. Deviations of less than a millimeter to the extent of the trajectory,\nensures the applicability of the software on physics, whether in research or in\nteaching topics."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1307.4214v1", 
    "title": "Review of simulating four classes of window materials for daylighting   with non-standard BSDF using the simulation program Radiance", 
    "arxiv-id": "1307.4214v1", 
    "author": "Peter Apian-Bennewitz", 
    "publish": "2013-07-16T09:25:53Z", 
    "summary": "This review describes the currently available simulation models for window\nmaterial to calculate daylighting with the program \"Radiance\". The review is\nbased on four abstract and general classes of window materials, depending on\ntheir scattering and redirecting properties (bidirectional scatter distribution\nfunction, BSDF). It lists potential and limits of the older models and includes\nthe most recent additions to the software. All models are demonstrated using an\nexemplary indoor scene and two typical sky conditions. It is intended as\nclarification for applying window material models in project work or teaching.\nThe underlying algorithmic problems apply to all lighting simulation programs,\nso the scenarios of materials and skies are applicable to other lighting\nprograms."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1307.6549v1", 
    "title": "Making Laplacians commute", 
    "arxiv-id": "1307.6549v1", 
    "author": "Terry A. Loring", 
    "publish": "2013-07-19T10:57:31Z", 
    "summary": "In this paper, we construct multimodal spectral geometry by finding a pair of\nclosest commuting operators (CCO) to a given pair of Laplacians. The CCOs are\njointly diagonalizable and hence have the same eigenbasis. Our construction\nnaturally extends classical data analysis tools based on spectral geometry,\nsuch as diffusion maps and spectral clustering. We provide several synthetic\nand real examples of applications in dimensionality reduction, shape analysis,\nand clustering, demonstrating that our method better captures the inherent\nstructure of multi-modal data."
},{
    "category": "math.DG", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1310.1710v1", 
    "title": "Landmark and Intensity Based Registration with Large Deformations via   Quasi-conformal Maps", 
    "arxiv-id": "1310.1710v1", 
    "author": "Lok Ming Lui", 
    "publish": "2013-10-07T09:17:00Z", 
    "summary": "Registration, which aims to find an optimal one-to-one correspondence between\ndifferent data, is an important problem in various fields. This problem is\nespecially challenging when large deformations occur. In this paper, we present\na novel algorithm to obtain diffeomorphic image or surface registrations with\nlarge deformations via quasi-conformal maps. The basic idea is to minimize an\nenergy functional involving a Beltrami coefficient term, which measures the\ndistortion of the quasi-conformal map. The Beltrami coefficient effectively\ncontrols the bijectivity and smoothness of the registration, even with very\nlarge deformations. Using the proposed algorithm, landmark-based registration\nbetween images or surfaces can be effectively computed. The obtained\nregistration is guaranteed to be diffeomorphic (1-1 and onto), even with a\nlarge deformation or large number of landmark constraints. The proposed\nalgorithm can also be combined with matching intensity (such as image intensity\nor surface curvature) to improve the accuracy of the registration. Experiments\nhave been carried out on both synthetic and real data. Results demonstrate the\nefficacy of the proposed algorithm to obtain diffeomorphic registration between\nimages or surfaces."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-0346-0405-5_36", 
    "link": "http://arxiv.org/pdf/1310.2923v1", 
    "title": "Composing DTI Visualizations with End-user Programming", 
    "arxiv-id": "1310.2923v1", 
    "author": "David H. Laidlaw", 
    "publish": "2013-10-10T19:16:17Z", 
    "summary": "We present the design and prototype implementation of a scientific\nvisualization language called Zifazah for composing 3D visualizations of\ndiffusion tensor magnetic resonance imaging (DT-MRI or DTI) data. Unlike\nexisting tools allowing flexible customization of data visualizations that are\nprogrammer-oriented, we focus on domain scientists as end users in order to\nenable them to freely compose visualizations of their scientific data set. We\nanalyzed end-user descriptions extracted from interviews with neurologists and\nphysicians conducting clinical practices using DTI about how they would build\nand use DTI visualizations to collect syntax and semantics for the language\ndesign, and have discovered the elements and structure of the proposed\nlanguage. Zifazah makes use of the initial set of lexical terms and semantics\nto provide a declarative language in the spirit of intuitive syntax and usage.\nThis work contributes three, among others, main design principles for\nscientific visualization language design as well as a practice of such language\nfor DTI visualization with Zifazah. First, Zifazah incorporated visual symbolic\nmapping based on color, size and shape, which is a sub-set of Bertin's taxonomy\nmigrated to scientific visualizations. Second, Zifazah is defined as a spatial\nlanguage whereby lexical representation of spatial relationship for 3D object\nvisualization and manipulations, which is characteristic of scientific data,\ncan be programmed. Third, built on top of Bertin's semiology, flexible data\nencoding specifically for scientific visualizations is integrated in our\nlanguage in order to allow end users to achieve optimal visual composition at\ntheir best. Along with sample scripts representative of our language design\nfeatures, some new DTI visualizations as the running results created by end\nusers using the novel visualization language have also been presented."
},{
    "category": "cs.GR", 
    "doi": "10.1080/00411450.2014.910231", 
    "link": "http://arxiv.org/pdf/1312.1412v5", 
    "title": "Rigorous asymptotic and moment-preserving diffusion approximations for   generalized linear Boltzmann transport in arbitrary dimension", 
    "arxiv-id": "1312.1412v5", 
    "author": "Eugene d'Eon", 
    "publish": "2013-12-05T02:29:46Z", 
    "summary": "We derive new diffusion solutions to the monoenergetic generalized linear\nBoltzmann transport equation (GLBE) for the stationary collision density and\nscalar flux about an isotropic point source in an infinite $d$-dimensional\nabsorbing medium with isotropic scattering. We consider both classical\ntransport theory with exponentially-distributed free paths in arbitrary\ndimensions as well as a number of non-classical transport theories\n(non-exponential random flights) that describe a broader class of transport\nprocesses within partially-correlated random media. New rigorous asymptotic\ndiffusion approximations are derived where possible. We also generalize\nGrosjean's moment-preserving approach of separating the first (or uncollided)\ndistribution from the collided portion and approximating only the latter using\ndiffusion. We find that for any spatial dimension and for many free-path\ndistributions Grosjean's approach produces compact, analytic approximations\nthat are, overall, more accurate for high absorption and for small\nsource-detector separations than either $P_1$ diffusion or rigorous asymptotic\ndiffusion. These diffusion-based approximations are exact in the first two even\nspatial moments, which we derive explicitly for various non-classical transport\ntypes. We also discuss connections between the random-flight-theory derivation\nof the Green's function and the discrete spectrum of the transport operator and\nreport some new observations regarding the discrete eigenvalues of the\ntransport operator for general dimensions and free-path distributions."
},{
    "category": "cs.CV", 
    "doi": "10.1080/00411450.2014.910231", 
    "link": "http://arxiv.org/pdf/1401.1742v1", 
    "title": "Content Based Image Indexing and Retrieval", 
    "arxiv-id": "1401.1742v1", 
    "author": "B. B. Meshram", 
    "publish": "2014-01-08T16:22:09Z", 
    "summary": "In this paper, we present the efficient content based image retrieval systems\nwhich employ the color, texture and shape information of images to facilitate\nthe retrieval process. For efficient feature extraction, we extract the color,\ntexture and shape feature of images automatically using edge detection which is\nwidely used in signal processing and image compression. For facilitated the\nspeedy retrieval we are implements the antipole-tree algorithm for indexing the\nimages."
},{
    "category": "cs.CG", 
    "doi": "10.1080/00411450.2014.910231", 
    "link": "http://arxiv.org/pdf/1401.3385v1", 
    "title": "A programme to determine the exact interior of any connected digital   picture", 
    "arxiv-id": "1401.3385v1", 
    "author": "Val\u00e9rio Ramos Batista", 
    "publish": "2014-01-14T23:21:58Z", 
    "summary": "Region filling is one of the most important and fundamental operations in\ncomputer graphics and image processing. Many filling algorithms and their\nimplementations are based on the Euclidean geometry, which are then translated\ninto computational models moving carelessly from the continuous to the finite\ndiscrete space of the computer. The consequences of this approach is that most\nimplementations fail when tested for challenging degenerate and nearly\ndegenerate regions. We present a correct integer-only procedure that works for\nall connected digital pictures. It finds all possible interior points, which\nare then displayed and stored in a locating matrix. Namely, we present a\nfilling and locating procedure that can be used in computer graphics and image\nprocessing applications."
},{
    "category": "cs.CV", 
    "doi": "10.1080/00411450.2014.910231", 
    "link": "http://arxiv.org/pdf/1403.0728v1", 
    "title": "A Novel Method for Vectorization", 
    "arxiv-id": "1403.0728v1", 
    "author": "Emrah Bala", 
    "publish": "2014-03-04T09:52:13Z", 
    "summary": "Vectorization of images is a key concern uniting computer graphics and\ncomputer vision communities. In this paper we are presenting a novel idea for\nefficient, customizable vectorization of raster images, based on Catmull Rom\nspline fitting. The algorithm maintains a good balance between photo-realism\nand photo abstraction, and hence is applicable to applications with artistic\nconcerns or applications where less information loss is crucial. The resulting\nalgorithm is fast, parallelizable and can satisfy general soft realtime\nrequirements. Moreover, the smoothness of the vectorized images aesthetically\noutperforms outputs of many polygon-based methods"
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-014-0754-0", 
    "link": "http://arxiv.org/pdf/1405.6563v1", 
    "title": "Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D   Articulated Bodies", 
    "arxiv-id": "1405.6563v1", 
    "author": "Radu Horaud", 
    "publish": "2014-05-26T13:12:05Z", 
    "summary": "In motion analysis and understanding it is important to be able to fit a\nsuitable model or structure to the temporal series of observed data, in order\nto describe motion patterns in a compact way, and to discriminate between them.\nIn an unsupervised context, i.e., no prior model of the moving object(s) is\navailable, such a structure has to be learned from the data in a bottom-up\nfashion. In recent times, volumetric approaches in which the motion is captured\nfrom a number of cameras and a voxel-set representation of the body is built\nfrom the camera views, have gained ground due to attractive features such as\ninherent view-invariance and robustness to occlusions. Automatic, unsupervised\nsegmentation of moving bodies along entire sequences, in a temporally-coherent\nand robust way, has the potential to provide a means of constructing a\nbottom-up model of the moving body, and track motion cues that may be later\nexploited for motion classification. Spectral methods such as locally linear\nembedding (LLE) can be useful in this context, as they preserve \"protrusions\",\ni.e., high-curvature regions of the 3D volume, of articulated shapes, while\nimproving their separation in a lower dimensional space, making them in this\nway easier to cluster. In this paper we therefore propose a spectral approach\nto unsupervised and temporally-coherent body-protrusion segmentation along time\nsequences. Volumetric shapes are clustered in an embedding space, clusters are\npropagated in time to ensure coherence, and merged or split to accommodate\nchanges in the body's topology. Experiments on both synthetic and real\nsequences of dense voxel-set data are shown. This supports the ability of the\nproposed method to cluster body-parts consistently over time in a totally\nunsupervised fashion, its robustness to sampling density and shape quality, and\nits potential for bottom-up model construction"
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11263-014-0754-0", 
    "link": "http://arxiv.org/pdf/1407.2107v1", 
    "title": "iGPSe: A Visual Analytic System for Integrative Genomic Based Cancer   Patient Stratification", 
    "arxiv-id": "1407.2107v1", 
    "author": "Raghu Machiraju", 
    "publish": "2014-07-08T14:30:15Z", 
    "summary": "Background: Cancers are highly heterogeneous with different subtypes. These\nsubtypes often possess different genetic variants, present different\npathological phenotypes, and most importantly, show various clinical outcomes\nsuch as varied prognosis and response to treatment and likelihood for\nrecurrence and metastasis. Recently, integrative genomics (or panomics)\napproaches are often adopted with the goal of combining multiple types of omics\ndata to identify integrative biomarkers for stratification of patients into\ngroups with different clinical outcomes. Results: In this paper we present a\nvisual analytic system called Interactive Genomics Patient Stratification\nexplorer (iGPSe) which significantly reduces the computing burden for\nbiomedical researchers in the process of exploring complicated integrative\ngenomics data. Our system integrates unsupervised clustering with graph and\nparallel sets visualization and allows direct comparison of clinical outcomes\nvia survival analysis. Using a breast cancer dataset obtained from the The\nCancer Genome Atlas (TCGA) project, we are able to quickly explore different\ncombinations of gene expression (mRNA) and microRNA features and identify\npotential combined markers for survival prediction. Conclusions: Visualization\nplays an important role in the process of stratifying given population\npatients. Visual tools allowed for the selection of possibly features across\nvarious datasets for the given patient population. We essentially made a case\nfor visualization for a very important problem in translational informatics."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11263-014-0754-0", 
    "link": "http://arxiv.org/pdf/1407.2112v1", 
    "title": "MCA: Multiresolution Correlation Analysis, a graphical tool for   subpopulation identification in single-cell gene expression data", 
    "arxiv-id": "1407.2112v1", 
    "author": "Carsten Marr", 
    "publish": "2014-07-08T14:42:30Z", 
    "summary": "Background: Biological data often originate from samples containing mixtures\nof subpopulations, corresponding e.g. to distinct cellular phenotypes. However,\nidentification of distinct subpopulations may be difficult if biological\nmeasurements yield distributions that are not easily separable. Results: We\npresent Multiresolution Correlation Analysis (MCA), a method for visually\nidentifying subpopulations based on the local pairwise correlation between\ncovariates, without needing to define an a priori interaction scale. We\ndemonstrate that MCA facilitates the identification of differentially regulated\nsubpopulations in simulated data from a small gene regulatory network, followed\nby application to previously published single-cell qPCR data from mouse\nembryonic stem cells. We show that MCA recovers previously identified\nsubpopulations, provides additional insight into the underlying correlation\nstructure, reveals potentially spurious compartmentalizations, and provides\ninsight into novel subpopulations. Conclusions: MCA is a useful method for the\nidentification of subpopulations in low-dimensional expression data, as\nemerging from qPCR or FACS measurements. With MCA it is possible to investigate\nthe robustness of covariate correlations with respect subpopulations,\ngraphically identify outliers, and identify factors contributing to\ndifferential regulation between pairs of covariates. MCA thus provides a\nframework for investigation of expression correlations for genes of interests\nand biological hypothesis generation."
},{
    "category": "cs.HC", 
    "doi": "10.1007/s11263-014-0754-0", 
    "link": "http://arxiv.org/pdf/1407.2117v1", 
    "title": "Visualization of gene expression information within the context of the   mouse anatomy", 
    "arxiv-id": "1407.2117v1", 
    "author": "Albert Burger", 
    "publish": "2014-07-08T14:50:04Z", 
    "summary": "Background: The eMouse Atlas of Gene Expression (EMAGE) is an online resource\nthat publishes the results of in situ gene expression experiments on the\ndevelopmental mouse. The resource provides comprehensive search facilities, but\nfew analytical tools or visual mechanisms for navigating the data set. To deal\nwith the missing visual navigation, this paper explores the application of\nsunburst and icicle visualizations within EMAGE. Results: A prototype solution\ndelivered a simple user interface that helps the user query EMAGE and generate\na sunburst/icicle diagram. An evaluation featuring test subjects from the EMAGE\nstaff studied the visualizations and provided a range of suggested\nimprovements. Moreover the evaluation discovered that in addition to providing\na visual means of walking through the data, when grouped, the sunburst delivers\nan interactive overview that assists with analysing sets of related genes.\nConclusions: The sunburst and icicle visualizations have been shown to be\neffective tools for summarising gene expression data. The sunburst with its\nspace saving radial layout was found especially useful for providing an\noverview of gene families or pathways. Work is ongoing to integrate these\nvisualizations into EMAGE."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s11263-014-0754-0", 
    "link": "http://arxiv.org/pdf/1407.2723v1", 
    "title": "Determining surfaces of revolution from their implicit equations", 
    "arxiv-id": "1407.2723v1", 
    "author": "Miroslav L\u00e1vi\u010dka", 
    "publish": "2014-07-10T08:24:59Z", 
    "summary": "Results of number of geometric operations (often used in technical practise,\nas e.g. the operation of blending) are in many cases surfaces described\nimplicitly. Then it is a challenging task to recognize the type of the obtained\nsurface, find its characteristics and for the rational surfaces compute also\ntheir parameterizations. In this contribution we will focus on surfaces of\nrevolution. These objects, widely used in geometric modelling, are generated by\nrotating a generatrix around a given axis. If the generatrix is an algebraic\ncurve then so is also the resulting surface, described uniquely by a polynomial\nwhich can be found by some well-established implicitation technique. However,\nstarting from a polynomial it is not known how to decide if the corresponding\nalgebraic surface is rotational or not. Motivated by this, our goal is to\nformulate a simple and efficient algorithm whose input is a polynomial with the\ncoefficients from some subfield of $\\mathbb{R}$ and the output is the answer\nwhether the shape is a surface of revolution. In the affirmative case we also\nfind the equations of its axis and generatrix. Furthermore, we investigate the\nproblem of rationality and unirationality of surfaces of revolution and show\nthat this question can be efficiently answered discussing the rationality of a\ncertain associated planar curve."
},{
    "category": "physics.optics", 
    "doi": "10.1016/j.optcom.2014.07.081", 
    "link": "http://arxiv.org/pdf/1407.2971v1", 
    "title": "Numerical investigation of lensless zoomable holographic multiple   projections to tilted planes", 
    "arxiv-id": "1407.2971v1", 
    "author": "Tomoyoshi Ito", 
    "publish": "2014-07-10T21:27:03Z", 
    "summary": "This paper numerically investigates the feasibility of lensless zoomable\nholographic multiple projections to tilted planes. We have already developed\nlensless zoomable holographic single projection using scaled diffraction, which\ncalculates diffraction between parallel planes with different sampling pitches.\nThe structure of this zoomable holographic projection is very simple because it\ndoes not need a lens; however, it only projects a single image to a plane\nparallel to the hologram. The lensless zoomable holographic projection in this\npaper is capable of projecting multiple images onto tilted planes\nsimultaneously."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.optcom.2014.07.081", 
    "link": "http://arxiv.org/pdf/1410.2320v1", 
    "title": "Computing minimum area homologies", 
    "arxiv-id": "1410.2320v1", 
    "author": "Mikael Vejdemo-Johansson", 
    "publish": "2014-10-09T00:40:45Z", 
    "summary": "Calculating and categorizing the similarity of curves is a fundamental\nproblem which has generated much recent interest. However, to date there are no\nimplementations of these algorithms for curves on surfaces with provable\nguarantees on the quality of the measure. In this paper, we present a\nsimilarity measure for any two cycles that are homologous, where we calculate\nthe minimum area of any homology (or connected bounding chain) between the two\ncycles. The minimum area homology exists for broader classes of cycles than\nprevious measures which are based on homotopy. It is also much easier to\ncompute than previously defined measures, yielding an efficient implementation\nthat is based on linear algebra tools. We demonstrate our algorithm on a range\nof inputs, showing examples which highlight the feasibility of this similarity\nmeasure."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.optcom.2014.07.081", 
    "link": "http://arxiv.org/pdf/1410.4598v1", 
    "title": "TiQuant: Software for tissue analysis, quantification and surface   reconstruction", 
    "arxiv-id": "1410.4598v1", 
    "author": "Stefan Hoehme", 
    "publish": "2014-10-16T22:29:10Z", 
    "summary": "Motivation: TiQuant is a modular software tool for efficient quantification\nof biological tissues based on volume data obtained by biomedical image\nmodalities. It includes a number of versatile image and volume processing\nchains tailored to the analysis of different tissue types which have been\nexperimentally verified. TiQuant implements a novel method for the\nreconstruction of three-dimensional surfaces of biological systems, data that\noften cannot be obtained experimentally but which is of utmost importance for\ntissue modelling in systems biology. Availability: TiQuant is freely available\nfor non-commercial use at msysbio.com/tiquant. Windows, OSX and Linux are\nsupported."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1088/0143-0807/35/6/065028", 
    "link": "http://arxiv.org/pdf/1410.6022v1", 
    "title": "The physics of volume rendering", 
    "arxiv-id": "1410.6022v1", 
    "author": "Thomas Peters", 
    "publish": "2014-10-22T12:36:34Z", 
    "summary": "Radiation transfer is an important topic in several physical disciplines,\nprobably most prominently in astrophysics. Computer scientists use radiation\ntransfer, among other things, for the visualisation of complex data sets with\ndirect volume rendering. In this note, I point out the connection between\nphysical radiation transfer and volume rendering, and I describe an\nimplementation of direct volume rendering in the astrophysical radiation\ntransfer code RADMC-3D. I show examples for the use of this module on\nanalytical models and simulation data."
},{
    "category": "cs.CV", 
    "doi": "10.1088/0143-0807/35/6/065028", 
    "link": "http://arxiv.org/pdf/1411.4098v1", 
    "title": "GASP : Geometric Association with Surface Patches", 
    "arxiv-id": "1411.4098v1", 
    "author": "Henrik I. Christensen", 
    "publish": "2014-11-15T01:31:55Z", 
    "summary": "A fundamental challenge to sensory processing tasks in perception and\nrobotics is the problem of obtaining data associations across views. We present\na robust solution for ascertaining potentially dense surface patch (superpixel)\nassociations, requiring just range information. Our approach involves\ndecomposition of a view into regularized surface patches. We represent them as\nsequences expressing geometry invariantly over their superpixel neighborhoods,\nas uniquely consistent partial orderings. We match these representations\nthrough an optimal sequence comparison metric based on the Damerau-Levenshtein\ndistance - enabling robust association with quadratic complexity (in contrast\nto hitherto employed joint matching formulations which are NP-complete). The\napproach is able to perform under wide baselines, heavy rotations, partial\noverlaps, significant occlusions and sensor noise.\n  The technique does not require any priors -- motion or otherwise, and does\nnot make restrictive assumptions on scene structure and sensor movement. It\ndoes not require appearance -- is hence more widely applicable than appearance\nreliant methods, and invulnerable to related ambiguities such as textureless or\naliased content. We present promising qualitative and quantitative results\nunder diverse settings, along with comparatives with popular approaches based\non range as well as RGB-D data."
},{
    "category": "cs.SI", 
    "doi": "10.1088/0143-0807/35/6/065028", 
    "link": "http://arxiv.org/pdf/1412.6706v1", 
    "title": "SVEN: Informative Visual Representation of Complex Dynamic Structure", 
    "arxiv-id": "1412.6706v1", 
    "author": "Leslie M. Blaha", 
    "publish": "2014-12-20T23:21:46Z", 
    "summary": "Graphs change over time, and typically variations on the small multiples or\nanimation pattern is used to convey this dynamism visually. However, both of\nthese classical techniques have significant drawbacks, so a new approach,\nStoryline Visualization of Events on a Network (SVEN) is proposed. SVEN builds\non storyline techniques, conveying nodes as contiguous lines over time. SVEN\nencodes time in a natural manner, along the horizontal axis, and optimizes the\nvertical placement of storylines to decrease clutter (line crossings,\nstraightness, and bends) in the drawing. This paper demonstrates SVEN on\nseveral different flavors of real-world dynamic data, and outlines the\nremaining near-term future work."
},{
    "category": "cs.CV", 
    "doi": "10.1088/0143-0807/35/6/065028", 
    "link": "http://arxiv.org/pdf/1412.7725v2", 
    "title": "Automatic Photo Adjustment Using Deep Neural Networks", 
    "arxiv-id": "1412.7725v2", 
    "author": "Yizhou Yu", 
    "publish": "2014-12-24T17:51:17Z", 
    "summary": "Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work."
},{
    "category": "cs.CV", 
    "doi": "10.1088/0143-0807/35/6/065028", 
    "link": "http://arxiv.org/pdf/1503.03167v4", 
    "title": "Deep Convolutional Inverse Graphics Network", 
    "arxiv-id": "1503.03167v4", 
    "author": "Joshua B. Tenenbaum", 
    "publish": "2015-03-11T04:08:42Z", 
    "summary": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a\nmodel that learns an interpretable representation of images. This\nrepresentation is disentangled with respect to transformations such as\nout-of-plane rotations and lighting variations. The DC-IGN model is composed of\nmultiple layers of convolution and de-convolution operators and is trained\nusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a\ntraining procedure to encourage neurons in the graphics code layer to represent\na specific transformation (e.g. pose or light). Given a single input image, our\nmodel can generate new images of the same object with variations in pose and\nlighting. We present qualitative and quantitative results of the model's\nefficacy at learning a 3D rendering engine."
},{
    "category": "cs.HC", 
    "doi": "10.13187/ejced.2015.11.52", 
    "link": "http://arxiv.org/pdf/1503.06958v1", 
    "title": "Developing Educational Computer Animation Based on Human Personality   Types", 
    "arxiv-id": "1503.06958v1", 
    "author": "Carol Griffiths", 
    "publish": "2015-03-24T09:27:14Z", 
    "summary": "Computer animation in the past decade has become one of the most noticeable\nfeatures of technology-based learning environments. With today's high\neducational demands as well as the lack of time provided for certain courses,\nclassical educational methods have shown deficiencies in keeping up with the\ndrastic changes observed in the digital era. Without taking into account\nvarious significant factors such as gender, age, level of interest and memory\nlevel, educational animation may turn out to be insufficient for learners or\nfail to meet their needs. However, we have noticed that the applications of\nanimation for education have been given only inadequate attention, and\nstudents' personality types have never been taken into account. We suggest\nthere is an interesting relationship here, and propose essential factors in\ncreating educational animations based on students' personality types.\nParticularly, we investigate how information in computer animation may be\npresented in a preferable way based on the fundamental elements of computer\nanimation. The present study believes that it is likely to have wide benefits\nin the field of education. Considering the personality types in designing\neducational computer animations with the aid of gathered empirical results\nmight be a promising avenue to enhance the learning process."
},{
    "category": "cs.HC", 
    "doi": "10.13187/ejced.2015.11.52", 
    "link": "http://arxiv.org/pdf/1504.01025v2", 
    "title": "Preprint Extending Touch-less Interaction on Vision Based Wearable   Device", 
    "arxiv-id": "1504.01025v2", 
    "author": "Haibo Li", 
    "publish": "2015-04-04T17:12:19Z", 
    "summary": "This is the preprint version of our paper on IEEE Virtual Reality Conference\n2015. A touch-less interaction technology on vision based wearable device is\ndesigned and evaluated. Users interact with the application with dynamic\nhands/feet gestures in front of the camera. Several proof-of-concept prototypes\nwith eleven dynamic gestures are developed based on the touch-less interaction.\nAt last, a comparing user study evaluation is proposed to demonstrate the\nusability of the touch-less approach, as well as the impact on user's emotion,\nrunning on a wearable framework or Google Glass."
},{
    "category": "cs.CV", 
    "doi": "10.13187/ejced.2015.11.52", 
    "link": "http://arxiv.org/pdf/1504.01052v1", 
    "title": "Fast algorithms for morphological operations using run-length encoded   binary images", 
    "arxiv-id": "1504.01052v1", 
    "author": "Felix Schwitzer", 
    "publish": "2015-04-04T20:51:43Z", 
    "summary": "This paper presents innovative algorithms to efficiently compute erosions and\ndilations of run-length encoded (RLE) binary images with arbitrary shaped\nstructuring elements. An RLE image is given by a set of runs, where a run is a\nhorizontal concatenation of foreground pixels. The proposed algorithms extract\nthe skeleton of the structuring element and build distance tables of the input\nimage, which are storing the distance to the next background pixel on the left\nand right hand sides. This information is then used to speed up the\ncalculations of the erosion and dilation operator by enabling the use of\ntechniques which allow to skip the analysis of certain pixels whenever a hit or\nmiss occurs. Additionally the input image gets trimmed during the preprocessing\nsteps on the base of two primitive criteria. Experimental results show the\nadvantages over other algorithms. The source code of our algorithms is\navailable in C++."
},{
    "category": "physics.soc-ph", 
    "doi": "10.1587/transfun.E98.A.1841", 
    "link": "http://arxiv.org/pdf/1504.06922v1", 
    "title": "Simple Derivation of the Lifetime and the Distribution of Faces for a   Binary Subdivision Model", 
    "arxiv-id": "1504.06922v1", 
    "author": "Yukio Hayashi", 
    "publish": "2015-04-27T03:58:12Z", 
    "summary": "The iterative random subdivision of rectangles is used as a generation model\nof networks in physics, computer science, and urban planning. However, these\nresearches were independent. We consider some relations in them, and derive\nfundamental properties for the average lifetime depending on birth-time and the\nbalanced distribution of rectangle faces."
},{
    "category": "math.DG", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1506.00783v4", 
    "title": "Shape Analysis on Lie Groups with Applications in Computer Animation", 
    "arxiv-id": "1506.00783v4", 
    "author": "Alexander Schmeding", 
    "publish": "2015-06-02T07:55:22Z", 
    "summary": "Shape analysis methods have in the past few years become very popular, both\nfor theoretical exploration as well as from an application point of view.\nOriginally developed for planar curves, these methods have been expanded to\nhigher dimensional curves, surfaces, activities, character motions and many\nother objects. In this paper, we develop a framework for shape analysis of\ncurves in Lie groups for problems of computer animations. In particular, we\nwill use these methods to find cyclic approximations of non-cyclic character\nanimations and interpolate between existing animations to generate new ones."
},{
    "category": "cs.GR", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1506.06668v1", 
    "title": "Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by   Focused Femtosecond Laser Combined with Computational Holographic Fields", 
    "arxiv-id": "1506.06668v1", 
    "author": "Yoshio Hayasaki", 
    "publish": "2015-06-22T16:20:34Z", 
    "summary": "We present a method of rendering aerial and volumetric graphics using\nfemtosecond lasers. A high-intensity laser excites a physical matter to emit\nlight at an arbitrary 3D position. Popular applications can then be explored\nespecially since plasma induced by a femtosecond laser is safer than that\ngenerated by a nanosecond laser. There are two methods of rendering graphics\nwith a femtosecond laser in air: Producing holograms using spatial light\nmodulation technology, and scanning of a laser beam by a galvano mirror. The\nholograms and workspace of the system proposed here occupy a volume of up to 1\ncm^3; however, this size is scalable depending on the optical devices and their\nsetup. This paper provides details of the principles, system setup, and\nexperimental evaluation, and discussions on scalability, design space, and\napplications of this system. We tested two laser sources: an adjustable (30-100\nfs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per\npulse, and a 269-fs laser which projects up to 200,000 pulses per second at an\nenergy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution\nof volumetric displays, implemented with these laser sources, is 4,000 and\n200,000 dots per second. Although we focus on laser-induced plasma in air, the\ndiscussion presented here is also applicable to other rendering principles such\nas fluorescence and microbubble in solid/liquid materials."
},{
    "category": "cs.GR", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1506.06745v1", 
    "title": "GraphMaps: Browsing Large Graphs as Interactive Maps", 
    "arxiv-id": "1506.06745v1", 
    "author": "Xiaoji Chen", 
    "publish": "2015-06-22T17:17:34Z", 
    "summary": "Algorithms for laying out large graphs have seen significant progress in the\npast decade. However, browsing large graphs remains a challenge. Rendering\nthousands of graphical elements at once often results in a cluttered image, and\nnavigating these elements naively can cause disorientation. To address this\nchallenge we propose a method called GraphMaps, mimicking the browsing\nexperience of online geographic maps.\n  GraphMaps creates a sequence of layers, where each layer refines the previous\none. During graph browsing, GraphMaps chooses the layer corresponding to the\nzoom level, and renders only those entities of the layer that intersect the\ncurrent viewport. The result is that, regardless of the graph size, the number\nof entities rendered at each view does not exceed a predefined threshold, yet\nall graph elements can be explored by the standard zoom and pan operations.\n  GraphMaps preprocesses a graph in such a way that during browsing, the\ngeometry of the entities is stable, and the viewer is responsive. Our case\nstudies indicate that GraphMaps is useful in gaining an overview of a large\ngraph, and also in exploring a graph on a finer level of detail."
},{
    "category": "cs.GR", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1506.07515v1", 
    "title": "The Vector Space of Convex Curves: How to Mix Shapes", 
    "arxiv-id": "1506.07515v1", 
    "author": "Dongsung Huh", 
    "publish": "2015-06-24T19:56:56Z", 
    "summary": "We present a novel, log-radius profile representation for convex curves and\ndefine a new operation for combining the shape features of curves. Unlike the\nstandard, angle profile-based methods, this operation accurately combines the\nshape features in a visually intuitive manner. This method have implications in\nshape analysis as well as in investigating how the brain perceives and\ngenerates curved shapes and motions."
},{
    "category": "cs.GR", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1507.04571v3", 
    "title": "GPU-based visualization of domain-coloured algebraic Riemann surfaces", 
    "arxiv-id": "1507.04571v3", 
    "author": "Stefan Kranich", 
    "publish": "2015-07-16T13:45:36Z", 
    "summary": "We examine an algorithm for the visualization of domain-coloured Riemann\nsurfaces of plane algebraic curves. The approach faithfully reproduces the\ntopology and the holomorphic structure of the Riemann surface. We discuss how\nthe algorithm can be implemented efficiently in OpenGL with geometry shaders,\nand (less efficiently) even in WebGL with multiple render targets and floating\npoint textures. While the generation of the surface takes noticeable time in\nboth implementations, the visualization of a cached Riemann surface mesh is\npossible with interactive performance. This allows us to visually explore\notherwise almost unimaginable mathematical objects. As examples, we look at the\ncomplex square root and the folium of Descartes. For the folium of Descartes,\nthe visualization reveals features of the algebraic curve which are not obvious\nfrom its equation."
},{
    "category": "cs.CG", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1507.07760v1", 
    "title": "A Hyperelastic Two-Scale Optimization Model for Shape Matching", 
    "arxiv-id": "1507.07760v1", 
    "author": "Ronen Basri", 
    "publish": "2015-07-28T13:27:51Z", 
    "summary": "We suggest a novel shape matching algorithm for three-dimensional surface\nmeshes of disk or sphere topology. The method is based on the physical theory\nof nonlinear elasticity and can hence handle large rotations and deformations.\nDeformation boundary conditions that supplement the underlying equations are\nusually unknown. Given an initial guess, these are optimized such that the\nmechanical boundary forces that are responsible for the deformation are of a\nsimple nature. We show a heuristic way to approximate the nonlinear\noptimization problem by a sequence of convex problems using finite elements.\nThe deformation cost, i.e, the forces, is measured on a coarse scale while\nICP-like matching is done on the fine scale. We demonstrate the plausibility of\nour algorithm on examples taken from different datasets."
},{
    "category": "cs.CG", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1508.00396v2", 
    "title": "A Linear Formulation for Disk Conformal Parameterization of   Simply-Connected Open Surfaces", 
    "arxiv-id": "1508.00396v2", 
    "author": "Lok Ming Lui", 
    "publish": "2015-08-03T12:25:25Z", 
    "summary": "Surface parameterization is widely used in computer graphics and geometry\nprocessing. It simplifies challenging tasks such as surface registrations,\nmorphing, remeshing and texture mapping. In this paper, we present an efficient\nalgorithm for computing the disk conformal parameterization of simply-connected\nopen surfaces. A double covering technique is used to turn a simply-connected\nopen surface into a genus-0 closed surface, and then a fast algorithm for\nparameterization of genus-0 closed surfaces can be applied. The symmetry of the\ndouble covered surface preserves the efficiency of the computation. A planar\nparameterization can then be obtained with the aid of a M\\\"obius transformation\nand the stereographic projection. After that, a normalization step is applied\nto guarantee the circular boundary. Finally, we achieve a bijective disk\nconformal parameterization by a composition of quasi-conformal mappings.\nExperimental results demonstrate a significant improvement in the computational\ntime by over 60%. At the same time, our proposed method retains comparable\naccuracy, bijectivity and robustness when compared with the state-of-the-art\napproaches. Applications to texture mapping are presented for illustrating the\neffectiveness of our proposed algorithm."
},{
    "category": "cs.CY", 
    "doi": "10.3934/jgm.2016008", 
    "link": "http://arxiv.org/pdf/1508.03649v1", 
    "title": "Borobudur was Built Algorithmically", 
    "arxiv-id": "1508.03649v1", 
    "author": "Hokky Situngkir", 
    "publish": "2015-08-13T04:08:27Z", 
    "summary": "The self-similarity of Indonesian Borobudur Temple is observed through the\ndimensionality of stupa that is hypothetically closely related to whole\narchitectural body. Fractal dimension is calculated by using the cube counting\nmethod and found that the dimension is 2.325, which is laid between the\ntwo-dimensional plane and three dimensional space. The applied fractal geometry\nand self-similarity of the building is emerged as the building process\nimplement the metric rules, since there is no universal metric standard known\nin ancient traditional Javanese culture thus the architecture is not based on\nfinal master plan. The paper also proposes how the hypothetical algorithmic\narchitecture might be applied computationally in order to see some experimental\ngenerations of similar building. The paper ends with some conjectures for\nfurther challenge and insights related to fractal geometry in Javanese\ntraditional cultural heritages."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-540-24670-1_8", 
    "link": "http://arxiv.org/pdf/1508.04981v1", 
    "title": "High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range   Imaging", 
    "arxiv-id": "1508.04981v1", 
    "author": "Rae-Hong Park", 
    "publish": "2015-08-20T13:43:46Z", 
    "summary": "For structured-light range imaging, color stripes can be used for increasing\nthe number of distinguishable light patterns compared to binary BW stripes.\nTherefore, an appropriate use of color patterns can reduce the number of light\nprojections and range imaging is achievable in single video frame or in \"one\nshot\". On the other hand, the reliability and range resolution attainable from\ncolor stripes is generally lower than those from multiply projected binary BW\npatterns since color contrast is affected by object color reflectance and\nambient light. This paper presents new methods for selecting stripe colors and\ndesigning multiple-stripe patterns for \"one-shot\" and \"two-shot\" imaging. We\nshow that maximizing color contrast between the stripes in one-shot imaging\nreduces the ambiguities resulting from colored object surfaces and limitations\nin sensor/projector resolution. Two-shot imaging adds an extra video frame and\nmaximizes the color contrast between the first and second video frames to\ndiminish the ambiguities even further. Experimental results demonstrate the\neffectiveness of the presented one-shot and two-shot color-stripe imaging\nschemes."
},{
    "category": "cs.CV", 
    "doi": "10.1364/OL.40.001940", 
    "link": "http://arxiv.org/pdf/1508.06171v1", 
    "title": "BREN: Body Reflection Essence-Neuter Model for Separation of Reflection   Components", 
    "arxiv-id": "1508.06171v1", 
    "author": "Hyung-Min Park", 
    "publish": "2015-08-25T14:47:18Z", 
    "summary": "We propose a novel reflection color model consisting of body essence and\n(mixed) neuter, and present an effective method for separating dichromatic\nreflection components using a single image. Body essence is an entity invariant\nto interface reflection, and has two degrees of freedom unlike hue and maximum\nchromaticity. As a result, the proposed method is insensitive to noise and\nproper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,\ngreen, and blue), contrary to the maximum chromaticity-based methods. Interface\nreflection is separated by using a Gaussian function, which removes a critical\nthresholding problem. Furthermore, the method does not require any region\nsegmentation. Experimental results show the efficacy of the proposed model and\nmethod."
},{
    "category": "cs.GR", 
    "doi": "10.1145/2077341.2077346", 
    "link": "http://arxiv.org/pdf/1508.06181v1", 
    "title": "PolyDepth: Real-time Penetration Depth Computation using Iterative   Contact-Space Projection", 
    "arxiv-id": "1508.06181v1", 
    "author": "Young J. Kim", 
    "publish": "2015-08-25T15:01:47Z", 
    "summary": "We present a real-time algorithm that finds the Penetration Depth (PD)\nbetween general polygonal models based on iterative and local optimization\ntechniques. Given an in-collision configuration of an object in configuration\nspace, we find an initial collision-free configuration using several methods\nsuch as centroid difference, maximally clear configuration, motion coherence,\nrandom configuration, and sampling-based search. We project this configuration\non to a local contact space using a variant of continuous collision detection\nalgorithm and construct a linear convex cone around the projected\nconfiguration. We then formulate a new projection of the in-collision\nconfiguration onto the convex cone as a Linear Complementarity Problem (LCP),\nwhich we solve using a type of Gauss-Seidel iterative algorithm. We repeat this\nprocedure until a locally optimal PD is obtained. Our algorithm can process\ncomplicated models consisting of tens of thousands triangles at interactive\nrates."
},{
    "category": "cs.CG", 
    "doi": "10.1145/2077341.2077346", 
    "link": "http://arxiv.org/pdf/1508.07569v3", 
    "title": "Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing", 
    "arxiv-id": "1508.07569v3", 
    "author": "Lok Ming Lui", 
    "publish": "2015-08-30T13:42:43Z", 
    "summary": "Point cloud is the most fundamental representation of 3D geometric objects.\nAnalyzing and processing point cloud surfaces is important in computer graphics\nand computer vision. However, most of the existing algorithms for surface\nanalysis require connectivity information. Therefore, it is desirable to\ndevelop a mesh structure on point clouds. This task can be simplified with the\naid of a parameterization. In particular, conformal parameterizations are\nadvantageous in preserving the geometric information of the point cloud data.\nIn this paper, we extend a state-of-the-art spherical conformal\nparameterization algorithm for genus-0 closed meshes to the case of point\nclouds, using an improved approximation of the Laplace-Beltrami operator on\ndata points. Then, we propose an iterative scheme called the North-South\nreiteration for achieving a spherical conformal parameterization. A balancing\nscheme is introduced to enhance the distribution of the spherical\nparameterization. High quality triangulations and quadrangulations can then be\nbuilt on the point clouds with the aid of the parameterizations. Also, the\nmeshes generated are guaranteed to be genus-0 closed meshes. Moreover, using\nour proposed spherical conformal parameterization, multilevel representations\nof point clouds can be easily constructed. Experimental results demonstrate the\neffectiveness of our proposed framework."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.image.2013.05.005", 
    "link": "http://arxiv.org/pdf/1508.07859v1", 
    "title": "Multi-Projector Color Structured-Light Vision", 
    "arxiv-id": "1508.07859v1", 
    "author": "Sang Wook Lee", 
    "publish": "2015-08-31T14:55:59Z", 
    "summary": "Research interest in rapid structured-light imaging has grown increasingly\nfor the modeling of moving objects, and a number of methods have been suggested\nfor the range capture in a single video frame. The imaging area of a 3D object\nusing a single projector is restricted since the structured light is projected\nonly onto a limited area of the object surface. Employing additional projectors\nto broaden the imaging area is a challenging problem since simultaneous\nprojection of multiple patterns results in their superposition in the\nlight-intersected areas and the recognition of original patterns is by no means\ntrivial. This paper presents a novel method of multi-projector color\nstructured-light vision based on projector-camera triangulation. By analyzing\nthe behavior of superposed-light colors in a chromaticity domain, we show that\nthe original light colors cannot be properly extracted by the conventional\ndirect estimation. We disambiguate multiple projectors by multiplexing the\norientations of projector patterns so that the superposed patterns can be\nseparated by explicit derivative computations. Experimental studies are carried\nout to demonstrate the validity of the presented method. The proposed method\nincreases the efficiency of range acquisition compared to conventional active\nstereo using multiple projectors."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.image.2013.05.005", 
    "link": "http://arxiv.org/pdf/1509.04115v1", 
    "title": "Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range   Imaging", 
    "arxiv-id": "1509.04115v1", 
    "author": "Rae-Hong Park", 
    "publish": "2015-09-14T14:35:20Z", 
    "summary": "Active range sensing using structured-light is the most accurate and reliable\nmethod for obtaining 3D information. However, most of the work has been limited\nto range sensing of static objects, and range sensing of dynamic (moving or\ndeforming) objects has been investigated recently only by a few researchers.\nSinusoidal structured-light is one of the well-known optical methods for 3D\nmeasurement. In this paper, we present a novel method for rapid high-resolution\nrange imaging using color sinusoidal pattern. We consider the real-world\nproblem of nonlinearity and color-band crosstalk in the color light projector\nand color camera, and present methods for accurate recovery of color-phase. For\nhigh-resolution ranging, we use high-frequency patterns and describe new\nunwrapping algorithms for reliable range recovery. The experimental results\ndemonstrate the effectiveness of our methods."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.image.2013.05.005", 
    "link": "http://arxiv.org/pdf/1509.05301v1", 
    "title": "Humans Are Easily Fooled by Digital Images", 
    "arxiv-id": "1509.05301v1", 
    "author": "Tiago J. Carvalho", 
    "publish": "2015-09-17T15:47:25Z", 
    "summary": "Digital images are ubiquitous in our modern lives, with uses ranging from\nsocial media to news, and even scientific papers. For this reason, it is\ncrucial evaluate how accurate people are when performing the task of identify\ndoctored images. In this paper, we performed an extensive user study evaluating\nsubjects capacity to detect fake images. After observing an image, users have\nbeen asked if it had been altered or not. If the user answered the image has\nbeen altered, he had to provide evidence in the form of a click on the image.\nWe collected 17,208 individual answers from 383 users, using 177 images\nselected from public forensic databases. Different from other previously\nstudies, our method propose different ways to avoid lucky guess when evaluating\nusers answers. Our results indicate that people show inaccurate skills at\ndifferentiating between altered and non-altered images, with an accuracy of\n58%, and only identifying the modified images 46.5% of the time. We also track\nuser features such as age, answering time, confidence, providing deep analysis\nof how such variables influence on the users' performance."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-540-76390-1_50", 
    "link": "http://arxiv.org/pdf/1509.05592v1", 
    "title": "Color-Stripe Structured Light Robust to Surface Color and Discontinuity", 
    "arxiv-id": "1509.05592v1", 
    "author": "Sang Wook Lee", 
    "publish": "2015-09-18T11:26:19Z", 
    "summary": "Multiple color stripes have been employed for structured light-based rapid\nrange imaging to increase the number of uniquely identifiable stripes. The use\nof multiple color stripes poses two problems: (1) object surface color may\ndisturb the stripe color and (2) the number of adjacent stripes required for\nidentifying a stripe may not be maintained near surface discontinuities such as\noccluding boundaries. In this paper, we present methods to alleviate those\nproblems. Log-gradient filters are employed to reduce the influence of object\ncolors, and color stripes in two and three directions are used to increase the\nchance of identifying correct stripes near surface discontinuities.\nExperimental results demonstrate the effectiveness of our methods."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-540-76390-1_50", 
    "link": "http://arxiv.org/pdf/1510.02710v1", 
    "title": "Procams-Based Cybernetics", 
    "arxiv-id": "1510.02710v1", 
    "author": "Noriko Takemura", 
    "publish": "2015-10-09T15:47:00Z", 
    "summary": "Procams-based cybernetics is a unique, emerging research field, which aims at\nenhancing and supporting our activities by naturally connecting human and\ncomputers/machines as a cooperative integrated system via projector-camera\nsystems (procams). It rests on various research domains such as\nvirtual/augmented reality, computer vision, computer graphics, projection\ndisplay, human computer interface, human robot interaction and so on. This\nlaboratory presentation provides a brief history including recent achievements\nof our procams-based cybernetics project."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-540-76390-1_50", 
    "link": "http://arxiv.org/pdf/1510.05104v1", 
    "title": "The Theory of Computational Quasi-conformal Geometry on Point Clouds", 
    "arxiv-id": "1510.05104v1", 
    "author": "Lok Ming Lui", 
    "publish": "2015-10-17T08:56:26Z", 
    "summary": "Quasi-conformal (QC) theory is an important topic in complex analysis, which\nstudies geometric patterns of deformations between shapes. Recently,\ncomputational QC geometry has been developed and has made significant\ncontributions to medical imaging, computer graphics and computer vision.\nExisting computational QC theories and algorithms have been built on\ntriangulation structures. In practical situations, many 3D acquisition\ntechniques often produce 3D point cloud (PC) data of the object, which does not\ncontain connectivity information. It calls for a need to develop computational\nQC theories on PCs. In this paper, we introduce the concept of computational QC\ngeometry on PCs. We define PC quasi-conformal (PCQC) maps and their associated\nPC Beltrami coefficients (PCBCs). The PCBC is analogous to the Beltrami\ndifferential in the continuous setting. Theoretically, we show that the PCBC\nconverges to its continuous counterpart as the density of the PC tends to zero.\nWe also theoretically and numerically validate the ability of PCBCs to measure\nlocal geometric distortions of PC deformations. With these concepts, many\nexisting QC based algorithms for geometry processing and shape analysis can be\neasily extended to PC data."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-540-76390-1_50", 
    "link": "http://arxiv.org/pdf/1511.06624v2", 
    "title": "TEMPO: Feature-Endowed Teichm\u00fcller Extremal Mappings of Point Clouds", 
    "arxiv-id": "1511.06624v2", 
    "author": "Lok Ming Lui", 
    "publish": "2015-11-20T14:57:05Z", 
    "summary": "In recent decades, the use of 3D point clouds has been widespread in computer\nindustry. The development of techniques in analyzing point clouds is\nincreasingly important. In particular, mapping of point clouds has been a\nchallenging problem. In this paper, we develop a discrete analogue of the\nTeichm\\\"{u}ller extremal mappings, which guarantee uniform conformality\ndistortions, on point cloud surfaces. Based on the discrete analogue, we\npropose a novel method called TEMPO for computing Teichm\\\"{u}ller extremal\nmappings between feature-endowed point clouds. Using our proposed method, the\nTeichm\\\"{u}ller metric is introduced for evaluating the dissimilarity of point\nclouds. Consequently, our algorithm enables accurate recognition and\nclassification of point clouds. Experimental results demonstrate the\neffectiveness of our proposed method."
},{
    "category": "cs.CV", 
    "doi": "10.1038/srep15373", 
    "link": "http://arxiv.org/pdf/1512.04582v1", 
    "title": "Interactive Volumetry Of Liver Ablation Zones", 
    "arxiv-id": "1512.04582v1", 
    "author": "Michael Moche", 
    "publish": "2015-10-21T08:14:32Z", 
    "summary": "Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique\nthat destroys cancer cells by heat. The heat results from focusing energy in\nthe radiofrequency spectrum through a needle. Amongst others, this can enable\nthe treatment of patients who are not eligible for an open surgery. However,\nthe possibility of recurrent liver cancer due to incomplete ablation of the\ntumor makes post-interventional monitoring via regular follow-up scans\nmandatory. These scans have to be carefully inspected for any conspicuousness.\nWithin this study, the RF ablation zones from twelve post-interventional CT\nacquisitions have been segmented semi-automatically to support the visual\ninspection. An interactive, graph-based contouring approach, which prefers\nspherically shaped regions, has been applied. For the quantitative and\nqualitative analysis of the algorithm's results, manual slice-by-slice\nsegmentations produced by clinical experts have been used as the gold standard\n(which have also been compared among each other). As evaluation metric for the\nstatistical validation, the Dice Similarity Coefficient (DSC) has been\ncalculated. The results show that the proposed tool provides lesion\nsegmentation with sufficient accuracy much faster than manual segmentation. The\nvisual feedback and interactivity make the proposed tool well suitable for the\nclinical workflow."
},{
    "category": "cs.GR", 
    "doi": "10.1038/srep20280", 
    "link": "http://arxiv.org/pdf/1602.01644v1", 
    "title": "A semi-automatic computer-aided method for surgical template design", 
    "arxiv-id": "1602.01644v1", 
    "author": "Jan Egger", 
    "publish": "2016-02-04T11:33:22Z", 
    "summary": "This paper presents a generalized integrated framework of semi-automatic\nsurgical template design. Several algorithms were implemented including the\nmesh segmentation, offset surface generation, collision detection, ruled\nsurface generation, etc., and a special software named TemDesigner was\ndeveloped. With a simple user interface, a customized template can be semi-\nautomatically designed according to the preoperative plan. Firstly, mesh\nsegmentation with signed scalar of vertex is utilized to partition the inner\nsurface from the input surface mesh based on the indicated point loop. Then,\nthe offset surface of the inner surface is obtained through contouring the\ndistance field of the inner surface, and segmented to generate the outer\nsurface. Ruled surface is employed to connect inner and outer surfaces.\nFinally, drilling tubes are generated according to the preoperative plan\nthrough collision detection and merging. It has been applied to the template\ndesign for various kinds of surgeries, including oral implantology, cervical\npedicle screw insertion, iliosacral screw insertion and osteotomy,\ndemonstrating the efficiency, functionality and generality of our method."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.877660", 
    "link": "http://arxiv.org/pdf/1602.02022v1", 
    "title": "Preoperative Volume Determination for Pituitary Adenoma", 
    "arxiv-id": "1602.02022v1", 
    "author": "Christopher Nimsky", 
    "publish": "2016-02-05T14:08:21Z", 
    "summary": "The most common sellar lesion is the pituitary adenoma, and sellar tumors are\napproximately 10-15% of all intracranial neoplasms. Manual slice-by-slice\nsegmentation takes quite some time that can be reduced by using the appropriate\nalgorithms. In this contribution, we present a segmentation method for\npituitary adenoma. The method is based on an algorithm that we have applied\nrecently to segmenting glioblastoma multiforme. A modification of this scheme\nis used for adenoma segmentation that is much harder to perform, due to lack of\ncontrast-enhanced boundaries. In our experimental evaluation, neurosurgeons\nperformed manual slice-by-slice segmentation of ten magnetic resonance imaging\n(MRI) cases. The segmentations were compared to the segmentation results of the\nproposed method using the Dice Similarity Coefficient (DSC). The average DSC\nfor all datasets was 75.92% +/- 7.24%. A manual segmentation took about four\nminutes and our algorithm required about one second."
},{
    "category": "cs.GR", 
    "doi": "10.1016/j.chaos.2016.08.002", 
    "link": "http://arxiv.org/pdf/1602.02139v2", 
    "title": "A simple method for estimating the fractal dimension from digital   images: The compression dimension", 
    "arxiv-id": "1602.02139v2", 
    "author": "P. Chamorro-Posada", 
    "publish": "2016-02-03T23:43:26Z", 
    "summary": "The fractal structure of real world objects is often analyzed using digital\nimages. In this context, the compression fractal dimension is put forward. It\nprovides a simple method for the direct estimation of the dimension of fractals\nstored as digital image files. The computational scheme can be implemented\nusing readily available free software. Its simplicity also makes it very\ninteresting for introductory elaborations of basic concepts of fractal\ngeometry, complexity, and information theory. A test of the computational\nscheme using limited-quality images of well-defined fractal sets obtained from\nthe Internet and free software has been performed. Also, a systematic\nevaluation of the proposed method using computer generated images of the\nWeierstrass cosine function shows an accuracy comparable to those of the\nmethods most commonly used to estimate the dimension of fractal data sequences\napplied to the same test problem."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.769414", 
    "link": "http://arxiv.org/pdf/1602.02881v1", 
    "title": "Detection and Visualization of Endoleaks in CT Data for Monitoring of   Thoracic and Abdominal Aortic Aneurysm Stents", 
    "arxiv-id": "1602.02881v1", 
    "author": "Bernd Freisleben", 
    "publish": "2016-02-09T07:42:05Z", 
    "summary": "In this paper we present an efficient algorithm for the segmentation of the\ninner and outer boundary of thoratic and abdominal aortic aneurysms (TAA & AAA)\nin computed tomography angiography (CTA) acquisitions. The aneurysm\nsegmentation includes two steps: first, the inner boundary is segmented based\non a grey level model with two thresholds; then, an adapted active contour\nmodel approach is applied to the more complicated outer boundary segmentation,\nwith its initialization based on the available inner boundary segmentation. An\nopacity image, which aims at enhancing important features while reducing\nspurious structures, is calculated from the CTA images and employed to guide\nthe deformation of the model. In addition, the active contour model is extended\nby a constraint force that prevents intersections of the inner and outer\nboundary and keeps the outer boundary at a distance, given by the thrombus\nthickness, to the inner boundary. Based upon the segmentation results, we can\nmeasure the aneurysm size at each centerline point on the centerline orthogonal\nmultiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model is\nreconstructed from the set of segmented contours, and the presence of endoleaks\nis detected and highlighted. The implemented method has been evaluated on nine\nclinical CTA data sets with variations in anatomy and location of the pathology\nand has shown promising results."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.769414", 
    "link": "http://arxiv.org/pdf/1602.03308v1", 
    "title": "Gabor Wavelets in Image Processing", 
    "arxiv-id": "1602.03308v1", 
    "author": "David Barina", 
    "publish": "2016-02-10T09:45:38Z", 
    "summary": "This work shows the use of a two-dimensional Gabor wavelets in image\nprocessing. Convolution with such a two-dimensional wavelet can be separated\ninto two series of one-dimensional ones. The key idea of this work is to\nutilize a Gabor wavelet as a multiscale partial differential operator of a\ngiven order. Gabor wavelets are used here to detect edges, corners and blobs. A\nperformance of such an interest point detector is compared to detectors\nutilizing a Haar wavelet and a derivative of a Gaussian function. The proposed\napproach may be useful when a fast implementation of the Gabor transform is\navailable or when the transform is already precomputed."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.769414", 
    "link": "http://arxiv.org/pdf/1602.04906v1", 
    "title": "Segmentation Rectification for Video Cutout via One-Class Structured   Learning", 
    "arxiv-id": "1602.04906v1", 
    "author": "Kun Zhou", 
    "publish": "2016-02-16T04:31:20Z", 
    "summary": "Recent works on interactive video object cutout mainly focus on designing\ndynamic foreground-background (FB) classifiers for segmentation propagation.\nHowever, the research on optimally removing errors from the FB classification\nis sparse, and the errors often accumulate rapidly, causing significant errors\nin the propagated frames. In this work, we take the initial steps to addressing\nthis problem, and we call this new task \\emph{segmentation rectification}. Our\nkey observation is that the possibly asymmetrically distributed false positive\nand false negative errors were handled equally in the conventional methods. We,\nalternatively, propose to optimally remove these two types of errors. To this\neffect, we propose a novel bilayer Markov Random Field (MRF) model for this new\ntask. We also adopt the well-established structured learning framework to learn\nthe optimal model from data. Additionally, we propose a novel one-class\nstructured SVM (OSSVM) which greatly speeds up the structured learning process.\nOur method naturally extends to RGB-D videos as well. Comprehensive experiments\non both RGB and RGB-D data demonstrate that our simple and effective method\nsignificantly outperforms the segmentation propagation methods adopted in the\nstate-of-the-art video cutout systems, and the results also suggest the\npotential usefulness of our method in image cutout system."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.769414", 
    "link": "http://arxiv.org/pdf/1602.07038v2", 
    "title": "Computer Aided Restoration of Handwritten Character Strokes", 
    "arxiv-id": "1602.07038v2", 
    "author": "David Levin", 
    "publish": "2016-02-23T04:47:28Z", 
    "summary": "This work suggests a new variational approach to the task of computer aided\nrestoration of incomplete characters, residing in a highly noisy document. We\nmodel character strokes as the movement of a pen with a varying radius.\nFollowing this model, a cubic spline representation is being utilized to\nperform gradient descent steps, while maintaining interpolation at some initial\n(manually sampled) points. The proposed algorithm was utilized in the process\nof restoring approximately 1000 ancient Hebrew characters (dating to ca.\n8th-7th century BCE), some of which are presented herein and show that the\nalgorithm yields plausible results when applied on deteriorated documents."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2216509", 
    "link": "http://arxiv.org/pdf/1603.00546v1", 
    "title": "US-Cut: Interactive Algorithm for rapid Detection and Segmentation of   Liver Tumors in Ultrasound Acquisitions", 
    "arxiv-id": "1603.00546v1", 
    "author": "Alexander Hann", 
    "publish": "2016-03-02T01:42:48Z", 
    "summary": "Ultrasound (US) is the most commonly used liver imaging modality worldwide.\nIt plays an important role in follow-up of cancer patients with liver\nmetastases. We present an interactive segmentation approach for liver tumors in\nUS acquisitions. Due to the low image quality and the low contrast between the\ntumors and the surrounding tissue in US images, the segmentation is very\nchallenging. Thus, the clinical practice still relies on manual measurement and\noutlining of the tumors in the US images. We target this problem by applying an\ninteractive segmentation algorithm to the US data, allowing the user to get\nreal-time feedback of the segmentation results. The algorithm has been\ndeveloped and tested hand-in-hand by physicians and computer scientists to make\nsure a future practical usage in a clinical setting is feasible. To cover\ntypical acquisitions from the clinical routine, the approach has been evaluated\nwith dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic\n(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.\nDue to the interactive real-time behavior of the approach, it was possible even\nin difficult cases to find satisfying segmentations of the tumors within\nseconds and without parameter settings, and the average tumor deviation was\nonly 1.4mm compared with manual measurements. However, the long term goal is to\nease the volumetric acquisition of liver tumors in order to evaluate for\ntreatment response. Additional aim is the registration of intraoperative US\nimages via the interactive segmentations to the patient's pre-interventional CT\nacquisitions."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2209039", 
    "link": "http://arxiv.org/pdf/1603.00960v1", 
    "title": "Cellular Automata Segmentation of the Boundary between the Compacta of   Vertebral Bodies and Surrounding Structures", 
    "arxiv-id": "1603.00960v1", 
    "author": "Christopher Nimsky", 
    "publish": "2016-03-03T03:35:07Z", 
    "summary": "Due to the aging population, spinal diseases get more and more common\nnowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white women\nand 13% for white men in the United States. Thus the numbers of surgical spinal\nprocedures are also increasing with the aging population and precise diagnosis\nplays a vital role in reducing complication and recurrence of symptoms. Spinal\nimaging of vertebral column is a tedious process subjected to interpretation\nerrors. In this contribution, we aim to reduce time and error for vertebral\ninterpretation by applying and studying the GrowCut-algorithm for boundary\nsegmentation between vertebral body compacta and surrounding structures.\nGrowCut is a competitive region growing algorithm using cellular automata. For\nour study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans were\nfirst manually outlined by neurosurgeons. Then, the vertebral bodies were\nsegmented in the medical images by a GrowCut-trained physician using the\nsemi-automated GrowCut-algorithm. Afterwards, results of both segmentation\nprocesses were compared using the Dice Similarity Coefficient (DSC) and the\nHausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of\n18.91+/-7.2 voxel, respectively. In addition, the times have been measured\nduring the manual and the GrowCut segmentations, showing that a\nGrowCut-segmentation - with an average time of less than six minutes\n(5.77+/-0.73) - is significantly shorter than a pure manual outlining."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2209039", 
    "link": "http://arxiv.org/pdf/1604.01720v1", 
    "title": "Reading Between the Pixels: Photographic Steganography for Camera   Display Messaging", 
    "arxiv-id": "1604.01720v1", 
    "author": "Narayan Mandayam", 
    "publish": "2016-04-06T18:43:18Z", 
    "summary": "We exploit human color metamers to send light-modulated messages less visible\nto the human eye, but recoverable by cameras. These messages are a key\ncomponent to camera-display messaging, such as handheld smartphones capturing\ninformation from electronic signage. Each color pixel in the display image is\nmodified by a particular color gradient vector. The challenge is to find the\ncolor gradient that maximizes camera response, while minimizing human response.\nThe mismatch in human spectral and camera sensitivity curves creates an\nopportunity for hidden messaging. Our approach does not require knowledge of\nthese sensitivity curves, instead we employ a data-driven method. We learn an\nellipsoidal partitioning of the six-dimensional space of colors and color\ngradients. This partitioning creates metamer sets defined by the base color at\nthe display pixel and the color gradient direction for message encoding. We\nsample from the resulting metamer sets to find color steps for each base color\nto embed a binary message into an arbitrary image with reduced visible\nartifacts. Unlike previous methods that rely on visually obtrusive intensity\nmodulation, we embed with color so that the message is more hidden. Ordinary\ndisplays and cameras are used without the need for expensive LEDs or high speed\ndevices. The primary contribution of this work is a framework to map the pixels\nin an arbitrary image to a metamer pair for steganographic photo messaging."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.2209039", 
    "link": "http://arxiv.org/pdf/1604.06525v2", 
    "title": "Opt: A Domain Specific Language for Non-linear Least Squares   Optimization in Graphics and Imaging", 
    "arxiv-id": "1604.06525v2", 
    "author": "Matthias Nie\u00dfner", 
    "publish": "2016-04-22T03:02:59Z", 
    "summary": "Many graphics and vision problems can be expressed as non-linear least\nsquares optimizations of objective functions over visual data, such as images\nand meshes. The mathematical descriptions of these functions are extremely\nconcise, but their implementation in real code is tedious, especially when\noptimized for real-time performance on modern GPUs in interactive applications.\nIn this work, we propose a new language, Opt (available under\nhttp://optlang.org), for writing these objective functions over image- or\ngraph-structured unknowns concisely and at a high level. Our compiler\nautomatically transforms these specifications into state-of-the-art GPU solvers\nbased on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate\ndifferent variations of the solver, so users can easily explore tradeoffs in\nnumerical precision, matrix-free methods, and solver approaches. In our\nresults, we implement a variety of real-world graphics and vision applications.\nTheir energy functions are expressible in tens of lines of code, and produce\nhighly-optimized GPU solver implementations. These solver have performance\ncompetitive with the best published hand-tuned, application-specific GPU\nsolvers, and orders of magnitude beyond a general-purpose auto-generated\nsolver."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.2209039", 
    "link": "http://arxiv.org/pdf/1604.07379v2", 
    "title": "Context Encoders: Feature Learning by Inpainting", 
    "arxiv-id": "1604.07379v2", 
    "author": "Alexei A. Efros", 
    "publish": "2016-04-25T19:42:46Z", 
    "summary": "We present an unsupervised visual feature learning algorithm driven by\ncontext-based pixel prediction. By analogy with auto-encoders, we propose\nContext Encoders -- a convolutional neural network trained to generate the\ncontents of an arbitrary image region conditioned on its surroundings. In order\nto succeed at this task, context encoders need to both understand the content\nof the entire image, as well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experimented with both a\nstandard pixel-wise reconstruction loss, as well as a reconstruction plus an\nadversarial loss. The latter produces much sharper results because it can\nbetter handle multiple modes in the output. We found that a context encoder\nlearns a representation that captures not just appearance but also the\nsemantics of visual structures. We quantitatively demonstrate the effectiveness\nof our learned features for CNN pre-training on classification, detection, and\nsegmentation tasks. Furthermore, context encoders can be used for semantic\ninpainting tasks, either stand-alone or as initialization for non-parametric\nmethods."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1604.08256v1", 
    "title": "Multiview Differential Geometry of Curves", 
    "arxiv-id": "1604.08256v1", 
    "author": "Benjamin Kimia", 
    "publish": "2016-04-27T21:55:39Z", 
    "summary": "The field of multiple view geometry has seen tremendous progress in\nreconstruction and calibration due to methods for extracting reliable point\nfeatures and key developments in projective geometry. Point features, however,\nare not available in certain applications and result in unstructured point\ncloud reconstructions. General image curves provide a complementary feature\nwhen keypoints are scarce, and result in 3D curve geometry, but face challenges\nnot addressed by the usual projective geometry of points and algebraic curves.\nWe address these challenges by laying the theoretical foundations of a\nframework based on the differential geometry of general curves, including\nstationary curves, occluding contours, and non-rigid curves, aiming at stereo\ncorrespondence, camera estimation (including calibration, pose, and multiview\nepipolar geometry), and 3D reconstruction given measured image curves. By\ngathering previous results into a cohesive theory, novel results were made\npossible, yielding three contributions. First we derive the differential\ngeometry of an image curve (tangent, curvature, curvature derivative) from that\nof the underlying space curve (tangent, curvature, curvature derivative,\ntorsion). Second, we derive the differential geometry of a space curve from\nthat of two corresponding image curves. Third, the differential motion of an\nimage curve is derived from camera motion and the differential geometry and\nmotion of the space curve. The availability of such a theory enables novel\ncurve-based multiview reconstruction and camera estimation systems to augment\nexisting point-based approaches. This theory has been used to reconstruct a \"3D\ncurve sketch\", to determine camera pose from local curve geometry, and\ntracking; other developments are underway."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1605.00423v1", 
    "title": "Isogeometric analysis using manifold-based smooth basis functions", 
    "arxiv-id": "1605.00423v1", 
    "author": "Fehmi Cirak", 
    "publish": "2016-05-02T10:35:23Z", 
    "summary": "We present an isogeometric analysis technique that builds on manifold-based\nsmooth basis functions for geometric modelling and analysis. Manifold-based\nsurface construction techniques are well known in geometric modelling and a\nnumber of variants exist. Common to all is the concept of constructing a smooth\nsurface by blending together overlapping patches (or, charts), as in\ndifferential geometry description of manifolds. Each patch on the surface has a\ncorresponding planar patch with a smooth one-to-one mapping onto the surface.\nIn our implementation, manifold techniques are combined with conformal\nparametrisations and the partition-of-unity method for deriving smooth basis\nfunctions on unstructured quadrilateral meshes. Each vertex and its adjacent\nelements on the surface control mesh have a corresponding planar patch of\nelements. The star-shaped planar patch with congruent wedge-shaped elements is\nsmoothly parameterised with copies of a conformally mapped unit square. The\nconformal maps can be easily inverted in order to compute the transition\nfunctions between the different planar patches that have an overlap on the\nsurface. On the collection of star-shaped planar patches the partition of unity\nmethod is used for approximation. The smooth partition of unity, or blending\nfunctions, are assembled from tensor-product b-spline segments defined on a\nunit square. On each patch a polynomial with a prescribed degree is used as a\nlocal approximant. To obtain a mesh-based approximation scheme, the\ncoefficients of the local approximants are expressed in dependence of vertex\ncoefficients. This yields a basis function for each vertex of the mesh which is\nsmooth and non-zero over a vertex and its adjacent elements. Our numerical\nsimulations indicate the optimal convergence of the resulting approximation\nscheme for Poisson problems and near optimal convergence for thin-plate and\nthin-shell problems."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1605.04731v1", 
    "title": "CNN based texture synthesize with Semantic segment", 
    "arxiv-id": "1605.04731v1", 
    "author": "Liangju He", 
    "publish": "2016-05-16T11:24:03Z", 
    "summary": "Deep learning algorithm display powerful ability in Computer Vision area, in\nrecent year, the CNN has been applied to solve problems in the subarea of\nImage-generating, which has been widely applied in areas such as photo editing,\nimage design, computer animation, real-time rendering for large scale of scenes\nand for visual effects in movies. However in the texture synthesize procedure.\nThe state-of-art CNN can not capture the spatial location of texture in image,\nlead to significant distortion after texture synthesize, we propose a new way\nto generating-image by adding the semantic segment step with deep learning\nalgorithm as Pre-Processing and analyze the outcome."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1605.06215v1", 
    "title": "TRIM: Triangulating Images for Efficient Registration", 
    "arxiv-id": "1605.06215v1", 
    "author": "Lok Ming Lui", 
    "publish": "2016-05-20T05:42:12Z", 
    "summary": "With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, the conventional registration\napproaches usually require long computational time for high quality images and\nvideo frames. This hinders the applications of the registration approaches in\nthe modern industries. In this work, we propose a novel approach called {\\em\nTRIM} to accelerate the computations of the registration by triangulating the\nimages. More specifically, given a high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, the computation of the registration can be simplified with\nthe aid of the coarse triangulation. Experimental results suggest that the\ncomputational time of the registration is significantly reduced using our\ntriangulation-based approach, meanwhile the accuracy of the registration is\nwell retained when compared with the conventional grid-based approach."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1606.01873v1", 
    "title": "Optically lightweight tracking of objects around a corner", 
    "arxiv-id": "1606.01873v1", 
    "author": "Matthias B. Hullin", 
    "publish": "2016-06-03T09:17:02Z", 
    "summary": "The observation of objects located in inaccessible regions is a recurring\nchallenge in a wide variety of important applications. Recent work has shown\nthat indirect diffuse light reflections can be used to reconstruct objects and\ntwo-dimensional (2D) patterns around a corner. However, these prior methods\nalways require some specialized setup involving either ultrafast detectors or\nnarrowband light sources. Here we show that occluded objects can be tracked in\nreal time using a standard 2D camera and a laser pointer. Unlike previous\nmethods based on the backprojection approach, we formulate the problem in an\nanalysis-by-synthesis sense. By repeatedly simulating light transport through\nthe scene, we determine the set of object parameters that most closely fits the\nmeasured intensity distribution. We experimentally demonstrate that this\napproach is capable of following the translation of unknown objects, and\ntranslation and orientation of a known object, in real time."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1606.07104v2", 
    "title": "Manifolds' Projective Approximation Using The Moving Least-Squares   (MMLS)", 
    "arxiv-id": "1606.07104v2", 
    "author": "David Levin", 
    "publish": "2016-06-22T20:59:12Z", 
    "summary": "In order to avoid the curse of dimensionality, frequently encountered in Big\nData analysis, there was a vast development in the field of linear and\nnon-linear dimension reduction techniques in recent years. These techniques\n(sometimes referred to as manifold learning) assume that the scattered input\ndata is lying on a lower dimensional manifold, thus the high dimensionality\nproblem can be overcome by learning the lower dimensionality behavior. However,\nin real life applications, data is often very noisy. In this work, we propose a\nmethod to approximate a $d$-dimensional $C^{m+1}$ smooth submanifold\n$\\mathcal{M}$ residing in $\\mathbb{R}^n$ ($d << n$) based upon scattered data\npoints (i.e., a data cloud). We assume that the data points are located \"near\"\nthe noisy lower dimensional manifold and perform a non-linear moving\nleast-squares projection on an approximating manifold. Under some mild\nassumptions, the resulting approximant is shown to be infinitely smooth and of\nhigh approximation order (i.e., $O(h^{m+1})$, where $h$ is the fill distance\nand $m$ is the degree of the local polynomial approximation). Furthermore, the\nmethod presented here assumes no analytic knowledge of the approximated\nmanifold and the approximation algorithm is linear in the large dimension $n$."
},{
    "category": "q-bio.NC", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1607.00697v1", 
    "title": "Neural ideals and stimulus space visualization", 
    "arxiv-id": "1607.00697v1", 
    "author": "Nora Youngs", 
    "publish": "2016-07-03T22:54:11Z", 
    "summary": "A neural code $\\mathcal{C}$ is a collection of binary vectors of a given\nlength n that record the co-firing patterns of a set of neurons. Our focus is\non neural codes arising from place cells, neurons that respond to geographic\nstimulus. In this setting, the stimulus space can be visualized as subset of\n$\\mathbb{R}^2$ covered by a collection $\\mathcal{U}$ of convex sets such that\nthe arrangement $\\mathcal{U}$ forms an Euler diagram for $\\mathcal{C}$. There\nare some methods to determine whether such a convex realization $\\mathcal{U}$\nexists; however, these methods do not describe how to draw a realization. In\nthis work, we look at the problem of algorithmically drawing Euler diagrams for\nneural codes using two polynomial ideals: the neural ideal, a pseudo-monomial\nideal; and the neural toric ideal, a binomial ideal. In particular, we study\nhow these objects are related to the theory of piercings in information\nvisualization, and we show how minimal generating sets of the ideals reveal\nwhether or not a code is $0$, $1$, or $2$-inductively pierced."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1607.04411v1", 
    "title": "Model-Driven Feed-Forward Prediction for Manipulation of Deformable   Objects", 
    "arxiv-id": "1607.04411v1", 
    "author": "Peter Allen", 
    "publish": "2016-07-15T08:01:13Z", 
    "summary": "Robotic manipulation of deformable objects is a difficult problem especially\nbecause of the complexity of the many different ways an object can deform.\nSearching such a high dimensional state space makes it difficult to recognize,\ntrack, and manipulate deformable objects. In this paper, we introduce a\npredictive, model-driven approach to address this challenge, using a\npre-computed, simulated database of deformable object models. Mesh models of\ncommon deformable garments are simulated with the garments picked up in\nmultiple different poses under gravity, and stored in a database for fast and\nefficient retrieval. To validate this approach, we developed a comprehensive\npipeline for manipulating clothing as in a typical laundry task. First, the\ndatabase is used for category and pose estimation for a garment in an arbitrary\nposition. A fully featured 3D model of the garment is constructed in real-time\nand volumetric features are then used to obtain the most similar model in the\ndatabase to predict the object category and pose. Second, the database can\nsignificantly benefit the manipulation of deformable objects via non-rigid\nregistration, providing accurate correspondences between the reconstructed\nobject model and the database models. Third, the accurate model simulation can\nalso be used to optimize the trajectories for manipulation of deformable\nobjects, such as the folding of garments. Extensive experimental results are\nshown for the tasks above using a variety of different clothing."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1607.07427v1", 
    "title": "Mesh Denoising based on Normal Voting Tensor and Binary Optimization", 
    "arxiv-id": "1607.07427v1", 
    "author": "K. Polthier", 
    "publish": "2016-07-20T20:39:37Z", 
    "summary": "This paper presents a tensor multiplication based smoothing algorithm that\nfollows a two step denoising method. Unlike other traditional averaging\napproaches, our approach uses an element based normal voting tensor to compute\nsmooth surfaces. By introducing a binary optimization on the proposed tensor\ntogether with a local binary neighborhood concept, our algorithm better retains\nsharp features and produces smoother umbilical regions than previous\napproaches. On top of that, we provide a stochastic analysis on the different\nkinds of noise based on the average edge length. The quantitative and visual\nresults demonstrate the performance our method is better compared to state of\nthe art smoothing approaches."
},{
    "category": "cs.CG", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1608.01140v1", 
    "title": "Fast Spherical Quasiconformal Parameterization of Genus-0 Closed   Surfaces with Application to Adaptive Remeshing", 
    "arxiv-id": "1608.01140v1", 
    "author": "Lok Ming Lui", 
    "publish": "2016-08-03T10:33:21Z", 
    "summary": "In this work, we are concerned with the spherical quasiconformal\nparameterization of genus-0 closed surfaces. Given a genus-0 closed\ntriangulated surface and an arbitrary user-defined quasiconformal distortion,\nwe propose a fast algorithm for computing a spherical parameterization of the\nsurface that satisfies the prescribed distortion. The proposed algorithm can be\neffectively applied to adaptive surface remeshing for improving the\nvisualization in computer graphics and animations. Experimental results are\npresented to illustrate the effectiveness of our algorithm."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1608.05205v3", 
    "title": "Tree-decomposable and Underconstrained Geometric Constraint Problems", 
    "arxiv-id": "1608.05205v3", 
    "author": "Robert Joan-Arinyo", 
    "publish": "2016-08-18T08:25:42Z", 
    "summary": "In this paper, we are concerned with geometric constraint solvers, i.e., with\nprograms that find one or more solutions of a geometric constraint problem. If\nno solution exists, the solver is expected to announce that no solution has\nbeen found. Owing to the complexity, type or difficulty of a constraint\nproblem, it is possible that the solver does not find a solution even though\none may exist. Thus, there may be false negatives, but there should never be\nfalse positives. Intuitively, the ability to find solutions can be considered a\nmeasure of solver's competence. We consider static constraint problems and\ntheir solvers. We do not consider dynamic constraint solvers, also known as\ndynamic geometry programs, in which specific geometric elements are moved,\ninteractively or along prescribed trajectories, while continually maintaining\nall stipulated constraints. However, if we have a solver for static constraint\nproblems that is sufficiently fast and competent, we can build a dynamic\ngeometry program from it by solving the static problem for a sufficiently dense\nsampling of the trajectory of the moving element(s). The work we survey has its\nroots in applications, especially in mechanical computer-aided design (MCAD).\nThe constraint solvers used in MCAD took a quantum leap in the 1990s. These\napproaches solve a geometric constraint problem by an initial, graph-based\nstructural analysis that extracts generic subproblems and determines how they\nwould combine to form a complete solution. These subproblems are then handed to\nan algebraic solver that solves the specific instances of the generic\nsubproblems and combines them."
},{
    "category": "cs.CV", 
    "doi": "10.1007/s11263-016-0912-7", 
    "link": "http://arxiv.org/pdf/1609.02612v3", 
    "title": "Generating Videos with Scene Dynamics", 
    "arxiv-id": "1609.02612v3", 
    "author": "Antonio Torralba", 
    "publish": "2016-09-08T22:29:52Z", 
    "summary": "We capitalize on large amounts of unlabeled video in order to learn a model\nof scene dynamics for both video recognition tasks (e.g. action classification)\nand video generation tasks (e.g. future prediction). We propose a generative\nadversarial network for video with a spatio-temporal convolutional architecture\nthat untangles the scene's foreground from the background. Experiments suggest\nthis model can generate tiny videos up to a second at full frame rate better\nthan simple baselines, and we show its utility at predicting plausible futures\nof static images. Moreover, experiments and visualizations show the model\ninternally learns useful features for recognizing actions with minimal\nsupervision, suggesting scene dynamics are a promising signal for\nrepresentation learning. We believe generative video models can impact many\napplications in video understanding and simulation."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1609.05561v1", 
    "title": "From Multiview Image Curves to 3D Drawings", 
    "arxiv-id": "1609.05561v1", 
    "author": "Benjamin B. Kimia", 
    "publish": "2016-09-18T22:20:35Z", 
    "summary": "Reconstructing 3D scenes from multiple views has made impressive strides in\nrecent years, chiefly by correlating isolated feature points, intensity\npatterns, or curvilinear structures. In the general setting - without\ncontrolled acquisition, abundant texture, curves and surfaces following\nspecific models or limiting scene complexity - most methods produce unorganized\npoint clouds, meshes, or voxel representations, with some exceptions producing\nunorganized clouds of 3D curve fragments. Ideally, many applications require\nstructured representations of curves, surfaces and their spatial relationships.\nThis paper presents a step in this direction by formulating an approach that\ncombines 2D image curves into a collection of 3D curves, with topological\nconnectivity between them represented as a 3D graph. This results in a 3D\ndrawing, which is complementary to surface representations in the same sense as\na 3D scaffold complements a tent taut over it. We evaluate our results against\ntruth on synthetic and real datasets."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1609.08685v2", 
    "title": "Understanding and Exploiting Object Interaction Landscapes", 
    "arxiv-id": "1609.08685v2", 
    "author": "Leonidas J. Guibas", 
    "publish": "2016-09-27T22:00:56Z", 
    "summary": "Interactions play a key role in understanding objects and scenes, for both\nvirtual and real world agents. We introduce a new general representation for\nproximal interactions among physical objects that is agnostic to the type of\nobjects or interaction involved. The representation is based on tracking\nparticles on one of the participating objects and then observing them with\nsensors appropriately placed in the interaction volume or on the interaction\nsurfaces. We show how to factorize these interaction descriptors and project\nthem into a particular participating object so as to obtain a new functional\ndescriptor for that object, its interaction landscape, capturing its observed\nuse in a spatio-temporal framework. Interaction landscapes are independent of\nthe particular interaction and capture subtle dynamic effects in how objects\nmove and behave when in functional use. Our method relates objects based on\ntheir function, establishes correspondences between shapes based on functional\nkey points and regions, and retrieves peer and partner objects with respect to\nan interaction."
},{
    "category": "cs.MM", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1609.08729v4", 
    "title": "Adaptive 360 VR Video Streaming: Divide and Conquer!", 
    "arxiv-id": "1609.08729v4", 
    "author": "Viswanathan Swaminathan", 
    "publish": "2016-09-28T02:07:12Z", 
    "summary": "While traditional multimedia applications such as games and videos are still\npopular, there has been a significant interest in the recent years towards new\n3D media such as 3D immersion and Virtual Reality (VR) applications, especially\n360 VR videos. 360 VR video is an immersive spherical video where the user can\nlook around during playback. Unfortunately, 360 VR videos are extremely\nbandwidth intensive, and therefore are difficult to stream at acceptable\nquality levels. In this paper, we propose an adaptive bandwidth-efficient 360\nVR video streaming system using a divide and conquer approach. In our approach,\nwe propose a dynamic view-aware adaptation technique to tackle the huge\nstreaming bandwidth demands of 360 VR videos. We spatially divide the videos\ninto multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe\nthe spatial relationship of tiles in the 360-degree space, and prioritize the\ntiles in the Field of View (FoV). In order to describe such tiled\nrepresentations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We\nspatially partition the underlying 3D mesh, and construct an efficient 3D\ngeometry mesh called hexaface sphere to optimally represent a tiled 360 VR\nvideo in the 3D space. Our initial evaluation results report up to 72%\nbandwidth savings on 360 VR video streaming with minor negative quality impacts\ncompared to the baseline scenario when no adaptations is applied."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1610.04531v1", 
    "title": "Numerical Inversion of SRNF Maps for Elastic Shape Analysis of   Genus-Zero Surfaces", 
    "arxiv-id": "1610.04531v1", 
    "author": "Anuj Srivastava", 
    "publish": "2016-10-14T16:56:49Z", 
    "summary": "Recent developments in elastic shape analysis (ESA) are motivated by the fact\nthat it provides comprehensive frameworks for simultaneous registration,\ndeformation, and comparison of shapes. These methods achieve computational\nefficiency using certain square-root representations that transform invariant\nelastic metrics into Euclidean metrics, allowing for applications of standard\nalgorithms and statistical tools. For analyzing shapes of embeddings of\n$\\mathbb{S}^2$ in $\\mathbb{R}^3$, Jermyn et al. introduced square-root normal\nfields (SRNFs) that transformed an elastic metric, with desirable invariant\nproperties, into the $\\mathbb{L}^2$ metric. These SRNFs are essentially surface\nnormals scaled by square-roots of infinitesimal area elements. A critical need\nin shape analysis is to invert solutions (deformations, averages, modes of\nvariations, etc) computed in the SRNF space, back to the original surface space\nfor visualizations and inferences. Due to the lack of theory for understanding\nSRNFs maps and their inverses, we take a numerical approach and derive an\nefficient multiresolution algorithm, based on solving an optimization problem\nin the surface space, that estimates surfaces corresponding to given SRNFs.\nThis solution is found effective, even for complex shapes, e.g. human bodies\nand animals, that undergo significant deformations including bending and\nstretching. Specifically, we use this inversion for computing elastic shape\ndeformations, transferring deformations, summarizing shapes, and for finding\nmodes of variability in a given collection, while simultaneously registering\nthe surfaces. We demonstrate the proposed algorithms using a statistical\nanalysis of human body shapes, classification of generic surfaces and analysis\nof brain structures."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1610.09534v3", 
    "title": "FlyCap: Markerless Motion Capture Using Multiple Autonomous Flying   Cameras", 
    "arxiv-id": "1610.09534v3", 
    "author": "Yebin Liu", 
    "publish": "2016-10-29T15:44:07Z", 
    "summary": "Aiming at automatic, convenient and non-instrusive motion capture, this paper\npresents a new generation markerless motion capture technique, the FlyCap\nsystem, to capture surface motions of moving characters using multiple\nautonomous flying cameras (autonomous unmanned aerial vehicles(UAV) each\nintegrated with an RGBD video camera). During data capture, three cooperative\nflying cameras automatically track and follow the moving target who performs\nlarge scale motions in a wide space. We propose a novel non-rigid surface\nregistration method to track and fuse the depth of the three flying cameras for\nsurface motion tracking of the moving target, and simultaneously calculate the\npose of each flying camera. We leverage the using of visual-odometry\ninformation provided by the UAV platform, and formulate the surface tracking\nproblem in a non-linear objective function that can be linearized and\neffectively minimized through a Gaussian-Newton method. Quantitative and\nqualitative experimental results demonstrate the competent and plausible\nsurface and motion reconstruction results"
},{
    "category": "cs.LG", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1611.01055v1", 
    "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space   Matter?", 
    "arxiv-id": "1611.01055v1", 
    "author": "Michiel van de Panne", 
    "publish": "2016-11-03T15:15:00Z", 
    "summary": "The use of deep reinforcement learning allows for high-dimensional state\ndescriptors, but little is known about how the choice of action representation\nimpacts the learning difficulty and the resulting performance. We compare the\nimpact of four different action parameterizations (torques, muscle-activations,\ntarget joint angles, and target joint-angle velocities) in terms of learning\ntime, policy robustness, motion quality, and policy query rates. Our results\nare evaluated on a gait-cycle imitation task for multiple planar articulated\nfigures and multiple gaits. We demonstrate that the local feedback provided by\nhigher-level action parameterizations can significantly impact the learning,\nrobustness, and quality of the resulting policies."
},{
    "category": "math.OC", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1611.07369v3", 
    "title": "Geometry of 3D Environments and Sum of Squares Polynomials", 
    "arxiv-id": "1611.07369v3", 
    "author": "Vikas Sindhwani", 
    "publish": "2016-11-22T15:40:14Z", 
    "summary": "Motivated by applications in robotics and computer vision, we study problems\nrelated to spatial reasoning of a 3D environment using sublevel sets of\npolynomials. These include: tightly containing a cloud of points (e.g.,\nrepresenting an obstacle) with convex or nearly-convex basic semialgebraic\nsets, computation of Euclidean distances between two such sets, separation of\ntwo convex basic semalgebraic sets that overlap, and tight containment of the\nunion of several basic semialgebraic sets with a single convex one. We use\nalgebraic techniques from sum of squares optimization that reduce all these\ntasks to semidefinite programs of small size and present numerical experiments\nin realistic scenarios."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1612.00132v1", 
    "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional   Variational Generation", 
    "arxiv-id": "1612.00132v1", 
    "author": "David Forsyth", 
    "publish": "2016-12-01T03:40:42Z", 
    "summary": "Problems such as predicting an optical flow field (Y) for an image (X) are\nambiguous: many very distinct solutions are good. Representing this ambiguity\nrequires building a conditional model P(Y|X) of the prediction, conditioned on\nthe image. It is hard because training data usually does not contain many\ndifferent flow fields for the same image. As a result, we need different images\nto share data to produce good models. We demonstrate an improved method for\nbuilding conditional models, the Co-Embedding Deep Variational Auto Encoder.\nOur CDVAE exploits multiple encoding and decoding layers for both X and Y.\nThese are tied during training to produce a model of the joint distribution\nP(X, Y), which provides the necessary smoothing. Our tying procedure is\ndesigned to yield a conditional model easy at test time. We demonstrate our\nmodel on three example tasks using real data: image saturation adjustment,\nimage relighting, and motion prediction. We describe quantitative evaluation\nmetrics to evaluate ambiguous generation results. Our results quantitatively\nand qualitatively advance the state of the art."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1612.00814v2", 
    "title": "Perspective Transformer Nets: Learning Single-View 3D Object   Reconstruction without 3D Supervision", 
    "arxiv-id": "1612.00814v2", 
    "author": "Honglak Lee", 
    "publish": "2016-12-01T05:51:37Z", 
    "summary": "Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved."
},{
    "category": "cs.IT", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1612.04227v1", 
    "title": "CFD results calibration from sparse sensor observations with a case   study for indoor thermal map", 
    "arxiv-id": "1612.04227v1", 
    "author": "Deqing Zhai", 
    "publish": "2016-12-13T15:19:37Z", 
    "summary": "Current CFD calibration work has mainly focused on the CFD model calibration.\nHowever no known work has considered the calibration of the CFD results. In\nthis paper, we take inspiration from the image editing problem to develop a\nmethodology to calibrate CFD simulation results based on sparse sensor\nobservations. We formulate the calibration of CFD results as an optimization\nproblem. The cost function consists of two terms. One term guarantees a good\nlocal adjustment of the simulation results based on the sparse sensor\nobservations. The other term transmits the adjustment from local regions around\nsensing locations to the global domain. The proposed method can enhance the CFD\nsimulation results while preserving the overall original profile. An experiment\nin an air-conditioned room was implemented to verify the effectiveness of the\nproposed method. In the experiment, four sensor observations were used to\ncalibrate a simulated thermal map with 167x365 data points. The experimental\nresults show that the proposed method is effective and practical."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1612.04337v1", 
    "title": "Fast Patch-based Style Transfer of Arbitrary Style", 
    "arxiv-id": "1612.04337v1", 
    "author": "Mark Schmidt", 
    "publish": "2016-12-13T20:05:37Z", 
    "summary": "Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1701.06641v1", 
    "title": "Perceptually Optimized Image Rendering", 
    "arxiv-id": "1701.06641v1", 
    "author": "Eero P. Simoncelli", 
    "publish": "2017-01-23T21:38:52Z", 
    "summary": "We develop a framework for rendering photographic images, taking into account\ndisplay limitations, so as to optimize perceptual similarity between the\nrendered image and the original scene. We formulate this as a constrained\noptimization problem, in which we minimize a measure of perceptual\ndissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics\nthe early stage transformations of the human visual system. When rendering\nimages acquired with higher dynamic range than that of the display, we find\nthat the optimized solution boosts the contrast of low-contrast features\nwithout introducing significant artifacts, yielding results of comparable\nvisual quality to current state-of-the art methods with no manual intervention\nor parameter settings. We also examine a variety of other display constraints,\nincluding limitations on minimum luminance (black point), mean luminance (as a\nproxy for energy consumption), and quantized luminance levels (halftoning).\nFinally, we show that the method may be used to enhance details and contrast of\nimages degraded by optical scattering (e.g. fog)."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1701.08893v2", 
    "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer   Using Histogram Losses", 
    "arxiv-id": "1701.08893v2", 
    "author": "Connelly Barnes", 
    "publish": "2017-01-31T02:37:19Z", 
    "summary": "Recently, methods have been proposed that perform texture synthesis and style\ntransfer by using convolutional neural networks (e.g. Gatys et al.\n[2015,2016]). These methods are exciting because they can in some cases create\nresults with state-of-the-art quality. However, in this paper, we show these\nmethods also have limitations in texture quality, stability, requisite\nparameter tuning, and lack of user controls. This paper presents a multiscale\nsynthesis pipeline based on convolutional neural networks that ameliorates\nthese issues. We first give a mathematical explanation of the source of\ninstabilities in many previous approaches. We then improve these instabilities\nby using histogram losses to synthesize textures that better statistically\nmatch the exemplar. We also show how to integrate localized style losses in our\nmultiscale framework. These losses can improve the quality of large features,\nimprove the separation of content and style, and offer artistic controls such\nas paint by numbers. We demonstrate that our approach offers improved quality,\nconvergence in fewer iterations, and more stability over the optimization."
},{
    "category": "cs.GR", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1702.00737v1", 
    "title": "HoNVis: Visualizing and Exploring Higher-Order Networks", 
    "arxiv-id": "1702.00737v1", 
    "author": "Nitesh V. Chawla", 
    "publish": "2017-02-02T16:18:39Z", 
    "summary": "Unlike the conventional first-order network (FoN), the higher-order network\n(HoN) provides a more accurate description of transitions by creating\nadditional nodes to encode higher-order dependencies. However, there exists no\nvisualization and exploration tool for the HoN. For applications such as the\ndevelopment of strategies to control species invasion through global shipping\nwhich is known to exhibit higher-order dependencies, the existing FoN\nvisualization tools are limited. In this paper, we present HoNVis, a novel\nvisual analytics framework for exploring higher-order dependencies of the\nglobal ocean shipping network. Our framework leverages coordinated multiple\nviews to reveal the network structure at three levels of detail (i.e., the\nglobal, local, and individual port levels). Users can quickly identify ports of\ninterest at the global level and specify a port to investigate its higher-order\nnodes at the individual port level. Investigating a larger-scale impact is\nenabled through the exploration of HoN at the local level. Using the global\nocean shipping network data, we demonstrate the effectiveness of our approach\nwith a real-world use case conducted by domain experts specializing in species\ninvasion. Finally, we discuss the generalizability of this framework to other\nreal-world applications such as information diffusion in social networks and\nepidemic spreading through air transportation."
},{
    "category": "cs.IT", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1702.01961v2", 
    "title": "A Region Based Easy Path Wavelet Transform For Sparse Image   Representation", 
    "arxiv-id": "1702.01961v2", 
    "author": "Renato Budinich", 
    "publish": "2017-02-07T11:13:08Z", 
    "summary": "The Easy Path Wavelet Transform is an adaptive transform for bivariate\nfunctions (in particular natural images) which has been proposed in [1]. It\nprovides a sparse representation by finding a path in the domain of the\nfunction leveraging the local correlations of the function values. It then\napplies a one dimensional wavelet transform to the obtained vector, decimates\nthe points and iterates the procedure. The main drawback of such method is the\nneed to store, for each level of the transform, the path which vectorizes the\ntwo dimensional data. Here we propose a variation on the method which consists\nof firstly applying a segmentation procedure to the function domain,\npartitioning it into regions where the variation in the function values is low;\nin a second step, inside each such region, a path is found in some\ndeterministic way, i.e. not data-dependent. This circumvents the need to store\nthe paths at each level, while still obtaining good quality lossy compression.\nThis method is particularly well suited to encode a Region of Interest in the\nimage with different quality than the rest of the image.\n  [1] Gerlind Plonka. The easy path wavelet transform: A new adaptive wavelet\ntransform for sparse representation of two-dimensional data. Multiscale\nModeling & Simulation, 7(3):1474$-$1496, 2008."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-3-319-46493-0_5", 
    "link": "http://arxiv.org/pdf/1702.02463v1", 
    "title": "Video Frame Synthesis using Deep Voxel Flow", 
    "arxiv-id": "1702.02463v1", 
    "author": "Aseem Agarwala", 
    "publish": "2017-02-08T15:20:14Z", 
    "summary": "We address the problem of synthesizing new video frames in an existing video,\neither in-between existing frames (interpolation), or subsequent to them\n(extrapolation). This problem is challenging because video appearance and\nmotion can be highly complex. Traditional optical-flow-based solutions often\nfail where flow estimation is challenging, while newer neural-network-based\nmethods that hallucinate pixel values directly often produce blurry results. We\ncombine the advantages of these two methods by training a deep network that\nlearns to synthesize video frames by flowing pixel values from existing ones,\nwhich we call deep voxel flow. Our method requires no human supervision, and\nany video can be used as training data by dropping, and then learning to\npredict, existing frames. The technique is efficient, and can be applied at any\nvideo resolution. We demonstrate that our method produces results that both\nquantitatively and qualitatively improve upon the state-of-the-art."
},{
    "category": "cs.CV", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1702.02514v1", 
    "title": "Monocular LSD-SLAM Integration within AR System", 
    "arxiv-id": "1702.02514v1", 
    "author": "Vincent Lepetit", 
    "publish": "2017-02-08T16:52:19Z", 
    "summary": "In this paper, we cover the process of integrating Large-Scale Direct\nSimultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR\nstereo engine, developed for our modified \"Augmented Reality Oculus Rift\". With\nthat, we are able to track one of our realworld cameras which are mounted on\nthe rift, within a complete unknown environment. This makes it possible to\nachieve a constant and full augmentation, synchronizing our 3D movement (x, y,\nz) in both worlds, the real world and the virtual world. The development for\nthe basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully\ndocumented in our previous paper. After an introduction to image-based\nregistration, we detail the LSD-SLAM algorithm and document our code\nimplementing our integration. The AR stereo engine with Oculus Rift support can\nbe accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and\nthe modified LSD-SLAM project used for the integration is available here\nhttps://github.com/MaXvanHeLL/LSD-SLAM.git."
},{
    "category": "cs.CV", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1702.06228v1", 
    "title": "Learning to Generate Posters of Scientific Papers by Probabilistic   Graphical Models", 
    "arxiv-id": "1702.06228v1", 
    "author": "Leonid Sigal", 
    "publish": "2017-02-21T01:02:56Z", 
    "summary": "Researchers often summarize their work in the form of scientific posters.\nPosters provide a coherent and efficient way to convey core ideas expressed in\nscientific papers. Generating a good scientific poster, however, is a complex\nand time consuming cognitive task, since such posters need to be readable,\ninformative, and visually aesthetic. In this paper, for the first time, we\nstudy the challenging problem of learning to generate posters from scientific\npapers. To this end, a data-driven framework, that utilizes graphical models,\nis proposed. Specifically, given content to display, the key elements of a good\nposter, including attributes of each panel and arrangements of graphical\nelements are learned and inferred from data. During the inference stage, an MAP\ninference framework is employed to incorporate some design principles. In order\nto bridge the gap between panel attributes and the composition within each\npanel, we also propose a recursive page splitting algorithm to generate the\npanel layout for a poster. To learn and validate our model, we collect and\nrelease a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which\nconsists of scientific papers and corresponding posters with exhaustively\nlabelled panels and attributes. Qualitative and quantitative results indicate\nthe effectiveness of our approach."
},{
    "category": "cs.GR", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1703.00050v1", 
    "title": "SceneSeer: 3D Scene Design with Natural Language", 
    "arxiv-id": "1703.00050v1", 
    "author": "Christopher D. Manning", 
    "publish": "2017-02-28T20:47:47Z", 
    "summary": "Designing 3D scenes is currently a creative task that requires significant\nexpertise and effort in using complex 3D design interfaces. This effortful\ndesign process starts in stark contrast to the easiness with which people can\nuse language to describe real and imaginary environments. We present SceneSeer:\nan interactive text to 3D scene generation system that allows a user to design\n3D scenes using natural language. A user provides input text from which we\nextract explicit constraints on the objects that should appear in the scene.\nGiven these explicit constraints, the system then uses a spatial knowledge base\nlearned from an existing database of 3D scenes and 3D object models to infer an\narrangement of the objects forming a natural scene matching the input\ndescription. Using textual commands the user can then iteratively refine the\ncreated scene by adding, removing, replacing, and manipulating objects. We\nevaluate the quality of 3D scenes generated by SceneSeer in a perceptual\nevaluation experiment where we compare against manually designed scenes and\nsimpler baselines for 3D scene generation. We demonstrate how the generated\nscenes can be iteratively refined through simple natural language commands."
},{
    "category": "stat.ML", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1703.01499v2", 
    "title": "A Machine-Learning Framework for Design for Manufacturability", 
    "arxiv-id": "1703.01499v2", 
    "author": "Adarsh Krishnamurthy", 
    "publish": "2017-03-04T17:37:32Z", 
    "summary": "this is a duplicate submission(original is arXiv:1612.02141). Hence want to\nwithdraw it"
},{
    "category": "physics.ins-det", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1703.02635v1", 
    "title": "A Computational Model of a Single-Photon Avalanche Diode Sensor for   Transient Imaging", 
    "arxiv-id": "1703.02635v1", 
    "author": "Adrian Jarabo", 
    "publish": "2017-02-23T10:17:34Z", 
    "summary": "Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable\nto collect extremely fast low-energy events, due to their single-photon\nsensibility. This makes them very suitable for time-of-flight-based range\nimaging systems, allowing to reduce costs and power requirements, without\nsacrifizing much temporal resolution. In this work we describe a computational\nmodel to simulate the behaviour of SPAD sensors, aiming to provide a realistic\ncamera model for time-resolved light transport simulation, with applications on\nprototyping new reconstructions techniques based on SPAD time-of-flight data.\nOur model accounts for the major effects of the sensor on the incoming signal.\nWe compare our model against real-world measurements, and apply it to a variety\nof scenarios, including complex multiply-scattered light transport."
},{
    "category": "cs.CV", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1703.04861v1", 
    "title": "Robust Non-Rigid Registration With Reweighted Dual Sparsities", 
    "arxiv-id": "1703.04861v1", 
    "author": "Daoliang Guo", 
    "publish": "2017-03-15T01:00:44Z", 
    "summary": "Non-rigid registration is challenging because it is ill-posed with high\ndegrees of freedom and is thus sensitive to noise and outliers. We propose a\nrobust non-rigid registration method using reweighted sparsities on position\nand transformation to estimate the deformations between 3-D shapes. We\nformulate the energy function with dual sparsities on both the data term and\nthe smoothness term, and define the smoothness constraint using local rigidity.\nThe dual-sparsity based non-rigid registration model is enhanced with a\nreweighting scheme, and solved by transferring the model into some alternating\noptimized subproblems which have exact solutions and guaranteed convergence.\nExperimental results on both public datasets and real scanned datasets show\nthat our method outperforms the state-of-the-art methods and is more robust to\nnoise and outliers than conventional non-rigid registration methods."
},{
    "category": "cs.CG", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1408.6974v1", 
    "title": "Fast Disk Conformal Parameterization of Simply-connected Open Surfaces", 
    "arxiv-id": "1408.6974v1", 
    "author": "Lok Ming Lui", 
    "publish": "2014-08-29T10:31:56Z", 
    "summary": "Surface parameterizations have been widely used in computer graphics and\ngeometry processing. In particular, as simply-connected open surfaces are\nconformally equivalent to the unit disk, it is desirable to compute the disk\nconformal parameterizations of the surfaces. In this paper, we propose a novel\nalgorithm for the conformal parameterization of a simply-connected open surface\nonto the unit disk, which significantly speeds up the computation, enhances the\nconformality and stability, and guarantees the bijectivity. The conformality\ndistortions at the inner region and on the boundary are corrected by two steps,\nwith the aid of an iterative scheme using quasi-conformal theories.\nExperimental results demonstrate the effectiveness of our proposed method."
},{
    "category": "cs.IT", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1207.0757v1", 
    "title": "Generalized Statistical Complexity of SAR Imagery", 
    "arxiv-id": "1207.0757v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-03T17:25:18Z", 
    "summary": "A new generalized Statistical Complexity Measure (SCM) was proposed by Rosso\net al in 2010. It is a functional that captures the notions of order/disorder\nand of distance to an equilibrium distribution. The former is computed by a\nmeasure of entropy, while the latter depends on the definition of a stochastic\ndivergence. When the scene is illuminated by coherent radiation, image data is\ncorrupted by speckle noise, as is the case of ultrasound-B, sonar, laser and\nSynthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats,\nthis noise is multiplicative and non-Gaussian requiring, thus, specialized\ntechniques for image processing and understanding. One of the most successful\nfamily of models for describing these images is the Multiplicative Model which\nleads, among other probability distributions, to the G0 law. This distribution\nhas been validated in the literature as an expressive and tractable model,\ndeserving the \"universal\" denomination for its ability to describe most types\nof targets. In order to compute the statistical complexity of a site in an\nimage corrupted by speckle noise, we assume that the equilibrium distribution\nis that of fully developed speckle, namely the Gamma law in intensity format,\nwhich appears in areas with little or no texture. We use the Shannon entropy\nalong with the Hellinger distance to measure the statistical complexity of\nintensity SAR images, and we show that it is an expressive feature capable of\nidentifying many types of targets."
},{
    "category": "cs.GR", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1505.01214v1", 
    "title": "Learning Style Similarity for Searching Infographics", 
    "arxiv-id": "1505.01214v1", 
    "author": "Zhicheng Liu", 
    "publish": "2015-05-05T22:59:32Z", 
    "summary": "Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test."
},{
    "category": "cs.GR", 
    "doi": "10.13140/RG.2.2.10054.27205", 
    "link": "http://arxiv.org/pdf/1512.03012v1", 
    "title": "ShapeNet: An Information-Rich 3D Model Repository", 
    "arxiv-id": "1512.03012v1", 
    "author": "Fisher Yu", 
    "publish": "2015-12-09T19:42:48Z", 
    "summary": "We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans."
},{
    "category": "cs.GR", 
    "doi": "10.1117/12.709260", 
    "link": "http://arxiv.org/pdf/1602.02490v1", 
    "title": "Simulation of bifurcated stent grafts to treat abdominal aortic   aneurysms (AAA)", 
    "arxiv-id": "1602.02490v1", 
    "author": "Bernd Freisleben", 
    "publish": "2016-02-08T08:09:06Z", 
    "summary": "In this paper a method is introduced, to visualize bifurcated stent grafts in\nCT-Data. The aim is to improve therapy planning for minimal invasive treatment\nof abdominal aortic aneurysms (AAA). Due to precise measurement of the\nabdominal aortic aneurysm and exact simulation of the bifurcated stent graft,\nphysicians are supported in choosing a suitable stent prior to an intervention.\nThe presented method can be used to measure the dimensions of the abdominal\naortic aneurysm as well as simulate a bifurcated stent graft. Both of these\nprocedures are based on a preceding segmentation and skeletonization of the\naortic, right and left iliac. Using these centerlines (aortic, right and left\niliac) a bifurcated initial stent is constructed. Through the implementation of\nan ACM method the initial stent is fit iteratively to the vessel walls - due to\nthe influence of external forces (distance- as well as balloonforce). Following\nthe fitting process, the crucial values for choosing a bifurcated stent graft\nare measured, e.g. aortic diameter, right and left common iliac diameter,\nminimum diameter of distal neck. The selected stent is then simulated to the\nCT-Data - starting with the initial stent. It hereby becomes apparent if the\ndimensions of the bifurcated stent graft are exact, i.e. the fitting to the\narteries was done properly and no ostium was covered."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2932710", 
    "link": "http://arxiv.org/pdf/1607.00623v1", 
    "title": "Visualizing Natural Language Descriptions: A Survey", 
    "arxiv-id": "1607.00623v1", 
    "author": "Won-Sook Lee", 
    "publish": "2016-07-03T10:30:40Z", 
    "summary": "A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field."
},{
    "category": "cs.CY", 
    "doi": "10.1109/TVCG.2016.2598585", 
    "link": "http://arxiv.org/pdf/1608.06949v1", 
    "title": "Urban Pulse: Capturing the Rhythm of Cities", 
    "arxiv-id": "1608.06949v1", 
    "author": "Cl\u00e1udio T. Silva", 
    "publish": "2016-08-24T20:02:46Z", 
    "summary": "Cities are inherently dynamic. Interesting patterns of behavior typically\nmanifest at several key areas of a city over multiple temporal resolutions.\nStudying these patterns can greatly help a variety of experts ranging from city\nplanners and architects to human behavioral experts. Recent technological\ninnovations have enabled the collection of enormous amounts of data that can\nhelp in these studies. However, techniques using these data sets typically\nfocus on understanding the data in the context of the city, thus failing to\ncapture the dynamic aspects of the city. The goal of this work is to instead\nunderstand the city in the context of multiple urban data sets. To do so, we\ndefine the concept of an \"urban pulse\" which captures the spatio-temporal\nactivity in a city across multiple temporal resolutions. The prominent pulses\nin a city are obtained using the topology of the data sets, and are\ncharacterized as a set of beats. The beats are then used to analyze and compare\ndifferent pulses. We also design a visual exploration framework that allows\nusers to explore the pulses within and across multiple cities under different\nconditions. Finally, we present three case studies carried out by experts from\ntwo different domains that demonstrate the utility of our framework."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TVCG.2016.2598585", 
    "link": "http://arxiv.org/pdf/1611.07233v1", 
    "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression   Artifact Suppression", 
    "arxiv-id": "1611.07233v1", 
    "author": "Luca Benini", 
    "publish": "2016-11-22T10:11:58Z", 
    "summary": "Lossy image compression algorithms are pervasively used to reduce the size of\nimages transmitted over the web and recorded on data storage media. However, we\npay for their high compression rate with visual artifacts degrading the user\nexperience. Deep convolutional neural networks have become a widespread tool to\naddress high-level computer vision tasks very successfully. Recently, they have\nfound their way into the areas of low-level computer vision and image\nprocessing to solve regression problems mostly with relatively shallow\nnetworks.\n  We present a novel 12-layer deep convolutional network for image compression\nartifact suppression with hierarchical skip connections and a multi-scale loss\nfunction. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an\nimprovement of up to 0.36 dB over the best previous ConvNet result. We show\nthat a network trained for a specific quality factor (QF) is resilient to the\nQF used to compress the input image - a single network trained for QF 60\nprovides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76."
},{
    "category": "math.CA", 
    "doi": "10.1109/TVCG.2016.2598585", 
    "link": "http://arxiv.org/pdf/math/0608789v7", 
    "title": "One method for proving inequalities by computer", 
    "arxiv-id": "math/0608789v7", 
    "author": "Branko J. Malesevic", 
    "publish": "2006-08-31T14:59:20Z", 
    "summary": "In this article we consider a method for proving a class of analytical\ninequalities via minimax rational approximations. All numerical calculations in\nthis paper are given by Maple computer program."
},{
    "category": "cs.GR", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1105.5678v1", 
    "title": "Time-Dependent 2-D Vector Field Topology: An Approach Inspired by   Lagrangian Coherent Structures", 
    "arxiv-id": "1105.5678v1", 
    "author": "Daniel Weiskopf", 
    "publish": "2011-05-28T02:28:19Z", 
    "summary": "This paper presents an approach to a time-dependent variant of the concept of\nvector field topology for 2-D vector fields. Vector field topology is defined\nfor steady vector fields and aims at discriminating the domain of a vector\nfield into regions of qualitatively different behaviour. The presented approach\nrepresents a generalization for saddle-type critical points and their\nseparatrices to unsteady vector fields based on generalized streak lines, with\nthe classical vector field topology as its special case for steady vector\nfields. The concept is closely related to that of Lagrangian coherent\nstructures obtained as ridges in the finite-time Lyapunov exponent field. The\nproposed approach is evaluated on both 2-D time-dependent synthetic and vector\nfields from computational fluid dynamics."
},{
    "category": "cs.IT", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1308.4338v1", 
    "title": "SAR Image Despeckling Algorithms using Stochastic Distances and Nonlocal   Means", 
    "arxiv-id": "1308.4338v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2013-08-20T15:58:19Z", 
    "summary": "This paper presents two approaches for filter design based on stochastic\ndistances for intensity speckle reduction. A window is defined around each\npixel, overlapping samples are compared and only those which pass a\ngoodness-of-fit test are used to compute the filtered value. The tests stem\nfrom stochastic divergences within the Information Theory framework. The\ntechnique is applied to intensity Synthetic Aperture Radar (SAR) data with\nhomogeneous regions using the Gamma model. The first approach uses a\nNagao-Matsuyama-type procedure for setting the overlapping samples, and the\nsecond uses the nonlocal method. The proposals are compared with the Improved\nSigma filter and with anisotropic diffusion for speckled data (SRAD) using a\nprotocol based on Monte Carlo simulation. Among the criteria used to quantify\nthe quality of filters, we employ the equivalent number of looks, and line and\nedge preservation. Moreover, we also assessed the filters by the Universal\nImage Quality Index and by the Pearson correlation between edges. Applications\nto real images are also discussed. The proposed methods show good results."
},{
    "category": "cs.IT", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1308.6487v1", 
    "title": "A New Algorithm of Speckle Filtering using Stochastic Distances", 
    "arxiv-id": "1308.6487v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2013-08-29T14:56:01Z", 
    "summary": "This paper presents a new approach for filter design based on stochastic\ndistances and tests between distributions. A window is defined around each\npixel, overlapping samples are compared and only those which pass a\ngoodness-of-fit test are used to compute the filtered value. The technique is\napplied to intensity SAR data with homogeneous regions using the Gamma model.\nThe proposal is compared with the Lee's filter using a protocol based on Monte\nCarlo. Among the criteria used to quantify the quality of filters, we employ\nthe equivalent number of looks, line and edge preservation. Moreover, we also\nassessed the filters by the Universal Image Quality Index and the Pearson's\ncorrelation on edges regions."
},{
    "category": "cs.IT", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1207.0704v1", 
    "title": "Speckle Reduction using Stochastic Distances", 
    "arxiv-id": "1207.0704v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-03T14:57:44Z", 
    "summary": "This paper presents a new approach for filter design based on stochastic\ndistances and tests between distributions. A window is defined around each\npixel, samples are compared and only those which pass a goodness-of-fit test\nare used to compute the filtered value. The technique is applied to intensity\nSynthetic Aperture Radar (SAR) data, using the Gamma model with varying number\nof looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyama\nwindows are used to define the samples. The proposal is compared with the Lee's\nfilter which is considered a standard, using a protocol based on simulation.\nAmong the criteria used to quantify the quality of filters, we employ the\nequivalent number of looks (related to the signal-to-noise ratio), line\ncontrast, and edge preservation. Moreover, we also assessed the filters by the\nUniversal Image Quality Index and the Pearson's correlation between edges."
},{
    "category": "cs.IT", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1207.0771v1", 
    "title": "Polarimetric SAR Image Smoothing with Stochastic Distances", 
    "arxiv-id": "1207.0771v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-03T18:11:46Z", 
    "summary": "Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as an\nimportant source of information in remote sensing applications. The most\ncomplete format this type of imaging produces consists of complex-valued\nHermitian matrices in every image coordinate and, as such, their visualization\nis challenging. They also suffer from speckle noise which reduces the\nsignal-to-noise ratio. Smoothing techniques have been proposed in the\nliterature aiming at preserving different features and, analogously,\nprojections from the cone of Hermitian positive matrices to different color\nrepresentation spaces are used for enhancing certain characteristics. In this\nwork we propose the use of stochastic distances between models that describe\nthis type of data in a Nagao-Matsuyama-type of smoothing technique. The\nresulting images are shown to present good visualization properties (noise\nreduction with preservation of fine details) in all the considered\nvisualization spaces."
},{
    "category": "cs.IT", 
    "doi": "10.1111/j.1467-8659.2009.01546.x", 
    "link": "http://arxiv.org/pdf/1304.4634v1", 
    "title": "Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances   and Nonlocal Means", 
    "arxiv-id": "1304.4634v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2013-04-16T22:11:53Z", 
    "summary": "This paper presents a technique for reducing speckle in Polarimetric\nSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a\nstatistical test based on stochastic divergences. The main objective is to\nselect homogeneous pixels in the filtering area through statistical tests\nbetween distributions. This proposal uses the complex Wishart model to describe\nPolSAR data, but the technique can be extended to other models. The weights of\nthe location-variant linear filter are function of the p-values of tests which\nverify the hypothesis that two samples come from the same distribution and,\ntherefore, can be used to compute a local mean. The test stems from the family\nof (h-phi) divergences which originated in Information Theory. This novel\ntechnique was compared with the Boxcar, Refined Lee and IDAN filters. Image\nquality assessment methods on simulated and real data are employed to validate\nthe performance of this approach. We show that the proposed filter also\nenhances the polarimetric entropy and preserves the scattering information of\nthe targets."
},{
    "category": "quant-ph", 
    "doi": "10.1103/PhysRevA.73.062303", 
    "link": "http://arxiv.org/pdf/quant-ph/0602063v2", 
    "title": "Topological Quantum Error Correction with Optimal Encoding Rate", 
    "arxiv-id": "quant-ph/0602063v2", 
    "author": "M. A. Martin-Delgado", 
    "publish": "2006-02-06T10:43:50Z", 
    "summary": "We prove the existence of topological quantum error correcting codes with\nencoding rates $k/n$ asymptotically approaching the maximum possible value.\nExplicit constructions of these topological codes are presented using surfaces\nof arbitrary genus. We find a class of regular toric codes that are optimal.\nFor physical implementations, we present planar topological codes."
},{
    "category": "gr-qc", 
    "doi": "10.1088/0264-9381/27/3/032001", 
    "link": "http://arxiv.org/pdf/0908.3889v2", 
    "title": "Integrating Post-Newtonian Equations on Graphics Processing Units", 
    "arxiv-id": "0908.3889v2", 
    "author": "Manuel Tiglio", 
    "publish": "2009-08-26T20:00:38Z", 
    "summary": "We report on early results of a numerical and statistical study of binary\nblack hole inspirals. The two black holes are evolved using post-Newtonian\napproximations starting with initially randomly distributed spin vectors. We\ncharacterize certain aspects of the distribution shortly before merger. In\nparticular we note the uniform distribution of black hole spin vector dot\nproducts shortly before merger and a high correlation between the initial and\nfinal black hole spin vector dot products in the equal-mass, maximally spinning\ncase. These simulations were performed on Graphics Processing Units, and we\ndemonstrate a speed-up of a factor 50 over a more conventional CPU\nimplementation."
}]