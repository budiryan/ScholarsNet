[{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809020v1", 
    "title": "Linear Segmentation and Segment Significance", 
    "arxiv-id": "cs/9809020v1", 
    "author": "Kathleen R. McKeown", 
    "publish": "1998-09-15T23:49:32Z", 
    "summary": "We present a new method for discovering a segmental discourse structure of a\ndocument while categorizing segment function. We demonstrate how retrieval of\nnoun phrases and pronominal forms, along with a zero-sum weighting scheme,\ndetermines topicalized segmentation. Futhermore, we use term distribution to\naid in identifying the role that the segment performs in the document. Finally,\nwe present results of evaluation in terms of precision and recall which surpass\nearlier approaches."
},{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809022v1", 
    "title": "Modelling Users, Intentions, and Structure in Spoken Dialog", 
    "arxiv-id": "cs/9809022v1", 
    "author": "Heinrich Niemann", 
    "publish": "1998-09-17T11:10:14Z", 
    "summary": "We outline how utterances in dialogs can be interpreted using a partial first\norder logic. We exploit the capability of this logic to talk about the truth\nstatus of formulae to define a notion of coherence between utterances and\nexplain how this coherence relation can serve for the construction of AND/OR\ntrees that represent the segmentation of the dialog. In a BDI model we\nformalize basic assumptions about dialog and cooperative behaviour of\nparticipants. These assumptions provide a basis for inferring speech acts from\ncoherence relations between utterances and attitudes of dialog participants.\nSpeech acts prove to be useful for determining dialog segments defined on the\nnotion of completing expectations of dialog participants. Finally, we sketch\nhow explicit segmentation signalled by cue phrases and performatives is covered\nby our dialog model."
},{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809024v2", 
    "title": "A Lexicalized Tree Adjoining Grammar for English", 
    "arxiv-id": "cs/9809024v2", 
    "author": "XTAG Research Group", 
    "publish": "1998-09-18T00:33:47Z", 
    "summary": "This document describes a sizable grammar of English written in the TAG\nformalism and implemented for use with the XTAG system. This report and the\ngrammar described herein supersedes the TAG grammar described in an earlier\n1995 XTAG technical report. The English grammar described in this report is\nbased on the TAG formalism which has been extended to include lexicalization,\nand unification-based feature structures. The range of syntactic phenomena that\ncan be handled is large and includes auxiliaries (including inversion), copula,\nraising and small clause constructions, topicalization, relative clauses,\ninfinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO\nconstructions, noun-noun modifications, extraposition, determiner sequences,\ngenitives, negation, noun-verb contractions, sentential adjuncts and\nimperatives. This technical report corresponds to the XTAG Release 8/31/98. The\nXTAG grammar is continuously updated with the addition of new analyses and\nmodification of old ones, and an online version of this report can be found at\nthe XTAG web page at http://www.cis.upenn.edu/~xtag/"
},{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809026v1", 
    "title": "Prefix Probabilities from Stochastic Tree Adjoining Grammars", 
    "arxiv-id": "cs/9809026v1", 
    "author": "Giorgio Satta", 
    "publish": "1998-09-18T03:45:45Z", 
    "summary": "Language models for speech recognition typically use a probability model of\nthe form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other\nhand, are typically used to assign structure to utterances. A language model of\nthe above form is constructed from such grammars by computing the prefix\nprobability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all\npossible terminations of the prefix a_1 ... a_n. The main result in this paper\nis an algorithm to compute such prefix probabilities given a stochastic Tree\nAdjoining Grammar (TAG). The algorithm achieves the required computation in\nO(n^6) time. The probability of subderivations that do not derive any words in\nthe prefix, but contribute structurally to its derivation, are precomputed to\nachieve termination. This algorithm enables existing corpus-based estimation\ntechniques for stochastic TAGs to be used for language modelling."
},{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809027v1", 
    "title": "Conditions on Consistency of Probabilistic Tree Adjoining Grammars", 
    "arxiv-id": "cs/9809027v1", 
    "author": "Anoop Sarkar", 
    "publish": "1998-09-18T03:58:57Z", 
    "summary": "Much of the power of probabilistic methods in modelling language comes from\ntheir ability to compare several derivations for the same string in the\nlanguage. An important starting point for the study of such cross-derivational\nproperties is the notion of _consistency_. The probability model defined by a\nprobabilistic grammar is said to be _consistent_ if the probabilities assigned\nto all the strings in the language sum to one. From the literature on\nprobabilistic context-free grammars (CFGs), we know precisely the conditions\nwhich ensure that consistency is true for a given CFG. This paper derives the\nconditions under which a given probabilistic Tree Adjoining Grammar (TAG) can\nbe shown to be consistent. It gives a simple algorithm for checking consistency\nand gives the formal justification for its correctness. The conditions derived\nhere can be used to ensure that probability models that use TAGs can be checked\nfor _deficiency_ (i.e. whether any probability mass is assigned to strings that\ncannot be generated)."
},{
    "category": "cs.CL", 
    "doi": "10.1145/1569901.1570023", 
    "link": "http://arxiv.org/pdf/cs/9809028v1", 
    "title": "Separating Dependency from Constituency in a Tree Rewriting System", 
    "arxiv-id": "cs/9809028v1", 
    "author": "Anoop Sarkar", 
    "publish": "1998-09-18T04:44:02Z", 
    "summary": "In this paper we present a new tree-rewriting formalism called Link-Sharing\nTree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using\nLSTAG we define an approach towards coordination where linguistic dependency is\ndistinguished from the notion of constituency. Such an approach towards\ncoordination that explicitly distinguishes dependencies from constituency gives\na better formal understanding of its representation when compared to previous\napproaches that use tree-rewriting systems which conflate the two issues."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809029v1", 
    "title": "Incremental Parser Generation for Tree Adjoining Grammars", 
    "arxiv-id": "cs/9809029v1", 
    "author": "Anoop Sarkar", 
    "publish": "1998-09-18T05:03:48Z", 
    "summary": "This paper describes the incremental generation of parse tables for the\nLR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented\nhandles modifications to the input grammar by updating the parser generated so\nfar. In this paper, a lazy generation of LR-type parsers for TALs is defined in\nwhich parse tables are created by need while parsing. We then describe an\nincremental parser generator for TALs which responds to modification of the\ninput grammar by updating parse tables built so far."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809050v1", 
    "title": "A Freely Available Morphological Analyzer, Disambiguator and Context   Sensitive Lemmatizer for German", 
    "arxiv-id": "cs/9809050v1", 
    "author": "Manfred Wettler", 
    "publish": "1998-09-23T12:59:39Z", 
    "summary": "In this paper we present Morphy, an integrated tool for German morphology,\npart-of-speech tagging and context-sensitive lemmatization. Its large lexicon\nof more than 320,000 word forms plus its ability to process German compound\nnouns guarantee a wide morphological coverage. Syntactic ambiguities can be\nresolved with a standard statistical part-of-speech tagger. By using the output\nof the tagger, the lemmatizer can determine the correct root even for ambiguous\nword forms. The complete package is freely available and can be downloaded from\nthe World Wide Web."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809106v1", 
    "title": "Processing Unknown Words in HPSG", 
    "arxiv-id": "cs/9809106v1", 
    "author": "Markus Walther", 
    "publish": "1998-09-25T11:02:08Z", 
    "summary": "The lexical acquisition system presented in this paper incrementally updates\nlinguistic properties of unknown words inferred from their surrounding context\nby parsing sentences with an HPSG grammar for German. We employ a gradual,\ninformation-based concept of ``unknownness'' providing a uniform treatment for\nthe range of completely known to maximally unknown lexical entries. ``Unknown''\ninformation is viewed as revisable information, which is either generalizable\nor specializable. Updating takes place after parsing, which only requires a\nmodified lexical lookup. Revisable pieces of information are identified by\ngrammar-specified declarations which provide access paths into the parse\nfeature structure. The updating mechanism revises the corresponding places in\nthe lexical feature structures iff the context actually provides new\ninformation. For revising generalizable information, type union is required. A\nworked-out example demonstrates the inferential capacity of our implemented\nsystem."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809107v1", 
    "title": "Computing Declarative Prosodic Morphology", 
    "arxiv-id": "cs/9809107v1", 
    "author": "Markus Walther", 
    "publish": "1998-09-25T14:32:38Z", 
    "summary": "This paper describes a computational, declarative approach to prosodic\nmorphology that uses inviolable constraints to denote small finite candidate\nsets which are filtered by a restrictive incremental optimization mechanism.\nThe new approach is illustrated with an implemented fragment of Modern Hebrew\nverbs couched in MicroCUF, an expressive constraint logic formalism. For\ngeneration and parsing of word forms, I propose a novel off-line technique to\neliminate run-time optimization. It produces a finite-state oracle that\nefficiently restricts the constraint interpreter's search space. As a\nbyproduct, unknown words can be analyzed without special mechanisms. Unlike\npure finite-state transducer approaches, this hybrid setup allows for more\nexpressivity in constraints to specify e.g. token identity for reduplication or\narithmetic constraints for phonetics."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809112v1", 
    "title": "On the Evaluation and Comparison of Taggers: The Effect of Noise in   Testing Corpora", 
    "arxiv-id": "cs/9809112v1", 
    "author": "L. Marquez", 
    "publish": "1998-09-28T07:49:11Z", 
    "summary": "This paper addresses the issue of {\\sc pos} tagger evaluation. Such\nevaluation is usually performed by comparing the tagger output with a reference\ntest corpus, which is assumed to be error-free. Currently used corpora contain\nnoise which causes the obtained performance to be a distortion of the real\nvalue. We analyze to what extent this distortion may invalidate the comparison\nbetween taggers or the measure of the improvement given by a new system. The\nmain conclusion is that a more rigorous testing experimentation\nsetting/designing is needed to reliably evaluate and compare tagger accuracies."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9809113v1", 
    "title": "Improving Tagging Performance by Using Voting Taggers", 
    "arxiv-id": "cs/9809113v1", 
    "author": "H. Rodriguez", 
    "publish": "1998-09-28T07:50:55Z", 
    "summary": "We present a bootstrapping method to develop an annotated corpus, which is\nspecially useful for languages with few available resources. The method is\nbeing applied to develop a corpus of Spanish of over 5Mw. The method consists\non taking advantage of the collaboration of two different POS taggers. The\ncases in which both taggers agree present a higher accuracy and are used to\nretrain the taggers."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9810014v1", 
    "title": "Resources for Evaluation of Summarization Techniques", 
    "arxiv-id": "cs/9810014v1", 
    "author": "Susan Lee", 
    "publish": "1998-10-13T20:33:05Z", 
    "summary": "We report on two corpora to be used in the evaluation of component systems\nfor the tasks of (1) linear segmentation of text and (2) summary-directed\nsentence extraction. We present characteristics of the corpora, methods used in\nthe collection of user judgments, and an overview of the application of the\ncorpora to evaluating the component system. Finally, we discuss the problems\nand issues with construction of the test set which apply broadly to the\nconstruction of evaluation resources for language technologies."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9810015v1", 
    "title": "Restrictions on Tree Adjoining Languages", 
    "arxiv-id": "cs/9810015v1", 
    "author": "William Schuler", 
    "publish": "1998-10-13T21:17:13Z", 
    "summary": "Several methods are known for parsing languages generated by Tree Adjoining\nGrammars (TAGs) in O(n^6) worst case running time. In this paper we investigate\nwhich restrictions on TAGs and TAG derivations are needed in order to lower\nthis O(n^6) time complexity, without introducing large runtime constants, and\nwithout losing any of the generative power needed to capture the syntactic\nconstructions in natural language that can be handled by unrestricted TAGs. In\nparticular, we describe an algorithm for parsing a strict subclass of TAG in\nO(n^5), and attempt to show that this subclass retains enough generative power\nto make it useful in the general case."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9811008v1", 
    "title": "Translating near-synonyms: Possibilities and preferences in the   interlingua", 
    "arxiv-id": "cs/9811008v1", 
    "author": "Philip Edmonds", 
    "publish": "1998-11-02T21:29:41Z", 
    "summary": "This paper argues that an interlingual representation must explicitly\nrepresent some parts of the meaning of a situation as possibilities (or\npreferences), not as necessary or definite components of meaning (or\nconstraints). Possibilities enable the analysis and generation of nuance,\nsomething required for faithful translation. Furthermore, the representation of\nthe meaning of words, especially of near-synonyms, is crucial, because it\nspecifies which nuances words can convey in which contexts."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9811009v1", 
    "title": "Choosing the Word Most Typical in Context Using a Lexical Co-occurrence   Network", 
    "arxiv-id": "cs/9811009v1", 
    "author": "Philip Edmonds", 
    "publish": "1998-11-02T23:06:19Z", 
    "summary": "This paper presents a partial solution to a component of the problem of\nlexical choice: choosing the synonym most typical, or expected, in context. We\napply a new statistical approach to representing the context of a word through\nlexical co-occurrence networks. The implementation was trained and evaluated on\na large corpus, and results show that the inclusion of second-order\nco-occurrence relations improves the performance of our implemented lexical\nchoice program."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9811016v1", 
    "title": "Comparing a statistical and a rule-based tagger for German", 
    "arxiv-id": "cs/9811016v1", 
    "author": "Gerold Schneider", 
    "publish": "1998-11-11T11:06:34Z", 
    "summary": "In this paper we present the results of comparing a statistical tagger for\nGerman based on decision trees and a rule-based Brill-Tagger for German. We\nused the same training corpus (and therefore the same tag-set) to train both\ntaggers. We then applied the taggers to the same test corpus and compared their\nrespective behavior and in particular their error rates. Both taggers perform\nsimilarly with an error rate of around 5%. From the detailed error analysis it\ncan be seen that the rule-based tagger has more problems with unknown words\nthan the statistical tagger. But the results are opposite for tokens that are\nmany-ways ambiguous. If the unknown words are fed into the taggers with the\nhelp of an external lexicon (such as the Gertwol system) the error rate of the\nrule-based tagger drops to 4.7%, and the respective rate of the statistical\ntaggers drops to around 3.7%. Combining the taggers by using the output of one\ntagger to help the other did not lead to any further improvement."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9811022v2", 
    "title": "Expoiting Syntactic Structure for Language Modeling", 
    "arxiv-id": "cs/9811022v2", 
    "author": "Frederick Jelinek", 
    "publish": "1998-11-12T17:31:17Z", 
    "summary": "The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words--binary-parse-structure with headword annotation and\noperates in a left-to-right manner --- therefore usable for automatic speech\nrecognition. The model, its probabilistic parameterization, and a set of\nexperiments meant to evaluate its predictive power are presented; an\nimprovement over standard trigram modeling is achieved."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9811025v2", 
    "title": "A Structured Language Model", 
    "arxiv-id": "cs/9811025v2", 
    "author": "Ciprian Chelba", 
    "publish": "1998-11-13T16:53:15Z", 
    "summary": "The paper presents a language model that develops syntactic structure and\nuses it to extract meaningful information from the word history, thus enabling\nthe use of long distance dependencies. The model assigns probability to every\njoint sequence of words - binary-parse-structure with headword annotation. The\nmodel, its probabilistic parametrization, and a set of experiments meant to\nevaluate its predictive power are presented."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9812001v3", 
    "title": "A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S   tructural Disambiguation", 
    "arxiv-id": "cs/9812001v3", 
    "author": "Hang LI", 
    "publish": "1998-12-01T11:43:32Z", 
    "summary": "In this thesis, I address the problem of automatically acquiring lexical\nsemantic knowledge, especially that of case frame patterns, from large corpus\ndata and using the acquired knowledge in structural disambiguation. The\napproach I adopt has the following characteristics: (1) dividing the problem\ninto three subproblems: case slot generalization, case dependency learning, and\nword clustering (thesaurus construction). (2) viewing each subproblem as that\nof statistical estimation and defining probability models for each subproblem,\n(3) adopting the Minimum Description Length (MDL) principle as learning\nstrategy, (4) employing efficient learning algorithms, and (5) viewing the\ndisambiguation problem as that of statistical prediction. Major contributions\nof this thesis include: (1) formalization of the lexical knowledge acquisition\nproblem, (2) development of a number of learning methods for lexical knowledge\nacquisition, and (3) development of a high-performance disambiguation method."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9812005v1", 
    "title": "Optimal Multi-Paragraph Text Segmentation by Dynamic Programming", 
    "arxiv-id": "cs/9812005v1", 
    "author": "Oskari Heinonen", 
    "publish": "1998-12-04T16:16:35Z", 
    "summary": "There exist several methods of calculating a similarity curve, or a sequence\nof similarity values, representing the lexical cohesion of successive text\nconstituents, e.g., paragraphs. Methods for deciding the locations of fragment\nboundaries are, however, scarce. We propose a fragmentation method based on\ndynamic programming. The method is theoretically sound and guaranteed to\nprovide an optimal splitting on the basis of a similarity curve, a preferred\nfragment length, and a cost function defined. The method is especially useful\nwhen control on fragment size is of importance."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9812018v1", 
    "title": "A Flexible Shallow Approach to Text Generation", 
    "arxiv-id": "cs/9812018v1", 
    "author": "Helmut Horacek", 
    "publish": "1998-12-16T16:37:01Z", 
    "summary": "In order to support the efficient development of NL generation systems, two\northogonal methods are currently pursued with emphasis: (1) reusable, general,\nand linguistically motivated surface realization components, and (2) simple,\ntask-oriented template-based techniques. In this paper we argue that, from an\napplication-oriented perspective, the benefits of both are still limited. In\norder to improve this situation, we suggest and evaluate shallow generation\nmethods associated with increased flexibility. We advise a close connection\nbetween domain-motivated and linguistic ontologies that supports the quick\nadaptation to new tasks and domains, rather than the reuse of general\nresources. Our method is especially designed for generating reports with\nlimited linguistic variations."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9901005v1", 
    "title": "An Empirical Approach to Temporal Reference Resolution (journal version)", 
    "arxiv-id": "cs/9901005v1", 
    "author": "Kenneth K. McKeever", 
    "publish": "1999-01-13T17:37:00Z", 
    "summary": "Scheduling dialogs, during which people negotiate the times of appointments,\nare common in everyday life. This paper reports the results of an in-depth\nempirical investigation of resolving explicit temporal references in scheduling\ndialogs. There are four phases of this work: data annotation and evaluation,\nmodel development, system implementation and evaluation, and model evaluation\nand analysis. The system and model were developed primarily on one set of data,\nand then applied later to a much more complex data set, to assess the\ngeneralizability of the model for the task being performed. Many different\ntypes of empirical methods are applied to pinpoint the strengths and weaknesses\nof the approach. Detailed annotation instructions were developed and an\nintercoder reliability study was performed, showing that naive annotators can\nreliably perform the targeted annotations. A fully automatic system has been\ndeveloped and evaluated on unseen test data, with good results on both data\nsets. We adopt a pure realization of a recency-based focus model to identify\nprecisely when it is and is not adequate for the task being addressed. In\naddition to system results, an in-depth evaluation of the model itself is\npresented, based on detailed manual annotations. The results are that few\nerrors occur specifically due to the model of focus being used, and the set of\nanaphoric relations defined in the model are low in ambiguity for both data\nsets."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9902001v1", 
    "title": "Compacting the Penn Treebank Grammar", 
    "arxiv-id": "cs/9902001v1", 
    "author": "Yorick Wilks", 
    "publish": "1999-01-31T18:57:45Z", 
    "summary": "Treebanks, such as the Penn Treebank (PTB), offer a simple approach to\nobtaining a broad coverage grammar: one can simply read the grammar off the\nparse trees in the treebank. While such a grammar is easy to obtain, a\nsquare-root rate of growth of the rule set with corpus size suggests that the\nderived grammar is far from complete and that much more treebanked text would\nbe required to obtain a complete grammar, if one exists at some limit. However,\nwe offer an alternative explanation in terms of the underspecification of\nstructures within the treebank. This hypothesis is explored by applying an\nalgorithm to compact the derived grammar by eliminating redundant rules --\nrules whose right hand sides can be parsed by other rules. The size of the\nresulting compacted grammar, which is significantly less than that of the full\ntreebank grammar, is shown to approach a limit. However, such a compacted\ngrammar does not yield very good performance figures. A version of the\ncompaction algorithm taking rule probabilities into account is proposed, which\nis argued to be more linguistically motivated. Combined with simple\nthresholding, this method can be used to give a 58% reduction in grammar size\nwithout significant change in parsing performance, and can produce a 69%\nreduction with some gain in recall, but a loss in precision."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9902029v1", 
    "title": "The \"Fodor\"-FODOR fallacy bites back", 
    "arxiv-id": "cs/9902029v1", 
    "author": "Yorick Wilks", 
    "publish": "1999-02-25T14:41:24Z", 
    "summary": "The paper argues that Fodor and Lepore are misguided in their attack on\nPustejovsky's Generative Lexicon, largely because their argument rests on a\ntraditional, but implausible and discredited, view of the lexicon on which it\nis effectively empty of content, a view that stands in the long line of\nexplaining word meaning (a) by ostension and then (b) explaining it by means of\na vacuous symbol in a lexicon, often the word itself after typographic\ntransmogrification. (a) and (b) both share the wrong belief that to a word must\ncorrespond a simple entity that is its meaning. I then turn to the semantic\nrules that Pustejovsky uses and argue first that, although they have novel\nfeatures, they are in a well-established Artificial Intelligence tradition of\nexplaining meaning by reference to structures that mention other structures\nassigned to words that may occur in close proximity to the first. It is argued\nthat Fodor and Lepore's view that there cannot be such rules is without\nfoundation, and indeed systems using such rules have proved their practical\nworth in computational systems. Their justification descends from line of\nargument, whose high points were probably Wittgenstein and Quine that meaning\nis not to be understood by simple links to the world, ostensive or otherwise,\nbut by the relationship of whole cultural representational structures to each\nother and to the world as a whole."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9902030v1", 
    "title": "Is Word Sense Disambiguation just one more NLP task?", 
    "arxiv-id": "cs/9902030v1", 
    "author": "Yorick Wilks", 
    "publish": "1999-02-25T14:41:32Z", 
    "summary": "This paper compares the tasks of part-of-speech (POS) tagging and\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\nrelated by fineness of grain or anything like that, but are quite different\nkinds of task, particularly becuase there is nothing in POS corresponding to\nsense novelty. The paper also argues for the reintegration of sub-tasks that\nare being separated for evaluation"
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9903003v1", 
    "title": "A Formal Framework for Linguistic Annotation", 
    "arxiv-id": "cs/9903003v1", 
    "author": "Mark Liberman", 
    "publish": "1999-03-02T12:30:55Z", 
    "summary": "`Linguistic annotation' covers any descriptive or analytic notations applied\nto raw language data. The basic data may be in the form of time functions --\naudio, video and/or physiological recordings -- or it may be textual. The added\nnotations may include transcriptions of all sorts (from phonetic features to\ndiscourse structures), part-of-speech and sense tagging, syntactic analysis,\n`named entity' identification, co-reference annotation, and so on. While there\nare several ongoing efforts to provide formats and tools for such annotations\nand to publish annotated linguistic databases, the lack of widely accepted\nstandards is becoming a critical problem. Proposed standards, to the extent\nthey exist, have focussed on file formats. This paper focuses instead on the\nlogical structure of linguistic annotations. We survey a wide variety of\nexisting annotation formats and demonstrate a common conceptual core, the\nannotation graph. This provides a formal framework for constructing,\nmaintaining and searching linguistic annotations, while remaining consistent\nwith many alternative data structures and file formats."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9903008v1", 
    "title": "Empirically Evaluating an Adaptable Spoken Dialogue System", 
    "arxiv-id": "cs/9903008v1", 
    "author": "Shimei Pan", 
    "publish": "1999-03-05T22:03:13Z", 
    "summary": "Recent technological advances have made it possible to build real-time,\ninteractive spoken dialogue systems for a wide variety of applications.\nHowever, when users do not respect the limitations of such systems, performance\ntypically degrades. Although users differ with respect to their knowledge of\nsystem limitations, and although different dialogue strategies make system\nlimitations more apparent to users, most current systems do not try to improve\nperformance by adapting dialogue behavior to individual users. This paper\npresents an empirical evaluation of TOOT, an adaptable spoken dialogue system\nfor retrieving train schedules on the web. We conduct an experiment in which 20\nusers carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,\nresulting in a corpus of 80 dialogues. The values for a wide range of\nevaluation measures are then extracted from this corpus. Our results show that\nadaptable TOOT generally outperforms non-adaptable TOOT, and that the utility\nof adaptation depends on TOOT's initial dialogue strategies."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9904008v1", 
    "title": "Transducers from Rewrite Rules with Backreferences", 
    "arxiv-id": "cs/9904008v1", 
    "author": "Gertjan van Noord", 
    "publish": "1999-04-15T14:00:41Z", 
    "summary": "Context sensitive rewrite rules have been widely used in several areas of\nnatural language processing, including syntax, morphology, phonology and speech\nprocessing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various\nalgorithms to compile such rewrite rules into finite-state transducers. The\npresent paper extends this work by allowing a limited form of backreferencing\nin such rules. The explicit use of backreferencing leads to more elegant and\ngeneral solutions."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9904009v1", 
    "title": "An ascription-based approach to speech acts", 
    "arxiv-id": "cs/9904009v1", 
    "author": "Yorick Wilks", 
    "publish": "1999-04-15T16:03:27Z", 
    "summary": "The two principal areas of natural language processing research in pragmatics\nare belief modelling and speech act processing. Belief modelling is the\ndevelopment of techniques to represent the mental attitudes of a dialogue\nparticipant. The latter approach, speech act processing, based on speech act\ntheory, involves viewing dialogue in planning terms. Utterances in a dialogue\nare modelled as steps in a plan where understanding an utterance involves\nderiving the complete plan a speaker is attempting to achieve. However,\nprevious speech act based approaches have been limited by a reliance upon\nrelatively simplistic belief modelling techniques and their relationship to\nplanning and plan recognition. In particular, such techniques assume\nprecomputed nested belief structures. In this paper, we will present an\napproach to speech act processing based on novel belief modelling techniques\nwhere nested beliefs are propagated on demand."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9904018v1", 
    "title": "A Computational Memory and Processing Model for Processing", 
    "arxiv-id": "cs/9904018v1", 
    "author": "Janet E. Cahn", 
    "publish": "1999-04-24T23:45:26Z", 
    "summary": "This paper links prosody to the information in a text and how it is processed\nby the speaker. It describes the operation and output of LOQ, a text-to-speech\nimplementation that includes a model of limited attention and working memory.\nAttentional limitations are key. Varying the attentional parameter in the\nsimulations varies in turn what counts as given and new in a text, and\ntherefore, the intonational contours with which it is uttered. Currently, the\nsystem produces prosody in three different styles: child-like, adult\nexpressive, and knowledgeable. This prosody also exhibits differences within\neach style -- no two simulations are alike. The limited resource approach\ncaptures some of the stylistic and individual variety found in natural prosody."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9905001v1", 
    "title": "Supervised Grammar Induction Using Training Data with Limited   Constituent Information", 
    "arxiv-id": "cs/9905001v1", 
    "author": "Rebecca Hwa", 
    "publish": "1999-05-02T20:48:21Z", 
    "summary": "Corpus-based grammar induction generally relies on hand-parsed training data\nto learn the structure of the language. Unfortunately, the cost of building\nlarge annotated corpora is prohibitively expensive. This work aims to improve\nthe induction strategy when there are few labels in the training data. We show\nthat the most informative linguistic constituents are the higher nodes in the\nparse trees, typically denoting complex noun phrases and sentential clauses.\nThey account for only 20% of all constituents. For inducing grammars from\nsparsely labeled training data (e.g., only higher-level constituent labels), we\npropose an adaptation strategy, which produces grammars that parse almost as\nwell as grammars induced from fully labeled corpora. Our results suggest that\nfor a partial parser to replace human annotators, it must be able to\nautomatically extract higher-level constituents rather than base noun phrases."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906003v1", 
    "title": "The syntactic processing of particles in Japanese spoken language", 
    "arxiv-id": "cs/9906003v1", 
    "author": "Melanie Siegel", 
    "publish": "1999-06-02T12:03:14Z", 
    "summary": "Particles fullfill several distinct central roles in the Japanese language.\nThey can mark arguments as well as adjuncts, can be functional or have semantic\nfuntions. There is, however, no straightforward matching from particles to\nfunctions, as, e.g., GA can mark the subject, the object or an adjunct of a\nsentence. Particles can cooccur. Verbal arguments that could be identified by\nparticles can be eliminated in the Japanese sentence. And finally, in spoken\nlanguage particles are often omitted. A proper treatment of particles is thus\nnecessary to make an analysis of Japanese sentences possible. Our treatment is\nbased on an empirical investigation of 800 dialogues. We set up a type\nhierarchy of particles motivated by their subcategorizational and\nmodificational behaviour. This type hierarchy is part of the Japanese syntax in\nVERBMOBIL."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906009v1", 
    "title": "Cascaded Markov Models", 
    "arxiv-id": "cs/9906009v1", 
    "author": "Thorsten Brants", 
    "publish": "1999-06-06T17:36:34Z", 
    "summary": "This paper presents a new approach to partial parsing of context-free\nstructures. The approach is based on Markov Models. Each layer of the resulting\nstructure is represented by its own Markov Model, and output of a lower layer\nis passed as input to the next higher layer. An empirical evaluation of the\nmethod yields very good results for NP/PP chunking of German newspaper texts."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906014v1", 
    "title": "Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System", 
    "arxiv-id": "cs/9906014v1", 
    "author": "Remko Bonnema", 
    "publish": "1999-06-14T10:06:31Z", 
    "summary": "The NWO Priority Programme Language and Speech Technology is a 5-year\nresearch programme aiming at the development of spoken language information\nsystems. In the Programme, two alternative natural language processing (NLP)\nmodules are developed in parallel: a grammar-based (conventional, rule-based)\nmodule and a data-oriented (memory-based, stochastic, DOP) module. In order to\ncompare the NLP modules, a formal evaluation has been carried out three years\nafter the start of the Programme. This paper describes the evaluation procedure\nand the evaluation results. The grammar-based component performs much better\nthan the data-oriented one in this comparison."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906015v1", 
    "title": "Learning Transformation Rules to Find Grammatical Relations", 
    "arxiv-id": "cs/9906015v1", 
    "author": "Alexander Yeh", 
    "publish": "1999-06-14T22:06:24Z", 
    "summary": "Grammatical relationships are an important level of natural language\nprocessing. We present a trainable approach to find these relationships through\ntransformation sequences and error-driven learning. Our approach finds\ngrammatical relationships between core syntax groups and bypasses much of the\nparsing phase. On our training and test set, our procedure achieves 63.6%\nrecall and 77.3% precision (f-score = 69.8)."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906020v1", 
    "title": "Temporal Meaning Representations in a Natural Language Front-End", 
    "arxiv-id": "cs/9906020v1", 
    "author": "I. Androutsopoulos", 
    "publish": "1999-06-22T08:28:26Z", 
    "summary": "Previous work in the context of natural language querying of temporal\ndatabases has established a method to map automatically from a large subset of\nEnglish time-related questions to suitable expressions of a temporal logic-like\nlanguage, called TOP. An algorithm to translate from TOP to the TSQL2 temporal\ndatabase language has also been defined. This paper shows how TOP expressions\ncould be translated into a simpler logic-like language, called BOT. BOT is very\nclose to traditional first-order predicate logic (FOPL), and hence existing\nmethods to manipulate FOPL expressions can be exploited to interface to\ntime-sensitive applications other than TSQL2 databases, maintaining the\nexisting English-to-TOP mapping."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906025v1", 
    "title": "Mapping Multilingual Hierarchies Using Relaxation Labeling", 
    "arxiv-id": "cs/9906025v1", 
    "author": "G. Rigau", 
    "publish": "1999-06-24T16:56:45Z", 
    "summary": "This paper explores the automatic construction of a multilingual Lexical\nKnowledge Base from pre-existing lexical resources. We present a new and robust\napproach for linking already existing lexical/semantic hierarchies. We used a\nconstraint satisfaction algorithm (relaxation labeling) to select --among all\nthe candidate translations proposed by a bilingual dictionary-- the right\nEnglish WordNet synset for each sense in a taxonomy automatically derived from\na Spanish monolingual dictionary. Although on average, there are 15 possible\nWordNet connections for each sense in the taxonomy, the method achieves an\naccuracy over 80%. Finally, we also propose several ways in which this\ntechnique could be applied to enrich and improve existing lexical databases."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906026v1", 
    "title": "Robust Grammatical Analysis for Spoken Dialogue Systems", 
    "arxiv-id": "cs/9906026v1", 
    "author": "Mark-Jan Nederhof", 
    "publish": "1999-06-25T08:16:23Z", 
    "summary": "We argue that grammatical analysis is a viable alternative to concept\nspotting for processing spoken input in a practical spoken dialogue system. We\ndiscuss the structure of the grammar, and a model for robust parsing which\ncombines linguistic sources of information and statistical sources of\ninformation. We discuss test results suggesting that grammatical processing\nallows fast and accurate processing of spoken input."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9906034v1", 
    "title": "A Unified Example-Based and Lexicalist Approach to Machine Translation", 
    "arxiv-id": "cs/9906034v1", 
    "author": "Janine Toole", 
    "publish": "1999-06-30T23:06:09Z", 
    "summary": "We present an approach to Machine Translation that combines the ideas and\nmethodologies of the Example-Based and Lexicalist theoretical frameworks. The\napproach has been implemented in a multilingual Machine Translation system."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907003v1", 
    "title": "Annotation graphs as a framework for multidimensional linguistic data   analysis", 
    "arxiv-id": "cs/9907003v1", 
    "author": "Mark Liberman", 
    "publish": "1999-07-05T14:51:26Z", 
    "summary": "In recent work we have presented a formal framework for linguistic annotation\nbased on labeled acyclic digraphs. These `annotation graphs' offer a simple yet\npowerful method for representing complex annotation structures incorporating\nhierarchy and overlap. Here, we motivate and illustrate our approach using\ndiscourse-level annotations of text and speech data drawn from the CALLHOME,\nCOCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain\nspecialists, we have constructed a hybrid multi-level annotation for a fragment\nof the Boston University Radio Speech Corpus which includes the following\nlevels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named\nentity. We show how annotation graphs can represent hybrid multi-level\nstructures which derive from a diverse set of file formats. We also show how\nthe approach facilitates substantive comparison of multiple annotations of a\nsingle signal based on different theoretical models. The discussion shows how\nannotation graphs open the door to wide-ranging integration of tools, formats\nand corpora."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907006v1", 
    "title": "Representing Text Chunks", 
    "arxiv-id": "cs/9907006v1", 
    "author": "Jorn Veenstra", 
    "publish": "1999-07-06T12:44:20Z", 
    "summary": "Dividing sentences in chunks of words is a useful preprocessing step for\nparsing, information extraction and information retrieval. (Ramshaw and Marcus,\n1995) have introduced a \"convenient\" data representation for chunking by\nconverting it to a tagging task. In this paper we will examine seven different\ndata representations for the problem of recognizing noun phrase chunks. We will\nshow that the the data representation choice has a minor influence on chunking\nperformance. However, equipped with the most suitable data representation, our\nmemory-based learning chunker was able to improve the best published chunking\nresults for a standard data set."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907007v2", 
    "title": "Cross-Language Information Retrieval for Technical Documents", 
    "arxiv-id": "cs/9907007v2", 
    "author": "Tetsuya Ishikawa", 
    "publish": "1999-07-06T16:25:46Z", 
    "summary": "This paper proposes a Japanese/English cross-language information retrieval\n(CLIR) system targeting technical documents. Our system first translates a\ngiven query containing technical terms into the target language, and then\nretrieves documents relevant to the translated query. The translation of\ntechnical terms is still problematic in that technical terms are often compound\nwords, and thus new terms can be progressively created simply by combining\nexisting base words. In addition, Japanese often represents loanwords based on\nits phonogram. Consequently, existing dictionaries find it difficult to achieve\nsufficient coverage. To counter the first problem, we use a compound word\ntranslation method, which uses a bilingual dictionary for base words and\ncollocational statistics to resolve translation ambiguity. For the second\nproblem, we propose a transliteration method, which identifies phonetic\nequivalents in the target language. We also show the effectiveness of our\nsystem using a test collection for CLIR."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907008v1", 
    "title": "Explanation-based Learning for Machine Translation", 
    "arxiv-id": "cs/9907008v1", 
    "author": "Paul McFetridge", 
    "publish": "1999-07-06T18:35:41Z", 
    "summary": "In this paper we present an application of explanation-based learning (EBL)\nin the parsing module of a real-time English-Spanish machine translation system\ndesigned to translate closed captions. We discuss the efficiency/coverage\ntrade-offs available in EBL and introduce the techniques we use to increase\ncoverage while maintaining a high level of space and time efficiency. Our\nperformance results indicate that this approach is effective."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907010v1", 
    "title": "Language Identification With Confidence Limits", 
    "arxiv-id": "cs/9907010v1", 
    "author": "David Elworthy", 
    "publish": "1999-07-07T09:28:40Z", 
    "summary": "A statistical classification algorithm and its application to language\nidentification from noisy input are described. The main innovation is to\ncompute confidence limits on the classification, so that the algorithm\nterminates when enough evidence to make a clear decision has been made, and so\navoiding problems with categories that have similar characteristics. A second\napplication, to genre identification, is briefly examined. The results show\nthat some of the problems of other language identification techniques can be\navoided, and illustrate a more important point: that a statistical language\nprocess can be used to provide feedback about its own success rate."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907012v1", 
    "title": "Selective Magic HPSG Parsing", 
    "arxiv-id": "cs/9907012v1", 
    "author": "Guido Minnen", 
    "publish": "1999-07-08T09:46:37Z", 
    "summary": "We propose a parser for constraint-logic grammars implementing HPSG that\ncombines the advantages of dynamic bottom-up and advanced top-down control. The\nparser allows the user to apply magic compilation to specific constraints in a\ngrammar which as a result can be processed dynamically in a bottom-up and\ngoal-directed fashion. State of the art top-down processing techniques are used\nto deal with the remaining constraints. We discuss various aspects concerning\nthe implementation of the parser as part of a grammar development system."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907013v1", 
    "title": "Corpus Annotation for Parser Evaluation", 
    "arxiv-id": "cs/9907013v1", 
    "author": "Ted Briscoe", 
    "publish": "1999-07-08T10:08:59Z", 
    "summary": "We describe a recently developed corpus annotation scheme for evaluating\nparsers that avoids shortcomings of current methods. The scheme encodes\ngrammatical relations between heads and dependents, and has been used to mark\nup a new public-domain corpus of naturally occurring English text. We show how\nthe corpus can be used to evaluate the accuracy of a robust parser, and relate\nthe corpus to extant resources."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907017v1", 
    "title": "A Bootstrap Approach to Automatically Generating Lexical Transfer Rules", 
    "arxiv-id": "cs/9907017v1", 
    "author": "Janine Toole", 
    "publish": "1999-07-09T22:39:52Z", 
    "summary": "We describe a method for automatically generating Lexical Transfer Rules\n(LTRs) from word equivalences using transfer rule templates. Templates are\nskeletal LTRs, unspecified for words. New LTRs are created by instantiating a\ntemplate with words, provided that the words belong to the appropriate lexical\ncategories required by the template. We define two methods for creating an\ninventory of templates and using them to generate new LTRs. A simpler method\nconsists of extracting a finite set of templates from a sample of hand coded\nLTRs and directly using them in the generation process. A further method\nconsists of abstracting over the initial finite set of templates to define\nhigher level templates, where bilingual equivalences are defined in terms of\ncorrespondences involving phrasal categories. Phrasal templates are then mapped\nonto sets of lexical templates with the aid of grammars. In this way an\ninfinite set of lexical templates is recursively defined. New LTRs are created\nby parsing input words, matching a template at the phrasal level and using the\ncorresponding lexical categories to instantiate the lexical template. The\ndefinition of an infinite set of templates enables the automatic creation of\nLTRs for multi-word, non-compositional word equivalences of any cardinality."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9907021v1", 
    "title": "Architectural Considerations for Conversational Systems -- The   Verbmobil/INTARC Experience", 
    "arxiv-id": "cs/9907021v1", 
    "author": "Hans Weber", 
    "publish": "1999-07-14T09:21:16Z", 
    "summary": "The paper describes the speech to speech translation system INTARC, developed\nduring the first phase of the Verbmobil project. The general design goals of\nthe INTARC system architecture were time synchronous processing as well as\nincrementality and interactivity as a means to achieve a higher degree of\nrobustness and scalability. Interactivity means that in addition to the\nbottom-up (in terms of processing levels) data flow the ability to process\ntop-down restrictions considering the same signal segment for all processing\nlevels. The construction of INTARC 2.0, which has been operational since fall\n1996, followed an engineering approach focussing on the integration of symbolic\n(linguistic) and stochastic (recognition) techniques which led to a\ngeneralization of the concept of a ``one pass'' beam search."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9908001v1", 
    "title": "Detecting Sub-Topic Correspondence through Bipartite Term Clustering", 
    "arxiv-id": "cs/9908001v1", 
    "author": "Eli Shamir", 
    "publish": "1999-08-01T14:02:57Z", 
    "summary": "This paper addresses a novel task of detecting sub-topic correspondence in a\npair of text fragments, enhancing common notions of text similarity. This task\nis addressed by coupling corresponding term subsets through bipartite\nclustering. The paper presents a cost-based clustering scheme and compares it\nwith a bipartite version of the single-link method, providing illustrating\nresults."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9909002v1", 
    "title": "Semantic robust parsing for noun extraction from natural language   queries", 
    "arxiv-id": "cs/9909002v1", 
    "author": "Vincenzo Pallotta", 
    "publish": "1999-09-02T15:53:07Z", 
    "summary": "This paper describes how robust parsing techniques can be fruitful applied\nfor building a query generation module which is part of a pipelined NLP\narchitecture aimed at process natural language queries in a restricted domain.\nWe want to show that semantic robustness represents a key issue in those NLP\nsystems where it is more likely to have partial and ill-formed utterances due\nto various factors (e.g. noisy environments, low quality of speech recognition\nmodules, etc...) and where it is necessary to succeed, even if partially, in\nextracting some meaningful information."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9910020v1", 
    "title": "Selective Sampling for Example-based Word Sense Disambiguation", 
    "arxiv-id": "cs/9910020v1", 
    "author": "Hozumi Tanaka", 
    "publish": "1999-10-23T11:19:35Z", 
    "summary": "This paper proposes an efficient example sampling method for example-based\nword sense disambiguation systems. To construct a database of practical size, a\nconsiderable overhead for manual sense disambiguation (overhead for\nsupervision) is required. In addition, the time complexity of searching a\nlarge-sized database poses a considerable problem (overhead for search). To\ncounter these problems, our method selectively samples a smaller-sized\neffective subset from a given example set for use in word sense disambiguation.\nOur method is characterized by the reliance on the notion of training utility:\nthe degree to which each example is informative for future example sampling\nwhen used for the training of the system. The system progressively collects\nexamples by selecting those with greatest utility. The paper reports the\neffectiveness of our method through experiments on about one thousand\nsentences. Compared to experiments with other example sampling methods, our\nmethod reduced both the overhead for supervision and the overhead for search,\nwithout the degeneration of the performance of the system."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9910022v1", 
    "title": "Practical experiments with regular approximation of context-free   languages", 
    "arxiv-id": "cs/9910022v1", 
    "author": "Mark-Jan Nederhof", 
    "publish": "1999-10-25T15:00:52Z", 
    "summary": "Several methods are discussed that construct a finite automaton given a\ncontext-free grammar, including both methods that lead to subsets and those\nthat lead to supersets of the original context-free language. Some of these\nmethods of regular approximation are new, and some others are presented here in\na more refined form with respect to existing literature. Practical experiments\nwith the different methods of regular approximation are performed for\nspoken-language input: hypotheses from a speech recognizer are filtered through\na finite automaton."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9911006v1", 
    "title": "Question Answering System Using Syntactic Information", 
    "arxiv-id": "cs/9911006v1", 
    "author": "H. Isahara", 
    "publish": "1999-11-15T05:48:03Z", 
    "summary": "Question answering task is now being done in TREC8 using English documents.\nWe examined question answering task in Japanese sentences. Our method selects\nthe answer by matching the question sentence with knowledge-based data written\nin natural language. We use syntactic information to obtain highly accurate\nanswers."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9911011v1", 
    "title": "One-Level Prosodic Morphology", 
    "arxiv-id": "cs/9911011v1", 
    "author": "Markus Walther", 
    "publish": "1999-11-19T16:10:51Z", 
    "summary": "Recent developments in theoretical linguistics have lead to a widespread\nacceptance of constraint-based analyses of prosodic morphology phenomena such\nas truncation, infixation, floating morphemes and reduplication. Of these,\nreduplication is particularly challenging for state-of-the-art computational\nmorphology, since it involves copying of some part of a phonological string. In\nthis paper I argue for certain extensions to the one-level model of phonology\nand morphology (Bird & Ellison 1994) to cover the computational aspects of\nprosodic morphology using finite-state methods. In a nutshell, enriched lexical\nrepresentations provide additional automaton arcs to repeat or skip sounds and\nalso to allow insertion of additional material. A kind of resource\nconsciousness is introduced to control this additional freedom, distinguishing\nbetween producer and consumer arcs. The non-finite-state copying aspect of\nreduplication is mapped to automata intersection, itself a non-finite-state\noperation. Bounded local optimization prunes certain automaton arcs that fail\nto contribute to linguistic optimisation criteria. The paper then presents\nimplemented case studies of Ulwa construct state infixation, German\nhypocoristic truncation and Tagalog over-applying reduplication that illustrate\nthe expressive power of this approach, before its merits and limitations are\ndiscussed and possible extensions are sketched. I conclude that the one-level\napproach to prosodic morphology presents an attractive way of extending\nfinite-state techniques to difficult phenomena that hitherto resisted elegant\ncomputational analyses."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9912003v1", 
    "title": "Resolution of Indirect Anaphora in Japanese Sentences Using Examples 'X   no Y (Y of X)'", 
    "arxiv-id": "cs/9912003v1", 
    "author": "M. Nagao", 
    "publish": "1999-12-13T04:42:25Z", 
    "summary": "A noun phrase can indirectly refer to an entity that has already been\nmentioned. For example, ``I went into an old house last night. The roof was\nleaking badly and ...'' indicates that ``the roof'' is associated with `` an\nold house}'', which was mentioned in the previous sentence. This kind of\nreference (indirect anaphora) has not been studied well in natural language\nprocessing, but is important for coherence resolution, language understanding,\nand machine translation. In order to analyze indirect anaphora, we need a case\nframe dictionary for nouns that contains knowledge of the relationships between\ntwo nouns but no such dictionary presently exists. Therefore, we are forced to\nuse examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead.\nWe tried estimating indirect anaphora using this information and obtained a\nrecall rate of 63% and a precision rate of 68% on test sentences. This\nindicates that the information of ``X no Y'' is useful to a certain extent when\nwe cannot make use of a noun case frame dictionary. We estimated the results\nthat would be given by a noun case frame dictionary, and obtained recall and\nprecision rates of 71% and 82% respectively. Finally, we proposed a way to\nconstruct a noun case frame dictionary by using examples of ``X no Y.''"
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9912004v1", 
    "title": "Pronoun Resolution in Japanese Sentences Using Surface Expressions and   Examples", 
    "arxiv-id": "cs/9912004v1", 
    "author": "M. Nagao", 
    "publish": "1999-12-13T04:46:20Z", 
    "summary": "In this paper, we present a method of estimating referents of demonstrative\npronouns, personal pronouns, and zero pronouns in Japanese sentences using\nexamples, surface expressions, topics and foci. Unlike conventional work which\nwas semantic markers for semantic constraints, we used examples for semantic\nconstraints and showed in our experiments that examples are as useful as\nsemantic markers. We also propose many new methods for estimating referents of\npronouns. For example, we use the form ``X of Y'' for estimating referents of\ndemonstrative adjectives. In addition to our new methods, we used many\nconventional methods. As a result, experiments using these methods obtained a\nprecision rate of 87% in estimating referents of demonstrative pronouns,\npersonal pronouns, and zero pronouns for training sentences, and obtained a\nprecision rate of 78% for test sentences."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9912005v1", 
    "title": "An Estimate of Referent of Noun Phrases in Japanese Sentences", 
    "arxiv-id": "cs/9912005v1", 
    "author": "M. Nagao", 
    "publish": "1999-12-13T05:20:40Z", 
    "summary": "In machine translation and man-machine dialogue, it is important to clarify\nreferents of noun phrases. We present a method for determining the referents of\nnoun phrases in Japanese sentences by using the referential properties,\nmodifiers, and possessors of noun phrases. Since the Japanese language has no\narticles, it is difficult to decide whether a noun phrase has an antecedent or\nnot. We had previously estimated the referential properties of noun phrases\nthat correspond to articles by using clue words in the sentences. By using\nthese referential properties, our system determined the referents of noun\nphrases in Japanese sentences. Furthermore we used the modifiers and possessors\nof noun phrases in determining the referents of noun phrases. As a result, on\ntraining sentences we obtained a precision rate of 82% and a recall rate of 85%\nin the determination of the referents of noun phrases that have antecedents. On\ntest sentences, we obtained a precision rate of 79% and a recall rate of 77%."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9912006v1", 
    "title": "Resolution of Verb Ellipsis in Japanese Sentence using Surface   Expressions and Examples", 
    "arxiv-id": "cs/9912006v1", 
    "author": "M. Nagao", 
    "publish": "1999-12-13T05:19:46Z", 
    "summary": "Verbs are sometimes omitted in Japanese sentences. It is necessary to recover\nomitted verbs for purposes of language understanding, machine translation, and\nconversational processing. This paper describes a practical way to recover\nomitted verbs by using surface expressions and examples. We experimented the\nresolution of verb ellipses by using this information, and obtained a recall\nrate of 73% and a precision rate of 66% on test sentences."
},{
    "category": "cs.CL", 
    "doi": "10.1063/1.1594535", 
    "link": "http://arxiv.org/pdf/cs/9912007v1", 
    "title": "An Example-Based Approach to Japanese-to-English Translation of Tense,   Aspect, and Modality", 
    "arxiv-id": "cs/9912007v1", 
    "author": "H. Isahara", 
    "publish": "1999-12-13T06:01:19Z", 
    "summary": "We have developed a new method for Japanese-to-English translation of tense,\naspect, and modality that uses an example-based method. In this method the\nsimilarity between input and example sentences is defined as the degree of\nsemantic matching between the expressions at the ends of the sentences. Our\nmethod also uses the k-nearest neighbor method in order to exclude the effects\nof noise; for example, wrongly tagged data in the bilingual corpora.\nExperiments show that our method can translate tenses, aspects, and modalities\nmore accurately than the top-level MT software currently available on the\nmarket can. Moreover, it does not require hand-craft rules."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/9912009v1", 
    "title": "Deduction over Mixed-Level Logic Representations for Text Passage   Retrieval", 
    "arxiv-id": "cs/9912009v1", 
    "author": "Michael Hess", 
    "publish": "1999-12-15T11:02:22Z", 
    "summary": "A system is described that uses a mixed-level representation of (part of)\nmeaning of natural language documents (based on standard Horn Clause Logic) and\na variable-depth search strategy that distinguishes between the different\nlevels of abstraction in the knowledge representation to locate specific\npassages in the documents. Mixed-level representations as well as\nvariable-depth search strategies are applicable in fields outside that of NLP."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/9912017v1", 
    "title": "Mixed-Level Knowledge Representation and Variable-Depth Inference in   Natural Language Processing", 
    "arxiv-id": "cs/9912017v1", 
    "author": "Michael Hess", 
    "publish": "1999-12-23T15:48:26Z", 
    "summary": "A system is described that uses a mixed-level knowledge representation based\non standard Horn Clause Logic to represent (part of) the meaning of natural\nlanguage documents. A variable-depth search strategy is outlined that\ndistinguishes between the different levels of abstraction in the knowledge\nrepresentation to locate specific passages in the documents. A detailed\ndescription of the linguistic aspects of the system is given. Mixed-level\nrepresentations as well as variable-depth search strategies are applicable in\nfields outside that of NLP."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001010v1", 
    "title": "A Real World Implementation of Answer Extraction", 
    "arxiv-id": "cs/0001010v1", 
    "author": "M. Hess", 
    "publish": "2000-01-14T16:01:31Z", 
    "summary": "In this paper we describe ExtrAns, an answer extraction system. Answer\nextraction (AE) aims at retrieving those exact passages of a document that\ndirectly answer a given user question. AE is more ambitious than information\nretrieval and information extraction in that the retrieval results are phrases,\nnot entire documents, and in that the queries may be arbitrarily specific. It\nis less ambitious than full-fledged question answering in that the answers are\nnot generated from a knowledge base but looked up in the text of documents. The\ncurrent version of ExtrAns is able to parse unedited Unix \"man pages\", and\nderive the logical form of their sentences. User queries are also translated\ninto logical forms. A theorem prover then retrieves the relevant phrases, which\nare presented through selective highlighting in their context."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001012v1", 
    "title": "Measures of Distributional Similarity", 
    "arxiv-id": "cs/0001012v1", 
    "author": "Lillian Lee", 
    "publish": "2000-01-18T23:19:22Z", 
    "summary": "We study distributional similarity measures for the purpose of improving\nprobability estimation for unseen cooccurrences. Our contributions are\nthree-fold: an empirical comparison of a broad range of measures; a\nclassification of similarity functions based on the information that they\nincorporate; and the introduction of a novel function that is superior at\nevaluating potential proxy distributions."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001020v1", 
    "title": "Exploiting Syntactic Structure for Natural Language Modeling", 
    "arxiv-id": "cs/0001020v1", 
    "author": "Ciprian Chelba", 
    "publish": "2000-01-24T20:18:16Z", 
    "summary": "The thesis presents an attempt at using the syntactic structure in natural\nlanguage for improved language models for speech recognition. The structured\nlanguage model merges techniques in automatic parsing and language modeling\nusing an original probabilistic parameterization of a shift-reduce parser. A\nmaximum likelihood reestimation procedure belonging to the class of\nexpectation-maximization algorithms is employed for training the model.\nExperiments on the Wall Street Journal, Switchboard and Broadcast News corpora\nshow improvement in both perplexity and word error rate - word lattice\nrescoring - over the standard 3-gram language model. The significance of the\nthesis lies in presenting an original approach to language modeling that uses\nthe hierarchical - syntactic - structure in natural language to improve on\ncurrent 3-gram modeling techniques for large vocabulary speech recognition."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001021v1", 
    "title": "Refinement of a Structured Language Model", 
    "arxiv-id": "cs/0001021v1", 
    "author": "Frederick Jelinek", 
    "publish": "2000-01-24T20:55:17Z", 
    "summary": "A new language model for speech recognition inspired by linguistic analysis\nis presented. The model develops hidden hierarchical structure incrementally\nand uses it to extract meaningful information from the word history - thus\nenabling the use of extended distance dependencies - in an attempt to\ncomplement the locality of currently used n-gram Markov models. The model, its\nprobabilistic parametrization, a reestimation algorithm for the model\nparameters and a set of experiments meant to evaluate its potential for speech\nrecognition are presented."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001022v1", 
    "title": "Recognition Performance of a Structured Language Model", 
    "arxiv-id": "cs/0001022v1", 
    "author": "Frederick Jelinek", 
    "publish": "2000-01-24T21:18:37Z", 
    "summary": "A new language model for speech recognition inspired by linguistic analysis\nis presented. The model develops hidden hierarchical structure incrementally\nand uses it to extract meaningful information from the word history - thus\nenabling the use of extended distance dependencies - in an attempt to\ncomplement the locality of currently used trigram models. The structured\nlanguage model, its probabilistic parameterization and performance in a\ntwo-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus\nshow an improvement in both perplexity and word error rate over conventional\ntrigram models."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0001023v1", 
    "title": "Structured Language Modeling for Speech Recognition", 
    "arxiv-id": "cs/0001023v1", 
    "author": "Frederick Jelinek", 
    "publish": "2000-01-25T19:35:01Z", 
    "summary": "A new language model for speech recognition is presented. The model develops\nhidden hierarchical syntactic-like structure incrementally and uses it to\nextract meaningful information from the word history, thus complementing the\nlocality of currently used trigram models. The structured language model (SLM)\nand its performance in a two-pass speech recognizer --- lattice decoding ---\nare presented. Experiments on the WSJ corpus show an improvement in both\nperplexity (PPL) and word error rate (WER) over conventional trigram models."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0002007v1", 
    "title": "Requirements of Text Processing Lexicons", 
    "arxiv-id": "cs/0002007v1", 
    "author": "K. Litkowski", 
    "publish": "2000-02-11T19:17:18Z", 
    "summary": "As text processing systems expand in scope, they will require ever larger\nlexicons along with a parsing capability for discriminating among many senses\nof a word. Existing systems do not incorporate such subtleties in meaning for\ntheir lexicons. Ordinary dictionaries contain such information, but are largely\nuntapped. When the contents of dictionaries are scrutinized, they reveal many\nrequirements that must be satisfied in representing meaning and in developing\nsemantic parsers. These requirements were identified in research designed to\nfind primitive verb concepts. The requirements are outlined and general\nprocedures for satisfying them through the use of ordinary dictionaries are\ndescribed, illustrated by building frames for and examining the definitions of\n\"change\" and its uses as a hypernym in other definitions."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0002017v1", 
    "title": "An Usage Measure Based on Psychophysical Relations", 
    "arxiv-id": "cs/0002017v1", 
    "author": "V. Kromer", 
    "publish": "2000-02-27T04:09:59Z", 
    "summary": "A new word usage measure is proposed. It is based on psychophysical relations\nand allows to reveal words by its degree of \"importance\" for making basic\ndictionaries of sublanguages."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0003055v1", 
    "title": "TnT - A Statistical Part-of-Speech Tagger", 
    "arxiv-id": "cs/0003055v1", 
    "author": "Thorsten Brants", 
    "publish": "2000-03-13T09:55:08Z", 
    "summary": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.\nContrary to claims found elsewhere in the literature, we argue that a tagger\nbased on Markov models performs at least as well as other current approaches,\nincluding the Maximum Entropy framework. A recent comparison has even shown\nthat TnT performs significantly better for the tested corpora. We describe the\nbasic model of TnT, the techniques used for smoothing and for handling unknown\nwords. Furthermore, we present evaluations on two corpora."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0003060v1", 
    "title": "Message Classification in the Call Center", 
    "arxiv-id": "cs/0003060v1", 
    "author": "Roman G. Arens", 
    "publish": "2000-03-14T13:09:28Z", 
    "summary": "Customer care in technical domains is increasingly based on e-mail\ncommunication, allowing for the reproduction of approved solutions. Identifying\nthe customer's problem is often time-consuming, as the problem space changes if\nnew products are launched. This paper describes a new approach to the\nclassification of e-mail requests based on shallow text processing and machine\nlearning techniques. It is implemented within an assistance system for call\ncenter agents that is used in a commercial setting."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0003074v1", 
    "title": "A Finite State and Data-Oriented Method for Grapheme to Phoneme   Conversion", 
    "arxiv-id": "cs/0003074v1", 
    "author": "Gosse Bouma", 
    "publish": "2000-03-23T11:29:15Z", 
    "summary": "A finite-state method, based on leftmost longest-match replacement, is\npresented for segmenting words into graphemes, and for converting graphemes\ninto phonemes. A small set of hand-crafted conversion rules for Dutch achieves\na phoneme accuracy of over 93%. The accuracy of the system is further improved\nby using transformation-based learning. The phoneme accuracy of the best system\n(using a large set of rule templates and a `lazy' variant of Brill's algoritm),\ntrained on only 40K words, reaches 99% accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0003081v1", 
    "title": "Variable Word Rate N-grams", 
    "arxiv-id": "cs/0003081v1", 
    "author": "Steve Renals", 
    "publish": "2000-03-29T16:35:58Z", 
    "summary": "The rate of occurrence of words is not uniform but varies from document to\ndocument. Despite this observation, parameters for conventional n-gram language\nmodels are usually derived using the assumption of a constant word rate. In\nthis paper we investigate the use of variable word rate assumption, modelled by\na Poisson distribution or a continuous mixture of Poissons. We present an\napproach to estimating the relative frequencies of words or n-grams taking\nprior information of their occurrences into account. Discounting and smoothing\nschemes are also considered. Using the Broadcast News task, the approach\ndemonstrates a reduction of perplexity up to 10%."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TAI.1996.560480", 
    "link": "http://arxiv.org/pdf/cs/0003083v1", 
    "title": "Advances in domain independent linear text segmentation", 
    "arxiv-id": "cs/0003083v1", 
    "author": "Freddy Y. Y. Choi", 
    "publish": "2000-03-30T16:56:02Z", 
    "summary": "This paper describes a method for linear text segmentation which is twice as\naccurate and over seven times as fast as the state-of-the-art (Reynar, 1998).\nInter-sentence similarity is replaced by rank in the local context. Boundary\nlocations are discovered by divisive clustering."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0003084v1", 
    "title": "Information Extraction from Broadcast News", 
    "arxiv-id": "cs/0003084v1", 
    "author": "Steve Renals", 
    "publish": "2000-03-30T16:52:50Z", 
    "summary": "This paper discusses the development of trainable statistical models for\nextracting content from television and radio news broadcasts. In particular we\nconcentrate on statistical finite state models for identifying proper names and\nother named entities in broadcast speech. Two models are presented: the first\nrepresents name class information as a word attribute; the second represents\nboth word-word and class-class transitions explicitly. A common n-gram based\nformulation is used for both models. The task of named entity identification is\ncharacterized by relatively sparse training data and issues related to\nsmoothing are discussed. Experiments are reported using the DARPA/NIST Hub-4E\nevaluation for North American Broadcast News."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0004016v1", 
    "title": "Looking at discourse in a corpus: The role of lexical cohesion", 
    "arxiv-id": "cs/0004016v1", 
    "author": "Tony Berber Sardinha", 
    "publish": "2000-04-28T03:18:09Z", 
    "summary": "This paper is aimed at reporting on the development and application of a\ncomputer model for discourse analysis through segmentation. Segmentation refers\nto the principled division of texts into contiguous constituents. Other studies\nhave looked at the application of a number of models to the analysis of\ndiscourse by computer. The segmentation procedure developed for the present\ninvestigation is called LSM ('Link Set Median'). It was applied to three corpus\nof 300 texts from three different genres. The results obtained by application\nof the LSM procedure on the corpus were then compared to segmentation carried\nout at random. Statistical analyses suggested that LSM significantly\noutperformed random segmentation, thus indicating that the segmentation was\nmeaningful."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005006v1", 
    "title": "A Simple Approach to Building Ensembles of Naive Bayesian Classifiers   for Word Sense Disambiguation", 
    "arxiv-id": "cs/0005006v1", 
    "author": "Ted Pedersen", 
    "publish": "2000-05-07T00:15:59Z", 
    "summary": "This paper presents a corpus-based approach to word sense disambiguation that\nbuilds an ensemble of Naive Bayesian classifiers, each of which is based on\nlexical features that represent co--occurring words in varying sized windows of\ncontext. Despite the simplicity of this approach, empirical results\ndisambiguating the widely studied nouns line and interest show that such an\nensemble achieves accuracy rivaling the best previously published results."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005015v1", 
    "title": "Noun Phrase Recognition by System Combination", 
    "arxiv-id": "cs/0005015v1", 
    "author": "Erik F. Tjong Kim Sang", 
    "publish": "2000-05-10T11:58:12Z", 
    "summary": "The performance of machine learning algorithms can be improved by combining\nthe output of different systems. In this paper we apply this idea to the\nrecognition of noun phrases.We generate different classifiers by using\ndifferent representations of the data. By combining the results with voting\ntechniques described in (Van Halteren et.al. 1998) we manage to improve the\nbest reported performances on standard data sets for base noun phrases and\narbitrary noun phrases."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005016v1", 
    "title": "Improving Testsuites via Instrumentation", 
    "arxiv-id": "cs/0005016v1", 
    "author": "Norbert Broeker", 
    "publish": "2000-05-10T14:51:14Z", 
    "summary": "This paper explores the usefulness of a technique from software engineering,\nnamely code instrumentation, for the development of large-scale natural\nlanguage grammars. Information about the usage of grammar rules in test\nsentences is used to detect untested rules, redundant test sentences, and\nlikely causes of overgeneration. Results show that less than half of a\nlarge-coverage grammar for German is actually tested by two large testsuites,\nand that 10-30% of testing time is redundant. The methodology applied can be\nseen as a re-use of grammar writing knowledge for testsuite compilation."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005019v1", 
    "title": "On the Scalability of the Answer Extraction System \"ExtrAns\"", 
    "arxiv-id": "cs/0005019v1", 
    "author": "Michael Hess", 
    "publish": "2000-05-12T14:12:15Z", 
    "summary": "This paper reports on the scalability of the answer extraction system\nExtrAns. An answer extraction system locates the exact phrases in the documents\nthat contain the explicit answers to the user queries. Answer extraction\nsystems are therefore more convenient than document retrieval systems in\nsituations where the user wants to find specific information in limited time.\n  ExtrAns performs answer extraction over UNIX manpages. It has been\nconstructed by combining available linguistic resources and implementing only a\nfew modules from scratch. A resolution procedure between the minimal logical\nform of the user query and the minimal logical forms of the manpage sentences\nfinds the answers to the queries. These answers are displayed to the user,\ntogether with pointers to the respective manpages, and the exact phrases that\ncontribute to the answer are highlighted.\n  This paper shows that the increase in response times is not a big issue when\nscaling the system up from 30 to 500 documents, and that the response times for\n500 documents are still acceptable for a real-time answer extraction system."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005025v1", 
    "title": "Finite-State Reduplication in One-Level Prosodic Morphology", 
    "arxiv-id": "cs/0005025v1", 
    "author": "Markus Walther", 
    "publish": "2000-05-22T14:07:28Z", 
    "summary": "Reduplication, a central instance of prosodic morphology, is particularly\nchallenging for state-of-the-art computational morphology, since it involves\ncopying of some part of a phonological string. In this paper I advocate a\nfinite-state method that combines enriched lexical representations via\nintersection to implement the copying. The proposal includes a\nresource-conscious variant of automata and can benefit from the existence of\nlazy algorithms. Finally, the implementation of a complex case from Koasati is\npresented."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0005029v1", 
    "title": "Ranking suspected answers to natural language questions using predictive   annotation", 
    "arxiv-id": "cs/0005029v1", 
    "author": "Valerie Samn", 
    "publish": "2000-05-30T17:10:33Z", 
    "summary": "In this paper, we describe a system to rank suspected answers to natural\nlanguage questions. We process both corpus and query using a new technique,\npredictive annotation, which augments phrases in texts with labels anticipating\ntheir being targets of certain kinds of questions. Given a natural language\nquestion, an IR system returns a set of matching passages, which are then\nanalyzed and ranked according to various criteria described in this paper. We\nprovide an evaluation of the techniques based on results from the TREC Q&A\nevaluation in which our system participated."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006003v1", 
    "title": "Exploiting Diversity in Natural Language Processing: Combining Parsers", 
    "arxiv-id": "cs/0006003v1", 
    "author": "Eric Brill", 
    "publish": "2000-06-01T18:42:24Z", 
    "summary": "Three state-of-the-art statistical parsers are combined to produce more\naccurate parses, as well as new bounds on achievable Treebank parsing accuracy.\nTwo general approaches are presented and two combination techniques are\ndescribed for each approach. Both parametric and non-parametric models are\nexplored. The resulting parsers surpass the best previously published\nperformance results for the Penn Treebank."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006011v1", 
    "title": "Bagging and Boosting a Treebank Parser", 
    "arxiv-id": "cs/0006011v1", 
    "author": "Eric Brill", 
    "publish": "2000-06-05T18:04:51Z", 
    "summary": "Bagging and boosting, two effective machine learning techniques, are applied\nto natural language parsing. Experiments using these techniques with a\ntrainable statistical parser are described. The best resulting system provides\nroughly as large of a gain in F-measure as doubling the corpus size. Error\nanalysis of the result of the boosting technique reveals some inconsistent\nannotations in the Penn Treebank, suggesting a semi-automatic method for\nfinding inconsistent treebank annotations."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006012v1", 
    "title": "Exploiting Diversity for Natural Language Parsing", 
    "arxiv-id": "cs/0006012v1", 
    "author": "John C. Henderson", 
    "publish": "2000-06-05T21:33:03Z", 
    "summary": "The popularity of applying machine learning methods to computational\nlinguistics problems has produced a large supply of trainable natural language\nprocessing systems. Most problems of interest have an array of off-the-shelf\nproducts or downloadable code implementing solutions using various techniques.\nWhere these solutions are developed independently, it is observed that their\nerrors tend to be independently distributed. This thesis is concerned with\napproaches for capitalizing on this situation in a sample problem domain, Penn\nTreebank-style parsing.\n  The machine learning community provides techniques for combining outputs of\nclassifiers, but parser output is more structured and interdependent than\nclassifications. To address this discrepancy, two novel strategies for\ncombining parsers are used: learning to control a switch between parsers and\nconstructing a hybrid parse from multiple parsers' outputs.\n  Off-the-shelf parsers are not developed with an intention to perform well in\na collaborative ensemble. Two techniques are presented for producing an\nensemble of parsers that collaborate. All of the ensemble members are created\nusing the same underlying parser induction algorithm, and the method for\nproducing complementary parsers is only loosely constrained by that chosen\nalgorithm."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006017v1", 
    "title": "Turning Speech Into Scripts", 
    "arxiv-id": "cs/0006017v1", 
    "author": "Frankie James", 
    "publish": "2000-06-09T17:28:40Z", 
    "summary": "We describe an architecture for implementing spoken natural language dialogue\ninterfaces to semi-autonomous systems, in which the central idea is to\ntransform the input speech signal through successive levels of representation\ncorresponding roughly to linguistic knowledge, dialogue knowledge, and domain\nknowledge. The final representation is an executable program in a simple\nscripting language equivalent to a subset of Cshell. At each stage of the\ntranslation process, an input is transformed into an output, producing as a\nbyproduct a \"meta-output\" which describes the nature of the transformation\nperformed. We show how consistent use of the output/meta-output distinction\npermits a simple and perspicuous treatment of apparently diverse topics\nincluding resolution of pronouns, correction of user misconceptions, and\noptimization of scripts. The methods described have been concretely realized in\na prototype speech interface to a simulation of the Personal Satellite\nAssistant."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006019v1", 
    "title": "A Compact Architecture for Dialogue Management Based on Scripts and   Meta-Outputs", 
    "arxiv-id": "cs/0006019v1", 
    "author": "Frankie James", 
    "publish": "2000-06-09T21:41:54Z", 
    "summary": "We describe an architecture for spoken dialogue interfaces to semi-autonomous\nsystems that transforms speech signals through successive representations of\nlinguistic, dialogue, and domain knowledge. Each step produces an output, and a\nmeta-output describing the transformation, with an executable program in a\nsimple scripting language as the final result. The output/meta-output\ndistinction permits perspicuous treatment of diverse tasks such as resolving\npronouns, correcting user misconceptions, and optimizing scripts."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006020v1", 
    "title": "A Comparison of the XTAG and CLE Grammars for English", 
    "arxiv-id": "cs/0006020v1", 
    "author": "Frankie James", 
    "publish": "2000-06-09T20:49:03Z", 
    "summary": "When people develop something intended as a large broad-coverage grammar,\nthey usually have a more specific goal in mind. Sometimes this goal is covering\na corpus; sometimes the developers have theoretical ideas they wish to\ninvestigate; most often, work is driven by a combination of these two main\ntypes of goal. What tends to happen after a while is that the community of\npeople working with the grammar starts thinking of some phenomena as\n``central'', and makes serious efforts to deal with them; other phenomena are\nlabelled ``marginal'', and ignored. Before long, the distinction between\n``central'' and ``marginal'' becomes so ingrained that it is automatic, and\npeople virtually stop thinking about the ``marginal'' phenomena. In practice,\nthe only way to bring the marginal things back into focus is to look at what\nother people are doing and compare it with one's own work. In this paper, we\nwill take two large grammars, XTAG and the CLE, and examine each of them from\nthe other's point of view. We will find in both cases not only that important\nthings are missing, but that the perspective offered by the other grammar\nsuggests simple and practical ways of filling in the holes. It turns out that\nthere is a pleasing symmetry to the picture. XTAG has a very good treatment of\ncomplement structure, which the CLE to some extent lacks; conversely, the CLE\noffers a powerful and general account of adjuncts, which the XTAG grammar does\nnot fully duplicate. If we examine the way in which each grammar does the thing\nit is good at, we find that the relevant methods are quite easy to port to the\nother framework, and in fact only involve generalization and systematization of\nexisting mechanisms."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006021v1", 
    "title": "Compiling Language Models from a Linguistically Motivated Unification   Grammar", 
    "arxiv-id": "cs/0006021v1", 
    "author": "Mark Gawron", 
    "publish": "2000-06-09T22:03:10Z", 
    "summary": "Systems now exist which are able to compile unification grammars into\nlanguage models that can be included in a speech recognizer, but it is so far\nunclear whether non-trivial linguistically principled grammars can be used for\nthis purpose. We describe a series of experiments which investigate the\nquestion empirically, by incrementally constructing a grammar and discovering\nwhat problems emerge when successively larger versions are compiled into finite\nstate graph representations and used as language models for a medium-vocabulary\nrecognition task."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006023v2", 
    "title": "Dialogue Act Modeling for Automatic Tagging and Recognition of   Conversational Speech", 
    "arxiv-id": "cs/0006023v2", 
    "author": "M. Meteer", 
    "publish": "2000-06-11T06:06:10Z", 
    "summary": "We describe a statistical approach for modeling dialogue acts in\nconversational speech, i.e., speech-act-like units such as Statement, Question,\nBackchannel, Agreement, Disagreement, and Apology. Our model detects and\npredicts dialogue acts based on lexical, collocational, and prosodic cues, as\nwell as on the discourse coherence of the dialogue act sequence. The dialogue\nmodel is based on treating the discourse structure of a conversation as a\nhidden Markov model and the individual dialogue acts as observations emanating\nfrom the model states. Constraints on the likely sequence of dialogue acts are\nmodeled via a dialogue act n-gram. The statistical dialogue grammar is combined\nwith word n-grams, decision trees, and neural networks modeling the\nidiosyncratic lexical and prosodic manifestations of each dialogue act. We\ndevelop a probabilistic integration of speech recognition with dialogue\nmodeling, to improve both speech recognition and dialogue act classification\naccuracy. Models are trained and evaluated using a large hand-labeled database\nof 1,155 conversations from the Switchboard corpus of spontaneous\nhuman-to-human telephone speech. We achieved good dialogue act labeling\naccuracy (65% based on errorful, automatically recognized words and prosody,\nand 71% based on word transcripts, compared to a chance baseline accuracy of\n35% and human accuracy of 84%) and a small reduction in word recognition error."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006024v1", 
    "title": "Can Prosody Aid the Automatic Classification of Dialog Acts in   Conversational Speech?", 
    "arxiv-id": "cs/0006024v1", 
    "author": "C. Van Ess-Dykema", 
    "publish": "2000-06-11T06:00:11Z", 
    "summary": "Identifying whether an utterance is a statement, question, greeting, and so\nforth is integral to effective automatic understanding of natural dialog.\nLittle is known, however, about how such dialog acts (DAs) can be automatically\nclassified in truly natural conversation. This study asks whether current\napproaches, which use mainly word information, could be improved by adding\nprosodic information. The study is based on more than 1000 conversations from\nthe Switchboard corpus. DAs were hand-annotated, and prosodic features\n(duration, pause, F0, energy, and speaking rate) were automatically extracted\nfor each DA. In training, decision trees based on these features were inferred;\ntrees were then applied to unseen test data to evaluate performance.\nPerformance was evaluated for prosody models alone, and after combining the\nprosody models with word information -- either from true words or from the\noutput of an automatic speech recognizer. For an overall classification task,\nas well as three subtasks, prosody made significant contributions to\nclassification. Feature-specific analyses further revealed that although\ncanonical features (such as F0 for questions) were important, less obvious\nfeatures could compensate if canonical features were removed. Finally, in each\ntask, integrating the prosodic model with a DA-specific statistical language\nmodel improved performance over that of the language model alone, especially\nfor the case of recognized words. Results suggest that DAs are redundantly\nmarked in natural conversation, and that a variety of automatically extractable\nprosodic features could aid dialog processing in speech applications."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006025v1", 
    "title": "Entropy-based Pruning of Backoff Language Models", 
    "arxiv-id": "cs/0006025v1", 
    "author": "A. Stolcke", 
    "publish": "2000-06-11T18:20:20Z", 
    "summary": "A criterion for pruning parameters from N-gram backoff language models is\ndeveloped, based on the relative entropy between the original and the pruned\nmodel. It is shown that the relative entropy resulting from pruning a single\nN-gram can be computed exactly and efficiently for backoff models. The relative\nentropy measure can be expressed as a relative change in training set\nperplexity. This leads to a simple pruning criterion whereby all N-grams that\nchange perplexity by less than a threshold are removed from the model.\nExperiments show that a production-quality Hub4 LM can be reduced to 26% its\noriginal size without increasing recognition error. We also compare the\napproach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and\nshow that their approach can be interpreted as an approximation to the relative\nentropy criterion. Experimentally, both approaches select similar sets of\nN-grams (about 85% overlap), with the exact relative entropy criterion giving\nmarginally better performance."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006028v1", 
    "title": "Trainable Methods for Surface Natural Language Generation", 
    "arxiv-id": "cs/0006028v1", 
    "author": "Adwait Ratnaparkhi", 
    "publish": "2000-06-13T21:06:27Z", 
    "summary": "We present three systems for surface natural language generation that are\ntrainable from annotated corpora. The first two systems, called NLG1 and NLG2,\nrequire a corpus marked only with domain-specific semantic attributes, while\nthe last system, called NLG3, requires a corpus marked with both semantic\nattributes and syntactic dependency information. All systems attempt to produce\na grammatical natural language phrase from a domain-specific semantic\nrepresentation. NLG1 serves a baseline system and uses phrase frequencies to\ngenerate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy\nprobability models to individually generate each word in the phrase. The\nsystems NLG2 and NLG3 learn to determine both the word choice and the word\norder of the phrase. We present experiments in which we generate phrases to\ndescribe flights in the air travel domain."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006036v1", 
    "title": "Prosody-Based Automatic Segmentation of Speech into Sentences and Topics", 
    "arxiv-id": "cs/0006036v1", 
    "author": "G. Tur", 
    "publish": "2000-06-27T04:39:57Z", 
    "summary": "A crucial step in processing speech audio data for information extraction,\ntopic detection, or browsing/playback is to segment the input into sentence and\ntopic units. Speech segmentation is challenging, since the cues typically\npresent for segmenting text (headers, paragraphs, punctuation) are absent in\nspoken language. We investigate the use of prosody (information gleaned from\nthe timing and melody of speech) for these tasks. Using decision tree and\nhidden Markov modeling techniques, we combine prosodic cues with word-based\napproaches, and evaluate performance on two speech corpora, Broadcast News and\nSwitchboard. Results show that the prosodic model alone performs on par with,\nor better than, word-based statistical language models -- for both true and\nautomatically recognized words in news speech. The prosodic model achieves\ncomparable performance with significantly less training data, and requires no\nhand-labeling of prosodic events. Across tasks and corpora, we obtain a\nsignificant improvement over word-only models using a probabilistic combination\nof prosodic and lexical information. Inspection reveals that the prosodic\nmodels capture language-independent boundary indicators described in the\nliterature. Finally, cue usage is task and corpus dependent. For example, pause\nand pitch features are highly informative for segmenting news speech, whereas\npause, duration and word-based cues dominate for natural conversation."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006038v1", 
    "title": "Approximation and Exactness in Finite State Optimality Theory", 
    "arxiv-id": "cs/0006038v1", 
    "author": "Gertjan van Noord", 
    "publish": "2000-06-28T10:06:02Z", 
    "summary": "Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that\nOptimality Theory with gradient constraints generally is not finite state. A\nnew finite-state treatment of gradient constraints is presented which improves\nupon the approximation of Karttunen (1998). The method turns out to be exact,\nand very compact, for the syllabification analysis of Prince and Smolensky\n(1993)."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0006044v1", 
    "title": "Finite-State Non-Concatenative Morphotactics", 
    "arxiv-id": "cs/0006044v1", 
    "author": "Lauri Karttunen", 
    "publish": "2000-06-30T13:22:33Z", 
    "summary": "Finite-state morphology in the general tradition of the Two-Level and Xerox\nimplementations has proved very successful in the production of robust\nmorphological analyzer-generators, including many large-scale commercial\nsystems. However, it has long been recognized that these implementations have\nserious limitations in handling non-concatenative phenomena. We describe a new\ntechnique for constructing finite-state transducers that involves reapplying\nthe regular-expression compiler to its own output. Implemented in an algorithm\ncalled compile-replace, this technique has proved useful for handling\nnon-concatenative phenomena; and we demonstrate it on Malay full-stem\nreduplication and Arabic stem interdigitation."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007009v1", 
    "title": "Incremental construction of minimal acyclic finite-state automata", 
    "arxiv-id": "cs/0007009v1", 
    "author": "Richard Watson", 
    "publish": "2000-07-06T14:15:26Z", 
    "summary": "In this paper, we describe a new method for constructing minimal,\ndeterministic, acyclic finite-state automata from a set of strings. Traditional\nmethods consist of two phases: the first to construct a trie, the second one to\nminimize it. Our approach is to construct a minimal automaton in a single phase\nby adding new strings one by one and minimizing the resulting automaton\non-the-fly. We present a general algorithm as well as a specialization that\nrelies upon the lexicographical ordering of the input strings."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007018v1", 
    "title": "Bootstrapping a Tagged Corpus through Combination of Existing   Heterogeneous Taggers", 
    "arxiv-id": "cs/0007018v1", 
    "author": "Walter Daelemans", 
    "publish": "2000-07-13T12:46:00Z", 
    "summary": "This paper describes a new method, Combi-bootstrap, to exploit existing\ntaggers and lexical resources for the annotation of corpora with new tagsets.\nCombi-bootstrap uses existing resources as features for a second level machine\nlearning module, that is trained to make the mapping to the new tagset on a\nvery small sample of annotated corpus material. Experiments show that\nCombi-bootstrap: i) can integrate a wide variety of existing resources, and ii)\nachieves much higher accuracy (up to 44.7 % error reduction) than both the best\nsingle tagger and an ensemble tagger constructed out of the same small training\nsample."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007022v1", 
    "title": "ATLAS: A flexible and extensible architecture for linguistic annotation", 
    "arxiv-id": "cs/0007022v1", 
    "author": "Mark Liberman", 
    "publish": "2000-07-13T18:26:38Z", 
    "summary": "We describe a formal model for annotating linguistic artifacts, from which we\nderive an application programming interface (API) to a suite of tools for\nmanipulating these annotations. The abstract logical model provides for a range\nof storage formats and promotes the reuse of tools that interact through this\nAPI. We focus first on ``Annotation Graphs,'' a graph model for annotations on\nlinear signals (such as text and speech) indexed by intervals, for which\nefficient database storage and querying techniques are applicable. We note how\na wide range of existing annotated corpora can be mapped to this annotation\ngraph model. This model is then generalized to encompass a wider variety of\nlinguistic ``signals,'' including both naturally occuring phenomena (as\nrecorded in images, video, multi-modal interactions, etc.), as well as the\nderived resources that are increasingly important to the engineering of natural\nlanguage processing systems (such as word lists, dictionaries, aligned\nbilingual corpora, etc.). We conclude with a review of the current efforts\ntowards implementing key pieces of this architecture."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007024v1", 
    "title": "Many uses, many annotations for large speech corpora: Switchboard and   TDT as case studies", 
    "arxiv-id": "cs/0007024v1", 
    "author": "Steven Bird", 
    "publish": "2000-07-13T18:51:48Z", 
    "summary": "This paper discusses the challenges that arise when large speech corpora\nreceive an ever-broadening range of diverse and distinct annotations. Two case\nstudies of this process are presented: the Switchboard Corpus of telephone\nconversations and the TDT2 corpus of broadcast news. Switchboard has undergone\ntwo independent transcriptions and various types of additional annotation, all\ncarried out as separate projects that were dispersed both geographically and\nchronologically. The TDT2 corpus has also received a variety of annotations,\nbut all directly created or managed by a core group. In both cases, issues\narise involving the propagation of repairs, consistency of references, and the\nability to integrate annotations having different formats and levels of detail.\nWe describe a general framework whereby these issues can be addressed\nsuccessfully."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007031v2", 
    "title": "Parameter-free Model of Rank Polysemantic Distribution", 
    "arxiv-id": "cs/0007031v2", 
    "author": "Victor Kromer", 
    "publish": "2000-07-21T06:07:31Z", 
    "summary": "A model of rank polysemantic distribution with a minimal number of fitting\nparameters is offered. In an ideal case a parameter-free description of the\ndependence on the basis of one or several immediate features of the\ndistribution is possible."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007035v1", 
    "title": "Mapping WordNets Using Structural Information", 
    "arxiv-id": "cs/0007035v1", 
    "author": "G. Rigau", 
    "publish": "2000-07-25T17:20:47Z", 
    "summary": "We present a robust approach for linking already existing lexical/semantic\nhierarchies. We used a constraint satisfaction algorithm (relaxation labeling)\nto select --among a set of candidates-- the node in a target taxonomy that\nbests matches each node in a source taxonomy. In particular, we use it to map\nthe nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision\nand a very low remaining ambiguity."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0007036v1", 
    "title": "Language identification of controlled systems: Modelling, control and   anomaly detection", 
    "arxiv-id": "cs/0007036v1", 
    "author": "R. Vilela Mendes", 
    "publish": "2000-07-25T17:40:00Z", 
    "summary": "Formal language techniques have been used in the past to study autonomous\ndynamical systems. However, for controlled systems, new features are needed to\ndistinguish between information generated by the system and input control. We\nshow how the modelling framework for controlled dynamical systems leads\nnaturally to a formulation in terms of context-dependent grammars. A learning\nalgorithm is proposed for on-line generation of the grammar productions, this\nformulation being then used for modelling, control and anomaly detection.\nPractical applications are described for electromechanical drives. Grammatical\ninterpolation techniques yield accurate results and the pattern detection\ncapabilities of the language-based formulation makes it a promising technique\nfor the early detection of anomalies or faulty behaviour."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008003v1", 
    "title": "Interfacing Constraint-Based Grammars and Generation Algorithms", 
    "arxiv-id": "cs/0008003v1", 
    "author": "Stephan Busemann", 
    "publish": "2000-08-07T16:06:15Z", 
    "summary": "Constraint-based grammars can, in principle, serve as the major linguistic\nknowledge source for both parsing and generation. Surface generation starts\nfrom input semantics representations that may vary across grammars. For many\ndeclarative grammars, the concept of derivation implicitly built in is that of\nparsing. They may thus not be interpretable by a generation algorithm. We show\nthat linguistically plausible semantic analyses can cause severe problems for\nsemantic-head-driven approaches for generation (SHDG). We use SeReal, a variant\nof SHDG and the DISCO grammar of German as our source of examples. We propose a\nnew, general approach that explicitly accounts for the interface between the\ngrammar and the generation algorithm by adding a control-oriented layer to the\nlinguistic knowledge base that reorganizes the semantics in a way suitable for\ngeneration."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008004v1", 
    "title": "Comparing two trainable grammatical relations finders", 
    "arxiv-id": "cs/0008004v1", 
    "author": "Alexander Yeh", 
    "publish": "2000-08-08T22:42:51Z", 
    "summary": "Grammatical relationships (GRs) form an important level of natural language\nprocessing, but different sets of GRs are useful for different purposes.\nTherefore, one may often only have time to obtain a small training corpus with\nthe desired GR annotations. On such a small training corpus, we compare two\nsystems. They use different learning techniques, but we find that this\ndifference by itself only has a minor effect. A larger factor is that in\nEnglish, a different GR length measure appears better suited for finding simple\nargument GRs than for finding modifier GRs. We also find that partitioning the\ndata may help memory-based learning."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008005v1", 
    "title": "More accurate tests for the statistical significance of result   differences", 
    "arxiv-id": "cs/0008005v1", 
    "author": "Alexander Yeh", 
    "publish": "2000-08-08T23:52:02Z", 
    "summary": "Statistical significance testing of differences in values of metrics like\nrecall, precision and balanced F-score is a necessary part of empirical natural\nlanguage processing. Unfortunately, we find in a set of experiments that many\ncommonly used tests often underestimate the significance and so are less likely\nto detect differences that exist between different techniques. This\nunderestimation comes from an independence assumption that is often violated.\nWe point out some useful tests that do not make this assumption, including\ncomputationally-intensive randomization tests."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008007v1", 
    "title": "Tagger Evaluation Given Hierarchical Tag Sets", 
    "arxiv-id": "cs/0008007v1", 
    "author": "Philip Resnik", 
    "publish": "2000-08-10T03:21:33Z", 
    "summary": "We present methods for evaluating human and automatic taggers that extend\ncurrent practice in three ways. First, we show how to evaluate taggers that\nassign multiple tags to each test instance, even if they do not assign\nprobabilities. Second, we show how to accommodate a common property of manually\nconstructed ``gold standards'' that are typically used for objective\nevaluation, namely that there is often more than one correct answer. Third, we\nshow how to measure performance when the set of possible tags is\ntree-structured in an IS-A hierarchy. To illustrate how our methods can be used\nto measure inter-annotator agreement, we show how to compute the kappa\ncoefficient over hierarchical tag sets."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008012v1", 
    "title": "Applying System Combination to Base Noun Phrase Identification", 
    "arxiv-id": "cs/0008012v1", 
    "author": "Dan Roth", 
    "publish": "2000-08-17T13:13:42Z", 
    "summary": "We use seven machine learning algorithms for one task: identifying base noun\nphrases. The results have been processed by different system combination\nmethods and all of these outperformed the best individual result. We have\napplied the seven learners with the best combinator, a majority vote of the top\nfive systems, to a standard data set and managed to improve the best published\nresult for this data set."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008013v1", 
    "title": "Meta-Learning for Phonemic Annotation of Corpora", 
    "arxiv-id": "cs/0008013v1", 
    "author": "Steven Gillis", 
    "publish": "2000-08-18T12:06:36Z", 
    "summary": "We apply rule induction, classifier combination and meta-learning (stacked\nclassifiers) to the problem of bootstrapping high accuracy automatic annotation\nof corpora with pronunciation information. The task we address in this paper\nconsists of generating phonemic representations reflecting the Flemish and\nDutch pronunciations of a word on the basis of its orthographic representation\n(which in turn is based on the actual speech recordings). We compare several\npossible approaches to achieve the text-to-pronunciation mapping task:\nmemory-based learning, transformation-based learning, rule induction, maximum\nentropy modeling, combination of classifiers in stacked learning, and stacking\nof meta-learners. We are interested both in optimal accuracy and in obtaining\ninsight into the linguistic regularities involved. As far as accuracy is\nconcerned, an already high accuracy level (93% for Celex and 86% for Fonilex at\nword level) for single classifiers is boosted significantly with additional\nerror reductions of 31% and 38% respectively using combination of classifiers,\nand a further 5% using combination of meta-learners, bringing overall word\nlevel accuracy to 96% for the Dutch variant and 92% for the Flemish variant. We\nalso show that the application of machine learning methods indeed leads to\nincreased insight into the linguistic regularities determining the variation\nbetween the two pronunciation variants studied."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008014v1", 
    "title": "Aspects of Pattern-Matching in Data-Oriented Parsing", 
    "arxiv-id": "cs/0008014v1", 
    "author": "Guy De Pauw", 
    "publish": "2000-08-18T13:56:10Z", 
    "summary": "Data-Oriented Parsing (dop) ranks among the best parsing schemes, pairing\nstate-of-the art parsing accuracy to the psycholinguistic insight that larger\nchunks of syntactic structures are relevant grammatical and probabilistic\nunits. Parsing with the dop-model, however, seems to involve a lot of CPU\ncycles and a considerable amount of double work, brought on by the concept of\nmultiple derivations, which is necessary for probabilistic processing, but\nwhich is not convincingly related to a proper linguistic backbone. It is\nhowever possible to re-interpret the dop-model as a pattern-matching model,\nwhich tries to maximize the size of the substructures that construct the parse,\nrather than the probability of the parse. By emphasizing this memory-based\naspect of the dop-model, it is possible to do away with multiple derivations,\nopening up possibilities for efficient Viterbi-style optimizations, while still\nretaining acceptable parsing accuracy through enhanced context-sensitivity."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008015v1", 
    "title": "Temiar Reduplication in One-Level Prosodic Morphology", 
    "arxiv-id": "cs/0008015v1", 
    "author": "Markus Walther", 
    "publish": "2000-08-18T16:07:17Z", 
    "summary": "Temiar reduplication is a difficult piece of prosodic morphology. This paper\npresents the first computational analysis of Temiar reduplication, using the\nnovel finite-state approach of One-Level Prosodic Morphology originally\ndeveloped by Walther (1999b, 2000). After reviewing both the data and the basic\ntenets of One-level Prosodic Morphology, the analysis is laid out in some\ndetail, using the notation of the FSA Utilities finite-state toolkit (van Noord\n1997). One important discovery is that in this approach one can easily define a\nregular expression operator which ambiguously scans a string in the left- or\nrightward direction for a certain prosodic property. This yields an elegant\naccount of base-length-dependent triggering of reduplication as found in\nTemiar."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008017v1", 
    "title": "Efficient probabilistic top-down and left-corner parsing", 
    "arxiv-id": "cs/0008017v1", 
    "author": "Mark Johnson", 
    "publish": "2000-08-21T19:27:18Z", 
    "summary": "This paper examines efficient predictive broad-coverage parsing without\ndynamic programming. In contrast to bottom-up methods, depth-first top-down\nparsing produces partial parses that are fully connected trees spanning the\nentire left context, from which any kind of non-local dependency or partial\nsemantic interpretation can in principle be read. We contrast two predictive\nparsing approaches, top-down and left-corner parsing, and find both to be\nviable. In addition, we find that enhancement with non-local information not\nonly improves parser accuracy, but also substantially improves the search\nefficiency."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008021v1", 
    "title": "Compact non-left-recursive grammars using the selective left-corner   transform and factoring", 
    "arxiv-id": "cs/0008021v1", 
    "author": "Brian Roark", 
    "publish": "2000-08-22T15:16:22Z", 
    "summary": "The left-corner transform removes left-recursion from (probabilistic)\ncontext-free grammars and unification grammars, permitting simple top-down\nparsing techniques to be used. Unfortunately the grammars produced by the\nstandard left-corner transform are usually much larger than the original. The\nselective left-corner transform described in this paper produces a transformed\ngrammar which simulates left-corner recognition of a user-specified set of the\noriginal productions, and top-down recognition of the others. Combined with two\nfactorizations, it produces non-left-recursive grammars that are not much\nlarger than the original."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008023v1", 
    "title": "Selectional Restrictions in HPSG", 
    "arxiv-id": "cs/0008023v1", 
    "author": "Robert Dale", 
    "publish": "2000-08-23T07:38:22Z", 
    "summary": "Selectional restrictions are semantic sortal constraints imposed on the\nparticipants of linguistic constructions to capture contextually-dependent\nconstraints on interpretation. Despite their limitations, selectional\nrestrictions have proven very useful in natural language applications, where\nthey have been used frequently in word sense disambiguation, syntactic\ndisambiguation, and anaphora resolution. Given their practical value, we\nexplore two methods to incorporate selectional restrictions in the HPSG theory,\nassuming that the reader is familiar with HPSG. The first method employs HPSG's\nBackground feature and a constraint-satisfaction component pipe-lined after the\nparser. The second method uses subsorts of referential indices, and blocks\nreadings that violate selectional restrictions during parsing. While\ntheoretically less satisfactory, we have found the second method particularly\nuseful in the development of practical systems."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008024v1", 
    "title": "Estimation of Stochastic Attribute-Value Grammars using an Informative   Sample", 
    "arxiv-id": "cs/0008024v1", 
    "author": "Miles Osborne", 
    "publish": "2000-08-23T12:38:08Z", 
    "summary": "We argue that some of the computational complexity associated with estimation\nof stochastic attribute-value grammars can be reduced by training upon an\ninformative subset of the full training set. Results using the parsed Wall\nStreet Journal corpus show that in some circumstances, it is possible to obtain\nbetter estimation results using an informative sample than when training upon\nall the available material. Further experimentation demonstrates that with\nunlexicalised models, a Gaussian Prior can reduce overfitting. However, when\nmodels are lexicalised and contain overlapping features, overfitting does not\nseem to be a problem, and a Gaussian Prior makes minimal difference to\nperformance. Our approach is applicable for situations when there are an\ninfeasibly large number of parses in the training set, or else for when\nrecovery of these parses from a packed representation is itself computationally\nexpensive."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008026v1", 
    "title": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon   construction", 
    "arxiv-id": "cs/0008026v1", 
    "author": "Eugene Charniak", 
    "publish": "2000-08-24T13:28:25Z", 
    "summary": "Generating semantic lexicons semi-automatically could be a great time saver,\nrelative to creating them by hand. In this paper, we present an algorithm for\nextracting potential entries for a category from an on-line corpus, based upon\na small set of exemplars. Our algorithm finds more correct terms and fewer\nincorrect ones than previous work in this area. Additionally, the entries that\nare generated potentially provide broader coverage of the category than would\noccur to an individual coding them by hand. Our algorithm finds many terms not\nincluded within Wordnet (many more than previous algorithms), and could be\nviewed as an ``enhancer'' of existing broad-coverage resources."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008027v1", 
    "title": "Measuring efficiency in high-accuracy, broad-coverage statistical   parsing", 
    "arxiv-id": "cs/0008027v1", 
    "author": "Eugene Charniak", 
    "publish": "2000-08-24T16:38:53Z", 
    "summary": "Very little attention has been paid to the comparison of efficiency between\nhigh accuracy statistical parsers. This paper proposes one machine-independent\nmetric that is general enough to allow comparisons across very different\nparsing architectures. This metric, which we call ``events considered'',\nmeasures the number of ``events'', however they are defined for a particular\nparser, for which a probability must be calculated, in order to find the parse.\nIt is applicable to single-pass or multi-stage parsers. We discuss the\nadvantages of the metric, and demonstrate its usefulness by using it to compare\ntwo parsers which differ in several fundamental ways."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008028v1", 
    "title": "Estimators for Stochastic ``Unification-Based'' Grammars", 
    "arxiv-id": "cs/0008028v1", 
    "author": "Stefan Riezler", 
    "publish": "2000-08-25T17:23:07Z", 
    "summary": "Log-linear models provide a statistically sound framework for Stochastic\n``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds\nof grammars. We describe two computationally-tractable ways of estimating the\nparameters of such grammars from a training corpus of syntactic analyses, and\napply these to estimate a stochastic version of Lexical-Functional Grammar."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008029v1", 
    "title": "Exploiting auxiliary distributions in stochastic unification-based   grammars", 
    "arxiv-id": "cs/0008029v1", 
    "author": "Stefan Riezler", 
    "publish": "2000-08-25T17:31:53Z", 
    "summary": "This paper describes a method for estimating conditional probability\ndistributions over the parses of ``unification-based'' grammars which can\nutilize auxiliary distributions that are estimated by other means. We show how\nthis can be used to incorporate information about lexical selectional\npreferences gathered from other sources into Stochastic ``Unification-based''\nGrammars (SUBGs). While we apply this estimator to a Stochastic\nLexical-Functional Grammar, the method is general, and should be applicable to\nstochastic versions of HPSGs, categorial grammars and transformational\ngrammars."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008030v1", 
    "title": "Metonymy Interpretation Using X NO Y Examples", 
    "arxiv-id": "cs/0008030v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2000-08-28T08:10:14Z", 
    "summary": "We developed on example-based method of metonymy interpretation. One\nadvantages of this method is that a hand-built database of metonymy is not\nnecessary because it instead uses examples in the form ``Noun X no Noun Y (Noun\nY of Noun X).'' Another advantage is that we will be able to interpret\nnewly-coined metonymic sentences by using a new corpus. We experimented with\nmetonymy interpretation and obtained a precision rate of 66% when using this\nmethod."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008031v1", 
    "title": "Bunsetsu Identification Using Category-Exclusive Rules", 
    "arxiv-id": "cs/0008031v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2000-08-28T08:17:18Z", 
    "summary": "This paper describes two new bunsetsu identification methods using supervised\nlearning. Since Japanese syntactic analysis is usually done after bunsetsu\nidentification, bunsetsu identification is important for analyzing Japanese\nsentences. In experiments comparing the four previously available\nmachine-learning methods (decision tree, maximum-entropy method, example-based\napproach and decision list) and two new methods using category-exclusive rules,\nthe new method using the category-exclusive rules with the highest similarity\nperformed best."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008032v1", 
    "title": "Japanese Probabilistic Information Retrieval Using Location and Category   Information", 
    "arxiv-id": "cs/0008032v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2000-08-28T08:27:22Z", 
    "summary": "Robertson's 2-poisson information retrieve model does not use location and\ncategory information. We constructed a framework using location and category\ninformation in a 2-poisson model. We submitted two systems based on this\nframework to the IREX contest, Japanese language information retrieval contest\nheld in Japan in 1999. For precision in the A-judgement measure they scored\n0.4926 and 0.4827, the highest values among the 15 teams and 22 systems that\nparticipated in the IREX contest. We describe our systems and the comparative\nexperiments done when various parameters were changed. These experiments\nconfirmed the effectiveness of using location and category information."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008033v1", 
    "title": "Temporal Expressions in Japanese-to-English Machine Translation", 
    "arxiv-id": "cs/0008033v1", 
    "author": "Hajime Uchino", 
    "publish": "2000-08-28T19:51:32Z", 
    "summary": "This paper describes in outline a method for translating Japanese temporal\nexpressions into English. We argue that temporal expressions form a special\nsubset of language that is best handled as a special module in machine\ntranslation. The paper deals with problems of lexical idiosyncrasy as well as\nthe choice of articles and prepositions within temporal expressions. In\naddition temporal expressions are considered as parts of larger structures, and\nthe question of whether to translate them as noun phrases or adverbials is\naddressed."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008034v1", 
    "title": "Lexicalized Stochastic Modeling of Constraint-Based Grammars using   Log-Linear Measures and EM Training", 
    "arxiv-id": "cs/0008034v1", 
    "author": "Mark Johnson", 
    "publish": "2000-08-30T13:14:58Z", 
    "summary": "We present a new approach to stochastic modeling of constraint-based grammars\nthat is based on log-linear models and uses EM for estimation from unannotated\ndata. The techniques are applied to an LFG grammar for German. Evaluation on an\nexact match task yields 86% precision for an ambiguity rate of 5.4, and 90%\nprecision on a subcat frame match for an ambiguity rate of 25. Experimental\ncomparison to training from a parsebank shows a 10% gain from EM training.\nAlso, a new class-based grammar lexicalization is presented, showing a 10% gain\nover unlexicalized models."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008035v1", 
    "title": "Using a Probabilistic Class-Based Lexicon for Lexical Ambiguity   Resolution", 
    "arxiv-id": "cs/0008035v1", 
    "author": "Mats Rooth", 
    "publish": "2000-08-30T13:24:06Z", 
    "summary": "This paper presents the use of probabilistic class-based lexica for\ndisambiguation in target-word selection. Our method employs minimal but precise\ncontextual information for disambiguation. That is, only information provided\nby the target-verb, enriched by the condensed information of a probabilistic\nclass-based lexicon, is used. Induction of classes and fine-tuning to verbal\narguments is done in an unsupervised manner by EM-based clustering techniques.\nThe method shows promising results in an evaluation on real-world translations."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0008036v1", 
    "title": "Probabilistic Constraint Logic Programming. Formal Foundations of   Quantitative and Statistical Inference in Constraint-Based Natural Language   Processing", 
    "arxiv-id": "cs/0008036v1", 
    "author": "Stefan Riezler", 
    "publish": "2000-08-30T13:57:19Z", 
    "summary": "In this thesis, we present two approaches to a rigorous mathematical and\nalgorithmic foundation of quantitative and statistical inference in\nconstraint-based natural language processing. The first approach, called\nquantitative constraint logic programming, is conceptualized in a clear logical\nframework, and presents a sound and complete system of quantitative inference\nfor definite clauses annotated with subjective weights. This approach combines\na rigorous formal semantics for quantitative inference based on subjective\nweights with efficient weight-based pruning for constraint-based systems. The\nsecond approach, called probabilistic constraint logic programming, introduces\na log-linear probability distribution on the proof trees of a constraint logic\nprogram and an algorithm for statistical inference of the parameters and\nproperties of such probability models from incomplete, i.e., unparsed data. The\npossibility of defining arbitrary properties of proof trees as properties of\nthe log-linear probability model and efficiently estimating appropriate\nparameter values for them permits the probabilistic modeling of arbitrary\ncontext-dependencies in constraint logic programs. The usefulness of these\nideas is evaluated empirically in a small-scale experiment on finding the\ncorrect parses of a constraint-based grammar. In addition, we address the\nproblem of computational intractability of the calculation of expectations in\nthe inference task and present various techniques to approximately solve this\ntask. Moreover, we present an approximate heuristic technique for searching for\nthe most probable analysis in probabilistic constraint logic programs."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009003v1", 
    "title": "Automatic Extraction of Subcategorization Frames for Czech", 
    "arxiv-id": "cs/0009003v1", 
    "author": "Daniel Zeman", 
    "publish": "2000-09-08T15:48:53Z", 
    "summary": "We present some novel machine learning techniques for the identification of\nsubcategorization information for verbs in Czech. We compare three different\nstatistical techniques applied to this problem. We show how the learning\nalgorithm can be used to discover previously unknown subcategorization frames\nfrom the Czech Prague Dependency Treebank. The algorithm can then be used to\nlabel dependents of a verb in the Czech treebank as either arguments or\nadjuncts. Using our techniques, we ar able to achieve 88% precision on unseen\nparsed text."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009008v1", 
    "title": "Introduction to the CoNLL-2000 Shared Task: Chunking", 
    "arxiv-id": "cs/0009008v1", 
    "author": "Sabine Buchholz", 
    "publish": "2000-09-18T12:08:54Z", 
    "summary": "We describe the CoNLL-2000 shared task: dividing text into syntactically\nrelated non-overlapping groups of words, so-called text chunking. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009011v1", 
    "title": "Anaphora Resolution in Japanese Sentences Using Surface Expressions and   Examples", 
    "arxiv-id": "cs/0009011v1", 
    "author": "Masaki Murata", 
    "publish": "2000-09-19T00:44:47Z", 
    "summary": "Anaphora resolution is one of the major problems in natural language\nprocessing. It is also one of the important tasks in machine translation and\nman/machine dialogue. We solve the problem by using surface expressions and\nexamples. Surface expressions are the words in sentences which provide clues\nfor anaphora resolution. Examples are linguistic data which are actually used\nin conversations and texts. The method using surface expressions and examples\nis a practical method. This thesis handles almost all kinds of anaphora: i. The\nreferential property and number of a noun phrase ii. Noun phrase direct\nanaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase\nellipsis"
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009015v1", 
    "title": "A Tableaux Calculus for Ambiguous Quantification", 
    "arxiv-id": "cs/0009015v1", 
    "author": "Maarten de Rijke", 
    "publish": "2000-09-20T13:23:17Z", 
    "summary": "Coping with ambiguity has recently received a lot of attention in natural\nlanguage processing. Most work focuses on the semantic representation of\nambiguous expressions. In this paper we complement this work in two ways.\nFirst, we provide an entailment relation for a language with ambiguous\nexpressions. Second, we give a sound and complete tableaux calculus for\nreasoning with statements involving ambiguous quantification. The calculus\ninterleaves partial disambiguation steps with steps in a traditional deductive\nprocess, so as to minimize and postpone branching in the proof process, and\nthereby increases its efficiency."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009025v1", 
    "title": "Parsing with the Shortest Derivation", 
    "arxiv-id": "cs/0009025v1", 
    "author": "Rens Bod", 
    "publish": "2000-09-27T13:22:54Z", 
    "summary": "Common wisdom has it that the bias of stochastic grammars in favor of shorter\nderivations of a sentence is harmful and should be redressed. We show that the\ncommon wisdom is wrong for stochastic grammars that use elementary trees\ninstead of context-free rules, such as Stochastic Tree-Substitution Grammars\nused by Data-Oriented Parsing models. For such grammars a non-probabilistic\nmetric based on the shortest derivation outperforms a probabilistic metric on\nthe ATIS and OVIS corpora, while it obtains very competitive results on the\nWall Street Journal corpus. This paper also contains the first published\nexperiments with DOP on the Wall Street Journal."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0009026v1", 
    "title": "An improved parser for data-oriented lexical-functional analysis", 
    "arxiv-id": "cs/0009026v1", 
    "author": "Rens Bod", 
    "publish": "2000-09-27T13:38:31Z", 
    "summary": "We present an LFG-DOP parser which uses fragments from LFG-annotated\nsentences to parse new sentences. Experiments with the Verbmobil and Homecentre\ncorpora show that (1) Viterbi n best search performs about 100 times faster\nthan Monte Carlo search while both achieve the same accuracy; (2) the DOP\nhypothesis which states that parse accuracy increases with increasing fragment\nsize is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator\nperforms worse than a discounted frequency estimator; and (4) LFG-DOP\nsignificantly outperforms Tree-DOP is evaluated on tree structures only."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010012v1", 
    "title": "Finding consensus in speech recognition: word error minimization and   other applications of confusion networks", 
    "arxiv-id": "cs/0010012v1", 
    "author": "A. Stolcke", 
    "publish": "2000-10-07T02:24:21Z", 
    "summary": "We describe a new framework for distilling information from word lattices to\nimprove the accuracy of speech recognition and obtain a more perspicuous\nrepresentation of a set of alternative hypotheses. In the standard MAP decoding\napproach the recognizer outputs the string of words corresponding to the path\nwith the highest posterior probability given the acoustics and a language\nmodel. However, even given optimal models, the MAP decoder does not necessarily\nminimize the commonly used performance metric, word error rate (WER). We\ndescribe a method for explicitly minimizing WER by extracting word hypotheses\nwith the highest posterior probabilities from word lattices. We change the\nstandard problem formulation by replacing global search over a large set of\nsentence hypotheses with local search over a small set of word candidates. In\naddition to improving the accuracy of the recognizer, our method produces a new\nrepresentation of the set of candidate hypotheses that specifies the sequence\nof word-level confusions in a compact lattice format. We study the properties\nof confusion networks and examine their use for other tasks, such as lattice\ncompression, word spotting, confidence annotation, and reevaluation of\nrecognition hypotheses using higher-level knowledge sources."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010020v1", 
    "title": "Using existing systems to supplement small amounts of annotated   grammatical relations training data", 
    "arxiv-id": "cs/0010020v1", 
    "author": "Alexander Yeh", 
    "publish": "2000-10-11T22:30:09Z", 
    "summary": "Grammatical relationships (GRs) form an important level of natural language\nprocessing, but different sets of GRs are useful for different purposes.\nTherefore, one may often only have time to obtain a small training corpus with\nthe desired GR annotations. To boost the performance from using such a small\ntraining corpus on a transformation rule learner, we use existing systems that\nfind related types of annotations."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010024v1", 
    "title": "Exploring automatic word sense disambiguation with decision lists and   the Web", 
    "arxiv-id": "cs/0010024v1", 
    "author": "David Martinez", 
    "publish": "2000-10-17T08:31:48Z", 
    "summary": "The most effective paradigm for word sense disambiguation, supervised\nlearning, seems to be stuck because of the knowledge acquisition bottleneck. In\nthis paper we take an in-depth study of the performance of decision lists on\ntwo publicly available corpora and an additional corpus automatically acquired\nfrom the Web, using the fine-grained highly polysemous senses in WordNet.\nDecision lists are shown a versatile state-of-the-art technique. The\nexperiments reveal, among other facts, that SemCor can be an acceptable (0.7\nprecision for polysemous words) starting point for an all-words system. The\nresults on the DSO corpus show that for some highly polysemous words 0.7\nprecision seems to be the current state-of-the-art limit. On the other hand,\nindependently constructed hand-tagged corpora are not mutually useful, and a\ncorpus automatically acquired from the Web is shown to fail."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010025v1", 
    "title": "Extraction of semantic relations from a Basque monolingual dictionary   using Constraint Grammar", 
    "arxiv-id": "cs/0010025v1", 
    "author": "Ruben Urizar", 
    "publish": "2000-10-17T09:05:27Z", 
    "summary": "This paper deals with the exploitation of dictionaries for the semi-automatic\nconstruction of lexicons and lexical knowledge bases. The final goal of our\nresearch is to enrich the Basque Lexical Database with semantic information\nsuch as senses, definitions, semantic relations, etc., extracted from a Basque\nmonolingual dictionary. The work here presented focuses on the extraction of\nthe semantic relations that best characterise the headword, that is, those of\nsynonymy, antonymy, hypernymy, and other relations marked by specific relators\nand derivation. All nominal, verbal and adjectival entries were treated. Basque\nuses morphological inflection to mark case, and therefore semantic relations\nhave to be inferred from suffixes rather than from prepositions. Our approach\ncombines a morphological analyser and surface syntax parsing (based on\nConstraint Grammar), and has proven very successful for highly inflected\nlanguages such as Basque. Both the effort to write the rules and the actual\nprocessing time of the dictionary have been very low. At present we have\nextracted 42,533 relations, leaving only 2,943 (9%) definitions without any\nextracted relation. The error rate is extremely low, as only 2.2% of the\nextracted relations are wrong."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010026v1", 
    "title": "Enriching very large ontologies using the WWW", 
    "arxiv-id": "cs/0010026v1", 
    "author": "David Martinez", 
    "publish": "2000-10-17T09:56:54Z", 
    "summary": "This paper explores the possibility to exploit text on the world wide web in\norder to enrich the concepts in existing ontologies. First, a method to\nretrieve documents from the WWW related to a concept is described. These\ndocument collections are used 1) to construct topic signatures (lists of\ntopically related words) for each concept in WordNet, and 2) to build\nhierarchical clusters of the concepts (the word senses) that lexicalize a given\nword. The overall goal is to overcome two shortcomings of WordNet: the lack of\ntopical links among concepts, and the proliferation of senses. Topic signatures\nare validated on a word sense disambiguation task with good results, which are\nimproved when the hierarchical clusters are used."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010027v1", 
    "title": "One Sense per Collocation and Genre/Topic Variations", 
    "arxiv-id": "cs/0010027v1", 
    "author": "Eneko Agirre", 
    "publish": "2000-10-17T10:26:33Z", 
    "summary": "This paper revisits the one sense per collocation hypothesis using\nfine-grained sense distinctions and two different corpora. We show that the\nhypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported\nearlier on 2-way ambiguities). We also show that one sense per collocation does\nhold across corpora, but that collocations vary from one corpus to the other,\nfollowing genre and topic variations. This explains the low results when\nperforming word sense disambiguation across corpora. In fact, we demonstrate\nthat when two independent corpora share a related genre/topic, the word sense\ndisambiguation results would be better. Future work on word sense\ndisambiguation will have to take into account genre and topic as important\nparameters on their models."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0010030v1", 
    "title": "Reduction of Intermediate Alphabets in Finite-State Transducer Cascades", 
    "arxiv-id": "cs/0010030v1", 
    "author": "Andre Kempe", 
    "publish": "2000-10-23T15:14:02Z", 
    "summary": "This article describes an algorithm for reducing the intermediate alphabets\nin cascades of finite-state transducers (FSTs). Although the method modifies\nthe component FSTs, there is no change in the overall relation described by the\nwhole cascade. No additional information or special algorithm, that could\ndecelerate the processing of input, is required at runtime. Two examples from\nNatural Language Processing are used to illustrate the effect of the algorithm\non the sizes of the FSTs and their alphabets. With some FSTs the number of arcs\nand symbols shrank considerably."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011001v1", 
    "title": "Utilizing the World Wide Web as an Encyclopedia: Extracting Term   Descriptions from Semi-Structured Texts", 
    "arxiv-id": "cs/0011001v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2000-11-02T09:02:12Z", 
    "summary": "In this paper, we propose a method to extract descriptions of technical terms\nfrom Web pages in order to utilize the World Wide Web as an encyclopedia. We\nuse linguistic patterns and HTML text structures to extract text fragments\ncontaining term descriptions. We also use a language model to discard\nextraneous descriptions, and a clustering method to summarize resultant\ndescriptions. We show the effectiveness of our method by way of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011002v1", 
    "title": "A Novelty-based Evaluation Method for Information Retrieval", 
    "arxiv-id": "cs/0011002v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2000-11-02T10:00:27Z", 
    "summary": "In information retrieval research, precision and recall have long been used\nto evaluate IR systems. However, given that a number of retrieval systems\nresembling one another are already available to the public, it is valuable to\nretrieve novel relevant documents, i.e., documents that cannot be retrieved by\nthose existing systems. In view of this problem, we propose an evaluation\nmethod that favors systems retrieving as many novel documents as possible. We\nalso used our method to evaluate systems that participated in the IREX\nworkshop."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011003v1", 
    "title": "Applying Machine Translation to Two-Stage Cross-Language Information   Retrieval", 
    "arxiv-id": "cs/0011003v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2000-11-02T10:32:08Z", 
    "summary": "Cross-language information retrieval (CLIR), where queries and documents are\nin different languages, needs a translation of queries and/or documents, so as\nto standardize both of them into a common representation. For this purpose, the\nuse of machine translation is an effective approach. However, computational\ncost is prohibitive in translating large-scale document collections. To resolve\nthis problem, we propose a two-stage CLIR method. First, we translate a given\nquery into the document language, and retrieve a limited number of foreign\ndocuments. Second, we machine translate only those documents into the user\nlanguage, and re-rank them based on the translation result. We also show the\neffectiveness of our method by way of experiments using Japanese queries and\nEnglish technical documents."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011020v2", 
    "title": "The Use of Instrumentation in Grammar Engineering", 
    "arxiv-id": "cs/0011020v2", 
    "author": "Norbert Broeker", 
    "publish": "2000-11-16T17:40:10Z", 
    "summary": "This paper explores the usefulness of a technique from software engineering,\ncode instrumentation, for the development of large-scale natural language\ngrammars. Information about the usage of grammar rules in test and corpus\nsentences is used to improve grammar and testsuite, as well as adapting a\ngrammar to a specific genre. Results show that less than half of a\nlarge-coverage grammar for German is actually tested by two large testsuites,\nand that 10--30% of testing time is redundant. This methodology applied can be\nseen as a re-use of grammar writing knowledge for testsuite compilation."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011034v1", 
    "title": "Semantic interpretation of temporal information by abductive inference", 
    "arxiv-id": "cs/0011034v1", 
    "author": "Frank Van Eynde", 
    "publish": "2000-11-22T15:35:46Z", 
    "summary": "Besides temporal information explicitly available in verbs and adjuncts, the\ntemporal interpretation of a text also depends on general world knowledge and\ndefault assumptions. We will present a theory for describing the relation\nbetween, on the one hand, verbs, their tenses and adjuncts and, on the other,\nthe eventualities and periods of time they represent and their relative\ntemporal locations.\n  The theory is formulated in logic and is a practical implementation of the\nconcepts described in Ness Schelkens et al. We will show how an abductive\nresolution procedure can be used on this representation to extract temporal\ninformation from texts."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011035v1", 
    "title": "Abductive reasoning with temporal information", 
    "arxiv-id": "cs/0011035v1", 
    "author": "Frank Van Eynde", 
    "publish": "2000-11-23T08:41:52Z", 
    "summary": "Texts in natural language contain a lot of temporal information, both\nexplicit and implicit. Verbs and temporal adjuncts carry most of the explicit\ninformation, but for a full understanding general world knowledge and default\nassumptions have to be taken into account. We will present a theory for\ndescribing the relation between, on the one hand, verbs, their tenses and\nadjuncts and, on the other, the eventualities and periods of time they\nrepresent and their relative temporal locations, while allowing interaction\nwith general world knowledge.\n  The theory is formulated in an extension of first order logic and is a\npractical implementation of the concepts described in Van Eynde 2001 and\nSchelkens et al. 2000. We will show how an abductive resolution procedure can\nbe used on this representation to extract temporal information from texts. The\ntheory presented here is an extension of that in Verdoolaege et al. 2000,\nadapted to VanEynde 2001, with a simplified and extended analysis of adjuncts\nand with more emphasis on how a model can be constructed."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0011040v1", 
    "title": "Do All Fragments Count?", 
    "arxiv-id": "cs/0011040v1", 
    "author": "Rens Bod", 
    "publish": "2000-11-24T23:51:21Z", 
    "summary": "We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank. We isolate a number of dependency relations which\nprevious models neglect but which contribute to higher parse accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0102020v1", 
    "title": "Multi-Syllable Phonotactic Modelling", 
    "arxiv-id": "cs/0102020v1", 
    "author": "Anja Belz", 
    "publish": "2001-02-22T21:05:01Z", 
    "summary": "This paper describes a novel approach to constructing phonotactic models. The\nunderlying theoretical approach to phonological description is the\nmultisyllable approach in which multiple syllable classes are defined that\nreflect phonotactically idiosyncratic syllable subcategories. A new\nfinite-state formalism, OFS Modelling, is used as a tool for encoding,\nautomatically constructing and generalising phonotactic descriptions.\nLanguage-independent prototype models are constructed which are instantiated on\nthe basis of data sets of phonological strings, and generalised with a\nclustering algorithm. The resulting approach enables the automatic construction\nof phonotactic models that encode arbitrarily close approximations of a\nlanguage's set of attested phonological forms. The approach is applied to the\nconstruction of multi-syllable word-level phonotactic models for German,\nEnglish and Dutch."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0102021v1", 
    "title": "Taking Primitive Optimality Theory Beyond the Finite State", 
    "arxiv-id": "cs/0102021v1", 
    "author": "Daniel Albro", 
    "publish": "2001-02-22T13:09:25Z", 
    "summary": "Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a\ncomputational model of Optimality Theory (Prince and Smolensky, 1993), employs\na finite state machine to represent the set of active candidates at each stage\nof an Optimality Theoretic derivation, as well as weighted finite state\nmachines to represent the constraints themselves. For some purposes, however,\nit would be convenient if the set of candidates were limited by some set of\ncriteria capable of being described only in a higher-level grammar formalism,\nsuch as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple\nContext Free Grammar (Seki et al., 1991). Examples include reduplication and\nphrasal stress models. Here we introduce a mechanism for OTP-like Optimality\nTheory in which the constraints remain weighted finite state machines, but sets\nof candidates are represented by higher-level grammars. In particular, we use\nmultiple context-free grammars to model reduplication in the manner of\nCorrespondence Theory (McCarthy and Prince, 1995), and develop an extended\nversion of the Earley Algorithm (Earley, 1970) to apply the constraints to a\nreduplicating candidate set."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0102022v2", 
    "title": "Finite-State Phonology: Proceedings of the 5th Workshop of the ACL   Special Interest Group in Computational Phonology (SIGPHON)", 
    "arxiv-id": "cs/0102022v2", 
    "author": "Alain Theriault", 
    "publish": "2001-02-22T14:10:20Z", 
    "summary": "Home page of the workshop proceedings, with pointers to the individually\narchived papers. Includes front matter from the printed version of the\nproceedings."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0102026v1", 
    "title": "Mathematical Model of Word Length on the Basis of the Cebanov-Fucks   Distribution with Uniform Parameter Distribution", 
    "arxiv-id": "cs/0102026v1", 
    "author": "Victor Kromer", 
    "publish": "2001-02-24T06:56:43Z", 
    "summary": "The data on 13 typologically different languages have been processed using a\ntwo-parameter word length model, based on 1-displaced uniform Poisson\ndistribution. Statistical dependencies of the 2nd parameter on the 1st one are\nrevealed for the German texts and genre of letters."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103007v1", 
    "title": "Two-parameter Model of Word Length \"Language - Genre\"", 
    "arxiv-id": "cs/0103007v1", 
    "author": "Victor Kromer", 
    "publish": "2001-03-08T06:30:27Z", 
    "summary": "A two-parameter model of word length measured by the number of syllables\ncomprising it is proposed. The first parameter is dependent on language type,\nthe second one - on text genre and reflects the degree of completion of\nsynergetic processes of language optimization."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103010v1", 
    "title": "Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition   in Japanese and English Sentences", 
    "arxiv-id": "cs/0103010v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-03-12T08:23:19Z", 
    "summary": "George A. Miller said that human beings have only seven chunks in short-term\nmemory, plus or minus two. We counted the number of bunsetsus (phrases) whose\nmodifiees are undetermined in each step of an analysis of the dependency\nstructure of Japanese sentences, and which therefore must be stored in\nshort-term memory. The number was roughly less than nine, the upper bound of\nseven plus or minus two. We also obtained similar results with English\nsentences under the assumption that human beings recognize a series of words,\nsuch as a noun phrase (NP), as a unit. This indicates that if we assume that\nthe human cognitive units in Japanese and English are bunsetsu and NP\nrespectively, analysis will support Miller's $7 \\pm 2$ theory."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103011v1", 
    "title": "A Machine-Learning Approach to Estimating the Referential Properties of   Japanese Noun Phrases", 
    "arxiv-id": "cs/0103011v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-03-12T08:29:59Z", 
    "summary": "The referential properties of noun phrases in the Japanese language, which\nhas no articles, are useful for article generation in Japanese-English machine\ntranslation and for anaphora resolution in Japanese noun phrases. They are\ngenerally classified as generic noun phrases, definite noun phrases, and\nindefinite noun phrases. In the previous work, referential properties were\nestimated by developing rules that used clue words. If two or more rules were\nin conflict with each other, the category having the maximum total score given\nby the rules was selected as the desired category. The score given by each rule\nwas established by hand, so the manpower cost was high. In this work, we\nautomatically adjusted these scores by using a machine-learning method and\nsucceeded in reducing the amount of manpower needed to adjust these scores."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103012v1", 
    "title": "Meaning Sort - Three examples: dictionary construction, tagged corpus   construction, and information presentation system", 
    "arxiv-id": "cs/0103012v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-03-12T08:43:11Z", 
    "summary": "It is often useful to sort words into an order that reflects relations among\ntheir meanings as obtained by using a thesaurus. In this paper, we introduce a\nmethod of arranging words semantically by using several types of `{\\sf is-a}'\nthesauri and a multi-dimensional thesaurus. We also describe three major\napplications where a meaning sort is useful and show the effectiveness of a\nmeaning sort. Since there is no doubt that a word list in meaning-order is\neasier to use than a word list in some random order, a meaning sort, which can\neasily produce a word list in meaning-order, must be useful and effective."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103013v1", 
    "title": "CRL at Ntcir2", 
    "arxiv-id": "cs/0103013v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-03-12T09:36:50Z", 
    "summary": "We have developed systems of two types for NTCIR2. One is an enhenced version\nof the system we developed for NTCIR1 and IREX. It submitted retrieval results\nfor JJ and CC tasks. A variety of parameters were tried with the system. It\nused such characteristics of newspapers as locational information in the CC\ntasks. The system got good results for both of the tasks. The other system is a\nportable system which avoids free parameters as much as possible. The system\nsubmitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system\nautomatically determined the number of top documents and the weight of the\noriginal query used in automatic-feedback retrieval. It also determined\nrelevant terms quite robustly. For EJ and JE tasks, it used document expansion\nto augment the initial queries. It achieved good results, except on the CC\ntasks."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0103026v1", 
    "title": "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense", 
    "arxiv-id": "cs/0103026v1", 
    "author": "Ted Pedersen", 
    "publish": "2001-03-29T23:08:33Z", 
    "summary": "This paper presents a corpus-based approach to word sense disambiguation\nwhere a decision tree assigns a sense to an ambiguous word based on the bigrams\nthat occur nearby. This approach is evaluated using the sense-tagged corpora\nfrom the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate\nthan the average results reported for 30 of 36 words, and is more accurate than\nthe best results for 19 of 36 words."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0104010v1", 
    "title": "Type Arithmetics: Computation based on the theory of types", 
    "arxiv-id": "cs/0104010v1", 
    "author": "Oleg Kiselyov", 
    "publish": "2001-04-03T23:22:17Z", 
    "summary": "The present paper shows meta-programming turn programming, which is rich\nenough to express arbitrary arithmetic computations. We demonstrate a type\nsystem that implements Peano arithmetics, slightly generalized to negative\nnumbers. Certain types in this system denote numerals. Arithmetic operations on\nsuch types-numerals - addition, subtraction, and even division - are expressed\nas type reduction rules executed by a compiler. A remarkable trait is that\ndivision by zero becomes a type error - and reported as such by a compiler."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0104019v1", 
    "title": "Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based   Adaptation", 
    "arxiv-id": "cs/0104019v1", 
    "author": "David Yarowsky", 
    "publish": "2001-04-27T22:50:31Z", 
    "summary": "This paper presents a novel method of generating and applying hierarchical,\ndynamic topic-based language models. It proposes and evaluates new cluster\ngeneration, hierarchical smoothing and adaptive topic-probability estimation\ntechniques. These combined models help capture long-distance lexical\ndependencies. Experiments on the Broadcast News corpus show significant\nimprovement in perplexity (10.5% overall and 33.5% on target vocabulary)."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0104022v1", 
    "title": "Microplanning with Communicative Intentions: The SPUD System", 
    "arxiv-id": "cs/0104022v1", 
    "author": "Martha Palmer", 
    "publish": "2001-04-30T15:12:52Z", 
    "summary": "The process of microplanning encompasses a range of problems in Natural\nLanguage Generation (NLG), such as referring expression generation, lexical\nchoice, and aggregation, problems in which a generator must bridge underlying\ndomain-specific representations and general linguistic representations. In this\npaper, we describe a uniform approach to microplanning based on declarative\nrepresentations of a generator's communicative intent. These representations\ndescribe the results of NLG: communicative intent associates the concrete\nlinguistic structure planned by the generator with inferences that show how the\nmeaning of that structure communicates needed information about some\napplication domain in the current discourse context. Our approach, implemented\nin the SPUD (sentence planning using description) microplanner, uses the\nlexicalized tree-adjoining grammar formalism (LTAG) to connect structure to\nmeaning and uses modal logic programming to connect meaning to context. At the\nsame time, communicative intent representations provide a resource for the\nprocess of NLG. Using representations of communicative intent, a generator can\naugment the syntax, semantics and pragmatics of an incomplete sentence\nsimultaneously, and can assess its progress on the various problems of\nmicroplanning incrementally. The declarative formulation of communicative\nintent translates into a well-defined methodology for designing grammatical and\nconceptual resources which the generator can use to achieve desired\nmicroplanning behavior in a specified domain."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105001v1", 
    "title": "Correction of Errors in a Modality Corpus Used for Machine Translation   by Using Machine-learning Method", 
    "arxiv-id": "cs/0105001v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-05-02T05:29:27Z", 
    "summary": "We performed corpus correction on a modality corpus for machine translation\nby using such machine-learning methods as the maximum-entropy method. We thus\nconstructed a high-quality modality corpus based on corpus correction. We\ncompared several kinds of methods for corpus correction in our experiments and\ndeveloped a good method for corpus correction."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105002v1", 
    "title": "Man [and Woman] vs. Machine: A Case Study in Base Noun Phrase Learning", 
    "arxiv-id": "cs/0105002v1", 
    "author": "Grace Ngai", 
    "publish": "2001-05-02T07:49:14Z", 
    "summary": "A great deal of work has been done demonstrating the ability of machine\nlearning algorithms to automatically extract linguistic knowledge from\nannotated corpora. Very little work has gone into quantifying the difference in\nability at this task between a person and a machine. This paper is a first step\nin that direction."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105005v1", 
    "title": "A Complete WordNet1.5 to WordNet1.6 Mapping", 
    "arxiv-id": "cs/0105005v1", 
    "author": "G. Rigau", 
    "publish": "2001-05-04T08:55:02Z", 
    "summary": "We describe a robust approach for linking already existing lexical/semantic\nhierarchies. We use a constraint satisfaction algorithm (relaxation labelling)\nto select --among a set of candidates-- the node in a target taxonomy that\nbests matches each node in a source taxonomy. In this paper we present the\ncomplete mapping of the nominal, verbal, adjectival and adverbial parts of\nWordNet 1.5 onto WordNet 1.6."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105012v1", 
    "title": "Joint and conditional estimation of tagging and parsing models", 
    "arxiv-id": "cs/0105012v1", 
    "author": "Mark Johnson", 
    "publish": "2001-05-07T14:25:22Z", 
    "summary": "This paper compares two different ways of estimating statistical language\nmodels. Many statistical NLP tagging and parsing models are estimated by\nmaximizing the (joint) likelihood of the fully-observed training data. However,\nsince these applications only require the conditional probability\ndistributions, these distributions can in principle be learnt by maximizing the\nconditional likelihood of the training data. Perhaps somewhat surprisingly,\nmodels estimated by maximizing the joint were superior to models estimated by\nmaximizing the conditional, even though some of the latter models intuitively\nhad access to ``more information''."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105016v1", 
    "title": "Probabilistic top-down parsing and language modeling", 
    "arxiv-id": "cs/0105016v1", 
    "author": "Brian Roark", 
    "publish": "2001-05-08T15:35:07Z", 
    "summary": "This paper describes the functioning of a broad-coverage probabilistic\ntop-down parser, and its application to the problem of language modeling for\nspeech recognition. The paper first introduces key notions in language modeling\nand probabilistic parsing, and briefly reviews some previous approaches to\nusing syntactic structure for language modeling. A lexicalized probabilistic\ntop-down parser is then presented, which performs very well, in terms of both\nthe accuracy of returned parses and the efficiency with which they are found,\nrelative to the best broad-coverage statistical parsers. A new language model\nwhich utilizes probabilistic top-down parsing is then outlined, and empirical\nresults show that it improves upon previous work in test corpus perplexity.\nInterpolation with a trigram model yields an exceptional improvement relative\nto the improvement observed by other models, demonstrating the degree to which\nthe information captured by our parsing model is orthogonal to that captured by\na trigram model. A small recognition experiment also demonstrates the utility\nof the model."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105019v1", 
    "title": "Robust Probabilistic Predictive Syntactic Processing", 
    "arxiv-id": "cs/0105019v1", 
    "author": "Brian Roark", 
    "publish": "2001-05-09T17:01:10Z", 
    "summary": "This thesis presents a broad-coverage probabilistic top-down parser, and its\napplication to the problem of language modeling for speech recognition. The\nparser builds fully connected derivations incrementally, in a single pass from\nleft-to-right across the string. We argue that the parsing approach that we\nhave adopted is well-motivated from a psycholinguistic perspective, as a model\nthat captures probabilistic dependencies between lexical items, as part of the\nprocess of building connected syntactic structures. The basic parser and\nconditional probability models are presented, and empirical results are\nprovided for its parsing accuracy on both newspaper text and spontaneous\ntelephone conversations. Modifications to the probability model are presented\nthat lead to improved performance. A new language model which uses the output\nof the parser is then defined. Perplexity and word error rate reduction are\ndemonstrated over trigram models, even when the trigram is trained on\nsignificantly more data. Interpolation on a word-by-word basis with a trigram\nmodel yields additional improvements."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105023v1", 
    "title": "Generating a 3D Simulation of a Car Accident from a Written Description   in Natural Language: the CarSim System", 
    "arxiv-id": "cs/0105023v1", 
    "author": "Pierre Nugues", 
    "publish": "2001-05-14T09:05:45Z", 
    "summary": "This paper describes a prototype system to visualize and animate 3D scenes\nfrom car accident reports, written in French. The problem of generating such a\n3D simulation can be divided into two subtasks: the linguistic analysis and the\nvirtual scene generation. As a means of communication between these two\nmodules, we first designed a template formalism to represent a written accident\nreport. The CarSim system first processes written reports, gathers relevant\ninformation, and converts it into a formal description. Then, it creates the\ncorresponding 3D scene and animates the vehicles."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105035v1", 
    "title": "Historical Dynamics of Lexical System as Random Walk Process", 
    "arxiv-id": "cs/0105035v1", 
    "author": "Victor Kromer", 
    "publish": "2001-05-30T03:55:23Z", 
    "summary": "It is offered to consider word meanings changes in diachrony as\nsemicontinuous random walk with reflecting and swallowing screens. The basic\ncharacteristics of word life cycle are defined. Verification of the model has\nbeen realized on the data of Russian words distribution on various age periods."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0105037v1", 
    "title": "Integrating Prosodic and Lexical Cues for Automatic Topic Segmentation", 
    "arxiv-id": "cs/0105037v1", 
    "author": "E. Shriberg", 
    "publish": "2001-05-31T18:08:57Z", 
    "summary": "We present a probabilistic model that uses both prosodic and lexical cues for\nthe automatic segmentation of speech into topically coherent units. We propose\ntwo methods for combining lexical and prosodic information using hidden Markov\nmodels and decision trees. Lexical information is obtained from a speech\nrecognizer, and prosodic features are extracted automatically from speech\nwaveforms. We evaluate our approach on the Broadcast News corpus, using the\nDARPA-TDT evaluation metrics. Results show that the prosodic model alone is\ncompetitive with word-based segmentation methods. Furthermore, we achieve a\nsignificant reduction in error by combining the prosodic and word-based\nknowledge sources."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0106015v1", 
    "title": "Organizing Encyclopedic Knowledge based on the Web and its Application   to Question Answering", 
    "arxiv-id": "cs/0106015v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2001-06-10T06:14:09Z", 
    "summary": "We propose a method to generate large-scale encyclopedic knowledge, which is\nvaluable for much NLP research, based on the Web. We first search the Web for\npages containing a term in question. Then we use linguistic patterns and HTML\nstructures to extract text fragments describing the term. Finally, we organize\nextracted term descriptions based on word senses and domains. In addition, we\napply an automatically generated encyclopedia to a question answering system\ntargeting the Japanese Information-Technology Engineers Examination."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0106043v1", 
    "title": "Using the Distribution of Performance for Studying Statistical NLP   Systems and Corpora", 
    "arxiv-id": "cs/0106043v1", 
    "author": "Yuval Krymolowski", 
    "publish": "2001-06-20T14:16:17Z", 
    "summary": "Statistical NLP systems are frequently evaluated and compared on the basis of\ntheir performances on a single split of training and test data. Results\nobtained using a single split are, however, subject to sampling noise. In this\npaper we argue in favour of reporting a distribution of performance figures,\nobtained by resampling the training data, rather than a single number. The\nadditional information from distributions can be used to make statistically\nquantified statements about differences across parameter settings, systems, and\ncorpora."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0106047v1", 
    "title": "Modeling informational novelty in a conversational system with a hybrid   statistical and grammar-based approach to natural language generation", 
    "arxiv-id": "cs/0106047v1", 
    "author": "Adwait Ratnaparkhi", 
    "publish": "2001-06-21T20:37:43Z", 
    "summary": "We present a hybrid statistical and grammar-based system for surface natural\nlanguage generation (NLG) that uses grammar rules, conditions on using those\ngrammar rules, and corpus statistics to determine the word order. We also\ndescribe how this surface NLG module is implemented in a prototype\nconversational system, and how it attempts to model informational novelty by\nvarying the word order. Using a combination of rules and statistical\ninformation, the conversational system expresses the novel information\ndifferently than the given information, based on the run-time dialog state. We\nalso discuss our plans for evaluating the generation strategy."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107005v1", 
    "title": "The Role of Conceptual Relations in Word Sense Disambiguation", 
    "arxiv-id": "cs/0107005v1", 
    "author": "Felisa Verdejo", 
    "publish": "2001-07-03T10:27:44Z", 
    "summary": "We explore many ways of using conceptual distance measures in Word Sense\nDisambiguation, starting with the Agirre-Rigau conceptual density measure. We\nuse a generalized form of this measure, introducing many (parameterized)\nrefinements and performing an exhaustive evaluation of all meaningful\ncombinations. We finally obtain a 42% improvement over the original algorithm,\nand show that measures of conceptual distance are not worse indicators for\nsense disambiguation than measures based on word-coocurrence (exemplified by\nthe Lesk algorithm). Our results, however, reinforce the idea that only a\ncombination of different sources of knowledge might eventually lead to accurate\nword sense disambiguation."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107006v1", 
    "title": "Looking Under the Hood : Tools for Diagnosing your Question Answering   Engine", 
    "arxiv-id": "cs/0107006v1", 
    "author": "Michael Thelen", 
    "publish": "2001-07-03T18:06:05Z", 
    "summary": "In this paper we analyze two question answering tasks : the TREC-8 question\nanswering task and a set of reading comprehension exams. First, we show that\nQ/A systems perform better when there are multiple answer opportunities per\nquestion. Next, we analyze common approaches to two subproblems: term overlap\nfor answer sentence identification, and answer typing for short answer\nextraction. We present general tools for analyzing the strengths and\nlimitations of techniques for these subproblems. Our results quantify the\nlimitations of both term overlap and answer typing to distinguish between\ncompeting answer candidates."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107016v1", 
    "title": "Introduction to the CoNLL-2001 Shared Task: Clause Identification", 
    "arxiv-id": "cs/0107016v1", 
    "author": "Herve Dejean", 
    "publish": "2001-07-15T12:51:01Z", 
    "summary": "We describe the CoNLL-2001 shared task: dividing text into clauses. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107017v1", 
    "title": "Learning Computational Grammars", 
    "arxiv-id": "cs/0107017v1", 
    "author": "Erik F. Tjong Kim Sang", 
    "publish": "2001-07-15T13:21:48Z", 
    "summary": "This paper reports on the \"Learning Computational Grammars\" (LCG) project, a\npostdoc network devoted to studying the application of machine learning\ntechniques to grammars suitable for computational use. We were interested in a\nmore systematic survey to understand the relevance of many factors to the\nsuccess of learning, esp. the availability of annotated data, the kind of\ndependencies in the data, and the availability of knowledge bases (grammars).\nWe focused on syntax, esp. noun phrase (NP) syntax."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107018v1", 
    "title": "Combining a self-organising map with memory-based learning", 
    "arxiv-id": "cs/0107018v1", 
    "author": "Erik F. Tjong Kim Sang", 
    "publish": "2001-07-15T13:32:36Z", 
    "summary": "Memory-based learning (MBL) has enjoyed considerable success in corpus-based\nnatural language processing (NLP) tasks and is thus a reliable method of\ngetting a high-level of performance when building corpus-based NLP systems.\nHowever there is a bottleneck in MBL whereby any novel testing item has to be\ncompared against all the training items in memory base. For this reason there\nhas been some interest in various forms of memory editing whereby some method\nof selecting a subset of the memory base is employed to reduce the number of\ncomparisons. This paper investigates the use of a modified self-organising map\n(SOM) to select a subset of the memory items for comparison. This method\ninvolves reducing the number of comparisons to a value proportional to the\nsquare root of the number of training items. The method is tested on the\nidentification of base noun-phrases in the Wall Street Journal corpus, using\nsections 15 to 18 for training and section 20 for testing."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107019v2", 
    "title": "Applying Natural Language Generation to Indicative Summarization", 
    "arxiv-id": "cs/0107019v2", 
    "author": "Judith L. Klavans", 
    "publish": "2001-07-16T22:59:15Z", 
    "summary": "The task of creating indicative summaries that help a searcher decide whether\nto read a particular document is a difficult task. This paper examines the\nindicative summarization task from a generation perspective, by first analyzing\nits required content via published guidelines and corpus analysis. We show how\nthese summaries can be factored into a set of document features, and how an\nimplemented content planner uses the topicality document feature to create\nindicative multidocument query-based summaries."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107020v1", 
    "title": "Transformation-Based Learning in the Fast Lane", 
    "arxiv-id": "cs/0107020v1", 
    "author": "Radu Florian", 
    "publish": "2001-07-17T15:26:13Z", 
    "summary": "Transformation-based learning has been successfully employed to solve many\nnatural language processing problems. It achieves state-of-the-art performance\non many natural language processing tasks and does not overtrain easily.\nHowever, it does have a serious drawback: the training time is often\nintorelably long, especially on the large corpora which are often used in NLP.\nIn this paper, we present a novel and realistic method for speeding up the\ntraining time of a transformation-based learner without sacrificing\nperformance. The paper compares and contrasts the training time needed and\nperformance achieved by our modified learner with two other systems: a standard\ntransformation-based learner, and the ICA system \\cite{hepple00:tbl}. The\nresults of these experiments show that our system is able to achieve a\nsignificant improvement in training time while still achieving the same\nperformance as a standard transformation-based learner. This is a valuable\ncontribution to systems and algorithms which utilize transformation-based\nlearning at any part of the execution."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0107021v1", 
    "title": "Multidimensional Transformation-Based Learning", 
    "arxiv-id": "cs/0107021v1", 
    "author": "Grace Ngai", 
    "publish": "2001-07-17T15:43:03Z", 
    "summary": "This paper presents a novel method that allows a machine learning algorithm\nfollowing the transformation-based learning paradigm \\cite{brill95:tagging} to\nbe applied to multiple classification tasks by training jointly and\nsimultaneously on all fields. The motivation for constructing such a system\nstems from the observation that many tasks in natural language processing are\nnaturally composed of multiple subtasks which need to be resolved\nsimultaneously; also tasks usually learned in isolation can possibly benefit\nfrom being learned in a joint framework, as the signals for the extra tasks\nusually constitute inductive bias.\n  The proposed algorithm is evaluated in two experiments: in one, the system is\nused to jointly predict the part-of-speech and text chunks/baseNP chunks of an\nEnglish corpus; and in the second it is used to learn the joint prediction of\nword segment boundaries and part-of-speech tagging for Chinese. The results\nshow that the simultaneous learning of multiple tasks does achieve an\nimprovement in each task upon training the same tasks sequentially. The\npart-of-speech tagging result of 96.63% is state-of-the-art for individual\nsystems on the particular train/test split."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0108005v1", 
    "title": "A Bit of Progress in Language Modeling", 
    "arxiv-id": "cs/0108005v1", 
    "author": "Joshua Goodman", 
    "publish": "2001-08-09T19:24:28Z", 
    "summary": "In the past several years, a number of different language modeling\nimprovements over simple trigram models have been found, including caching,\nhigher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and\nclustering. We present explorations of variations on, or of the limits of, each\nof these techniques, including showing that sentence mixture models may have\nmore potential. While all of these techniques have been studied separately,\nthey have rarely been studied in combination. We find some significant\ninteractions, especially with smoothing and clustering techniques. We compare a\ncombination of all techniques together to a Katz smoothed trigram model with no\ncount cutoffs. We achieve perplexity reductions between 38% and 50% (1 bit of\nentropy), depending on training data size, as well as a word error rate\nreduction of 8.9%. Our perplexity reductions are perhaps the highest reported\ncompared to a fair baseline. This is the extended version of the paper; it\ncontains additional details and proofs, and is designed to be a good\nintroduction to the state of the art in language modeling."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0108006v1", 
    "title": "Classes for Fast Maximum Entropy Training", 
    "arxiv-id": "cs/0108006v1", 
    "author": "Joshua Goodman", 
    "publish": "2001-08-09T19:17:58Z", 
    "summary": "Maximum entropy models are considered by many to be one of the most promising\navenues of language modeling research. Unfortunately, long training times make\nmaximum entropy research difficult. We present a novel speedup technique: we\nchange the form of the model to use classes. Our speedup works by creating two\nmaximum entropy models, the first of which predicts the class of each word, and\nthe second of which predicts the word itself. This factoring of the model leads\nto fewer non-zero indicator functions, and faster normalization, achieving\nspeedups of up to a factor of 35 over one of the best previous techniques. It\nalso results in typically slightly lower perplexities. The same trick can be\nused to speed training of other machine learning techniques, e.g. neural\nnetworks, applied to any problem with a large number of outputs, such as\nlanguage modeling."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0108022v1", 
    "title": "Portability of Syntactic Structure for Language Modeling", 
    "arxiv-id": "cs/0108022v1", 
    "author": "Ciprian Chelba", 
    "publish": "2001-08-29T03:59:31Z", 
    "summary": "The paper presents a study on the portability of statistical syntactic\nknowledge in the framework of the structured language model (SLM). We\ninvestigate the impact of porting SLM statistics from the Wall Street Journal\n(WSJ) to the Air Travel Information System (ATIS) domain. We compare this\napproach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data\nand to using a small amount of data manually parsed at UPenn for gathering the\nintial SLM statistics. Surprisingly, despite the fact that it performs modestly\nin perplexity (PPL), the model initialized on WSJ parses outperforms the other\ninitialization methods based on in-domain annotated data, achieving a\nsignificant 0.4% absolute and 7% relative reduction in word error rate (WER)\nover a baseline system whose word error rate is 5.8%; the improvement measured\nrelative to the minimum WER achievable on the N-best lists we worked with is\n12%."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109010v2", 
    "title": "Anaphora and Discourse Structure", 
    "arxiv-id": "cs/0109010v2", 
    "author": "Alistair Knott", 
    "publish": "2001-09-09T16:41:59Z", 
    "summary": "We argue in this paper that many common adverbial phrases generally taken to\nsignal a discourse relation between syntactically connected units within\ndiscourse structure, instead work anaphorically to contribute relational\nmeaning, with only indirect dependence on discourse structure. This allows a\nsimpler discourse structure to provide scaffolding for compositional semantics,\nand reveals multiple ways in which the relational meaning conveyed by adverbial\nconnectives can interact with that associated with discourse structure. We\nconclude by sketching out a lexicalised grammar for discourse that facilitates\ndiscourse interpretation as a product of compositional rules, anaphor\nresolution and inference."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109015v1", 
    "title": "Boosting Trees for Anti-Spam Email Filtering", 
    "arxiv-id": "cs/0109015v1", 
    "author": "Lluis Marquez", 
    "publish": "2001-09-13T14:52:41Z", 
    "summary": "This paper describes a set of comparative experiments for the problem of\nautomatically filtering unwanted electronic mail messages. Several variants of\nthe AdaBoost algorithm with confidence-rated predictions [Schapire & Singer,\n99] have been applied, which differ in the complexity of the base learners\nconsidered. Two main conclusions can be drawn from our experiments: a) The\nboosting-based methods clearly outperform the baseline learning algorithms\n(Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very\nhigh levels of the F1 measure; b) Increasing the complexity of the base\nlearners allows to obtain better ``high-precision'' classifiers, which is a\nvery important issue when misclassification costs are considered."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109020v1", 
    "title": "Modelling Semantic Association and Conceptual Inheritance for Semantic   Analysis", 
    "arxiv-id": "cs/0109020v1", 
    "author": "Pascal Vaillant", 
    "publish": "2001-09-15T22:44:55Z", 
    "summary": "Allowing users to interact through language borders is an interesting\nchallenge for information technology. For the purpose of a computer assisted\nlanguage learning system, we have chosen icons for representing meaning on the\ninput interface, since icons do not depend on a particular language. However, a\nkey limitation of this type of communication is the expression of articulated\nideas instead of isolated concepts. We propose a method to interpret sequences\nof icons as complex messages by reconstructing the relations between concepts,\nso as to build conceptual graphs able to represent meaning and to be used for\nnatural language sentence generation. This method is based on an electronic\ndictionary containing semantic information."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109029v1", 
    "title": "Learning class-to-class selectional preferences", 
    "arxiv-id": "cs/0109029v1", 
    "author": "D. Martinez", 
    "publish": "2001-09-18T14:00:27Z", 
    "summary": "Selectional preference learning methods have usually focused on word-to-class\nrelations, e.g., a verb selects as its subject a given nominal class. This\npapers extends previous statistical models to class-to-class preferences, and\npresents a model that learns selectional preferences for classes of verbs. The\nmotivation is twofold: different senses of a verb may have different\npreferences, and some classes of verbs can share preferences. The model is\ntested on a word sense disambiguation task which uses subject-verb and\nobject-verb relationships extracted from a small sense-disambiguated corpus."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109030v1", 
    "title": "Knowledge Sources for Word Sense Disambiguation", 
    "arxiv-id": "cs/0109030v1", 
    "author": "David Martinez", 
    "publish": "2001-09-18T14:14:52Z", 
    "summary": "Two kinds of systems have been defined during the long history of WSD:\nprincipled systems that define which knowledge types are useful for WSD, and\nrobust systems that use the information sources at hand, such as, dictionaries,\nlight-weight ontologies or hand-tagged corpora. This paper tries to systematize\nthe relation between desired knowledge types and actual information sources. We\nalso compare the results for a wide range of algorithms that have been\nevaluated on a common test setting in our research group. We hope that this\nanalysis will help change the shift from systems based on information sources\nto systems based on knowledge sources. This study might also shed some light on\nsemi-automatic acquisition of desired knowledge types from existing resources."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109031v2", 
    "title": "Enriching WordNet concepts with topic signatures", 
    "arxiv-id": "cs/0109031v2", 
    "author": "David Martinez", 
    "publish": "2001-09-18T14:18:58Z", 
    "summary": "This paper explores the possibility of enriching the content of existing\nontologies. The overall goal is to overcome the lack of topical links among\nconcepts in WordNet. Each concept is to be associated to a topic signature,\ni.e., a set of related words with associated weights. The signatures can be\nautomatically constructed from the WWW or from sense-tagged corpora. Both\napproaches are compared and evaluated on a word sense disambiguation task. The\nresults show that it is possible to construct clean signatures from the WWW\nusing some filtering techniques."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0109039v1", 
    "title": "Testing for Mathematical Lineation in Jim Crace's \"Quarantine\" and T. S.   Eliot's \"Four Quartets\"", 
    "arxiv-id": "cs/0109039v1", 
    "author": "Hideaki Aoyama", 
    "publish": "2001-09-20T06:42:11Z", 
    "summary": "The mathematical distinction between prose and verse may be detected in\nwritings that are not apparently lineated, for example in T. S. Eliot's \"Burnt\nNorton\", and Jim Crace's \"Quarantine\". In this paper we offer comments on\nappropriate statistical methods for such work, and also on the nature of formal\ninnovation in these two texts. Additional remarks are made on the roots of\nlineation as a metrical form, and on the prose-verse continuum."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0110015v1", 
    "title": "Richer Syntactic Dependencies for Structured Language Modeling", 
    "arxiv-id": "cs/0110015v1", 
    "author": "Peng Xu", 
    "publish": "2001-10-03T18:34:36Z", 
    "summary": "The paper investigates the use of richer syntactic dependencies in the\nstructured language model (SLM). We present two simple methods of enriching the\ndependencies in the syntactic parse trees used for intializing the SLM. We\nevaluate the impact of both methods on the perplexity (PPL) and\nword-error-rate(WER, N-best rescoring) performance of the SLM. We show that the\nnew model achieves an improvement in PPL and WER over the baseline results\nreported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)\ncorpora, respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0110027v1", 
    "title": "Part-of-Speech Tagging with Two Sequential Transducers", 
    "arxiv-id": "cs/0110027v1", 
    "author": "Andre Kempe", 
    "publish": "2001-10-11T11:29:49Z", 
    "summary": "We present a method of constructing and using a cascade consisting of a left-\nand a right-sequential finite-state transducer (FST), T1 and T2, for\npart-of-speech (POS) disambiguation. Compared to an HMM, this FST cascade has\nthe advantage of significantly higher processing speed, but at the cost of\nslightly lower accuracy. Applications such as Information Retrieval, where the\nspeed can be more important than accuracy, could benefit from this approach.\n  In the process of tagging, we first assign every word a unique ambiguity\nclass c_i that can be looked up in a lexicon encoded by a sequential FST. Every\nc_i is denoted by a single symbol, e.g. [ADJ_NOUN], although it represents a\nset of alternative tags that a given word can occur with. The sequence of the\nc_i of all words of one sentence is the input to our FST cascade. It is mapped\nby T1, from left to right, to a sequence of reduced ambiguity classes r_i.\nEvery r_i is denoted by a single symbol, although it represents a set of\nalternative tags. Intuitively, T1 eliminates the less likely tags from c_i,\nthus creating r_i. Finally, T2 maps the sequence of r_i, from right to left, to\na sequence of single POS tags t_i. Intuitively, T2 selects the most likely t_i\nfrom every r_i.\n  The probabilities of all t_i, r_i, and c_i are used only at compile time, not\nat run time. They do not (directly) occur in the FSTs, but are \"implicitly\ncontained\" in their structure."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0110050v1", 
    "title": "What is the minimal set of fragments that achieves maximal parse   accuracy?", 
    "arxiv-id": "cs/0110050v1", 
    "author": "Rens Bod", 
    "publish": "2001-10-24T11:01:08Z", 
    "summary": "We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank (a precision of 90.8% and a recall of 90.6%). We\nisolate some dependency relations which previous models neglect but which\ncontribute to higher parse accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0110051v1", 
    "title": "Combining semantic and syntactic structure for language modeling", 
    "arxiv-id": "cs/0110051v1", 
    "author": "Rens Bod", 
    "publish": "2001-10-24T11:30:50Z", 
    "summary": "Structured language models for speech recognition have been shown to remedy\nthe weaknesses of n-gram models. All current structured language models are,\nhowever, limited in that they do not take into account dependencies between\nnon-headwords. We show that non-headword dependencies contribute to\nsignificantly improved word error rate, and that a data-oriented parsing model\ntrained on semantically and syntactically annotated data can exploit these\ndependencies. This paper also contains the first DOP model trained by means of\na maximum likelihood reestimation procedure, which solves some of the\ntheoretical shortcomings of previous DOP models."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0111064v1", 
    "title": "A procedure for unsupervised lexicon learning", 
    "arxiv-id": "cs/0111064v1", 
    "author": "Anand Venkataraman", 
    "publish": "2001-11-30T20:30:52Z", 
    "summary": "We describe an incremental unsupervised procedure to learn words from\ntranscribed continuous speech. The algorithm is based on a conservative and\ntraditional statistical model, and results of empirical tests show that it is\ncompetitive with other algorithms that have been proposed recently for this\ntask."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0111065v1", 
    "title": "A Statistical Model for Word Discovery in Transcribed Speech", 
    "arxiv-id": "cs/0111065v1", 
    "author": "Anand Venkataraman", 
    "publish": "2001-11-30T20:40:50Z", 
    "summary": "A statistical model for segmentation and word discovery in continuous speech\nis presented. An incremental unsupervised learning algorithm to infer word\nboundaries based on this model is described. Results of empirical tests showing\nthat the algorithm is competitive with other models that have been used for\nsimilar tasks are also presented."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0112003v1", 
    "title": "Using a Support-Vector Machine for Japanese-to-English Translation of   Tense, Aspect, and Modality", 
    "arxiv-id": "cs/0112003v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-12-05T05:35:07Z", 
    "summary": "This paper describes experiments carried out using a variety of\nmachine-learning methods, including the k-nearest neighborhood method that was\nused in a previous study, for the translation of tense, aspect, and modality.\nIt was found that the support-vector machine method was the most precise of all\nthe methods tested."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0112004v1", 
    "title": "Part of Speech Tagging in Thai Language Using Support Vector Machine", 
    "arxiv-id": "cs/0112004v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-12-05T05:48:21Z", 
    "summary": "The elastic-input neuro tagger and hybrid tagger, combined with a neural\nnetwork and Brill's error-driven learning, have already been proposed for the\npurpose of constructing a practical tagger using as little training data as\npossible. When a small Thai corpus is used for training, these taggers have\ntagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words\nin terms of the part of speech), respectively. In this study, in order to\nconstruct more accurate taggers we developed new tagging methods using three\nmachine learning methods: the decision-list, maximum entropy, and support\nvector machine methods. We then performed tagging experiments by using these\nmethods. Our results showed that the support vector machine method has the best\nprecision (96.1%), and that it is capable of improving the accuracy of tagging\nin the Thai language. Finally, we theoretically examined all these methods and\ndiscussed how the improvements were achived."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0112005v1", 
    "title": "Universal Model for Paraphrasing -- Using Transformation Based on a   Defined Criteria --", 
    "arxiv-id": "cs/0112005v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2001-12-05T05:56:13Z", 
    "summary": "This paper describes a universal model for paraphrasing that transforms\naccording to defined criteria. We showed that by using different criteria we\ncould construct different kinds of paraphrasing systems including one for\nanswering questions, one for compressing sentences, one for polishing up, and\none for transforming written language to spoken language."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204003v1", 
    "title": "Blind Normalization of Speech From Different Channels and Speakers", 
    "arxiv-id": "cs/0204003v1", 
    "author": "David N. Levin", 
    "publish": "2002-04-02T21:14:40Z", 
    "summary": "This paper describes representations of time-dependent signals that are\ninvariant under any invertible time-independent transformation of the signal\ntime series. Such a representation is created by rescaling the signal in a\nnon-linear dynamic manner that is determined by recently encountered signal\nlevels. This technique may make it possible to normalize signals that are\nrelated by channel-dependent and speaker-dependent transformations, without\nhaving to characterize the form of the signal transformations, which remain\nunknown. The technique is illustrated by applying it to the time-dependent\nspectra of speech that has been filtered to simulate the effects of different\nchannels. The experimental results show that the rescaled speech\nrepresentations are largely normalized (i.e., channel-independent), despite the\nchannel-dependence of the raw (unrescaled) speech."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204007v1", 
    "title": "An Integrated Framework for Treebanks and Multilayer Annotations", 
    "arxiv-id": "cs/0204007v1", 
    "author": "Steven Bird", 
    "publish": "2002-04-03T18:55:01Z", 
    "summary": "Treebank formats and associated software tools are proliferating rapidly,\nwith little consideration for interoperability. We survey a wide variety of\ntreebank structures and operations, and show how they can be mapped onto the\nannotation graph model, and leading to an integrated framework encompassing\ntree and non-tree annotations alike. This development opens up new\npossibilities for managing and exploiting multilayer annotations."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204022v1", 
    "title": "Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure   for Interdisciplinary Education, Research and Development", 
    "arxiv-id": "cs/0204022v1", 
    "author": "Steven Bird", 
    "publish": "2002-04-10T15:37:26Z", 
    "summary": "Annotation graphs and annotation servers offer infrastructure to support the\nanalysis of human language resources in the form of time-series data such as\ntext, audio and video. This paper outlines areas of common need among empirical\nlinguists and computational linguists. After reviewing examples of data and\ntools used or under development for each of several areas, it proposes a common\nframework for future tool development, data annotation and resource sharing\nbased upon annotation graphs and servers."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204023v1", 
    "title": "Computational Phonology", 
    "arxiv-id": "cs/0204023v1", 
    "author": "Steven Bird", 
    "publish": "2002-04-10T15:49:24Z", 
    "summary": "Phonology, as it is practiced, is deeply computational. Phonological analysis\nis data-intensive and the resulting models are nothing other than specialized\ndata structures and algorithms. In the past, phonological computation -\nmanaging data and developing analyses - was done manually with pencil and\npaper. Increasingly, with the proliferation of affordable computers, IPA fonts\nand drawing software, phonologists are seeking to move their computation work\nonline. Computational Phonology provides the theoretical and technological\nframework for this migration, building on methodologies and tools from\ncomputational linguistics. This piece consists of an apology for computational\nphonology, a history, and an overview of current research."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204025v1", 
    "title": "Phonology", 
    "arxiv-id": "cs/0204025v1", 
    "author": "Steven Bird", 
    "publish": "2002-04-11T11:22:43Z", 
    "summary": "Phonology is the systematic study of the sounds used in language, their\ninternal structure, and their composition into syllables, words and phrases.\nComputational phonology is the application of formal and computational\ntechniques to the representation and processing of phonological information.\nThis chapter will present the fundamentals of descriptive phonology along with\na brief overview of computational phonology."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204027v1", 
    "title": "Integrating selectional preferences in WordNet", 
    "arxiv-id": "cs/0204027v1", 
    "author": "David Martinez", 
    "publish": "2002-04-11T16:41:28Z", 
    "summary": "Selectional preference learning methods have usually focused on word-to-class\nrelations, e.g., a verb selects as its subject a given nominal class. This\npaper extends previous statistical models to class-to-class preferences, and\npresents a model that learns selectional preferences for classes of verbs,\ntogether with an algorithm to integrate the learned preferences in WordNet. The\ntheoretical motivation is twofold: different senses of a verb may have\ndifferent preferences, and classes of verbs may share preferences. On the\npractical side, class-to-class selectional preferences can be learned from\nuntagged corpora (the same as word-to-class), they provide selectional\npreferences for less frequent word senses via inheritance, and more important,\nthey allow for easy integration in WordNet. The model is trained on\nsubject-verb and object-verb relationships extracted from a small corpus\ndisambiguated with WordNet senses. Examples are provided illustrating that the\ntheoretical motivations are well founded, and showing that the approach is\nfeasible. Experimental results on a word sense disambiguation task are also\nprovided."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204028v1", 
    "title": "Decision Lists for English and Basque", 
    "arxiv-id": "cs/0204028v1", 
    "author": "David Martinez", 
    "publish": "2002-04-12T07:41:55Z", 
    "summary": "In this paper we describe the systems we developed for the English (lexical\nand all-words) and Basque tasks. They were all supervised systems based on\nYarowsky's Decision Lists. We used Semcor for training in the English all-words\ntask. We defined different feature sets for each language. For Basque, in order\nto extract all the information from the text, we defined features that have not\nbeen used before in the literature, using a morphological analyzer. We also\nimplemented systems that selected automatically good features and were able to\nobtain a prefixed precision (85%) at the cost of coverage. The systems that\nused all the features were identified as BCU-ehu-dlist-all and the systems that\nselected some features as BCU-ehu-dlist-best."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204029v1", 
    "title": "The Basque task: did systems perform in the upperbound?", 
    "arxiv-id": "cs/0204029v1", 
    "author": "Eli Pociello", 
    "publish": "2002-04-12T07:49:32Z", 
    "summary": "In this paper we describe the Senseval 2 Basque lexical-sample task. The task\ncomprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal\nHiztegia, the main Basque dictionary. Most examples were taken from the\nEgunkaria newspaper. The method used to hand-tag the examples produced low\ninter-tagger agreement (75%) before arbitration. The four competing systems\nattained results well above the most frequent baseline and the best system\nscored 75% precision at 100% coverage. The paper includes an analysis of the\ntagging procedure used, as well as the performance of the competing systems. In\nparticular, we argue that inter-tagger agreement is not a real upperbound for\nthe Basque WSD task."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0204049v1", 
    "title": "Memory-Based Shallow Parsing", 
    "arxiv-id": "cs/0204049v1", 
    "author": "Erik F. Tjong Kim Sang", 
    "publish": "2002-04-24T14:48:31Z", 
    "summary": "We present memory-based learning approaches to shallow parsing and apply\nthese to five tasks: base noun phrase identification, arbitrary base phrase\nrecognition, clause detection, noun phrase parsing and full parsing. We use\nfeature selection techniques and system combination methods for improving the\nperformance of the memory-based learner. Our approach is evaluated on standard\ndata sets and the results are compared with that of other systems. This reveals\nthat our approach works well for base phrase identification while its\napplication towards recognizing embedded structures leaves some room for\nimprovement."
},{
    "category": "cs.CL", 
    "doi": "10.1098/rsta.2000.0587", 
    "link": "http://arxiv.org/pdf/cs/0205006v1", 
    "title": "Unsupervised discovery of morphologically related words based on   orthographic and semantic similarity", 
    "arxiv-id": "cs/0205006v1", 
    "author": "Harald Trost", 
    "publish": "2002-05-08T14:39:19Z", 
    "summary": "We present an algorithm that takes an unannotated corpus as its input, and\nreturns a ranked list of probable morphologically related pairs as its output.\nThe algorithm tries to discover morphologically related pairs by looking for\npairs that are both orthographically and semantically similar, where\northographic similarity is measured in terms of minimum edit distance, and\nsemantic similarity is measured in terms of mutual information. The procedure\ndoes not rely on a morpheme concatenation model, nor on distributional\nproperties of word substrings (such as affix frequency). Experiments with\nGerman and English input give encouraging results, both in terms of precision\n(proportion of good pairs found at various cutoff points of the ranked list),\nand in terms of a qualitative analysis of the types of morphological patterns\ndiscovered by the algorithm."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205009v1", 
    "title": "Mostly-Unsupervised Statistical Segmentation of Japanese Kanji Sequences", 
    "arxiv-id": "cs/0205009v1", 
    "author": "Lillian Lee", 
    "publish": "2002-05-10T18:55:25Z", 
    "summary": "Given the lack of word delimiters in written Japanese, word segmentation is\ngenerally considered a crucial first step in processing Japanese texts. Typical\nJapanese segmentation algorithms rely either on a lexicon and syntactic\nanalysis or on pre-segmented data; but these are labor-intensive, and the\nlexico-syntactic techniques are vulnerable to the unknown word problem. In\ncontrast, we introduce a novel, more robust statistical method utilizing\nunsegmented training data. Despite its simplicity, the algorithm yields\nperformance on long kanji sequences comparable to and sometimes surpassing that\nof state-of-the-art morphological analyzers over a variety of error metrics.\nThe algorithm also outperforms another mostly-unsupervised statistical\nalgorithm previously proposed for Chinese.\n  Additionally, we present a two-level annotation scheme for Japanese to\nincorporate multiple segmentation granularities, and introduce two novel\nevaluation metrics, both based on the notion of a compatible bracket, that can\naccount for multiple granularities simultaneously."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205017v1", 
    "title": "Ellogon: A New Text Engineering Platform", 
    "arxiv-id": "cs/0205017v1", 
    "author": "Constantine D. Spyropoulos", 
    "publish": "2002-05-13T11:18:08Z", 
    "summary": "This paper presents Ellogon, a multi-lingual, cross-platform, general-purpose\ntext engineering environment. Ellogon was designed in order to aid both\nresearchers in natural language processing, as well as companies that produce\nlanguage engineering systems for the end-user. Ellogon provides a powerful\nTIPSTER-based infrastructure for managing, storing and exchanging textual data,\nembedding and managing text processing components as well as visualising\ntextual data and their associated linguistic information. Among its key\nfeatures are full Unicode support, an extensive multi-lingual graphical user\ninterface, its modular architecture and the reduced hardware requirements."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205027v1", 
    "title": "A variable-free dynamic semantics", 
    "arxiv-id": "cs/0205027v1", 
    "author": "Chung-chieh Shan", 
    "publish": "2002-05-17T09:33:52Z", 
    "summary": "I propose a variable-free treatment of dynamic semantics. By \"dynamic\nsemantics\" I mean analyses of donkey sentences (\"Every farmer who owns a donkey\nbeats it\") and other binding and anaphora phenomena in natural language where\nmeanings of constituents are updates to information states, for instance as\nproposed by Groenendijk and Stokhof. By \"variable-free\" I mean denotational\nsemantics in which functional combinators replace variable indices and\nassignment functions, for instance as advocated by Jacobson.\n  The new theory presented here achieves a compositional treatment of dynamic\nanaphora that does not involve assignment functions, and separates the\ncombinatorics of variable-free semantics from the particular linguistic\nphenomena it treats. Integrating variable-free semantics and dynamic semantics\ngives rise to interactions that make new empirical predictions, for example\n\"donkey weak crossover\" effects."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205028v1", 
    "title": "NLTK: The Natural Language Toolkit", 
    "arxiv-id": "cs/0205028v1", 
    "author": "Steven Bird", 
    "publish": "2002-05-17T12:51:00Z", 
    "summary": "NLTK, the Natural Language Toolkit, is a suite of open source program\nmodules, tutorials and problem sets, providing ready-to-use computational\nlinguistics courseware. NLTK covers symbolic and statistical natural language\nprocessing, and is interfaced to annotated corpora. Students augment and\nreplace existing components, learn structured programming by example, and\nmanipulate sophisticated models from the outset."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205057v1", 
    "title": "Unsupervised Discovery of Morphemes", 
    "arxiv-id": "cs/0205057v1", 
    "author": "Krista Lagus", 
    "publish": "2002-05-21T14:37:22Z", 
    "summary": "We present two methods for unsupervised segmentation of words into\nmorpheme-like units. The model utilized is especially suited for languages with\na rich morphology, such as Finnish. The first method is based on the Minimum\nDescription Length (MDL) principle and works online. In the second method,\nMaximum Likelihood (ML) optimization is used. The quality of the segmentations\nis measured using an evaluation method that compares the segmentations produced\nto an existing morphological analysis. Experiments on both Finnish and English\ncorpora show that the presented methods perform well compared to a current\nstate-of-the-art system."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205065v1", 
    "title": "Bootstrapping Lexical Choice via Multiple-Sequence Alignment", 
    "arxiv-id": "cs/0205065v1", 
    "author": "Lillian Lee", 
    "publish": "2002-05-25T21:32:09Z", 
    "summary": "An important component of any generation system is the mapping dictionary, a\nlexicon of elementary semantic expressions and corresponding natural language\nrealizations. Typically, labor-intensive knowledge-based methods are used to\nconstruct the dictionary. We instead propose to acquire it automatically via a\nnovel multiple-pass algorithm employing multiple-sequence alignment, a\ntechnique commonly used in bioinformatics. Crucially, our method leverages\nlatent information contained in multi-parallel corpora -- datasets that supply\nseveral verbalizations of the corresponding semantics rather than just one.\n  We used our techniques to generate natural language versions of\ncomputer-generated mathematical proofs, with good results on both a\nper-component and overall-output basis. For example, in evaluations involving a\ndozen human judges, our system produced output whose readability and\nfaithfulness to the semantic input rivaled that of a traditional generation\nsystem."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205067v1", 
    "title": "Evaluating the Effectiveness of Ensembles of Decision Trees in   Disambiguating Senseval Lexical Samples", 
    "arxiv-id": "cs/0205067v1", 
    "author": "Ted Pedersen", 
    "publish": "2002-05-27T18:42:10Z", 
    "summary": "This paper presents an evaluation of an ensemble--based system that\nparticipated in the English and Spanish lexical sample tasks of Senseval-2. The\nsystem combines decision trees of unigrams, bigrams, and co--occurrences into a\nsingle classifier. The analysis is extended to include the Senseval-1 data."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205068v1", 
    "title": "Assessing System Agreement and Instance Difficulty in the Lexical Sample   Tasks of Senseval-2", 
    "arxiv-id": "cs/0205068v1", 
    "author": "Ted Pedersen", 
    "publish": "2002-05-27T18:49:01Z", 
    "summary": "This paper presents a comparative evaluation among the systems that\nparticipated in the Spanish and English lexical sample tasks of Senseval-2. The\nfocus is on pairwise comparisons among systems to assess the degree to which\nthey agree, and on measuring the difficulty of the test instances included in\nthese tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0205069v1", 
    "title": "Machine Learning with Lexical Features: The Duluth Approach to   Senseval-2", 
    "arxiv-id": "cs/0205069v1", 
    "author": "Ted Pedersen", 
    "publish": "2002-05-27T18:57:11Z", 
    "summary": "This paper describes the sixteen Duluth entries in the Senseval-2 comparative\nexercise among word sense disambiguation systems. There were eight pairs of\nDuluth systems entered in the Spanish and English lexical sample tasks. These\nare all based on standard machine learning algorithms that induce classifiers\nfrom sense-tagged training text where the context in which ambiguous words\noccur are represented by simple lexical features. These are highly portable,\nrobust methods that can serve as a foundation for more tailored approaches."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0206014v1", 
    "title": "A Method for Open-Vocabulary Speech-Driven Text Retrieval", 
    "arxiv-id": "cs/0206014v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-09T10:07:36Z", 
    "summary": "While recent retrieval techniques do not limit the number of index terms,\nout-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at\nretrieving information with spoken queries, we fill the gap between speech\nrecognition and text retrieval in terms of the vocabulary size. Given a spoken\nquery, we generate a transcription and detect OOV words through speech\nrecognition. We then correspond detected OOV words to terms indexed in a target\ncollection to complete the transcription, and search the collection for\ndocuments relevant to the completed transcription. We show the effectiveness of\nour method by way of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0206015v1", 
    "title": "Japanese/English Cross-Language Information Retrieval: Exploration of   Query Translation and Transliteration", 
    "arxiv-id": "cs/0206015v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-09T10:53:49Z", 
    "summary": "Cross-language information retrieval (CLIR), where queries and documents are\nin different languages, has of late become one of the major topics within the\ninformation retrieval community. This paper proposes a Japanese/English CLIR\nsystem, where we combine a query translation and retrieval modules. We\ncurrently target the retrieval of technical documents, and therefore the\nperformance of our system is highly dependent on the quality of the translation\nof technical terms. However, the technical term translation is still\nproblematic in that technical terms are often compound words, and thus new\nterms are progressively created by combining existing base words. In addition,\nJapanese often represents loanwords based on its special phonogram.\nConsequently, existing dictionaries find it difficult to achieve sufficient\ncoverage. To counter the first problem, we produce a Japanese/English\ndictionary for base words, and translate compound words on a word-by-word\nbasis. We also use a probabilistic method to resolve translation ambiguity. For\nthe second problem, we use a transliteration method, which corresponds words\nunlisted in the base word dictionary to their phonetic equivalents in the\ntarget language. We evaluate our system using a test collection for CLIR, and\nshow that both the compound word translation and transliteration methods\nimprove the system performance."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0206030v1", 
    "title": "A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero   Pronoun Detection and Resolution", 
    "arxiv-id": "cs/0206030v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-20T07:13:59Z", 
    "summary": "This paper proposes a method to analyze Japanese anaphora, in which zero\npronouns (omitted obligatory cases) are used to refer to preceding entities\n(antecedents). Unlike the case of general coreference resolution, zero pronouns\nhave to be detected prior to resolution because they are not expressed in\ndiscourse. Our method integrates two probability parameters to perform zero\npronoun detection and resolution in a single framework. The first parameter\nquantifies the degree to which a given case is a zero pronoun. The second\nparameter quantifies the degree to which a given entity is the antecedent for a\ndetected zero pronoun. To compute these parameters efficiently, we use corpora\nwith/without annotations of anaphoric relations. We show the effectiveness of\nour method by way of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0206034v1", 
    "title": "Applying a Hybrid Query Translation Method to Japanese/English   Cross-Language Patent Retrieval", 
    "arxiv-id": "cs/0206034v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-24T07:46:06Z", 
    "summary": "This paper applies an existing query translation method to cross-language\npatent retrieval. In our method, multiple dictionaries are used to derive all\npossible translations for an input query, and collocational statistics are used\nto resolve translation ambiguity. We used Japanese/English parallel patent\nabstracts to perform comparative experiments, where our method outperformed a\nsimple dictionary-based query translation method, and achieved 76% of\nmonolingual retrieval in terms of average precision."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S1351324902002954", 
    "link": "http://arxiv.org/pdf/cs/0206035v1", 
    "title": "PRIME: A System for Multi-lingual Patent Retrieval", 
    "arxiv-id": "cs/0206035v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-24T08:00:45Z", 
    "summary": "Given the growing number of patents filed in multiple countries, users are\ninterested in retrieving patents across languages. We propose a multi-lingual\npatent retrieval system, which translates a user query into the target\nlanguage, searches a multilingual database for patents relevant to the query,\nand improves the browsing efficiency by way of machine translation and\nclustering. Our system also extracts new translations from patent families\nconsisting of comparable patents, to enhance the translation dictionary."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0206036v1", 
    "title": "Language Modeling for Multi-Domain Speech-Driven Text Retrieval", 
    "arxiv-id": "cs/0206036v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-24T08:38:36Z", 
    "summary": "We report experimental results associated with speech-driven text retrieval,\nwhich facilitates retrieving information in multiple domains with spoken\nqueries. Since users speak contents related to a target collection, we produce\nlanguage models used for speech recognition based on the target collection, so\nas to improve both the recognition and retrieval accuracy. Experiments using\nexisting test collections combined with dictated queries showed the\neffectiveness of our method."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0206037v1", 
    "title": "Speech-Driven Text Retrieval: Using Target IR Collections for   Statistical Language Model Adaptation in Speech Recognition", 
    "arxiv-id": "cs/0206037v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2002-06-24T10:28:02Z", 
    "summary": "Speech recognition has of late become a practical technology for real world\napplications. Aiming at speech-driven text retrieval, which facilitates\nretrieving information with spoken queries, we propose a method to integrate\nspeech recognition and retrieval methods. Since users speak contents related to\na target collection, we adapt statistical language models used for speech\nrecognition based on the target collection, so as to improve both the\nrecognition and retrieval accuracy. Experiments using existing test collections\ncombined with dictated queries showed the effectiveness of our method."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0207002v1", 
    "title": "Using eigenvectors of the bigram graph to infer morpheme identity", 
    "arxiv-id": "cs/0207002v1", 
    "author": "John Goldsmith", 
    "publish": "2002-07-02T01:15:33Z", 
    "summary": "This paper describes the results of some experiments exploring statistical\nmethods to infer syntactic behavior of words and morphemes from a raw corpus in\nan unsupervised fashion. It shares certain points in common with Brown et al\n(1992) and work that has grown out of that: it employs statistical techniques\nto analyze syntactic behavior based on what words occur adjacent to a given\nword. However, we use an eigenvector decomposition of a nearest-neighbor graph\nto produce a two-dimensional rendering of the words of a corpus in which words\nof the same syntactic category tend to form neighborhoods. We exploit this\ntechnique for extending the value of automatic learning of morphology. In\nparticular, we look at the suffixes derived from a corpus by unsupervised\nlearning of morphology, and we ask which of these suffixes have a consistent\nsyntactic function (e.g., in English, -tion is primarily a mark of nouns, but\n-s marks both noun plurals and 3rd person present on verbs), and we determine\nthat this method works well for this task."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0207003v1", 
    "title": "Analysis of Titles and Readers For Title Generation Centered on the   Readers", 
    "arxiv-id": "cs/0207003v1", 
    "author": "Yasusi Sinohara", 
    "publish": "2002-07-02T15:08:44Z", 
    "summary": "The title of a document has two roles, to give a compact summary and to lead\nthe reader to read the document. Conventional title generation focuses on\nfinding key expressions from the author's wording in the document to give a\ncompact summary and pays little attention to the reader's interest. To make the\ntitle play its second role properly, it is indispensable to clarify the content\n(``what to say'') and wording (``how to say'') of titles that are effective to\nattract the target reader's interest. In this article, we first identify\ntypical content and wording of titles aimed at general readers in a comparative\nstudy between titles of technical papers and headlines rewritten for\nnewspapers. Next, we describe the results of a questionnaire survey on the\neffects of the content and wording of titles on the reader's interest. The\nsurvey of general and knowledgeable readers shows both common and different\ntendencies in interest."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0207005v1", 
    "title": "Efficient Deep Processing of Japanese", 
    "arxiv-id": "cs/0207005v1", 
    "author": "Emily M. Bender", 
    "publish": "2002-07-03T11:54:21Z", 
    "summary": "We present a broad coverage Japanese grammar written in the HPSG formalism\nwith MRS semantics. The grammar is created for use in real world applications,\nsuch that robustness and performance issues play an important role. It is\nconnected to a POS tagging and word segmentation tool. This grammar is being\ndeveloped in a multilingual context, requiring MRS structures that are easily\ncomparable across languages."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0208020v1", 
    "title": "Using the DIFF Command for Natural Language Processing", 
    "arxiv-id": "cs/0208020v1", 
    "author": "Hitoshi Isahara", 
    "publish": "2002-08-13T03:39:20Z", 
    "summary": "Diff is a software program that detects differences between two data sets and\nis useful in natural language processing. This paper shows several examples of\nthe application of diff. They include the detection of differences between two\ndifferent datasets, extraction of rewriting rules, merging of two different\ndatasets, and the optimal matching of two different data sets. Since diff comes\nwith any standard UNIX system, it is readily available and very easy to use.\nOur studies showed that diff is a practical tool for research into natural\nlanguage processing."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0208035v1", 
    "title": "Evaluation of Coreference Rules on Complex Narrative Texts", 
    "arxiv-id": "cs/0208035v1", 
    "author": "Isabelle Robba", 
    "publish": "2002-08-21T14:09:48Z", 
    "summary": "This article studies the problem of assessing relevance to each of the rules\nof a reference resolution system. The reference solver described here stems\nfrom a formal model of reference and is integrated in a reference processing\nworkbench. Evaluation of the reference resolution is essential, as it enables\ndifferential evaluation of individual rules. Numerical values of these measures\nare given, and discussed, for simple selection rules and other processing\nrules; such measures are then studied for numerical parameters."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0208036v1", 
    "title": "Three New Methods for Evaluating Reference Resolution", 
    "arxiv-id": "cs/0208036v1", 
    "author": "Isabelle Robba", 
    "publish": "2002-08-21T14:28:51Z", 
    "summary": "Reference resolution on extended texts (several thousand references) cannot\nbe evaluated manually. An evaluation algorithm has been proposed for the MUC\ntests, using equivalence classes for the coreference relation. However, we show\nhere that this algorithm is too indulgent, yielding good scores even for poor\nresolution strategies. We elaborate on the same formalism to propose two new\nevaluation algorithms, comparing them first with the MUC algorithm and giving\nthen results on a variety of examples. A third algorithm using only\ndistributional comparison of equivalence classes is finally described; it\nassesses the relative importance of the recall vs. precision errors."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0208037v1", 
    "title": "Cooperation between Pronoun and Reference Resolution for Unrestricted   Texts", 
    "arxiv-id": "cs/0208037v1", 
    "author": "Isabelle Robba", 
    "publish": "2002-08-21T14:36:13Z", 
    "summary": "Anaphora resolution is envisaged in this paper as part of the reference\nresolution process. A general open architecture is proposed, which can be\nparticularized and configured in order to simulate some classic anaphora\nresolution methods. With the aim of improving pronoun resolution, the system\ntakes advantage of elementary cues about characters of the text, which are\nrepresented through a particular data structure. In its most robust\nconfiguration, the system uses only a general lexicon, a local morpho-syntactic\nparser and a dictionary of synonyms. A short comparative corpus analysis shows\nthat narrative texts are the most suitable for testing such a system."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0208038v1", 
    "title": "Reference Resolution Beyond Coreference: a Conceptual Frame and its   Application", 
    "arxiv-id": "cs/0208038v1", 
    "author": "Gerard Sabah", 
    "publish": "2002-08-21T14:43:18Z", 
    "summary": "A model for reference use in communication is proposed, from a\nrepresentationist point of view. Both the sender and the receiver of a message\nhandle representations of their common environment, including mental\nrepresentations of objects. Reference resolution by a computer is viewed as the\nconstruction of object representations using referring expressions from the\ndiscourse, whereas often only coreference links between such expressions are\nlooked for. Differences between these two approaches are discussed. The model\nhas been implemented with elementary rules, and tested on complex narrative\ntexts (hundreds to thousands of referring expressions). The results support the\nmental representations paradigm."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0209002v1", 
    "title": "A Chart-Parsing Algorithm for Efficient Semantic Analysis", 
    "arxiv-id": "cs/0209002v1", 
    "author": "Pascal Vaillant", 
    "publish": "2002-09-02T16:55:34Z", 
    "summary": "In some contexts, well-formed natural language cannot be expected as input to\ninformation or communication systems. In these contexts, the use of\ngrammar-independent input (sequences of uninflected semantic units like e.g.\nlanguage-independent icons) can be an answer to the users' needs. A semantic\nanalysis can be performed, based on lexical semantic knowledge: it is\nequivalent to a dependency analysis with no syntactic or morphological clues.\nHowever, this requires that an intelligent system should be able to interpret\nthis input with reasonable accuracy and in reasonable time. Here we propose a\nmethod allowing a purely semantic-based analysis of sequences of semantic\nunits. It uses an algorithm inspired by the idea of ``chart parsing'' known in\nNatural Language Processing, which stores intermediate parsing results in order\nto bring the calculation time down. In comparison with using declarative logic\nprogramming - where the calculation time, left to a prolog engine, is\nhyperexponential -, this method brings the calculation time down to a\npolynomial time, where the order depends on the valency of the predicates."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0209003v1", 
    "title": "Rerendering Semantic Ontologies: Automatic Extensions to UMLS through   Corpus Analytics", 
    "arxiv-id": "cs/0209003v1", 
    "author": "J. Castano", 
    "publish": "2002-09-03T05:28:56Z", 
    "summary": "In this paper, we discuss the utility and deficiencies of existing ontology\nresources for a number of language processing applications. We describe a\ntechnique for increasing the semantic type coverage of a specific ontology, the\nNational Library of Medicine's UMLS, with the use of robust finite state\nmethods used in conjunction with large-scale corpus analytics of the domain\ncorpus. We call this technique \"semantic rerendering\" of the ontology. This\nresearch has been done in the context of Medstract, a joint Brandeis-Tufts\neffort aimed at developing tools for analyzing biomedical language (i.e.,\nMedline), as well as creating targeted databases of bio-entities, biological\nrelations, and pathway data for biological researchers. Motivating the current\nresearch is the need to have robust and reliable semantic typing of syntactic\nelements in the Medline corpus, in order to improve the overall performance of\nthe information extraction applications mentioned above."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0209010v1", 
    "title": "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named   Entity Recognition", 
    "arxiv-id": "cs/0209010v1", 
    "author": "Erik F. Tjong Kim Sang", 
    "publish": "2002-09-05T19:03:06Z", 
    "summary": "We describe the CoNLL-2002 shared task: language-independent named entity\nrecognition. We give background information on the data sets and the evaluation\nmethod, present a general overview of the systems that have taken part in the\ntask and discuss their performance."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0211017v1", 
    "title": "Probabilistic Parsing Strategies", 
    "arxiv-id": "cs/0211017v1", 
    "author": "Giorgio Satta", 
    "publish": "2002-11-14T16:16:44Z", 
    "summary": "We present new results on the relation between purely symbolic context-free\nparsing strategies and their probabilistic counter-parts. Such parsing\nstrategies are seen as constructions of push-down devices from grammars. We\nshow that preservation of probability distribution is possible under two\nconditions, viz. the correct-prefix property and the property of strong\npredictiveness. These results generalize existing results in the literature\nthat were obtained by considering parsing strategies in isolation. From our\ngeneral results we also derive negative results on so-called generalized LR\nparsing."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0212015v1", 
    "title": "Answering Subcognitive Turing Test Questions: A Reply to French", 
    "arxiv-id": "cs/0212015v1", 
    "author": "Peter D. Turney", 
    "publish": "2002-12-09T13:09:10Z", 
    "summary": "Robert French has argued that a disembodied computer is incapable of passing\na Turing Test that includes subcognitive questions. Subcognitive questions are\ndesigned to probe the network of cultural and perceptual associations that\nhumans naturally develop as we live, embodied and embedded in the world. In\nthis paper, I show how it is possible for a disembodied computer to answer\nsubcognitive questions appropriately, contrary to French's claim. My approach\nto answering subcognitive questions is to use statistical information extracted\nfrom a very large collection of text. In particular, I show how it is possible\nto answer a sample of subcognitive questions taken from French, by issuing\nqueries to a search engine that indexes about 350 million Web pages. This\nsimple algorithm may shed light on the nature of human (sub-) cognition, but\nthe scope of this paper is limited to demonstrating that French is mistaken: a\ndisembodied computer can answer subcognitive questions."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0302014v1", 
    "title": "An Algorithm for Aligning Sentences in Bilingual Corpora Using Lexical   Information", 
    "arxiv-id": "cs/0302014v1", 
    "author": "S. M. Bendre", 
    "publish": "2003-02-12T06:31:54Z", 
    "summary": "In this paper we describe an algorithm for aligning sentences with their\ntranslations in a bilingual corpus using lexical information of the languages.\nExisting efficient algorithms ignore word identities and consider only the\nsentence lengths (Brown, 1991; Gale and Church, 1993). For a sentence in the\nsource language text, the proposed algorithm picks the most likely translation\nfrom the target language text using lexical information and certain heuristics.\nIt does not do statistical analysis using sentence lengths. The algorithm is\nlanguage independent. It also aids in detecting addition and deletion of text\nin translations. The algorithm gives comparable results with the existing\nalgorithms in most of the cases while it does better in cases where statistical\nalgorithms do not give good results."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0302032v1", 
    "title": "Empirical Methods for Compound Splitting", 
    "arxiv-id": "cs/0302032v1", 
    "author": "Kevin Knight", 
    "publish": "2003-02-22T23:37:26Z", 
    "summary": "Compounded words are a challenge for NLP applications such as machine\ntranslation (MT). We introduce methods to learn splitting rules from\nmonolingual and parallel corpora. We evaluate them against a gold standard and\nmeasure their impact on performance of statistical MT systems. Results show\naccuracy of 99.1% and performance gains for MT of 0.039 BLEU on a\nGerman-English noun phrase translation task."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0303002v2", 
    "title": "About compression of vocabulary in computer oriented languages", 
    "arxiv-id": "cs/0303002v2", 
    "author": "V. P. Maslov", 
    "publish": "2003-03-05T08:50:42Z", 
    "summary": "The author uses the entropy of the ideal Bose-Einstein gas to minimize losses\nin computer-oriented languages."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0303007v1", 
    "title": "Glottochronology and problems of protolanguage reconstruction", 
    "arxiv-id": "cs/0303007v1", 
    "author": "Kromer Victor", 
    "publish": "2003-03-14T05:15:20Z", 
    "summary": "A method of languages genealogical trees construction is proposed."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ASRU.2001.1034653", 
    "link": "http://arxiv.org/pdf/cs/0304006v1", 
    "title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence   Alignment", 
    "arxiv-id": "cs/0304006v1", 
    "author": "Lillian Lee", 
    "publish": "2003-04-02T23:02:44Z", 
    "summary": "We address the text-to-text generation problem of sentence-level paraphrasing\n-- a phenomenon distinct from and more difficult than word- or phrase-level\nparaphrasing. Our approach applies multiple-sequence alignment to sentences\ngathered from unannotated comparable corpora: it learns a set of paraphrasing\npatterns represented by word lattice pairs and automatically determines how to\napply these patterns to rewrite new sentences. The results of our evaluation\nexperiments show that the system derives accurate paraphrases, outperforming\nbaseline systems."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304019v1", 
    "title": "Blind Normalization of Speech From Different Channels", 
    "arxiv-id": "cs/0304019v1", 
    "author": "David N. Levin", 
    "publish": "2003-04-10T22:25:01Z", 
    "summary": "We show how to construct a channel-independent representation of speech that\nhas propagated through a noisy reverberant channel. This is done by blindly\nrescaling the cepstral time series by a non-linear function, with the form of\nthis scale function being determined by previously encountered cepstra from\nthat channel. The rescaled form of the time series is an invariant property of\nit in the following sense: it is unaffected if the time series is transformed\nby any time-independent invertible distortion. Because a linear channel with\nstationary noise and impulse response transforms cepstra in this way, the new\ntechnique can be used to remove the channel dependence of a cepstral time\nseries. In experiments, the method achieved greater channel-independence than\ncepstral mean normalization, and it was comparable to the combination of\ncepstral mean normalization and spectral subtraction, despite the fact that no\nmeasurements of channel noise or reverberations were required (unlike spectral\nsubtraction)."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304024v1", 
    "title": "Glottochronologic Retrognostic of Language System", 
    "arxiv-id": "cs/0304024v1", 
    "author": "Kromer Victor", 
    "publish": "2003-04-17T02:22:06Z", 
    "summary": "A glottochronologic retrognostic of language system is proposed"
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304027v1", 
    "title": "\"I'm sorry Dave, I'm afraid I can't do that\": Linguistics, Statistics,   and Natural Language Processing circa 2001", 
    "arxiv-id": "cs/0304027v1", 
    "author": "Lillian Lee", 
    "publish": "2003-04-21T22:10:21Z", 
    "summary": "A brief, general-audience overview of the history of natural language\nprocessing, focusing on data-driven approaches.Topics include \"Ambiguity and\nlanguage analysis\", \"Firth things first\", \"A 'C' change\", and \"The empiricists\nstrike back\"."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304029v1", 
    "title": "An XML based Document Suite", 
    "arxiv-id": "cs/0304029v1", 
    "author": "Manuela Kunze", 
    "publish": "2003-04-22T13:45:37Z", 
    "summary": "We report about the current state of development of a document suite and its\napplications. This collection of tools for the flexible and robust processing\nof documents in German is based on the use of XML as unifying formalism for\nencoding input and output data as well as process information. It is organized\nin modules with limited responsibilities that can easily be combined into\npipelines to solve complex tasks. Strong emphasis is laid on a number of\ntechniques to deal with lexical and conceptual gaps that are typical when\nstarting a new application."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304035v1", 
    "title": "Exploiting Sublanguage and Domain Characteristics in a Bootstrapping   Approach to Lexicon and Ontology Creation", 
    "arxiv-id": "cs/0304035v1", 
    "author": "Manuela Kunze", 
    "publish": "2003-04-23T08:02:53Z", 
    "summary": "It is very costly to build up lexical resources and domain ontologies.\nEspecially when confronted with a new application domain lexical gaps and a\npoor coverage of domain concepts are a problem for the successful exploitation\nof natural language document analysis systems that need and exploit such\nknowledge sources. In this paper we report about ongoing experiments with\n`bootstrapping techniques' for lexicon and ontology creation."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0304036v1", 
    "title": "An Approach for Resource Sharing in Multilingual NLP", 
    "arxiv-id": "cs/0304036v1", 
    "author": "Chun Xiao", 
    "publish": "2003-04-23T08:32:19Z", 
    "summary": "In this paper we describe an approach for the analysis of documents in German\nand English with a shared pool of resources. For the analysis of German\ndocuments we use a document suite, which supports the user in tasks like\ninformation retrieval and information extraction. The core of the document\nsuite is based on our tool XDOC. Now we want to exploit these methods for the\nanalysis of English documents as well. For this aim we need a multilingual\npresentation format of the resources. These resources must be transformed into\nan unified format, in which we can set additional information about linguistic\ncharacteristics of the language depending on the analyzed documents. In this\npaper we describe our approach for such an exchange model for multilingual\nresources based on XML."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0305041v2", 
    "title": "Factorization of Language Models through Backing-Off Lattices", 
    "arxiv-id": "cs/0305041v2", 
    "author": "Wei Wang", 
    "publish": "2003-05-23T19:38:22Z", 
    "summary": "Factorization of statistical language models is the task that we resolve the\nmost discriminative model into factored models and determine a new model by\ncombining them so as to provide better estimate. Most of previous works mainly\nfocus on factorizing models of sequential events, each of which allows only one\nfactorization manner. To enable parallel factorization, which allows a model\nevent to be resolved in more than one ways at the same time, we propose a\ngeneral framework, where we adopt a backing-off lattice to reflect parallel\nfactorizations and to define the paths along which a model is resolved into\nfactored models, we use a mixture model to combine parallel paths in the\nlattice, and generalize Katz's backing-off method to integrate all the mixture\nmodels got by traversing the entire lattice. Based on this framework, we\nformulate two types of model factorizations that are used in natural language\nmodeling."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0306050v1", 
    "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named   Entity Recognition", 
    "arxiv-id": "cs/0306050v1", 
    "author": "Fien De Meulder", 
    "publish": "2003-06-12T12:35:00Z", 
    "summary": "We describe the CoNLL-2003 shared task: language-independent named entity\nrecognition. We give background information on the data sets (English and\nGerman) and the evaluation method, present a general overview of the systems\nthat have taken part in the task and discuss their performance."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0306062v1", 
    "title": "Learning to Order Facts for Discourse Planning in Natural Language   Generation", 
    "arxiv-id": "cs/0306062v1", 
    "author": "Ion Androutsopoulos", 
    "publish": "2003-06-13T09:05:10Z", 
    "summary": "This paper presents a machine learning approach to discourse planning in\nnatural language generation. More specifically, we address the problem of\nlearning the most natural ordering of facts in discourse plans for a specific\ndomain. We discuss our methodology and how it was instantiated using two\ndifferent machine learning algorithms. A quantitative evaluation performed in\nthe domain of museum exhibit descriptions indicates that our approach performs\nsignificantly better than manually constructed ordering rules. Being\nretrainable, the resulting planners can be ported easily to other similar\ndomains, without requiring language technology expertise."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0306099v1", 
    "title": "An Improved k-Nearest Neighbor Algorithm for Text Categorization", 
    "arxiv-id": "cs/0306099v1", 
    "author": "Qin Lu", 
    "publish": "2003-06-16T13:54:03Z", 
    "summary": "k is the most important parameter in a text categorization system based on\nk-Nearest Neighbor algorithm (kNN).In the classification process, k nearest\ndocuments to the test one in the training set are determined firstly. Then, the\npredication can be made according to the category distribution among these k\nnearest neighbors. Generally speaking, the class distribution in the training\nset is uneven. Some classes may have more samples than others. Therefore, the\nsystem performance is very sensitive to the choice of the parameter k. And it\nis very likely that a fixed k value will result in a bias on large categories.\nTo deal with these problems, we propose an improved kNN algorithm, which uses\ndifferent numbers of nearest neighbors for different categories, rather than a\nfixed number across all categories. More samples (nearest neighbors) will be\nused for deciding whether a test document should be classified to a category,\nwhich has more samples in the training set. Preliminary experiments on Chinese\ntext categorization show that our method is less sensitive to the parameter k\nthan the traditional one, and it can properly classify documents belonging to\nsmaller classes with a large k. The method is promising for some cases, where\nestimating the parameter k via cross-validation is not allowed."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0307028v1", 
    "title": "Issues in Communication Game", 
    "arxiv-id": "cs/0307028v1", 
    "author": "Koiti Hasida", 
    "publish": "2003-07-11T14:43:51Z", 
    "summary": "As interaction between autonomous agents, communication can be analyzed in\ngame-theoretic terms. Meaning game is proposed to formalize the core of\nintended communication in which the sender sends a message and the receiver\nattempts to infer its meaning intended by the sender. Basic issues involved in\nthe game of natural language communication are discussed, such as salience,\ngrammaticality, common sense, and common belief, together with some\ndemonstration of the feasibility of game-theoretic account of language."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0307030v1", 
    "title": "Parsing and Generation with Tabulation and Compilation", 
    "arxiv-id": "cs/0307030v1", 
    "author": "Takashi Miyata", 
    "publish": "2003-07-11T15:42:51Z", 
    "summary": "The standard tabulation techniques for logic programming presuppose fixed\norder of computation. Some data-driven control should be introduced in order to\ndeal with diverse contexts. The present paper describes a data-driven method of\nconstraint transformation with a sort of compilation which subsumes\naccessibility check and last-call optimization, which characterize standard\nnatural-language parsing techniques, semantic-head-driven generation, etc."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0307044v1", 
    "title": "The Linguistic DS: Linguisitic Description in MPEG-7", 
    "arxiv-id": "cs/0307044v1", 
    "author": "Koiti Hasida", 
    "publish": "2003-07-19T12:24:33Z", 
    "summary": "MPEG-7 (Moving Picture Experts Group Phase 7) is an XML-based international\nstandard on semantic description of multimedia content. This document discusses\nthe Linguistic DS and related tools. The linguistic DS is a tool, based on the\nGDA tag set (http://i-content.org/GDA/tagset.html), for semantic annotation of\nlinguistic data in or associated with multimedia content. The current document\ntext reflects `Study of FPDAM - MPEG-7 MDS Extensions' issued in March 2003,\nand not most part of MPEG-7 MDS, for which the readers are referred to the\nfirst version of MPEG-7 MDS document available from ISO (http://www.iso.org).\nWithout that reference, however, this document should be mostly intelligible to\nthose who are familiar with XML and linguistic theories. Comments are welcome\nand will be considered in the standardization process."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0308016v1", 
    "title": "Collaborative Creation of Digital Content in Indian Languages", 
    "arxiv-id": "cs/0308016v1", 
    "author": "Rajeev Sangal", 
    "publish": "2003-08-07T09:01:56Z", 
    "summary": "The world is passing through a major revolution called the information\nrevolution, in which information and knowledge is becoming available to people\nin unprecedented amounts wherever and whenever they need it. Those societies\nwhich fail to take advantage of the new technology will be left behind, just\nlike in the industrial revolution.\n  The information revolution is based on two major technologies: computers and\ncommunication. These technologies have to be delivered in a COST EFFECTIVE\nmanner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable\ntechnology choices, and to allow people to access through shared resources.\nThis could be done throuch street corner shops (for computer usage, e-mail\netc.), schools, community centres and local library centres."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0308017v1", 
    "title": "Information Revolution", 
    "arxiv-id": "cs/0308017v1", 
    "author": "Rajeev Sangal", 
    "publish": "2003-08-07T09:17:16Z", 
    "summary": "The world is passing through a major revolution called the information\nrevolution, in which information and knowledge is becoming available to people\nin unprecedented amounts wherever and whenever they need it. Those societies\nwhich fail to take advantage of the new technology will be left behind, just\nlike in the industrial revolution.\n  The information revolution is based on two major technologies: computers and\ncommunication. These technologies have to be delivered in a COST EFFECTIVE\nmanner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable\ntechnology choices (discussed later), and to allow people to access through\nshared resources. This could be done throuch street corner shops (for computer\nusage, e-mail etc.), schools, community centers and local library centres."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0308018v1", 
    "title": "Anusaaraka: Overcoming the Language Barrier in India", 
    "arxiv-id": "cs/0308018v1", 
    "author": "G Umamaheshwara Rao", 
    "publish": "2003-08-07T09:24:46Z", 
    "summary": "The anusaaraka system makes text in one Indian language accessible in another\nIndian language. In the anusaaraka approach, the load is so divided between man\nand computer that the language load is taken by the machine, and the\ninterpretation of the text is left to the man. The machine presents an image of\nthe source text in a language close to the target language.In the image, some\nconstructions of the source language (which do not have equivalents) spill over\nto the output. Some special notation is also devised. The user after some\ntraining learns to read and understand the output. Because the Indian languages\nare close, the learning time of the output language is short, and is expected\nto be around 2 weeks.\n  The output can also be post-edited by a trained user to make it grammatically\ncorrect in the target language. Style can also be changed, if necessary. Thus,\nin this scenario, it can function as a human assisted translation system.\n  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali\nand Punjabi to Hindi. They can be built for all Indian languages in the near\nfuture. Everybody must pitch in to build such systems connecting all Indian\nlanguages, using the free software model."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0308019v1", 
    "title": "Language Access: An Information Based Approach", 
    "arxiv-id": "cs/0308019v1", 
    "author": "Rajeev Sangal", 
    "publish": "2003-08-07T09:40:04Z", 
    "summary": "The anusaaraka system (a kind of machine translation system) makes text in\none Indian language accessible through another Indian language. The machine\npresents an image of the source text in a language close to the target\nlanguage. In the image, some constructions of the source language (which do not\nhave equivalents in the target language) spill over to the output. Some special\nnotation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,\nMarathi, Bengali and Punjabi to Hindi. They are available for use through Email\nservers.\n  Anusaarkas follows the principle of substitutibility and reversibility of\nstrings produced. This implies preservation of information while going from a\nsource language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject\ndomain knowledge into the system, which produce good quality grammatical\noutput. However, it should be remembered, that such modules will work only in\nnarrow areas, and will sometimes go wrong. In such a situation, anusaaraka\noutput will still remain useful."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0308020v1", 
    "title": "LERIL : Collaborative Effort for Creating Lexical Resources", 
    "arxiv-id": "cs/0308020v1", 
    "author": "Durgesh D Rao", 
    "publish": "2003-08-07T10:08:43Z", 
    "summary": "The paper reports on efforts taken to create lexical resources pertaining to\nIndian languages, using the collaborative model. The lexical resources being\ndeveloped are: (1) Transfer lexicon and grammar from English to several Indian\nlanguages. (2) Dependencey tree bank of annotated corpora for several Indian\nlanguages. The dependency trees are based on the Paninian model. (3) Bilingual\ndictionary of 'core meanings'."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0309019v1", 
    "title": "Building a Test Collection for Speech-Driven Web Retrieval", 
    "arxiv-id": "cs/0309019v1", 
    "author": "Katunobu Itou", 
    "publish": "2003-09-12T12:43:00Z", 
    "summary": "This paper describes a test collection (benchmark data) for retrieval systems\ndriven by spoken queries. This collection was produced in the subtask of the\nNTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation\nworkshop. The search topics and document collection for the Web retrieval task\nwere used to produce spoken queries and language models for speech recognition,\nrespectively. We used this collection to evaluate the performance of our\nretrieval system. Experimental results showed that (a) the use of target\ndocuments for language modeling and (b) enhancement of the vocabulary size in\nspeech recognition were effective in improving the system performance."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0309021v1", 
    "title": "A Cross-media Retrieval System for Lecture Videos", 
    "arxiv-id": "cs/0309021v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2003-09-13T06:54:58Z", 
    "summary": "We propose a cross-media lecture-on-demand system, in which users can\nselectively view specific segments of lecture videos by submitting text\nqueries. Users can easily formulate queries by using the textbook associated\nwith a target lecture, even if they cannot come up with effective keywords. Our\nsystem extracts the audio track from a target lecture video, generates a\ntranscription by large vocabulary continuous speech recognition, and produces a\ntext index. Experimental results showed that by adapting speech recognition to\nthe topic of the lecture, the recognition accuracy increased and the retrieval\naccuracy was comparable with that obtained by human transcription."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0310014v1", 
    "title": "Effective XML Representation for Spoken Language in Organisations", 
    "arxiv-id": "cs/0310014v1", 
    "author": "Dali Dong", 
    "publish": "2003-10-08T16:40:33Z", 
    "summary": "Spoken Language can be used to provide insights into organisational\nprocesses, unfortunately transcription and coding stages are very time\nconsuming and expensive. The concept of partial transcription and coding is\nproposed in which spoken language is indexed prior to any subsequent\nprocessing. The functional linguistic theory of texture is used to describe the\neffects of partial transcription on observational records. The standard used to\nencode transcript context and metadata is called CHAT, but a previous XML\nschema developed to implement it contains design assumptions that make it\ndifficult to support partial transcription for example. This paper describes a\nmore effective XML schema that overcomes many of these problems and is intended\nfor use in applications that support the rapid development of spoken language\ndeliverables."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.1755235", 
    "link": "http://arxiv.org/pdf/cs/0310058v1", 
    "title": "Application Architecture for Spoken Language Resources in Organisational   Settings", 
    "arxiv-id": "cs/0310058v1", 
    "author": "Philip C. Windridge", 
    "publish": "2003-10-29T20:13:30Z", 
    "summary": "Special technologies need to be used to take advantage of, and overcome, the\nchallenges associated with acquiring, transforming, storing, processing, and\ndistributing spoken language resources in organisations. This paper introduces\nan application architecture consisting of tools and supporting utilities for\nindexing and transcription, and describes how these tools, together with\ndownstream processing and distribution systems, can be integrated into a\nworkflow. Two sample applications for this architecture are outlined- the\nanalysis of decision-making processes in organisations and the deployment of\nsystems development methods by designers in the field."
},{
    "category": "cs.CL", 
    "doi": "10.1080/0929617042000314912", 
    "link": "http://arxiv.org/pdf/cs/0311033v1", 
    "title": "The Rank-Frequency Analysis for the Functional Style Corpora in the   Ukrainian Language", 
    "arxiv-id": "cs/0311033v1", 
    "author": "Andrij A. Rovenchak", 
    "publish": "2003-11-21T19:48:17Z", 
    "summary": "We use the rank-frequency analysis for the estimation of Kernel Vocabulary\nsize within specific corpora of Ukrainian. The extrapolation of high-rank\nbehaviour is utilized for estimation of the total vocabulary size."
},{
    "category": "cs.CL", 
    "doi": "10.1080/0929617042000314912", 
    "link": "http://arxiv.org/pdf/cs/0311036v1", 
    "title": "Measuring the Functional Load of Phonological Contrasts", 
    "arxiv-id": "cs/0311036v1", 
    "author": "Partha Niyogi", 
    "publish": "2003-11-24T18:05:40Z", 
    "summary": "Frequency counts are a measure of how much use a language makes of a\nlinguistic unit, such as a phoneme or word. However, what is often important is\nnot the units themselves, but the contrasts between them. A measure is\ntherefore needed for how much use a language makes of a contrast, i.e. the\nfunctional load (FL) of the contrast. We generalize previous work in\nlinguistics and speech recognition and propose a family of measures for the FL\nof several phonological contrasts, including phonemic oppositions, distinctive\nfeatures, suprasegmentals, and phonological rules. We then test it for\nrobustness to changes of corpora. Finally, we provide examples in Cantonese,\nDutch, English, German and Mandarin, in the context of historical linguistics,\nlanguage acquisition and speech recognition. More information can be found at\nhttp://dinoj.info/research/fload"
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0402055v1", 
    "title": "Lexical Base as a Compressed Language Model of the World (on the   material of the Ukrainian language)", 
    "arxiv-id": "cs/0402055v1", 
    "author": "Solomiya Buk", 
    "publish": "2004-02-24T09:34:16Z", 
    "summary": "In the article the fact is verified that the list of words selected by formal\nstatistical methods (frequency and functional genre unrestrictedness) is not a\nconglomerate of non-related words. It creates a system of interrelated items\nand it can be named \"lexical base of language\". This selected list of words\ncovers all the spheres of human activities. To verify this statement the\ninvariant synoptical scheme common for ideographic dictionaries of different\nlanguage was determined."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0404007v1", 
    "title": "Polarity sensitivity and evaluation order in type-logical grammar", 
    "arxiv-id": "cs/0404007v1", 
    "author": "Chung-chieh Shan", 
    "publish": "2004-04-05T02:14:50Z", 
    "summary": "We present a novel, type-logical analysis of_polarity sensitivity_: how\nnegative polarity items (like \"any\" and \"ever\") or positive ones (like \"some\")\nare licensed or prohibited. It takes not just scopal relations but also linear\norder into account, using the programming-language notions of delimited\ncontinuations and evaluation order, respectively. It thus achieves greater\nempirical coverage than previous proposals."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0404009v1", 
    "title": "Tabular Parsing", 
    "arxiv-id": "cs/0404009v1", 
    "author": "Giorgio Satta", 
    "publish": "2004-04-05T11:51:43Z", 
    "summary": "This is a tutorial on tabular parsing, on the basis of tabulation of\nnondeterministic push-down automata. Discussed are Earley's algorithm, the\nCocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse\ntrees, and further issues."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0404025v1", 
    "title": "Test Collections for Patent-to-Patent Retrieval and Patent Map   Generation in NTCIR-4 Workshop", 
    "arxiv-id": "cs/0404025v1", 
    "author": "Noriko Kando", 
    "publish": "2004-04-10T08:43:31Z", 
    "summary": "This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop,\nand the test collections produced in this task. We perform the invalidity\nsearch task, in which each participant group searches a patent collection for\nthe patents that can invalidate the demand in an existing claim. We also\nperform the automatic patent map generation task, in which the patents\nassociated with a specific topic are organized in a multi-dimensional matrix."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0405037v1", 
    "title": "A Probabilistic Model of Machine Translation", 
    "arxiv-id": "cs/0405037v1", 
    "author": "V. K. Petrov", 
    "publish": "2004-05-10T16:05:23Z", 
    "summary": "A probabilistic model for computer-based generation of a machine translation\nsystem on the basis of English-Russian parallel text corpora is suggested. The\nmodel is trained using parallel text corpora with pre-aligned source and target\nsentences. The training of the model results in a bilingual dictionary of words\nand \"word blocks\" with relevant translation probability."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0405039v1", 
    "title": "Catching the Drift: Probabilistic Content Models, with Applications to   Generation and Summarization", 
    "arxiv-id": "cs/0405039v1", 
    "author": "Lillian Lee", 
    "publish": "2004-05-12T14:14:52Z", 
    "summary": "We consider the problem of modeling the content structure of texts within a\nspecific domain, in terms of the topics the texts address and the order in\nwhich these topics appear. We first present an effective knowledge-lean method\nfor learning content models from un-annotated documents, utilizing a novel\nadaptation of algorithms for Hidden Markov Models. We then apply our method to\ntwo complementary tasks: information ordering and extractive summarization. Our\nexperiments show that incorporating content models in these applications yields\nsubstantial improvement over previously-proposed methods."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0406031v1", 
    "title": "A Public Reference Implementation of the RAP Anaphora Resolution   Algorithm", 
    "arxiv-id": "cs/0406031v1", 
    "author": "Tat-Seng Chua", 
    "publish": "2004-06-17T12:29:29Z", 
    "summary": "This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002)."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0406054v1", 
    "title": "Building a linguistic corpus from bee dance data", 
    "arxiv-id": "cs/0406054v1", 
    "author": "J. J. Paijmans", 
    "publish": "2004-06-28T10:25:22Z", 
    "summary": "This paper discusses the problems and possibility of collecting bee dance\ndata in a linguistic \\textit{corpus} and use linguistic instruments such as\nZipf's law and entropy statistics to decide on the question whether the dance\ncarries information of any kind. We describe this against the historical\nbackground of attempts to analyse non-human communication systems."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407002v1", 
    "title": "Annotating Predicate-Argument Structure for a Parallel Treebank", 
    "arxiv-id": "cs/0407002v1", 
    "author": "Frank Schumacher", 
    "publish": "2004-07-01T16:18:52Z", 
    "summary": "We report on a recently initiated project which aims at building a\nmulti-layered parallel treebank of English and German. Particular attention is\ndevoted to a dedicated predicate-argument layer which is used for aligning\ntranslationally equivalent sentences of the two languages. We describe both our\nconceptual decisions and aspects of their technical realisation. We discuss\nsome selected problems and conclude with a few remarks on how this project\nrelates to similar projects in the field."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407005v3", 
    "title": "Statistical Machine Translation by Generalized Parsing", 
    "arxiv-id": "cs/0407005v3", 
    "author": "Wei Wang", 
    "publish": "2004-07-01T22:02:10Z", 
    "summary": "Designers of statistical machine translation (SMT) systems have begun to\nemploy tree-structured translation models. Systems involving tree-structured\ntranslation models tend to be complex. This article aims to reduce the\nconceptual complexity of such systems, in order to make them easier to design,\nimplement, debug, use, study, understand, explain, modify, and improve. In\nservice of this goal, the article extends the theory of semiring parsing to\narrive at a novel abstract parsing algorithm with five functional parameters: a\nlogic, a grammar, a semiring, a search strategy, and a termination condition.\nThe article then shows that all the common algorithms that revolve around\ntree-structured translation models, including hierarchical alignment, inference\nfor parameter estimation, translation, and structured evaluation, can be\nderived by generalizing two of these parameters -- the grammar and the logic.\nThe article culminates with a recipe for using such generalized parsers to\ntrain, apply, and evaluate an SMT system that is driven by tree-structured\ntranslation models."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407026v1", 
    "title": "Summarizing Encyclopedic Term Descriptions on the Web", 
    "arxiv-id": "cs/0407026v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2004-07-10T11:18:42Z", 
    "summary": "We are developing an automatic method to compile an encyclopedic corpus from\nthe Web. In our previous work, paragraph-style descriptions for a term are\nextracted from Web pages and organized based on domains. However, these\ndescriptions are independent and do not comprise a condensed text as in\nhand-crafted encyclopedias. To resolve this problem, we propose a summarization\nmethod, which produces a single text from multiple descriptions. The resultant\nsummary concisely describes a term from different viewpoints. We also show the\neffectiveness of our method by means of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407027v1", 
    "title": "Unsupervised Topic Adaptation for Lecture Speech Retrieval", 
    "arxiv-id": "cs/0407027v1", 
    "author": "Tetsuya Ishikawa", 
    "publish": "2004-07-10T11:45:57Z", 
    "summary": "We are developing a cross-media information retrieval system, in which users\ncan view specific segments of lecture videos by submitting text queries. To\nproduce a text index, the audio track is extracted from a lecture video and a\ntranscription is generated by automatic speech recognition. In this paper, to\nimprove the quality of our retrieval system, we extensively investigate the\neffects of adapting acoustic and language models on speech recognition. We\nperform an MLLR-based method to adapt an acoustic model. To obtain a corpus for\nlanguage model adaptation, we use the textbook for a target lecture to search a\nWeb collection for the pages associated with the lecture topic. We show the\neffectiveness of our method by means of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407028v1", 
    "title": "Effects of Language Modeling on Speech-driven Question Answering", 
    "arxiv-id": "cs/0407028v1", 
    "author": "Katunobu Itou", 
    "publish": "2004-07-10T11:57:17Z", 
    "summary": "We integrate automatic speech recognition (ASR) and question answering (QA)\nto realize a speech-driven QA system, and evaluate its performance. We adapt an\nN-gram language model to natural language questions, so that the input of our\nsystem can be recognized with a high accuracy. We target WH-questions which\nconsist of the topic part and fixed phrase used to ask about something. We\nfirst produce a general N-gram model intended to recognize the topic and\nemphasize the counts of the N-grams that correspond to the fixed phrases. Given\na transcription by the ASR engine, the QA engine extracts the answer candidates\nfrom target documents. We propose a passage retrieval method robust against\nrecognition errors in the transcription. We use the QA test collection produced\nin NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness\nof our method by means of experiments."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0407046v1", 
    "title": "A Bimachine Compiler for Ranked Tagging Rules", 
    "arxiv-id": "cs/0407046v1", 
    "author": "Kathrine Hammervold", 
    "publish": "2004-07-19T11:57:42Z", 
    "summary": "This paper describes a novel method of compiling ranked tagging rules into a\ndeterministic finite-state device called a bimachine. The rules are formulated\nin the framework of regular rewrite operations and allow unrestricted regular\nexpressions in both left and right rule contexts. The compiler is illustrated\nby an application within a speech synthesis system."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0408052v1", 
    "title": "Application of the Double Metaphone Algorithm to Amharic Orthography", 
    "arxiv-id": "cs/0408052v1", 
    "author": "Daniel Yacob", 
    "publish": "2004-08-22T19:32:48Z", 
    "summary": "The Metaphone algorithm applies the phonetic encoding of orthographic\nsequences to simplify words prior to comparison. While Metaphone has been\nhighly successful for the English language, for which it was designed, it may\nnot be applied directly to Ethiopian languages. The paper details how the\nprinciples of Metaphone can be applied to Ethiopic script and uses Amharic as a\ncase study. Match results improve as specific considerations are made for\nAmharic writing practices. Results are shown to improve further when common\nerrors from Amharic input methods are considered."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0408059v1", 
    "title": "Proofing Tools Technology at Neurosoft S.A.", 
    "arxiv-id": "cs/0408059v1", 
    "author": "A. Vagelatos", 
    "publish": "2004-08-26T10:50:45Z", 
    "summary": "The aim of this paper is to present the R&D activities carried out at\nNeurosoft S.A. regarding the development of proofing tools for Modern Greek.\nFirstly, we focus on infrastructure issues that we faced during our initial\nsteps. Subsequently, we describe the most important insights of three proofing\ntools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the\nthesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss\nsome improvement ideas and give our future directions."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0408060v1", 
    "title": "Verbal chunk extraction in French using limited resources", 
    "arxiv-id": "cs/0408060v1", 
    "author": "Francois Trouilleux", 
    "publish": "2004-08-26T12:44:15Z", 
    "summary": "A way of extracting French verbal chunks, inflected and infinitive, is\nexplored and tested on effective corpus. Declarative morphological and local\ngrammar rules specifying chunks and some simple contextual structures are used,\nrelying on limited lexical information and some simple heuristic/statistic\nproperties obtained from restricted corpora. The specific goals, the\narchitecture and the formalism of the system, the linguistic information on\nwhich it relies and the obtained results on effective corpus are presented."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0408061v1", 
    "title": "An electronic dictionary as a basis for NLP tools: The Greek case", 
    "arxiv-id": "cs/0408061v1", 
    "author": "G. Orphanos", 
    "publish": "2004-08-26T13:17:38Z", 
    "summary": "The existence of a Dictionary in electronic form for Modern Greek (MG) is\nmandatory if one is to process MG at the morphological and syntactic levels\nsince MG is a highly inflectional language with marked stress and a spelling\nsystem with many characteristics carried over from Ancient Greek. Moreover,\nsuch a tool becomes necessary if one is to create efficient and sophisticated\nNLP applications with substantial linguistic backing and coverage. The present\npaper will focus on the deployment of such an electronic dictionary for Modern\nGreek, which was built in two phases: first it was constructed to be the basis\nfor a spelling correction schema and then it was reconstructed in order to\nbecome the platform for the deployment of a wider spectrum of NLP tools."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0409008v1", 
    "title": "A Model for Fine-Grained Alignment of Multilingual Texts", 
    "arxiv-id": "cs/0409008v1", 
    "author": "Hendrik Feddes", 
    "publish": "2004-09-07T13:46:50Z", 
    "summary": "While alignment of texts on the sentential level is often seen as being too\ncoarse, and word alignment as being too fine-grained, bi- or multilingual texts\nwhich are aligned on a level in-between are a useful resource for many\npurposes. Starting from a number of examples of non-literal translations, which\ntend to make alignment difficult, we describe an alignment model which copes\nwith these cases by explicitly coding them. The model is based on\npredicate-argument structures and thus covers the middle ground between\nsentence and word alignment. The model is currently used in a recently\ninitiated project of a parallel English-German treebank (FuSe), which can in\nprinciple be extended with additional languages."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0409058v1", 
    "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity   Summarization Based on Minimum Cuts", 
    "arxiv-id": "cs/0409058v1", 
    "author": "Lillian Lee", 
    "publish": "2004-09-29T20:34:04Z", 
    "summary": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;\nan example application is classifying a movie review as \"thumbs up\" or \"thumbs\ndown\". To determine this sentiment polarity, we propose a novel\nmachine-learning method that applies text-categorization techniques to just the\nsubjective portions of the document. Extracting these portions can be\nimplemented using efficient techniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence contextual constraints."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0412015v2", 
    "title": "A Tutorial on the Expectation-Maximization Algorithm Including   Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free   Grammars", 
    "arxiv-id": "cs/0412015v2", 
    "author": "Detlef Prescher", 
    "publish": "2004-12-03T17:10:17Z", 
    "summary": "The paper gives a brief review of the expectation-maximization algorithm\n(Dempster 1977) in the comprehensible framework of discrete mathematics. In\nSection 2, two prominent estimation methods, the relative-frequency estimation\nand the maximum-likelihood estimation are presented. Section 3 is dedicated to\nthe expectation-maximization algorithm and a simpler variant, the generalized\nexpectation-maximization algorithm. In Section 4, two loaded dice are rolled. A\nmore interesting example is presented in Section 5: The estimation of\nprobabilistic context-free grammars."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0412016v1", 
    "title": "Inside-Outside Estimation Meets Dynamic EM", 
    "arxiv-id": "cs/0412016v1", 
    "author": "Detlef Prescher", 
    "publish": "2004-12-03T18:10:17Z", 
    "summary": "We briefly review the inside-outside and EM algorithm for probabilistic\ncontext-free grammars. As a result, we formally prove that inside-outside\nestimation is a dynamic-programming variant of EM. This is interesting in its\nown right, but even more when considered in a theoretical context since the\nwell-known convergence behavior of inside-outside estimation has been confirmed\nby many experiments but apparently has never been formally proved. However,\nbeing a version of EM, inside-outside estimation also inherits the good\nconvergence behavior of EM. Therefore, the as yet imperfect line of\nargumentation can be transformed into a coherent proof."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0412114v1", 
    "title": "State of the Art, Evaluation and Recommendations regarding \"Document   Processing and Visualization Techniques\"", 
    "arxiv-id": "cs/0412114v1", 
    "author": "Pierre Andrews", 
    "publish": "2004-12-29T15:19:03Z", 
    "summary": "Several Networks of Excellence have been set up in the framework of the\nEuropean FP5 research program. Among these Networks of Excellence, the NEMIS\nproject focuses on the field of Text Mining.\n  Within this field, document processing and visualization was identified as\none of the key topics and the WG1 working group was created in the NEMIS\nproject, to carry out a detailed survey of techniques associated with the text\nmining process and to identify the relevant research topics in related research\nareas.\n  In this document we present the results of this comprehensive survey. The\nreport includes a description of the current state-of-the-art and practice, a\nroadmap for follow-up research in the identified areas, and recommendations for\nanticipated technological development in the domain of text mining."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0412117v1", 
    "title": "Thematic Annotation: extracting concepts out of documents", 
    "arxiv-id": "cs/0412117v1", 
    "author": "Martin Rajman", 
    "publish": "2004-12-30T02:01:45Z", 
    "summary": "Contrarily to standard approaches to topic annotation, the technique used in\nthis work does not centrally rely on some sort of -- possibly statistical --\nkeyword extraction. In fact, the proposed annotation algorithm uses a large\nscale semantic database -- the EDR Electronic Dictionary -- that provides a\nconcept hierarchy based on hyponym and hypernym relations. This concept\nhierarchy is used to generate a synthetic representation of the document by\naggregating the words present in topically homogeneous document segments into a\nset of concepts best preserving the document's content.\n  This new extraction technique uses an unexplored approach to topic selection.\nInstead of using semantic similarity measures based on a semantic resource, the\nlater is processed to extract the part of the conceptual hierarchy relevant to\nthe document content. Then this conceptual hierarchy is searched to extract the\nmost relevant set of concepts to represent the topics discussed in the\ndocument. Notice that this algorithm is able to extract generic concepts that\nare not directly present in the document."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0501078v1", 
    "title": "Multi-document Biography Summarization", 
    "arxiv-id": "cs/0501078v1", 
    "author": "Eduard Hovy", 
    "publish": "2005-01-26T22:43:17Z", 
    "summary": "In this paper we describe a biography summarization system using sentence\nclassification and ideas from information retrieval. Although the individual\ntechniques are not new, assembling and applying them to generate multi-document\nbiographies is new. Our system was evaluated in DUC2004. It is among the top\nperformers in task 5-short summaries focused by person questions."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0504022v1", 
    "title": "A Matter of Opinion: Sentiment Analysis and Business Intelligence   (position paper)", 
    "arxiv-id": "cs/0504022v1", 
    "author": "Lillian Lee", 
    "publish": "2005-04-06T20:04:55Z", 
    "summary": "A general-audience introduction to the area of \"sentiment analysis\", the\ncomputational treatment of subjective, opinion-oriented language (an example\napplication is determining whether a review is \"thumbs up\" or \"thumbs down\").\nSome challenges, applications to business-intelligence tasks, and potential\nfuture directions are described."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0510015v1", 
    "title": "Word sense disambiguation criteria: a systematic study", 
    "arxiv-id": "cs/0510015v1", 
    "author": "Laurent Audibert", 
    "publish": "2005-10-05T14:23:19Z", 
    "summary": "This article describes the results of a systematic in-depth study of the\ncriteria used for word sense disambiguation. Our study is based on 60 target\nwords: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line\nwith some practices in the field. For example, we show that omitting\nnon-content words decreases performance and that bigrams yield better results\nthan unigrams."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0511076v1", 
    "title": "Using phonetic constraints in acoustic-to-articulatory inversion", 
    "arxiv-id": "cs/0511076v1", 
    "author": "Yves Laprie", 
    "publish": "2005-11-21T14:50:52Z", 
    "summary": "The goal of this work is to recover articulatory information from the speech\nsignal by acoustic-to-articulatory inversion. One of the main difficulties with\ninversion is that the problem is underdetermined and inversion methods\ngenerally offer no guarantee on the phonetical realism of the inverse\nsolutions. A way to adress this issue is to use additional phonetic\nconstraints. Knowledge of the phonetic caracteristics of French vowels enable\nthe derivation of reasonable articulatory domains in the space of Maeda\nparameters: given the formants frequencies (F1,F2,F3) of a speech sample, and\nthus the vowel identity, an \"ideal\" articulatory domain can be derived. The\nspace of formants frequencies is partitioned into vowels, using either\nspeaker-specific data or generic information on formants. Then, to each\narticulatory vector can be associated a phonetic score varying with the\ndistance to the \"ideal domain\" associated with the corresponding vowel.\nInversion experiments were conducted on isolated vowels and vowel-to-vowel\ntransitions. Articulatory parameters were compared with those obtained without\nusing these constraints and those measured from X-ray data."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0511079v1", 
    "title": "An elitist approach for extracting automatically well-realized speech   sounds with high confidence", 
    "arxiv-id": "cs/0511079v1", 
    "author": "Yves Laprie", 
    "publish": "2005-11-22T07:06:43Z", 
    "summary": "This paper presents an \"elitist approach\" for extracting automatically\nwell-realized speech sounds with high confidence. The elitist approach uses a\nspeech recognition system based on Hidden Markov Models (HMM). The HMM are\ntrained on speech sounds which are systematically well-detected in an iterative\nprocedure. The results show that, by using the HMM models defined in the\ntraining phase, the speech recognizer detects reliably specific speech sounds\nwith a small rate of errors."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0512102v1", 
    "title": "Statistical Parameters of the Novel \"Perekhresni stezhky\" (\"The   Cross-Paths\") by Ivan Franko", 
    "arxiv-id": "cs/0512102v1", 
    "author": "Andrij Rovenchak", 
    "publish": "2005-12-28T13:45:54Z", 
    "summary": "In the paper, a complex statistical characteristics of a Ukrainian novel is\ngiven for the first time. The distribution of word-forms with respect to their\nsize is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath\nare analyzed."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0601005v1", 
    "title": "Analyzing language development from a network approach", 
    "arxiv-id": "cs/0601005v1", 
    "author": "Y. Yao", 
    "publish": "2006-01-04T18:33:53Z", 
    "summary": "In this paper we propose some new measures of language development using\nnetwork analyses, which is inspired by the recent surge of interests in network\nstudies of many real-world systems. Children's and care-takers' speech data\nfrom a longitudinal study are represented as a series of networks, word forms\nbeing taken as nodes and collocation of words as links. Measures on the\nproperties of the networks, such as size, connectivity, hub and authority\nanalyses, etc., allow us to make quantitative comparison so as to reveal\ndifferent paths of development. For example, the asynchrony of development in\nnetwork size and average degree suggests that children cannot be simply\nclassified as early talkers or late talkers by one or two measures. Children\nfollow different paths in a multi-dimensional space. They may develop faster in\none dimension but slower in another dimension. The network approach requires\nlittle preprocessing of words and analyses on sentence structures, and the\ncharacteristics of words and their usage emerge from the network and are\nindependent of any grammatical presumptions. We show that the change of the two\narticles \"the\" and \"a\" in their roles as important nodes in the network\nreflects the progress of children's syntactic development: the two articles\noften start in children's networks as hubs and later shift to authorities,\nwhile they are authorities constantly in the adult's networks. The network\nanalyses provide a new approach to study language development, and at the same\ntime language development also presents a rich area for network theories to\nexplore."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0604027v1", 
    "title": "Unification of multi-lingual scientific terminological resources using   the ISO 16642 standard. The TermSciences initiative", 
    "arxiv-id": "cs/0604027v1", 
    "author": "the termsciences Collaboration", 
    "publish": "2006-04-07T13:10:30Z", 
    "summary": "This paper presents the TermSciences portal, which deals with the\nimplementation of a conceptual model that uses the recent ISO 16642 standard\n(Terminological Markup Framework). This standard turns out to be suitable for\nconcept modeling since it allowed for organizing the original resources by\nconcepts and to associate the various terms for a given concept. Additional\nstructuring is produced by sharing conceptual relationships, that is,\ncross-linking of resource results through the introduction of semantic\nrelations which may have initially be missing."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0606006v1", 
    "title": "Foundations of Modern Language Resource Archives", 
    "arxiv-id": "cs/0606006v1", 
    "author": "Laurent Romary", 
    "publish": "2006-06-01T09:14:51Z", 
    "summary": "A number of serious reasons will convince an increasing amount of researchers\nto store their relevant material in centers which we will call \"language\nresource archives\". They combine the duty of taking care of long-term\npreservation as well as the task to give access to their material to different\nuser groups. Access here is meant in the sense that an active interaction with\nthe data will be made possible to support the integration of new data, new\nversions or commentaries of all sort. Modern Language Resource Archives will\nhave to adhere to a number of basic principles to fulfill all requirements and\nthey will have to be involved in federations to create joint language resource\ndomains making it even more simple for the researchers to access the data. This\npaper makes an attempt to formulate the essential pillars language resource\narchives have to adhere to."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0606096v1", 
    "title": "Building a resource for studying translation shifts", 
    "arxiv-id": "cs/0606096v1", 
    "author": "Lea Cyrus", 
    "publish": "2006-06-22T13:26:52Z", 
    "summary": "This paper describes an interdisciplinary approach which brings together the\nfields of corpus linguistics and translation studies. It presents ongoing work\non the creation of a corpus resource in which translation shifts are explicitly\nannotated. Translation shifts denote departures from formal correspondence\nbetween source and target text, i.e. deviations that have occurred during the\ntranslation process. A resource in which such shifts are annotated in a\nsystematic way will make it possible to study those phenomena that need to be\naddressed if machine translation output is to resemble human translation. The\nresource described in this paper contains English source texts (parliamentary\nproceedings) and their German translations. The shift annotation is based on\npredicate-argument structures and proceeds in two steps: first, predicates and\ntheir arguments are annotated monolingually in a straightforward manner. Then,\nthe corresponding English and German predicates and arguments are aligned with\neach other. Whenever a shift - mainly grammatical or semantic -has occurred,\nthe alignment is tagged accordingly."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0607051v1", 
    "title": "Raisonner avec des diagrammes : perspectives cognitives et   computationnelles", 
    "arxiv-id": "cs/0607051v1", 
    "author": "Catherine Recanati", 
    "publish": "2006-07-11T19:13:25Z", 
    "summary": "Diagrammatic, analogical or iconic representations are often contrasted with\nlinguistic or logical representations, in which the shape of the symbols is\narbitrary. The aim of this paper is to make a case for the usefulness of\ndiagrams in inferential knowledge representation systems. Although commonly\nused, diagrams have for a long time suffered from the reputation of being only\na heuristic tool or a mere support for intuition. The first part of this paper\nis an historical background paying tribute to the logicians, psychologists and\ncomputer scientists who put an end to this formal prejudice against diagrams.\nThe second part is a discussion of their characteristics as opposed to those of\nlinguistic forms. The last part is aimed at reviving the interest for\nheterogeneous representation systems including both linguistic and diagrammatic\nrepresentations."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0609019v1", 
    "title": "Improving Term Extraction with Terminological Resources", 
    "arxiv-id": "cs/0609019v1", 
    "author": "Thierry Hamon", 
    "publish": "2006-09-06T11:41:27Z", 
    "summary": "Studies of different term extractors on a corpus of the biomedical domain\nrevealed decreasing performances when applied to highly technical texts. The\ndifficulty or impossibility of customising them to new domains is an additional\nlimitation. In this paper, we propose to use external terminologies to\ninfluence generic linguistic data in order to augment the quality of the\nextraction. The tool we implemented exploits testified terms at different steps\nof the process: chunking, parsing and extraction of term candidates.\nExperiments reported here show that, using this method, more term candidates\ncan be acquired with a higher level of reliability. We further describe the\nextraction process involving endogenous disambiguation implemented in the term\nextractor YaTeA."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0609043v1", 
    "title": "Challenging the principle of compositionality in interpreting natural   language texts", 
    "arxiv-id": "cs/0609043v1", 
    "author": "Fran\u00e7ois L\u00e9vy", 
    "publish": "2006-09-08T14:01:57Z", 
    "summary": "The paper aims at emphasizing that, even relaxed, the hypothesis of\ncompositionality has to face many problems when used for interpreting natural\nlanguage texts. Rather than fixing these problems within the compositional\nframework, we believe that a more radical change is necessary, and propose\nanother approach."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0609044v1", 
    "title": "The role of time in considering collections", 
    "arxiv-id": "cs/0609044v1", 
    "author": "Fran\u00e7ois L\u00e9vy", 
    "publish": "2006-09-08T14:11:50Z", 
    "summary": "The paper concerns the understanding of plurals in the framework of\nArtificial Intelligence and emphasizes the role of time. The construction of\ncollection(s) and their evolution across time is often crucial and has to be\naccounted for. The paper contrasts a \"de dicto\" collection where the collection\ncan be considered as persisting over these situations even if its members\nchange with a \"de re\" collection whose composition does not vary through time.\nIt expresses different criteria of choice between the two interpretations (de\nre and de dicto) depending on the context of enunciation."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0609058v1", 
    "title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+   languages", 
    "arxiv-id": "cs/0609058v1", 
    "author": "Daniel Varga", 
    "publish": "2006-09-12T07:10:15Z", 
    "summary": "We present a new, unique and freely available parallel corpus containing\nEuropean Union (EU) documents of mostly legal nature. It is available in all 20\nofficial EUanguages, with additional documents being available in the languages\nof the EU candidate countries. The corpus consists of almost 8,000 documents\nper language, with an average size of nearly 9 million words per language.\nPair-wise paragraph alignment information produced by two different aligners\n(Vanilla and HunAlign) is available for all 190+ language pair combinations.\nMost texts have been manually classified according to the EUROVOC subject\ndomains so that the collection can also be used to train and test multi-label\nclassification algorithms and keyword-assignment software. The corpus is\nencoded in XML, according to the Text Encoding Initiative Guidelines. Due to\nthe large number of parallel texts in many languages, the JRC-Acquis is\nparticularly suitable to carry out all types of cross-language research, as\nwell as to test and benchmark text analysis software across different languages\n(for instance for alignment, sentence splitting and term extraction)."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0610116v1", 
    "title": "DepAnn - An Annotation Tool for Dependency Treebanks", 
    "arxiv-id": "cs/0610116v1", 
    "author": "Tuomo Kakkonen", 
    "publish": "2006-10-19T17:42:57Z", 
    "summary": "DepAnn is an interactive annotation tool for dependency treebanks, providing\nboth graphical and text-based annotation interfaces. The tool is aimed for\nsemi-automatic creation of treebanks. It aids the manual inspection and\ncorrection of automatically created parses, making the annotation process\nfaster and less error-prone. A novel feature of the tool is that it enables the\nuser to view outputs from several parsers as the basis for creating the final\ntree to be saved to the treebank. DepAnn uses TIGER-XML, an XML-based general\nencoding format for both, representing the parser outputs and saving the\nannotated treebank. The tool includes an automatic consistency checker for\nsentence structures. In addition, the tool enables users to build structures\nmanually, add comments on the annotations, modify the tagsets, and mark\nsentences for further revision."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0610124v1", 
    "title": "Dependency Treebanks: Methods, Annotation Schemes and Tools", 
    "arxiv-id": "cs/0610124v1", 
    "author": "Tuomo Kakkonen", 
    "publish": "2006-10-20T11:48:38Z", 
    "summary": "In this paper, current dependencybased treebanks are introduced and analyzed.\nThe methods used for building the resources, the annotation schemes applied,\nand the tools used (such as POS taggers, parsers and annotation software) are\ndiscussed."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0611026v1", 
    "title": "Un mod\u00e8le g\u00e9n\u00e9rique d'organisation de corpus en ligne: application   \u00e0 la FReeBank", 
    "arxiv-id": "cs/0611026v1", 
    "author": "Jean-Marie Pierrel", 
    "publish": "2006-11-06T14:37:27Z", 
    "summary": "The few available French resources for evaluating linguistic models or\nalgorithms on other linguistic levels than morpho-syntax are either\ninsufficient from quantitative as well as qualitative point of view or not\nfreely accessible. Based on this fact, the FREEBANK project intends to create\nFrench corpora constructed using manually revised output from a hybrid\nConstraint Grammar parser and annotated on several linguistic levels\n(structure, morpho-syntax, syntax, coreference), with the objective to make\nthem available on-line for research purposes. Therefore, we will focus on using\nstandard annotation schemes, integration of existing resources and maintenance\nallowing for continuous enrichment of the annotations. Prior to the actual\npresentation of the prototype that has been implemented, this paper describes a\ngeneric model for the organization and deployment of a linguistic resource\narchive, in compliance with the various works currently conducted within\ninternational standardization initiatives (TEI and ISO/TC 37/SC 4)."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0611069v1", 
    "title": "Scaling Construction Grammar up to Production Systems: the SCIM", 
    "arxiv-id": "cs/0611069v1", 
    "author": "Guillaume Pitel", 
    "publish": "2006-11-15T12:35:45Z", 
    "summary": "While a great effort has concerned the development of fully integrated\nmodular understanding systems, few researches have focused on the problem of\nunifying existing linguistic formalisms with cognitive processing models. The\nSituated Constructional Interpretation Model is one of these attempts. In this\nmodel, the notion of \"construction\" has been adapted in order to be able to\nmimic the behavior of Production Systems. The Construction Grammar approach\nestablishes a model of the relations between linguistic forms and meaning, by\nthe mean of constructions. The latter can be considered as pairings from a\ntopologically structured space to an unstructured space, in some way a special\nkind of production rules."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0611113v1", 
    "title": "An Anthological Review of Research Utilizing MontyLingua, a Python-Based   End-to-End Text Processor", 
    "arxiv-id": "cs/0611113v1", 
    "author": "Maurice HT Ling", 
    "publish": "2006-11-22T03:24:54Z", 
    "summary": "MontyLingua, an integral part of ConceptNet which is currently the largest\ncommonsense knowledge base, is an English text processor developed using Python\nprogramming language in MIT Media Lab. The main feature of MontyLingua is the\ncoverage for all aspects of English text processing from raw input text to\nsemantic meanings and summary generation, yet each component in MontyLingua is\nloosely-coupled to each other at the architectural and code level, which\nenabled individual components to be used independently or substituted. However,\nthere has been no review exploring the role of MontyLingua in recent research\nwork utilizing it. This paper aims to review the use of and roles played by\nMontyLingua and its components in research work published in 19 articles\nbetween October 2004 and August 2006. We had observed a diversified use of\nMontyLingua in many different areas, both generic and domain-specific. Although\nthe use of text summarizing component had not been observe, we are optimistic\nthat it will have a crucial role in managing the current trend of information\noverload in future research."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0701135v1", 
    "title": "Complex networks and human language", 
    "arxiv-id": "cs/0701135v1", 
    "author": "Jinyun KE", 
    "publish": "2007-01-22T00:45:31Z", 
    "summary": "This paper introduces how human languages can be studied in light of recent\ndevelopment of network theories. There are two directions of exploration. One\nis to study networks existing in the language system. Various lexical networks\ncan be built based on different relationships between words, being semantic or\nsyntactic. Recent studies have shown that these lexical networks exhibit\nsmall-world and scale-free features. The other direction of exploration is to\nstudy networks of language users (i.e. social networks of people in the\nlinguistic community), and their role in language evolution. Social networks\nalso show small-world and scale-free features, which cannot be captured by\nrandom or regular network models. In the past, computational models of language\nchange and language emergence often assume a population to have a random or\nregular structure, and there has been little discussion how network structures\nmay affect the dynamics. In the second part of the paper, a series of\nsimulation models of diffusion of linguistic innovation are used to illustrate\nthe importance of choosing realistic conditions of population structure for\nmodeling language change. Four types of social networks are compared, which\nexhibit two categories of diffusion dynamics. While the questions about which\ntype of networks are more appropriate for modeling still remains, we give some\npreliminary suggestions for choosing the type of social networks for modeling."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0701181v1", 
    "title": "A Note on Local Ultrametricity in Text", 
    "arxiv-id": "cs/0701181v1", 
    "author": "Fionn Murtagh", 
    "publish": "2007-01-27T19:09:53Z", 
    "summary": "High dimensional, sparsely populated data spaces have been characterized in\nterms of ultrametric topology. This implies that there are natural, not\nnecessarily unique, tree or hierarchy structures defined by the ultrametric\ntopology. In this note we study the extent of local ultrametric topology in\ntexts, with the aim of finding unique ``fingerprints'' for a text or corpus,\ndiscriminating between texts from different domains, and opening up the\npossibility of exploiting hierarchical structures in the data. We use coherent\nand meaningful collections of over 1000 texts, comprising over 1.3 million\nwords."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0701194v1", 
    "title": "Menzerath-Altmann Law for Syntactic Structures in Ukrainian", 
    "arxiv-id": "cs/0701194v1", 
    "author": "Andrij Rovenchak", 
    "publish": "2007-01-30T16:58:07Z", 
    "summary": "In the paper, the definition of clause suitable for an automated processing\nof a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the\nsentence level and the parameters for the dependences of the clause length\ncounted in words and syllables on the sentence length counted in clauses are\ncalculated for \"Perekhresni Stezhky\" (\"The Cross-Paths\"), a novel by Ivan\nFranko."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/cs/0702081v1", 
    "title": "Random Sentences from a Generalized Phrase-Structure Grammar Interpreter", 
    "arxiv-id": "cs/0702081v1", 
    "author": "Rick Dale", 
    "publish": "2007-02-14T06:05:20Z", 
    "summary": "In numerous domains in cognitive science it is often useful to have a source\nfor randomly generated corpora. These corpora may serve as a foundation for\nartificial stimuli in a learning experiment (e.g., Ellefson & Christiansen,\n2000), or as input into computational models (e.g., Christiansen & Dale, 2001).\nThe following compact and general C program interprets a phrase-structure\ngrammar specified in a text file. It follows parameters set at a Unix or\nUnix-based command-line and generates a corpus of random sentences from that\ngrammar."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0704.3708v2", 
    "title": "Network statistics on early English Syntax: Structural criteria", 
    "arxiv-id": "0704.3708v2", 
    "author": "Bernat Corominas-Murtra", 
    "publish": "2007-04-27T17:13:37Z", 
    "summary": "This paper includes a reflection on the role of networks in the study of\nEnglish language acquisition, as well as a collection of practical criteria to\nannotate free-speech corpora from children utterances. At the theoretical\nlevel, the main claim of this paper is that syntactic networks should be\ninterpreted as the outcome of the use of the syntactic machinery. Thus, the\nintrinsic features of such machinery are not accessible directly from (known)\nnetwork properties. Rather, what one can see are the global patterns of its use\nand, thus, a global view of the power and organization of the underlying\ngrammar. Taking a look into more practical issues, the paper examines how to\nbuild a net from the projection of syntactic relations. Recall that, as opposed\nto adult grammars, early-child language has not a well-defined concept of\nstructure. To overcome such difficulty, we develop a set of systematic criteria\nassuming constituency hierarchy and a grammar based on lexico-thematic\nrelations. At the end, what we obtain is a well defined corpora annotation that\nenables us i) to perform statistics on the size of structures and ii) to build\na network from syntactic relations over which we can perform the standard\nmeasures of complexity. We also provide a detailed example."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0707.3269v1", 
    "title": "International Standard for a Linguistic Annotation Framework", 
    "arxiv-id": "0707.3269v1", 
    "author": "Nancy Ide", 
    "publish": "2007-07-22T15:24:48Z", 
    "summary": "This paper describes the Linguistic Annotation Framework under development\nwithin ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to\nserve as a basis for harmonizing existing language resources as well as\ndeveloping new ones."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0707.3270v1", 
    "title": "A Formal Model of Dictionary Structure and Content", 
    "arxiv-id": "0707.3270v1", 
    "author": "Adam Kilgarriff", 
    "publish": "2007-07-22T15:25:27Z", 
    "summary": "We show that a general model of lexical information conforms to an abstract\nmodel that reflects the hierarchy of information found in a typical dictionary\nentry. We show that this model can be mapped into a well-formed XML document,\nand how the XSL transformation language can be used to implement a semantics\ndefined over the abstract model to enable extraction and manipulation of the\ninformation in any format."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0708.1564v1", 
    "title": "Learning Phonotactics Using ILP", 
    "arxiv-id": "0708.1564v1", 
    "author": "Stasinos Konstantopoulos", 
    "publish": "2007-08-11T13:09:27Z", 
    "summary": "This paper describes experiments on learning Dutch phonotactic rules using\nInductive Logic Programming, a machine learning discipline based on inductive\nlogical operators. Two different ways of approaching the problem are\nexperimented with, and compared against each other as well as with related work\non the task. The results show a direct correspondence between the quality and\ninformedness of the background knowledge and the constructed theory,\ndemonstrating the ability of ILP to take good advantage of the prior domain\nknowledge available. Further research is outlined."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0709.2401v1", 
    "title": "Bootstrapping Deep Lexical Resources: Resources for Courses", 
    "arxiv-id": "0709.2401v1", 
    "author": "Timothy Baldwin", 
    "publish": "2007-09-15T01:37:21Z", 
    "summary": "We propose a range of deep lexical acquisition methods which make use of\nmorphological, syntactic and ontological language resources to model word\nsimilarity and bootstrap from a seed lexicon. The different methods are\ndeployed in learning lexical items for a precision grammar, and shown to each\nhave strengths and weaknesses over different word classes. A particular focus\nof this paper is the relative accessibility of different language resource\ntypes, and predicted ``bang for the buck'' associated with each in deep lexical\nacquisition applications."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0710.0225v1", 
    "title": "On the role of autocorrelations in texts", 
    "arxiv-id": "0710.0225v1", 
    "author": "A. A. Snarskii", 
    "publish": "2007-10-01T08:23:24Z", 
    "summary": "The task of finding a criterion allowing to distinguish a text from an\narbitrary set of words is rather relevant in itself, for instance, in the\naspect of development of means for internet-content indexing or separating\nsignals and noise in communication channels. The Zipf law is currently\nconsidered to be the most reliable criterion of this kind [3]. At any rate,\nconventional stochastic word sets do not meet this law. The present paper deals\nwith one of possible criteria based on the determination of the degree of data\ncompression."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0710.0228v1", 
    "title": "On the fractal nature of mutual relevance sequences in the Internet news   message flows", 
    "arxiv-id": "0710.0228v1", 
    "author": "A. Snarskii", 
    "publish": "2007-10-01T08:31:56Z", 
    "summary": "In the task of information retrieval the term relevance is taken to mean\nformal conformity of a document given by the retrieval system to user's\ninformation query. As a rule, the documents found by the retrieval system\nshould be submitted to the user in a certain order. Therefore, a retrieval\nperceived as a selection of documents formally solving the user's query, should\nbe supplemented with a certain procedure of processing a relevant set. It would\nbe natural to introduce a quantitative measure of document conformity to query,\ni.e. the relevance measure. Since no single rule exists for the determination\nof the relevance measure, we shall consider two of them which are the simplest\nin our opinion. The proposed approach does not suppose any restrictions and can\nbe applied to other relevance measures."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0710.2852v1", 
    "title": "Generating models for temporal representations", 
    "arxiv-id": "0710.2852v1", 
    "author": "S\u00e9bastien Hinderer", 
    "publish": "2007-10-15T15:45:13Z", 
    "summary": "We discuss the use of model building for temporal representations. We chose\nPolish to illustrate our discussion because it has an interesting aspectual\nsystem, but the points we wish to make are not language specific. Rather, our\ngoal is to develop theoretical and computational tools for temporal model\nbuilding tasks in computational semantics. To this end, we present a\nfirst-order theory of time and events which is rich enough to capture\ninteresting semantic distinctions, and an algorithm which takes minimal models\nfor first-order theories and systematically attempts to ``perturb'' their\ntemporal component to provide non-minimal, but semantically significant,\nmodels."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0710.2988v1", 
    "title": "Using Description Logics for Recognising Textual Entailment", 
    "arxiv-id": "0710.2988v1", 
    "author": "Paul Bedaride", 
    "publish": "2007-10-16T09:16:24Z", 
    "summary": "The aim of this paper is to show how we can handle the Recognising Textual\nEntailment (RTE) task by using Description Logics (DLs). To do this, we propose\na representation of natural language semantics in DLs inspired by existing\nrepresentations in first-order logic. But our most significant contribution is\nthe definition of two novel inference tasks: A-Box saturation and subgraph\ndetection which are crucial for our approach to RTE."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0710.5382v1", 
    "title": "Some Reflections on the Task of Content Determination in the Context of   Multi-Document Summarization of Evolving Events", 
    "arxiv-id": "0710.5382v1", 
    "author": "Stergos D. Afantenos", 
    "publish": "2007-10-29T10:48:48Z", 
    "summary": "Despite its importance, the task of summarizing evolving events has received\nsmall attention by researchers in the field of multi-document summariztion. In\na previous paper (Afantenos et al. 2007) we have presented a methodology for\nthe automatic summarization of documents, emitted by multiple sources, which\ndescribe the evolution of an event. At the heart of this methodology lies the\nidentification of similarities and differences between the various documents,\nin two axes: the synchronic and the diachronic. This is achieved by the\nintroduction of the notion of Synchronic and Diachronic Relations. Those\nrelations connect the messages that are found in the documents, resulting thus\nin a graph which we call grid. Although the creation of the grid completes the\nDocument Planning phase of a typical NLG architecture, it can be the case that\nthe number of messages contained in a grid is very large, exceeding thus the\nrequired compression rate. In this paper we provide some initial thoughts on a\nprobabilistic model which can be applied at the Content Determination stage,\nand which tries to alleviate this problem."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.0666v1", 
    "title": "Discriminative Phoneme Sequences Extraction for Non-Native Speaker's   Origin Classification", 
    "arxiv-id": "0711.0666v1", 
    "author": "Jean-Paul Haton", 
    "publish": "2007-11-05T15:20:47Z", 
    "summary": "In this paper we present an automated method for the classification of the\norigin of non-native speakers. The origin of non-native speakers could be\nidentified by a human listener based on the detection of typical pronunciations\nfor each nationality. Thus we suppose the existence of several phoneme\nsequences that might allow the classification of the origin of non-native\nspeakers. Our new method is based on the extraction of discriminative sequences\nof phonemes from a non-native English speech database. These sequences are used\nto construct a probabilistic classifier for the speakers' origin. The existence\nof discriminative phone sequences in non-native speech is a significant result\nof this work. The system that we have developed achieved a significant correct\nclassification rate of 96.3% and a significant error reduction compared to some\nother tested techniques."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.0811v1", 
    "title": "Combined Acoustic and Pronunciation Modelling for Non-Native Speech   Recognition", 
    "arxiv-id": "0711.0811v1", 
    "author": "Irina Illina", 
    "publish": "2007-11-06T08:23:49Z", 
    "summary": "In this paper, we present several adaptation methods for non-native speech\nrecognition. We have tested pronunciation modelling, MLLR and MAP non-native\npronunciation adaptation and HMM models retraining on the HIWIRE foreign\naccented English speech database. The ``phonetic confusion'' scheme we have\ndeveloped consists in associating to each spoken phone several sequences of\nconfused phones. In our experiments, we have used different combinations of\nacoustic models representing the canonical and the foreign pronunciations:\nspoken and native models, models adapted to the non-native accent with MAP and\nMLLR. The joint use of pronunciation modelling and acoustic adaptation led to\nfurther improvements in recognition accuracy. The best combination of the above\nmentioned techniques resulted in a relative word error reduction ranging from\n46% to 71%."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.1038v1", 
    "title": "Am\u00e9lioration des Performances des Syst\u00e8mes Automatiques de   Reconnaissance de la Parole pour la Parole Non Native", 
    "arxiv-id": "0711.1038v1", 
    "author": "Jean-Paul Haton", 
    "publish": "2007-11-07T08:51:09Z", 
    "summary": "In this article, we present an approach for non native automatic speech\nrecognition (ASR). We propose two methods to adapt existing ASR systems to the\nnon-native accents. The first method is based on the modification of acoustic\nmodels through integration of acoustic models from the mother tong. The\nphonemes of the target language are pronounced in a similar manner to the\nnative language of speakers. We propose to combine the models of confused\nphonemes so that the ASR system could recognize both concurrent\npronounciations. The second method we propose is a refinment of the\npronounciation error detection through the introduction of graphemic\nconstraints. Indeed, non native speakers may rely on the writing of words in\ntheir uttering. Thus, the pronounctiation errors might depend on the characters\ncomposing the words. The average error rate reduction that we observed is\n(22.5%) relative for the sentence error rate, and 34.5% (relative) in word\nerror rate."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.2444v1", 
    "title": "Proof nets for display logic", 
    "arxiv-id": "0711.2444v1", 
    "author": "Richard Moot", 
    "publish": "2007-11-15T15:39:48Z", 
    "summary": "This paper explores several extensions of proof nets for the Lambek calculus\nin order to handle the different connectives of display logic in a natural way.\nThe new proof net calculus handles some recent additions to the Lambek\nvocabulary such as Galois connections and Grishin interactions. It concludes\nwith an exploration of the generative capacity of the Lambek-Grishin calculus,\npresenting an embedding of lexicalized tree adjoining grammars into the\nLambek-Grishin calculus."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3412v1", 
    "title": "Morphological annotation of Korean with Directly Maintainable Resources", 
    "arxiv-id": "0711.3412v1", 
    "author": "Jee-Sun Nam", 
    "publish": "2007-11-21T16:47:57Z", 
    "summary": "This article describes an exclusively resource-based method of morphological\nannotation of written Korean text. Korean is an agglutinative language. Our\nannotator is designed to process text before the operation of a syntactic\nparser. In its present state, it annotates one-stem words only. The output is a\ngraph of morphemes annotated with accurate linguistic information. The\ngranularity of the tagset is 3 to 5 times higher than usual tagsets. A\ncomparison with a reference annotated corpus showed that it achieves 89% recall\nwithout any corpus training. The language resources used by the system are\nlexicons of stems, transducers of suffixes and transducers of generation of\nallomorphs. All can be easily updated, which allows users to control the\nevolution of the performances of the system. It has been claimed that\nmorphological annotation of Korean text could only be performed by a\nmorphological analysis module accessing a lexicon of morphemes. We show that it\ncan also be performed directly with a lexicon of words and without applying\nmorphological rules at annotation time, which speeds up annotation to 1,210\nword/s. The lexicon of words is obtained from the maintainable language\nresources through a fully automated compilation process."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3449v1", 
    "title": "Lexicon management and standard formats", 
    "arxiv-id": "0711.3449v1", 
    "author": "Eric Laporte", 
    "publish": "2007-11-21T20:34:08Z", 
    "summary": "International standards for lexicon formats are in preparation. To a certain\nextent, the proposed formats converge with prior results of standardization\nprojects. However, their adequacy for (i) lexicon management and (ii)\nlexicon-driven applications have been little debated in the past, nor are they\nas a part of the present standardization effort. We examine these issues. IGM\nhas developed XML formats compatible with the emerging international standards,\nand we report experimental results on large-coverage lexica."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3452v1", 
    "title": "In memoriam Maurice Gross", 
    "arxiv-id": "0711.3452v1", 
    "author": "Eric Laporte", 
    "publish": "2007-11-21T20:38:29Z", 
    "summary": "Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural\nlanguage processing. This article is written in homage to his memory"
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3453v1", 
    "title": "A resource-based Korean morphological annotation system", 
    "arxiv-id": "0711.3453v1", 
    "author": "Eric Laporte", 
    "publish": "2007-11-21T20:41:59Z", 
    "summary": "We describe a resource-based method of morphological annotation of written\nKorean text. Korean is an agglutinative language. The output of our system is a\ngraph of morphemes annotated with accurate linguistic information. The language\nresources used by the system can be easily updated, which allows us-ers to\ncontrol the evolution of the per-formances of the system. We show that\nmorphological annotation of Korean text can be performed directly with a\nlexicon of words and without morpho-logical rules."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3454v1", 
    "title": "Graphes param\u00e9tr\u00e9s et outils de lexicalisation", 
    "arxiv-id": "0711.3454v1", 
    "author": "S\u00e9bastien Paumier", 
    "publish": "2007-11-21T20:44:04Z", 
    "summary": "Shifting to a lexicalized grammar reduces the number of parsing errors and\nimproves application results. However, such an operation affects a syntactic\nparser in all its aspects. One of our research objectives is to design a\nrealistic model for grammar lexicalization. We carried out experiments for\nwhich we used a grammar with a very simple content and formalism, and a very\ninformative syntactic lexicon, the lexicon-grammar of French elaborated by the\nLADL. Lexicalization was performed by applying the parameterized-graph\napproach. Our results tend to show that most information in the lexicon-grammar\ncan be transferred into a grammar and exploited successfully for the syntactic\nparsing of sentences."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3457v1", 
    "title": "Evaluation of a Grammar of French Determiners", 
    "arxiv-id": "0711.3457v1", 
    "author": "Eric Laporte", 
    "publish": "2007-11-21T20:49:21Z", 
    "summary": "Existing syntactic grammars of natural languages, even with a far from\ncomplete coverage, are complex objects. Assessments of the quality of parts of\nsuch grammars are useful for the validation of their construction. We evaluated\nthe quality of a grammar of French determiners that takes the form of a\nrecursive transition network. The result of the application of this local\ngrammar gives deeper syntactic information than chunking or information\navailable in treebanks. We performed the evaluation by comparison with a corpus\nindependently annotated with information on determiners. We obtained 86%\nprecision and 92% recall on text not tagged for parts of speech."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3605v1", 
    "title": "Very strict selectional restrictions", 
    "arxiv-id": "0711.3605v1", 
    "author": "Maria Carmelita P. Dias", 
    "publish": "2007-11-22T15:54:31Z", 
    "summary": "We discuss the characteristics and behaviour of two parallel classes of verbs\nin two Romance languages, French and Portuguese. Examples of these verbs are\nPort. abater [gado] and Fr. abattre [b\\'etail], both meaning \"slaughter\n[cattle]\". In both languages, the definition of the class of verbs includes\nseveral features: - They have only one essential complement, which is a direct\nobject. - The nominal distribution of the complement is very limited, i.e., few\nnouns can be selected as head nouns of the complement. However, this selection\nis not restricted to a single noun, as would be the case for verbal idioms such\nas Fr. monter la garde \"mount guard\". - We excluded from the class\nconstructions which are reductions of more complex constructions, e.g. Port.\nafinar [instrumento] com \"tune [instrument] with\"."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3691v2", 
    "title": "Outilex, plate-forme logicielle de traitement de textes \u00e9crits", 
    "arxiv-id": "0711.3691v2", 
    "author": "Eric Laporte", 
    "publish": "2007-11-23T09:45:13Z", 
    "summary": "The Outilex software platform, which will be made available to research,\ndevelopment and industry, comprises software components implementing all the\nfundamental operations of written text processing: processing without lexicons,\nexploitation of lexicons and grammars, language resource management. All data\nare structured in XML formats, and also in more compact formats, either\nreadable or binary, whenever necessary; the required format converters are\nincluded in the platform; the grammar formats allow for combining statistical\napproaches with resource-based approaches. Manually constructed lexicons for\nFrench and English, originating from the LADL, and of substantial coverage,\nwill be distributed with the platform under LGPL-LR license."
},{
    "category": "cs.CL", 
    "doi": "10.2478/v10057-009-0008-3", 
    "link": "http://arxiv.org/pdf/0711.3726v1", 
    "title": "Let's get the student into the driver's seat", 
    "arxiv-id": "0711.3726v1", 
    "author": "Stergos D. Afantenos", 
    "publish": "2007-11-23T13:44:55Z", 
    "summary": "Speaking a language and achieving proficiency in another one is a highly\ncomplex process which requires the acquisition of various kinds of knowledge\nand skills, like the learning of words, rules and patterns and their connection\nto communicative goals (intentions), the usual starting point. To help the\nlearner to acquire these skills we propose an enhanced, electronic version of\nan age old method: pattern drills (henceforth PDs). While being highly regarded\nin the fifties, PDs have become unpopular since then, partially because of\ntheir lack of grounding (natural context) and rigidity. Despite these\nshortcomings we do believe in the virtues of this approach, at least with\nregard to the acquisition of basic linguistic reflexes or skills (automatisms),\nnecessary to survive in the new language. Of course, the method needs\nimprovement, and we will show here how this can be achieved. Unlike tapes or\nbooks, computers are open media, allowing for dynamic changes, taking users'\nperformances and preferences into account. Building an electronic version of\nPDs amounts to building an open resource, accomodatable to the users' ever\nchanging needs."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0711.4475v6", 
    "title": "Valence extraction using EM selection and co-occurrence matrices", 
    "arxiv-id": "0711.4475v6", 
    "author": "\u0141ukasz D\u0119bowski", 
    "publish": "2007-11-28T12:16:08Z", 
    "summary": "This paper discusses two new procedures for extracting verb valences from raw\ntexts, with an application to the Polish language. The first novel technique,\nthe EM selection algorithm, performs unsupervised disambiguation of valence\nframe forests, obtained by applying a non-probabilistic deep grammar parser and\nsome post-processing to the text. The second new idea concerns filtering of\nincorrect frames detected in the parsed text and is motivated by an observation\nthat verbs which take similar arguments tend to have similar frames. This\nphenomenon is described in terms of newly introduced co-occurrence matrices.\nUsing co-occurrence matrices, we split filtering into two steps. The list of\nvalid arguments is first determined for each verb, whereas the pattern\naccording to which the arguments are combined into frames is computed in the\nfollowing stage. Our best extracted dictionary reaches an $F$-score of 45%,\ncompared to an $F$-score of 39% for the standard frame-based BHT filtering."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0712.3705v1", 
    "title": "Framework and Resources for Natural Language Parser Evaluation", 
    "arxiv-id": "0712.3705v1", 
    "author": "Tuomo Kakkonen", 
    "publish": "2007-12-21T08:55:17Z", 
    "summary": "Because of the wide variety of contemporary practices used in the automatic\nsyntactic parsing of natural languages, it has become necessary to analyze and\nevaluate the strengths and weaknesses of different approaches. This research is\nall the more necessary because there are currently no genre- and\ndomain-independent parsers that are able to analyze unrestricted text with 100%\npreciseness (I use this term to refer to the correctness of analyses assigned\nby a parser). All these factors create a need for methods and resources that\ncan be used to evaluate and compare parsing systems. This research describes:\n(1) A theoretical analysis of current achievements in parsing and parser\nevaluation. (2) A framework (called FEPa) that can be used to carry out\npractical parser evaluations and comparisons. (3) A set of new evaluation\nresources: FiEval is a Finnish treebank under construction, and MGTS and RobSet\nare parser evaluation resources in English. (4) The results of experiments in\nwhich the developed evaluation framework and the two resources for English were\nused for evaluating a set of selected parsers."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0801.3817v1", 
    "title": "Robustness Evaluation of Two CCG, a PCFG and a Link Grammar Parsers", 
    "arxiv-id": "0801.3817v1", 
    "author": "Tuomo Kakkonen", 
    "publish": "2008-01-24T18:41:01Z", 
    "summary": "Robustness in a parser refers to an ability to deal with exceptional\nphenomena. A parser is robust if it deals with phenomena outside its normal\nrange of inputs. This paper reports on a series of robustness evaluations of\nstate-of-the-art parsers in which we concentrated on one aspect of robustness:\nits ability to parse sentences containing misspelled words. We propose two\nmeasures for robustness evaluation based on a comparison of a parser's output\nfor grammatical input sentences and their noisy counterparts. In this paper, we\nuse these measures to compare the overall robustness of the four evaluated\nparsers, and we present an analysis of the decline in parser performance with\nincreasing error levels. Our results indicate that performance typically\ndeclines tens of percentage units when parsers are presented with texts\ncontaining misspellings. When it was tested on our purpose-built test set of\n443 sentences, the best parser in the experiment (C&C parser) was able to\nreturn exactly the same parse tree for the grammatical and ungrammatical\nsentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three\nmisspelled words respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0801.4716v1", 
    "title": "Methods to integrate a language model with semantic information for a   word prediction component", 
    "arxiv-id": "0801.4716v1", 
    "author": "Jean-Yves Antoine", 
    "publish": "2008-01-30T17:10:24Z", 
    "summary": "Most current word prediction systems make use of n-gram language models (LM)\nto estimate the probability of the following word in a phrase. In the past\nyears there have been many attempts to enrich such language models with further\nsyntactic or semantic information. We want to explore the predictive powers of\nLatent Semantic Analysis (LSA), a method that has been shown to provide\nreliable information on long-distance semantic dependencies between words in a\ncontext. We present and evaluate here several methods that integrate LSA-based\ninformation with a standard language model: a semantic cache, partial\nreranking, and different forms of interpolation. We found that all methods show\nsignificant improvements, compared to the 4-gram baseline, and most of them to\na simple cache model as well."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0802.4198v1", 
    "title": "Some properties of the Ukrainian writing system", 
    "arxiv-id": "0802.4198v1", 
    "author": "Andrij Rovenchak", 
    "publish": "2008-02-28T12:58:49Z", 
    "summary": "We investigate the grapheme-phoneme relation in Ukrainian and some properties\nof the Ukrainian version of the Cyrillic alphabet."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0804.0143v1", 
    "title": "Effects of High-Order Co-occurrences on Word Semantic Similarities", 
    "arxiv-id": "0804.0143v1", 
    "author": "Guy Denhi\u00e8re", 
    "publish": "2008-04-01T11:33:39Z", 
    "summary": "A computational model of the construction of word meaning through exposure to\ntexts is built in order to simulate the effects of co-occurrence values on word\nsemantic similarities, paragraph by paragraph. Semantic similarity is here\nviewed as association. It turns out that the similarity between two words W1\nand W2 strongly increases with a co-occurrence, decreases with the occurrence\nof W1 without W2 or W2 without W1, and slightly increases with high-order\nco-occurrences. Therefore, operationalizing similarity as a frequency of\nco-occurrence probably introduces a bias: first, there are cases in which there\nis similarity without co-occurrence and, second, the frequency of co-occurrence\noverestimates similarity."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0804.4584v1", 
    "title": "Feature Unification in TAG Derivation Trees", 
    "arxiv-id": "0804.4584v1", 
    "author": "Joseph Le Roux", 
    "publish": "2008-04-29T11:39:18Z", 
    "summary": "The derivation trees of a tree adjoining grammar provide a first insight into\nthe sentence semantics, and are thus prime targets for generation systems. We\ndefine a formalism, feature-based regular tree grammars, and a translation from\nfeature based tree adjoining grammars into this new formalism. The translation\npreserves the derivation structures of the original grammar, and accounts for\nfeature unification."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.2303v1", 
    "title": "Graph Algorithms for Improving Type-Logical Proof Search", 
    "arxiv-id": "0805.2303v1", 
    "author": "Richard Moot", 
    "publish": "2008-05-15T13:30:08Z", 
    "summary": "Proof nets are a graph theoretical representation of proofs in various\nfragments of type-logical grammar. In spite of this basis in graph theory,\nthere has been relatively little attention to the use of graph theoretic\nalgorithms for type-logical proof search. In this paper we will look at several\nways in which standard graph theoretic algorithms can be used to restrict the\nsearch space. In particular, we will provide an O(n4) algorithm for selecting\nan optimal axiom link at any stage in the proof search as well as a O(kn3)\nalgorithm for selecting the k best proof candidates."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.2537v1", 
    "title": "A toolkit for a generative lexicon", 
    "arxiv-id": "0805.2537v1", 
    "author": "Christian Bassac", 
    "publish": "2008-05-16T13:58:44Z", 
    "summary": "In this paper we describe the conception of a software toolkit designed for\nthe construction, maintenance and collaborative use of a Generative Lexicon. In\norder to ease its portability and spreading use, this tool was built with free\nand open source products. We eventually tested the toolkit and showed it\nfilters the adequate form of anaphoric reference to the modifier in endocentric\ncompounds."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.3366v1", 
    "title": "Computational Representation of Linguistic Structures using   Domain-Specific Languages", 
    "arxiv-id": "0805.3366v1", 
    "author": "Paul O. Samuelsdorff", 
    "publish": "2008-05-21T23:44:06Z", 
    "summary": "We describe a modular system for generating sentences from formal definitions\nof underlying linguistic structures using domain-specific languages. The system\nuses Java in general, Prolog for lexical entries and custom domain-specific\nlanguages based on Functional Grammar and Functional Discourse Grammar\nnotation, implemented using the ANTLR parser generator. We show how linguistic\nand technological parts can be brought together in a natural language\nprocessing system and how domain-specific languages can be used as a tool for\nconsistent formal notation in linguistic description."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.3410v1", 
    "title": "Exploring a type-theoretic approach to accessibility constraint   modelling", 
    "arxiv-id": "0805.3410v1", 
    "author": "Sylvain Pogodalla", 
    "publish": "2008-05-22T08:48:28Z", 
    "summary": "The type-theoretic modelling of DRT that [degroote06] proposed features\ncontinuations for the management of the context in which a clause has to be\ninterpreted. This approach, while keeping the standard definitions of\nquantifier scope, translates the rules of the accessibility constraints of\ndiscourse referents inside the semantic recipes. In this paper, we deal with\nadditional rules for these accessibility constraints. In particular in the case\nof discourse referents introduced by proper nouns, that negation does not\nblock, and in the case of rhetorical relations that structure discourses. We\nshow how this continuation-based approach applies to those accessibility\nconstraints and how we can consider the parallel management of various\nprinciples."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.4369v1", 
    "title": "A semantic space for modeling children's semantic memory", 
    "arxiv-id": "0805.4369v1", 
    "author": "Sandra Jhean", 
    "publish": "2008-05-28T14:56:18Z", 
    "summary": "The goal of this paper is to present a model of children's semantic memory,\nwhich is based on a corpus reproducing the kinds of texts children are exposed\nto. After presenting the literature in the development of the semantic memory,\na preliminary French corpus of 3.2 million words is described. Similarities in\nthe resulting semantic space are compared to human data on four tests:\nassociation norms, vocabulary test, semantic judgments and memory tasks. A\nsecond corpus is described, which is composed of subcorpora corresponding to\nvarious ages. This stratified corpus is intended as a basis for developmental\nstudies. Finally, two applications of these models of semantic memory are\npresented: the first one aims at tracing the development of semantic\nsimilarities paragraph by paragraph; the second one describes an implementation\nof a model of text comprehension derived from the Construction-integration\nmodel (Kintsch, 1988, 1998) and based on such models of semantic memory."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0805.4521v1", 
    "title": "Textual Entailment Recognizing by Theorem Proving Approach", 
    "arxiv-id": "0805.4521v1", 
    "author": "Militon Frentiu", 
    "publish": "2008-05-29T11:53:39Z", 
    "summary": "In this paper we present two original methods for recognizing textual\ninference. First one is a modified resolution method such that some linguistic\nconsiderations are introduced in the unification of two atoms. The approach is\npossible due to the recent methods of transforming texts in logic formulas.\nSecond one is based on semantic relations in text, as presented in WordNet.\nSome similarities between these two methods are remarked."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0806.2581v1", 
    "title": "A chain dictionary method for Word Sense Disambiguation and applications", 
    "arxiv-id": "0806.2581v1", 
    "author": "Militon Frentiu", 
    "publish": "2008-06-16T13:48:55Z", 
    "summary": "A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)\nis that of dictionary-based methods. Various algorithms have as the root Lesk's\nalgorithm, which exploits the sense definitions in the dictionary directly. Our\napproach uses the lexical base WordNet for a new algorithm originated in\nLesk's, namely \"chain algorithm for disambiguation of all words\", CHAD. We show\nhow translation from a language into another one and also text entailment\nverification could be accomplished by this disambiguation."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0806.3787v2", 
    "title": "Computational Approaches to Measuring the Similarity of Short Contexts :   A Review of Applications and Methods", 
    "arxiv-id": "0806.3787v2", 
    "author": "Ted Pedersen", 
    "publish": "2008-06-23T23:27:20Z", 
    "summary": "Measuring the similarity of short written contexts is a fundamental problem\nin Natural Language Processing. This article provides a unifying framework by\nwhich short context problems can be categorized both by their intended\napplication and proposed solution. The goal is to show that various problems\nand methodologies that appear quite different on the surface are in fact very\nclosely related. The axes by which these categorizations are made include the\nformat of the contexts (headed versus headless), the way in which the contexts\nare to be measured (first-order versus second-order similarity), and the\ninformation used to represent the features in the contexts (micro versus macro\nviews). The unifying thread that binds together many short context applications\nand methods is the fact that similarity decisions must be made between contexts\nthat share few (if any) words in common."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0807.0311v1", 
    "title": "About the creation of a parallel bilingual corpora of web-publications", 
    "arxiv-id": "0807.0311v1", 
    "author": "V. V. Zhygalo", 
    "publish": "2008-07-02T09:49:14Z", 
    "summary": "The algorithm of the creation texts parallel corpora was presented. The\nalgorithm is based on the use of \"key words\" in text documents, and on the\nmeans of their automated translation. Key words were singled out by means of\nusing Russian and Ukrainian morphological dictionaries, as well as dictionaries\nof the translation of nouns for the Russian and Ukrainianlanguages. Besides, to\ncalculate the weights of the terms in the documents, empiric-statistic rules\nwere used. The algorithm under consideration was realized in the form of a\nprogram complex, integrated into the content-monitoring InfoStream system. As a\nresult, a parallel bilingual corpora of web-publications containing about 30\nthousand documents, was created"
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0807.3622v1", 
    "title": "TuLiPA: Towards a Multi-Formalism Parsing Environment for Grammar   Engineering", 
    "arxiv-id": "0807.3622v1", 
    "author": "Kilian Evang", 
    "publish": "2008-07-23T09:05:50Z", 
    "summary": "In this paper, we present an open-source parsing environment (Tuebingen\nLinguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar\n(RCG) as a pivot formalism, thus opening the way to the parsing of several\nmildly context-sensitive formalisms. This environment currently supports\ntree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component\nTree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not\nonly of syntactic structures, but also of the corresponding semantic\nrepresentations. It is used for the development of a tree-based grammar for\nGerman."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0808.2904v1", 
    "title": "Investigation of the Zipf-plot of the extinct Meroitic language", 
    "arxiv-id": "0808.2904v1", 
    "author": "Reginald D. Smith", 
    "publish": "2008-08-21T10:54:54Z", 
    "summary": "The ancient and extinct language Meroitic is investigated using Zipf's Law.\nIn particular, since Meroitic is still undeciphered, the Zipf law analysis\nallows us to assess the quality of current texts and possible avenues for\nfuture investigation using statistical techniques."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0808.3563v1", 
    "title": "What It Feels Like To Hear Voices: Fond Memories of Julian Jaynes", 
    "arxiv-id": "0808.3563v1", 
    "author": "Stevan Harnad", 
    "publish": "2008-08-26T18:17:44Z", 
    "summary": "Julian Jaynes's profound humanitarian convictions not only prevented him from\ngoing to war, but would have prevented him from ever kicking a dog. Yet\naccording to his theory, not only are language-less dogs unconscious, but so\ntoo were the speaking/hearing Greeks in the Bicameral Era, when they heard\ngods' voices telling them what to do rather than thinking for themselves. I\nargue that to be conscious is to be able to feel, and that all mammals (and\nprobably lower vertebrates and invertebrates too) feel, hence are conscious.\nJulian Jaynes's brilliant analysis of our concepts of consciousness\nnevertheless keeps inspiring ever more inquiry and insights into the age-old\nmind/body problem and its relation to cognition and language."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0808.3616v3", 
    "title": "Constructing word similarities in Meroitic as an aid to decipherment", 
    "arxiv-id": "0808.3616v3", 
    "author": "Reginald D. Smith", 
    "publish": "2008-08-27T02:02:40Z", 
    "summary": "Meroitic is the still undeciphered language of the ancient civilization of\nKush. Over the years, various techniques for decipherment such as finding a\nbilingual text or cognates from modern or other ancient languages in the Sudan\nand surrounding areas has not been successful. Using techniques borrowed from\ninformation theory and natural language statistics, similar words are paired\nand attempts are made to use currently defined words to extract at least\npartial meaning from unknown words."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0808.3889v1", 
    "title": "Open architecture for multilingual parallel texts", 
    "arxiv-id": "0808.3889v1", 
    "author": "M. T. Carrasco Benitez", 
    "publish": "2008-08-28T11:59:34Z", 
    "summary": "Multilingual parallel texts (abbreviated to parallel texts) are linguistic\nversions of the same content (\"translations\"); e.g., the Maastricht Treaty in\nEnglish and Spanish are parallel texts. This document is about creating an open\narchitecture for the whole Authoring, Translation and Publishing Chain\n(ATP-chain) for the processing of parallel texts."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0809.3250v1", 
    "title": "Using descriptive mark-up to formalize translation quality assessment", 
    "arxiv-id": "0809.3250v1", 
    "author": "Andrey Kutuzov", 
    "publish": "2008-09-18T20:48:13Z", 
    "summary": "The paper deals with using descriptive mark-up to emphasize translation\nmistakes. The author postulates the necessity to develop a standard and formal\nXML-based way of describing translation mistakes. It is considered to be\nimportant for achieving impersonal translation quality assessment. Marked-up\ntranslations can be used in corpus translation studies; moreover, automatic\ntranslation assessment based on marked-up mistakes is possible. The paper\nconcludes with setting up guidelines for further activity within the described\nfield."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0810.0200v1", 
    "title": "Distribution of complexities in the Vai script", 
    "arxiv-id": "0810.0200v1", 
    "author": "Charles Riley", 
    "publish": "2008-10-01T15:53:36Z", 
    "summary": "In the paper, we analyze the distribution of complexities in the Vai script,\nan indigenous syllabic writing system from Liberia. It is found that the\nuniformity hypothesis for complexities fails for this script. The models using\nPoisson distribution for the number of components and hyper-Poisson\ndistribution for connections provide good fits in the case of the Vai script."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0810.1199v1", 
    "title": "Une grammaire formelle du cr\u00e9ole martiniquais pour la g\u00e9n\u00e9ration   automatique", 
    "arxiv-id": "0810.1199v1", 
    "author": "Pascal Vaillant", 
    "publish": "2008-10-07T14:40:19Z", 
    "summary": "In this article, some first elements of a computational modelling of the\ngrammar of the Martiniquese French Creole dialect are presented. The sources of\ninspiration for the modelling is the functional description given by Damoiseau\n(1984), and Pinalie's & Bernabe's (1999) grammar manual. Based on earlier works\nin text generation (Vaillant, 1997), a unification grammar formalism, namely\nTree Adjoining Grammars (TAG), and a modelling of lexical functional categories\nbased on syntactic and semantic properties, are used to implement a grammar of\nMartiniquese Creole which is used in a prototype of text generation system. One\nof the main applications of the system could be its use as a tool software\nsupporting the task of learning Creole as a second language. -- Nous\npr\\'esenterons dans cette communication les premiers travaux de mod\\'elisation\ninformatique d'une grammaire de la langue cr\\'eole martiniquaise, en nous\ninspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du\nmanuel de Pinalie & Bernab\\'e (1999). Prenant appui sur des travaux\nant\\'erieurs en g\\'en\\'eration de texte (Vaillant, 1997), nous utilisons un\nformalisme de grammaires d'unification, les grammaires d'adjonction d'arbres\n(TAG d'apr\\`es l'acronyme anglais), ainsi qu'une mod\\'elisation de cat\\'egories\nlexicales fonctionnelles \\`a base syntaxico-s\\'emantique, pour mettre en oeuvre\nune grammaire du cr\\'eole martiniquais utilisable dans une maquette de\nsyst\\`eme de g\\'en\\'eration automatique. L'un des int\\'er\\^ets principaux de ce\nsyst\\`eme pourrait \\^etre son utilisation comme logiciel outil pour l'aide \\`a\nl'apprentissage du cr\\'eole en tant que langue seconde."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0810.1207v1", 
    "title": "A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common   Syntactic Kernel for Related Dialects", 
    "arxiv-id": "0810.1207v1", 
    "author": "Pascal Vaillant", 
    "publish": "2008-10-07T14:50:59Z", 
    "summary": "This article describes the design of a common syntactic description for the\ncore grammar of a group of related dialects. The common description does not\nrely on an abstract sub-linguistic structure like a metagrammar: it consists in\na single FS-LTAG where the actual specific language is included as one of the\nattributes in the set of attribute types defined for the features. When the\nlang attribute is instantiated, the selected subset of the grammar is\nequivalent to the grammar of one dialect. When it is not, we have a model of a\nhybrid multidialectal linguistic system. This principle is used for a group of\ncreole languages of the West-Atlantic area, namely the French-based Creoles of\nHaiti, Guadeloupe, Martinique and French Guiana."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0811.0579v1", 
    "title": "UNL-French deconversion as transfer & generation from an interlingua   with possible quality enhancement through offline human interaction", 
    "arxiv-id": "0811.0579v1", 
    "author": "Christian Boitet", 
    "publish": "2008-11-04T19:31:58Z", 
    "summary": "We present the architecture of the UNL-French deconverter, which \"generates\"\nfrom the UNL interlingua by first\"localizing\" the UNL form for French, within\nUNL, and then applying slightly adapted but classical transfer and generation\ntechniques, implemented in GETA's Ariane-G5 environment, supplemented by some\nUNL-specific tools. Online interaction can be used during deconversion to\nenhance output quality and is now used for development purposes. We show how\ninteraction could be delayed and embedded in the postedition phase, which would\nthen interact not directly with the output text, but indirectly with several\ncomponents of the deconverter. Interacting online or offline can improve the\nquality not only of the utterance at hand, but also of the utterances processed\nlater, as various preferences may be automatically changed to let the\ndeconverter \"learn\"."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-009-9100-5", 
    "link": "http://arxiv.org/pdf/0811.1260v1", 
    "title": "The Application of Fuzzy Logic to Collocation Extraction", 
    "arxiv-id": "0811.1260v1", 
    "author": "H. S. Dhami", 
    "publish": "2008-11-08T10:44:43Z", 
    "summary": "Collocations are important for many tasks of Natural language processing such\nas information retrieval, machine translation, computational lexicography etc.\nSo far many statistical methods have been used for collocation extraction.\nAlmost all the methods form a classical crisp set of collocation. We propose a\nfuzzy logic approach of collocation extraction to form a fuzzy set of\ncollocations in which each word combination has a certain grade of membership\nfor being collocation. Fuzzy logic provides an easy way to express natural\nlanguage into fuzzy logic rules. Two existing methods; Mutual information and\nt-test have been utilized for the input of the fuzzy inference system. The\nresulting membership function could be easily seen and demonstrated. To show\nthe utility of the fuzzy logic some word pairs have been examined as an\nexample. The working data has been based on a corpus of about one million words\ncontained in different novels constituting project Gutenberg available on\nwww.gutenberg.org. The proposed method has all the advantages of the two\nmethods, while overcoming their drawbacks. Hence it provides a better result\nthan the two methods."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0901.3017v1", 
    "title": "Statistical analysis of the Indus script using $n$-grams", 
    "arxiv-id": "0901.3017v1", 
    "author": "R. Adhikari", 
    "publish": "2009-01-20T12:55:55Z", 
    "summary": "The Indus script is one of the major undeciphered scripts of the ancient\nworld. The small size of the corpus, the absence of bilingual texts, and the\nlack of definite knowledge of the underlying language has frustrated efforts at\ndecipherment since the discovery of the remains of the Indus civilisation.\nRecently, some researchers have questioned the premise that the Indus script\nencodes spoken language. Building on previous statistical approaches, we apply\nthe tools of statistical language processing, specifically $n$-gram Markov\nchains, to analyse the Indus script for syntax. Our main results are that the\nscript has well-defined signs which begin and end texts, that there is\ndirectionality and strong correlations in the sign order, and that there are\ngroups of signs which appear to have identical syntactic function. All these\nrequire no {\\it a priori} suppositions regarding the syntactic or semantic\ncontent of the signs, but follow directly from the statistical analysis. Using\ninformation theoretic measures, we find the information in the script to be\nintermediate between that of a completely random and a completely fixed\nordering of signs. Our study reveals that the Indus script is a structured sign\nsystem showing features of a formal language, but, at present, cannot\nconclusively establish that it encodes {\\it natural} language. Our $n$-gram\nMarkov model is useful for predicting signs which are missing or illegible in a\ncorpus of Indus texts. This work forms the basis for the development of a\nstochastic grammar which can be used to explore the syntax of the Indus script\nin greater detail."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0901.4180v2", 
    "title": "Google distance between words", 
    "arxiv-id": "0901.4180v2", 
    "author": "Alberto J. Evangelista", 
    "publish": "2009-01-27T06:29:10Z", 
    "summary": "Cilibrasi and Vitanyi have demonstrated that it is possible to extract the\nmeaning of words from the world-wide web. To achieve this, they rely on the\nnumber of webpages that are found through a Google search containing a given\nword and they associate the page count to the probability that the word appears\non a webpage. Thus, conditional probabilities allow them to correlate one word\nwith another word's meaning. Furthermore, they have developed a similarity\ndistance function that gauges how closely related a pair of words is. We\npresent a specific counterexample to the triangle inequality for this\nsimilarity distance function."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0902.1033v1", 
    "title": "New Confidence Measures for Statistical Machine Translation", 
    "arxiv-id": "0902.1033v1", 
    "author": "Kamel Sma\u00efli", 
    "publish": "2009-02-06T09:28:58Z", 
    "summary": "A confidence measure is able to estimate the reliability of an hypothesis\nprovided by a machine translation system. The problem of confidence measure can\nbe seen as a process of testing : we want to decide whether the most probable\nsequence of words provided by the machine translation system is correct or not.\nIn the following we describe several original word-level confidence measures\nfor machine translation, based on mutual information, n-gram language model and\nlexical features language model. We evaluate how well they perform individually\nor together, and show that using a combination of confidence measures based on\nmutual information yields a classification error rate as low as 25.1% with an\nF-measure of 0.708."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0902.2345v1", 
    "title": "What's in a Message?", 
    "arxiv-id": "0902.2345v1", 
    "author": "Nicolas Hernandez", 
    "publish": "2009-02-13T17:08:10Z", 
    "summary": "In this paper we present the first step in a larger series of experiments for\nthe induction of predicate/argument structures. The structures that we are\ninducing are very similar to the conceptual structures that are used in Frame\nSemantics (such as FrameNet). Those structures are called messages and they\nwere previously used in the context of a multi-document summarization system of\nevolving events. The series of experiments that we are proposing are\nessentially composed from two stages. In the first stage we are trying to\nextract a representative vocabulary of words. This vocabulary is later used in\nthe second stage, during which we apply to it various clustering approaches in\norder to identify the clusters of predicates and arguments--or frames and\nsemantic roles, to use the jargon of Frame Semantics. This paper presents in\ndetail and evaluates the first stage."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0902.3072v1", 
    "title": "Syntactic variation of support verb constructions", 
    "arxiv-id": "0902.3072v1", 
    "author": "Anastasia Yannacopoulou", 
    "publish": "2009-02-18T08:51:28Z", 
    "summary": "We report experiments about the syntactic variations of support verb\nconstructions, a special type of multiword expressions (MWEs) containing\npredicative nouns. In these expressions, the noun can occur with or without the\nverb, with no clear-cut semantic difference. We extracted from a large French\ncorpus a set of examples of the two situations and derived statistical results\nfrom these data. The extraction involved large-coverage language resources and\nfinite-state techniques. The results show that, most frequently, predicative\nnouns occur without a support verb. This fact has consequences on methods of\nextracting or recognising MWEs."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0903.5168v1", 
    "title": "Mathematical Model for Transformation of Sentences from Active Voice to   Passive Voice", 
    "arxiv-id": "0903.5168v1", 
    "author": "H. S. Dhami", 
    "publish": "2009-03-30T09:45:20Z", 
    "summary": "Formal work in linguistics has both produced and used important mathematical\ntools. Motivated by a survey of models for context and word meaning, syntactic\ncategories, phrase structure rules and trees, an attempt is being made in the\npresent paper to present a mathematical model for structuring of sentences from\nactive voice to passive voice, which is is the form of a transitive verb whose\ngrammatical subject serves as the patient, receiving the action of the verb.\n  For this purpose we have parsed all sentences of a corpus and have generated\nBoolean groups for each of them. It has been observed that when we take\nconstituents of the sentences as subgroups, the sequences of phrases form\npermutation roups. Application of isomorphism property yields permutation\nmapping between the important subgroups. It has resulted in a model for\ntransformation of sentences from active voice to passive voice. A computer\nprogram has been written to enable the software developers to evolve grammar\nsoftware for sentence transformations."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0009506", 
    "link": "http://arxiv.org/pdf/0905.1609v1", 
    "title": "Acquisition of morphological families and derivational series from a   machine readable dictionary", 
    "arxiv-id": "0905.1609v1", 
    "author": "Nabil Hathout", 
    "publish": "2009-05-11T12:17:36Z", 
    "summary": "The paper presents a linguistic and computational model aiming at making the\nmorphological structure of the lexicon emerge from the formal and semantic\nregularities of the words it contains. The model is word-based. The proposed\nmorphological structure consists of (1) binary relations that connect each\nheadword with words that are morphologically related, and especially with the\nmembers of its morphological family and its derivational series, and of (2) the\nanalogies that hold between the words. The model has been tested on the lexicon\nof French using the TLFi machine readable dictionary."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0906.0675v1", 
    "title": "Encoding models for scholarly literature", 
    "arxiv-id": "0906.0675v1", 
    "author": "Laurent Romary", 
    "publish": "2009-06-03T09:53:12Z", 
    "summary": "We examine the issue of digital formats for document encoding, archiving and\npublishing, through the specific example of \"born-digital\" scholarly journal\narticles. We will begin by looking at the traditional workflow of journal\nediting and publication, and how these practices have made the transition into\nthe online domain. We will examine the range of different file formats in which\nelectronic articles are currently stored and published. We will argue strongly\nthat, despite the prevalence of binary and proprietary formats such as PDF and\nMS Word, XML is a far superior encoding choice for journal articles. Next, we\nlook at the range of XML document structures (DTDs, Schemas) which are in\ncommon use for encoding journal articles, and consider some of their strengths\nand weaknesses. We will suggest that, despite the existence of specialized\nschemas intended specifically for journal articles (such as NLM), and more\nbroadly-used publication-oriented schemas such as DocBook, there are strong\narguments in favour of developing a subset or customization of the Text\nEncoding Initiative (TEI) schema for the purpose of journal-article encoding;\nTEI is already in use in a number of journal publication projects, and the\nscale and precision of the TEI tagset makes it particularly appropriate for\nencoding scholarly articles. We will outline the document structure of a\nTEI-encoded journal article, and look in detail at suggested markup patterns\nfor specific features of journal articles."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0906.2415v1", 
    "title": "Without a 'doubt'? Unsupervised discovery of downward-entailing   operators", 
    "arxiv-id": "0906.2415v1", 
    "author": "Richard Ducott", 
    "publish": "2009-06-12T23:50:06Z", 
    "summary": "An important part of textual inference is making deductions involving\nmonotonicity, that is, determining whether a given assertion entails\nrestrictions or relaxations of that assertion. For instance, the statement 'We\nknow the epidemic spread quickly' does not entail 'We know the epidemic spread\nquickly via fleas', but 'We doubt the epidemic spread quickly' entails 'We\ndoubt the epidemic spread quickly via fleas'. Here, we present the first\nalgorithm for the challenging lexical-semantics problem of learning linguistic\nconstructions that, like 'doubt', are downward entailing (DE). Our algorithm is\nunsupervised, resource-lean, and effective, accurately recovering many DE\noperators that are missing from the hand-constructed lists that\ntextual-inference systems currently use."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0906.5114v1", 
    "title": "Non-Parametric Bayesian Areal Linguistics", 
    "arxiv-id": "0906.5114v1", 
    "author": "Hal Daum\u00e9 III", 
    "publish": "2009-06-28T02:32:53Z", 
    "summary": "We describe a statistical model over linguistic areas and phylogeny.\n  Our model recovers known areas and identifies a plausible hierarchy of areal\nfeatures. The use of areas improves genetic reconstruction of languages both\nqualitatively and quantitatively according to a variety of metrics. We model\nlinguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman's\ncoalescent."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.0785v1", 
    "title": "A Bayesian Model for Discovering Typological Implications", 
    "arxiv-id": "0907.0785v1", 
    "author": "Lyle Campbell", 
    "publish": "2009-07-04T18:43:16Z", 
    "summary": "A standard form of analysis for linguistic typology is the universal\nimplication. These implications state facts about the range of extant\nlanguages, such as ``if objects come after verbs, then adjectives come after\nnouns.'' Such implications are typically discovered by painstaking hand\nanalysis over a small sample of languages. We propose a computational model for\nassisting at this process. Our model is able to discover both well-known\nimplications as well as some novel implications that deserve further study.\nMoreover, through a careful application of hierarchical analysis, we are able\nto cope with the well-known sampling problem: languages are not independent."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.0804v1", 
    "title": "Induction of Word and Phrase Alignments for Automatic Document   Summarization", 
    "arxiv-id": "0907.0804v1", 
    "author": "Daniel Marcu", 
    "publish": "2009-07-04T22:57:26Z", 
    "summary": "Current research in automatic single document summarization is dominated by\ntwo effective, yet naive approaches: summarization by sentence extraction, and\nheadline generation via bag-of-words models. While successful in some tasks,\nneither of these models is able to adequately capture the large set of\nlinguistic devices utilized by humans when they produce summaries. One possible\nexplanation for the widespread use of these models is that good techniques have\nbeen developed to extract appropriate training data for them from existing\ndocument/abstract and document/headline corpora. We believe that future\nprogress in automatic summarization will be driven both by the development of\nmore sophisticated, linguistically informed models, as well as a more effective\nleveraging of document/abstract corpora. In order to open the doors to\nsimultaneously achieving both of these goals, we have developed techniques for\nautomatically producing word-to-word and phrase-to-phrase alignments between\ndocuments and their human-written abstracts. These alignments make explicit the\ncorrespondences that exist in such document/abstract pairs, and create a\npotentially rich data source from which complex summarization algorithms may\nlearn. This paper describes experiments we have carried out to analyze the\nability of humans to perform such alignments, and based on these analyses, we\ndescribe experiments for creating them automatically. Our model for the\nalignment task is based on an extension of the standard hidden Markov model,\nand learns to create alignments in a completely unsupervised fashion. We\ndescribe our model in detail and present experimental results that show that\nour model is able to learn to reliably identify word- and phrase-level\nalignments in a corpus of <document,abstract> pairs."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.0806v1", 
    "title": "A Noisy-Channel Model for Document Compression", 
    "arxiv-id": "0907.0806v1", 
    "author": "Daniel Marcu", 
    "publish": "2009-07-04T22:26:47Z", 
    "summary": "We present a document compression system that uses a hierarchical\nnoisy-channel model of text production. Our compression system first\nautomatically derives the syntactic structure of each sentence and the overall\ndiscourse structure of the text given as input. The system then uses a\nstatistical hierarchical model of text production in order to drop\nnon-important syntactic and discourse constituents so as to generate coherent,\ngrammatical document compressions of arbitrary length. The system outperforms\nboth a baseline and a sentence-based compression system that operates by\nsimplifying sequentially all sentences in a text. Our results support the claim\nthat discourse knowledge plays an important role in document summarization."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.0807v1", 
    "title": "A Large-Scale Exploration of Effective Global Features for a Joint   Entity Detection and Tracking Model", 
    "arxiv-id": "0907.0807v1", 
    "author": "Daniel Marcu", 
    "publish": "2009-07-04T22:28:15Z", 
    "summary": "Entity detection and tracking (EDT) is the task of identifying textual\nmentions of real-world entities in documents, extending the named entity\ndetection and coreference resolution task by considering mentions other than\nnames (pronouns, definite descriptions, etc.). Like NE tagging and coreference\nresolution, most solutions to the EDT task separate out the mention detection\naspect from the coreference aspect. By doing so, these solutions are limited to\nusing only local features for learning. In contrast, by modeling both aspects\nof the EDT task simultaneously, we are able to learn using highly complex,\nnon-local features. We develop a new joint EDT model and explore the utility of\nmany features, demonstrating their effectiveness on this task."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.2452v1", 
    "title": "Pattern Based Term Extraction Using ACABIT System", 
    "arxiv-id": "0907.2452v1", 
    "author": "Laurent Romary", 
    "publish": "2009-07-14T21:02:14Z", 
    "summary": "In this paper, we propose a pattern-based term extraction approach for\nJapanese, applying ACABIT system originally developed for French. The proposed\napproach evaluates termhood using morphological patterns of basic terms and\nterm variants. After extracting term candidates, ACABIT system filters out\nnon-terms from the candidates based on log-likelihood. This approach is\nsuitable for Japanese term extraction because most of Japanese terms are\ncompound nouns or simple phrasal patterns."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0907.3781v1", 
    "title": "Un syst\u00e8me modulaire d'acquisition automatique de traductions \u00e0   partir du Web", 
    "arxiv-id": "0907.3781v1", 
    "author": "St\u00e9phanie L\u00e9on", 
    "publish": "2009-07-22T06:25:59Z", 
    "summary": "We present a method of automatic translation (French/English) of Complex\nLexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular\nsystem is based on linguistic properties (compositionality, polysemy, etc.).\nDifferent aspects of the multilingual Web are used to validate candidate\ntranslations and collect new terms. We first build a French corpus of Web pages\nto collect CLU. Three adapted processing stages are applied for each linguistic\nproperty : compositional and non polysemous translations, compositional\npolysemous translations and non compositional translations. Our evaluation on a\nsample of CLU shows that our technique based on the Web can reach a very high\nprecision."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0908.4413v1", 
    "title": "Multiple Retrieval Models and Regression Models for Prior Art Search", 
    "arxiv-id": "0908.4413v1", 
    "author": "Laurent Romary", 
    "publish": "2009-08-30T18:50:19Z", 
    "summary": "This paper presents the system called PATATRAS (PATent and Article Tracking,\nRetrieval and AnalysiS) realized for the IP track of CLEF 2009. Our approach\npresents three main characteristics: 1. The usage of multiple retrieval models\n(KL, Okapi) and term index definitions (lemma, phrase, concept) for the three\nlanguages considered in the present track (English, French, German) producing\nten different sets of ranked results. 2. The merging of the different results\nbased on multiple regression models using an additional validation set created\nfrom the patent collection. 3. The exploitation of patent metadata and of the\ncitation structures for creating restricted initial working sets of patents and\nfor producing a final re-ranking regression model. As we exploit specific\nmetadata of the patent documents and the citation relations only at the\ncreation of initial working sets and during the final post ranking step, our\narchitecture remains generic and easy to extend."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0908.4431v1", 
    "title": "An OLAC Extension for Dravidian Languages", 
    "arxiv-id": "0908.4431v1", 
    "author": "B Prabhulla Chandran Pillai", 
    "publish": "2009-08-30T23:20:41Z", 
    "summary": "OLAC was founded in 2000 for creating online databases of language resources.\nThis paper intends to review the bottom-up distributed character of the project\nand proposes an extension of the architecture for Dravidian languages. An\nontological structure is considered for effective natural language processing\n(NLP) and its advantages over statistical methods are reviewed"
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.1147v1", 
    "title": "Empowering OLAC Extension using Anusaaraka and Effective text processing   using Double Byte coding", 
    "arxiv-id": "0909.1147v1", 
    "author": "B Prabhulla Chandran Pillai", 
    "publish": "2009-09-07T06:29:51Z", 
    "summary": "The paper reviews the hurdles while trying to implement the OLAC extension\nfor Dravidian / Indian languages. The paper further explores the possibilities\nwhich could minimise or solve these problems. In this context, the Chinese\nsystem of text processing and the anusaaraka system are scrutinised."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.2379v1", 
    "title": "Implementation of Rule Based Algorithm for Sandhi-Vicheda Of Compound   Hindi Words", 
    "arxiv-id": "0909.2379v1", 
    "author": "Vishal Goyal", 
    "publish": "2009-09-12T22:27:57Z", 
    "summary": "Sandhi means to join two or more words to coin new word. Sandhi literally\nmeans `putting together' or combining (of sounds), It denotes all combinatory\nsound-changes effected (spontaneously) for ease of pronunciation.\nSandhi-vicheda describes [5] the process by which one letter (whether single or\ncojoined) is broken to form two words. Part of the broken letter remains as the\nlast letter of the first word and part of the letter forms the first letter of\nthe next letter. Sandhi- Vicheda is an easy and interesting way that can give\nentirely new dimension that add new way to traditional approach to Hindi\nTeaching. In this paper using the Rule based algorithm we have reported an\naccuracy of 60-80% depending upon the number of rules to be implemented."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.2626v1", 
    "title": "Reference Resolution within the Framework of Cognitive Grammar", 
    "arxiv-id": "0909.2626v1", 
    "author": "Laurent Romary", 
    "publish": "2009-09-14T19:10:59Z", 
    "summary": "Following the principles of Cognitive Grammar, we concentrate on a model for\nreference resolution that attempts to overcome the difficulties previous\napproaches, based on the fundamental assumption that all reference (independent\non the type of the referring expression) is accomplished via access to and\nrestructuring of domains of reference rather than by direct linkage to the\nentities themselves. The model accounts for entities not explicitly mentioned\nbut understood in a discourse, and enables exploitation of discursive and\nperceptual context to limit the set of potential referents for a given\nreferring expression. As the most important feature, we note that a single\nmechanism is required to handle what are typically treated as diverse\nphenomena. Our approach, then, provides a fresh perspective on the relations\nbetween Cognitive Grammar and the problem of reference."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.2715v1", 
    "title": "Marking-up multiple views of a Text: Discourse and Reference", 
    "arxiv-id": "0909.2715v1", 
    "author": "Laurent Romary", 
    "publish": "2009-09-15T06:02:56Z", 
    "summary": "We describe an encoding scheme for discourse structure and reference, based\non the TEI Guidelines and the recommendations of the Corpus Encoding\nSpecification (CES). A central feature of the scheme is a CES-based data\narchitecture enabling the encoding of and access to multiple views of a\nmarked-up document. We describe a tool architecture that supports the encoding\nscheme, and then show how we have used the encoding scheme and the tools to\nperform a discourse analytic task in support of a model of global discourse\ncohesion called Veins Theory (Cristea & Ide, 1998)."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.2718v1", 
    "title": "A Common XML-based Framework for Syntactic Annotations", 
    "arxiv-id": "0909.2718v1", 
    "author": "Tomaz Erjavec", 
    "publish": "2009-09-15T06:09:47Z", 
    "summary": "It is widely recognized that the proliferation of annotation schemes runs\ncounter to the need to re-use language resources, and that standards for\nlinguistic annotation are becoming increasingly mandatory. To answer this need,\nwe have developed a framework comprised of an abstract model for a variety of\ndifferent annotation types (e.g., morpho-syntactic tagging, syntactic\nannotation, co-reference annotation, etc.), which can be instantiated in\ndifferent ways depending on the annotator's approach and goals. In this paper\nwe provide an overview of the framework, demonstrate its applicability to\nsyntactic annotation, and show how it can contribute to comparative evaluation\nof parser output and diverse syntactic annotation schemes."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.2719v1", 
    "title": "Standards for Language Resources", 
    "arxiv-id": "0909.2719v1", 
    "author": "Laurent Romary", 
    "publish": "2009-09-15T06:10:45Z", 
    "summary": "This paper presents an abstract data model for linguistic annotations and its\nimplementation using XML, RDF and related standards; and to outline the work of\na newly formed committee of the International Standards Organization (ISO),\nISO/TC 37/SC 4 Language Resource Management, which will use this work as its\nstarting point. The primary motive for presenting the latter is to solicit the\nparticipation of members of the research community to contribute to the work of\nthe committee."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.3027v1", 
    "title": "Language Models for Handwritten Short Message Services", 
    "arxiv-id": "0909.3027v1", 
    "author": "Emmanuel Morin", 
    "publish": "2009-09-16T14:38:19Z", 
    "summary": "Handwriting is an alternative method for entering texts composing Short\nMessage Services. However, a whole new language features the texts which are\nproduced. They include for instance abbreviations and other consonantal writing\nwhich sprung up for time saving and fashion. We have collected and processed a\nsignificant number of such handwriting SMS, and used various strategies to\ntackle this challenging area of handwriting recognition. We proposed to study\nmore specifically three different phenomena: consonant skeleton, rebus, and\nphonetic writing. For each of them, we compare the rough results produced by a\nstandard recognition system with those obtained when using a specific language\nmodel."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.3028v1", 
    "title": "Vers la reconnaissance de mini-messages manuscrits", 
    "arxiv-id": "0909.3028v1", 
    "author": "Christian Viard-Gaudin", 
    "publish": "2009-09-16T14:39:05Z", 
    "summary": "Handwriting is an alternative method for entering texts which composed Short\nMessage Services. However, a whole new language features the texts which are\nproduced. They include for instance abbreviations and other consonantal writing\nwhich sprung up for time saving and fashion. We have collected and processed a\nsignificant number of such handwritten SMS, and used various strategies to\ntackle this challenging area of handwriting recognition. We proposed to study\nmore specifically three different phenomena: consonant skeleton, rebus, and\nphonetic writing. For each of them, we compare the rough results produced by a\nstandard recognition system with those obtained when using a specific language\nmodel to take care of them."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.3444v1", 
    "title": "Analyse en d\u00e9pendances \u00e0 l'aide des grammaires d'interaction", 
    "arxiv-id": "0909.3444v1", 
    "author": "Guy Perrier", 
    "publish": "2009-09-18T14:23:05Z", 
    "summary": "This article proposes a method to extract dependency structures from\nphrase-structure level parsing with Interaction Grammars. Interaction Grammars\nare a formalism which expresses interactions among words using a polarity\nsystem. Syntactical composition is led by the saturation of polarities.\nInteractions take place between constituents, but as grammars are lexicalized,\nthese interactions can be translated at the level of words. Dependency\nrelations are extracted from the parsing process: every dependency is the\nconsequence of a polarity saturation. The dependency relations we obtain can be\nseen as a refinement of the usual dependency tree. Generally speaking, this\nwork sheds new light on links between phrase structure and dependency parsing."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.3445v1", 
    "title": "Grouping Synonyms by Definitions", 
    "arxiv-id": "0909.3445v1", 
    "author": "Fabienne Venant", 
    "publish": "2009-09-18T14:25:29Z", 
    "summary": "We present a method for grouping the synonyms of a lemma according to its\ndictionary senses. The senses are defined by a large machine readable\ndictionary for French, the TLFi (Tr\\'esor de la langue fran\\c{c}aise\ninformatis\\'e) and the synonyms are given by 5 synonym dictionaries (also for\nFrench). To evaluate the proposed method, we manually constructed a gold\nstandard where for each (word, definition) pair and given the set of synonyms\ndefined for that word by the 5 synonym dictionaries, 4 lexicographers specified\nthe set of synonyms they judge adequate. While inter-annotator agreement ranges\non that task from 67% to at best 88% depending on the annotator pair and on the\nsynonym dictionary being considered, the automatic procedure we propose scores\na precision of 67% and a recall of 71%. The proposed method is compared with\nrelated work namely, word sense disambiguation, synonym lexicon acquisition and\nWordNet construction."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.3591v1", 
    "title": "Mathematics, Recursion, and Universals in Human Languages", 
    "arxiv-id": "0909.3591v1", 
    "author": "A. Karousou", 
    "publish": "2009-09-19T15:38:55Z", 
    "summary": "There are many scientific problems generated by the multiple and conflicting\nalternative definitions of linguistic recursion and human recursive processing\nthat exist in the literature. The purpose of this article is to make available\nto the linguistic community the standard mathematical definition of recursion\nand to apply it to discuss linguistic recursion. As a byproduct, we obtain an\ninsight into certain \"soft universals\" of human languages, which are related to\ncognitive constructs necessary to implement mathematical reasoning, i.e.\nmathematical model theory."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0909.4280v1", 
    "title": "Towards Multimodal Content Representation", 
    "arxiv-id": "0909.4280v1", 
    "author": "Laurent Romary", 
    "publish": "2009-09-23T18:55:53Z", 
    "summary": "Multimodal interfaces, combining the use of speech, graphics, gestures, and\nfacial expressions in input and output, promise to provide new possibilities to\ndeal with information in more effective and efficient ways, supporting for\ninstance: - the understanding of possibly imprecise, partial or ambiguous\nmultimodal input; - the generation of coordinated, cohesive, and coherent\nmultimodal presentations; - the management of multimodal interaction (e.g.,\ntask completion, adapting the interface, error prevention) by representing and\nexploiting models of the user, the domain, the task, the interactive context,\nand the media (e.g. text, audio, video). The present document is intended to\nsupport the discussion on multimodal content representation, its possible\nobjectives and basic constraints, and how the definition of a generic\nrepresentation framework for multimodal content representation may be\napproached. It takes into account the results of the Dagstuhl workshop, in\nparticular those of the informal working group on multimodal meaning\nrepresentation that was active during the workshop (see\nhttp://www.dfki.de/~wahlster/Dagstuhl_Multi_Modality, Working Group 4)."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0910.0537v1", 
    "title": "A Note On Higher Order Grammar", 
    "arxiv-id": "0910.0537v1", 
    "author": "Victor Gluzberg", 
    "publish": "2009-10-03T12:26:47Z", 
    "summary": "Both syntax-phonology and syntax-semantics interfaces in Higher Order Grammar\n(HOG) are expressed as axiomatic theories in higher-order logic (HOL), i.e. a\nlanguage is defined entirely in terms of provability in the single logical\nsystem. An important implication of this elegant architecture is that the\nmeaning of a valid expression turns out to be represented not by a single, nor\neven by a few \"discrete\" terms (in case of ambiguity), but by a \"continuous\"\nset of logically equivalent terms. The note is devoted to precise formulation\nand proof of this observation."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0910.1484v1", 
    "title": "Ludics and its Applications to natural Language Semantics", 
    "arxiv-id": "0910.1484v1", 
    "author": "Myriam Quatrini", 
    "publish": "2009-10-08T12:21:22Z", 
    "summary": "Proofs, in Ludics, have an interpretation provided by their counter-proofs,\nthat is the objects they interact with. We follow the same idea by proposing\nthat sentence meanings are given by the counter-meanings they are opposed to in\na dialectical interaction. The conception is at the intersection of a\nproof-theoretic and a game-theoretic accounts of semantics, but it enlarges\nthem by allowing to deal with possibly infinite processes."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0910.1868v1", 
    "title": "Evaluation of Hindi to Punjabi Machine Translation System", 
    "arxiv-id": "0910.1868v1", 
    "author": "Gurpreet Singh Lehal", 
    "publish": "2009-10-09T21:40:26Z", 
    "summary": "Machine Translation in India is relatively young. The earliest efforts date\nfrom the late 80s and early 90s. The success of every system is judged from its\nevaluation experimental results. Number of machine translation systems has been\nstarted for development but to the best of author knowledge, no high quality\nsystem has been completed which can be used in real applications. Recently,\nPunjabi University, Patiala, India has developed Punjabi to Hindi Machine\ntranslation system with high accuracy of about 92%. Both the systems i.e.\nsystem under question and developed system are between same closely related\nlanguages. Thus, this paper presents the evaluation results of Hindi to Punjabi\nmachine translation system. It makes sense to use same evaluation criteria as\nthat of Punjabi to Hindi Punjabi Machine Translation System. After evaluation,\nthe accuracy of the system is found to be about 95%."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.0894v1", 
    "title": "A New Computational Schema for Euphonic Conjunctions in Sanskrit   Processing", 
    "arxiv-id": "0911.0894v1", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2009-11-04T19:33:21Z", 
    "summary": "Automated language processing is central to the drive to enable facilitated\nreferencing of increasingly available Sanskrit E texts. The first step towards\nprocessing Sanskrit text involves the handling of Sanskrit compound words that\nare an integral part of Sanskrit texts. This firstly necessitates the\nprocessing of euphonic conjunctions or sandhis, which are points in words or\nbetween words, at which adjacent letters coalesce and transform. The ancient\nSanskrit grammarian Panini's codification of the Sanskrit grammar is the\naccepted authority in the subject. His famed sutras or aphorisms, numbering\napproximately four thousand, tersely, precisely and comprehensively codify the\nrules of the grammar, including all the rules pertaining to sandhis. This work\npresents a fresh new approach to processing sandhis in terms of a computational\nschema. This new computational model is based on Panini's complex codification\nof the rules of grammar. The model has simple beginnings and is yet powerful,\ncomprehensive and computationally lean."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.0907v1", 
    "title": "ANN-based Innovative Segmentation Method for Handwritten text in   Assamese", 
    "arxiv-id": "0911.0907v1", 
    "author": "Kandarpa Kumar Sarma", 
    "publish": "2009-11-04T18:32:08Z", 
    "summary": "Artificial Neural Network (ANN) s has widely been used for recognition of\noptically scanned character, which partially emulates human thinking in the\ndomain of the Artificial Intelligence. But prior to recognition, it is\nnecessary to segment the character from the text to sentences, words etc.\nSegmentation of words into individual letters has been one of the major\nproblems in handwriting recognition. Despite several successful works all over\nthe work, development of such tools in specific languages is still an ongoing\nprocess especially in the Indian context. This work explores the application of\nANN as an aid to segmentation of handwritten characters in Assamese- an\nimportant language in the North Eastern part of India. The work explores the\nperformance difference obtained in applying an ANN-based dynamic segmentation\nalgorithm compared to projection- based static segmentation. The algorithm\ninvolves, first training of an ANN with individual handwritten characters\nrecorded from different individuals. Handwritten sentences are separated out\nfrom text using a static segmentation method. From the segmented line,\nindividual characters are separated out by first over segmenting the entire\nline. Each of the segments thus obtained, next, is fed to the trained ANN. The\npoint of segmentation at which the ANN recognizes a segment or a combination of\nseveral segments to be similar to a handwritten character, a segmentation\nboundary for the character is assumed to exist and segmentation performed. The\nsegmented character is next compared to the best available match and the\nsegmentation boundary confirmed."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.1516v2", 
    "title": "A Discourse-based Approach in Text-based Machine Translation", 
    "arxiv-id": "0911.1516v2", 
    "author": "Kyung Sup Kwak", 
    "publish": "2009-11-08T10:57:04Z", 
    "summary": "This paper presents a theoretical research based approach to ellipsis\nresolution in machine translation. The formula of discourse is applied in order\nto resolve ellipses. The validity of the discourse formula is analyzed by\napplying it to the real world text, i.e., newspaper fragments. The source text\nis converted into mono-sentential discourses where complex discourses require\nfurther dissection either directly into primitive discourses or first into\ncompound discourses and later into primitive ones. The procedure of dissection\nneeds further improvement, i.e., discovering as many primitive discourse forms\nas possible. An attempt has been made to investigate new primitive discourses\nor patterns from the given text."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.1517v2", 
    "title": "Resolution of Unidentified Words in Machine Translation", 
    "arxiv-id": "0911.1517v2", 
    "author": "Kyung Sup Kwak", 
    "publish": "2009-11-09T13:52:05Z", 
    "summary": "This paper presents a mechanism of resolving unidentified lexical units in\nText-based Machine Translation (TBMT). In a Machine Translation (MT) system it\nis unlikely to have a complete lexicon and hence there is intense need of a new\nmechanism to handle the problem of unidentified words. These unknown words\ncould be abbreviations, names, acronyms and newly introduced terms. We have\nproposed an algorithm for the resolution of the unidentified words. This\nalgorithm takes discourse unit (primitive discourse) as a unit of analysis and\nprovides real time updates to the lexicon. We have manually applied the\nalgorithm to news paper fragments. Along with anaphora and cataphora\nresolution, many unknown words especially names and abbreviations were updated\nto the lexicon."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.1842v1", 
    "title": "Standards for Language Resources", 
    "arxiv-id": "0911.1842v1", 
    "author": "Laurent Romary", 
    "publish": "2009-11-10T07:12:03Z", 
    "summary": "The goal of this paper is two-fold: to present an abstract data model for\nlinguistic annotations and its implementation using XML, RDF and related\nstandards; and to outline the work of a newly formed committee of the\nInternational Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource\nManagement, which will use this work as its starting point."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.2284v2", 
    "title": "A New Look at the Classical Entropy of Written English", 
    "arxiv-id": "0911.2284v2", 
    "author": "Fabio G. Guerrero", 
    "publish": "2009-11-12T01:48:12Z", 
    "summary": "A simple method for finding the entropy and redundancy of a reasonable long\nsample of English text by direct computer processing and from first principles\naccording to Shannon theory is presented. As an example, results on the entropy\nof the English language have been obtained based on a total of 20.3 million\ncharacters of written English, considering symbols from one to five hundred\ncharacters in length. Besides a more realistic value of the entropy of English,\na new perspective on some classic entropy-related concepts is presented. This\nmethod can also be extended to other Latin languages. Some implications for\npractical applications such as plagiarism-detection software, and the minimum\nnumber of words that should be used in social Internet network messaging, are\ndiscussed."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0911.5116v1", 
    "title": "Standardization of the formal representation of lexical information for   NLP", 
    "arxiv-id": "0911.5116v1", 
    "author": "Laurent Romary", 
    "publish": "2009-11-26T16:27:58Z", 
    "summary": "A survey of dictionary models and formats is presented as well as a\npresentation of corresponding recent standardisation activities."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0912.1820v1", 
    "title": "Parsing of part-of-speech tagged Assamese Texts", 
    "arxiv-id": "0912.1820v1", 
    "author": "Utpal Sharma", 
    "publish": "2009-12-09T18:03:20Z", 
    "summary": "A natural language (or ordinary language) is a language that is spoken,\nwritten, or signed by humans for general-purpose communication, as\ndistinguished from formal languages (such as computer-programming languages or\nthe \"languages\" used in the study of formal logic). The computational\nactivities required for enabling a computer to carry out information processing\nusing natural language is called natural language processing. We have taken\nAssamese language to check the grammars of the input sentence. Our aim is to\nproduce a technique to check the grammatical structures of the sentences in\nAssamese text. We have made grammar rules by analyzing the structures of\nAssamese sentences. Our parsing program finds the grammatical errors, if any,\nin the Assamese sentence. If there is no error, the program will generate the\nparse tree for the Assamese sentence"
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/0912.2881v2", 
    "title": "Representing human and machine dictionaries in Markup languages", 
    "arxiv-id": "0912.2881v2", 
    "author": "Andreas Witt", 
    "publish": "2009-12-15T13:30:14Z", 
    "summary": "In this chapter we present the main issues in representing machine readable\ndictionaries in XML, and in particular according to the Text Encoding\nDictionary (TEI) guidelines."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1001.2267v1", 
    "title": "Speech Recognition by Machine, A Review", 
    "arxiv-id": "1001.2267v1", 
    "author": "S. K. Katti", 
    "publish": "2010-01-13T19:02:18Z", 
    "summary": "This paper presents a brief survey on Automatic Speech Recognition and\ndiscusses the major themes and advances made in the past 60 years of research,\nso as to provide a technological perspective and an appreciation of the\nfundamental progress that has been accomplished in this important area of\nspeech communication. After years of research and development the accuracy of\nautomatic speech recognition remains one of the important research challenges\n(e.g., variations of the context, speakers, and environment).The design of\nSpeech Recognition system requires careful attentions to the following issues:\nDefinition of various types of speech classes, speech representation, feature\nextraction techniques, speech classifiers, database and performance evaluation.\nThe problems that are existing in ASR and the various techniques to solve these\nproblems constructed by various research workers have been presented in a\nchronological order. Hence authors hope that this work shall be a contribution\nin the area of speech recognition. The objective of this review paper is to\nsummarize and compare some of the well known methods used in various stages of\nspeech recognition system and identify research topic and applications which\nare at the forefront of this exciting and challenging field."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1001.4273v1", 
    "title": "Sentence Simplification Aids Protein-Protein Interaction Extraction", 
    "arxiv-id": "1001.4273v1", 
    "author": "Graciela Gonzalez", 
    "publish": "2010-01-24T20:23:10Z", 
    "summary": "Accurate systems for extracting Protein-Protein Interactions (PPIs)\nautomatically from biomedical articles can help accelerate biomedical research.\nBiomedical Informatics researchers are collaborating to provide metaservices\nand advance the state-of-art in PPI extraction. One problem often neglected by\ncurrent Natural Language Processing systems is the characteristic complexity of\nthe sentences in biomedical literature. In this paper, we report on the impact\nthat automatic simplification of sentences has on the performance of a\nstate-of-art PPI extraction system, showing a substantial improvement in recall\n(8%) when the sentence simplification method is applied, without significant\nimpact to precision."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1001.4277v1", 
    "title": "Towards Effective Sentence Simplification for Automatic Processing of   Biomedical Text", 
    "arxiv-id": "1001.4277v1", 
    "author": "Graciela Gonzalez", 
    "publish": "2010-01-24T20:35:29Z", 
    "summary": "The complexity of sentences characteristic to biomedical articles poses a\nchallenge to natural language parsers, which are typically trained on\nlarge-scale corpora of non-technical text. We propose a text simplification\nprocess, bioSimplify, that seeks to reduce the complexity of sentences in\nbiomedical abstracts in order to improve the performance of syntactic parsers\non the processed sentences. Syntactic parsing is typically one of the first\nsteps in a text mining pipeline. Thus, any improvement in performance would\nhave a ripple effect over all processing steps. We evaluated our method using a\ncorpus of biomedical sentences annotated with syntactic links. Our empirical\nresults show an improvement of 2.90% for the Charniak-McClosky parser and of\n4.23% for the Link Grammar parser when processing simplified sentences rather\nthan the original sentences in the corpus."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0478v1", 
    "title": "\u00c9tude et traitement automatique de l'anglais du XVIIe si\u00e8cle :   outils morphosyntaxiques et dictionnaires", 
    "arxiv-id": "1002.0478v1", 
    "author": "H\u00e9l\u00e8ne Pignot", 
    "publish": "2010-02-02T13:23:47Z", 
    "summary": "In this article, we record the main linguistic differences or singularities\nof 17th century English, analyse them morphologically and syntactically and\npropose equivalent forms in contemporary English. We show how 17th century\ntexts may be transcribed into modern English, combining the use of electronic\ndictionaries with rules of transcription implemented as transducers. Apr\\`es\navoir expos\\'e la constitution du corpus, nous recensons les principales\ndiff\\'erences ou particularit\\'es linguistiques de la langue anglaise du XVIIe\nsi\\`ecle, les analysons du point de vue morphologique et syntaxique et\nproposons des \\'equivalents en anglais contemporain (AC). Nous montrons comment\nnous pouvons effectuer une transcription automatique de textes anglais du XVIIe\nsi\\`ecle en anglais moderne, en combinant l'utilisation de dictionnaires\n\\'electroniques avec des r\\`egles de transcriptions impl\\'ement\\'ees sous forme\nde transducteurs."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0479v1", 
    "title": "\"Mind your p's and q's\": or the peregrinations of an apostrophe in 17th   Century English", 
    "arxiv-id": "1002.0479v1", 
    "author": "H\u00e9l\u00e8ne Pignot", 
    "publish": "2010-02-02T13:24:20Z", 
    "summary": "If the use of the apostrophe in contemporary English often marks the Saxon\ngenitive, it may also indicate the omission of one or more let-ters. Some\nwriters (wrongly?) use it to mark the plural in symbols or abbreviations,\nvisual-ised thanks to the isolation of the morpheme \"s\". This punctuation mark\nwas imported from the Continent in the 16th century. During the 19th century\nits use was standardised. However the rules of its usage still seem problematic\nto many, including literate speakers of English. \"All too often, the apostrophe\nis misplaced\", or \"errant apostrophes are springing up every-where\" is a\ncomplaint that Internet users fre-quently come across when visiting grammar\nwebsites. Many of them detail its various uses and misuses, and attempt to\ncorrect the most common mistakes about it, especially its mis-use in the\nplural, called greengrocers' apostro-phes and humorously misspelled\n\"greengro-cers apostrophe's\". While studying English travel accounts published\nin the seventeenth century, we noticed that the different uses of this symbol\nmay accompany various models of metaplasms. We were able to highlight the\nlinguistic variations of some lexemes, and trace the origin of modern grammar\nrules gov-erning its usage."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0481v2", 
    "title": "Recognition and translation Arabic-French of Named Entities: case of the   Sport places", 
    "arxiv-id": "1002.0481v2", 
    "author": "H\u00e9la Fehri", 
    "publish": "2010-02-02T13:24:38Z", 
    "summary": "The recognition of Arabic Named Entities (NE) is a problem in different\ndomains of Natural Language Processing (NLP) like automatic translation.\nIndeed, NE translation allows the access to multilingual in-formation. This\ntranslation doesn't always lead to expected result especially when NE contains\na person name. For this reason and in order to ameliorate translation, we can\ntransliterate some part of NE. In this context, we propose a method that\nintegrates translation and transliteration together. We used the linguis-tic\nNooJ platform that is based on local grammars and transducers. In this paper,\nwe focus on sport domain. We will firstly suggest a refinement of the\ntypological model presented at the MUC Conferences we will describe the\nintegration of an Arabic transliteration module into translation system.\nFinally, we will detail our method and give the results of the evaluation."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0485v1", 
    "title": "Morphological study of Albanian words, and processing with NooJ", 
    "arxiv-id": "1002.0485v1", 
    "author": "Klara Lagji", 
    "publish": "2010-02-02T13:35:02Z", 
    "summary": "We are developing electronic dictionaries and transducers for the automatic\nprocessing of the Albanian Language. We will analyze the words inside a linear\nsegment of text. We will also study the relationship between units of sense and\nunits of form. The composition of words takes different forms in Albanian. We\nhave found that morphemes are frequently concatenated or simply juxtaposed or\ncontracted. The inflected grammar of NooJ allows constructing the dictionaries\nof flexed forms (declensions or conjugations). The diversity of word structures\nrequires tools to identify words created by simple concatenation, or to treat\ncontractions. The morphological tools of NooJ allow us to create grammatical\ntools to represent and treat these phenomena. But certain problems exceed the\nmorphological analysis and must be represented by syntactical grammars."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0773v1", 
    "title": "Approximations to the MMI criterion and their effect on lattice-based   MMI", 
    "arxiv-id": "1002.0773v1", 
    "author": "Steven Wegmann", 
    "publish": "2010-02-03T16:06:24Z", 
    "summary": "Maximum mutual information (MMI) is a model selection criterion used for\nhidden Markov model (HMM) parameter estimation that was developed more than\ntwenty years ago as a discriminative alternative to the maximum likelihood\ncriterion for HMM-based speech recognition. It has been shown in the speech\nrecognition literature that parameter estimation using the current MMI\nparadigm, lattice-based MMI, consistently outperforms maximum likelihood\nestimation, but this is at the expense of undesirable convergence properties.\nIn particular, recognition performance is sensitive to the number of times that\nthe iterative MMI estimation algorithm, extended Baum-Welch, is performed. In\nfact, too many iterations of extended Baum-Welch will lead to degraded\nperformance, despite the fact that the MMI criterion improves at each\niteration. This phenomenon is at variance with the analogous behavior of\nmaximum likelihood estimation -- at least for the HMMs used in speech\nrecognition -- and it has previously been attributed to `over fitting'. In this\npaper, we present an analysis of lattice-based MMI that demonstrates, first of\nall, that the asymptotic behavior of lattice-based MMI is much worse than was\npreviously understood, i.e. it does not appear to converge at all, and, second\nof all, that this is not due to `over fitting'. Instead, we demonstrate that\nthe `over fitting' phenomenon is the result of standard methodology that\nexacerbates the poor behavior of two key approximations in the lattice-based\nMMI machinery. We also demonstrate that if we modify the standard methodology\nto improve the validity of these approximations, then the convergence\nproperties of lattice-based MMI become benign without sacrificing improvements\nto recognition accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.0904v1", 
    "title": "On Event Structure in the Torn Dress", 
    "arxiv-id": "1002.0904v1", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-02-04T06:30:14Z", 
    "summary": "Using Pustejovsky's \"The Syntax of Event Structure\" and Fong's \"On Mending a\nTorn Dress\" we give a glimpse of a Pustejovsky-like analysis to some example\nsentences in Fong. We attempt to give a framework for semantics to the noun\nphrases and adverbs as appropriate as well as the lexical entries for all words\nin the examples and critique both papers in light of our findings and\ndifficulties."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.1095v1", 
    "title": "Towards a Heuristic Categorization of Prepositional Phrases in English   with WordNet", 
    "arxiv-id": "1002.1095v1", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-02-04T22:48:31Z", 
    "summary": "This document discusses an approach and its rudimentary realization towards\nautomatic classification of PPs; the topic, that has not received as much\nattention in NLP as NPs and VPs. The approach is a rule-based heuristics\noutlined in several levels of our research. There are 7 semantic categories of\nPPs considered in this document that we are able to classify from an annotated\ncorpus."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.1919v2", 
    "title": "Thai Rhetorical Structure Analysis", 
    "arxiv-id": "1002.1919v2", 
    "author": "Ohm Sornil", 
    "publish": "2010-02-09T20:01:06Z", 
    "summary": "Rhetorical structure analysis (RSA) explores discourse relations among\nelementary discourse units (EDUs) in a text. It is very useful in many text\nprocessing tasks employing relationships among EDUs such as text understanding,\nsummarization, and question-answering. Thai language with its distinctive\nlinguistic characteristics requires a unique technique. This article proposes\nan approach for Thai rhetorical structure analysis. First, EDUs are segmented\nby two hidden Markov models derived from syntactic rules. A rhetorical\nstructure tree is constructed from a clustering technique with its similarity\nmeasure derived from Thai semantic rules. Then, a decision tree whose features\nderived from the semantic rules is used to determine discourse relations."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.3320v1", 
    "title": "Co-channel Interference Cancellation for Space-Time Coded OFDM Systems   Using Adaptive Beamforming and Null Deepening", 
    "arxiv-id": "1002.3320v1", 
    "author": "Raungrong Suleesathira", 
    "publish": "2010-02-17T17:30:22Z", 
    "summary": "Combined with space-time coding, the orthogonal frequency division\nmultiplexing (OFDM) system explores space diversity. It is a potential scheme\nto offer spectral efficiency and robust high data rate transmissions over\nfrequency-selective fading channel. However, space-time coding impairs the\nsystem ability to suppress interferences as the signals transmitted from two\ntransmit antennas are superposed and interfered at the receiver antennas. In\nthis paper, we developed an adaptive beamforming based on least mean squared\nerror algorithm and null deepening to combat co-channel interference (CCI) for\nthe space-time coded OFDM (STC-OFDM) system. To illustrate the performance of\nthe presented approach, it is compared to the null steering beamformer which\nrequires a prior knowledge of directions of arrival (DOAs). The structure of\nspace-time decoders are preserved although there is the use of beamformers\nbefore decoding. By incorporating the proposed beamformer as a CCI canceller in\nthe STC-OFDM systems, the performance improvement is achieved as shown in the\nsimulation results."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1002.4820v1", 
    "title": "SLAM : Solutions lexicales automatique pour m\u00e9taphores", 
    "arxiv-id": "1002.4820v1", 
    "author": "Karine Duvignau", 
    "publish": "2010-02-25T16:27:36Z", 
    "summary": "This article presents SLAM, an Automatic Solver for Lexical Metaphors like\n?d\\'eshabiller* une pomme? (to undress* an apple). SLAM calculates a\nconventional solution for these productions. To carry on it, SLAM has to\nintersect the paradigmatic axis of the metaphorical verb ?d\\'eshabiller*?,\nwhere ?peler? (?to peel?) comes closer, with a syntagmatic axis that comes from\na corpus where ?peler une pomme? (to peel an apple) is semantically and\nsyntactically regular. We test this model on DicoSyn, which is a ?small world?\nnetwork of synonyms, to compute the paradigmatic axis and on Frantext.20, a\nFrench corpus, to compute the syntagmatic axis. Further, we evaluate the model\nwith a sample of an experimental corpus of the database of Flexsem"
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.0206v1", 
    "title": "Why has (reasonably accurate) Automatic Speech Recognition been so hard   to achieve?", 
    "arxiv-id": "1003.0206v1", 
    "author": "Larry Gillick", 
    "publish": "2010-02-28T19:00:12Z", 
    "summary": "Hidden Markov models (HMMs) have been successfully applied to automatic\nspeech recognition for more than 35 years in spite of the fact that a key HMM\nassumption -- the statistical independence of frames -- is obviously violated\nby speech data. In fact, this data/model mismatch has inspired many attempts to\nmodify or replace HMMs with alternative models that are better able to take\ninto account the statistical dependence of frames. However it is fair to say\nthat in 2010 the HMM is the consensus model of choice for speech recognition\nand that HMMs are at the heart of both commercially available products and\ncontemporary research systems. In this paper we present a preliminary\nexploration aimed at understanding how speech data depart from HMMs and what\neffect this departure has on the accuracy of HMM-based speech recognition. Our\nanalysis uses standard diagnostic tools from the field of statistics --\nhypothesis testing, simulation and resampling -- which are rarely used in the\nfield of speech recognition. Our main result, obtained by novel manipulations\nof real and resampled data, demonstrates that real data have statistical\ndependency and that this dependency is responsible for significant numbers of\nrecognition errors. We also demonstrate, using simulation and resampling, that\nif we `remove' the statistical dependency from data, then the resulting\nrecognition error rates become negligible. Taken together, these results\nsuggest that a better understanding of the structure of the statistical\ndependency in speech data is a crucial first step towards improving HMM-based\nspeech recognition."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.0337v1", 
    "title": "Change of word types to word tokens ratio in the course of translation   (based on Russian translations of K. Vonnegut novels)", 
    "arxiv-id": "1003.0337v1", 
    "author": "Andrey Kutuzov", 
    "publish": "2010-03-01T18:04:39Z", 
    "summary": "The article provides lexical statistical analysis of K. Vonnegut's two novels\nand their Russian translations. It is found out that there happen some changes\nbetween the speed of word types and word tokens ratio change in the source and\ntarget texts. The author hypothesizes that these changes are typical for\nEnglish-Russian translations, and moreover, they represent an example of\nBaker's translation feature of levelling out."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.0628v1", 
    "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", 
    "arxiv-id": "1003.0628v1", 
    "author": "Guy Lebanon", 
    "publish": "2010-03-02T16:52:32Z", 
    "summary": "Text documents are complex high dimensional objects. To effectively visualize\nsuch data it is important to reduce its dimensionality and visualize the low\ndimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore\ndimensionality reduction methods that draw upon domain knowledge in order to\nachieve a better low dimensional embedding and visualization of documents. We\nconsider the use of geometries specified manually by an expert, geometries\nderived automatically from corpus statistics, and geometries computed from\nlinguistic resources."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.1399v1", 
    "title": "Automatic derivation of domain terms and concept location based on the   analysis of the identifiers", 
    "arxiv-id": "1003.1399v1", 
    "author": "Marek Mezei", 
    "publish": "2010-03-06T16:34:40Z", 
    "summary": "Developers express the meaning of the domain ideas in specifically selected\nidentifiers and comments that form the target implemented code. Software\nmaintenance requires knowledge and understanding of the encoded ideas. This\npaper presents a way how to create automatically domain vocabulary. Knowledge\nof domain vocabulary supports the comprehension of a specific domain for later\ncode maintenance or evolution. We present experiments conducted in two selected\ndomains: application servers and web frameworks. Knowledge of domain terms\nenables easy localization of chunks of code that belong to a certain term. We\nconsider these chunks of code as \"concepts\" and their placement in the code as\n\"concept location\". Application developers may also benefit from the obtained\ndomain terms. These terms are parts of speech that characterize a certain\nconcept. Concepts are encoded in \"classes\" (OO paradigm) and the obtained\nvocabulary of terms supports the selection and the comprehension of the class'\nappropriate identifiers. We measured the following software products with our\ntool: JBoss, JOnAS, GlassFish, Tapestry, Google Web Toolkit and Echo2."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.1455v1", 
    "title": "A Computational Algorithm based on Empirical Analysis, that Composes   Sanskrit Poetry", 
    "arxiv-id": "1003.1455v1", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2010-03-07T11:28:08Z", 
    "summary": "Poetry-writing in Sanskrit is riddled with problems for even those who know\nthe language well. This is so because the rules that govern Sanskrit prosody\nare numerous and stringent. We propose a computational algorithm that converts\nprose given as E-text into poetry in accordance with the metrical rules of\nSanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic\nconjunction, which is compulsory in verse, is handled. The algorithm is\nconsiderably speeded up by a novel method of reducing the target search\ndatabase. The algorithm further gives suggestions to the poet in case what\nhe/she has given as the input prose is impossible to fit into any allowed\nmetrical format. There is also an interactive component of the algorithm by\nwhich the algorithm interacts with the poet to resolve ambiguities. In\naddition, this unique work, which provides a solution to a problem that has\nnever been addressed before, provides a simple yet effective speech recognition\ninterface that would help the visually impaired dictate words in E-text, which\nis in turn versified by our Poetry Composer Engine."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.4149v1", 
    "title": "Les Entit\u00e9s Nomm\u00e9es : usage et degr\u00e9s de pr\u00e9cision et de   d\u00e9sambigu\u00efsation", 
    "arxiv-id": "1003.4149v1", 
    "author": "Stavroula Voyatzi", 
    "publish": "2010-03-22T13:09:57Z", 
    "summary": "The recognition and classification of Named Entities (NER) are regarded as an\nimportant component for many Natural Language Processing (NLP) applications.\nThe classification is usually made by taking into account the immediate context\nin which the NE appears. In some cases, this immediate context does not allow\ngetting the right classification. We show in this paper that the use of an\nextended syntactic context and large-scale resources could be very useful in\nthe NER task."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.4894v1", 
    "title": "La repr\u00e9sentation formelle des concepts spatiaux dans la langue", 
    "arxiv-id": "1003.4894v1", 
    "author": "Andr\u00e9e Borillo", 
    "publish": "2010-03-25T14:03:51Z", 
    "summary": "In this chapter, we assume that systematically studying spatial markers\nsemantics in language provides a means to reveal fundamental properties and\nconcepts characterizing conceptual representations of space. We propose a\nformal system accounting for the properties highlighted by the linguistic\nanalysis, and we use these tools for representing the semantic content of\nseveral spatial relations of French. The first part presents a semantic\nanalysis of the expression of space in French aiming at describing the\nconstraints that formal representations have to take into account. In the\nsecond part, after presenting the structure of our formal system, we set out\nits components. A commonsense geometry is sketched out and several functional\nand pragmatic spatial concepts are formalized. We take a special attention in\nshowing that these concepts are well suited to representing the semantic\ncontent of several prepositions of French ('sur' (on), 'dans' (in), 'devant'\n(in front of), 'au-dessus' (above)), and in illustrating the inferential\nadequacy of these representations."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.4898v1", 
    "title": "Les entit\u00e9s spatiales dans la langue : \u00e9tude descriptive, formelle   et exp\u00e9rimentale de la cat\u00e9gorisation", 
    "arxiv-id": "1003.4898v1", 
    "author": "Laure Vieu", 
    "publish": "2010-03-25T14:08:44Z", 
    "summary": "While previous linguistic and psycholinguistic research on space has mainly\nanalyzed spatial relations, the studies reported in this paper focus on how\nlanguage distinguishes among spatial entities. Descriptive and experimental\nstudies first propose a classification of entities, which accounts for both\nstatic and dynamic space, has some cross-linguistic validity, and underlies\nadults' cognitive processing. Formal and computational analyses then introduce\ntheoretical elements aiming at modelling these categories, while fulfilling\nvarious properties of formal ontologies (generality, parsimony, coherence...).\nThis formal framework accounts, in particular, for functional dependences among\nentities underlying some part-whole descriptions. Finally, developmental\nresearch shows that language-specific properties have a clear impact on how\nchildren talk about space. The results suggest some cross-linguistic\nvariability in children's spatial representations from an early age onwards,\nbringing into question models in which general cognitive capacities are the\nonly determinants of spatial cognition during the course of development."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1003.5372v1", 
    "title": "Learning Recursive Segments for Discourse Parsing", 
    "arxiv-id": "1003.5372v1", 
    "author": "Laurence Danlos", 
    "publish": "2010-03-28T15:17:22Z", 
    "summary": "Automatically detecting discourse segments is an important preliminary step\ntowards full discourse parsing. Previous research on discourse segmentation\nhave relied on the assumption that elementary discourse units (EDUs) in a\ndocument always form a linear sequence (i.e., they can never be nested).\nUnfortunately, this assumption turns out to be too strong, for some theories of\ndiscourse like SDRT allows for nested discourse units. In this paper, we\npresent a simple approach to discourse segmentation that is able to produce\nnested EDUs. Our approach builds on standard multi-class classification\ntechniques combined with a simple repairing heuristic that enforces global\ncoherence. Our system was developed and evaluated on the first round of\nannotations provided by the French Annodis project (an ongoing effort to create\na discourse bank for French). Cross-validated on only 47 documents (1,445\nEDUs), our system achieves encouraging performance results with an F-score of\n73% for finding EDUs."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1004.4181v1", 
    "title": "Displacement Calculus", 
    "arxiv-id": "1004.4181v1", 
    "author": "Oriol Valent\u00edn", 
    "publish": "2010-04-23T17:00:18Z", 
    "summary": "The Lambek calculus provides a foundation for categorial grammar in the form\nof a logic of concatenation. But natural language is characterized by\ndependencies which may also be discontinuous. In this paper we introduce the\ndisplacement calculus, a generalization of Lambek calculus, which preserves its\ngood proof-theoretic properties while embracing discontinuiity and subsuming\nit. We illustrate linguistic applications and prove Cut-elimination, the\nsubformula property, and decidability"
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1005.3902v1", 
    "title": "Morphonette: a morphological network of French", 
    "arxiv-id": "1005.3902v1", 
    "author": "Nabil Hathout", 
    "publish": "2010-05-21T08:12:12Z", 
    "summary": "This paper describes in details the first version of Morphonette, a new\nFrench morphological resource and a new radically lexeme-based method of\nmorphological analysis. This research is grounded in a paradigmatic conception\nof derivational morphology where the morphological structure is a structure of\nthe entire lexicon and not one of the individual words it contains. The\ndiscovery of this structure relies on a measure of morphological similarity\nbetween words, on formal analogy and on the properties of two morphological\nparadigms:"
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1005.4697v3", 
    "title": "The Lambek-Grishin calculus is NP-complete", 
    "arxiv-id": "1005.4697v3", 
    "author": "Jeroen Bransen", 
    "publish": "2010-05-25T20:36:09Z", 
    "summary": "The Lambek-Grishin calculus LG is the symmetric extension of the\nnon-associative Lambek calculus NL. In this paper we prove that the\nderivability problem for LG is NP-complete."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1005.5466v1", 
    "title": "Quantitative parametrization of texts written by Ivan Franko: An attempt   of the project", 
    "arxiv-id": "1005.5466v1", 
    "author": "Solomiya Buk", 
    "publish": "2010-05-29T16:37:02Z", 
    "summary": "In the article, the project of quantitative parametrization of all texts by\nIvan Franko is manifested. It can be made only by using modern computer\ntechniques after the frequency dictionaries for all Franko's works are\ncompiled. The paper describes the application spheres, methodology, stages,\nprinciples and peculiarities in the compilation of the frequency dictionary of\nthe second half of the 19th century - the beginning of the 20th century. The\nrelation between the Ivan Franko frequency dictionary, explanatory dictionary\nof writer's language and text corpus is discussed."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1005.5596v1", 
    "title": "A generic tool to generate a lexicon for NLP from Lexicon-Grammar tables", 
    "arxiv-id": "1005.5596v1", 
    "author": "Elsa Tolone", 
    "publish": "2010-05-31T06:37:40Z", 
    "summary": "Lexicon-Grammar tables constitute a large-coverage syntactic lexicon but they\ncannot be directly used in Natural Language Processing (NLP) applications\nbecause they sometimes rely on implicit information. In this paper, we\nintroduce LGExtract, a generic tool for generating a syntactic lexicon for NLP\nfrom the Lexicon-Grammar tables. It is based on a global table that contains\nundefined information and on a unique extraction script including all\noperations to be performed for all tables. We also present an experiment that\nhas been conducted to generate a new lexicon of French verbs and predicative\nnouns."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1006.0153v1", 
    "title": "Ivan Franko's novel Dlja domashnjoho ohnyshcha (For the Hearth) in the   light of the frequency dictionary", 
    "arxiv-id": "1006.0153v1", 
    "author": "Solomiya Buk", 
    "publish": "2010-06-01T15:20:59Z", 
    "summary": "In the article, the methodology and the principles of the compilation of the\nFrequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For\nthe Hearth) are described. The following statistical parameters of the novel\nvocabulary are obtained: variety, exclusiveness, concentration indexes,\ncorrelation between word rank and text coverage, etc. The main quantitative\ncharacteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and\nDlja domashnjoho ohnyshcha are compared on the basis of their frequency\ndictionaries."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1006.2809v1", 
    "title": "Offline Arabic Handwriting Recognition Using Artificial Neural Network", 
    "arxiv-id": "1006.2809v1", 
    "author": "Rami Alnaqeib", 
    "publish": "2010-06-14T19:20:31Z", 
    "summary": "The ambition of a character recognition system is to transform a text\ndocument typed on paper into a digital format that can be manipulated by word\nprocessor software Unlike other languages, Arabic has unique features, while\nother language doesn't have, from this language these are seven or eight\nlanguage such as ordo, jewie and Persian writing, Arabic has twenty eight\nletters, each of which can be linked in three different ways or separated\ndepending on the case. The difficulty of the Arabic handwriting recognition is\nthat, the accuracy of the character recognition which affects on the accuracy\nof the word recognition, in additional there is also two or three from for each\ncharacter, the suggested solution by using artificial neural network can solve\nthe problem and overcome the difficulty of Arabic handwriting recognition."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1006.2835v1", 
    "title": "Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit   Grammar", 
    "arxiv-id": "1006.2835v1", 
    "author": "P. Venkata Subba Reddy", 
    "publish": "2010-06-14T20:07:32Z", 
    "summary": "Indian languages have long history in World Natural languages. Panini was the\nfirst to define Grammar for Sanskrit language with about 4000 rules in fifth\ncentury. These rules contain uncertainty information. It is not possible to\nComputer processing of Sanskrit language with uncertain information. In this\npaper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate\nuncertain information for reasoning with Sanskrit grammar. The Sanskrit\nlanguage processing is also discussed in this paper."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1006.3787v7", 
    "title": "Complete Complementary Results Report of the MARF's NLP Approach to the   DEFT 2010 Competition", 
    "arxiv-id": "1006.3787v7", 
    "author": "Serguei A. Mokhov", 
    "publish": "2010-06-18T19:54:29Z", 
    "summary": "This companion paper complements the main DEFT'10 article describing the MARF\napproach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described at\nhttp://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimed\nto present the complete result sets of all the conducted experiments and their\nsettings in the resulting tables highlighting the approach and the best\nresults, but also showing the worse and the worst and their subsequent\nanalysis. This particular work focuses on application of the MARF's classical\nand NLP pipelines to identification tasks within various francophone corpora to\nidentify decades when certain articles were published for the first track\n(Piste 1) and place of origin of a publication (Piste 2), such as the journal\nand location (France vs. Quebec). This is the sixth iteration of the release of\nthe results."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1006.5880v1", 
    "title": "Testing SDRT's Right Frontier", 
    "arxiv-id": "1006.5880v1", 
    "author": "Nicholas Asher", 
    "publish": "2010-06-30T15:10:33Z", 
    "summary": "The Right Frontier Constraint (RFC), as a constraint on the attachment of new\nconstituents to an existing discourse structure, has important implications for\nthe interpretation of anaphoric elements in discourse and for Machine Learning\n(ML) approaches to learning discourse structures. In this paper we provide\nstrong empirical support for SDRT's version of RFC. The analysis of about 100\ndoubly annotated documents by five different naive annotators shows that SDRT's\nRFC is respected about 95% of the time. The qualitative analysis of presumed\nviolations that we have performed shows that they are either click-errors or\nstructural misconceptions."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1008.0170v1", 
    "title": "Symmetric categorial grammar: residuation and Galois connections", 
    "arxiv-id": "1008.0170v1", 
    "author": "Michael Moortgat", 
    "publish": "2010-08-01T12:45:58Z", 
    "summary": "The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:\nin addition to the residuated family of product, left and right division\noperations of Lambek's original calculus, one also considers a family of\ncoproduct, right and left difference operations, related to the former by an\narrow-reversing duality. Communication between the two families is implemented\nin terms of linear distributivity principles. The aim of this paper is to\ncomplement the symmetry between (dual) residuated type-forming operations with\nan orthogonal opposition that contrasts residuated and Galois connected\noperations. Whereas the (dual) residuated operations are monotone, the Galois\nconnected operations (and their duals) are antitone. We discuss the algebraic\nproperties of the (dual) Galois connected operations, and generalize the\n(co)product distributivity principles to include the negative operations. We\ngive a continuation-passing-style translation for the new type-forming\noperations, and discuss some linguistic applications."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1008.1986v1", 
    "title": "For the sake of simplicity: Unsupervised extraction of lexical   simplifications from Wikipedia", 
    "arxiv-id": "1008.1986v1", 
    "author": "Lillian Lee", 
    "publish": "2010-08-11T20:01:59Z", 
    "summary": "We report on work in progress on extracting lexical simplifications (e.g.,\n\"collaborate\" -> \"work together\"), focusing on utilizing edit histories in\nSimple English Wikipedia for this task. We consider two main approaches: (1)\nderiving simplification probabilities via an edit model that accounts for a\nmixture of different operations, and (2) using metadata to focus on edits that\nare more likely to be simplification operations. We find our methods to\noutperform a reasonable baseline and yield many high-quality lexical\nsimplifications not included in an independently-created manually prepared\nlist."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1008.3169v2", 
    "title": "Don't 'have a clue'? Unsupervised co-learning of downward-entailing   operators", 
    "arxiv-id": "1008.3169v2", 
    "author": "Lillian Lee", 
    "publish": "2010-08-18T20:08:22Z", 
    "summary": "Researchers in textual entailment have begun to consider inferences involving\n'downward-entailing operators', an interesting and important class of lexical\nitems that change the way inferences are made. Recent work proposed a method\nfor learning English downward-entailing operators that requires access to a\nhigh-quality collection of 'negative polarity items' (NPIs). However, English\nis one of the very few languages for which such a list exists. We propose the\nfirst approach that can be applied to the many languages for which there is no\npre-existing high-precision database of NPIs. As a case study, we apply our\nmethod to Romanian and show that our method yields good results. Also, we\nperform a cross-linguistic analysis that suggests interesting connections to\nsome findings in linguistic typology."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1009.1117v2", 
    "title": "Constructions d\u00e9finitoires des tables du Lexique-Grammaire", 
    "arxiv-id": "1009.1117v2", 
    "author": "Christian Lecl\u00e8re", 
    "publish": "2010-09-06T18:35:08Z", 
    "summary": "Lexicon-Grammar tables are a very rich syntactic lexicon for the French\nlanguage. This linguistic database is nevertheless not directly suitable for\nuse by computer programs, as it is incomplete and lacks consistency. Tables are\ndefined on the basis of features which are not explicitly recorded in the\nlexicon. These features are only described in literature. Our aim is to define\nfor each tables these essential properties to make them usable in various\nNatural Language Processing (NLP) applications, such as parsing."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1009.3238v1", 
    "title": "Tableaux for the Lambek-Grishin calculus", 
    "arxiv-id": "1009.3238v1", 
    "author": "Arno Bastenhof", 
    "publish": "2010-09-16T18:19:57Z", 
    "summary": "Categorial type logics, pioneered by Lambek, seek a proof-theoretic\nunderstanding of natural language syntax by identifying categories with\nformulas and derivations with proofs. We typically observe an intuitionistic\nbias: a structural configuration of hypotheses (a constituent) derives a single\nconclusion (the category assigned to it). Acting upon suggestions of Grishin to\ndualize the logical vocabulary, Moortgat proposed the Lambek-Grishin calculus\n(LG) with the aim of restoring symmetry between hypotheses and conclusions. We\ndevelop a theory of labeled modal tableaux for LG, inspired by the\ninterpretation of its connectives as binary modal operators in the relational\nsemantics of Kurtonina and Moortgat. As a linguistic application of our method,\nwe show that grammars based on LG are context-free through use of an\ninterpolation lemma. This result complements that of Melissen, who proved that\nLG augmented by mixed associativity and -commutativity was exceeds LTAG in\nexpressive power."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1010.1826v1", 
    "title": "A probabilistic top-down parser for minimalist grammars", 
    "arxiv-id": "1010.1826v1", 
    "author": "Thomas Mainguy", 
    "publish": "2010-10-09T10:09:18Z", 
    "summary": "This paper describes a probabilistic top-down parser for minimalist grammars.\nTop-down parsers have the great advantage of having a certain predictive power\nduring the parsing, which takes place in a left-to-right reading of the\nsentence. Such parsers have already been well-implemented and studied in the\ncase of Context-Free Grammars, which are already top-down, but these are\ndifficult to adapt to Minimalist Grammars, which generate sentences bottom-up.\nI propose here a way of rewriting Minimalist Grammars as Linear Context-Free\nRewriting Systems, allowing to easily create a top-down parser. This rewriting\nallows also to put a probabilistic field on these grammars, which can be used\nto accelerate the parser. Finally, I propose a method of refining the\nprobabilistic field by using algorithms used in data compression."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1010.2384v1", 
    "title": "Learning Taxonomy for Text Segmentation by Formal Concept Analysis", 
    "arxiv-id": "1010.2384v1", 
    "author": "Zsuzsana Marian", 
    "publish": "2010-10-12T13:20:30Z", 
    "summary": "In this paper the problems of deriving a taxonomy from a text and\nconcept-oriented text segmentation are approached. Formal Concept Analysis\n(FCA) method is applied to solve both of these linguistic problems. The\nproposed segmentation method offers a conceptual view for text segmentation,\nusing a context-driven clustering of sentences. The Concept-oriented Clustering\nSegmentation algorithm (COCS) is based on k-means linear clustering of the\nsentences. Experimental results obtained using COCS algorithm are presented."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1011.0519v1", 
    "title": "Stabilizing knowledge through standards - A perspective for the   humanities", 
    "arxiv-id": "1011.0519v1", 
    "author": "Laurent Romary", 
    "publish": "2010-11-02T06:37:31Z", 
    "summary": "It is usual to consider that standards generate mixed feelings among\nscientists. They are often seen as not really reflecting the state of the art\nin a given domain and a hindrance to scientific creativity. Still, scientists\nshould theoretically be at the best place to bring their expertise into\nstandard developments, being even more neutral on issues that may typically be\nrelated to competing industrial interests. Even if it could be thought of as\neven more complex to think about developping standards in the humanities, we\nwill show how this can be made feasible through the experience gained both\nwithin the Text Encoding Initiative consortium and the International\nOrganisation for Standardisation. By taking the specific case of lexical\nresources, we will try to show how this brings about new ideas for designing\nfuture research infrastructures in the human and social sciences."
},{
    "category": "cs.CL", 
    "doi": "10.4018/978-1-60960-031-0", 
    "link": "http://arxiv.org/pdf/1011.0835v1", 
    "title": "A PDTB-Styled End-to-End Discourse Parser", 
    "arxiv-id": "1011.0835v1", 
    "author": "Min-Yen Kan", 
    "publish": "2010-11-03T10:05:15Z", 
    "summary": "We have developed a full discourse parser in the Penn Discourse Treebank\n(PDTB) style. Our trained parser first identifies all discourse and\nnon-discourse relations, locates and labels their arguments, and then\nclassifies their relation types. When appropriate, the attribution spans to\nthese relations are also determined. We present a comprehensive evaluation from\nboth component-wise and error-cascading perspectives."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-12397-9_2", 
    "link": "http://arxiv.org/pdf/1011.2922v1", 
    "title": "Emoticonsciousness", 
    "arxiv-id": "1011.2922v1", 
    "author": "Jerom Janssen", 
    "publish": "2010-11-12T14:43:04Z", 
    "summary": "A temporal analysis of emoticon use in Swedish, Italian, German and English\nasynchronous electronic communication is reported. Emoticons are classified as\npositive, negative and neutral. Postings to newsgroups over a 66 week period\nare considered. The aggregate analysis of emoticon use in newsgroups for\nscience and politics tend on the whole to be consistent over the entire time\nperiod. Where possible, events that coincide with divergences from trends in\nlanguage-subject pairs are noted. Political discourse in Italian over the\nperiod shows marked use of negative emoticons, and in Swedish, positive\nemoticons."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-12397-9_2", 
    "link": "http://arxiv.org/pdf/1011.4155v1", 
    "title": "Motifs de graphe pour le calcul de d\u00e9pendances syntaxiques compl\u00e8tes", 
    "arxiv-id": "1011.4155v1", 
    "author": "Guy Perrier", 
    "publish": "2010-11-18T08:59:55Z", 
    "summary": "This article describes a method to build syntactical dependencies starting\nfrom the phrase structure parsing process. The goal is to obtain all the\ninformation needed for a detailled semantical analysis. Interaction Grammars\nare used for parsing; the saturation of polarities which is the core of this\nformalism can be mapped to dependency relation. Formally, graph patterns are\nused to express the set of constraints which control dependency creations."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-12397-9_2", 
    "link": "http://arxiv.org/pdf/1011.4623v1", 
    "title": "Opinion Polarity Identification through Adjectives", 
    "arxiv-id": "1011.4623v1", 
    "author": "Fred Popowich", 
    "publish": "2010-11-21T00:15:27Z", 
    "summary": "\"What other people think\" has always been an important piece of information\nduring various decision-making processes. Today people frequently make their\nopinions available via the Internet, and as a result, the Web has become an\nexcellent source for gathering consumer opinions. There are now numerous Web\nresources containing such opinions, e.g., product reviews forums, discussion\ngroups, and Blogs. But, due to the large amount of information and the wide\nrange of sources, it is essentially impossible for a customer to read all of\nthe reviews and make an informed decision on whether to purchase the product.\nIt is also difficult for the manufacturer or seller of a product to accurately\nmonitor customer opinions. For this reason, mining customer reviews, or opinion\nmining, has become an important issue for research in Web information\nextraction. One of the important topics in this research area is the\nidentification of opinion polarity. The opinion polarity of a review is usually\nexpressed with values 'positive', 'negative' or 'neutral'. We propose a\ntechnique for identifying polarity of reviews by identifying the polarity of\nthe adjectives that appear in them. Our evaluation shows the technique can\nprovide accuracy in the area of 73%, which is well above the 58%-64% provided\nby naive Bayesian classifiers."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-12397-9_2", 
    "link": "http://arxiv.org/pdf/1011.5188v2", 
    "title": "La r\u00e9duction de termes complexes dans les langues de sp\u00e9cialit\u00e9", 
    "arxiv-id": "1011.5188v2", 
    "author": "Elisa Lavagnino", 
    "publish": "2010-11-23T18:20:40Z", 
    "summary": "Our study applies statistical methods to French and Italian corpora to\nexamine the phenomenon of multi-word term reduction in specialty languages.\nThere are two kinds of reduction: anaphoric and lexical. We show that anaphoric\nreduction depends on the discourse type (vulgarization, pedagogical,\nspecialized) but is independent of both domain and language; that lexical\nreduction depends on domain and is more frequent in technical, rapidly evolving\ndomains; and that anaphoric reductions tend to follow full terms rather than\nprecede them. We define the notion of the anaphoric tree of the term and study\nits properties. Concerning lexical reduction, we attempt to prove statistically\nthat there is a notion of term lifecycle, where the full form is progressively\nreplaced by a lexical reduction. ----- Nous \\'etudions par des m\\'ethodes\nstatistiques sur des corpus fran\\c{c}ais et italiens, le ph\\'enom\\`ene de\nr\\'eduction des termes complexes dans les langues de sp\\'ecialit\\'e. Il existe\ndeux types de r\\'eductions : anaphorique et lexicale. Nous montrons que la\nr\\'eduction anaphorique d\\'epend du type de discours (de vulgarisation,\np\\'edagogique, sp\\'ecialis\\'e) mais ne d\\'epend ni du domaine, ni de la langue,\nalors que la r\\'eduction lexicale d\\'epend du domaine et est plus fr\\'equente\ndans les domaines techniques \\`a \\'evolution rapide. D'autre part, nous\nmontrons que la r\\'eduction anaphorique a tendance \\`a suivre la forme pleine\ndu terme, nous d\\'efinissons une notion d'arbre anaphorique de terme et nous\n\\'etudions ses propri\\'et\\'es. Concernant la r\\'eduction lexicale, nous tentons\nde d\\'emontrer statistiquement qu'il existe une notion de cycle de vie de\nterme, o\\`u la forme pleine est progressivement remplac\\'ee par une r\\'eduction\nlexicale."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-12397-9_2", 
    "link": "http://arxiv.org/pdf/1012.5962v2", 
    "title": "Annotated English", 
    "arxiv-id": "1012.5962v2", 
    "author": "Jose Hernandez-Orallo", 
    "publish": "2010-12-29T15:40:09Z", 
    "summary": "This document presents Annotated English, a system of diacritical symbols\nwhich turns English pronunciation into a precise and unambiguous process. The\nannotations are defined and located in such a way that the original English\ntext is not altered (not even a letter), thus allowing for a consistent reading\nand learning of the English language with and without annotations. The\nannotations are based on a set of general rules that make the frequency of\nannotations not dramatically high. This makes the reader easily associate\nannotations with exceptions, and makes it possible to shape, internalise and\nconsolidate some rules for the English language which otherwise are weakened by\nthe enormous amount of exceptions in English pronunciation. The advantages of\nthis annotation system are manifold. Any existing text can be annotated without\na significant increase in size. This means that we can get an annotated version\nof any document or book with the same number of pages and fontsize. Since no\nletter is affected, the text can be perfectly read by a person who does not\nknow the annotation rules, since annotations can be simply ignored. The\nannotations are based on a set of rules which can be progressively learned and\nrecognised, even in cases where the reader has no access or time to read the\nrules. This means that a reader can understand most of the annotations after\nreading a few pages of Annotated English, and can take advantage from that\nknowledge for any other annotated document she may read in the future."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9164-2", 
    "link": "http://arxiv.org/pdf/1101.5076v6", 
    "title": "Geometric representations for minimalist grammars", 
    "arxiv-id": "1101.5076v6", 
    "author": "Sabrina Gerth", 
    "publish": "2011-01-26T14:56:20Z", 
    "summary": "We reformulate minimalist grammars as partial functions on term algebras for\nstrings and trees. Using filler/role bindings and tensor product\nrepresentations, we construct homomorphisms for these data structures into\ngeometric vector spaces. We prove that the structure-building functions as well\nas simple processors for minimalist languages can be realized by piecewise\nlinear operators in representation space. We also propose harmony, i.e. the\ndistance of an intermediate processing step from the final well-formed state in\nrepresentation space, as a measure of processing complexity. Finally, we\nillustrate our findings by means of two particular arithmetic and fractal\nrepresentations."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9164-2", 
    "link": "http://arxiv.org/pdf/1101.5494v1", 
    "title": "Developing a New Approach for Arabic Morphological Analysis and   Generation", 
    "arxiv-id": "1101.5494v1", 
    "author": "Noureddine Chenfour", 
    "publish": "2011-01-28T09:58:39Z", 
    "summary": "Arabic morphological analysis is one of the essential stages in Arabic\nNatural Language Processing. In this paper we present an approach for Arabic\nmorphological analysis. This approach is based on Arabic morphological\nautomaton (AMAUT). The proposed technique uses a morphological database\nrealized using XMODEL language. Arabic morphology represents a special type of\nmorphological systems because it is based on the concept of scheme to represent\nArabic words. We use this concept to develop the Arabic morphological automata.\nThe proposed approach has development standardization aspect. It can be\nexploited by NLP applications such as syntactic and semantic analysis,\ninformation retrieval, machine translation and orthographical correction. The\nproposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic\nAnalyzer."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9164-2", 
    "link": "http://arxiv.org/pdf/1101.5757v1", 
    "title": "Polarized Montagovian Semantics for the Lambek-Grishin calculus", 
    "arxiv-id": "1101.5757v1", 
    "author": "Arno Bastenhof", 
    "publish": "2011-01-30T10:41:46Z", 
    "summary": "Grishin proposed enriching the Lambek calculus with multiplicative\ndisjunction (par) and coresiduals. Applications to linguistics were discussed\nby Moortgat, who spoke of the Lambek-Grishin calculus (LG). In this paper, we\nadapt Girard's polarity-sensitive double negation embedding for classical logic\nto extract a compositional Montagovian semantics from a display calculus for\nfocused proof search in LG. We seize the opportunity to illustrate our approach\nalongside an analysis of extraction, providing linguistic motivation for linear\ndistributivity of tensor over par, thus answering a question of\nKurtonina&Moortgat. We conclude by comparing our proposal to the continuation\nsemantics of Bernardi&Moortgat, corresponding to call-by- name and\ncall-by-value evaluation strategies."
},{
    "category": "cs.CL", 
    "doi": "10.1155/2011/251753", 
    "link": "http://arxiv.org/pdf/1103.1898v1", 
    "title": "Recognizing Uncertainty in Speech", 
    "arxiv-id": "1103.1898v1", 
    "author": "Stuart M. Shieber", 
    "publish": "2011-03-09T21:43:46Z", 
    "summary": "We address the problem of inferring a speaker's level of certainty based on\nprosodic information in the speech signal, which has application in\nspeech-based dialogue systems. We show that using phrase-level prosodic\nfeatures centered around the phrases causing uncertainty, in addition to\nutterance-level prosodic features, improves our model's level of certainty\nclassification. In addition, our models can be used to predict which phrase a\nperson is uncertain about. These results rely on a novel method for eliciting\nutterances of varying levels of certainty that allows us to compare the utility\nof contextually-based feature sets. We elicit level of certainty ratings from\nboth the speakers themselves and a panel of listeners, finding that there is\noften a mismatch between speakers' internal states and their perceived states,\nand highlighting the importance of this distinction."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2011.608606", 
    "link": "http://arxiv.org/pdf/1103.2950v1", 
    "title": "Fitting Ranked English and Spanish Letter Frequency Distribution in U.S.   and Mexican Presidential Speeches", 
    "arxiv-id": "1103.2950v1", 
    "author": "Pedro Miramontes", 
    "publish": "2011-03-15T16:21:24Z", 
    "summary": "The limited range in its abscissa of ranked letter frequency distributions\ncauses multiple functions to fit the observed distribution reasonably well. In\norder to critically compare various functions, we apply the statistical model\nselections on ten functions, using the texts of U.S. and Mexican presidential\nspeeches in the last 1-2 centuries. Dispite minor switching of ranking order of\ncertain letters during the temporal evolution for both datasets, the letter\nusage is generally stable. The best fitting function, judged by either\nleast-square-error or by AIC/BIC model selection, is the Cocho/Beta function.\nWe also use a novel method to discover clusters of letters by their\nobserved-over-expected frequency ratios."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2011.608606", 
    "link": "http://arxiv.org/pdf/1103.5676v1", 
    "title": "Codeco: A Grammar Notation for Controlled Natural Language in Predictive   Editors", 
    "arxiv-id": "1103.5676v1", 
    "author": "Tobias Kuhn", 
    "publish": "2011-03-29T15:05:11Z", 
    "summary": "Existing grammar frameworks do not work out particularly well for controlled\nnatural languages (CNL), especially if they are to be used in predictive\neditors. I introduce in this paper a new grammar notation, called Codeco, which\nis designed specifically for CNLs and predictive editors. Two different parsers\nhave been implemented and a large subset of Attempto Controlled English (ACE)\nhas been represented in Codeco. The results show that Codeco is practical,\nadequate and efficient."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2011.608606", 
    "link": "http://arxiv.org/pdf/1105.1072v1", 
    "title": "English-Lithuanian-English Machine Translation lexicon and engine:   current state and future work", 
    "arxiv-id": "1105.1072v1", 
    "author": "B. Tamulynas", 
    "publish": "2011-05-05T13:51:46Z", 
    "summary": "This article overviews the current state of the English-Lithuanian-English\nmachine translation system. The first part of the article describes the\nproblems that system poses today and what actions will be taken to solve them\nin the future. The second part of the article tackles the main issue of the\ntranslation process. Article briefly overviews the word sense disambiguation\nfor MT technique using Google."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2011.608606", 
    "link": "http://arxiv.org/pdf/1105.1226v1", 
    "title": "Multilingual lexicon design tool and database management system for MT", 
    "arxiv-id": "1105.1226v1", 
    "author": "B. Tamulynas", 
    "publish": "2011-05-06T05:26:43Z", 
    "summary": "The paper presents the design and development of English-Lithuanian-English\ndictionarylexicon tool and lexicon database management system for MT. The\nsystem is oriented to support two main requirements: to be open to the user and\nto describe much more attributes of speech parts as a regular dictionary that\nare required for the MT. Programming language Java and database management\nsystem MySql is used to implement the designing tool and lexicon database\nrespectively. This solution allows easily deploying this system in the\nInternet. The system is able to run on various OS such as: Windows, Linux, Mac\nand other OS where Java Virtual Machine is supported. Since the modern lexicon\ndatabase managing system is used, it is not a problem accessing the same\ndatabase for several users."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2011.608606", 
    "link": "http://arxiv.org/pdf/1105.6162v2", 
    "title": "A statistical learning algorithm for word segmentation", 
    "arxiv-id": "1105.6162v2", 
    "author": "Jerry R. Van Aken", 
    "publish": "2011-05-31T05:03:06Z", 
    "summary": "In natural speech, the speaker does not pause between words, yet a human\nlistener somehow perceives this continuous stream of phonemes as a series of\ndistinct words. The detection of boundaries between spoken words is an instance\nof a general capability of the human neocortex to remember and to recognize\nrecurring sequences. This paper describes a computer algorithm that is designed\nto solve the problem of locating word boundaries in blocks of English text from\nwhich the spaces have been removed. This problem avoids the complexities of\nspeech processing but requires similar capabilities for detecting recurring\nsequences. The algorithm relies entirely on statistical relationships between\nletters in the input stream to infer the locations of word boundaries. A\nViterbi trellis is used to simultaneously evaluate a set of hypothetical\nsegmentations of a block of adjacent words. This technique improves accuracy\nbut incurs a small latency between the arrival of letters in the input stream\nand the sending of words to the output stream. The source code for a C++\nversion of this algorithm is presented in an appendix."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.848", 
    "link": "http://arxiv.org/pdf/1106.0673v1", 
    "title": "Computational Approach to Anaphora Resolution in Spanish Dialogues", 
    "arxiv-id": "1106.0673v1", 
    "author": "M. Palomar", 
    "publish": "2011-06-03T14:54:46Z", 
    "summary": "This paper presents an algorithm for identifying noun-phrase antecedents of\npronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora\nresolution requires numerous sources of information in order to find the\ncorrect antecedent of the anaphor. These sources can be of different kinds,\ne.g., linguistic information, discourse/dialogue structure information, or\ntopic information. For this reason, our algorithm uses various different kinds\nof information (hybrid information). The algorithm is based on linguistic\nconstraints and preferences and uses an anaphoric accessibility space within\nwhich the algorithm finds the noun phrase. We present some experiments related\nto this algorithm and this space using a corpus of 204 dialogues. The algorithm\nis implemented in Prolog. According to this study, 95.9% of antecedents were\nlocated in the proposed space, a precision of 81.3% was obtained for pronominal\nanaphora resolution, and 81.5% for adjectival anaphora."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1176", 
    "link": "http://arxiv.org/pdf/1106.5264v1", 
    "title": "Acquiring Correct Knowledge for Natural Language Generation", 
    "arxiv-id": "1106.5264v1", 
    "author": "S. G. Sripada", 
    "publish": "2011-06-26T21:05:14Z", 
    "summary": "Natural language generation (NLG) systems are computer software systems that\nproduce texts in English and other human languages, often from non-linguistic\ninput data. NLG systems, like most AI systems, need substantial amounts of\nknowledge. However, our experience in two NLG projects suggests that it is\ndifficult to acquire correct knowledge for NLG systems; indeed, every knowledge\nacquisition (KA) technique we tried had significant problems. In general terms,\nthese problems were due to the complexity, novelty, and poorly understood\nnature of the tasks our systems attempted, and were worsened by the fact that\npeople write so differently. This meant in particular that corpus-based KA\napproaches suffered because it was impossible to assemble a sizable corpus of\nhigh-quality consistent manually written texts in our domains; and structured\nexpert-oriented KA techniques suffered because experts disagreed and because we\ncould not get enough information about special and unusual cases to build\nrobust systems. We believe that such problems are likely to affect many other\nNLG systems as well. In the long term, we hope that new KA techniques may\nemerge to help NLG system builders. In the shorter term, we believe that\nunderstanding how individual KA techniques can fail, and using a mixture of\ndifferent KA techniques with different strengths and weaknesses, can help\ndevelopers acquire NLG knowledge that is mostly correct."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1176", 
    "link": "http://arxiv.org/pdf/1106.5973v1", 
    "title": "Entropy of Telugu", 
    "arxiv-id": "1106.5973v1", 
    "author": "Venkata Ravinder Paruchuri", 
    "publish": "2011-06-27T20:13:07Z", 
    "summary": "This paper presents an investigation of the entropy of the Telugu script.\nSince this script is syllabic, and not alphabetic, the computation of entropy\nis somewhat complicated."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.0193v3", 
    "title": "On the origin of ambiguity in efficient communication", 
    "arxiv-id": "1107.0193v3", 
    "author": "Bernat Corominas-Murtra", 
    "publish": "2011-07-01T11:01:52Z", 
    "summary": "This article studies the emergence of ambiguity in communication through the\nconcept of logical irreversibility and within the framework of Shannon's\ninformation theory. This leads us to a precise and general expression of the\nintuition behind Zipf's vocabulary balance in terms of a symmetry equation\nbetween the complexities of the coding and the decoding processes that imposes\nan unavoidable amount of logical uncertainty in natural communication.\nAccordingly, the emergence of irreversible computations is required if the\ncomplexities of the coding and the decoding processes are balanced in a\nsymmetric scenario, which means that the emergence of ambiguous codes is a\nnecessary condition for natural communication to succeed."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.1753v1", 
    "title": "Notes on Electronic Lexicography", 
    "arxiv-id": "1107.1753v1", 
    "author": "Yavor Parvanov", 
    "publish": "2011-07-09T00:40:06Z", 
    "summary": "These notes are a continuation of topics covered by V. Selegej in his article\n\"Electronic Dictionaries and Computational lexicography\". How can an electronic\ndictionary have as its object the description of closely related languages?\nObviously, such a question allows multiple answers."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.4687v2", 
    "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven   Language Specification", 
    "arxiv-id": "1107.4687v2", 
    "author": "Francisco J. Cortijo", 
    "publish": "2011-07-23T12:56:02Z", 
    "summary": "Model-based language specification has applications in the implementation of\nlanguage processors, the design of domain-specific languages, model-driven\nsoftware development, data integration, text mining, natural language\nprocessing, and corpus-based induction of models. Model-based language\nspecification decouples language design from language processing and, unlike\ntraditional grammar-driven approaches, which constrain language designers to\nspecific kinds of grammars, it needs general parser generators able to deal\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\nparsing algorithm with lexical and syntactic ambiguity support that enables the\nuse of model-based language specification in practice."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.4723v2", 
    "title": "A Semantic Relatedness Measure Based on Combined Encyclopedic,   Ontological and Collocational Knowledge", 
    "arxiv-id": "1107.4723v2", 
    "author": "Vitaly Klyuev", 
    "publish": "2011-07-24T06:27:56Z", 
    "summary": "We describe a new semantic relatedness measure combining the Wikipedia-based\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\ncollocation index. Our measure achieves the currently highest results on the\nWS-353 test: a Spearman rho coefficient of 0.79 (vs. 0.75 in (Gabrilovich and\nMarkovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs.\n0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM\nclassifier trained on our measure.\n  In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as\nwell as various unsuccessful attempts to enhance ESA by filtering at word,\nsentence, and section level."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.4734v1", 
    "title": "Design of Arabic Diacritical Marks", 
    "arxiv-id": "1107.4734v1", 
    "author": "Azzeddine Lazrek", 
    "publish": "2011-07-24T08:35:12Z", 
    "summary": "Diacritical marks play a crucial role in meeting the criteria of usability of\ntypographic text, such as: homogeneity, clarity and legibility. To change the\ndiacritic of a letter in a word could completely change its semantic. The\nsituation is very complicated with multilingual text. Indeed, the problem of\ndesign becomes more difficult by the presence of diacritics that come from\nvarious scripts; they are used for different purposes, and are controlled by\nvarious typographic rules. It is quite challenging to adapt rules from one\nscript to another. This paper aims to study the placement and sizing of\ndiacritical marks in Arabic script, with a comparison with the Latin's case.\nThe Arabic script is cursive and runs from right-to-left; its criteria and\nrules are quite distinct from those of the Latin script. In the beginning, we\ncompare the difficulty of processing diacritics in both scripts. After, we will\nstudy the limits of Latin resolution strategies when applied to Arabic. At the\nend, we propose an approach to resolve the problem for positioning and resizing\ndiacritics. This strategy includes creating an Arabic font, designed in\nOpenType format, along with suitable justification in TEX."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.4796v1", 
    "title": "Use Pronunciation by Analogy for text to speech system in Persian   language", 
    "arxiv-id": "1107.4796v1", 
    "author": "Mohammad hosein Yektaee", 
    "publish": "2011-07-24T20:37:57Z", 
    "summary": "The interest in text to speech synthesis increased in the world .text to\nspeech have been developed formany popular languages such as English, Spanish\nand French and many researches and developmentshave been applied to those\nlanguages. Persian on the other hand, has been given little attentioncompared\nto other languages of similar importance and the research in Persian is still\nin its infancy.Persian language possess many difficulty and exceptions that\nincrease complexity of text to speechsystems. For example: short vowels is\nabsent in written text or existence of homograph words. in thispaper we propose\na new method for persian text to phonetic that base on pronunciations by\nanalogy inwords, semantic relations and grammatical rules for finding proper\nphonetic. Keywords:PbA, text to speech, Persian language, FPbA"
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.5743v1", 
    "title": "NEMO: Extraction and normalization of organization names from PubMed   affiliation strings", 
    "arxiv-id": "1107.5743v1", 
    "author": "Philip Topham", 
    "publish": "2011-07-28T15:37:56Z", 
    "summary": "We propose NEMO, a system for extracting organization names in the\naffiliation and normalizing them to a canonical organization name. Our parsing\nprocess involves multi-layered rule matching with multiple dictionaries. The\nsystem achieves more than 98% f-score in extracting organization names. Our\nprocess of normalization that involves clustering based on local sequence\nalignment metrics and local learning based on finding connected components. A\nhigh precision was also observed in normalization. NEMO is the missing link in\nassociating each biomedical paper and its authors to an organization name in\nits canonical form and the Geopolitical location of the organization. This\nresearch could potentially help in analyzing large social networks of\norganizations for landscaping a particular topic, improving performance of\nauthor disambiguation, adding weak links in the co-author network of authors,\naugmenting NLM's MARS system for correcting errors in OCR output of affiliation\nfield, and automatically indexing the PubMed citations with the normalized\norganization name and country. Our system is available as a graphical user\ninterface available for download along with this paper."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.5744v1", 
    "title": "BioSimplify: an open source sentence simplification engine to improve   recall in automatic biomedical information extraction", 
    "arxiv-id": "1107.5744v1", 
    "author": "Graciela Gonzalez", 
    "publish": "2011-07-28T15:40:31Z", 
    "summary": "BioSimplify is an open source tool written in Java that introduces and\nfacilitates the use of a novel model for sentence simplification tuned for\nautomatic discourse analysis and information extraction (as opposed to sentence\nsimplification for improving human readability). The model is based on a\n\"shot-gun\" approach that produces many different (simpler) versions of the\noriginal sentence by combining variants of its constituent elements. This tool\nis optimized for processing biomedical scientific literature such as the\nabstracts indexed in PubMed. We tested our tool on its impact to the task of\nPPI extraction and it improved the f-score of the PPI tool by around 7%, with\nan improvement in recall of around 20%. The BioSimplify tool and test corpus\ncan be downloaded from https://biosimplify.sourceforge.net."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1107.5752v2", 
    "title": "An Effective Approach to Biomedical Information Extraction with Limited   Training Data", 
    "arxiv-id": "1107.5752v2", 
    "author": "Siddhartha Jonnalagadda", 
    "publish": "2011-07-28T15:59:21Z", 
    "summary": "Overall, the two main contributions of this work include the application of\nsentence simplification to association extraction as described above, and the\nuse of distributional semantics for concept extraction. The proposed work on\nconcept extraction amalgamates for the first time two diverse research areas\n-distributional semantics and information extraction. This approach renders all\nthe advantages offered in other semi-supervised machine learning systems, and,\nunlike other proposed semi-supervised approaches, it can be used on top of\ndifferent basic frameworks and algorithms.\nhttp://gradworks.umi.com/34/49/3449837.html"
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.0353v2", 
    "title": "Cross-moments computation for stochastic context-free grammars", 
    "arxiv-id": "1108.0353v2", 
    "author": "Miomir S. Stankovic", 
    "publish": "2011-08-01T16:20:50Z", 
    "summary": "In this paper we consider the problem of efficient computation of\ncross-moments of a vector random variable represented by a stochastic\ncontext-free grammar. Two types of cross-moments are discussed. The sample\nspace for the first one is the set of all derivations of the context-free\ngrammar, and the sample space for the second one is the set of all derivations\nwhich generate a string belonging to the language of the grammar. In the past,\nthis problem was widely studied, but mainly for the cross-moments of scalar\nvariables and up to the second order. This paper presents new algorithms for\ncomputing the cross-moments of an arbitrary order, and the previously developed\nones are derived as special cases."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.0631v3", 
    "title": "Serialising the ISO SynAF Syntactic Object Model", 
    "arxiv-id": "1108.0631v3", 
    "author": "Florian Zipser", 
    "publish": "2011-08-02T17:33:36Z", 
    "summary": "This paper introduces, an XML format developed to serialise the object model\ndefined by the ISO Syntactic Annotation Framework SynAF. Based on widespread\nbest practices we adapt a popular XML format for syntactic annotation,\nTigerXML, with additional features to support a variety of syntactic phenomena\nincluding constituent and dependency structures, binding, and different node\ntypes such as compounds or empty elements. We also define interfaces to other\nformats and standards including the Morpho-syntactic Annotation Framework MAF\nand the ISOCat Data Category Registry. Finally a case study of the German\nTreebank TueBa-D/Z is presented, showcasing the handling of constituent\nstructures, topological fields and coreference annotation in tandem."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.1966v1", 
    "title": "A Concise Query Language with Search and Transform Operations for   Corpora with Multiple Levels of Annotation", 
    "arxiv-id": "1108.1966v1", 
    "author": "Anil Kumar Singh", 
    "publish": "2011-08-09T16:03:24Z", 
    "summary": "The usefulness of annotated corpora is greatly increased if there is an\nassociated tool that can allow various kinds of operations to be performed in a\nsimple way. Different kinds of annotation frameworks and many query languages\nfor them have been proposed, including some to deal with multiple layers of\nannotation. We present here an easy to learn query language for a particular\nkind of annotation framework based on 'threaded trees', which are somewhere\nbetween the complete order of a tree and the anarchy of a graph. Through\n'typed' threads, they can allow multiple levels of annotation in the same\ndocument. Our language has a simple, intuitive and concise syntax and high\nexpressive power. It allows not only to search for complicated patterns with\nshort queries but also allows data manipulation and specification of arbitrary\nreturn values. Many of the commonly used tasks that otherwise require writing\nprograms, can be performed with one or more queries. We compare the language\nwith some others and try to evaluate it."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.3843v1", 
    "title": "Using Inverse lambda and Generalization to Translate English to Formal   Languages", 
    "arxiv-id": "1108.3843v1", 
    "author": "Jiayu Zhou", 
    "publish": "2011-08-18T20:04:28Z", 
    "summary": "We present a system to translate natural language sentences to formulas in a\nformal or a knowledge representation language. Our system uses two inverse\nlambda-calculus operators and using them can take as input the semantic\nrepresentation of some words, phrases and sentences and from that derive the\nsemantic representation of other words and phrases. Our inverse lambda operator\nworks on many formal languages including first order logic, database query\nlanguages and answer set programming. Our system uses a syntactic combinatorial\ncategorial parser to parse natural language sentences and also to construct the\nsemantic meaning of the sentences as directed by their parsing. The same parser\nis used for both. In addition to the inverse lambda-calculus operators, our\nsystem uses a notion of generalization to learn semantic representation of\nwords from the semantic representation of other words that are of the same\ncategory. Together with this, we use an existing statistical learning approach\nto assign weights to deal with multiple meanings of words. Our system produces\nimproved results on standard corpora on natural language interfaces for robot\ncommand and control and database queries."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.3848v1", 
    "title": "Language understanding as a step towards human level intelligence -   automatizing the construction of the initial dictionary from example   sentences", 
    "arxiv-id": "1108.3848v1", 
    "author": "Juraj Dzifcak", 
    "publish": "2011-08-18T20:12:50Z", 
    "summary": "For a system to understand natural language, it needs to be able to take\nnatural language text and answer questions given in natural language with\nrespect to that text; it also needs to be able to follow instructions given in\nnatural language. To achieve this, a system must be able to process natural\nlanguage and be able to capture the knowledge within that text. Thus it needs\nto be able to translate natural language text into a formal language. We\ndiscuss our approach to do this, where the translation is achieved by composing\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\nmethod that we developed (and other methods) to learn meaning of words from\nmeaning of sentences and an initial lexicon. We then present an improved method\nwhere the initial lexicon is also learned by analyzing the training sentence\nand meaning pairs. We evaluate our methods and compare them with other existing\nmethods on a corpora of database querying and robot command and control."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.4052v1", 
    "title": "Query Expansion: Term Selection using the EWC Semantic Relatedness   Measure", 
    "arxiv-id": "1108.4052v1", 
    "author": "Yannis Haralambous", 
    "publish": "2011-08-19T21:41:29Z", 
    "summary": "This paper investigates the efficiency of the EWC semantic relatedness\nmeasure in an ad-hoc retrieval task. This measure combines the Wikipedia-based\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\ncollocation index. In the experiments, the open source search engine Terrier\nwas utilised as a tool to index and retrieve data. The proposed technique was\ntested on the NTCIR data collection. The experiments demonstrated promising\nresults."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1108.5096v1", 
    "title": "Minimalist Grammars and Minimalist Categorial Grammars, definitions   toward inclusion of generated languages", 
    "arxiv-id": "1108.5096v1", 
    "author": "Maxime Amblard", 
    "publish": "2011-08-25T14:15:46Z", 
    "summary": "Stabler proposes an implementation of the Chomskyan Minimalist Program,\nChomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a\nlong linguistic tradition. But the semantic calculus is more easily added if\none uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,\nbased on an extension of the Lambek calculus, the mixed logic, were introduced\nto provide a theoretically-motivated syntax-semantics interface, Amblard 07. In\nthis article, we give full definitions of MG with algebraic tree descriptions\nand of MCG, and take the first steps towards giving a proof of inclusion of\ntheir generated languages."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1110.1470v3", 
    "title": "A Constraint-Satisfaction Parser for Context-Free Grammars", 
    "arxiv-id": "1110.1470v3", 
    "author": "Francisco J. Cortijo", 
    "publish": "2011-10-07T09:53:41Z", 
    "summary": "Traditional language processing tools constrain language designers to\nspecific kinds of grammars. In contrast, model-based language specification\ndecouples language design from language processing. As a consequence,\nmodel-based language specification tools need general parsers able to parse\nunrestricted context-free grammars. As languages specified following this\napproach may be ambiguous, parsers must deal with ambiguities. Model-based\nlanguage specification also allows the definition of associativity, precedence,\nand custom constraints. Therefore parsers generated by model-driven language\nspecification tools need to enforce constraints. In this paper, we propose\nFence, an efficient bottom-up chart parser with lexical and syntactic ambiguity\nsupport that allows the specification of constraints and, therefore, enables\nthe use of model-based language specification in practice."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-013-9179-3", 
    "link": "http://arxiv.org/pdf/1110.1758v2", 
    "title": "Data formats for phonological corpora", 
    "arxiv-id": "1110.1758v2", 
    "author": "Andreas Witt", 
    "publish": "2011-10-08T19:15:12Z", 
    "summary": "The goal of the present chapter is to explore the possibility of providing\nthe research (but also the industrial) community that commonly uses spoken\ncorpora with a stable portfolio of well-documented standardised formats that\nallow a high re-use rate of annotated spoken resources and, as a consequence,\nbetter interoperability across tools used to produce or exploit such resources."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2179", 
    "link": "http://arxiv.org/pdf/1110.2215v1", 
    "title": "NP Animacy Identification for Anaphora Resolution", 
    "arxiv-id": "1110.2215v1", 
    "author": "C. Orasan", 
    "publish": "2011-10-10T22:13:24Z", 
    "summary": "In anaphora resolution for English, animacy identification can play an\nintegral role in the application of agreement restrictions between pronouns and\ncandidates, and as a result, can improve the accuracy of anaphora resolution\nsystems. In this paper, two methods for animacy identification are proposed and\nevaluated using intrinsic and extrinsic measures. The first method is a\nrule-based one which uses information about the unique beginners in WordNet to\nclassify NPs on the basis of their animacy. The second method relies on a\nmachine learning algorithm which exploits a WordNet enriched with animacy\ninformation for each sense. The effect of word sense disambiguation on the two\nmethods is also assessed. The intrinsic evaluation reveals that the machine\nlearning method reaches human levels of performance. The extrinsic evaluation\ndemonstrates that animacy identification can be beneficial in anaphora\nresolution, especially in the cases where animate entities are identified with\nhigh precision."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2179", 
    "link": "http://arxiv.org/pdf/1110.4248v1", 
    "title": "Ideogram Based Chinese Sentiment Word Orientation Computation", 
    "arxiv-id": "1110.4248v1", 
    "author": "Luojie Xiang", 
    "publish": "2011-10-19T11:45:16Z", 
    "summary": "This paper presents a novel algorithm to compute sentiment orientation of\nChinese sentiment word. The algorithm uses ideograms which are a distinguishing\nfeature of Chinese language. The proposed algorithm can be applied to any\nsentiment classification scheme. To compute a word's sentiment orientation\nusing the proposed algorithm, only the word itself and a precomputed character\nontology is required, rather than a corpus. The influence of three parameters\nover the algorithm performance is analyzed and verified by experiment.\nExperiment also shows that proposed algorithm achieves an F Measure of 85.02%\noutperforming existing ideogram based algorithm."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.0048v1", 
    "title": "Individual and Domain Adaptation in Sentence Planning for Dialogue", 
    "arxiv-id": "1111.0048v1", 
    "author": "M. A. Walker", 
    "publish": "2011-10-31T21:53:32Z", 
    "summary": "One of the biggest challenges in the development and deployment of spoken\ndialogue systems is the design of the spoken language generation module. This\nchallenge arises from the need for the generator to adapt to many features of\nthe dialogue domain, user population, and dialogue context. A promising\napproach is trainable generation, which uses general-purpose linguistic\nknowledge that is automatically adapted to the features of interest, such as\nthe application domain, individual user, or user group. In this paper we\npresent and evaluate a trainable sentence planner for providing restaurant\ninformation in the MATCH dialogue system. We show that trainable sentence\nplanning can produce complex information presentations whose quality is\ncomparable to the output of a template-based generator tuned to this domain. We\nalso show that our method easily supports adapting the sentence planner to\nindividuals, and that the individualized sentence planners generally perform\nbetter than models trained and tested on a population of individuals. Previous\nwork has documented and utilized individual preferences for content selection,\nbut to our knowledge, these results provide the first demonstration of\nindividual preferences for sentence planning operations, affecting the content\norder, discourse structure and sentence structure of system responses. Finally,\nwe evaluate the contribution of different feature sets, and show that, in our\napplication, n-gram features often do as well as features based on higher-level\nlinguistic representations."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.3122v1", 
    "title": "ESLO: from transcription to speakers' personal information annotation", 
    "arxiv-id": "1111.3122v1", 
    "author": "Nathalie Friburger", 
    "publish": "2011-11-14T07:41:42Z", 
    "summary": "This paper presents the preliminary works to put online a French oral corpus\nand its transcription. This corpus is the Socio-Linguistic Survey in Orleans,\nrealized in 1968. First, we numerized the corpus, then we handwritten\ntranscribed it with the Transcriber software adding different tags about\nspeakers, time, noise, etc. Each document (audio file and XML file of the\ntranscription) was described by a set of metadata stored in an XML format to\nallow an easy consultation. Second, we added different levels of annotations,\nrecognition of named entities and annotation of personal information about\nspeakers. This two annotation tasks used the CasSys system of transducer\ncascades. We used and modified a first cascade to recognize named entities.\nThen we built a second cascade to annote the designating entities, i.e.\ninformation about the speaker. These second cascade parsed the named entity\nannotated corpus. The objective is to locate information about the speaker and,\nalso, what kind of information can designate him/her. These two cascades was\nevaluated with precision and recall measures."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.3152v1", 
    "title": "\u00c9valuation de lexiques syntaxiques par leur int\u00e9gartion dans   l'analyseur syntaxiques FRMG", 
    "arxiv-id": "1111.3152v1", 
    "author": "Sagot Benoit", 
    "publish": "2011-11-14T09:34:34Z", 
    "summary": "In this paper, we evaluate various French lexica with the parser FRMG: the\nLefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,\nthe lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,\nobtained by merging with DICOVALENCE and partial manual validation. For this,\nall these lexica have been converted to the format of the Lefff, Alexina\nformat. The evaluation was made on the part of the EASy corpus used in the\nfirst evaluation campaign Passage."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.3153v1", 
    "title": "Construction du lexique LGLex \u00e0 partir des tables du Lexique-Grammaire   des verbes du grec moderne", 
    "arxiv-id": "1111.3153v1", 
    "author": "Elsa Tolone", 
    "publish": "2011-11-14T09:34:59Z", 
    "summary": "In this paper, we summerize the work done on the resources of Modern Greek on\nthe Lexicon-Grammar of verbs. We detail the definitional features of each\ntable, and all changes made to the names of features to make them consistent.\nThrough the development of the table of classes, including all the features, we\nhave considered the conversion of tables in a syntactic lexicon: LGLex. The\nlexicon, in plain text format or XML, is generated by the LGExtract tool\n(Constant & Tolone, 2010). This format is directly usable in applications of\nNatural Language Processing (NLP)."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.3462v1", 
    "title": "Extending the adverbial coverage of a NLP oriented resource for French", 
    "arxiv-id": "1111.3462v1", 
    "author": "Voyatzi Stavroula", 
    "publish": "2011-11-15T09:24:36Z", 
    "summary": "This paper presents a work on extending the adverbial entries of LGLex: a NLP\noriented syntactic resource for French. Adverbs were extracted from the\nLexicon-Grammar tables of both simple adverbs ending in -ment '-ly' (Molinier\nand Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies\non the exploitation of fine-grained linguistic information provided in existing\nresources. Various features are encoded in both LG tables and they haven't been\nexploited yet. They describe the relations of deleting, permuting, intensifying\nand paraphrasing that associate, on the one hand, the simple and compound\nadverbs and, on the other hand, different types of compound adverbs. The\nresulting syntactic resource is manually evaluated and freely available under\nthe LGPL-LR license."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.4343v1", 
    "title": "Question Answering in a Natural Language Understanding System Based on   Object-Oriented Semantics", 
    "arxiv-id": "1111.4343v1", 
    "author": "Yuriy Ostapov", 
    "publish": "2011-11-18T12:31:49Z", 
    "summary": "Algorithms of question answering in a computer system oriented on input and\nlogical processing of text information are presented. A knowledge domain under\nconsideration is social behavior of a person. A database of the system includes\nan internal representation of natural language sentences and supplemental\ninformation. The answer {\\it Yes} or {\\it No} is formed for a general question.\nA special question containing an interrogative word or group of interrogative\nwords permits to find a subject, object, place, time, cause, purpose and way of\naction or event. Answer generation is based on identification algorithms of\npersons, organizations, machines, things, places, and times. Proposed\nalgorithms of question answering can be realized in information systems closely\nconnected with text processing (criminology, operation of business, medicine,\ndocument systems)."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.5293v1", 
    "title": "Rule based Part of speech Tagger for Homoeopathy Clinical realm", 
    "arxiv-id": "1111.5293v1", 
    "author": "Pramod P. Sukhadeve", 
    "publish": "2011-11-13T18:19:15Z", 
    "summary": "A tagger is a mandatory segment of most text scrutiny systems, as it\nconsigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every\nword in a sentence. In this paper, we present a simple part of speech tagger\nfor homoeopathy clinical language. This paper reports about the anticipated\npart of speech tagger for homoeopathy clinical language. It exploit standard\npattern for evaluating sentences, untagged clinical corpus of 20085 words is\nused, from which we had selected 125 sentences (2322 tokens). The problem of\ntagging in natural language processing is to find a way to tag every word in a\ntext as a meticulous part of speech. The basic idea is to apply a set of rules\non clinical sentences and on each word, Accuracy is the leading factor in\nevaluating any POS tagger so the accuracy of proposed tagger is also conversed."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1111.6553v1", 
    "title": "Exploring Twitter Hashtags", 
    "arxiv-id": "1111.6553v1", 
    "author": "Jan P\u00f6schko", 
    "publish": "2011-11-28T19:17:57Z", 
    "summary": "Twitter messages often contain so-called hashtags to denote keywords related\nto them. Using a dataset of 29 million messages, I explore relations among\nthese hashtags with respect to co-occurrences. Furthermore, I present an\nattempt to classify hashtags into five intuitive classes, using a\nmachine-learning approach. The overall outcome is an interactive Web\napplication to explore Twitter hashtags."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1112.0168v1", 
    "title": "Statistical Sign Language Machine Translation: from English written text   to American Sign Language Gloss", 
    "arxiv-id": "1112.0168v1", 
    "author": "Mohamed Jemni", 
    "publish": "2011-12-01T12:52:22Z", 
    "summary": "This works aims to design a statistical machine translation from English text\nto American Sign Language (ASL). The system is based on Moses tool with some\nmodifications and the results are synthesized through a 3D avatar for\ninterpretation. First, we translate the input text to gloss, a written form of\nASL. Second, we pass the output to the WebSign Plug-in to play the sign.\nContributions of this work are the use of a new couple of language English/ASL\nand an improvement of statistical machine translation based on string matching\nthanks to Jaro-distance."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2329", 
    "link": "http://arxiv.org/pdf/1112.0396v1", 
    "title": "Grammatical Relations of Myanmar Sentences Augmented by   Transformation-Based Learning of Function Tagging", 
    "arxiv-id": "1112.0396v1", 
    "author": "Ni Lar Thein", 
    "publish": "2011-12-02T07:15:49Z", 
    "summary": "In this paper we describe function tagging using Transformation Based\nLearning (TBL) for Myanmar that is a method of extensions to the previous\nstatistics-based function tagger. Contextual and lexical rules (developed using\nTBL) were critical in achieving good results. First, we describe a method for\nexpressing lexical relations in function tagging that statistical function\ntagging are currently unable to express. Function tagging is the preprocessing\nstep to show grammatical relations of the sentences. Then we use the context\nfree grammar technique to clarify the grammatical relations in Myanmar\nsentences or to output the parse trees. The grammatical relations are the\nfunctional structure of a language. They rely very much on the function tag of\nthe tokens. We augment the grammatical relations of Myanmar sentences with\ntransformation-based learning of function tagging."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-012-9197-9", 
    "link": "http://arxiv.org/pdf/1112.2468v1", 
    "title": "Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus", 
    "arxiv-id": "1112.2468v1", 
    "author": "Min-Yen Kan", 
    "publish": "2011-12-12T08:07:49Z", 
    "summary": "Short Message Service (SMS) messages are largely sent directly from one\nperson to another from their mobile phones. They represent a means of personal\ncommunication that is an important communicative artifact in our current\ndigital era. As most existing studies have used private access to SMS corpora,\ncomparative studies using the same raw SMS data has not been possible up to\nnow. We describe our efforts to collect a public SMS corpus to address this\nproblem. We use a battery of methodologies to collect the corpus, paying\nparticular attention to privacy issues to address contributors' concerns. Our\nlive project collects new SMS message submissions, checks their quality and\nadds the valid messages, releasing the resultant corpus as XML and as SQL\ndumps, along with corpus statistics, every month. We opportunistically collect\nas much metadata about the messages and their sender as possible, so as to\nenable different types of analyses. To date, we have collected about 60,000\nmessages, focusing on English and Mandarin Chinese."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-012-9197-9", 
    "link": "http://arxiv.org/pdf/1112.6286v1", 
    "title": "Visualization and Analysis of Frames in Collections of Messages: Content   Analysis and the Measurement of Meaning", 
    "arxiv-id": "1112.6286v1", 
    "author": "Loet Leydesdorff", 
    "publish": "2011-12-29T11:47:05Z", 
    "summary": "A step-to-step introduction is provided on how to generate a semantic map\nfrom a collection of messages (full texts, paragraphs or statements) using\nfreely available software and/or SPSS for the relevant statistics and the\nvisualization. The techniques are discussed in the various theoretical contexts\nof (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and\nsocial systems theory (e.g., the communication of meaning), and (iii)\ncommunication studies (e.g., framing and agenda-setting). We distinguish\nbetween the communication of information in the network space (social network\nanalysis) and the communication of meaning in the vector space. The vector\nspace can be considered a generated as an architecture by the network of\nrelations in the network space; words are then not only related, but also\npositioned. These positions are expected rather than observed and therefore one\ncan communicate meaning. Knowledge can be generated when these meanings can\nrecursively be communicated and therefore also further codified."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-012-9197-9", 
    "link": "http://arxiv.org/pdf/1112.6384v1", 
    "title": "Proof nets for the Lambek-Grishin calculus", 
    "arxiv-id": "1112.6384v1", 
    "author": "Richard Moot", 
    "publish": "2011-12-29T19:16:20Z", 
    "summary": "Grishin's generalization of Lambek's Syntactic Calculus combines a\nnon-commutative multiplicative conjunction and its residuals (product, left and\nright division) with a dual family: multiplicative disjunction, right and left\ndifference. Interaction between these two families takes the form of linear\ndistributivity principles. We study proof nets for the Lambek-Grishin calculus\nand the correspondence between these nets and unfocused and focused versions of\nits sequent calculus."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-012-9197-9", 
    "link": "http://arxiv.org/pdf/1201.1192v1", 
    "title": "Formalization of semantic network of image constructions in electronic   content", 
    "arxiv-id": "1201.1192v1", 
    "author": "Irina Kravchuk", 
    "publish": "2012-01-05T15:22:05Z", 
    "summary": "A formal theory based on a binary operator of directional associative\nrelation is constructed in the article and an understanding of an associative\nnormal form of image constructions is introduced. A model of a commutative\nsemigroup, which provides a presentation of a sentence as three components of\nan interrogative linguistic image construction, is considered."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1201.2010v1", 
    "title": "Recognizing Bangla Grammar using Predictive Parser", 
    "arxiv-id": "1201.2010v1", 
    "author": "Amit Saha", 
    "publish": "2012-01-10T10:33:18Z", 
    "summary": "We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1201.6224v2", 
    "title": "Wikipedia Arborification and Stratified Explicit Semantic Analysis", 
    "arxiv-id": "1201.6224v2", 
    "author": "Vitaly Klyuev", 
    "publish": "2012-01-30T14:26:31Z", 
    "summary": "[This is the translation of paper \"Arborification de Wikip\\'edia et analyse\ns\\'emantique explicite stratifi\\'ee\" submitted to TALN 2012.]\n  We present an extension of the Explicit Semantic Analysis method by\nGabrilovich and Markovitch. Using their semantic relatedness measure, we weight\nthe Wikipedia categories graph. Then, we extract a minimal spanning tree, using\nChu-Liu & Edmonds' algorithm. We define a notion of stratified tfidf where the\nstratas, for a given Wikipedia page and a given term, are the classical tfidf\nand categorical tfidfs of the term in the ancestor categories of the page\n(ancestors in the sense of the minimal spanning tree). Our method is based on\nthis stratified tfidf, which adds extra weight to terms that \"survive\" when\nclimbing up the category tree. We evaluate our method by a text classification\non the WikiNews corpus: it increases precision by 18%. Finally, we provide\nhints for future research"
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1202.0116v1", 
    "title": "Inference and Plausible Reasoning in a Natural Language Understanding   System Based on Object-Oriented Semantics", 
    "arxiv-id": "1202.0116v1", 
    "author": "Yuriy Ostapov", 
    "publish": "2012-02-01T08:36:50Z", 
    "summary": "Algorithms of inference in a computer system oriented to input and semantic\nprocessing of text information are presented. Such inference is necessary for\nlogical questions when the direct comparison of objects from a question and\ndatabase can not give a result. The following classes of problems are\nconsidered: a check of hypotheses for persons and non-typical actions, the\ndetermination of persons and circumstances for non-typical actions, planning\nactions, the determination of event cause and state of persons. To form an\nanswer both deduction and plausible reasoning are used. As a knowledge domain\nunder consideration is social behavior of persons, plausible reasoning is based\non laws of social psychology. Proposed algorithms of inference and plausible\nreasoning can be realized in computer systems closely connected with text\nprocessing (criminology, operation of business, medicine, document systems)."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1202.1054v1", 
    "title": "Considering a resource-light approach to learning verb valencies", 
    "arxiv-id": "1202.1054v1", 
    "author": "Alex Rudnick", 
    "publish": "2012-02-06T06:33:03Z", 
    "summary": "Here we describe work on learning the subcategories of verbs in a\nmorphologically rich language using only minimal linguistic resources. Our goal\nis to learn verb subcategorizations for Quechua, an under-resourced\nmorphologically rich language, from an unannotated corpus. We compare results\nfrom applying this approach to an unannotated Arabic corpus with those achieved\nby processing the same text in treebank form. The original plan was to use only\na morphological analyzer and an unannotated corpus, but experiments suggest\nthat this approach by itself will not be effective for learning the\ncombinatorial potential of Arabic verbs in general. The lower bound on\nresources for acquiring this information is somewhat higher, apparently\nrequiring a a part-of-speech tagger and chunker for most languages, and a\nmorphological disambiguater for Arabic."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1202.1568v2", 
    "title": "Beyond Sentiment: The Manifold of Human Emotions", 
    "arxiv-id": "1202.1568v2", 
    "author": "Irfan Essa", 
    "publish": "2012-02-08T00:49:36Z", 
    "summary": "Sentiment analysis predicts the presence of positive or negative emotions in\na text document. In this paper we consider higher dimensional extensions of the\nsentiment concept, which represent a richer set of human emotions. Our approach\ngoes beyond previous work in that our model contains a continuous manifold\nrather than a finite set of human emotions. We investigate the resulting model,\ncompare it to psychological observations, and explore its predictive\ncapabilities. Besides obtaining significant improvements over a baseline\nwithout manifold, we are also able to visualize different notions of positive\nsentiment in different domains."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1202.6266v1", 
    "title": "Realisation d'un systeme de reconnaissance automatique de la parole   arabe base sur CMU Sphinx", 
    "arxiv-id": "1202.6266v1", 
    "author": "Noureddine Chenfour", 
    "publish": "2012-02-28T16:04:36Z", 
    "summary": "This paper presents the continuation of the work completed by Satori and all.\n[SCH07] by the realization of an automatic speech recognition system (ASR) for\nArabic language based SPHINX 4 system. The previous work was limited to the\nrecognition of the first ten digits, whereas the present work is a remarkable\nprojection consisting in continuous Arabic speech recognition with a rate of\nrecognition of surroundings 96%."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.0145v2", 
    "title": "The Horse Raced Past: Gardenpath Processing in Dynamical Systems", 
    "arxiv-id": "1203.0145v2", 
    "author": "Peter beim Graben", 
    "publish": "2012-03-01T11:06:32Z", 
    "summary": "I pinpoint an interesting similarity between a recent account to rational\nparsing and the treatment of sequential decisions problems in a dynamical\nsystems approach. I argue that expectation-driven search heuristics aiming at\nfast computation resembles a high-risk decision strategy in favor of large\ntransition velocities. Hale's rational parser, combining generalized\nleft-corner parsing with informed $\\mathrm{A}^*$ search to resolve processing\nconflicts, explains gardenpath effects in natural sentence processing by\nmisleading estimates of future processing costs that are to be minimized. On\nthe other hand, minimizing the duration of cognitive computations in\ntime-continuous dynamical systems can be described by combining vector space\nrepresentations of cognitive states by means of filler/role decompositions and\nsubsequent tensor product representations with the paradigm of stable\nheteroclinic sequences. Maximizing transition velocities according to a\nhigh-risk decision strategy could account for a fast race even between states\nthat are apparently remote in representation space."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.1685v1", 
    "title": "Statistical Function Tagging and Grammatical Relations of Myanmar   Sentences", 
    "arxiv-id": "1203.1685v1", 
    "author": "Ni Lar Thein", 
    "publish": "2012-03-08T03:06:29Z", 
    "summary": "This paper describes a context free grammar (CFG) based grammatical relations\nfor Myanmar sentences which combine corpus-based function tagging system. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step to show grammatical relations\nof Myanmar sentences. In the task of function tagging, which tags the function\nof Myanmar sentences with correct segmentation, POS (part-of-speech) tagging\nand chunking information, we use Naive Bayesian theory to disambiguate the\npossible function tags of a word. We apply context free grammar (CFG) to find\nout the grammatical relations of the function tags. We also create a functional\nannotated tagged corpus for Myanmar and propose the grammar rules for Myanmar\nsentences. Experiments show that our analysis achieves a good result with\nsimple sentences and complex sentences."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.1858v1", 
    "title": "Distributional Measures of Semantic Distance: A Survey", 
    "arxiv-id": "1203.1858v1", 
    "author": "Graeme Hirst", 
    "publish": "2012-03-08T17:29:33Z", 
    "summary": "The ability to mimic human notions of semantic distance has widespread\napplications. Some measures rely only on raw text (distributional measures) and\nsome rely on knowledge sources such as WordNet. Although extensive studies have\nbeen performed to compare WordNet-based measures with human judgment, the use\nof distributional measures as proxies to estimate semantic distance has\nreceived little attention. Even though they have traditionally performed poorly\nwhen compared to WordNet-based measures, they lay claim to certain uniquely\nattractive features, such as their applicability in resource-poor languages and\ntheir ability to mimic both semantic similarity and semantic relatedness.\nTherefore, this paper presents a detailed study of distributional measures.\nParticular attention is paid to flesh out the strengths and limitations of both\nWordNet-based and distributional measures, and how distributional measures of\ndistance can be brought more in line with human notions of semantic distance.\nWe conclude with a brief discussion of recent work on hybrid measures."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.1889v1", 
    "title": "Distributional Measures as Proxies for Semantic Relatedness", 
    "arxiv-id": "1203.1889v1", 
    "author": "Graeme Hirst", 
    "publish": "2012-03-08T19:03:37Z", 
    "summary": "The automatic ranking of word pairs as per their semantic relatedness and\nability to mimic human notions of semantic relatedness has widespread\napplications. Measures that rely on raw data (distributional measures) and\nthose that use knowledge-rich ontologies both exist. Although extensive studies\nhave been performed to compare ontological measures with human judgment, the\ndistributional measures have primarily been evaluated by indirect means. This\npaper is a detailed study of some of the major distributional measures; it\nlists their respective merits and limitations. New measures that overcome these\ndrawbacks, that are more in line with the human notions of semantic\nrelatedness, are suggested. The paper concludes with an exhaustive comparison\nof the distributional and ontology-based measures. Along the way, significant\nresearch problems are identified. Work on these problems may lead to a better\nunderstanding of how semantic relatedness is to be measured."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.2498v2", 
    "title": "Fault detection system for Arabic language", 
    "arxiv-id": "1203.2498v2", 
    "author": "Houda Amraoui", 
    "publish": "2012-03-08T13:28:03Z", 
    "summary": "The study of natural language, especially Arabic, and mechanisms for the\nimplementation of automatic processing is a fascinating field of study, with\nvarious potential applications. The importance of tools for natural language\nprocessing is materialized by the need to have applications that can\neffectively treat the vast mass of information available nowadays on electronic\nforms. Among these tools, mainly driven by the necessity of a fast writing in\nalignment to the actual daily life speed, our interest is on the writing\nauditors. The morphological and syntactic properties of Arabic make it a\ndifficult language to master, and explain the lack in the processing tools for\nthat language. Among these properties, we can mention: the complex structure of\nthe Arabic word, the agglutinative nature, lack of vocalization, the\nsegmentation of the text, the linguistic richness, etc."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.3023v1", 
    "title": "Toward an example-based machine translation from written text to ASL   using virtual agent animation", 
    "arxiv-id": "1203.3023v1", 
    "author": "Mohamed Jemni", 
    "publish": "2012-03-14T08:51:14Z", 
    "summary": "Modern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth information\nmeaning. Our goals are: to translate written text from any language to ASL\nanimation; to model maximum raw information using machine learning and\ncomputational techniques; and to produce a more adapted and expressive form to\nnatural looking and understandable ASL animations. Our methods include\nlinguistic annotation of initial text and semantic orientation to generate the\nfacial expression. We use the genetic algorithms coupled to learning/recognized\nsystems to produce the most natural form. To detect emotion we are based on\nfuzzy logic to produce the degree of interpolation between facial expressions.\nRoughly, we present a new expressive language Text Adapted Sign Modeling\nLanguage TASML that describes all maximum aspects related to a natural sign\nlanguage interpretation. This paper is organized as follow: the next section is\ndevoted to present the comprehension effect of using Space/Time/SVO form in ASL\nanimation based on experimentation. In section 3, we describe our technical\nconsiderations. We present the general approach we adopted to develop our tool\nin section 4. Finally, we give some perspectives and future works."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.3584v1", 
    "title": "An Accurate Arabic Root-Based Lemmatizer for Information Retrieval   Purposes", 
    "arxiv-id": "1203.3584v1", 
    "author": "Fatma El-Ghannam", 
    "publish": "2012-03-15T22:49:20Z", 
    "summary": "In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma\nlevel analysis and generation does not yet focused in Arabic NLP literatures.\nIn the current research, we propose the first non-statistical accurate Arabic\nlemmatizer algorithm that is suitable for information retrieval (IR) systems.\nThe proposed lemmatizer makes use of different Arabic language knowledge\nresources to generate accurate lemma form and its relevant features that\nsupport IR purposes. As a POS tagger, the experimental results show that, the\nproposed algorithm achieves a maximum accuracy of 94.8%. For first seen\ndocuments, an accuracy of 89.15% is achieved, compared to 76.7% of up to date\nStanford accurate Arabic model, for the same, dataset."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2011.3605", 
    "link": "http://arxiv.org/pdf/1203.4605v1", 
    "title": "Arabic Keyphrase Extraction using Linguistic knowledge and Machine   Learning Techniques", 
    "arxiv-id": "1203.4605v1", 
    "author": "Abdulwahab Al-sammak", 
    "publish": "2012-03-20T21:52:35Z", 
    "summary": "In this paper, a supervised learning technique for extracting keyphrases of\nArabic documents is presented. The extractor is supplied with linguistic\nknowledge to enhance its efficiency instead of relying only on statistical\ninformation such as term frequency and distance. During analysis, an annotated\nArabic corpus is used to extract the required lexical features of the document\nwords. The knowledge also includes syntactic rules based on part of speech tags\nand allowed word sequences to extract the candidate keyphrases. In this work,\nthe abstract form of Arabic words is used instead of its stem form to represent\nthe candidate terms. The Abstract form hides most of the inflections found in\nArabic words. The paper introduces new features of keyphrases based on\nlinguistic knowledge, to capture titles and subtitles of a document. A simple\nANOVA test is used to evaluate the validity of selected features. Then, the\nlearning model is built using the LDA - Linear Discriminant Analysis - and\ntraining documents. Although, the presented system is trained using documents\nin the IT domain, experiments carried out show that it has a significantly\nbetter performance than the existing Arabic extractor systems, where precision\nand recall values reach double their corresponding values in the other systems\nespecially for lengthy and non-scientific articles."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.4933v1", 
    "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS   Tagger", 
    "arxiv-id": "1203.4933v1", 
    "author": "Sivaji Bandyopadhyay", 
    "publish": "2012-03-22T09:50:51Z", 
    "summary": "This paper gives a detail overview about the modified features selection in\nCRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.\nSelection of features is so important in CRF that the better are the features\nthen the better are the outputs. This work is an attempt or an experiment to\nmake the previous work more efficient. Multiple new features are tried to run\nthe CRF and again tried with the Reduplicated Multiword Expression (RMWE) as\nanother feature. The CRF run with RMWE because Manipuri is rich of RMWE and\nidentification of RMWE becomes one of the necessities to bring up the result of\nPOS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%\nand F-measure of 75.60%. With the identification of RMWE and considering it as\na feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and\nF-measure of 77.14%."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5051v1", 
    "title": "Analysing Temporally Annotated Corpora with CAVaT", 
    "arxiv-id": "1203.5051v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T17:45:39Z", 
    "summary": "We present CAVaT, a tool that performs Corpus Analysis and Validation for\nTimeML. CAVaT is an open source, modular checking utility for statistical\nanalysis of features specific to temporally-annotated natural language corpora.\nIt provides reporting, highlights salient links between a variety of general\nand time-specific linguistic features, and also validates a temporal annotation\nto ensure that it is logically consistent and sufficiently annotated. Uniquely,\nCAVaT provides analysis specific to TimeML-annotated temporal information.\nTimeML is a standard for annotating temporal information in natural language\ntext. In this paper, we present the reporting part of CAVaT, and then its\nerror-checking ability, including the workings of several novel TimeML document\nverification methods. This is followed by the execution of some example tasks\nusing the tool to show relations between times, events, signals and links. We\nalso demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been\ndetected with CAVaT."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5055v1", 
    "title": "Using Signals to Improve Automatic Classification of Temporal Relations", 
    "arxiv-id": "1203.5055v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T17:50:08Z", 
    "summary": "Temporal information conveyed by language describes how the world around us\nchanges through time. Events, durations and times are all temporal elements\nthat can be viewed as intervals. These intervals are sometimes temporally\nrelated in text. Automatically determining the nature of such relations is a\ncomplex and unsolved problem. Some words can act as \"signals\" which suggest a\ntemporal ordering between intervals. In this paper, we use these signal words\nto improve the accuracy of a recent approach to classification of temporal\nlinks."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5060v1", 
    "title": "USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2", 
    "arxiv-id": "1203.5060v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T17:59:22Z", 
    "summary": "We describe the University of Sheffield system used in the TempEval-2\nchallenge, USFD2. The challenge requires the automatic identification of\ntemporal entities and relations in text. USFD2 identifies and anchors temporal\nexpressions, and also attempts two of the four temporal relation assignment\ntasks. A rule-based system picks out and anchors temporal expressions, and a\nmaximum entropy classifier assigns temporal link labels, based on features that\ninclude descriptions of associated temporal signal words. USFD2 identified\ntemporal expressions successfully, and correctly classified their type in 90%\nof cases. Determining the relation between an event and time expression in the\nsame sentence was performed at 63% accuracy, the second highest score in this\npart of the challenge."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5062v1", 
    "title": "An Annotation Scheme for Reichenbach's Verbal Tense Structure", 
    "arxiv-id": "1203.5062v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T18:05:26Z", 
    "summary": "In this paper we present RTMML, a markup language for the tenses of verbs and\ntemporal relations between verbs. There is a richness to tense in language that\nis not fully captured by existing temporal annotation schemata. Following\nReichenbach we present an analysis of tense in terms of abstract time points,\nwith the aim of supporting automated processing of tense and temporal relations\nin language. This allows for precise reasoning about tense in documents, and\nthe deduction of temporal relations between the times and verbal events in a\ndiscourse. We define the syntax of RTMML, and demonstrate the markup in a range\nof situations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5066v1", 
    "title": "A Corpus-based Study of Temporal Signals", 
    "arxiv-id": "1203.5066v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T18:08:47Z", 
    "summary": "Automatic temporal ordering of events described in discourse has been of\ngreat interest in recent years. Event orderings are conveyed in text via va\nrious linguistic mechanisms including the use of expressions such as \"before\",\n\"after\" or \"during\" that explicitly assert a temporal relation -- temporal\nsignals. In this paper, we investigate the role of temporal signals in temporal\nrelation extraction and provide a quantitative analysis of these expres sions\nin the TimeBank annotated corpus."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5073v1", 
    "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", 
    "arxiv-id": "1203.5073v1", 
    "author": "Robert Gaizauskas", 
    "publish": "2012-03-22T18:34:19Z", 
    "summary": "This paper describes the University of Sheffield's entry in the 2011 TAC KBP\nentity linking and slot filling tasks. We chose to participate in the\nmonolingual entity linking task, the monolingual slot filling task and the\ntemporal slot filling tasks. We set out to build a framework for\nexperimentation with knowledge base population. This framework was created, and\napplied to multiple KBP tasks. We demonstrated that our proposed framework is\neffective and suitable for collaborative development efforts, as well as useful\nin a teaching environment. Finally we present results that, while very modest,\nprovide improvements an order of magnitude greater than our 2010 attempt."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5076v1", 
    "title": "Massively Increasing TIMEX3 Resources: A Transduction Approach", 
    "arxiv-id": "1203.5076v1", 
    "author": "Estela Saquete", 
    "publish": "2012-03-22T18:45:07Z", 
    "summary": "Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. Gold standard\ntemporally-annotated resources are limited in size, which makes research using\nthem difficult. Standards have also evolved over the past decade, so not all\ntemporally annotated data is in the same format. We vastly increase available\nhuman-annotated temporal expression resources by converting older format\nresources to TimeML/TIMEX3. This task is difficult due to differing annotation\nmethods. We present a robust conversion tool and a new, large temporal\nexpression resource. Using this, we evaluate our conversion process by using it\nas training data for an existing TimeML annotation tool, achieving a 0.87 F1\nmeasure -- better than any system in the TempEval-2 timex recognition exercise."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5255v1", 
    "title": "Post-Editing Error Correction Algorithm for Speech Recognition using   Bing Spelling Suggestion", 
    "arxiv-id": "1203.5255v1", 
    "author": "Mohammad Alwani", 
    "publish": "2012-03-23T14:32:50Z", 
    "summary": "ASR short for Automatic Speech Recognition is the process of converting a\nspoken speech into text that can be manipulated by a computer. Although ASR has\nseveral applications, it is still erroneous and imprecise especially if used in\na harsh surrounding wherein the input speech is of low quality. This paper\nproposes a post-editing ASR error correction method and algorithm based on\nBing's online spelling suggestion. In this approach, the ASR recognized output\ntext is spell-checked using Bing's spelling suggestion technology to detect and\ncorrect misrecognized words. More specifically, the proposed algorithm breaks\ndown the ASR output text into several word-tokens that are submitted as search\nqueries to Bing search engine. A returned spelling suggestion implies that a\nquery is misspelled; and thus it is replaced by the suggested correction;\notherwise, no correction is performed and the algorithm continues with the next\ntoken until all tokens get validated. Experiments carried out on various\nspeeches in different languages indicated a successful decrease in the number\nof ASR errors and an improvement in the overall error correction rate. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor computers."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.5262v1", 
    "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", 
    "arxiv-id": "1203.5262v1", 
    "author": "Paul Semaan", 
    "publish": "2012-03-23T14:51:05Z", 
    "summary": "At the present time, computers are employed to solve complex tasks and\nproblems ranging from simple calculations to intensive digital image processing\nand intricate algorithmic optimization problems to computationally-demanding\nweather forecasting problems. ASR short for Automatic Speech Recognition is yet\nanother type of computational problem whose purpose is to recognize human\nspoken speech and convert it into text that can be processed by a computer.\nDespite that ASR has many versatile and pervasive real-world applications,it is\nstill relatively erroneous and not perfectly solved as it is prone to produce\nspelling errors in the recognized text, especially if the ASR system is\noperating in a noisy environment, its vocabulary size is limited, and its input\nspeech is of bad or low quality. This paper proposes a post-editing ASR error\ncorrection method based on MicrosoftN-Gram dataset for detecting and correcting\nspelling errors generated by ASR systems. The proposed method comprises an\nerror detection algorithm for detecting word errors; a candidate corrections\ngeneration algorithm for generating correction suggestions for the detected\nword errors; and a context-sensitive error correction algorithm for selecting\nthe best candidate for correction. The virtue of using the Microsoft N-Gram\ndataset is that it contains real-world data and word sequences extracted from\nthe web which canmimica comprehensive dictionary of words having a large and\nall-inclusive vocabulary. Experiments conducted on numerous speeches, performed\nby different speakers, showed a remarkable reduction in ASR errors. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor and distributed systems."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1203.6136v1", 
    "title": "Tree Transducers, Machine Translation, and Cross-Language Divergences", 
    "arxiv-id": "1203.6136v1", 
    "author": "Alex Rudnick", 
    "publish": "2012-03-28T02:13:39Z", 
    "summary": "Tree transducers are formal automata that transform trees into other trees.\nMany varieties of tree transducers have been explored in the automata theory\nliterature, and more recently, in the machine translation literature. In this\npaper I review T and xT transducers, situate them among related formalisms, and\nshow how they can be used to implement rules for machine translation systems\nthat cover all of the cross-language structural divergences described in Bonnie\nDorr's influential article on the topic. I also present an implementation of xT\ntransduction, suitable and convenient for experimenting with translation rules."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.0627v1", 
    "title": "Rule-weighted and terminal-weighted context-free grammars have identical   expressivity", 
    "arxiv-id": "1205.0627v1", 
    "author": "Yann Ponty", 
    "publish": "2012-05-03T06:49:59Z", 
    "summary": "Two formalisms, both based on context-free grammars, have recently been\nproposed as a basis for a non-uniform random generation of combinatorial\nobjects. The former, introduced by Denise et al, associates weights with\nletters, while the latter, recently explored by Weinberg et al in the context\nof random generation, associates weights to transitions. In this short note, we\nuse a simple modification of the Greibach Normal Form transformation algorithm,\ndue to Blum and Koch, to show the equivalent expressivities, in term of their\ninduced distributions, of these two formalisms."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.1603v1", 
    "title": "Parsing of Myanmar sentences with function tagging", 
    "arxiv-id": "1205.1603v1", 
    "author": "Ni Lar Thein", 
    "publish": "2012-05-08T07:01:40Z", 
    "summary": "This paper describes the use of Naive Bayes to address the task of assigning\nfunction tags and context free grammar (CFG) to parse Myanmar sentences. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step for parsing. In the task of\nfunction tagging, we use the functional annotated corpus and tag Myanmar\nsentences with correct segmentation, POS (part-of-speech) tagging and chunking\ninformation. We propose Myanmar grammar rules and apply context free grammar\n(CFG) to find out the parse tree of function tagged Myanmar sentences.\nExperiments show that our analysis achieves a good result with parsing of\nsimple sentences and three types of complex sentences."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.3183v1", 
    "title": "A Model-Driven Probabilistic Parser Generator", 
    "arxiv-id": "1205.3183v1", 
    "author": "Francisco J. Cortijo", 
    "publish": "2012-05-14T20:12:06Z", 
    "summary": "Existing probabilistic scanners and parsers impose hard constraints on the\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\ncontext into account. In this paper, we propose a model-driven tool that allows\nfor statistical language models with arbitrary probability estimators. Our work\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\nparser generator, and enables the probabilistic interpretation and resolution\nof anaphoric, cataphoric, and recursive references in the disambiguation of\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\ndescribe the design of a general-purpose natural language parser."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.3316v1", 
    "title": "Arabic Language Learning Assisted by Computer, based on Automatic Speech   Recognition", 
    "arxiv-id": "1205.3316v1", 
    "author": "Mounir Zrigui", 
    "publish": "2012-05-15T10:34:05Z", 
    "summary": "This work consists of creating a system of the Computer Assisted Language\nLearning (CALL) based on a system of Automatic Speech Recognition (ASR) for the\nArabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.\nTo this work, we have constructed a corpus of six hours of speech recordings\nwith a number of nine speakers. we find in the robustness to noise a grounds\nfor the choice of the HMM approach [2]. the results achieved are encouraging\nsince our corpus is made by only nine speakers, but they are always reasons\nthat open the door for other improvement works."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.4298v1", 
    "title": "Task-specific Word-Clustering for Part-of-Speech Tagging", 
    "arxiv-id": "1205.4298v1", 
    "author": "Yoav Goldberg", 
    "publish": "2012-05-19T05:04:31Z", 
    "summary": "While the use of cluster features became ubiquitous in core NLP tasks, most\ncluster features in NLP are based on distributional similarity. We propose a\nnew type of clustering criteria, specific to the task of part-of-speech\ntagging. Instead of distributional similarity, these clusters are based on the\nbeha vior of a baseline tagger when applied to a large corpus. These cluster\nfeatures provide similar gains in accuracy to those achieved by\ndistributional-similarity derived clusters. Using both types of cluster\nfeatures together further improve tagging accuracies. We show that the method\nis effective for both the in-domain and out-of-domain scenarios for English,\nand for French, German and Italian. The effect is larger for out-of-domain\ntext."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijitcs.2012.210", 
    "link": "http://arxiv.org/pdf/1205.4387v1", 
    "title": "Precision-biased Parsing and High-Quality Parse Selection", 
    "arxiv-id": "1205.4387v1", 
    "author": "Michael Elhadad", 
    "publish": "2012-05-20T06:36:19Z", 
    "summary": "We introduce precision-biased parsing: a parsing task which favors precision\nover recall by allowing the parser to abstain from decisions deemed uncertain.\nWe focus on dependency-parsing and present an ensemble method which is capable\nof assigning parents to 84% of the text tokens while being over 96% accurate on\nthese tokens. We use the precision-biased parsing task to solve the related\nhigh-quality parse-selection task: finding a subset of high-quality (accurate)\ntrees in a large collection of parsed text. We present a method for choosing\nover a third of the input trees while keeping unlabeled dependency parsing\naccuracy of 97% on these trees. We also present a method which is not based on\nan ensemble but rather on directly predicting the risk associated with\nindividual parser decisions. In addition to its efficiency, this method\ndemonstrates that a parsing system can provide reasonable estimates of\nconfidence in its predictions without relying on ensembles or aggregate corpus\ncounts."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1205.5407v2", 
    "title": "FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely   Lexical Substitutes Based on an N-gram Language Model", 
    "arxiv-id": "1205.5407v2", 
    "author": "Deniz Yuret", 
    "publish": "2012-05-24T11:53:41Z", 
    "summary": "Lexical substitutes have found use in areas such as paraphrasing, text\nsimplification, machine translation, word sense disambiguation, and part of\nspeech induction. However the computational complexity of accurately\nidentifying the most likely substitutes for a word has made large scale\nexperiments difficult. In this paper I introduce a new search algorithm,\nFASTSUBS, that is guaranteed to find the K most likely lexical substitutes for\na given word in a sentence based on an n-gram language model. The computation\nis sub-linear in both K and the vocabulary size V. An implementation of the\nalgorithm and a dataset with the top 100 substitutes of each token in the WSJ\nsection of the Penn Treebank are available at http://goo.gl/jzKH0."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1205.6832v1", 
    "title": "Syst\u00e8me d'aide \u00e0 l'acc\u00e8s lexical : trouver le mot qu'on a sur le   bout de la langue", 
    "arxiv-id": "1205.6832v1", 
    "author": "Michael Zock", 
    "publish": "2012-01-20T17:33:47Z", 
    "summary": "The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues\nand insights concerning the organisation of the mental lexicon (meaning, number\nof syllables, relation with other words, etc.). This paper describes a tool\nbased on psycho-linguistic observations concerning the TOT phenomenon. We've\nbuilt it to enable a speaker/writer to find the word he is looking for, word he\nmay know, but which he is unable to access in time. We try to simulate the TOT\nphenomenon by creating a situation where the system knows the target word, yet\nis unable to access it. In order to find the target word we make use of the\nparadigmatic and syntagmatic associations stored in the linguistic databases.\nOur experiment allows the following conclusion: a tool like SVETLAN, capable to\nstructure (automatically) a dictionary by domains can be used sucessfully to\nhelp the speaker/writer to find the word he is looking for, if it is combined\nwith a database rich in terms of paradigmatic links like EuroWordNet."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.0042v1", 
    "title": "Language Acquisition in Computers", 
    "arxiv-id": "1206.0042v1", 
    "author": "Jorge H. Roman", 
    "publish": "2012-05-31T22:02:32Z", 
    "summary": "This project explores the nature of language acquisition in computers, guided\nby techniques similar to those used in children. While existing natural\nlanguage processing methods are limited in scope and understanding, our system\naims to gain an understanding of language from first principles and hence\nminimal initial input. The first portion of our system was implemented in Java\nand is focused on understanding the morphology of language using bigrams. We\nuse frequency distributions and differences between them to define and\ndistinguish languages. English and French texts were analyzed to determine a\ndifference threshold of 55 before the texts are considered to be in different\nlanguages, and this threshold was verified using Spanish texts. The second\nportion of our system focuses on gaining an understanding of the syntax of a\nlanguage using a recursive method. The program uses one of two possible methods\nto analyze given sentences based on either sentence patterns or surrounding\nwords. Both methods have been implemented in C++. The program is able to\nunderstand the structure of simple sentences and learn new words. In addition,\nwe have provided some suggestions regarding future work and potential\nextensions of the existing program."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.0381v1", 
    "title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser   Approach", 
    "arxiv-id": "1206.0381v1", 
    "author": "Shaikh Muhammad Allayear", 
    "publish": "2012-06-02T13:23:18Z", 
    "summary": "Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.1066v1", 
    "title": "Hedge detection as a lens on framing in the GMO debates: A position   paper", 
    "arxiv-id": "1206.1066v1", 
    "author": "Jennifer Spindel", 
    "publish": "2012-06-05T20:15:35Z", 
    "summary": "Understanding the ways in which participants in public discussions frame\ntheir arguments is important in understanding how public opinion is formed. In\nthis paper, we adopt the position that it is time for more\ncomputationally-oriented research on problems involving framing. In the\ninterests of furthering that goal, we propose the following specific,\ninteresting and, we believe, relatively accessible question: In the controversy\nregarding the use of genetically-modified organisms (GMOs) in agriculture, do\npro- and anti-GMO articles differ in whether they choose to adopt a\n\"scientific\" tone?\n  Prior work on the rhetoric and sociology of science suggests that hedging may\ndistinguish popular-science text from text written by professional scientists\nfor their colleagues. We propose a detailed approach to studying whether hedge\ndetection can be used to understanding scientific framing in the GMO debates,\nand provide corpora to facilitate this study. Some of our preliminary analyses\nsuggest that hedges occur less frequently in scientific discourse than in\npopular text, a finding that contradicts prior assertions in the literature. We\nhope that our initial work and data will encourage others to pursue this\npromising line of inquiry."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.2009v1", 
    "title": "Developing a model for a text database indexed pedagogically for   teaching the Arabic language", 
    "arxiv-id": "1206.2009v1", 
    "author": "Mounir Zrigui", 
    "publish": "2012-06-10T09:27:51Z", 
    "summary": "In this memory we made the design of an indexing model for Arabic language\nand adapting standards for describing learning resources used (the LOM and\ntheir application profiles) with learning conditions such as levels education\nof students, their levels of understanding...the pedagogical context with\ntaking into account the repre-sentative elements of the text, text's\nlength,...in particular, we highlight the specificity of the Arabic language\nwhich is a complex language, characterized by its flexion, its voyellation and\nits agglutination."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.4522v1", 
    "title": "BADREX: In situ expansion and coreference of biomedical abbreviations   using dynamic regular expressions", 
    "arxiv-id": "1206.4522v1", 
    "author": "Phil Gooch", 
    "publish": "2012-06-20T15:06:48Z", 
    "summary": "BADREX uses dynamically generated regular expressions to annotate term\ndefinition-term abbreviation pairs, and corefers unpaired acronyms and\nabbreviations back to their initial definition in the text. Against the\nMedstract corpus BADREX achieves precision and recall of 98% and 97%, and\nagainst a much larger corpus, 90% and 85%, respectively. BADREX yields improved\nperformance over previous approaches, requires no training data and allows\nruntime customisation of its input parameters. BADREX is freely available from\nhttps://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a\nplugin for the General Architecture for Text Engineering (GATE) framework and\nis licensed under the GPLv3."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1206.5333v2", 
    "title": "TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations", 
    "arxiv-id": "1206.5333v2", 
    "author": "James Pustejovsky", 
    "publish": "2012-06-22T22:30:44Z", 
    "summary": "We describe the TempEval-3 task which is currently in preparation for the\nSemEval-2013 evaluation exercise. The aim of TempEval is to advance research on\ntemporal information processing. TempEval-3 follows on from previous TempEval\nevents, incorporating: a three-part task structure covering event, temporal\nexpression and temporal relation extraction; a larger dataset; and single\noverall task quality scores."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.0200v1", 
    "title": "Adaptation of pedagogical resources description standard (LOM) with the   specificity of Arabic language", 
    "arxiv-id": "1208.0200v1", 
    "author": "Mounir Zrigui", 
    "publish": "2012-08-01T13:06:54Z", 
    "summary": "In this article we focus firstly on the principle of pedagogical indexing and\ncharacteristics of Arabic language and secondly on the possibility of adapting\nthe standard for describing learning resources used (the LOM and its\nApplication Profiles) with learning conditions such as the educational levels\nof students and their levels of understanding,... the educational context with\ntaking into account the representative elements of text, text length, ... in\nparticular, we put in relief the specificity of the Arabic language which is a\ncomplex language, characterized by its flexion, its voyellation and\nagglutination."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.2777v1", 
    "title": "A Method for Selecting Noun Sense using Co-occurrence Relation in   English-Korean Translation", 
    "arxiv-id": "1208.2777v1", 
    "author": "Changil Choe", 
    "publish": "2012-08-14T03:25:33Z", 
    "summary": "The sense analysis is still critical problem in machine translation system,\nespecially such as English-Korean translation which the syntactical different\nbetween source and target languages is very great. We suggest a method for\nselecting the noun sense using contextual feature in English-Korean\nTranslation."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.3001v1", 
    "title": "More than Word Frequencies: Authorship Attribution via Natural Frequency   Zoned Word Distribution Analysis", 
    "arxiv-id": "1208.3001v1", 
    "author": "Haibo Miao", 
    "publish": "2012-08-15T00:53:39Z", 
    "summary": "With such increasing popularity and availability of digital text data,\nauthorships of digital texts can not be taken for granted due to the ease of\ncopying and parsing. This paper presents a new text style analysis called\nnatural frequency zoned word distribution analysis (NFZ-WDA), and then a basic\nauthorship attribution scheme and an open authorship attribution scheme for\ndigital texts based on the analysis. NFZ-WDA is based on the observation that\nall authors leave distinct intrinsic word usage traces on texts written by them\nand these intrinsic styles can be identified and employed to analyze the\nauthorship. The intrinsic word usage styles can be estimated through the\nanalysis of word distribution within a text, which is more than normal word\nfrequency analysis and can be expressed as: which groups of words are used in\nthe text; how frequently does each group of words occur; how are the\noccurrences of each group of words distributed in the text. Next, the basic\nauthorship attribution scheme and the open authorship attribution scheme\nprovide solutions for both closed and open authorship attribution problems.\nThrough analysis and extensive experimental studies, this paper demonstrates\nthe efficiency of the proposed method for authorship attribution."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.4079v1", 
    "title": "Recent Technological Advances in Natural Language Processing and   Artificial Intelligence", 
    "arxiv-id": "1208.4079v1", 
    "author": "Nishal Pradeepkumar Shah", 
    "publish": "2012-08-20T18:34:27Z", 
    "summary": "A recent advance in computer technology has permitted scientists to implement\nand test algorithms that were known from quite some time (or not) but which\nwere computationally expensive. Two such projects are IBM's Jeopardy as a part\nof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods\nimplement natural language processing (another goal of AI scientists) and try\nto answer questions as asked by the user. Though the goal of the two projects\nis similar, both of them have a different procedure at it's core. In the\nfollowing sections, the mechanism and history of IBM's Jeopardy and Wolfram\nalpha has been explained followed by the implications of these projects in\nrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe\nof taking the above projects to a new level is also explained."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.4503v1", 
    "title": "Introduction of the weight edition errors in the Levenshtein distance", 
    "arxiv-id": "1208.4503v1", 
    "author": "Gueddah Hicham", 
    "publish": "2012-08-22T14:11:07Z", 
    "summary": "In this paper, we present a new approach dedicated to correcting the spelling\nerrors of the Arabic language. This approach corrects typographical errors like\ninserting, deleting, and permutation. Our method is inspired from the\nLevenshtein algorithm, and allows a finer and better scheduling than\nLevenshtein. The results obtained are very satisfactory and encouraging, which\nshows the interest of our new approach."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1208.6109v1", 
    "title": "Average word length dynamics as indicator of cultural changes in society", 
    "arxiv-id": "1208.6109v1", 
    "author": "Valery D. Solovyev", 
    "publish": "2012-08-30T08:30:32Z", 
    "summary": "Dynamics of average length of words in Russian and English is analysed in the\narticle. Words belonging to the diachronic text corpus Google Books Ngram and\ndated back to the last two centuries are studied. It was found out that average\nword length slightly increased in the 19th century, and then it was growing\nrapidly most of the 20th century and started decreasing over the period from\nthe end of the 20th - to the beginning of the 21th century. Words which\ncontributed mostly to increase or decrease of word average length were\nidentified. At that, content words and functional words are analysed\nseparately. Long content words contribute mostly to word average length of\nword. As it was shown, these words reflect the main tendencies of social\ndevelopment and thus, are used frequently. Change of frequency of personal\npronouns also contributes significantly to change of average word length. The\nother parameters connected with average length of word were also analysed."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1209.1300v1", 
    "title": "Input Scheme for Hindi Using Phonetic Mapping", 
    "arxiv-id": "1209.1300v1", 
    "author": "Iti Mathur", 
    "publish": "2012-08-19T02:38:12Z", 
    "summary": "Written Communication on Computers requires knowledge of writing text for the\ndesired language using Computer. Mostly people do not use any other language\nbesides English. This creates a barrier. To resolve this issue we have\ndeveloped a scheme to input text in Hindi using phonetic mapping scheme. Using\nthis scheme we generate intermediate code strings and match them with\npronunciations of input text. Our system show significant success over other\ninput systems available."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1209.1301v1", 
    "title": "Evaluation of Computational Grammar Formalisms for Indian Languages", 
    "arxiv-id": "1209.1301v1", 
    "author": "Iti Mathur", 
    "publish": "2012-08-19T02:31:29Z", 
    "summary": "Natural Language Parsing has been the most prominent research area since the\ngenesis of Natural Language Processing. Probabilistic Parsers are being\ndeveloped to make the process of parser development much easier, accurate and\nfast. In Indian context, identification of which Computational Grammar\nFormalism is to be used is still a question which needs to be answered. In this\npaper we focus on this problem and try to analyze different formalisms for\nIndian languages."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1209.2400v1", 
    "title": "Identification of Fertile Translations in Medical Comparable Corpora: a   Morpho-Compositional Approach", 
    "arxiv-id": "1209.2400v1", 
    "author": "Claire Lemaire", 
    "publish": "2012-09-11T19:18:26Z", 
    "summary": "This paper defines a method for lexicon in the biomedical domain from\ncomparable corpora. The method is based on compositional translation and\nexploits morpheme-level translation equivalences. It can generate translations\nfor a large variety of morphologically constructed words and can also generate\n'fertile' translations. We show that fertile translations increase the overall\nquality of the extracted lexicon for English to French translation."
},{
    "category": "cs.CL", 
    "doi": "10.1109/LSP.2012.2215587", 
    "link": "http://arxiv.org/pdf/1209.6238v1", 
    "title": "Natural Language Processing - A Survey", 
    "arxiv-id": "1209.6238v1", 
    "author": "Kevin Mote", 
    "publish": "2012-09-25T21:05:08Z", 
    "summary": "The utility and power of Natural Language Processing (NLP) seems destined to\nchange our technological society in profound and fundamental ways. However\nthere are, to date, few accessible descriptions of the science of NLP that have\nbeen written for a popular audience, or even for an audience of intelligent,\nbut uninitiated scientists. This paper aims to provide just such an overview.\nIn short, the objective of this article is to describe the purpose, procedures\nand practical applications of NLP in a clear, balanced, and readable way. We\nwill examine the most recent literature describing the methods and processes of\nNLP, analyze some of the challenges that researchers are faced with, and\nbriefly survey some of the current and future applications of this science to\nIT research in general."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.4567v2", 
    "title": "Gender identity and lexical variation in social media", 
    "arxiv-id": "1210.4567v2", 
    "author": "Tyler Schnoebelen", 
    "publish": "2012-10-16T20:22:56Z", 
    "summary": "We present a study of the relationship between gender, linguistic style, and\nsocial networks, using a novel corpus of 14,000 Twitter users. Prior\nquantitative work on gender often treats this social variable as a female/male\nbinary; we argue for a more nuanced approach. By clustering Twitter users, we\nfind a natural decomposition of the dataset into various styles and topical\ninterests. Many clusters have strong gender orientations, but their use of\nlinguistic resources sometimes directly conflicts with the population-level\nlanguage statistics. We view these clusters as a more accurate reflection of\nthe multifaceted nature of gendered language styles. Previous corpus-based work\nhas also had little to say about individuals whose linguistic styles defy\npopulation-level gender patterns. To identify such individuals, we train a\nstatistical classifier, and measure the classifier confidence for each\nindividual in the dataset. Examining individuals whose language does not match\nthe classifier's model for their gender, we find that they have social networks\nthat include significantly fewer same-gender social connections and that, in\ngeneral, social network homophily is correlated with the use of same-gender\nlanguage markers. Pairing computational methods and social theory thus offers a\nnew perspective on how gender emerges as individuals position themselves\nrelative to audiences, topics, and mainstream gender norms."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.5486v2", 
    "title": "A Lightweight Stemmer for Gujarati", 
    "arxiv-id": "1210.5486v2", 
    "author": "Iti Mathur", 
    "publish": "2012-10-19T17:49:06Z", 
    "summary": "Gujarati is a resource poor language with almost no language processing tools\nbeing available. In this paper we have shown an implementation of a rule based\nstemmer of Gujarati. We have shown the creation of rules for stemming and the\nrichness in morphology that Gujarati possesses. We have also evaluated our\nresults by verifying it with a human expert."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.5517v1", 
    "title": "Design of English-Hindi Translation Memory for Efficient Translation", 
    "arxiv-id": "1210.5517v1", 
    "author": "Iti Mathur", 
    "publish": "2012-10-19T17:59:56Z", 
    "summary": "Developing parallel corpora is an important and a difficult activity for\nMachine Translation. This requires manual annotation by Human Translators.\nTranslating same text again is a useless activity. There are tools available to\nimplement this for European Languages, but no such tool is available for Indian\nLanguages. In this paper we present a tool for Indian Languages which not only\nprovides automatic translations of the previously available translation but\nalso provides multiple translations, in cases where a sentence has multiple\ntranslations, in ranked list of suggestive translations for a sentence.\nMoreover this tool also lets translators have global and local saving options\nof their work, so that they may share it with others, which further lightens\nthe task."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.5751v1", 
    "title": "Extraction of domain-specific bilingual lexicon from comparable corpora:   compositional translation and ranking", 
    "arxiv-id": "1210.5751v1", 
    "author": "Claire Lemaire", 
    "publish": "2012-10-21T19:06:11Z", 
    "summary": "This paper proposes a method for extracting translations of morphologically\nconstructed terms from comparable corpora. The method is based on compositional\ntranslation and exploits translation equivalences at the morpheme-level, which\nallows for the generation of \"fertile\" translations (translation pairs in which\nthe target term has more words than the source term). Ranking methods relying\non corpus-based and translation-based features are used to select the best\ncandidate translation. We obtain an average precision of 91% on the Top1\ncandidate translation. The method was tested on two language pairs\n(English-French and English-German) and with a small specialized comparable\ncorpora (400k words per language)."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.5965v1", 
    "title": "Classification Analysis Of Authorship Fiction Texts in The Space Of   Semantic Fields", 
    "arxiv-id": "1210.5965v1", 
    "author": "Bohdan Pavlyshenko", 
    "publish": "2012-10-22T16:40:35Z", 
    "summary": "The use of naive Bayesian classifier (NB) and the classifier by the k nearest\nneighbors (kNN) in classification semantic analysis of authors' texts of\nEnglish fiction has been analysed. The authors' works are considered in the\nvector space the basis of which is formed by the frequency characteristics of\nsemantic fields of nouns and verbs. Highly precise classification of authors'\ntexts in the vector space of semantic fields indicates about the presence of\nparticular spheres of author's idiolect in this space which characterizes the\nindividual author's style."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.7282v1", 
    "title": "The Hangulphabet: A Descriptive Alphabet", 
    "arxiv-id": "1210.7282v1", 
    "author": "Ruggero Micheletto", 
    "publish": "2012-10-27T02:34:35Z", 
    "summary": "This paper describes the Hangulphabet, a new writing system that should prove\nuseful in a number of contexts. Using the Hangulphabet, a user can instantly\nsee voicing, manner and place of articulation of any phoneme found in human\nlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axis\nrepresenting the place of articulation and the y-axis representing manner of\narticulation. Each individual grapheme contains radicals from both axes where\nthe points intersect. The top radical represents manner of articulation where\nthe bottom represents place of articulation. A horizontal line running through\nthe middle of the bottom radical represents voicing. For vowels, place of\narticulation is located on a grid that represents the position of the tongue in\nthe mouth. This grid is similar to that of the IPA vowel chart (International\nPhonetic Association, 1999). The difference with the Hangulphabet being the\ntrapezoid representing the vocal apparatus is on a slight tilt. Place of\narticulation for a vowel is represented by a breakout figure from the grid.\nThis system can be used as an alternative to the International Phonetic\nAlphabet (IPA) or as a complement to it. Beginning students of linguistics may\nfind it particularly useful. A Hangulphabet font has been created to facilitate\nswitching between the Hangulphabet and the IPA."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1210.8440v1", 
    "title": "Large Scale Language Modeling in Automatic Speech Recognition", 
    "arxiv-id": "1210.8440v1", 
    "author": "Shankar Kumar", 
    "publish": "2012-10-31T18:57:14Z", 
    "summary": "Large language models have been proven quite beneficial for a variety of\nautomatic speech recognition tasks in Google. We summarize results on Voice\nSearch and a few YouTube speech transcription tasks to highlight the impact\nthat one can expect from increasing both the amount of training data, and the\nsize of the language model estimated from such data. Depending on the task,\navailability and amount of training data used, language model size and amount\nof work and care put into integrating them in the lattice rescoring step we\nobserve reductions in word error rate between 6% and 10% relative, for systems\non a wide range of operating points between 17% and 52% word error rate."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1211.0074v1", 
    "title": "Transition-Based Dependency Parsing With Pluggable Classifiers", 
    "arxiv-id": "1211.0074v1", 
    "author": "Alex Rudnick", 
    "publish": "2012-11-01T02:10:06Z", 
    "summary": "In principle, the design of transition-based dependency parsers makes it\npossible to experiment with any general-purpose classifier without other\nchanges to the parsing algorithm. In practice, however, it often takes\nsubstantial software engineering to bridge between the different\nrepresentations used by two software packages. Here we present extensions to\nMaltParser that allow the drop-in use of any classifier conforming to the\ninterface of the Weka machine learning package, a wrapper for the TiMBL\nmemory-based learner to this interface, and experiments on multilingual\ndependency parsing with a variety of classifiers. While earlier work had\nsuggested that memory-based learners might be a good choice for low-resource\nparsing scenarios, we cannot support that hypothesis in this work. We observed\nthat support-vector machines give better parsing performance than the\nmemory-based learner, regardless of the size of the training set."
},{
    "category": "cs.CL", 
    "doi": "10.1111/josl.12080", 
    "link": "http://arxiv.org/pdf/1211.0498v1", 
    "title": "Detecting English Writing Styles For Non-native Speakers", 
    "arxiv-id": "1211.0498v1", 
    "author": "Rami Al-Rfou'", 
    "publish": "2012-11-02T17:37:06Z", 
    "summary": "Analyzing writing styles of non-native speakers is a challenging task. In\nthis paper, we analyze the comments written in the discussion pages of the\nEnglish Wikipedia. Using learning algorithms, we are able to detect native\nspeakers' writing style with an accuracy of 74%. Given the diversity of the\nEnglish Wikipedia users and the large number of languages they speak, we\nmeasure the similarities among their native languages by comparing the\ninfluence they have on their English writing style. Our results show that\nlanguages known to have the same origin and development path have similar\nfootprint on their speakers' English writing style. To enable further studies,\nthe dataset we extracted from Wikipedia will be made available publicly."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1211.3643v1", 
    "title": "A Principled Approach to Grammars for Controlled Natural Languages and   Predictive Editors", 
    "arxiv-id": "1211.3643v1", 
    "author": "Tobias Kuhn", 
    "publish": "2012-11-15T16:24:25Z", 
    "summary": "Controlled natural languages (CNL) with a direct mapping to formal logic have\nbeen proposed to improve the usability of knowledge representation systems,\nquery interfaces, and formal specifications. Predictive editors are a popular\napproach to solve the problem that CNLs are easy to read but hard to write.\nSuch predictive editors need to be able to \"look ahead\" in order to show all\npossible continuations of a given unfinished sentence. Such lookahead features,\nhowever, are difficult to implement in a satisfying way with existing grammar\nframeworks, especially if the CNL supports complex nonlocal structures such as\nanaphoric references. Here, methods and algorithms are presented for a new\ngrammar notation called Codeco, which is specifically designed for controlled\nnatural languages and predictive editors. A parsing approach for Codeco based\non an extended chart parsing algorithm is presented. A large subset of Attempto\nControlled English (ACE) has been represented in Codeco. Evaluation of this\ngrammar and the parser implementation shows that the approach is practical,\nadequate and efficient."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1211.4161v1", 
    "title": "Semantic Polarity of Adjectival Predicates in Online Reviews", 
    "arxiv-id": "1211.4161v1", 
    "author": "Jee-Sun Nam", 
    "publish": "2012-11-17T20:27:06Z", 
    "summary": "Web users produce more and more documents expressing opinions. Because these\nhave become important resources for customers and manufacturers, many have\nfocused on them. Opinions are often expressed through adjectives with positive\nor negative semantic values. In extracting information from users' opinion in\nonline reviews, exact recognition of the semantic polarity of adjectives is one\nof the most important requirements. Since adjectives have different semantic\norientations according to contexts, it is not satisfying to extract opinion\ninformation without considering the semantic and lexical relations between the\nadjectives and the feature nouns appropriate to a given domain. In this paper,\nwe present a classification of adjectives by polarity, and we analyze\nadjectives that are undetermined in the absence of contexts. Our research\nshould be useful for accurately predicting semantic orientations of opinion\nsentences, and should be taken into account before relying on an automatic\nmethods."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.1192v2", 
    "title": "Using external sources of bilingual information for on-the-fly word   alignment", 
    "arxiv-id": "1212.1192v2", 
    "author": "Mikel L. Forcada", 
    "publish": "2012-12-05T22:10:04Z", 
    "summary": "In this paper we present a new and simple language-independent method for\nword-alignment based on the use of external sources of bilingual information\nsuch as machine translation systems. We show that the few parameters of the\naligner can be trained on a very small corpus, which leads to results\ncomparable to those obtained by the state-of-the-art tool GIZA++ in terms of\nprecision. Regarding other metrics, such as alignment error rate or F-measure,\nthe parametric aligner, when trained on a very small gold-standard (450 pairs\nof sentences), provides results comparable to those produced by GIZA++ when\ntrained on an in-domain corpus of around 10,000 pairs of sentences.\nFurthermore, the results obtained indicate that the training is\ndomain-independent, which enables the use of the trained aligner 'on the fly'\non any new pair of sentences."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.3138v1", 
    "title": "Identifying Metaphor Hierarchies in a Corpus Analysis of Finance   Articles", 
    "arxiv-id": "1212.3138v1", 
    "author": "Mark Keane", 
    "publish": "2012-12-13T11:47:09Z", 
    "summary": "Using a corpus of over 17,000 financial news reports (involving over 10M\nwords), we perform an analysis of the argument-distributions of the UP- and\nDOWN-verbs used to describe movements of indices, stocks, and shares. Using\nmeasures of the overlap in the argument distributions of these verbs and\nk-means clustering of their distributions, we advance evidence for the proposal\nthat the metaphors referred to by these verbs are organised into hierarchical\nstructures of superordinate and subordinate groups."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.3139v2", 
    "title": "Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles", 
    "arxiv-id": "1212.3139v2", 
    "author": "Mark Keane", 
    "publish": "2012-12-13T11:53:25Z", 
    "summary": "Using a corpus of 17,000+ financial news reports (involving over 10M words),\nwe perform an analysis of the argument-distributions of the UP and DOWN verbs\nused to describe movements of indices, stocks and shares. In Study 1\nparticipants identified antonyms of these verbs in a free-response task and a\nmatching task from which the most commonly identified antonyms were compiled.\nIn Study 2, we determined whether the argument-distributions for the verbs in\nthese antonym-pairs were sufficiently similar to predict the most\nfrequently-identified antonym. Cosine similarity correlates moderately with the\nproportions of antonym-pairs identified by people (r = 0.31). More\nimpressively, 87% of the time the most frequently-identified antonym is either\nthe first- or second-most similar pair in the set of alternatives. The\nimplications of these results for distributional approaches to determining\nmetaphoric knowledge are discussed."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.3162v1", 
    "title": "Diachronic Variation in Grammatical Relations", 
    "arxiv-id": "1212.3162v1", 
    "author": "Khurshid Ahmad", 
    "publish": "2012-12-13T13:00:55Z", 
    "summary": "We present a method of finding and analyzing shifts in grammatical relations\nfound in diachronic corpora. Inspired by the econometric technique of measuring\nreturn and volatility instead of relative frequencies, we propose them as a way\nto better characterize changes in grammatical patterns like nominalization,\nmodification and comparison. To exemplify the use of these techniques, we\nexamine a corpus of NIPS papers and report trends which manifest at the token,\npart-of-speech and grammatical levels. Building up from frequency observations\nto a second-order analysis, we show that shifts in frequencies overlook deeper\ntrends in language, even when part-of-speech information is included. Examining\ntoken, POS and grammatical levels of variation enables a summary view of\ndiachronic text as a whole. We conclude with a discussion about how these\nmethods can inform intuitions about specialist domains as well as changes in\nlanguage use as a whole."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.4315v1", 
    "title": "Assessing Sentiment Strength in Words Prior Polarities", 
    "arxiv-id": "1212.4315v1", 
    "author": "Marco Guerini", 
    "publish": "2012-12-18T11:33:50Z", 
    "summary": "Many approaches to sentiment analysis rely on lexica where words are tagged\nwith their prior polarity - i.e. if a word out of context evokes something\npositive or something negative. In particular, broad-coverage resources like\nSentiWordNet provide polarities for (almost) every word. Since words can have\nmultiple senses, we address the problem of how to compute the prior polarity of\na word starting from the polarity of each sense and returning its polarity\nstrength as an index between -1 and 1. We compare 14 such formulae that appear\nin the literature, and assess which one best approximates the human judgement\nof prior polarities, with both regression and classification models."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10849-012-9167-z", 
    "link": "http://arxiv.org/pdf/1212.4674v1", 
    "title": "Natural Language Understanding Based on Semantic Relations between   Sentences", 
    "arxiv-id": "1212.4674v1", 
    "author": "Hyeok Kong", 
    "publish": "2012-12-19T14:40:38Z", 
    "summary": "In this paper, we define event expression over sentences of natural language\nand semantic relations between events. Based on this definition, we formally\nconsider text understanding process having events as basic unit."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASL.2013.2245649", 
    "link": "http://arxiv.org/pdf/1302.1123v1", 
    "title": "Large Scale Distributed Acoustic Modeling With Back-off N-grams", 
    "arxiv-id": "1302.1123v1", 
    "author": "Thomas Richardson", 
    "publish": "2013-02-05T17:09:49Z", 
    "summary": "The paper revives an older approach to acoustic modeling that borrows from\nn-gram language modeling in an attempt to scale up both the amount of training\ndata and model size (as measured by the number of parameters in the model), to\napproximately 100 times larger than current sizes used in automatic speech\nrecognition. In such a data-rich setting, we can expand the phonetic context\nsignificantly beyond triphones, as well as increase the number of Gaussian\nmixture components for the context-dependent states that allow it. We have\nexperimented with contexts that span seven or more context-independent phones,\nand up to 620 mixture components per state. Dealing with unseen phonetic\ncontexts is accomplished using the familiar back-off technique used in language\nmodeling due to implementation simplicity. The back-off acoustic model is\nestimated, stored and served using MapReduce distributed computing\ninfrastructure.\n  Speech recognition experiments are carried out in an N-best list rescoring\nframework for Google Voice Search. Training big models on large amounts of data\nproves to be an effective way to increase the accuracy of a state-of-the-art\nautomatic speech recognition system. We use 87,000 hours of training data\n(speech along with transcription) obtained by filtering utterances in Voice\nSearch logs on automatic speech recognition confidence. Models ranging in size\nbetween 20--40 million Gaussians are estimated using maximum likelihood\ntraining. They achieve relative reductions in word-error-rate of 11% and 6%\nwhen combined with first-pass models trained using maximum likelihood, and\nboosted maximum mutual information, respectively. Increasing the context size\nbeyond five phones (quinphones) does not help."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASL.2013.2245649", 
    "link": "http://arxiv.org/pdf/1302.1380v1", 
    "title": "Towards the Rapid Development of a Natural Language Understanding Module", 
    "arxiv-id": "1302.1380v1", 
    "author": "Bruno Martins", 
    "publish": "2013-02-06T14:17:55Z", 
    "summary": "When developing a conversational agent, there is often an urgent need to have\na prototype available in order to test the application with real users. A\nWizard of Oz is a possibility, but sometimes the agent should be simply\ndeployed in the environment where it will be used. Here, the agent should be\nable to capture as many interactions as possible and to understand how people\nreact to failure. In this paper, we focus on the rapid development of a natural\nlanguage understanding module by non experts. Our approach follows the learning\nparadigm and sees the process of understanding natural language as a\nclassification problem. We test our module with a conversational agent that\nanswers questions in the art domain. Moreover, we show how our approach can be\nused by a natural language interface to a cinema database."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASL.2013.2245649", 
    "link": "http://arxiv.org/pdf/1302.1422v2", 
    "title": "S\u00e9mantique des d\u00e9terminants dans un cadre richement typ\u00e9", 
    "arxiv-id": "1302.1422v2", 
    "author": "Christian Retor\u00e9", 
    "publish": "2013-02-06T16:26:49Z", 
    "summary": "The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASL.2013.2245649", 
    "link": "http://arxiv.org/pdf/1302.1572v1", 
    "title": "Lexical Access for Speech Understanding using Minimum Message Length   Encoding", 
    "arxiv-id": "1302.1572v1", 
    "author": "Bhavani Raskutti", 
    "publish": "2013-02-06T15:59:24Z", 
    "summary": "The Lexical Access Problem consists of determining the intended sequence of\nwords corresponding to an input sequence of phonemes (basic speech sounds) that\ncome from a low-level phoneme recognizer. In this paper we present an\ninformation-theoretic approach based on the Minimum Message Length Criterion\nfor solving the Lexical Access Problem. We model sentences using phoneme\nrealizations seen in training, and word and part-of-speech information obtained\nfrom text corpora. We show results on multiple-speaker, continuous, read speech\nand discuss a heuristic using equivalence classes of similar sounding words\nwhich speeds up the recognition process without significant deterioration in\nrecognition accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASL.2013.2245649", 
    "link": "http://arxiv.org/pdf/1302.3057v1", 
    "title": "Building a reordering system using tree-to-string hierarchical model", 
    "arxiv-id": "1302.3057v1", 
    "author": "Irina Galinskaya", 
    "publish": "2013-02-13T11:54:56Z", 
    "summary": "This paper describes our submission to the First Workshop on Reordering for\nStatistical Machine Translation. We have decided to build a reordering system\nbased on tree-to-string model, using only publicly available tools to\naccomplish this task. With the provided training data we have built a\ntranslation model using Moses toolkit, and then we applied a chart decoder,\nimplemented in Moses, to reorder the sentences. Even though our submission only\ncovered English-Farsi language pair, we believe that the approach itself should\nwork regardless of the choice of the languages, so we have also carried out the\nexperiments for English-Italian and English-Urdu. For these language pairs we\nhave noticed a significant improvement over the baseline in BLEU, Kendall-Tau\nand Hamming metrics. A detailed description is given, so that everyone can\nreproduce our results. Also, some possible directions for further improvements\nare discussed."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-36337-5_15", 
    "link": "http://arxiv.org/pdf/1302.4489v1", 
    "title": "Termhood-based Comparability Metrics of Comparable Corpus in Special   Domain", 
    "arxiv-id": "1302.4489v1", 
    "author": "Chengzhi Zhang", 
    "publish": "2013-02-19T00:30:57Z", 
    "summary": "Cross-Language Information Retrieval (CLIR) and machine translation (MT)\nresources, such as dictionaries and parallel corpora, are scarce and hard to\ncome by for special domains. Besides, these resources are just limited to a few\nlanguages, such as English, French, and Spanish and so on. So, obtaining\ncomparable corpora automatically for such domains could be an answer to this\nproblem effectively. Comparable corpora, that the subcorpora are not\ntranslations of each other, can be easily obtained from web. Therefore,\nbuilding and using comparable corpora is often a more feasible option in\nmultilingual information processing. Comparability metrics is one of key issues\nin the field of building and using comparable corpus. Currently, there is no\nwidely accepted definition or metrics method of corpus comparability. In fact,\nDifferent definitions or metrics methods of comparability might be given to\nsuit various tasks about natural language processing. A new comparability,\nnamely, termhood-based metrics, oriented to the task of bilingual terminology\nextraction, is proposed in this paper. In this method, words are ranked by\ntermhood not frequency, and then the cosine similarities, calculated based on\nthe ranking lists of word termhood, is used as comparability. Experiments\nresults show that termhood-based metrics performs better than traditional\nfrequency-based metrics."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.4492v1", 
    "title": "Bilingual Terminology Extraction Using Multi-level Termhood", 
    "arxiv-id": "1302.4492v1", 
    "author": "Dan Wu", 
    "publish": "2013-02-19T00:37:21Z", 
    "summary": "Purpose: Terminology is the set of technical words or expressions used in\nspecific contexts, which denotes the core concept in a formal discipline and is\nusually applied in the fields of machine translation, information retrieval,\ninformation extraction and text categorization, etc. Bilingual terminology\nextraction plays an important role in the application of bilingual dictionary\ncompilation, bilingual Ontology construction, machine translation and\ncross-language information retrieval etc. This paper addresses the issues of\nmonolingual terminology extraction and bilingual term alignment based on\nmulti-level termhood.\n  Design/methodology/approach: A method based on multi-level termhood is\nproposed. The new method computes the termhood of the terminology candidate as\nwell as the sentence that includes the terminology by the comparison of the\ncorpus. Since terminologies and general words usually have differently\ndistribution in the corpus, termhood can also be used to constrain and enhance\nthe performance of term alignment when aligning bilingual terms on the parallel\ncorpus. In this paper, bilingual term alignment based on termhood constraints\nis presented.\n  Findings: Experiment results show multi-level termhood can get better\nperformance than existing method for terminology extraction. If termhood is\nused as constrain factor, the performance of bilingual term alignment can be\nimproved."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.4811v1", 
    "title": "Towards a Semantic-based Approach for Modeling Regulatory Documents in   Building Industry", 
    "arxiv-id": "1302.4811v1", 
    "author": "Le-Thanh Nhan", 
    "publish": "2013-02-20T05:46:53Z", 
    "summary": "Regulations in the Building Industry are becoming increasingly complex and\ninvolve more than one technical area. They cover products, components and\nproject implementation. They also play an important role to ensure the quality\nof a building, and to minimize its environmental impact. In this paper, we are\nparticularly interested in the modeling of the regulatory constraints derived\nfrom the Technical Guides issued by CSTB and used to validate Technical\nAssessments. We first describe our approach for modeling regulatory constraints\nin the SBVR language, and formalizing them in the SPARQL language. Second, we\ndescribe how we model the processes of compliance checking described in the\nCSTB Technical Guides. Third, we show how we implement these processes to\nassist industrials in drafting Technical Documents in order to acquire a\nTechnical Assessment; a compliance report is automatically generated to explain\nthe compliance or noncompliance of this Technical Documents."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.4813v1", 
    "title": "Probabilistic Frame Induction", 
    "arxiv-id": "1302.4813v1", 
    "author": "Lucy Vanderwende", 
    "publish": "2013-02-20T05:47:32Z", 
    "summary": "In natural-language discourse, related events tend to appear near each other\nto describe a larger scenario. Such structures can be formalized by the notion\nof a frame (a.k.a. template), which comprises a set of related events and\nprototypical participants and event transitions. Identifying frames is a\nprerequisite for information extraction and natural language generation, and is\nusually done manually. Methods for inducing frames have been proposed recently,\nbut they typically use ad hoc procedures and are difficult to diagnose or\nextend. In this paper, we propose the first probabilistic approach to frame\ninduction, which incorporates frames, events, participants as latent topics and\nlearns those frame and event transitions that best explain the text. The number\nof frames is inferred by a novel application of a split-merge method from\nsyntactic parsing. In end-to-end evaluations from text to induced frames and\nextracted facts, our method produced state-of-the-art results while\nsubstantially reducing engineering effort."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.4814v1", 
    "title": "NLP and CALL: integration is working", 
    "arxiv-id": "1302.4814v1", 
    "author": "Virginie Zampa", 
    "publish": "2013-02-20T05:47:44Z", 
    "summary": "In the first part of this article, we explore the background of\ncomputer-assisted learning from its beginnings in the early XIXth century and\nthe first teaching machines, founded on theories of learning, at the start of\nthe XXth century. With the arrival of the computer, it became possible to offer\nlanguage learners different types of language activities such as comprehension\ntasks, simulations, etc. However, these have limits that cannot be overcome\nwithout some contribution from the field of natural language processing (NLP).\nIn what follows, we examine the challenges faced and the issues raised by\nintegrating NLP into CALL. We hope to demonstrate that the key to success in\nintegrating NLP into CALL is to be found in multidisciplinary work between\ncomputer experts, linguists, language teachers, didacticians and NLP\nspecialists."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.5645v1", 
    "title": "Role of temporal inference in the recognition of textual inference", 
    "arxiv-id": "1302.5645v1", 
    "author": "Djallel Bouneffouf", 
    "publish": "2013-02-18T15:28:51Z", 
    "summary": "This project is a part of nature language processing and its aims to develop\na system of recognition inference text-appointed TIMINF. This type of system\ncan detect, given two portions of text, if a text is semantically deducted from\nthe other. We focused on making the inference time in this type of system. For\nthat we have built and analyzed a body built from questions collected through\nthe web. This study has enabled us to classify different types of times\ninferences and for designing the architecture of TIMINF which seeks to\nintegrate a module inference time in a detection system inference text. We also\nassess the performance of sorties TIMINF system on a test corpus with the same\nstrategy adopted in the challenge RTE."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1302.6777v1", 
    "title": "Ending-based Strategies for Part-of-speech Tagging", 
    "arxiv-id": "1302.6777v1", 
    "author": "Tim Philip", 
    "publish": "2013-02-27T14:13:10Z", 
    "summary": "Probabilistic approaches to part-of-speech tagging rely primarily on\nwhole-word statistics about word/tag combinations as well as contextual\ninformation. But experience shows about 4 per cent of tokens encountered in\ntest sets are unknown even when the training set is as large as a million\nwords. Unseen words are tagged using secondary strategies that exploit word\nfeatures such as endings, capitalizations and punctuation marks. In this work,\nword-ending statistics are primary and whole-word statistics are secondary.\nFirst, a tagger was trained and tested on word endings only. Subsequent\nexperiments added back whole-word statistics for the words occurring most\nfrequently in the training set. As grew larger, performance was expected to\nimprove, in the limit performing the same as word-based taggers. Surprisingly,\nthe ending-based tagger initially performed nearly as well as the word-based\ntagger; in the best case, its performance significantly exceeded that of the\nword-based tagger. Lastly, and unexpectedly, an effect of negative returns was\nobserved - as grew larger, performance generally improved and then declined. By\nvarying factors such as ending length and tag-list strategy, we achieved a\nsuccess rate of 97.5 percent."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.0446v1", 
    "title": "Statistical sentiment analysis performance in Opinum", 
    "arxiv-id": "1303.0446v1", 
    "author": "Sergio Ortiz Rojas", 
    "publish": "2013-03-03T01:38:03Z", 
    "summary": "The classification of opinion texts in positive and negative is becoming a\nsubject of great interest in sentiment analysis. The existence of many labeled\nopinions motivates the use of statistical and machine-learning methods.\nFirst-order statistics have proven to be very limited in this field. The Opinum\napproach is based on the order of the words without using any syntactic and\nsemantic information. It consists of building one probabilistic model for the\npositive and another one for the negative opinions. Then the test opinions are\ncompared to both models and a decision and confidence measure are calculated.\nIn order to reduce the complexity of the training corpus we first lemmatize the\ntexts and we replace most named-entities with wildcards. Opinum presents an\naccuracy above 81% for Spanish opinions in the financial products domain. In\nthis work we discuss which are the most important factors that have impact on\nthe classification performance."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.1929v1", 
    "title": "Towards the Fully Automatic Merging of Lexical Resources: A Step Forward", 
    "arxiv-id": "1303.1929v1", 
    "author": "Silvia Necsulescu", 
    "publish": "2013-03-08T10:13:56Z", 
    "summary": "This article reports on the results of the research done towards the fully\nautomatically merging of lexical resources. Our main goal is to show the\ngenerality of the proposed approach, which have been previously applied to\nmerge Spanish Subcategorization Frames lexica. In this work we extend and apply\nthe same technique to perform the merging of morphosyntactic lexica encoded in\nLMF. The experiments showed that the technique is general enough to obtain good\nresults in these two different tasks which is an important step towards\nperforming the merging of lexical resources fully automatically."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.1930v1", 
    "title": "Automatic lexical semantic classification of nouns", 
    "arxiv-id": "1303.1930v1", 
    "author": "Muntsa Padr\u00f3", 
    "publish": "2013-03-08T10:14:04Z", 
    "summary": "The work we present here addresses cue-based noun classification in English\nand Spanish. Its main objective is to automatically acquire lexical semantic\ninformation by classifying nouns into previously known noun lexical classes.\nThis is achieved by using particular aspects of linguistic contexts as cues\nthat identify a specific lexical class. Here we concentrate on the task of\nidentifying such cues and the theoretical background that allows for an\nassessment of the complexity of the task. The results show that, despite of the\na-priori complexity of the task, cue-based classification is a useful tool in\nthe automatic acquisition of lexical semantic classes."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.1931v1", 
    "title": "A Classification of Adjectives for Polarity Lexicons Enhancement", 
    "arxiv-id": "1303.1931v1", 
    "author": "N\u00faria Bel", 
    "publish": "2013-03-08T10:14:31Z", 
    "summary": "Subjective language detection is one of the most important challenges in\nSentiment Analysis. Because of the weight and frequency in opinionated texts,\nadjectives are considered a key piece in the opinion extraction process. These\nsubjective units are more and more frequently collected in polarity lexicons in\nwhich they appear annotated with their prior polarity. However, at the moment,\nany polarity lexicon takes into account prior polarity variations across\ndomains. This paper proves that a majority of adjectives change their prior\npolarity value depending on the domain. We propose a distinction between domain\ndependent and domain independent adjectives. Moreover, our analysis led us to\npropose a further classification related to subjectivity degree: constant,\nmixed and highly subjective adjectives. Following this classification, polarity\nvalues will be a better support for Sentiment Analysis."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.1932v1", 
    "title": "Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform", 
    "arxiv-id": "1303.1932v1", 
    "author": "Victoria Arranz", 
    "publish": "2013-03-08T10:15:57Z", 
    "summary": "The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform\nthat automates the stages involved in the acquisition, production, updating and\nmaintenance of the large language resources required by, among others, MT\nsystems. The development of a Corpus Acquisition Component (CAC) for extracting\nmonolingual and bilingual data from the web is one of the most innovative\nbuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEA\npipeline for building Language Resources, adopts an efficient and distributed\nmethodology to crawl for web documents with rich textual content in specific\nlanguages and predefined domains. The CAC includes modules that can acquire\nparallel data from sites with in-domain content available in more than one\nlanguage. In order to extrinsically evaluate the CAC methodology, we have\nconducted several experiments that used crawled parallel corpora for the\nidentification and extraction of parallel sentences using sentence alignment.\nThe corpora were then successfully used for domain adaptation of Machine\nTranslation Systems."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.2448v1", 
    "title": "Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon   Production", 
    "arxiv-id": "1303.2448v1", 
    "author": "Gabriela Resnik", 
    "publish": "2013-03-11T08:21:17Z", 
    "summary": "In this work we present the results of our experimental work on the\ndevelop-ment of lexical class-based lexica by automatic means. The objective is\nto as-sess the use of linguistic lexical-class based information as a feature\nselection methodology for the use of classifiers in quick lexical development.\nThe results show that the approach can help in re-ducing the human effort\nrequired in the development of language resources sig-nificantly."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.2449v1", 
    "title": "Using qualia information to identify lexical semantic classes in an   unsupervised clustering task", 
    "arxiv-id": "1303.2449v1", 
    "author": "N\u00faria Bel", 
    "publish": "2013-03-11T08:21:48Z", 
    "summary": "Acquiring lexical information is a complex problem, typically approached by\nrelying on a number of contexts to contribute information for classification.\nOne of the first issues to address in this domain is the determination of such\ncontexts. The work presented here proposes the use of automatically obtained\nFORMAL role descriptors as features used to draw nouns from the same lexical\nsemantic class together in an unsupervised clustering task. We have dealt with\nthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The\nresults obtained show that it is possible to discriminate between elements from\ndifferent lexical semantic classes using only FORMAL role information, hence\nvalidating our initial hypothesis. Also, iterating our method accurately\naccounts for fine-grained distinctions within lexical classes, namely\ndistinctions involving ambiguous expressions. Moreover, a filtering and\nbootstrapping strategy employed in extracting FORMAL role descriptors proved to\nminimize effects of sparse data and noise in our task."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.2826v1", 
    "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", 
    "arxiv-id": "1303.2826v1", 
    "author": "Fei Song", 
    "publish": "2013-03-12T10:20:50Z", 
    "summary": "This article presents a probabilistic generative model for text based on\nsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).\nPOSLDA simultaneously uncovers short-range syntactic patterns (syntax) and\nlong-range semantic patterns (topics) that exist in document collections. This\nresults in word distributions that are specific to both topics (sports,\neducation, ...) and parts-of-speech (nouns, verbs, ...). For example,\nmultinomial distributions over words are uncovered that can be understood as\n\"nouns about weather\" or \"verbs about law\". We describe the model and an\napproximate inference algorithm and then demonstrate the quality of the learned\ntopics both qualitatively and quantitatively. Then, we discuss an NLP\napplication where the output of POSLDA can lead to strong improvements in\nquality: unsupervised part-of-speech tagging. We describe algorithms for this\ntask that make use of POSLDA-learned distributions that result in improved\nperformance beyond the state of the art."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1303.5960v3", 
    "title": "SYNTAGMA. A Linguistic Approach to Parsing", 
    "arxiv-id": "1303.5960v3", 
    "author": "Daniel Christen", 
    "publish": "2013-03-24T15:27:51Z", 
    "summary": "SYNTAGMA is a rule-based parsing system, structured on two levels: a general\nparsing engine and a language specific grammar. The parsing engine is a\nlanguage independent program, while grammar and language specific rules and\nresources are given as text files, consisting in a list of constituent\nstructuresand a lexical database with word sense related features and\nconstraints. Since its theoretical background is principally Tesniere's\nElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument\nstructure (valency) in constraint satisfaction, and allows also horizontal\nbounds, for instance treating coordination. Notions such as Pro, traces, empty\ncategories are derived from Generative Grammar and some solutions are close to\nGovernment&Binding Theory, although they are the result of an autonomous\nresearch. These properties allow SYNTAGMA to manage complex syntactic\nconfigurations and well known weak points in parsing engineering. An important\nresource is the semantic network, which is used in disambiguation tasks.\nParsing process follows a bottom-up, rule driven strategy. Its behavior can be\ncontrolled and fine-tuned."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1308.1004v3", 
    "title": "Boundary identification of events in clinical named entity recognition", 
    "arxiv-id": "1308.1004v3", 
    "author": "Azad Dehghan", 
    "publish": "2013-08-05T15:14:14Z", 
    "summary": "The problem of named entity recognition in the medical/clinical domain has\ngained increasing attention do to its vital role in a wide range of clinical\ndecision support applications. The identification of complete and correct term\nspan is vital for further knowledge synthesis (e.g., coding/mapping concepts\nthesauruses and classification standards). This paper investigates boundary\nadjustment by sequence labeling representations models and post-processing\ntechniques in the problem of clinical named entity recognition (recognition of\nclinical events). Using current state-of-the-art sequence labeling algorithm\n(conditional random fields), we show experimentally that sequence labeling\nrepresentation and post-processing can be significantly helpful in strict\nboundary identification of clinical events."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1308.1507v1", 
    "title": "Logical analysis of natural language semantics to solve the problem of   computer understanding", 
    "arxiv-id": "1308.1507v1", 
    "author": "Yuriy Ostapov", 
    "publish": "2013-08-07T09:09:47Z", 
    "summary": "An object--oriented approach to create a natural language understanding\nsystem is considered. The understanding program is a formal system built on the\nbase of predicative calculus. Horn's clauses are used as well--formed formulas.\nAn inference is based on the principle of resolution. Sentences of natural\nlanguage are represented in the view of typical predicate set. These predicates\ndescribe physical objects and processes, abstract objects, categories and\nsemantic relations between objects. Predicates for concrete assertions are\nsaved in a database. To describe the semantics of classes for physical objects,\nabstract concepts and processes, a knowledge base is applied. The proposed\nrepresentation of natural language sentences is a semantic net. Nodes of such\nnet are typical predicates. This approach is perspective as, firstly, such\ntypification of nodes facilitates essentially forming of processing algorithms\nand object descriptions, secondly, the effectiveness of algorithms is increased\n(particularly for the great number of nodes), thirdly, to describe the\nsemantics of words, encyclopedic knowledge is used, and this permits\nessentially to extend the class of solved problems."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1308.2428v2", 
    "title": "Hidden Structure and Function in the Lexicon", 
    "arxiv-id": "1308.2428v2", 
    "author": "Stevan Harnad", 
    "publish": "2013-08-11T20:50:27Z", 
    "summary": "How many words are needed to define all the words in a dictionary?\nGraph-theoretic analysis reveals that about 10% of a dictionary is a unique\nKernel of words that define one another and all the rest, but this is not the\nsmallest such subset. The Kernel consists of one huge strongly connected\ncomponent (SCC), about half its size, the Core, surrounded by many small SCCs,\nthe Satellites. Core words can define one another but not the rest of the\ndictionary. The Kernel also contains many overlapping Minimal Grounding Sets\n(MGSs), each about the same size as the Core, each part-Core, part-Satellite.\nMGS words can define all the rest of the dictionary. They are learned earlier,\nmore concrete and more frequent than the rest of the dictionary. Satellite\nwords, not correlated with age or frequency, are less concrete (more abstract)\nwords that are also needed for full lexical power."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1308.2696v1", 
    "title": "B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language", 
    "arxiv-id": "1308.2696v1", 
    "author": "R. Dale", 
    "publish": "2013-08-12T20:57:02Z", 
    "summary": "Discourse analysis may seek to characterize not only the overall composition\nof a given text but also the dynamic patterns within the data. This technical\nreport introduces a data format intended to facilitate multi-level\ninvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by\nthe long-form data format required for mixed-effects modeling, B(eo)W(u)LF\nstructures linguistic data into an expanded matrix encoding any number of\nresearchers-specified markers, making it ideal for recurrence-based analyses.\nWhile we do not necessarily claim to be the first to use methods along these\nlines, we have created a series of tools utilizing Python and MATLAB to enable\nsuch discourse analyses and demonstrate them using 319 lines of the Old English\nepic poem, Beowulf, translated into modern English."
},{
    "category": "cs.CL", 
    "doi": "10.1108/02640471211221395", 
    "link": "http://arxiv.org/pdf/1308.3839v2", 
    "title": "Consensus Sequence Segmentation", 
    "arxiv-id": "1308.3839v2", 
    "author": "Arko Banerjee", 
    "publish": "2013-08-18T07:09:03Z", 
    "summary": "In this paper we introduce a method to detect words or phrases in a given\nsequence of alphabets without knowing the lexicon. Our linear time unsupervised\nalgorithm relies entirely on statistical relationships among alphabets in the\ninput sequence to detect location of word boundaries. We compare our algorithm\nto previous approaches from unsupervised sequence segmentation literature and\nprovide superior segmentation over number of benchmarks."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.4479v1", 
    "title": "An Investigation of the Sampling-Based Alignment Method and Its   Contributions", 
    "arxiv-id": "1308.4479v1", 
    "author": "Yves Lepage", 
    "publish": "2013-08-21T03:44:04Z", 
    "summary": "By investigating the distribution of phrase pairs in phrase translation\ntables, the work in this paper describes an approach to increase the number of\nn-gram alignments in phrase translation tables output by a sampling-based\nalignment method. This approach consists in enforcing the alignment of n-grams\nin distinct translation subtables so as to increase the number of n-grams.\nStandard normal distribution is used to allot alignment time among translation\nsubtables, which results in adjustment of the distribution of n- grams. This\nleads to better evaluation results on statistical machine translation tasks\nthan the original sampling-based alignment approach. Furthermore, the\ntranslation quality obtained by merging phrase translation tables computed from\nthe sampling-based alignment method and from MGIZA++ is examined."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.5423v1", 
    "title": "A Literature Review: Stemming Algorithms for Indian Languages", 
    "arxiv-id": "1308.5423v1", 
    "author": "R. Manavalan", 
    "publish": "2013-08-25T16:41:06Z", 
    "summary": "Stemming is the process of extracting root word from the given inflection\nword. It also plays significant role in numerous application of Natural\nLanguage Processing (NLP). The stemming problem has addressed in many contexts\nand by researchers in many disciplines. This expository paper presents survey\nof some of the latest developments on stemming algorithms in data mining and\nalso presents with some of the solutions for various Indian language stemming\nalgorithms along with the results."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.5499v1", 
    "title": "Linear models and linear mixed effects models in R with linguistic   applications", 
    "arxiv-id": "1308.5499v1", 
    "author": "Bodo Winter", 
    "publish": "2013-08-26T07:18:17Z", 
    "summary": "This text is a conceptual introduction to mixed effects modeling with\nlinguistic applications, using the R programming environment. The reader is\nintroduced to linear modeling and assumptions, as well as to mixed\neffects/multilevel modeling, including a discussion of random intercepts,\nrandom slopes and likelihood ratio tests. The example used throughout the text\nfocuses on the phonetic analysis of voice pitch data."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.6242v1", 
    "title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of   Tweets", 
    "arxiv-id": "1308.6242v1", 
    "author": "Xiaodan Zhu", 
    "publish": "2013-08-28T18:23:03Z", 
    "summary": "In this paper, we describe how we created two state-of-the-art SVM\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\n(message-level task) and one to detect the sentiment of a term within a\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\nin the message-level task and 88.93 in the term-level task. We implemented a\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\nhashtags, and one from tweets with emoticons. In the message-level task, the\nlexicon-based features provided a gain of 5 F-score points over all others.\nBoth of our systems can be replicated us available resources."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.6297v1", 
    "title": "Crowdsourcing a Word-Emotion Association Lexicon", 
    "arxiv-id": "1308.6297v1", 
    "author": "Peter D. Turney", 
    "publish": "2013-08-28T20:13:32Z", 
    "summary": "Even though considerable attention has been given to the polarity of words\n(positive and negative) and the creation of large polarity lexicons, research\nin emotion analysis has had to rely on limited and small emotion lexicons. In\nthis paper we show how the combined strength and wisdom of the crowds can be\nused to generate a large, high-quality, word-emotion and word-polarity\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\nemotion annotation in a crowdsourcing scenario and propose solutions to address\nthem. Most notably, in addition to questions about emotions associated with\nterms, we show how the inclusion of a word choice question can discourage\nmalicious data entry, help identify instances where the annotator may not be\nfamiliar with the target term (allowing us to reject such annotations), and\nhelp obtain annotations at sense level (rather than at word level). We\nconducted experiments on how to formulate the emotion-annotation questions, and\nshow that asking if a term is associated with an emotion leads to markedly\nhigher inter-annotator agreement than that obtained by asking if a term evokes\nan emotion."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1308.6300v1", 
    "title": "Computing Lexical Contrast", 
    "arxiv-id": "1308.6300v1", 
    "author": "Peter D. Turney", 
    "publish": "2013-08-28T20:24:27Z", 
    "summary": "Knowing the degree of semantic contrast between words has widespread\napplication in natural language processing, including machine translation,\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\nopposites, such as {\\rm hot} and {\\rm cold}. Opposites are of many kinds such\nas antipodals, complementaries, and gradable. However, existing lexicons often\ndo not classify opposites into the different kinds. They also do not explicitly\nlist word pairs that are not opposites but yet have some degree of contrast in\nmeaning, such as {\\rm warm} and {\\rm cold} or {\\rm tropical} and {\\rm\nfreezing}. We propose an automatic method to identify contrasting word pairs\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\nthere exists the pair of opposites {\\rm hot} and {\\rm cold} such that {\\rm\ntropical} is related to {\\rm hot,} and {\\rm freezing} is related to {\\rm\ncold}.) We will call this the contrast hypothesis. We begin with a large\ncrowdsourcing experiment to determine the amount of human agreement on the\nconcept of oppositeness and its different kinds. In the process, we flesh out\nkey features of different kinds of opposites. We then present an automatic and\nempirical measure of lexical contrast that relies on the contrast hypothesis,\ncorpus statistics, and the structure of a {\\it Roget}-like thesaurus. We show\nthat the proposed measure of lexical contrast obtains high precision and large\ncoverage, outperforming existing methods."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1309.1014v1", 
    "title": "Advances in the Logical Representation of Lexical Semantics", 
    "arxiv-id": "1309.1014v1", 
    "author": "Christian Retor\u00e9", 
    "publish": "2013-09-04T12:56:37Z", 
    "summary": "The integration of lexical semantics and pragmatics in the analysis of the\nmeaning of natural lan- guage has prompted changes to the global framework\nderived from Montague. In those works, the original lexicon, in which words\nwere assigned an atomic type of a single-sorted logic, has been re- placed by a\nset of many-facetted lexical items that can compose their meaning with salient\ncontextual properties using a rich typing system as a guide. Having related our\nproposal for such an expanded framework \\LambdaTYn, we present some recent\nadvances in the logical formalisms associated, including constraints on lexical\ntransformations and polymorphic quantifiers, and ongoing discussions and\nresearch on the granularity of the type system and the limits of transitivity."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4402", 
    "link": "http://arxiv.org/pdf/1309.1125v1", 
    "title": "Learning to answer questions", 
    "arxiv-id": "1309.1125v1", 
    "author": "S\u00e9rgio Curto", 
    "publish": "2013-09-04T18:10:22Z", 
    "summary": "We present an open-domain Question-Answering system that learns to answer\nquestions based on successful past interactions. We follow a pattern-based\napproach to Answer-Extraction, where (lexico-syntactic) patterns that relate a\nquestion to its answer are automatically learned and used to answer future\nquestions. Results show that our approach contributes to the system's best\nperformance when it is conjugated with typical Answer-Extraction strategies.\nMoreover, it allows the system to learn with the answered questions and to\nrectify wrong or unsolved past questions."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.1129v1", 
    "title": "Analysing Quality of English-Hindi Machine Translation Engine Outputs   Using Bayesian Classification", 
    "arxiv-id": "1309.1129v1", 
    "author": "Iti Mathur", 
    "publish": "2013-09-04T18:23:30Z", 
    "summary": "This paper considers the problem for estimating the quality of machine\ntranslation outputs which are independent of human intervention and are\ngenerally addressed using machine learning techniques.There are various\nmeasures through which a machine learns translations quality. Automatic\nEvaluation metrics produce good co-relation at corpus level but cannot produce\nthe same results at the same segment or sentence level. In this paper 16\nfeatures are extracted from the input sentences and their translations and a\nquality score is obtained based on Bayesian inference produced from training\ndata."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.1649v2", 
    "title": "Preparing Korean Data for the Shared Task on Parsing Morphologically   Rich Languages", 
    "arxiv-id": "1309.1649v2", 
    "author": "Jinho D. Choi", 
    "publish": "2013-09-06T14:28:02Z", 
    "summary": "This document gives a brief description of Korean data prepared for the SPMRL\n2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for\nthe shared task. All constituent trees are collected from the KAIST Treebank\nand transformed to the Penn Treebank style. All dependency trees are converted\nfrom the transformed constituent trees using heuristics and labeling rules de-\nsigned specifically for the KAIST Treebank. In addition to the gold-standard\nmorphological analysis provided by the KAIST Treebank, two sets of automatic\nmorphological analysis are provided for the shared task, one is generated by\nthe HanNanum morphological analyzer, and the other is generated by the Sejong\nmorphological analyzer."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.2471v1", 
    "title": "Implementation of nlization framework for verbs, pronouns and   determiners with eugene", 
    "arxiv-id": "1309.2471v1", 
    "author": "Parteek Kumar", 
    "publish": "2013-09-10T12:03:32Z", 
    "summary": "UNL system is designed and implemented by a nonprofit organization, UNDL\nFoundation at Geneva in 1999. UNL applications are application softwares that\nallow end users to accomplish natural language tasks, such as translating,\nsummarizing, retrieving or extracting information, etc. Two major web based\napplication softwares are Interactive ANalyzer (IAN), which is a natural\nlanguage analysis system. It represents natural language sentences as semantic\nnetworks in the UNL format. Other application software is dEep-to-sUrface\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\nnatural language sentences out of semantic networks represented in the UNL\nformat. In this paper, NLization framework with EUGENE is focused, while using\nUNL system for accomplishing the task of machine translation. In whole\nNLization process, EUGENE takes a UNL input and delivers an output in natural\nlanguage without any human intervention. It is language-independent and has to\nbe parametrized to the natural language input through a dictionary and a\ngrammar, provided as separate interpretable files. In this paper, it is\nexplained that how UNL input is syntactically and semantically analyzed with\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\nand determiners for Punjabi natural language."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.2853v1", 
    "title": "General Purpose Textual Sentiment Analysis and Emotion Detection Tools", 
    "arxiv-id": "1309.2853v1", 
    "author": "Nadia Bellalem", 
    "publish": "2013-09-11T15:16:26Z", 
    "summary": "Textual sentiment analysis and emotion detection consists in retrieving the\nsentiment or emotion carried by a text or document. This task can be useful in\nmany domains: opinion mining, prediction, feedbacks, etc. However, building a\ngeneral purpose tool for doing sentiment analysis and emotion detection raises\na number of issues, theoretical issues like the dependence to the domain or to\nthe language but also pratical issues like the emotion representation for\ninteroperability. In this paper we present our sentiment/emotion analysis\ntools, the way we propose to circumvent the di culties and the applications\nthey are used for."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.4168v1", 
    "title": "Exploiting Similarities among Languages for Machine Translation", 
    "arxiv-id": "1309.4168v1", 
    "author": "Ilya Sutskever", 
    "publish": "2013-09-17T03:23:13Z", 
    "summary": "Dictionaries and phrase tables are the basis of modern statistical machine\ntranslation systems. This paper develops a method that can automate the process\nof generating and extending dictionaries and phrase tables. Our method can\ntranslate missing word and phrase entries by learning language structures based\non large monolingual data and mapping between languages from small bilingual\ndata. It uses distributed representation of words and learns a linear mapping\nbetween vector spaces of languages. Despite its simplicity, our method is\nsurprisingly effective: we can achieve almost 90% precision@5 for translation\nof words between English and Spanish. This method makes little assumption about\nthe languages, so it can be used to extend and refine dictionaries and\ntranslation tables for any language pairs."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.4628v1", 
    "title": "Text segmentation with character-level text embeddings", 
    "arxiv-id": "1309.4628v1", 
    "author": "Grzegorz Chrupa\u0142a", 
    "publish": "2013-09-18T12:38:34Z", 
    "summary": "Learning word representations has recently seen much success in computational\nlinguistics. However, assuming sequences of word tokens as input to linguistic\nanalysis is often unjustified. For many languages word segmentation is a\nnon-trivial task and naturally occurring text is sometimes a mixture of natural\nlanguage strings and other character data. We propose to learn text\nrepresentations directly from raw character sequences by training a Simple\nrecurrent Network to predict the next character in text. The network uses its\nhidden layer to evolve abstract representations of the character sequences it\nsees. To demonstrate the usefulness of the learned text embeddings, we use them\nas features in a supervised character level text segmentation and labeling\ntask: recognizing spans of text containing programming language code. By using\nthe embeddings as features we are able to substantially improve over a baseline\nwhich uses only surface character n-grams."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5223v1", 
    "title": "JRC EuroVoc Indexer JEX - A freely available multi-label categorisation   tool", 
    "arxiv-id": "1309.5223v1", 
    "author": "Marco Turchi", 
    "publish": "2013-09-20T09:51:59Z", 
    "summary": "EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700\nhierarchically organised subject domains used by European Institutions and many\nauthorities in Member States of the European Union (EU) for the classification\nand retrieval of official documents. JEX is JRC-developed multi-label\nclassification software that learns from manually labelled data to\nautomatically assign EuroVoc descriptors to new documents in a profile-based\ncategory-ranking task. The JEX release consists of trained classifiers for 22\nofficial EU languages, of parallel training data in the same languages, of an\ninterface that allows viewing and amending the assignment results, and of a\nmodule that allows users to re-train the tool on their own document\ncollections. JEX allows advanced users to change the document representation so\nas to possibly improve the categorisation result through linguistic\npre-processing. JEX can be used as a tool for interactive EuroVoc descriptor\nassignment to increase speed and consistency of the human categorisation\nprocess, or it can be used fully automatically. The output of JEX is a\nlanguage-independent EuroVoc feature vector lending itself also as input to\nvarious other Language Technology tasks, including cross-lingual clustering and\nclassification, cross-lingual plagiarism detection, sentence selection and\nranking, and more."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5226v1", 
    "title": "DGT-TM: A freely Available Translation Memory in 22 Languages", 
    "arxiv-id": "1309.5226v1", 
    "author": "Patrick Schl\u00fcter", 
    "publish": "2013-09-20T10:02:58Z", 
    "summary": "The European Commission's (EC) Directorate General for Translation, together\nwith the EC's Joint Research Centre, is making available a large translation\nmemory (TM; i.e. sentences and their professionally produced translations)\ncovering twenty-two official European Union (EU) languages and their 231\nlanguage pairs. Such a resource is typically used by translation professionals\nin combination with TM software to improve speed and consistency of their\ntranslations. However, this resource has also many uses for translation studies\nand for language technology applications, including Statistical Machine\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\nmultilingual classification and clustering, and many more. In this reference\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\nits size, and explain how it was produced and how to use it."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5290v1", 
    "title": "An introduction to the Europe Media Monitor family of applications", 
    "arxiv-id": "1309.5290v1", 
    "author": "Erik van der Goot", 
    "publish": "2013-09-20T15:03:58Z", 
    "summary": "Most large organizations have dedicated departments that monitor the media to\nkeep up-to-date with relevant developments and to keep an eye on how they are\nrepresented in the news. Part of this media monitoring work can be automated.\nIn the European Union with its 23 official languages, it is particularly\nimportant to cover media reports in many languages in order to capture the\ncomplementary news content published in the different countries. It is also\nimportant to be able to access the news content across languages and to merge\nthe extracted information. We present here the four publicly accessible systems\nof the Europe Media Monitor (EMM) family of applications, which cover between\n19 and 50 languages (see http://press.jrc.it/overview.html). We give an\noverview of their functionality and discuss some of the implications of the\nfact that they cover quite so many languages. We discuss design issues\nnecessary to be able to achieve this high multilinguality, as well as the\nbenefits of this multilinguality."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5391v1", 
    "title": "Even the Abstract have Colour: Consensus in Word-Colour Associations", 
    "arxiv-id": "1309.5391v1", 
    "author": "Saif M. Mohammad", 
    "publish": "2013-09-20T21:06:46Z", 
    "summary": "Colour is a key component in the successful dissemination of information.\nSince many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complemented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. A word-choice question was used to obtain sense-level\nannotations and to ensure data quality. We focus especially on abstract\nconcepts and emotions to show that even they tend to have strong colour\nassociations. Thus, using the right colours can not only improve semantic\ncoherence, but also inspire the desired emotional response."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5652v1", 
    "title": "LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual", 
    "arxiv-id": "1309.5652v1", 
    "author": "Ryan Roth", 
    "publish": "2013-09-22T21:09:07Z", 
    "summary": "The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\nfor natural language processing (NLP) research. Among these are a number of\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\nsingle collection of annotated documents. NLP research, however, usually\nrequires multiple data sets for the purposes of training models, developing\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\ncorpora used into the required data sets (divisions). This document details a\nset of rules that have been defined to enable consistent divisions for old and\nnew Arabic treebanks (ATB) and related corpora."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5657v1", 
    "title": "A Hybrid Algorithm for Matching Arabic Names", 
    "arxiv-id": "1309.5657v1", 
    "author": "T. El-Shishtawy", 
    "publish": "2013-09-22T22:06:26Z", 
    "summary": "In this paper, a new hybrid algorithm which combines both of token-based and\ncharacter-based approaches is presented. The basic Levenshtein approach has\nbeen extended to token-based distance metric. The distance metric is enhanced\nto set the proper granularity level behavior of the algorithm. It smoothly maps\na threshold of misspellings differences at the character level, and the\nimportance of token level errors in terms of token's position and frequency.\nUsing a large Arabic dataset, the experimental results show that the proposed\nalgorithm overcomes successfully many types of errors such as: typographical\nerrors, omission or insertion of middle name components, omission of\nnon-significant popular name components, and different writing styles character\nvariations. When compared the results with other classical algorithms, using\nthe same dataset, the proposed algorithm was found to increase the minimum\nsuccess level of best tested algorithms, while achieving higher upper limits ."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5843v1", 
    "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", 
    "arxiv-id": "1309.5843v1", 
    "author": "Marco Turchi", 
    "publish": "2013-09-23T15:26:09Z", 
    "summary": "Assigning a positive or negative score to a word out of context (i.e. a\nword's prior polarity) is a challenging task for sentiment analysis. In the\nliterature, various approaches based on SentiWordNet have been proposed. In\nthis paper, we compare the most often used techniques together with newly\nproposed ones and incorporate all of them in a learning framework to see\nwhether blending them can further improve the estimation of prior polarity\nscores. Using two different versions of SentiWordNet and testing regression and\nclassification models across tasks and datasets, our learning approach\nconsistently outperforms the single metrics, providing a new state-of-the-art\napproach in computing words' prior polarity for sentiment analysis. We conclude\nour investigation showing interesting biases in calculated prior polarity\nscores when word Part of Speech and annotator gender are considered."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5909v1", 
    "title": "From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels   and Fairy Tales", 
    "arxiv-id": "1309.5909v1", 
    "author": "Saif Mohammad", 
    "publish": "2013-09-23T18:43:56Z", 
    "summary": "Today we have access to unprecedented amounts of literary texts. However,\nsearch still relies heavily on key words. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in both individual books and across very large collections. We\nintroduce the concept of emotion word density, and using the Brothers Grimm\nfairy tales as example, we show how collections of text can be organized for\nbetter search. Using the Google Books Corpus we show how to determine an\nentity's emotion associations from co-occurring words. Finally, we compare\nemotion words in fairy tales and novels, to show that fairy tales have a much\nwider range of emotion word densities than novels."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.5942v1", 
    "title": "Colourful Language: Measuring Word-Colour Associations", 
    "arxiv-id": "1309.5942v1", 
    "author": "Saif Mohammad", 
    "publish": "2013-09-20T21:10:56Z", 
    "summary": "Since many real-world concepts are associated with colour, for example danger\nwith red, linguistic information is often complimented with the use of\nappropriate colours in information visualization and product marketing. Yet,\nthere is no comprehensive resource that captures concept-colour associations.\nWe present a method to create a large word-colour association lexicon by\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\nthat even though they cannot be physically visualized, they too tend to have\nstrong colour associations. Finally, we show how word-colour associations\nmanifest themselves in language, and quantify usefulness of co-occurrence and\npolarity cues in automatically detecting colour associations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6162v1", 
    "title": "JRC-Names: A freely available, highly multilingual named entity resource", 
    "arxiv-id": "1309.6162v1", 
    "author": "Erik van der Goot", 
    "publish": "2013-09-24T14:09:53Z", 
    "summary": "This paper describes a new, freely available, highly multilingual named\nentity resource for person and organisation names that has been compiled over\nseven years of large-scale multilingual news analysis combined with Wikipedia\nmining, resulting in 205,000 per-son and organisation names plus about the same\nnumber of spelling variants written in over 20 different scripts and in many\nmore languages. This resource, produced as part of the Europe Media Monitor\nactivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number\nof purposes. These include improving name search in databases or on the\ninternet, seeding machine learning systems to learn named entity recognition\nrules, improve machine translation results, and more. We describe here how this\nresource was created; we give statistics on its current size; we address the\nissue of morphological inflection; and we give details regarding its\nfunctionality. Updates to this resource will be made available daily."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6185v1", 
    "title": "Acronym recognition and processing in 22 languages", 
    "arxiv-id": "1309.6185v1", 
    "author": "Hristo Tanev", 
    "publish": "2013-09-24T14:41:33Z", 
    "summary": "We are presenting work on recognising acronyms of the form Long-Form\n(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\narticles in twenty-two languages, as part of our more general effort to\nrecognise entities and their variants in news text and to use them for the\nautomatic analysis of the news, including the linking of related news across\nlanguages. We show how the acronym recognition patterns, initially developed\nfor medical terms, needed to be adapted to the more general news domain and we\npresent evaluation results. We describe our effort to automatically merge the\nnumerous long-form variants referring to the same short-form, while keeping\nnon-related long-forms separate. Finally, we provide extensive statistics on\nthe frequency and the distribution of short-form/long-form pairs across\nlanguages."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6202v1", 
    "title": "Sentiment Analysis in the News", 
    "arxiv-id": "1309.6202v1", 
    "author": "Jenya Belyaeva", 
    "publish": "2013-09-24T15:11:43Z", 
    "summary": "Recent years have brought a significant growth in the volume of research in\nsentiment analysis, mostly on highly subjective text types (movie or product\nreviews). The main difference these texts have with news articles is that their\ntarget is clearly defined and unique across the text. Following different\nannotation efforts and the analysis of the issues encountered, we realised that\nnews opinion mining is different from that of other text types. We identified\nthree subtasks that need to be addressed: definition of the target; separation\nof the good and bad news content from the good and bad sentiment expressed on\nthe target; and analysis of clearly marked opinion that is expressed\nexplicitly, not needing interpretation or the use of world knowledge.\nFurthermore, we distinguish three different possible views on newspaper\narticles - author, reader and text, which have to be addressed differently at\nthe time of analysing sentiment. Given these definitions, we present work on\nmining opinions about entities in English language news, in which (a) we test\nthe relative suitability of various sentiment dictionaries and (b) we attempt\nto separate positive or negative opinion from good or bad news. In the\nexperiments described here, we tested whether or not subject domain-defining\nvocabulary should be ignored. Results showed that this idea is more appropriate\nin the context of news opinion mining and that the approaches taking this into\nconsideration produce a better performance."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6347v1", 
    "title": "Tracking Sentiment in Mail: How Genders Differ on Emotional Axes", 
    "arxiv-id": "1309.6347v1", 
    "author": "Yang", 
    "publish": "2013-09-24T21:14:45Z", 
    "summary": "With the widespread use of email, we now have access to unprecedented amounts\nof text that we ourselves have written. In this paper, we show how sentiment\nanalysis can be used in tandem with effective visualizations to quantify and\ntrack emotions in many types of mail. We create a large word--emotion\nassociation lexicon by crowdsourcing, and use it to compare emotions in love\nletters, hate mail, and suicide notes. We show that there are marked\ndifferences across genders in how they use emotion words in work-place email.\nFor example, women use many words from the joy--sadness axis, whereas men\nprefer terms from the fear--trust axis. Finally, we show visualizations that\ncan help people track emotions in their emails."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6352v1", 
    "title": "Using Nuances of Emotion to Identify Personality", 
    "arxiv-id": "1309.6352v1", 
    "author": "Svetlana Kiritchenko", 
    "publish": "2013-09-24T21:21:02Z", 
    "summary": "Past work on personality detection has shown that frequency of lexical\ncategories such as first person pronouns, past tense verbs, and sentiment words\nhave significant correlations with personality traits. In this paper, for the\nfirst time, we show that fine affect (emotion) categories such as that of\nexcitement, guilt, yearning, and admiration are significant indicators of\npersonality. Additionally, we perform experiments to show that the gains\nprovided by the fine affect categories are not obtained by using coarse affect\ncategories alone or with specificity features alone. We employ these features\nin five SVM classifiers for detecting five personality traits through essays.\nWe find that the use of fine emotion features leads to statistically\nsignificant improvement over a competitive baseline, whereas the use of coarse\naffect and specificity features does not."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.6722v1", 
    "title": "Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern   Generation", 
    "arxiv-id": "1309.6722v1", 
    "author": "Liu Ting", 
    "publish": "2013-09-26T05:18:12Z", 
    "summary": "This paper focuses on the automatic extraction of domain-specific sentiment\nword (DSSW), which is a fundamental subtask of sentiment analysis. Most\nprevious work utilizes manual patterns for this task. However, the performance\nof those methods highly relies on the labelled patterns or selected seeds. In\norder to overcome the above problem, this paper presents an automatic framework\nto detect large-scale domain-specific patterns for DSSW extraction. To this\nend, sentiment seeds are extracted from massive dataset of user comments.\nSubsequently, these sentiment seeds are expanded by synonyms using a\nbootstrapping mechanism. Simultaneously, a synonymy graph is built and the\ngraph propagation algorithm is applied on the built synonymy graph. Afterwards,\nsyntactic and sequential relations between target words and high-ranked\nsentiment words are extracted automatically to construct large-scale patterns,\nwhich are further used to extracte DSSWs. The experimental results in three\ndomains reveal the effectiveness of our method."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1309.7312v1", 
    "title": "Development and Transcription of Assamese Speech Corpus", 
    "arxiv-id": "1309.7312v1", 
    "author": "Mancha Jyoti Malakar", 
    "publish": "2013-09-27T17:54:14Z", 
    "summary": "A balanced speech corpus is the basic need for any speech processing task. In\nthis report we describe our effort on development of Assamese speech corpus. We\nmainly focused on some issues and challenges faced during development of the\ncorpus. Being a less computationally aware language, this is the first effort\nto develop speech corpus for Assamese. As corpus development is an ongoing\nprocess, in this paper we report only the initial task."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1311.0833v1", 
    "title": "A Comparative Study on Linguistic Feature Selection in Sentiment   Polarity Classification", 
    "arxiv-id": "1311.0833v1", 
    "author": "Zitao Liu", 
    "publish": "2013-11-04T20:11:35Z", 
    "summary": "Sentiment polarity classification is perhaps the most widely studied topic.\nIt classifies an opinionated document as expressing a positive or negative\nopinion. In this paper, using movie review dataset, we perform a comparative\nstudy with different single kind linguistic features and the combinations of\nthese features. We find that the classic topic-based classifier(Naive Bayes and\nSupport Vector Machine) do not perform as well on sentiment polarity\nclassification. And we find that with some combination of different linguistic\nfeatures, the classification accuracy can be boosted a lot. We give some\nreasonable explanations about these boosting outcomes."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1311.1169v1", 
    "title": "Using Robust PCA to estimate regional characteristics of language use   from geo-tagged Twitter messages", 
    "arxiv-id": "1311.1169v1", 
    "author": "G\u00e1bor Vattay", 
    "publish": "2013-11-05T19:31:33Z", 
    "summary": "Principal component analysis (PCA) and related techniques have been\nsuccessfully employed in natural language processing. Text mining applications\nin the age of the online social media (OSM) face new challenges due to\nproperties specific to these use cases (e.g. spelling issues specific to texts\nposted by users, the presence of spammers and bots, service announcements,\netc.). In this paper, we employ a Robust PCA technique to separate typical\noutliers and highly localized topics from the low-dimensional structure present\nin language use in online social networks. Our focus is on identifying\ngeospatial features among the messages posted by the users of the Twitter\nmicroblogging service. Using a dataset which consists of over 200 million\ngeolocated tweets collected over the course of a year, we investigate whether\nthe information present in word usage frequencies can be used to identify\nregional features of language use and topics of interest. Using the PCA pursuit\nmethod, we are able to identify important low-dimensional features, which\nconstitute smoothly varying functions of the geographic location."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1311.1194v1", 
    "title": "Identifying Purpose Behind Electoral Tweets", 
    "arxiv-id": "1311.1194v1", 
    "author": "Joel Martin", 
    "publish": "2013-11-05T20:55:23Z", 
    "summary": "Tweets pertaining to a single event, such as a national election, can number\nin the hundreds of millions. Automatically analyzing them is beneficial in many\ndownstream natural language applications such as question answering and\nsummarization. In this paper, we propose a new task: identifying the purpose\nbehind electoral tweets--why do people post election-oriented tweets? We show\nthat identifying purpose is correlated with the related phenomenon of sentiment\nand emotion detection, but yet significantly different. Detecting purpose has a\nnumber of applications including detecting the mood of the electorate,\nestimating the popularity of policies, identifying key issues of contention,\nand predicting the course of events. We create a large dataset of electoral\ntweets and annotate a few thousand tweets for purpose. We develop a system that\nautomatically classifies electoral tweets as per their purpose, obtaining an\naccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class\ntask (both accuracies well above the most-frequent-class baseline). Finally, we\nshow that resources developed for emotion detection are also helpful for\ndetecting purpose."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1311.2978v1", 
    "title": "Authorship Attribution Using Word Network Features", 
    "arxiv-id": "1311.2978v1", 
    "author": "Rada Mihalcea", 
    "publish": "2013-11-12T23:11:40Z", 
    "summary": "In this paper, we explore a set of novel features for authorship attribution\nof documents. These features are derived from a word network representation of\nnatural language text. As has been noted in previous studies, natural language\ntends to show complex network structure at word level, with low degrees of\nseparation and scale-free (power law) degree distribution. There has also been\nwork on authorship attribution that incorporates ideas from complex networks.\nThe goal of our paper is to explore properties of these complex networks that\nare suitable as features for machine-learning-based authorship attribution of\ndocuments. We performed experiments on three different datasets, and obtained\npromising results."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2013.4415", 
    "link": "http://arxiv.org/pdf/1311.3011v2", 
    "title": "Cornell SPF: Cornell Semantic Parsing Framework", 
    "arxiv-id": "1311.3011v2", 
    "author": "Yoav Artzi", 
    "publish": "2013-11-13T03:58:38Z", 
    "summary": "The Cornell Semantic Parsing Framework (SPF) is a learning and inference\nframework for mapping natural language to formal representation of its meaning."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2502", 
    "link": "http://arxiv.org/pdf/1311.3961v1", 
    "title": "HEVAL: Yet Another Human Evaluation Metric", 
    "arxiv-id": "1311.3961v1", 
    "author": "Ajai Kumar", 
    "publish": "2013-11-15T19:45:25Z", 
    "summary": "Machine translation evaluation is a very important activity in machine\ntranslation development. Automatic evaluation metrics proposed in literature\nare inadequate as they require one or more human reference translations to\ncompare them with output produced by machine translation. This does not always\ngive accurate results as a text can have several different translations. Human\nevaluation metrics, on the other hand, lacks inter-annotator agreement and\nrepeatability. In this paper we have proposed a new human evaluation metric\nwhich addresses these issues. Moreover this metric also provides solid grounds\nfor making sound assumptions on the quality of the text produced by a machine\ntranslation."
},{
    "category": "cs.CL", 
    "doi": "10.5120/14217-2463", 
    "link": "http://arxiv.org/pdf/1311.5836v1", 
    "title": "Automatic Ranking of MT Outputs using Approximations", 
    "arxiv-id": "1311.5836v1", 
    "author": "Iti Mathur", 
    "publish": "2013-11-22T18:13:06Z", 
    "summary": "Since long, research on machine translation has been ongoing. Still, we do\nnot get good translations from MT engines so developed. Manual ranking of these\noutputs tends to be very time consuming and expensive. Identifying which one is\nbetter or worse than the others is a very taxing task. In this paper, we show\nan approach which can provide automatic ranks to MT outputs (translations)\ntaken from different MT Engines and which is based on N-gram approximations. We\nprovide a solution where no human intervention is required for ranking systems.\nFurther we also show the evaluations of our results which show equivalent\nresults as that of human ranking."
},{
    "category": "cs.CL", 
    "doi": "10.5120/14217-2463", 
    "link": "http://arxiv.org/pdf/1311.6045v1", 
    "title": "Build Electronic Arabic Lexicon", 
    "arxiv-id": "1311.6045v1", 
    "author": "Adel Al-Nasrawi", 
    "publish": "2013-11-23T20:10:24Z", 
    "summary": "There are many known Arabic lexicons organized on different ways, each of\nthem has a different number of Arabic words according to its organization way.\nThis paper has used mathematical relations to count a number of Arabic words,\nwhich proofs the number of Arabic words presented by Al Farahidy. The paper\nalso presents new way to build an electronic Arabic lexicon by using a hash\nfunction that converts each word (as input) to correspond a unique integer\nnumber (as output), these integer numbers will be used as an index to a lexicon\nentry."
},{
    "category": "cs.CL", 
    "doi": "10.5120/14217-2463", 
    "link": "http://arxiv.org/pdf/1311.6063v4", 
    "title": "A Short Introduction to NILE", 
    "arxiv-id": "1311.6063v4", 
    "author": "Tianxi Cai", 
    "publish": "2013-11-23T22:39:52Z", 
    "summary": "In this paper, we briefly introduce the Narrative Information Linear\nExtraction (NILE) system, a natural language processing library for clinical\nnarratives. NILE is an experiment of our ideas on efficient and effective\nmedical language processing. We introduce the overall design of NILE and its\nmajor components, and show the performance of it in real projects."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3786", 
    "link": "http://arxiv.org/pdf/1402.0563v1", 
    "title": "Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine   Translation", 
    "arxiv-id": "1402.0563v1", 
    "author": "Rafael E. Banchs", 
    "publish": "2014-02-04T01:34:56Z", 
    "summary": "Although, Chinese and Spanish are two of the most spoken languages in the\nworld, not much research has been done in machine translation for this language\npair. This paper focuses on investigating the state-of-the-art of\nChinese-to-Spanish statistical machine translation (SMT), which nowadays is one\nof the most popular approaches to machine translation. For this purpose, we\nreport details of the available parallel corpus which are Basic Traveller\nExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we\nconduct experimental work with the largest of these three corpora to explore\nalternative SMT strategies by means of using a pivot language. Three\nalternatives are considered for pivoting: cascading, pseudo-corpus and\ntriangulation. As pivot language, we use either English, Arabic or French.\nResults show that, for a phrase-based SMT system, English is the best pivot\nlanguage between Chinese and Spanish. We propose a system output combination\nusing the pivot strategies which is capable of outperforming the direct\ntranslation strategy. The main objective of this work is motivating and\ninvolving the research community to work in this important pair of languages\ngiven their demographic impact."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3892", 
    "link": "http://arxiv.org/pdf/1402.0578v1", 
    "title": "Natural Language Inference for Arabic Using Extended Tree Edit Distance   with Subtrees", 
    "arxiv-id": "1402.0578v1", 
    "author": "Allan Ramsay", 
    "publish": "2014-02-04T01:40:42Z", 
    "summary": "Many natural language processing (NLP) applications require the computation\nof similarities between pairs of syntactic or semantic trees. Many researchers\nhave used tree edit distance for this task, but this technique suffers from the\ndrawback that it deals with single node operations only. We have extended the\nstandard tree edit distance algorithm to deal with subtree transformation\noperations as well as single nodes. The extended algorithm with subtree\noperations, TED+ST, is more effective and flexible than the standard algorithm,\nespecially for applications that pay attention to relations among nodes (e.g.\nin linguistic trees, deleting a modifier subtree should be cheaper than the sum\nof deleting its components individually). We describe the use of TED+ST for\nchecking entailment between two Arabic text snippets. The preliminary results\nof using TED+ST were encouraging when compared with two string-based approaches\nand with the standard algorithm."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3940", 
    "link": "http://arxiv.org/pdf/1402.0586v1", 
    "title": "Topic Segmentation and Labeling in Asynchronous Conversations", 
    "arxiv-id": "1402.0586v1", 
    "author": "Raymond T Ng", 
    "publish": "2014-02-04T01:43:35Z", 
    "summary": "Topic segmentation and labeling is often considered a prerequisite for\nhigher-level conversation analysis and has been shown to be useful in many\nNatural Language Processing (NLP) applications. We present two new corpora of\nemail and blog conversations annotated with topics, and evaluate annotator\nreliability for the segmentation and labeling tasks in these asynchronous\nconversations. We propose a complete computational framework for topic\nsegmentation and labeling in asynchronous conversations. Our approach extends\nstate-of-the-art methods by considering a fine-grained structure of an\nasynchronous conversation, along with other conversational features by applying\nrecent graph-based methods for NLP. For topic segmentation, we propose two\nnovel unsupervised models that exploit the fine-grained conversational\nstructure, and a novel graph-theoretic supervised model that combines lexical,\nconversational and topic features. For topic labeling, we propose two novel\n(unsupervised) random walk models that respectively capture conversation\nspecific clues from two different sources: the leading sentences and the\nfine-grained conversational structure. Empirical evaluation shows that the\nsegmentation and the labeling performed by our best models beat the\nstate-of-the-art, and are highly correlated with human annotations."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3456", 
    "link": "http://arxiv.org/pdf/1402.2561v1", 
    "title": "The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance   a Bilingual Dictionary", 
    "arxiv-id": "1402.2561v1", 
    "author": "Roberto Navigli", 
    "publish": "2014-01-18T21:08:40Z", 
    "summary": "Bilingual machine-readable dictionaries are knowledge resources useful in\nmany automatic tasks. However, compared to monolingual computational lexicons\nlike WordNet, bilingual dictionaries typically provide a lower amount of\nstructured information, such as lexical and semantic relations, and often do\nnot cover the entire range of possible translations for a word of interest. In\nthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the\nautomated disambiguation of ambiguous translations in the lexical entries of a\nbilingual machine-readable dictionary. The dictionary is represented as a\ngraph, and cyclic patterns are sought in the graph to assign an appropriate\nsense tag to each translation in a lexical entry. Further, we use the\nalgorithms output to improve the quality of the dictionary itself, by\nsuggesting accurate solutions to structural problems such as misalignments,\npartial alignments and missing entries. Finally, we successfully apply CQC to\nthe task of synonym extraction."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3456", 
    "link": "http://arxiv.org/pdf/1402.2796v1", 
    "title": "PR2: A Language Independent Unsupervised Tool for Personality   Recognition from Text", 
    "arxiv-id": "1402.2796v1", 
    "author": "Massimo Poesio", 
    "publish": "2014-02-12T11:55:31Z", 
    "summary": "We present PR2, a personality recognition system available online, that\nperforms instance-based classification of Big5 personality types from\nunstructured text, using language-independent features. It has been tested on\nEnglish and Italian, achieving performances up to f=.68."
},{
    "category": "cs.CL", 
    "doi": "10.1142/S179384061240003X", 
    "link": "http://arxiv.org/pdf/1402.3040v1", 
    "title": "Event Structure of Transitive Verb: A MARVS perspective", 
    "arxiv-id": "1402.3040v1", 
    "author": "Chu-Ren Huang", 
    "publish": "2014-02-13T06:44:24Z", 
    "summary": "Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of\nthe representation of verbal semantics that is based on Mandarin Chinese data\n(Huang et al. 2000). In the MARVS theory, there are two different types of\nmodules: Event Structure Modules and Role Modules. There are also two sets of\nattributes: Event-Internal Attributes and Role-Internal Attributes, which are\nlinked to the Event Structure Module and the Role Module, respectively. In this\nstudy, we focus on four transitive verbs as chi1(eat), wan2(play),\nhuan4(change) and shao1(burn) and explore their event structures by the MARVS\ntheory."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10707-013-0197-8", 
    "link": "http://arxiv.org/pdf/1402.3371v1", 
    "title": "An evaluative baseline for geo-semantic relatedness and similarity", 
    "arxiv-id": "1402.3371v1", 
    "author": "David C. Wilson", 
    "publish": "2014-02-14T06:06:47Z", 
    "summary": "In geographic information science and semantics, the computation of semantic\nsimilarity is widely recognised as key to supporting a vast number of tasks in\ninformation integration and retrieval. By contrast, the role of geo-semantic\nrelatedness has been largely ignored. In natural language processing, semantic\nrelatedness is often confused with the more specific semantic similarity. In\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\nsemantic fields, and we compare it with geo-semantic similarity. We then\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\nnew open dataset designed to evaluate computational measures of geo-semantic\nrelatedness and similarity. This dataset is larger than existing datasets of\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\nby 203 human subjects. GeReSiD is available online and can be used as an\nevaluation baseline to determine empirically to what degree a given\ncomputational model approximates geo-semantic relatedness and similarity."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10707-013-0197-8", 
    "link": "http://arxiv.org/pdf/1402.3382v1", 
    "title": "Machine Learning of Phonologically Conditioned Noun Declensions For   Tamil Morphological Generators", 
    "arxiv-id": "1402.3382v1", 
    "author": "Dr. M. Ganesan", 
    "publish": "2014-02-14T06:46:44Z", 
    "summary": "This paper presents machine learning solutions to a practical problem of\nNatural Language Generation (NLG), particularly the word formation in\nagglutinative languages like Tamil, in a supervised manner. The morphological\ngenerator is an important component of Natural Language Processing in\nArtificial Intelligence. It generates word forms given a root and affixes. The\nmorphophonemic changes like addition, deletion, alternation etc., occur when\ntwo or more morphemes or words joined together. The Sandhi rules should be\nexplicitly specified in the rule based morphological analyzers and generators.\nIn machine learning framework, these rules can be learned automatically by the\nsystem from the training samples and subsequently be applied for new inputs. In\nthis paper we proposed the machine learning models which learn the\nmorphophonemic rules for noun declensions from the given training data. These\nmodels are trained to learn sandhi rules using various learning algorithms and\nthe performance of those algorithms are presented. From this we conclude that\nmachine learning of morphological processing such as word form generation can\nbe successfully learned in a supervised manner, without explicit description of\nrules. The performance of Decision trees and Bayesian machine learning\nalgorithms on noun declensions are discussed."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10707-013-0197-8", 
    "link": "http://arxiv.org/pdf/1402.4380v1", 
    "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text   Classification", 
    "arxiv-id": "1402.4380v1", 
    "author": "Owen Johnson", 
    "publish": "2014-02-18T16:02:05Z", 
    "summary": "A Verbal Autopsy is the record of an interview about the circumstances of an\nuncertified death. In developing countries, if a death occurs away from health\nfacilities, a field-worker interviews a relative of the deceased about the\ncircumstances of the death; this Verbal Autopsy can be reviewed off-site. We\nreport on a comparative study of the processes involved in Text Classification\napplied to classifying Cause of Death: feature value representation; machine\nlearning classification algorithms; and feature reduction strategies in order\nto identify the suitable approaches applicable to the classification of Verbal\nAutopsy text. We demonstrate that normalised term frequency and the standard\nTFiDF achieve comparable performance across a number of classifiers. The\nresults also show Support Vector Machine is superior to other classification\nalgorithms employed in this research. Finally, we demonstrate the effectiveness\nof employing a \"locally-semi-supervised\" feature reduction strategy in order to\nincrease performance accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10707-013-0197-8", 
    "link": "http://arxiv.org/pdf/1402.4678v1", 
    "title": "When Learners Surpass their Sources: Mathematical Modeling of Learning   from an Inconsistent Source", 
    "arxiv-id": "1402.4678v1", 
    "author": "Natalia Komarova", 
    "publish": "2014-02-18T02:18:10Z", 
    "summary": "We present a new algorithm to model and investigate the learning process of a\nlearner mastering a set of grammatical rules from an inconsistent source. The\ncompelling interest of human language acquisition is that the learning succeeds\nin virtually every case, despite the fact that the input data are formally\ninadequate to explain the success of learning. Our model explains how a learner\ncan successfully learn from or even surpass its imperfect source without\npossessing any additional biases or constraints about the types of patterns\nthat exist in the language. We use the data collected by Singleton and Newport\n(2004) on the performance of a 7-year boy Simon, who mastered the American Sign\nLanguage (ASL) by learning it from his parents, both of whom were imperfect\nspeakers of ASL. We show that the algorithm possesses a frequency-boosting\nproperty, whereby the frequency of the most common form of the source is\nincreased by the learner. We also explain several key features of Simon's ASL."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10707-013-0197-8", 
    "link": "http://arxiv.org/pdf/1402.6516v1", 
    "title": "Modelling the Lexicon in Unsupervised Part of Speech Induction", 
    "arxiv-id": "1402.6516v1", 
    "author": "Phil Blunsom", 
    "publish": "2014-02-26T12:37:04Z", 
    "summary": "Automatically inducing the syntactic part-of-speech categories for words in\ntext is a fundamental task in Computational Linguistics. While the performance\nof unsupervised tagging models has been slowly improving, current\nstate-of-the-art systems make the obviously incorrect assumption that all\ntokens of a given word type must share a single part-of-speech tag. This\none-tag-per-type heuristic counters the tendency of Hidden Markov Model based\ntaggers to over generate tags for a given word type. However, it is clearly\nincompatible with basic syntactic theory. In this paper we extend a\nstate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model\nof the lexicon. In doing so we are able to incorporate a soft bias towards\ninducing few tags per type. We develop a particle filter for drawing samples\nfrom the posterior of our model and present empirical results that show that\nour model is competitive with and faster than the state-of-the-art without\nmaking any unrealistic restrictions."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1402.6880v1", 
    "title": "It's distributions all the way down!: Second order changes in   statistical distributions also occur", 
    "arxiv-id": "1402.6880v1", 
    "author": "A. Gerow", 
    "publish": "2014-02-27T12:12:11Z", 
    "summary": "The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et\nals) message on distributions; it largely examines the first-order effects of\nhow a single, signature distribution can predict population behaviour,\nneglecting second-order effects involving distributional shifts, either between\nsignature distributions or within a given signature distribution. Indeed,\nBentley et al. themselves under-emphasise the potential richness of the latter,\nwithin-distribution effects."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1402.7265v1", 
    "title": "Semantics, Modelling, and the Problem of Representation of Meaning -- a   Brief Survey of Recent Literature", 
    "arxiv-id": "1402.7265v1", 
    "author": "Yarin Gal", 
    "publish": "2014-02-28T14:49:31Z", 
    "summary": "Over the past 50 years many have debated what representation should be used\nto capture the meaning of natural language utterances. Recently new needs of\nsuch representations have been raised in research. Here I survey some of the\ninteresting representations suggested to answer for these new needs."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.1847v1", 
    "title": "Evaluation and Ranking of Machine Translated Output in Hindi Language   using Precision and Recall Oriented Metrics", 
    "arxiv-id": "1404.1847v1", 
    "author": "Hemant Darbari", 
    "publish": "2014-04-07T16:45:42Z", 
    "summary": "Evaluation plays a crucial role in development of Machine translation\nsystems. In order to judge the quality of an existing MT system i.e. if the\ntranslated output is of human translation quality or not, various automatic\nmetrics exist. We here present the implementation results of different metrics\nwhen used on Hindi language along with their comparisons, illustrating how\neffective are these metrics on languages like Hindi (free word order language)."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.1872v1", 
    "title": "Int\u00e9gration des donn\u00e9es d'un lexique syntaxique dans un analyseur   syntaxique probabiliste", 
    "arxiv-id": "1404.1872v1", 
    "author": "Eric Laporte", 
    "publish": "2014-04-07T18:12:08Z", 
    "summary": "This article reports the evaluation of the integration of data from a\nsyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic\nparser. We show that by changing the set of labels for verbs and predicational\nnouns, we can improve the performance on French of a non-lexicalized\nprobabilistic parser."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.2071v1", 
    "title": "Extracting a bilingual semantic grammar from FrameNet-annotated corpora", 
    "arxiv-id": "1404.2071v1", 
    "author": "Normunds Gr\u016bz\u012btis", 
    "publish": "2014-04-08T10:08:22Z", 
    "summary": "We present the creation of an English-Swedish FrameNet-based grammar in\nGrammatical Framework. The aim of this research is to make existing framenets\ncomputationally accessible for multilingual natural language applications via a\ncommon semantic grammar API, and to facilitate the porting of such grammar to\nother languages. In this paper, we describe the abstract syntax of the semantic\ngrammar while focusing on its automatic extraction possibilities. We have\nextracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley\nFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The\nabstract syntax defines 769 frame-specific valence patterns that cover 77.8%\nexamples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.\nAs a side result, we provide a unified method for comparing semantic and\nsyntactic valence patterns across framenets."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.2188v1", 
    "title": "A Convolutional Neural Network for Modelling Sentences", 
    "arxiv-id": "1404.2188v1", 
    "author": "Phil Blunsom", 
    "publish": "2014-04-08T15:46:44Z", 
    "summary": "The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.2878v1", 
    "title": "Overview of Stemming Algorithms for Indian and Non-Indian Languages", 
    "arxiv-id": "1404.2878v1", 
    "author": "Suthar Sanket", 
    "publish": "2014-04-10T17:16:01Z", 
    "summary": "Stemming is a pre-processing step in Text Mining applications as well as a\nvery common requirement of Natural Language processing functions. Stemming is\nthe process for reducing inflected words to their stem. The main purpose of\nstemming is to reduce different grammatical forms / word forms of a word like\nits noun, adjective, verb, adverb etc. to its root form. Stemming is widely\nuses in Information Retrieval system and reduces the size of index files. We\ncan say that the goal of stemming is to reduce inflectional forms and sometimes\nderivationally related forms of a word to a common base form. In this paper we\nhave discussed different stemming algorithm for non-Indian and Indian language,\nmethods of stemming, accuracy and errors."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.3377v1", 
    "title": "A Generalized Language Model as the Combination of Skipped n-grams and   Modified Kneser-Ney Smoothing", 
    "arxiv-id": "1404.3377v1", 
    "author": "Steffen Staab", 
    "publish": "2014-04-13T12:39:41Z", 
    "summary": "We introduce a novel approach for building language models based on a\nsystematic, recursive exploration of skip n-gram models which are interpolated\nusing modified Kneser-Ney smoothing. Our approach generalizes language models\nas it contains the classical interpolation with lower order models as a special\ncase. In this paper we motivate, formalize and present our approach. In an\nextensive empirical experiment over English text corpora we demonstrate that\nour generalized language models lead to a substantial reduction of perplexity\nbetween 3.1% and 12.7% in comparison to traditional language models using\nmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over\nthree other languages and a domain specific corpus where we observed consistent\nimprovements. Finally, we also show that the strength of our approach lies in\nits ability to cope in particular with sparse training data. Using a very small\ntraining data set of only 736 KB text we yield improvements of even 25.7%\nreduction of perplexity."
},{
    "category": "cs.CL", 
    "doi": "10.1017/S0140525X13001763", 
    "link": "http://arxiv.org/pdf/1404.3759v1", 
    "title": "Meta-evaluation of comparability metrics using parallel corpora", 
    "arxiv-id": "1404.3759v1", 
    "author": "Anthony Hartley", 
    "publish": "2014-04-14T21:33:42Z", 
    "summary": "Metrics for measuring the comparability of corpora or texts need to be\ndeveloped and evaluated systematically. Applications based on a corpus, such as\ntraining Statistical MT systems in specialised narrow domains, require finding\na reasonable balance between the size of the corpus and its consistency, with\ncontrolled and benchmarked levels of comparability for any newly added\nsections. In this article we propose a method that can meta-evaluate\ncomparability metrics by calculating monolingual comparability scores\nseparately on the 'source' and 'target' sides of parallel corpora. The range of\nscores on the source side is then correlated (using Pearson's r coefficient)\nwith the range of 'target' scores; the higher the correlation - the more\nreliable is the metric. The intuition is that a good metric should yield the\nsame distance between different domains in different languages. Our method\ngives consistent results for the same metrics on different data sets, which\nindicates that it is reliable and can be used for metric comparison or for\noptimising settings of parametrised metrics."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.3992v1", 
    "title": "Assessing the Quality of MT Systems for Hindi to English Translation", 
    "arxiv-id": "1404.3992v1", 
    "author": "Ajai Kumar", 
    "publish": "2014-04-15T17:13:26Z", 
    "summary": "Evaluation plays a vital role in checking the quality of MT output. It is\ndone either manually or automatically. Manual evaluation is very time consuming\nand subjective, hence use of automatic metrics is done most of the times. This\npaper evaluates the translation quality of different MT Engines for\nHindi-English (Hindi data is provided as input and English is obtained as\noutput) using various automatic metrics like BLEU, METEOR etc. Further the\ncomparison automatic evaluation results with Human ranking have also been\ngiven."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.4314v1", 
    "title": "An Empirical Comparison of Parsing Methods for Stanford Dependencies", 
    "arxiv-id": "1404.4314v1", 
    "author": "Noah A. Smith", 
    "publish": "2014-04-16T17:06:35Z", 
    "summary": "Stanford typed dependencies are a widely desired representation of natural\nlanguage sentences, but parsing is one of the major computational bottlenecks\nin text analysis systems. In light of the evolving definition of the Stanford\ndependencies and developments in statistical dependency parsing algorithms,\nthis paper revisits the question of Cer et al. (2010): what is the tradeoff\nbetween accuracy and speed in obtaining Stanford dependencies in particular? We\nalso explore the effects of input representations on this tradeoff:\npart-of-speech tags, the novel use of an alternative dependency representation\nas input, and distributional representaions of words. We find that direct\ndependency parsing is a more viable solution than it was found to be in the\npast. An accompanying software release can be found at:\nhttp://www.ark.cs.cmu.edu/TBSD"
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.4572v1", 
    "title": "The First Parallel Multilingual Corpus of Persian: Toward a Persian   BLARK", 
    "arxiv-id": "1404.4572v1", 
    "author": "Behrooz Mahmoodi Bakhtiari", 
    "publish": "2014-04-17T16:22:40Z", 
    "summary": "In this article, we have introduced the first parallel corpus of Persian with\nmore than 10 other European languages. This article describes primary steps\ntoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,\nwe have proposed morphosyntactic specification of Persian based on\nEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article\nintroduces Persian Language, with emphasis on its orthography and\nmorphosyntactic features, then a new Part-of-Speech categorization and\northography for Persian in digital environments is proposed. Finally, the\ncorpus and related statistic will be analyzed."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.4641v1", 
    "title": "Multilingual Models for Compositional Distributed Semantics", 
    "arxiv-id": "1404.4641v1", 
    "author": "Phil Blunsom", 
    "publish": "2014-04-17T20:18:03Z", 
    "summary": "We present a novel technique for learning semantic representations, which\nextends the distributional hypothesis to multilingual data and joint-space\nembeddings. Our models leverage parallel data and learn to strongly align the\nembeddings of semantically equivalent sentences, while maintaining sufficient\ndistance between those of dissimilar sentences. The models do not rely on word\nalignments or any syntactic information and are successfully applied to a\nnumber of diverse languages. We extend our approach to learn semantic\nrepresentations at the document level, too. We evaluate these models on two\ncross-lingual document classification tasks, outperforming the prior state of\nthe art. Through qualitative analysis and the study of pivoting effects we\ndemonstrate that our representations are semantically plausible and can capture\nsemantic relationships across languages without parallel data."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.4714v1", 
    "title": "Radical-Enhanced Chinese Character Embedding", 
    "arxiv-id": "1404.4714v1", 
    "author": "Xiaolong Wang", 
    "publish": "2014-04-18T07:48:02Z", 
    "summary": "We present a method to leverage radical for learning Chinese character\nembedding. Radical is a semantic and phonetic component of Chinese character.\nIt plays an important role as characters with the same radical usually have\nsimilar semantic meaning and grammatical usage. However, existing Chinese\nprocessing algorithms typically regard word or character as the basic unit but\nignore the crucial radical information. In this paper, we fill this gap by\nleveraging radical for learning continuous representation of Chinese character.\nWe develop a dedicated neural architecture to effectively learn character\nembedding and apply it on Chinese character similarity judgement and Chinese\nword segmentation. Experiment results show that our radical-enhanced method\noutperforms existing embedding learning algorithms on both tasks."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15711-4629", 
    "link": "http://arxiv.org/pdf/1404.4740v1", 
    "title": "Challenges in Persian Electronic Text Analysis", 
    "arxiv-id": "1404.4740v1", 
    "author": "Mehdi Safaee Ghalati", 
    "publish": "2014-04-18T10:30:47Z", 
    "summary": "Farsi, also known as Persian, is the official language of Iran and Tajikistan\nand one of the two main languages spoken in Afghanistan. Farsi enjoys a unified\nArabic script as its writing system. In this paper we briefly introduce the\nwriting standards of Farsi and highlight problems one would face when analyzing\nFarsi electronic texts, especially during development of Farsi corpora\nregarding to transcription and encoding of Farsi e-texts. The pointes mentioned\nmay sounds easy but they are crucial when developing and processing written\ncorpora of Farsi."
},{
    "category": "cs.CL", 
    "doi": "10.1093/logcom/ext044", 
    "link": "http://arxiv.org/pdf/1404.5278v1", 
    "title": "The Frobenius anatomy of word meanings I: subject and object relative   pronouns", 
    "arxiv-id": "1404.5278v1", 
    "author": "Bob Coecke", 
    "publish": "2014-04-21T19:31:48Z", 
    "summary": "This paper develops a compositional vector-based semantics of subject and\nobject relative pronouns within a categorical framework. Frobenius algebras are\nused to formalise the operations required to model the semantics of relative\npronouns, including passing information between the relative clause and the\nmodified noun phrase, as well as copying, combining, and discarding parts of\nthe relative clause. We develop two instantiations of the abstract semantics,\none based on a truth-theoretic approach and one based on corpus statistics."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-54906-9_16", 
    "link": "http://arxiv.org/pdf/1404.5357v1", 
    "title": "Morphological Analysis of the Bishnupriya Manipuri Language using Finite   State Transducers", 
    "arxiv-id": "1404.5357v1", 
    "author": "Smriti Kumar Sinha", 
    "publish": "2014-04-22T00:17:26Z", 
    "summary": "In this work we present a morphological analysis of Bishnupriya Manipuri\nlanguage, an Indo-Aryan language spoken in the north eastern India. As of now,\nthere is no computational work available for the language. Finite state\nmorphology is one of the successful approaches applied in a wide variety of\nlanguages over the year. Therefore we adapted the finite state approach to\nanalyse morphology of the Bishnupriya Manipuri language."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-54906-9_16", 
    "link": "http://arxiv.org/pdf/1404.5367v1", 
    "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", 
    "arxiv-id": "1404.5367v1", 
    "author": "Andrew McCallum", 
    "publish": "2014-04-22T02:12:06Z", 
    "summary": "Most state-of-the-art approaches for named-entity recognition (NER) use semi\nsupervised information in the form of word clusters and lexicons. Recently\nneural network-based language models have been explored, as they as a byproduct\ngenerate highly informative vector representations for words, known as word\nembeddings. In this paper we present two contributions: a new form of learning\nword embeddings that can leverage information from relevant lexicons to improve\nthe representations, and the first system to use neural word embeddings to\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\nCoNLL 2003---significantly better than any previous system trained on public\ndata, and matching a system employing massive private industrial query-log\ndata."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-54906-9_16", 
    "link": "http://arxiv.org/pdf/1404.6312v2", 
    "title": "Reconstructing Native Language Typology from Foreign Language Usage", 
    "arxiv-id": "1404.6312v2", 
    "author": "Boris Katz", 
    "publish": "2014-04-25T04:10:57Z", 
    "summary": "Linguists and psychologists have long been studying cross-linguistic\ntransfer, the influence of native language properties on linguistic performance\nin a foreign language. In this work we provide empirical evidence for this\nprocess in the form of a strong correlation between language similarities\nderived from structural features in English as Second Language (ESL) texts and\nequivalent similarities obtained from the typological features of the native\nlanguages. We leverage this finding to recover native language typological\nsimilarity structure directly from ESL text, and perform prediction of\ntypological features in an unsupervised fashion with respect to the target\nlanguages. Our method achieves 72.2% accuracy on the typology prediction task,\na result that is highly competitive with equivalent methods that rely on\ntypological resources."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-54906-9_16", 
    "link": "http://arxiv.org/pdf/1404.7296v1", 
    "title": "A Deep Architecture for Semantic Parsing", 
    "arxiv-id": "1404.7296v1", 
    "author": "Karl Moritz Hermann", 
    "publish": "2014-04-29T10:10:13Z", 
    "summary": "Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.0032v1", 
    "title": "Comparing and Combining Sentiment Analysis Methods", 
    "arxiv-id": "1406.0032v1", 
    "author": "Meeyoung Cha", 
    "publish": "2014-05-30T22:47:49Z", 
    "summary": "Several messages express opinions about events, products, and services,\npolitical views or even their author's emotional state and mood. Sentiment\nanalysis has been used in several applications including analysis of the\nrepercussions of events in social networks, analysis of opinions about products\nand services, and simply to better understand aspects of social communication\nin Online Social Networks (OSNs). There are multiple methods for measuring\nsentiments, including lexical-based approaches and supervised machine learning\nmethods. Despite the wide use and popularity of some methods, it is unclear\nwhich method is better for identifying the polarity (i.e., positive or\nnegative) of a message as the current literature does not provide a method of\ncomparison among existing methods. Such a comparison is crucial for\nunderstanding the potential limitations, advantages, and disadvantages of\npopular methods in analyzing the content of OSNs messages. Our study aims at\nfilling this gap by presenting comparisons of eight popular sentiment analysis\nmethods in terms of coverage (i.e., the fraction of messages whose sentiment is\nidentified) and agreement (i.e., the fraction of identified sentiments that are\nin tune with ground truth). We develop a new method that combines existing\napproaches, providing the best coverage results and competitive agreement. We\nalso present a free Web service called iFeel, which provides an open API for\naccessing and comparing results across different sentiment methods for a given\ntext."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.1203v1", 
    "title": "A Semantic Approach to Summarization", 
    "arxiv-id": "1406.1203v1", 
    "author": "Ashudeep Singh", 
    "publish": "2014-06-04T20:22:30Z", 
    "summary": "Sentence extraction based summarization methods has some limitations as it\ndoesn't go into the semantics of the document. Also, it lacks the capability of\nsentence generation which is intuitive to humans. Here we present a novel\nmethod to summarize text documents taking the process to semantic levels with\nthe use of WordNet and other resources, and using a technique for sentence\ngeneration. We involve semantic role labeling to get the semantic\nrepresentation of text and use of segmentation to form clusters of the related\npieces of text. Picking out the centroids and sentence generation completes the\ntask. We evaluate our system against human composed summaries and also present\nan evaluation done by humans to measure the quality attributes of our\nsummaries."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.1241v1", 
    "title": "The Best Templates Match Technique For Example Based Machine Translation", 
    "arxiv-id": "1406.1241v1", 
    "author": "A. El-Sammak", 
    "publish": "2014-06-04T23:56:08Z", 
    "summary": "It has been proved that large scale realistic Knowledge Based Machine\nTranslation applications require acquisition of huge knowledge about language\nand about the world. This knowledge is encoded in computational grammars,\nlexicons and domain models. Another approach which avoids the need for\ncollecting and analyzing massive knowledge, is the Example Based approach,\nwhich is the topic of this paper. We show through the paper that using Example\nBased in its native form is not suitable for translating into Arabic. Therefore\na modification to the basic approach is presented to improve the accuracy of\nthe translation process. The basic idea of the new approach is to improve the\ntechnique by which template-based approaches select the appropriate templates."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.1280v1", 
    "title": "Basis Identification for Automatic Creation of Pronunciation Lexicon for   Proper Names", 
    "arxiv-id": "1406.1280v1", 
    "author": "M Laxminarayana", 
    "publish": "2014-06-05T07:07:44Z", 
    "summary": "Development of a proper names pronunciation lexicon is usually a manual\neffort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,\nin literature, are usually rule based and work best for non-proper names in a\nparticular language. Proper names are foreign to a G2P module. We follow an\noptimization approach to enable automatic construction of proper names\npronunciation lexicon. The idea is to construct a small orthogonal set of words\n(basis) which can span the set of names in a given database. We propose two\nalgorithms for the construction of this basis. The transcription lexicon of all\nthe proper names in a database can be produced by the manual transcription of\nonly the small set of basis words. We first construct a cost function and show\nthat the minimization of the cost function results in a basis. We derive\nconditions for convergence of this cost function and validate them\nexperimentally on a very large proper name database. Experiments show the\ntranscription can be achieved by transcribing a set of small number of basis\nwords. The algorithms proposed are generic and independent of language; however\nperformance is better if the proper names have same origin, namely, same\nlanguage or geographical region."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.1870v1", 
    "title": "Toward verbalizing ontologies in isiZulu", 
    "arxiv-id": "1406.1870v1", 
    "author": "Langa Khumalo", 
    "publish": "2014-06-07T07:41:54Z", 
    "summary": "IsiZulu is one of the eleven official languages of South Africa and roughly\nhalf the population can speak it. It is the first (home) language for over 10\nmillion people in South Africa. Only a few computational resources exist for\nisiZulu and its related Nguni languages, yet the imperative for tool\ndevelopment exists. We focus on natural language generation, and the grammar\noptions and preferences in particular, which will inform verbalization of\nknowledge representation languages and could contribute to machine translation.\nThe verbalization pattern specification shows that the grammar rules are\nelaborate and there are several options of which one may have preference. We\ndevised verbalization patterns for subsumption, basic disjointness, existential\nand universal quantification, and conjunction. This was evaluated in a survey\namong linguists and non-linguists. Some differences between linguists and\nnon-linguists can be observed, with the former much more in agreement, and\npreferences depend on the overall structure of the sentence, such as singular\nfor subsumption and plural in other cases."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2512938.2512951", 
    "link": "http://arxiv.org/pdf/1406.2204v1", 
    "title": "How Easy is it to Learn a Controlled Natural Language for Building a   Knowledge Base?", 
    "arxiv-id": "1406.2204v1", 
    "author": "Allan Third", 
    "publish": "2014-06-09T14:54:22Z", 
    "summary": "Recent developments in controlled natural language editors for knowledge\nengineering (KE) have given rise to expectations that they will make KE tasks\nmore accessible and perhaps even enable non-engineers to build knowledge bases.\nThis exploratory research focussed on novices and experts in knowledge\nengineering during their attempts to learn a controlled natural language (CNL)\nknown as OWL Simplified English and use it to build a small knowledge base.\nParticipants' behaviours during the task were observed through eye-tracking and\nscreen recordings. This was an attempt at a more ambitious user study than in\nprevious research because we used a naturally occurring text as the source of\ndomain knowledge, and left them without guidance on which information to\nselect, or how to encode it. We have identified a number of skills\n(competencies) required for this difficult task and key problems that authors\nface."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10223-8_15", 
    "link": "http://arxiv.org/pdf/1406.2400v1", 
    "title": "Controlled Natural Language Generation from a Multilingual   FrameNet-based Grammar", 
    "arxiv-id": "1406.2400v1", 
    "author": "Normunds Gr\u016bz\u012btis", 
    "publish": "2014-06-10T01:01:48Z", 
    "summary": "This paper presents a currently bilingual but potentially multilingual\nFrameNet-based grammar library implemented in Grammatical Framework. The\ncontribution of this paper is two-fold. First, it offers a methodological\napproach to automatically generate the grammar based on semantico-syntactic\nvalence patterns extracted from FrameNet-annotated corpora. Second, it provides\na proof of concept for two use cases illustrating how the acquired multilingual\ngrammar can be exploited in different CNL applications in the domains of arts\nand tourism."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10223-8_15", 
    "link": "http://arxiv.org/pdf/1406.2903v2", 
    "title": "A Brief State of the Art for Ontology Authoring", 
    "arxiv-id": "1406.2903v2", 
    "author": "Brian Davis", 
    "publish": "2014-06-11T13:47:22Z", 
    "summary": "One of the main challenges for building the Semantic web is Ontology\nAuthoring. Controlled Natural Languages CNLs offer a user friendly means for\nnon-experts to author ontologies. This paper provides a snapshot of the\nstate-of-the-art for the core CNLs for ontology authoring and reviews their\nrespective evaluations."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10223-8_15", 
    "link": "http://arxiv.org/pdf/1406.3460v1", 
    "title": "Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG", 
    "arxiv-id": "1406.3460v1", 
    "author": "Karolina Suchowolec", 
    "publish": "2014-06-13T09:23:53Z", 
    "summary": "Controlled natural languages for industrial application are often regarded as\na response to the challenges of translation and multilingual communication.\nThis paper presents a quite different approach taken by Koenig & Bauer AG,\nwhere the main goal was the improvement of the authoring process for technical\ndocumentation. Most importantly, this paper explores the notion of a controlled\nlanguage and demonstrates how style guides can emerge from non-linguistic\nconsiderations. Moreover, it shows the transition from loose language\nrecommendations into precise and prescriptive rules and investigates whether\nsuch rules can be regarded as a full-fledged controlled language."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10223-8_15", 
    "link": "http://arxiv.org/pdf/1406.3676v3", 
    "title": "Question Answering with Subgraph Embeddings", 
    "arxiv-id": "1406.3676v3", 
    "author": "Jason Weston", 
    "publish": "2014-06-14T03:00:23Z", 
    "summary": "This paper presents a system which learns to answer questions on a broad\nrange of topics from a knowledge base using few hand-crafted features. Our\nmodel learns low-dimensional embeddings of words and knowledge base\nconstituents; these representations are used to score natural language\nquestions against candidate answers. Training our system using pairs of\nquestions and structured representations of their answers, and pairs of\nquestion paraphrases, yields competitive results on a competitive benchmark of\nthe literature."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.3969v1", 
    "title": "Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine   Translation", 
    "arxiv-id": "1406.3969v1", 
    "author": "Kalyani U. R. S", 
    "publish": "2014-06-16T10:59:03Z", 
    "summary": "In todays digital world automated Machine Translation of one language to\nanother has covered a long way to achieve different kinds of success stories.\nWhereas Babel Fish supports a good number of foreign languages and only Hindi\nfrom Indian languages, the Google Translator takes care of about 10 Indian\nlanguages. Though most of the Automated Machine Translation Systems are doing\nwell but handling Indian languages needs a major care while handling the local\nproverbs/ idioms. Most of the Machine Translation system follows the direct\ntranslation approach while translating one Indian language to other. Our\nresearch at KMIT R&D Lab found that handling the local proverbs/idioms is not\ngiven enough attention by the earlier research work. This paper focuses on two\nof the majorly spoken Indian languages Marathi and Telugu, and translation\nbetween them. Handling proverbs and idioms of both the languages have been\ngiven a special care, and the research outcome shows a significant achievement\nin this direction."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.3976v1", 
    "title": "Handling non-compositionality in multilingual CNLs", 
    "arxiv-id": "1406.3976v1", 
    "author": "Prasanth Kolachina", 
    "publish": "2014-06-16T11:30:51Z", 
    "summary": "In this paper, we describe methods for handling multilingual\nnon-compositional constructions in the framework of GF. We specifically look at\nmethods to detect and extract non-compositional phrases from parallel texts and\npropose methods to handle such constructions in GF grammars. We expect that the\nmethods to handle non-compositional constructions will enrich CNLs by providing\nmore flexibility in the design of controlled languages. We look at two specific\nuse cases of non-compositional constructions: a general-purpose method to\ndetect and extract multilingual multiword expressions and a procedure to\nidentify nominal compounds in German. We evaluate our procedure for multiword\nexpressions by performing a qualitative analysis of the results. For the\nexperiments on nominal compounds, we incorporate the detected compounds in a\nfull SMT pipeline and evaluate the impact of our method in machine translation\nprocess."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.3987v1", 
    "title": "Towards an Error Correction Memory to Enhance Technical Texts Authoring   in LELIE", 
    "arxiv-id": "1406.3987v1", 
    "author": "Patrick Saint Dizier", 
    "publish": "2014-06-16T12:03:49Z", 
    "summary": "In this paper, we investigate and experiment the notion of error correction\nmemory applied to error correction in technical texts. The main purpose is to\ninduce relatively generic correction patterns associated with more contextual\ncorrection recommendations, based on previously memorized and analyzed\ncorrections. The notion of error correction memory is developed within the\nframework of the LELIE project and illustrated on the case of fuzzy lexical\nitems, which is a major problem in technical texts."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.4057v1", 
    "title": "Embedded Controlled Languages", 
    "arxiv-id": "1406.4057v1", 
    "author": "Aarne Ranta", 
    "publish": "2014-06-16T16:11:32Z", 
    "summary": "Inspired by embedded programming languages, an embedded CNL (controlled\nnatural language) is a proper fragment of an entire natural language (its host\nlanguage), but it has a parser that recognizes the entire host language. This\nmakes it possible to process out-of-CNL input and give useful feedback to\nusers, instead of just reporting syntax errors. This extended abstract explains\nthe main concepts of embedded CNL implementation in GF (Grammatical Framework),\nwith examples from machine translation and some other ongoing work."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.4211v1", 
    "title": "Mapping the Economic Crisis: Some Preliminary Investigations", 
    "arxiv-id": "1406.4211v1", 
    "author": "Thierry Poibeau", 
    "publish": "2014-06-17T01:34:22Z", 
    "summary": "In this paper we describe our contribution to the PoliInformatics 2014\nChallenge on the 2007-2008 financial crisis. We propose a state of the art\ntechnique to extract information from texts and provide different\nrepresentations, giving first a static overview of the domain and then a\ndynamic representation of its main evolutions. We show that this strategy\nprovides a practical solution to some recent theories in social sciences that\nare facing a lack of methods and tools to automatically extract information\nfrom natural language texts."
},{
    "category": "cs.CL", 
    "doi": "10.5121/csit.2014.4501", 
    "link": "http://arxiv.org/pdf/1406.5598v1", 
    "title": "A survey on phrase structure learning methods for text classification", 
    "arxiv-id": "1406.5598v1", 
    "author": "Mary Priya Sebastian", 
    "publish": "2014-06-21T11:30:05Z", 
    "summary": "Text classification is a task of automatic classification of text into one of\nthe predefined categories. The problem of text classification has been widely\nstudied in different communities like natural language processing, data mining\nand information retrieval. Text classification is an important constituent in\nmany information management tasks like topic identification, spam filtering,\nemail routing, language identification, genre classification, readability\nassessment etc. The performance of text classification improves notably when\nphrase patterns are used. The use of phrase patterns helps in capturing\nnon-local behaviours and thus helps in the improvement of text classification\ntask. Phrase structure extraction is the first step to continue with the phrase\npattern identification. In this survey, detailed study of phrase structure\nlearning methods have been carried out. This will enable future work in several\nNLP tasks, which uses syntactic information from phrase structure like grammar\ncheckers, question answering, information extraction, machine translation, text\nclassification. The paper also provides different levels of classification and\ndetailed comparison of the phrase structure learning methods."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1406.6844v1", 
    "title": "FrameNet Resource Grammar Library for GF", 
    "arxiv-id": "1406.6844v1", 
    "author": "Guntis Barzdins", 
    "publish": "2014-06-26T11:14:44Z", 
    "summary": "In this paper we present an ongoing research investigating the possibility\nand potential of integrating frame semantics, particularly FrameNet, in the\nGrammatical Framework (GF) application grammar development. An important\ncomponent of GF is its Resource Grammar Library (RGL) that encapsulates the\nlow-level linguistic knowledge about morphology and syntax of currently more\nthan 20 languages facilitating rapid development of multilingual applications.\nIn the ideal case, porting a GF application grammar to a new language would\nonly require introducing the domain lexicon - translation equivalents that are\ninterlinked via common abstract terms. While it is possible for a highly\nrestricted CNL, developing and porting a less restricted CNL requires above\naverage linguistic knowledge about the particular language, and above average\nGF experience. Specifying a lexicon is mostly straightforward in the case of\nnouns (incl. multi-word units), however, verbs are the most complex category\n(in terms of both inflectional paradigms and argument structure), and adding\nthem to a GF application grammar is not a straightforward task. In this paper\nwe are focusing on verbs, investigating the possibility of creating a\nmultilingual FrameNet-based GF library. We propose an extension to the current\nRGL, allowing GF application developers to define clauses on the semantic\nlevel, thus leaving the language-specific syntactic mapping to this extension.\nWe demonstrate our approach by reengineering the MOLTO Phrasebook application\ngrammar."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1406.7483v1", 
    "title": "Jabalin: a Comprehensive Computational Model of Modern Standard Arabic   Verbal Morphology Based on Traditional Arabic Prosody", 
    "arxiv-id": "1406.7483v1", 
    "author": "Antonio Moreno Sandoval", 
    "publish": "2014-06-29T10:08:54Z", 
    "summary": "The computational handling of Modern Standard Arabic is a challenge in the\nfield of natural language processing due to its highly rich morphology.\nHowever, several authors have pointed out that the Arabic morphological system\nis in fact extremely regular. The existing Arabic morphological analyzers have\nexploited this regularity to variable extent, yet we believe there is still\nsome scope for improvement. Taking inspiration in traditional Arabic prosody,\nwe have designed and implemented a compact and simple morphological system\nwhich in our opinion takes further advantage of the regularities encountered in\nthe Arabic morphological system. The output of the system is a large-scale\nlexicon of inflected forms that has subsequently been used to create an Online\nInterface for a morphological analyzer of Arabic verbs. The Jabalin Online\nInterface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the\nLLI-UAM lab. The generation system is also available under a GNU GPL 3 license."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1408.0782v1", 
    "title": "Targetable Named Entity Recognition in Social Media", 
    "arxiv-id": "1408.0782v1", 
    "author": "Jinho D. Choi", 
    "publish": "2014-08-04T19:31:32Z", 
    "summary": "We present a novel approach for recognizing what we call targetable named\nentities; that is, named entities in a targeted set (e.g, movies, books, TV\nshows). Unlike many other NER systems that need to retrain their statistical\nmodels as new entities arrive, our approach does not require such retraining,\nwhich makes it more adaptable for types of entities that are frequently\nupdated. For this preliminary study, we focus on one entity type, movie title,\nusing data collected from Twitter. Our system is tested on two evaluation sets,\none including only entities corresponding to movies in our training set, and\nthe other excluding any of those entities. Our final model shows F1-scores of\n76.19% and 78.70% on these evaluation sets, which gives strong evidence that\nour approach is completely unbiased to any par- ticular set of entities found\nduring training."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1408.1928v1", 
    "title": "Microtask crowdsourcing for disease mention annotation in PubMed   abstracts", 
    "arxiv-id": "1408.1928v1", 
    "author": "Andrew I. Su", 
    "publish": "2014-08-08T17:49:02Z", 
    "summary": "Identifying concepts and relationships in biomedical text enables knowledge\nto be applied in computational analyses. Many biological natural language\nprocess (BioNLP) projects attempt to address this challenge, but the state of\nthe art in BioNLP still leaves much room for improvement. Progress in BioNLP\nresearch depends on large, annotated corpora for evaluating information\nextraction systems and training machine learning models. Traditionally, such\ncorpora are created by small numbers of expert annotators often working over\nextended periods of time. Recent studies have shown that workers on microtask\ncrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in\naggregate, generate high-quality annotations of biomedical text. Here, we\ninvestigated the use of the AMT in capturing disease mentions in PubMed\nabstracts. We used the NCBI Disease corpus as a gold standard for refining and\nbenchmarking our crowdsourcing protocol. After several iterations, we arrived\nat a protocol that reproduced the annotations of the 593 documents in the\ntraining set of this gold standard with an overall F measure of 0.872\n(precision 0.862, recall 0.883). The output can also be tuned to optimize for\nprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when\nprecision = 0.436). Each document was examined by 15 workers, and their\nannotations were merged based on a simple voting method. In total 145 workers\ncombined to complete all 593 documents in the span of 1 week at a cost of $.06\nper abstract per worker. The quality of the annotations, as judged with the F\nmeasure, increases with the number of workers assigned to each task such that\nthe system can be tuned to balance cost against quality. These results\ndemonstrate that microtask crowdsourcing can be a valuable tool for generating\nwell-annotated corpora in BioNLP."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1408.2359v2", 
    "title": "Gap-weighted subsequences for automatic cognate identification and   phylogenetic inference", 
    "arxiv-id": "1408.2359v2", 
    "author": "Taraka Rama", 
    "publish": "2014-08-11T09:32:39Z", 
    "summary": "In this paper, we describe the problem of cognate identification and its\nrelation to phylogenetic inference. We introduce subsequence based features for\ndiscriminating cognates from non-cognates. We show that subsequence based\nfeatures perform better than the state-of-the-art string similarity measures\nfor the purpose of cognate identification. We use the cognate judgments for the\npurpose of phylogenetic inference and observe that these classifiers infer a\ntree which is close to the gold standard tree. The contribution of this paper\nis the use of subsequence features for cognate identification and to employ the\ncognate judgments for phylogenetic inference."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1408.3153v2", 
    "title": "Detection is the central problem in real-word spelling correction", 
    "arxiv-id": "1408.3153v2", 
    "author": "L. Amber Wilcox-O'Hearn", 
    "publish": "2014-08-13T22:09:23Z", 
    "summary": "Real-word spelling correction differs from non-word spelling correction in\nits aims and its challenges. Here we show that the central problem in real-word\nspelling correction is detection. Methods from non-word spelling correction,\nwhich focus instead on selection among candidate corrections, do not address\ndetection adequately, because detection is either assumed in advance or heavily\nconstrained. As we demonstrate in this paper, merely discriminating between the\nintended word and a random close variation of it within the context of a\nsentence is a task that can be performed with high accuracy using\nstraightforward models. Trigram models are sufficient in almost all cases. The\ndifficulty comes when every word in the sentence is a potential error, with a\nlarge set of possible candidate corrections. Despite their strengths, trigram\nmodels cannot reliably find true errors without introducing many more, at least\nnot when used in the obvious sequential way without added structure. The\ndetection task exposes weakness not visible in the selection task."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-642-32612-7_9", 
    "link": "http://arxiv.org/pdf/1408.3456v1", 
    "title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity   Estimation", 
    "arxiv-id": "1408.3456v1", 
    "author": "Anna Korhonen", 
    "publish": "2014-08-15T01:59:29Z", 
    "summary": "We present SimLex-999, a gold standard resource for evaluating distributional\nsemantic models that improves on existing resources in several important ways.\nFirst, in contrast to gold standards such as WordSim-353 and MEN, it explicitly\nquantifies similarity rather than association or relatedness, so that pairs of\nentities that are associated but not actually similar [Freud, psychology] have\na low rating. We show that, via this focus on similarity, SimLex-999\nincentivizes the development of models with a different, and arguably wider\nrange of applications than those which reflect conceptual association. Second,\nSimLex-999 contains a range of concrete and abstract adjective, noun and verb\npairs, together with an independent rating of concreteness and (free)\nassociation strength for each pair. This diversity enables fine-grained\nanalyses of the performance of models on concepts of different types, and\nconsequently greater insight into how architectures can be improved. Further,\nunlike existing gold standard evaluations, for which automatic approaches have\nreached or surpassed the inter-annotator agreement ceiling, state-of-the-art\nmodels perform well below this ceiling on SimLex-999. There is therefore plenty\nof scope for SimLex-999 to quantify future improvements to distributional\nsemantic models, guiding the development of the next generation of\nrepresentation-learning architectures."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1408.3731v2", 
    "title": "Unsupervised Keyword Extraction from Polish Legal Texts", 
    "arxiv-id": "1408.3731v2", 
    "author": "Micha\u0142 \u0141opuszy\u0144ski", 
    "publish": "2014-08-16T10:09:28Z", 
    "summary": "In this work, we present an application of the recently proposed unsupervised\nkeyword extraction algorithm RAKE to a corpus of Polish legal texts from the\nfield of public procurement. RAKE is essentially a language and domain\nindependent method. Its only language-specific input is a stoplist containing a\nset of non-content words. The performance of the method heavily depends on the\nchoice of such a stoplist, which should be domain adopted. Therefore, we\ncomplement RAKE algorithm with an automatic approach to selecting non-content\nwords, which is based on the statistical properties of term distribution."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1408.4753v1", 
    "title": "Be Careful When Assuming the Obvious: Commentary on \"The placement of   the head that minimizes online memory: a complex systems approach\"", 
    "arxiv-id": "1408.4753v1", 
    "author": "Phillip M. Alday", 
    "publish": "2014-08-20T18:40:13Z", 
    "summary": "Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic\nand diachronic nature of word order based on the assumption that memory costs\nare a never decreasing function of distance and a few very general linguistic\nassumptions. However, even these minimal and seemingly obvious assumptions are\nnot as safe as they appear in light of recent typological and psycholinguistic\nevidence. The interaction of word order and memory has further depths to be\nexplored."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1408.6179v1", 
    "title": "Evaluating Neural Word Representations in Tensor-Based Compositional   Settings", 
    "arxiv-id": "1408.6179v1", 
    "author": "Matthew Purver", 
    "publish": "2014-08-26T16:28:21Z", 
    "summary": "We provide a comparative study between neural word representations and\ntraditional vector spaces based on co-occurrence counts, in a number of\ncompositional tasks. We use three different semantic spaces and implement seven\ntensor-based compositional models, which we then test (together with simpler\nadditive and multiplicative approaches) in tasks involving verb disambiguation\nand sentence similarity. To check their scalability, we additionally evaluate\nthe spaces using simple compositional methods on larger-scale tasks with less\nconstrained language: paraphrase detection and dialogue act tagging. In the\nmore constrained tasks, co-occurrence vectors are competitive, although choice\nof compositional method is important; on the larger-scale tasks, they are\noutperformed by neural word embeddings, which show robust, stable performance\nacross the tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1408.6181v1", 
    "title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", 
    "arxiv-id": "1408.6181v1", 
    "author": "Mehrnoosh Sadrzadeh", 
    "publish": "2014-08-26T16:43:30Z", 
    "summary": "This paper provides a method for improving tensor-based compositional\ndistributional models of meaning by the addition of an explicit disambiguation\nstep prior to composition. In contrast with previous research where this\nhypothesis has been successfully tested against relatively simple compositional\nmodels, in our work we use a robust model trained with linear regression. The\nresults we get in two experiments show the superiority of the prior\ndisambiguation method and suggest that the effectiveness of this approach is\nmodel-independent."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1408.6788v2", 
    "title": "Strongly Incremental Repair Detection", 
    "arxiv-id": "1408.6788v2", 
    "author": "Matthew Purver", 
    "publish": "2014-08-28T17:29:55Z", 
    "summary": "We present STIR (STrongly Incremental Repair detection), a system that\ndetects speech repairs and edit terms on transcripts incrementally with minimal\nlatency. STIR uses information-theoretic measures from n-gram models as its\nprincipal decision features in a pipeline of classifiers detecting the\ndifferent stages of repairs. Results on the Switchboard disfluency tagged\ncorpus show utterance-final accuracy on a par with state-of-the-art incremental\nrepair detection methods, but with better incremental accuracy, faster\ntime-to-detection and less computational overhead. We evaluate its performance\nusing incremental metrics and propose new repair processing evaluation\nstandards."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.0314v2", 
    "title": "Empirical Evaluation of Tree distances for Parser Evaluation", 
    "arxiv-id": "1409.0314v2", 
    "author": "Taraka Rama", 
    "publish": "2014-09-01T08:01:54Z", 
    "summary": "In this empirical study, I compare various tree distance measures --\noriginally developed in computational biology for the purpose of tree\ncomparison -- for the purpose of parser evaluation. I will control for the\nparser setting by comparing the automatically generated parse trees from the\nstate-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The\narticle describes two different tree distance measures (RF and QD) along with\nits variants (GRF and GQD) for the purpose of parser evaluation. The article\nwill argue that RF measure captures similar information as the standard EvalB\nmetric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,\n1989) applied by Tsarfaty et al. (2011). Finally, the article also provides\nempirical evidence by reporting high correlations between the different tree\ndistances and EvalB metric's scores."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.2073v1", 
    "title": "An NLP Assistant for Clide", 
    "arxiv-id": "1409.2073v1", 
    "author": "Tobias Kortkamp", 
    "publish": "2014-09-07T02:31:03Z", 
    "summary": "This report describes an NLP assistant for the collaborative development\nenvironment Clide, that supports the development of NLP applications by\nproviding easy access to some common NLP data structures. The assistant\nvisualizes text fragments and their dependencies by displaying the semantic\ngraph of a sentence, the coreference chain of a paragraph and mined triples\nthat are extracted from a paragraph's semantic graphs and linked using its\ncoreference chain. Using this information and a logic programming library, we\ncreate an NLP database which is used by a series of queries to mine the\ntriples. The algorithm is tested by translating a natural language text\ndescribing a graph to an actual graph that is shown as an annotation in the\ntext editor."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.3005v1", 
    "title": "A Study of Association Measures and their Combination for Arabic MWT   Extraction", 
    "arxiv-id": "1409.3005v1", 
    "author": "Eric Gaussier", 
    "publish": "2014-09-10T09:52:41Z", 
    "summary": "Automatic Multi-Word Term (MWT) extraction is a very important issue to many\napplications, such as information retrieval, question answering, and text\ncategorization. Although many methods have been used for MWT extraction in\nEnglish and other European languages, few studies have been applied to Arabic.\nIn this paper, we propose a novel, hybrid method which combines linguistic and\nstatistical approaches for Arabic Multi-Word Term extraction. The main\ncontribution of our method is to consider contextual information and both\ntermhood and unithood for association measures at the statistical filtering\nstep. In addition, our technique takes into account the problem of MWT\nvariation in the linguistic filtering step. The performance of the proposed\nstatistical measure (NLC-value) is evaluated using an Arabic environment corpus\nby comparing it with some existing competitors. Experimental results show that\nour NLC-value measure outperforms the other ones in term of precision for both\nbi-grams and tri-grams."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.3512v1", 
    "title": "Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words", 
    "arxiv-id": "1409.3512v1", 
    "author": "Bharat Sharma", 
    "publish": "2014-09-10T19:01:18Z", 
    "summary": "This paper presents a new model of WordNet that is used to disambiguate the\ncorrect sense of polysemy word based on the clue words. The related words for\neach sense of a polysemy word as well as single sense word are referred to as\nthe clue words. The conventional WordNet organizes nouns, verbs, adjectives and\nadverbs together into sets of synonyms called synsets each expressing a\ndifferent concept. In contrast to the structure of WordNet, we developed a new\nmodel of WordNet that organizes the different senses of polysemy words as well\nas the single sense words based on the clue words. These clue words for each\nsense of a polysemy word as well as for single sense word are used to\ndisambiguate the correct meaning of the polysemy word in the given context\nusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word\ncan be a noun, verb, adjective or adverb."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.3813v1", 
    "title": "Incorporating Semi-supervised Features into Discontinuous Easy-First   Constituent Parsing", 
    "arxiv-id": "1409.3813v1", 
    "author": "Yannick Versley", 
    "publish": "2014-09-12T18:37:35Z", 
    "summary": "This paper describes adaptations for EaFi, a parser for easy-first parsing of\ndiscontinuous constituents, to adapt it to multiple languages as well as make\nuse of the unlabeled data that was provided as part of the SPMRL shared task\n2014."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.4169v1", 
    "title": "An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech   Synthesis of Sanskrit Verse", 
    "arxiv-id": "1409.4169v1", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2014-09-15T08:05:36Z", 
    "summary": "The rendering of Sanskrit poetry from text to speech is a problem that has\nnot been solved before. One reason may be the complications in the language\nitself. We present unique algorithms based on extensive empirical analysis, to\nsynthesize speech from a given text input of Sanskrit verses. Using a\npre-recorded audio units database which is itself tremendously reduced in size\ncompared to the colossal size that would otherwise be required, the algorithms\nwork on producing the best possible, tunefully rendered chanting of the given\nverse. His would enable the visually impaired and those with reading\ndisabilities to easily access the contents of Sanskrit verses otherwise\navailable only in writing."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.4354v1", 
    "title": "A Binary Schema and Computational Algorithms to Process Vowel-based   Euphonic Conjunctions for Word Searches", 
    "arxiv-id": "1409.4354v1", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2014-09-15T18:08:33Z", 
    "summary": "Comprehensively searching for words in Sanskrit E-text is a non-trivial\nproblem because words could change their forms in different contexts. One such\ncontext is sandhi or euphonic conjunctions, which cause a word to change owing\nto the presence of adjacent letters or words. The change wrought by these\npossible conjunctions can be so significant in Sanskrit that a simple search\nfor the word in its given form alone can significantly reduce the success level\nof the search. This work presents a representational schema that represents\nletters in a binary format and reduces Paninian rules of euphonic conjunctions\nto simple bit set-unset operations. The work presents an efficient algorithm to\nprocess vowel-based sandhis using this schema. It further presents another\nalgorithm that uses the sandhi processor to generate the possible transformed\nword forms of a given word to use in a comprehensive word search."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.4364v1", 
    "title": "Computational Algorithms Based on the Paninian System to Process   Euphonic Conjunctions for Word Searches", 
    "arxiv-id": "1409.4364v1", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2014-09-15T18:19:06Z", 
    "summary": "Searching for words in Sanskrit E-text is a problem that is accompanied by\ncomplexities introduced by features of Sanskrit such as euphonic conjunctions\nor sandhis. A word could occur in an E-text in a transformed form owing to the\noperation of rules of sandhi. Simple word search would not yield these\ntransformed forms of the word. Further, there is no search engine in the\nliterature that can comprehensively search for words in Sanskrit E-texts taking\neuphonic conjunctions into account. This work presents an optimal binary\nrepresentational schema for letters of the Sanskrit alphabet along with\nalgorithms to efficiently process the sandhi rules of Sanskrit grammar. The\nwork further presents an algorithm that uses the sandhi processing algorithm to\nperform a comprehensive word search on E-text."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.4614v4", 
    "title": "Lexical Normalisation of Twitter Data", 
    "arxiv-id": "1409.4614v4", 
    "author": "Bilal Ahmed", 
    "publish": "2014-09-16T12:59:07Z", 
    "summary": "Twitter with over 500 million users globally, generates over 100,000 tweets\nper minute . The 140 character limit per tweet, perhaps unintentionally,\nencourages users to use shorthand notations and to strip spellings to their\nbare minimum \"syllables\" or elisions e.g. \"srsly\". The analysis of twitter\nmessages which typically contain misspellings, elisions, and grammatical\nerrors, poses a challenge to established Natural Language Processing (NLP)\ntools which are generally designed with the assumption that the data conforms\nto the basic grammatical structure commonly used in English language. In order\nto make sense of Twitter messages it is necessary to first transform them into\na canonical form, consistent with the dictionary or grammar. This process,\nperformed at the level of individual tokens (\"words\"), is called lexical\nnormalisation. This paper investigates various techniques for lexical\nnormalisation of Twitter data and presents the findings as the techniques are\napplied to process raw data from Twitter."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.5502v1", 
    "title": "Using crowdsourcing system for creating site-specific statistical   machine translation engine", 
    "arxiv-id": "1409.5502v1", 
    "author": "George Savchenko", 
    "publish": "2014-09-19T02:50:04Z", 
    "summary": "A crowdsourcing translation approach is an effective tool for globalization\nof site content, but it is also an important source of parallel linguistic\ndata. For the given site, processed with a crowdsourcing system, a\nsentence-aligned corpus can be fetched, which covers a very narrow domain of\nterminology and language patterns - a site-specific domain. These data can be\nused for training and estimation of site-specific statistical machine\ntranslation engine"
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.7386v1", 
    "title": "Performance of Stanford and Minipar Parser on Biomedical Texts", 
    "arxiv-id": "1409.7386v1", 
    "author": "Rushdi Shams", 
    "publish": "2014-09-25T15:35:27Z", 
    "summary": "In this paper, the performance of two dependency parsers, namely Stanford and\nMinipar, on biomedical texts has been reported. The performance of te parsers\nto assignm dependencies between two biomedical concepts that are already proved\nto be connected is not satisfying. Both Stanford and Minipar, being statistical\nparsers, fail to assign dependency relation between two connected concepts if\nthey are distant by at least one clause. Minipar's performance, in terms of\nprecision, recall and the F-score of the attachment score (e.g., correctly\nidentified head in a dependency), to parse biomedical text is also measured\ntaking the Stanford's as a gold standard. The results suggest that Minipar is\nnot suitable yet to parse biomedical texts. In addition, a qualitative\ninvestigation reveals that the difference between working principles of the\nparsers also play a vital role for Minipar's degraded performance."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.7619v1", 
    "title": "Generating Conceptual Metaphors from Proposition Stores", 
    "arxiv-id": "1409.7619v1", 
    "author": "Ross Israel", 
    "publish": "2014-09-25T13:54:37Z", 
    "summary": "Contemporary research on computational processing of linguistic metaphors is\ndivided into two main branches: metaphor recognition and metaphor\ninterpretation. We take a different line of research and present an automated\nmethod for generating conceptual metaphors from linguistic data. Given the\ngenerated conceptual metaphors, we find corresponding linguistic metaphors in\ncorpora. In this paper, we describe our approach and its evaluation using\nEnglish and Russian data."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.8008v1", 
    "title": "CRF-based Named Entity Recognition @ICON 2013", 
    "arxiv-id": "1409.8008v1", 
    "author": "Utpal Garain", 
    "publish": "2014-09-29T07:11:30Z", 
    "summary": "This paper describes performance of CRF based systems for Named Entity\nRecognition (NER) in Indian language as a part of ICON 2013 shared task. In\nthis task we have considered a set of language independent features for all the\nlanguages. Only for English a language specific feature, i.e. capitalization,\nhas been added. Next the use of gazetteer is explored for Bengali, Hindi and\nEnglish. The gazetteers are built from Wikipedia and other sources. Test\nresults show that the system achieves the highest F measure of 88% for English\nand the lowest F measure of 69% for both Tamil and Telugu. Note that for the\nleast performing two languages no gazetteer was used. NER in Bengali and Hindi\nfinds accuracy (F measure) of 87% and 79%, respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1409.8581v1", 
    "title": "Improving the Performance of English-Tamil Statistical Machine   Translation System using Source-Side Pre-Processing", 
    "arxiv-id": "1409.8581v1", 
    "author": "V. Sharmiladevi", 
    "publish": "2014-09-29T04:54:03Z", 
    "summary": "Machine Translation is one of the major oldest and the most active research\narea in Natural Language Processing. Currently, Statistical Machine Translation\n(SMT) dominates the Machine Translation research. Statistical Machine\nTranslation is an approach to Machine Translation which uses models to learn\ntranslation patterns directly from data, and generalize them to translate a new\nunseen text. The SMT approach is largely language independent, i.e. the models\ncan be applied to any language pair. Statistical Machine Translation (SMT)\nattempts to generate translations using statistical methods based on bilingual\ntext corpora. Where such corpora are available, excellent results can be\nattained translating similar texts, but such corpora are still not available\nfor many language pairs. Statistical Machine Translation systems, in general,\nhave difficulty in handling the morphology on the source or the target side\nespecially for morphologically rich languages. Errors in morphology or syntax\nin the target language can have severe consequences on meaning of the sentence.\nThey change the grammatical function of words or the understanding of the\nsentence through the incorrect tense information in verb. Baseline SMT also\nknown as Phrase Based Statistical Machine Translation (PBSMT) system does not\nuse any linguistic information and it only operates on surface word form.\nRecent researches shown that adding linguistic information helps to improve the\naccuracy of the translation with less amount of bilingual corpora. Adding\nlinguistic information can be done using the Factored Statistical Machine\nTranslation system through pre-processing steps. This paper investigates about\nhow English side pre-processing is used to improve the accuracy of\nEnglish-Tamil SMT system."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.00841v1", 
    "title": "Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama   in translation", 
    "arxiv-id": "1501.00841v1", 
    "author": "Carl Vogel", 
    "publish": "2015-01-05T12:55:03Z", 
    "summary": "Research into the stylistic properties of translations is an issue which has\nreceived some attention in computational stylistics. Previous work by Rybicki\n(2006) on the distinguishing of character idiolects in the work of Polish\nauthor Henryk Sienkiewicz and two corresponding English translations using\nBurrow's Delta method concluded that idiolectal differences could be observed\nin the source texts and this variation was preserved to a large degree in both\ntranslations. This study also found that the two translations were also highly\ndistinguishable from one another. Burrows (2002) examined English translations\nof Juvenal also using the Delta method, results of this work suggest that some\ntranslators are more adept at concealing their own style when translating the\nworks of another author whereas other authors tend to imprint their own style\nto a greater extent on the work they translate. Our work examines the writing\nof a single author, Norwegian playwright Henrik Ibsen, and these writings\ntranslated into both German and English from Norwegian, in an attempt to\ninvestigate the preservation of characterization, defined here as the\ndistinctiveness of textual contributions of characters."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.01243v1", 
    "title": "Un r\u00e9sumeur \u00e0 base de graphes, ind\u00e9p\u00e9ndant de la langue", 
    "arxiv-id": "1501.01243v1", 
    "author": "Iria da Cunha", 
    "publish": "2015-01-06T17:27:40Z", 
    "summary": "In this paper we present REG, a graph-based approach for study a fundamental\nproblem of Natural Language Processing (NLP): the automatic text summarization.\nThe algorithm maps a document as a graph, then it computes the weight of their\nsentences. We have applied this approach to summarize documents in three\nlanguages."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.01254v1", 
    "title": "Unknown Words Analysis in POS tagging of Sinhala Language", 
    "arxiv-id": "1501.01254v1", 
    "author": "N. G. J. Dias", 
    "publish": "2015-01-06T18:03:13Z", 
    "summary": "Part of Speech (POS) is a very vital topic in Natural Language Processing\n(NLP) task in any language, which involves analysing the construction of the\nlanguage, behaviours and the dynamics of the language, the knowledge that could\nbe utilized in computational linguistics analysis and automation applications.\nIn this context, dealing with unknown words (words do not appear in the lexicon\nreferred as unknown words) is also an important task, since growing NLP systems\nare used in more and more new applications. One aid of predicting lexical\ncategories of unknown words is the use of syntactical knowledge of the\nlanguage. The distinction between open class words and closed class words\ntogether with syntactical features of the language used in this research to\npredict lexical categories of unknown words in the tagging process. An\nexperiment is performed to investigate the ability of the approach to parse\nunknown words using syntactical knowledge without human intervention. This\nexperiment shows that the performance of the tagging process is enhanced when\nword class distinction is used together with syntactic rules to parse sentences\ncontaining unknown words in Sinhala language."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.01894v1", 
    "title": "Quantifying Scripts: Defining metrics of characters for quantitative and   descriptive analysis", 
    "arxiv-id": "1501.01894v1", 
    "author": "Vinodh Rajan", 
    "publish": "2015-01-08T16:12:34Z", 
    "summary": "Analysis of scripts plays an important role in paleography and in\nquantitative linguistics. Especially in the field of digital paleography\nquantitative features are much needed to differentiate glyphs. We describe an\nelaborate set of metrics that quantify qualitative information contained in\ncharacters and hence indirectly also quantify the scribal features. We broadly\ndivide the metrics into several categories and describe each individual metric\nwith its underlying qualitative significance. The metrics are largely derived\nfrom the related area of gesture design and recognition. We also propose\nseveral novel metrics. The proposed metrics are soundly grounded on the\nprinciples of handwriting production and handwriting analysis. These computed\nmetrics could serve as descriptors for scripts and also be used for comparing\nand analyzing scripts. We illustrate some quantitative analysis based on the\nproposed metrics by applying it to the paleographic evolution of the medieval\nTamil script from Brahmi. We also outline future work."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.02670v1", 
    "title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", 
    "arxiv-id": "1501.02670v1", 
    "author": "Magnus Sahlgren", 
    "publish": "2015-01-12T14:48:54Z", 
    "summary": "This paper is concerned with nearest neighbor search in distributional\nsemantic models. A normal nearest neighbor search only returns a ranked list of\nneighbors, with no information about the structure or topology of the local\nneighborhood. This is a potentially serious shortcoming of the mode of querying\na distributional semantic model, since a ranked list of neighbors may conflate\nseveral different senses. We argue that the topology of neighborhoods in\nsemantic space provides important information about the different senses of\nterms, and that such topological structures can be used for word-sense\ninduction. We also argue that the topology of the neighborhoods in semantic\nspace can be used to determine the semantic horizon of a point, which we define\nas the set of neighbors that have a direct connection to the point. We\nintroduce relative neighborhood graphs as method to uncover the topological\nproperties of neighborhoods in semantic models. We also provide examples of\nrelative neighborhood graphs for three well-known semantic models; the PMI\nmodel, the GloVe model, and the skipgram model."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.03191v1", 
    "title": "Annotating Cognates and Etymological Origin in Turkic Languages", 
    "arxiv-id": "1501.03191v1", 
    "author": "Michael Bloodgood", 
    "publish": "2015-01-13T22:14:57Z", 
    "summary": "Turkic languages exhibit extensive and diverse etymological relationships\namong lexical items. These relationships make the Turkic languages promising\nfor exploring automated translation lexicon induction by leveraging cognate and\nother etymological information. However, due to the extent and diversity of the\ntypes of relationships between words, it is not clear how to annotate such\ninformation. In this paper, we present a methodology for annotating cognates\nand etymological origin in Turkic languages. Our method strives to balance the\namount of research effort the annotator expends with the utility of the\nannotations for supporting research on improving automated translation lexicon\ninduction."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.04324v1", 
    "title": "Phrase Based Language Model For Statistical Machine Translation", 
    "arxiv-id": "1501.04324v1", 
    "author": "Geliang Chen", 
    "publish": "2015-01-18T16:37:53Z", 
    "summary": "We consider phrase based Language Models (LM), which generalize the commonly\nused word level models. Similar concept on phrase based LMs appears in speech\nrecognition, which is rather specialized and thus less suitable for machine\ntranslation (MT). In contrast to the dependency LM, we first introduce the\nexhaustive phrase-based LMs tailored for MT use. Preliminary experimental\nresults show that our approach outperform word based LMs with the respect to\nperplexity and translation quality."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.05203v3", 
    "title": "Phrase Based Language Model for Statistical Machine Translation:   Empirical Study", 
    "arxiv-id": "1501.05203v3", 
    "author": "Geliang Chen", 
    "publish": "2015-01-21T15:48:28Z", 
    "summary": "Reordering is a challenge to machine translation (MT) systems. In MT, the\nwidely used approach is to apply word based language model (LM) which considers\nthe constituent units of a sentence as words. In speech recognition (SR), some\nphrase based LM have been proposed. However, those LMs are not necessarily\nsuitable or optimal for reordering. We propose two phrase based LMs which\nconsiders the constituent units of a sentence as phrases. Experiments show that\nour phrase based LMs outperform the word based LM with the respect of\nperplexity and n-best list re-ranking."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1501.07005v1", 
    "title": "Survey:Natural Language Parsing For Indian Languages", 
    "arxiv-id": "1501.07005v1", 
    "author": "Deepak C. Vegda", 
    "publish": "2015-01-28T07:04:18Z", 
    "summary": "Syntactic parsing is a necessary task which is required for NLP applications\nincluding machine translation. It is a challenging task to develop a\nqualitative parser for morphological rich and agglutinative languages.\nSyntactic analysis is used to understand the grammatical structure of a natural\nlanguage sentence. It outputs all the grammatical information of each word and\nits constituent. Also issues related to it help us to understand the language\nin a more detailed way. This literature survey is groundwork to understand the\ndifferent parser development for Indian languages and various approaches that\nare used to develop such tools and techniques. This paper provides a survey of\nresearch papers from well known journals and conferences."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1104.2034v1", 
    "title": "Materials to the Russian-Bulgarian Comparative Dictionary \"EAD\"", 
    "arxiv-id": "1104.2034v1", 
    "author": "Yavor Angelov Parvanov", 
    "publish": "2011-04-11T19:50:50Z", 
    "summary": "This article presents a fragment of a new comparative dictionary \"A\ncomparative dictionary of names of expansive action in Russian and Bulgarian\nlanguages\". Main features of the new web-based comparative dictionary are\nplaced, the principles of its formation are shown, primary links between the\nword-matches are classified. The principal difference between translation\ndictionaries and the model of double comparison is also shown. The\nclassification scheme of the pages is proposed. New concepts and keywords have\nbeen introduced. The real prototype of the dictionary with a few key pages is\npublished. The broad debate about the possibility of this prototype to become a\nversion of Russian-Bulgarian comparative dictionary of a new generation is\navailable."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1104.2086v1", 
    "title": "A Universal Part-of-Speech Tagset", 
    "arxiv-id": "1104.2086v1", 
    "author": "Ryan McDonald", 
    "publish": "2011-04-11T23:06:54Z", 
    "summary": "To facilitate future research in unsupervised induction of syntactic\nstructure and to standardize best-practices, we propose a tagset that consists\nof twelve universal part-of-speech categories. In addition to the tagset, we\ndevelop a mapping from 25 different treebank tagsets to this universal set. As\na result, when combined with the original treebank data, this universal tagset\nand mapping produce a dataset consisting of common parts-of-speech for 22\ndifferent languages. We highlight the use of this resource via two experiments,\nincluding one that reports competitive accuracies for unsupervised grammar\ninduction without gold standard part-of-speech tags."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1104.4321v1", 
    "title": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and   Compounds", 
    "arxiv-id": "1104.4321v1", 
    "author": "Yannis Haralambous", 
    "publish": "2011-04-21T17:56:50Z", 
    "summary": "Chinese characters can be compared to a molecular structure: a character is\nanalogous to a molecule, radicals are like atoms, calligraphic strokes\ncorrespond to elementary particles, and when characters form compounds, they\nare like molecular structures. In chemistry the conjunction of all of these\nstructural levels produces what we perceive as matter. In language, the\nconjunction of strokes, radicals, characters, and compounds produces meaning.\nBut when does meaning arise? We all know that radicals are, in some sense, the\nbasic semantic components of Chinese script, but what about strokes?\nConsidering the fact that many characters are made by adding individual strokes\nto (combinations of) radicals, we can legitimately ask the question whether\nstrokes carry meaning, or not. In this talk I will present my project of\nextending traditional NLP techniques to radicals and strokes, aiming to obtain\na deeper understanding of the way ideographic languages model the world."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1104.4681v1", 
    "title": "Performance Evaluation of Statistical Approaches for Text Independent   Speaker Recognition Using Source Feature", 
    "arxiv-id": "1104.4681v1", 
    "author": "A. Nagesh", 
    "publish": "2011-04-25T05:00:25Z", 
    "summary": "This paper introduces the performance evaluation of statistical approaches\nfor TextIndependent speaker recognition system using source feature. Linear\nprediction LP residual is used as a representation of excitation information in\nspeech. The speaker-specific information in the excitation of voiced speech is\ncaptured using statistical approaches such as Gaussian Mixture Models GMMs and\nHidden Markov Models HMMs. The decrease in the error during training and\nrecognizing speakers during testing phase close to 100 percent accuracy\ndemonstrates that the excitation component of speech contains speaker-specific\ninformation and is indeed being effectively captured by continuous Ergodic HMM\nthan GMM. The performance of the speaker recognition system is evaluated on GMM\nand 2 state ergodic HMM with different mixture components and test speech\nduration. We demonstrate the speaker recognition studies on TIMIT database for\nboth GMM and Ergodic HMM."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1109.0069v2", 
    "title": "Inter-rater Agreement on Sentence Formality", 
    "arxiv-id": "1109.0069v2", 
    "author": "Xiaofei Lu", 
    "publish": "2011-09-01T02:20:12Z", 
    "summary": "Formality is one of the most important dimensions of writing style variation.\nIn this study we conducted an inter-rater reliability experiment for assessing\nsentence formality on a five-point Likert scale, and obtained good agreement\nresults as well as different rating distributions for different sentence\ncategories. We also performed a difficulty analysis to identify the bottlenecks\nof our rating procedure. Our main objective is to design an automatic scoring\nmechanism for sentence-level formality, and this study is important for that\npurpose."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-3-319-10888-9_7", 
    "link": "http://arxiv.org/pdf/1109.0624v1", 
    "title": "Building Ontologies to Understand Spoken Tunisian Dialect", 
    "arxiv-id": "1109.0624v1", 
    "author": "Lamia Hadrich Belguith", 
    "publish": "2011-09-03T14:30:44Z", 
    "summary": "This paper presents a method to understand spoken Tunisian dialect based on\nlexical semantic. This method takes into account the specificity of the\nTunisian dialect which has no linguistic processing tools. This method is\nontology-based which allows exploiting the ontological concepts for semantic\nannotation and ontological relations for speech interpretation. This\ncombination increases the rate of comprehension and limits the dependence on\nlinguistic resources. This paper also details the process of building the\nontology used for annotation and interpretation of Tunisian dialect in the\ncontext of speech understanding in dialogue systems for restricted domain."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1523", 
    "link": "http://arxiv.org/pdf/1109.2128v2", 
    "title": "LexRank: Graph-based Lexical Centrality as Salience in Text   Summarization", 
    "arxiv-id": "1109.2128v2", 
    "author": "Dragomir R. Radev", 
    "publish": "2011-09-09T20:20:38Z", 
    "summary": "We introduce a stochastic graph-based method for computing relative\nimportance of textual units for Natural Language Processing. We test the\ntechnique on the problem of Text Summarization (TS). Extractive TS relies on\nthe concept of sentence salience to identify the most important sentences in a\ndocument or set of documents. Salience is typically defined in terms of the\npresence of particular important words or in terms of similarity to a centroid\npseudo-sentence. We consider a new approach, LexRank, for computing sentence\nimportance based on the concept of eigenvector centrality in a graph\nrepresentation of sentences. In this model, a connectivity matrix based on\nintra-sentence cosine similarity is used as the adjacency matrix of the graph\nrepresentation of sentences. Our system, based on LexRank ranked in first place\nin more than one task in the recent DUC 2004 evaluation. In this paper we\npresent a detailed analysis of our approach and apply it to a larger data set\nincluding data from earlier DUC evaluations. We discuss several methods to\ncompute centrality using the similarity graph. The results show that\ndegree-based methods (including LexRank) outperform both centroid-based methods\nand other systems participating in DUC in most of the cases. Furthermore, the\nLexRank with threshold method outperforms the other degree-based techniques\nincluding continuous LexRank. We also show that our approach is quite\ninsensitive to the noise in the data that may result from an imperfect topical\nclustering of documents."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1529", 
    "link": "http://arxiv.org/pdf/1109.2130v1", 
    "title": "Combining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods", 
    "arxiv-id": "1109.2130v1", 
    "author": "A. Suarez", 
    "publish": "2011-09-09T20:22:04Z", 
    "summary": "In this paper we concentrate on the resolution of the lexical ambiguity that\narises when a given word has several different meanings. This specific task is\ncommonly referred to as word sense disambiguation (WSD). The task of WSD\nconsists of assigning the correct sense to words using an electronic dictionary\nas the source of word definitions. We present two WSD methods based on two main\nmethodological approaches in this research area: a knowledge-based method and a\ncorpus-based method. Our hypothesis is that word-sense disambiguation requires\nseveral knowledge sources in order to solve the semantic ambiguity of the\nwords. These sources can be of different kinds--- for example, syntagmatic,\nparadigmatic or statistical information. Our approach combines various sources\nof knowledge, through combinations of the two WSD methods mentioned above.\nMainly, the paper concentrates on how to combine these methods and sources of\ninformation in order to achieve good results in the disambiguation. Finally,\nthis paper presents a comprehensive study and experimental work on evaluation\nof the methods and their combinations."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1109.2136v1", 
    "title": "Learning Content Selection Rules for Generating Object Descriptions in   Dialogue", 
    "arxiv-id": "1109.2136v1", 
    "author": "M. A. Walker", 
    "publish": "2011-09-09T20:24:57Z", 
    "summary": "A fundamental requirement of any task-oriented dialogue system is the ability\nto generate object descriptions that refer to objects in the task domain. The\nsubproblem of content selection for object descriptions in task-oriented\ndialogue has been the focus of much previous work and a large number of models\nhave been proposed. In this paper, we use the annotated COCONUT corpus of\ntask-oriented design dialogues to develop feature sets based on Dale and\nReiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact\nmodel, and Jordans (2000b) intentional influences model, and use these feature\nsets in a machine learning experiment to automatically learn a model of content\nselection for object descriptions. Since Dale and Reiters model requires a\nrepresentation of discourse structure, the corpus annotations are used to\nderive a representation based on Grosz and Sidners (1986) theory of the\nintentional structure of discourse, as well as two very simple representations\nof discourse structure based purely on recency. We then apply the\nrule-induction program RIPPER to train and test the content selection component\nof an object description generator on a set of 393 object descriptions from the\ncorpus. To our knowledge, this is the first reported experiment of a trainable\ncontent selection component for object description generation in dialogue.\nThree separate content selection models that are based on the three theoretical\nmodels, all independently achieve accuracies significantly above the majority\nclass baseline (17%) on unseen test data, with the intentional influences model\n(42.4%) performing significantly better than either the incremental model\n(30.4%) or the conceptual pact model (28.9%). But the best performing models\ncombine all the feature sets, achieving accuracies near 60%. Surprisingly, a\nsimple recency-based representation of discourse structure does as well as one\nbased on intentional structure. To our knowledge, this is also the first\nempirical comparison of a representation of Grosz and Sidners model of\ndiscourse structure with a simpler model for any generation task."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1109.4531v1", 
    "title": "A Probabilistic Approach to Pronunciation by Analogy", 
    "arxiv-id": "1109.4531v1", 
    "author": "Aleksi Keurulainen", 
    "publish": "2011-09-21T13:57:49Z", 
    "summary": "The relationship between written and spoken words is convoluted in languages\nwith a deep orthography such as English and therefore it is difficult to devise\nexplicit rules for generating the pronunciations for unseen words.\nPronunciation by analogy (PbA) is a data-driven method of constructing\npronunciations for novel words from concatenated segments of known words and\ntheir pronunciations. PbA performs relatively well with English and outperforms\nseveral other proposed methods. However, the best published word accuracy of\n65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for\nimprovement in it.\n  Previous PbA algorithms have used several different scoring strategies such\nas the product of the frequencies of the component pronunciations of the\nsegments, or the number of different segmentations that yield the same\npronunciation, and different combinations of these methods, to evaluate the\ncandidate pronunciations. In this article, we instead propose to use a\nprobabilistically justified scoring rule. We show that this principled approach\nalone yields better accuracy (66.21% for the NETtalk corpus) than any\npreviously published PbA algorithm. Furthermore, combined with certain ad hoc\nmodifications motivated by earlier algorithms, the performance climbs up to\n66.6%, and further improvements are possible by combining this method with\nother methods."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1109.4906v1", 
    "title": "Automatic transcription of 17th century English text in Contemporary   English with NooJ: Method and Evaluation", 
    "arxiv-id": "1109.4906v1", 
    "author": "H\u00e9l\u00e8ne Pignot", 
    "publish": "2011-09-22T18:37:17Z", 
    "summary": "Since 2006 we have undertaken to describe the differences between 17th\ncentury English and contemporary English thanks to NLP software. Studying a\ncorpus spanning the whole century (tales of English travellers in the Ottoman\nEmpire in the 17th century, Mary Astell's essay A Serious Proposal to the\nLadies and other literary texts) has enabled us to highlight various lexical,\nmorphological or grammatical singularities. Thanks to the NooJ linguistic\nplatform, we created dictionaries indexing the lexical variants and their\ntranscription in CE. The latter is often the result of the validation of forms\nrecognized dynamically by morphological graphs. We also built syntactical\ngraphs aimed at transcribing certain archaic forms in contemporary English. Our\nprevious research implied a succession of elementary steps alternating textual\nanalysis and result validation. We managed to provide examples of\ntranscriptions, but we have not created a global tool for automatic\ntranscription. Therefore we need to focus on the results we have obtained so\nfar, study the conditions for creating such a tool, and analyze possible\ndifficulties. In this paper, we will be discussing the technical and linguistic\naspects we have not yet covered in our previous work. We are using the results\nof previous research and proposing a transcription method for words or\nsequences identified as archaic."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1109.5798v1", 
    "title": "Object-oriented semantics of English in natural language understanding   system", 
    "arxiv-id": "1109.5798v1", 
    "author": "Yuriy Ostapov", 
    "publish": "2011-09-27T08:00:46Z", 
    "summary": "A new approach to the problem of natural language understanding is proposed.\nThe knowledge domain under consideration is the social behavior of people.\nEnglish sentences are translated into set of predicates of a semantic database,\nwhich describe persons, occupations, organizations, projects, actions, events,\nmessages, machines, things, animals, location and time of actions, relations\nbetween objects, thoughts, cause-and-effect relations, abstract objects. There\nis a knowledge base containing the description of semantics of objects\n(functions and structure), actions (motives and causes), and operations."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0140v1", 
    "title": "Roget's Thesaurus as a Lexical Resource for Natural Language Processing", 
    "arxiv-id": "1204.0140v1", 
    "author": "Mario Jarmasz", 
    "publish": "2012-03-31T21:53:56Z", 
    "summary": "WordNet proved that it is possible to construct a large-scale electronic\nlexical database on the principles of lexical semantics. It has been accepted\nand used extensively by computational linguists ever since it was released.\nInspired by WordNet's success, we propose as an alternative a similar resource,\nbased on the 1987 Penguin edition of Roget's Thesaurus of English Words and\nPhrases.\n  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless\nwriters, orators and students of the English language have used it.\nComputational linguists have employed Roget's for almost 50 years in Natural\nLanguage Processing, however hesitated in accepting Roget's Thesaurus because a\nproper machine tractable version was not available.\n  This dissertation presents an implementation of a machine-tractable version\nof the 1987 Penguin edition of Roget's Thesaurus - the first implementation of\nits kind to use an entire current edition. It explains the steps necessary for\ntaking a machine-readable file and transforming it into a tractable system.\nThis involves converting the lexical material into a format that can be more\neasily exploited, identifying data structures and designing classes to\ncomputerize the Thesaurus. Roget's organization is studied in detail and\ncontrasted with WordNet's.\n  We show two applications of the computerized Thesaurus: computing semantic\nsimilarity between words and phrases, and building lexical chains in a text.\nThe experiments are performed using well-known benchmarks and the results are\ncompared to those of other systems that use Roget's, WordNet and statistical\ntechniques. Roget's has turned out to be an excellent resource for measuring\nsemantic similarity; lexical chains are easily built but more difficult to\nevaluate. We also explain ways in which Roget's Thesaurus and WordNet can be\ncombined."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0184v1", 
    "title": "Parallel Spell-Checking Algorithm Based on Yahoo! N-Grams Dataset", 
    "arxiv-id": "1204.0184v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-04-01T09:28:20Z", 
    "summary": "Spell-checking is the process of detecting and sometimes providing\nsuggestions for incorrectly spelled words in a text. Basically, the larger the\ndictionary of a spell-checker is, the higher is the error detection rate;\notherwise, misspellings would pass undetected. Unfortunately, traditional\ndictionaries suffer from out-of-vocabulary and data sparseness problems as they\ndo not encompass large vocabulary of words indispensable to cover proper names,\ndomain-specific terms, technical jargons, special acronyms, and terminologies.\nAs a result, spell-checkers will incur low error detection and correction rate\nand will fail to flag all errors in the text. This paper proposes a new\nparallel shared-memory spell-checking algorithm that uses rich real-world word\nstatistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors\nin computer text. Essentially, the proposed algorithm can be divided into three\nsub-algorithms that run in a parallel fashion: The error detection algorithm\nthat detects misspellings, the candidates generation algorithm that generates\ncorrection suggestions, and the error correction algorithm that performs\ncontextual error correction. Experiments conducted on a set of text articles\ncontaining misspellings, showed a remarkable spelling error correction rate\nthat resulted in a radical reduction of both non-word and real-word errors in\nelectronic text. In a further study, the proposed algorithm is to be optimized\nfor message-passing systems so as to become more flexible and less costly to\nscale over distributed machines."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0191v1", 
    "title": "OCR Post-Processing Error Correction Algorithm using Google Online   Spelling Suggestion", 
    "arxiv-id": "1204.0191v1", 
    "author": "Mohammad Alwani", 
    "publish": "2012-04-01T10:34:38Z", 
    "summary": "With the advent of digital optical scanners, a lot of paper-based books,\ntextbooks, magazines, articles, and documents are being transformed into an\nelectronic version that can be manipulated by a computer. For this purpose,\nOCR, short for Optical Character Recognition was developed to translate scanned\ngraphical text into editable computer text. Unfortunately, OCR is still\nimperfect as it occasionally mis-recognizes letters and falsely identifies\nscanned text, leading to misspellings and linguistics errors in the OCR output\ntext. This paper proposes a post-processing context-based error correction\nalgorithm for detecting and correcting OCR non-word and real-word errors. The\nproposed algorithm is based on Google's online spelling suggestion which\nharnesses an internal database containing a huge collection of terms and word\nsequences gathered from all over the web, convenient to suggest possible\nreplacements for words that have been misspelled during the OCR process.\nExperiments carried out revealed a significant improvement in OCR error\ncorrection rate. Future research can improve upon the proposed algorithm so\nmuch so that it can be parallelized and executed over multiprocessing\nplatforms."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0245v1", 
    "title": "Roget's Thesaurus and Semantic Similarity", 
    "arxiv-id": "1204.0245v1", 
    "author": "Stan Szpakowicz", 
    "publish": "2012-04-01T17:04:13Z", 
    "summary": "We have implemented a system that measures semantic similarity using a\ncomputerized 1987 Roget's Thesaurus, and evaluated it by performing a few\ntypical tests. We compare the results of these tests with those produced by\nWordNet-based similarity measures. One of the benchmarks is Miller and Charles'\nlist of 30 noun pairs to which human judges had assigned similarity measures.\nWe correlate these measures with those computed by several NLP systems. The 30\npairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have\nalso studied. Our Roget's-based system gets correlations of .878 for the\nsmaller and .818 for the larger list of noun pairs; this is quite close to the\n.885 that Resnik obtained when he employed humans to replicate the Miller and\nCharles experiment. We further evaluate our measure by using Roget's and\nWordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the\ncorrect synonym must be selected amongst a group of four words. Our system gets\n78.75%, 82.00% and 74.33% of the questions respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0257v1", 
    "title": "Not As Easy As It Seems: Automating the Construction of Lexical Chains   Using Roget's Thesaurus", 
    "arxiv-id": "1204.0257v1", 
    "author": "Stan Szpakowicz", 
    "publish": "2012-04-01T19:19:36Z", 
    "summary": "Morris and Hirst present a method of linking significant words that are about\nthe same topic. The resulting lexical chains are a means of identifying\ncohesive regions in a text, with applications in many natural language\nprocessing tasks, including text summarization. The first lexical chains were\nconstructed manually using Roget's International Thesaurus. Morris and Hirst\nwrote that automation would be straightforward given an electronic thesaurus.\nAll applications so far have used WordNet to produce lexical chains, perhaps\nbecause adequate electronic versions of Roget's were not available until\nrecently. We discuss the building of lexical chains using an electronic version\nof Roget's Thesaurus. We implement a variant of the original algorithm, and\nexplain the necessary design decisions. We include a comparison with other\nimplementations."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.0258v1", 
    "title": "Roget's Thesaurus: a Lexical Resource to Treasure", 
    "arxiv-id": "1204.0258v1", 
    "author": "Stan Szpakowicz", 
    "publish": "2012-04-01T19:25:29Z", 
    "summary": "This paper presents the steps involved in creating an electronic lexical\nknowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic\nrelations are labelled with the help of WordNet. The two resources are compared\nin a qualitative and quantitative manner. Differences in the organization of\nthe lexical material are discussed, as well as the possibility of merging both\nresources."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.2847v2", 
    "title": "Segmentation Similarity and Agreement", 
    "arxiv-id": "1204.2847v2", 
    "author": "Diana Inkpen", 
    "publish": "2012-04-12T22:01:27Z", 
    "summary": "We propose a new segmentation evaluation metric, called segmentation\nsimilarity (S), that quantifies the similarity between two segmentations as the\nproportion of boundaries that are not transformed when comparing them using\nedit distance, essentially using edit distance as a penalty function and\nscaling penalties by segmentation size. We propose several adapted\ninter-annotator agreement coefficients which use S that are suitable for\nsegmentation. We show that S is configurable enough to suit a wide variety of\nsegmentation evaluations, and is an improvement upon the state of the art. We\nalso propose using inter-annotator agreement coefficients to evaluate automatic\nsegmenters in terms of human performance."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.1591", 
    "link": "http://arxiv.org/pdf/1204.3800v1", 
    "title": "Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha)", 
    "arxiv-id": "1204.3800v1", 
    "author": "Srinivasan Kalyanaraman", 
    "publish": "2012-04-17T14:14:26Z", 
    "summary": "Jules Bloch's work on formation of the Marathi language has to be expanded\nfurther to provide for a study of evolution and formation of Indian languages\nin the Indian language union (sprachbund). The paper analyses the stages in the\nevolution of early writing systems which began with the evolution of counting\nin the ancient Near East. A stage anterior to the stage of syllabic\nrepresentation of sounds of a language, is identified. Unique geometric shapes\nrequired for tokens to categorize objects became too large to handle to\nabstract hundreds of categories of goods and metallurgical processes during the\nproduction of bronze-age goods. About 3500 BCE, Indus script as a writing\nsystem was developed to use hieroglyphs to represent the 'spoken words'\nidentifying each of the goods and processes. A rebus method of representing\nsimilar sounding words of the lingua franca of the artisans was used in Indus\nscript. This method is recognized and consistently applied for the lingua\nfranca of the Indian sprachbund. That the ancient languages of India,\nconstituted a sprachbund (or language union) is now recognized by many\nlinguists. The sprachbund area is proximate to the area where most of the Indus\nscript inscriptions were discovered, as documented in the corpora. That\nhundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced\nby their use on early punch-marked coins. This explains the combined use of\nsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on\nRampurva copper bolt, and Sohgaura copper plate from about 6th century\nBCE.Indian hieroglyphs constitute a writing system for meluhha language and are\nrebus representations of archaeo-metallurgy lexemes. The rebus principle was\nemployed by the early scripts and can legitimately be used to decipher the\nIndus script, after secure pictorial identification."
},{
    "category": "cs.CL", 
    "doi": "10.5539/cis.v5n3p37", 
    "link": "http://arxiv.org/pdf/1204.5852v1", 
    "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram   Information", 
    "arxiv-id": "1204.5852v1", 
    "author": "Mohammad Alwani", 
    "publish": "2012-04-26T07:44:18Z", 
    "summary": "In computing, spell checking is the process of detecting and sometimes\nproviding spelling suggestions for incorrectly spelled words in a text.\nBasically, a spell checker is a computer program that uses a dictionary of\nwords to perform spell checking. The bigger the dictionary is, the higher is\nthe error detection rate. The fact that spell checkers are based on regular\ndictionaries, they suffer from data sparseness problem as they cannot capture\nlarge vocabulary of words including proper names, domain-specific terms,\ntechnical jargons, special acronyms, and terminologies. As a result, they\nexhibit low error detection rate and often fail to catch major errors in the\ntext. This paper proposes a new context-sensitive spelling correction method\nfor detecting and correcting non-word and real-word errors in digital text\ndocuments. The approach hinges around data statistics from Google Web 1T 5-gram\ndata set which consists of a big volume of n-gram word sequences, extracted\nfrom the World Wide Web. Fundamentally, the proposed method comprises an error\ndetector that detects misspellings, a candidate spellings generator based on a\ncharacter 2-gram model that generates correction suggestions, and an error\ncorrector that performs contextual error correction. Experiments conducted on a\nset of text documents from different domains and containing misspellings,\nshowed an outstanding spelling error correction rate and a drastic reduction of\nboth non-word and real-word errors. In a further study, the proposed algorithm\nis to be parallelized so as to lower the computational cost of the error\ndetection and correction processes."
},{
    "category": "cs.CL", 
    "doi": "10.5539/cis.v5n3p37", 
    "link": "http://arxiv.org/pdf/1204.6364v1", 
    "title": "A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping   Prototype", 
    "arxiv-id": "1204.6364v1", 
    "author": "Quazi Mah-Zereen Akter", 
    "publish": "2012-04-28T03:52:21Z", 
    "summary": "The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)\nPrototype. The prototype is domain-specific, the purpose of which is to map\ninstructional text onto a knowledge domain. The context of the knowledge domain\nis DC electrical circuit. During development, the prototype has been tested\nwith a limited data set from the domain. The prototype reached a stage where it\nneeds to be evaluated with a representative linguistic data set called corpus.\nA corpus is a collection of text drawn from typical sources which can be used\nas a test data set to evaluate NLP systems. As there is no available corpus for\nthe domain, we developed and annotated a representative corpus. The evaluation\nof the prototype considers two of its major components- lexical components and\nknowledge model. Evaluation on lexical components enriches the lexical\nresources of the prototype like vocabulary and grammar structures. This leads\nthe prototype to parse a reasonable amount of sentences in the corpus. While\ndealing with the lexicon was straight forward, the identification and\nextraction of appropriate semantic relations was much more involved. It was\nnecessary, therefore, to manually develop a conceptual structure for the domain\nto formulate a domain-specific framework of semantic relations. The framework\nof semantic relationsthat has resulted from this study consisted of 55\nrelations, out of which 42 have inverse relations. We also conducted rhetorical\nanalysis on the corpus to prove its representativeness in conveying semantic.\nFinally, we conducted a topical and discourse analysis on the corpus to analyze\nthe coverage of discourse by the prototype."
},{
    "category": "cs.CL", 
    "doi": "10.5539/cis.v5n3p37", 
    "link": "http://arxiv.org/pdf/1207.0245v2", 
    "title": "Adversarial Evaluation for Models of Natural Language", 
    "arxiv-id": "1207.0245v2", 
    "author": "Noah A. Smith", 
    "publish": "2012-07-01T21:13:05Z", 
    "summary": "We now have a rich and growing set of modeling tools and algorithms for\ninducing linguistic structure from text that is less than fully annotated. In\nthis paper, we discuss some of the weaknesses of our current methodology. We\npresent a new abstract framework for evaluating natural language processing\n(NLP) models in general and unsupervised NLP models in particular. The central\nidea is to make explicit certain adversarial roles among researchers, so that\nthe different roles in an evaluation are more clearly defined and performers of\nall roles are offered ways to make measurable contributions to the larger goal.\nAdopting this approach may help to characterize model successes and failures by\nencouraging earlier consideration of error analysis. The framework can be\ninstantiated in a variety of ways, simulating some familiar intrinsic and\nextrinsic evaluations as well as some new evaluations."
},{
    "category": "cs.CL", 
    "doi": "10.5539/cis.v5n3p37", 
    "link": "http://arxiv.org/pdf/1207.1420v1", 
    "title": "Learning to Map Sentences to Logical Form: Structured Classification   with Probabilistic Categorial Grammars", 
    "arxiv-id": "1207.1420v1", 
    "author": "Michael Collins", 
    "publish": "2012-07-04T16:27:56Z", 
    "summary": "This paper addresses the problem of mapping natural language sentences to\nlambda-calculus encodings of their meaning. We describe a learning algorithm\nthat takes as input a training set of sentences labeled with expressions in the\nlambda calculus. The algorithm induces a grammar for the problem, along with a\nlog-linear model that represents a distribution over syntactic and semantic\nanalyses conditioned on the input sentence. We apply the method to the task of\nlearning natural language interfaces to databases and show that the learned\nparsers outperform previous methods in two benchmark database domains."
},{
    "category": "cs.CL", 
    "doi": "10.5539/cis.v5n3p37", 
    "link": "http://arxiv.org/pdf/1207.2714v1", 
    "title": "Clustering based approach extracting collocations", 
    "arxiv-id": "1207.2714v1", 
    "author": "Mohsen Maraoui", 
    "publish": "2012-07-11T17:31:11Z", 
    "summary": "The following study presents a collocation extraction approach based on\nclustering technique. This study uses a combination of several classical\nmeasures which cover all aspects of a given corpus then it suggests separating\nbigrams found in the corpus in several disjoint groups according to the\nprobability of presence of collocations. This will allow excluding groups where\nthe presence of collocations is very unlikely and thus reducing in a meaningful\nway the search space."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1207.3932v1", 
    "title": "Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units", 
    "arxiv-id": "1207.3932v1", 
    "author": "Sivaji Bandyopadhyay", 
    "publish": "2012-07-17T10:14:24Z", 
    "summary": "The work of automatic segmentation of a Manipuri language (or Meiteilon) word\ninto syllabic units is demonstrated in this paper. This language is a scheduled\nIndian language of Tibeto-Burman origin, which is also a very highly\nagglutinative language. This language usages two script: a Bengali script and\nMeitei Mayek (Script). The present work is based on the second script. An\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\nkind for this language."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1207.4625v1", 
    "title": "Appropriate Nouns with Obligatory Modifiers", 
    "arxiv-id": "1207.4625v1", 
    "author": "E. Laporte", 
    "publish": "2012-07-19T12:04:26Z", 
    "summary": "The notion of appropriate sequence as introduced by Z. Harris provides a\npowerful syntactic way of analysing the detailed meaning of various sentences,\nincluding ambiguous ones. In an adjectival sentence like 'The leather was\nyellow', the introduction of an appropriate noun, here 'colour', specifies\nwhich quality the adjective describes. In some other adjectival sentences with\nan appropriate noun, that noun plays the same part as 'colour' and seems to be\nrelevant to the description of the adjective. These appropriate nouns can\nusually be used in elementary sentences like 'The leather had some colour', but\nin many cases they have a more or less obligatory modifier. For example, you\ncan hardly mention that an object has a colour without qualifying that colour\nat all. About 300 French nouns are appropriate in at least one adjectival\nsentence and have an obligatory modifier. They enter in a number of sentence\nstructures related by several syntactic transformations. The appropriateness of\nthe noun and the fact that the modifier is obligatory are reflected in these\ntransformations. The description of these syntactic phenomena provides a basis\nfor a classification of these nouns. It also concerns the lexical properties of\nthousands of predicative adjectives, and in particular the relations between\nthe sentence without the noun : 'The leather was yellow' and the adjectival\nsentence with the noun : 'The colour of the leather was yellow'."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1207.5328v3", 
    "title": "A prototype for projecting HPSG syntactic lexica towards LMF", 
    "arxiv-id": "1207.5328v3", 
    "author": "Laurent Romary", 
    "publish": "2012-07-23T09:02:18Z", 
    "summary": "The comparative evaluation of Arabic HPSG grammar lexica requires a deep\nstudy of their linguistic coverage. The complexity of this task results mainly\nfrom the heterogeneity of the descriptive components within those lexica\n(underlying linguistic resources and different data categories, for example).\nIt is therefore essential to define more homogeneous representations, which in\nturn will enable us to compare them and eventually merge them. In this context,\nwe present a method for comparing HPSG lexica based on a rule system. This\nmethod is implemented within a prototype for the projection from Arabic HPSG to\na normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup\nFramework) and serialised using a TEI (Text Encoding Initiative) based\nrepresentation. The design of this system is based on an initial study of the\nHPSG formalism looking at its adequacy for the representation of Arabic, and\nfrom this, we identify the appropriate feature structures corresponding to each\nArabic lexical category and their possible LMF counterparts."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.2444v3", 
    "title": "TEI and LMF crosswalks", 
    "arxiv-id": "1301.2444v3", 
    "author": "Laurent Romary", 
    "publish": "2013-01-11T10:38:09Z", 
    "summary": "The present paper explores various arguments in favour of making the Text\nEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO\nstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the\nissues that would have to be resolved in order to reach an appropriate\nimplementation of these ideas, in particular in terms of infor-mational\ncoverage. We show how the customisation facilities offered by the TEI\nguidelines can provide an adequate background, not only to cover missing\ncomponents within the current Dictionary chapter of the TEI guidelines, but\nalso to allow specific lexical projects to deal with local constraints. We\nexpect this proposal to be a basis for a future ISO project in the context of\nthe on going revision of LMF."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.2857v1", 
    "title": "SpeedRead: A Fast Named Entity Recognition Pipeline", 
    "arxiv-id": "1301.2857v1", 
    "author": "Steven Skiena", 
    "publish": "2013-01-14T04:01:25Z", 
    "summary": "Online content analysis employs algorithmic methods to identify entities in\nunstructured text. Both machine learning and knowledge-base approaches lie at\nthe foundation of contemporary named entities extraction systems. However, the\nprogress in deploying these approaches on web-scale has been been hampered by\nthe computational cost of NLP over massive text corpora. We present SpeedRead\n(SR), a named entity recognition pipeline that runs at least 10 times faster\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\ntagger and knowledge-based named entity recognizer."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.3214v1", 
    "title": "The Manifold of Human Emotions", 
    "arxiv-id": "1301.3214v1", 
    "author": "Irfan Essa", 
    "publish": "2013-01-15T03:45:27Z", 
    "summary": "Sentiment analysis predicts the presence of positive or negative emotions in\na text document. In this paper, we consider higher dimensional extensions of\nthe sentiment concept, which represent a richer set of human emotions. Our\napproach goes beyond previous work in that our model contains a continuous\nmanifold rather than a finite set of human emotions. We investigate the\nresulting model, compare it to psychological observations, and explore its\npredictive capabilities."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.3614v2", 
    "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine   Translation", 
    "arxiv-id": "1301.3614v2", 
    "author": "Tsuyoshi Okita", 
    "publish": "2013-01-16T07:56:20Z", 
    "summary": "A neural probabilistic language model (NPLM) provides an idea to achieve the\nbetter perplexity than n-gram language model and their smoothed language\nmodels. This paper investigates application area in bilingual NLP, specifically\nStatistical Machine Translation (SMT). We focus on the perspectives that NPLM\nhas potential to open the possibility to complement potentially `huge'\nmonolingual resources into the `resource-constraint' bilingual resources. We\nintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesian\nconstruction. In order to facilitate the application to various tasks, we\npropose the joint space model of ngram-HMM language model. We show an\nexperiment of system combination in the area of SMT. One discovery was that our\ntreatment of noise improved the results 0.20 BLEU points if NPLM is trained in\nrelatively small corpus, in our case 500,000 sentence pairs, which is often the\ncase due to the long training time of NPLM."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.3781v3", 
    "title": "Efficient Estimation of Word Representations in Vector Space", 
    "arxiv-id": "1301.3781v3", 
    "author": "Jeffrey Dean", 
    "publish": "2013-01-16T18:24:43Z", 
    "summary": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1301.4432v1", 
    "title": "Language learning from positive evidence, reconsidered: A   simplicity-based approach", 
    "arxiv-id": "1301.4432v1", 
    "author": "Paul M. B. Vit\u00e1nyi", 
    "publish": "2013-01-18T16:53:13Z", 
    "summary": "Children learn their native language by exposure to their linguistic and\ncommunicative environment, but apparently without requiring that their mistakes\nare corrected. Such learning from positive evidence has been viewed as raising\nlogical problems for language acquisition. In particular, without correction,\nhow is the child to recover from conjecturing an over-general grammar, which\nwill be consistent with any sentence that the child hears? There have been many\nproposals concerning how this logical problem can be dissolved. Here, we review\nrecent formal results showing that the learner has sufficient data to learn\nsuccessfully from positive evidence, if it favours the simplest encoding of the\nlinguistic input. Results include the ability to learn a linguistic prediction,\ngrammaticality judgements, language production, and form-meaning mappings. The\nsimplicity approach can also be scaled-down to analyse the ability to learn a\nspecific linguistic constructions, and is amenable to empirical test as a\nframework for describing human language acquisition."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.3265v1", 
    "title": "Extension of hidden markov model for recognizing large vocabulary of   sign language", 
    "arxiv-id": "1304.3265v1", 
    "author": "Mohamed Jemni", 
    "publish": "2013-04-11T11:56:39Z", 
    "summary": "Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.4520v1", 
    "title": "Sentiment Analysis : A Literature Survey", 
    "arxiv-id": "1304.4520v1", 
    "author": "Pushpak Bhattacharyya", 
    "publish": "2013-04-16T17:06:24Z", 
    "summary": "Our day-to-day life has always been influenced by what people think. Ideas\nand opinions of others have always affected our own opinions. The explosion of\nWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,\nContributing to RSS, Social Bookmarking, and Social Networking. As a result\nthere has been an eruption of interest in people to mine these vast resources\nof data for opinions. Sentiment Analysis or Opinion Mining is the computational\ntreatment of opinions, sentiments and subjectivity of text. In this report, we\ntake a look at the various challenges and applications of Sentiment Analysis.\nWe will discuss in details various approaches to perform a computational\ntreatment of sentiments and opinions. Various supervised or data-driven\ntechniques to SA like Na\\\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons\nwill be discussed and their strengths and drawbacks will be touched upon. We\nwill also see a new dimension of analyzing sentiments by Cognitive Psychology\nmainly through the work of Janyce Wiebe, where we will see ways to detect\nsubjectivity, perspective in narrative and understanding the discourse\nstructure. We will also study some specific topics in Sentiment Analysis and\nthe contemporary works in those areas."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.5880v1", 
    "title": "Dealing with natural language interfaces in a geolocation context", 
    "arxiv-id": "1304.5880v1", 
    "author": "Anna Pappa", 
    "publish": "2013-04-22T09:06:36Z", 
    "summary": "In the geolocation field where high-level programs and low-level devices\ncoexist, it is often difficult to find a friendly user inter- face to configure\nall the parameters. The challenge addressed in this paper is to propose\nintuitive and simple, thus natural lan- guage interfaces to interact with\nlow-level devices. Such inter- faces contain natural language processing and\nfuzzy represen- tations of words that facilitate the elicitation of\nbusiness-level objectives in our context."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.7282v1", 
    "title": "An Improved Approach for Word Ambiguity Removal", 
    "arxiv-id": "1304.7282v1", 
    "author": "Urmila Shrawankar", 
    "publish": "2013-04-25T10:25:41Z", 
    "summary": "Word ambiguity removal is a task of removing ambiguity from a word, i.e.\ncorrect sense of word is identified from ambiguous sentences. This paper\ndescribes a model that uses Part of Speech tagger and three categories for word\nsense disambiguation (WSD). Human Computer Interaction is very needful to\nimprove interactions between users and computers. For this, the Supervised and\nUnsupervised methods are combined. The WSD algorithm is used to find the\nefficient and accurate sense of a word based on domain information. The\naccuracy of this work is evaluated with the aim of finding best suitable domain\nof word."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.7289v1", 
    "title": "TimeML-strict: clarifying temporal annotation", 
    "arxiv-id": "1304.7289v1", 
    "author": "Naushad UzZaman", 
    "publish": "2013-04-26T21:31:08Z", 
    "summary": "TimeML is an XML-based schema for annotating temporal information over\ndiscourse. The standard has been used to annotate a variety of resources and is\nfollowed by a number of tools, the creation of which constitute hundreds of\nthousands of man-hours of research work. However, the current state of\nresources is such that many are not valid, or do not produce valid output, or\ncontain ambiguous or custom additions and removals. Difficulties arising from\nthese variances were highlighted in the TempEval-3 exercise, which included its\nown extra stipulations over conventional TimeML as a response.\n  To unify the state of current resources, and to make progress toward easy\nadoption of its current incarnation ISO-TimeML, this paper introduces\nTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We\nalso introduce three resources -- a schema for TimeML-strict; a validator tool\nfor TimeML-strict, so that one may ensure documents are in the correct form;\nand a repair tool that corrects common invalidating errors and adds\ndisambiguating markup in order to convert documents from the laxer TimeML\nstandard to TimeML-strict."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1304.7942v1", 
    "title": "ManTIME: Temporal expression identification and normalization in the   TempEval-3 challenge", 
    "arxiv-id": "1304.7942v1", 
    "author": "Goran Nenadic", 
    "publish": "2013-04-30T10:12:54Z", 
    "summary": "This paper describes a temporal expression identification and normalization\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\nphase combines the use of conditional random fields along with a\npost-processing identification pipeline, whereas the normalization phase is\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\ninvestigate the performance variation with respect to different feature types.\nSpecifically, we show that the use of WordNet-based features in the\nidentification task negatively affects the overall performance, and that there\nis no statistically significant difference in using gazetteers, shallow parsing\nand propositional noun phrases labels on top of the morphological features. On\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\naddition to the gold annotated ones) does not improve the performance."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.1319v1", 
    "title": "New Alignment Methods for Discriminative Book Summarization", 
    "arxiv-id": "1305.1319v1", 
    "author": "Noah A. Smith", 
    "publish": "2013-05-06T20:27:55Z", 
    "summary": "We consider the unsupervised alignment of the full text of a book with a\nhuman-written summary. This presents challenges not seen in other text\nalignment problems, including a disparity in length and, consequent to this, a\nviolation of the expectation that individual words and phrases should align,\nsince large passages and chapters can be distilled into a single summary\nphrase. We present two new methods, based on hidden Markov models, specifically\ntargeted to this problem, and demonstrate gains on an extractive book\nsummarization task. While there is still much room for improvement,\nunsupervised alignment holds intrinsic value in offering insight into what\nfeatures of a book are deemed worthy of summarization."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.3882v2", 
    "title": "Rule-Based Semantic Tagging. An Application Undergoing Dictionary   Glosses", 
    "arxiv-id": "1305.3882v2", 
    "author": "Daniel Christen", 
    "publish": "2013-05-16T18:09:21Z", 
    "summary": "The project presented in this article aims to formalize criteria and\nprocedures in order to extract semantic information from parsed dictionary\nglosses. The actual purpose of the project is the generation of a semantic\nnetwork (nearly an ontology) issued from a monolingual Italian dictionary,\nthrough unsupervised procedures. Since the project involves rule-based Parsing,\nSemantic Tagging and Word Sense Disambiguation techniques, its outcomes may\nfind an interest also beyond this immediate intent. The cooperation of both\nsyntactic and semantic features in meaning construction are investigated, and\nprocedures which allows a translation of syntactic dependencies in semantic\nrelations are discussed. The procedures that rise from this project can be\napplied also to other text types than dictionary glosses, as they convert the\noutput of a parsing process into a semantic representation. In addition some\nmechanism are sketched that may lead to a kind of procedural semantics, through\nwhich multiple paraphrases of an given expression can be generated. Which means\nthat these techniques may find an application also in 'query expansion'\nstrategies, interesting Information Retrieval, Search Engines and Question\nAnswering Systems."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.3981v1", 
    "title": "Binary Tree based Chinese Word Segmentation", 
    "arxiv-id": "1305.3981v1", 
    "author": "Maosong Sun", 
    "publish": "2013-05-17T05:14:43Z", 
    "summary": "Chinese word segmentation is a fundamental task for Chinese language\nprocessing. The granularity mismatch problem is the main cause of the errors.\nThis paper showed that the binary tree representation can store outputs with\ndifferent granularity. A binary tree based framework is also designed to\novercome the granularity mismatch problem. There are two steps in this\nframework, namely tree building and tree pruning. The tree pruning step is\nspecially designed to focus on the granularity problem. Previous work for\nChinese word segmentation such as the sequence tagging can be easily employed\nin this framework. This framework can also provide quantitative error analysis\nmethods. The experiments showed that after using a more sophisticated tree\npruning function for a state-of-the-art conditional random field based\nbaseline, the error reduction can be up to 20%."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.5753v3", 
    "title": "A probabilistic framework for analysing the compositionality of   conceptual combinations", 
    "arxiv-id": "1305.5753v3", 
    "author": "Laurianne Sitbon", 
    "publish": "2013-05-23T03:14:50Z", 
    "summary": "Conceptual combination performs a fundamental role in creating the broad\nrange of compound phrases utilized in everyday language. This article provides\na novel probabilistic framework for assessing whether the semantics of\nconceptual combinations are compositional, and so can be considered as a\nfunction of the semantics of the constituent concepts, or not. While the\nsystematicity and productivity of language provide a strong argument in favor\nof assuming compositionality, this very assumption is still regularly\nquestioned in both cognitive science and philosophy. Additionally, the\nprinciple of semantic compositionality is underspecified, which means that\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\nRather than adjudicating between different grades of compositionality, the\nframework presented here contributes formal methods for determining a clear\ndividing line between compositional and non-compositional semantics. In\naddition, we suggest that the distinction between these is contextually\nsensitive. Utilizing formal frameworks developed for analyzing composite\nsystems in quantum theory, we present two methods that allow the semantics of\nconceptual combinations to be classified as \"compositional\" or\n\"non-compositional\". Compositionality is first formalised by factorising the\njoint probability distribution modeling the combination, where the terms in the\nfactorisation correspond to individual concepts. This leads to the necessary\nand sufficient condition for the joint probability distribution to exist. A\nfailure to meet this condition implies that the underlying concepts cannot be\nmodeled in a single probability space when considering their combination, and\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\nare demonstrated by applying them to an empirical study of twenty-four\nnon-lexicalised conceptual combinations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.5785v1", 
    "title": "An Inventory of Preposition Relations", 
    "arxiv-id": "1305.5785v1", 
    "author": "Dan Roth", 
    "publish": "2013-05-24T16:34:22Z", 
    "summary": "We describe an inventory of semantic relations that are expressed by\nprepositions. We define these relations by building on the word sense\ndisambiguation task for prepositions and propose a mapping from preposition\nsenses to the relation labels by collapsing semantically related senses across\nprepositions."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.5918v1", 
    "title": "Reduce Meaningless Words for Joint Chinese Word Segmentation and   Part-of-speech Tagging", 
    "arxiv-id": "1305.5918v1", 
    "author": "Maosong Sun", 
    "publish": "2013-05-25T13:20:31Z", 
    "summary": "Conventional statistics-based methods for joint Chinese word segmentation and\npart-of-speech tagging (S&T) have generalization ability to recognize new words\nthat do not appear in the training data. An undesirable side effect is that a\nnumber of meaningless words will be incorrectly created. We propose an\neffective and efficient framework for S&T that introduces features to\nsignificantly reduce meaningless words generation. A general lexicon, Wikepedia\nand a large-scale raw corpus of 200 billion characters are used to generate\nword-based features for the wordhood. The word-lattice based framework consists\nof a character-based model and a word-based model in order to employ our\nword-based features. Experiments on Penn Chinese treebank 5 show that this\nmethod has a 62.9% reduction of meaningless word generation in comparison with\nthe baseline. As a result, the F1 measure for segmentation is increased to\n0.984."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1305.6211v2", 
    "title": "Development of a Hindi Lemmatizer", 
    "arxiv-id": "1305.6211v2", 
    "author": "Iti Mathur", 
    "publish": "2013-05-24T18:01:34Z", 
    "summary": "We live in a translingual society, in order to communicate with people from\ndifferent parts of the world we need to have an expertise in their respective\nlanguages. Learning all these languages is not at all possible; therefore we\nneed a mechanism which can do this task for us. Machine translators have\nemerged as a tool which can perform this task. In order to develop a machine\ntranslator we need to develop several different rules. The very first module\nthat comes in machine translation pipeline is morphological analysis. Stemming\nand lemmatization comes under morphological analysis. In this paper we have\ncreated a lemmatizer which generates rules for removing the affixes along with\nthe addition of rules for creating a proper root word."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1306.2091v2", 
    "title": "A framework for (under)specifying dependency syntax without overloading   annotators", 
    "arxiv-id": "1306.2091v2", 
    "author": "Jason Baldridge", 
    "publish": "2013-06-10T02:54:10Z", 
    "summary": "We introduce a framework for lightweight dependency syntax annotation. Our\nformalism builds upon the typical representation for unlabeled dependencies,\npermitting a simple notation and annotation workflow. Moreover, the formalism\nencourages annotators to underspecify parts of the syntax if doing so would\nstreamline the annotation process. We demonstrate the efficacy of this\nannotation on three languages and develop algorithms to evaluate and compare\nunderspecified annotations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1306.2158v1", 
    "title": "\"Not not bad\" is not \"bad\": A distributional account of negation", 
    "arxiv-id": "1306.2158v1", 
    "author": "Phil Blunsom", 
    "publish": "2013-06-10T10:29:09Z", 
    "summary": "With the increasing empirical success of distributional models of\ncompositional semantics, it is timely to consider the types of textual logic\nthat such models are capable of capturing. In this paper, we address\nshortcomings in the ability of current models to capture logical operations\nsuch as negation. As a solution we propose a tripartite formulation for a\ncontinuous vector space representation of semantics and subsequently use this\nrepresentation to develop a formal compositional notion of negation within such\nmodels."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1306.3584v1", 
    "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", 
    "arxiv-id": "1306.3584v1", 
    "author": "Phil Blunsom", 
    "publish": "2013-06-15T14:52:17Z", 
    "summary": "The compositionality of meaning extends beyond the single sentence. Just as\nwords combine to form the meaning of sentences, so do sentences combine to form\nthe meaning of paragraphs, dialogues and general discourse. We introduce both a\nsentence model and a discourse model corresponding to the two levels of\ncompositionality. The sentence model adopts convolution as the central\noperation for composing semantic vectors and is based on a novel hierarchical\nconvolutional neural network. The discourse model extends the sentence model\nand is based on a recurrent neural network that is conditioned in a novel way\nboth on the current sentence and on the current speaker. The discourse model is\nable to capture both the sequentiality of sentences and the interaction between\ndifferent speakers. Without feature engineering or pretraining and with simple\ngreedy decoding, the discourse model coupled to the sentence model obtains\nstate of the art performance on a dialogue act classification experiment."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1306.4134v1", 
    "title": "Dialogue System: A Brief Review", 
    "arxiv-id": "1306.4134v1", 
    "author": "Sarabjit Singh", 
    "publish": "2013-06-18T10:32:43Z", 
    "summary": "A Dialogue System is a system which interacts with human in natural language.\nAt present many universities are developing the dialogue system in their\nregional language. This paper will discuss about dialogue system, its\ncomponents, challenges and its evaluation. This paper helps the researchers for\ngetting info regarding dialogues system."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1306.6130v1", 
    "title": "Competency Tracking for English as a Second or Foreign Language Learners", 
    "arxiv-id": "1306.6130v1", 
    "author": "Robert Bishop Jr", 
    "publish": "2013-06-26T05:02:26Z", 
    "summary": "My system utilizes the outcomes feature found in Moodle and other learning\ncontent management systems (LCMSs) to keep track of where students are in terms\nof what language competencies they have mastered and the competencies they need\nto get where they want to go. These competencies are based on the Common\nEuropean Framework for (English) Language Learning. This data can be available\nfor everyone involved with a given student's progress (e.g. educators, parents,\nsupervisors and the students themselves). A given student's record of past\naccomplishments can also be meshed with those of his classmates. Not only are a\nstudent's competencies easily seen and tracked, educators can view competencies\nof a group of students that were achieved prior to enrollment in the class.\nThis should make curriculum decision making easier and more efficient for\neducators."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1307.0596v1", 
    "title": "Improving Pointwise Mutual Information (PMI) by Incorporating   Significant Co-occurrence", 
    "arxiv-id": "1307.0596v1", 
    "author": "Om P. Damani", 
    "publish": "2013-07-02T06:25:51Z", 
    "summary": "We design a new co-occurrence based word association measure by incorporating\nthe concept of significant cooccurrence in the popular word association measure\nPointwise Mutual Information (PMI). By extensive experiments with a large\nnumber of publicly available datasets we show that the newly introduced measure\nperforms better than other co-occurrence based measures and despite being\nresource-light, compares well with the best known resource-heavy distributional\nsimilarity and knowledge based word association measures. We investigate the\nsource of this performance improvement and find that of the two types of\nsignificant co-occurrence - corpus-level and document-level, the concept of\ncorpus level significance combined with the use of document counts in place of\nword counts is responsible for all the performance gains observed. The concept\nof document level significance is not helpful for PMI adaptation."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit.2012.4311", 
    "link": "http://arxiv.org/pdf/1307.1872v1", 
    "title": "Intelligent Hybrid Man-Machine Translation Quality Estimation", 
    "arxiv-id": "1307.1872v1", 
    "author": "Mona Habib", 
    "publish": "2013-07-07T15:04:11Z", 
    "summary": "Inferring evaluation scores based on human judgments is invaluable compared\nto using current evaluation metrics which are not suitable for real-time\napplications e.g. post-editing. However, these judgments are much more\nexpensive to collect especially from expert translators, compared to evaluation\nbased on indicators contrasting source and translation texts. This work\nintroduces a novel approach for quality estimation by combining learnt\nconfidence scores from a probabilistic inference model based on human\njudgments, with selective linguistic features-based scores, where the proposed\ninference model infers the credibility of given human ranks to solve the\nscarcity and inconsistency issues of human judgments. Experimental results,\nusing challenging language-pairs, demonstrate improvement in correlation with\nhuman judgments over traditional evaluation metrics."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2305", 
    "link": "http://arxiv.org/pdf/1307.3310v1", 
    "title": "Improving the quality of Gujarati-Hindi Machine Translation through   part-of-speech tagging and stemmer-assisted transliteration", 
    "arxiv-id": "1307.3310v1", 
    "author": "Iti Mathur", 
    "publish": "2013-07-12T03:05:29Z", 
    "summary": "Machine Translation for Indian languages is an emerging research area.\nTransliteration is one such module that we design while designing a translation\nsystem. Transliteration means mapping of source language text into the target\nlanguage. Simple mapping decreases the efficiency of overall translation\nsystem. We propose the use of stemming and part-of-speech tagging for\ntransliteration. The effectiveness of translation can be improved if we use\npart-of-speech tagging and stemming assisted transliteration.We have shown that\nmuch of the content in Gujarati gets transliterated while being processed for\ntranslation to Hindi language."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijait.2013.3203", 
    "link": "http://arxiv.org/pdf/1307.4299v1", 
    "title": "Part of Speech Tagging of Marathi Text Using Trigram Method", 
    "arxiv-id": "1307.4299v1", 
    "author": "Iti Mathur", 
    "publish": "2013-07-15T15:59:12Z", 
    "summary": "In this paper we present a Marathi part of speech tagger. It is a\nmorphologically rich language. It is spoken by the native people of\nMaharashtra. The general approach used for development of tagger is statistical\nusing trigram Method. The main concept of trigram is to explore the most likely\nPOS for a token based on given information of previous two tags by calculating\nprobabilities to determine which is the best sequence of a tag. In this paper\nwe show the development of the tagger. Moreover we have also shown the\nevaluation done."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1307.4300v1", 
    "title": "Rule Based Transliteration Scheme for English to Punjabi", 
    "arxiv-id": "1307.4300v1", 
    "author": "Iti Mathur", 
    "publish": "2013-07-15T15:54:43Z", 
    "summary": "Machine Transliteration has come out to be an emerging and a very important\nresearch area in the field of machine translation. Transliteration basically\naims to preserve the phonological structure of words. Proper transliteration of\nname entities plays a very significant role in improving the quality of machine\ntranslation. In this paper we are doing machine transliteration for\nEnglish-Punjabi language pair using rule based approach. We have constructed\nsome rules for syllabification. Syllabification is the process to extract or\nseparate the syllable from the words. In this we are calculating the\nprobabilities for name entities (Proper names and location). For those words\nwhich do not come under the category of name entities, separate probabilities\nare being calculated by using relative frequency through a statistical machine\ntranslation toolkit known as MOSES. Using these probabilities we are\ntransliterating our input text from English to Punjabi."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1307.5393v1", 
    "title": "Clustering Algorithm for Gujarati Language", 
    "arxiv-id": "1307.5393v1", 
    "author": "Prem Balani", 
    "publish": "2013-07-20T09:05:15Z", 
    "summary": "Natural language processing area is still under research. But now a day it is\non platform for worldwide researchers. Natural language processing includes\nanalyzing the language based on its structure and then tagging of each word\nappropriately with its grammar base. Here we have 50,000 tagged words set and\nwe try to cluster those Gujarati words based on proposed algorithm, we have\ndefined our own algorithm for processing. Many clustering techniques are\navailable Ex. Single linkage, complete, linkage,average linkage, Hear no of\nclusters to be formed are not known, so it is all depends on the type of data\nset provided . Clustering is preprocess for stemming . Stemming is the process\nwhere root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and\nplural form."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1307.6163v2", 
    "title": "Human and Automatic Evaluation of English-Hindi Machine Translation", 
    "arxiv-id": "1307.6163v2", 
    "author": "Iti Mathur", 
    "publish": "2013-07-23T17:15:34Z", 
    "summary": "For the past 60 years, Research in machine translation is going on. For the\ndevelopment in this field, a lot of new techniques are being developed each\nday. As a result, we have witnessed development of many automatic machine\ntranslators. A manager of machine translation development project needs to know\nthe performance increase/decrease, after changes have been done in his system.\nDue to this reason, a need for evaluation of machine translation systems was\nfelt. In this article, we shall present the evaluation of some machine\ntranslators. This evaluation will be done by a human evaluator and by some\nautomatic evaluation metrics, which will be done at sentence, document and\nsystem level. In the end we shall also discuss the comparison between the\nevaluations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1307.7382v1", 
    "title": "Learning Frames from Text with an Unsupervised Latent Variable Model", 
    "arxiv-id": "1307.7382v1", 
    "author": "Brendan O'Connor", 
    "publish": "2013-07-28T16:55:27Z", 
    "summary": "We develop a probabilistic latent-variable model to discover semantic\nframes---types of events and their participants---from corpora. We present a\nDirichlet-multinomial model in which frames are latent categories that explain\nthe linking of verb-subject-object triples, given document-level sparsity. We\nanalyze what the model learns, and compare it to FrameNet, noting it learns\nsome novel and interesting frames. This document also contains a discussion of\ninference issues, including concentration parameter learning; and a small-scale\nerror analysis of syntactic parsing accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.0573v1", 
    "title": "Improving the Quality of MT Output using Novel Name Entity Translation   Scheme", 
    "arxiv-id": "1310.0573v1", 
    "author": "Iti Mathur", 
    "publish": "2013-10-02T05:58:52Z", 
    "summary": "This paper presents a novel approach to machine translation by combining the\nstate of art name entity translation scheme. Improper translation of name\nentities lapse the quality of machine translated output. In this work, name\nentities are transliterated by using statistical rule based approach. This\npaper describes the translation and transliteration of name entities from\nEnglish to Punjabi. We have experimented on four types of name entities which\nare: Proper names, Location names, Organization names and miscellaneous.\nVarious rules for the purpose of syllabification have been constructed.\nTransliteration of name entities is accomplished with the help of Probability\ncalculation. N-Gram probabilities for the extracted syllables have been\ncalculated using statistical machine translation toolkit MOSES."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.0575v2", 
    "title": "Development of Marathi Part of Speech Tagger Using Statistical Approach", 
    "arxiv-id": "1310.0575v2", 
    "author": "Iti Mathur", 
    "publish": "2013-10-02T06:04:53Z", 
    "summary": "Part-of-speech (POS) tagging is a process of assigning the words in a text\ncorresponding to a particular part of speech. A fundamental version of POS\ntagging is the identification of words as nouns, verbs, adjectives etc. For\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\none of the simplest as well as most constant and statistical model for many NLP\napplications. POS Tagging is an initial stage of linguistics, text analysis\nlike information retrieval, machine translator, text to speech synthesis,\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\neach word in a sentence and literature. Various approaches have been proposed\nto implement POS taggers. In this paper we present a Marathi part of speech\ntagger. It is morphologically rich language. Marathi is spoken by the native\npeople of Maharashtra. The general approach used for development of tagger is\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\nidea about all the algorithms with suitable examples. It also introduces a tag\nset for Marathi which can be used for tagging Marathi text. In this paper we\nhave shown the development of the tagger as well as compared to check the\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\nrespectively."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.0578v1", 
    "title": "Subjective and Objective Evaluation of English to Urdu Machine   Translation", 
    "arxiv-id": "1310.0578v1", 
    "author": "Iti Mathur", 
    "publish": "2013-10-02T06:10:49Z", 
    "summary": "Machine translation is research based area where evaluation is very important\nphenomenon for checking the quality of MT output. The work is based on the\nevaluation of English to Urdu Machine translation. In this research work we\nhave evaluated the translation quality of Urdu language which has been\ntranslated by using different Machine Translation systems like Google, Babylon\nand Ijunoon. The evaluation process is done by using two approaches - Human\nevaluation and Automatic evaluation. We have worked for both the approaches\nwhere in human evaluation emphasis is given to scales and parameters while in\nautomatic evaluation emphasis is given to some automatic metric such as BLEU,\nGTM, METEOR and ATEC."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.0581v1", 
    "title": "Rule Based Stemmer in Urdu", 
    "arxiv-id": "1310.0581v1", 
    "author": "Iti Mathur", 
    "publish": "2013-10-02T06:15:03Z", 
    "summary": "Urdu is a combination of several languages like Arabic, Hindi, English,\nTurkish, Sanskrit etc. It has a complex and rich morphology. This is the reason\nwhy not much work has been done in Urdu language processing. Stemming is used\nto convert a word into its respective root form. In stemming, we separate the\nsuffix and prefix from the word. It is useful in search engines, natural\nlanguage processing and word processing, spell checkers, word parsing, word\nfrequency and count studies. This paper presents a rule based stemmer for Urdu.\nThe stemmer that we have discussed here is used in information retrieval. We\nhave also evaluated our results by verifying it with a human expert."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.0754v1", 
    "title": "Stemmers for Tamil Language: Performance Analysis", 
    "arxiv-id": "1310.0754v1", 
    "author": "R. Manavalan", 
    "publish": "2013-10-02T16:23:00Z", 
    "summary": "Stemming is the process of extracting root word from the given inflection\nword and also plays significant role in numerous application of Natural\nLanguage Processing (NLP). Tamil Language raises several challenges to NLP,\nsince it has rich morphological patterns than other languages. The rule based\napproach light-stemmer is proposed in this paper, to find stem word for given\ninflection Tamil word. The performance of proposed approach is compared to a\nrule based suffix removal stemmer based on correctly and incorrectly predicted.\nThe experimental result clearly show that the proposed approach light stemmer\nfor Tamil language perform better than suffix removal stemmer and also more\neffective in Information Retrieval System (IRS)."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1285v3", 
    "title": "Semantic Measures for the Comparison of Units of Language, Concepts or   Instances from Text and Knowledge Base Analysis", 
    "arxiv-id": "1310.1285v3", 
    "author": "Jacky Montmain", 
    "publish": "2013-10-04T14:21:42Z", 
    "summary": "Semantic measures are widely used today to estimate the strength of the\nsemantic relationship between elements of various types: units of language\n(e.g., words, sentences, documents), concepts or even instances semantically\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\nmeasures play an important role to compare such elements according to semantic\nproxies: texts and knowledge representations, which support their meaning or\ndescribe their nature. Semantic measures are therefore essential for designing\nintelligent agents which will for example take advantage of semantic analysis\nto mimic human ability to compare abstract or concrete objects. This paper\nproposes a comprehensive survey of the broad notion of semantic measure for the\ncomparison of units of language, concepts or instances based on semantic proxy\nanalyses. Semantic measures generalize the well-known notions of semantic\nsimilarity, semantic relatedness and semantic distance, which have been\nextensively studied by various communities over the last decades (e.g.,\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few)."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1425v1", 
    "title": "A State of the Art of Word Sense Induction: A Way Towards Word Sense   Disambiguation for Under-Resourced Languages", 
    "arxiv-id": "1310.1425v1", 
    "author": "Mohammad Nasiruddin", 
    "publish": "2013-10-05T00:33:46Z", 
    "summary": "Word Sense Disambiguation (WSD), the process of automatically identifying the\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\npromising developments in the field of NLP and its applications. Indeed,\nimprovement over current performance levels could allow us to take a first step\ntowards natural language understanding. Due to the lack of lexical resources it\nis sometimes difficult to perform WSD for under-resourced languages. This paper\nis an investigation on how to initiate research in WSD for under-resourced\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\ntopics to focus on."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1426v1", 
    "title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is   Better for MLN-Based Bangla Speech Recognition?", 
    "arxiv-id": "1310.1426v1", 
    "author": "Mohammad Nurul Huda", 
    "publish": "2013-10-05T00:39:02Z", 
    "summary": "This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1590v1", 
    "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study", 
    "arxiv-id": "1310.1590v1", 
    "author": "Arnab Bhattacharya", 
    "publish": "2013-10-06T14:37:05Z", 
    "summary": "Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1964v1", 
    "title": "Named entity recognition using conditional random fields with non-local   relational constraints", 
    "arxiv-id": "1310.1964v1", 
    "author": "Elisabetta Fersini", 
    "publish": "2013-10-07T22:08:18Z", 
    "summary": "We begin by introducing the Computer Science branch of Natural Language\nProcessing, then narrowing the attention on its subbranch of Information\nExtraction and particularly on Named Entity Recognition, discussing briefly its\nmain methodological approaches. It follows an introduction to state-of-the-art\nConditional Random Fields under the form of linear chains. Subsequently, the\nidea of constrained inference as a way to model long-distance relationships in\na text is presented, based on an Integer Linear Programming representation of\nthe problem. Adding such relationships to the problem as automatically inferred\nlogical formulas, translatable into linear conditions, we propose to solve the\nresulting more complex problem with the aid of Lagrangian relaxation, of which\nsome technical details are explained. Lastly, we give some experimental\nresults."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2013.2207", 
    "link": "http://arxiv.org/pdf/1310.1975v1", 
    "title": "ARKref: a rule-based coreference resolution system", 
    "arxiv-id": "1310.1975v1", 
    "author": "Michael Heilman", 
    "publish": "2013-10-08T00:30:51Z", 
    "summary": "ARKref is a tool for noun phrase coreference. It is a deterministic,\nrule-based system that uses syntactic information from a constituent parser,\nand semantic information from an entity recognition component. Its architecture\nis based on the work of Haghighi and Klein (2009). ARKref was originally\nwritten in 2009. At the time of writing, the last released version was in March\n2011. This document describes that version, which is open-source and publicly\navailable at: http://www.ark.cs.cmu.edu/ARKref"
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1310.8059v1", 
    "title": "Description and Evaluation of Semantic Similarity Measures Approaches", 
    "arxiv-id": "1310.8059v1", 
    "author": "Thabet Slimani", 
    "publish": "2013-10-30T08:08:43Z", 
    "summary": "In recent years, semantic similarity measure has a great interest in Semantic\nWeb and Natural Language Processing (NLP). Several similarity measures have\nbeen developed, being given the existence of a structured knowledge\nrepresentation offered by ontologies and corpus which enable semantic\ninterpretation of terms. Semantic similarity measures compute the similarity\nbetween concepts/terms included in knowledge sources in order to perform\nestimations. This paper discusses the existing semantic similarity methods\nbased on structure, information content and feature approaches. Additionally,\nwe present a critical evaluation of several categories of semantic similarity\napproaches based on two standard benchmarks. The aim of this paper is to give\nan efficient evaluation of all these measures which help researcher and\npractitioners to select the measure that best fit for their requirements."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.0482v1", 
    "title": "Learning Semantic Representations for the Phrase Translation Model", 
    "arxiv-id": "1312.0482v1", 
    "author": "Li Deng", 
    "publish": "2013-11-28T04:58:59Z", 
    "summary": "This paper presents a novel semantic-based phrase translation model. A pair\nof source and target phrases are projected into continuous-valued vector\nrepresentations in a low-dimensional latent semantic space, where their\ntranslation score is computed by the distance between the pair in this new\nspace. The projection is performed by a multi-layer neural network whose\nweights are learned on parallel training data. The learning is aimed to\ndirectly optimize the quality of end-to-end machine translation results.\nExperimental evaluation has been performed on two Europarl translation tasks,\nEnglish-French and German-English. The results show that the new semantic-based\nphrase translation model significantly improves the performance of a\nstate-of-the-art phrase-based statistical machine translation sys-tem, leading\nto a gain of 0.7-1.0 BLEU points."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.2087v1", 
    "title": "Towards Structural Natural Language Formalization: Mapping Discourse to   Controlled Natural Language", 
    "arxiv-id": "1312.2087v1", 
    "author": "Nicholas H. Kirk", 
    "publish": "2013-12-07T11:19:20Z", 
    "summary": "The author describes a conceptual study towards mapping grounded natural\nlanguage discourse representation structures to instances of controlled\nlanguage statements. This can be achieved via a pipeline of preexisting state\nof the art technologies, namely natural language syntax to semantic discourse\nmapping, and a reduction of the latter to controlled language discourse, given\na set of previously learnt reduction rules. Concludingly a description on\nevaluation, potential and limitations for ontology-based reasoning is\npresented."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.3005v3", 
    "title": "One Billion Word Benchmark for Measuring Progress in Statistical   Language Modeling", 
    "arxiv-id": "1312.3005v3", 
    "author": "Tony Robinson", 
    "publish": "2013-12-11T00:25:57Z", 
    "summary": "We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.3168v1", 
    "title": "Semantic Types, Lexical Sorts and Classifiers", 
    "arxiv-id": "1312.3168v1", 
    "author": "Christian Retor\u00e9", 
    "publish": "2013-12-11T14:04:52Z", 
    "summary": "We propose a cognitively and linguistically motivated set of sorts for\nlexical semantics in a compositional setting: the classifiers in languages that\ndo have such pronouns. These sorts are needed to include lexical considerations\nin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical\nextensions of usual Montague semantics to model restriction of selection,\nfelicitous and infelicitous copredication require a rich and refined type\nsystem whose base types are the lexical sorts, the basis of the many-sorted\nlogic in which semantical representations of sentences are stated. However,\nnone of those approaches define precisely the actual base types or sorts to be\nused in the lexicon. In this article, we shall discuss some of the options\ncommonly adopted by researchers in formal lexical semantics, and defend the\nview that classifiers in the languages which have such pronouns are an\nappealing solution, both linguistically and cognitively motivated."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.3251v1", 
    "title": "Towards The Development of a Bishnupriya Manipuri Corpus", 
    "arxiv-id": "1312.3251v1", 
    "author": "Smriti Kumar Sinha", 
    "publish": "2013-12-11T17:24:35Z", 
    "summary": "For any deep computational processing of language we need evidences, and one\nsuch set of evidences is corpus. This paper describes the development of a\ntext-based corpus for the Bishnupriya Manipuri language. A Corpus is considered\nas a building block for any language processing tasks. Due to the lack of\nawareness like other Indian languages, it is also studied less frequently. As a\nresult the language still lacks a good corpus and basic language processing\ntools. As per our knowledge this is the first effort to develop a corpus for\nBishnupriya Manipuri language."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.3258v1", 
    "title": "Implicit Sensitive Text Summarization based on Data Conveyed by   Connectives", 
    "arxiv-id": "1312.3258v1", 
    "author": "Henda Chorfi Ouertani", 
    "publish": "2013-12-11T17:50:21Z", 
    "summary": "So far and trying to reach human capabilities, research in automatic\nsummarization has been based on hypothesis that are both enabling and limiting.\nSome of these limitations are: how to take into account and reflect (in the\ngenerated summary) the implicit information conveyed in the text, the author\nintention, the reader intention, the context influence, the general world\nknowledge. Thus, if we want machines to mimic human abilities, then they will\nneed access to this same large variety of knowledge. The implicit is affecting\nthe orientation and the argumentation of the text and consequently its summary.\nMost of Text Summarizers (TS) are processing as compressing the initial data\nand they necessarily suffer from information loss. TS are focusing on features\nof the text only, not on what the author intended or why the reader is reading\nthe text. In this paper, we address this problem and we present a system\nfocusing on acquiring knowledge that is implicit. We principally spotlight the\nimplicit information conveyed by the argumentative connectives such as: but,\neven, yet and their effect on the summary."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.5129v2", 
    "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", 
    "arxiv-id": "1312.5129v2", 
    "author": "Hinrich Sch\u00fctze", 
    "publish": "2013-12-18T13:34:16Z", 
    "summary": "Deep learning embeddings have been successfully used for many natural\nlanguage processing problems. Embeddings are mostly computed for word forms\nalthough a number of recent papers have extended this to other linguistic units\nlike morphemes and phrases. In this paper, we argue that learning embeddings\nfor discontinuous linguistic units should also be considered. In an\nexperimental evaluation on coreference resolution, we show that such embeddings\nperform better than word form embeddings."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.5559v3", 
    "title": "Distributional Models and Deep Learning Embeddings: Combining the Best   of Both Worlds", 
    "arxiv-id": "1312.5559v3", 
    "author": "Hinrich Sch\u00fctze", 
    "publish": "2013-12-19T14:18:14Z", 
    "summary": "There are two main approaches to the distributed representation of words:\nlow-dimensional deep learning embeddings and high-dimensional distributional\nmodels, in which each dimension corresponds to a context word. In this paper,\nwe combine these two approaches by learning embeddings based on\ndistributional-model vectors - as opposed to one-hot vectors as is standardly\ndone in deep learning. We show that the combined approach has better\nperformance on a word relatedness judgment task."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.6173v4", 
    "title": "Multilingual Distributed Representations without Word Alignment", 
    "arxiv-id": "1312.6173v4", 
    "author": "Phil Blunsom", 
    "publish": "2013-12-20T23:13:38Z", 
    "summary": "Distributed representations of meaning are a natural way to encode covariance\nrelationships between words and phrases in NLP. By overcoming data sparsity\nproblems, as well as providing information about semantic relatedness which is\nnot available in discrete representations, distributed representations have\nproven useful in many NLP tasks. Recent work has shown how compositional\nsemantic representations can successfully be applied to a number of monolingual\napplications such as sentiment analysis. At the same time, there has been some\ninitial success in work on learning shared word-level representations across\nlanguages. We combine these two approaches by proposing a method for learning\ndistributed representations in a multilingual setup. Our model learns to assign\nsimilar embeddings to aligned sentences and dissimilar ones to sentence which\nare not aligned while not requiring word alignments. We show that our\nrepresentations are semantically informative and apply them to a cross-lingual\ndocument classification task where we outperform the previous state of the art.\nFurther, by employing parallel corpora of multiple language pairs we find that\nour model learns representations that capture semantic relationships across\nlanguages for which no parallel data was used."
},{
    "category": "cs.CL", 
    "doi": "10.5120/13897-1851", 
    "link": "http://arxiv.org/pdf/1312.7223v1", 
    "title": "Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier", 
    "arxiv-id": "1312.7223v1", 
    "author": "Iti Mathur", 
    "publish": "2013-12-27T09:39:52Z", 
    "summary": "In this paper we present an approach for estimating the quality of machine\ntranslation system. There are various methods for estimating the quality of\noutput sentences, but in this paper we focus on Na\\\"ive Bayes classifier to\nbuild model using features which are extracted from the input sentences. These\nfeatures are used for finding the likelihood of each of the sentences of the\ntraining data which are then further used for determining the scores of the\ntest data. On the basis of these scores we determine the class labels of the\ntest data."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-1-4939-0847-9_16", 
    "link": "http://arxiv.org/pdf/1401.0569v2", 
    "title": "Natural Language Processing in Biomedicine: A Unified System   Architecture Overview", 
    "arxiv-id": "1401.0569v2", 
    "author": "Lucila Ohno-Machado", 
    "publish": "2014-01-03T00:57:13Z", 
    "summary": "In modern electronic medical records (EMR) much of the clinically important\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\nprovided in structured data fields, but rather are encoded in clinician\ngenerated narrative text. Natural language processing (NLP) provides a means of\n\"unlocking\" this important data source for applications in clinical decision\nsupport, quality assurance, and public health. This chapter provides an\noverview of representative NLP systems in biomedicine based on a unified\narchitectural view. A general architecture in an NLP system consists of two\nmain components: background knowledge that includes biomedical knowledge\nresources and a framework that integrates NLP tools to process text. Systems\ndiffer in both components, which we will review briefly. Additionally,\nchallenges facing current research efforts in biomedical NLP include the\npaucity of large, publicly available annotated corpora, although initiatives\nthat facilitate data sharing, system evaluation, and collaborative work between\nresearchers in clinical NLP are starting to emerge."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit", 
    "link": "http://arxiv.org/pdf/1401.0640v1", 
    "title": "Multi-Topic Multi-Document Summarizer", 
    "arxiv-id": "1401.0640v1", 
    "author": "Tarek El-Shishtawy", 
    "publish": "2014-01-03T13:07:29Z", 
    "summary": "Current multi-document summarization systems can successfully extract summary\nsentences, however with many limitations including: low coverage, inaccurate\nextraction to important sentences, redundancy and poor coherence among the\nselected sentences. The present study introduces a new concept of centroid\napproach and reports new techniques for extracting summary sentences for\nmulti-document. In both techniques keyphrases are used to weigh sentences and\ndocuments. The first summarization technique (Sen-Rich) prefers maximum\nrichness sentences. While the second (Doc-Rich), prefers sentences from\ncentroid document. To demonstrate the new summarization system application to\nextract summaries of Arabic documents we performed two experiments. First, we\napplied Rouge measure to compare the new techniques among systems presented at\nTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.\nSecond, the system was applied to summarize multi-topic documents. Using human\nevaluators, the results show that Doc-Rich is the superior, where summary\nsentences characterized by extra coverage and more cohesion."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit", 
    "link": "http://arxiv.org/pdf/1401.0660v1", 
    "title": "Plurals: individuals and sets in a richly typed semantics", 
    "arxiv-id": "1401.0660v1", 
    "author": "Christian Retor\u00e9", 
    "publish": "2014-01-03T15:37:19Z", 
    "summary": "We developed a type-theoretical framework for natural lan- guage semantics\nthat, in addition to the usual Montagovian treatment of compositional\nsemantics, includes a treatment of some phenomena of lex- ical semantic:\ncoercions, meaning, transfers, (in)felicitous co-predication. In this setting\nwe see how the various readings of plurals (collective, dis- tributive,\ncoverings,...) can be modelled."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsit", 
    "link": "http://arxiv.org/pdf/1401.1158v1", 
    "title": "Effective Slot Filling Based on Shallow Distant Supervision Methods", 
    "arxiv-id": "1401.1158v1", 
    "author": "Dietrich Klakow", 
    "publish": "2014-01-06T18:03:11Z", 
    "summary": "Spoken Language Systems at Saarland University (LSV) participated this year\nwith 5 runs at the TAC KBP English slot filling track. Effective algorithms for\nall parts of the pipeline, from document retrieval to relation prediction and\nresponse post-processing, are bundled in a modular end-to-end relation\nextraction system called RelationFactory. The main run solely focuses on\nshallow techniques and achieved significant improvements over LSV's last year's\nsystem, while using the same training data and patterns. Improvements mainly\nhave been obtained by a feature representation focusing on surface skip n-grams\nand improved scoring for extracted distant supervision patterns. Important\nfactors for effective extraction are the training and tuning scheme for distant\nsupervision classifiers, and the query expansion by a translation model based\non Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the\nsubmitted main run of the LSV RelationFactory system achieved the top-ranked\nF1-score of 37.3%."
},{
    "category": "cs.CL", 
    "doi": "10.5311/JOSIS.2013.7.128", 
    "link": "http://arxiv.org/pdf/1401.2517v1", 
    "title": "The semantic similarity ensemble", 
    "arxiv-id": "1401.2517v1", 
    "author": "David C. Wilson", 
    "publish": "2014-01-11T10:35:37Z", 
    "summary": "Computational measures of semantic similarity between geographic terms\nprovide valuable support across geographic information retrieval, data mining,\nand information integration. To date, a wide variety of approaches to\ngeo-semantic similarity have been devised. A judgment of similarity is not\nintrinsically right or wrong, but obtains a certain degree of cognitive\nplausibility, depending on how closely it mimics human behavior. Thus selecting\nthe most appropriate measure for a specific task is a significant challenge. To\naddress this issue, we make an analogy between computational similarity\nmeasures and soliciting domain expert opinions, which incorporate a subjective\nset of beliefs, perceptions, hypotheses, and epistemic biases. Following this\nanalogy, we define the semantic similarity ensemble (SSE) as a composition of\ndifferent similarity measures, acting as a panel of experts having to reach a\ndecision on the semantic similarity of a set of geographic terms. The approach\nis evaluated in comparison to human judgments, and results indicate that an SSE\nperforms better than the average of its parts. Although the best member tends\nto outperform the ensemble, all ensembles outperform the average performance of\neach ensemble's member. Hence, in contexts where the best measure is unknown,\nthe ensemble provides a more cognitively plausible approach."
},{
    "category": "cs.CL", 
    "doi": "10.5311/JOSIS.2013.7.128", 
    "link": "http://arxiv.org/pdf/1401.2641v1", 
    "title": "Towards a Generic Framework for the Development of Unicode Based Digital   Sindhi Dictionaries", 
    "arxiv-id": "1401.2641v1", 
    "author": "Azhar Ali Shah", 
    "publish": "2014-01-12T16:49:53Z", 
    "summary": "Dictionaries are essence of any language providing vital linguistic recourse\nfor the language learners, researchers and scholars. This paper focuses on the\nmethodology and techniques used in developing software architecture for a\nUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The\nproposed system provides an accurate solution for construction and\nrepresentation of Unicode based Sindhi characters in a dictionary implementing\nHash Structure algorithm and a custom java Object as its internal data\nstructure saved in a file. The System provides facilities for Insertion,\nDeletion and Editing of new records of Sindhi. Through this framework any type\nof Sindhi to English and English to Sindhi Dictionary (belonging to different\ndomains of knowledge, e.g. engineering, medicine, computer, biology etc.) could\nbe developed easily with accurate representation of Unicode Characters in font\nindependent manner."
},{
    "category": "cs.CL", 
    "doi": "10.5311/JOSIS.2013.7.128", 
    "link": "http://arxiv.org/pdf/1401.2663v1", 
    "title": "Dictionary-Based Concept Mining: An Application for Turkish", 
    "arxiv-id": "1401.2663v1", 
    "author": "Hidayet Tak\u00e7\u0131", 
    "publish": "2014-01-12T19:52:49Z", 
    "summary": "In this study, a dictionary-based method is used to extract expressive\nconcepts from documents. So far, there have been many studies concerning\nconcept mining in English, but this area of study for Turkish, an agglutinative\nlanguage, is still immature. We used dictionary instead of WordNet, a lexical\ndatabase grouping words into synsets that is widely used for concept\nextraction. The dictionaries are rarely used in the domain of concept mining,\nbut taking into account that dictionary entries have synonyms, hypernyms,\nhyponyms and other relationships in their meaning texts, the success rate has\nbeen high for determining concepts. This concept extraction method is\nimplemented on documents, that are collected from different corpora."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-011-9165-9", 
    "link": "http://arxiv.org/pdf/1401.2937v1", 
    "title": "A survey of methods to ease the development of highly multilingual text   mining applications", 
    "arxiv-id": "1401.2937v1", 
    "author": "Ralf Steinberger", 
    "publish": "2014-01-13T18:05:28Z", 
    "summary": "Multilingual text processing is useful because the information content found\nin different languages is complementary, both regarding facts and opinions.\nWhile Information Extraction and other text mining software can, in principle,\nbe developed for many languages, most text analysis tools have only been\napplied to small sets of languages because the development effort per language\nis large. Self-training tools obviously alleviate the problem, but even the\neffort of providing training data and of manually tuning the results is usually\nconsiderable. In this paper, we gather insights by various multilingual system\ndevelopers on how to minimise the effort of developing natural language\nprocessing applications for many languages. We also explain the main guidelines\nunderlying our own effort to develop complex text mining software for tens of\nlanguages. While these guidelines - most of all: extreme simplicity - can be\nvery restrictive and limiting, we believe to have shown the feasibility of the\napproach through the development of the Europe Media Monitor (EMM) family of\napplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex\nmedia monitoring tools that process and analyse up to 100,000 online news\narticles per day in between twenty and fifty languages. We will also touch upon\nthe kind of language resources that would make it easier for all to develop\nhighly multilingual text mining applications. We will argue that - to achieve\nthis - the most needed resources would be freely available, simple, parallel\nand uniform multilingual dictionaries, corpora and software tools."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-011-9165-9", 
    "link": "http://arxiv.org/pdf/1401.2943v1", 
    "title": "ONTS: \"Optima\" News Translation System", 
    "arxiv-id": "1401.2943v1", 
    "author": "Erik Van der Goot", 
    "publish": "2014-01-13T18:25:10Z", 
    "summary": "We propose a real-time machine translation system that allows users to select\na news category and to translate the related live news articles from Arabic,\nCzech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and\nTurkish into English. The Moses-based system was optimised for the news domain\nand differs from other available systems in four ways: (1) News items are\nautomatically categorised on the source side, before translation; (2) Named\nentity translation is optimised by recognising and extracting them on the\nsource side and by re-inserting their translation in the target language,\nmaking use of a separate entity repository; (3) News titles are translated with\na separate translation system which is optimised for the specific style of news\ntitles; (4) The system was optimised for speed in order to cope with the large\nvolume of daily news articles."
},{
    "category": "cs.CL", 
    "doi": "10.1007/s10579-011-9165-9", 
    "link": "http://arxiv.org/pdf/1401.3669v1", 
    "title": "Hrebs and Cohesion Chains as similar tools for semantic text properties   research", 
    "arxiv-id": "1401.3669v1", 
    "author": "E. Kapetanios", 
    "publish": "2014-01-15T17:01:36Z", 
    "summary": "In this study it is proven that the Hrebs used in Denotation analysis of\ntexts and Cohesion Chains (defined as a fusion between Lexical Chains and\nCoreference Chains) represent similar linguistic tools. This result gives us\nthe possibility to extend to Cohesion Chains (CCs) some important indicators\nas, for example the Kernel of CCs, the topicality of a CC, text concentration,\nCC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in\nthe Lexical Chains or Coreference Chains literature these kinds of indicators\nare introduced and used since now. Similarly, some applications of CCs in the\nstudy of a text (as for example segmentation or summarization of a text) could\nbe realized starting from hrebs. As an illustration of the similarity between\nHrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is\ngiven."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3500", 
    "link": "http://arxiv.org/pdf/1401.5674v1", 
    "title": "Generalized Biwords for Bitext Compression and Translation Spotting", 
    "arxiv-id": "1401.5674v1", 
    "author": "Joaquin Adiego", 
    "publish": "2014-01-18T21:11:30Z", 
    "summary": "Large bilingual parallel texts (also known as bitexts) are usually stored in\na compressed form, and previous work has shown that they can be more\nefficiently compressed if the fact that the two texts are mutual translations\nis exploited. For example, a bitext can be seen as a sequence of biwords\n---pairs of parallel words with a high probability of co-occurrence--- that can\nbe used as an intermediate representation in the compression process. However,\nthe simple biword approach described in the literature can only exploit\none-to-one word alignments and cannot tackle the reordering of words. We\ntherefore introduce a generalization of biwords which can describe multi-word\nexpressions and reorderings. We also describe some methods for the binary\ncompression of generalized biword sequences, and compare their performance when\ndifferent schemes are applied to the extraction of the biword sequence. In\naddition, we show that this generalization of biwords allows for the\nimplementation of an efficient algorithm to look on the compressed bitext for\nwords or text segments in one of the texts and retrieve their counterpart\ntranslations in the other text ---an application usually referred to as\ntranslation spotting--- with only some minor modifications in the compression\nalgorithm."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2655", 
    "link": "http://arxiv.org/pdf/1401.5693v1", 
    "title": "Sentence Compression as Tree Transduction", 
    "arxiv-id": "1401.5693v1", 
    "author": "Mirella Lapata", 
    "publish": "2014-01-15T05:19:15Z", 
    "summary": "This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2863", 
    "link": "http://arxiv.org/pdf/1401.5694v1", 
    "title": "Cross-lingual Annotation Projection for Semantic Roles", 
    "arxiv-id": "1401.5694v1", 
    "author": "Mirella Lapata", 
    "publish": "2014-01-15T05:40:37Z", 
    "summary": "This article considers the task of automatically inducing role-semantic\nannotations in the FrameNet paradigm for new languages. We propose a general\nframework that is based on annotation projection, phrased as a graph\noptimization problem. It is relatively inexpensive and has the potential to\nreduce the human effort involved in creating role-semantic resources. Within\nthis framework, we present projection models that exploit lexical and syntactic\ninformation. We provide an experimental evaluation on an English-German\nparallel corpus which demonstrates the feasibility of inducing high-precision\nGerman semantic role annotation both for manually and automatically annotated\nEnglish data."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2843", 
    "link": "http://arxiv.org/pdf/1401.5695v1", 
    "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", 
    "arxiv-id": "1401.5695v1", 
    "author": "Regina Barzilay", 
    "publish": "2014-01-15T05:39:01Z", 
    "summary": "We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2772", 
    "link": "http://arxiv.org/pdf/1401.5696v1", 
    "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the   Web", 
    "arxiv-id": "1401.5696v1", 
    "author": "Oren Etzioni", 
    "publish": "2014-01-15T05:33:07Z", 
    "summary": "The task of identifying synonymous relations and objects, or synonym\nresolution, is critical for high-quality information extraction. This paper\ninvestigates synonym resolution in the context of unsupervised information\nextraction, where neither hand-tagged training examples nor domain knowledge is\navailable. The paper presents a scalable, fully-implemented system that runs in\nO(KN log N) time in the number of extractions, N, and the maximum number of\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\nrelational model for predicting whether two strings are co-referential based on\nthe similarity of the assertions containing them. On a set of two million\nassertions extracted from the Web, Resolver resolves objects with 78% precision\nand 68% recall, and resolves relations with 90% precision and 35% recall.\nSeveral variations of resolvers probabilistic model are explored, and\nexperiments demonstrate that under appropriate conditions these variations can\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\npolysemous names with 97% precision and 95% recall on a data set from the TREC\ncorpus."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2669", 
    "link": "http://arxiv.org/pdf/1401.5697v1", 
    "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", 
    "arxiv-id": "1401.5697v1", 
    "author": "Shaul Markovitch", 
    "publish": "2014-01-15T05:21:01Z", 
    "summary": "Adequate representation of natural language semantics requires access to vast\namounts of common sense and domain-specific world knowledge. Prior work in the\nfield was based on purely statistical techniques that did not make use of\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\nor on huge manual efforts such as the CYC project. Here we propose a novel\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\ninterpretation of unrestricted natural language texts. Our method represents\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\nlargest encyclopedia in existence. We explicitly represent the meaning of any\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\nmethod on text categorization and on computing the degree of semantic\nrelatedness between fragments of natural language text. Using ESA results in\nsignificant improvements over the previous state of the art in both tasks.\nImportantly, due to the use of natural concepts, the ESA model is easy to\nexplain to human users."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2622", 
    "link": "http://arxiv.org/pdf/1401.5698v1", 
    "title": "Identification of Pleonastic It Using the Web", 
    "arxiv-id": "1401.5698v1", 
    "author": "Loren Wyard-Scott", 
    "publish": "2014-01-15T05:11:43Z", 
    "summary": "In a significant minority of cases, certain pronouns, especially the pronoun\nit, can be used without referring to any specific entity. This phenomenon of\npleonastic pronoun usage poses serious problems for systems aiming at even a\nshallow understanding of natural language texts. In this paper, a novel\napproach is proposed to identify such uses of it: the extrapositional cases are\nidentified using a series of queries against the web, and the cleft cases are\nidentified using a simple set of syntactic rules. The system is evaluated with\nfour sets of news articles containing 679 extrapositional cases as well as 78\ncleft constructs. The identification results are comparable to those obtained\nby human efforts."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2880", 
    "link": "http://arxiv.org/pdf/1401.5699v1", 
    "title": "Text Relatedness Based on a Word Thesaurus", 
    "arxiv-id": "1401.5699v1", 
    "author": "Michalis Vazirgiannis", 
    "publish": "2014-01-15T05:41:08Z", 
    "summary": "The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2735", 
    "link": "http://arxiv.org/pdf/1401.5700v1", 
    "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel   Corpora", 
    "arxiv-id": "1401.5700v1", 
    "author": "Mikel L. Forcada", 
    "publish": "2014-01-15T05:28:26Z", 
    "summary": "This paper describes a method for the automatic inference of structural\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\nfrom small parallel corpora. The structural transfer rules are based on\nalignment templates, like those used in statistical MT. Alignment templates are\nextracted from sentence-aligned parallel corpora and extended with a set of\nrestrictions which are derived from the bilingual dictionary of the MT system\nand control their application as transfer rules. The experiments conducted\nusing three different language pairs in the free/open-source MT platform\nApertium show that translation quality is improved as compared to word-for-word\ntranslation (when no transfer rules are used), and that the resulting\ntranslation quality is close to that obtained using hand-coded transfer rules.\nThe method we present is entirely unsupervised and benefits from information in\nthe rest of modules of the MT system in which the inferred rules are applied."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3717", 
    "link": "http://arxiv.org/pdf/1401.6050v1", 
    "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale   Feature Selection", 
    "arxiv-id": "1401.6050v1", 
    "author": "Chunyu Kit", 
    "publish": "2014-01-23T16:45:39Z", 
    "summary": "Semantic parsing, i.e., the automatic derivation of meaning representation\nsuch as an instantiated predicate-argument structure for a sentence, plays a\ncritical role in deep processing of natural language. Unlike all other top\nsystems of semantic dependency parsing that have to rely on a pipeline\nframework to chain up a series of submodels each specialized for a specific\nsubtask, the one presented in this article integrates everything into one\nmodel, in hopes of achieving desirable integrity and practicality for real\napplications while maintaining a competitive performance. This integrative\napproach tackles semantic parsing as a word pair classification problem using a\nmaximum entropy classifier. We leverage adaptive pruning of argument candidates\nand large-scale feature selection engineering to allow the largest feature\nspace ever in use so far in this field, it achieves a state-of-the-art\nperformance on the evaluation data set for CoNLL-2008 shared task, on top of\nall but one top pipeline system, confirming its feasibility and effectiveness."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3717", 
    "link": "http://arxiv.org/pdf/1401.6122v1", 
    "title": "Identifying Bengali Multiword Expressions using Semantic Clustering", 
    "arxiv-id": "1401.6122v1", 
    "author": "Sivaji Bandyopadhyay", 
    "publish": "2014-01-23T19:03:18Z", 
    "summary": "One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3717", 
    "link": "http://arxiv.org/pdf/1401.6330v2", 
    "title": "A Statistical Parsing Framework for Sentiment Classification", 
    "arxiv-id": "1401.6330v2", 
    "author": "Ke Xu", 
    "publish": "2014-01-24T12:56:36Z", 
    "summary": "We present a statistical parsing framework for sentence-level sentiment\nclassification in this article. Unlike previous works that employ syntactic\nparsing results for sentiment analysis, we develop a statistical parser to\ndirectly analyze the sentiment structure of a sentence. We show that\ncomplicated phenomena in sentiment analysis (e.g., negation, intensification,\nand contrast) can be handled the same as simple and straightforward sentiment\nexpressions in a unified and probabilistic way. We formulate the sentiment\ngrammar upon Context-Free Grammars (CFGs), and provide a formal description of\nthe sentiment parsing framework. We develop the parsing model to obtain\npossible sentiment parse trees for a sentence, from which the polarity model is\nproposed to derive the sentiment strength and polarity, and the ranking model\nis dedicated to selecting the best sentiment tree. We train the parser directly\nfrom examples of sentences annotated only with sentiment polarity labels but\nwithout any syntactic annotations or polarity annotations of constituents\nwithin sentences. Therefore we can obtain training data easily. In particular,\nwe train a sentiment parser, s.parser, from a large amount of review sentences\nwith users' ratings as rough sentiment polarity labels. Extensive experiments\non existing benchmark datasets show significant improvements over baseline\nsentiment classification approaches."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3647", 
    "link": "http://arxiv.org/pdf/1401.6422v1", 
    "title": "Automatic Aggregation by Joint Modeling of Aspects and Values", 
    "arxiv-id": "1401.6422v1", 
    "author": "Regina Barzilay", 
    "publish": "2014-01-23T02:48:07Z", 
    "summary": "We present a model for aggregation of product review snippets by joint aspect\nidentification and sentiment analysis. Our model simultaneously identifies an\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\nsushi and miso for a Japanese restaurant) and determines the corresponding\nsentiment of each aspect. This approach directly enables discovery of\nhighly-rated or inconsistent aspects of a product. Our generative model admits\nan efficient variational mean-field inference algorithm. It is also easily\nextensible, and we describe several modifications and their effects on model\nstructure and inference. We test our model on two tasks, joint aspect\nidentification and sentiment analysis on a set of Yelp reviews and aspect\nidentification alone on a set of medical summaries. We evaluate the performance\nof the model on aspect identification, sentiment analysis, and per-word\nlabeling accuracy. We demonstrate that our model outperforms applicable\nbaselines by a considerable margin, yielding up to 32% relative error reduction\non aspect identification and up to 20% relative error reduction on sentiment\nanalysis."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2912", 
    "link": "http://arxiv.org/pdf/1401.6875v1", 
    "title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World", 
    "arxiv-id": "1401.6875v1", 
    "author": "Joyce Y. Chai", 
    "publish": "2014-01-16T04:48:43Z", 
    "summary": "To tackle the vocabulary problem in conversational systems, previous work has\napplied unsupervised learning approaches on co-occurring speech and eye gaze\nduring interaction to automatically acquire new words. Although these\napproaches have shown promise, several issues related to human language\nbehavior and human-machine conversation have not been addressed. First,\npsycholinguistic studies have shown certain temporal regularities between human\neye movement and language production. While these regularities can potentially\nguide the acquisition process, they have not been incorporated in the previous\nunsupervised approaches. Second, conversational systems generally have an\nexisting knowledge base about the domain and vocabulary. While the existing\nknowledge can potentially help bootstrap and constrain the acquired new words,\nit has not been incorporated in the previous models. Third, eye gaze could\nserve different functions in human-machine conversation. Some gaze streams may\nnot be closely coupled with speech stream, and thus are potentially detrimental\nto word acquisition. Automated recognition of closely-coupled speech-gaze\nstreams based on conversation context is important. To address these issues, we\ndeveloped new approaches that incorporate user language behavior, domain\nknowledge, and conversation context in word acquisition. We evaluated these\napproaches in the context of situated dialogue in a virtual world. Our\nexperimental results have shown that incorporating the above three types of\ncontextual information significantly improves word acquisition performance."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3540", 
    "link": "http://arxiv.org/pdf/1401.6876v1", 
    "title": "Improving Statistical Machine Translation for a Resource-Poor Language   Using Related Resource-Rich Languages", 
    "arxiv-id": "1401.6876v1", 
    "author": "Hwee Tou Ng", 
    "publish": "2014-01-23T02:42:12Z", 
    "summary": "We propose a novel language-independent approach for improving machine\ntranslation for resource-poor languages by exploiting their similarity to\nresource-rich ones. More precisely, we improve the translation from a\nresource-poor source language X_1 into a resource-rich language Y given a\nbi-text containing a limited number of parallel sentences for X_1-Y and a\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\nrelated to X_1. This is achieved by taking advantage of the opportunities that\nvocabulary overlap and similarities between the languages X_1 and X_2 in\nspelling, word order, and syntax offer: (1) we improve the word alignments for\nthe resource-poor language, (2) we further augment it with additional\ntranslation options, and (3) we take care of potential spelling differences\nthrough appropriate transliteration. The evaluation for Indonesian- >English\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\nrespectively, which is an improvement over the best rivaling approaches, while\nusing much less additional data. Overall, our method cuts the amount of\nnecessary \"real training data by a factor of 2--5."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1401.7077v4", 
    "title": "Quantifying literature quality using complexity criteria", 
    "arxiv-id": "1401.7077v4", 
    "author": "Klaus Jaffe", 
    "publish": "2014-01-28T03:48:01Z", 
    "summary": "We measured entropy and symbolic diversity for English and Spanish texts\nincluding literature Nobel laureates and other famous authors. Entropy, symbol\ndiversity and symbol frequency profiles were compared for these four groups. We\nalso built a scale sensitive to the quality of writing and evaluated its\nrelationship with the Flesch's readability index for English and the\nSzigriszt's perspicuity index for Spanish. Results suggest a correlation\nbetween entropy and word diversity with quality of writing. Text genre also\ninfluences the resulting entropy and diversity of the text. Results suggest the\nplausibility of automated quality assessment of texts."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1403.0052v1", 
    "title": "TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding   Initiative guidelines", 
    "arxiv-id": "1403.0052v1", 
    "author": "Laurent Romary", 
    "publish": "2014-03-01T06:46:11Z", 
    "summary": "This paper presents an attempt to customise the TEI (Text Encoding\nInitiative) guidelines in order to offer the possibility to incorporate TBX\n(TermBase eXchange) based terminological entries within any kind of TEI\ndocuments. After presenting the general historical, conceptual and technical\ncontexts, we describe the various design choices we had to take while creating\nthis customisation, which in turn have led to make various changes in the\nactual TBX serialisation. Keeping in mind the objective to provide the TEI\nguidelines with, again, an onomasiological model, we try to identify the best\ncomprise in maintaining both the isomorphism with the existing TBX Basic\nstandard and the characteristics of the TEI framework."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1403.0531v1", 
    "title": "We Tweet Like We Talk and Other Interesting Observations: An Analysis of   English Communication Modalities", 
    "arxiv-id": "1403.0531v1", 
    "author": "Josiah P. Zayner", 
    "publish": "2014-03-03T19:27:23Z", 
    "summary": "Modalities of communication for human beings are gradually increasing in\nnumber with the advent of new forms of technology. Many human beings can\nreadily transition between these different forms of communication with little\nor no effort, which brings about the question: How similar are these different\ncommunication modalities? To understand technology$\\text{'}$s influence on\nEnglish communication, four different corpora were analyzed and compared:\nWriting from Books using the 1-grams database from the Google Books project,\nTwitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices\nrevealed that Talking has the most similarity when compared to the other modes\nof communication, while 1-grams were the least similar form of communication\nanalyzed. Based on the analysis of word usage, word usage frequency\ndistributions, and word class usage, among other things, Talking is also the\nmost similar to Twitter and IRC Chat. This suggests that communicating using\nTwitter and IRC Chat evolved from Talking rather than Writing. When we\ncommunicate online, even though we are writing, we do not Tweet or Chat how we\nwrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing\nwere clearly differentiable from our analysis with Twitter and Chat being much\nmore similar to Fiction than Nonfiction writing. These hypotheses were then\ntested using author and journalists Cory Doctorow. Mr. Doctorow$\\text{'}$s\nWriting, Twitter usage, and Talking were all found to have very similar\nvocabulary usage patterns as the amalgamized populations, as long as the\nwriting was Fiction. However, Mr. Doctorow$\\text{'}$s Nonfiction writing is\ndifferent from 1-grams and other collected Nonfiction writings. This data could\nperhaps be used to create more entertaining works of Nonfiction."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1403.0801v2", 
    "title": "Is getting the right answer just about choosing the right words? The   role of syntactically-informed features in short answer scoring", 
    "arxiv-id": "1403.0801v2", 
    "author": "John Blackmore", 
    "publish": "2014-03-04T14:45:56Z", 
    "summary": "Developments in the educational landscape have spurred greater interest in\nthe problem of automatically scoring short answer questions. A recent shared\ntask on this topic revealed a fundamental divide in the modeling approaches\nthat have been applied to this problem, with the best-performing systems split\nbetween those that employ a knowledge engineering approach and those that\nalmost solely leverage lexical information (as opposed to higher-level\nsyntactic information) in assigning a score to a given response. This paper\naims to introduce the NLP community to the largest corpus currently available\nfor short-answer scoring, provide an overview of methods used in the shared\ntask using this data, and explore the extent to which more\nsyntactically-informed features can contribute to the short answer scoring task\nin a way that avoids the question-specific manual effort of the knowledge\nengineering approach."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1403.2004v1", 
    "title": "Natural Language Feature Selection via Cooccurrence", 
    "arxiv-id": "1403.2004v1", 
    "author": "Michael Stewart", 
    "publish": "2014-03-08T20:10:37Z", 
    "summary": "Specificity is important for extracting collocations, keyphrases, multi-word\nand index terms [Newman et al. 2012]. It is also useful for tagging, ontology\nconstruction [Ryu and Choi 2006], and automatic summarization of documents\n[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and\ninverse-document frequency (TF-IDF) are typically used to do this, but fail to\ntake advantage of the semantic relationships between terms [Church and Gale\n1995]. The result is that general idiomatic terms are mistaken for specific\nterms. We demonstrate use of relational data for estimation of term\nspecificity. The specificity of a term can be learned from its distribution of\nrelations with other terms. This technique is useful for identifying relevant\nwords or terms for other natural language processing tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1080/09296174.2016.1169847", 
    "link": "http://arxiv.org/pdf/1403.2124v1", 
    "title": "Generating Music from Literature", 
    "arxiv-id": "1403.2124v1", 
    "author": "Saif M. Mohammad", 
    "publish": "2014-03-10T01:41:09Z", 
    "summary": "We present a system, TransProse, that automatically generates musical pieces\nfrom text. TransProse uses known relations between elements of music such as\ntempo and scale, and the emotions they evoke. Further, it uses a novel\nmechanism to determine sequences of notes that capture the emotional activity\nin the text. The work has applications in information visualization, in\ncreating audio-visual e-books, and in developing music apps."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.2837v1", 
    "title": "HPS: a hierarchical Persian stemming method", 
    "arxiv-id": "1403.2837v1", 
    "author": "Mina Zolfy Lighvan", 
    "publish": "2014-03-12T08:08:49Z", 
    "summary": "In this paper, a novel hierarchical Persian stemming approach based on the\nPart-Of-Speech of the word in a sentence is presented. The implemented stemmer\nincludes hash tables and several deterministic finite automata in its different\nlevels of hierarchy for removing the prefixes and suffixes of the words. We had\ntwo intentions in using hash tables in our method. The first one is that the\nDFA don't support some special words, so hash table can partly solve the\naddressed problem. the second goal is to speed up the implemented stemmer with\nomitting the time that deterministic finite automata need. Because of the\nhierarchical organization, this method is fast and flexible enough. Our\nexperiments on test sets from Hamshahri collection and security news (istna.ir)\nshow that our method has the average accuracy of 95.37% which is even improved\nin using the method on a test set with common topics."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.3351v1", 
    "title": "Semantic Unification A sheaf theoretic approach to natural language", 
    "arxiv-id": "1403.3351v1", 
    "author": "Mehrnoosh Sadrzadeh", 
    "publish": "2014-03-13T18:20:10Z", 
    "summary": "Language is contextual and sheaf theory provides a high level mathematical\nframework to model contextuality. We show how sheaf theory can model the\ncontextual nature of natural language and how gluing can be used to provide a\nglobal semantics for a discourse by putting together the local logical\nsemantics of each sentence within the discourse. We introduce a presheaf\nstructure corresponding to a basic form of Discourse Representation Structures.\nWithin this setting, we formulate a notion of semantic unification --- gluing\nmeanings of parts of a discourse into a coherent whole --- as a form of\nsheaf-theoretic gluing. We illustrate this idea with a number of examples where\nit can used to represent resolutions of anaphoric references. We also discuss\nmultivalued gluing, described using a distributions functor, which can be used\nto represent situations where multiple gluings are possible, and where we may\nneed to rank them using quantitative measures.\n  Dedicated to Jim Lambek on the occasion of his 90th birthday."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.3668v2", 
    "title": "Language Heedless of Logic - Philosophy Mindful of What? Failures of   Distributive and Absorption Laws", 
    "arxiv-id": "1403.3668v2", 
    "author": "Arthur Merin", 
    "publish": "2014-03-14T18:53:51Z", 
    "summary": "Much of philosophical logic and all of philosophy of language make empirical\nclaims about the vernacular natural language. They presume semantics under\nwhich `and' and `or' are related by the dually paired distributive and\nabsorption laws. However, at least one of each pair of laws fails in the\nvernacular. `Implicature'-based auxiliary theories associated with the\nprogramme of H.P. Grice do not prove remedial. Conceivable alternatives that\nmight replace the familiar logics as descriptive instruments are briefly noted:\n(i) substructural logics and (ii) meaning composition in linear algebras over\nthe reals, occasionally constrained by norms of classical logic. Alternative\n(ii) locates the problem in violations of one of the idempotent laws. Reasons\nfor a lack of curiosity about elementary and easily testable implications of\nthe received theory are considered. The concept of `reflective equilibrium' is\ncritically examined for its role in reconciling normative desiderata and\ndescriptive commitments."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4024v3", 
    "title": "Measuring Global Similarity between Texts", 
    "arxiv-id": "1403.4024v3", 
    "author": "Axel Legay", 
    "publish": "2014-03-17T08:22:54Z", 
    "summary": "We propose a new similarity measure between texts which, contrary to the\ncurrent state-of-the-art approaches, takes a global view of the texts to be\ncompared. We have implemented a tool to compute our textual distance and\nconducted experiments on several corpuses of texts. The experiments show that\nour methods can reliably identify different global types of texts."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4467v2", 
    "title": "A hybrid formalism to parse Sign Languages", 
    "arxiv-id": "1403.4467v2", 
    "author": "Christophe Collet", 
    "publish": "2014-03-18T14:16:45Z", 
    "summary": "Sign Language (SL) linguistic is dependent on the expensive task of\nannotating. Some automation is already available for low-level information (eg.\nbody part tracking) and the lexical level has shown significant progresses. The\nsyntactic level lacks annotated corpora as well as complete and consistent\nmodels. This article presents a solution for the automatic annotation of SL\nsyntactic elements. It exposes a formalism able to represent both\nconstituency-based and dependency-based models. The first enable the\nrepresentation the structures one may want to annotate, the second aims at\nfulfilling the holes of the first. A parser is presented and used to conduct\ntwo experiments on the solution. One experiment is on a real corpus, the other\nis on a synthetic corpus."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4473v1", 
    "title": "Sign Language Gibberish for syntactic parsing evaluation", 
    "arxiv-id": "1403.4473v1", 
    "author": "Christophe Collet", 
    "publish": "2014-03-18T14:31:51Z", 
    "summary": "Sign Language (SL) automatic processing slowly progresses bottom-up. The\nfield has seen proposition to handle the video signal, to recognize and\nsynthesize sublexical and lexical units. It starts to see the development of\nsupra-lexical processing. But the recognition, at this level, lacks data. The\nsyntax of SL appears very specific as it uses massively the multiplicity of\narticulators and its access to the spatial dimensions. Therefore new parsing\ntechniques are developed. However these need to be evaluated. The shortage on\nreal data restrains the corpus-based models to small sizes. We propose here a\nsolution to produce data-sets for the evaluation of parsers on the specific\nproperties of SL. The article first describes the general model used to\ngenerates dependency grammars and the phrase generation from these lasts. It\nthen discusses the limits of approach. The solution shows to be of particular\ninterest to evaluate the scalability of the techniques on big models."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4759v1", 
    "title": "Spelling Error Trends and Patterns in Sindhi", 
    "arxiv-id": "1403.4759v1", 
    "author": "Waseem Javaid", 
    "publish": "2014-03-19T10:45:29Z", 
    "summary": "Statistical error Correction technique is the most accurate and widely used\napproach today, but for a language like Sindhi which is a low resourced\nlanguage the trained corpora's are not available, so the statistical techniques\nare not possible at all. Instead a useful alternative would be to exploit\nvarious spelling error trends in Sindhi by using a Rule based approach. For\ndesigning such technique an essential prerequisite would be to study the\nvarious error patterns in a language. This pa per presents various studies of\nspelling error trends and their types in Sindhi Language. The research shows\nthat the error trends common to all languages are also encountered in Sindhi\nbut their do exist some error patters that are catered specifically to a Sindhi\nlanguage."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4887v1", 
    "title": "Using Entropy Estimates for DAG-Based Ontologies", 
    "arxiv-id": "1403.4887v1", 
    "author": "Joao Setubal", 
    "publish": "2014-03-19T17:29:24Z", 
    "summary": "Motivation: Entropy measurements on hierarchical structures have been used in\nmethods for information retrieval and natural language modeling. Here we\nexplore its application to semantic similarity. By finding shared ontology\nterms, semantic similarity can be established between annotated genes. A common\nprocedure for establishing semantic similarity is to calculate the\ndescriptiveness (information content) of ontology terms and use these values to\ndetermine the similarity of annotations. Most often information content is\ncalculated for an ontology term by analyzing its frequency in an annotation\ncorpus. The inherent problems in using these values to model functional\nsimilarity motivates our work. Summary: We present a novel calculation for\nestablishing the entropy of a DAG-based ontology, which can be used in an\nalternative method for establishing the information content of its terms. We\nalso compare our IC metric to two others using semantic and sequence\nsimilarity."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.4928v1", 
    "title": "Clinical TempEval", 
    "arxiv-id": "1403.4928v1", 
    "author": "Marc Verhagen", 
    "publish": "2014-03-19T19:59:49Z", 
    "summary": "We describe the Clinical TempEval task which is currently in preparation for\nthe SemEval-2015 evaluation exercise. This task involves identifying and\ndescribing events, times and the relations between them in clinical text. Six\ndiscrete subtasks are included, focusing on recognising mentions of times and\nevents, describing those mentions for both entity types, identifying the\nrelation between an event and the document creation time, and identifying\nnarrative container relations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.6381v1", 
    "title": "An efficiency dependency parser using hybrid approach for tamil language", 
    "arxiv-id": "1403.6381v1", 
    "author": "S. Suganthi", 
    "publish": "2014-03-21T04:54:28Z", 
    "summary": "Natural language processing is a prompt research area across the country.\nParsing is one of the very crucial tool in language analysis system which aims\nto forecast the structural relationship among the words in a given sentence.\nMany researchers have already developed so many language tools but the accuracy\nis not meet out the human expectation level, thus the research is still exists.\nMachine translation is one of the major application area under Natural Language\nProcessing. While translation between one language to another language, the\nstructure identification of a sentence play a key role. This paper introduces\nthe hybrid way to solve the identification of relationship among the given\nwords in a sentence. In existing system is implemented using rule based\napproach, which is not suited in huge amount of data. The machine learning\napproaches is suitable for handle larger amount of data and also to get better\naccuracy via learning and training the system. The proposed approach takes a\nTamil sentence as an input and produce the result of a dependency relation as a\ntree like structure using hybrid approach. This proposed tool is very helpful\nfor researchers and act as an odd-on improve the quality of existing\napproaches."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.6392v2", 
    "title": "Implementation of an Automatic Sign Language Lexical Annotation   Framework based on Propositional Dynamic Logic", 
    "arxiv-id": "1403.6392v2", 
    "author": "Christophe Collet", 
    "publish": "2014-03-25T15:36:36Z", 
    "summary": "In this paper, we present the implementation of an automatic Sign Language\n(SL) sign annotation framework based on a formal logic, the Propositional\nDynamic Logic (PDL). Our system relies heavily on the use of a specific variant\nof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets\nus describe SL signs as formulae and corpora videos as labeled transition\nsystems (LTSs). Here, we intend to show how a generic annotation system can be\nconstructed upon these underlying theoretical principles, regardless of the\ntracking technologies available or the input format of corpora. With this in\nmind, we generated a development framework that adapts the system to specific\nuse cases. Furthermore, we present some results obtained by our application\nwhen adapted to one distinct case, 2D corpora analysis with pre-processed\ntracking information. We also present some insights on how such a technology\ncan be used to analyze 3D real-time data, captured with a depth device."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.6636v1", 
    "title": "Sign Language Lexical Recognition With Propositional Dynamic Logic", 
    "arxiv-id": "1403.6636v1", 
    "author": "Christophe Collet", 
    "publish": "2014-03-26T11:47:37Z", 
    "summary": "This paper explores the use of Propositional Dynamic Logic (PDL) as a\nsuitable formal framework for describing Sign Language (SL), the language of\ndeaf people, in the context of natural language processing. SLs are visual,\ncomplete, standalone languages which are just as expressive as oral languages.\nSigns in SL usually correspond to sequences of highly specific body postures\ninterleaved with movements, which make reference to real world objects,\ncharacters or situations. Here we propose a formal representation of SL signs,\nthat will help us with the analysis of automatically-collected hand tracking\ndata from French Sign Language (FSL) video corpora. We further show how such a\nrepresentation could help us with the design of computer aided SL verification\ntools, which in turn would bring us closer to the development of an automatic\nrecognition system for these languages."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1403.7455v1", 
    "title": "Hybrid Approach to English-Hindi Name Entity Transliteration", 
    "arxiv-id": "1403.7455v1", 
    "author": "Varun Prakash Saxena", 
    "publish": "2014-03-28T17:30:32Z", 
    "summary": "Machine translation (MT) research in Indian languages is still in its\ninfancy. Not much work has been done in proper transliteration of name entities\nin this domain. In this paper we address this issue. We have used English-Hindi\nlanguage pair for our experiments and have used a hybrid approach. At first we\nhave processed English words using a rule based approach which extracts\nindividual phonemes from the words and then we have applied statistical\napproach which converts the English into its equivalent Hindi phoneme and in\nturn the corresponding Hindi word. Through this approach we have attained\n83.40% accuracy."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1405.0145v1", 
    "title": "Contextual Semantic Parsing using Crowdsourced Spatial Descriptions", 
    "arxiv-id": "1405.0145v1", 
    "author": "Kais Dukes", 
    "publish": "2014-05-01T12:29:30Z", 
    "summary": "We describe a contextual parser for the Robot Commands Treebank, a new\ncrowdsourced resource. In contrast to previous semantic parsers that select the\nmost-probable parse, we consider the different problem of parsing using\nadditional situational context to disambiguate between different readings of a\nsentence. We show that multiple semantic analyses can be searched using dynamic\nprogramming via interaction with a spatial planner, to guide the parsing\nprocess. We are able to parse sentences in near linear-time by ruling out\nanalyses early on that are incompatible with spatial context. We report a 34%\nupper bound on accuracy, as our planner correctly processes spatial context for\n3,394 out of 10,000 sentences. However, our parser achieves a 96.53%\nexact-match score for parsing within the subset of sentences recognized by the\nplanner, compared to 82.14% for a non-contextual parser."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1405.0603v1", 
    "title": "Extracting Family Relationship Networks from Novels", 
    "arxiv-id": "1405.0603v1", 
    "author": "Grzegorz Kondrak", 
    "publish": "2014-05-03T16:07:19Z", 
    "summary": "We present an approach to the extraction of family relations from literary\nnarrative, which incorporates a technique for utterance attribution proposed\nrecently by Elson and McKeown (2010). In our work this technique is used in\ncombination with the detection of vocatives - the explicit forms of address\nused by the characters in a novel. We take advantage of the fact that certain\nvocatives indicate family relations between speakers. The extracted relations\nare then propagated using a set of rules. We report the results of the\napplication of our method to Jane Austen's Pride and Prejudice."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1405.0701v1", 
    "title": "\"Translation can't change a name\": Using Multilingual Data for Named   Entity Recognition", 
    "arxiv-id": "1405.0701v1", 
    "author": "Manaal Faruqui", 
    "publish": "2014-05-04T14:23:53Z", 
    "summary": "Named Entities (NEs) are often written with no orthographic changes across\ndifferent languages that share a common alphabet. We show that this can be\nleveraged so as to improve named entity recognition (NER) by using unsupervised\nword clusters from secondary languages as features in state-of-the-art\ndiscriminative NER systems. We observe significant increases in performance,\nfinding that person and location identification is particularly improved, and\nthat phylogenetically close languages provide more valuable features than more\ndistant languages."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1405.0947v1", 
    "title": "Learning Bilingual Word Representations by Marginalizing Alignments", 
    "arxiv-id": "1405.0947v1", 
    "author": "Phil Blunsom", 
    "publish": "2014-05-05T16:24:09Z", 
    "summary": "We present a probabilistic model that simultaneously learns alignments and\ndistributed representations for bilingual data. By marginalizing over word\nalignments the model captures a larger semantic context than prior work relying\non hard alignments. The advantage of this approach is demonstrated in a\ncross-lingual classification task, where we outperform the prior published\nstate of the art."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3102", 
    "link": "http://arxiv.org/pdf/1405.1346v1", 
    "title": "Automatic Method Of Domain Ontology Construction based on   Characteristics of Corpora POS-Analysis", 
    "arxiv-id": "1405.1346v1", 
    "author": "Olena Orobinska", 
    "publish": "2014-05-06T16:28:16Z", 
    "summary": "It is now widely recognized that ontologies, are one of the fundamental\ncornerstones of knowledge-based systems. What is lacking, however, is a\ncurrently accepted strategy of how to build ontology; what kinds of the\nresources and techniques are indispensables to optimize the expenses and the\ntime on the one hand and the amplitude, the completeness, the robustness of en\nontology on the other hand. The paper offers a semi-automatic ontology\nconstruction method from text corpora in the domain of radiological protection.\nThis method is composed from next steps: 1) text annotation with part-of-speech\ntags; 2) revelation of the significant linguistic structures and forming the\ntemplates; 3) search of text fragments corresponding to these templates; 4)\nbasic ontology instantiation process"
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121575", 
    "link": "http://arxiv.org/pdf/1405.1359v3", 
    "title": "Latent semantics of action verbs reflect phonetic parameters of   intensity and emotional content", 
    "arxiv-id": "1405.1359v3", 
    "author": "Michael Kai Petersen", 
    "publish": "2014-05-06T16:52:34Z", 
    "summary": "Conjuring up our thoughts, language reflects statistical patterns of word\nco-occurrences which in turn come to describe how we perceive the world.\nWhether counting how frequently nouns and verbs combine in Google search\nqueries, or extracting eigenvectors from term document matrices made up of\nWikipedia lines and Shakespeare plots, the resulting latent semantics capture\nnot only the associative links which form concepts, but also spatial dimensions\nembedded within the surface structure of language. As both the shape and\nmovements of objects have been found to be associated with phonetic contrasts\nalready in toddlers, this study explores whether articulatory and acoustic\nparameters may likewise differentiate the latent semantics of action verbs.\nSelecting 3 x 20 emotion, face, and hand related verbs known to activate\npremotor areas in the brain, their mutual cosine similarities were computed\nusing latent semantic analysis LSA, and the resulting adjacency matrices were\ncompared based on two different large scale text corpora; HAWIK and TASA.\nApplying hierarchical clustering to identify common structures across the two\ntext corpora, the verbs largely divide into combined mouth and hand movements\nversus emotional expressions. Transforming the verbs into their constituent\nphonemes, the clustered small and large size movements appear differentiated by\nfront versus back vowels corresponding to increasing levels of arousal. Whereas\nthe clustered emotional verbs seem characterized by sequences of close versus\nopen jaw produced phonemes, generating up- or downwards shifts in formant\nfrequencies that may influence their perceived valence. Suggesting, that the\nlatent semantics of action verbs reflect parameters of intensity and emotional\npolarity that appear correlated with the articulatory contrasts and acoustic\ncharacteristics of phonemes"
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121575", 
    "link": "http://arxiv.org/pdf/1405.1406v1", 
    "title": "D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving   Word Sense Disambiguation", 
    "arxiv-id": "1405.1406v1", 
    "author": "Karl-Heinz Zimmermann", 
    "publish": "2014-05-06T19:26:35Z", 
    "summary": "Word sense disambiguation (WSD) is a problem in the field of computational\nlinguistics given as finding the intended sense of a word (or a set of words)\nwhen it is activated within a certain context. WSD was recently addressed as a\ncombinatorial optimization problem in which the goal is to find a sequence of\nsenses that maximize the semantic relatedness among the target words. In this\narticle, a novel algorithm for solving the WSD problem called D-Bees is\nproposed which is inspired by bee colony optimization (BCO)where artificial bee\nagents collaborate to solve the problem. The D-Bees algorithm is evaluated on a\nstandard dataset (SemEval 2007 coarse-grained English all-words task corpus)and\nis compared to simulated annealing, genetic algorithms, and two ant colony\noptimization techniques (ACO). It will be observed that the BCO and ACO\napproaches are on par."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121575", 
    "link": "http://arxiv.org/pdf/1405.1439v2", 
    "title": "A Corpus of Sentence-level Revisions in Academic Writing: A Step towards   Understanding Statement Strength in Communication", 
    "arxiv-id": "1405.1439v2", 
    "author": "Lillian Lee", 
    "publish": "2014-05-06T20:05:23Z", 
    "summary": "The strength with which a statement is made can have a significant impact on\nthe audience. For example, international relations can be strained by how the\nmedia in one country describes an event in another; and papers can be rejected\nbecause they overstate or understate their findings. It is thus important to\nunderstand the effects of statement strength. A first step is to be able to\ndistinguish between strong and weak statements. However, even this problem is\nunderstudied, partly due to a lack of data. Since strength is inherently\nrelative, revisions of texts that make claims are a natural source of data on\nstrength differences. In this paper, we introduce a corpus of sentence-level\nrevisions from academic writing. We also describe insights gained from our\nannotation efforts for this task."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.1924v1", 
    "title": "An Expert System for Automatic Reading of A Text Written in Standard   Arabic", 
    "arxiv-id": "1405.1924v1", 
    "author": "Azzoune Hamid", 
    "publish": "2014-05-08T13:47:36Z", 
    "summary": "In this work we present our expert system of Automatic reading or speech\nsynthesis based on a text written in Standard Arabic, our work is carried out\nin two great stages: the creation of the sound data base, and the\ntransformation of the written text into speech (Text To Speech TTS). This\ntransformation is done firstly by a Phonetic Orthographical Transcription (POT)\nof any written Standard Arabic text with the aim of transforming it into his\ncorresponding phonetics sequence, and secondly by the generation of the voice\nsignal which corresponds to the chain transcribed. We spread out the different\nof conception of the system, as well as the results obtained compared to others\nworks studied to realize TTS based on Standard Arabic."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.2434v1", 
    "title": "Coordinate System Selection for Minimum Error Rate Training in   Statistical Machine Translation", 
    "arxiv-id": "1405.2434v1", 
    "author": "Chen Lijiang", 
    "publish": "2014-05-10T13:55:03Z", 
    "summary": "Minimum error rate training (MERT) is a widely used training procedure for\nstatistical machine translation. A general problem of this approach is that the\nsearch space is easy to converge to a local optimum and the acquired weight set\nis not in accord with the real distribution of feature functions. This paper\nintroduces coordinate system selection (RSS) into the search algorithm for\nMERT. Contrary to previous approaches in which every dimension only corresponds\nto one independent feature function, we create several coordinate systems by\nmoving one of the dimensions to a new direction. The basic idea is quite simple\nbut critical that the training procedure of MERT should be based on a\ncoordinate system formed by search directions but not directly on feature\nfunctions. Experiments show that by selecting coordinate systems with tuning\nset results, better results can be obtained without any other language\nknowledge."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.3033v1", 
    "title": "Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker   System", 
    "arxiv-id": "1405.3033v1", 
    "author": "Waseem Javaid Soomro", 
    "publish": "2014-05-13T04:33:04Z", 
    "summary": "This paper presents a novel combinational phonetic algorithm for Sindhi\nLanguage, to be used in developing Sindhi Spell Checker which has yet not been\ndeveloped prior to this work. The compound textual forms and glyphs of Sindhi\nlanguage presents a substantial challenge for developing Sindhi spell checker\nsystem and generating similar suggestion list for misspelled words. In order to\nimplement such a system, phonetic based Sindhi language rules and patterns must\nbe considered into account for increasing the accuracy and efficiency. The\nproposed system is developed with a blend between Phonetic based SoundEx\nalgorithm and ShapeEx algorithm for pattern or glyph matching, generating\naccurate and efficient suggestion list for incorrect or misspelled Sindhi\nwords. A table of phonetically similar sounding Sindhi characters for SoundEx\nalgorithm is also generated along with another table containing similar glyph\nor shape based character groups for ShapeEx algorithm. Both these are first\never attempt of any such type of categorization and representation for Sindhi\nLanguage."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.3515v1", 
    "title": "Temporal Analysis of Language through Neural Language Models", 
    "arxiv-id": "1405.3515v1", 
    "author": "Slav Petrov", 
    "publish": "2014-05-14T14:47:22Z", 
    "summary": "We provide a method for automatically detecting change in language across\ntime through a chronologically trained neural language model. We train the\nmodel on the Google Books Ngram corpus to obtain word vector representations\nspecific to each year, and identify words that have changed significantly from\n1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having\nchanged during that time period. The model simultaneously identifies the\nspecific years during which such words underwent change."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.3772v1", 
    "title": "INAUT, a Controlled Language for the French Coast Pilot Books   Instructions nautiques", 
    "arxiv-id": "1405.3772v1", 
    "author": "John Puentes", 
    "publish": "2014-05-15T09:11:53Z", 
    "summary": "We describe INAUT, a controlled natural language dedicated to collaborative\nupdate of a knowledge base on maritime navigation and to automatic generation\nof coast pilot books (Instructions nautiques) of the French National\nHydrographic and Oceanographic Service SHOM. INAUT is based on French language\nand abundantly uses georeferenced entities. After describing the structure of\nthe overall system, giving details on the language and on its generation, and\ndiscussing the three major applications of INAUT (document production,\ninteraction with ENCs and collaborative updates of the knowledge base), we\nconclude with future extensions and open problems."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.3925v1", 
    "title": "M\u00e9thodes pour la repr\u00e9sentation informatis\u00e9e de donn\u00e9es   lexicales / Methoden der Speicherung lexikalischer Daten", 
    "arxiv-id": "1405.3925v1", 
    "author": "Andreas Witt", 
    "publish": "2014-05-15T18:10:36Z", 
    "summary": "In recent years, new developments in the area of lexicography have altered\nnot only the management, processing and publishing of lexicographical data, but\nalso created new types of products such as electronic dictionaries and\nthesauri. These expand the range of possible uses of lexical data and support\nusers with more flexibility, for instance in assisting human translation. In\nthis article, we give a short and easy-to-understand introduction to the\nproblematic nature of the storage, display and interpretation of lexical data.\nWe then describe the main methods and specifications used to build and\nrepresent lexical data. This paper is targeted for the following groups of\npeople: linguists, lexicographers, IT specialists, computer linguists and all\nothers who wish to learn more about the modelling, representation and\nvisualization of lexical knowledge. This paper is written in two languages:\nFrench and German."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.4097v2", 
    "title": "A preliminary study of Croatian Language Syllable Networks", 
    "arxiv-id": "1405.4097v2", 
    "author": "Ana Me\u0161trovi\u0107", 
    "publish": "2014-05-16T08:57:40Z", 
    "summary": "This paper presents preliminary results of Croatian syllable networks\nanalysis. Syllable network is a network in which nodes are syllables and links\nbetween them are constructed according to their connections within words. In\nthis paper we analyze networks of syllables generated from texts collected from\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\nanalysis methods which provide mechanisms that can reveal new patterns in a\nlanguage structure. We aim to show that syllable networks have much higher\nclustering coefficient in comparison to Erd\\\"os-Renyi random networks. The\nresults indicate that Croatian syllable networks exhibit certain properties of\na small world networks. Furthermore, we compared Croatian syllable networks\nwith Portuguese and Chinese syllable networks and we showed that they have\nsimilar properties."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.4248v1", 
    "title": "Les math\u00e9matiques de la langue : l'approche formelle de Montague", 
    "arxiv-id": "1405.4248v1", 
    "author": "Yannis Haralambous", 
    "publish": "2014-05-16T17:17:19Z", 
    "summary": "We present a natural language modelization method which is strongely relying\non mathematics. This method, called \"Formal Semantics,\" has been initiated by\nthe American linguist Richard M. Montague in the 1970's. It uses mathematical\ntools such as formal languages and grammars, first-order logic, type theory and\n$\\lambda$-calculus. Our goal is to have the reader discover both Montagovian\nformal semantics and the mathematical tools that he used in his method.\n  -----\n  Nous pr\\'esentons une m\\'ethode de mod\\'elisation de la langue naturelle qui\nest fortement bas\\'ee sur les math\\'ematiques. Cette m\\'ethode, appel\\'ee\n{\\guillemotleft}s\\'emantique formelle{\\guillemotright}, a \\'et\\'e initi\\'ee par\nle linguiste am\\'ericain Richard M. Montague dans les ann\\'ees 1970. Elle\nutilise des outils math\\'ematiques tels que les langages et grammaires formels,\nla logique du 1er ordre, la th\\'eorie de types et le $\\lambda$-calcul. Nous\nnous proposons de faire d\\'ecouvrir au lecteur tant la s\\'emantique formelle de\nMontague que les outils math\\'ematiques dont il s'est servi."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.4273v1", 
    "title": "Compositional Morphology for Word Representations and Language Modelling", 
    "arxiv-id": "1405.4273v1", 
    "author": "Phil Blunsom", 
    "publish": "2014-05-16T19:08:14Z", 
    "summary": "This paper presents a scalable method for integrating compositional\nmorphological representations into a vector-based probabilistic language model.\nOur approach is evaluated in the context of log-bilinear language models,\nrendered suitably efficient for implementation inside a machine translation\ndecoder by factoring the vocabulary. We perform both intrinsic and extrinsic\nevaluations, presenting results on a range of languages which demonstrate that\nour model learns morphological representations that both perform well on word\nsimilarity tasks and lead to substantial reductions in perplexity. When used\nfor translation into morphologically rich languages with large vocabularies,\nour models obtain improvements of up to 1.2 BLEU points relative to a baseline\nsystem using back-off n-gram models."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3201", 
    "link": "http://arxiv.org/pdf/1405.4364v1", 
    "title": "Thematically Reinforced Explicit Semantic Analysis", 
    "arxiv-id": "1405.4364v1", 
    "author": "Vitaly Klyuev", 
    "publish": "2014-05-17T07:58:58Z", 
    "summary": "We present an extended, thematically reinforced version of Gabrilovich and\nMarkovitch's Explicit Semantic Analysis (ESA), where we obtain thematic\ninformation through the category structure of Wikipedia. For this we first\ndefine a notion of categorical tfidf which measures the relevance of terms in\ncategories. Using this measure as a weight we calculate a maximal spanning tree\nof the Wikipedia corpus considered as a directed graph of pages and categories.\nThis tree provides us with a unique path of \"most related categories\" between\neach page and the top of the hierarchy. We reinforce tfidf of words in a page\nby aggregating it with categorical tfidfs of the nodes of these paths, and\ndefine a thematically reinforced ESA semantic relatedness measure which is more\nrobust than standard ESA and less sensitive to noise caused by out-of-context\nwords. We apply our method to the French Wikipedia corpus, evaluate it through\na text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a\nprecision increase of 9-10% compared with standard ESA."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.5202v1", 
    "title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference   Resolution", 
    "arxiv-id": "1405.5202v1", 
    "author": "Vincent Ng", 
    "publish": "2014-01-16T05:06:09Z", 
    "summary": "Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.5474v1", 
    "title": "New Perspectives in Sinographic Language Processing Through the Use of   Character Structure", 
    "arxiv-id": "1405.5474v1", 
    "author": "Yannis Haralambous", 
    "publish": "2014-05-21T16:49:50Z", 
    "summary": "Chinese characters have a complex and hierarchical graphical structure\ncarrying both semantic and phonetic information. We use this structure to\nenhance the text model and obtain better results in standard NLP operations.\nFirst of all, to tackle the problem of graphical variation we define\nallographic classes of characters. Next, the relation of inclusion of a\nsubcharacter in a characters, provides us with a directed graph of allographic\nclasses. We provide this graph with two weights: semanticity (semantic relation\nbetween subcharacter and character) and phoneticity (phonetic relation) and\ncalculate \"most semantic subcharacter paths\" for each character. Finally,\nadding the information contained in these paths to unigrams we claim to\nincrease the efficiency of text mining methods. We evaluate our method on a\ntext classification task on two corpora (Chinese and Japanese) of a total of 18\nmillion characters and get an improvement of 3% on an already high baseline of\n89.6% precision, obtained by a linear SVM classifier. Other possible\napplications and perspectives of the system are discussed."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.5654v1", 
    "title": "Machine Translation Model based on Non-parallel Corpus and   Semi-supervised Transductive Learning", 
    "arxiv-id": "1405.5654v1", 
    "author": "Lijiang Chen", 
    "publish": "2014-05-22T07:58:56Z", 
    "summary": "Although the parallel corpus has an irreplaceable role in machine\ntranslation, its scale and coverage is still beyond the actual needs.\nNon-parallel corpus resources on the web have an inestimable potential value in\nmachine translation and other natural language processing tasks. This article\nproposes a semi-supervised transductive learning method for expanding the\ntraining corpus in statistical machine translation system by extracting\nparallel sentences from the non-parallel corpus. This method only requires a\nsmall amount of labeled corpus and a large unlabeled corpus to build a\nhigh-performance classifier, especially for when there is short of labeled\ncorpus. The experimental results show that by combining the non-parallel corpus\nalignment and the semi-supervised transductive learning method, we can more\neffectively use their respective strengths to improve the performance of\nmachine translation system."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.5674v1", 
    "title": "Mot\u00e0Mot project: conversion of a French-Khmer published dictionary for   building a multilingual lexical system", 
    "arxiv-id": "1405.5674v1", 
    "author": "Mathieu Mangeot", 
    "publish": "2014-05-22T08:57:54Z", 
    "summary": "Economic issues related to the information processing techniques are very\nimportant. The development of such technologies is a major asset for developing\ncountries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and\nThailand. The MotAMot project aims to computerize an under-resourced language:\nKhmer, spoken mainly in Cambodia. The main goal of the project is the\ndevelopment of a multilingual lexical system targeted for Khmer. The\nmacrostructure is a pivot one with each word sense of each language linked to a\npivot axi. The microstructure comes from a simplification of the explanatory\nand combinatory dictionary. The lexical system has been initialized with data\ncoming mainly from the conversion of the French-Khmer bilingual dictionary of\nDenis Richer from Word to XML format. The French part was completed with\npronunciation and parts-of-speech coming from the FeM French-english-Malay\ndictionary. The Khmer headwords noted in IPA in the Richer dictionary were\nconverted to Khmer writing with OpenFST, a finite state transducer tool. The\nresulting resource is available online for lookup, editing, download and remote\nprogramming via a REST API on a Jibiki platform."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.5893v1", 
    "title": "Computerization of African languages-French dictionaries", 
    "arxiv-id": "1405.5893v1", 
    "author": "Mathieu Mangeot", 
    "publish": "2014-05-22T20:15:57Z", 
    "summary": "This paper relates work done during the DiLAF project. It consists in\nconverting 5 bilingual African language-French dictionaries originally in Word\nformat into XML following the LMF model. The languages processed are Bambara,\nHausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced\nlanguages concerning Natural Language Processing tools. Once converted, the\ndictionaries are available online on the Jibiki platform for lookup and\nmodification. The DiLAF project is first presented. A description of each\ndictionary follows. Then, the conversion methodology from .doc format to XML\nfiles is presented. A specific point on the usage of Unicode follows. Then,\neach step of the conversion into XML and LMF is detailed. The last part\npresents the Jibiki lexical resources management platform used for the project."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.6068v1", 
    "title": "Building of Networks of Natural Hierarchies of Terms Based on Analysis   of Texts Corpora", 
    "arxiv-id": "1405.6068v1", 
    "author": "Dmitry Lande", 
    "publish": "2014-05-23T14:05:45Z", 
    "summary": "The technique of building of networks of hierarchies of terms based on the\nanalysis of chosen text corpora is offered. The technique is based on the\nmethodology of horizontal visibility graphs. Constructed and investigated\nlanguage network, formed on the basis of electronic preprints arXiv on topics\nof information retrieval."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.6103v1", 
    "title": "Evaluating the fully automatic multi-language translation of the Swiss   avalanche bulletin", 
    "arxiv-id": "1405.6103v1", 
    "author": "Martin Volk", 
    "publish": "2014-05-23T15:51:10Z", 
    "summary": "The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.6293v1", 
    "title": "Cross-Language Personal Name Mapping", 
    "arxiv-id": "1405.6293v1", 
    "author": "Ahmed H. Yousef", 
    "publish": "2014-05-24T11:39:34Z", 
    "summary": "Name matching between multiple natural languages is an important step in\ncross-enterprise integration applications and data mining. It is difficult to\ndecide whether or not two syntactic values (names) from two heterogeneous data\nsources are alternative designation of the same semantic entity (person), this\nprocess becomes more difficult with Arabic language due to several factors\nincluding spelling and pronunciation variation, dialects and special vowel and\nconsonant distinction and other linguistic characteristics. This paper proposes\na new framework for name matching between the Arabic language and other\nlanguages. The framework uses a dictionary based on a new proposed version of\nthe Soundex algorithm to encapsulate the recognition of special features of\nArabic names. The framework proposes a new proximity matching algorithm to suit\nthe high importance of order sensitivity in Arabic name matching. New\nperformance evaluation metrics are proposed as well. The framework is\nimplemented and verified empirically in several case studies demonstrating\nsubstantial improvements compared to other well-known techniques found in\nliterature."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.6682v1", 
    "title": "Optimality Theory as a Framework for Lexical Acquisition", 
    "arxiv-id": "1405.6682v1", 
    "author": "Thierry Poibeau", 
    "publish": "2014-05-26T18:51:06Z", 
    "summary": "This paper re-investigates a lexical acquisition system initially developed\nfor French.We show that, interestingly, the architecture of the system\nreproduces and implements the main components of Optimality Theory. However, we\nformulate the hypothesis that some of its limitations are mainly due to a poor\nrepresentation of the constraints used. Finally, we show how a better\nrepresentation of the constraints used would yield better results."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.3120", 
    "link": "http://arxiv.org/pdf/1405.7397v1", 
    "title": "An HMM Based Named Entity Recognition System for Indian Languages: The   JU System at ICON 2013", 
    "arxiv-id": "1405.7397v1", 
    "author": "Kamal Sarkar", 
    "publish": "2014-05-28T21:05:00Z", 
    "summary": "This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named\nEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,\nPunjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model\nhas been used to implement our system. The system has been trained and tested\non the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of\n0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,\nEnglish, Hindi, Marathi, Punjabi, Tamil and Telugu respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1405.7711v1", 
    "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn   Language", 
    "arxiv-id": "1405.7711v1", 
    "author": "Raymond J. Mooney", 
    "publish": "2014-01-16T04:29:26Z", 
    "summary": "We present a novel framework for learning to interpret and generate language\nusing only perceptual context as supervision. We demonstrate its capabilities\nby developing a system that learns to sportscast simulated robot soccer games\nin both English and Korean without any language-specific prior knowledge.\nTraining employs only ambiguous supervision consisting of a stream of\ndescriptive textual comments and a sequence of events extracted from the\nsimulation trace. The system simultaneously establishes correspondences between\nindividual comments and the events that they describe while building a\ntranslation model that supports both parsing and generation. We also present a\nnovel algorithm for learning which events are worth describing. Human\nevaluations of the generated commentaries indicate they are of reasonable\nquality and in some cases even on par with those produced by humans for our\nlimited domain."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.1605v1", 
    "title": "Les noms propres se traduisent-ils ? \u00c9tude d'un corpus multilingue", 
    "arxiv-id": "1407.1605v1", 
    "author": "Dusko Vitas", 
    "publish": "2014-07-07T07:08:07Z", 
    "summary": "In this paper, we tackle the problem of the translation of proper names. We\nintroduce our hypothesis according to which proper names can be translated more\noften than most people seem to think. Then, we describe the construction of a\nparallel multilingual corpus used to illustrate our point. We eventually\nevaluate both the advantages and limits of this corpus in our study."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.1976v1", 
    "title": "Inter-Rater Agreement Study on Readability Assessment in Bengali", 
    "arxiv-id": "1407.1976v1", 
    "author": "Arindam Biswas", 
    "publish": "2014-07-08T07:35:16Z", 
    "summary": "An inter-rater agreement study is performed for readability assessment in\nBengali. A 1-7 rating scale was used to indicate different levels of\nreadability. We obtained moderate to fair agreement among seven independent\nannotators on 30 text passages written by four eminent Bengali authors. As a by\nproduct of our study, we obtained a readability-annotated ground truth dataset\nin Bengali. ."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.2019v1", 
    "title": "Assamese-English Bilingual Machine Translation", 
    "arxiv-id": "1407.2019v1", 
    "author": "Shikhar Kr. Sarma", 
    "publish": "2014-07-08T10:04:07Z", 
    "summary": "Machine translation is the process of translating text from one language to\nanother. In this paper, Statistical Machine Translation is done on Assamese and\nEnglish language by taking their respective parallel corpus. A statistical\nphrase based translation toolkit Moses is used here. To develop the language\nmodel and to align the words we used two another tools IRSTLM, GIZA\nrespectively. BLEU score is used to check our translation system performance,\nhow good it is. A difference in BLEU scores is obtained while translating\nsentences from Assamese to English and vice-versa. Since Indian languages are\nmorphologically very rich hence translation is relatively harder from English\nto Assamese resulting in a low BLEU score. A statistical transliteration system\nis also introduced with our translation system to deal basically with proper\nnouns, OOV (out of vocabulary) words which are not present in our corpus."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.2694v1", 
    "title": "Quality Estimation Of Machine Translation Outputs Through Stemming", 
    "arxiv-id": "1407.2694v1", 
    "author": "Iti Mathur", 
    "publish": "2014-07-10T05:26:14Z", 
    "summary": "Machine Translation is the challenging problem for Indian languages. Every\nday we can see some machine translators being developed, but getting a high\nquality automatic translation is still a very distant dream . The correct\ntranslated sentence for Hindi language is rarely found. In this paper, we are\nemphasizing on English-Hindi language pair, so in order to preserve the correct\nMT output we present a ranking system, which employs some machine learning\ntechniques and morphological features. In ranking no human intervention is\nrequired. We have also validated our results by comparing it with human\nranking."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.2918v1", 
    "title": "A Survey of Named Entity Recognition in Assamese and other Indian   Languages", 
    "arxiv-id": "1407.2918v1", 
    "author": "Arup Baruah", 
    "publish": "2014-07-09T13:59:27Z", 
    "summary": "Named Entity Recognition is always important when dealing with major Natural\nLanguage Processing tasks such as information extraction, question-answering,\nmachine translation, document summarization etc so in this paper we put forward\na survey of Named Entities in Indian Languages with particular reference to\nAssamese. There are various rule-based and machine learning approaches\navailable for Named Entity Recognition. At the very first of the paper we give\nan idea of the available approaches for Named Entity Recognition and then we\ndiscuss about the related research in this field. Assamese like other Indian\nlanguages is agglutinative and suffers from lack of appropriate resources as\nNamed Entity Recognition requires large data sets, gazetteer list, dictionary\netc and some useful feature like capitalization as found in English cannot be\nfound in Assamese. Apart from this we also describe some of the issues faced in\nAssamese while doing Named Entity Recognition."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.2989v1", 
    "title": "Hidden Markov Model Based Part of Speech Tagger for Sinhala Language", 
    "arxiv-id": "1407.2989v1", 
    "author": "N. G. J. Dias", 
    "publish": "2014-07-10T23:57:54Z", 
    "summary": "In this paper we present a fundamental lexical semantics of Sinhala language\nand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala\nlanguage. In any Natural Language processing task, Part of Speech is a very\nvital topic, which involves analysing of the construction, behaviour and the\ndynamics of the language, which the knowledge could utilized in computational\nlinguistics analysis and automation applications. Though Sinhala is a\nmorphologically rich and agglutinative language, in which words are inflected\nwith various grammatical features, tagging is very essential for further\nanalysis of the language. Our research is based on statistical based approach,\nin which the tagging process is done by computing the tag sequence probability\nand the word-likelihood probability from the given corpus, where the linguistic\nknowledge is automatically extracted from the annotated corpus. The current\ntagger could reach more than 90% of accuracy for known words."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.6853v1", 
    "title": "Substitute Based SCODE Word Embeddings in Supervised NLP Tasks", 
    "arxiv-id": "1407.6853v1", 
    "author": "Deniz Yuret", 
    "publish": "2014-07-25T11:17:28Z", 
    "summary": "We analyze a word embedding method in supervised tasks. It maps words on a\nsphere such that words co-occurring in similar contexts lie closely. The\nsimilarity of contexts is measured by the distribution of substitutes that can\nfill them. We compared word embeddings, including more recent representations,\nin Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine\nour framework in multilingual dependency parsing as well. The results show that\nthe proposed method achieves as good as or better results compared to the other\nword embeddings in the tasks we investigate. It achieves state-of-the-art\nresults in multilingual dependency parsing. Word embeddings in 7 languages are\navailable for public use."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1407.8215v1", 
    "title": "Two-pass Discourse Segmentation with Pairing and Global Features", 
    "arxiv-id": "1407.8215v1", 
    "author": "Graeme Hirst", 
    "publish": "2014-07-30T21:00:25Z", 
    "summary": "Previous attempts at RST-style discourse segmentation typically adopt\nfeatures centered on a single token to predict whether to insert a boundary\nbefore that token. In contrast, we develop a discourse segmenter utilizing a\nset of pairing features, which are centered on a pair of adjacent tokens in the\nsentence, by equally taking into account the information from both tokens.\nMoreover, we propose a novel set of global features, which encode\ncharacteristics of the segmentation as a whole, once we have an initial\nsegmentation. We show that both the pairing and global features are useful on\ntheir own, and their combination achieved an $F_1$ of 92.6% of identifying\nin-sentence discourse boundaries, which is a 17.8% error-rate reduction over\nthe state-of-the-art performance, approaching 95% of human performance. In\naddition, similar improvement is observed across different classification\nframeworks."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.0286v1", 
    "title": "LAF-Fabric: a data analysis tool for Linguistic Annotation Framework   with an application to the Hebrew Bible", 
    "arxiv-id": "1410.0286v1", 
    "author": "Andreas van Cranenburgh", 
    "publish": "2014-10-01T16:53:35Z", 
    "summary": "The Linguistic Annotation Framework (LAF) provides a general, extensible\nstand-off markup system for corpora. This paper discusses LAF-Fabric, a new\ntool to analyse LAF resources in general with an extension to process the\nHebrew Bible in particular. We first walk through the history of the Hebrew\nBible as text database in decennium-wide steps. Then we describe how LAF-Fabric\nmay serve as an analysis tool for this corpus. Finally, we describe three\nanalytic projects/workflows that benefit from the new LAF representation:\n  1) the study of linguistic variation: extract cooccurrence data of common\nnouns between the books of the Bible (Martijn Naaijer); 2) the study of the\ngrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);\n3) construction of a parser of classical Hebrew by Data Oriented Parsing:\ngenerate tree structures from the database (Andreas van Cranenburgh)."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.0291v2", 
    "title": "A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives", 
    "arxiv-id": "1410.0291v2", 
    "author": "Yanchuan Sim", 
    "publish": "2014-10-01T17:03:18Z", 
    "summary": "We present an open source morphological analyzer for Japanese nouns, verbs\nand adjectives. The system builds upon the morphological analyzing capabilities\nof MeCab to incorporate finer details of classification such as politeness,\ntense, mood and voice attributes. We implemented our analyzer in the form of a\nfinite state transducer using the open source finite state compiler FOMA\ntoolkit. The source code and tool is available at\nhttps://bitbucket.org/skylander/yc-nlplab/."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.0718v2", 
    "title": "Not All Neural Embeddings are Born Equal", 
    "arxiv-id": "1410.0718v2", 
    "author": "Yoshua Bengio", 
    "publish": "2014-10-02T21:35:35Z", 
    "summary": "Neural language models learn word representations that capture rich\nlinguistic and conceptual information. Here we investigate the embeddings\nlearned by neural machine translation models. We show that translation-based\nembeddings outperform those learned by cutting-edge monolingual models at\nsingle-language tasks requiring knowledge of conceptual similarity and/or\nsyntactic role. The findings suggest that, while monolingual models learn\ninformation about how concepts are related, neural-translation models better\ncapture their true ontological status."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.1135v1", 
    "title": "Corpora Preparation and Stopword List Generation for Arabic data in   Social Network", 
    "arxiv-id": "1410.1135v1", 
    "author": "Hoda Korashy", 
    "publish": "2014-10-05T09:02:31Z", 
    "summary": "This paper proposes a methodology to prepare corpora in Arabic language from\nonline social network (OSN) and review site for Sentiment Analysis (SA) task.\nThe paper also proposes a methodology for generating a stopword list from the\nprepared corpora. The aim of the paper is to investigate the effect of removing\nstopwords on the SA task. The problem is that the stopwords lists generated\nbefore were on Modern Standard Arabic (MSA) which is not the common language\nused in OSN. We have generated a stopword list of Egyptian dialect and a\ncorpus-based list to be used with the OSN corpora. We compare the efficiency of\ntext classification when using the generated lists along with previously\ngenerated lists of MSA and combining the Egyptian dialect list with the MSA\nlist. The text classification was performed using Na\\\"ive Bayes and Decision\nTree classifiers and two feature selection approaches, unigrams and bigram. The\nexperiments show that the general lists containing the Egyptian dialects words\ngive better performance than using lists of MSA stopwords only."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.2082v2", 
    "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", 
    "arxiv-id": "1410.2082v2", 
    "author": "Maosong Sun", 
    "publish": "2014-10-08T12:24:38Z", 
    "summary": "Word alignment is an important natural language processing task that\nindicates the correspondence between natural languages. Recently, unsupervised\nlearning of log-linear models for word alignment has received considerable\nattention as it combines the merits of generative and discriminative\napproaches. However, a major challenge still remains: it is intractable to\ncalculate the expectations of non-local features that are critical for\ncapturing the divergence between natural languages. We propose a contrastive\napproach that aims to differentiate observed training examples from noises. It\nnot only introduces prior knowledge to guide unsupervised learning but also\ncancels out partition functions. Based on the observation that the probability\nmass of log-linear models for word alignment is usually highly concentrated, we\npropose to use top-n alignments to approximate the expectations with respect to\nposterior distributions. This allows for efficient and accurate calculation of\nexpectations of non-local features. Experiments show that our approach achieves\nsignificant improvements over state-of-the-art unsupervised word alignment\nmethods."
},{
    "category": "cs.CL", 
    "doi": "10.1613/jair.2962", 
    "link": "http://arxiv.org/pdf/1410.2149v1", 
    "title": "Language-based Examples in the Statistics Classroom", 
    "arxiv-id": "1410.2149v1", 
    "author": "Roger Bilisoly", 
    "publish": "2014-10-05T01:36:30Z", 
    "summary": "Statistics pedagogy values using a variety of examples. Thanks to text\nresources on the Web, and since statistical packages have the ability to\nanalyze string data, it is now easy to use language-based examples in a\nstatistics class. Three such examples are discussed here. First, many types of\nwordplay (e.g., crosswords and hangman) involve finding words with letters that\nsatisfy a certain pattern. Second, linguistics has shown that idiomatic pairs\nof words often appear together more frequently than chance. For example, in the\nBrown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)\nThird, a pangram contains all the letters of the alphabet at least once. These\nare searched for in Charles Dickens' A Christmas Carol, and their lengths are\ncompared to the expected value given by the unequal probability coupon\ncollector's problem as well as simulations."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.2646v1", 
    "title": "Hybrid approaches for automatic vowelization of Arabic texts", 
    "arxiv-id": "1410.2646v1", 
    "author": "Lakhouaja Abdelhak", 
    "publish": "2014-10-09T22:56:44Z", 
    "summary": "Hybrid approaches for automatic vowelization of Arabic texts are presented in\nthis article. The process is made up of two modules. In the first one, a\nmorphological analysis of the text words is performed using the open source\nmorphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out\nof context, are its different possible vowelizations. The integration of this\nAnalyzer in our vowelization system required the addition of a lexical database\ncontaining the most frequent words in Arabic language. Using a statistical\napproach based on two hidden Markov models (HMM), the second module aims to\neliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic\nwords are the observed states and the vowelized words are the hidden states.\nThe observed states of the second HMM are identical to those of the first, but\nthe hidden states are the lists of possible diacritics of the word without its\nArabic letters. Our system uses Viterbi algorithm to select the optimal path\namong the solutions proposed by Al Khalil Morpho Sys. Our approach opens an\nimportant way to improve the performance of automatic vowelization of Arabic\ntexts for other uses in automatic natural language processing."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.2871v2", 
    "title": "An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of   Sanskrit Grammar", 
    "arxiv-id": "1410.2871v2", 
    "author": "Meenakshi Lakshmanan", 
    "publish": "2014-10-10T19:19:31Z", 
    "summary": "Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit\nmorphology and phonology. The traditional and modern methods of studying about\neuphonic conjunctions in Sanskrit follow different methodologies. The former\ninvolves a rigorous study of the Paninian system embodied in Panini's\nAshtadhyayi, while the latter usually involves the study of a few important\nsandhi rules with the use of examples. The former is not suitable for\nbeginners, and the latter, not sufficient to gain a comprehensive understanding\nof the operation of sandhi rules. This is so since there are not only numerous\nsandhi rules and exceptions, but also complex precedence rules involved. The\nneed for a new ontology for sandhi-tutoring was hence felt. This work presents\na comprehensive ontology designed to enable a student-user to learn in stages\nall about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar\nand to test and evaluate the progress of the student-user. The ontology forms\nthe basis of a multimedia sandhi tutor that was given to different categories\nof users including Sanskrit scholars for extensive and rigorous testing."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.4176v1", 
    "title": "Learning Distributed Word Representations for Natural Logic Reasoning", 
    "arxiv-id": "1410.4176v1", 
    "author": "Christopher D. Manning", 
    "publish": "2014-10-15T19:27:10Z", 
    "summary": "Natural logic offers a powerful relational conception of meaning that is a\nnatural counterpart to distributed semantic representations, which have proven\nvaluable in a wide range of sophisticated language tasks. However, it remains\nan open question whether it is possible to train distributed representations to\nsupport the rich, diverse logical reasoning captured by natural logic. We\naddress this question using two neural network-based models for learning\nembeddings: plain neural networks and neural tensor networks. Our experiments\nevaluate the models' ability to learn the basic algebra of natural logic\nrelations from simulated data and from the WordNet noun graph. The overall\npositive results are promising for the future of learned distributed\nrepresentations in the applied modeling of logical semantics."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.4639v3", 
    "title": "Dependent Types for Pragmatics", 
    "arxiv-id": "1410.4639v3", 
    "author": "Jonathan Sterling", 
    "publish": "2014-10-17T05:19:12Z", 
    "summary": "This paper proposes the use of dependent types for pragmatic phenomena such\nas pronoun binding and presupposition resolution as a type-theoretic\nalternative to formalisms such as Discourse Representation Theory and Dynamic\nSemantics."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.4863v1", 
    "title": "Arabic Language Text Classification Using Dependency Syntax-Based   Feature Selection", 
    "arxiv-id": "1410.4863v1", 
    "author": "Philippe Lenca", 
    "publish": "2014-10-17T21:05:01Z", 
    "summary": "We study the performance of Arabic text classification combining various\ntechniques: (a) tfidf vs. dependency syntax, for feature selection and\nweighting; (b) class association rules vs. support vector machines, for\nclassification. The Arabic text is used in two forms: rootified and lightly\nstemmed. The results we obtain show that lightly stemmed text leads to better\nperformance than rootified text; that class association rules are better suited\nfor small feature sets obtained by dependency syntax constraints; and, finally,\nthat support vector machines are better suited for large feature sets based on\nmorphological feature selection criteria."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.4868v1", 
    "title": "A Modality Lexicon and its use in Automatic Tagging", 
    "arxiv-id": "1410.4868v1", 
    "author": "Christine Piatko", 
    "publish": "2014-10-17T21:19:15Z", 
    "summary": "This paper describes our resource-building results for an eight-week JHU\nHuman Language Technology Center of Excellence Summer Camp for Applied Language\nExploration (SCALE-2009) on Semantically-Informed Machine Translation.\nSpecifically, we describe the construction of a modality annotation scheme, a\nmodality lexicon, and two automated modality taggers that were built using the\nlexicon and annotation scheme. Our annotation scheme is based on identifying\nthree components of modality: a trigger, a target and a holder. We describe how\nour modality lexicon was produced semi-automatically, expanding from an initial\nhand-selected list of modality trigger words and phrases. The resulting\nexpanded modality lexicon is being made publicly available. We demonstrate that\none tagger---a structure-based tagger---results in precision around 86%\n(depending on genre) for tagging of a standard LDC data set. In a machine\ntranslation application, using the structure-based tagger to annotate English\nmodalities on an English-Urdu training corpus improved the translation quality\nscore for Urdu by 0.3 Bleu points in the face of sparse training data."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2014.3404", 
    "link": "http://arxiv.org/pdf/1410.4966v1", 
    "title": "The Visualization of Change in Word Meaning over Time using Temporal   Word Embeddings", 
    "arxiv-id": "1410.4966v1", 
    "author": "Shay B. Cohen", 
    "publish": "2014-10-18T14:53:19Z", 
    "summary": "We describe a visualization tool that can be used to view the change in\nmeaning of words over time. The tool makes use of existing (static) word\nembedding datasets together with a timestamped $n$-gram corpus to create {\\em\ntemporal} word embeddings."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1410.7182v1", 
    "title": "Analysis of Named Entity Recognition and Linking for Tweets", 
    "arxiv-id": "1410.7182v1", 
    "author": "Kalina Bontcheva", 
    "publish": "2014-10-27T11:09:36Z", 
    "summary": "Applying natural language processing for mining and intelligent information\naccess to tweets (a form of microblog) is a challenging, emerging research\narea. Unlike carefully authored news text and other longer content, tweets pose\na number of new challenges, due to their short, noisy, context-dependent, and\ndynamic nature. Information extraction from tweets is typically performed in a\npipeline, comprising consecutive stages of language identification,\ntokenisation, part-of-speech tagging, named entity recognition and entity\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\nnamed entity recognition and disambiguation, investigating how robust a number\nof state-of-the-art systems are on such noisy texts, what the main sources of\nerror are, and which problems should be further investigated to improve the\nstate of the art."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1410.7787v1", 
    "title": "Correcting Errors in Digital Lexicographic Resources Using a Dictionary   Manipulation Language", 
    "arxiv-id": "1410.7787v1", 
    "author": "Michael Bloodgood", 
    "publish": "2014-10-28T20:12:50Z", 
    "summary": "We describe a paradigm for combining manual and automatic error correction of\nnoisy structured lexicographic data. Modifications to the structure and\nunderlying text of the lexicographic data are expressed in a simple,\ninterpreted programming language. Dictionary Manipulation Language (DML)\ncommands identify nodes by unique identifiers, and manipulations are performed\nusing simple commands such as create, move, set text, etc. Corrected lexicons\nare produced by applying sequences of DML commands to the source version of the\nlexicon. DML commands can be written manually to repair one-off errors or\ngenerated automatically to correct recurring problems. We discuss advantages of\nthe paradigm for the task of editing digital bilingual dictionaries."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1410.8668v1", 
    "title": "Experiments to Improve Named Entity Recognition on Turkish Tweets", 
    "arxiv-id": "1410.8668v1", 
    "author": "Ralf Steinberger", 
    "publish": "2014-10-31T08:35:55Z", 
    "summary": "Social media texts are significant information sources for several\napplication areas including trend analysis, event monitoring, and opinion\nmining. Unfortunately, existing solutions for tasks such as named entity\nrecognition that perform well on formal texts usually perform poorly when\napplied to social media texts. In this paper, we report on experiments that\nhave the purpose of improving named entity recognition on Turkish tweets, using\ntwo different annotated data sets. In these experiments, starting with a\nbaseline named entity recognition system, we adapt its recognition rules and\nresources to better fit Twitter language by relaxing its capitalization\nconstraint and by diacritics-based expansion of its lexical resources, and we\nemploy a simplistic normalization scheme on tweets to observe the effects of\nthese on the overall named entity recognition performance on Turkish tweets.\nThe evaluation results of the system with these different settings are provided\nwith discussions of these results."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.0588v1", 
    "title": "On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using   GATE", 
    "arxiv-id": "1411.0588v1", 
    "author": "Elena Karashtranova", 
    "publish": "2014-11-03T18:05:54Z", 
    "summary": "In this article, we describe an approach for automatic detection of\nnoun-adjective agreement errors in Bulgarian texts by explaining the necessary\nsteps required to develop a simple Java-based language processing application.\nFor this purpose, we use the GATE language processing framework, which is\ncapable of analyzing texts in Bulgarian language and can be embedded in\nsoftware applications, accessed through a set of Java APIs. In our example\napplication we also demonstrate how to use the functionality of GATE to perform\nregular expressions over annotations for detecting agreement errors in simple\nnoun phrases formed by two words - attributive adjective and a noun, where the\nattributive adjective precedes the noun. The provided code samples can also be\nused as a starting point for implementing natural language processing\nfunctionalities in software applications related to language processing tasks\nlike detection, annotation and retrieval of word groups meeting a specific set\nof criteria."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.0778v1", 
    "title": "Detecting Suicidal Ideation in Chinese Microblogs with Psychological   Lexicons", 
    "arxiv-id": "1411.0778v1", 
    "author": "Xin Li", 
    "publish": "2014-11-04T03:48:20Z", 
    "summary": "Suicide is among the leading causes of death in China. However, technical\napproaches toward preventing suicide are challenging and remaining under\ndevelopment. Recently, several actual suicidal cases were preceded by users who\nposted microblogs with suicidal ideation to Sina Weibo, a Chinese social media\nnetwork akin to Twitter. It would therefore be desirable to detect suicidal\nideations from microblogs in real-time, and immediately alert appropriate\nsupport groups, which may lead to successful prevention. In this paper, we\npropose a real-time suicidal ideation detection system deployed over Weibo,\nusing machine learning and known psychological techniques. Currently, we have\nidentified 53 known suicidal cases who posted suicide notes on Weibo prior to\ntheir deaths.We explore linguistic features of these known cases using a\npsychological lexicon dictionary, and train an effective suicidal Weibo post\ndetection model. 6714 tagged posts and several classifiers are used to verify\nthe model. By combining both machine learning and psychological knowledge, SVM\nclassifier has the best performance of different classifiers, yielding an\nF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.2738v4", 
    "title": "word2vec Parameter Learning Explained", 
    "arxiv-id": "1411.2738v4", 
    "author": "Xin Rong", 
    "publish": "2014-11-11T09:24:00Z", 
    "summary": "The word2vec model and application by Mikolov et al. have attracted a great\namount of attention in recent two years. The vector representations of words\nlearned by word2vec models have been shown to carry semantic meanings and are\nuseful in various NLP tasks. As an increasing number of researchers would like\nto experiment with word2vec or similar techniques, I notice that there lacks a\nmaterial that comprehensively explains the parameter learning process of word\nembedding models in details, thus preventing researchers that are non-experts\nin neural networks from understanding the working mechanism of such models.\n  This note provides detailed derivations and explanations of the parameter\nupdate equations of the word2vec models, including the original continuous\nbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization\ntechniques, including hierarchical softmax and negative sampling. Intuitive\ninterpretations of the gradient equations are also provided alongside\nmathematical derivations.\n  In the appendix, a review on the basics of neuron networks and\nbackpropagation is provided. I also created an interactive demo, wevi, to\nfacilitate the intuitive understanding of the model."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.3146v1", 
    "title": "Distributed Representations for Compositional Semantics", 
    "arxiv-id": "1411.3146v1", 
    "author": "Karl Moritz Hermann", 
    "publish": "2014-11-12T11:26:51Z", 
    "summary": "The mathematical representation of semantics is a key issue for Natural\nLanguage Processing (NLP). A lot of research has been devoted to finding ways\nof representing the semantics of individual words in vector spaces.\nDistributional approaches --- meaning distributed representations that exploit\nco-occurrence statistics of large corpora --- have proved popular and\nsuccessful across a number of tasks. However, natural language usually comes in\nstructures beyond the word level, with meaning arising not only from the\nindividual words but also the structure they are contained in at the phrasal or\nsentential level. Modelling the compositional process by which the meaning of\nan utterance arises from the meaning of its parts is an equally fundamental\ntask of NLP.\n  This dissertation explores methods for learning distributed semantic\nrepresentations and models for composing these into representations for larger\nlinguistic units. Our underlying hypothesis is that neural models are a\nsuitable vehicle for learning semantically rich representations and that such\nrepresentations in turn are suitable vehicles for solving important tasks in\nnatural language processing. The contribution of this thesis is a thorough\nevaluation of our hypothesis, as part of which we introduce several new\napproaches to representation learning and compositional semantics, as well as\nmultiple state-of-the-art models which apply distributed semantic\nrepresentations to various tasks in NLP."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.3561v1", 
    "title": "A Text to Speech (TTS) System with English to Punjabi Conversion", 
    "arxiv-id": "1411.3561v1", 
    "author": "Amritpal Singh", 
    "publish": "2014-11-13T14:44:00Z", 
    "summary": "The paper aims to show how an application can be developed that converts the\nEnglish language into the Punjabi Language, and the same application can\nconvert the Text to Speech(TTS) i.e. pronounce the text. This application can\nbe really beneficial for those with special needs."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.4166v4", 
    "title": "Retrofitting Word Vectors to Semantic Lexicons", 
    "arxiv-id": "1411.4166v4", 
    "author": "Noah A. Smith", 
    "publish": "2014-11-15T17:34:20Z", 
    "summary": "Vector space word representations are learned from distributional information\nof words in large corpora. Although such statistics are semantically\ninformative, they disregard the valuable information that is contained in\nsemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This\npaper proposes a method for refining vector space representations using\nrelational information from semantic lexicons by encouraging linked words to\nhave similar vector representations, and it makes no assumptions about how the\ninput vectors were constructed. Evaluated on a battery of standard lexical\nsemantic evaluation tasks in several languages, we obtain substantial\nimprovements starting with a variety of word vector models. Our refinement\nmethod outperforms prior techniques for incorporating semantic lexicons into\nthe word vector training algorithms."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.4472v1", 
    "title": "Opinion mining of text documents written in Macedonian language", 
    "arxiv-id": "1411.4472v1", 
    "author": "Ljupco Kocarev", 
    "publish": "2014-11-17T13:36:49Z", 
    "summary": "The ability to extract public opinion from web portals such as review sites,\nsocial networks and blogs will enable companies and individuals to form a view,\nan attitude and make decisions without having to do lengthy and costly\nresearches and surveys. In this paper machine learning techniques are used for\ndetermining the polarity of forum posts on kajgana which are written in\nMacedonian language. The posts are classified as being positive, negative or\nneutral. We test different feature metrics and classifiers and provide detailed\nevaluation of their participation in improving the overall performance on a\nmanually generated dataset. By achieving 92% accuracy, we show that the\nperformance of systems for automated opinion mining is comparable to a human\nevaluator, thus making it a viable option for text data analysis. Finally, we\npresent a few statistics derived from the forum posts using the developed\nsystem."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.4614v1", 
    "title": "Using graph transformation algorithms to generate natural language   equivalents of icons expressing medical concepts", 
    "arxiv-id": "1411.4614v1", 
    "author": "Jean-Baptiste Lamy", 
    "publish": "2014-09-26T05:09:40Z", 
    "summary": "A graphical language addresses the need to communicate medical information in\na synthetic way. Medical concepts are expressed by icons conveying fast visual\ninformation about patients' current state or about the known effects of drugs.\nIn order to increase the visual language's acceptance and usability, a natural\nlanguage generation interface is currently developed. In this context, this\npaper describes the use of an informatics method ---graph transformation--- to\nprepare data consisting of concepts in an OWL-DL ontology for use in a natural\nlanguage generation component. The OWL concept may be considered as a\nstar-shaped graph with a central node. The method transforms it into a graph\nrepresenting the deep semantic structure of a natural language phrase. This\nwork may be of future use in other contexts where ontology concepts have to be\nmapped to half-formalized natural language expressions."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.4960v1", 
    "title": "Network Motifs Analysis of Croatian Literature", 
    "arxiv-id": "1411.4960v1", 
    "author": "Ana Me\u0161trovi\u0107", 
    "publish": "2014-11-18T18:46:36Z", 
    "summary": "In this paper we analyse network motifs in the co-occurrence directed\nnetworks constructed from five different texts (four books and one portal) in\nthe Croatian language. After preparing the data and network construction, we\nperform the network motif analysis. We analyse the motif frequencies and\nZ-scores in the five networks. We present the triad significance profile for\nfive datasets. Furthermore, we compare our results with the existing results\nfor the linguistic networks. Firstly, we show that the triad significance\nprofile for the Croatian language is very similar with the other languages and\nall the networks belong to the same family of networks. However, there are\ncertain differences between the Croatian language and other analysed languages.\nWe conclude that this is due to the free word-order of the Croatian language."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ipm.2014.10.006", 
    "link": "http://arxiv.org/pdf/1411.5379v3", 
    "title": "Type-Driven Incremental Semantic Parsing with Polymorphism", 
    "arxiv-id": "1411.5379v3", 
    "author": "Liang Huang", 
    "publish": "2014-11-19T21:06:15Z", 
    "summary": "Semantic parsing has made significant progress, but most current semantic\nparsers are extremely slow (CKY-based) and rather primitive in representation.\nWe introduce three new techniques to tackle these problems. First, we design\nthe first linear-time incremental shift-reduce-style semantic parsing algorithm\nwhich is more efficient than conventional cubic-time bottom-up semantic\nparsers. Second, our parser, being type-driven instead of syntax-driven, uses\ntype-checking to decide the direction of reduction, which eliminates the need\nfor a syntactic grammar such as CCG. Third, to fully exploit the power of\ntype-driven semantic parsing beyond simple types (such as entities and truth\nvalues), we borrow from programming language theory the concepts of subtype\npolymorphism and parametric polymorphism to enrich the type system in order to\nbetter guide the parsing. Our system learns very accurate parses in GeoQuery,\nJobs and Atis domains."
},{
    "category": "cs.CL", 
    "doi": "10.14445/22315381/IJETT-V17P229", 
    "link": "http://arxiv.org/pdf/1411.5796v1", 
    "title": "Pre-processing of Domain Ontology Graph Generation System in Punjabi", 
    "arxiv-id": "1411.5796v1", 
    "author": "Saurabh Sharma", 
    "publish": "2014-11-21T08:50:30Z", 
    "summary": "This paper describes pre-processing phase of ontology graph generation system\nfrom Punjabi text documents of different domains. This research paper focuses\non pre-processing of Punjabi text documents. Pre-processing is structured\nrepresentation of the input text. Pre-processing of ontology graph generation\nincludes allowing input restrictions to the text, removal of special symbols\nand punctuation marks, removal of duplicate terms, removal of stop words,\nextract terms by matching input terms with dictionary and gazetteer lists\nterms."
},{
    "category": "cs.CL", 
    "doi": "10.14445/22315381/IJETT-V17P229", 
    "link": "http://arxiv.org/pdf/1411.7820v1", 
    "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic   Models and Encyclopedic Knowledge", 
    "arxiv-id": "1411.7820v1", 
    "author": "Angela Fahrni", 
    "publish": "2014-11-28T11:33:02Z", 
    "summary": "We present a method for coarse-grained cross-lingual alignment of comparable\ntexts: segments consisting of contiguous paragraphs that discuss the same theme\n(e.g. history, economy) are aligned based on induced multilingual topics. The\nmethod combines three ideas: a two-level LDA model that filters out words that\ndo not convey themes, an HMM that models the ordering of themes in the\ncollection of documents, and language-independent concept annotations to serve\nas a cross-language bridge and to strengthen the connection between paragraphs\nin the same segment through concept relations. The method is evaluated on\nEnglish and French data previously used for monolingual alignment. The results\nshow state-of-the-art performance in both monolingual and cross-lingual\nsettings."
},{
    "category": "cs.CL", 
    "doi": "10.14445/22315381/IJETT-V17P229", 
    "link": "http://arxiv.org/pdf/1411.7942v2", 
    "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs", 
    "arxiv-id": "1411.7942v2", 
    "author": "Stephen Clark", 
    "publish": "2014-11-28T16:57:34Z", 
    "summary": "The functional approach to compositional distributional semantics considers\ntransitive verbs to be linear maps that transform the distributional vectors\nrepresenting nouns into a vector representing a sentence. We conduct an initial\ninvestigation that uses a matrix consisting of the parameters of a logistic\nregression classifier trained on a plausibility task as a transitive verb\nfunction. We compare our method to a commonly used corpus-based method for\nconstructing a verb matrix and find that the plausibility training may be more\neffective for disambiguation tasks."
},{
    "category": "cs.CL", 
    "doi": "10.14445/22315381/IJETT-V17P229", 
    "link": "http://arxiv.org/pdf/1412.0751v1", 
    "title": "Tiered Clustering to Improve Lexical Entailment", 
    "arxiv-id": "1412.0751v1", 
    "author": "John Wieting", 
    "publish": "2014-12-02T00:53:35Z", 
    "summary": "Many tasks in Natural Language Processing involve recognizing lexical\nentailment. Two different approaches to this problem have been proposed\nrecently that are quite different from each other. The first is an asymmetric\nsimilarity measure designed to give high scores when the contexts of the\nnarrower term in the entailment are a subset of those of the broader term. The\nsecond is a supervised approach where a classifier is learned to predict\nentailment given a concatenated latent vector representation of the word. Both\nof these approaches are vector space models that use a single context vector as\na representation of the word. In this work, I study the effects of clustering\nwords into senses and using these multiple context vectors to infer entailment\nusing extensions of these two algorithms. I find that this approach offers some\nimprovement to these entailment algorithms."
},{
    "category": "cs.CL", 
    "doi": "10.14445/22315381/IJETT-V17P229", 
    "link": "http://arxiv.org/pdf/1412.1215v1", 
    "title": "Mary Astell's words in A Serious Proposal to the Ladies (part I), a   lexicographic inquiry with NooJ", 
    "arxiv-id": "1412.1215v1", 
    "author": "Odile Piton", 
    "publish": "2014-12-03T07:16:04Z", 
    "summary": "In the following article we elected to study with NooJ the lexis of a 17 th\ncentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,\npart I, published in 1694. We first focused on the semantics to see how Astell\nbuilds her vindication of the female sex, which words she uses to sensitise\nwomen to their alienated condition and promote their education. Then we studied\nthe morphology of the lexemes (which is different from contemporary English)\nused by the author, thanks to the NooJ tools we have devised for this purpose.\nNooJ has great functionalities for lexicographic work. Its commands and graphs\nprove to be most efficient in the spotting of archaic words or variants in\nspelling. Introduction In our previous articles, we have studied the\nsingularities of 17 th century English within the framework of a diachronic\nanalysis thanks to syntactical and morphological graphs and thanks to the\ndictionaries we have compiled from a corpus that may be expanded overtime. Our\nearly work was based on a limited corpus of English travel literature to Greece\nin the 17 th century. This article deals with a late seventeenth century text\nwritten by a woman philosopher and essayist, Mary Astell (1666--1731),\nconsidered as one of the first English feminists. Astell wrote her essay at a\ntime in English history when women were \"the weaker vessel\" and their main\nbusiness in life was to charm and please men by their looks and submissiveness.\nIn this essay we will see how NooJ can help us analyse Astell's rhetoric (what\npoint of view does she adopt, does she speak in her own name, in the name of\nall women, what is her representation of men and women and their relationships\nin the text, what are the goals of education?). Then we will turn our attention\nto the morphology of words in the text and use NooJ commands and graphs to\ncarry out a lexicographic inquiry into Astell's lexemes."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.1342v1", 
    "title": "A perspective on the advancement of natural language processing tasks   via topological analysis of complex networks", 
    "arxiv-id": "1412.1342v1", 
    "author": "Diego R. Amancio", 
    "publish": "2014-12-03T14:37:36Z", 
    "summary": "Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618)."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.1632v1", 
    "title": "Deep Learning for Answer Sentence Selection", 
    "arxiv-id": "1412.1632v1", 
    "author": "Stephen Pulman", 
    "publish": "2014-12-04T11:53:02Z", 
    "summary": "Answer sentence selection is the task of identifying sentences that contain\nthe answer to a given question. This is an important problem in its own right\nas well as in the larger context of open domain question answering. We propose\na novel approach to solving this task via means of distributed representations,\nand learn to match questions with answers by considering their semantic\nencoding. This contrasts prior work on this task, which typically relies on\nclassifiers with large numbers of hand-crafted syntactic and semantic features\nand various external resources. Our approach does not require any feature\nengineering nor does it involve specialist linguistic data, making this model\neasily applicable to a wide range of domains and languages. Experimental\nresults on a standard benchmark dataset from TREC demonstrate that---despite\nits simplicity---our model matches state of the art performance on the answer\nsentence selection task."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.1820v2", 
    "title": "Context-Dependent Fine-Grained Entity Type Tagging", 
    "arxiv-id": "1412.1820v2", 
    "author": "David Huynh", 
    "publish": "2014-12-03T23:26:33Z", 
    "summary": "Entity type tagging is the task of assigning category labels to each mention\nof an entity in a document. While standard systems focus on a small set of\ntypes, recent work (Ling and Weld, 2012) suggests that using a large\nfine-grained label set can lead to dramatic improvements in downstream tasks.\nIn the absence of labeled training data, existing fine-grained tagging systems\nobtain examples automatically, using resolved entities and their types\nextracted from a knowledge base. However, since the appropriate type often\ndepends on context (e.g. Washington could be tagged either as city or\ngovernment), this procedure can result in spurious labels, leading to poorer\ngeneralization. We propose the task of context-dependent fine type tagging,\nwhere the set of acceptable labels for a mention is restricted to only those\ndeducible from the local context (e.g. sentence or document). We introduce new\nresources for this task: 12,017 mentions annotated with their context-dependent\nfine types, and we provide baseline experimental results on this data."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.2007v2", 
    "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", 
    "arxiv-id": "1412.2007v2", 
    "author": "Yoshua Bengio", 
    "publish": "2014-12-05T14:26:27Z", 
    "summary": "Neural machine translation, a recently proposed approach to machine\ntranslation based purely on neural networks, has shown promising results\ncompared to the existing approaches such as phrase-based statistical machine\ntranslation. Despite its recent success, neural machine translation has its\nlimitation in handling a larger vocabulary, as training complexity as well as\ndecoding complexity increase proportionally to the number of target words. In\nthis paper, we propose a method that allows us to use a very large target\nvocabulary without increasing training complexity, based on importance\nsampling. We show that decoding can be efficiently done even with the model\nhaving a very large target vocabulary by selecting only a small subset of the\nwhole target vocabulary. The models trained by the proposed approach are\nempirically found to outperform the baseline models with a small vocabulary as\nwell as the LSTM-based neural machine translation models. Furthermore, when we\nuse the ensemble of a few models with very large target vocabularies, we\nachieve the state-of-the-art translation performance (measured by BLEU) on the\nEnglish->German translation and almost as high performance as state-of-the-art\nEnglish->French translation system."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.2197v3", 
    "title": "Practice in Synonym Extraction at Large Scale", 
    "arxiv-id": "1412.2197v3", 
    "author": "Chang Wang", 
    "publish": "2014-12-06T04:40:18Z", 
    "summary": "Synonym extraction is an important task in natural language processing and\noften used as a submodule in query expansion, question answering and other\napplications. Automatic synonym extractor is highly preferred for large scale\napplications. Previous studies in synonym extraction are most limited to small\nscale datasets. In this paper, we build a large dataset with 3.4 million\nsynonym/non-synonym pairs to capture the challenges in real world scenarios. We\nproposed (1) a new cost function to accommodate the unbalanced learning\nproblem, and (2) a feature learning based deep neural network to model the\ncomplicated relationships in synonym pairs. We compare several different\napproaches based on SVMs and neural networks, and find out a novel feature\nlearning based neural network outperforms the methods with hand-assigned\nfeatures. Specifically, the best performance of our model surpasses the SVM\nbaseline with a significant 97\\% relative improvement."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.2378v1", 
    "title": "Learning Word Representations from Relational Graphs", 
    "arxiv-id": "1412.2378v1", 
    "author": "Ken-ichi Kawarabayashi", 
    "publish": "2014-12-07T17:49:53Z", 
    "summary": "Attributes of words and relations between two words are central to numerous\ntasks in Artificial Intelligence such as knowledge representation, similarity\nmeasurement, and analogy detection. Often when two words share one or more\nattributes in common, they are connected by some semantic relations. On the\nother hand, if there are numerous semantic relations between two words, we can\nexpect some of the attributes of one of the words to be inherited by the other.\nMotivated by this close connection between attributes and relations, given a\nrelational graph in which words are inter- connected via numerous semantic\nrelations, we propose a method to learn a latent representation for the\nindividual words. The proposed method considers not only the co-occurrences of\nwords as done by existing approaches for word representation learning, but also\nthe semantic relations in which two words co-occur. To evaluate the accuracy of\nthe word representations learnt using the proposed method, we use the learnt\nword representations to solve semantic word analogy problems. Our experimental\nresults show that it is possible to learn better word representations by using\nsemantic semantics between words."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.2442v1", 
    "title": "Rediscovering the Alphabet - On the Innate Universal Grammar", 
    "arxiv-id": "1412.2442v1", 
    "author": "Asaad Kaadan", 
    "publish": "2014-12-08T04:14:05Z", 
    "summary": "Universal Grammar (UG) theory has been one of the most important research\ntopics in linguistics since introduced five decades ago. UG specifies the\nrestricted set of languages learnable by human brain, and thus, many\nresearchers believe in its biological roots. Numerous empirical studies of\nneurobiological and cognitive functions of the human brain, and of many natural\nlanguages, have been conducted to unveil some aspects of UG. This, however,\nresulted in different and sometimes contradicting theories that do not indicate\na universally unique grammar. In this research, we tackle the UG problem from\nan entirely different perspective. We search for the Unique Universal Grammar\n(UUG) that facilitates communication and knowledge transfer, the sole purpose\nof a language. We formulate this UG and show that it is unique, intrinsic, and\ncosmic, rather than humanistic. Initial analysis on a widespread natural\nlanguage already showed some positive results."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.plrev.2014.07.010", 
    "link": "http://arxiv.org/pdf/1412.3336v1", 
    "title": "Statistical Patterns in Written Language", 
    "arxiv-id": "1412.3336v1", 
    "author": "Dami\u00e1n H. Zanette", 
    "publish": "2014-12-10T15:22:08Z", 
    "summary": "Quantitative linguistics has been allowed, in the last few decades, within\nthe admittedly blurry boundaries of the field of complex systems. A growing\nhost of applied mathematicians and statistical physicists devote their efforts\nto disclose regularities, correlations, patterns, and structural properties of\nlanguage streams, using techniques borrowed from statistics and information\ntheory. Overall, results can still be categorized as modest, but the prospects\nare promising: medium- and long-range features in the organization of human\nlanguage -which are beyond the scope of traditional linguistics- have already\nemerged from this kind of analysis and continue to be reported, contributing a\nnew perspective to our understanding of this most complex communication system.\nThis short book is intended to review some of these recent contributions."
},{
    "category": "cs.CL", 
    "doi": "10.3233/AIC-150698", 
    "link": "http://arxiv.org/pdf/1412.4021v5", 
    "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules   for Part-of-Speech Tagging", 
    "arxiv-id": "1412.4021v5", 
    "author": "Son Bao Pham", 
    "publish": "2014-12-12T15:26:43Z", 
    "summary": "In this paper, we propose a new approach to construct a system of\ntransformation rules for the Part-of-Speech (POS) tagging task. Our approach is\nbased on an incremental knowledge acquisition method where rules are stored in\nan exception structure and new rules are only added to correct the errors of\nexisting rules; thus allowing systematic control of the interaction between the\nrules. Experimental results on 13 languages show that our approach is fast in\nterms of training time and tagging speed. Furthermore, our approach obtains\nvery competitive accuracy in comparison to state-of-the-art POS and\nmorphological taggers."
},{
    "category": "cs.CL", 
    "doi": "10.3233/AIC-150698", 
    "link": "http://arxiv.org/pdf/1412.4369v3", 
    "title": "Incorporating Both Distributional and Relational Semantics in Word   Representations", 
    "arxiv-id": "1412.4369v3", 
    "author": "Kevin Duh", 
    "publish": "2014-12-14T15:18:18Z", 
    "summary": "We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases."
},{
    "category": "cs.CL", 
    "doi": "10.3233/AIC-150698", 
    "link": "http://arxiv.org/pdf/1412.4682v1", 
    "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on   Plutchik's Wheel", 
    "arxiv-id": "1412.4682v1", 
    "author": "Mykola Pechenizkiy", 
    "publish": "2014-12-15T17:20:47Z", 
    "summary": "We study sentiment analysis beyond the typical granularity of polarity and\ninstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an\nextension to the Rule-Based Emission Model algorithm to deduce such emotions\nfrom human-written messages. We evaluate our approach on two different datasets\nand compare its performance with the current state-of-the-art techniques for\nemotion detection, including a recursive auto-encoder. The results of the\nexperimental study suggest that RBEM-Emo is a promising approach advancing the\ncurrent state-of-the-art in emotion detection."
},{
    "category": "cs.CL", 
    "doi": "10.3233/AIC-150698", 
    "link": "http://arxiv.org/pdf/1412.4930v2", 
    "title": "Rehabilitation of Count-based Models for Word Vector Representations", 
    "arxiv-id": "1412.4930v2", 
    "author": "Ronan Collobert", 
    "publish": "2014-12-16T09:43:56Z", 
    "summary": "Recent works on word representations mostly rely on predictive models.\nDistributed word representations (aka word embeddings) are trained to optimally\npredict the contexts in which the corresponding words tend to appear. Such\nmodels have succeeded in capturing word similarties as well as semantic and\nsyntactic regularities. Instead, we aim at reviving interest in a model based\non counts. We present a systematic study of the use of the Hellinger distance\nto extract semantic representations from the word co-occurence statistics of\nlarge text corpora. We show that this distance gives good performance on word\nsimilarity and analogy tasks, with a proper type and size of context, and a\ndimensionality reduction based on a stochastic low-rank approximation. Besides\nbeing both simple and intuitive, this method also provides an encoding function\nwhich can be used to infer unseen words or phrases. This becomes a clear\nadvantage compared to predictive models which must train these new words."
},{
    "category": "cs.CL", 
    "doi": "10.3233/978-1-61499-468-8-131", 
    "link": "http://arxiv.org/pdf/1412.5212v1", 
    "title": "Application of Topic Models to Judgments from Public Procurement Domain", 
    "arxiv-id": "1412.5212v1", 
    "author": "Micha\u0142 \u0141opuszy\u0144ski", 
    "publish": "2014-12-16T22:00:52Z", 
    "summary": "In this work, automatic analysis of themes contained in a large corpora of\njudgments from public procurement domain is performed. The employed technique\nis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,\nto use LDA in conjunction with recently developed method of unsupervised\nkeyword extraction. Such an approach improves the interpretability of the\nautomatically obtained topics and allows for better computational performance.\nThe described analysis illustrates a potential of the method in detecting\nrecurring themes and discovering temporal trends in lodged contract appeals.\nThese results may be in future applied to improve information retrieval from\nrepositories of legal texts or as auxiliary material for legal analyses carried\nout by human experts."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.5477v1", 
    "title": "Computational Model to Generate Case-Inflected Forms of Masculine Nouns   for Word Search in Sanskrit E-Text", 
    "arxiv-id": "1412.5477v1", 
    "author": "Lakshmanan Meenakshi", 
    "publish": "2014-12-17T16:56:43Z", 
    "summary": "The problem of word search in Sanskrit is inseparable from complexities that\ninclude those caused by euphonic conjunctions and case-inflections. The\ncase-inflectional forms of a noun normally number 24 owing to the fact that in\nSanskrit there are eight cases and three numbers-singular, dual and plural. The\ntraditional method of generating these inflectional forms is rather elaborate\nowing to the fact that there are differences in the forms generated between\neven very similar words and there are subtle nuances involved. Further, it\nwould be a cumbersome exercise to generate and search for 24 forms of a word\nduring a word search in a large text, using the currently available\ncase-inflectional form generators. This study presents a new approach to\ngenerating case-inflectional forms that is simpler to compute. Further, an\noptimized model that is sufficient for generating only those word forms that\nare required in a word search and is more than 80% efficient compared to the\ncomplete case-inflectional forms generator, is presented in this study for the\nfirst time."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.5836v3", 
    "title": "Incorporating Both Distributional and Relational Semantics in Word   Representations", 
    "arxiv-id": "1412.5836v3", 
    "author": "Kevin Duh", 
    "publish": "2014-12-18T12:30:55Z", 
    "summary": "We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6045v2", 
    "title": "A Simple and Efficient Method To Generate Word Sense Representations", 
    "arxiv-id": "1412.6045v2", 
    "author": "Richard Johansson", 
    "publish": "2014-12-18T20:14:10Z", 
    "summary": "Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6264v1", 
    "title": "Supertagging: Introduction, learning, and application", 
    "arxiv-id": "1412.6264v1", 
    "author": "Taraka Rama K", 
    "publish": "2014-12-19T09:53:57Z", 
    "summary": "Supertagging is an approach originally developed by Bangalore and Joshi\n(1999) to improve the parsing efficiency. In the beginning, the scholars used\nsmall training datasets and somewhat na\\\"ive smoothing techniques to learn the\nprobability distributions of supertags. Since its inception, the applicability\nof Supertags has been explored for TAG (tree-adjoining grammar) formalism as\nwell as other related yet, different formalisms such as CCG. This article will\ntry to summarize the various chapters, relevant to statistical parsing, from\nthe most recent edited book volume (Bangalore and Joshi, 2010). The chapters\nwere selected so as to blend the learning of supertags, its integration into\nfull-scale parsing, and in semantic parsing."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6277v2", 
    "title": "N-gram-Based Low-Dimensional Representation for Document Classification", 
    "arxiv-id": "1412.6277v2", 
    "author": "Ronan Collobert", 
    "publish": "2014-12-19T10:29:33Z", 
    "summary": "The bag-of-words (BOW) model is the common approach for classifying\ndocuments, where words are used as feature for training a classifier. This\ngenerally involves a huge number of features. Some techniques, such as Latent\nSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been\ndesigned to summarize documents in a lower dimension with the least semantic\ninformation loss. Some semantic information is nevertheless always lost, since\nonly words are considered. Instead, we aim at using information coming from\nn-grams to overcome this limitation, while remaining in a low-dimension space.\nMany approaches, such as the Skip-gram model, provide good word vector\nrepresentations very quickly. We propose to average these representations to\nobtain representations of n-grams. All n-grams are thus embedded in a same\nsemantic space. A K-means clustering can then group them into semantic\nconcepts. The number of features is therefore dramatically reduced and\ndocuments can be represented as bag of semantic concepts. We show that this\nmodel outperforms LSA and LDA on a sentiment classification task, and yields\nsimilar results than a traditional BOW-model with far less features."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6334v4", 
    "title": "Leveraging Monolingual Data for Crosslingual Compositional Word   Representations", 
    "arxiv-id": "1412.6334v4", 
    "author": "Akiko Aizawa", 
    "publish": "2014-12-19T13:23:35Z", 
    "summary": "In this work, we present a novel neural network based architecture for\ninducing compositional crosslingual word representations. Unlike previously\nproposed methods, our method fulfills the following three criteria; it\nconstrains the word-level representations to be compositional, it is capable of\nleveraging both bilingual and monolingual data, and it is scalable to large\nvocabularies and large quantities of data. The key component of our approach is\nwhat we refer to as a monolingual inclusion criterion, that exploits the\nobservation that phrases are more closely semantically related to their\nsub-phrases than to other randomly sampled phrases. We evaluate our method on a\nwell-established crosslingual document classification task and achieve results\nthat are either comparable, or greatly improve upon previous state-of-the-art\nmethods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for\nthe English to German and German to English sub-tasks respectively. The former\nadvances the state of the art by 0.9% points of accuracy, the latter is an\nabsolute improvement upon the previous state of the art by 7.7% points of\naccuracy and an improvement of 33.0% in error reduction."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6448v4", 
    "title": "Embedding Word Similarity with Neural Machine Translation", 
    "arxiv-id": "1412.6448v4", 
    "author": "Yoshua Bengio", 
    "publish": "2014-12-19T17:22:03Z", 
    "summary": "Neural language models learn word representations, or embeddings, that\ncapture rich linguistic and conceptual information. Here we investigate the\nembeddings learned by neural machine translation models, a recently-developed\nclass of neural language model. We show that embeddings from translation models\noutperform those learned by monolingual models at tasks that require knowledge\nof both conceptual similarity and lexical-syntactic role. We further show that\nthese effects hold when translating from both English to French and English to\nGerman, and argue that the desirable properties of translation embeddings\nshould emerge largely independently of the source and target languages.\nFinally, we apply a new method for training neural translation models with very\nlarge vocabularies, and show that this vocabulary expansion algorithm results\nin minimal degradation of embedding quality. Our embedding spaces can be\nqueried in an online demo and downloaded from our web page. Overall, our\nanalyses indicate that translation-based embeddings should be used in\napplications that require concepts to be organised according to similarity\nand/or lexical function, while monolingual embeddings are better suited to\nmodelling (nonspecific) inter-word relatedness."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.6575v4", 
    "title": "Embedding Entities and Relations for Learning and Inference in Knowledge   Bases", 
    "arxiv-id": "1412.6575v4", 
    "author": "Li Deng", 
    "publish": "2014-12-20T01:37:16Z", 
    "summary": "We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.7119v3", 
    "title": "Pragmatic Neural Language Modelling in Machine Translation", 
    "arxiv-id": "1412.7119v3", 
    "author": "Phil Blunsom", 
    "publish": "2014-12-22T20:08:06Z", 
    "summary": "This paper presents an in-depth investigation on integrating neural language\nmodels in translation systems. Scaling neural language models is a difficult\ntask, but crucial for real-world applications. This paper evaluates the impact\non end-to-end MT quality of both new and existing scaling techniques. We show\nwhen explicitly normalising neural models is necessary and what optimisation\ntricks one should use in such scenarios. We also focus on scalable training\nalgorithms and investigate noise contrastive estimation and diagonal contexts\nas sources for further speed improvements. We explore the trade-offs between\nneural models and back-off n-gram models and find that neural models make\nstrong candidates for natural language applications in memory constrained\nenvironments, yet still lag behind traditional models in raw translation\nquality. We conclude with a set of recommendations one should follow to build a\nscalable neural language model for MT."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.7415v2", 
    "title": "A prototype Malayalam to Sign Language Automatic Translator", 
    "arxiv-id": "1412.7415v2", 
    "author": "Kannan Balakrishnan", 
    "publish": "2014-12-23T15:51:41Z", 
    "summary": "Sign language, which is a medium of communication for deaf people, uses\nmanual communication and body language to convey meaning, as opposed to using\nsound. This paper presents a prototype Malayalam text to sign language\ntranslation system. The proposed system takes Malayalam text as input and\ngenerates corresponding Sign Language. Output animation is rendered using a\ncomputer generated model. This system will help to disseminate information to\nthe deaf people in public utility places like railways, banks, hospitals etc.\nThis will also act as an educational tool in learning Sign Language."
},{
    "category": "cs.CL", 
    "doi": "10.3844/jcssp.2014.2260.2268", 
    "link": "http://arxiv.org/pdf/1412.8010v1", 
    "title": "Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary", 
    "arxiv-id": "1412.8010v1", 
    "author": "Seong-Bae Park", 
    "publish": "2014-12-27T01:54:15Z", 
    "summary": "SentiWordNet is an important lexical resource supporting sentiment analysis\nin opinion mining applications. In this paper, we propose a novel approach to\nconstruct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated\nfrom WordNet in which each synset has numerical scores to indicate its opinion\npolarities. Many previous studies obtained these scores by applying a machine\nlearning method to WordNet. However, Vietnamese WordNet is not available\nunfortunately by the time of this paper. Therefore, we propose a method to\nconstruct VSWN from a Vietnamese dictionary, not from WordNet. We show the\neffectiveness of the proposed method by generating a VSWN with 39,561 synsets\nautomatically. The method is experimentally tested with 266 synsets with aspect\nof positivity and negativity. It attains a competitive result compared with\nEnglish SentiWordNet that is 0.066 and 0.052 differences for positivity and\nnegativity sets respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.01245v1", 
    "title": "Authorship recognition via fluctuation analysis of network topology and   word intermittency", 
    "arxiv-id": "1502.01245v1", 
    "author": "Diego R. Amancio", 
    "publish": "2015-02-04T16:12:45Z", 
    "summary": "Statistical methods have been widely employed in many practical natural\nlanguage processing applications. More specifically, complex networks concepts\nand methods from dynamical systems theory have been successfully applied to\nrecognize stylistic patterns in written texts. Despite the large amount of\nstudies devoted to represent texts with physical models, only a few studies\nhave assessed the relevance of attributes derived from the analysis of\nstylistic fluctuations. Because fluctuations represent a pivotal factor for\ncharacterizing a myriad of real systems, this study focused on the analysis of\nthe properties of stylistic fluctuations in texts via topological analysis of\ncomplex networks and intermittency measurements. The results showed that\ndifferent authors display distinct fluctuation patterns. In particular, it was\nfound that it is possible to identify the authorship of books using the\nintermittency of specific words. Taken together, the results described here\nsuggest that the patterns found in stylistic fluctuations could be used to\nanalyze other related complex systems. Furthermore, the discovery of novel\npatterns related to textual stylistic fluctuations indicates that these\npatterns could be useful to improve the state of the art of many\nstylistic-based natural language processing tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.01271v2", 
    "title": "INRIASAC: Simple Hypernym Extraction Methods", 
    "arxiv-id": "1502.01271v2", 
    "author": "Gregory Grefenstette", 
    "publish": "2015-02-04T17:53:01Z", 
    "summary": "Given a set of terms from a given domain, how can we structure them into a\ntaxonomy without manual intervention? This is the task 17 of SemEval 2015. Here\nwe present our simple taxonomy structuring techniques which, despite their\nsimplicity, ranked first in this 2015 benchmark. We use large quantities of\ntext (English Wikipedia) and simple heuristics such as term overlap and\ndocument and sentence co-occurrence to produce hypernym lists. We describe\nthese techniques and pre-sent an initial evaluation of results."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.01446v1", 
    "title": "Beyond Word-based Language Model in Statistical Machine Translation", 
    "arxiv-id": "1502.01446v1", 
    "author": "Chengqing Zong", 
    "publish": "2015-02-05T07:42:18Z", 
    "summary": "Language model is one of the most important modules in statistical machine\ntranslation and currently the word-based language model dominants this\ncommunity. However, many translation models (e.g. phrase-based models) generate\nthe target language sentences by rendering and compositing the phrases rather\nthan the words. Thus, it is much more reasonable to model dependency between\nphrases, but few research work succeed in solving this problem. In this paper,\nwe tackle this problem by designing a novel phrase-based language model which\nattempts to solve three key sub-problems: 1, how to define a phrase in language\nmodel; 2, how to determine the phrase boundary in the large-scale monolingual\ndata in order to enlarge the training set; 3, how to alleviate the data\nsparsity problem due to the huge vocabulary size of phrases. By carefully\nhandling these issues, the extensive experiments on Chinese-to-English\ntranslation show that our phrase-based language model can significantly improve\nthe translation quality by up to +1.47 absolute BLEU score."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.02655v1", 
    "title": "An investigation into language complexity of World-of-Warcraft   game-external texts", 
    "arxiv-id": "1502.02655v1", 
    "author": "Simon \u0160uster", 
    "publish": "2015-02-07T21:59:21Z", 
    "summary": "We present a language complexity analysis of World of Warcraft (WoW)\ncommunity texts, which we compare to texts from a general corpus of web\nEnglish. Results from several complexity types are presented, including lexical\ndiversity, density, readability and syntactic complexity. The language of WoW\ntexts is found to be comparable to the general corpus on some complexity\nmeasures, yet more specialized on other measures. Our findings can be used by\neducators willing to include game-related activities into school curricula."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.03671v2", 
    "title": "Phrase-based Image Captioning", 
    "arxiv-id": "1502.03671v2", 
    "author": "Ronan Collobert", 
    "publish": "2015-02-12T14:17:15Z", 
    "summary": "Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\nin two popular datasets for the task: Flickr30k and the recently proposed\nMicrosoft COCO."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.03752v1", 
    "title": "A new hybrid metric for verifying parallel corpora of Arabic-English", 
    "arxiv-id": "1502.03752v1", 
    "author": "William J. Teahan", 
    "publish": "2015-02-12T17:49:45Z", 
    "summary": "This paper discusses a new metric that has been applied to verify the quality\nin translation between sentence pairs in parallel corpora of Arabic-English.\nThis metric combines two techniques, one based on sentence length and the other\nbased on compression code length. Experiments on sample test parallel\nArabic-English corpora indicate the combination of these two techniques\nimproves accuracy of the identification of satisfactory and unsatisfactory\nsentence pairs compared to sentence length and compression code length alone.\nThe new method proposed in this research is effective at filtering noise and\nreducing mis-translations resulting in greatly improved quality."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2015/03/P03005", 
    "link": "http://arxiv.org/pdf/1502.04174v1", 
    "title": "Probabilistic Models for High-Order Projective Dependency Parsing", 
    "arxiv-id": "1502.04174v1", 
    "author": "Hai Zhao", 
    "publish": "2015-02-14T06:47:34Z", 
    "summary": "This paper presents generalized probabilistic models for high-order\nprojective dependency parsing and an algorithmic framework for learning these\nstatistical models involving dependency trees. Partition functions and\nmarginals for high-order dependency trees can be computed efficiently, by\nadapting our algorithms which extend the inside-outside algorithm to\nhigher-order cases. To show the effectiveness of our algorithms, we perform\nexperiments on three languages---English, Chinese and Czech, using maximum\nconditional likelihood estimation for model training and L-BFGS for parameter\nestimation. Our methods achieve competitive performance for English, and\noutperform all previously reported dependency parsers for Chinese and Czech."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1502.04938v2", 
    "title": "A Survey of Word Reordering in Statistical Machine Translation:   Computational Models and Language Phenomena", 
    "arxiv-id": "1502.04938v2", 
    "author": "Marcello Federico", 
    "publish": "2015-02-17T15:59:09Z", 
    "summary": "Word reordering is one of the most difficult aspects of statistical machine\ntranslation (SMT), and an important factor of its quality and efficiency.\nDespite the vast amount of research published to date, the interest of the\ncommunity in this problem has not decreased, and no single method appears to be\nstrongly dominant across language pairs. Instead, the choice of the optimal\napproach for a new translation task still seems to be mostly driven by\nempirical trials. To orientate the reader in this vast and complex research\narea, we present a comprehensive survey of word reordering viewed as a\nstatistical modeling challenge and as a natural language phenomenon. The survey\ndescribes in detail how word reordering is modeled within different\nstring-based and tree-based SMT frameworks and as a stand-alone task, including\nsystematic overviews of the literature in advanced reordering modeling. We then\nquestion why some approaches are more successful than others in different\nlanguage pairs. We argue that, besides measuring the amount of reordering, it\nis important to understand which kinds of reordering occur in a given language\npair. To this end, we conduct a qualitative analysis of word reordering\nphenomena in a diverse sample of language pairs, based on a large collection of\nlinguistic knowledge. Empirical results in the SMT literature are shown to\nsupport the hypothesis that a few linguistic facts can be very useful to\nanticipate the reordering characteristics of a language pair and to select the\nSMT framework that best suits them."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1502.07038v1", 
    "title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing", 
    "arxiv-id": "1502.07038v1", 
    "author": "James R. Curran", 
    "publish": "2015-02-25T03:27:38Z", 
    "summary": "We develop novel first- and second-order features for dependency parsing\nbased on the Google Syntactic Ngrams corpus, a collection of subtree counts of\nparsed sentences from scanned books. We also extend previous work on surface\n$n$-gram features from Web1T to the Google Books corpus and from first-order to\nsecond-order, comparing and analysing performance over newswire and web\ntreebanks.\n  Surface and syntactic $n$-grams both produce substantial and complementary\ngains in parsing accuracy across domains. Our best system combines the two\nfeature sets, achieving up to 0.8% absolute UAS improvements on newswire and\n1.4% on web text."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1502.07257v2", 
    "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", 
    "arxiv-id": "1502.07257v2", 
    "author": "Dmitry Vetrov", 
    "publish": "2015-02-25T17:15:56Z", 
    "summary": "Recently proposed Skip-gram model is a powerful method for learning\nhigh-dimensional word representations that capture rich semantic relationships\nbetween words. However, Skip-gram as well as most prior work on learning word\nrepresentations does not take into account word ambiguity and maintain only\nsingle representation per word. Although a number of Skip-gram modifications\nwere proposed to overcome this limitation and learn multi-prototype word\nrepresentations, they either require a known number of word meanings or learn\nthem using greedy heuristic approaches. In this paper we propose the Adaptive\nSkip-gram model which is a nonparametric Bayesian extension of Skip-gram\ncapable to automatically learn the required number of representations for all\nwords at desired semantic resolution. We derive efficient online variational\nlearning algorithm for the model and empirically demonstrate its efficiency on\nword-sense induction task."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1502.07504v1", 
    "title": "Rational Kernels for Arabic Stemming and Text Classification", 
    "arxiv-id": "1502.07504v1", 
    "author": "Hadda Cherroun", 
    "publish": "2015-02-26T11:09:59Z", 
    "summary": "In this paper, we address the problems of Arabic Text Classification and\nstemming using Transducers and Rational Kernels. We introduce a new stemming\ntechnique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns\nare modelled using transducers and stemming is done without depending on any\ndictionary. Using transducers for stemming, documents are transformed into\nfinite state transducers. This document representation allows us to use and\nexplore rational kernels as a framework for Arabic Text Classification.\nStemming experiments are conducted on three word collections and classification\nexperiments are done on the Saudi Press Agency dataset. Results show that our\napproach, when compared with other approaches, is promising specially in terms\nof Accuracy, Recall and F1."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1502.07920v1", 
    "title": "Local Translation Prediction with Global Sentence Representation", 
    "arxiv-id": "1502.07920v1", 
    "author": "Jiajun Zhang", 
    "publish": "2015-02-27T14:55:15Z", 
    "summary": "Statistical machine translation models have made great progress in improving\nthe translation quality. However, the existing models predict the target\ntranslation with only the source- and target-side local context information. In\npractice, distinguishing good translations from bad ones does not only depend\non the local features, but also rely on the global sentence-level information.\nIn this paper, we explore the source-side global sentence-level features for\ntarget-side local translation prediction. We propose a novel\nbilingually-constrained chunk-based convolutional neural network to learn\nsentence semantic representations. With the sentence-level feature\nrepresentation, we further design a feed-forward neural network to better\npredict translations using both local and global information. The large-scale\nexperiments show that our method can obtain substantial improvements in\ntranslation quality over the strong baseline: the hierarchical phrase-based\ntranslation model augmented with the neural network joint model."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.00030v1", 
    "title": "Parsing as Reduction", 
    "arxiv-id": "1503.00030v1", 
    "author": "Andr\u00e9 F. T. Martins", 
    "publish": "2015-02-27T22:52:37Z", 
    "summary": "We reduce phrase-representation parsing to dependency parsing. Our reduction\nis grounded on a new intermediate representation, \"head-ordered dependency\ntrees\", shown to be isomorphic to constituent trees. By encoding order\ninformation in the dependency labels, we show that any off-the-shelf, trainable\ndependency parser can be used to produce constituents. When this parser is\nnon-projective, we can perform discontinuous parsing in a very natural manner.\nDespite the simplicity of our approach, experiments show that the resulting\nparsers are on par with strong baselines, such as the Berkeley parser for\nEnglish and the best single system in the SPMRL-2014 shared task. Results are\nparticularly striking for discontinuous parsing of German, where we surpass the\ncurrent state of the art by a wide margin."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.00095v3", 
    "title": "Task-Oriented Learning of Word Embeddings for Semantic Relation   Classification", 
    "arxiv-id": "1503.00095v3", 
    "author": "Yoshimasa Tsuruoka", 
    "publish": "2015-02-28T07:59:59Z", 
    "summary": "We present a novel learning method for word embeddings designed for relation\nclassification. Our word embeddings are trained by predicting words between\nnoun pairs using lexical relation-specific features on a large unlabeled\ncorpus. This allows us to explicitly incorporate relation-specific information\ninto the word embeddings. The learned word embeddings are then used to\nconstruct feature vectors for a relation classification model. On a\nwell-established semantic relation classification task, our method\nsignificantly outperforms a baseline based on a previously introduced word\nembedding method, and compares favorably to previous state-of-the-art models\nthat use syntactic information or manually constructed external resources."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.00168v1", 
    "title": "The NLP Engine: A Universal Turing Machine for NLP", 
    "arxiv-id": "1503.00168v1", 
    "author": "Eduard Hovy", 
    "publish": "2015-02-28T19:46:50Z", 
    "summary": "It is commonly accepted that machine translation is a more complex task than\npart of speech tagging. But how much more complex? In this paper we make an\nattempt to develop a general framework and methodology for computing the\ninformational and/or processing complexity of NLP applications and tasks. We\ndefine a universal framework akin to a Turning Machine that attempts to fit\n(most) NLP tasks into one paradigm. We calculate the complexities of various\nNLP tasks using measures of Shannon Entropy, and compare `simple' ones such as\npart of speech tagging to `complex' ones such as machine translation. This\npaper provides a first, though far from perfect, attempt to quantify NLP tasks\nunder a uniform paradigm. We point out current deficiencies and suggest some\navenues for fruitful research."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.01655v2", 
    "title": "Studying the Wikipedia Hyperlink Graph for Relatedness and   Disambiguation", 
    "arxiv-id": "1503.01655v2", 
    "author": "Aitor Soroa", 
    "publish": "2015-03-05T15:08:21Z", 
    "summary": "Hyperlinks and other relations in Wikipedia are a extraordinary resource\nwhich is still not fully understood. In this paper we study the different types\nof links in Wikipedia, and contrast the use of the full graph with respect to\njust direct links. We apply a well-known random walk algorithm on two tasks,\nword relatedness and named-entity disambiguation. We show that using the full\ngraph is more effective than just direct links by a large margin, that\nnon-reciprocal links harm performance, and that there is no benefit from\ncategories and infoboxes, with coherent results on both tasks. We set new\nstate-of-the-art figures for systems based on Wikipedia links, comparable to\nsystems exploiting several information sources and/or supervised machine\nlearning. Our approach is open source, with instruction to reproduce results,\nand amenable to be integrated with complementary text-based methods."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.02335v1", 
    "title": "An Unsupervised Method for Uncovering Morphological Chains", 
    "arxiv-id": "1503.02335v1", 
    "author": "Tommi Jaakkola", 
    "publish": "2015-03-08T22:18:30Z", 
    "summary": "Most state-of-the-art systems today produce morphological analysis based only\non orthographic patterns. In contrast, we propose a model for unsupervised\nmorphological analysis that integrates orthographic and semantic views of\nwords. We model word formation in terms of morphological chains, from base\nwords to the observed words, breaking the chains into parent-child relations.\nWe use log-linear models with morpheme and word-level features to predict\npossible parents, including their modifications, for each word. The limited set\nof candidate parents for each word render contrastive estimation feasible. Our\nmodel consistently matches or outperforms five state-of-the-art systems on\nArabic, English and Turkish."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.03535v2", 
    "title": "On Using Monolingual Corpora in Neural Machine Translation", 
    "arxiv-id": "1503.03535v2", 
    "author": "Yoshua Bengio", 
    "publish": "2015-03-11T23:50:04Z", 
    "summary": "Recent work on end-to-end neural network-based architectures for machine\ntranslation has shown promising results for En-Fr and En-De translation.\nArguably, one of the major factors behind this success has been the\navailability of high quality parallel corpora. In this work, we investigate how\nto leverage abundant monolingual corpora for neural machine translation.\nCompared to a phrase-based and hierarchical baseline, we obtain up to $1.96$\nBLEU improvement on the low-resource language pair Turkish-English, and $1.59$\nBLEU on the focused domain task of Chinese-English chat messages. While our\nmethod was initially targeted toward such tasks with less parallel data, we\nshow that it also extends to high resource languages such as Cs-En and De-En\nwhere we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural\nmachine translation baselines, respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.03989v1", 
    "title": "An implementation of Apertium based Assamese morphological analyzer", 
    "arxiv-id": "1503.03989v1", 
    "author": "Shikhar Kumar Sarma", 
    "publish": "2015-03-13T09:03:21Z", 
    "summary": "Morphological Analysis is an important branch of linguistics for any Natural\nLanguage Processing Technology. Morphology studies the word structure and\nformation of word of a language. In current scenario of NLP research,\nmorphological analysis techniques have become more popular day by day. For\nprocessing any language, morphology of the word should be first analyzed.\nAssamese language contains very complex morphological structure. In our work we\nhave used Apertium based Finite-State-Transducers for developing morphological\nanalyzer for Assamese Language with some limited domain and we get 72.7%\naccuracy"
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.05034v2", 
    "title": "$gen$CNN: A Convolutional Architecture for Word Sequence Prediction", 
    "arxiv-id": "1503.05034v2", 
    "author": "Qun Liu", 
    "publish": "2015-03-17T13:26:08Z", 
    "summary": "We propose a novel convolutional architecture, named $gen$CNN, for word\nsequence prediction. Different from previous work on neural network-based\nlanguage modeling and generation (e.g., RNN or LSTM), we choose not to greedily\nsummarize the history of words as a fixed length vector. Instead, we use a\nconvolutional neural network to predict the next word with the history of words\nof variable length. Also different from the existing feedforward networks for\nlanguage modeling, our model can effectively fuse the local correlation and\nglobal correlation in the word sequence, with a convolution-gating strategy\nspecifically designed for the task. We argue that our model can give adequate\nrepresentation of the history, and therefore can naturally exploit both the\nshort and long range dependencies. Our model is fast, easy to train, and\nreadily parallelized. Our extensive experiments on text generation and $n$-best\nre-ranking in machine translation show that $gen$CNN outperforms the\nstate-of-the-arts with big margins."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.05123v1", 
    "title": "Prediction Using Note Text: Synthetic Feature Creation with word2vec", 
    "arxiv-id": "1503.05123v1", 
    "author": "Yelena Rozenfeld", 
    "publish": "2015-03-17T17:04:27Z", 
    "summary": "word2vec affords a simple yet powerful approach of extracting quantitative\nvariables from unstructured textual data. Over half of healthcare data is\nunstructured and therefore hard to model without involved expertise in data\nengineering and natural language processing. word2vec can serve as a bridge to\nquickly gather intelligence from such data sources.\n  In this study, we ran 650 megabytes of unstructured, medical chart notes from\nthe Providence Health & Services electronic medical record through word2vec. We\nused two different approaches in creating predictive variables and tested them\non the risk of readmission for patients with COPD (Chronic Obstructive Lung\nDisease). As a comparative benchmark, we ran the same test using the LACE risk\nmodel (a single score based on length of stay, acuity, comorbid conditions, and\nemergency department visits).\n  Using only free text and mathematical might, we found word2vec comparable to\nLACE in predicting the risk of readmission of COPD patients."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.05626v1", 
    "title": "Phrase database Approach to structural and semantic disambiguation in   English-Korean Machine Translation", 
    "arxiv-id": "1503.05626v1", 
    "author": "Myong-Chol Pak", 
    "publish": "2015-03-19T01:37:40Z", 
    "summary": "In machine translation it is common phenomenon that machine-readable\ndictionaries and standard parsing rules are not enough to ensure accuracy in\nparsing and translating English phrases into Korean language, which is revealed\nin misleading translation results due to consequent structural and semantic\nambiguities. This paper aims to suggest a solution to structural and semantic\nambiguities due to the idiomaticity and non-grammaticalness of phrases commonly\nused in English language by applying bilingual phrase database in\nEnglish-Korean Machine Translation (EKMT). This paper firstly clarifies what\nthe phrase unit in EKMT is based on the definition of the English phrase,\nsecondly clarifies what kind of language unit can be the target of the phrase\ndatabase for EKMT, thirdly suggests a way to build the phrase database by\npresenting the format of the phrase database with examples, and finally\ndiscusses briefly the method to apply this bilingual phrase database to the\nEKMT for structural and semantic disambiguation."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.05907v1", 
    "title": "Syntagma Lexical Database", 
    "arxiv-id": "1503.05907v1", 
    "author": "Daniel Christen", 
    "publish": "2015-03-19T19:45:24Z", 
    "summary": "This paper discusses the structure of Syntagma's Lexical Database (focused on\nItalian). The basic database consists in four tables. Table Forms contains word\ninflections, used by the POS-tagger for the identification of input-words.\nForms is related to Lemma. Table Lemma stores all kinds of grammatical features\nof words, word-level semantic data and restrictions. In the table Meanings\nmeaning-related data are stored: definition, examples, domain, and semantic\ninformation. Table Valency contains the argument structure of each meaning,\nwith syntactic and semantic features for each argument. The extended version of\nSLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of\nother languages."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.06151v1", 
    "title": "On measuring linguistic intelligence", 
    "arxiv-id": "1503.06151v1", 
    "author": "Maxim Litvak", 
    "publish": "2015-03-20T16:41:05Z", 
    "summary": "This work addresses the problem of measuring how many languages a person\n\"effectively\" speaks given that some of the languages are close to each other.\nIn other words, to assign a meaningful number to her language portfolio.\nIntuition says that someone who speaks fluently Spanish and Portuguese is\nlinguistically less proficient compared to someone who speaks fluently Spanish\nand Chinese since it takes more effort for a native Spanish speaker to learn\nChinese than Portuguese. As the number of languages grows and their proficiency\nlevels vary, it gets even more complicated to assign a score to a language\nportfolio. In this article we propose such a measure (\"linguistic quotient\" -\nLQ) that can account for these effects.\n  We define the properties that such a measure should have. They are based on\nthe idea of coherent risk measures from the mathematical finance. Having laid\ndown the foundation, we propose one such a measure together with the algorithm\nthat works on languages classification tree as input.\n  The algorithm together with the input is available online at lingvometer.com"
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.06450v2", 
    "title": "Multilingual Open Relation Extraction Using Cross-lingual Projection", 
    "arxiv-id": "1503.06450v2", 
    "author": "Shankar Kumar", 
    "publish": "2015-03-22T18:05:08Z", 
    "summary": "Open domain relation extraction systems identify relation and argument\nphrases in a sentence without relying on any underlying schema. However,\ncurrent state-of-the-art relation extraction systems are available only for\nEnglish because of their heavy reliance on linguistic tools such as\npart-of-speech taggers and dependency parsers. We present a cross-lingual\nannotation projection method for language independent relation extraction. We\nevaluate our method on a manually annotated test set and present results on\nthree typologically different languages. We release these manual annotations\nand extracted relations in 61 languages from Wikipedia."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.06733v2", 
    "title": "Yara Parser: A Fast and Accurate Dependency Parser", 
    "arxiv-id": "1503.06733v2", 
    "author": "Joel Tetreault", 
    "publish": "2015-03-23T17:20:54Z", 
    "summary": "Dependency parsers are among the most crucial tools in natural language\nprocessing as they have many important applications in downstream tasks such as\ninformation retrieval, machine translation and knowledge acquisition. We\nintroduce the Yara Parser, a fast and accurate open-source dependency parser\nbased on the arc-eager algorithm and beam search. It achieves an unlabeled\naccuracy of 93.32 on the standard WSJ test set which ranks it among the top\ndependency parsers. At its fastest, Yara can parse about 4000 sentences per\nsecond when in greedy mode (1 beam). When optimizing for accuracy (using 64\nbeams and Brown cluster features), Yara can parse 45 sentences per second. The\nparser can be trained on any syntactic dependency treebank and different\noptions are provided in order to make it more flexible and tunable for specific\ntasks. It is released with the Apache version 2.0 license and can be used for\nboth commercial and academic purposes. The parser can be found at\nhttps://github.com/yahoo/YaraParser."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.06760v1", 
    "title": "Unsupervised POS Induction with Word Embeddings", 
    "arxiv-id": "1503.06760v1", 
    "author": "Lori Levin", 
    "publish": "2015-03-23T18:32:56Z", 
    "summary": "Unsupervised word embeddings have been shown to be valuable as features in\nsupervised learning problems; however, their role in unsupervised problems has\nbeen less thoroughly explored. In this paper, we show that embeddings can\nlikewise add value to the problem of unsupervised POS induction. In two\nrepresentative models of POS induction, we replace multinomial distributions\nover the vocabulary with multivariate Gaussian distributions over word\nembeddings and observe consistent improvements in eight languages. We also\nanalyze the effect of various choices while inducing word embeddings on\n\"downstream\" POS induction results."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.07283v1", 
    "title": "Morphological Analyzer and Generator for Russian and Ukrainian Languages", 
    "arxiv-id": "1503.07283v1", 
    "author": "Mikhail Korobov", 
    "publish": "2015-03-25T05:28:50Z", 
    "summary": "pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian\nlanguages. It uses large efficiently encoded lexi- cons built from OpenCorpora\nand LanguageTool data. A set of linguistically motivated rules is developed to\nenable morphological analysis and generation of out-of-vocabulary words\nobserved in real-world documents. For Russian pymorphy2 provides\nstate-of-the-arts morphological analysis quality. The analyzer is implemented\nin Python programming language with optional C++ extensions. Emphasis is put on\nease of use, documentation and extensibility. The package is distributed under\na permissive open-source license, encouraging its use in both academic and\ncommercial setting."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.07613v1", 
    "title": "Unsupervised authorship attribution", 
    "arxiv-id": "1503.07613v1", 
    "author": "Emil Lunde", 
    "publish": "2015-03-26T04:02:26Z", 
    "summary": "We describe a technique for attributing parts of a written text to a set of\nunknown authors. Nothing is assumed to be known a priori about the writing\nstyles of potential authors. We use multiple independent clusterings of an\ninput text to identify parts that are similar and dissimilar to one another. We\ndescribe algorithms necessary to combine the multiple clusterings into a\nmeaningful output. We show results of the application of the technique on texts\nhaving multiple writing styles."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.08167v2", 
    "title": "Normalization of Non-Standard Words in Croatian Texts", 
    "arxiv-id": "1503.08167v2", 
    "author": "Sanda Martin\u010di\u0107-Ip\u0161i\u0107", 
    "publish": "2015-03-27T17:57:00Z", 
    "summary": "This paper presents text normalization which is an integral part of any\ntext-to-speech synthesis system. Text normalization is a set of methods with a\ntask to write non-standard words, like numbers, dates, times, abbreviations,\nacronyms and the most common symbols, in their full expanded form are\npresented. The whole taxonomy for classification of non-standard words in\nCroatian language together with rule-based normalization methods combined with\na lookup dictionary are proposed. Achieved token rate for normalization of\nCroatian texts is 95%, where 80% of expanded words are in correct morphological\nform."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1503.09144v1", 
    "title": "Towards Using Machine Translation Techniques to Induce Multilingual   Lexica of Discourse Markers", 
    "arxiv-id": "1503.09144v1", 
    "author": "Ana Isabel Mata", 
    "publish": "2015-03-31T17:56:07Z", 
    "summary": "Discourse markers are universal linguistic events subject to language\nvariation. Although an extensive literature has already reported language\nspecific traits of these events, little has been said on their cross-language\nbehavior and on building an inventory of multilingual lexica of discourse\nmarkers. This work describes new methods and approaches for the description,\nclassification, and annotation of discourse markers in the specific domain of\nthe Europarl corpus. The study of discourse markers in the context of\ntranslation is crucial due to the idiomatic nature of these structures.\nMultilingual lexica together with the functional analysis of such structures\nare useful tools for the hard task of translating discourse markers into\npossible equivalents from one language to another. Using Daniel Marcu's\nvalidated discourse markers for English, extracted from the Brown Corpus, our\npurpose is to build multilingual lexica of discourse markers for other\nlanguages, based on machine translation techniques. The major assumption in\nthis study is that the usage of a discourse marker is independent of the\nlanguage, i.e., the rhetorical function of a discourse marker in a sentence in\none language is equivalent to the rhetorical function of the same discourse\nmarker in another language."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1504.00548v4", 
    "title": "Learning to Understand Phrases by Embedding the Dictionary", 
    "arxiv-id": "1504.00548v4", 
    "author": "Yoshua Bengio", 
    "publish": "2015-04-02T13:30:27Z", 
    "summary": "Distributional models that learn rich semantic word representations are a\nsuccess story of recent NLP research. However, developing models that learn\nuseful representations of phrases and sentences has proved far harder. We\npropose using the definitions found in everyday dictionaries as a means of\nbridging this gap between lexical and phrasal semantics. Neural language\nembedding models can be effectively trained to map dictionary definitions\n(phrases) to (lexical) representations of the words defined by those\ndefinitions. We present two applications of these architectures: \"reverse\ndictionaries\" that return the name of a concept given a definition or\ndescription and general-knowledge crossword question answerers. On both tasks,\nneural language embedding models trained on definitions from a handful of\nfreely-available lexical resources perform as well or better than existing\ncommercial systems that rely on significant task-specific engineering. The\nresults highlight the effectiveness of both neural embedding architectures and\ndefinition-based training for developing models that understand phrases and\nsentences."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1504.01182v1", 
    "title": "Bengali to Assamese Statistical Machine Translation using Moses (Corpus   Based)", 
    "arxiv-id": "1504.01182v1", 
    "author": "Baharul Islam", 
    "publish": "2015-04-06T01:18:24Z", 
    "summary": "Machine dialect interpretation assumes a real part in encouraging man-machine\ncorrespondence and in addition men-men correspondence in Natural Language\nProcessing (NLP). Machine Translation (MT) alludes to utilizing machine to\nchange one dialect to an alternate. Statistical Machine Translation is a type\nof MT consisting of Language Model (LM), Translation Model (TM) and decoder. In\nthis paper, Bengali to Assamese Statistical Machine Translation Model has been\ncreated by utilizing Moses. Other translation tools like IRSTLM for Language\nModel and GIZA-PP-V1.0.7 for Translation model are utilized within this\nframework which is accessible in Linux situations. The purpose of the LM is to\nencourage fluent output and the purpose of TM is to encourage similarity\nbetween input and output, the decoder increases the probability of translated\ntext in target language. A parallel corpus of 17100 sentences in Bengali and\nAssamese has been utilized for preparing within this framework. Measurable MT\nprocedures have not so far been generally investigated for Indian dialects. It\nmight be intriguing to discover to what degree these models can help the\nimmense continuous MT deliberations in the nation."
},{
    "category": "cs.CL", 
    "doi": "10.1162/COLI_a_00245", 
    "link": "http://arxiv.org/pdf/1504.01427v1", 
    "title": "A Metric to Classify Style of Spoken Speech", 
    "arxiv-id": "1504.01427v1", 
    "author": "P. V. S. Rao", 
    "publish": "2015-04-06T22:00:12Z", 
    "summary": "The ability to classify spoken speech based on the style of speaking is an\nimportant problem. With the advent of BPO's in recent times, specifically those\nthat cater to a population other than the local population, it has become\nnecessary for BPO's to identify people with certain style of speaking\n(American, British etc). Today BPO's employ accent analysts to identify people\nhaving the required style of speaking. This process while involving human bias,\nit is becoming increasingly infeasible because of the high attrition rate in\nthe BPO industry. In this paper, we propose a new metric, which robustly and\naccurately helps classify spoken speech based on the style of speaking. The\nrole of the proposed metric is substantiated by using it to classify real\nspeech data collected from over seventy different people working in a BPO. We\ncompare the performance of the metric against human experts who independently\ncarried out the classification process. Experimental results show that the\nperformance of the system using the novel metric performs better than two\ndifferent human expert."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-90-481-3658-2_18", 
    "link": "http://arxiv.org/pdf/1504.01496v1", 
    "title": "Voice based self help System: User Experience Vs Accuracy", 
    "arxiv-id": "1504.01496v1", 
    "author": "Sunil Kumar Kopparapu", 
    "publish": "2015-04-07T07:02:38Z", 
    "summary": "In general, self help systems are being increasingly deployed by service\nbased industries because they are capable of delivering better customer service\nand increasingly the switch is to voice based self help systems because they\nprovide a natural interface for a human to interact with a machine. A speech\nbased self help system ideally needs a speech recognition engine to convert\nspoken speech to text and in addition a language processing engine to take care\nof any misrecognitions by the speech recognition engine. Any off-the-shelf\nspeech recognition engine is generally a combination of acoustic processing and\nspeech grammar. While this is the norm, we believe that ideally a speech\nrecognition application should have in addition to a speech recognition engine\na separate language processing engine to give the system better performance. In\nthis paper, we discuss ways in which the speech recognition engine and the\nlanguage processing engine can be combined to give a better user experience."
},{
    "category": "cs.CL", 
    "doi": "10.1007/978-90-481-3658-2_18", 
    "link": "http://arxiv.org/pdf/1504.01683v4", 
    "title": "Jointly Embedding Relations and Mentions for Knowledge Population", 
    "arxiv-id": "1504.01683v4", 
    "author": "Ralph Grishman", 
    "publish": "2015-04-07T17:44:30Z", 
    "summary": "This paper contributes a joint embedding model for predicting relations\nbetween a pair of entities in the scenario of relation inference. It differs\nfrom most stand-alone approaches which separately operate on either knowledge\nbases or free texts. The proposed model simultaneously learns low-dimensional\nvector representations for both triplets in knowledge repositories and the\nmentions of relations in free texts, so that we can leverage the evidence both\nresources to make more accurate predictions. We use NELL to evaluate the\nperformance of our approach, compared with cutting-edge methods. Results of\nextensive experiments show that our model achieves significant improvement on\nrelation extraction."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.02162v2", 
    "title": "Concentric network symmetry grasps authors' styles in word adjacency   networks", 
    "arxiv-id": "1504.02162v2", 
    "author": "Luciano da F. Costa", 
    "publish": "2015-04-09T00:49:36Z", 
    "summary": "Several characteristics of written texts have been inferred from statistical\nanalysis derived from networked models. Even though many network measurements\nhave been adapted to study textual properties at several levels of complexity,\nsome textual aspects have been disregarded. In this paper, we study the\nsymmetry of word adjacency networks, a well-known representation of text as a\ngraph. A statistical analysis of the symmetry distribution performed in several\nnovels showed that most of the words do not display symmetric patterns of\nconnectivity. More specifically, the merged symmetry displayed a distribution\nsimilar to the ubiquitous power-law distribution. Our experiments also revealed\nthat the studied metrics do not correlate with other traditional network\nmeasurements, such as the degree or betweenness centrality. The effectiveness\nof the symmetry measurements was verified in the authorship attribution task.\nInterestingly, we found that specific authors prefer particular types of\nsymmetric motifs. As a consequence, the authorship of books could be accurately\nidentified in 82.5% of the cases, in a dataset comprising books written by 8\nauthors. Because the proposed measurements for text analysis are complementary\nto the traditional approach, they can be used to improve the characterization\nof text networks, which might be useful for related applications, such as those\nrelying on the identification of topical words and information retrieval."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.02490v1", 
    "title": "Leveraging Twitter for Low-Resource Conversational Speech Language   Modeling", 
    "arxiv-id": "1504.02490v1", 
    "author": "Mari Ostendorf", 
    "publish": "2015-04-09T20:21:32Z", 
    "summary": "In applications involving conversational speech, data sparsity is a limiting\nfactor in building a better language model. We propose a simple,\nlanguage-independent method to quickly harvest large amounts of data from\nTwitter to supplement a smaller training set that is more closely matched to\nthe domain. The techniques lead to a significant reduction in perplexity on\nfour low-resource languages even though the presence on Twitter of these\nlanguages is relatively small. We also find that the Twitter text is more\nuseful for learning word classes than the in-domain text and that use of these\nword classes leads to further reductions in perplexity. Additionally, we\nintroduce a method of using social and textual information to prioritize the\ndownload queue during the Twitter crawling. This maximizes the amount of useful\ndata that can be collected, impacting both perplexity and vocabulary coverage."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.04751v1", 
    "title": "A Knowledge-poor Pronoun Resolution System for Turkish", 
    "arxiv-id": "1504.04751v1", 
    "author": "Meltem Turhan Y\u00f6ndem", 
    "publish": "2015-04-18T18:34:19Z", 
    "summary": "A pronoun resolution system which requires limited syntactic knowledge to\nidentify the antecedents of personal and reflexive pronouns in Turkish is\npresented. As in its counterparts for languages like English, Spanish and\nFrench, the core of the system is the constraints and preferences determined\nempirically. In the evaluation phase, it performed considerably better than the\nbaseline algorithm used for comparison. The system is significant for its being\nthe first fully specified knowledge-poor computational framework for pronoun\nresolution in Turkish where Turkish possesses different structural properties\nfrom the languages for which knowledge-poor systems had been developed."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.05319v2", 
    "title": "Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word:   The Impact of Word Representation on Sequence Labelling Tasks", 
    "arxiv-id": "1504.05319v2", 
    "author": "Timothy Baldwin", 
    "publish": "2015-04-21T06:58:26Z", 
    "summary": "Word embeddings -- distributed word representations that can be learned from\nunlabelled data -- have been shown to have high utility in many natural\nlanguage processing applications. In this paper, we perform an extrinsic\nevaluation of five popular word embedding methods in the context of four\nsequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE\nidentification. A particular focus of the paper is analysing the effects of\ntask-based updating of word representations. We show that when using word\nembeddings as features, as few as several hundred training instances are\nsufficient to achieve competitive results, and that word embeddings lead to\nimprovements over OOV words and out of domain. Perhaps more surprisingly, our\nresults indicate there is little difference between the different word\nembedding methods, and that simple Brown clusters are often competitive with\nword embeddings across all tasks we consider."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.06391v1", 
    "title": "On the Stability of Online Language Features: How Much Text do you Need   to know a Person?", 
    "arxiv-id": "1504.06391v1", 
    "author": "Eben M. Haber", 
    "publish": "2015-04-24T04:45:55Z", 
    "summary": "In recent years, numerous studies have inferred personality and other traits\nfrom people's online writing. While these studies are encouraging, more\ninformation is needed in order to use these techniques with confidence. How do\nlinguistic features vary across different online media, and how much text is\nrequired to have a representative sample for a person? In this paper, we\nexamine several large sets of online, user-generated text, drawn from Twitter,\nemail, blogs, and online discussion forums. We examine and compare\npopulation-wide results for the linguistic measure LIWC, and the inferred\ntraits of Big5 Personality and Basic Human Values. We also empirically measure\nthe stability of these traits across different sized samples for each\nindividual. Our results highlight the importance of tuning models to each\nonline medium, and include guidelines for the minimum amount of text required\nfor a representative result."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.07678v1", 
    "title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity   Disambiguation", 
    "arxiv-id": "1504.07678v1", 
    "author": "Heng Ji", 
    "publish": "2015-04-28T22:47:25Z", 
    "summary": "Entity Disambiguation aims to link mentions of ambiguous entities to a\nknowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for\nthis task based on the assumption that information from the same semantic\ncontext tends to belong to the same topic. This paper presents a novel deep\nsemantic relatedness model (DSRM) based on deep neural networks (DNN) and\nsemantic knowledge graphs (KGs) to measure entity semantic relatedness for\ntopical coherence modeling. The DSRM is directly trained on large-scale KGs and\nit maps heterogeneous types of knowledge of an entity from KGs to numerical\nfeature vectors in a latent space such that the distance between two\nsemantically-related entities is minimized. Compared with the state-of-the-art\nrelatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains\n19.4% and 24.5% reductions in entity disambiguation errors on two publicly\navailable datasets respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.08102v1", 
    "title": "Detecting and ordering adjectival scalemates", 
    "arxiv-id": "1504.08102v1", 
    "author": "Emiel van Miltenburg", 
    "publish": "2015-04-30T07:27:56Z", 
    "summary": "This paper presents a pattern-based method that can be used to infer\nadjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically,\nthe proposed method uses lexical patterns to automatically identify and order\npairs of scalemates, followed by a filtering phase in which unrelated pairs are\ndiscarded. For the filtering phase, several different similarity measures are\nimplemented and compared. The model presented in this paper is evaluated using\nthe current standard, along with a novel evaluation set, and shown to be at\nleast as good as the current state-of-the-art."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1504.08183v1", 
    "title": "Texts in, meaning out: neural language models in semantic similarity   task for Russian", 
    "arxiv-id": "1504.08183v1", 
    "author": "Igor Andreev", 
    "publish": "2015-04-30T12:03:10Z", 
    "summary": "Distributed vector representations for natural language vocabulary get a lot\nof attention in contemporary computational linguistics. This paper summarizes\nthe experience of applying neural network language models to the task of\ncalculating semantic similarity for Russian. The experiments were performed in\nthe course of Russian Semantic Similarity Evaluation track, where our models\ntook from the 2nd to the 5th position, depending on the task.\n  We introduce the tools and corpora used, comment on the nature of the shared\ntask and describe the achieved results. It was found out that Continuous\nSkip-gram and Continuous Bag-of-words models, previously successfully applied\nto English material, can be used for semantic modeling of Russian as well.\nMoreover, we show that texts in Russian National Corpus (RNC) provide an\nexcellent training material for such models, outperforming other, much larger\ncorpora. It is especially true for semantic relatedness tasks (although\nstacking models trained on larger corpora on top of RNC models improves\nperformance even more).\n  High-quality semantic vectors learned in such a way can be used in a variety\nof linguistic tasks and promise an exciting field for further study."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1505.00161v1", 
    "title": "Embedding Semantic Relations into Word Representations", 
    "arxiv-id": "1505.00161v1", 
    "author": "Ken-ichi Kawarabayashi", 
    "publish": "2015-05-01T11:43:34Z", 
    "summary": "Learning representations for semantic relations is important for various\ntasks such as analogy detection, relational search, and relation\nclassification. Although there have been several proposals for learning\nrepresentations for individual words, learning word representations that\nexplicitly capture the semantic relations between words remains under\ndeveloped. We propose an unsupervised method for learning vector\nrepresentations for words such that the learnt representations are sensitive to\nthe semantic relations that exist between two words. First, we extract lexical\npatterns from the co-occurrence contexts of two words in a corpus to represent\nthe semantic relations that exist between those two words. Second, we represent\na lexical pattern as the weighted sum of the representations of the words that\nco-occur with that lexical pattern. Third, we train a binary classifier to\ndetect relationally similar vs. non-similar lexical pattern pairs. The proposed\nmethod is unsupervised in the sense that the lexical pattern pairs we use as\ntrain data are automatically sampled from a corpus, without requiring any\nmanual intervention. Our proposed method statistically significantly\noutperforms the current state-of-the-art word representations on three\nbenchmark datasets for proportional analogy detection, demonstrating its\nability to accurately capture the semantic relations among words."
},{
    "category": "cs.CL", 
    "doi": "10.1209/0295-5075/110/68001", 
    "link": "http://arxiv.org/pdf/1505.02425v1", 
    "title": "Fast Rhetorical Structure Theory Discourse Parsing", 
    "arxiv-id": "1505.02425v1", 
    "author": "Kenji Sagae", 
    "publish": "2015-05-10T19:26:31Z", 
    "summary": "In recent years, There has been a variety of research on discourse parsing,\nparticularly RST discourse parsing. Most of the recent work on RST parsing has\nfocused on implementing new types of features or learning algorithms in order\nto improve accuracy, with relatively little focus on efficiency, robustness, or\npractical use. Also, most implementations are not widely available. Here, we\ndescribe an RST segmentation and parsing system that adapts models and feature\nsets from various previous work, as described below. Its accuracy is near\nstate-of-the-art, and it was developed to be fast, robust, and practical. For\nexample, it can process short documents such as news articles or essays in less\nthan a second."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2015.4208", 
    "link": "http://arxiv.org/pdf/1505.03081v1", 
    "title": "Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and   Instance Messages", 
    "arxiv-id": "1505.03081v1", 
    "author": "Mervat Gheith", 
    "publish": "2015-05-12T16:33:03Z", 
    "summary": "Text segmentation task is an essential processing task for many of Natural\nLanguage Processing (NLP) such as text summarization, text translation,\ndialogue language understanding, among others. Turns segmentation considered\nthe key player in dialogue understanding task for building automatic\nHuman-Computer systems. In this paper, we introduce a novel approach to turn\nsegmentation into utterances for Egyptian spontaneous dialogues and Instance\nMessages (IM) using Machine Learning (ML) approach as a part of automatic\nunderstanding Egyptian spontaneous dialogues and IM task. Due to the lack of\nEgyptian dialect dialogue corpus the system evaluated by our corpus includes\n3001 turns, which are collected, segmented, and annotated manually from\nEgyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of\n95.98%."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2015.4206", 
    "link": "http://arxiv.org/pdf/1505.03084v1", 
    "title": "A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and   Instant Message", 
    "arxiv-id": "1505.03084v1", 
    "author": "Mervat Gheith", 
    "publish": "2015-05-12T16:38:39Z", 
    "summary": "Building dialogues systems interaction has recently gained considerable\nattention, but most of the resources and systems built so far are tailored to\nEnglish and other Indo-European languages. The need for designing systems for\nother languages is increasing such as Arabic language. For this reasons, there\nare more interest for Arabic dialogue acts classification task because it a key\nplayer in Arabic language understanding to building this systems. This paper\nsurveys different techniques for dialogue acts classification for Arabic. We\ndescribe the main existing techniques for utterances segmentations and\nclassification, annotation schemas, and test corpora for Arabic dialogues\nunderstanding that have introduced in the literature"
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICACSIS.2013.6761575", 
    "link": "http://arxiv.org/pdf/1505.03085v1", 
    "title": "Indonesian Social Media Sentiment Analysis With Sarcasm Detection", 
    "arxiv-id": "1505.03085v1", 
    "author": "Ayu Purwarianti", 
    "publish": "2015-05-12T16:45:52Z", 
    "summary": "Sarcasm is considered one of the most difficult problem in sentiment\nanalysis. In our ob-servation on Indonesian social media, for cer-tain topics,\npeople tend to criticize something using sarcasm. Here, we proposed two\nadditional features to detect sarcasm after a common sentiment analysis is\nconducted. The features are the negativity information and the number of\ninterjection words. We also employed translated SentiWordNet in the sentiment\nclassification. All the classifications were conducted with machine learning\nalgorithms. The experimental results showed that the additional features are\nquite effective in the sarcasm detection."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2015.4207", 
    "link": "http://arxiv.org/pdf/1505.03105v1", 
    "title": "Sentiment Analysis For Modern Standard Arabic And Colloquial", 
    "arxiv-id": "1505.03105v1", 
    "author": "Mervat Gheith", 
    "publish": "2015-05-12T18:10:53Z", 
    "summary": "The rise of social media such as blogs and social networks has fueled\ninterest in sentiment analysis. With the proliferation of reviews, ratings,\nrecommendations and other forms of online expression, online opinion has turned\ninto a kind of virtual currency for businesses looking to market their\nproducts, identify new opportunities and manage their reputations, therefore\nmany are now looking to the field of sentiment analysis. In this paper, we\npresent a feature-based sentence level approach for Arabic sentiment analysis.\nOur approach is using Arabic idioms/saying phrases lexicon as a key importance\nfor improving the detection of the sentiment polarity in Arabic sentences as\nwell as a number of novels and rich set of linguistically motivated features\ncontextual Intensifiers, contextual Shifter and negation handling), syntactic\nfeatures for conflicting phrases which enhance the sentiment classification\naccuracy. Furthermore, we introduce an automatic expandable wide coverage\npolarity lexicon of Arabic sentiment words. The lexicon is built with\ngold-standard sentiment words as a seed which is manually collected and\nannotated and it expands and detects the sentiment orientation automatically of\nnew sentiment words using synset aggregation technique and free online Arabic\nlexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and\nEgyptian dialectal Arabic tweets and microblogs (hotel reservation, product\nreviews, etc.). The experimental results using our resources and techniques\nwith SVM classifier indicate high performance levels, with accuracies of over\n95%."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijnlc.2015.4207", 
    "link": "http://arxiv.org/pdf/1505.03239v1", 
    "title": "Feature selection using Fisher's ratio technique for automatic speech   recognition", 
    "arxiv-id": "1505.03239v1", 
    "author": "Surendra Shetty", 
    "publish": "2015-05-13T04:50:27Z", 
    "summary": "Automatic Speech Recognition involves mainly two steps; feature extraction\nand classification . Mel Frequency Cepstral Coefficient is used as one of the\nprominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC\ncoefficients is used as the feature vector in the classification step. But the\nquestion is whether the same or improved classification accuracy can be\nachieved by using a subset of 12 MFCC as feature vector. In this paper,\nFisher's ratio technique is used for selecting a subset of 12 MFCC coefficients\nthat contribute more in discriminating a pattern. The selected coefficients are\nused in classification with Hidden Markov Model algorithm. The classification\naccuracies that we get by using 12 coefficients and by using the selected\ncoefficients are compared."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.03783v1", 
    "title": "Rank diversity of languages: Generic behavior in computational   linguistics", 
    "arxiv-id": "1505.03783v1", 
    "author": "Sergio S\u00e1nchez", 
    "publish": "2015-05-14T16:21:02Z", 
    "summary": "Statistical studies of languages have focused on the rank-frequency\ndistribution of words. Instead, we introduce here a measure of how word ranks\nchange in time and call this distribution \\emph{rank diversity}. We calculate\nthis diversity for books published in six European languages since 1800, and\nfind that it follows a universal lognormal distribution. Based on the mean and\nstandard deviation associated with the lognormal distribution, we define three\ndifferent word regimes of languages: \"heads\" consist of words which almost do\nnot change their rank in time, \"bodies\" are words of general use, while \"tails\"\nare comprised by context-specific words and vary their rank considerably in\ntime. The heads and bodies reflect the size of language cores identified by\nlinguists for basic communication. We propose a Gaussian random walk model\nwhich reproduces the rank variation of words in time and thus the diversity.\nRank diversity of words can be understood as the result of random variations in\nrank, where the size of the variation depends on the rank itself. We find that\nthe core size is similar for all languages studied."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.04197v1", 
    "title": "Arabic Inquiry-Answer Dialogue Acts Annotation Schema", 
    "arxiv-id": "1505.04197v1", 
    "author": "Mervat Gheith", 
    "publish": "2015-05-15T20:13:16Z", 
    "summary": "We present an annotation schema as part of an effort to create a manually\nannotated corpus for Arabic dialogue language understanding including spoken\ndialogue and written \"chat\" dialogue for inquiry-answer domain. The proposed\nschema handles mainly the request and response acts that occurs frequently in\ninquiry-answer debate conversations expressing request services, suggests, and\noffers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.04342v6", 
    "title": "Sifting Robotic from Organic Text: A Natural Language Approach for   Detecting Automation on Twitter", 
    "arxiv-id": "1505.04342v6", 
    "author": "Peter Sheridan Dodds", 
    "publish": "2015-05-17T01:22:00Z", 
    "summary": "Twitter, a popular social media outlet, has evolved into a vast source of\nlinguistic data, rich with opinion, sentiment, and discussion. Due to the\nincreasing popularity of Twitter, its perceived potential for exerting social\ninfluence has led to the rise of a diverse community of automatons, commonly\nreferred to as bots. These inorganic and semi-organic Twitter entities can\nrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)\nto the malevolent (e.g., spamming messages, advertisements, or radical\nopinions). Existing detection algorithms typically leverage meta-data (time\nbetween tweets, number of followers, etc.) to identify robotic accounts. Here,\nwe present a powerful classification scheme that exclusively uses the natural\nlanguage text from organic users to provide a criterion for identifying\naccounts posting automated messages. Since the classifier operates on text\nalone, it is flexible and may be applied to any textual data beyond the\nTwitter-sphere."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.04420v1", 
    "title": "CCG Parsing and Multiword Expressions", 
    "arxiv-id": "1505.04420v1", 
    "author": "Miryam de Lhoneux", 
    "publish": "2015-05-17T17:26:36Z", 
    "summary": "This thesis presents a study about the integration of information about\nMultiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar\n(CCG). We build on previous work which has shown the benefit of adding\ninformation about MWEs to syntactic parsing by implementing a similar pipeline\nwith CCG parsing. More specifically, we collapse MWEs to one token in training\nand test data in CCGbank, a corpus which contains sentences annotated with CCG\nderivations. Our collapsing algorithm however can only deal with MWEs when they\nform a constituent in the data which is one of the limitations of our approach.\n  We study the effect of collapsing training and test data. A parsing effect\ncan be obtained if collapsed data help the parser in its decisions and a\ntraining effect can be obtained if training on the collapsed data improves\nresults. We also collapse the gold standard and show that our model\nsignificantly outperforms the baseline model on our gold standard, which\nindicates that there is a training effect. We show that the baseline model\nperforms significantly better on our gold standard when the data are collapsed\nbefore parsing than when the data are collapsed after parsing which indicates\nthat there is a parsing effect. We show that these results can lead to improved\nperformance on the non-collapsed standard benchmark although we fail to show\nthat it does so significantly. We conclude that despite the limited settings,\nthere are noticeable improvements from using MWEs in parsing. We discuss ways\nin which the incorporation of MWEs into parsing can be improved and hypothesize\nthat this will lead to more substantial results.\n  We finally show that turning the MWE recognition part of the pipeline into an\nexperimental part is a useful thing to do as we obtain different results with\ndifferent recognizers."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.04891v2", 
    "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of   Knowledge Graph", 
    "arxiv-id": "1505.04891v2", 
    "author": "Tie-Yan Liu", 
    "publish": "2015-05-19T07:08:10Z", 
    "summary": "Word embedding, which refers to low-dimensional dense vector representations\nof natural words, has demonstrated its power in many natural language\nprocessing tasks. However, it may suffer from the inaccurate and incomplete\ninformation contained in the free text corpus as training data. To tackle this\nchallenge, there have been quite a few works that leverage knowledge graphs as\nan additional information source to improve the quality of word embedding.\nAlthough these works have achieved certain success, they have neglected some\nimportant facts about knowledge graphs: (i) many relationships in knowledge\ngraphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many},\nrather than simply \\emph{one-to-one}; (ii) most head entities and tail entities\nin knowledge graphs come from very different semantic spaces. To address these\nissues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet\nmodels the relationships between head and tail entities after transforming them\nwith different low-rank projection matrices. The low-rank projection can allow\nnon \\emph{one-to-one} relationships between entities, while different\nprojection matrices for head and tail entities allow them to originate in\ndifferent semantic spaces. The experimental results demonstrate that ProjectNet\nyields more accurate word embedding than previous works, thus leads to clear\nimprovements in various natural language processing tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.05008v2", 
    "title": "Boosting Named Entity Recognition with Neural Character Embeddings", 
    "arxiv-id": "1505.05008v2", 
    "author": "Victor Guimar\u00e3es", 
    "publish": "2015-05-19T14:21:37Z", 
    "summary": "Most state-of-the-art named entity recognition (NER) systems rely on\nhandcrafted features and on the output of other NLP tasks such as\npart-of-speech (POS) tagging and text chunking. In this work we propose a\nlanguage-independent NER system that uses automatically learned features only.\nOur approach is based on the CharWNN deep neural network, which uses word-level\nand character-level representations (embeddings) to perform sequential\nclassification. We perform an extensive number of experiments using two\nannotated corpora in two different languages: HAREM I corpus, which contains\ntexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in\nSpanish. Our experimental results shade light on the contribution of neural\ncharacter embeddings for NER. Moreover, we demonstrate that the same neural\nnetwork which has been successfully applied to POS tagging can also achieve\nstate-of-the-art results for language-independet NER, using the same\nhyperparameters, and without any handcrafted features. For the HAREM I corpus,\nCharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score\nfor the total scenario (ten NE classes), and by 7.2 points in the F1 for the\nselective scenario (five NE classes)."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.05253v2", 
    "title": "Knowlege Graph Embedding by Flexible Translation", 
    "arxiv-id": "1505.05253v2", 
    "author": "Xiaoyan Zhu", 
    "publish": "2015-05-20T05:57:32Z", 
    "summary": "Knowledge graph embedding refers to projecting entities and relations in\nknowledge graph into continuous vector spaces. State-of-the-art methods, such\nas TransE, TransH, and TransR build embeddings by treating relation as\ntranslation from head entity to tail entity. However, previous models can not\ndeal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or\nlack of scalability and efficiency. Thus, we propose a novel method, flexible\ntranslation, named TransF, to address the above issues. TransF regards relation\nas translation between head entity vector and tail entity vector with flexible\nmagnitude. To evaluate the proposed model, we conduct link prediction and\ntriple classification on benchmark datasets. Experimental results show that our\nmethod remarkably improve the performance compared with several\nstate-of-the-art baselines."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.05841v1", 
    "title": "Translation Memory Retrieval Methods", 
    "arxiv-id": "1505.05841v1", 
    "author": "Benjamin Strauss", 
    "publish": "2015-05-21T18:57:34Z", 
    "summary": "Translation Memory (TM) systems are one of the most widely used translation\ntechnologies. An important part of TM systems is the matching algorithm that\ndetermines what translations get retrieved from the bank of available\ntranslations to assist the human translator. Although detailed accounts of the\nmatching algorithms used in commercial systems can't be found in the\nliterature, it is widely believed that edit distance algorithms are used. This\npaper investigates and evaluates the use of several matching algorithms,\nincluding the edit distance algorithm that is believed to be at the heart of\nmost modern commercial TM systems. This paper presents results showing how well\nvarious matching algorithms correlate with human judgments of helpfulness\n(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm\nbased on weighted n-gram precision that can be adjusted for translator length\npreferences consistently returns translations judged to be most helpful by\ntranslators for multiple domains and language pairs."
},{
    "category": "cs.CL", 
    "doi": "10.1371/journal.pone.0121898", 
    "link": "http://arxiv.org/pdf/1505.05899v1", 
    "title": "The IBM 2015 English Conversational Telephone Speech Recognition System", 
    "arxiv-id": "1505.05899v1", 
    "author": "Michael Picheny", 
    "publish": "2015-05-21T20:49:32Z", 
    "summary": "We describe the latest improvements to the IBM English conversational\ntelephone speech recognition system. Some of the techniques that were found\nbeneficial are: maxout networks with annealed dropout rates; networks with a\nvery large number of outputs trained on 2000 hours of data; joint modeling of\npartially unfolded recurrent neural networks and convolutional nets by\ncombining the bottleneck and output layers and retraining the resulting model;\nand lastly, sophisticated language model rescoring with exponential and neural\nnetwork LMs. These techniques result in an 8.0% word error rate on the\nSwitchboard part of the Hub5-2000 evaluation test set which is 23% relative\nbetter than our previous best published result."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.06228v1", 
    "title": "Keyphrase Based Evaluation of Automatic Text Summarization", 
    "arxiv-id": "1505.06228v1", 
    "author": "Tarek El-Shishtawy", 
    "publish": "2015-05-22T21:12:35Z", 
    "summary": "The development of methods to deal with the informative contents of the text\nunits in the matching process is a major challenge in automatic summary\nevaluation systems that use fixed n-gram matching. The limitation causes\ninaccurate matching between units in a peer and reference summaries. The\npresent study introduces a new Keyphrase based Summary Evaluator KpEval for\nevaluating automatic summaries. The KpEval relies on the keyphrases since they\nconvey the most important concepts of a text. In the evaluation process, the\nkeyphrases are used in their lemma form as the matching text unit. The system\nwas applied to evaluate different summaries of Arabic multi-document data set\npresented at TAC2011. The results showed that the new evaluation technique\ncorrelates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,\nand AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG\nMeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667\nrespectively."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.06816v5", 
    "title": "Representing Meaning with a Combination of Logical and Distributional   Models", 
    "arxiv-id": "1505.06816v5", 
    "author": "Raymond J. Mooney", 
    "publish": "2015-05-26T06:19:18Z", 
    "summary": "NLP tasks differ in the semantic information they require, and at this time\nno single se- mantic representation fulfills all requirements. Logic-based\nrepresentations characterize sentence structure, but do not capture the graded\naspect of meaning. Distributional models give graded similarity ratings for\nwords and phrases, but do not capture sentence structure in the same detail as\nlogic-based approaches. So it has been argued that the two are complementary.\nWe adopt a hybrid approach that combines logic-based and distributional\nsemantics through probabilistic logic inference in Markov Logic Networks\n(MLNs). In this paper, we focus on the three components of a practical system\nintegrating logical and distributional models: 1) Parsing and task\nrepresentation is the logic-based part where input problems are represented in\nprobabilistic logic. This is quite different from representing them in standard\nfirst-order logic. 2) For knowledge base construction we form weighted\ninference rules. We integrate and compare distributional information with other\nsources, notably WordNet and an existing paraphrase collection. In particular,\nwe use our system to evaluate distributional lexical entailment approaches. We\nuse a variant of Robinson resolution to determine the necessary inference\nrules. More sources can easily be added by mapping them to logical rules; our\nsystem learns a resource-specific weight that corrects for scaling differences\nbetween resources. 3) In discussing probabilistic inference, we show how to\nsolve the inference problems efficiently. To evaluate our approach, we use the\ntask of textual entailment (RTE), which can utilize the strengths of both\nlogic-based and distributional representations. In particular we focus on the\nSICK dataset, where we achieve state-of-the-art results."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.07184v1", 
    "title": "Unsupervised Cross-Domain Word Representation Learning", 
    "arxiv-id": "1505.07184v1", 
    "author": "Ken-ichi Kawarabayashi", 
    "publish": "2015-05-27T04:02:56Z", 
    "summary": "Meaning of a word varies from one domain to another. Despite this important\ndomain dependence in word semantics, existing word representation learning\nmethods are bound to a single domain. Given a pair of\n\\emph{source}-\\emph{target} domains, we propose an unsupervised method for\nlearning domain-specific word representations that accurately capture the\ndomain-specific aspects of word semantics. First, we select a subset of\nfrequent words that occur in both domains as \\emph{pivots}. Next, we optimize\nan objective function that enforces two constraints: (a) for both source and\ntarget domain documents, pivots that appear in a document must accurately\npredict the co-occurring non-pivots, and (b) word representations learnt for\npivots must be similar in the two domains. Moreover, we propose a method to\nperform domain adaptation using the learnt word representations. Our proposed\nmethod significantly outperforms competitive baselines including the\nstate-of-the-art domain-insensitive word representations, and reports best\nsentiment classification accuracies for all domain-pairs in a benchmark\ndataset."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.07599v3", 
    "title": "Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and   POS Tagging for Micro-blog Texts", 
    "arxiv-id": "1505.07599v3", 
    "author": "Xuanjing Huang", 
    "publish": "2015-05-28T08:54:13Z", 
    "summary": "In this paper, we give an overview for the shared task at the 4th CCF\nConference on Natural Language Processing \\& Chinese Computing (NLPCC 2015):\nChinese word segmentation and part-of-speech (POS) tagging for micro-blog\ntexts. Different with the popular used newswire datasets, the dataset of this\nshared task consists of the relatively informal micro-texts. The shared task\nhas two sub-tasks: (1) individual Chinese word segmentation and (2) joint\nChinese word segmentation and POS Tagging. Each subtask has three tracks to\ndistinguish the systems with different resources. We first introduce the\ndataset and task, then we characterize the different approaches of the\nparticipating systems, report the test results, and provide a overview analysis\nof these results. An online system is available for open registration and\nevaluation at http://nlp.fudan.edu.cn/nlpcc2015."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.07931v1", 
    "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", 
    "arxiv-id": "1505.07931v1", 
    "author": "Kezhi Mao", 
    "publish": "2015-05-29T06:11:00Z", 
    "summary": "Learning vector representation for words is an important research field which\nmay benefit many natural language processing tasks. Two limitations exist in\nnearly all available models, which are the bias caused by the context\ndefinition and the lack of knowledge utilization. They are difficult to tackle\nbecause these algorithms are essentially unsupervised learning approaches.\nInspired by deep learning, the authors propose a supervised framework for\nlearning vector representation of words to provide additional supervised fine\ntuning after unsupervised learning. The framework is knowledge rich approacher\nand compatible with any numerical vectors word representation. The authors\nperform both intrinsic evaluation like attributional and relational similarity\nprediction and extrinsic evaluations like the sentence completion and sentiment\nanalysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show\nthat the proposed fine tuning framework may significantly improve the quality\nof the vector representation of words."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1505.08149v2", 
    "title": "Modeling meaning: computational interpreting and understanding of   natural language fragments", 
    "arxiv-id": "1505.08149v2", 
    "author": "Pavlo Kapustin", 
    "publish": "2015-05-29T19:06:42Z", 
    "summary": "In this introductory article we present the basics of an approach to\nimplementing computational interpreting of natural language aiming to model the\nmeanings of words and phrases. Unlike other approaches, we attempt to define\nthe meanings of text fragments in a composable and computer interpretable way.\nWe discuss models and ideas for detecting different types of semantic\nincomprehension and choosing the interpretation that makes most sense in a\ngiven context. Knowledge representation is designed for handling\ncontext-sensitive and uncertain / imprecise knowledge, and for easy\naccommodation of new information. It stores quantitative information capturing\nthe essence of the concepts, because it is crucial for working with natural\nlanguage understanding and reasoning. Still, the representation is general\nenough to allow for new knowledge to be learned, and even generated by the\nsystem. The article concludes by discussing some reasoning-related topics:\npossible approaches to generation of new abstract concepts, and describing\nsituations and concepts in words (e.g. for specifying interpretation\ndifficulties)."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1506.00037v1", 
    "title": "Using Syntactic Features for Phishing Detection", 
    "arxiv-id": "1506.00037v1", 
    "author": "Julia M. Taylor", 
    "publish": "2015-05-29T21:51:04Z", 
    "summary": "This paper reports on the comparison of the subject and object of verbs in\ntheir usage between phishing emails and legitimate emails. The purpose of this\nresearch is to explore whether the syntactic structures and subjects and\nobjects of verbs can be distinguishable features for phishing detection. To\nachieve the objective, we have conducted two series of experiments: the\nsyntactic similarity for sentences, and the subject and object of verb\ncomparison. The results of the experiments indicated that both features can be\nused for some verbs, but more work has to be done for others."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1506.00196v3", 
    "title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme   Conversion", 
    "arxiv-id": "1506.00196v3", 
    "author": "Geoffrey Zweig", 
    "publish": "2015-05-31T05:14:06Z", 
    "summary": "Sequence-to-sequence translation methods based on generation with a\nside-conditioned language model have recently shown promising results in\nseveral tasks. In machine translation, models conditioned on source side words\nhave been used to produce target-language text, and in image captioning, models\nconditioned images have been used to generate caption text. Past work with this\napproach has focused on large vocabulary tasks, and measured quality in terms\nof BLEU. In this paper, we explore the applicability of such models to the\nqualitatively different grapheme-to-phoneme task. Here, the input and output\nside vocabularies are small, plain n-gram models do well, and credit is only\ngiven when the output is exactly correct. We find that the simple\nside-conditioned generation approach is able to rival the state-of-the-art, and\nwe are able to significantly advance the stat-of-the-art with bi-directional\nlong short-term memory (LSTM) neural networks that use the same alignment\ninformation that is used in conventional approaches."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1506.00275v2", 
    "title": "Diversity in Spectral Learning for Natural Language Parsing", 
    "arxiv-id": "1506.00275v2", 
    "author": "Shay B. Cohen", 
    "publish": "2015-05-31T19:21:26Z", 
    "summary": "We describe an approach to create a diverse set of predictions with spectral\nlearning of latent-variable PCFGs (L-PCFGs). Our approach works by creating\nmultiple spectral models where noise is added to the underlying features in the\ntraining set before the estimation of each model. We describe three ways to\ndecode with multiple models. In addition, we describe a simple variant of the\nspectral algorithm for L-PCFGs that is fast and leads to compact models. Our\nexperiments for natural language parsing, for English and German, show that we\nget a significant improvement over baselines comparable to state of the art.\nFor English, we achieve the $F_1$ score of 90.18, and for German we achieve the\n$F_1$ score of 83.38."
},{
    "category": "cs.CL", 
    "doi": "10.5120/20564-2953", 
    "link": "http://arxiv.org/pdf/1506.00379v2", 
    "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", 
    "arxiv-id": "1506.00379v2", 
    "author": "Song Liu", 
    "publish": "2015-06-01T08:22:49Z", 
    "summary": "Representation learning of knowledge bases (KBs) aims to embed both entities\nand relations into a low-dimensional space. Most existing methods only consider\ndirect relations in representation learning. We argue that multiple-step\nrelation paths also contain rich inference patterns between entities, and\npropose a path-based representation learning model. This model considers\nrelation paths as translations between entities for representation learning,\nand addresses two key challenges: (1) Since not all relation paths are\nreliable, we design a path-constraint resource allocation algorithm to measure\nthe reliability of relation paths. (2) We represent relation paths via semantic\ncomposition of relation embeddings. Experimental results on real-world datasets\nshow that, as compared with baselines, our model achieves significant and\nconsistent improvements on knowledge base completion and relation extraction\nfrom text."
}]