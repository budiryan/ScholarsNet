[{
    "category": "cs.DC", 
    "author": "Michael Manthey", 
    "title": "Distributed Computation, the Twisted Isomorphism, and Auto-Poiesis", 
    "publish": "1998-09-14T14:42:26Z", 
    "summary": "This paper presents a synchronization-based, multi-process computational\nmodel of anticipatory systems called the Phase Web. It describes a\nself-organizing paradigm that explicitly recognizes and exploits the existence\nof a boundary between inside and outside, accepts and exploits intentionality,\nand uses explicit self-reference to describe eg. auto-poiesis. The model\nexplicitly connects computation to a discrete Clifford algebraic formalization\nthat is in turn extended into homology and co-homology, wherein the recursive\nnature of objects and boundaries becomes apparent and itself subject to\nhierarchical recursion. Topsy, a computer program embodying the Phase Web, is\navailable at www.cs.auc.dk/topsy.", 
    "link": "http://arxiv.org/pdf/cs/9809125v1", 
    "arxiv-id": "cs/9809125v1"
},{
    "category": "cs.DC", 
    "author": "Michael Ward", 
    "title": "Gryphon: An Information Flow Based Approach to Message Brokering", 
    "publish": "1998-10-21T18:43:47Z", 
    "summary": "Gryphon is a distributed computing paradigm for message brokering, which is\nthe transferring of information in the form of streams of events from\ninformation providers to information consumers. This extended abstract outlines\nthe major problems in message brokering and Gryphon's approach to solving them.", 
    "link": "http://arxiv.org/pdf/cs/9810019v1", 
    "arxiv-id": "cs/9810019v1"
},{
    "category": "cs.DC", 
    "author": "Jaap-Henk Hoepman", 
    "title": "Self-stabilizing mutual exclusion on a ring, even if K=N", 
    "publish": "1999-09-21T13:39:03Z", 
    "summary": "We show that, contrary to common belief, Dijkstra's self-stabilizing mutual\nexclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of\nstates per node is one less than the number of nodes on the ring.", 
    "link": "http://arxiv.org/pdf/cs/9909013v1", 
    "arxiv-id": "cs/9909013v1"
},{
    "category": "cs.DC", 
    "author": "Joseph Y. Halpern", 
    "title": "A decision-theoretic approach to reliable message delivery", 
    "publish": "1999-09-21T20:51:37Z", 
    "summary": "We argue that the tools of decision theory need to be taken more seriously in\nthe specification and analysis of systems. We illustrate this by considering a\nsimple problem involving reliable communication, showing how considerations of\nutility and probability can be used to decide when it is worth sending\nheartbeat messages and, if they are sent, how often they should be sent.", 
    "link": "http://arxiv.org/pdf/cs/9909015v1", 
    "arxiv-id": "cs/9909015v1"
},{
    "category": "cs.DC", 
    "author": "R. F. C. Walters", 
    "title": "On Automata with Boundary", 
    "publish": "2000-02-16T02:45:39Z", 
    "summary": "We present a theory of automata with boundary for designing, modelling and\nanalysing distributed systems. Notions of behaviour, design and simulation\nappropriate to the theory are defined. The problem of model checking for\ndeadlock detection is discussed, and an algorithm for state space reduction in\nexhaustive search, based on the theory presented here, is described. Three\nexamples of the application of the theory are given, one in the course of the\ndevelopment of the ideas and two as illustrative examples of the use of the\ntheory.", 
    "link": "http://arxiv.org/pdf/cs/0002008v1", 
    "arxiv-id": "cs/0002008v1"
},{
    "category": "cs.DC", 
    "author": "Ian Foster", 
    "title": "A Problem-Specific Fault-Tolerance Mechanism for Asynchronous,   Distributed Systems", 
    "publish": "2000-03-12T00:19:24Z", 
    "summary": "The idle computers on a local area, campus area, or even wide area network\nrepresent a significant computational resource---one that is, however, also\nunreliable, heterogeneous, and opportunistic. This type of resource has been\nused effectively for embarrassingly parallel problems but not for more tightly\ncoupled problems. We describe an algorithm that allows branch-and-bound\nproblems to be solved in such environments. In designing this algorithm, we\nfaced two challenges: (1) scalability, to effectively exploit the variably\nsized pools of resources available, and (2) fault tolerance, to ensure the\nreliability of services. We achieve scalability through a fully decentralized\nalgorithm, by using a membership protocol for managing dynamically available\nresources. However, this fully decentralized design makes achieving reliability\neven more challenging. We guarantee fault tolerance in the sense that the loss\nof up to all but one resource will not affect the quality of the solution. For\npropagating information efficiently, we use epidemic communication for both the\nmembership protocol and the fault-tolerance mechanism. We have developed a\nsimulation framework that allows us to evaluate design alternatives. Results\nobtained in this framework suggest that our techniques can execute scalably and\nreliably.", 
    "link": "http://arxiv.org/pdf/cs/0003054v1", 
    "arxiv-id": "cs/0003054v1"
},{
    "category": "cs.DC", 
    "author": "Andrew Lynes", 
    "title": "Sorting Integers on the AP1000", 
    "publish": "2000-04-23T04:32:18Z", 
    "summary": "Sorting is one of the classic problems of computer science. Whilst well\nunderstood on sequential machines, the diversity of architectures amongst\nparallel systems means that algorithms do not perform uniformly on all\nplatforms. This document describes the implementation of a radix based\nalgorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer,\nwhich was constructed as an entry in the Joint Symposium on Parallel Processing\n(JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also\ngiven to a full radix sort conducted in parallel across the machine.", 
    "link": "http://arxiv.org/pdf/cs/0004013v1", 
    "arxiv-id": "cs/0004013v1"
},{
    "category": "cs.DC", 
    "author": "S. A. Mondal", 
    "title": "A Note on \"Optimal Static Load Balancing in Distributed Computer   Systems\"", 
    "publish": "2000-06-02T03:14:07Z", 
    "summary": "The problem of minimizing mean response time of generic jobs submitted to a\nheterogenous distributed computer systems is considered in this paper. A static\nload balancing strategy, in which decision of redistribution of loads does not\ndepend on the state of the system, is used for this purpose. The article is\nclosely related to a previous article on the same topic. The present article\npoints out number of inconsistencies in the previous article, provides a new\nformulation, and discusses the impact of new findings, based on the improved\nformulation, on the results of the previous article.", 
    "link": "http://arxiv.org/pdf/cs/0006004v1", 
    "arxiv-id": "cs/0006004v1"
},{
    "category": "cs.DC", 
    "author": "O. Waarts", 
    "title": "Performing work efficiently in the presence of faults", 
    "publish": "2000-06-02T18:35:55Z", 
    "summary": "We consider a system of t synchronous processes that communicate only by\nsending messages to one another, and that together must perform $n$ independent\nunits of work. Processes may fail by crashing; we want to guarantee that in\nevery execution of the protocol in which at least one process survives, all n\nunits of work will be performed. We consider three parameters: the number of\nmessages sent, the total number of units of work performed (including\nmultiplicities), and time. We present three protocols for solving the problem.\nAll three are work-optimal, doing O(n+t) work. The first has moderate costs in\nthe remaining two parameters, sending O(t\\sqrt{t}) messages, and taking O(n+t)\ntime. This protocol can be easily modified to run in any completely\nasynchronous system equipped with a failure detection mechanism. The second\nsends only O(t log{t}) messages, but its running time is large (exponential in\nn and t). The third is essentially time-optimal in the (usual) case in which\nthere are no failures, and its time complexity degrades gracefully as the\nnumber of failures increases.", 
    "link": "http://arxiv.org/pdf/cs/0006008v1", 
    "arxiv-id": "cs/0006008v1"
},{
    "category": "cs.DC", 
    "author": "Ted Herman", 
    "title": "Phase Clocks for Transient Fault Repair", 
    "publish": "2000-07-10T15:59:03Z", 
    "summary": "Phase clocks are synchronization tools that implement a form of logical time\nin distributed systems. For systems tolerating transient faults by self-repair\nof damaged data, phase clocks can enable reasoning about the progress of\ndistributed repair procedures. This paper presents a phase clock algorithm\nsuited to the model of transient memory faults in asynchronous systems with\nread/write registers. The algorithm is self-stabilizing and guarantees accuracy\nof phase clocks within O(k) time following an initial state that is k-faulty.\nComposition theorems show how the algorithm can be used for the timing of\ndistributed procedures that repair system outputs.", 
    "link": "http://arxiv.org/pdf/cs/0007015v1", 
    "arxiv-id": "cs/0007015v1"
},{
    "category": "cs.DC", 
    "author": "Jon Giddy", 
    "title": "Nimrod/G: An Architecture of a Resource Management and Scheduling System   in a Global Computational Grid", 
    "publish": "2000-09-22T10:44:16Z", 
    "summary": "The availability of powerful microprocessors and high-speed networks as\ncommodity components has enabled high performance computing on distributed\nsystems (wide-area cluster computing). In this environment, as the resources\nare usually distributed geographically at various levels (department,\nenterprise, or worldwide) there is a great challenge in integrating,\ncoordinating and presenting them as a single resource to the user; thus forming\na computational grid. Another challenge comes from the distributed ownership of\nresources with each resource having its own access policy, cost, and mechanism.\n  The proposed Nimrod/G grid-enabled resource management and scheduling system\nbuilds on our earlier work on Nimrod and follows a modular and component-based\narchitecture enabling extensibility, portability, ease of development, and\ninteroperability of independently developed components. It uses the Globus\ntoolkit services and can be easily extended to operate with any other emerging\ngrid middleware services. It focuses on the management and scheduling of\ncomputations over dynamic resources scattered geographically across the\nInternet at department, enterprise, or global level with particular emphasis on\ndeveloping scheduling schemes based on the concept of computational economy for\na real test bed, namely, the Globus testbed (GUSTO).", 
    "link": "http://arxiv.org/pdf/cs/0009021v1", 
    "arxiv-id": "cs/0009021v1"
},{
    "category": "cs.DC", 
    "author": "David Metcalf", 
    "title": "Byzantine Agreement with Faulty Majority using Bounded Broadcast", 
    "publish": "2000-12-26T23:05:58Z", 
    "summary": "Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely\nused building block of reliable distributed protocols. It simulates broadcast\ndespite the presence of faulty parties within the network, traditionally using\nonly private unicast links. Under such conditions, Byzantine Agreement requires\nmore than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed\na Byzantine Agreement protocol for any compliant majority based on an\nadditional primitive allowing transmission to any two parties simultaneously.\nThey proposed a problem of generalizing these results to wider channels and\nfewer compliant parties. We prove that 2f < kh condition is necessary and\nsufficient for implementing broadcast with h compliant and f faulty parties\nusing k-cast channels.", 
    "link": "http://arxiv.org/pdf/cs/0012024v4", 
    "arxiv-id": "cs/0012024v4"
},{
    "category": "cs.DC", 
    "author": "Alok Choudhary", 
    "title": "A Scientific Data Management System for Irregular Applications", 
    "publish": "2001-02-20T20:17:22Z", 
    "summary": "Many scientific applications are I/O intensive and generate or access large\ndata sets, spanning hundreds or thousands of \"files.\" Management, storage,\nefficient access, and analysis of this data present an extremely challenging\ntask. We have developed a software system, called Scientific Data Manager\n(SDM), that uses a combination of parallel file I/O and database support for\nhigh-performance scientific data management. SDM provides a high-level API to\nthe user and internally, uses a parallel file system to store real data and a\ndatabase to store application-related metadata. In this paper, we describe how\nwe designed and implemented SDM to support irregular applications. SDM can\nefficiently handle the reading and writing of data in an irregular mesh as well\nas the distribution of index values. We describe the SDM user interface and how\nwe implemented it to achieve high performance. SDM makes extensive use of\nMPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a\nhistory file to optimize the cost of the index distribution using the metadata\nstored in the database. We present performance results with two irregular\napplications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,\non the SGI Origin2000 at Argonne National Laboratory.", 
    "link": "http://arxiv.org/pdf/cs/0102016v1", 
    "arxiv-id": "cs/0102016v1"
},{
    "category": "cs.DC", 
    "author": "Ewing Lusk", 
    "title": "Components and Interfaces of a Process Management System for Parallel   Programs", 
    "publish": "2001-02-21T16:04:21Z", 
    "summary": "Parallel jobs are different from sequential jobs and require a different type\nof process management. We present here a process management system for parallel\nprograms such as those written using MPI. A primary goal of the system, which\nwe call MPD (for multipurpose daemon), is to be scalable. By this we mean that\nstartup of interactive parallel jobs comprising thousands of processes is\nquick, that signals can be quickly delivered to processes, and that stdin,\nstdout, and stderr are managed intuitively. Our primary target is parallel\nmachines made up of clusters of SMPs, but the system is also useful in more\ntightly integrated environments. We describe how MPD enables much faster\nstartup and better runtime management of parallel jobs. We show how close\ncontrol of stdio can support the easy implementation of a number of convenient\nsystem utilities, even a parallel debugger. We describe a simple but general\ninterface that can be used to separate any process manager from a parallel\nlibrary, which we use to keep MPD separate from MPICH.", 
    "link": "http://arxiv.org/pdf/cs/0102017v1", 
    "arxiv-id": "cs/0102017v1"
},{
    "category": "cs.DC", 
    "author": "Ian Foster", 
    "title": "Replica Selection in the Globus Data Grid", 
    "publish": "2001-04-02T18:19:06Z", 
    "summary": "The Globus Data Grid architecture provides a scalable infrastructure for the\nmanagement of storage resources and data that are distributed across Grid\nenvironments. These services are designed to support a variety of scientific\napplications, ranging from high-energy physics to computational genomics, that\nrequire access to large amounts of data (terabytes or even petabytes) with\nvaried quality of service requirements. By layering on a set of core services,\nsuch as data transport, security, and replica cataloging, one can construct\nvarious higher-level services. In this paper, we discuss the design and\nimplementation of a high-level replica selection service that uses information\nregarding replica location and user preferences to guide selection from among\nstorage replica alternatives. We first present a basic replica selection\nservice design, then show how dynamic information collected using Globus\ninformation service capabilities concerning storage system properties can help\nimprove and optimize the selection process. We demonstrate the use of Condor's\nClassAds resource description and matchmaking mechanism as an efficient tool\nfor representing and matching storage resource capabilities and policies\nagainst application requirements.", 
    "link": "http://arxiv.org/pdf/cs/0104002v1", 
    "arxiv-id": "cs/0104002v1"
},{
    "category": "cs.DC", 
    "author": "Ted Herman", 
    "title": "Dijkstra's Self-Stabilizing Algorithm in Unsupportive Environments", 
    "publish": "2001-05-07T14:29:59Z", 
    "summary": "The first self-stabilizing algorithm [Dij73] assumed the existence of a\ncentral daemon, that activates one processor at time to change state as a\nfunction of its own state and the state of a neighbor. Subsequent research has\nreconsidered this algorithm without the assumption of a central daemon, and\nunder different forms of communication, such as the model of link registers. In\nall of these investigations, one common feature is the atomicity of\ncommunication, whether by shared variables or read/write registers. This paper\nweakens the atomicity assumptions for the communication model, proposing\nversions of [Dij73] that tolerate various weaker forms of atomicity. First, a\nsolution for the case of regular registers is presented. Then the case of safe\nregisters is considered, with both negative and positive results presented. The\npaper also presents an implementation of [Dij73] based on registers that have\nprobabilistically correct behavior, which requires a notion of weak\nstabilization.", 
    "link": "http://arxiv.org/pdf/cs/0105013v1", 
    "arxiv-id": "cs/0105013v1"
},{
    "category": "cs.DC", 
    "author": "Lee Guan", 
    "title": "On the Area of Hypercube Layouts", 
    "publish": "2001-05-29T21:59:18Z", 
    "summary": "This paper precisely analyzes the wire density and required area in standard\nlayout styles for the hypercube. The most natural, regular layout of a\nhypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses\nfloor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of\ntracks per row can be reduced by 1 with a less regular design.) This paper also\ngives a simple formula for the wire density at any cut position and a full\ncharacterization of all places where the wire density is maximized (which does\nnot occur at the bisection).", 
    "link": "http://arxiv.org/pdf/cs/0105034v1", 
    "arxiv-id": "cs/0105034v1"
},{
    "category": "cs.DC", 
    "author": "David Abrams", 
    "title": "Economic Models for Management of Resources in Grid Computing", 
    "publish": "2001-06-11T07:46:56Z", 
    "summary": "The accelerated development in Grid and peer-to-peer computing has positioned\nthem as promising next generation computing platforms. They enable the creation\nof Virtual Enterprises (VE) for sharing resources distributed across the world.\nHowever, resource management, application development and usage models in these\nenvironments is a complex undertaking. This is due to the geographic\ndistribution of resources that are owned by different organizations. The\nresource owners of each of these resources have different usage or access\npolicies and cost models, and varying loads and availability. In order to\naddress complex resource management issues, we have proposed a computational\neconomy framework for resource allocation and for regulating supply and demand\nin Grid computing environments. The framework provides mechanisms for\noptimizing resource provider and consumer objective functions through trading\nand brokering services. In a real world market, there exist various economic\nmodels for setting the price for goods based on supply-and-demand and their\nvalue to the user. They include commodity market, posted price, tenders and\nauctions. In this paper, we discuss the use of these models for interaction\nbetween Grid components in deciding resource value and the necessary\ninfrastructure to realize them. In addition to normal services offered by Grid\ncomputing systems, we need an infrastructure to support interaction protocols,\nallocation mechanisms, currency, secure banking, and enforcement services.\nFurthermore, we demonstrate the usage of some of these economic models in\nresource brokering through Nimrod/G deadline and cost-based scheduling for two\ndifferent optimization strategies on the World Wide Grid (WWG) testbed.", 
    "link": "http://arxiv.org/pdf/cs/0106020v1", 
    "arxiv-id": "cs/0106020v1"
},{
    "category": "cs.DC", 
    "author": "David M. Mackie", 
    "title": "Simple and Effective Distributed Computing with a Scheduling Service", 
    "publish": "2001-06-15T17:09:49Z", 
    "summary": "High-throughput computing projects require the solution of large numbers of\nproblems. In many cases, these problems can be solved on desktop PCs, or can be\nbroken down into independent \"PC-solvable\" sub-problems. In such cases, the\nprojects are high-performance computing projects, but only because of the sheer\nnumber of the needed calculations. We briefly describe our efforts to increase\nthe throughput of one such project. We then explain how to easily set up a\ndistributed computing facility composed of standard networked PCs running\nWindows 95, 98, 2000, or NT. The facility requires no special software or\nhardware, involves little or no re-coding of application software, and operates\nalmost invisibly to the owners of the PCs. Depending on the number and quality\nof PCs recruited, performance can rival that of supercomputers.", 
    "link": "http://arxiv.org/pdf/cs/0106038v1", 
    "arxiv-id": "cs/0106038v1"
},{
    "category": "cs.DC", 
    "author": "Paul Vitanyi", 
    "title": "Randomized Two-Process Wait-Free Test-and-Set", 
    "publish": "2001-06-28T15:45:35Z", 
    "summary": "We present the first explicit, and currently simplest, randomized algorithm\nfor 2-process wait-free test-and-set. It is implemented with two 4-valued\nsingle writer single reader atomic variables. A test-and-set takes at most 11\nexpected elementary steps, while a reset takes exactly 1 elementary step. Based\non a finite-state analysis, the proofs of correctness and expected length are\ncompressed into one table.", 
    "link": "http://arxiv.org/pdf/cs/0106056v2", 
    "arxiv-id": "cs/0106056v2"
},{
    "category": "cs.DC", 
    "author": "Elizabeth D. Dolan", 
    "title": "NEOS Server 4.0 Administrative Guide", 
    "publish": "2001-07-26T15:19:00Z", 
    "summary": "The NEOS Server 4.0 provides a general Internet-based client/server as a link\nbetween users and software applications. The administrative guide covers the\nfundamental principals behind the operation of the NEOS Server, installation\nand trouble-shooting of the Server software, and implementation details of\npotential interest to a NEOS Server administrator. The guide also discusses\nmaking new software applications available through the Server, including areas\nof concern to remote solver administrators such as maintaining security,\nproviding usage instructions, and enforcing reasonable restrictions on jobs.\nThe administrative guide is intended both as an introduction to the NEOS Server\nand as a reference for use when running the Server.", 
    "link": "http://arxiv.org/pdf/cs/0107034v1", 
    "arxiv-id": "cs/0107034v1"
},{
    "category": "cs.DC", 
    "author": "John Shalf", 
    "title": "The Cactus Worm: Experiments with Dynamic Resource Discovery and   Allocation in a Grid Environment", 
    "publish": "2001-08-01T23:14:34Z", 
    "summary": "The ability to harness heterogeneous, dynamically available \"Grid\" resources\nis attractive to typically resource-starved computational scientists and\nengineers, as in principle it can increase, by significant factors, the number\nof cycles that can be delivered to applications. However, new adaptive\napplication structures and dynamic runtime system mechanisms are required if we\nare to operate effectively in Grid environments. In order to explore some of\nthese issues in a practical setting, we are developing an experimental\nframework, called Cactus, that incorporates both adaptive application\nstructures for dealing with changing resource characteristics and adaptive\nresource selection mechanisms that allow applications to change their resource\nallocations (e.g., via migration) when performance falls outside specified\nlimits. We describe here the adaptive resource selection mechanisms and\ndescribe how they are used to achieve automatic application migration to\n\"better\" resources following performance degradation. Our results provide\ninsights into the architectural structures required to support adaptive\nresource selection. In addition, we suggest that this \"Cactus Worm\" is an\ninteresting challenge problem for Grid computing.", 
    "link": "http://arxiv.org/pdf/cs/0108001v1", 
    "arxiv-id": "cs/0108001v1"
},{
    "category": "cs.DC", 
    "author": "Paul Vitanyi", 
    "title": "Bounded Concurrent Timestamp Systems Using Vector Clocks", 
    "publish": "2001-08-02T17:13:48Z", 
    "summary": "Shared registers are basic objects used as communication mediums in\nasynchronous concurrent computation. A concurrent timestamp system is a higher\ntyped communication object, and has been shown to be a powerful tool to solve\nmany concurrency control problems. It has turned out to be possible to\nconstruct such higher typed objects from primitive lower typed ones. The next\nstep is to find efficient constructions. We propose a very efficient wait-free\nconstruction of bounded concurrent timestamp systems from 1-writer multireader\nregisters. This finalizes, corrects, and extends, a preliminary bounded\nmultiwriter construction proposed by the second author in 1986. That work\npartially initiated the current interest in wait-free concurrent objects, and\nintroduced a notion of discrete vector clocks in distributed algorithms.", 
    "link": "http://arxiv.org/pdf/cs/0108002v1", 
    "arxiv-id": "cs/0108002v1"
},{
    "category": "cs.DC", 
    "author": "W. Gropp", 
    "title": "Scalable Unix Commands for Parallel Processors: A High-Performance   Implementation", 
    "publish": "2001-08-27T15:54:41Z", 
    "summary": "We describe a family of MPI applications we call the Parallel Unix Commands.\nThese commands are natural parallel versions of common Unix user commands such\nas ls, ps, and find, together with a few similar commands particular to the\nparallel environment. We describe the design and implementation of these\nprograms and present some performance results on a 256-node Linux cluster. The\nParallel Unix Commands are open source and freely available.", 
    "link": "http://arxiv.org/pdf/cs/0108019v1", 
    "arxiv-id": "cs/0108019v1"
},{
    "category": "cs.DC", 
    "author": "William D. Gropp", 
    "title": "Learning from the Success of MPI", 
    "publish": "2001-09-13T21:14:21Z", 
    "summary": "The Message Passing Interface (MPI) has been extremely successful as a\nportable way to program high-performance parallel computers. This success has\noccurred in spite of the view of many that message passing is difficult and\nthat other approaches, including automatic parallelization and directive-based\nparallelism, are easier to use. This paper argues that MPI has succeeded\nbecause it addresses all of the important issues in providing a parallel\nprogramming model.", 
    "link": "http://arxiv.org/pdf/cs/0109017v1", 
    "arxiv-id": "cs/0109017v1"
},{
    "category": "cs.DC", 
    "author": "Yi Pan", 
    "title": "Teaching Parallel Programming Using Both High-Level and Low-Level   Languages", 
    "publish": "2001-10-29T19:00:15Z", 
    "summary": "We discuss the use of both MPI and OpenMP in the teaching of senior\nundergraduate and junior graduate classes in parallel programming. We briefly\nintroduce the OpenMP standard and discuss why we have chosen to use it in\nparallel programming classes. Advantages of using OpenMP over message passing\nmethods are discussed. We also include a brief enumeration of some of the\ndrawbacks of using OpenMP and how these drawbacks are being addressed by\nsupplementing OpenMP with additional MPI codes and projects. Several projects\ngiven in my class are also described in this paper.", 
    "link": "http://arxiv.org/pdf/cs/0110058v1", 
    "arxiv-id": "cs/0110058v1"
},{
    "category": "cs.DC", 
    "author": "P. Verdier", 
    "title": "The ESRF TANGO control system status", 
    "publish": "2001-11-09T14:29:16Z", 
    "summary": "TANGO is an object oriented control system toolkit based on CORBA presently\nunder development at the ESRF. IN this paper, the TANGO philosophy is briefly\npresented. All the existing tools developed around TANGO will also be\npresented. This include a code genrator, a WEB interface to TANGO objects, an\nadministration tool and an interface to LabView. Finally, an xample of a TANGO\ndevice server for OPC device is given.", 
    "link": "http://arxiv.org/pdf/cs/0111028v1", 
    "arxiv-id": "cs/0111028v1"
},{
    "category": "cs.DC", 
    "author": "John P. Woodruff", 
    "title": "Large-Scale Corba-Distributed Software Framework for Nif Controls", 
    "publish": "2001-11-09T16:40:09Z", 
    "summary": "The Integrated Computer Control System (ICCS) is based on a scalable software\nframework that is distributed over some 325 computers throughout the NIF\nfacility. The framework provides templates and services at multiple levels of\nabstraction for the construction of software applications that communicate via\nCORBA (Common Object Request Broker Architecture). Various forms of\nobject-oriented software design patterns are implemented as templates to be\nextended by application software. Developers extend the framework base classes\nto model the numerous physical control points, thereby sharing the\nfunctionality defined by the base classes. About 56,000 software objects each\nindividually addressed through CORBA are to be created in the complete ICCS.\nMost objects have a persistent state that is initialized at system start-up and\nstored in a database. Additional framework services are provided by centralized\nserver programs that implement events, alerts, reservations, message logging,\ndatabase/file persistence, name services, and process management. The ICCS\nsoftware framework approach allows for efficient construction of a software\nsystem that supports a large number of distributed control points representing\na complex control application.", 
    "link": "http://arxiv.org/pdf/cs/0111031v1", 
    "arxiv-id": "cs/0111031v1"
},{
    "category": "cs.DC", 
    "author": "W. D. Klotz", 
    "title": "Modernising the ESRF control system with GNU/Linux", 
    "publish": "2001-11-09T20:08:18Z", 
    "summary": "he ESRF control system is in the process of being modernised. The present\ncontrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,\nMotif and C. The new control system will be based on compact PCI, 100 MHz\nEthernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main\nfrontend operating system will be GNU/Linux running on Intel/x86 and\nMotorola/68k. Linux will also be used on handheld devices for mobile control.\nThis poster describes how GNU/Linux is being used to modernise the control\nsystem and what problems have been encountered so far", 
    "link": "http://arxiv.org/pdf/cs/0111033v1", 
    "arxiv-id": "cs/0111033v1"
},{
    "category": "cs.DC", 
    "author": "David Abramson", 
    "title": "Virtual Laboratory: Enabling On-Demand Drug Design with the World Wide   Grid", 
    "publish": "2001-11-17T07:20:58Z", 
    "summary": "Computational Grids are emerging as a popular paradigm for solving\nlarge-scale compute and data intensive problems in science, engineering, and\ncommerce. However, application composition, resource management and scheduling\nin these environments is a complex undertaking. In this paper, we illustrate\nthe creation of a virtual laboratory environment by leveraging existing Grid\ntechnologies to enable molecular modeling for drug design on distributed\nresources. It involves screening millions of molecules of chemical compounds\nagainst a protein target, chemical database (CDB) to identify those with\npotential use for drug design. We have grid-enabled the molecular docking\nprocess by composing it as a parameter sweep application using the Nimrod-G\ntools. We then developed new tools for remote access to molecules in CDB small\nmolecule database. The Nimrod-G resource broker along with molecule CDB data\nbroker is used for scheduling and on-demand processing of jobs on distributed\ngrid resources. The results demonstrate the ease of use and suitability of the\nNimrod-G and virtual laboratory tools.", 
    "link": "http://arxiv.org/pdf/cs/0111047v1", 
    "arxiv-id": "cs/0111047v1"
},{
    "category": "cs.DC", 
    "author": "Jonathan Giddy", 
    "title": "A Computational Economy for Grid Computing and its Implementation in the   Nimrod-G Resource Brok", 
    "publish": "2001-11-17T07:32:44Z", 
    "summary": "Computational Grids, coupling geographically distributed resources such as\nPCs, workstations, clusters, and scientific instruments, have emerged as a next\ngeneration computing platform for solving large-scale problems in science,\nengineering, and commerce. However, application development, resource\nmanagement, and scheduling in these environments continue to be a complex\nundertaking. In this article, we discuss our efforts in developing a resource\nmanagement system for scheduling computations on resources distributed across\nthe world with varying quality of service. Our service-oriented grid computing\nsystem called Nimrod-G manages all operations associated with remote execution\nincluding resource discovery, trading, scheduling based on economic principles\nand a user defined quality of service requirement. The Nimrod-G resource broker\nis implemented by leveraging existing technologies such as Globus, and provides\nnew services that are essential for constructing industrial-strength Grids. We\ndiscuss results of preliminary experiments on scheduling some parametric\ncomputations using the Nimrod-G resource broker on a world-wide grid testbed\nthat spans five continents.", 
    "link": "http://arxiv.org/pdf/cs/0111048v1", 
    "arxiv-id": "cs/0111048v1"
},{
    "category": "cs.DC", 
    "author": "Paul Vitanyi", 
    "title": "Simple Optimal Wait-free Multireader Registers", 
    "publish": "2002-02-04T17:29:15Z", 
    "summary": "Multireader shared registers are basic objects used as communication medium\nin asynchronous concurrent computation. We propose a surprisingly simple and\nnatural scheme to obtain several wait-free constructions of bounded 1-writer\nmultireader registers from atomic 1-writer 1-reader registers, that is easier\nto prove correct than any previous construction. Our main construction is the\nfirst symmetric pure timestamp one that is optimal with respect to the\nworst-case local use of control bits; the other one is optimal with respect to\nglobal use of control bits; both are optimal in time.", 
    "link": "http://arxiv.org/pdf/cs/0202003v3", 
    "arxiv-id": "cs/0202003v3"
},{
    "category": "cs.DC", 
    "author": "Manzur Murshed", 
    "title": "GridSim: A Toolkit for the Modeling and Simulation of Distributed   Resource Management and Scheduling for Grid Computing", 
    "publish": "2002-03-14T03:44:18Z", 
    "summary": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular\nparadigms for next generation parallel and distributed computing. The\nmanagement of resources and scheduling of applications in such large-scale\ndistributed systems is a complex undertaking. In order to prove the\neffectiveness of resource brokers and associated scheduling algorithms, their\nperformance needs to be evaluated under different scenarios such as varying\nnumber of resources and users with different requirements. In a grid\nenvironment, it is hard and even impossible to perform scheduler performance\nevaluation in a repeatable and controllable manner as resources and users are\ndistributed across multiple organizations with their own policies. To overcome\nthis limitation, we have developed a Java-based discrete-event grid simulation\ntoolkit called GridSim. The toolkit supports modeling and simulation of\nheterogeneous grid resources (both time- and space-shared), users and\napplication models. It provides primitives for creation of application tasks,\nmapping of tasks to resources, and their management. To demonstrate suitability\nof the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker\nand evaluated the performance of deadline and budget constrained cost- and\ntime-minimization scheduling algorithms.", 
    "link": "http://arxiv.org/pdf/cs/0203019v1", 
    "arxiv-id": "cs/0203019v1"
},{
    "category": "cs.DC", 
    "author": "Manzur Murshed", 
    "title": "A Deadline and Budget Constrained Cost-Time Optimisation Algorithm for   Scheduling Task Farming Applications on Global Grids", 
    "publish": "2002-03-14T03:50:19Z", 
    "summary": "Computational Grids and peer-to-peer (P2P) networks enable the sharing,\nselection, and aggregation of geographically distributed resources for solving\nlarge-scale problems in science, engineering, and commerce. The management and\ncomposition of resources and services for scheduling applications, however,\nbecomes a complex undertaking. We have proposed a computational economy\nframework for regulating the supply and demand for resources and allocating\nthem for applications based on the users quality of services requirements. The\nframework requires economy driven deadline and budget constrained (DBC)\nscheduling algorithms for allocating resources to application jobs in such a\nway that the users requirements are met. In this paper, we propose a new\nscheduling algorithm, called DBC cost-time optimisation, which extends the DBC\ncost-optimisation algorithm to optimise for time, keeping the cost of\ncomputation at the minimum. The superiority of this new scheduling algorithm,\nin achieving lower job completion time, is demonstrated by simulating the\nWorld-Wide Grid and scheduling task-farming applications for different deadline\nand budget scenarios using both this new and the cost optimisation scheduling\nalgorithms.", 
    "link": "http://arxiv.org/pdf/cs/0203020v1", 
    "arxiv-id": "cs/0203020v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Economic-based Distributed Resource Management and Scheduling for Grid   Computing", 
    "publish": "2002-04-22T20:06:42Z", 
    "summary": "Computational Grids, emerging as an infrastructure for next generation\ncomputing, enable the sharing, selection, and aggregation of geographically\ndistributed resources for solving large-scale problems in science, engineering,\nand commerce. As the resources in the Grid are heterogeneous and geographically\ndistributed with varying availability and a variety of usage and cost policies\nfor diverse users at different times and, priorities as well as goals that vary\nwith time. The management of resources and application scheduling in such a\nlarge and distributed environment is a complex task. This thesis proposes a\ndistributed computational economy as an effective metaphor for the management\nof resources and application scheduling. It proposes an architectural framework\nthat supports resource trading and quality of services based scheduling. It\nenables the regulation of supply and demand for resources and provides an\nincentive for resource owners for participating in the Grid and motives the\nusers to trade-off between the deadline, budget, and the required level of\nquality of service. The thesis demonstrates the capability of economic-based\nsystems for peer-to-peer distributed computing by developing users'\nquality-of-service requirements driven scheduling strategies and algorithms. It\ndemonstrates their effectiveness by performing scheduling experiments on the\nWorld-Wide Grid for solving parameter sweep applications.", 
    "link": "http://arxiv.org/pdf/cs/0204048v1", 
    "arxiv-id": "cs/0204048v1"
},{
    "category": "cs.DC", 
    "author": "O. Smirnova", 
    "title": "An Overview of a Grid Architecture for Scientific Computing", 
    "publish": "2002-05-14T19:22:00Z", 
    "summary": "This document gives an overview of a Grid testbed architecture proposal for\nthe NorduGrid project. The aim of the project is to establish an inter-Nordic\ntestbed facility for implementation of wide area computing and data handling.\nThe architecture is supposed to define a Grid system suitable for solving data\nintensive problems at the Large Hadron Collider at CERN. We present the various\narchitecture components needed for such a system. After that we go on to give a\ndescription of the dynamics by showing the task flow.", 
    "link": "http://arxiv.org/pdf/cs/0205021v1", 
    "arxiv-id": "cs/0205021v1"
},{
    "category": "cs.DC", 
    "author": "A. Waananen", 
    "title": "Performance evaluation of the GridFTP within the NorduGrid project", 
    "publish": "2002-05-14T19:55:37Z", 
    "summary": "This report presents results of the tests measuring the performance of\nmulti-threaded file transfers, using the GridFTP implementation of the Globus\nproject over the NorduGrid network resources. Point to point WAN tests, carried\nout between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. It\nwas found that multiple threaded download via the high performance GridFTP\nprotocol can significantly improve file transfer performance, and can serve as\na reliable data", 
    "link": "http://arxiv.org/pdf/cs/0205023v2", 
    "arxiv-id": "cs/0205023v2"
},{
    "category": "cs.DC", 
    "author": "E. Lusk", 
    "title": "A Multilevel Approach to Topology-Aware Collective Operations in   Computational Grids", 
    "publish": "2002-06-24T18:28:21Z", 
    "summary": "The efficient implementation of collective communiction operations has\nreceived much attention. Initial efforts produced \"optimal\" trees based on\nnetwork communication models that assumed equal point-to-point latencies\nbetween any two processes. This assumption is violated in most practical\nsettings, however, particularly in heterogeneous systems such as clusters of\nSMPs and wide-area \"computational Grids,\" with the result that collective\noperations perform suboptimally. In response, more recent work has focused on\ncreating topology-aware trees for collective operations that minimize\ncommunication across slower channels (e.g., a wide-area network). While these\nefforts have significant communication benefits, they all limit their view of\nthe network to only two layers. We present a strategy based upon a multilayer\nview of the network. By creating multilevel topology-aware trees we take\nadvantage of communication cost differences at every level in the network. We\nused this strategy to implement topology-aware versions of several MPI\ncollective operations in MPICH-G2, the Globus Toolkit[tm]-enabled version of\nthe popular MPICH implementation of the MPI standard. Using information about\ntopology provided by MPICH-G2, we construct these multilevel topology-aware\ntrees automatically during execution. We present results demonstrating the\nadvantages of our multilevel approach by comparing it to the default\n(topology-unaware) implementation provided by MPICH and a topology-aware\ntwo-layer implementation.", 
    "link": "http://arxiv.org/pdf/cs/0206038v1", 
    "arxiv-id": "cs/0206038v1"
},{
    "category": "cs.DC", 
    "author": "I. Foster", 
    "title": "MPICH-G2: A Grid-Enabled Implementation of the Message Passing Interface", 
    "publish": "2002-06-25T19:55:45Z", 
    "summary": "Application development for distributed computing \"Grids\" can benefit from\ntools that variously hide or enable application-level management of critical\naspects of the heterogeneous environment. As part of an investigation of these\nissues, we have developed MPICH-G2, a Grid-enabled implementation of the\nMessage Passing Interface (MPI) that allows a user to run MPI programs across\nmultiple computers, at the same or different sites, using the same commands\nthat would be used on a parallel computer. This library extends the Argonne\nMPICH implementation of MPI to use services provided by the Globus Toolkit for\nauthentication, authorization, resource allocation, executable staging, and\nI/O, as well as for process creation, monitoring, and control. Various\nperformance-critical operations, including startup and collective operations,\nare configured to exploit network topology information. The library also\nexploits MPI constructs for performance management; for example, the MPI\ncommunicator construct is used for application-level discovery of, and\nadaptation to, both network topology and network quality-of-service mechanisms.\nWe describe the MPICH-G2 design and implementation, present performance\nresults, and review application experiences, including record-setting\ndistributed simulations.", 
    "link": "http://arxiv.org/pdf/cs/0206040v2", 
    "arxiv-id": "cs/0206040v2"
},{
    "category": "cs.DC", 
    "author": "T. Terriberry", 
    "title": "System Description for a Scalable, Fault-Tolerant, Distributed Garbage   Collector", 
    "publish": "2002-07-10T00:53:23Z", 
    "summary": "We describe an efficient and fault-tolerant algorithm for distributed cyclic\ngarbage collection. The algorithm imposes few requirements on the local\nmachines and allows for flexibility in the choice of local collector and\ndistributed acyclic garbage collector to use with it. We have emphasized\nreducing the number and size of network messages without sacrificing the\npromptness of collection throughout the algorithm. Our proposed collector is a\nvariant of back tracing to avoid extensive synchronization between machines. We\nhave added an explicit forward tracing stage to the standard back tracing stage\nand designed a tuned heuristic to reduce the total amount of work done by the\ncollector. Of particular note is the development of fault-tolerant cooperation\nbetween traces and a heuristic that aggressively reduces the set of suspect\nobjects.", 
    "link": "http://arxiv.org/pdf/cs/0207036v1", 
    "arxiv-id": "cs/0207036v1"
},{
    "category": "cs.DC", 
    "author": "William Gropp", 
    "title": "Noncontiguous I/O through PVFS", 
    "publish": "2002-07-29T16:20:25Z", 
    "summary": "With the tremendous advances in processor and memory technology, I/O has\nrisen to become the bottleneck in high-performance computing for many\napplications. The development of parallel file systems has helped to ease the\nperformance gap, but I/O still remains an area needing significant performance\nimprovement. Research has found that noncontiguous I/O access patterns in\nscientific applications combined with current file system methods to perform\nthese accesses lead to unacceptable performance for large data sets. To enhance\nperformance of noncontiguous I/O we have created list I/O, a native version of\nnoncontiguous I/O. We have used the Parallel Virtual File System (PVFS) to\nimplement our ideas. Our research and experimentation shows that list I/O\noutperforms current noncontiguous I/O access methods in most I/O situations and\ncan substantially enhance the performance of real-world scientific\napplications.", 
    "link": "http://arxiv.org/pdf/cs/0207096v1", 
    "arxiv-id": "cs/0207096v1"
},{
    "category": "cs.DC", 
    "author": "G. A. Kohring", 
    "title": "Implicit Simulations using Messaging Protocols", 
    "publish": "2002-08-14T09:11:20Z", 
    "summary": "A novel algorithm for performing parallel, distributed computer simulations\non the Internet using IP control messages is introduced. The algorithm employs\ncarefully constructed ICMP packets which enable the required computations to be\ncompleted as part of the standard IP communication protocol. After providing a\ndetailed description of the algorithm, experimental applications in the areas\nof stochastic neural networks and deterministic cellular automata are\ndiscussed. As an example of the algorithms potential power, a simulation of a\ndeterministic cellular automaton involving 10^5 Internet connected devices was\nperformed.", 
    "link": "http://arxiv.org/pdf/cs/0208021v1", 
    "arxiv-id": "cs/0208021v1"
},{
    "category": "cs.DC", 
    "author": "Gary J. Nutt", 
    "title": "A Unified Theory of Shared Memory Consistency", 
    "publish": "2002-08-19T19:09:04Z", 
    "summary": "Memory consistency models have been developed to specify what values may be\nreturned by a read given that, in a distributed system, memory operations may\nonly be partially ordered. Before this work, consistency models were defined\nindependently. Each model followed a set of rules which was separate from the\nrules of every other model. In our work we have defined a set of four\nconsistency properties. Any subset of the four properties yields a set of rules\nwhich constitute a consistency model. Every consistency model previously\ndescribed in the literature can be defined based on our four properties.\nTherefore, we present these properties as a unfied theory of shared memory\nconsistency.", 
    "link": "http://arxiv.org/pdf/cs/0208027v1", 
    "arxiv-id": "cs/0208027v1"
},{
    "category": "cs.DC", 
    "author": "Alexander Barmouta Rajkumar Buyya", 
    "title": "GridBank: A Grid Accounting Services Architecture (GASA) for Distributed   Systems Sharing and Integration", 
    "publish": "2002-10-01T10:41:52Z", 
    "summary": "Computational Grids are emerging as new infrastructure for Internet-based\nparallel and distributed computing. They enable the sharing, exchange,\ndiscovery, and aggregation of resources distributed across multiple\nadministrative domains, organizations and enterprises. To accomplish this,\nGrids need infrastructure that supports various services: security, uniform\naccess, resource management, scheduling, application composition, computational\neconomy, and accountability. Many Grid projects have developed technologies\nthat provide many of these services with an exception of accountability. To\novercome this limitation, we propose a new infrastructure called Grid Bank that\nprovides services for accounting. This paper presents requirements of Grid\naccountability and different models within which it can operate and proposes\nGrid Bank Services Architecture that meets them. The paper highlights\nimplementation issues with detailed discussion on format for various\nrecords/database that the GridBank need to maintain. It also presents protocols\nfor interaction between GridBank and various components within Grid computing\nenvironments.", 
    "link": "http://arxiv.org/pdf/cs/0210002v1", 
    "arxiv-id": "cs/0210002v1"
},{
    "category": "cs.DC", 
    "author": "D. McCune", 
    "title": "Computational Grids in Action: The Natinal Fusion Collaboratory", 
    "publish": "2003-01-28T20:11:16Z", 
    "summary": "The National Fusion Collaboratory (NFC) project was created to advance\nscientific understanding and innovation in magnetic fusion research by enabling\nmore efficient use of existing experimental facilities through more effective\nintegration of experiment, theory, and modeling. To achieve this objective, NFC\nintroduced the concept of \"network services\", which build on top of\ncomputational Grids, and provide Fusion codes, together with their maintenance\nand hardware resources as a service to the community. This mode of operation\nrequires the development of new authorization and enforcement capabilities. In\naddition, the nature of Fusion experiments places strident quality of service\nrequirements on codes run during the experimental cycle. In this paper, we\ndescribe Grid computing requirements of the Fusion community, and present our\nfirst experiments in meeting those requirements.", 
    "link": "http://arxiv.org/pdf/cs/0301033v1", 
    "arxiv-id": "cs/0301033v1"
},{
    "category": "cs.DC", 
    "author": "Alan Wagner", 
    "title": "On the Complexity of Buffer Allocation in Message Passing Systems", 
    "publish": "2003-01-31T01:21:21Z", 
    "summary": "Message passing programs commonly use buffers to avoid unnecessary\nsynchronizations and to improve performance by overlapping communication with\ncomputation. Unfortunately, using buffers makes the program no longer portable,\npotentially unable to complete on systems without a sufficient number of\nbuffers. Effective buffer use entails that the minimum number needed for a safe\nexecution be allocated.\n  We explore a variety of problems related to buffer allocation for safe and\nefficient execution of message passing programs. We show that determining the\nminimum number of buffers or verifying a buffer assignment are intractable\nproblems. However, we give a polynomial time algorithm to determine the minimum\nnumber of buffers needed to allow for asynchronous execution. We extend these\nresults to several different buffering schemes, which in some cases make the\nproblems tractable.", 
    "link": "http://arxiv.org/pdf/cs/0301035v1", 
    "arxiv-id": "cs/0301035v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Grid Market Directory: A Web Services based Grid Service Publication   Directory", 
    "publish": "2003-02-06T03:31:02Z", 
    "summary": "As Grids are emerging as the next generation service-oriented computing\nplatforms, they need to support Grid economy that helps in the management of\nsupply and demand for resources and offers an economic incentive for Grid\nresource providers. To enable this Grid economy, a market-like Grid environment\nincluding an infrastructure that supports the publication of services and their\ndiscovery is needed. As part of the Gridbus project, we proposed and have\ndeveloped a Grid Market Directory (GMD) that serves as a registry for\nhigh-level service publication and discovery in Virtual Organisations.", 
    "link": "http://arxiv.org/pdf/cs/0302006v1", 
    "arxiv-id": "cs/0302006v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "G-Monitor: Gridbus web portal for monitoring and steering application   execution on global grids", 
    "publish": "2003-02-06T03:36:29Z", 
    "summary": "Grids are experiencing a rapid growth in their application and along with\nthis there is a requirement for a portal which is easy to use and scalable. We\nhave responded to this requirement by developing an easy to use, scalable,\nweb-based portal called G-Monitor. This paper proposes a generic architecture\nfor a web portal into a grid environment and discusses our implementation and\nits application.", 
    "link": "http://arxiv.org/pdf/cs/0302007v1", 
    "arxiv-id": "cs/0302007v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Visual Environment for Rapid Composition of Parameter-Sweep Applications   for Distributed Processing on Global Grids", 
    "publish": "2003-02-06T04:06:02Z", 
    "summary": "Computational Grids are emerging as a platform for next-generation parallel\nand distributed computing. Large-scale parametric studies and parameter sweep\napplications find a natural place in the Grid?s distribution model. There is\nlittle or no communication between jobs. The task of parallelizing and\ndistributing existing applications is conceptually trivial. These properties of\nparametric studies make it an ideal place to start developing integrated\ndevelopment environments (IDEs) for rapidly Grid-enabling applications.\nHowever, the availability of IDEs for scientists to Grid-enable their\napplications, without the need of developing them as parallel applications\nexplicitly, is still lacking. This paper presents a Java based IDE called\nVisual Parametric Tool (VPT), developed as part of the Gridbus project, for\nrapid creation of parameter sweep (data parallel/SPMD) applications. It\nsupports automatic creation of parameter script and parameterisation of the\ninput data files, which is compatible with the Nimrod-G parameter specification\nlanguage. The usefulness of VPT is demonstrated by a case study on composition\nof molecular docking application as a parameter sweep application. Such\napplications can be deployed on clusters using the Nimrod/enFuzion system and\non global Grids using the Nimrod-G grid resource broker.", 
    "link": "http://arxiv.org/pdf/cs/0302008v1", 
    "arxiv-id": "cs/0302008v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Guided Google: A Meta Search Engine and its Implementation using the   Google Distributed Web Services", 
    "publish": "2003-02-13T04:49:26Z", 
    "summary": "With the advent of the Internet, search engines have begun sprouting like\nmushrooms after a rainfall. Only in recent years, have developers become more\ninnovative, and came up with guided searching facilities online. The goals of\nthese applications are to help ease and guide the searching efforts of a novice\nweb user toward their desired objectives. A number of implementations of such\nservices are emerging. This paper proposes a guided meta-search engine, called\n\"Guided Google\", as it serves as an interface to the actual Google.com search\nengine, using the Google Web Services.", 
    "link": "http://arxiv.org/pdf/cs/0302018v1", 
    "arxiv-id": "cs/0302018v1"
},{
    "category": "cs.DC", 
    "author": "D. Abramson", 
    "title": "Economic and On Demand Brain Activity Analysis on Global Grids", 
    "publish": "2003-02-13T04:53:10Z", 
    "summary": "The lack of computational power within an organization for analyzing\nscientific data, and the distribution of knowledge (by scientists) and\ntechnologies (advanced scientific devices) are two major problems commonly\nobserved in scientific disciplines. One such scientific discipline is brain\nscience. The analysis of brain activity data gathered from the MEG\n(Magnetoencephalography) instrument is an important research topic in medical\nscience since it helps doctors in identifying symptoms of diseases. The data\nneeds to be analyzed exhaustively to efficiently diagnose and analyze brain\nfunctions and requires access to large-scale computational resources. The\npotential platform for solving such resource intensive applications is the\nGrid. This paper describes a MEG data analysis system developed by us,\nleveraging Grid technologies, primarily Nimrod-G, Gridbus, and Globus. This\npaper explains the application of economy-based grid scheduling algorithms to\nthe problem domain for on-demand processing of analysis jobs.", 
    "link": "http://arxiv.org/pdf/cs/0302019v1", 
    "arxiv-id": "cs/0302019v1"
},{
    "category": "cs.DC", 
    "author": "H. Ballhausen", 
    "title": "Fair Solution to the Reader-Writer-Problem with Semaphores only", 
    "publish": "2003-03-08T17:15:00Z", 
    "summary": "The reader-writer-problem is a standard problem in concurrent programming. A\nresource is shared by several processes which need either inclusive reading or\nexclusive writing access. The known solutions to this problem typically involve\na number of global counters and queues. Here a very simple algorithm is\npresented which needs only two semaphores for synchronisation and no other\nglobal objects. The approach yields a fair solution without starving.", 
    "link": "http://arxiv.org/pdf/cs/0303005v1", 
    "arxiv-id": "cs/0303005v1"
},{
    "category": "cs.DC", 
    "author": "Jennifer M. Schopf", 
    "title": "Using Regression Techniques to Predict Large Data Transfers", 
    "publish": "2003-04-23T20:36:09Z", 
    "summary": "The recent proliferation of Data Grids and the increasingly common practice\nof using resources as distributed data stores provide a convenient environment\nfor communities of researchers to share, replicate, and manage access to copies\nof large datasets. This has led to the question of which replica can be\naccessed most efficiently. In such environments, fetching data from one of the\nseveral replica locations requires accurate predictions of end-to-end transfer\ntimes. The answer to this question can depend on many factors, including\nphysical characteristics of the resources and the load behavior on the CPUs,\nnetworks, and storage devices that are part of the end-to-end data path linking\npossible sources and sinks. Our approach combines end-to-end application\nthroughput observations with network and disk load variations and captures\nwhole-system performance and variations in load patterns. Our predictions\ncharacterize the effect of load variations of several shared devices (network\nand disk) on file transfer times. We develop a suite of univariate and\nmultivariate predictors that can use multiple data sources to improve the\naccuracy of the predictions as well as address Data Grid variations\n(availability of data and sporadic nature of transfers). We ran a large set of\ndata transfer experiments using GridFTP and observed performance predictions\nwithin 15% error for our testbed sites, which is quite promising for a\npragmatic system.", 
    "link": "http://arxiv.org/pdf/cs/0304037v1", 
    "arxiv-id": "cs/0304037v1"
},{
    "category": "cs.DC", 
    "author": "Olof Barring", 
    "title": "Towards automation of computing fabrics using tools from the fabric   management workpackage of the EU DataGrid project", 
    "publish": "2003-05-28T16:25:45Z", 
    "summary": "The EU DataGrid project workpackage 4 has as an objective to provide the\nnecessary tools for automating the management of medium size to very large\ncomputing fabrics. At the end of the second project year subsystems for\ncentralized configuration management (presented at LISA'02) and\nperformance/exception monitoring have been delivered. This will soon be\naugmented with a subsystem for node installation and service configuration,\nwhich is based on existing widely used standards where available (e.g. rpm,\nkickstart, init.d scripts) and clean interfaces to OS dependent components\n(e.g. base installation and service management). The three subsystems together\nallow for centralized management of very large computer farms. Finally, a fault\ntolerance system is being developed for tying together the above subsystems to\nform a complete framework for automated enterprise computing management by\n3Q03. All software developed is open source covered by the EU DataGrid project\nlicense agreements. This article describes the architecture behind the designed\nfabric management system and the status of the different developments. It also\ncovers the experience with an existing tool for automated configuration and\ninstallation that have been adapted and used from the beginning to manage the\nEU DataGrid testbed, which is now used for LHC data challenges.", 
    "link": "http://arxiv.org/pdf/cs/0305050v2", 
    "arxiv-id": "cs/0305050v2"
},{
    "category": "cs.DC", 
    "author": "L. Vaccarossa", 
    "title": "ATLAS and CMS applications on the WorldGrid testbed", 
    "publish": "2003-05-30T09:18:55Z", 
    "summary": "WorldGrid is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. It has been developed in the context of the DataTAG and iVDGL\nprojects, and successfully demonstrated during the WorldGrid demos at IST2002\n(Copenhagen) and SC2002 (Baltimore). Two HEP experiments, ATLAS and CMS,\nsuccessful exploited the WorldGrid testbed for executing jobs simulating the\nresponse of their detectors to physics eve nts produced by real collisions\nexpected at the LHC accelerator starting from 2007. This data intensive\nactivity has been run since many years on local dedicated computing farms\nconsisting of hundreds of nodes and Terabytes of disk and tape storage. Within\nthe WorldGrid testbed, for the first time HEP simulation jobs were submitted\nand run indifferently on US and European resources, despite of their underlying\ndifferent Grid implementations, and produced data which could be retrieved and\nfurther analysed on the submitting machine, or simply stored on the remote\nresources and registered on a Replica Catalogue which made them available to\nthe Grid for further processing. In this contribution we describe the job\nsubmission from Europe for both ATLAS and CMS applications, performed through\nthe GENIUS portal operating on top of an EDG User Interface submitting to an\nEDG Resource Broker, pointing out the chosen interoperability solutions which\nmade US and European resources equivalent from the applications point of view,\nthe data management in the WorldGrid environment, and the CMS specific\nproduction tools which were interfaced to the GENIUS portal.", 
    "link": "http://arxiv.org/pdf/cs/0305058v2", 
    "arxiv-id": "cs/0305058v2"
},{
    "category": "cs.DC", 
    "author": "M. W. Schulz", 
    "title": "EU DataGRID testbed management and support at CERN", 
    "publish": "2003-05-30T10:07:12Z", 
    "summary": "In this paper we report on the first two years of running the CERN testbed\nsite for the EU DataGRID project. The site consists of about 120 dual-processor\nPCs distributed over several testbeds used for different purposes: software\ndevelopment, system integration, and application tests. Activities at the site\nincluded test productions of MonteCarlo data for LHC experiments, tutorials and\ndemonstrations of GRID technologies, and support for individual users analysis.\nThis paper focuses on node installation and configuration techniques, service\nmanagement, user support in a gridified environment, and includes\nconsiderations on scalability and security issues and comparisons with\n\"traditional\" production systems, as seen from the administrator point of view.", 
    "link": "http://arxiv.org/pdf/cs/0305059v1", 
    "arxiv-id": "cs/0305059v1"
},{
    "category": "cs.DC", 
    "author": "Markus Schulz", 
    "title": "A Secure Infrastructure For System Console and Reset Access", 
    "publish": "2003-05-30T11:42:37Z", 
    "summary": "During the last years large farms have been built using commodity hardware.\nThis hardware lacks components for remote and automated administration.\nProducts that can be retrofitted to these systems are either costly or\ninherently insecure. We present a system based on serial ports and simple\nmachine controlled relays. We report on experience gained by setting up a\n50-machine test environment as well as current work in progress in the area.", 
    "link": "http://arxiv.org/pdf/cs/0305061v1", 
    "arxiv-id": "cs/0305061v1"
},{
    "category": "cs.DC", 
    "author": "Harvey Newman", 
    "title": "DIAMOnDS - DIstributed Agents for MObile & Dynamic Services", 
    "publish": "2003-05-30T11:48:57Z", 
    "summary": "Distributed Services Architecture with support for mobile agents between\nservices, offer significantly improved communication and computational\nflexibility. The uses of agents allow execution of complex operations that\ninvolve large amounts of data to be processed effectively using distributed\nresources. The prototype system Distributed Agents for Mobile and Dynamic\nServices (DIAMOnDS), allows a service to send agents on its behalf, to other\nservices, to perform data manipulation and processing. Agents have been\nimplemented as mobile services that are discovered using the Jini Lookup\nmechanism and used by other services for task management and communication.\nAgents provide proxies for interaction with other services as well as specific\nGUI to monitor and control the agent activity. Thus agents acting on behalf of\none service cooperate with other services to carry out a job, providing\ninter-operation of loosely coupled services in a semi-autonomous way. Remote\nfile system access functionality has been incorporated by the agent framework\nand allows services to dynamically share and browse the file system resources\nof hosts, running the services. Generic database access functionality has been\nimplemented in the mobile agent framework that allows performing complex data\nmining and processing operations efficiently in distributed system. A basic\ndata searching agent is also implemented that performs a query based search in\na file system. The testing of the framework was carried out on WAN by moving\nConnectivity Test agents between AgentStations in CERN, Switzerland and NUST,\nPakistan.", 
    "link": "http://arxiv.org/pdf/cs/0305062v2", 
    "arxiv-id": "cs/0305062v2"
},{
    "category": "cs.DC", 
    "author": "Iain Bertram", 
    "title": "McRunjob: A High Energy Physics Workflow Planner for Grid Production   Processing", 
    "publish": "2003-05-30T19:53:14Z", 
    "summary": "McRunjob is a powerful grid workflow manager used to manage the generation of\nlarge numbers of production processing jobs in High Energy Physics. In use at\nboth the DZero and CMS experiments, McRunjob has been used to manage large\nMonte Carlo production processing since 1999 and is being extended to uses in\nregular production processing for analysis and reconstruction. Described at\nCHEP 2001, McRunjob converts core metadata into jobs submittable in a variety\nof environments. The powerful core metadata description language includes\nmethods for converting the metadata into persistent forms, job descriptions,\nmulti-step workflows, and data provenance information. The language features\nallow for structure in the metadata by including full expressions, namespaces,\nfunctional dependencies, site specific parameters in a grid environment, and\nontological definitions. It also has simple control structures for\nparallelization of large jobs. McRunjob features a modular design which allows\nfor easy expansion to new job description languages or new application level\ntasks.", 
    "link": "http://arxiv.org/pdf/cs/0305063v2", 
    "arxiv-id": "cs/0305063v2"
},{
    "category": "cs.DC", 
    "author": "Rainer Bartoldus", 
    "title": "A Generic Multi-node State Monitoring Subsystem", 
    "publish": "2003-05-30T17:43:03Z", 
    "summary": "The BaBar online data acquisition (DAQ) system includes approximately fifty\nUnix systems that collectively implement the level-three trigger. These systems\nall run the same code. Each of these systems has its own state, and this state\nis expected to change in response to changes in the overall DAQ system. A\nspecialized subsystem has been developed to initiate processing on this\ncollection of systems, and to monitor them both for error conditions and to\nensure that they all follow the same state trajectory within a specifiable\nperiod of time. This subsystem receives start commands from the main DAQ run\ncontrol system, and reports major coherent state changes, as well as error\nconditions, back to the run control system. This state monitoring subsystem has\nthe novel feature that it does not know anything about the state machines that\nit is monitoring, and hence does not introduce any fundamentally new state\nmachine into the overall system. This feature makes it trivially applicable to\nother multi-node subsystems. Indeed it has already found a second application\nbeyond the level-three trigger, within the BaBar experiment.", 
    "link": "http://arxiv.org/pdf/cs/0305065v1", 
    "arxiv-id": "cs/0305065v1"
},{
    "category": "cs.DC", 
    "author": "Todd Tannenbaum", 
    "title": "The CMS Integration Grid Testbed", 
    "publish": "2003-05-30T19:45:44Z", 
    "summary": "The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2\nhardware at the following sites: the California Institute of Technology, Fermi\nNational Accelerator Laboratory, the University of California at San Diego, and\nthe University of Florida at Gainesville. The IGT runs jobs using the Globus\nToolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is\nmanaged using VO management scripts from the European Data Grid (EDG). Gridwide\nmonitoring is accomplished using local tools such as Ganglia interfaced into\nthe Globus Metadata Directory Service (MDS) and the agent based Mona Lisa.\nDomain specific software is packaged and installed using the Distrib ution\nAfter Release (DAR) tool of CMS, while middleware under the auspices of the\nVirtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us\ntwo month span in Fall of 2002, over 1 million official CMS GEANT based Monte\nCarlo events were generated and returned to CERN for analysis while being\ndemonstrated at SC2002. In this paper, we describe the process that led to one\nof the world's first continuously available, functioning grids.", 
    "link": "http://arxiv.org/pdf/cs/0305066v2", 
    "arxiv-id": "cs/0305066v2"
},{
    "category": "cs.DC", 
    "author": "Frank van Lingen", 
    "title": "Clarens Client and Server Applications", 
    "publish": "2003-05-30T20:25:50Z", 
    "summary": "Several applications have been implemented with access via the Clarens web\nservice infrastructure, including virtual organization management, JetMET\nphysics data analysis using relational databases, and Storage Resource Broker\n(SRB) access. This functionality is accessible transparently from Python\nscripts, the Root analysis framework and from Java applications and browser\napplets.", 
    "link": "http://arxiv.org/pdf/cs/0306001v2", 
    "arxiv-id": "cs/0306001v2"
},{
    "category": "cs.DC", 
    "author": "Frank van Lingen", 
    "title": "The Clarens web services architecture", 
    "publish": "2003-05-30T20:34:05Z", 
    "summary": "Clarens is a uniquely flexible web services infrastructure providing a\nunified access protocol to a diverse set of functions useful to the HEP\ncommunity. It uses the standard HTTP protocol combined with application layer,\ncertificate based authentication to provide single sign-on to individuals,\norganizations and hosts, with fine-grained access control to services, files\nand virtual organization (VO) management. This contribution describes the\nserver functionality, while client applications are described in a subsequent\ntalk.", 
    "link": "http://arxiv.org/pdf/cs/0306002v2", 
    "arxiv-id": "cs/0306002v2"
},{
    "category": "cs.DC", 
    "author": "Xiaomei Zhu", 
    "title": "R-GMA: First results after deployment", 
    "publish": "2003-05-30T20:39:27Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which is being\ndeveloped within the European DataGrid Project as an Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each VO had one large relational database. We provide a number of different\nProducer types with different characteristics; for example some support\nstreaming of information. We also provide combined Consumer/Producers, which\nare able to combine information and republish it. At the heart of the system is\nthe mediator, which for any query is able to find and connect to the best\nProducers to do the job. We are able to invoke MDS info-provider scripts and\npublish the resulting information via R-GMA in addition to having some of our\nown sensors. APIs are available which allow the user to deploy monitoring and\ninformation services for any application that may be needed in the future. We\nhave used it both for information about the grid (primarily to find what\nservices are available at any one time) and for application monitoring. R-GMA\nhas been deployed in Grid testbeds, we describe the results and experiences of\nthis deployment.", 
    "link": "http://arxiv.org/pdf/cs/0306003v2", 
    "arxiv-id": "cs/0306003v2"
},{
    "category": "cs.DC", 
    "author": "K. Lhorentey", 
    "title": "Managing Dynamic User Communities in a Grid of Autonomous Resources", 
    "publish": "2003-05-30T21:04:49Z", 
    "summary": "One of the fundamental concepts in Grid computing is the creation of Virtual\nOrganizations (VO's): a set of resource consumers and providers that join\nforces to solve a common problem. Typical examples of Virtual Organizations\ninclude collaborations formed around the Large Hadron Collider (LHC)\nexperiments. To date, Grid computing has been applied on a relatively small\nscale, linking dozens of users to a dozen resources, and management of these\nVO's was a largely manual operation. With the advance of large collaboration,\nlinking more than 10000 users with a 1000 sites in 150 counties, a\ncomprehensive, automated management system is required. It should be simple\nenough not to deter users, while at the same time ensuring local site autonomy.\nThe VO Management Service (VOMS), developed by the EU DataGrid and DataTAG\nprojects[1, 2], is a secured system for managing authorization for users and\nresources in virtual organizations. It extends the existing Grid Security\nInfrastructure[3] architecture with embedded VO affiliation assertions that can\nbe independently verified by all VO members and resource providers. Within the\nEU DataGrid project, Grid services for job submission, file- and database\naccess are being equipped with fine- grained authorization systems that take VO\nmembership into account. These also give resource owners the ability to ensure\nsite security and enforce local access policies. This paper will describe the\nEU DataGrid security architecture, the VO membership service and the local site\nenforcement mechanisms Local Centre Authorization Service (LCAS), Local\nCredential Mapping Service(LCMAPS) and the Java Trust and Authorization\nManager.", 
    "link": "http://arxiv.org/pdf/cs/0306004v2", 
    "arxiv-id": "cs/0306004v2"
},{
    "category": "cs.DC", 
    "author": "A. Werbrouck", 
    "title": "The first deployment of workload management services on the EU DataGrid   Testbed: feedback on design and implementation", 
    "publish": "2003-05-31T19:39:54Z", 
    "summary": "Application users have now been experiencing for about a year with the\nstandardized resource brokering services provided by the 'workload management'\npackage of the EU DataGrid project (WP1). Understanding, shaping and pushing\nthe limits of the system has provided valuable feedback on both its design and\nimplementation. A digest of the lessons, and \"better practices\", that were\nlearned, and that were applied towards the second major release of the\nsoftware, is given.", 
    "link": "http://arxiv.org/pdf/cs/0306007v1", 
    "arxiv-id": "cs/0306007v1"
},{
    "category": "cs.DC", 
    "author": "T. M. Pulliam", 
    "title": "The new BaBar Data Reconstruction Control System", 
    "publish": "2003-05-31T20:48:53Z", 
    "summary": "The BaBar experiment is characterized by extremely high luminosity, a complex\ndetector, and a huge data volume, with increasing requirements each year. To\nfulfill these requirements a new control system has been designed and developed\nfor the offline data reconstruction system. The new control system described in\nthis paper provides the performance and flexibility needed to manage a large\nnumber of small computing farms, and takes full benefit of OO design. The\ninfrastructure is well isolated from the processing layer, it is generic and\nflexible, based on a light framework providing message passing and cooperative\nmultitasking. The system is actively distributed, enforces the separation\nbetween different processing tiers by using different naming domains, and glues\nthem together by dedicated brokers. It provides a powerful Finite State Machine\nframework to describe custom processing models in a simple regular language.\nThis paper describes this new control system, currently in use at SLAC and\nPadova on ~450 CPUs organized in 12 farms.", 
    "link": "http://arxiv.org/pdf/cs/0306008v3", 
    "arxiv-id": "cs/0306008v3"
},{
    "category": "cs.DC", 
    "author": "Paul Millar", 
    "title": "Grid Data Management in Action: Experience in Running and Supporting   Data Management Services in the EU DataGrid Project", 
    "publish": "2003-06-02T08:50:48Z", 
    "summary": "In the first phase of the EU DataGrid (EDG) project, a Data Management System\nhas been implemented and provided for deployment. The components of the current\nEDG Testbed are: a prototype of a Replica Manager Service built around the\nbasic services provided by Globus, a centralised Replica Catalogue to store\ninformation about physical locations of files, and the Grid Data Mirroring\nPackage (GDMP) that is widely used in various HEP collaborations in Europe and\nthe US for data mirroring. During this year these services have been refined\nand made more robust so that they are fit to be used in a pre-production\nenvironment. Application users have been using this first release of the Data\nManagement Services for more than a year. In the paper we present the\ncomponents and their interaction, our implementation and experience as well as\nthe feedback received from our user communities. We have resolved not only\nissues regarding integration with other EDG service components but also many of\nthe interoperability issues with components of our partner projects in Europe\nand the U.S. The paper concludes with the basic lessons learned during this\noperation. These conclusions provide the motivation for the architecture of the\nnext generation of Data Management Services that will be deployed in EDG during\n2003.", 
    "link": "http://arxiv.org/pdf/cs/0306011v1", 
    "arxiv-id": "cs/0306011v1"
},{
    "category": "cs.DC", 
    "author": "G. Tortone", 
    "title": "A monitoring tool for a GRID operation center", 
    "publish": "2003-06-03T17:22:07Z", 
    "summary": "WorldGRID is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. The WorldGRID testbed has been successfully demonstrated during the\nWorldGRID demos at SuperComputing 2002 (Baltimore) and IST2002 (Copenhagen)\nwhere real HEP application jobs were transparently submitted from US and Europe\nusing \"native\" mechanisms and run where resources were available, independently\nof their location. To monitor the behavior and performance of such testbed and\nspot problems as soon as they arise, DataTAG has developed the EDT-Monitor tool\nbased on the Nagios package that allows for Virtual Organization centric views\nof the Grid through dynamic geographical maps. The tool has been used to spot\nseveral problems during the WorldGRID operations, such as malfunctioning\nResource Brokers or Information Servers, sites not correctly configured, job\ndispatching problems, etc. In this paper we give an overview of the package,\nits features and scalability solutions and we report on the experience acquired\nand the benefit that a GRID operation center would gain from such a tool.", 
    "link": "http://arxiv.org/pdf/cs/0306018v1", 
    "arxiv-id": "cs/0306018v1"
},{
    "category": "cs.DC", 
    "author": "S. Burke", 
    "title": "HEP Applications Evaluation of the EDG Testbed and Middleware", 
    "publish": "2003-06-05T15:35:20Z", 
    "summary": "Workpackage 8 of the European Datagrid project was formed in January 2001\nwith representatives from the four LHC experiments, and with experiment\nindependent people from five of the six main EDG partners. In September 2002\nWP8 was strengthened by the addition of effort from BaBar and D0. The original\nmandate of WP8 was, following the definition of short- and long-term\nrequirements, to port experiment software to the EDG middleware and testbed\nenvironment. A major additional activity has been testing the basic\nfunctionality and performance of this environment. This paper reviews\nexperiences and evaluations in the areas of job submission, data management,\nmass storage handling, information systems and monitoring. It also comments on\nthe problems of remote debugging, the portability of code, and scaling problems\nwith increasing numbers of jobs, sites and nodes. Reference is made to the\npioneeering work of Atlas and CMS in integrating the use of the EDG Testbed\ninto their data challenges. A forward look is made to essential software\ndevelopments within EDG and to the necessary cooperation between EDG and LCG\nfor the LCG prototype due in mid 2003.", 
    "link": "http://arxiv.org/pdf/cs/0306027v1", 
    "arxiv-id": "cs/0306027v1"
},{
    "category": "cs.DC", 
    "author": "Heinz Tilsner", 
    "title": "A Software Data Transport Framework for Trigger Applications on Clusters", 
    "publish": "2003-06-06T07:02:25Z", 
    "summary": "In the future ALICE heavy ion experiment at CERN's Large Hadron Collider\ninput data rates of up to 25 GB/s have to be handled by the High Level Trigger\n(HLT) system, which has to scale them down to at most 1.25 GB/s before being\nwritten to permanent storage. The HLT system that is being designed to cope\nwith these data rates consists of a large PC cluster, up to the order of a 1000\nnodes, connected by a fast network. For the software that will run on these\nnodes a flexible data transport and distribution software framework has been\ndeveloped. This framework consists of a set of separate components, that can be\nconnected via a common interface, allowing to construct different\nconfigurations for the HLT, that are even changeable at runtime. To ensure a\nfault-tolerant operation of the HLT, the framework includes a basic fail-over\nmechanism that will be further expanded in the future, utilizing the runtime\nreconnection feature of the framework's component interface. First performance\ntests show very promising results for the software, indicating that it can\nachieve an event rate for the data transport sufficiently high to satisfy\nALICE's requirements.", 
    "link": "http://arxiv.org/pdf/cs/0306029v1", 
    "arxiv-id": "cs/0306029v1"
},{
    "category": "cs.DC", 
    "author": "A. McNab", 
    "title": "Grid-based access control for Unix environments, Filesystems and Web   Sites", 
    "publish": "2003-06-06T10:45:43Z", 
    "summary": "The EU DataGrid has deployed a grid testbed at approximately 20 sites across\nEurope, with several hundred registered users. This paper describes\nauthorisation systems produced by GridPP and currently used on the EU DataGrid\nTestbed, including local Unix pool accounts and fine-grained access control\nwith Access Control Lists and Grid-aware filesystems, fileservers and web\ndevelopement environments.", 
    "link": "http://arxiv.org/pdf/cs/0306030v1", 
    "arxiv-id": "cs/0306030v1"
},{
    "category": "cs.DC", 
    "author": "Marco Verlato", 
    "title": "The WorldGrid transatlantic testbed: a successful example of Grid   interoperability across EU and U.S. domains", 
    "publish": "2003-06-11T16:47:17Z", 
    "summary": "The European DataTAG project has taken a major step towards making the\nconcept of a worldwide computing Grid a reality. In collaboration with the\ncompanion U.S. project iVDGL, DataTAG has realized an intercontinental testbed\nspanning Europe and the U.S. integrating architecturally different Grid\nimplementations based on the Globus toolkit. The WorldGrid testbed has been\nsuccessfully demonstrated at SuperComputing 2002 and IST2002 where real HEP\napplication jobs were transparently submitted from U.S. and Europe using native\nmechanisms and run where resources were available, independently of their\nlocation. In this paper we describe the architecture of the WorldGrid testbed,\nthe problems encountered and the solutions taken in realizing such a testbed.\nWith our work we present an important step towards interoperability of Grid\nmiddleware developed and deployed in Europe and the U.S.. Some of the solutions\ndeveloped in WorldGrid will be adopted by the LHC Computing Grid first service.\nTo the best of our knowledge, this is the first large-scale testbed that\ncombines middleware components and makes them work together.", 
    "link": "http://arxiv.org/pdf/cs/0306045v1", 
    "arxiv-id": "cs/0306045v1"
},{
    "category": "cs.DC", 
    "author": "Rob Latham", 
    "title": "Parallel netCDF: A Scientific High-Performance I/O Interface", 
    "publish": "2003-06-11T20:25:52Z", 
    "summary": "Dataset storage, exchange, and access play a critical role in scientific\napplications. For such purposes netCDF serves as a portable and efficient file\nformat and programming interface, which is popular in numerous scientific\napplication domains. However, the original interface does not provide an\nefficient mechanism for parallel data storage and access. In this work, we\npresent a new parallel interface for writing and reading netCDF datasets. This\ninterface is derived with minimum changes from the serial netCDF interface but\ndefines semantics for parallel access and is tailored for high performance. The\nunderlying parallel I/O is achieved through MPI-IO, allowing for dramatic\nperformance gains through the use of collective I/O optimizations. We compare\nthe implementation strategies with HDF5 and analyze both. Our tests indicate\nprogramming convenience and significant I/O performance improvement with this\nparallel netCDF interface.", 
    "link": "http://arxiv.org/pdf/cs/0306048v1", 
    "arxiv-id": "cs/0306048v1"
},{
    "category": "cs.DC", 
    "author": "Shigeo Yashiro", 
    "title": "A data Grid testbed environment in Gigabit WAN with HPSS", 
    "publish": "2003-06-12T08:48:16Z", 
    "summary": "For data analysis of large-scale experiments such as LHC Atlas and other\nJapanese high energy and nuclear physics projects, we have constructed a Grid\ntest bed at ICEPP and KEK. These institutes are connected to national\nscientific gigabit network backbone called SuperSINET. In our test bed, we have\ninstalled NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK\nas a large scale data store. Atlas simulation data at ICEPP has been\ntransferred and accessed using SuperSINET. We have tested various performances\nand characteristics of HPSS through this high speed WAN. The measurement\nincludes comparison between computing and storage resources are tightly coupled\nwith low latency LAN and long distant WAN.", 
    "link": "http://arxiv.org/pdf/cs/0306051v2", 
    "arxiv-id": "cs/0306051v2"
},{
    "category": "cs.DC", 
    "author": "Gilbert Poulard", 
    "title": "ATLAS Data Challenge 1", 
    "publish": "2003-06-12T12:59:39Z", 
    "summary": "In 2002 the ATLAS experiment started a series of Data Challenges (DC) of\nwhich the goals are the validation of the Computing Model, of the complete\nsoftware suite, of the data model, and to ensure the correctness of the\ntechnical choices to be made. A major feature of the first Data Challenge (DC1)\nwas the preparation and the deployment of the software required for the\nproduction of large event samples for the High Level Trigger (HLT) and physics\ncommunities, and the production of those samples as a world-wide distributed\nactivity. The first phase of DC1 was run during summer 2002, and involved 39\ninstitutes in 18 countries. More than 10 million physics events and 30 million\nsingle particle events were fully simulated. Over a period of about 40 calendar\ndays 71000 CPU-days were used producing 30 Tbytes of data in about 35000\npartitions. In the second phase the next processing step was performed with the\nparticipation of 56 institutes in 21 countries (~ 4000 processors used in\nparallel). The basic elements of the ATLAS Monte Carlo production system are\ndescribed. We also present how the software suite was validated and the\nparticipating sites were certified. These productions were already partly\nperformed by using different flavours of Grid middleware at ~ 20 sites.", 
    "link": "http://arxiv.org/pdf/cs/0306052v1", 
    "arxiv-id": "cs/0306052v1"
},{
    "category": "cs.DC", 
    "author": "David Bengali", 
    "title": "BlueOx: A Java Framework for Distributed Data Analysis", 
    "publish": "2003-06-12T15:53:17Z", 
    "summary": "High energy physics experiments including those at the Tevatron and the\nupcoming LHC require analysis of large data sets which are best handled by\ndistributed computation. We present the design and development of a distributed\ndata analysis framework based on Java. Analysis jobs run through three phases:\ndiscovery of data sets available, brokering/assignment of data sets to analysis\nservers, and job execution. Each phase is represented by a set of abstract\ninterfaces. These interfaces allow different techniques to be used without\nmodification to the framework. For example, the communications interface has\nbeen implemented by both a packet protocol and a SOAP-based scheme. User\nauthentication can be provided either through simple passwords or through a GSI\ncertificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a\nhypothetical high-energy linear collider experiment have been interfaced with\nthe framework.", 
    "link": "http://arxiv.org/pdf/cs/0306055v1", 
    "arxiv-id": "cs/0306055v1"
},{
    "category": "cs.DC", 
    "author": "Tim Smith", 
    "title": "Installing, Running and Maintaining Large Linux Clusters at CERN", 
    "publish": "2003-06-12T18:45:34Z", 
    "summary": "Having built up Linux clusters to more than 1000 nodes over the past five\nyears, we already have practical experience confronting some of the LHC scale\ncomputing challenges: scalability, automation, hardware diversity, security,\nand rolling OS upgrades. This paper describes the tools and processes we have\nimplemented, working in close collaboration with the EDG project [1],\nespecially with the WP4 subtask, to improve the manageability of our clusters,\nin particular in the areas of system installation, configuration, and\nmonitoring. In addition to the purely technical issues, providing shared\ninteractive and batch services which can adapt to meet the diverse and changing\nrequirements of our users is a significant challenge. We describe the\ndevelopments and tuning that we have introduced on our LSF based systems to\nmaximise both responsiveness to users and overall system utilisation. Finally,\nthis paper will describe the problems we are facing in enlarging our\nheterogeneous Linux clusters, the progress we have made in dealing with the\ncurrent issues and the steps we are taking to gridify the clusters", 
    "link": "http://arxiv.org/pdf/cs/0306058v1", 
    "arxiv-id": "cs/0306058v1"
},{
    "category": "cs.DC", 
    "author": "A. Washbrook", 
    "title": "DIRAC - Distributed Infrastructure with Remote Agent Control", 
    "publish": "2003-06-12T23:54:24Z", 
    "summary": "This paper describes DIRAC, the LHCb Monte Carlo production system. DIRAC has\na client/server architecture based on: Compute elements distributed among the\ncollaborating institutes; Databases for production management, bookkeeping (the\nmetadata catalogue) and software configuration; Monitoring and cataloguing\nservices for updating and accessing the databases. Locally installed software\nagents implemented in Python monitor the local batch queue, interrogate the\nproduction database for any outstanding production requests using the XML-RPC\nprotocol and initiate the job submission. The agent checks and, if necessary,\ninstalls any required software automatically. After the job has processed the\nevents, the agent transfers the output data and updates the metadata catalogue.\nDIRAC has been successfully installed at 18 collaborating institutes, including\nthe DataGRID, and has been used in recent Physics Data Challenges. In the near\nto medium term future we must use a mixed environment with different types of\ngrid middleware or no middleware. We describe how this flexibility has been\nachieved and how ubiquitously available grid middleware would improve DIRAC.", 
    "link": "http://arxiv.org/pdf/cs/0306060v1", 
    "arxiv-id": "cs/0306060v1"
},{
    "category": "cs.DC", 
    "author": "Tomasz Wlodek", 
    "title": "A Model for Grid User Management", 
    "publish": "2003-06-13T17:01:45Z", 
    "summary": "Registration and management of users in a large scale Grid computing\nenvironment presents new challenges that are not well addressed by existing\nprotocols. Within a single Virtual Organization (VO), thousands of users will\npotentially need access to hundreds of computing sites, and the traditional\nmodel where users register for local accounts at each site will present\nsignificant scaling problems. However, computing sites must maintain control\nover access to the site and site policies generally require individual local\naccounts for every user. We present here a model that allows users to register\nonce with a VO and yet still provides all of the computing sites the\ninformation they require with the required level of trust. We have developed\ntools to allow sites to automate the management of local accounts and the\nmappings between Grid identities and local accounts.", 
    "link": "http://arxiv.org/pdf/cs/0306063v1", 
    "arxiv-id": "cs/0306063v1"
},{
    "category": "cs.DC", 
    "author": "Arshad Ali", 
    "title": "Exploiting peer group concept for adaptive and highly available services", 
    "publish": "2003-06-13T13:38:01Z", 
    "summary": "This paper presents a prototype for redundant, highly available and fault\ntolerant peer to peer framework for data management. Peer to peer computing is\ngaining importance due to its flexible organization, lack of central authority,\ndistribution of functionality to participating nodes and ability to utilize\nunused computational resources. Emergence of GRID computing has provided much\nneeded infrastructure and administrative domain for peer to peer computing. The\ncomponents of this framework exploit peer group concept to scope service and\ninformation search, arrange services and information in a coherent manner,\nprovide selective redundancy and ensure availability in face of failure and\nhigh load conditions. A prototype system has been implemented using JXTA peer\nto peer technology and XML is used for service description and interfaces,\nallowing peers to communicate with services implemented in various platforms\nincluding web services and JINI services. It utilizes code mobility to achieve\nrole interchange among services and ensure dynamic group membership. Security\nis ensured by using Public Key Infrastructure (PKI) to implement group level\nsecurity policies for membership and service access.", 
    "link": "http://arxiv.org/pdf/cs/0306064v2", 
    "arxiv-id": "cs/0306064v2"
},{
    "category": "cs.DC", 
    "author": "A. J. Peters", 
    "title": "The AliEn system, status and perspectives", 
    "publish": "2003-06-13T15:43:15Z", 
    "summary": "AliEn is a production environment that implements several components of the\nGrid paradigm needed to simulate, reconstruct and analyse HEP data in a\ndistributed way. The system is built around Open Source components, uses the\nWeb Services model and standard network protocols to implement the computing\nplatform that is currently being used to produce and analyse Monte Carlo data\nat over 30 sites on four continents. The aim of this paper is to present the\ncurrent AliEn architecture and outline its future developments in the light of\nemerging standards.", 
    "link": "http://arxiv.org/pdf/cs/0306067v1", 
    "arxiv-id": "cs/0306067v1"
},{
    "category": "cs.DC", 
    "author": "Andreas J. Peters", 
    "title": "AliEn Resource Brokers", 
    "publish": "2003-06-13T16:00:45Z", 
    "summary": "AliEn (ALICE Environment) is a lightweight GRID framework developed by the\nAlice Collaboration. When the experiment starts running, it will collect data\nat a rate of approximately 2 PB per year, producing O(109) files per year. All\nthese files, including all simulated events generated during the preparation\nphase of the experiment, must be accounted and reliably tracked in the GRID\nenvironment. The backbone of AliEn is a distributed file catalogue, which\nassociates universal logical file name to physical file names for each dataset\nand provides transparent access to datasets independently of physical location.\nThe file replication and transport is carried out under the control of the File\nTransport Broker. In addition, the file catalogue maintains information about\nevery job running in the system. The jobs are distributed by the Job Resource\nBroker that is implemented using a simplified pull (as opposed to traditional\npush) architecture. This paper describes the Job and File Transport Resource\nBrokers and shows that a similar architecture can be applied to solve both\nproblems.", 
    "link": "http://arxiv.org/pdf/cs/0306068v1", 
    "arxiv-id": "cs/0306068v1"
},{
    "category": "cs.DC", 
    "author": "Alvise Dorigo", 
    "title": "Distributed Offline Data Reconstruction in BaBar", 
    "publish": "2003-06-13T16:16:44Z", 
    "summary": "The BaBar experiment at SLAC is in its fourth year of running. The data\nprocessing system has been continuously evolving to meet the challenges of\nhigher luminosity running and the increasing bulk of data to re-process each\nyear. To meet these goals a two-pass processing architecture has been adopted,\nwhere 'rolling calibrations' are quickly calculated on a small fraction of the\nevents in the first pass and the bulk data reconstruction done in the second.\nThis allows for quick detector feedback in the first pass and allows for the\nparallelization of the second pass over two or more separate farms. This\ntwo-pass system allows also for distribution of processing farms off-site. The\nfirst such site has been setup at INFN Padova. The challenges met here were\nmany. The software was ported to a full Linux-based, commodity hardware system.\nThe raw dataset, 90 TB, was imported from SLAC utilizing a 155 Mbps network\nlink. A system for quality control and export of the processed data back to\nSLAC was developed. Between SLAC and Padova we are currently running three\npass-one farms, with 32 CPUs each, and nine pass-two farms with 64 to 80 CPUs\neach. The pass-two farms can process between 2 and 4 million events per day.\nDetails about the implementation and performance of the system will be\npresented.", 
    "link": "http://arxiv.org/pdf/cs/0306069v1", 
    "arxiv-id": "cs/0306069v1"
},{
    "category": "cs.DC", 
    "author": "P. Buncic", 
    "title": "AliEnFS - a Linux File System for the AliEn Grid Services", 
    "publish": "2003-06-13T18:18:59Z", 
    "summary": "Among the services offered by the AliEn (ALICE Environment\nhttp://alien.cern.ch) Grid framework there is a virtual file catalogue to allow\ntransparent access to distributed data-sets using various file transfer\nprotocols. $alienfs$ (AliEn File System) integrates the AliEn file catalogue as\na new file system type into the Linux kernel using LUFS, a hybrid user space\nfile system framework (Open Source http://lufs.sourceforge.net). LUFS uses a\nspecial kernel interface level called VFS (Virtual File System Switch) to\ncommunicate via a generalised file system interface to the AliEn file system\ndaemon. The AliEn framework is used for authentication, catalogue browsing,\nfile registration and read/write transfer operations. A C++ API implements the\ngeneric file system operations. The goal of AliEnFS is to allow users easy\ninteractive access to a worldwide distributed virtual file system using\nfamiliar shell commands (f.e. cp,ls,rm ...) The paper discusses general aspects\nof Grid File Systems, the AliEn implementation and present and future\ndevelopments for the AliEn Grid File System.", 
    "link": "http://arxiv.org/pdf/cs/0306071v1", 
    "arxiv-id": "cs/0306071v1"
},{
    "category": "cs.DC", 
    "author": "A. Werbrouck", 
    "title": "The EU DataGrid Workload Management System: towards the second major   release", 
    "publish": "2003-06-13T18:57:35Z", 
    "summary": "In the first phase of the European DataGrid project, the 'workload\nmanagement' package (WP1) implemented a working prototype, providing users with\nan environment allowing to define and submit jobs to the Grid, and able to find\nand use the ``best'' resources for these jobs. Application users have now been\nexperiencing for about a year now with this first release of the workload\nmanagement system. The experiences acquired, the feedback received by the user\nand the need to plug new components implementing new functionalities, triggered\nan update of the existing architecture. A description of this revised and\ncomplemented workload management system is given.", 
    "link": "http://arxiv.org/pdf/cs/0306072v1", 
    "arxiv-id": "cs/0306072v1"
},{
    "category": "cs.DC", 
    "author": "Jim Kowalkowski", 
    "title": "Understanding and Coping with Hardware and Software Failures in a Very   Large Trigger Farm", 
    "publish": "2003-06-13T21:24:05Z", 
    "summary": "When thousands of processors are involved in performing event filtering on a\ntrigger farm, there is likely to be a large number of failures within the\nsoftware and hardware systems. BTeV, a proton/antiproton collider experiment at\nFermi National Accelerator Laboratory, has designed a trigger, which includes\nseveral thousand processors. If fault conditions are not given proper\ntreatment, it is conceivable that this trigger system will experience failures\nat a high enough rate to have a negative impact on its effectiveness. The RTES\n(Real Time Embedded Systems) collaboration is a group of physicists, engineers,\nand computer scientists working to address the problem of reliability in\nlarge-scale clusters with real-time constraints such as this. Resulting\ninfrastructure must be highly scalable, verifiable, extensible by users, and\ndynamically changeable.", 
    "link": "http://arxiv.org/pdf/cs/0306074v1", 
    "arxiv-id": "cs/0306074v1"
},{
    "category": "cs.DC", 
    "author": "Qiu Zhiping", 
    "title": "Data Management for Physics Analysis in Phenix (BNL, RHIC)", 
    "publish": "2003-06-13T21:24:55Z", 
    "summary": "Every year the PHENIX collaboration deals with increasing volume of data (now\nabout 1/4 PB/year). Apparently the more data the more questions how to process\nall the data in most efficient way. In recent past many developments in HEP\ncomputing were dedicated to the production environment. Now we need more tools\nto help to obtain physics results from the analysis of distributed simulated\nand experimental data. Developments in Grid architectures gave many examples\nhow distributed computing facilities can be organized to meet physics analysis\nneeds. We feel that our main task in this area is to try to use already\ndeveloped systems or system components in PHENIX environment.\n  We are concentrating here on the followed problems: file/replica catalog\nwhich keep names of our files, data moving over WAN, job submission in\nmulticluster environment.\n  PHENIX is a running experiment and this fact narrowed our ability to test new\nsoftware on the collaboration computer facilities. We are experimenting with\nsystem prototypes at State University of New York at Stony Brook (SUNYSB) where\nwe run midrange computing cluster for physics analysis. The talk is dedicated\nto discuss some experience with Grid software and achieved results.", 
    "link": "http://arxiv.org/pdf/cs/0306075v1", 
    "arxiv-id": "cs/0306075v1"
},{
    "category": "cs.DC", 
    "author": "T. Adye", 
    "title": "BaBar Web job submission with Globus authentication and AFS access", 
    "publish": "2003-06-14T02:39:07Z", 
    "summary": "We present two versions of a grid job submission system produced for the\nBaBar experiment. Both use globus job submission to process data spread across\nvarious sites, producing output which can be combined for analysis. The\nproblems encountered with authorisation and authentication, data location, job\nsubmission, and the input and output sandboxes are described, as are the\nsolutions. The total system is still some way short of the aims of enterprises\nsuch as the EDG, but represent a significant step along the way.", 
    "link": "http://arxiv.org/pdf/cs/0306084v1", 
    "arxiv-id": "cs/0306084v1"
},{
    "category": "cs.DC", 
    "author": "A. Dell'Acqua", 
    "title": "Building A High Performance Parallel File System Using Grid Datafarm and   ROOT I/O", 
    "publish": "2003-06-14T16:29:16Z", 
    "summary": "Sheer amount of petabyte scale data foreseen in the LHC experiments require a\ncareful consideration of the persistency design and the system design in the\nworld-wide distributed computing. Event parallelism of the HENP data analysis\nenables us to take maximum advantage of the high performance cluster computing\nand networking when we keep the parallelism both in the data processing phase,\nin the data management phase, and in the data transfer phase. A modular\narchitecture of FADS/ Goofy, a versatile detector simulation framework for\nGeant4, enables an easy choice of plug-in facilities for persistency\ntechnologies such as Objectivity/DB and ROOT I/O. The framework is designed to\nwork naturally with the parallel file system of Grid Datafarm (Gfarm).\nFADS/Goofy is proven to generate 10^6 Geant4-simulated Atlas Mockup events\nusing a 512 CPU PC cluster. The data in ROOT I/O files is replicated using\nGfarm file system. The histogram information is collected from the distributed\nROOT files. During the data replication it has been demonstrated to achieve\nmore than 2.3 Gbps data transfer rate between the PC clusters over seven\nparticipating PC clusters in the United States and in Japan.", 
    "link": "http://arxiv.org/pdf/cs/0306092v1", 
    "arxiv-id": "cs/0306092v1"
},{
    "category": "cs.DC", 
    "author": "Jaime E. Villate", 
    "title": "Grid-Brick Event Processing Framework in GEPS", 
    "publish": "2003-06-14T22:33:35Z", 
    "summary": "Experiments like ATLAS at LHC involve a scale of computing and data\nmanagement that greatly exceeds the capability of existing systems, making it\nnecessary to resort to Grid-based Parallel Event Processing Systems (GEPS).\nTraditional Grid systems concentrate the data in central data servers which\nhave to be accessed by many nodes each time an analysis or processing job\nstarts. These systems require very powerful central data servers and make\nlittle use of the distributed disk space that is available in commodity\ncomputers. The Grid-Brick system, which is described in this paper, follows a\ndifferent approach. The data storage is split among all grid nodes having each\none a piece of the whole information. Users submit queries and the system will\ndistribute the tasks through all the nodes and retrieve the result, merging\nthem together in the Job Submit Server. The main advantage of using this system\nis the huge scalability it provides, while its biggest disadvantage appears in\nthe case of failure of one of the nodes. A workaround for this problem involves\ndata replication or backup.", 
    "link": "http://arxiv.org/pdf/cs/0306093v1", 
    "arxiv-id": "cs/0306093v1"
},{
    "category": "cs.DC", 
    "author": "C. Cirstoiu", 
    "title": "MonALISA : A Distributed Monitoring Service Architecture", 
    "publish": "2003-06-16T08:33:44Z", 
    "summary": "The MonALISA (Monitoring Agents in A Large Integrated Services Architecture)\nsystem provides a distributed monitoring service. MonALISA is based on a\nscalable Dynamic Distributed Services Architecture which is designed to meet\nthe needs of physics collaborations for monitoring global Grid systems, and is\nimplemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the\nsystem derives from the use of multithreaded Station Servers to host a variety\nof loosely coupled self-describing dynamic services, the ability of each\nservice to register itself and then to be discovered and used by any other\nservices, or clients that require such information, and the ability of all\nservices and clients subscribing to a set of events (state changes) in the\nsystem to be notified automatically. The framework integrates several existing\nmonitoring tools and procedures to collect parameters describing computational\nnodes, applications and network performance. It has built-in SNMP support and\nnetwork-performance monitoring algorithms that enable it to monitor end-to-end\nnetwork performance as well as the performance and state of site facilities in\na Grid. MonALISA is currently running around the clock on the US CMS test Grid\nas well as an increasing number of other sites. It is also being used to\nmonitor the performance and optimize the interconnections among the reflectors\nin the VRVS system.", 
    "link": "http://arxiv.org/pdf/cs/0306096v1", 
    "arxiv-id": "cs/0306096v1"
},{
    "category": "cs.DC", 
    "author": "Dane Skow", 
    "title": "Site Authorization Service (SAZ)", 
    "publish": "2003-06-16T16:07:24Z", 
    "summary": "In this paper we present a methodology to provide an additional level of\ncentralized control for the grid resources. This centralized control is applied\nto site-wide distribution of various grids and thus providing an upper hand in\nthe maintenance.", 
    "link": "http://arxiv.org/pdf/cs/0306100v1", 
    "arxiv-id": "cs/0306100v1"
},{
    "category": "cs.DC", 
    "author": "L. Zangrando", 
    "title": "Run Control and Monitor System for the CMS Experiment", 
    "publish": "2003-06-18T16:34:11Z", 
    "summary": "The Run Control and Monitor System (RCMS) of the CMS experiment is the set of\nhardware and software components responsible for controlling and monitoring the\nexperiment during data-taking. It provides users with a \"virtual counting\nroom\", enabling them to operate the experiment and to monitor detector status\nand data quality from any point in the world. This paper describes the\narchitecture of the RCMS with particular emphasis on its scalability through a\ndistributed collection of nodes arranged in a tree-based hierarchy. The current\nimplementation of the architecture in a prototype RCMS used in test beam\nsetups, detector validations and DAQ demonstrators is documented. A discussion\nof the key technologies used, including Web Services, and the results of tests\nperformed with a 128-node system are presented.", 
    "link": "http://arxiv.org/pdf/cs/0306110v1", 
    "arxiv-id": "cs/0306110v1"
},{
    "category": "cs.DC", 
    "author": "Cristina Vistoli", 
    "title": "Sharing a conceptual model of grid resources and services", 
    "publish": "2003-06-18T14:55:40Z", 
    "summary": "Grid technologies aim at enabling a coordinated resource-sharing and\nproblem-solving capabilities over local and wide area networks and span\nlocations, organizations, machine architectures and software boundaries. The\nheterogeneity of involved resources and the need for interoperability among\ndifferent grid middlewares require the sharing of a common information model.\nAbstractions of different flavors of resources and services and conceptual\nschemas of domain specific entities require a collaboration effort in order to\nenable a coherent information services cooperation.\n  With this paper, we present the result of our experience in grid resources\nand services modelling carried out within the Grid Laboratory Uniform\nEnvironment (GLUE) effort, a joint US and EU High Energy Physics projects\ncollaboration towards grid interoperability. The first implementation-neutral\nagreement on services such as batch computing and storage manager, resources\nsuch as the hierarchy cluster, sub-cluster, host and the storage library are\npresented. Design guidelines and operational results are depicted together with\nopen issues and future evolutions.", 
    "link": "http://arxiv.org/pdf/cs/0306111v1", 
    "arxiv-id": "cs/0306111v1"
},{
    "category": "cs.DC", 
    "author": "R. St. Denis", 
    "title": "Adapting SAM for CDF", 
    "publish": "2003-06-18T16:28:02Z", 
    "summary": "The CDF and D0 experiments probe the high-energy frontier and as they do so\nhave accumulated hundreds of Terabytes of data on the way to petabytes of data\nover the next two years. The experiments have made a commitment to use the\ndeveloping Grid based on the SAM system to handle these data. The D0 SAM has\nbeen extended for use in CDF as common patterns of design emerged to meet the\nsimilar requirements of these experiments. The process by which the merger was\nachieved is explained with particular emphasis on lessons learned concerning\nthe database design patterns plus realization of the use cases.", 
    "link": "http://arxiv.org/pdf/cs/0306112v1", 
    "arxiv-id": "cs/0306112v1"
},{
    "category": "cs.DC", 
    "author": "representing the D0 Remote Analysis Task Force", 
    "title": "D0 Regional Analysis Center Concepts", 
    "publish": "2003-06-19T19:58:07Z", 
    "summary": "The D0 experiment is facing many exciting challenges providing a computing\nenvironment for its worldwide collaboration. Transparent access to data for\nprocessing and analysis has been enabled through deployment of its SAM system\nto collaborating sites and additional functionality will be provided soon with\nSAMGrid components. In order to maximize access to global storage,\ncomputational and intellectual resources, and to enable the system to scale to\nthe large demands soon to be realized, several strategic sites have been\nidentified as Regional Analysis Centers (RAC's). These sites play an expanded\nrole within the system. The philosophy and function of these centers is\ndiscussed and details of their composition and operation are outlined. The plan\nfor future additional centers is also addressed.", 
    "link": "http://arxiv.org/pdf/cs/0306115v1", 
    "arxiv-id": "cs/0306115v1"
},{
    "category": "cs.DC", 
    "author": "R. Bramley", 
    "title": "GRAPPA: Grid Access Portal for Physics Applications", 
    "publish": "2003-06-26T17:09:07Z", 
    "summary": "Grappa is a Grid portal effort designed to provide physicists convenient\naccess to Grid tools and services. The ATLAS analysis and control framework,\nAthena, was used as the target application. Grappa provides basic Grid\nfunctionality such as resource configuration, credential testing, job\nsubmission, job monitoring, results monitoring, and preliminary integration\nwith the ATLAS replica catalog system, MAGDA. Grappa uses Jython to combine the\nease of scripting with the power of java-based toolkits. This provides a\npowerful framework for accessing diverse Grid resources with uniform\ninterfaces. The initial prototype system was based on the XCAT Science Portal\ndeveloped at the Indiana University Extreme Computing Lab and was demonstrated\nby running Monte Carlo production on the U.S. ATLAS test-bed. The portal also\ncommunicated with a European resource broker on WorldGrid as part of the joint\niVDGL-DataTAG interoperability project for the IST2002 and SC2002\ndemonstrations. The current prototype replaces the XCAT Science Portal with an\nxbooks jetspeed portlet for managing user scripts.", 
    "link": "http://arxiv.org/pdf/cs/0306133v1", 
    "arxiv-id": "cs/0306133v1"
},{
    "category": "cs.DC", 
    "author": "T. Rockwell", 
    "title": "Management of Grid Jobs and Information within SAMGrid", 
    "publish": "2003-07-03T20:26:13Z", 
    "summary": "We describe some of the key aspects of the SAMGrid system, used by the D0 and\nCDF experiments at Fermilab. Having sustained success of the data handling part\nof SAMGrid, we have developed new services for job and information services.\nOur job management is rooted in \\CondorG and uses enhancements that are general\napplicability for HEP grids. Our information system is based on a uniform\nframework for configuration management based on XML data representation and\nprocessing.", 
    "link": "http://arxiv.org/pdf/cs/0307007v2", 
    "arxiv-id": "cs/0307007v2"
},{
    "category": "cs.DC", 
    "author": "J. Simone", 
    "title": "Lattice QCD Production on Commodity Clusters at Fermilab", 
    "publish": "2003-07-08T15:36:56Z", 
    "summary": "We describe the construction and results to date of Fermilab's three\nMyrinet-networked lattice QCD production clusters (an 80-node dual Pentium III\ncluster, a 48-node dual Xeon cluster, and a 128-node dual Xeon cluster). We\nexamine a number of aspects of performance of the MILC lattice QCD code running\non these clusters.", 
    "link": "http://arxiv.org/pdf/cs/0307019v1", 
    "arxiv-id": "cs/0307019v1"
},{
    "category": "cs.DC", 
    "author": "S. Epsteyn", 
    "title": "Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at   Fermilab", 
    "publish": "2003-07-08T16:58:57Z", 
    "summary": "Fermilab operates several clusters for lattice gauge computing. Minimal\nmanpower is available to manage these clusters. We have written a number of\ntools and developed techniques to cope with this task. We describe our tools\nwhich use the IPMI facilities of our systems for hardware management tasks such\nas remote power control, remote system resets, and health monitoring. We\ndiscuss our techniques involving network booting for installation and upgrades\nof the operating system on these computers, and for reloading BIOS and other\nfirmware. Finally, we discuss our tools for parallel command processing and\ntheir use in monitoring and administrating the PBS batch queue system used on\nour clusters.", 
    "link": "http://arxiv.org/pdf/cs/0307021v1", 
    "arxiv-id": "cs/0307021v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Gridscape: A Tool for the Creation of Interactive and Dynamic Grid   Testbed Web Portals", 
    "publish": "2003-07-22T12:27:28Z", 
    "summary": "The notion of grid computing has gained an increasing popularity recently as\na realistic solution to many of our large-scale data storage and processing\nneeds. It enables the sharing, selection and aggregation of resources\ngeographically distributed across collaborative organisations. Now more and\nmore people are beginning to embrace grid computing and thus are seeing the\nneed to set up their own grids and grid testbeds. With this comes the need to\nhave some means to enable them to view and monitor the status of the resources\nin these testbeds (eg. Web based Grid portal). Generally developers invest a\nsubstantial amount of time and effort developing custom monitoring software. To\novercome this limitation, this paper proposes Gridscape ? a tool that enables\nthe rapid creation of interactive and dynamic testbed portals (without any\nprogramming effort). Gridscape primarily aims to provide a solution for those\nusers who need to be able to create a grid testbed portal but don?t necessarily\nhave the time or resources to build a system of their own from scratch.", 
    "link": "http://arxiv.org/pdf/cs/0307052v1", 
    "arxiv-id": "cs/0307052v1"
},{
    "category": "cs.DC", 
    "author": "Franck Cappello", 
    "title": "Augernome & XtremWeb: Monte Carlos computation on a global computing   platform", 
    "publish": "2003-07-29T14:12:07Z", 
    "summary": "In this paper, we present XtremWeb, a Global Computing platform used to\ngenerate monte carlos showers in Auger, an HEP experiment to study the highest\nenergy cosmic rays at Mallargue-Mendoza, Argentina.\n  XtremWeb main goal, as a Global Computing platform, is to compute distributed\napplications using idle time of widely interconnected machines. It is\nespecially dedicated to -but not limited to- multi-parameters applications such\nas monte carlos computations; its security mechanisms ensuring not only hosts\nintegrity but also results certification and its fault tolerant features,\nencouraged us to test it and, finally, to deploy it as to support our CPU needs\nto simulate showers.\n  We first introduce Auger computing needs and how Global Computing could help.\nWe then detail XtremWeb architecture and goals. The fourth and last part\npresents the profits we have gained to choose this platform. We conclude on\nwhat could be done next.", 
    "link": "http://arxiv.org/pdf/cs/0307066v1", 
    "arxiv-id": "cs/0307066v1"
},{
    "category": "cs.DC", 
    "author": "Antony J Wilson", 
    "title": "Relational Grid Monitoring Architecture (R-GMA)", 
    "publish": "2003-08-15T23:53:49Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which has been\ndeveloped within the European DataGrid Project as a Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each Virtual Organisation had one large relational database. We provide a\nnumber of different Producer types with different characteristics; for example\nsome support streaming of information. We also provide combined\nConsumer/Producers, which are able to combine information and republish it. At\nthe heart of the system is the mediator, which for any query is able to find\nand connect to the best Producers for the job. We have developed components to\nallow a measure of inter-working between MDS and R-GMA. We have used it both\nfor information about the grid (primarily to find out about what services are\navailable at any one time) and for application monitoring. R-GMA has been\ndeployed in various testbeds; we describe some preliminary results and\nexperiences of this deployment.", 
    "link": "http://arxiv.org/pdf/cs/0308024v1", 
    "arxiv-id": "cs/0308024v1"
},{
    "category": "cs.DC", 
    "author": "Mark C. Little", 
    "title": "A thought experiment on Quantum Mechanics and Distributed Failure   Detection", 
    "publish": "2003-09-15T10:43:47Z", 
    "summary": "One of the biggest problems in current distributed systems is that presented\nby one machine attempting to determine the liveness of another in a timely\nmanner. Unfortunately, the symptoms exhibited by a failed machine can also be\nthe result of other causes, e.g., an overloaded machine or network which drops\nmessages, making it impossible to detect a machine failure with cetainty until\nthat machine recovers. This is a well understood problem and one which has led\nto a large body of research into failure suspectors: since it is not possible\nto detect a failure, the best one can do is suspect a failure and program\naccordingly. However, one machine's suspicions may not be the same as\nanother's; therefore, these algorithms spend a considerable effort in ensuring\na consistent view among all available machines of who is suspects of being\nfailed. This paper describes a thought experiment on how quantum mechanics may\nbe used to provide a failure detector that is guaranteed to give both accurate\nand instantaneous information about the liveness of machines, no matter the\ndistances involved.", 
    "link": "http://arxiv.org/pdf/cs/0309026v1", 
    "arxiv-id": "cs/0309026v1"
},{
    "category": "cs.DC", 
    "author": "V. C. Barbosa", 
    "title": "A distributed algorithm to find k-dominating sets", 
    "publish": "2003-09-23T01:14:43Z", 
    "summary": "We consider a connected undirected graph $G(n,m)$ with $n$ nodes and $m$\nedges. A $k$-dominating set $D$ in $G$ is a set of nodes having the property\nthat every node in $G$ is at most $k$ edges away from at least one node in $D$.\nFinding a $k$-dominating set of minimum size is NP-hard. We give a new\nsynchronous distributed algorithm to find a $k$-dominating set in $G$ of size\nno greater than $\\lfloor n/(k+1)\\rfloor$. Our algorithm requires $O(k\\log^*n)$\ntime and $O(m\\log k+n\\log k\\log^*n)$ messages to run. It has the same time\ncomplexity as the best currently known algorithm, but improves on that\nalgorithm's message complexity and is, in addition, conceptually simpler.", 
    "link": "http://arxiv.org/pdf/cs/0309040v1", 
    "arxiv-id": "cs/0309040v1"
},{
    "category": "cs.DC", 
    "author": "V. C. Barbosa", 
    "title": "On reducing the complexity of matrix clocks", 
    "publish": "2003-09-23T12:57:59Z", 
    "summary": "Matrix clocks are a generalization of the notion of vector clocks that allows\nthe local representation of causal precedence to reach into an asynchronous\ndistributed computation's past with depth $x$, where $x\\ge 1$ is an integer.\nMaintaining matrix clocks correctly in a system of $n$ nodes requires that\neverymessage be accompanied by $O(n^x)$ numbers, which reflects an exponential\ndependency of the complexity of matrix clocks upon the desired depth $x$. We\nintroduce a novel type of matrix clock, one that requires only $nx$ numbers to\nbe attached to each message while maintaining what for many applications may be\nthe most significant portion of the information that the original matrix clock\ncarries. In order to illustrate the new clock's applicability, we demonstrate\nits use in the monitoring of certain resource-sharing computations.", 
    "link": "http://arxiv.org/pdf/cs/0309042v1", 
    "arxiv-id": "cs/0309042v1"
},{
    "category": "cs.DC", 
    "author": "Vitor Moreira", 
    "title": "Control and Debugging of Distributed Programs Using Fiddle", 
    "publish": "2003-09-26T11:49:10Z", 
    "summary": "The main goal of Fiddle, a distributed debugging engine, is to provide a\nflexible platform for developing debugging tools. Fiddle provides a layered set\nof interfaces with a minimal set of debugging functionalities, for the\ninspection and control of distributed and multi-threaded applications.\n  This paper illustrates how Fiddle is used to support integrated testing and\ndebugging. The approach described is based on a tool, called Deipa, that\ninterprets sequences of commands read from an input file, generated by an\nindependent testing tool. Deipa acts as a Fiddle client, in order to enforce\nspecific execution paths in a distributed PVM program. Other Fiddle clients may\nbe used along with Deipa for the fine debugging at process level. Fiddle and\nDeipa functionalities and architectures are described, and a working example\nshows a step-by-step application of these tools.", 
    "link": "http://arxiv.org/pdf/cs/0309049v1", 
    "arxiv-id": "cs/0309049v1"
},{
    "category": "cs.DC", 
    "author": "Ewing Lusk", 
    "title": "Optimizing Noncontiguous Accesses in MPI-IO", 
    "publish": "2003-10-15T19:35:00Z", 
    "summary": "The I/O access patterns of many parallel applications consist of accesses to\na large number of small, noncontiguous pieces of data. If an application's I/O\nneeds are met by making many small, distinct I/O requests, however, the I/O\nperformance degrades drastically. To avoid this problem, MPI-IO allows users to\naccess noncontiguous data with a single I/O function call, unlike in Unix I/O.\nIn this paper, we explain how critical this feature of MPI-IO is for high\nperformance and how it enables implementations to perform optimizations. We\nfirst provide a classification of the different ways of expressing an\napplication's I/O needs in MPI-IO--we classify them into four levels, called\nlevel~0 through level~3. We demonstrate that, for applications with\nnoncontiguous access patterns, the I/O performance improves dramatically if\nusers write their applications to make level-3 requests (noncontiguous,\ncollective) rather than level-0 requests (Unix style). We then describe how our\nMPI-IO implementation, ROMIO, delivers high performance for noncontiguous\nrequests. We explain in detail the two key optimizations ROMIO performs: data\nsieving for noncontiguous requests from one process and collective I/O for\nnoncontiguous requests from multiple processes. We describe how we have\nimplemented these optimizations portably on multiple machines and file systems,\ncontrolled their memory requirements, and also achieved high performance. We\ndemonstrate the performance and portability with performance results for three\napplications--an astrophysics-application template (DIST3D), the NAS BTIO\nbenchmark, and an unstructured code (UNSTRUC)--on five different parallel\nmachines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.", 
    "link": "http://arxiv.org/pdf/cs/0310029v1", 
    "arxiv-id": "cs/0310029v1"
},{
    "category": "cs.DC", 
    "author": "Oliver Oppitz", 
    "title": "A Particular Bug Trap: Execution Replay Using Virtual Machines", 
    "publish": "2003-10-15T20:54:14Z", 
    "summary": "Execution-replay (ER) is well known in the literature but has been restricted\nto special system architectures for many years. Improved hardware resources and\nthe maturity of virtual machine technology promise to make ER useful for a\nbroader range of development projects.\n  This paper describes an approach to create a practical, generic ER\ninfrastructure for desktop PC systems using virtual machine technology. In the\ncreated VM environment arbitrary application programs will run and be replayed\nunmodified, neither instrumentation nor recompilation are required.", 
    "link": "http://arxiv.org/pdf/cs/0310030v1", 
    "arxiv-id": "cs/0310030v1"
},{
    "category": "cs.DC", 
    "author": "C. Wang", 
    "title": "OGSA/Globus Evaluation for Data Intensive Applications", 
    "publish": "2003-11-10T11:12:26Z", 
    "summary": "We present an architecture of Globus Toolkit 3 based testbed intended for\nevaluation of applicability of the Open Grid Service Architecture (OGSA) for\nData Intensive Applications.", 
    "link": "http://arxiv.org/pdf/cs/0311009v1", 
    "arxiv-id": "cs/0311009v1"
},{
    "category": "cs.DC", 
    "author": "A. Kryukov", 
    "title": "Problem of Application Job Monitoring in GRID Systems", 
    "publish": "2003-11-10T11:39:04Z", 
    "summary": "We present a new approach to monitoring of the execution process of an\napplication job in the GRID environment. The main point of the approach is use\nof GRID ervices to access monitoring information with the security level\navailable in GRID.", 
    "link": "http://arxiv.org/pdf/cs/0311010v1", 
    "arxiv-id": "cs/0311010v1"
},{
    "category": "cs.DC", 
    "author": "L. Shamardin", 
    "title": "LCG-1 Deployment and usage experience", 
    "publish": "2003-11-17T13:19:31Z", 
    "summary": "LCG-1 is the second release of the software framework for the LHC Computing\nGrid project. In our work we describe the installation process, arising\nproblems and their solutions, and configuration tuning details of the complete\nLCG-1 site, including all LCG elements required for the self-sufficient site.", 
    "link": "http://arxiv.org/pdf/cs/0311021v1", 
    "arxiv-id": "cs/0311021v1"
},{
    "category": "cs.DC", 
    "author": "Baikunth Nath", 
    "title": "GridEmail: A Case for Economically Regulated Internet-based   Interpersonal Communications", 
    "publish": "2003-12-12T11:42:17Z", 
    "summary": "Email has emerged as a dominant form of electronic communication between\npeople. Spam is a major problem for email users, with estimates of up to 56% of\nemail falling into that category. Control of Spam is being attempted with\ntechnical and legislative methods. In this paper we look at email and spam from\na supply-demand perspective. We propose Gridemail, an email system based on an\neconomy of communicating parties, where participants? motivations are\nrepresented as pricing policies and profiles. This system is expected to help\npeople regulate their personal communications to suit their conditions, and\nhelp in removing unwanted messages.", 
    "link": "http://arxiv.org/pdf/cs/0312022v1", 
    "arxiv-id": "cs/0312022v1"
},{
    "category": "cs.DC", 
    "author": "Gianluca Argentini", 
    "title": "Using virtual processors for SPMD parallel programs", 
    "publish": "2003-12-21T14:37:24Z", 
    "summary": "In this paper I describe some results on the use of virtual processors\ntechnology for parallelize some SPMD computational programs. The tested\ntechnology is the INTEL Hyper Threading on real processors, and the programs\nare MATLAB scripts for floating points computation. The conclusions of the work\nconcern on the utility and limits of the used approach. The main result is that\nusing virtual processors is a good technique for improving parallel programs\nnot only for memory-based computations, but in the case of massive disk-storage\noperations too.", 
    "link": "http://arxiv.org/pdf/cs/0312049v1", 
    "arxiv-id": "cs/0312049v1"
},{
    "category": "cs.DC", 
    "author": "Duraid Madina", 
    "title": "ClassdescMP: Easy MPI programming in C++", 
    "publish": "2004-01-27T03:55:27Z", 
    "summary": "ClassdescMP is a distributed memory parallel programming system for use with\nC++ and MPI. It uses the Classdesc reflection system to ease the task of\nbuilding complicated messages to be sent between processes. It doesn't hide the\nunderlying MPI API, so it is an augmentation of MPI capabilities. Users can\nstill call standard MPI function calls if needed for performance reasons.", 
    "link": "http://arxiv.org/pdf/cs/0401027v1", 
    "arxiv-id": "cs/0401027v1"
},{
    "category": "cs.DC", 
    "author": "Aleta Ricciardi", 
    "title": "A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and   Failure Detectors", 
    "publish": "2004-02-05T17:15:08Z", 
    "summary": "It is shown that, in a precise sense, if there is no bound on the number of\nfaulty processes in a system with unreliable but fair communication, Uniform\nDistributed Coordination (UDC) can be attained if and only if a system has\nperfect failure detectors. This result is generalized to the case where there\nis a bound t on the number of faulty processes. It is shown that a certain type\nof generalized failure detector is necessary and sufficient for achieving UDC\nin a context with at most t faulty processes. Reasoning about processes'\nknowledge as to which other processes are faulty plays a key role in the\nanalysis.", 
    "link": "http://arxiv.org/pdf/cs/0402012v1", 
    "arxiv-id": "cs/0402012v1"
},{
    "category": "cs.DC", 
    "author": "Srikumar Venugopal", 
    "title": "Alchemi: A .NET-based Grid Computing Framework and its Integration into   Global Grids", 
    "publish": "2004-02-10T09:18:07Z", 
    "summary": "Computational grids that couple geographically distributed resources are\nbecoming the de-facto computing platform for solving large-scale problems in\nscience, engineering, and commerce. Software to enable grid computing has been\nprimarily written for Unix-class operating systems, thus severely limiting the\nability to effectively utilize the computing resources of the vast majority of\ndesktop computers i.e. those running variants of the Microsoft Windows\noperating system. Addressing Windows-based grid computing is particularly\nimportant from the software industry's viewpoint where interest in grids is\nemerging rapidly. Microsoft's .NET Framework has become near-ubiquitous for\nimplementing commercial distributed systems for Windows-based platforms,\npositioning it as the ideal platform for grid computing in this context. In\nthis paper we present Alchemi, a .NET-based grid computing framework that\nprovides the runtime machinery and programming environment required to\nconstruct desktop grids and develop grid applications. It allows flexible\napplication composition by supporting an object-oriented grid application\nprogramming model in addition to a grid job model. Cross-platform support is\nprovided via a web services interface and a flexible execution model supports\ndedicated and non-dedicated (voluntary) execution by grid nodes.", 
    "link": "http://arxiv.org/pdf/cs/0402017v1", 
    "arxiv-id": "cs/0402017v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "P2P Networks for Content Sharing", 
    "publish": "2004-02-10T14:24:48Z", 
    "summary": "Peer-to-peer (P2P) technologies have been widely used for content sharing,\npopularly called \"file-swapping\" networks. This chapter gives a broad overview\nof content sharing P2P technologies. It starts with the fundamental concept of\nP2P computing followed by the analysis of network topologies used in\npeer-to-peer systems. Next, three milestone peer-to-peer technologies: Napster,\nGnutella, and Fasttrack are explored in details, and they are finally concluded\nwith the comparison table in the last section.", 
    "link": "http://arxiv.org/pdf/cs/0402018v1", 
    "arxiv-id": "cs/0402018v1"
},{
    "category": "cs.DC", 
    "author": "Masahiko Yokoyama", 
    "title": "Belle Computing System", 
    "publish": "2004-03-11T09:20:20Z", 
    "summary": "We describe the present status of the computing system in the Belle\nexperiment at the KEKB $e^+e^-$ asymmetric-energy collider. So far, we have\nlogged more than 160 fb$^{-1}$ of data, corresponding to the world's largest\ndata sample of 170M $B\\bar{B}$ pairs at the $\\Upsilon(4S)$ energy region. A\nlarge amount of event data has to be processed to produce an analysis event\nsample in a timely fashion. In addition, Monte Carlo events have to be created\nto control systematic errors accurately. This requires stable and efficient\nusage of computing resources. Here we review our computing model and then\ndescribe how we efficiently proceed DST/MC productions in our system.", 
    "link": "http://arxiv.org/pdf/cs/0403015v2", 
    "arxiv-id": "cs/0403015v2"
},{
    "category": "cs.DC", 
    "author": "Timm M. Steinbeck", 
    "title": "A Modular and Fault-Tolerant Data Transport Framework", 
    "publish": "2004-04-06T13:35:55Z", 
    "summary": "The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to\nreduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output\nbefore the data is written to permanent storage. To cope with these data rates\na large PC cluster system is being designed to scale to several 1000 nodes,\nconnected by a fast network. For the software that will run on these nodes a\nflexible data transport and distribution software framework, described in this\nthesis, has been developed. The framework consists of a set of separate\ncomponents, that can be connected via a common interface. This allows to\nconstruct different configurations for the HLT, that are even changeable at\nruntime. To ensure a fault-tolerant operation of the HLT, the framework\nincludes a basic fail-over mechanism that allows to replace whole nodes after a\nfailure. The mechanism will be further expanded in the future, utilizing the\nruntime reconnection feature of the framework's component interface. To connect\ncluster nodes a communication class library is used that abstracts from the\nactual network technology and protocol used to retain flexibility in the\nhardware choice. It contains already two working prototype versions for the TCP\nprotocol as well as SCI network adapters. Extensions can be added to the\nlibrary without modifications to other parts of the framework. Extensive tests\nand measurements have been performed with the framework. Their results as well\nas conclusions drawn from them are also presented in this thesis. Performance\ntests show very promising results for the system, indicating that it can\nfulfill ALICE's requirements concerning the data transport.", 
    "link": "http://arxiv.org/pdf/cs/0404014v1", 
    "arxiv-id": "cs/0404014v1"
},{
    "category": "cs.DC", 
    "author": "Ahmet A. Husainov", 
    "title": "The study of distributed computing algorithms by multithread   applications", 
    "publish": "2004-04-07T01:27:32Z", 
    "summary": "The material in this note is used as an introduction to distributed\nalgorithms in a four year course on software and automatic control system in\nthe computer technology department of the Komsomolsk-on-Amur state technical\nuniversity. All our the program examples are written in Borland C/C++ 5.02 for\nWindows 95/98/2000/NT/XP, and hence suit to compile and execute by Visual\nC/C++. We consider the following approaches of the distributed computing: the\nconversion of recursive algorithms to multithread applications, a realization\nof the pairing algorithm, the building of wave systems by Petri nets and object\noriented programming.", 
    "link": "http://arxiv.org/pdf/cs/0404015v2", 
    "arxiv-id": "cs/0404015v2"
},{
    "category": "cs.DC", 
    "author": "Srikumar Venugopal", 
    "title": "The Gridbus Toolkit for Service Oriented Grid and Utility Computing: An   Overview and Status Report", 
    "publish": "2004-04-13T09:16:54Z", 
    "summary": "Grids aim at exploiting synergies that result from cooperation of autonomous\ndistributed entities. The synergies that result from grid cooperation include\nthe sharing, exchange, selection, and aggregation of geographically distributed\nresources such as computers, data bases, software, and scientific instruments\nfor solving large-scale problems in science, engineering, and commerce. For\nthis cooperation to be sustainable, participants need to have economic\nincentive. Therefore, \"incentive\" mechanisms should be considered as one of key\ndesign parameters of Grid architectures. In this article, we present an\noverview and status of an open source Grid toolkit, called Gridbus, whose\narchitecture is fundamentally driven by the requirements of Grid economy.\nGridbus technologies provide services for both computational and data grids\nthat power the emerging eScience and eBusiness applications.", 
    "link": "http://arxiv.org/pdf/cs/0404027v1", 
    "arxiv-id": "cs/0404027v1"
},{
    "category": "cs.DC", 
    "author": "Lyle Winton", 
    "title": "A Grid Service Broker for Scheduling Distributed Data-Oriented   Applications on Global Grids", 
    "publish": "2004-05-06T10:15:26Z", 
    "summary": "The next generation of scientific experiments and studies, popularly called\nas e-Science, is carried out by large collaborations of researchers distributed\naround the world engaged in analysis of huge collections of data generated by\nscientific instruments. Grid computing has emerged as an enabler for e-Science\nas it permits the creation of virtual organizations that bring together\ncommunities with common objectives. Within a community, data collections are\nstored or replicated on distributed resources to enhance storage capability or\nefficiency of access. In such an environment, scientists need to have the\nability to carry out their studies by transparently accessing distributed data\nand computational resources. In this paper, we propose and develop a Grid\nbroker that mediates access to distributed resources by (a) discovering\nsuitable data sources for a given analysis scenario, (b) suitable computational\nresources, (c) optimally mapping analysis jobs to resources, (d) deploying and\nmonitoring job execution on selected resources, (e) accessing data from local\nor remote data source during job execution and (f) collating and presenting\nresults. The broker supports a declarative and dynamic parametric programming\nmodel for creating grid applications. We have used this model in grid-enabling\na high energy physics analysis application (Belle Analysis Software Framework).\nThe broker has been used in deploying Belle experiment data analysis jobs on a\ngrid testbed, called Belle Analysis Data Grid, having resources distributed\nacross Australia interconnected through GrangeNet.", 
    "link": "http://arxiv.org/pdf/cs/0405023v1", 
    "arxiv-id": "cs/0405023v1"
},{
    "category": "cs.DC", 
    "author": "Shinji Shimojo", 
    "title": "A New Dynamical Domain Decomposition Method for Parallel Molecular   Dynamics Simulation on Grid", 
    "publish": "2004-05-24T07:29:13Z", 
    "summary": "We develop a new Lagrangian material particle -- dynamical domain\ndecomposition method (MPD^3) for large scale parallel molecular dynamics (MD)\nsimulation of nonstationary heterogeneous systems on a heterogeneous computing\nnet. MPD^3 is based on Voronoi decomposition of simulated matter. The map of\nVoronoi polygons is known as the Dirichlet tessellation and used for grid\ngeneration in computational fluid dynamics. From the hydrodynamics point of\nview the moving Voronoi polygon looks as a material particle (MP). MPs can\nexchange particles and information. To balance heterogeneous computing\nconditions the MP centers should be dependent on timing data. We propose a\nsimple and efficient iterative algorithm which based on definition of the\ntiming-dependent balancing displacement of MP center for next simulation step.\n  The MPD^3 program was tested in various computing environments and physical\nproblems. We have demonstrated that MPD^3 is a high-adaptive decomposition\nalgorithm for MD simulation. It was shown that the well-balanced decomposition\ncan result from dynamical Voronoi polygon tessellation. One would expect the\nsimilar approach can be successfully applied for other particle methods like\nMonte Carlo, particle-in-cell, and smooth-particle-hydrodynamics.", 
    "link": "http://arxiv.org/pdf/cs/0405086v1", 
    "arxiv-id": "cs/0405086v1"
},{
    "category": "cs.DC", 
    "author": "Srikumar Venugopal", 
    "title": "Global Grids and Software Toolkits: A Study of Four Grid Middleware   Technologies", 
    "publish": "2004-07-01T11:54:06Z", 
    "summary": "Grid is an infrastructure that involves the integrated and collaborative use\nof computers, networks, databases and scientific instruments owned and managed\nby multiple organizations. Grid applications often involve large amounts of\ndata and/or computing resources that require secure resource sharing across\norganizational boundaries. This makes Grid application management and\ndeployment a complex undertaking. Grid middlewares provide users with seamless\ncomputing ability and uniform access to resources in the heterogeneous Grid\nenvironment. Several software toolkits and systems have been developed, most of\nwhich are results of academic research projects, all over the world. This\nchapter will focus on four of these middlewares--UNICORE, Globus, Legion and\nGridbus. It also presents our implementation of a resource broker for UNICORE\nas this functionality was not supported in it. A comparison of these systems on\nthe basis of the architecture, implementation model and several other features\nis included.", 
    "link": "http://arxiv.org/pdf/cs/0407001v1", 
    "arxiv-id": "cs/0407001v1"
},{
    "category": "cs.DC", 
    "author": "Conrad Steenberg", 
    "title": "A Taxonomy and Survey of Grid Resource Planning and Reservation Systems   for Grid Enabled Analysis Environment", 
    "publish": "2004-07-05T15:48:43Z", 
    "summary": "The concept of coupling geographically distributed resources for solving\nlarge scale problems is becoming increasingly popular forming what is popularly\ncalled grid computing. Management of resources in the Grid environment becomes\ncomplex as the resources are geographically distributed, heterogeneous in\nnature and owned by different individuals and organizations each having their\nown resource management policies and different access and cost models. There\nhave been many projects that have designed and implemented the resource\nmanagement systems with a variety of architectures and services. In this paper\nwe have presented the general requirements that a Resource Management system\nshould satisfy. The taxonomy has also been defined based on which survey of\nresource management systems in different existing Grid projects has been\nconducted to identify the key areas where these systems lack the desired\nfunctionality.", 
    "link": "http://arxiv.org/pdf/cs/0407012v1", 
    "arxiv-id": "cs/0407012v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "Distributed Analysis and Load Balancing System for Grid Enabled Analysis   on Hand-held devices using Multi-Agents Systems", 
    "publish": "2004-07-05T16:08:36Z", 
    "summary": "Handheld devices, while growing rapidly, are inherently constrained and lack\nthe capability of executing resource hungry applications. This paper presents\nthe design and implementation of distributed analysis and load-balancing system\nfor hand-held devices using multi-agents system. This system enables low\nresource mobile handheld devices to act as potential clients for Grid enabled\napplications and analysis environments. We propose a system, in which mobile\nagents will transport, schedule, execute and return results for heavy\ncomputational jobs submitted by handheld devices. Moreover, in this way, our\nsystem provides high throughput computing environment for hand-held devices.", 
    "link": "http://arxiv.org/pdf/cs/0407013v1", 
    "arxiv-id": "cs/0407013v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "A Grid-enabled Interface to Condor for Interactive Analysis on Handheld   and Resource-limited Devices", 
    "publish": "2004-07-05T19:40:00Z", 
    "summary": "This paper was withdrawn by the authors.", 
    "link": "http://arxiv.org/pdf/cs/0407014v2", 
    "arxiv-id": "cs/0407014v2"
},{
    "category": "cs.DC", 
    "author": "Reid Sherman", 
    "title": "A Low Cost Distributed Computing Approach to Pulsar Searches at a Small   College", 
    "publish": "2004-07-07T18:55:20Z", 
    "summary": "We describe a distributed processing cluster of inexpensive Linux machines\ndeveloped jointly by the Astronomy and Computer Science departments at\nHaverford College which has been successfully used to search a large volume of\ndata from a recent radio pulsar survey. Analysis of radio pulsar surveys\nrequires significant computational resources to handle the demanding data\nstorage and processing needs. One goal of this project was to explore issues\nencountered when processing a large amount of pulsar survey data with limited\ncomputational resources. This cluster, which was developed and activated in\nonly a few weeks by supervised undergraduate summer research students, used\nexisting decommissioned computers, the campus network, and a script-based,\nclient-oriented, self-scheduled data distribution approach to process the data.\nThis setup provided simplicity, efficiency, and \"on-the-fly\" scalability at low\ncost. The entire 570 GB data set from the pulsar survey was processed at\nHaverford over the course of a ten-week summer period using this cluster. We\nconclude that this cluster can serve as a useful computational model in cases\nwhere data processing must be carried out on a limited budget. We have also\nconstructed a DVD archive of the raw survey data in order to investigate the\nfeasibility of using DVD as an inexpensive and easily accessible raw data\nstorage format for pulsar surveys. DVD-based storage has not been widely\nexplored in the pulsar community, but it has several advantages. The DVD\narchive we have constructed is reliable, portable, inexpensive, and can be\neasily read by any standard modern machine.", 
    "link": "http://arxiv.org/pdf/cs/0407017v1", 
    "arxiv-id": "cs/0407017v1"
},{
    "category": "cs.DC", 
    "author": "Gregory Mounie", 
    "title": "Performance Characterisation of Intra-Cluster Collective Communications", 
    "publish": "2004-08-14T06:31:10Z", 
    "summary": "Although recent works try to improve collective communication in grid systems\nby separating intra and inter-cluster communication, the optimisation of\ncommunications focus only on inter-cluster communications. We believe, instead,\nthat the overall performance of the application may be improved if\nintra-cluster collective communications performance is known in advance. Hence,\nit is important to have an accurate model of the intra-cluster collective\ncommunications, which provides the necessary evidences to tune and to predict\ntheir performance correctly. In this paper we present our experience on\nmodelling such communication strategies. We describe and compare different\nimplementation strategies with their communication models, evaluating the\nmodels' accuracy and describing the practical challenges that can be found when\nmodelling collective communications.", 
    "link": "http://arxiv.org/pdf/cs/0408032v2", 
    "arxiv-id": "cs/0408032v2"
},{
    "category": "cs.DC", 
    "author": "Gregory Mounie", 
    "title": "Identifying Logical Homogeneous Clusters for Efficient Wide-area   Communications", 
    "publish": "2004-08-14T07:39:57Z", 
    "summary": "Recently, many works focus on the implementation of collective communication\noperations adapted to wide area computational systems, like computational Grids\nor global-computing. Due to the inherently heterogeneity of such environments,\nmost works separate \"clusters\" in different hierarchy levels. to better model\nthe communication. However, in our opinion, such works do not give enough\nattention to the delimitation of such clusters, as they normally use the\nlocality or the IP subnet from the machines to delimit a cluster without\nverifying the \"homogeneity\" of such clusters. In this paper, we describe a\nstrategy to gather network information from different local-area networks and\nto construct \"logical homogeneous clusters\", better suited to the performance\nmodelling.", 
    "link": "http://arxiv.org/pdf/cs/0408033v3", 
    "arxiv-id": "cs/0408033v3"
},{
    "category": "cs.DC", 
    "author": "Gregory Mounie", 
    "title": "Fast Tuning of Intra-Cluster Collective Communications", 
    "publish": "2004-08-14T07:40:33Z", 
    "summary": "Recent works try to optimise collective communication in grid systems\nfocusing mostly on the optimisation of communications among different clusters.\nWe believe that intra-cluster collective communications should also be\noptimised, as a way to improve the overall efficiency and to allow the\nconstruction of multi-level collective operations. Indeed, inside homogeneous\nclusters, a simple optimisation approach rely on the comparison from different\nimplementation strategies, through their communication models. In this paper we\nevaluate this approach, comparing different implementation strategies with\ntheir predicted performances. As a result, we are able to choose the\ncommunication strategy that better adapts to each network environment.", 
    "link": "http://arxiv.org/pdf/cs/0408034v3", 
    "arxiv-id": "cs/0408034v3"
},{
    "category": "cs.DC", 
    "author": "Aaron Harwood", 
    "title": "Diffusive Load Balancing of Loosely-Synchronous Parallel Programs over   Peer-to-Peer Networks", 
    "publish": "2004-10-05T08:23:45Z", 
    "summary": "The use of under-utilized Internet resources is widely recognized as a viable\nform of high performance computing. Sustained processing power of roughly 40T\nFLOPS using 4 million volunteered Internet hosts has been reported for\nembarrassingly parallel problems. At the same time, peer-to-peer (P2P) file\nsharing networks, with more than 50 million participants, have demonstrated the\ncapacity for scale in distributed systems. This paper contributes a study of\nload balancing techniques for a general class of loosely-synchronous parallel\nalgorithms when executed over a P2P network. We show that decentralized,\ndiffusive load balancing can be effective at balancing load and is facilitated\nby the dynamic properties of P2P. While a moderate degree of dynamicity can\nbenefit load balancing, significant dynamicity hinders the parallel program\nperformance due to the need for increased load migration. To the best of our\nknowledge this study provides new insight into the performance of\nloosely-synchronous parallel programs over the Internet.", 
    "link": "http://arxiv.org/pdf/cs/0410009v1", 
    "arxiv-id": "cs/0410009v1"
},{
    "category": "cs.DC", 
    "author": "Pedro Andrade", 
    "title": "HEP@Home - A distributed computing system based on BOINC", 
    "publish": "2004-10-07T12:42:10Z", 
    "summary": "Project SETI@HOME has proven to be one of the biggest successes of\ndistributed computing during the last years. With a quite simple approach SETI\nmanages to process large volumes of data using a vast amount of distributed\ncomputer power.\n  To extend the generic usage of this kind of distributed computing tools,\nBOINC is being developed. In this paper we propose HEP@HOME, a BOINC version\ntailored to the specific requirements of the High Energy Physics (HEP)\ncommunity.\n  The HEP@HOME will be able to process large amounts of data using virtually\nunlimited computing power, as BOINC does, and it should be able to work\naccording to HEP specifications. In HEP the amounts of data to be analyzed or\nreconstructed are of central importance. Therefore, one of the design\nprinciples of this tool is to avoid data transfer. This will allow scientists\nto run their analysis applications and taking advantage of a large number of\nCPUs. This tool also satisfies other important requirements in HEP, namely,\nsecurity, fault-tolerance and monitoring.", 
    "link": "http://arxiv.org/pdf/cs/0410016v1", 
    "arxiv-id": "cs/0410016v1"
},{
    "category": "cs.DC", 
    "author": "Wen-Jing Hsu", 
    "title": "ReCord: A Distributed Hash Table with Recursive Structure", 
    "publish": "2004-10-29T02:56:21Z", 
    "summary": "We propose a simple distributed hash table called ReCord, which is a\ngeneralized version of Randomized-Chord and offers improved tradeoffs in\nperformance and topology maintenance over existing P2P systems. ReCord is\nscalable and can be easily implemented as an overlay network, and offers a good\ntradeoff between the node degree and query latency. For instance, an $n$-node\nReCord with $O(\\log n)$ node degree has an expected latency of $\\Theta(\\log n)$\nhops. Alternatively, it can also offer $\\Theta(\\frac{\\log n}{\\log \\log n})$\nhops latency at a higher cost of $O(\\frac{\\log^2 n}{\\log\n  \\log n})$ node degree. Meanwhile, simulations of the dynamic behaviors of\nReCord are studied.", 
    "link": "http://arxiv.org/pdf/cs/0410074v2", 
    "arxiv-id": "cs/0410074v2"
},{
    "category": "cs.DC", 
    "author": "Ian Foster", 
    "title": "Usage Policy-based CPU Sharing in VOs", 
    "publish": "2004-11-12T22:48:00Z", 
    "summary": "Resource sharing within Grid collaborations usually implies specific sharing\nmechanisms at participating sites. Challenging policy issues can arise within\nvirtual organizations (VOs) that integrate participants and resources spanning\nmultiple physical institutions. Resource owners may wish to grant to one or\nmore VOs the right to use certain resources subject to local policy and service\nlevel agreements, and each VO may then wish to use those resources subject to\nVO policy. Thus, we must address the question of what usage policies (UPs)\nshould be considered for resource sharing in VOs. As a first step in addressing\nthis question, we develop and evaluate different UP scenarios within a\nspecialized context that mimics scientific Grids within which the resources to\nbe shared are computers. We also present a UP architecture and define roles and\nfunctions for scheduling resources in such grid environments while satisfying\nresource owner policies.", 
    "link": "http://arxiv.org/pdf/cs/0411045v1", 
    "arxiv-id": "cs/0411045v1"
},{
    "category": "cs.DC", 
    "author": "Vwani P. Roychowdhury", 
    "title": "Balanced Overlay Networks (BON): Decentralized Load Balancing via   Self-Organized Random Networks", 
    "publish": "2004-11-15T20:05:00Z", 
    "summary": "We present a novel framework, called balanced overlay networks (BON), that\nprovides scalable, decentralized load balancing for distributed computing using\nlarge-scale pools of heterogeneous computers. Fundamentally, BON encodes the\ninformation about each node's available computational resources in the\nstructure of the links connecting the nodes in the network. This distributed\nencoding is self-organized, with each node managing its in-degree and local\nconnectivity via random-walk sampling. Assignment of incoming jobs to nodes\nwith the most free resources is also accomplished by sampling the nodes via\nshort random walks. Extensive simulations show that the resulting highly\ndynamic and self-organized graph structure can efficiently balance\ncomputational load throughout large-scale networks. These simulations cover a\nwide spectrum of cases, including significant heterogeneity in available\ncomputing resources and high burstiness in incoming load. We provide analytical\nresults that prove BON's scalability for truly large-scale networks: in\nparticular we show that under certain ideal conditions, the network structure\nconverges to Erdos-Renyi (ER) random graphs; our simulation results, however,\nshow that the algorithm does much better, and the structures seem to approach\nthe ideal case of d-regular random graphs. We also make a connection between\nhighly-loaded BONs and the well-known ball-bin randomized load balancing\nframework.", 
    "link": "http://arxiv.org/pdf/cs/0411046v2", 
    "arxiv-id": "cs/0411046v2"
},{
    "category": "cs.DC", 
    "author": "Bernadette Charron-Bost", 
    "title": "Reductions in Distributed Computing Part I: Consensus and Atomic   Commitment Tasks", 
    "publish": "2004-12-29T19:50:21Z", 
    "summary": "We introduce several notions of reduction in distributed computing, and\ninvestigate reduction properties of two fundamental agreement tasks, namely\nConsensus and Atomic Commitment.\n  We first propose the notion of reduction \"a la Karp'', an analog for\ndistributed computing of the classical Karp reduction. We then define a weaker\nreduction which is the analog of Cook reduction. These two reductions are\ncalled K-reduction and C-reduction, respectively.\n  We also introduce the notion of C*-reduction which has no counterpart in\nclassical (namely, non distributed) systems, and which naturally arises when\ndealing with symmetric tasks.\n  We establish various reducibility and irreducibility theorems with respect to\nthese three reductions. Our main result is an incomparability statement for\nConsensus and Atomic Commitment tasks: we show that they are incomparable with\nrespect to the C-reduction, except when the resiliency degree is 1, in which\ncase Atomic Commitment is strictly harder than Consensus. A side consequence of\nthese results is that our notion of C-reduction is strictly weaker than the one\nof K-reduction, even for unsolvable tasks.", 
    "link": "http://arxiv.org/pdf/cs/0412115v1", 
    "arxiv-id": "cs/0412115v1"
},{
    "category": "cs.DC", 
    "author": "Bernadette Charron-Bost", 
    "title": "Reductions in Distributed Computing Part II: k-Threshold Agreement Tasks", 
    "publish": "2004-12-29T20:51:58Z", 
    "summary": "We extend the results of Part I by considering a new class of agreement\ntasks, the so-called k-Threshold Agreement tasks (previously introduced by\nCharron-Bost and Le Fessant). These tasks naturally interpolate between Atomic\nCommitment and Consensus. Moreover, they constitute a valuable tool to derive\nirreducibility results between Consensus tasks only. In particular, they allow\nus to show that (A) for a fixed set of processes, the higher the resiliency\ndegree is, the harder the Consensus task is, and (B) for a fixed resiliency\ndegree, the smaller the set of processes is, the harder the Consensus task is.\n  The proofs of these results lead us to consider new oracle-based reductions,\ninvolving a weaker variant of the C-reduction introduced in Part I. We also\ndiscuss the relationship between our results and previous ones relating\nf-resiliency and wait-freedom.", 
    "link": "http://arxiv.org/pdf/cs/0412116v1", 
    "arxiv-id": "cs/0412116v1"
},{
    "category": "cs.DC", 
    "author": "William Yurcik", 
    "title": "A Distributed Economics-based Infrastructure for Utility Computing", 
    "publish": "2004-12-31T19:16:38Z", 
    "summary": "Existing attempts at utility computing revolve around two approaches. The\nfirst consists of proprietary solutions involving renting time on dedicated\nutility computing machines. The second requires the use of heavy, monolithic\napplications that are difficult to deploy, maintain, and use.\n  We propose a distributed, community-oriented approach to utility computing.\nOur approach provides an infrastructure built on Web Services in which modular\ncomponents are combined to create a seemingly simple, yet powerful system. The\ncommunity-oriented nature generates an economic environment which results in\nfair transactions between consumers and providers of computing cycles while\nsimultaneously encouraging improvements in the infrastructure of the\ncomputational grid itself.", 
    "link": "http://arxiv.org/pdf/cs/0412121v1", 
    "arxiv-id": "cs/0412121v1"
},{
    "category": "cs.DC", 
    "author": "Michael Treaster", 
    "title": "A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel   Systems", 
    "publish": "2005-01-01T03:32:51Z", 
    "summary": "Supercomputing systems today often come in the form of large numbers of\ncommodity systems linked together into a computing cluster. These systems, like\nany distributed system, can have large numbers of independent hardware\ncomponents cooperating or collaborating on a computation. Unfortunately, any of\nthis vast number of components can fail at any time, resulting in potentially\nerroneous output. In order to improve the robustness of supercomputing\napplications in the presence of failures, many techniques have been developed\nto provide resilience to these kinds of system faults. This survey provides an\noverview of these various fault-tolerance techniques.", 
    "link": "http://arxiv.org/pdf/cs/0501002v1", 
    "arxiv-id": "cs/0501002v1"
},{
    "category": "cs.DC", 
    "author": "James Annis", 
    "title": "When Database Systems Meet the Grid", 
    "publish": "2005-02-03T21:43:50Z", 
    "summary": "We illustrate the benefits of combining database systems and Grid\ntechnologies for data-intensive applications. Using a cluster of SQL servers,\nwe reimplemented an existing Grid application that finds galaxy clusters in a\nlarge astronomical database. The SQL implementation runs an order of magnitude\nfaster than the earlier Tcl-C-file-based implementation. We discuss why and how\nGrid applications can take advantage of database systems.", 
    "link": "http://arxiv.org/pdf/cs/0502018v1", 
    "arxiv-id": "cs/0502018v1"
},{
    "category": "cs.DC", 
    "author": "Farooq Ahmad", 
    "title": "A Semantic Grid-based E-Learning Framework (SELF)", 
    "publish": "2005-02-09T17:22:21Z", 
    "summary": "E-learning can be loosely defined as a wide set of applications and\nprocesses, which uses available electronic media (and tools) to deliver\nvocational education and training. With its increasing recognition as an\nubiquitous mode of instruction and interaction in the academic as well as\ncorporate world, the need for a scaleable and realistic model is becoming\nimportant. In this paper we introduce SELF; a Semantic grid-based E-Learning\nFramework. SELF aims to identify the key-enablers in a practical grid-based\nE-learning environment and to minimize technological reworking by proposing a\nwell-defined interaction plan among currently available tools and technologies.\nWe define a dichotomy with E-learning specific application layers on top and\nsemantic grid-based support layers underneath. We also map the latest open and\nfreeware technologies with various components in SELF.", 
    "link": "http://arxiv.org/pdf/cs/0502051v1", 
    "arxiv-id": "cs/0502051v1"
},{
    "category": "cs.DC", 
    "author": "Dennis Pfisterer", 
    "title": "Koordinatenfreies Lokationsbewusstsein (Localization without   Coordinates)", 
    "publish": "2005-02-15T16:35:13Z", 
    "summary": "Localization is one of the fundamental issues in sensor networks. It is\nalmost always assumed that it must be solved by assigning coordinates to the\nnodes. This article discusses positioning algorithms from a theoretical,\npractical and simulative point of view, and identifies difficulties and\nlimitations. Ideas for more abstract means of location awareness are presented\nand the resulting possible improvements for applications are shown. Nodes with\ncertain topological or environmental properties are clustered, and the\nneighborhood structure of the clusters is modeled as a graph. Eines der\nfundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer\ndie Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon\nausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In\ndiesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein\nkritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen\naufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine\nabstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird\ngezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf\neiner graphenbasierten Modellierung des Netzes: Knoten mit bestimmten\ntopologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und\nClusternachbarschaften dann als Graphen modelliert.", 
    "link": "http://arxiv.org/pdf/cs/0502069v1", 
    "arxiv-id": "cs/0502069v1"
},{
    "category": "cs.DC", 
    "author": "Romeo Rizzi", 
    "title": "Online Permutation Routing in Partitioned Optical Passive Star Networks", 
    "publish": "2005-02-26T00:07:43Z", 
    "summary": "This paper establishes the state of the art in both deterministic and\nrandomized online permutation routing in the POPS network. Indeed, we show that\nany permutation can be routed online on a POPS network either with\n$O(\\frac{d}{g}\\log g)$ deterministic slots, or, with high probability, with\n$5c\\lceil d/g\\rceil+o(d/g)+O(\\log\\log g)$ randomized slots, where constant\n$c=\\exp (1+e^{-1})\\approx 3.927$. When $d=\\Theta(g)$, that we claim to be the\n\"interesting\" case, the randomized algorithm is exponentially faster than any\nother algorithm in the literature, both deterministic and randomized ones. This\nis true in practice as well. Indeed, experiments show that it outperforms its\nrivals even starting from as small a network as a POPS(2,2), and the gap grows\nexponentially with the size of the network. We can also show that, under proper\nhypothesis, no deterministic algorithm can asymptotically match its\nperformance.", 
    "link": "http://arxiv.org/pdf/cs/0502093v1", 
    "arxiv-id": "cs/0502093v1"
},{
    "category": "cs.DC", 
    "author": "Gr\u00e9gory Mouni\u00e9", 
    "title": "Pr\u00e9diction de Performances pour les Communications Collectives", 
    "publish": "2005-03-04T17:09:23Z", 
    "summary": "Des travaux r\\'{e}cents visent l'optimisation des op\\'{e}rations de\ncommunication collective dans les environnements de type grille de calcul. La\nsolution la plus r\\'{e}pandue est la s\\'{e}paration des communications internes\net externes \\`{a} chaque grappe, mais cela n'exclut pas le d\\'{e}coupage des\ncommunications en plusieurs couches, pratique efficace d\\'{e}montr\\'{e}e par\nKaronis et al. [10]. Dans les deux cas, la pr\\'{e}diction des performances est\nun facteur essentiel, soit pour le r\\'{e}glage fin des param\\`{e}tres de\ncommunication, soit pour le calcul de la distribution et de la hi\\'{e}rarchie\ndes communications. Pour cela, il est tr\\`{e}s important d'avoir des\nmod\\`{e}les pr\\'{e}cis des communications collectives, lesquels seront\nutilis\\'{e}s pour pr\\'{e}dire ces performances. Cet article d\\'{e}crit notre\nexp\\'{e}rience sur la mod\\'{e}lisation des op\\'{e}rations de communication\ncollective. Nous pr\\'{e}sentons des mod\\`{e}les de communication pour\ndiff\\'{e}rents patrons de communication collective comme un vers plusieurs, un\nvers plusieurs personnalis\\'{e} et plusieurs vers plusieurs. Pour \\'{e}valuer\nla pr\\'{e}cision des mod\\`{e}les, nous comparons les pr\\'{e}dictions obtenues\navec les r\\'{e}sultats des exp\\'{e}rimentations effectu\\'{e}es sur deux\nenvironnements r\\'{e}seaux diff\\'{e}rents, Fast Ethernet et Myrinet.", 
    "link": "http://arxiv.org/pdf/cs/0503013v1", 
    "arxiv-id": "cs/0503013v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "A Taxonomy of Workflow Management Systems for Grid Computing", 
    "publish": "2005-03-11T01:13:07Z", 
    "summary": "With the advent of Grid and application technologies, scientists and\nengineers are building more and more complex applications to manage and process\nlarge data sets, and execute scientific experiments on distributed resources.\nSuch application scenarios require means for composing and executing complex\nworkflows. Therefore, many efforts have been made towards the development of\nworkflow management systems for Grid computing. In this paper, we propose a\ntaxonomy that characterizes and classifies various approaches for building and\nexecuting workflows on Grids. We also survey several representative Grid\nworkflow systems developed by various projects world-wide to demonstrate the\ncomprehensiveness of the taxonomy. The taxonomy not only highlights the design\nand engineering similarities and differences of state-of-the-art in Grid\nworkflow systems, but also identifies the areas that need further research.", 
    "link": "http://arxiv.org/pdf/cs/0503025v2", 
    "arxiv-id": "cs/0503025v2"
},{
    "category": "cs.DC", 
    "author": "Peter Love", 
    "title": "Contextual Constraint Modeling in Grid Application Workflows", 
    "publish": "2005-03-19T01:28:48Z", 
    "summary": "This paper introduces a new mechanism for specifying constraints in\ndistributed workflows. By introducing constraints in a contextual form, it is\nshown how different people and groups within collaborative communities can\ncooperatively constrain workflows. A comparison with existing state-of-the-art\nworkflow systems is made. These ideas are explored in practice with an\nillustrative example from High Energy Physics.", 
    "link": "http://arxiv.org/pdf/cs/0503045v1", 
    "arxiv-id": "cs/0503045v1"
},{
    "category": "cs.DC", 
    "author": "Muhammad Adeel Zafar", 
    "title": "Resource Management Services for a Grid Analysis Environment", 
    "publish": "2005-04-10T11:59:25Z", 
    "summary": "Selecting optimal resources for submitting jobs on a computational Grid or\naccessing data from a data grid is one of the most important tasks of any Grid\nmiddleware. Most modern Grid software today satisfies this responsibility and\ngives a best-effort performance to solve this problem. Almost all decisions\nregarding scheduling and data access are made by the software automatically,\ngiving users little or no control over the entire process. To solve this\nproblem, a more interactive set of services and middleware is desired that\nprovides users more information about Grid weather, and gives them more control\nover the decision making process. This paper presents a set of services that\nhave been developed to provide more interactive resource management\ncapabilities within the Grid Analysis Environment (GAE) being developed\ncollaboratively by Caltech, NUST and several other institutes. These include a\nsteering service, a job monitoring service and an estimator service that have\nbeen designed and written using a common Grid-enabled Web Services framework\nnamed Clarens. The paper also presents a performance analysis of the developed\nservices to show that they have indeed resulted in a more interactive and\npowerful system for user-centric Grid-enabled physics analysis.", 
    "link": "http://arxiv.org/pdf/cs/0504033v1", 
    "arxiv-id": "cs/0504033v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "Heterogeneous Relational Databases for a Grid-enabled Analysis   Environment", 
    "publish": "2005-04-10T12:05:03Z", 
    "summary": "Grid based systems require a database access mechanism that can provide\nseamless homogeneous access to the requested data through a virtual data access\nsystem, i.e. a system which can take care of tracking the data that is stored\nin geographically distributed heterogeneous databases. This system should\nprovide an integrated view of the data that is stored in the different\nrepositories by using a virtual data access mechanism, i.e. a mechanism which\ncan hide the heterogeneity of the backend databases from the client\napplications. This paper focuses on accessing data stored in disparate\nrelational databases through a web service interface, and exploits the features\nof a Data Warehouse and Data Marts. We present a middleware that enables\napplications to access data stored in geographically distributed relational\ndatabases without being aware of their physical locations and underlying\nschema. A web service interface is provided to enable applications to access\nthis middleware in a language and platform independent way. A prototype\nimplementation was created based on Clarens [4], Unity [7] and POOL [8]. This\nability to access the data stored in the distributed relational databases\ntransparently is likely to be a very powerful one for Grid users, especially\nthe scientific community wishing to collate and analyze data distributed over\nthe Grid.", 
    "link": "http://arxiv.org/pdf/cs/0504034v1", 
    "arxiv-id": "cs/0504034v1"
},{
    "category": "cs.DC", 
    "author": "Jang Uk In", 
    "title": "JClarens: A Java Framework for Developing and Deploying Web Services for   Grid Computing", 
    "publish": "2005-04-11T21:45:07Z", 
    "summary": "High Energy Physics (HEP) and other scientific communities have adopted\nService Oriented Architectures (SOA) as part of a larger Grid computing effort.\nThis effort involves the integration of many legacy applications and\nprogramming libraries into a SOA framework. The Grid Analysis Environment (GAE)\nis such a service oriented architecture based on the Clarens Grid Services\nFramework and is being developed as part of the Compact Muon Solenoid (CMS)\nexperiment at the Large Hadron Collider (LHC) at European Laboratory for\nParticle Physics (CERN). Clarens provides a set of authorization, access\ncontrol, and discovery services, as well as XMLRPC and SOAP access to all\ndeployed services. Two implementations of the Clarens Web Services Framework\n(Python and Java) offer integration possibilities for a wide range of\nprogramming languages. This paper describes the Java implementation of the\nClarens Web Services Framework called JClarens. and several web services of\ninterest to the scientific and Grid community that have been deployed using\nJClarens.", 
    "link": "http://arxiv.org/pdf/cs/0504044v1", 
    "arxiv-id": "cs/0504044v1"
},{
    "category": "cs.DC", 
    "author": "Sean Keller", 
    "title": "SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters", 
    "publish": "2005-05-31T21:05:03Z", 
    "summary": "Modern high-performance computing relies heavily on the use of commodity\nprocessors arranged together in clusters. These clusters consist of individual\nnodes (typically off-the-shelf single or dual processor machines) connected\ntogether with a high speed interconnect. Using cluster computation has many\nbenefits, but also carries the liability of being failure prone due to the\nsheer number of components involved. Many effective solutions have been\nproposed to aid failure recovery in clusters, their one significant downside\nbeing the failure models they support. Most of the work in the area has focused\non detecting and correcting fail-stop errors. We propose a system that will\nalso detect more general error models, such as Byzantine errors, thus allowing\nexisting failure recovery methods to handle them correctly.", 
    "link": "http://arxiv.org/pdf/cs/0506001v1", 
    "arxiv-id": "cs/0506001v1"
},{
    "category": "cs.DC", 
    "author": "Olivier Richard", 
    "title": "A batch scheduler with high level components", 
    "publish": "2005-06-02T13:04:14Z", 
    "summary": "In this article we present the design choices and the evaluation of a batch\nscheduler for large clusters, named OAR. This batch scheduler is based upon an\noriginal design that emphasizes on low software complexity by using high level\ntools. The global architecture is built upon the scripting language Perl and\nthe relational database engine Mysql. The goal of the project OAR is to prove\nthat it is possible today to build a complex system for ressource management\nusing such tools without sacrificing efficiency and scalability. Currently, our\nsystem offers most of the important features implemented by other batch\nschedulers such as priority scheduling (by queues), reservations, backfilling\nand some global computing support. Despite the use of high level tools, our\nexperiments show that our system has performances close to other systems.\nFurthermore, OAR is currently exploited for the management of 700 nodes (a\nmetropolitan GRID) and has shown good efficiency and robustness.", 
    "link": "http://arxiv.org/pdf/cs/0506006v1", 
    "arxiv-id": "cs/0506006v1"
},{
    "category": "cs.DC", 
    "author": "Samuel Thibault", 
    "title": "A Flexible Thread Scheduler for Hierarchical Multiprocessor Machines", 
    "publish": "2005-06-27T14:32:50Z", 
    "summary": "With the current trend of multiprocessor machines towards more and more\nhierarchical architectures, exploiting the full computational power requires\ncareful distribution of execution threads and data so as to limit expensive\nremote memory accesses. Existing multi-threaded libraries provide only limited\nfacilities to let applications express distribution indications, so that\nprogrammers end up with explicitly distributing tasks according to the\nunderlying architecture, which is difficult and not portable. In this article,\nwe present: (1) a model for dynamically expressing the structure of the\ncomputation; (2) a scheduler interpreting this model so as to make judicious\nhierarchical distribution decisions; (3) an implementation within the Marcel\nuser-level thread library. We experimented our proposal on a scientific\napplication running on a ccNUMA Bull NovaScale with 16 Intel Itanium II\nprocessors; results show a 30% gain compared to a classical scheduler, and are\nsimilar to what a handmade scheduler achieves in a non-portable way.", 
    "link": "http://arxiv.org/pdf/cs/0506097v1", 
    "arxiv-id": "cs/0506097v1"
},{
    "category": "cs.DC", 
    "author": "Jose Rolim", 
    "title": "Energy Optimal Data Propagation in Wireless Sensor Networks", 
    "publish": "2005-08-10T15:54:13Z", 
    "summary": "We propose an algorithm which produces a randomized strategy reaching optimal\ndata propagation in wireless sensor networks (WSN).In [6] and [8], an energy\nbalanced solution is sought using an approximation algorithm. Our algorithm\nimproves by (a) when an energy-balanced solution does not exist, it still finds\nan optimal solution (whereas previous algorithms did not consider this case and\nprovide no useful solution) (b) instead of being an approximation algorithm, it\nfinds the exact solution in one pass. We also provide a rigorous proof of the\noptimality of our solution.", 
    "link": "http://arxiv.org/pdf/cs/0508052v1", 
    "arxiv-id": "cs/0508052v1"
},{
    "category": "cs.DC", 
    "author": "Vittorio Selmin", 
    "title": "Virtual Environments for multiphysics code validation on Computing Grids", 
    "publish": "2005-10-26T12:26:57Z", 
    "summary": "We advocate in this paper the use of grid-based infrastructures that are\ndesigned for seamless approaches to the numerical expert users, i.e., the\nmultiphysics applications designers. It relies on sophisticated computing\nenvironments based on computing grids, connecting heterogeneous computing\nresources: mainframes, PC-clusters and workstations running multiphysics codes\nand utility software, e.g., visualization tools. The approach is based on\nconcepts defined by the HEAVEN* consortium. HEAVEN is a European scientific\nconsortium including industrial partners from the aerospace, telecommunication\nand software industries, as well as academic research institutes. Currently,\nthe HEAVEN consortium works on a project that aims to create advanced services\nplatforms. It is intended to enable \"virtual private grids\" supporting various\nenvironments for users manipulating a suitable high-level interface. This will\nbecome the basis for future generalized services allowing the integration of\nvarious services without the need to deploy specific grid infrastructures.", 
    "link": "http://arxiv.org/pdf/cs/0510081v1", 
    "arxiv-id": "cs/0510081v1"
},{
    "category": "cs.DC", 
    "author": "Jose Rolim", 
    "title": "Gradient Based Routing in Wireless Sensor Networks: a Mixed Strategy", 
    "publish": "2005-11-23T18:13:18Z", 
    "summary": "We show how recent theoretical advances for data-propagation in Wireless\nSensor Networks (WSNs) can be combined to improve gradient-based routing (GBR)\nin Wireless Sensor Networks. We propose a mixed-strategy of direct transmission\nand multi-hop propagation of data which improves the lifespan of WSNs by\nreaching better energy-load-balancing amongst sensor nodes.", 
    "link": "http://arxiv.org/pdf/cs/0511083v1", 
    "arxiv-id": "cs/0511083v1"
},{
    "category": "cs.DC", 
    "author": "Agostinho C. Rosa", 
    "title": "On Ants, Bacteria and Dynamic Environments", 
    "publish": "2005-12-01T04:52:00Z", 
    "summary": "Wasps, bees, ants and termites all make effective use of their environment\nand resources by displaying collective swarm intelligence. Termite colonies -\nfor instance - build nests with a complexity far beyond the comprehension of\nthe individual termite, while ant colonies dynamically allocate labor to\nvarious vital tasks such as foraging or defense without any central\ndecision-making ability. Recent research suggests that microbial life can be\neven richer: highly social, intricately networked, and teeming with\ninteractions, as found in bacteria. What strikes from these observations is\nthat both ant colonies and bacteria have similar natural mechanisms based on\nStigmergy and Self-Organization in order to emerge coherent and sophisticated\npatterns of global behaviour. Keeping in mind the above characteristics we will\npresent a simple model to tackle the collective adaptation of a social swarm\nbased on real ant colony behaviors (SSA algorithm) for tracking extrema in\ndynamic environments and highly multimodal complex functions described in the\nwell-know De Jong test suite. Then, for the purpose of comparison, a recent\nmodel of artificial bacterial foraging (BFOA algorithm) based on similar\nstigmergic features is described and analyzed. Final results indicate that the\nSSA collective intelligence is able to cope and quickly adapt to unforeseen\nsituations even when over the same cooperative foraging period, the community\nis requested to deal with two different and contradictory purposes, while\noutperforming BFOA in adaptive speed.", 
    "link": "http://arxiv.org/pdf/cs/0512005v1", 
    "arxiv-id": "cs/0512005v1"
},{
    "category": "cs.DC", 
    "author": "Nuno Santos", 
    "title": "Exploring high performance distributed file storage using LDPC codes", 
    "publish": "2006-01-17T13:30:47Z", 
    "summary": "We explore the feasibility of implementing a reliable, high performance,\ndistributed storage system on a commodity computing cluster. Files are\ndistributed across storage nodes using erasure coding with small Low-Density\nParity-Check (LDPC) codes which provide high reliability while keeping the\nstorage and performance overhead small. We present performance measurements\ndone on a prototype system comprising 50 nodes which are self organised using a\npeer-to-peer overlay.", 
    "link": "http://arxiv.org/pdf/cs/0601078v1", 
    "arxiv-id": "cs/0601078v1"
},{
    "category": "cs.DC", 
    "author": "Ted Herman", 
    "title": "Localization in Wireless Sensor Grids", 
    "publish": "2006-01-25T23:48:12Z", 
    "summary": "This work reports experiences on using radio ranging to position sensors in a\ngrid topology. The implementation is simple, efficient, and could be\npractically distributed. The paper describes an implementation and experimental\nresult based on RSSI distance estimation. Novel techniques such as fuzzy\nmembership functions and table lookup are used to obtain more accurate result\nand simplify the computation. An 86% accuracy is achieved in the experiment in\nspite of inaccurate RSSI distance estimates with errors up to 60%.", 
    "link": "http://arxiv.org/pdf/cs/0601111v1", 
    "arxiv-id": "cs/0601111v1"
},{
    "category": "cs.DC", 
    "author": "David Pritchard", 
    "title": "An Optimal Distributed Edge-Biconnectivity Algorithm", 
    "publish": "2006-02-05T20:47:23Z", 
    "summary": "We describe a synchronous distributed algorithm which identifies the\nedge-biconnected components of a connected network. It requires a leader, and\nuses messages of size O(log |V|). The main idea is to preorder a BFS spanning\ntree, and then to efficiently compute least common ancestors so as to mark\ncycle edges. This algorithm takes O(Diam) time and uses O(|E|) messages.\nFurthermore, we show that no correct singly-initiated edge-biconnectivity\nalgorithm can beat either bound on any graph by more than a constant factor. We\nalso describe a near-optimal local algorithm for edge-biconnectivity.", 
    "link": "http://arxiv.org/pdf/cs/0602013v1", 
    "arxiv-id": "cs/0602013v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "Bulk Scheduling with DIANA Scheduler", 
    "publish": "2006-02-07T16:47:16Z", 
    "summary": "Results from and progress on the development of a Data Intensive and Network\nAware (DIANA) Scheduling engine, primarily for data intensive sciences such as\nphysics analysis, are described. Scientific analysis tasks can involve\nthousands of computing, data handling, and network resources and the size of\nthe input and output files and the amount of overall storage space allocated to\na user necessarily can have significant bearing on the scheduling of data\nintensive applications. If the input or output files must be retrieved from a\nremote location, then the time required transferring the files must also be\ntaken into consideration when scheduling compute resources for the given\napplication. The central problem in this study is the coordinated management of\ncomputation and data at multiple locations and not simply data movement.\nHowever, this can be a very costly operation and efficient scheduling can be a\nchallenge if compute and data resources are mapped without network cost. We\nhave implemented an adaptive algorithm within the DIANA Scheduler which takes\ninto account data location and size, network performance and computation\ncapability to make efficient global scheduling decisions. DIANA is a\nperformance-aware as well as an economy-guided Meta Scheduler. It iteratively\nallocates each job to the site that is likely to produce the best performance\nas well as optimizing the global queue for any remaining pending jobs.\nTherefore it is equally suitable whether a single job is being submitted or\nbulk scheduling is being performed. Results suggest that considerable\nperformance improvements are to be gained by adopting the DIANA scheduling\napproach.", 
    "link": "http://arxiv.org/pdf/cs/0602026v1", 
    "arxiv-id": "cs/0602026v1"
},{
    "category": "cs.DC", 
    "author": "Ruth Warren", 
    "title": "Final Results from and Exploitation Plans for MammoGrid", 
    "publish": "2006-03-09T13:49:14Z", 
    "summary": "The MammoGrid project has delivered the first deployed instance of a\nhealthgrid for clinical mammography that spans national boundaries. During the\nlast year, the final MammoGrid prototype has undergone a series of rigorous\ntests undertaken by radiologists in the UK and Italy and this paper draws\nconclusions from those tests for the benefit of the Healthgrid community. In\naddition, lessons learned during the lifetime of the project are detailed and\nrecommendations drawn for future health applications using grids. Following the\ncompletion of the project, plans have been put in place for the\ncommercialisation of the MammoGrid system and this is also reported in this\narticle. Particular emphasis is placed on the issues surrounding the transition\nfrom collaborative research project to a marketable product. This paper\nconcludes by highlighting some of the potential areas of future development and\nresearch.", 
    "link": "http://arxiv.org/pdf/cs/0603035v2", 
    "arxiv-id": "cs/0603035v2"
},{
    "category": "cs.DC", 
    "author": "ZHOU", 
    "title": "Health-e-Child : An Integrated Biomedical Platform for Grid-Based   Paediatric Applications", 
    "publish": "2006-03-09T13:54:15Z", 
    "summary": "There is a compelling demand for the integration and exploitation of\nheterogeneous biomedical information for improved clinical practice, medical\nresearch, and personalised healthcare across the EU. The Health-e-Child project\naims at developing an integrated healthcare platform for European Paediatrics,\nproviding seamless integration of traditional and emerging sources of\nbiomedical information. The long-term goal of the project is to provide\nuninhibited access to universal biomedical knowledge repositories for\npersonalised and preventive healthcare, large-scale information-based\nbiomedical research and training, and informed policy making. The project focus\nwill be on individualised disease prevention, screening, early diagnosis,\ntherapy and follow-up of paediatric heart diseases, inflammatory diseases, and\nbrain tumours. The project will build a Grid-enabled European network of\nleading clinical centres that will share and annotate biomedical data, validate\nsystems clinically, and diffuse clinical excellence across Europe by setting up\nnew technologies, clinical workflows, and standards. This paper outlines the\ndesign approach being adopted in Health-e-Child to enable the delivery of an\nintegrated biomedical information platform.", 
    "link": "http://arxiv.org/pdf/cs/0603036v2", 
    "arxiv-id": "cs/0603036v2"
},{
    "category": "cs.DC", 
    "author": "Vwani P. Roychowdhury", 
    "title": "A General Framework for Scalability and Performance Analysis of DHT   Routing Systems", 
    "publish": "2006-03-28T22:54:37Z", 
    "summary": "In recent years, many DHT-based P2P systems have been proposed, analyzed, and\ncertain deployments have reached a global scale with nearly one million nodes.\nOne is thus faced with the question of which particular DHT system to choose,\nand whether some are inherently more robust and scalable.\n  Toward developing such a comparative framework, we present the reachable\ncomponent method (RCM) for analyzing the performance of different DHT routing\nsystems subject to random failures. We apply RCM to five DHT systems and obtain\nanalytical expressions that characterize their routability as a continuous\nfunction of system size and node failure probability. An important consequence\nis that in the large-network limit, the routability of certain DHT systems go\nto zero for any non-zero probability of node failure. These DHT routing\nalgorithms are therefore unscalable, while some others, including Kademlia,\nwhich powers the popular eDonkey P2P system, are found to be scalable.", 
    "link": "http://arxiv.org/pdf/cs/0603112v1", 
    "arxiv-id": "cs/0603112v1"
},{
    "category": "cs.DC", 
    "author": "for the CMS Collaboration", 
    "title": "CMS Software Distribution on the LCG and OSG Grids", 
    "publish": "2006-04-27T16:14:32Z", 
    "summary": "The efficient exploitation of worldwide distributed storage and computing\nresources available in the grids require a robust, transparent and fast\ndeployment of experiment specific software. The approach followed by the CMS\nexperiment at CERN in order to enable Monte-Carlo simulations, data analysis\nand software development in an international collaboration is presented. The\ncurrent status and future improvement plans are described.", 
    "link": "http://arxiv.org/pdf/cs/0604109v1", 
    "arxiv-id": "cs/0604109v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Gridscape II: A Customisable and Pluggable Grid Monitoring Portal and   its Integration with Google Maps", 
    "publish": "2006-05-12T09:41:28Z", 
    "summary": "Grid computing has emerged as an effective means of facilitating the sharing\nof distributed heterogeneous resources, enabling collaboration in large scale\nenvironments. However, the nature of Grid systems, coupled with the\noverabundance and fragmentation of information, makes it difficult to monitor\nresources, services, and computations in order to plan and make decisions. In\nthis paper we present Gridscape II, a customisable portal component that can be\nused on its own or plugged in to compliment existing Grid portals. Gridscape II\nmanages the gathering of information from arbitrary, heterogeneous and\ndistributed sources and presents them together seamlessly within a single\ninterface. It also leverages the Google Maps API in order to provide a highly\ninteractive user interface. Gridscape II is simple and easy to use, providing a\nsolution to those users who do not wish to invest heavily in developing their\nown monitoring portal from scratch, and also for those users who want something\nthat is easy to customise and extend for their specific needs.", 
    "link": "http://arxiv.org/pdf/cs/0605053v1", 
    "arxiv-id": "cs/0605053v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Utility Computing and Global Grids", 
    "publish": "2006-05-12T20:36:45Z", 
    "summary": "This chapter focuses on the use of Grid technologies to achieve utility\ncomputing. An overview of how Grids can support utility computing is first\npresented through the architecture of Utility Grids. Then, utility-based\nresource allocation is described in detail at each level of the architecture.\nFinally, some industrial solutions for utility computing are discussed.", 
    "link": "http://arxiv.org/pdf/cs/0605056v1", 
    "arxiv-id": "cs/0605056v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "SLA-Based Coordinated Superscheduling Scheme and Performance for   Computational Grids", 
    "publish": "2006-05-15T13:41:27Z", 
    "summary": "The Service Level Agreement~(SLA) based grid superscheduling approach\npromotes coordinated resource sharing. Superscheduling is facilitated between\nadministratively and topologically distributed grid sites by grid schedulers\nsuch as Resource brokers. In this work, we present a market-based SLA\ncoordination mechanism. We based our SLA model on a well known \\emph{contract\nnet protocol}.\n  The key advantages of our approach are that it allows:~(i) resource owners to\nhave finer degree of control over the resource allocation that was previously\nnot possible through traditional mechanism; and (ii) superschedulers to bid for\nSLA contracts in the contract net with focus on completing the job within the\nuser specified deadline. In this work, we use simulation to show the\neffectiveness of our proposed approach.", 
    "link": "http://arxiv.org/pdf/cs/0605057v1", 
    "arxiv-id": "cs/0605057v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "A Case for Cooperative and Incentive-Based Coupling of Distributed   Clusters", 
    "publish": "2006-05-15T10:21:22Z", 
    "summary": "Research interest in Grid computing has grown significantly over the past\nfive years. Management of distributed resources is one of the key issues in\nGrid computing. Central to management of resources is the effectiveness of\nresource allocation as it determines the overall utility of the system. The\ncurrent approaches to superscheduling in a grid environment are non-coordinated\nsince application level schedulers or brokers make scheduling decisions\nindependently of the others in the system. Clearly, this can exacerbate the\nload sharing and utilization problems of distributed resources due to\nsuboptimal schedules that are likely to occur. To overcome these limitations,\nwe propose a mechanism for coordinated sharing of distributed clusters based on\ncomputational economy. The resulting environment, called\n\\emph{Grid-Federation}, allows the transparent use of resources from the\nfederation when local resources are insufficient to meet its users'\nrequirements. The use of computational economy methodology in coordinating\nresource allocation not only facilitates the QoS based scheduling, but also\nenhances utility delivered by resources.", 
    "link": "http://arxiv.org/pdf/cs/0605060v1", 
    "arxiv-id": "cs/0605060v1"
},{
    "category": "cs.DC", 
    "author": "Amos Korman", 
    "title": "General Compact Labeling Schemes for Dynamic Trees", 
    "publish": "2006-05-30T13:54:26Z", 
    "summary": "Let $F$ be a function on pairs of vertices. An {\\em $F$- labeling scheme} is\ncomposed of a {\\em marker} algorithm for labeling the vertices of a graph with\nshort labels, coupled with a {\\em decoder} algorithm allowing one to compute\n$F(u,v)$ of any two vertices $u$ and $v$ directly from their labels. As\napplications for labeling schemes concern mainly large and dynamically changing\nnetworks, it is of interest to study {\\em distributed dynamic} labeling\nschemes. This paper investigates labeling schemes for dynamic trees.\n  This paper presents a general method for constructing labeling schemes for\ndynamic trees. Our method is based on extending an existing {\\em static} tree\nlabeling scheme to the dynamic setting. This approach fits many natural\nfunctions on trees, such as ancestry relation, routing (in both the adversary\nand the designer port models), nearest common ancestor etc.. Our resulting\ndynamic schemes incur overheads (over the static scheme) on the label size and\non the communication complexity. Informally, for any function $k(n)$ and any\nstatic $F$-labeling scheme on trees, we present an $F$-labeling scheme on\ndynamic trees incurring multiplicative overhead factors\n  (over the static scheme) of $O(\\log_{k(n)} n)$ on the label size and\n$O(k(n)\\log_{k(n)} n)$ on the amortized message complexity. In particular, by\nsetting $k(n)=n^{\\epsilon}$ for any $0<\\epsilon<1$, we obtain dynamic labeling\nschemes with asymptotically optimal label sizes and sublinear amortized message\ncomplexity for all the above mentioned functions.", 
    "link": "http://arxiv.org/pdf/cs/0605141v1", 
    "arxiv-id": "cs/0605141v1"
},{
    "category": "cs.DC", 
    "author": "N. A. Likhoded", 
    "title": "Affine Transformations of Loop Nests for Parallel Execution and   Distribution of Data over Processors", 
    "publish": "2006-06-07T12:59:49Z", 
    "summary": "The paper is devoted to the problem of mapping affine loop nests onto\ndistributed memory parallel computers. A method to find affine transformations\nof loop nests for parallel execution and distribution of data over processors\nis presented. The method tends to minimize the number of communications between\nprocessors and to improve locality of data within one processor. A problem of\ndetermination of data exchange sequence is investigated. Conditions to\ndetermine the ability to arrange broadcast is presented.", 
    "link": "http://arxiv.org/pdf/cs/0606028v1", 
    "arxiv-id": "cs/0606028v1"
},{
    "category": "cs.DC", 
    "author": "Daniel B. Szyld", 
    "title": "Asynchronous iterative computations with Web information retrieval   structures: The PageRank case", 
    "publish": "2006-06-11T09:36:26Z", 
    "summary": "There are several ideas being used today for Web information retrieval, and\nspecifically in Web search engines. The PageRank algorithm is one of those that\nintroduce a content-neutral ranking function over Web pages. This ranking is\napplied to the set of pages returned by the Google search engine in response to\nposting a search query. PageRank is based in part on two simple common sense\nconcepts: (i)A page is important if many important pages include links to it.\n(ii)A page containing many links has reduced impact on the importance of the\npages it links to. In this paper we focus on asynchronous iterative schemes to\ncompute PageRank over large sets of Web pages. The elimination of the\nsynchronizing phases is expected to be advantageous on heterogeneous platforms.\nThe motivation for a possible move to such large scale distributed platforms\nlies in the size of matrices representing Web structure. In orders of\nmagnitude: $10^{10}$ pages with $10^{11}$ nonzero elements and $10^{12}$ bytes\njust to store a small percentage of the Web (the already crawled); distributed\nmemory machines are necessary for such computations. The present research is\npart of our general objective, to explore the potential of asynchronous\ncomputational models as an underlying framework for very large scale\ncomputations over the Grid. The area of ``internet algorithmics'' appears to\noffer many occasions for computations of unprecedent dimensionality that would\nbe good candidates for this framework.", 
    "link": "http://arxiv.org/pdf/cs/0606047v1", 
    "arxiv-id": "cs/0606047v1"
},{
    "category": "cs.DC", 
    "author": "Jean Krivine", 
    "title": "A verification algorithm for Declarative Concurrent Programming", 
    "publish": "2006-06-22T13:23:15Z", 
    "summary": "A verification method for distributed systems based on decoupling forward and\nbackward behaviour is proposed. This method uses an event structure based\nalgorithm that, given a CCS process, constructs its causal compression relative\nto a choice of observable actions. Verifying the original process equipped with\ndistributed backtracking on non-observable actions, is equivalent to verifying\nits relative compression which in general is much smaller. We call this method\nDeclarative Concurrent Programming (DCP). DCP technique compares well with\ndirect bisimulation based methods. Benchmarks for the classic dining\nphilosophers problem show that causal compression is rather efficient both\ntime- and space-wise. State of the art verification tools can successfully\nhandle more than 15 agents, whereas they can handle no more than 5 following\nthe traditional direct method; an altogether spectacular improvement, since in\nthis example the specification size is exponential in the number of agents.", 
    "link": "http://arxiv.org/pdf/cs/0606095v1", 
    "arxiv-id": "cs/0606095v1"
},{
    "category": "cs.DC", 
    "author": "Athar Mohsin", 
    "title": "From Grid Middleware to a Grid Operating System", 
    "publish": "2006-08-08T17:41:26Z", 
    "summary": "Grid computing has made substantial advances during the last decade. Grid\nmiddleware such as Globus has contributed greatly in making this possible.\nThere are, however, significant barriers to the adoption of Grid computing in\nother fields, most notably day-to-day user computing environments. We will\ndemonstrate in this paper that this is primarily due to the limitations of the\nexisting Grid middleware which does not take into account the needs of everyday\nscientific and business users. In this paper we will formally advocate a Grid\nOperating System and propose an architecture to migrate Grid computing into a\nGrid operating system which we believe would help remove most of the technical\nbarriers to the adoption of Grid computing and make it relevant to the\nday-to-day user. We believe this proposed transition to a Grid operating system\nwill drive more pervasive Grid computing research and application development\nand deployment in future.", 
    "link": "http://arxiv.org/pdf/cs/0608046v1", 
    "arxiv-id": "cs/0608046v1"
},{
    "category": "cs.DC", 
    "author": "A. E. Solomonides", 
    "title": "Lessons Learned from MammoGrid for Integrated Biomedical Solutions", 
    "publish": "2006-08-08T17:49:28Z", 
    "summary": "This paper presents an overview of the MammoGrid project and some of its\nachievements. In terms of the global grid project, and European research in\nparticular, the project has successfully demonstrated the capacity of a\ngrid-based system to support effective collaboration between physicians,\nincluding handling and querying image databases, as well as using grid\nservices, such as image standardization and Computer-Aided Detection (CADe) of\nsuspect or indicative features. In terms of scientific results, in radiology,\nthere have been significant epidemiological findings in the assessment of\nbreast density as a risk factor, but the results for CADe are less clear-cut.\nFinally, the foundations of a technology transfer process to establish a\nworking MammoGrid plus system in Spain through the company Maat GKnowledge and\nthe collaboration of CIEMAT and hospitals in Extremadura.", 
    "link": "http://arxiv.org/pdf/cs/0608047v1", 
    "arxiv-id": "cs/0608047v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "Bulk Scheduling with the DIANA Scheduler", 
    "publish": "2006-08-08T17:53:15Z", 
    "summary": "Results from the research and development of a Data Intensive and Network\nAware (DIANA) scheduling engine, to be used primarily for data intensive\nsciences such as physics analysis, are described. In Grid analyses, tasks can\ninvolve thousands of computing, data handling, and network resources. The\ncentral problem in the scheduling of these resources is the coordinated\nmanagement of computation and data at multiple locations and not just data\nreplication or movement. However, this can prove to be a rather costly\noperation and efficient sing can be a challenge if compute and data resources\nare mapped without considering network costs. We have implemented an adaptive\nalgorithm within the so-called DIANA Scheduler which takes into account data\nlocation and size, network performance and computation capability in order to\nenable efficient global scheduling. DIANA is a performance-aware and\neconomy-guided Meta Scheduler. It iteratively allocates each job to the site\nthat is most likely to produce the best performance as well as optimizing the\nglobal queue for any remaining jobs. Therefore it is equally suitable whether a\nsingle job is being submitted or bulk scheduling is being performed. Results\nindicate that considerable performance improvements can be gained by adopting\nthe DIANA scheduling approach.", 
    "link": "http://arxiv.org/pdf/cs/0608048v1", 
    "arxiv-id": "cs/0608048v1"
},{
    "category": "cs.DC", 
    "author": "Danny Dolev", 
    "title": "Self-Stabilizing Byzantine Pulse Synchronization", 
    "publish": "2006-08-24T16:07:07Z", 
    "summary": "The ``Pulse Synchronization'' problem can be loosely described as targeting\nto invoke a recurring distributed event as simultaneously as possible at the\ndifferent nodes and with a frequency that is as regular as possible. This\ntarget becomes surprisingly subtle and difficult to achieve when facing both\ntransient and permanent failures. In this paper we present an algorithm for\npulse synchronization that self-stabilizes while at the same time tolerating a\npermanent presence of Byzantine faults. The Byzantine nodes might incessantly\ntry to de-synchronize the correct nodes. Transient failures might throw the\nsystem into an arbitrary state in which correct nodes have no common notion\nwhat-so-ever, such as time or round numbers, and can thus not infer anything\nfrom their own local states upon the state of other correct nodes. The\npresented algorithm grants nodes the ability to infer that eventually all\ncorrect nodes will invoke their pulses within a very short time interval of\neach other and will do so regularly.\n  Pulse synchronization has previously been shown to be a powerful tool for\ndesigning general self-stabilizing Byzantine algorithms and is hitherto the\nonly method that provides for the general design of efficient practical\nprotocols in the confluence of these two fault models. The difficulty, in\ngeneral, to design any algorithm in this fault model may be indicated by the\nremarkably few algorithms resilient to both fault models. The few published\nself-stabilizing Byzantine algorithms are typically complicated and sometimes\nconverge from an arbitrary initial state only after exponential or super\nexponential time.", 
    "link": "http://arxiv.org/pdf/cs/0608092v2", 
    "arxiv-id": "cs/0608092v2"
},{
    "category": "cs.DC", 
    "author": "Hanna Parnas", 
    "title": "Linear-time Self-stabilizing Byzantine Clock Synchronization", 
    "publish": "2006-08-25T03:11:28Z", 
    "summary": "Clock synchronization is a very fundamental task in distributed system. It\nthus makes sense to require an underlying clock synchronization mechanism to be\nhighly fault-tolerant. A self-stabilizing algorithm seeks to attain\nsynchronization once lost; a Byzantine algorithm assumes synchronization is\nnever lost and focuses on containing the influence of the permanent presence of\nfaulty nodes. There are efficient self-stabilizing solutions for clock\nsynchronization as well as efficient solutions that are resilient to Byzantine\nfaults. In contrast, to the best of our knowledge there is no practical\nsolution that is self-stabilizing while tolerating the permanent presence of\nByzantine nodes. We present the first linear-time self-stabilizing Byzantine\nclock synchronization algorithm. Our deterministic clock synchronization\nalgorithm is based on the observation that all clock synchronization algorithms\nrequire events for exchanging clock values and re-synchronizing the clocks to\nwithin safe bounds. These events usually need to happen synchronously at the\ndifferent nodes. In classic Byzantine algorithms this is fulfilled or aided by\nhaving the clocks initially close to each other and thus the actual clock\nvalues can be used for synchronizing the events. This implies that clock values\ncannot differ arbitrarily, which necessarily renders these solutions to be\nnon-stabilizing. Our scheme suggests using an underlying distributed pulse\nsynchronization module that is uncorrelated to the clock values.", 
    "link": "http://arxiv.org/pdf/cs/0608096v1", 
    "arxiv-id": "cs/0608096v1"
},{
    "category": "cs.DC", 
    "author": "M-T. Kechadi", 
    "title": "Entity Based Peer-to-Peer in a Data Grid Environment", 
    "publish": "2006-08-29T11:58:50Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, the grid characterisation,\nprogramming environments, etc. In this paper we present a new Grid system\ndedicated to deal with data issues, called DGET (Data Grid Environment and\nTools). DGET is characterized by its peer-to-peer communication system and\nentity-based architecture, therefore, taking advantage of the main\nfunctionality of both systems; P2P and Grid. DGET is currently under\ndevelopment and a prototype implementing the main components is in its first\nphase of testing. In this paper we limit our description to the system\narchitectural features and to the main differences with other systems.", 
    "link": "http://arxiv.org/pdf/cs/0608112v1", 
    "arxiv-id": "cs/0608112v1"
},{
    "category": "cs.DC", 
    "author": "T. Kechadi", 
    "title": "A Java Based Architecture of P2P-Grid Middleware", 
    "publish": "2006-08-29T13:11:05Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, characterization, programming\nenvironments, etc. We present a new Grid system dedicated to dealing with data\nissues, called DGET (Data Grid Environment and Tools). DGET is characterized by\nits peerto- peer communication system and entity-based architecture, therefore,\ntaking advantage of the main functionality of both systems; P2P and Grid. DGET\nis currently under development and a prototype implementing the main components\nis in its first phase of testing. In this paper we limit our description to the\nsystem architectural features and to the main differences with other systems.\nKeywords: Grid Computing, Peer to Peer, Peer to Peer Grid", 
    "link": "http://arxiv.org/pdf/cs/0608113v1", 
    "arxiv-id": "cs/0608113v1"
},{
    "category": "cs.DC", 
    "author": "Serge Petiton", 
    "title": "Reliable multicast fault tolerant MPI in the Grid environment", 
    "publish": "2006-08-29T13:14:29Z", 
    "summary": "Grid environments have recently been developed with low stretch and overheads\nthat increase with the logarithm of the number of nodes in the system. Getting\nand sending data to/from a large numbers of nodes is gaining importance due to\nan increasing number of independent data providers and the heterogeneity of the\nnetwork/Grid. One of the key challenges is to achieve a balance between low\nbandwidth consumption and good reliability. In this paper we present an\nimplementation of a reliable multicast protocol over a fault tolerant MPI:\nMPICHV2. It can provide one way to solve the problem of transferring large\nchunks of data between applications running on a grid with limited network\nlinks. We first show that we can achieve similar performance as the MPICH-P4\nimplementation by using multicast with data compression in a cluster. Next, we\nprovide a theoretical cluster organization and GRID network architecture to\nharness the performance provided by using multicast. Finally, we present the\nconclusion and future work.", 
    "link": "http://arxiv.org/pdf/cs/0608114v1", 
    "arxiv-id": "cs/0608114v1"
},{
    "category": "cs.DC", 
    "author": "T. Kechadi", 
    "title": "Transparent Migration of Multi-Threaded Applications on a Java Based   Grid", 
    "publish": "2006-08-29T13:29:29Z", 
    "summary": "Grid computing has enabled pooling a very large number of heterogeneous\nresource administered by different security domains. Applications are\ndynamically deployed on the resources available at the time. Dynamic nature of\nthe resources and applications requirements makes needs the grid middleware to\nsupport the ability of migrating a running application to a different resource.\nEspecially, Grid applications are typically long running and thus stoping them\nand starting them from scratch is not a feasible option. This paper presents an\noverview of migration support in a java based grid middleware called DGET.\nMigration support in DGET includes multi-threaded migration and asynchronous\nmigration as well.", 
    "link": "http://arxiv.org/pdf/cs/0608116v1", 
    "arxiv-id": "cs/0608116v1"
},{
    "category": "cs.DC", 
    "author": "A. Ottewill", 
    "title": "TreeP: A Tree-Based P2P Network Architecture", 
    "publish": "2006-08-29T15:08:42Z", 
    "summary": "In this paper we proposed a hierarchical P2P network based on a dynamic\npartitioning on a 1-D space. This hierarchy is created and maintained\ndynamically and provides a gridmiddleware (like DGET) a P2P basic functionality\nfor resource discovery and load-balancing.This network architecture is called\nTreeP (Tree based P2P network architecture) and is based on atessellation of a\n1-D space. We show that this topology exploits in an efficient way\ntheheterogeneity feature of the network while limiting the overhead introduced\nby the overlaymaintenance. Experimental results show that this topology is\nhighly resilient to a large number ofnetwork failures.", 
    "link": "http://arxiv.org/pdf/cs/0608118v1", 
    "arxiv-id": "cs/0608118v1"
},{
    "category": "cs.DC", 
    "author": "Zahir Tari", 
    "title": "A Case for Peering of Content Delivery Networks", 
    "publish": "2006-09-06T23:41:32Z", 
    "summary": "The proliferation of Content Delivery Networks (CDN) reveals that existing\ncontent networks are owned and operated by individual companies. As a\nconsequence, closed delivery networks are evolved which do not cooperate with\nother CDNs and in practice, islands of CDNs are formed. Moreover, the logical\nseparation between contents and services in this context results in two content\nnetworking domains. But present trends in content networks and content\nnetworking capabilities give rise to the interest in interconnecting content\nnetworks. Finding ways for distinct content networks to coordinate and\ncooperate with other content networks is necessary for better overall service.\nIn addition to that, meeting the QoS requirements of users according to the\nnegotiated Service Level Agreements between the user and the content network is\na burning issue in this perspective. In this article, we present an open,\nscalable and Service-Oriented Architecture based system to assist the creation\nof open Content and Service Delivery Networks (CSDN) that scale and support\nsharing of resources with other CSDNs.", 
    "link": "http://arxiv.org/pdf/cs/0609027v1", 
    "arxiv-id": "cs/0609027v1"
},{
    "category": "cs.DC", 
    "author": "Shay Kutten", 
    "title": "Labeling Schemes with Queries", 
    "publish": "2006-09-29T12:31:35Z", 
    "summary": "We study the question of ``how robust are the known lower bounds of labeling\nschemes when one increases the number of consulted labels''. Let $f$ be a\nfunction on pairs of vertices. An $f$-labeling scheme for a family of graphs\n$\\cF$ labels the vertices of all graphs in $\\cF$ such that for every graph\n$G\\in\\cF$ and every two vertices $u,v\\in G$, the value $f(u,v)$ can be inferred\nby merely inspecting the labels of $u$ and $v$.\n  This paper introduces a natural generalization: the notion of $f$-labeling\nschemes with queries, in which the value $f(u,v)$ can be inferred by inspecting\nnot only the labels of $u$ and $v$ but possibly the labels of some additional\nvertices. We show that inspecting the label of a single additional vertex (one\n{\\em query}) enables us to reduce the label size of many labeling schemes\nsignificantly.", 
    "link": "http://arxiv.org/pdf/cs/0609163v1", 
    "arxiv-id": "cs/0609163v1"
},{
    "category": "cs.DC", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "title": "Scheduling and data redistribution strategies on star platforms", 
    "publish": "2006-10-23T07:45:01Z", 
    "summary": "In this work we are interested in the problem of scheduling and\nredistributing data on master-slave platforms. We consider the case were the\nworkers possess initial loads, some of which having to be redistributed in\norder to balance their completion times. We examine two different scenarios.\nThe first model assumes that the data consists of independent and identical\ntasks. We prove the NP-completeness in the strong sense for the general case,\nand we present two optimal algorithms for special platform types. Furthermore\nwe propose three heuristics for the general case. Simulations consolidate the\ntheoretical results. The second data model is based on Divisible Load Theory.\nThis problem can be solved in polynomial time by a combination of linear\nprogramming and simple analytical manipulations.", 
    "link": "http://arxiv.org/pdf/cs/0610131v1", 
    "arxiv-id": "cs/0610131v1"
},{
    "category": "cs.DC", 
    "author": "Veronika Rehn", 
    "title": "Strategies for Replica Placement in Tree Networks", 
    "publish": "2006-11-08T08:16:55Z", 
    "summary": "In this paper, we discuss and compare several policies to place replicas in\ntree networks, subject to server capacity and QoS constraints. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. The standard approach in the literature is to enforce that\nall requests of a client be served by the closest server in the tree. We\nintroduce and study two new policies. In the first policy, all requests from a\ngiven client are still processed by the same server, but this server can be\nlocated anywhere in the path from the client to the root. In the second policy,\nthe requests of a given client can be processed by multiple servers. One major\ncontribution of this paper is to assess the impact of these new policies on the\ntotal replication cost. Another important goal is to assess the impact of\nserver heterogeneity, both from a theoretical and a practical perspective. In\nthis paper, we establish several new complexity results, and provide several\nefficient polynomial heuristics for NP-complete instances of the problem. These\nheuristics are compared to an absolute lower bound provided by the formulation\nof the problem in terms of the solution of an integer linear program.", 
    "link": "http://arxiv.org/pdf/cs/0611034v1", 
    "arxiv-id": "cs/0611034v1"
},{
    "category": "cs.DC", 
    "author": "Xavier Thirioux", 
    "title": "Static Safety for an Actor Dedicated Process Calculus by Abstract   Interpretation", 
    "publish": "2006-11-28T07:48:18Z", 
    "summary": "The actor model eases the definition of concurrent programs with non uniform\nbehaviors. Static analysis of such a model was previously done in a data-flow\noriented way, with type systems. This approach was based on constraint set\nresolution and was not able to deal with precise properties for communications\nof behaviors. We present here a new approach, control-flow oriented, based on\nthe abstract interpretation framework, able to deal with communication of\nbehaviors. Within our new analyses, we are able to verify most of the previous\nproperties we observed as well as new ones, principally based on occurrence\ncounting.", 
    "link": "http://arxiv.org/pdf/cs/0611139v1", 
    "arxiv-id": "cs/0611139v1"
},{
    "category": "cs.DC", 
    "author": "Valmir C. Barbosa", 
    "title": "Partially ordered distributed computations on asynchronous   point-to-point networks", 
    "publish": "2006-11-30T13:01:36Z", 
    "summary": "Asynchronous executions of a distributed algorithm differ from each other due\nto the nondeterminism in the order in which the messages exchanged are handled.\nIn many situations of interest, the asynchronous executions induced by\nrestricting nondeterminism are more efficient, in an application-specific\nsense, than the others. In this work, we define partially ordered executions of\na distributed algorithm as the executions satisfying some restricted orders of\ntheir actions in two different frameworks, those of the so-called event- and\npulse-driven computations. The aim of these restrictions is to characterize\nasynchronous executions that are likely to be more efficient for some important\nclasses of applications. Also, an asynchronous algorithm that ensures the\noccurrence of partially ordered executions is given for each case. Two of the\napplications that we believe may benefit from the restricted nondeterminism are\nbacktrack search, in the event-driven case, and iterative algorithms for\nsystems of linear equations, in the pulse-driven case.", 
    "link": "http://arxiv.org/pdf/cs/0611165v1", 
    "arxiv-id": "cs/0611165v1"
},{
    "category": "cs.DC", 
    "author": "Kris Bubendorfer", 
    "title": "Economy-based Content Replication for Peering Content Delivery Networks", 
    "publish": "2006-12-04T06:29:16Z", 
    "summary": "Existing Content Delivery Networks (CDNs) exhibit the nature of closed\ndelivery networks which do not cooperate with other CDNs and in practice,\nislands of CDNs are formed. The logical separation between contents and\nservices in this context results in two content networking domains. In addition\nto that, meeting the Quality of Service requirements of users according to\nnegotiated Service Level Agreement is crucial for a CDN. Present trends in\ncontent networks and content networking capabilities give rise to the interest\nin interconnecting content networks. Hence, in this paper, we present an open,\nscalable, and Service-Oriented Architecture (SOA)-based system that assist the\ncreation of open Content and Service Delivery Networks (CSDNs), which scale and\nsupports sharing of resources through peering with other CSDNs. To encourage\nresource sharing and peering arrangements between different CDN providers at\nglobal level, we propose using market-based models by introducing an\neconomy-based strategy for content replication.", 
    "link": "http://arxiv.org/pdf/cs/0612013v2", 
    "arxiv-id": "cs/0612013v2"
},{
    "category": "cs.DC", 
    "author": "Paul M. B. Vitanyi", 
    "title": "Registers", 
    "publish": "2006-12-05T16:51:25Z", 
    "summary": "Entry in: Encyclopedia of Algorithms, Ming-Yang Kao, Ed., Springer, To\nappear.\n  Synonyms: Wait-free registers, wait-free shared variables, asynchronous\ncommunication hardware. Problem Definition: Consider a system of asynchronous\nprocesses that communicate among themselves by only executing read and write\noperations on a set of shared variables (also known as shared registers). The\nsystem has no global clock or other synchronization primitives.", 
    "link": "http://arxiv.org/pdf/cs/0612025v1", 
    "arxiv-id": "cs/0612025v1"
},{
    "category": "cs.DC", 
    "author": "Michel Raynal", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "publish": "2006-12-06T13:57:54Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services that are\ncapable of dealing with resource assignment and management in a large-scale,\nheterogeneous and unreliable environment. One such service, the slicing\nservice, has been proposed to allow for an automatic partitioning of P2P\nnetworks into groups (slices) that represent a controllable amount of some\nresource and that are also relatively homogeneous with respect to that\nresource, in the face of churn and other failures. In this report we propose\ntwo algorithms to solve the distributed slicing problem. The first algorithm\nimproves upon an existing algorithm that is based on gossip-based sorting of a\nset of uniform random numbers. We speed up convergence via a heuristic for\ngossip peer selection. The second algorithm is based on a different approach:\nstatistical approximation of the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms relies on\ntheir gossip-based models. We present theoretical and experimental results to\nprove the viability of these algorithms.", 
    "link": "http://arxiv.org/pdf/cs/0612035v1", 
    "arxiv-id": "cs/0612035v1"
},{
    "category": "cs.DC", 
    "author": "Bruno Schulze", 
    "title": "Heterogeneous Strong Computation Migration", 
    "publish": "2006-12-22T18:27:25Z", 
    "summary": "The continuous increase in performance requirements, for both scientific\ncomputation and industry, motivates the need of a powerful computing\ninfrastructure. The Grid appeared as a solution for inexpensive execution of\nheavy applications in a parallel and distributed manner. It allows combining\nresources independently of their physical location and architecture to form a\nglobal resource pool available to all grid users. However, grid environments\nare highly unstable and unpredictable. Adaptability is a crucial issue in this\ncontext, in order to guarantee an appropriate quality of service to users.\nMigration is a technique frequently used for achieving adaptation. The\nobjective of this report is to survey the problem of strong migration in\nheterogeneous environments like the grids', the related implementation issues\nand the current solutions.", 
    "link": "http://arxiv.org/pdf/cs/0612125v2", 
    "arxiv-id": "cs/0612125v2"
},{
    "category": "cs.DC", 
    "author": "Fabiola Greve", 
    "title": "Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants", 
    "publish": "2007-01-03T12:52:58Z", 
    "summary": "We consider the problem of failure detection in dynamic networks such as\nMANETs. Unreliable failure detectors are classical mechanisms which provide\ninformation about process failures. However, most of current implementations\nconsider that the network is fully connected and that the initial number of\nnodes of the system is known. This assumption is not applicable to dynamic\nenvironments. Furthermore, such implementations are usually timer-based while\nin dynamic networks there is no upper bound for communication delays since\nnodes can move. This paper presents an asynchronous implementation of a failure\ndetector for unknown and mobile networks. Our approach does not rely on timers\nand neither the composition nor the number of nodes in the system are known. We\nprove that our algorithm can implement failure detectors of class <>S when\nbehavioral properties and connectivity conditions are satisfied by the\nunderlying system.", 
    "link": "http://arxiv.org/pdf/cs/0701015v2", 
    "arxiv-id": "cs/0701015v2"
},{
    "category": "cs.DC", 
    "author": "Yoram Moses", 
    "title": "Causing Communication Closure: Safe Program Composition with Reliable   Non-FIFO Channels", 
    "publish": "2007-01-09T12:18:14Z", 
    "summary": "A semantic framework for analyzing safe composition of distributed programs\nis presented. Its applicability is illustrated by a study of program\ncomposition when communication is reliable but not necessarily FIFO\\@. In this\nmodel, special care must be taken to ensure that messages do not accidentally\novertake one another in the composed program. We show that barriers do not\nexist in this model. Indeed, no program that sends or receives messages can\nautomatically be composed with arbitrary programs without jeopardizing their\nintended behavior. Safety of composition becomes context-sensitive and new\ntools are needed for ensuring it. A notion of \\emph{sealing} is defined, where\nif a program $P$ is immediately followed by a program $Q$ that seals $P$ then\n$P$ will be communication-closed--it will execute as if it runs in isolation.\nThe investigation of sealing in this model reveals a novel connection between\nLamport causality and safe composition. A characterization of sealable programs\nis given, as well as efficient algorithms for testing if $Q$ seals $P$ and for\nconstructing a seal for a significant class of programs. It is shown that every\nsealable program that is open to interference on $O(n^2)$ channels can be\nsealed using O(n) messages.", 
    "link": "http://arxiv.org/pdf/cs/0701064v1", 
    "arxiv-id": "cs/0701064v1"
},{
    "category": "cs.DC", 
    "author": "Wenbing Zhao", 
    "title": "Byzantine Fault Tolerance for Nondeterministic Applications", 
    "publish": "2007-01-21T20:44:52Z", 
    "summary": "All practical applications contain some degree of nondeterminism. When such\napplications are replicated to achieve Byzantine fault tolerance (BFT), their\nnondeterministic operations must be controlled to ensure replica consistency.\nTo the best of our knowledge, only the most simplistic types of replica\nnondeterminism have been dealt with. Furthermore, there lacks a systematic\napproach to handling common types of nondeterminism. In this paper, we propose\na classification of common types of replica nondeterminism with respect to the\nrequirement of achieving Byzantine fault tolerance, and describe the design and\nimplementation of the core mechanisms necessary to handle such nondeterminism\nwithin a Byzantine fault tolerance framework.", 
    "link": "http://arxiv.org/pdf/cs/0701134v2", 
    "arxiv-id": "cs/0701134v2"
},{
    "category": "cs.DC", 
    "author": "Franck Petit", 
    "title": "Scatter of Weak Robots", 
    "publish": "2007-01-27T06:43:04Z", 
    "summary": "In this paper, we first formalize the problem to be solved, i.e., the Scatter\nProblem (SP). We then show that SP cannot be deterministically solved. Next, we\npropose a randomized algorithm for this problem. The proposed solution is\ntrivially self-stabilizing. We then show how to design a self-stabilizing\nversion of any deterministic solution for the Pattern Formation and the\nGathering problems.", 
    "link": "http://arxiv.org/pdf/cs/0701179v1", 
    "arxiv-id": "cs/0701179v1"
},{
    "category": "cs.DC", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "title": "Comments on \"Design and performance evaluation of load distribution   strategies for multiple loads on heterogeneous linear daisy chain networks''", 
    "publish": "2007-02-10T17:43:35Z", 
    "summary": "Min, Veeravalli, and Barlas proposed strategies to minimize the overall\nexecution time of one or several divisible loads on a heterogeneous linear\nnetwork, using one or more installments. We show on a very simple example that\nthe proposed approach does not always produce a solution and that, when it\ndoes, the solution is often suboptimal. We also show how to find an optimal\nscheduling for any instance, once the number of installments per load is given.\nFinally, we formally prove that under a linear cost model, as in the original\npaper, an optimal schedule has an infinite number of installments. Such a cost\nmodel can therefore not be sed to design practical multi-installment\nstrategies.", 
    "link": "http://arxiv.org/pdf/cs/0702066v1", 
    "arxiv-id": "cs/0702066v1"
},{
    "category": "cs.DC", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "title": "A First Step Towards Automatically Building Network Representations", 
    "publish": "2007-02-13T16:35:04Z", 
    "summary": "To fully harness Grids, users or middlewares must have some knowledge on the\ntopology of the platform interconnection network. As such knowledge is usually\nnot available, one must uses tools which automatically build a topological\nnetwork model through some measurements. In this article, we define a\nmethodology to assess the quality of these network model building tools, and we\napply this methodology to representatives of the main classes of model builders\nand to two new algorithms. We show that none of the main existing techniques\nbuild models that enable to accurately predict the running time of simple\napplication kernels for actual platforms. However some of the new algorithms we\npropose give excellent results in a wide range of situations.", 
    "link": "http://arxiv.org/pdf/cs/0702076v2", 
    "arxiv-id": "cs/0702076v2"
},{
    "category": "cs.DC", 
    "author": "David Pritchard", 
    "title": "Nearest Neighbor Network Traversal", 
    "publish": "2007-02-20T03:54:12Z", 
    "summary": "A mobile agent in a network wants to visit every node of an n-node network,\nusing a small number of steps. We investigate the performance of the following\n``nearest neighbor'' heuristic: always go to the nearest unvisited node. If the\nnetwork graph never changes, then from (Rosenkrantz, Stearns and Lewis, 1977)\nand (Hurkens and Woeginger, 2004) it follows that Theta(n log n) steps are\nnecessary and sufficient in the worst case. We give a simpler proof of the\nupper bound and an example that improves the best known lower bound.\n  We investigate how the performance of this heuristic changes when it is\ndistributively implemented in a network. Even if network edges are allow to\nfail over time, we show that the nearest neighbor strategy never runs for more\nthan O(n^2) iterations. We also show that any strategy can be forced to take at\nleast n(n-1)/2 steps before all nodes are visited, if the edges of the network\nare deleted in an adversarial way.", 
    "link": "http://arxiv.org/pdf/cs/0702114v1", 
    "arxiv-id": "cs/0702114v1"
},{
    "category": "cs.DC", 
    "author": "Sotiris Nikolesteas", 
    "title": "Geographic Routing Around Obstacles in Wireless Sensor Networks", 
    "publish": "2007-03-20T12:17:57Z", 
    "summary": "Geographic routing is becoming the protocol of choice for many sensor network\napplications. The current state of the art is unsatisfactory: some algorithms\nare very efficient, however they require a preliminary planarization of the\ncommunication graph. Planarization induces overhead and is not realistic in\nmany scenarios. On the otherhand, georouting algorithms which do not rely on\nplanarization have fairly low success rates and either fail to route messages\naround all but the simplest obstacles or have a high topology control overhead\n(e.g. contour detection algorithms). To overcome these limitations, we propose\nGRIC, the first lightweight and efficient on demand (i.e. all-to-all)\ngeographic routing algorithm which does not require planarization and has\nalmost 100% delivery rates (when no obstacles are added). Furthermore, the\nexcellent behavior of our algorithm is maintained even in the presence of large\nconvex obstacles. The case of hard concave obstacles is also studied; such\nobstacles are hard instances for which performance diminishes.", 
    "link": "http://arxiv.org/pdf/cs/0703094v1", 
    "arxiv-id": "cs/0703094v1"
},{
    "category": "cs.DC", 
    "author": "Srinidhi Varadarajan", 
    "title": "User-level DSM System for Modern High-Performance Interconnection   Networks", 
    "publish": "2007-03-22T19:15:00Z", 
    "summary": "In this paper, we introduce a new user-level DSM system which has the ability\nto directly interact with underlying interconnection networks. The DSM system\nprovides the application programmer a flexible API to program parallel\napplications either using shared memory semantics over physically distributed\nmemory or to use an efficient remote memory demand paging technique. We also\nintroduce a new time slice based memory consistency protocol which is used by\nthe DSM system. We present preliminary results from our implementation on a\nsmall Opteron Linux cluster interconnected over Myrinet.", 
    "link": "http://arxiv.org/pdf/cs/0703112v1", 
    "arxiv-id": "cs/0703112v1"
},{
    "category": "cs.DC", 
    "author": "J. J. Merelo", 
    "title": "Self-adaptive Gossip Policies for Distributed Population-based   Algorithms", 
    "publish": "2007-03-23T11:29:10Z", 
    "summary": "Gossipping has demonstrate to be an efficient mechanism for spreading\ninformation among P2P networks. Within the context of P2P computing, we propose\nthe so-called Evolvable Agent Model for distributed population-based algorithms\nwhich uses gossipping as communication policy, and represents every individual\nas a self-scheduled single thread. The model avoids obsolete nodes in the\npopulation by defining a self-adaptive refresh rate which depends on the\nlatency and bandwidth of the network. Such a mechanism balances the migration\nrate to the congestion of the links pursuing global population coherence. We\nperform an experimental evaluation of this model on a real parallel system and\nobserve how solution quality and algorithm speed scale with the number of\nprocessors with this seamless approach.", 
    "link": "http://arxiv.org/pdf/cs/0703117v1", 
    "arxiv-id": "cs/0703117v1"
},{
    "category": "cs.DC", 
    "author": "Richard Kr\u00e1lovi\u010d", 
    "title": "Rapid Almost-Complete Broadcasting in Faulty Networks", 
    "publish": "2007-03-23T22:38:06Z", 
    "summary": "This paper studies the problem of broadcasting in synchronous point-to-point\nnetworks, where one initiator owns a piece of information that has to be\ntransmitted to all other vertices as fast as possible. The model of fractional\ndynamic faults with threshold is considered: in every step either a fixed\nnumber $T$, or a fraction $\\alpha$, of sent messages can be lost depending on\nwhich quantity is larger.\n  As the main result we show that in complete graphs and hypercubes it is\npossible to inform all but a constant number of vertices, exhibiting only a\nlogarithmic slowdown, i.e. in time $O(D\\log n)$ where $D$ is the diameter of\nthe network and $n$ is the number of vertices.\n  Moreover, for complete graphs under some additional conditions (sense of\ndirection, or $\\alpha<0.55$) the remaining constant number of vertices can be\ninformed in the same time, i.e. $O(\\log n)$.", 
    "link": "http://arxiv.org/pdf/cs/0703122v1", 
    "arxiv-id": "cs/0703122v1"
},{
    "category": "cs.DC", 
    "author": "Calvin J. Ribbens", 
    "title": "ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous   Applications in a Parallel Environment", 
    "publish": "2007-03-27T23:27:54Z", 
    "summary": "Applications in science and engineering often require huge computational\nresources for solving problems within a reasonable time frame. Parallel\nsupercomputers provide the computational infrastructure for solving such\nproblems. A traditional application scheduler running on a parallel cluster\nonly supports static scheduling where the number of processors allocated to an\napplication remains fixed throughout the lifetime of execution of the job. Due\nto the unpredictability in job arrival times and varying resource requirements,\nstatic scheduling can result in idle system resources thereby decreasing the\noverall system throughput. In this paper we present a prototype framework\ncalled ReSHAPE, which supports dynamic resizing of parallel MPI applications\nexecuted on distributed memory platforms. The framework includes a scheduler\nthat supports resizing of applications, an API to enable applications to\ninteract with the scheduler, and a library that makes resizing viable.\nApplications executed using the ReSHAPE scheduler framework can expand to take\nadvantage of additional free processors or can shrink to accommodate a high\npriority application, without getting suspended. In our research, we have\nmainly focused on structured applications that have two-dimensional data arrays\ndistributed across a two-dimensional processor grid. The resize library\nincludes algorithms for processor selection and processor mapping. Experimental\nresults show that the ReSHAPE framework can improve individual job turn-around\ntime and overall system throughput.", 
    "link": "http://arxiv.org/pdf/cs/0703137v1", 
    "arxiv-id": "cs/0703137v1"
},{
    "category": "cs.DC", 
    "author": "Gerald Krafft", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids", 
    "publish": "2007-04-06T15:59:27Z", 
    "summary": "This paper analyses the possibilities of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems.", 
    "link": "http://arxiv.org/pdf/0704.1827v1", 
    "arxiv-id": "0704.1827v1"
},{
    "category": "cs.DC", 
    "author": "Alain Nicolas", 
    "title": "Parallel computing for the finite element method", 
    "publish": "2007-04-18T13:20:25Z", 
    "summary": "A finite element method is presented to compute time harmonic microwave\nfields in three dimensional configurations. Nodal-based finite elements have\nbeen coupled with an absorbing boundary condition to solve open boundary\nproblems. This paper describes how the modeling of large devices has been made\npossible using parallel computation, New algorithms are then proposed to\nimplement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and\non a CRAY C98. Analysis of the computation efficiency is performed using simple\nproblems. The electromagnetic scattering of a plane wave by a perfect electric\nconducting airplane is finally given as example.", 
    "link": "http://arxiv.org/pdf/0704.2344v1", 
    "arxiv-id": "0704.2344v1"
},{
    "category": "cs.DC", 
    "author": "Luigi Santocanale", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3", 
    "publish": "2007-04-18T14:39:09Z", 
    "summary": "We address the problem of &#64257;nding nice labellings for event structures\nof degree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3.", 
    "link": "http://arxiv.org/pdf/0704.2355v1", 
    "arxiv-id": "0704.2355v1"
},{
    "category": "cs.DC", 
    "author": "Valmir C. Barbosa", 
    "title": "An algorithm for clock synchronization with the gradient property in   sensor networks", 
    "publish": "2007-04-30T19:59:14Z", 
    "summary": "We introduce a distributed algorithm for clock synchronization in sensor\nnetworks. Our algorithm assumes that nodes in the network only know their\nimmediate neighborhoods and an upper bound on the network's diameter.\nClock-synchronization messages are only sent as part of the communication,\nassumed reasonably frequent, that already takes place among nodes. The\nalgorithm has the gradient property of [2], achieving an O(1) worst-case skew\nbetween the logical clocks of neighbors. As in the case of [3,8], the\nalgorithm's actions are such that no constant lower bound exists on the rate at\nwhich logical clocks progress in time, and for this reason the lower bound of\n[2,5] that forbids constant skew between neighbors does not apply.", 
    "link": "http://arxiv.org/pdf/0704.3890v1", 
    "arxiv-id": "0704.3890v1"
},{
    "category": "cs.DC", 
    "author": "Pierre-Andr\u00e9 Wacrenier", 
    "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors:   the BubbleSched Framework", 
    "publish": "2007-06-14T09:35:30Z", 
    "summary": "Exploiting full computational power of current more and more hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. Unfortunately, most\noperating systems only provide a poor scheduling API that does not allow\napplications to transmit valuable scheduling hints to the system. In a previous\npaper, we showed that using a bubble-based thread scheduler can significantly\nimprove applications' performance in a portable way. However, since\nmultithreaded applications have various scheduling requirements, there is no\nuniversal scheduler that could meet all these needs. In this paper, we present\na framework that allows scheduling experts to implement and experiment with\ncustomized thread schedulers. It provides a powerful API for dynamically\ndistributing bubbles among the machine in a high-level, portable, and efficient\nway. Several examples show how experts can then develop, debug and tune their\nown portable bubble schedulers.", 
    "link": "http://arxiv.org/pdf/0706.2069v1", 
    "arxiv-id": "0706.2069v1"
},{
    "category": "cs.DC", 
    "author": "Calvin J. Ribbens", 
    "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel   Computations", 
    "publish": "2007-06-14T15:54:10Z", 
    "summary": "Traditional parallel schedulers running on cluster supercomputers support\nonly static scheduling, where the number of processors allocated to an\napplication remains fixed throughout the execution of the job. This results in\nunder-utilization of idle system resources thereby decreasing overall system\nthroughput. In our research, we have developed a prototype framework called\nReSHAPE, which supports dynamic resizing of parallel MPI applications executing\non distributed memory platforms. The resizing library in ReSHAPE includes\nsupport for releasing and acquiring processors and efficiently redistributing\napplication state to a new set of processors. In this paper, we derive an\nalgorithm for redistributing two-dimensional block-cyclic arrays from $P$ to\n$Q$ processors, organized as 2-D processor grids. The algorithm ensures a\ncontention-free communication schedule for data redistribution if $P_r \\leq\nQ_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row\nand column shifts on the communication schedule to minimize node contention.", 
    "link": "http://arxiv.org/pdf/0706.2146v1", 
    "arxiv-id": "0706.2146v1"
},{
    "category": "cs.DC", 
    "author": "Philippe Merle", 
    "title": "A Generic Deployment Framework for Grid Computing and Distributed   Applications", 
    "publish": "2007-06-20T15:17:47Z", 
    "summary": "Deployment of distributed applications on large systems, and especially on\ngrid infrastructures, becomes a more and more complex task. Grid users spend a\nlot of time to prepare, install and configure middleware and application\nbinaries on nodes, and eventually start their applications. The problem is that\nthe deployment process is composed of many heterogeneous tasks that have to be\norchestrated in a specific correct order. As a consequence, the automatization\nof the deployment process is currently very difficult to reach. To address this\nproblem, we propose in this paper a generic deployment framework allowing to\nautomatize the execution of heterogeneous tasks composing the whole deployment\nprocess. Our approach is based on a reification as software components of all\nrequired deployment mechanisms or existing tools. Grid users only have to\ndescribe the configuration to deploy in a simple natural language instead of\nprogramming or scripting how the deployment process is executed. As a toy\nexample, this framework is used to deploy CORBA component-based applications\nand OpenCCM middleware on one thousand nodes of the French Grid5000\ninfrastructure.", 
    "link": "http://arxiv.org/pdf/0706.3008v1", 
    "arxiv-id": "0706.3008v1"
},{
    "category": "cs.DC", 
    "author": "Veronika Rehn-Sonigo", 
    "title": "Optimal Replica Placement in Tree Networks with QoS and Bandwidth   Constraints and the Closest Allocation Policy", 
    "publish": "2007-06-22T15:01:35Z", 
    "summary": "This paper deals with the replica placement problem on fully homogeneous tree\nnetworks known as the Replica Placement optimization problem. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. We investigate the latter problem using the Closest access\npolicy when adding QoS and bandwidth constraints. We propose an optimal\nalgorithm in two passes using dynamic programming.", 
    "link": "http://arxiv.org/pdf/0706.3350v3", 
    "arxiv-id": "0706.3350v3"
},{
    "category": "cs.DC", 
    "author": "Abdullah Gharaibeh", 
    "title": "stdchk: A Checkpoint Storage System for Desktop Grid Computing", 
    "publish": "2007-06-25T01:24:46Z", 
    "summary": "Checkpointing is an indispensable technique to provide fault tolerance for\nlong-running high-throughput applications like those running on desktop grids.\nThis paper argues that a dedicated checkpoint storage system, optimized to\noperate in these environments, can offer multiple benefits: reduce the load on\na traditional file system, offer high-performance through specialization, and,\nfinally, optimize data management by taking into account checkpoint application\nsemantics. Such a storage system can present a unifying abstraction to\ncheckpoint operations, while hiding the fact that there are no dedicated\nresources to store the checkpoint data. We prototype stdchk, a checkpoint\nstorage system that uses scavenged disk space from participating desktops to\nbuild a low-cost storage system, offering a traditional file system interface\nfor easy integration with applications. This paper presents the stdchk\narchitecture, key performance optimizations, support for incremental\ncheckpointing, and increased data availability. Our evaluation confirms that\nthe stdchk approach is viable in a desktop grid setting and offers a low cost\nstorage system with desirable performance characteristics: high write\nthroughput and reduced storage space and network effort to save checkpoint\nimages.", 
    "link": "http://arxiv.org/pdf/0706.3546v2", 
    "arxiv-id": "0706.3546v2"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Multi-criteria scheduling of pipeline workflows", 
    "publish": "2007-06-27T13:43:16Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonist criteria should be optimized, such as throughput and latency (or a\ncombination). In this paper, we study the complexity of the bi-criteria mapping\nproblem for pipeline graphs on communication homogeneous platforms. In\nparticular, we assess the complexity of the well-known chains-to-chains problem\nfor different-speed processors, which turns out to be NP-hard. We provide\nseveral efficient polynomial bi-criteria heuristics, and their relative\nperformance is evaluated through extensive simulations.", 
    "link": "http://arxiv.org/pdf/0706.4009v2", 
    "arxiv-id": "0706.4009v2"
},{
    "category": "cs.DC", 
    "author": "Franck Petit", 
    "title": "Self-Stabilizing Wavelets and r-Hops Coordination", 
    "publish": "2007-06-27T12:53:06Z", 
    "summary": "We introduce a simple tool called the wavelet (or, r-wavelet) scheme.\nWavelets deals with coordination among processes which are at most r hops away\nof each other. We present a selfstabilizing solution for this scheme. Our\nsolution requires no underlying structure and works in arbritrary anonymous\nnetworks, i.e., no process identifier is required. Moreover, our solution works\nunder any (even unfair) daemon. Next, we use the wavelet scheme to design\nself-stabilizing layer clocks. We show that they provide an efficient device in\nthe design of local coordination problems at distance r, i.e., r-barrier\nsynchronization and r-local resource allocation (LRA) such as r-local mutual\nexclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some\nsolutions to the r-LRA problem (e.g., r-LME) also provide transformers to\ntransform algorithms written assuming any r-central daemon into algorithms\nworking with any distributed daemon.", 
    "link": "http://arxiv.org/pdf/0706.4015v1", 
    "arxiv-id": "0706.4015v1"
},{
    "category": "cs.DC", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "title": "Scheduling multiple divisible loads on a linear processor network", 
    "publish": "2007-06-27T14:43:13Z", 
    "summary": "Min, Veeravalli, and Barlas have recently proposed strategies to minimize the\noverall execution time of one or several divisible loads on a heterogeneous\nlinear network, using one or more installments. We show on a very simple\nexample that their approach does not always produce a solution and that, when\nit does, the solution is often suboptimal. We also show how to find an optimal\nschedule for any instance, once the number of installments per load is given.\nThen, we formally state that any optimal schedule has an infinite number of\ninstallments under a linear cost model as the one assumed in the original\npapers. Therefore, such a cost model cannot be used to design practical\nmulti-installment strategies. Finally, through extensive simulations we\nconfirmed that the best solution is always produced by the linear programming\napproach, while solutions of the original papers can be far away from the\noptimal.", 
    "link": "http://arxiv.org/pdf/0706.4038v2", 
    "arxiv-id": "0706.4038v2"
},{
    "category": "cs.DC", 
    "author": "Christian Boulinier", 
    "title": "Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous   Anonymous Networks", 
    "publish": "2007-06-28T18:51:36Z", 
    "summary": "How to pass from local to global scales in anonymous networks? How to\norganize a selfstabilizing propagation of information with feedback. From the\nAngluin impossibility results, we cannot elect a leader in a general anonymous\nnetwork. Thus, it is impossible to build a rooted spanning tree. Many problems\ncan only be solved by probabilistic methods. In this paper we show how to use\nUnison to design a self-stabilizing barrier synchronization in an anonymous\nnetwork. We show that the commuication structure of this barrier\nsynchronization designs a self-stabilizing wave-stream, or pipelining wave, in\nanonymous networks. We introduce two variants of Wave: the strong waves and the\nwavelets. A strong wave can be used to solve the idempotent r-operator\nparametrized computation problem. A wavelet deals with k-distance computation.\nWe show how to use Unison to design a self-stabilizing wave stream, a\nself-stabilizing strong wave stream and a self-stabilizing wavelet stream.", 
    "link": "http://arxiv.org/pdf/0706.4298v1", 
    "arxiv-id": "0706.4298v1"
},{
    "category": "cs.DC", 
    "author": "Mohamed Jemni", 
    "title": "Performance Analysis of Publish/Subscribe Systems", 
    "publish": "2007-07-03T09:02:45Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. Its technology consists\nmainly in exploiting resources, geographically dispersed, to treat complex\napplications needing big power of calculation and/or important storage\ncapacity. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfigurations, decentralisation and performance\nbecomes more and more essential. Since such properties are exhibited by P2P\nsystems, the convergence of grid computing and P2P computing seems natural. In\nthis context, this paper evaluates the scalability and performance of P2P tools\nfor discovering and registering services. Three protocols are used for this\npurpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of\ntheses protocols related to two criteria: the elapsed time for registrations\nservices and the needed time to discover new services. Our aim is to analyse\nthese results in order to choose the best protocol we can use in order to\ncreate a decentralised middleware for desktop grid.", 
    "link": "http://arxiv.org/pdf/0707.0365v1", 
    "arxiv-id": "0707.0365v1"
},{
    "category": "cs.DC", 
    "author": "Ian Willers", 
    "title": "A Multi Interface Grid Discovery System", 
    "publish": "2007-07-05T09:22:45Z", 
    "summary": "Discovery Systems (DS) can be considered as entry points for global loosely\ncoupled distributed systems. An efficient Discovery System in essence increases\nthe performance, reliability and decision making capability of distributed\nsystems. With the rapid increase in scale of distributed applications, existing\nsolutions for discovery systems are fast becoming either obsolete or incapable\nof handling such complexity. They are particularly ineffective when handling\nservice lifetimes and providing up-to-date information, poor at enabling\ndynamic service access and they can also impose unwanted restrictions on\ninterfaces to widely available information repositories. In this paper we\npresent essential the design characteristics, an implementation and a\nperformance analysis for a discovery system capable of overcoming these\ndeficiencies in large, globally distributed environments.", 
    "link": "http://arxiv.org/pdf/0707.0740v1", 
    "arxiv-id": "0707.0740v1"
},{
    "category": "cs.DC", 
    "author": "I. Willers", 
    "title": "Mobile Computing in Physics Analysis - An Indicator for eScience", 
    "publish": "2007-07-05T09:32:29Z", 
    "summary": "This paper presents the design and implementation of a Grid-enabled physics\nanalysis environment for handheld and other resource-limited computing devices\nas one example of the use of mobile devices in eScience. Handheld devices offer\ngreat potential because they provide ubiquitous access to data and\nround-the-clock connectivity over wireless links. Our solution aims to provide\nusers of handheld devices the capability to launch heavy computational tasks on\ncomputational and data Grids, monitor the jobs status during execution, and\nretrieve results after job completion. Users carry their jobs on their handheld\ndevices in the form of executables (and associated libraries). Users can\ntransparently view the status of their jobs and get back their outputs without\nhaving to know where they are being executed. In this way, our system is able\nto act as a high-throughput computing environment where devices ranging from\npowerful desktop machines to small handhelds can employ the power of the Grid.\nThe results shown in this paper are readily applicable to the wider eScience\ncommunity.", 
    "link": "http://arxiv.org/pdf/0707.0742v1", 
    "arxiv-id": "0707.0742v1"
},{
    "category": "cs.DC", 
    "author": "O. Alvi", 
    "title": "DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling", 
    "publish": "2007-07-05T09:36:18Z", 
    "summary": "The use of meta-schedulers for resource management in large-scale distributed\nsystems often leads to a hierarchy of schedulers. In this paper, we discuss why\nexisting meta-scheduling hierarchies are sometimes not sufficient for Grid\nsystems due to their inability to re-organise jobs already scheduled locally.\nSuch a job re-organisation is required to adapt to evolving loads which are\ncommon in heavily used Grid infrastructures. We propose a peer-to-peer\nscheduling model and evaluate it using case studies and mathematical modelling.\nWe detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and\nits queue management system for coping with the load distribution and for\nsupporting bulk job scheduling. We demonstrate that such a system is beneficial\nfor dynamic, distributed and self-organizing resource management and can assist\nin optimizing load or job distribution in complex Grid infrastructures.", 
    "link": "http://arxiv.org/pdf/0707.0743v1", 
    "arxiv-id": "0707.0743v1"
},{
    "category": "cs.DC", 
    "author": "T. Solomonides", 
    "title": "Experiences of Engineering Grid-Based Medical Software", 
    "publish": "2007-07-05T10:06:41Z", 
    "summary": "Objectives: Grid-based technologies are emerging as potential solutions for\nmanaging and collaborating distributed resources in the biomedical domain. Few\nexamples exist, however, of successful implementations of Grid-enabled medical\nsystems and even fewer have been deployed for evaluation in practice. The\nobjective of this paper is to evaluate the use in clinical practice of a\nGrid-based imaging prototype and to establish directions for engineering future\nmedical Grid developments and their subsequent deployment. Method: The\nMammoGrid project has deployed a prototype system for clinicians using the Grid\nas its information infrastructure. To assist in the specification of the system\nrequirements (and for the first time in healthgrid applications), use-case\nmodelling has been carried out in close collaboration with clinicians and\nradiologists who had no prior experience of this modelling technique. A\ncritical qualitative and, where possible, quantitative analysis of the\nMammoGrid prototype is presented leading to a set of recommendations from the\ndelivery of the first deployed Grid-based medical imaging application. Results:\nWe report critically on the application of software engineering techniques in\nthe specification and implementation of the MammoGrid project and show that\nuse-case modelling is a suitable vehicle for representing medical requirements\nand for communicating effectively with the clinical community. This paper also\ndiscusses the practical advantages and limitations of applying the Grid to\nreal-life clinical applications and presents the consequent lessons learned.", 
    "link": "http://arxiv.org/pdf/0707.0748v1", 
    "arxiv-id": "0707.0748v1"
},{
    "category": "cs.DC", 
    "author": "Peter Bloodsworth", 
    "title": "PhantomOS: A Next Generation Grid Operating System", 
    "publish": "2007-07-05T11:14:45Z", 
    "summary": "Grid Computing has made substantial advances in the past decade; these are\nprimarily due to the adoption of standardized Grid middleware. However Grid\ncomputing has not yet become pervasive because of some barriers that we believe\nhave been caused by the adoption of middleware centric approaches. These\nbarriers include: scant support for major types of applications such as\ninteractive applications; lack of flexible, autonomic and scalable Grid\narchitectures; lack of plug-and-play Grid computing and, most importantly, no\nstraightforward way to setup and administer Grids. PhantomOS is a project which\naims to address many of these barriers. Its goal is the creation of a user\nfriendly pervasive Grid computing platform that facilitates the rapid\ndeployment and easy maintenance of Grids whilst providing support for major\ntypes of applications on Grids of almost any topology. In this paper we present\nthe detailed system architecture and an overview of its implementation.", 
    "link": "http://arxiv.org/pdf/0707.0762v1", 
    "arxiv-id": "0707.0762v1"
},{
    "category": "cs.DC", 
    "author": "Michael Thomas", 
    "title": "Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments", 
    "publish": "2007-07-05T19:46:51Z", 
    "summary": "In Grids scheduling decisions are often made on the basis of jobs being\neither data or computation intensive: in data intensive situations jobs may be\npushed to the data and in computation intensive situations data may be pulled\nto the jobs. This kind of scheduling, in which there is no consideration of\nnetwork characteristics, can lead to performance degradation in a Grid\nenvironment and may result in large processing queues and job execution delays\ndue to site overloads. In this paper we describe a Data Intensive and Network\nAware (DIANA) meta-scheduling approach, which takes into account data,\nprocessing power and network characteristics when making scheduling decisions\nacross multiple sites. Through a practical implementation on a Grid testbed, we\ndemonstrate that queue and execution times of data-intensive jobs can be\nsignificantly improved when we introduce our proposed DIANA scheduler. The\nbasic scheduling decisions are dictated by a weighting factor for each\npotential target location which is a calculated function of network\ncharacteristics, processing cycles and data location and size. The job\nscheduler provides a global ranking of the computing resources and then selects\nan optimal one on the basis of this overall access and execution cost. The\nDIANA approach considers the Grid as a combination of active network elements\nand takes network characteristics as a first class criterion in the scheduling\ndecision matrix along with computation and data. The scheduler can then make\ninformed decisions by taking into account the changing state of the network,\nlocality and size of the data and the pool of available processing cycles.", 
    "link": "http://arxiv.org/pdf/0707.0862v1", 
    "arxiv-id": "0707.0862v1"
},{
    "category": "cs.DC", 
    "author": "John Shalf", 
    "title": "Cactus Framework: Black Holes to Gamma Ray Bursts", 
    "publish": "2007-07-11T13:01:50Z", 
    "summary": "Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of\ncosmological origin. They are among the most scientifically interesting\nastrophysical systems, and the riddle concerning their central engines and\nemission mechanisms is one of the most complex and challenging problems of\nastrophysics today. In this article we outline our petascale approach to the\nGRB problem and discuss the computational toolkits and numerical codes that are\ncurrently in use and that will be scaled up to run on emerging petaflop scale\ncomputing platforms in the near future.\n  Petascale computing will require additional ingredients over conventional\nparallelism. We consider some of the challenges which will be caused by future\npetascale architectures, and discuss our plans for the future development of\nthe Cactus framework and its applications to meet these challenges in order to\nprofit from these new architectures.", 
    "link": "http://arxiv.org/pdf/0707.1607v1", 
    "arxiv-id": "0707.1607v1"
},{
    "category": "cs.DC", 
    "author": "L. T. Handoko", 
    "title": "Resource Allocation in Public Cluster with Extended Optimization   Algorithm", 
    "publish": "2007-08-04T05:15:05Z", 
    "summary": "We introduce an optimization algorithm for resource allocation in the LIPI\nPublic Cluster to optimize its usage according to incoming requests from users.\nThe tool is an extended and modified genetic algorithm developed to match\nspecific natures of public cluster. We present a detail analysis of\noptimization, and compare the results with the exact calculation. We show that\nit would be very useful and could realize an automatic decision making system\nfor public clusters.", 
    "link": "http://arxiv.org/pdf/0708.0608v2", 
    "arxiv-id": "0708.0608v2"
},{
    "category": "cs.DC", 
    "author": "L. T. Handoko", 
    "title": "Multi and Independent Block Approach in Public Cluster", 
    "publish": "2007-08-25T19:46:52Z", 
    "summary": "We present extended multi block approach in the LIPI Public Cluster. The\nmulti block approach enables a cluster to be divided into several independent\nblocks which run jobs owned by different users simultaneously. Previously, we\nhave maintained the blocks using single master node for all blocks due to\nefficiency and resource limitations. Following recent advancements and\nexpansion of node\\'s number, we have modified the multi block approach with\nmultiple master nodes, each of them is responsible for a single block. We argue\nthat this approach improves the overall performance significantly, for\nespecially data intensive computational works.", 
    "link": "http://arxiv.org/pdf/0708.3446v2", 
    "arxiv-id": "0708.3446v2"
},{
    "category": "cs.DC", 
    "author": "Rossano Venturini", 
    "title": "Searching for a dangerous host: randomized vs. deterministic", 
    "publish": "2007-08-28T09:24:13Z", 
    "summary": "A Black Hole is an harmful host in a network that destroys incoming agents\nwithout leaving any trace of such event. The problem of locating the black hole\nin a network through a team of agent coordinated by a common protocol is\nusually referred in literature as the Black Hole Search problem (or BHS for\nbrevity) and it is a consolidated research topic in the area of distributed\nalgorithms. The aim of this paper is to extend the results for BHS by\nconsidering more general (and hence harder) classes of dangerous host. In\nparticular we introduce rB-hole as a probabilistic generalization of the Black\nHole, in which the destruction of an incoming agent is a purely random event\nhappening with some fixed probability (like flipping a biased coin). The main\nresult we present is that if we tolerate an arbitrarily small error probability\nin the result then the rB-hole Search problem, or RBS, is not harder than the\nusual BHS. We establish this result in two different communication model,\nspecifically both in presence or absence of whiteboards non-located at the\nhomebase. The core of our methods is a general reduction tool for transforming\nalgorithms for the black hole into algorithms for the rB-hole.", 
    "link": "http://arxiv.org/pdf/0708.3734v1", 
    "arxiv-id": "0708.3734v1"
},{
    "category": "cs.DC", 
    "author": "L. Lo Iacono", 
    "title": "Non-Blocking Signature of very large SOAP Messages", 
    "publish": "2007-09-17T13:50:42Z", 
    "summary": "Data transfer and staging services are common components in Grid-based, or\nmore generally, in service-oriented applications. Security mechanisms play a\ncentral role in such services, especially when they are deployed in sensitive\napplication fields like e-health. The adoption of WS-Security and related\nstandards to SOAP-based transfer services is, however, problematic as a\nstraightforward adoption of SOAP with MTOM introduces considerable\ninefficiencies in the signature generation process when large data sets are\ninvolved. This paper proposes a non-blocking, signature generation approach\nenabling a stream-like processing with considerable performance enhancements.", 
    "link": "http://arxiv.org/pdf/0709.2635v1", 
    "arxiv-id": "0709.2635v1"
},{
    "category": "cs.DC", 
    "author": "Fan Zhi-Hua", 
    "title": "Static Deadlock Detection in MPI Synchronization Communication", 
    "publish": "2007-09-24T05:33:29Z", 
    "summary": "It is very common to use dynamic methods to detect deadlocks in MPI programs\nfor the reason that static methods have some restrictions. To guarantee high\nreliability of some important MPI-based application software, a model of MPI\nsynchronization communication is abstracted and a type of static method is\ndevised to examine deadlocks in such modes. The model has three forms with\ndifferent complexity: sequential model, single-loop model and nested-loop\nmodel. Sequential model is a base for all models. Single-loop model must be\ntreated with a special type of equation group and nested-loop model extends the\nmethods for the other two models. A standard Java-based software framework\noriginated from these methods is constructed for determining whether MPI\nprograms are free from synchronization communication deadlocks. Our practice\nshows the software framework is better than those tools using dynamic methods\nbecause it can dig out all synchronization communication deadlocks before an\nMPI-based program goes into running.", 
    "link": "http://arxiv.org/pdf/0709.3689v1", 
    "arxiv-id": "0709.3689v1"
},{
    "category": "cs.DC", 
    "author": "Zhi-hua Fan", 
    "title": "Deadlock Detection in Basic Models of MPI Synchronization Communication   Programs", 
    "publish": "2007-09-24T06:02:29Z", 
    "summary": "A model of MPI synchronization communication programs is presented and its\nthree basic simplified models are also defined. A series of theorems and\nmethods for deciding whether deadlocks will occur among the three models are\ngiven and proved strictly. These theories and methods for simple models'\ndeadlock detection are the necessary base for real MPI program deadlock\ndetection. The methods are based on a static analysis through programs and with\nruntime detection in necessary cases and they are able to determine before\ncompiling whether it will be deadlocked for two of the three basic models. For\nanother model, some deadlock cases can be found before compiling and others at\nruntime. Our theorems can be used to prove the correctness of currently popular\nMPI program deadlock detection algorithms. Our methods may decrease codes that\nthose algorithms need to change to MPI source or profiling interface and may\ndetects deadlocks ahead of program execution, thus the overheads can be reduced\ngreatly.", 
    "link": "http://arxiv.org/pdf/0709.3692v2", 
    "arxiv-id": "0709.3692v2"
},{
    "category": "cs.DC", 
    "author": "Fan Zhi-Hua", 
    "title": "Algorithm of Static Deadlock Detection in MPI Synchronization   Communication Sequential Model", 
    "publish": "2007-09-24T06:06:34Z", 
    "summary": "Detecting deadlocks in MPI synchronization communication programs is very\ndifficult and need building program models. All complex models are based on\nsequential models. The sequential model is mapped into a set of character\nstrings and its deadlock detection problem is translated into an equivalent\nmulti-queue string matching problem. An algorithm is devised and implemented to\nstatically detect deadlocks in sequential models of MPI synchronization\ncommunication programs. The time and space complexity of the algorithm is O(n)\nwhere n is the amount of message in model. The algorithm is better than usual\ncircle-detection methods and can adapt well to dynamic message stream.", 
    "link": "http://arxiv.org/pdf/0709.3693v1", 
    "arxiv-id": "0709.3693v1"
},{
    "category": "cs.DC", 
    "author": "Gerald Krafft", 
    "title": "Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids", 
    "publish": "2007-09-24T19:25:27Z", 
    "summary": "This paper analyses the requirements of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the most promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems.", 
    "link": "http://arxiv.org/pdf/0709.3826v1", 
    "arxiv-id": "0709.3826v1"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "Approximating max-min linear programs with local algorithms", 
    "publish": "2007-10-08T09:46:47Z", 
    "summary": "A local algorithm is a distributed algorithm where each node must operate\nsolely based on the information that was available at system startup within a\nconstant-size neighbourhood of the node. We study the applicability of local\nalgorithms to max-min LPs where the objective is to maximise $\\min_k \\sum_v\nc_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$\nfor each $v$. Here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $V_i =\n\\{v : a_{iv} > 0 \\}$, $V_k = \\{v : c_{kv}>0 \\}$, $I_v = \\{i : a_{iv} > 0 \\}$\nand $K_v = \\{k : c_{kv} > 0 \\}$ have bounded size. In the distributed setting,\neach agent $v$ is responsible for choosing the value of $x_v$, and the\ncommunication network is a hypergraph $\\mathcal{H}$ where the sets $V_k$ and\n$V_i$ constitute the hyperedges. We present inapproximability results for a\nwide range of structural assumptions; for example, even if $|V_i|$ and $|V_k|$\nare bounded by some constants larger than 2, there is no local approximation\nscheme. To contrast the negative results, we present a local approximation\nalgorithm which achieves good approximation ratios if we can bound the relative\ngrowth of the vertex neighbourhoods in $\\mathcal{H}$.", 
    "link": "http://arxiv.org/pdf/0710.1499v1", 
    "arxiv-id": "0710.1499v1"
},{
    "category": "cs.DC", 
    "author": "Nuno Pregui\u00e7a", 
    "title": "Designing a commutative replicated data type", 
    "publish": "2007-10-09T14:38:50Z", 
    "summary": "Commuting operations greatly simplify consistency in distributed systems.\nThis paper focuses on designing for commutativity, a topic neglected\npreviously. We show that the replicas of \\emph{any} data type for which\nconcurrent operations commute converges to a correct value, under some simple\nand standard assumptions. We also show that such a data type supports\ntransactions with very low cost. We identify a number of approaches and\ntechniques to ensure commutativity. We re-use some existing ideas\n(non-destructive updates coupled with invariant identification), but propose a\nmuch more efficient implementation. Furthermore, we propose a new technique,\nbackground consensus. We illustrate these ideas with a shared edit buffer data\ntype.", 
    "link": "http://arxiv.org/pdf/0710.1784v1", 
    "arxiv-id": "0710.1784v1"
},{
    "category": "cs.DC", 
    "author": "Virginie Legrand Contes", 
    "title": "Towards Grid Monitoring and deployment in Jade, using ProActive", 
    "publish": "2007-10-29T10:44:35Z", 
    "summary": "This document describes our current effort to gridify Jade, a java-based\nenvironment for the autonomic management of clustered J2EE application servers,\ndeveloped in the INRIA SARDES research team. Towards this objective, we use the\njava ProActive grid technology. We first present some of the challenges to turn\nsuch an autonomic management system initially dedicated to distributed\napplications running on clusters of machines, into one that can provide\nself-management capabilities to large-scale systems, i.e. deployed on grid\ninfrastructures. This leads us to a brief state of the art on grid monitoring\nsystems. Then, we recall the architecture of Jade, and consequently propose to\nreorganize it in a potentially more scalable way. Practical experiments pertain\nto the use of the grid deployment feature offered by ProActive to easily\nconduct the deployment of the Jade system or its revised version on any sort of\ngrid.", 
    "link": "http://arxiv.org/pdf/0710.5348v1", 
    "arxiv-id": "0710.5348v1"
},{
    "category": "cs.DC", 
    "author": "Lionel Sacks", 
    "title": "Resource and Application Models for Advanced Grid Schedulers", 
    "publish": "2007-11-02T14:03:46Z", 
    "summary": "As Grid computing is becoming an inevitable future, managing, scheduling and\nmonitoring dynamic, heterogeneous resources will present new challenges.\nSolutions will have to be agile and adaptive, support self-organization and\nautonomous management, while maintaining optimal resource utilisation.\nPresented in this paper are basic principles and architectural concepts for\nefficient resource allocation in heterogeneous Grid environment.", 
    "link": "http://arxiv.org/pdf/0711.0314v1", 
    "arxiv-id": "0711.0314v1"
},{
    "category": "cs.DC", 
    "author": "Lionel Sacks", 
    "title": "Measuring and Monitoring Grid Resource Utilisation", 
    "publish": "2007-11-02T14:12:27Z", 
    "summary": "Effective resource utilisation monitoring and highly granular yet adaptive\nmeasurements are prerequisites for a more efficient Grid scheduler. We present\na suite of measurement applications able to monitor per-process resource\nutilisation, and a customisable tool for emulating observed utilisation models.", 
    "link": "http://arxiv.org/pdf/0711.0315v1", 
    "arxiv-id": "0711.0315v1"
},{
    "category": "cs.DC", 
    "author": "Lionel Sacks", 
    "title": "A Study of Grid Applications: Scheduling Perspective", 
    "publish": "2007-11-02T14:15:45Z", 
    "summary": "As the Grid evolves from a high performance cluster middleware to a\nmultipurpose utility computing framework, a good understanding of Grid\napplications, their statistics and utilisation patterns is required. This study\nlooks at job execution times and resource utilisations in a Grid environment,\nand their significance in cluster and network dimensioning, local level\nscheduling and resource management.", 
    "link": "http://arxiv.org/pdf/0711.0316v1", 
    "arxiv-id": "0711.0316v1"
},{
    "category": "cs.DC", 
    "author": "Paul McKee", 
    "title": "Self-Organising management of Grid environments", 
    "publish": "2007-11-02T15:26:48Z", 
    "summary": "This paper presents basic concepts, architectural principles and algorithms\nfor efficient resource and security management in cluster computing\nenvironments and the Grid. The work presented in this paper is funded by\nBTExacT and the EPSRC project SO-GRM (GR/S21939).", 
    "link": "http://arxiv.org/pdf/0711.0325v1", 
    "arxiv-id": "0711.0325v1"
},{
    "category": "cs.DC", 
    "author": "Ognjen Prnjat", 
    "title": "Enabling Adaptive Grid Scheduling and Resource Management", 
    "publish": "2007-11-02T15:30:53Z", 
    "summary": "Wider adoption of the Grid concept has led to an increasing amount of\nfederated computational, storage and visualisation resources being available to\nscientists and researchers. Distributed and heterogeneous nature of these\nresources renders most of the legacy cluster monitoring and management\napproaches inappropriate, and poses new challenges in workflow scheduling on\nsuch systems. Effective resource utilisation monitoring and highly granular yet\nadaptive measurements are prerequisites for a more efficient Grid scheduler. We\npresent a suite of measurement applications able to monitor per-process\nresource utilisation, and a customisable tool for emulating observed\nutilisation models. We also outline our future work on a predictive and\nprobabilistic Grid scheduler. The research is undertaken as part of UK\ne-Science EPSRC sponsored project SO-GRM (Self-Organising Grid Resource\nManagement) in cooperation with BT.", 
    "link": "http://arxiv.org/pdf/0711.0326v1", 
    "arxiv-id": "0711.0326v1"
},{
    "category": "cs.DC", 
    "author": "Ognjen Prnjat", 
    "title": "Managing Uncertainty: A Case for Probabilistic Grid Scheduling", 
    "publish": "2007-11-02T15:36:36Z", 
    "summary": "The Grid technology is evolving into a global, service-orientated\narchitecture, a universal platform for delivering future high demand\ncomputational services. Strong adoption of the Grid and the utility computing\nconcept is leading to an increasing number of Grid installations running a wide\nrange of applications of different size and complexity. In this paper we\naddress the problem of elivering deadline/economy based scheduling in a\nheterogeneous application environment using statistical properties of job\nhistorical executions and its associated meta-data. This approach is motivated\nby a study of six-month computational load generated by Grid applications in a\nmulti-purpose Grid cluster serving a community of twenty e-Science projects.\nThe observed job statistics, resource utilisation and user behaviour is\ndiscussed in the context of management approaches and models most suitable for\nsupporting a probabilistic and autonomous scheduling architecture.", 
    "link": "http://arxiv.org/pdf/0711.0327v1", 
    "arxiv-id": "0711.0327v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Optimizing Latency and Reliability of Pipeline Workflow Applications", 
    "publish": "2007-11-08T14:45:12Z", 
    "summary": "Mapping applications onto heterogeneous platforms is a difficult challenge,\neven for simple application patterns such as pipeline graphs. The problem is\neven more complex when processors are subject to failure during the execution\nof the application. In this paper, we study the complexity of a bi-criteria\nmapping which aims at optimizing the latency (i.e., the response time) and the\nreliability (i.e., the probability that the computation will be successful) of\nthe application. Latency is minimized by using faster processors, while\nreliability is increased by replicating computations on a set of processors.\nHowever, replication increases latency (additional communications, slower\nprocessors). The application fails to be executed only if all the processors\nfail during execution. While simple polynomial algorithms can be found for\nfully homogeneous platforms, the problem becomes NP-hard when tackling\nheterogeneous platforms. This is yet another illustration of the additional\ncomplexity added by heterogeneity.", 
    "link": "http://arxiv.org/pdf/0711.1231v3", 
    "arxiv-id": "0711.1231v3"
},{
    "category": "cs.DC", 
    "author": "Fabrice Mourlin", 
    "title": "A Mobile Computing Architecture for Numerical Simulation", 
    "publish": "2007-11-12T14:39:34Z", 
    "summary": "The domain of numerical simulation is a place where the parallelization of\nnumerical code is common. The definition of a numerical context means the\nconfiguration of resources such as memory, processor load and communication\ngraph, with an evolving feature: the resources availability. A feature is often\nmissing: the adaptability. It is not predictable and the adaptable aspect is\nessential. Without calling into question these implementations of these codes,\nwe create an adaptive use of these implementations. Because the execution has\nto be driven by the availability of main resources, the components of a numeric\ncomputation have to react when their context changes. This paper offers a new\narchitecture, a mobile computing architecture, based on mobile agents and\nJavaSpace. At the end of this paper, we apply our architecture to several case\nstudies and obtain our first results.", 
    "link": "http://arxiv.org/pdf/0711.1786v1", 
    "arxiv-id": "0711.1786v1"
},{
    "category": "cs.DC", 
    "author": "Thomas Sandholm", 
    "title": "Autoregressive Time Series Forecasting of Computational Demand", 
    "publish": "2007-11-14T00:57:13Z", 
    "summary": "We study the predictive power of autoregressive moving average models when\nforecasting demand in two shared computational networks, PlanetLab and Tycoon.\nDemand in these networks is very volatile, and predictive techniques to plan\nusage in advance can improve the performance obtained drastically.\n  Our key finding is that a random walk predictor performs best for\none-step-ahead forecasts, whereas ARIMA(1,1,0) and adaptive exponential\nsmoothing models perform better for two and three-step-ahead forecasts. A Monte\nCarlo bootstrap test is proposed to evaluate the continuous prediction\nperformance of different models with arbitrary confidence and statistical\nsignificance levels. Although the prediction results differ between the Tycoon\nand PlanetLab networks, we observe very similar overall statistical properties,\nsuch as volatility dynamics.", 
    "link": "http://arxiv.org/pdf/0711.2062v1", 
    "arxiv-id": "0711.2062v1"
},{
    "category": "cs.DC", 
    "author": "Aaron Harwood", 
    "title": "An Adaptive Checkpointing Scheme for Peer-to-Peer Based Volunteer   Computing Work Flows", 
    "publish": "2007-11-26T06:41:23Z", 
    "summary": "Volunteer Computing, sometimes called Public Resource Computing, is an\nemerging computational model that is very suitable for work-pooled parallel\nprocessing. As more complex grid applications make use of work flows in their\ndesign and deployment it is reasonable to consider the impact of work flow\ndeployment over a Volunteer Computing infrastructure. In this case, the inter\nwork flow I/O can lead to a significant increase in I/O demands at the work\npool server. A possible solution is the use of a Peer-to- Peer based parallel\ncomputing architecture to off-load this I/O demand to the workers; where the\nworkers can fulfill some aspects of work flow coordination and I/O checking,\netc. However, achieving robustness in such a large scale system is a\nchallenging hurdle towards the decentralized execution of work flows and\ngeneral parallel processes. To increase robustness, we propose and show the\nmerits of using an adaptive checkpoint scheme that efficiently checkpoints the\nstatus of the parallel processes according to the estimation of relevant\nnetwork and peer parameters. Our scheme uses statistical data observed during\nruntime to dynamically make checkpoint decisions in a completely de-\ncentralized manner. The results of simulation show support for our proposed\napproach in terms of reduced required runtime.", 
    "link": "http://arxiv.org/pdf/0711.3949v1", 
    "arxiv-id": "0711.3949v1"
},{
    "category": "cs.DC", 
    "author": "Michel Raynal", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "publish": "2007-12-26T13:55:47Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services capable of\ndealing with resource assignment and management in a large-scale, heterogeneous\nand unreliable environment. The slicing service, has been proposed to allow for\nan automatic partitioning of P2P networks into groups (slices) that represent a\ncontrollable amount of some resource and that are also relatively homogeneous\nwith respect to that resource. In this paper we propose two gossip-based\nalgorithms to solve the distributed slicing problem. The first algorithm speeds\nup an existing algorithm sorting a set of uniform random numbers. The second\nalgorithm statistically approximates the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms rely on\ntheir gossip-based models. These algorithms are proved viable theoretically and\nexperimentally.", 
    "link": "http://arxiv.org/pdf/0712.3980v1", 
    "arxiv-id": "0712.3980v1"
},{
    "category": "cs.DC", 
    "author": "K. Sharman", 
    "title": "Increasing GP Computing Power via Volunteer Computing", 
    "publish": "2008-01-08T11:36:35Z", 
    "summary": "This paper describes how it is possible to increase GP Computing Power via\nVolunteer Computing (VC) using the BOINC framework. Two experiments using\nwell-known GP tools -Lil-gp & ECJ- are performed in order to demonstrate the\nbenefit of using VC in terms of computing power and speed up. Finally we\npresent an extension of the model where any GP tool or framework can be used\ninside BOINC regardless of its programming language, complexity or required\noperating system.", 
    "link": "http://arxiv.org/pdf/0801.1210v1", 
    "arxiv-id": "0801.1210v1"
},{
    "category": "cs.DC", 
    "author": "Bruno Sericola", 
    "title": "Core Persistence in Peer-to-Peer Systems: Relating Size to Lifetime", 
    "publish": "2008-01-09T12:41:15Z", 
    "summary": "Distributed systems are now both very large and highly dynamic. Peer to peer\noverlay networks have been proved efficient to cope with this new deal that\ntraditional approaches can no longer accommodate. While the challenge of\norganizing peers in an overlay network has generated a lot of interest leading\nto a large number of solutions, maintaining critical data in such a network\nremains an open issue. In this paper, we are interested in defining the portion\nof nodes and frequency one has to probe, given the churn observed in the\nsystem, in order to achieve a given probability of maintaining the persistence\nof some critical data. More specifically, we provide a clear result relating\nthe size and the frequency of the probing set along with its proof as well as\nan analysis of the way of leveraging such an information in a large scale\ndynamic distributed system.", 
    "link": "http://arxiv.org/pdf/0801.1419v1", 
    "arxiv-id": "0801.1419v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Bi-criteria Pipeline Mappings for Parallel Image Processing", 
    "publish": "2008-01-11T14:48:43Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonistic criteria should be optimized, such as throughput and latency (or a\ncombination). Typical applications include digital image processing, where\nimages are processed in steady-state mode. In this paper, we study the mapping\nof a particular image processing application, the JPEG encoding. Mapping\npipelined JPEG encoding onto parallel platforms is useful for instance for\nencoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete,\nwe concentrate on the evaluation and performance of polynomial heuristics.", 
    "link": "http://arxiv.org/pdf/0801.1772v1", 
    "arxiv-id": "0801.1772v1"
},{
    "category": "cs.DC", 
    "author": "M. Uzcategui", 
    "title": "e-Science perspectives in Venezuela", 
    "publish": "2008-01-27T20:22:29Z", 
    "summary": "We describe the e-Science strategy in Venezuela, in particular initiatives by\nthe Centro Nacional de Calculo Cientifico Universidad de Los Andes (CECALCULA),\nMerida, the Universidad de Los Andes (ULA), Merida, and the Instituto\nVenezolano de Investigaciones Cientificas (IVIC), Caracas. We present the plans\nfor the Venezuelan Academic Grid and the current status of Grid ULA supported\nby Internet2. We show different web-based scientific applications that are\nbeing developed in quantum chemistry, atomic physics, structural damage\nanalysis, biomedicine and bioclimate within the framework of the\nE-Infrastructure shared between Europe and Latin America (EELA)", 
    "link": "http://arxiv.org/pdf/0801.4150v1", 
    "arxiv-id": "0801.4150v1"
},{
    "category": "cs.DC", 
    "author": "Marin Bertier", 
    "title": "Energy Aware Self-Organizing Density Management in Wireless Sensor   Networks", 
    "publish": "2008-02-05T07:03:28Z", 
    "summary": "Energy consumption is the most important factor that determines sensor node\nlifetime. The optimization of wireless sensor network lifetime targets not only\nthe reduction of energy consumption of a single sensor node but also the\nextension of the entire network lifetime. We propose a simple and adaptive\nenergy-conserving topology management scheme, called SAND (Self-Organizing\nActive Node Density). SAND is fully decentralized and relies on a distributed\nprobing approach and on the redundancy resolution of sensors for energy\noptimizations, while preserving the data forwarding and sensing capabilities of\nthe network. We present the SAND's algorithm, its analysis of convergence, and\nsimulation results. Simulation results show that, though slightly increasing\npath lengths from sensor to sink nodes, the proposed scheme improves\nsignificantly the network lifetime for different neighborhood densities\ndegrees, while preserving both sensing and routing fidelity.", 
    "link": "http://arxiv.org/pdf/0802.0550v1", 
    "arxiv-id": "0802.0550v1"
},{
    "category": "cs.DC", 
    "author": "Gene Cooperman", 
    "title": "A Bit-Compatible Shared Memory Parallelization for ILU(k)   Preconditioning and a Bit-Compatible Generalization to Distributed Memory", 
    "publish": "2008-03-01T07:21:27Z", 
    "summary": "ILU(k) is a commonly used preconditioner for iterative linear solvers for\nsparse, non-symmetric systems. It is often preferred for the sake of its\nstability. We present TPILU(k), the first efficiently parallelized ILU(k)\npreconditioner that maintains this important stability property. Even better,\nTPILU(k) preconditioning produces an answer that is bit-compatible with the\nsequential ILU(k) preconditioning. In terms of performance, the TPILU(k)\npreconditioning is shown to run faster whenever more cores are made available\nto it --- while continuing to be as stable as sequential ILU(k). This is in\ncontrast to some competing methods that may become unstable if the degree of\nthread parallelism is raised too far. Where Block Jacobi ILU(k) fails in an\napplication, it can be replaced by TPILU(k) in order to maintain good\nperformance, while also achieving full stability. As a further optimization,\nTPILU(k) offers an optional level-based incomplete inverse method as a fast\napproximation for the original ILU(k) preconditioned matrix. Although this\nenhancement is not bit-compatible with classical ILU(k), it is bit-compatible\nwith the output from the single-threaded version of the same algorithm. In\nexperiments on a 16-core computer, the enhanced TPILU(k)-based iterative linear\nsolver performed up to 9 times faster. As we approach an era of many-core\ncomputing, the ability to efficiently take advantage of many cores will become\never more important. TPILU(k) also demonstrates good performance on cluster or\nGrid. For example, the new algorithm achieves 50 times speedup with 80 nodes\nfor general sparse matrices of dimension 160,000 that are diagonally dominant.", 
    "link": "http://arxiv.org/pdf/0803.0048v4", 
    "arxiv-id": "0803.0048v4"
},{
    "category": "cs.DC", 
    "author": "Hanna Parnas", 
    "title": "Self-Stabilizing Pulse Synchronization Inspired by Biological Pacemaker   Networks", 
    "publish": "2008-03-03T13:46:45Z", 
    "summary": "We define the ``Pulse Synchronization'' problem that requires nodes to\nachieve tight synchronization of regular pulse events, in the settings of\ndistributed computing systems. Pulse-coupled synchronization is a phenomenon\ndisplayed by a large variety of biological systems, typically overcoming a high\nlevel of noise. Inspired by such biological models, a robust and\nself-stabilizing Byzantine pulse synchronization algorithm for distributed\ncomputer systems is presented. The algorithm attains near optimal\nsynchronization tightness while tolerating up to a third of the nodes\nexhibiting Byzantine behavior concurrently. Pulse synchronization has been\npreviously shown to be a powerful building block for designing algorithms in\nthis severe fault model. We have previously shown how to stabilize general\nByzantine algorithms, using pulse synchronization. To the best of our knowledge\nthere is no other scheme to do this without the use of synchronized pulses.", 
    "link": "http://arxiv.org/pdf/0803.0241v2", 
    "arxiv-id": "0803.0241v2"
},{
    "category": "cs.DC", 
    "author": "Wenbing Zhao", 
    "title": "Integrity-Enhancing Replica Coordination for Byzantine Fault Tolerant   Systems", 
    "publish": "2008-03-11T04:20:06Z", 
    "summary": "Strong replica consistency is often achieved by writing deterministic\napplications, or by using a variety of mechanisms to render replicas\ndeterministic. There exists a large body of work on how to render replicas\ndeterministic under the benign fault model. However, when replicas can be\nsubject to malicious faults, most of the previous work is no longer effective.\nFurthermore, the determinism of the replicas is often considered harmful from\nthe security perspective and for many applications, their integrity strongly\ndepends on the randomness of some of their internal operations. This calls for\nnew approaches towards achieving replica consistency while preserving the\nreplica randomness. In this paper, we present two such approaches. One is based\non Byzantine agreement and the other on threshold coin-tossing. Each approach\nhas its strength and weaknesses. We compare the performance of the two\napproaches and outline their respective best use scenarios.", 
    "link": "http://arxiv.org/pdf/0803.1520v1", 
    "arxiv-id": "0803.1520v1"
},{
    "category": "cs.DC", 
    "author": "Wenbing Zhao", 
    "title": "Proactive Service Migration for Long-Running Byzantine Fault Tolerant   Systems", 
    "publish": "2008-03-11T04:34:04Z", 
    "summary": "In this paper, we describe a novel proactive recovery scheme based on service\nmigration for long-running Byzantine fault tolerant systems. Proactive recovery\nis an essential method for ensuring long term reliability of fault tolerant\nsystems that are under continuous threats from malicious adversaries. The\nprimary benefit of our proactive recovery scheme is a reduced vulnerability\nwindow. This is achieved by removing the time-consuming reboot step from the\ncritical path of proactive recovery. Our migration-based proactive recovery is\ncoordinated among the replicas, therefore, it can automatically adjust to\ndifferent system loads and avoid the problem of excessive concurrent proactive\nrecoveries that may occur in previous work with fixed watchdog timeouts.\nMoreover, the fast proactive recovery also significantly improves the system\navailability in the presence of faults.", 
    "link": "http://arxiv.org/pdf/0803.1521v1", 
    "arxiv-id": "0803.1521v1"
},{
    "category": "cs.DC", 
    "author": "Jose Rolim", 
    "title": "Lighweight Target Tracking Using Passive Traces in Sensor Networks", 
    "publish": "2008-03-14T18:01:17Z", 
    "summary": "We study the important problem of tracking moving targets in wireless sensor\nnetworks. We try to overcome the limitations of standard state of the art\ntracking methods based on continuous location tracking, i.e. the high energy\ndissipation and communication overhead imposed by the active participation of\nsensors in the tracking process and the low scalability, especially in sparse\nnetworks. Instead, our approach uses sensors in a passive way: they just record\nand judiciously spread information about observed target presence in their\nvicinity; this information is then used by the (powerful) tracking agent to\nlocate the target by just following the traces left at sensors. Our protocol is\ngreedy, local, distributed, energy efficient and very successful, in the sense\nthat (as shown by extensive simulations) the tracking agent manages to quickly\nlocate and follow the target; also, we achieve good trade-offs between the\nenergy dissipation and latency.", 
    "link": "http://arxiv.org/pdf/0803.2219v1", 
    "arxiv-id": "0803.2219v1"
},{
    "category": "cs.DC", 
    "author": "Hariharan Narayanan", 
    "title": "Distributed Averaging in the presence of a Sparse Cut", 
    "publish": "2008-03-25T22:04:50Z", 
    "summary": "We consider the question of averaging on a graph that has one sparse cut\nseparating two subgraphs that are internally well connected.\n  While there has been a large body of work devoted to algorithms for\ndistributed averaging, nearly all algorithms involve only {\\it convex} updates.\nIn this paper, we suggest that {\\it non-convex} updates can lead to significant\nimprovements. We do so by exhibiting a decentralized algorithm for graphs with\none sparse cut that uses non-convex averages and has an averaging time that can\nbe significantly smaller than the averaging time of known distributed\nalgorithms, such as those of \\cite{tsitsiklis, Boyd}. We use stochastic\ndominance to prove this result in a way that may be of independent interest.", 
    "link": "http://arxiv.org/pdf/0803.3642v3", 
    "arxiv-id": "0803.3642v3"
},{
    "category": "cs.DC", 
    "author": "A. Nedic", 
    "title": "Distributed and Recursive Parameter Estimation in Parametrized Linear   State-Space Models", 
    "publish": "2008-04-10T03:47:05Z", 
    "summary": "We consider a network of sensors deployed to sense a spatio-temporal field\nand estimate a parameter of interest. We are interested in the case where the\ntemporal process sensed by each sensor can be modeled as a state-space process\nthat is perturbed by random noise and parametrized by an unknown parameter. To\nestimate the unknown parameter from the measurements that the sensors\nsequentially collect, we propose a distributed and recursive estimation\nalgorithm, which we refer to as the incremental recursive prediction error\nalgorithm. This algorithm has the distributed property of incremental gradient\nalgorithms and the on-line property of recursive prediction error algorithms.\nWe study the convergence behavior of the algorithm and provide sufficient\nconditions for its convergence. Our convergence result is rather general and\ncontains as special cases the known convergence results for the incremental\nversions of the least-mean square algorithm. Finally, we use the algorithm\ndeveloped in this paper to identify the source of a gas-leak (diffusing source)\nin a closed warehouse and also report numerical simulations to verify\nconvergence.", 
    "link": "http://arxiv.org/pdf/0804.1607v2", 
    "arxiv-id": "0804.1607v2"
},{
    "category": "cs.DC", 
    "author": "Mohamed Jemni", 
    "title": "\u00c9tude de performance des syst\u00e8mes de d\u00e9couverte de ressources", 
    "publish": "2008-04-29T11:51:19Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. This technology consists\nmainly in exploiting PC resources, geographically dispersed, to treat time\nconsuming applications and/or important storage capacity requiring\napplications. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfiguration, decentralization and performance\nbecomes more and more essential. In this context, this paper evaluates the\nscalability and performance of P2P tools for registering and discovering\nservices (Publish/Subscribe systems). Three protocols are used in this purpose:\nBonjour, Avahi and Pastry. We have studied the behaviour of these protocols\nrelated to two criteria: the elapsed time for registrations services and the\nneeded time to discover new services.", 
    "link": "http://arxiv.org/pdf/0804.4590v1", 
    "arxiv-id": "0804.4590v1"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "Tight local approximation results for max-min linear programs", 
    "publish": "2008-04-30T12:54:34Z", 
    "summary": "In a bipartite max-min LP, we are given a bipartite graph $\\myG = (V \\cup I\n\\cup K, E)$, where each agent $v \\in V$ is adjacent to exactly one constraint\n$i \\in I$ and exactly one objective $k \\in K$. Each agent $v$ controls a\nvariable $x_v$. For each $i \\in I$ we have a nonnegative linear constraint on\nthe variables of adjacent agents. For each $k \\in K$ we have a nonnegative\nlinear objective function of the variables of adjacent agents. The task is to\nmaximise the minimum of the objective functions. We study local algorithms\nwhere each agent $v$ must choose $x_v$ based on input within its\nconstant-radius neighbourhood in $\\myG$. We show that for every $\\epsilon>0$\nthere exists a local algorithm achieving the approximation ratio ${\\Delta_I (1\n- 1/\\Delta_K)} + \\epsilon$. We also show that this result is the best possible\n-- no local algorithm can achieve the approximation ratio ${\\Delta_I (1 -\n1/\\Delta_K)}$. Here $\\Delta_I$ is the maximum degree of a vertex $i \\in I$, and\n$\\Delta_K$ is the maximum degree of a vertex $k \\in K$. As a methodological\ncontribution, we introduce the technique of graph unfolding for the design of\nlocal approximation algorithms.", 
    "link": "http://arxiv.org/pdf/0804.4815v1", 
    "arxiv-id": "0804.4815v1"
},{
    "category": "cs.DC", 
    "author": "David Eisenstat", 
    "title": "Two-enqueuer queue in Common2", 
    "publish": "2008-05-04T21:08:50Z", 
    "summary": "The question of whether all shared objects with consensus number 2 belong to\nCommon2, the set of objects that can be implemented in a wait-free manner by\nany type of consensus number 2, was first posed by Herlihy. In the absence of\ngeneral results, several researchers have obtained implementations for\nrestricted-concurrency versions of FIFO queues. We present the first Common2\nalgorithm for a queue with two enqueuers and any number of dequeuers.", 
    "link": "http://arxiv.org/pdf/0805.0444v2", 
    "arxiv-id": "0805.0444v2"
},{
    "category": "cs.DC", 
    "author": "Alexander Shraer", 
    "title": "Fork Sequential Consistency is Blocking", 
    "publish": "2008-05-14T14:23:53Z", 
    "summary": "We consider an untrusted server storing shared data on behalf of clients. We\nshow that no storage access protocol can on the one hand preserve sequential\nconsistency and wait-freedom when the server is correct, and on the other hand\nalways preserve fork sequential consistency.", 
    "link": "http://arxiv.org/pdf/0805.2068v1", 
    "arxiv-id": "0805.2068v1"
},{
    "category": "cs.DC", 
    "author": "Jean-Ren\u00e9 Ruault", 
    "title": "Coupling Component Systems towards Systems of Systems", 
    "publish": "2008-05-21T05:02:22Z", 
    "summary": "Systems of systems (SoS) are a hot topic in our \"fully connected global\nworld\". Our aim is not to provide another definition of what SoS are, but\nrather to focus on the adequacy of reusing standard system architecting\ntechniques within this approach in order to improve performance, fault\ndetection and safety issues in large-scale coupled systems that definitely\nqualify as SoS, whatever the definition is. A key issue will be to secure the\navailability of the services provided by the SoS despite the evolution of the\nvarious systems composing the SoS. We will also tackle contracting issues and\nresponsibility transfers, as they should be addressed to ensure the expected\nbehavior of the SoS whilst the various independently contracted systems evolve\nasynchronously.", 
    "link": "http://arxiv.org/pdf/0805.3196v1", 
    "arxiv-id": "0805.3196v1"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "Local approximation algorithms for a class of 0/1 max-min linear   programs", 
    "publish": "2008-06-02T14:27:46Z", 
    "summary": "We study the applicability of distributed, local algorithms to 0/1 max-min\nLPs where the objective is to maximise ${\\min_k \\sum_v c_{kv} x_v}$ subject to\n${\\sum_v a_{iv} x_v \\le 1}$ for each $i$ and ${x_v \\ge 0}$ for each $v$. Here\n$c_{kv} \\in \\{0,1\\}$, $a_{iv} \\in \\{0,1\\}$, and the support sets ${V_i = \\{v :\na_{iv} > 0 \\}}$ and ${V_k = \\{v : c_{kv}>0 \\}}$ have bounded size; in\nparticular, we study the case $|V_k| \\le 2$. Each agent $v$ is responsible for\nchoosing the value of $x_v$ based on information within its constant-size\nneighbourhood; the communication network is the hypergraph where the sets $V_k$\nand $V_i$ constitute the hyperedges. We present a local approximation algorithm\nwhich achieves an approximation ratio arbitrarily close to the theoretical\nlower bound presented in prior work.", 
    "link": "http://arxiv.org/pdf/0806.0282v1", 
    "arxiv-id": "0806.0282v1"
},{
    "category": "cs.DC", 
    "author": "Murat Yuksel", 
    "title": "Ad-hoc Limited Scale-Free Models for Unstructured Peer-to-Peer Networks", 
    "publish": "2008-06-14T19:01:50Z", 
    "summary": "Several protocol efficiency metrics (e.g., scalability, search success rate,\nrouting reachability and stability) depend on the capability of preserving\nstructure even over the churn caused by the ad-hoc nodes joining or leaving the\nnetwork. Preserving the structure becomes more prohibitive due to the\ndistributed and potentially uncooperative nature of such networks, as in the\npeer-to-peer (P2P) networks. Thus, most practical solutions involve\nunstructured approaches while attempting to maintain the structure at various\nlevels of protocol stack. The primary focus of this paper is to investigate\nconstruction and maintenance of scale-free topologies in a distributed manner\nwithout requiring global topology information at the time when nodes join or\nleave. We consider the uncooperative behavior of peers by limiting the number\nof neighbors to a pre-defined hard cutoff value (i.e., no peer is a major hub),\nand the ad-hoc behavior of peers by rewiring the neighbors of nodes leaving the\nnetwork. We also investigate the effect of these hard cutoffs and rewiring of\nad-hoc nodes on the P2P search efficiency.", 
    "link": "http://arxiv.org/pdf/0806.2395v1", 
    "arxiv-id": "0806.2395v1"
},{
    "category": "cs.DC", 
    "author": "Athanasios K. Tsakalidis", 
    "title": "TRANS-Net: an Efficient Peer-to-Peer Overlay Network Based on a Full   Transposition Graph", 
    "publish": "2008-06-19T08:28:28Z", 
    "summary": "In this paper we propose a new practical P2P system based on a full\ntransposition network topology named TRANS-Net. Full transposition networks\nachieve higher fault-tolerance and lower congestion among the class of\ntransposition networks. TRANS-Net provides an efficient lookup service i.e. k\nhops with high probability, where k satisfies Theta(log_n m) less than k less\nthan Theta(log_2 m), where m denotes the number of system nodes and n is a\nsystem parameter related to the maximum number that m can take (up to n!).\nExperiments show that the look-up performance achieves the lower limit of the\ncomplexity relation. TRANS-Net also preserves data locality and provides\nefficient look-up performance for complex queries such as multi-dimensional\nqueries.", 
    "link": "http://arxiv.org/pdf/0806.3152v2", 
    "arxiv-id": "0806.3152v2"
},{
    "category": "cs.DC", 
    "author": "David K. Y. Chiu", 
    "title": "MOHCS: Towards Mining Overlapping Highly Connected Subgraphs", 
    "publish": "2008-06-19T15:13:38Z", 
    "summary": "Many networks in real-life typically contain parts in which some nodes are\nmore highly connected to each other than the other nodes of the network. The\ncollection of such nodes are usually called clusters, communities, cohesive\ngroups or modules. In graph terminology, it is called highly connected graph.\nIn this paper, we first prove some properties related to highly connected\ngraph. Based on these properties, we then redefine the highly connected\nsubgraph which results in an algorithm that determines whether a given graph is\nhighly connected in linear time. Then we present a computationally efficient\nalgorithm, called MOHCS, for mining overlapping highly connected subgraphs. We\nhave evaluated experimentally the performance of MOHCS using real and synthetic\ndata sets from computer-generated graph and yeast protein network. Our results\nshow that MOHCS is effective and reliable in finding overlapping highly\nconnected subgraphs. Keywords-component; Highly connected subgraph, clustering\nalgorithms, minimum cut, minimum degree", 
    "link": "http://arxiv.org/pdf/0806.3215v1", 
    "arxiv-id": "0806.3215v1"
},{
    "category": "cs.DC", 
    "author": "Sriram V. Pemmaraju", 
    "title": "Localized Spanners for Wireless Networks", 
    "publish": "2008-06-26T02:09:17Z", 
    "summary": "We present a new efficient localized algorithm to construct, for any given\nquasi-unit disk graph G=(V,E) and any e > 0, a (1+e)-spanner for G of maximum\ndegree O(1) and total weight O(w(MST)), where w(MST) denotes the weight of a\nminimum spanning tree for V. We further show that similar localized techniques\ncan be used to construct, for a given unit disk graph G = (V, E), a planar\nCdel(1+e)(1+pi/2)-spanner for G of maximum degree O(1) and total weight\nO(w(MST)). Here Cdel denotes the stretch factor of the unit Delaunay\ntriangulation for V. Both constructions can be completed in O(1) communication\nrounds, and require each node to know its own coordinates.", 
    "link": "http://arxiv.org/pdf/0806.4221v1", 
    "arxiv-id": "0806.4221v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Resource Allocation Strategies for In-Network Stream Processing", 
    "publish": "2008-07-10T19:14:14Z", 
    "summary": "In this paper we consider the operator mapping problem for in-network stream\nprocessing applications. In-network stream processing consists in applying a\ntree of operators in steady-state to multiple data objects that are continually\nupdated at various locations on a network. Examples of in-network stream\nprocessing include the processing of data in a sensor network, or of continuous\nqueries on distributed relational databases. We study the operator mapping\nproblem in a ``constructive'' scenario, i.e., a scenario in which one builds a\nplatform dedicated to the application buy purchasing processing servers with\nvarious costs and capabilities. The objective is to minimize the cost of the\nplatform while ensuring that the application achieves a minimum steady-state\nthroughput. The first contribution of this paper is the formalization of a set\nof relevant operator-placement problems as linear programs, and a proof that\neven simple versions of the problem are NP-complete. Our second contribution is\nthe design of several polynomial time heuristics, which are evaluated via\nextensive simulations and compared to theoretical bounds for optimal solutions.", 
    "link": "http://arxiv.org/pdf/0807.1720v1", 
    "arxiv-id": "0807.1720v1"
},{
    "category": "cs.DC", 
    "author": "A. B. Mutiara", 
    "title": "Analisis Kinerja Sistem Cluster Terhadapa Aplikasi Simulasi Dinamika   Molekular NAMD Memanfaatkan Pustaka CHARM++", 
    "publish": "2008-07-29T09:18:21Z", 
    "summary": "Tingkat kompleksitas dari program simulasi dinamika molekular membutuhkan\nmesin pemroses dengan kemampuan yang sangat besar. Mesin-mesin paralel terbukti\nmemiliki potensi untuk menjawab tantangan komputasi ini. Untuk memanfaatkan\npotensi ini secara maksimal, diperlukan suatu program paralel dengan tingkat\nefisiensi, efektifitas, skalabilitas, dan ekstensibilitas yang maksimal pula.\nProgram NAMD yang dibahas pada penulisan ini dianggap mampu untuk memenuhi\nsemua kriteria yang diinginkan. Program ini dirancang dengan\nmengimplementasikan pustaka Charm++ untuk pembagian tugas perhitungan secara\nparalel. NAMD memiliki sistem automatic load balancing secara periodik yang\ncerdas, sehingga dapat memaksimalkan penggunaan kemampuan mesin yang tersedia.\nProgram ini juga dirancang secara modular, sehingga dapat dimodifikasi dan\nditambah dengan sangat mudah. NAMD menggunakan banyak kombinasi algoritma\nperhitungan dan tehnik-tehnik numerik lainnya dalam melakukan tugasnya. NAMD\n2.5 mengimplementasikan semua tehnik dan persamaan perhitungan yang digunakan\ndalam dunia simulasi dinamika molekular saat ini. NAMD dapat berjalan diatas\nberbagai mesin paralel termasuk arsitektur cluster, dengan hasil speedup yang\nmengejutkan. Tulisan ini akan menjelaskan dan membuktikan kemampuan NAMD secara\nparalel diatas lima buah mesin cluster. Penulisan ini juga akan memaparkan\nkinerja NAMD pada beberapa.", 
    "link": "http://arxiv.org/pdf/0807.4609v1", 
    "arxiv-id": "0807.4609v1"
},{
    "category": "cs.DC", 
    "author": "Joseph Y. Halpern", 
    "title": "An Almost-Surely Terminating Polynomial Protocol for Asynchronous   Byzantine Agreement with Optimal Resilience", 
    "publish": "2008-08-11T12:22:12Z", 
    "summary": "Consider an asynchronous system with private channels and $n$ processes, up\nto $t$ of which may be faulty. We settle a longstanding open question by\nproviding a Byzantine agreement protocol that simultaneously achieves three\nproperties:\n  1. (optimal) resilience: it works as long as $n>3t$\n  2. (almost-sure) termination: with probability one, all nonfaulty processes\nterminate\n  3. (polynomial) efficiency: the expected computation time, memory\nconsumption, message size, and number of messages sent are all polynomial in\n$n$.\n  Earlier protocols have achieved only two of these three properties. In\nparticular, the protocol of Bracha is not polynomially efficient, the protocol\nof Feldman and Micali is not optimally resilient, and the protocol of Canetti\nand Rabin does not have almost-sure termination. Our protocol utilizes a new\nprimitive called shunning (asynchronous) verifiable secret sharing (SVSS),\nwhich ensures, roughly speaking, that either a secret is successfully shared or\na new faulty process is ignored from this point onwards by some nonfaulty\nprocess.", 
    "link": "http://arxiv.org/pdf/0808.1505v1", 
    "arxiv-id": "0808.1505v1"
},{
    "category": "cs.DC", 
    "author": "Wanzhi Zhang", 
    "title": "Compute and Storage Clouds Using Wide Area High Performance Networks", 
    "publish": "2008-08-13T09:48:37Z", 
    "summary": "We describe a cloud based infrastructure that we have developed that is\noptimized for wide area, high performance networks and designed to support data\nmining applications. The infrastructure consists of a storage cloud called\nSector and a compute cloud called Sphere. We describe two applications that we\nhave built using the cloud and some experimental studies.", 
    "link": "http://arxiv.org/pdf/0808.1802v1", 
    "arxiv-id": "0808.1802v1"
},{
    "category": "cs.DC", 
    "author": "Yunhong Gu", 
    "title": "Data Mining Using High Performance Data Clouds: Experimental Studies   Using Sector and Sphere", 
    "publish": "2008-08-22T01:24:06Z", 
    "summary": "We describe the design and implementation of a high performance cloud that we\nhave used to archive, analyze and mine large distributed data sets. By a cloud,\nwe mean an infrastructure that provides resources and/or services over the\nInternet. A storage cloud provides storage services, while a compute cloud\nprovides compute services. We describe the design of the Sector storage cloud\nand how it provides the storage services required by the Sphere compute cloud.\nWe also describe the programming paradigm supported by the Sphere compute\ncloud. Sector and Sphere are designed for analyzing large data sets using\ncomputer clusters connected with wide area high performance networks (for\nexample, 10+ Gb/s). We describe a distributed data mining application that we\nhave developed using Sector and Sphere. Finally, we describe some experimental\nstudies comparing Sector/Sphere to Hadoop.", 
    "link": "http://arxiv.org/pdf/0808.3019v1", 
    "arxiv-id": "0808.3019v1"
},{
    "category": "cs.DC", 
    "author": "Alex Szalay", 
    "title": "Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for   Data Intensive Applications", 
    "publish": "2008-08-26T15:19:44Z", 
    "summary": "Data intensive applications often involve the analysis of large datasets that\nrequire large amounts of compute and storage resources. While dedicated compute\nand/or storage farms offer good task/data throughput, they suffer low resource\nutilization problem under varying workloads conditions. If we instead move such\ndata to distributed computing resources, then we incur expensive data transfer\ncost. In this paper, we propose a data diffusion approach that combines dynamic\nresource provisioning, on-demand data replication and caching, and data\nlocality-aware scheduling to achieve improved resource efficiency under varying\nworkloads. We define an abstract \"data diffusion model\" that takes into\nconsideration the workload characteristics, data accessing cost, application\nthroughput and resource utilization; we validate the model using a real-world\nlarge-scale astronomy application. Our results show that data diffusion can\nincrease the performance index by as much as 34X, and improve application\nresponse time by over 506X, while achieving near-optimal throughputs and\nexecution times.", 
    "link": "http://arxiv.org/pdf/0808.3535v1", 
    "arxiv-id": "0808.3535v1"
},{
    "category": "cs.DC", 
    "author": "Ian Foster", 
    "title": "Enabling Loosely-Coupled Serial Job Execution on the IBM BlueGene/P   Supercomputer and the SiCortex SC5832", 
    "publish": "2008-08-26T16:59:41Z", 
    "summary": "Our work addresses the enabling of the execution of highly parallel\ncomputations composed of loosely coupled serial jobs with no modifications to\nthe respective applications, on large-scale systems. This approach allows\nnew-and potentially far larger-classes of application to leverage systems such\nas the IBM Blue Gene/P supercomputer and similar emerging petascale\narchitectures. We present here the challenges of I/O performance encountered in\nmaking this model practical, and show results using both micro-benchmarks and\nreal applications on two large-scale systems, the BG/P and the SiCortex SC5832.\nOur preliminary benchmarks show that we can scale to 4096 processors on the\nBlue Gene/P and 5832 processors on the SiCortex with high efficiency, and can\nachieve thousands of tasks/sec sustained execution rates for parallel workloads\nof ordinary serial applications. We measured applications from two domains,\neconomic energy modeling and molecular dynamics.", 
    "link": "http://arxiv.org/pdf/0808.3536v1", 
    "arxiv-id": "0808.3536v1"
},{
    "category": "cs.DC", 
    "author": "Ben Clifford", 
    "title": "Towards Loosely-Coupled Programming on Petascale Systems", 
    "publish": "2008-08-26T16:48:14Z", 
    "summary": "We have extended the Falkon lightweight task execution framework to make\nloosely coupled programming on petascale systems a practical and useful\nprogramming model. This work studies and measures the performance factors\ninvolved in applying this approach to enable the use of petascale systems by a\nbroader user community, and with greater ease. Our work enables the execution\nof highly parallel computations composed of loosely coupled serial jobs with no\nmodifications to the respective applications. This approach allows a new-and\npotentially far larger-class of applications to leverage petascale systems,\nsuch as the IBM Blue Gene/P supercomputer. We present the challenges of I/O\nperformance encountered in making this model practical, and show results using\nboth microbenchmarks and real applications from two domains: economic energy\nmodeling and molecular dynamics. Our benchmarks show that we can scale up to\n160K processor-cores with high efficiency, and can achieve sustained execution\nrates of thousands of tasks per second.", 
    "link": "http://arxiv.org/pdf/0808.3540v2", 
    "arxiv-id": "0808.3540v2"
},{
    "category": "cs.DC", 
    "author": "Alex Szalay", 
    "title": "Accelerating Large-scale Data Exploration through Data Diffusion", 
    "publish": "2008-08-26T16:02:50Z", 
    "summary": "Data-intensive applications often require exploratory analysis of large\ndatasets. If analysis is performed on distributed resources, data locality can\nbe crucial to high throughput and performance. We propose a \"data diffusion\"\napproach that acquires compute and storage resources dynamically, replicates\ndata in response to demand, and schedules computations close to data. As demand\nincreases, more resources are acquired, thus allowing faster response to\nsubsequent requests that refer to the same data; when demand drops, resources\nare released. This approach can provide the benefits of dedicated hardware\nwithout the associated high costs, depending on workload and resource\ncharacteristics. The approach is reminiscent of cooperative caching,\nweb-caching, and peer-to-peer storage systems, but addresses different\napplication demands. Other data-aware scheduling approaches assume dedicated\nresources, which can be expensive and/or inefficient if load varies\nsignificantly. To explore the feasibility of the data diffusion approach, we\nhave extended the Falkon resource provisioning and task scheduling system to\nsupport data caching and data-aware scheduling. Performance results from both\nmicro-benchmarks and a large scale astronomy application demonstrate that our\napproach improves performance relative to alternative approaches, as well as\nprovides improved scalability as aggregated I/O bandwidth scales linearly with\nthe number of data cache nodes.", 
    "link": "http://arxiv.org/pdf/0808.3546v1", 
    "arxiv-id": "0808.3546v1"
},{
    "category": "cs.DC", 
    "author": "Srikumar Venugopal", 
    "title": "Market-Oriented Cloud Computing: Vision, Hype, and Reality for   Delivering IT Services as Computing Utilities", 
    "publish": "2008-08-26T17:16:11Z", 
    "summary": "This keynote paper: presents a 21st century vision of computing; identifies\nvarious computing paradigms promising to deliver the vision of computing\nutilities; defines Cloud computing and provides the architecture for creating\nmarket-oriented Clouds by leveraging technologies such as VMs; provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; presents some representative Cloud platforms\nespecially those developed in industries along with our current work towards\nrealising market-oriented resource allocation of Clouds by leveraging the 3rd\ngeneration Aneka enterprise Grid technology; reveals our early thoughts on\ninterconnecting Clouds for dynamically creating an atmospheric computing\nenvironment along with pointers to future community research; and concludes\nwith the need for convergence of competing IT paradigms for delivering our 21st\ncentury vision.", 
    "link": "http://arxiv.org/pdf/0808.3558v1", 
    "arxiv-id": "0808.3558v1"
},{
    "category": "cs.DC", 
    "author": "J. M. Dana", 
    "title": "Providing Virtual Execution Environments: A Twofold Illustration", 
    "publish": "2008-08-27T12:48:39Z", 
    "summary": "Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management.", 
    "link": "http://arxiv.org/pdf/0808.3693v1", 
    "arxiv-id": "0808.3693v1"
},{
    "category": "cs.DC", 
    "author": "Robert L Grossman", 
    "title": "Sector and Sphere: Towards Simplified Storage and Processing of Large   Scale Distributed Data", 
    "publish": "2008-09-06T18:37:51Z", 
    "summary": "Cloud computing has demonstrated that processing very large datasets over\ncommodity clusters can be done simply given the right programming model and\ninfrastructure. In this paper, we describe the design and implementation of the\nSector storage cloud and the Sphere compute cloud. In contrast to existing\nstorage and compute clouds, Sector can manage data not only within a data\ncenter, but also across geographically distributed data centers. Similarly, the\nSphere compute cloud supports User Defined Functions (UDF) over data both\nwithin a data center and across data centers. As a special case, MapReduce\nstyle programming can be implemented in Sphere by using a Map UDF followed by a\nReduce UDF. We describe some experimental studies comparing Sector/Sphere and\nHadoop using the Terasort Benchmark. In these studies, Sector is about twice as\nfast as Hadoop. Sector/Sphere is open source.", 
    "link": "http://arxiv.org/pdf/0809.1181v2", 
    "arxiv-id": "0809.1181v2"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "An optimal local approximation algorithm for max-min linear programs", 
    "publish": "2008-09-09T06:11:31Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\napproximating max-min LPs. The objective is to maximise $\\omega$ subject to $Ax\n\\le 1$, $Cx \\ge \\omega 1$, and $x \\ge 0$ for nonnegative matrices $A$ and $C$.\nThe approximation ratio of our algorithm is the best possible for any local\nalgorithm; there is a matching unconditional lower bound.", 
    "link": "http://arxiv.org/pdf/0809.1489v1", 
    "arxiv-id": "0809.1489v1"
},{
    "category": "cs.DC", 
    "author": "Mohamed Kaaniche", 
    "title": "Modelling interdependencies between the electricity and information   infrastructures", 
    "publish": "2008-09-24T07:26:09Z", 
    "summary": "The aim of this paper is to provide qualitative models characterizing\ninterdependencies related failures of two critical infrastructures: the\nelectricity infrastructure and the associated information infrastructure. The\ninterdependencies of these two infrastructures are increasing due to a growing\nconnection of the power grid networks to the global information infrastructure,\nas a consequence of market deregulation and opening. These interdependencies\nincrease the risk of failures. We focus on cascading, escalating and\ncommon-cause failures, which correspond to the main causes of failures due to\ninterdependencies. We address failures in the electricity infrastructure, in\ncombination with accidental failures in the information infrastructure, then we\nshow briefly how malicious attacks in the information infrastructure can be\naddressed.", 
    "link": "http://arxiv.org/pdf/0809.4107v1", 
    "arxiv-id": "0809.4107v1"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "A simple local 3-approximation algorithm for vertex cover", 
    "publish": "2008-10-13T12:45:15Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\nfinding a 3-approximate vertex cover in bounded-degree graphs. The algorithm is\ndeterministic, and no auxiliary information besides port numbering is required.", 
    "link": "http://arxiv.org/pdf/0810.2175v1", 
    "arxiv-id": "0810.2175v1"
},{
    "category": "cs.DC", 
    "author": "Luc Boug\u00e9", 
    "title": "Enabling Lock-Free Concurrent Fine-Grain Access to Massive Distributed   Data: Application to Supernovae Detection", 
    "publish": "2008-10-13T13:07:18Z", 
    "summary": "We consider the problem of efficiently managing massive data in a large-scale\ndistributed environment. We consider data strings of size in the order of\nTerabytes, shared and accessed by concurrent clients. On each individual\naccess, a segment of a string, of the order of Megabytes, is read or modified.\nOur goal is to provide the clients with efficient fine-grain access the data\nstring as concurrently as possible, without locking the string itself. This\nissue is crucial in the context of applications in the field of astronomy,\ndatabases, data mining and multimedia. We illustrate these requiremens with the\ncase of an application for searching supernovae. Our solution relies on\ndistributed, RAM-based data storage, while leveraging a DHT-based, parallel\nmetadata management scheme. The proposed architecture and algorithms have been\nvalidated through a software prototype and evaluated in a cluster environment.", 
    "link": "http://arxiv.org/pdf/0810.2226v1", 
    "arxiv-id": "0810.2226v1"
},{
    "category": "cs.DC", 
    "author": "Luc Boug\u00e9", 
    "title": "Distributed Management of Massive Data: an Efficient Fine-Grain Data   Access Scheme", 
    "publish": "2008-10-13T13:08:14Z", 
    "summary": "This paper addresses the problem of efficiently storing and accessing massive\ndata blocks in a large-scale distributed environment, while providing efficient\nfine-grain access to data subsets. This issue is crucial in the context of\napplications in the field of databases, data mining and multimedia. We propose\na data sharing service based on distributed, RAM-based storage of data, while\nleveraging a DHT-based, natively parallel metadata management scheme. As\nopposed to the most commonly used grid storage infrastructures that provide\nmechanisms for explicit data localization and transfer, we provide a\ntransparent access model, where data are accessed through global identifiers.\nOur proposal has been validated through a prototype implementation whose\npreliminary evaluation provides promising results.", 
    "link": "http://arxiv.org/pdf/0810.2227v1", 
    "arxiv-id": "0810.2227v1"
},{
    "category": "cs.DC", 
    "author": "Alberto Sangiovanni-Vincentelli", 
    "title": "Distributed Estimation over Wireless Sensor Networks with Packet Losses", 
    "publish": "2008-10-21T01:00:26Z", 
    "summary": "A distributed adaptive algorithm to estimate a time-varying signal, measured\nby a wireless sensor network, is designed and analyzed. One of the major\nfeatures of the algorithm is that no central coordination among the nodes needs\nto be assumed. The measurements taken by the nodes of the network are affected\nby noise, and the communication among the nodes is subject to packet losses.\nNodes exchange local estimates and measurements with neighboring nodes. Each\nnode of the network locally computes adaptive weights that minimize the\nestimation error variance. Decentralized conditions on the weights, needed for\nthe convergence of the estimation error throughout the overall network, are\npresented. A Lipschitz optimization problem is posed to guarantee stability and\nthe minimization of the variance. An efficient strategy to distribute the\ncomputation of the optimal solution is investigated. A theoretical performance\nanalysis of the distributed algorithm is carried out both in the presence of\nperfect and lossy links. Numerical simulations illustrate performance for\nvarious network topologies and packet loss probabilities.", 
    "link": "http://arxiv.org/pdf/0810.3715v1", 
    "arxiv-id": "0810.3715v1"
},{
    "category": "cs.DC", 
    "author": "Franck Petit", 
    "title": "Best-effort Group Service in Dynamic Networks", 
    "publish": "2008-10-21T13:58:50Z", 
    "summary": "We propose a group membership service for dynamic ad hoc networks. It\nmaintains as long as possible the existing groups and ensures that each group\ndiameter is always smaller than a constant, fixed according to the application\nusing the groups. The proposed protocol is self-stabilizing and works in\ndynamic distributed systems. Moreover, it ensures a kind of continuity in the\nservice offer to the application while the system is converging, except if too\nstrong topology changes happen. Such a best effort behavior allows applications\nto rely on the groups while the stabilization has not been reached, which is\nvery useful in dynamic ad hoc networks.", 
    "link": "http://arxiv.org/pdf/0810.3836v3", 
    "arxiv-id": "0810.3836v3"
},{
    "category": "cs.DC", 
    "author": "Nir Tzachar", 
    "title": "Randomization Adaptive Self-Stabilization", 
    "publish": "2008-10-24T12:15:40Z", 
    "summary": "We present a scheme to convert self-stabilizing algorithms that use\nrandomization during and following convergence to self-stabilizing algorithms\nthat use randomization only during convergence. We thus reduce the number of\nrandom bits from an infinite number to a bounded number. The scheme is\napplicable to the cases in which there exits a local predicate for each node,\nsuch that global consistency is implied by the union of the local predicates.\nWe demonstrate our scheme over the token circulation algorithm of Herman and\nthe recent constant time Byzantine self-stabilizing clock synchronization\nalgorithm by Ben-Or, Dolev and Hoch. The application of our scheme results in\nthe first constant time Byzantine self-stabilizing clock synchronization\nalgorithm that uses a bounded number of random bits.", 
    "link": "http://arxiv.org/pdf/0810.4440v1", 
    "arxiv-id": "0810.4440v1"
},{
    "category": "cs.DC", 
    "author": "Peter Tr\u00f6ger", 
    "title": "The Multi-Core Era - Trends and Challenges", 
    "publish": "2008-10-30T08:34:48Z", 
    "summary": "Since the very beginning of hardware development, computer processors were\ninvented with ever-increasing clock frequencies and sophisticated in-build\noptimization strategies. Due to physical limitations, this 'free lunch' of\nspeedup has come to an end.\n  The following article gives a summary and bibliography for recent trends and\nchallenges in CMP architectures. It discusses how 40 years of parallel\ncomputing research need to be considered in the upcoming multi-core era. We\nargue that future research must be driven from two sides - a better expression\nof hardware structures, and a domain-specific understanding of software\nparallelism.", 
    "link": "http://arxiv.org/pdf/0810.5439v1", 
    "arxiv-id": "0810.5439v1"
},{
    "category": "cs.DC", 
    "author": "R. Nuriyev", 
    "title": "Programming languages with algorithmically parallelizing problem", 
    "publish": "2008-10-31T00:25:15Z", 
    "summary": "The study consists of two parts. Objective of the first part is modern\nlanguage constructions responsible for algorithmically insolvability of\nparallelizing problem. Second part contains several ways to modify the\nconstructions to make the problem algorithmically solvable", 
    "link": "http://arxiv.org/pdf/0810.5596v1", 
    "arxiv-id": "0810.5596v1"
},{
    "category": "cs.DC", 
    "author": "R. Nuriyev", 
    "title": "Practical language based on systems of definitions", 
    "publish": "2008-10-31T16:42:45Z", 
    "summary": "The article suggests a description of a system of tables with a set of\nspecial lists absorbing a semantics of data and reflects a fullness of data. It\nshows how their parallel processing can be constructed based on the\ndescriptions. The approach also might be used for definition intermediate\ntargets for data mining and unstructured data processing.", 
    "link": "http://arxiv.org/pdf/0810.5732v1", 
    "arxiv-id": "0810.5732v1"
},{
    "category": "cs.DC", 
    "author": "Renat Nuriyev", 
    "title": "Non procedural language for parallel programs", 
    "publish": "2008-10-31T18:44:38Z", 
    "summary": "Probably building non procedural languages is the most prospective way for\nparallel programming just because non procedural means no fixed way for\nexecution. The article consists of 3 parts. In first part we consider formal\nsystems for definition a named datasets and studying an expression power of\ndifferent subclasses. In the second part we consider a complexity of algorithms\nof building sets by the definitions. In third part we consider a fullness and\nflexibility of the class of program based data set definitions.", 
    "link": "http://arxiv.org/pdf/0810.5758v1", 
    "arxiv-id": "0810.5758v1"
},{
    "category": "cs.DC", 
    "author": "R. Nuriyev", 
    "title": "Parallel execution of portfolio optimization", 
    "publish": "2008-11-10T15:52:25Z", 
    "summary": "Analysis of asset liability management (ALM) strategies especially for long\nterm horizon is a crucial issue for banks, funds and insurance companies.\n  Modern economic models, investment strategies and optimization criteria make\nALM studies computationally very intensive task. It attracts attention to\nmultiprocessor system and especially to the cheapest one: multi core PCs and PC\nclusters.\n  In this article we are analyzing problem of parallel organization of\nportfolio optimization, results of using clusters for optimization and the most\nefficient cluster architecture for these kinds of tasks.", 
    "link": "http://arxiv.org/pdf/0811.1504v1", 
    "arxiv-id": "0811.1504v1"
},{
    "category": "cs.DC", 
    "author": "Danny Dolev", 
    "title": "Self-stabilizing Numerical Iterative Computation", 
    "publish": "2008-11-19T19:11:46Z", 
    "summary": "Many challenging tasks in sensor networks, including sensor calibration,\nranking of nodes, monitoring, event region detection, collaborative filtering,\ncollaborative signal processing, {\\em etc.}, can be formulated as a problem of\nsolving a linear system of equations. Several recent works propose different\ndistributed algorithms for solving these problems, usually by using linear\niterative numerical methods.\n  In this work, we extend the settings of the above approaches, by adding\nanother dimension to the problem. Specifically, we are interested in {\\em\nself-stabilizing} algorithms, that continuously run and converge to a solution\nfrom any initial state. This aspect of the problem is highly important due to\nthe dynamic nature of the network and the frequent changes in the measured\nenvironment.\n  In this paper, we link together algorithms from two different domains. On the\none hand, we use the rich linear algebra literature of linear iterative methods\nfor solving systems of linear equations, which are naturally distributed with\nrapid convergence properties. On the other hand, we are interested in\nself-stabilizing algorithms, where the input to the computation is constantly\nchanging, and we would like the algorithms to converge from any initial state.\nWe propose a simple novel method called \\syncAlg as a self-stabilizing variant\nof the linear iterative methods. We prove that under mild conditions the\nself-stabilizing algorithm converges to a desired result. We further extend\nthese results to handle the asynchronous case.\n  As a case study, we discuss the sensor calibration problem and provide\nsimulation results to support the applicability of our approach.", 
    "link": "http://arxiv.org/pdf/0811.3176v1", 
    "arxiv-id": "0811.3176v1"
},{
    "category": "cs.DC", 
    "author": "Cyril Rabat", 
    "title": "Fully distributed and fault tolerant task management based on diffusions", 
    "publish": "2008-12-03T14:58:19Z", 
    "summary": "The task management is a critical component for the computational grids. The\naim is to assign tasks on nodes according to a global scheduling policy and a\nview of local resources of nodes. A peer-to-peer approach for the task\nmanagement involves a better scalability for the grid and a higher fault\ntolerance. But some mechanisms have to be proposed to avoid the computation of\nreplicated tasks that can reduce the efficiency and increase the load of nodes.\nIn the same way, these mechanisms have to limit the number of exchanged\nmessages to avoid the overload of the network.\n  In a previous paper, we have proposed two methods for the task management\ncalled active and passive. These methods are based on a random walk: they are\nfully distributed and fault tolerant. Each node owns a local tasks states set\nupdated thanks to a random walk and each node is in charge of the local\nassignment. Here, we propose three methods to improve the efficiency of the\nactive method. These new methods are based on a circulating word. The nodes\nlocal tasks states sets are updated thanks to periodical diffusions along trees\nbuilt from the circulating word. Particularly, we show that these methods\nincrease the efficiency of the active method: they produce less replicated\ntasks. These three methods are also fully distributed and fault tolerant. On\nthe other way, the circulating word can be exploited for other applications\nlike the resources management or the nodes synchronization.", 
    "link": "http://arxiv.org/pdf/0812.0736v1", 
    "arxiv-id": "0812.0736v1"
},{
    "category": "cs.DC", 
    "author": "M. Williams", 
    "title": "Ganga: a tool for computational-task management and easy access to Grid   resources", 
    "publish": "2009-02-16T13:31:44Z", 
    "summary": "In this paper, we present the computational task-management tool Ganga, which\nallows for the specification, submission, bookkeeping and post-processing of\ncomputational tasks on a wide set of distributed resources. Ganga has been\ndeveloped to solve a problem increasingly common in scientific projects, which\nis that researchers must regularly switch between different processing systems,\neach with its own command set, to complete their computational tasks. Ganga\nprovides a homogeneous environment for processing data on heterogeneous\nresources. We give examples from High Energy Physics, demonstrating how an\nanalysis can be developed on a local system and then transparently moved to a\nGrid system for processing of all available data. Ganga has an API that can be\nused via an interactive interface, in scripts, or through a GUI. Specific\nknowledge about types of tasks or computational resources is provided at\nrun-time through a plugin system, making new developments easy to integrate. We\ngive an overview of the Ganga architecture, give examples of current use, and\ndemonstrate how Ganga can be used in many different areas of science.", 
    "link": "http://arxiv.org/pdf/0902.2685v2", 
    "arxiv-id": "0902.2685v2"
},{
    "category": "cs.DC", 
    "author": "Jim Shank", 
    "title": "Minimal Economic Distributed Computing", 
    "publish": "2009-02-27T00:00:55Z", 
    "summary": "In an ideal distributed computing infrastructure, users would be able to use\ndiverse distributed computing resources in a simple coherent way, with\nguaranteed security and efficient use of shared resources in accordance with\nthe wishes of the owners of the resources. Our strategy for approaching this\nideal is to first find the simplest structure within which these goals can\nplausibly be achieved. This structure, we find, is given by a particular\nrecursive distributive lattice freely constructed from a presumed partially\nordered set of all data in the infrastructure. Minor syntactic adjustments to\nthe resulting algebra yields a simple language resembling a UNIX shell, a\nconcept of execution and an interprocess protocol. Persons, organizations and\nservers within the system express their interests explicitly via a hierarchical\ncurrency. The currency provides a common framework for treating authentication,\naccess control and resource sharing as economic problems while also introducing\na new dimension for improving the infrastructure over time by designing system\ncomponents which compete with each other to earn the currency. We explain these\nresults, discuss experience with an implementation called egg and point out\nareas where more research is needed.", 
    "link": "http://arxiv.org/pdf/0902.4730v1", 
    "arxiv-id": "0902.4730v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Resource Allocation for Multiple Concurrent In-Network Stream-Processing   Applications", 
    "publish": "2009-03-04T08:15:00Z", 
    "summary": "This paper investigates the operator mapping problem for in-network\nstream-processing applications. In-network stream-processing amounts to\napplying one or more trees of operators in steady-state, to multiple data\nobjects that are continuously updated at different locations in the network.\nThe goal is to compute some final data at some desired rate. Different operator\ntrees may share common subtrees. Therefore, it may be possible to reuse some\nintermediate results in different application trees. The first contribution of\nthis work is to provide complexity results for different instances of the basic\nproblem, as well as integer linear program formulations of various problem\ninstances. The second second contribution is the design of several\npolynomial-time heuristics. One of the primary objectives of these heuristics\nis to reuse intermediate results shared by multiple applications. Our\nquantitative comparisons of these heuristics in simulation demonstrates the\nimportance of choosing appropriate processors for operator mapping. It also\nallow us to identify a heuristic that achieves good results in practice.", 
    "link": "http://arxiv.org/pdf/0903.0710v1", 
    "arxiv-id": "0903.0710v1"
},{
    "category": "cs.DC", 
    "author": "Delia Sabina Stinga", 
    "title": "Grid Technologies", 
    "publish": "2009-03-04T11:08:20Z", 
    "summary": "This paper contains the most important aspects of computing grids. Grid\ncomputing allows high performance distributed systems to act as a single\ncomputer. An overview of grids structure and techniques is given in order to\nunderstand the way grids work.", 
    "link": "http://arxiv.org/pdf/0903.0730v1", 
    "arxiv-id": "0903.0730v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Multi-Objective Problem Solving With Offspring on Enterprise Clouds", 
    "publish": "2009-03-08T04:37:33Z", 
    "summary": "In this paper, we present a distributed implementation of a network based\nmulti-objective evolutionary algorithm, called EMO, by using Offspring. Network\nbased evolutionary algorithms have proven to be effective for multi-objective\nproblem solving. They feature a network of connections between individuals that\ndrives the evolution of the algorithm. Unfortunately, they require large\npopulations to be effective and a distributed implementation can leverage the\ncomputation time. Most of the existing frameworks are limited to providing\nsolutions that are basic or specific to a given algorithm. Our Offspring\nframework is a plug-in based software environment that allows rapid deployment\nand execution of evolutionary algorithms on distributed computing environments\nsuch as Enterprise Clouds. Its features and benefits are presented by\ndescribing the distributed implementation of EMO.", 
    "link": "http://arxiv.org/pdf/0903.1386v1", 
    "arxiv-id": "0903.1386v1"
},{
    "category": "cs.DC", 
    "author": "Marimuthu Palaniswami", 
    "title": "Jeeva: Enterprise Grid-enabled Web Portal for Protein Secondary   Structure Prediction", 
    "publish": "2009-03-08T04:50:53Z", 
    "summary": "This paper presents a Grid portal for protein secondary structure prediction\ndeveloped by using services of Aneka, a .NET-based enterprise Grid technology.\nThe portal is used by research scientists to discover new prediction structures\nin a parallel manner. An SVM (Support Vector Machine)-based prediction\nalgorithm is used with 64 sample protein sequences as a case study to\ndemonstrate the potential of enterprise Grids.", 
    "link": "http://arxiv.org/pdf/0903.1388v1", 
    "arxiv-id": "0903.1388v1"
},{
    "category": "cs.DC", 
    "author": "Luigi Santocanale", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3 (Extended   Version)", 
    "publish": "2009-03-20T07:54:05Z", 
    "summary": "We address the problem of finding nice labellings for event structures of\ndegree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3.", 
    "link": "http://arxiv.org/pdf/0903.3462v1", 
    "arxiv-id": "0903.3462v1"
},{
    "category": "cs.DC", 
    "author": "Weiwu Hu", 
    "title": "Global Clock, Physical Time Order and Pending Period Analysis in   Multiprocessor Systems", 
    "publish": "2009-03-29T10:34:34Z", 
    "summary": "In multiprocessor systems, various problems are treated with Lamport's\nlogical clock and the resultant logical time orders between operations.\nHowever, one often needs to face the high complexities caused by the lack of\nlogical time order information in practice. In this paper, we utilize the\n\\emph{global clock} to infuse the so-called \\emph{pending period} to each\noperation in a multiprocessor system, where the pending period is a time\ninterval that contains the performed time of the operation. Further, we define\nthe \\emph{physical time order} for any two operations with disjoint pending\nperiods. The physical time order is obeyed by any real execution in\nmultiprocessor systems due to that it is part of the truly happened operation\norders restricted by global clock, and it is then proven to be independent and\nconsistent with traditional logical time orders. The above novel yet\nfundamental concepts enables new effective approaches for analyzing\nmultiprocessor systems, which are named \\emph{pending period analysis} as a\nwhole. As a consequence of pending period analysis, many important problems of\nmultiprocessor systems can be tackled effectively. As a significant application\nexample, complete memory consistency verification, which was known as an\nNP-hard problem, can be solved with the complexity of $O(n^2)$ (where $n$ is\nthe number of operations). Moreover, the two event ordering problems, which\nwere proven to be Co-NP-Hard and NP-hard respectively, can both be solved with\nthe time complexity of O(n) if restricted by pending period information.", 
    "link": "http://arxiv.org/pdf/0903.4961v2", 
    "arxiv-id": "0903.4961v2"
},{
    "category": "cs.DC", 
    "author": "Michel Salomon", 
    "title": "Java Technology : a Strategic Solution for Interactive Distributed   Applications", 
    "publish": "2009-04-27T15:16:32Z", 
    "summary": "In a world demanding the best performance from financial investments,\ndistributed applications occupy the first place among the proposed solutions.\nThis particularity is due to their distributed architecture which is able to\nacheives high performance. Currently, many research works aim to develop tools\nthat facilitate the implementation of such applications. The urgent need for\nsuch applications in all areas pushes researchers to accelerate this process.\nHowever, the lack of standardization results in the absence of strategic\ndecisions taken by computer science community. In this article, we argue that\nJava technology represents an elegant compromise ahead of the list of the\ncurrently available solutions. In fact, by promoting the independence of\nhardware and software, Java technology makes it possible to overcome pitfalls\nthat are inherent to the creation of distributed applications.", 
    "link": "http://arxiv.org/pdf/0904.4181v1", 
    "arxiv-id": "0904.4181v1"
},{
    "category": "cs.DC", 
    "author": "Luc Boug\u00e9", 
    "title": "BlobSeer: How to Enable Efficient Versioning for Large Object Storage   under Heavy Access Concurrency", 
    "publish": "2009-05-07T19:37:48Z", 
    "summary": "To accommodate the needs of large-scale distributed P2P systems, scalable\ndata management strategies are required, allowing applications to efficiently\ncope with continuously growing, highly dis tributed data. This paper addresses\nthe problem of efficiently stor ing and accessing very large binary data\nobjects (blobs). It proposesan efficient versioning scheme allowing a large\nnumber of clients to concurrently read, write and append data to huge blobs\nthat are fragmented and distributed at a very large scale. Scalability under\nheavy concurrency is achieved thanks to an original metadata scheme, based on a\ndistributed segment tree built on top of a Distributed Hash Table (DHT). Our\napproach has been implemented and experimented within our BlobSeer prototype on\nthe Grid'5000 testbed, using up to 175 nodes.", 
    "link": "http://arxiv.org/pdf/0905.1113v1", 
    "arxiv-id": "0905.1113v1"
},{
    "category": "cs.DC", 
    "author": "Vincent Villain", 
    "title": "Une CNS pour l'acheminement de messages instantan\u00e9ment stabilisant", 
    "publish": "2009-05-12T08:28:47Z", 
    "summary": "A snap-stabilizing algorithm ensures that it always behaves according to its\nspecifications whenever it starts from an arbitrary configuration. In this\npaper, we interest in the message forwarding problem in a message-switched\nnetwork. We must manage network ressources in order to deliver messages to any\nprocessor of the network. In this goal, we need information given by a routing\nalgorithm. But, due to the context of stabilization, this information can be\ninitially corrupted. It is why the existence of snap-stabilizing algorithms for\nthis task (proved in [CDV09]) implies that we can ask the system to begin\nforwarding messages even if routing tables are initially corrupted. In this\npaper, we generalize the previous result given a necessary and sufficient\ncondition to solve the forwarding problem in a snap-stabilizing way.", 
    "link": "http://arxiv.org/pdf/0905.1786v1", 
    "arxiv-id": "0905.1786v1"
},{
    "category": "cs.DC", 
    "author": "Boaz Patt-Shamir", 
    "title": "Distributed Discovery of Large Near-Cliques", 
    "publish": "2009-05-26T09:51:35Z", 
    "summary": "Given an undirected graph and $0\\le\\epsilon\\le1$, a set of nodes is called\n$\\epsilon$-near clique if all but an $\\epsilon$ fraction of the pairs of nodes\nin the set have a link between them. In this paper we present a fast\nsynchronous network algorithm that uses small messages and finds a near-clique.\nSpecifically, we present a constant-time algorithm that finds, with constant\nprobability of success, a linear size $\\epsilon$-near clique if there exists an\n$\\epsilon^3$-near clique of linear size in the graph. The algorithm uses\nmessages of $O(\\log n)$ bits. The failure probability can be reduced to\n$n^{-\\Omega(1)}$ in $O(\\log n)$ time, and the algorithm also works if the graph\ncontains a clique of size $\\Omega(n/\\log^{\\alpha}\\log n)$ for some $\\alpha \\in\n(0,1)$.", 
    "link": "http://arxiv.org/pdf/0905.4147v1", 
    "arxiv-id": "0905.4147v1"
},{
    "category": "cs.DC", 
    "author": "L. T. Handoko", 
    "title": "Microcontroller based distributed and networked control system for   public cluster", 
    "publish": "2009-06-01T13:25:21Z", 
    "summary": "We present the architecture and application of the distributed control in\npublic cluster, a parallel machine which is open for public access. Following\nthe nature of public cluster, the integrated distributed control system is\nfully accessible through network using a user-friendly web interface. The\nsystem is intended mainly to control the power of each node in a block of\nparallel computers provided to certain users. This is especially important to\nextend the life-time of related hardwares, and to reduce the whole running and\nmaintainance costs. The system consists of two parts : the master- and\nnode-controllers, and both are connected each other through RS-485 interface.\nEach node-controller is assigned with a unique address to distinguish each of\nthem. We also discuss briefly the implementation of the system at the LIPI\nPublic Cluster.", 
    "link": "http://arxiv.org/pdf/0906.0281v1", 
    "arxiv-id": "0906.0281v1"
},{
    "category": "cs.DC", 
    "author": "Dan Meng", 
    "title": "Multidimensional Analysis of System Logs in Large-scale Cluster Systems", 
    "publish": "2009-06-07T06:03:14Z", 
    "summary": "It is effective to improve the reliability and availability of large-scale\ncluster systems through the analysis of failures. Existed failure analysis\nmethods understand and analyze failures from one or few dimension. The analysis\nresults are partial and with less precision because of the limitation of data\nsource. This paper presents multidimensional analysis based on graph mining to\nanalyze multi-source system logs, which is a promising failure analysis method\nto get more complete and precise failure knowledge.", 
    "link": "http://arxiv.org/pdf/0906.1328v1", 
    "arxiv-id": "0906.1328v1"
},{
    "category": "cs.DC", 
    "author": "Dan Meng", 
    "title": "Phoenix Cloud: Consolidating Different Computing Loads on Shared Cluster   System for Large Organization", 
    "publish": "2009-06-07T10:15:28Z", 
    "summary": "Different departments of a large organization often run dedicated cluster\nsystems for different computing loads, like HPC (high performance computing)\njobs or Web service applications. In this paper, we have designed and\nimplemented a cloud management system software Phoenix Cloud to consolidate\nheterogeneous workloads from different departments affiliated to the same\norganization on the shared cluster system. We have also proposed cooperative\nresource provisioning and management policies for a large organization and its\naffiliated departments, running HPC jobs and Web service applications, to share\nthe consolidated cluster system. The experiments show that in comparison with\nthe case that each department operates its dedicated cluster system, Phoenix\nCloud significantly decreases the scale of the required cluster system for a\nlarge organization, improves the benefit of the scientific computing\ndepartment, and at the same time provisions enough resources to the other\ndepartment running Web services with varying loads.", 
    "link": "http://arxiv.org/pdf/0906.1346v6", 
    "arxiv-id": "0906.1346v6"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Ideal Stabilization", 
    "publish": "2009-06-10T14:31:18Z", 
    "summary": "We define and explore the concept of ideal stabilization. The program is\nideally stabilizing if its every state is legitimate. Ideal stabilization\nallows the specification designer to prescribe with arbitrary degree of\nprecision not only the fault-free program behavior but also its recovery\noperation. Specifications may or may not mention all possible states. We\nidentify approaches to designing ideal stabilization to both kinds of\nspecifications. For the first kind, we state the necessary condition for an\nideally stabilizing solution. On the basis of this condition we prove that\nthere is no ideally stabilizing solution to the leader election problem. We\nillustrate the utility of the concept by providing examples of well-known\nprograms and proving them ideally stabilizing. Specifically, we prove ideal\nstabilization of the conflict manager, the alternator, the propagation of\ninformation with feedback and the alternating bit protocol.", 
    "link": "http://arxiv.org/pdf/0906.1947v1", 
    "arxiv-id": "0906.1947v1"
},{
    "category": "cs.DC", 
    "author": "A. Muraru", 
    "title": "Dependable Distributed Computing for the International Telecommunication   Union Regional Radio Conference RRC06", 
    "publish": "2009-06-11T16:04:12Z", 
    "summary": "The International Telecommunication Union (ITU) Regional Radio Conference\n(RRC06) established in 2006 a new frequency plan for the introduction of\ndigital broadcasting in European, African, Arab, CIS countries and Iran. The\npreparation of the plan involved complex calculations under short deadline and\nrequired dependable and efficient computing capability. The ITU designed and\ndeployed in-situ a dedicated PC farm, in parallel to the European Organization\nfor Nuclear Research (CERN) which provided and supported a system based on the\nEGEE Grid. The planning cycle at the RRC06 required a periodic execution in the\norder of 200,000 short jobs, using several hundreds of CPU hours, in a period\nof less than 12 hours. The nature of the problem required dynamic\nworkload-balancing and low-latency access to the computing resources. We\npresent the strategy and key technical choices that delivered a reliable\nservice to the RRC06.", 
    "link": "http://arxiv.org/pdf/0906.2143v1", 
    "arxiv-id": "0906.2143v1"
},{
    "category": "cs.DC", 
    "author": "Michal \u0160umbera", 
    "title": "Efficient Multi-site Data Movement Using Constraint Programming for Data   Hungry Science", 
    "publish": "2009-06-16T12:33:25Z", 
    "summary": "For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data\nsets that have been increasing in size as a function of time. In order to\noptimize all available resources (geographically spread) and minimize the\nprocessing time, it is necessary to face also the question of efficient data\ntransfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to\nthe truly distributed task scheduling we present the technique using a\nConstraint Programming (CP) approach. The CP technique schedules data transfers\nfrom multiple resources considering all available paths of diverse\ncharacteristic (capacity, sharing and storage) having minimum user's waiting\ntime as an objective. We introduce a model for planning data transfers to a\nsingle destination (data transfer) as well as its extension for an optimal data\nset spreading strategy (data placement). Several enhancements for a solver of\nthe CP model will be shown, leading to a faster schedule computation time using\nsymmetry breaking, branch cutting, well studied principles from job-shop\nscheduling field and several heuristics. Finally, we will present the design\nand implementation of a corner-stone application aimed at moving datasets\naccording to the schedule. Results will include comparison of performance and\ntrade-off between CP techniques and a Peer-2-Peer model from simulation\nframework as well as the real case scenario taken from a practical usage of a\nCP scheduler.", 
    "link": "http://arxiv.org/pdf/0906.2914v2", 
    "arxiv-id": "0906.2914v2"
},{
    "category": "cs.DC", 
    "author": "Kotagiri Ramamohanarao", 
    "title": "Decentralized Traffic Management Strategies for Sensor-Enabled Cars", 
    "publish": "2009-06-18T12:16:06Z", 
    "summary": "Traffic Congestions and accidents are major concerns in today's\ntransportation systems. This thesis investigates how to optimize traffic flow\non highways, in particular for merging situations such as intersections where a\nramp leads onto the highway. In our work, cars are equipped with sensors that\ncan detect distance to neighboring cars, and communicate their velocity and\nacceleration readings with one another. Sensor-enabled cars can locally\nexchange sensed information about the traffic and adapt their behavior much\nearlier than regular cars.\n  We propose proactive algorithms for merging different streams of\nsensor-enabled cars into a single stream. A proactive merging algorithm\ndecouples the decision point from the actual merging point. Sensor-enabled cars\nallow us to decide where and when a car merges before it arrives at the actual\nmerging point. This leads to a significant improvement in traffic flow as\nvelocities can be adjusted appropriately. We compare proactive merging\nalgorithms against the conventional priority-based merging algorithm in a\ncontrolled simulation environment. Experiment results show that proactive\nmerging algorithms outperform the priority-based merging algorithm in terms of\nflow and delay.", 
    "link": "http://arxiv.org/pdf/0906.3424v1", 
    "arxiv-id": "0906.3424v1"
},{
    "category": "cs.DC", 
    "author": "Carlos Molina-Jimenez", 
    "title": "A Peer to Peer Protocol for Online Dispute Resolution over Storage   Consumption", 
    "publish": "2009-06-23T16:30:51Z", 
    "summary": "In bilateral accounting of resource consumption both the consumer and\nprovider independently measure the amount of resources consumed by the\nconsumer. The problem here is that potential disparities between the provider's\nand consumer's accountings, might lead to conflicts between the two parties\nthat need to be resolved. We argue that with the proper mechanisms available,\nmost of these conflicts can be solved online, as opposite to in court\nresolution; the design of such mechanisms is still a research topic; to help\ncover the gap, in this paper we propose a peer--to--peer protocol for online\ndispute resolution over storage consumption. The protocol is peer--to--peer and\ntakes into consideration the possible causes (e.g, transmission delays,\nunsynchronized metric collectors, etc.) of the disparity between the provider's\nand consumer's accountings to make, if possible, the two results converge.", 
    "link": "http://arxiv.org/pdf/0906.4302v1", 
    "arxiv-id": "0906.4302v1"
},{
    "category": "cs.DC", 
    "author": "Olivier Passalacqua", 
    "title": "Reconfiguration of Distributed Information Fusion System ? A case study", 
    "publish": "2009-06-25T12:35:57Z", 
    "summary": "Information Fusion Systems are now widely used in different fusion contexts,\nlike scientific processing, sensor networks, video and image processing. One of\nthe current trends in this area is to cope with distributed systems. In this\ncontext, we have defined and implemented a Dynamic Distributed Information\nFusion System runtime model. It allows us to cope with dynamic execution\nsupports while trying to maintain the functionalities of a given Dynamic\nDistributed Information Fusion System. The paper presents our system, the\nreconfiguration problems we are faced with and our solutions.", 
    "link": "http://arxiv.org/pdf/0906.4680v1", 
    "arxiv-id": "0906.4680v1"
},{
    "category": "cs.DC", 
    "author": "Marc Shapiro", 
    "title": "CRDTs: Consistency without concurrency control", 
    "publish": "2009-07-06T08:01:05Z", 
    "summary": "A CRDT is a data type whose operations commute when they are concurrent.\nReplicas of a CRDT eventually converge without any complex concurrency control.\nAs an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer\ncalled Treedoc. We outline the design, implementation and performance of\nTreedoc. We discuss how the CRDT concept can be generalised, and its\nlimitations.", 
    "link": "http://arxiv.org/pdf/0907.0929v1", 
    "arxiv-id": "0907.0929v1"
},{
    "category": "cs.DC", 
    "author": "Fran\u00e7ois Pellegrini", 
    "title": "PT-Scotch: A tool for efficient parallel graph ordering", 
    "publish": "2009-07-08T15:11:00Z", 
    "summary": "The parallel ordering of large graphs is a difficult problem, because on the\none hand minimum degree algorithms do not parallelize well, and on the other\nhand the obtainment of high quality orderings with the nested dissection\nalgorithm requires efficient graph bipartitioning heuristics, the best\nsequential implementations of which are also hard to parallelize. This paper\npresents a set of algorithms, implemented in the PT-Scotch software package,\nwhich allows one to order large graphs in parallel, yielding orderings the\nquality of which is only slightly worse than the one of state-of-the-art\nsequential algorithms. Our implementation uses the classical nested dissection\napproach but relies on several novel features to solve the parallel graph\nbipartitioning problem. Thanks to these improvements, PT-Scotch produces\nconsistently better orderings than ParMeTiS on large numbers of processors.", 
    "link": "http://arxiv.org/pdf/0907.1375v1", 
    "arxiv-id": "0907.1375v1"
},{
    "category": "cs.DC", 
    "author": "John N. Tsitsiklis", 
    "title": "Distributed anonymous function computation in information fusion and   multiagent systems", 
    "publish": "2009-07-16T22:42:40Z", 
    "summary": "We propose a model for deterministic distributed function computation by a\nnetwork of identical and anonymous nodes, with bounded computation and storage\ncapabilities that do not scale with the network size. Our goal is to\ncharacterize the class of functions that can be computed within this model. In\nour main result, we exhibit a class of non-computable functions, and prove that\nevery function outside this class can at least be approximated. The problem of\ncomputing averages in a distributed manner plays a central role in our\ndevelopment.", 
    "link": "http://arxiv.org/pdf/0907.2949v2", 
    "arxiv-id": "0907.2949v2"
},{
    "category": "cs.DC", 
    "author": "Xavier Koegler", 
    "title": "On the Convergence of Population Protocols When Population Goes to   Infinity", 
    "publish": "2009-07-17T17:22:12Z", 
    "summary": "Population protocols have been introduced as a model of sensor networks\nconsisting of very limited mobile agents with no control over their own\nmovement. A population protocol corresponds to a collection of anonymous\nagents, modeled by finite automata, that interact with one another to carry out\ncomputations, by updating their states, using some rules. Their computational\npower has been investigated under several hypotheses but always when restricted\nto finite size populations. In particular, predicates stably computable in the\noriginal model have been characterized as those definable in Presburger\narithmetic. We study mathematically the convergence of population protocols\nwhen the size of the population goes to infinity. We do so by giving general\nresults, that we illustrate through the example of a particular population\nprotocol for which we even obtain an asymptotic development. This example shows\nin particular that these protocols seem to have a rather different\ncomputational power when a huge population hypothesis is considered.", 
    "link": "http://arxiv.org/pdf/0907.3118v1", 
    "arxiv-id": "0907.3118v1"
},{
    "category": "cs.DC", 
    "author": "Joe Mambratti", 
    "title": "The Open Cloud Testbed: A Wide Area Testbed for Cloud Computing   Utilizing High Performance Network Services", 
    "publish": "2009-07-28T00:54:23Z", 
    "summary": "Recently, a number of cloud platforms and services have been developed for\ndata intensive computing, including Hadoop, Sector, CloudStore (formerly KFS),\nHBase, and Thrift. In order to benchmark the performance of these systems, to\ninvestigate their interoperability, and to experiment with new services based\non flexible compute node and network provisioning capabilities, we have\ndesigned and implemented a large scale testbed called the Open Cloud Testbed\n(OCT). Currently the OCT has 120 nodes in four data centers: Baltimore, Chicago\n(two locations), and San Diego. In contrast to other cloud testbeds, which are\nin small geographic areas and which are based on commodity Internet services,\nthe OCT is a wide area testbed and the four data centers are connected with a\nhigh performance 10Gb/s network, based on a foundation of dedicated lightpaths.\nThis testbed can address the requirements of extremely large data streams that\nchallenge other types of distributed infrastructure. We have also developed\nseveral utilities to support the development of cloud computing systems and\nservices, including novel node and network provisioning services, a monitoring\nsystem, and a RPC system. In this paper, we describe the OCT architecture and\nmonitoring system. We also describe some benchmarks that we developed and some\ninteroperability studies we performed using these benchmarks.", 
    "link": "http://arxiv.org/pdf/0907.4810v1", 
    "arxiv-id": "0907.4810v1"
},{
    "category": "cs.DC", 
    "author": "Danny Dolev", 
    "title": "Self-stabilizing Byzantine Agreement", 
    "publish": "2009-08-02T21:09:20Z", 
    "summary": "Byzantine agreement algorithms typically assume implicit initial state\nconsistency and synchronization among the correct nodes and then operate in\ncoordinated rounds of information exchange to reach agreement based on the\ninput values. The implicit initial assumptions enable correct nodes to infer\nabout the progression of the algorithm at other nodes from their local state.\nThis paper considers a more severe fault model than permanent Byzantine\nfailures, one in which the system can in addition be subject to severe\ntransient failures that can temporarily throw the system out of its assumption\nboundaries. When the system eventually returns to behave according to the\npresumed assumptions it may be in an arbitrary state in which any\nsynchronization among the nodes might be lost, and each node may be at an\narbitrary state. We present a self-stabilizing Byzantine agreement algorithm\nthat reaches agreement among the correct nodes in an optimal ration of faulty\nto correct, by using only the assumption of eventually bounded message\ntransmission delay. In the process of solving the problem, two additional\nimportant and challenging building blocks were developed: a unique\nself-stabilizing protocol for assigning consistent relative times to protocol\ninitialization and a Reliable Broadcast primitive that progresses at the speed\nof actual message delivery time.", 
    "link": "http://arxiv.org/pdf/0908.0160v1", 
    "arxiv-id": "0908.0160v1"
},{
    "category": "cs.DC", 
    "author": "Ted Herman", 
    "title": "Separation of Circulating Tokens", 
    "publish": "2009-08-12T20:52:52Z", 
    "summary": "Self-stabilizing distributed control is often modeled by token abstractions.\nA system with a single token may implement mutual exclusion; a system with\nmultiple tokens may ensure that immediate neighbors do not simultaneously enjoy\na privilege. For a cyber-physical system, tokens may represent physical objects\nwhose movement is controlled. The problem studied in this paper is to ensure\nthat a synchronous system with m circulating tokens has at least d distance\nbetween tokens. This problem is first considered in a ring where d is given\nwhilst m and the ring size n are unknown. The protocol solving this problem can\nbe uniform, with all processes running the same program, or it can be\nnon-uniform, with some processes acting only as token relays. The protocol for\nthis first problem is simple, and can be expressed with Petri net formalism. A\nsecond problem is to maximize d when m is given, and n is unknown. For the\nsecond problem, the paper presents a non-uniform protocol with a single\ncorrective process.", 
    "link": "http://arxiv.org/pdf/0908.1797v2", 
    "arxiv-id": "0908.1797v2"
},{
    "category": "cs.DC", 
    "author": "Vladimir Getov", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids: Design and Experience", 
    "publish": "2009-08-16T08:30:48Z", 
    "summary": "In this paper we analyse the requirements of performing parallel\ntransaction-oriented simulations within loosely coupled systems like ad hoc\ngrids. We focus especially on the space-parallel approach to parallel\nsimulation and on discrete event synchronisation algorithms that are suitable\nfor transaction-oriented simulation and the target environment of ad hoc grids.\nTo demonstrate our findings, a Java-based parallel simulator for the\ntransaction-oriented language GPSS/H is implemented on the basis of the most\npromising shock-resistant Time Warp (SRTW) synchronisation algorithm and using\nthe grid framework ProActive. The analysis of our parallel simulator, based on\nexperiments using the Grid5000 platform, shows that the SRTW algorithm can\nsuccessfully reduce the number of rolled back transaction moves but it also\nreveals circumstances in which the SRTW algorithm can be outperformed by the\nnormal Time Warp algorithm. Finally, possible improvements to the SRTW\nalgorithm are proposed in order to avoid such problems.", 
    "link": "http://arxiv.org/pdf/0908.2222v1", 
    "arxiv-id": "0908.2222v1"
},{
    "category": "cs.DC", 
    "author": "Yoram Moses", 
    "title": "An Optimal Self-Stabilizing Firing Squad", 
    "publish": "2009-08-17T07:39:06Z", 
    "summary": "Consider a fully connected network where up to $t$ processes may crash, and\nall processes start in an arbitrary memory state. The self-stabilizing firing\nsquad problem consists of eventually guaranteeing simultaneous response to an\nexternal input. This is modeled by requiring that the non-crashed processes\n\"fire\" simultaneously if some correct process received an external \"GO\" input,\nand that they only fire as a response to some process receiving such an input.\nThis paper presents FireAlg, the first self-stabilizing firing squad algorithm.\n  The FireAlg algorithm is optimal in two respects: (a) Once the algorithm is\nin a safe state, it fires in response to a GO input as fast as any other\nalgorithm does, and (b) Starting from an arbitrary state, it converges to a\nsafe state as fast as any other algorithm does.", 
    "link": "http://arxiv.org/pdf/0908.2295v1", 
    "arxiv-id": "0908.2295v1"
},{
    "category": "cs.DC", 
    "author": "Que Thu Dung Nguyen", 
    "title": "Design and Implementation of a Distributed Middleware for Parallel   Execution of Legacy Enterprise Applications", 
    "publish": "2009-08-20T16:26:21Z", 
    "summary": "A typical enterprise uses a local area network of computers to perform its\nbusiness. During the off-working hours, the computational capacities of these\nnetworked computers are underused or unused. In order to utilize this\ncomputational capacity an application has to be recoded to exploit concurrency\ninherent in a computation which is clearly not possible for legacy applications\nwithout any source code. This thesis presents the design an implementation of a\ndistributed middleware which can automatically execute a legacy application on\nmultiple networked computers by parallelizing it. This middleware runs multiple\ncopies of the binary executable code in parallel on different hosts in the\nnetwork. It wraps up the binary executable code of the legacy application in\norder to capture the kernel level data access system calls and perform them\ndistributively over multiple computers in a safe and conflict free manner. The\nmiddleware also incorporates a dynamic scheduling technique to execute the\ntarget application in minimum time by scavenging the available CPU cycles of\nthe hosts in the network. This dynamic scheduling also supports the CPU\navailability of the hosts to change over time and properly reschedule the\nreplicas performing the computation to minimize the execution time. A prototype\nimplementation of this middleware has been developed as a proof of concept of\nthe design. This implementation has been evaluated with a few typical case\nstudies and the test results confirm that the middleware works as expected.", 
    "link": "http://arxiv.org/pdf/0908.2958v1", 
    "arxiv-id": "0908.2958v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Energy-Efficient Scheduling of HPC Applications in Cloud Computing   Environments", 
    "publish": "2009-09-07T06:13:40Z", 
    "summary": "The use of High Performance Computing (HPC) in commercial and consumer IT\napplications is becoming popular. They need the ability to gain rapid and\nscalable access to high-end computing capabilities. Cloud computing promises to\ndeliver such a computing infrastructure using data centers so that HPC users\ncan access applications and data from a Cloud anywhere in the world on demand\nand pay based on what they use. However, the growing demand drastically\nincreases the energy consumption of data centers, which has become a critical\nissue. High energy consumption not only translates to high energy cost, which\nwill reduce the profit margin of Cloud providers, but also high carbon\nemissions which is not environmentally sustainable. Hence, energy-efficient\nsolutions are required that can address the high increase in the energy\nconsumption from the perspective of not only Cloud provider but also from the\nenvironment. To address this issue we propose near-optimal scheduling policies\nthat exploits heterogeneity across multiple data centers for a Cloud provider.\nWe consider a number of energy efficiency factors such as energy cost, carbon\nemission rate, workload, and CPU power efficiency which changes across\ndifferent data center depending on their location, architectural design, and\nmanagement system. Our carbon/energy based scheduling policies are able to\nachieve on average up to 30% of energy savings in comparison to profit based\nscheduling policies leading to higher profit and less carbon emissions.", 
    "link": "http://arxiv.org/pdf/0909.1146v1", 
    "arxiv-id": "0909.1146v1"
},{
    "category": "cs.DC", 
    "author": "Peter Kilpatrick", 
    "title": "Autonomic management of multiple non-functional concerns in behavioural   skeletons", 
    "publish": "2009-09-08T17:02:58Z", 
    "summary": "We introduce and address the problem of concurrent autonomic management of\ndifferent non-functional concerns in parallel applications build as a\nhierarchical composition of behavioural skeletons. We first define the problems\narising when multiple concerns are dealt with by independent managers, then we\npropose a methodology supporting coordinated management, and finally we discuss\nhow autonomic management of multiple concerns may be implemented in a typical\nuse case. The paper concludes with an outline of the challenges involved in\nrealizing the proposed methodology on distributed target architectures such as\nclusters and grids. Being based on the behavioural skeleton concept proposed in\nthe CoreGRID GCM, it is anticipated that the methodology will be readily\nintegrated into the current reference implementation of GCM based on Java\nProActive and running on top of major grid middleware systems.", 
    "link": "http://arxiv.org/pdf/0909.1517v1", 
    "arxiv-id": "0909.1517v1"
},{
    "category": "cs.DC", 
    "author": "David Campbell", 
    "title": "Building on Quicksand", 
    "publish": "2009-09-09T18:10:57Z", 
    "summary": "Reliable systems have always been built out of unreliable components. Early\non, the reliable components were small such as mirrored disks or ECC (Error\nCorrecting Codes) in core memory. These systems were designed such that\nfailures of these small components were transparent to the application. Later,\nthe size of the unreliable components grew larger and semantic challenges crept\ninto the application when failures occurred.\n  As the granularity of the unreliable component grows, the latency to\ncommunicate with a backup becomes unpalatable. This leads to a more relaxed\nmodel for fault tolerance. The primary system will acknowledge the work request\nand its actions without waiting to ensure that the backup is notified of the\nwork. This improves the responsiveness of the system.\n  There are two implications of asynchronous state capture: 1) Everything\npromised by the primary is probabilistic. There is always a chance that an\nuntimely failure shortly after the promise results in a backup proceeding\nwithout knowledge of the commitment. Hence, nothing is guaranteed! 2)\nApplications must ensure eventual consistency. Since work may be stuck in the\nprimary after a failure and reappear later, the processing order for work\ncannot be guaranteed.\n  Platform designers are struggling to make this easier for their applications.\nEmerging patterns of eventual consistency and probabilistic execution may soon\nyield a way for applications to express requirements for a \"looser\" form of\nconsistency while providing availability in the face of ever larger failures.\n  This paper recounts portions of the evolution of these trends, attempts to\nshow the patterns that span these changes, and talks about future directions as\nwe continue to \"build on quicksand\".", 
    "link": "http://arxiv.org/pdf/0909.1788v1", 
    "arxiv-id": "0909.1788v1"
},{
    "category": "cs.DC", 
    "author": "Antonio Ortega", 
    "title": "Transform-based Distributed Data Gathering", 
    "publish": "2009-09-28T19:57:13Z", 
    "summary": "A general class of unidirectional transforms is presented that can be\ncomputed in a distributed manner along an arbitrary routing tree. Additionally,\nwe provide a set of conditions under which these transforms are invertible.\nThese transforms can be computed as data is routed towards the collection (or\nsink) node in the tree and exploit data correlation between nodes in the tree.\nMoreover, when used in wireless sensor networks, these transforms can also\nleverage data received at nodes via broadcast wireless communications. Various\nconstructions of unidirectional transforms are also provided for use in data\ngathering in wireless sensor networks. New wavelet transforms are also proposed\nwhich provide significant improvements over existing unidirectional transforms.", 
    "link": "http://arxiv.org/pdf/0909.5177v3", 
    "arxiv-id": "0909.5177v3"
},{
    "category": "cs.DC", 
    "author": "M. Iqbal b. Saripan", 
    "title": "A New Fuzzy Approach for Dynamic Load Balancing Algorithm", 
    "publish": "2009-10-02T03:32:09Z", 
    "summary": "Load balancing is the process of improving the Performance of a parallel and\ndistributed system through is distribution of load among the processors [1-2].\nMost of the previous work in load balancing and distributed decision making in\ngeneral, do not effectively take into account the uncertainty and inconsistency\nin state information but in fuzzy logic, we have advantage of using crisps\ninputs. In this paper, we present a new approach for implementing dynamic load\nbalancing algorithm with fuzzy logic, which can face to uncertainty and\ninconsistency of previous algorithms, further more our algorithm shows better\nresponse time than round robin and randomize algorithm respectively 30.84\npercent and 45.45 percent.", 
    "link": "http://arxiv.org/pdf/0910.0317v1", 
    "arxiv-id": "0910.0317v1"
},{
    "category": "cs.DC", 
    "author": "Ahmad Khademzadeh", 
    "title": "DAMQ-Based Schemes for Efficiently Using the Buffer Spaces of a NoC   Router", 
    "publish": "2009-10-09T20:24:57Z", 
    "summary": "In this paper we present high performance dynamically allocated multi-queue\n(DAMQ) buffer schemes for fault tolerance systems on chip applications that\nrequire an interconnection network. Two or four virtual channels shared the\nsame buffer space. On the message switching layer, we make improvement to boost\nsystem performance when there are faults involved in the components\ncommunication. The proposed schemes are when a node or a physical channel is\ndeemed as faulty, the previous hop node will terminate the buffer occupancy of\nmessages destined to the failed link. The buffer usage decisions are made at\nswitching layer without interactions with higher abstract layer, thus buffer\nspace will be released to messages destined to other healthy nodes quickly.\nTherefore, the buffer space will be efficiently used in case fault occurs at\nsome nodes.", 
    "link": "http://arxiv.org/pdf/0910.1852v1", 
    "arxiv-id": "0910.1852v1"
},{
    "category": "cs.DC", 
    "author": "Christian Vecchiola", 
    "title": "Cloudbus Toolkit for Market-Oriented Cloud Computing", 
    "publish": "2009-10-11T06:26:29Z", 
    "summary": "This keynote paper: (1) presents the 21st century vision of computing and\nidentifies various IT paradigms promising to deliver computing as a utility;\n(2) defines the architecture for creating market-oriented Clouds and computing\natmosphere by leveraging technologies such as virtual machines; (3) provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; (4) presents the work carried out as part of\nour new Cloud Computing initiative, called Cloudbus: (i) Aneka, a Platform as a\nService software system containing SDK (Software Development Kit) for\nconstruction of Cloud applications and deployment on private or public Clouds,\nin addition to supporting market-oriented resource management; (ii)\ninternetworking of Clouds for dynamic creation of federated computing\nenvironments for scaling of elastic applications; (iii) creation of 3rd party\nCloud brokering services for building content delivery networks and e-Science\napplications and their deployment on capabilities of IaaS providers such as\nAmazon along with Grid mashups; (iv) CloudSim supporting modelling and\nsimulation of Clouds for performance studies; (v) Energy Efficient Resource\nAllocation Mechanisms and Techniques for creation and management of Green\nClouds; and (vi) pathways for future research.", 
    "link": "http://arxiv.org/pdf/0910.1974v1", 
    "arxiv-id": "0910.1974v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "High-Performance Cloud Computing: A View of Scientific Applications", 
    "publish": "2009-10-11T07:42:44Z", 
    "summary": "Scientific computing often requires the availability of a massive number of\ncomputers for performing large scale experiments. Traditionally, these needs\nhave been addressed by using high-performance computing solutions and installed\nfacilities such as clusters and super computers, which are difficult to setup,\nmaintain, and operate. Cloud computing provides scientists with a completely\nnew model of utilizing the computing infrastructure. Compute resources, storage\nresources, as well as applications, can be dynamically provisioned (and\nintegrated within the existing infrastructure) on a pay per use basis. These\nresources can be released when they are no more needed. Such services are often\noffered within the context of a Service Level Agreement (SLA), which ensure the\ndesired Quality of Service (QoS). Aneka, an enterprise Cloud computing\nsolution, harnesses the power of compute resources by relying on private and\npublic Clouds and delivers to users the desired QoS. Its flexible and service\nbased infrastructure supports multiple programming paradigms that make Aneka\naddress a variety of different scenarios: from finance applications to\ncomputational science. As examples of scientific computing in the Cloud, we\npresent a preliminary case study on using Aneka for the classification of gene\nexpression data and the execution of fMRI brain imaging workflow.", 
    "link": "http://arxiv.org/pdf/0910.1979v1", 
    "arxiv-id": "0910.1979v1"
},{
    "category": "cs.DC", 
    "author": "Graeme Stewart", 
    "title": "ScotGrid: Providing an Effective Distributed Tier-2 in the LHC Era", 
    "publish": "2009-10-23T13:02:19Z", 
    "summary": "ScotGrid is a distributed Tier-2 centre in the UK with sites in Durham,\nEdinburgh and Glasgow. ScotGrid has undergone a huge expansion in hardware in\nanticipation of the LHC and now provides more than 4MSI2K and 500TB to the LHC\nVOs. Scaling up to this level of provision has brought many challenges to the\nTier-2 and we show in this paper how we have adopted new methods of organising\nthe centres, from fabric management and monitoring to remote management of\nsites to management and operational procedures, to meet these challenges. We\ndescribe how we have coped with different operational models at the sites,\nwhere Glagsow and Durham sites are managed \"in house\" but resources at\nEdinburgh are managed as a central university resource. This required the\nadoption of a different fabric management model at Edinburgh and a special\nengagement with the cluster managers. Challenges arose from the different job\nmodels of local and grid submission that required special attention to resolve.\nWe show how ScotGrid has successfully provided an infrastructure for ATLAS and\nLHCb Monte Carlo production. Special attention has been paid to ensuring that\nuser analysis functions efficiently, which has required optimisation of local\nstorage and networking to cope with the demands of user analysis. Finally,\nalthough these Tier-2 resources are pledged to the whole VO, we have\nestablished close links with our local physics user communities as being the\nbest way to ensure that the Tier-2 functions effectively as a part of the LHC\ngrid computing framework..", 
    "link": "http://arxiv.org/pdf/0910.4507v1", 
    "arxiv-id": "0910.4507v1"
},{
    "category": "cs.DC", 
    "author": "Graeme Stewart", 
    "title": "Optimised access to user analysis data using the gLite DPM", 
    "publish": "2009-10-23T13:29:44Z", 
    "summary": "The ScotGrid distributed Tier-2 now provides more that 4MSI2K and 500TB for\nLHC computing, which is spread across three sites at Durham, Edinburgh and\nGlasgow. Tier-2 sites have a dual role to play in the computing models of the\nLHC VOs. Firstly, their CPU resources are used for the generation of Monte\nCarlo event data. Secondly, the end user analysis data is distributed across\nthe grid to the site's storage system and held on disk ready for processing by\nphysicists' analysis jobs. In this paper we show how we have designed the\nScotGrid storage and data management resources in order to optimise access by\nphysicists to LHC data. Within ScotGrid, all sites use the gLite DPM storage\nmanager middleware. Using the EGEE grid to submit real ATLAS analysis code to\nprocess VO data stored on the ScotGrid sites, we present an analysis of the\nperformance of the architecture at one site, and procedures that may be\nundertaken to improve such. The results will be presented from the point of\nview of the end user (in terms of number of events processed/second) and from\nthe point of view of the site, which wishes to minimise load and the impact\nthat analysis activity has on other users of the system.", 
    "link": "http://arxiv.org/pdf/0910.4510v1", 
    "arxiv-id": "0910.4510v1"
},{
    "category": "cs.DC", 
    "author": "Ilango Sriram", 
    "title": "SPECI, a simulation tool exploring cloud-scale data centres", 
    "publish": "2009-10-23T19:05:29Z", 
    "summary": "There is a rapid increase in the size of data centres (DCs) used to provide\ncloud computing services. It is commonly agreed that not all properties in the\nmiddleware that manages DCs will scale linearly with the number of components.\nFurther, \"normal failure\" complicates the assessment of the per-formance of a\nDC. However, unlike in other engineering domains, there are no well established\ntools that allow the prediction of the performance and behav-iour of future\ngenerations of DCs. SPECI, Simulation Program for Elastic Cloud\nInfrastructures, is a simulation tool which allows exploration of aspects of\nscaling as well as performance properties of future DCs.", 
    "link": "http://arxiv.org/pdf/0910.4568v1", 
    "arxiv-id": "0910.4568v1"
},{
    "category": "cs.DC", 
    "author": "Siddhartha Khaitan", 
    "title": "Domain Decomposition Based High Performance Parallel Computing", 
    "publish": "2009-11-04T18:56:03Z", 
    "summary": "The study deals with the parallelization of finite element based\nNavier-Stokes codes using domain decomposition and state-ofart sparse direct\nsolvers. There has been significant improvement in the performance of sparse\ndirect solvers. Parallel sparse direct solvers are not found to exhibit good\nscalability. Hence, the parallelization of sparse direct solvers is done using\ndomain decomposition techniques. A highly efficient sparse direct solver\nPARDISO is used in this study. The scalability of both Newton and modified\nNewton algorithms are tested.", 
    "link": "http://arxiv.org/pdf/0911.0910v1", 
    "arxiv-id": "0911.0910v1"
},{
    "category": "cs.DC", 
    "author": "Xiao Ming Zhang", 
    "title": "A Semantic Grid Oriented to E-Tourism", 
    "publish": "2009-11-20T01:51:30Z", 
    "summary": "With increasing complexity of tourism business models and tasks, there is a\nclear need of the next generation e-Tourism infrastructure to support flexible\nautomation, integration, computation, storage, and collaboration. Currently\nseveral enabling technologies such as semantic Web, Web service, agent and grid\ncomputing have been applied in the different e-Tourism applications, however\nthere is no a unified framework to be able to integrate all of them. So this\npaper presents a promising e-Tourism framework based on emerging semantic grid,\nin which a number of key design issues are discussed including architecture,\nontologies structure, semantic reconciliation, service and resource discovery,\nrole based authorization and intelligent agent. The paper finally provides the\nimplementation of the framework.", 
    "link": "http://arxiv.org/pdf/0911.3945v1", 
    "arxiv-id": "0911.3945v1"
},{
    "category": "cs.DC", 
    "author": "Gary Berosik", 
    "title": "Building and Installing a Hadoop/MapReduce Cluster from Commodity   Components", 
    "publish": "2009-11-28T23:50:28Z", 
    "summary": "This tutorial presents a recipe for the construction of a compute cluster for\nprocessing large volumes of data, using cheap, easily available personal\ncomputer hardware (Intel/AMD based PCs) and freely available open source\nsoftware (Ubuntu Linux, Apache Hadoop).", 
    "link": "http://arxiv.org/pdf/0911.5438v1", 
    "arxiv-id": "0911.5438v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Checkpointing vs. Migration for Post-Petascale Machines", 
    "publish": "2009-11-30T09:39:15Z", 
    "summary": "We craft a few scenarios for the execution of sequential and parallel jobs on\nfuture generation machines. Checkpointing or migration, which technique to\nchoose?", 
    "link": "http://arxiv.org/pdf/0911.5593v1", 
    "arxiv-id": "0911.5593v1"
},{
    "category": "cs.DC", 
    "author": "T. Thein", 
    "title": "High Availability Cluster System for Local Disaster Recovery with Markov   Modeling Approach", 
    "publish": "2009-12-09T18:50:52Z", 
    "summary": "The need for high availability (HA) and disaster recovery (DR) in IT\nenvironment is more stringent than most of the other sectors of enterprises.\nMany businesses require the availability of business-critical applications 24\nhours a day, seven days a week, and can afford no data loss in the event of a\ndisaster. It is vital that the IT infrastructure is resilient with regard to\ndisruption, even site failures, and that business operations can continue\nwithout significant impact. As a result, DR has gained great importance in IT.\nClustering of multiple industries standard servers together to allow workload\nsharing and fail-over capabilities is a low cost approach. In this paper, we\npresent the availability model through Semi-Markov Process (SMP) and also\nanalyze the difference in downtime of the SMP model and the approximate\nContinuous Time Markov Chain (CTMC) model. To acquire system availability, we\nperform numerical analysis and SHARPE tool evaluation.", 
    "link": "http://arxiv.org/pdf/0912.1835v1", 
    "arxiv-id": "0912.1835v1"
},{
    "category": "cs.DC", 
    "author": "Javier Tordable", 
    "title": "MapReduce for Integer Factorization", 
    "publish": "2010-01-04T00:15:58Z", 
    "summary": "Integer factorization is a very hard computational problem. Currently no\nefficient algorithm for integer factorization is publicly known. However, this\nis an important problem on which it relies the security of many real world\ncryptographic systems.\n  I present an implementation of a fast factorization algorithm on MapReduce.\nMapReduce is a programming model for high performance applications developed\noriginally at Google. The quadratic sieve algorithm is split into the different\nMapReduce phases and compared against a standard implementation.", 
    "link": "http://arxiv.org/pdf/1001.0421v1", 
    "arxiv-id": "1001.0421v1"
},{
    "category": "cs.DC", 
    "author": "Gerard Tel", 
    "title": "Termination Detection of Local Computations", 
    "publish": "2010-01-18T20:12:54Z", 
    "summary": "Contrary to the sequential world, the processes involved in a distributed\nsystem do not necessarily know when a computation is globally finished. This\npaper investigates the problem of the detection of the termination of local\ncomputations. We define four types of termination detection: no detection,\ndetection of the local termination, detection by a distributed observer,\ndetection of the global termination. We give a complete characterisation\n(except in the local termination detection case where a partial one is given)\nfor each of this termination detection and show that they define a strict\nhierarchy. These results emphasise the difference between computability of a\ndistributed task and termination detection. Furthermore, these\ncharacterisations encompass all standard criteria that are usually formulated :\ntopological restriction (tree, rings, or triangu- lated networks ...),\ntopological knowledge (size, diameter ...), and local knowledge to distinguish\nnodes (identities, sense of direction). These results are now presented as\ncorollaries of generalising theorems. As a very special and important case, the\ntechniques are also applied to the election problem. Though given in the model\nof local computations, these results can give qualitative insight for similar\nresults in other standard models. The necessary conditions involve graphs\ncovering and quasi-covering; the sufficient conditions (constructive local\ncomputations) are based upon an enumeration algorithm of Mazurkiewicz and a\nstable properties detection algorithm of Szymanski, Shi and Prywes.", 
    "link": "http://arxiv.org/pdf/1001.2785v2", 
    "arxiv-id": "1001.2785v2"
},{
    "category": "cs.DC", 
    "author": "Ali Khajeh-Hosseini", 
    "title": "Research Agenda in Cloud Technologies", 
    "publish": "2010-01-19T15:53:41Z", 
    "summary": "Cloud computing is the latest effort in delivering computing resources as a\nservice. It represents a shift away from computing as a product that is\npurchased, to computing as a service that is delivered to consumers over the\ninternet from large-scale data centres - or \"clouds\". Whilst cloud computing is\ngaining growing popularity in the IT industry, academia appeared to be lagging\nbehind the rapid developments in this field. This paper is the first systematic\nreview of peer-reviewed academic research published in this field, and aims to\nprovide an overview of the swiftly developing advances in the technical\nfoundations of cloud computing and their research efforts. Structured along the\ntechnical aspects on the cloud agenda, we discuss lessons from related\ntechnologies; advances in the introduction of protocols, interfaces, and\nstandards; techniques for modelling and building clouds; and new use-cases\narising through cloud computing.", 
    "link": "http://arxiv.org/pdf/1001.3259v1", 
    "arxiv-id": "1001.3259v1"
},{
    "category": "cs.DC", 
    "author": "T. R. Gopalakrsihnan Nair", 
    "title": "Severity Prediction of Drought in A Large Geographical Area Using   Distributed Wireless Sensor Networks", 
    "publish": "2010-01-21T04:47:58Z", 
    "summary": "In this paper, the severity prediction of drought through the implementation\nof modern sensor networks is discussed. We describe how to design a drought\nprediction system using wireless sensor networks. This paper will describe a\nterrestrial interconnected wireless sensor network paradigm for the prediction\nof severity of drought over a vast area of 10,000 sq km. The communication\narchitecture for sensor network is outlined and the protocols developed for\neach layer is explored. The data integration model and sensor data analysis at\nthe central computer is explained. The advantages and limitations are discussed\nalong with the use of wireless standards. They are analyzed for its relevance.\nFinally a conclusion is presented along with open research issues.", 
    "link": "http://arxiv.org/pdf/1001.3718v1", 
    "arxiv-id": "1001.3718v1"
},{
    "category": "cs.DC", 
    "author": "Federico D. Sacerdoti", 
    "title": "Performance and Fault Tolerance in the StoreTorrent Parallel Filesystem", 
    "publish": "2010-01-21T15:17:30Z", 
    "summary": "With a goal of supporting the timely and cost-effective analysis of Terabyte\ndatasets on commodity components, we present and evaluate StoreTorrent, a\nsimple distributed filesystem with integrated fault tolerance for efficient\nhandling of small data records. Our contributions include an application-OS\npipelining technique and metadata structure to increase small write and read\nperformance by a factor of 1-10, and the use of peer-to-peer communication of\nreplica-location indexes to avoid transferring data during parallel analysis\neven in a degraded state. We evaluated StoreTorrent, PVFS, and Gluster\nfilesystems using 70 storage nodes and 560 parallel clients on an 8-core/node\nEthernet cluster with directly attached SATA disks. StoreTorrent performed\nparallel small writes at an aggregate rate of 1.69 GB/s, and supported reads\nover the network at 8.47 GB/s. We ported a parallel analysis task and\ndemonstrate it achieved parallel reads at the full aggregate speed of the\nstorage node local filesystems.", 
    "link": "http://arxiv.org/pdf/1001.3824v1", 
    "arxiv-id": "1001.3824v1"
},{
    "category": "cs.DC", 
    "author": "P. Parthiban", 
    "title": "Optimization of Multiple Vehicle Routing Problems using Approximation   Algorithms", 
    "publish": "2010-01-23T19:28:13Z", 
    "summary": "This paper deals with generating of an optimized route for multiple Vehicle\nrouting Problems (mVRP). We used a methodology of clustering the given cities\ndepending upon the number of vehicles and each cluster is allotted to a\nvehicle. k- Means clustering algorithm has been used for easy clustering of the\ncities. In this way the mVRP has been converted into VRP which is simple in\ncomputation compared to mVRP. After clustering, an optimized route is generated\nfor each vehicle in its allotted cluster. Once the clustering had been done and\nafter the cities were allocated to the various vehicles, each cluster/tour was\ntaken as an individual Vehicle Routing problem and the steps of Genetic\nAlgorithm were applied to the cluster and iterated to obtain the most optimal\nvalue of the distance after convergence takes place. After the application of\nthe various heuristic techniques, it was found that the Genetic algorithm gave\na better result and a more optimal tour for mVRPs in short computational time\nthan other Algorithms due to the extensive search and constructive nature of\nthe algorithm.", 
    "link": "http://arxiv.org/pdf/1001.4197v1", 
    "arxiv-id": "1001.4197v1"
},{
    "category": "cs.DC", 
    "author": "Jara Uitto", 
    "title": "Local algorithms in (weakly) coloured graphs", 
    "publish": "2010-01-31T13:46:27Z", 
    "summary": "A local algorithm is a distributed algorithm that completes after a constant\nnumber of synchronous communication rounds. We present local approximation\nalgorithms for the minimum dominating set problem and the maximum matching\nproblem in 2-coloured and weakly 2-coloured graphs. In a weakly 2-coloured\ngraph, both problems admit a local algorithm with the approximation factor\n$(\\Delta+1)/2$, where $\\Delta$ is the maximum degree of the graph. We also give\na matching lower bound proving that there is no local algorithm with a better\napproximation factor for either of these problems. Furthermore, we show that\nthe stronger assumption of a 2-colouring does not help in the case of the\ndominating set problem, but there is a local approximation scheme for the\nmaximum matching problem in 2-coloured graphs.", 
    "link": "http://arxiv.org/pdf/1002.0125v1", 
    "arxiv-id": "1002.0125v1"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "Cloud Migration: A Case Study of Migrating an Enterprise IT System to   IaaS", 
    "publish": "2010-02-18T11:25:49Z", 
    "summary": "This case study illustrates the potential benefits and risks associated with\nthe migration of an IT system in the oil & gas industry from an in-house data\ncenter to Amazon EC2 from a broad variety of stakeholder perspectives across\nthe enterprise, thus transcending the typical, yet narrow, financial and\ntechnical analysis offered by providers. Our results show that the system\ninfrastructure in the case study would have cost 37% less over 5 years on EC2,\nand using cloud computing could have potentially eliminated 21% of the support\ncalls for this system. These findings seem significant enough to call for a\nmigration of the system to the cloud but our stakeholder impact analysis\nrevealed that there are significant risks associated with this. Whilst the\nbenefits of using the cloud are attractive, we argue that it is important that\nenterprise decision-makers consider the overall organizational implications of\nthe changes brought about with cloud computing to avoid implementing local\noptimizations at the cost of organization-wide performance.", 
    "link": "http://arxiv.org/pdf/1002.3492v1", 
    "arxiv-id": "1002.3492v1"
},{
    "category": "cs.DC", 
    "author": "Jing Li", 
    "title": "Mobile Wireless Localization through Cooperation", 
    "publish": "2010-02-18T20:38:03Z", 
    "summary": "This paper considers N mobile nodes that move together in the vicinity of\neach other, whose initial poses as well as subsequent movements must be\naccurately tracked in real time with the assist of M(>=3) reference nodes. By\nengaging the neighboring mobile nodes in a simple but effective cooperation,\nand by exploiting both the time-of-arrival (TOA) information (between mobile\nnodes and reference nodes) and the received-signal-strength (RSS) information\n(between mobile nodes), an effective new localization strategy, termed\ncooperative TOA and RSS (COTAR), is developed. An optimal maximum likelihood\ndetector is first formulated, followed by the derivation of a low-complexity\niterative approach that can practically achieve the Cramer-Rao lower bound.\nInstead of using simplified channel models as in many previous studies, a\nsophisticated and realistic channel model is used, which can effectively\naccount for the critical fact that the direct path is not necessarily the\nstrongest path. Extensive simulations are conducted in static and mobile\nsettings, and various practical issues and system parameters are evaluated. It\nis shown that COTAR significantly outperforms the existing strategies,\nachieving a localization accuracy of only a few tenths of a meter in clear\nenvironments and a couple of meters in heavily obstructed environments.", 
    "link": "http://arxiv.org/pdf/1002.3602v2", 
    "arxiv-id": "1002.3602v2"
},{
    "category": "cs.DC", 
    "author": "Jing Li", 
    "title": "Generalized Adaptive Network Coded Cooperation (GANCC): A Unified   Framework for Network Coding and Channel Coding", 
    "publish": "2010-02-18T22:28:03Z", 
    "summary": "This paper considers distributed coding for multi-source single-sink data\ncollection wireless networks. A unified framework for network coding and\nchannel coding, termed \"generalized adaptive network coded cooperation\"\n(GANCC), is proposed. Key ingredients of GANCC include: matching code graphs\nwith the dynamic network graphs on-the-fly, and integrating channel coding with\nnetwork coding through circulant low-density parity-check codes. Several code\nconstructing methods and several families of sparse-graph codes are proposed,\nand information theoretical analysis is performed. It is shown that GANCC is\nsimple to operate, adaptive in real time, distributed in nature, and capable of\nproviding remarkable coding gains even with a very limited number of\ncooperating users.", 
    "link": "http://arxiv.org/pdf/1002.3629v1", 
    "arxiv-id": "1002.3629v1"
},{
    "category": "cs.DC", 
    "author": "Riccardo Silvestri", 
    "title": "Fast Flooding over Manhattan", 
    "publish": "2010-02-19T15:02:50Z", 
    "summary": "We consider a Mobile Ad-hoc NETwork (MANET) formed by n agents that move at\nspeed V according to the Manhattan Random-Way Point model over a square region\nof side length L. The resulting stationary (agent) spatial probability\ndistribution is far to be uniform: the average density over the \"central zone\"\nis asymptotically higher than that over the \"suburb\". Agents exchange data iff\nthey are at distance at most R within each other.\n  We study the flooding time of this MANET: the number of time steps required\nto broadcast a message from one source agent to all agents of the network in\nthe stationary phase. We prove the first asymptotical upper bound on the\nflooding time. This bound holds with high probability, it is a decreasing\nfunction of R and V, and it is tight for a wide and relevant range of the\nnetwork parameters (i.e. L, R and V).\n  A consequence of our result is that flooding over the sparse and\nhighly-disconnected suburb can be as fast as flooding over the dense and\nconnected central zone. Rather surprisingly, this property holds even when R is\nexponentially below the connectivity threshold of the MANET and the speed V is\nvery low.", 
    "link": "http://arxiv.org/pdf/1002.3757v1", 
    "arxiv-id": "1002.3757v1"
},{
    "category": "cs.DC", 
    "author": "Priti Bansal", 
    "title": "A Cluster-based Approach for Outlier Detection in Dynamic Data Streams   (KORM: k-median OutlieR Miner)", 
    "publish": "2010-02-21T19:39:35Z", 
    "summary": "Outlier detection in data streams has gained wide importance presently due to\nthe increasing cases of fraud in various applications of data streams. The\ntechniques for outlier detection have been divided into either statistics\nbased, distance based, density based or deviation based. Till now, most of the\nwork in the field of fraud detection was distance based but it is incompetent\nfrom computational point of view. In this paper we introduced a new clustering\nbased approach, which divides the stream in chunks and clusters each chunk\nusing kmedian into variable number of clusters. Instead of storing complete\ndata stream chunk in memory, we replace it with the weighted medians found\nafter mining a data stream chunk and pass that information along with the newly\narrived data chunk to the next phase. The weighted medians found in each phase\nare tested for outlierness and after a given number of phases, it is either\ndeclared as a real outlier or an inlier. Our technique is theoretically better\nthan the k-means as it does not fix the number of clusters to k rather gives a\nrange to it and provides a more stable and better solution which runs in\npoly-logarithmic space.", 
    "link": "http://arxiv.org/pdf/1002.4003v1", 
    "arxiv-id": "1002.4003v1"
},{
    "category": "cs.DC", 
    "author": "Jared Saia", 
    "title": "Breaking the O(n^2) Bit Barrier: Scalable Byzantine agreement with an   Adaptive Adversary", 
    "publish": "2010-02-24T15:19:55Z", 
    "summary": "We describe an algorithm for Byzantine agreement that is scalable in the\nsense that each processor sends only $\\tilde{O}(\\sqrt{n})$ bits, where $n$ is\nthe total number of processors. Our algorithm succeeds with high probability\nagainst an \\emph{adaptive adversary}, which can take over processors at any\ntime during the protocol, up to the point of taking over arbitrarily close to a\n1/3 fraction. We assume synchronous communication but a \\emph{rushing}\nadversary. Moreover, our algorithm works in the presence of flooding:\nprocessors controlled by the adversary can send out any number of messages. We\nassume the existence of private channels between all pairs of processors but\nmake no other cryptographic assumptions. Finally, our algorithm has latency\nthat is polylogarithmic in $n$. To the best of our knowledge, ours is the first\nalgorithm to solve Byzantine agreement against an adaptive adversary, while\nrequiring $o(n^{2})$ total bits of communication.", 
    "link": "http://arxiv.org/pdf/1002.4561v1", 
    "arxiv-id": "1002.4561v1"
},{
    "category": "cs.DC", 
    "author": "Alvaro Fernandes", 
    "title": "An Approach to Ad hoc Cloud Computing", 
    "publish": "2010-02-25T10:19:37Z", 
    "summary": "We consider how underused computing resources within an enterprise may be\nharnessed to improve utilization and create an elastic computing\ninfrastructure. Most current cloud provision involves a data center model, in\nwhich clusters of machines are dedicated to running cloud infrastructure\nsoftware. We propose an additional model, the ad hoc cloud, in which\ninfrastructure software is distributed over resources harvested from machines\nalready in existence within an enterprise. In contrast to the data center cloud\nmodel, resource levels are not established a priori, nor are resources\ndedicated exclusively to the cloud while in use. A participating machine is not\ndedicated to the cloud, but has some other primary purpose such as running\ninteractive processes for a particular user. We outline the major\nimplementation challenges and one approach to tackling them.", 
    "link": "http://arxiv.org/pdf/1002.4738v1", 
    "arxiv-id": "1002.4738v1"
},{
    "category": "cs.DC", 
    "author": "Wei Zhou", 
    "title": "LogMaster: Mining Event Correlations in Logs of Large scale Cluster   Systems", 
    "publish": "2010-03-04T02:47:07Z", 
    "summary": "This paper presents a methodology and a system, named LogMaster, for mining\ncorrelations of events that have multiple attributions, i.e., node ID,\napplication ID, event type, and event severity, in logs of large-scale cluster\nsystems. Different from traditional transactional data, e.g., supermarket\npurchases, system logs have their unique characteristic, and hence we propose\nseveral innovative approaches to mine their correlations. We present a simple\nmetrics to measure correlations of events that may happen interleavedly. On the\nbasis of the measurement of correlations, we propose two approaches to mine\nevent correlations; meanwhile, we propose an innovative abstraction: event\ncorrelation graphs (ECGs) to represent event correlations, and present an ECGs\nbased algorithm for predicting events. For two system logs of a production\nHadoop-based cloud computing system at Research Institution of China Mobile and\na production HPC cluster system at Los Alamos National Lab (LANL), we evaluate\nour approaches in three scenarios: (a) predicting all events on the basis of\nboth failure and non-failure events; (b) predicting only failure events on the\nbasis of both failure and non-failure events; (c) predicting failure events\nafter removing non-failure events.", 
    "link": "http://arxiv.org/pdf/1003.0951v2", 
    "arxiv-id": "1003.0951v2"
},{
    "category": "cs.DC", 
    "author": "Fernando L. B. Ribeiro", 
    "title": "Parallel structurally-symmetric sparse matrix-vector products on   multi-core processors", 
    "publish": "2010-03-04T03:25:41Z", 
    "summary": "We consider the problem of developing an efficient multi-threaded\nimplementation of the matrix-vector multiplication algorithm for sparse\nmatrices with structural symmetry. Matrices are stored using the compressed\nsparse row-column format (CSRC), designed for profiting from the symmetric\nnon-zero pattern observed in global finite element matrices. Unlike classical\ncompressed storage formats, performing the sparse matrix-vector product using\nthe CSRC requires thread-safe access to the destination vector. To avoid race\nconditions, we have implemented two partitioning strategies. In the first one,\neach thread allocates an array for storing its contributions, which are later\ncombined in an accumulation step. We analyze how to perform this accumulation\nin four different ways. The second strategy employs a coloring algorithm for\ngrouping rows that can be concurrently processed by threads. Our results\nindicate that, although incurring an increase in the working set size, the\nformer approach leads to the best performance improvements for most matrices.", 
    "link": "http://arxiv.org/pdf/1003.0952v3", 
    "arxiv-id": "1003.0952v3"
},{
    "category": "cs.DC", 
    "author": "Mikel Larrea", 
    "title": "Algorithms For Extracting Timeliness Graphs", 
    "publish": "2010-03-04T14:47:17Z", 
    "summary": "We consider asynchronous message-passing systems in which some links are\ntimely and processes may crash. Each run defines a timeliness graph among\ncorrect processes: (p; q) is an edge of the timeliness graph if the link from p\nto q is timely (that is, there is bound on communication delays from p to q).\nThe main goal of this paper is to approximate this timeliness graph by graphs\nhaving some properties (such as being trees, rings, ...). Given a family S of\ngraphs, for runs such that the timeliness graph contains at least one graph in\nS then using an extraction algorithm, each correct process has to converge to\nthe same graph in S that is, in a precise sense, an approximation of the\ntimeliness graph of the run. For example, if the timeliness graph contains a\nring, then using an extraction algorithm, all correct processes eventually\nconverge to the same ring and in this ring all nodes will be correct processes\nand all links will be timely. We first present a general extraction algorithm\nand then a more specific extraction algorithm that is communication efficient\n(i.e., eventually all the messages of the extraction algorithm use only links\nof the extracted graph).", 
    "link": "http://arxiv.org/pdf/1003.1058v2", 
    "arxiv-id": "1003.1058v2"
},{
    "category": "cs.DC", 
    "author": "Ignacio M. Llorente", 
    "title": "The Grid[Way] Job Template Manager, a tool for parameter sweeping", 
    "publish": "2010-03-05T15:34:09Z", 
    "summary": "Parameter sweeping is a widely used algorithmic technique in computational\nscience. It is specially suited for high-throughput computing since the jobs\nevaluating the parameter space are loosely coupled or independent.\n  A tool that integrates the modeling of a parameter study with the control of\njobs in a distributed architecture is presented. The main task is to facilitate\nthe creation and deletion of job templates, which are the elements describing\nthe jobs to be run. Extra functionality relies upon the GridWay Metascheduler,\nacting as the middleware layer for job submission and control. It supports\ninteresting features like multi-dimensional sweeping space, wildcarding of\nparameters, functional evaluation of ranges, value-skipping and job template\nautomatic indexation.\n  The use of this tool increases the reliability of the parameter sweep study\nthanks to the systematic bookkeping of job templates and respective job\nstatuses. Furthermore, it simplifies the porting of the target application to\nthe grid reducing the required amount of time and effort.", 
    "link": "http://arxiv.org/pdf/1003.1291v1", 
    "arxiv-id": "1003.1291v1"
},{
    "category": "cs.DC", 
    "author": "Antal T\u00e1trai", 
    "title": "Start-phase control of distributed systems written in Erlang/OTP", 
    "publish": "2010-03-06T15:47:21Z", 
    "summary": "This paper presents a realization for the reliable and fast startup of\ndistributed systems written in Erlang. The traditional startup provided by the\nErlang/OTP library is sequential, parallelization usually requires unsafe and\nad-hoc solutions. The proposed method calls only for slight modifications in\nthe Erlang/OTP stdlib by applying a system dependency graph. It makes the\nstartup safe, quick, and it is equally easy to use in newly developed and\nlegacy systems.", 
    "link": "http://arxiv.org/pdf/1003.1395v2", 
    "arxiv-id": "1003.1395v2"
},{
    "category": "cs.DC", 
    "author": "Branislav Sobota", 
    "title": "Using Coloured Petri Nets for design of parallel raytracing environment", 
    "publish": "2010-03-06T16:24:29Z", 
    "summary": "This paper deals with the parallel raytracing part of virtual-reality system\nPROLAND, developed at the home institution of authors. It describes an actual\nimplementation of the raytracing part and introduces a Coloured Petri Nets\nmodel of the implementation. The model is used for an evaluation of the\nimplementation by means of simulation-based performance analysis and also forms\nthe basis for future improvements of its parallelization strategy.", 
    "link": "http://arxiv.org/pdf/1003.1397v1", 
    "arxiv-id": "1003.1397v1"
},{
    "category": "cs.DC", 
    "author": "Michael Elkin", 
    "title": "Deterministic Distributed Vertex Coloring in Polylogarithmic Time", 
    "publish": "2010-03-08T12:00:09Z", 
    "summary": "Consider an n-vertex graph G = (V,E) of maximum degree Delta, and suppose\nthat each vertex v \\in V hosts a processor. The processors are allowed to\ncommunicate only with their neighbors in G. The communication is synchronous,\ni.e., it proceeds in discrete rounds. In the distributed vertex coloring\nproblem the objective is to color G with Delta + 1, or slightly more than Delta\n+ 1, colors using as few rounds of communication as possible. (The number of\nrounds of communication will be henceforth referred to as running time.)\nEfficient randomized algorithms for this problem are known for more than twenty\nyears \\cite{L86, ABI86}. Specifically, these algorithms produce a (Delta +\n1)-coloring within O(log n) time, with high probability. On the other hand, the\nbest known deterministic algorithm that requires polylogarithmic time employs\nO(Delta^2) colors. This algorithm was devised in a seminal FOCS'87 paper by\nLinial \\cite{L87}. Its running time is O(log^* n). In the same paper Linial\nasked whether one can color with significantly less than Delta^2 colors in\ndeterministic polylogarithmic time. By now this question of Linial became one\nof the most central long-standing open questions in this area. In this paper we\nanswer this question in the affirmative, and devise a deterministic algorithm\nthat employs \\Delta^{1 +o(1)} colors, and runs in polylogarithmic time.\nSpecifically, the running time of our algorithm is O(f(Delta) log Delta log n),\nfor an arbitrarily slow-growing function f(Delta) = \\omega(1). We can also\nproduce O(Delta^{1 + \\eta})-coloring in O(log Delta log n)-time, for an\narbitrarily small constant \\eta > 0, and O(Delta)-coloring in\nO(Delta^{\\epsilon} log n) time, for an arbitrarily small constant \\epsilon > 0.", 
    "link": "http://arxiv.org/pdf/1003.1608v1", 
    "arxiv-id": "1003.1608v1"
},{
    "category": "cs.DC", 
    "author": "Tunisia)", 
    "title": "Towards trusted volunteer grid environments", 
    "publish": "2010-03-17T06:28:40Z", 
    "summary": "Intensive experiences show and confirm that grid environments can be\nconsidered as the most promising way to solve several kinds of problems\nrelating either to cooperative work especially where involved collaborators are\ndispersed geographically or to some very greedy applications which require\nenough power of computing or/and storage. Such environments can be classified\ninto two categories; first, dedicated grids where the federated computers are\nsolely devoted to a specific work through its end. Second, Volunteer grids\nwhere federated computers are not completely devoted to a specific work but\ninstead they can be randomly and intermittently used, at the same time, for any\nother purpose or they can be connected or disconnected at will by their owners\nwithout any prior notification. Each category of grids includes surely several\nadvantages and disadvantages; nevertheless, we think that volunteer grids are\nvery promising and more convenient especially to build a general multipurpose\ndistributed scalable environment. Unfortunately, the big challenge of such\nenvironments is, however, security and trust. Indeed, owing to the fact that\nevery federated computer in such an environment can randomly be used at the\nsame time by several users or can be disconnected suddenly, several security\nproblems will automatically arise. In this paper, we propose a novel solution\nbased on identity federation, agent technology and the dynamic enforcement of\naccess control policies that lead to the design and implementation of trusted\nvolunteer grid environments.", 
    "link": "http://arxiv.org/pdf/1003.3305v1", 
    "arxiv-id": "1003.3305v1"
},{
    "category": "cs.DC", 
    "author": "Keith Henderson", 
    "title": "Parallel Generation of Massive Scale-Free Graphs", 
    "publish": "2010-03-18T22:11:14Z", 
    "summary": "One of the biggest huddles faced by researchers studying algorithms for\nmassive graphs is the lack of large input graphs that are essential for the\ndevelopment and test of the graph algorithms. This paper proposes two efficient\nand highly scalable parallel graph generation algorithms that can produce\nmassive realistic graphs to address this issue. The algorithms, designed to\nachieve high degree of parallelism by minimizing inter-processor\ncommunications, are two of the fastest graph generators which are capable of\ngenerating scale-free graphs with billions of vertices and edges. The synthetic\ngraphs generated by the proposed methods possess the most common properties of\nreal complex networks such as power-law degree distribution, small-worldness,\nand communities-within-communities. Scalability was tested on a large cluster\nat Lawrence Livermore National Laboratory. In the experiment, we were able to\ngenerate a graph with 1 billion vertices and 5 billion edges in less than 13\nseconds. To the best of our knowledge, this is the largest synthetic scale-free\ngraph reported in the literature.", 
    "link": "http://arxiv.org/pdf/1003.3684v1", 
    "arxiv-id": "1003.3684v1"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "The Cloud Adoption Toolkit: Addressing the Challenges of Cloud Adoption   in Enterprise", 
    "publish": "2010-03-19T19:40:41Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper: i) describes the challenges that\ndecision makers face when attempting to determine the feasibility of the\nadoption of cloud computing in their organisations; ii) illustrates a lack of\nexisting work to address the feasibility challenges of cloud adoption in the\nenterprise; iii) introduces the Cloud Adoption Toolkit that provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. The paper adopts a position paper methodology such that case\nstudy evidence is provided, where available, to support claims. We conclude\nthat the Cloud Adoption Toolkit, whilst still under development, shows signs\nthat it is a useful tool for decision makers as it helps address the\nfeasibility challenges of cloud adoption in the enterprise.", 
    "link": "http://arxiv.org/pdf/1003.3866v2", 
    "arxiv-id": "1003.3866v2"
},{
    "category": "cs.DC", 
    "author": "Rodrigo N. Calheiros", 
    "title": "InterCloud: Utility-Oriented Federation of Cloud Computing Environments   for Scaling of Application Services", 
    "publish": "2010-03-20T10:54:43Z", 
    "summary": "Cloud computing providers have setup several data centers at different\ngeographical locations over the Internet in order to optimally serve needs of\ntheir customers around the world. However, existing systems do not support\nmechanisms and policies for dynamically coordinating load distribution among\ndifferent Cloud-based data centers in order to determine optimal location for\nhosting application services to achieve reasonable QoS levels. Further, the\nCloud computing providers are unable to predict geographic distribution of\nusers consuming their services, hence the load coordination must happen\nautomatically, and distribution of services must change in response to changes\nin the load. To counter this problem, we advocate creation of federated Cloud\ncomputing environment (InterCloud) that facilitates just-in-time,\nopportunistic, and scalable provisioning of application services, consistently\nachieving QoS targets under variable workload, resource and network conditions.\nThe overall goal is to create a computing environment that supports dynamic\nexpansion or contraction of capabilities (VMs, services, storage, and database)\nfor handling sudden variations in service demands.\n  This paper presents vision, challenges, and architectural elements of\nInterCloud for utility-oriented federation of Cloud computing environments. The\nproposed InterCloud environment supports scaling of applications across\nmultiple vendor clouds. We have validated our approach by conducting a set of\nrigorous performance evaluation study using the CloudSim toolkit. The results\ndemonstrate that federated Cloud computing model has immense potential as it\noffers significant performance gains as regards to response time and cost\nsaving under dynamic workload scenarios.", 
    "link": "http://arxiv.org/pdf/1003.3920v1", 
    "arxiv-id": "1003.3920v1"
},{
    "category": "cs.DC", 
    "author": "Samih Mohemmed Mostafa", 
    "title": "Improving Waiting Time of Tasks Scheduled Under Preemptive Round Robin   Using Changeable Time Quantum", 
    "publish": "2010-03-28T06:34:20Z", 
    "summary": "Minimizing waiting time for tasks waiting in the queue for execution is one\nof the important scheduling cri-teria which took a wide area in scheduling\npreemptive tasks. In this paper we present Changeable Time Quan-tum (CTQ)\napproach combined with the round-robin algorithm, we try to adjust the time\nquantum according to the burst times of the tasks in the ready queue. There are\ntwo important benefits of using (CTQ) approach: minimizing the average waiting\ntime of the tasks, consequently minimizing the average turnaround time, and\nkeeping the number of context switches as low as possible, consequently\nminimizing the scheduling overhead. In this paper, we consider the scheduling\nproblem for preemptive tasks, where the time costs of these tasks are known a\npriori. Our experimental results demonstrate that CTQ can provide much lower\nscheduling overhead and better scheduling criteria.", 
    "link": "http://arxiv.org/pdf/1003.5342v1", 
    "arxiv-id": "1003.5342v1"
},{
    "category": "cs.DC", 
    "author": "Jianfeng Zhan", 
    "title": "Scalable Group Management in Large-Scale Virtualized Clusters", 
    "publish": "2010-03-30T11:22:55Z", 
    "summary": "To save cost, recently more and more users choose to provision virtual\nmachine resources in cluster systems, especially in data centres. Maintaining a\nconsistent member view is the foundation of reliable cluster managements, and\nit also raises several challenge issues for large scale cluster systems\ndeployed with virtual machines (which we call virtualized clusters). In this\npaper, we introduce our experiences in design and implementation of scalable\nmember view management on large-scale virtual clusters. Our research\ncontributions are three-fold: 1) we propose a scalable and reliable management\ninfrastructure that combines a peer-to-peer structure and a hierarchy structure\nto maintain a consistent member view in virtual clusters; 2) we present a\nlight-weighted group membership algorithm that can reach the consistent member\nview within a single round of message exchange; and 3) we design and implement\na scalable membership service that can provision virtual machines and maintain\na consistent member view in virtual clusters. Our work is verified on Dawning\n5000A, which ranked No.10 of Top 500 super computers in November, 2008.", 
    "link": "http://arxiv.org/pdf/1003.5794v3", 
    "arxiv-id": "1003.5794v3"
},{
    "category": "cs.DC", 
    "author": "Dave Cliff", 
    "title": "Effects of component-subscription network topology on large-scale data   centre performance scaling", 
    "publish": "2010-04-05T22:08:46Z", 
    "summary": "Modern large-scale date centres, such as those used for cloud computing\nservice provision, are becoming ever-larger as the operators of those data\ncentres seek to maximise the benefits from economies of scale. With these\nincreases in size comes a growth in system complexity, which is usually\nproblematic. There is an increased desire for automated \"self-star\"\nconfiguration, management, and failure-recovery of the data-centre\ninfrastructure, but many traditional techniques scale much worse than linearly\nas the number of nodes to be managed increases. As the number of nodes in a\nmedian-sized data-centre looks set to increase by two or three orders of\nmagnitude in coming decades, it seems reasonable to attempt to explore and\nunderstand the scaling properties of the data-centre middleware before such\ndata-centres are constructed. In [1] we presented SPECI, a simulator that\npredicts aspects of large-scale data-centre middleware performance,\nconcentrating on the influence of status changes such as policy updates or\nroutine node failures. [...]. In [1] we used a first-approximation assumption\nthat such subscriptions are distributed wholly at random across the data\ncentre. In this present paper, we explore the effects of introducing more\nrealistic constraints to the structure of the internal network of\nsubscriptions. We contrast the original results [...] exploring the effects of\nmaking the data-centre's subscription network have a regular lattice-like\nstructure, and also semi-random network structures resulting from parameterised\nnetwork generation functions that create \"small-world\" and \"scale-free\"\nnetworks. We show that for distributed middleware topologies, the structure and\ndistribution of tasks carried out in the data centre can significantly\ninfluence the performance overhead imposed by the middleware.", 
    "link": "http://arxiv.org/pdf/1004.0728v1", 
    "arxiv-id": "1004.0728v1"
},{
    "category": "cs.DC", 
    "author": "Yi Liang", 
    "title": "In Cloud, Can Scientific Communities Benefit from the Economies of   Scale?", 
    "publish": "2010-04-08T08:07:08Z", 
    "summary": "The basic idea behind Cloud computing is that resource providers offer\nelastic resources to end users. In this paper, we intend to answer one key\nquestion to the success of Cloud computing: in Cloud, can small or medium-scale\nscientific computing communities benefit from the economies of scale? Our\nresearch contributions are three-fold: first, we propose an enhanced scientific\npublic cloud model (ESP) that encourages small- or medium-scale organizations\nto rent elastic resources from a public cloud provider; second, on a basis of\nthe ESP model, we design and implement the DawningCloud system that can\nconsolidate heterogeneous scientific workloads on a Cloud site; third, we\npropose an innovative emulation methodology and perform a comprehensive\nevaluation. We found that for two typical workloads: high throughput computing\n(HTC) and many task computing (MTC), DawningCloud saves the resource\nconsumption maximally by 44.5% (HTC) and 72.6% (MTC) for service providers, and\nsaves the total resource consumption maximally by 47.3% for a resource provider\nwith respect to the previous two public Cloud solutions. To this end, we\nconclude that for typical workloads: HTC and MTC, DawningCloud can enable\nscientific communities to benefit from the economies of scale of public Clouds.", 
    "link": "http://arxiv.org/pdf/1004.1276v1", 
    "arxiv-id": "1004.1276v1"
},{
    "category": "cs.DC", 
    "author": "Neil Audsley", 
    "title": "Distributed Fault-Tolerant Avionic Systems - A Real-Time Perspective", 
    "publish": "2010-04-08T13:03:11Z", 
    "summary": "This paper examines the problem of introducing advanced forms of\nfault-tolerance via reconfiguration into safety-critical avionic systems. This\nis required to enable increased availability after fault occurrence in\ndistributed integrated avionic systems(compared to static federated systems).\nThe approach taken is to identify a migration path from current architectures\nto those that incorporate re-configuration to a lesser or greater degree. Other\nchallenges identified include change of the development process; incremental\nand flexible timing and safety analyses; configurable kernels applicable for\nsafety-critical systems.", 
    "link": "http://arxiv.org/pdf/1004.1324v1", 
    "arxiv-id": "1004.1324v1"
},{
    "category": "cs.DC", 
    "author": "Mrityunjay Singh", 
    "title": "Internet ware cloud computing :Challenges", 
    "publish": "2010-04-10T22:17:18Z", 
    "summary": "After decades of engineering development and infrastructural investment,\nInternet connections have become commodity product in many countries, and\nInternet scale \"cloud computing\" has started to compete with traditional\nsoftware business through its technological advantages and economy of scale.\nCloud computing is a promising enabling technology of Internet ware Cloud\nComputing is termed as the next big thing in the modern corporate world. Apart\nfrom the present day software and technologies, cloud computing will have a\ngrowing impact on enterprise IT and business activities in many large\norganizations. This paper provides an insight to cloud computing, its impacts\nand discusses various issues that business organizations face while\nimplementing cloud computing. Further, it recommends various strategies that\norganizations need to adopt while migrating to cloud computing. The purpose of\nthis paper is to develop an understanding of cloud computing in the modern\nworld and its impact on organizations and businesses. Initially the paper\nprovides a brief description of the cloud computing model introduction and its\npurposes. Further it discusses various technical and non-technical issues that\nneed to be overcome in order for the benefits of cloud computing to be realized\nin corporate businesses and organizations. It then provides various\nrecommendations and strategies that businesses need to work on before stepping\ninto new technologies.", 
    "link": "http://arxiv.org/pdf/1004.1746v1", 
    "arxiv-id": "1004.1746v1"
},{
    "category": "cs.DC", 
    "author": "L. E. Moser", 
    "title": "The Low Latency Fault Tolerance System", 
    "publish": "2010-04-12T01:25:49Z", 
    "summary": "The Low Latency Fault Tolerance (LLFT) system provides fault tolerance for\ndistributed applications, using the leader-follower replication technique. The\nLLFT system provides application-transparent replication, with strong replica\nconsistency, for applications that involve multiple interacting processes or\nthreads. The LLFT system comprises a Low Latency Messaging Protocol, a\nLeader-Determined Membership Protocol, and a Virtual Determinizer Framework.\nThe Low Latency Messaging Protocol provides reliable, totally ordered message\ndelivery by employing a direct group-to-group multicast, where the message\nordering is determined by the primary replica in the group. The\nLeader-Determined Membership Protocol provides reconfiguration and recovery\nwhen a replica becomes faulty and when a replica joins or leaves a group, where\nthe membership of the group is determined by the primary replica. The Virtual\nDeterminizer Framework captures the ordering information at the primary replica\nand enforces the same ordering at the backup replicas for major sources of\nnon-determinism, including multi-threading, time-related operations and socket\ncommunication. The LLFT system achieves low latency message delivery during\nnormal operation and low latency reconfiguration and recovery when a fault\noccurs.", 
    "link": "http://arxiv.org/pdf/1004.1864v2", 
    "arxiv-id": "1004.1864v2"
},{
    "category": "cs.DC", 
    "author": "Renato Figueiredo", 
    "title": "Addressing the P2P Bootstrap Problem for Small Networks", 
    "publish": "2010-04-14T03:14:49Z", 
    "summary": "P2P overlays provide a framework for building distributed applications\nconsisting of few to many resources with features including self-configuration,\nscalability, and resilience to node failures. Such systems have been\nsuccessfully adopted in large-scale services for content delivery networks,\nfile sharing, and data storage. In small-scale systems, they can be useful to\naddress privacy concerns and for network applications that lack dedicated\nservers. The bootstrap problem, finding an existing peer in the overlay,\nremains a challenge to enabling these services for small-scale P2P systems. In\nlarge networks, the solution to the bootstrap problem has been the use of\ndedicated services, though creating and maintaining these systems requires\nexpertise and resources, which constrain their usefulness and make them\nunappealing for small-scale systems. This paper surveys and summarizes\nrequirements that allow peers potentially constrained by network connectivity\nto bootstrap small-scale overlays through the use of existing public overlays.\nIn order to support bootstrapping, a public overlay must support the following\nrequirements: a method for reflection in order to obtain publicly reachable\naddresses, so peers behind network address translators and firewalls can\nreceive incoming connection requests; communication relaying to share public\naddresses and communicate when direct communication is not feasible; and\nrendezvous for discovering remote peers, when the overlay lacks stable\nmembership. After presenting a survey of various public overlays, we identify\ntwo overlays that match the requirements: XMPP overlays, such as Google Talk\nand Live Journal Talk, and Brunet, a structured overlay based upon Symphony. We\npresent qualitative experiences with prototypes that demonstrate the ability to\nbootstrap small-scale private structured overlays from public Brunet or XMPP\ninfrastructures.", 
    "link": "http://arxiv.org/pdf/1004.2308v1", 
    "arxiv-id": "1004.2308v1"
},{
    "category": "cs.DC", 
    "author": "Michael Weber", 
    "title": "Boosting Multi-Core Reachability Performance with Shared Hash Tables", 
    "publish": "2010-04-16T07:53:38Z", 
    "summary": "This paper focuses on data structures for multi-core reachability, which is a\nkey component in model checking algorithms and other verification methods. A\ncornerstone of an efficient solution is the storage of visited states. In\nrelated work, static partitioning of the state space was combined with\nthread-local storage and resulted in reasonable speedups, but left open whether\nimprovements are possible. In this paper, we present a scaling solution for\nshared state storage which is based on a lockless hash table implementation.\nThe solution is specifically designed for the cache architecture of modern\nCPUs. Because model checking algorithms impose loose requirements on the hash\ntable operations, their design can be streamlined substantially compared to\nrelated work on lockless hash tables. Still, an implementation of the hash\ntable presented here has dozens of sensitive performance parameters (bucket\nsize, cache line size, data layout, probing sequence, etc.). We analyzed their\nimpact and compared the resulting speedups with related tools. Our\nimplementation outperforms two state-of-the-art multi-core model checkers (SPIN\nand DiVinE) by a substantial margin, while placing fewer constraints on the\nload balancing and search algorithms.", 
    "link": "http://arxiv.org/pdf/1004.2772v2", 
    "arxiv-id": "1004.2772v2"
},{
    "category": "cs.DC", 
    "author": "Paul G. Spirakis", 
    "title": "Passively Mobile Communicating Logarithmic Space Machines", 
    "publish": "2010-04-20T10:03:01Z", 
    "summary": "We propose a new theoretical model for passively mobile Wireless Sensor\nNetworks. We call it the PALOMA model, standing for PAssively mobile\nLOgarithmic space MAchines. The main modification w.r.t. the Population\nProtocol model is that agents now, instead of being automata, are Turing\nMachines whose memory is logarithmic in the population size n. Note that the\nnew model is still easily implementable with current technology. We focus on\ncomplete communication graphs. We define the complexity class PLM, consisting\nof all symmetric predicates on input assignments that are stably computable by\nthe PALOMA model. We assume that the agents are initially identical.\nSurprisingly, it turns out that the PALOMA model can assign unique consecutive\nids to the agents and inform them of the population size! This allows us to\ngive a direct simulation of a Deterministic Turing Machine of O(nlogn) space,\nthus, establishing that any symmetric predicate in SPACE(nlogn) also belongs to\nPLM. We next prove that the PALOMA model can simulate the Community Protocol\nmodel, thus, improving the previous lower bound to all symmetric predicates in\nNSPACE(nlogn). Going one step further, we generalize the simulation of the\ndeterministic TM to prove that the PALOMA model can simulate a Nondeterministic\nTM of O(nlogn) space. Although providing the same lower bound, the important\nremark here is that the bound is now obtained in a direct manner, in the sense\nthat it does not depend on the simulation of a TM by a Pointer Machine.\nFinally, by showing that a Nondeterministic TM of O(nlogn) space decides any\nlanguage stably computable by the PALOMA model, we end up with an exact\ncharacterization for PLM: it is precisely the class of all symmetric predicates\nin NSPACE(nlogn).", 
    "link": "http://arxiv.org/pdf/1004.3395v1", 
    "arxiv-id": "1004.3395v1"
},{
    "category": "cs.DC", 
    "author": "C. Chellappan", 
    "title": "An Economic-based Resource Management and Scheduling for Grid Computing   Applications", 
    "publish": "2010-04-20T20:32:31Z", 
    "summary": "Resource management and scheduling plays a crucial role in achieving high\nutilization of resources in grid computing environments. Due to heterogeneity\nof resources, scheduling an application is significantly complicated and\nchallenging task in grid system. Most of the researches in this area are mainly\nfocused on to improve the performance of the grid system. There were some\nallocation model has been proposed based on divisible load theory with\ndifferent type of workloads and a single originating processor. In this paper\nwe introduce a new resource allocation model with multiple load originating\nprocessors as an economic model. Solutions for an optimal allocation of\nfraction of loads to nodes obtained to minimize the cost of the grid users via\nlinear programming approach. It is found that the resource allocation model can\nefficiently and effectively allocate workloads to proper resources.\nExperimental results showed that the proposed model obtained the better\nsolution in terms of cost and time.", 
    "link": "http://arxiv.org/pdf/1004.3566v1", 
    "arxiv-id": "1004.3566v1"
},{
    "category": "cs.DC", 
    "author": "Fetahi Wuhib", 
    "title": "The Accuracy of Tree-based Counting in Dynamic Networks", 
    "publish": "2010-04-26T15:51:23Z", 
    "summary": "Tree-based protocols are ubiquitous in distributed systems. They are\nflexible, they perform generally well, and, in static conditions, their\nanalysis is mostly simple. Under churn, however, node joins and failures can\nhave complex global effects on the tree overlays, making analysis surprisingly\nsubtle. To our knowledge, few prior analytic results for performance estimation\nof tree based protocols under churn are currently known. We study a simple\nBellman-Ford-like protocol which performs network size estimation over a\ntree-shaped overlay. A continuous time Markov model is constructed which allows\nkey protocol characteristics to be estimated, including the expected number of\nnodes at a given (perceived) distance to the root and, for each such node, the\nexpected (perceived) size of the subnetwork rooted at that node. We validate\nthe model by simulation, using a range of network sizes, node degrees, and\nchurn-to-protocol rates, with convincing results.", 
    "link": "http://arxiv.org/pdf/1004.4559v1", 
    "arxiv-id": "1004.4559v1"
},{
    "category": "cs.DC", 
    "author": "Petr Kuznetsov", 
    "title": "Relating L-Resilience and Wait-Freedom via Hitting Sets", 
    "publish": "2010-04-27T03:29:43Z", 
    "summary": "The condition of t-resilience stipulates that an n-process program is only\nobliged to make progress when at least n-t processes are correct. Put another\nway, the live sets, the collection of process sets such that progress is\nrequired if all the processes in one of these sets are correct, are all sets\nwith at least n-t processes.\n  We show that the ability of arbitrary collection of live sets L to solve\ndistributed tasks is tightly related to the minimum hitting set of L, a minimum\ncardinality subset of processes that has a non-empty intersection with every\nlive set. Thus, finding the computing power of L is NP-complete.\n  For the special case of colorless tasks that allow participating processes to\nadopt input or output values of each other, we use a simple simulation to show\nthat a task can be solved L-resiliently if and only if it can be solved\n(h-1)-resiliently, where h is the size of the minimum hitting set of L.\n  For general tasks, we characterize L-resilient solvability of tasks with\nrespect to a limited notion of weak solvability: in every execution where all\nprocesses in some set in L are correct, outputs must be produced for every\nprocess in some (possibly different) participating set in L. Given a task T, we\nconstruct another task T_L such that T is solvable weakly L-resiliently if and\nonly if T_L is solvable weakly wait-free.", 
    "link": "http://arxiv.org/pdf/1004.4701v3", 
    "arxiv-id": "1004.4701v3"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Construction auto-stabilisante d'arbre couvrant en d\u00e9pit d'actions   malicieuses", 
    "publish": "2010-04-29T12:02:13Z", 
    "summary": "A self-stabilizing protocol provides by definition a tolerance to transient\nfailures. Recently, a new class of self-stabilizing protocols appears. These\nprotocols provides also a tolerance to a given number of permanent failures. In\nthis article, we are interested in self-stabilizing protocols that deal with\nByzantines failures. We prove that, for some problems which not allow strict\nstabilization (see [Nesterenko,Arora,2002]), there exist solutions that\ntolerates Byzantine faults if we define a new criteria of tolerance.", 
    "link": "http://arxiv.org/pdf/1004.5256v1", 
    "arxiv-id": "1004.5256v1"
},{
    "category": "cs.DC", 
    "author": "Abdul Hanan Abdullah", 
    "title": "Improving Overhead Computation and pre-processing Time for Grid   Scheduling System", 
    "publish": "2010-05-06T08:30:16Z", 
    "summary": "Computational Grid is enormous environments with heterogeneous resources and\nstable infrastructures among other Internet-based computing systems. However,\nthe managing of resources in such systems has its special problems. Scheduler\nsystems need to get last information about participant nodes from information\ncenters for the purpose of firmly job scheduling. In this paper, we focus on\nonline updating resource information centers with processed and provided data\nbased on the assumed hierarchical model. A hybrid knowledge extraction method\nhas been used to classifying grid nodes based on prediction of jobs' features.\nAn affirmative point of this research is that scheduler systems don't waste\nextra time for getting up-to-date information of grid nodes. The experimental\nresult shows the advantages of our approach compared to other conservative\nmethods, especially due to its ability to predict the behavior of nodes based\non comprehensive data tables on each node.", 
    "link": "http://arxiv.org/pdf/1005.0925v1", 
    "arxiv-id": "1005.0925v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "The Impact of Topology on Byzantine Containment in Stabilization", 
    "publish": "2010-05-07T12:33:41Z", 
    "summary": "Self-stabilization is an versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed system that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties prove difficult: we demonstrate that it is impossible to contain the\nimpact of Byzantine nodes in a self-stabilizing context for maximum metric tree\nconstruction (strict stabilization). We propose a weaker containment scheme\ncalled topology-aware strict stabilization, and present a protocol for\ncomputing maximum metric trees that is optimal for this scheme with respect to\nimpossibility result.", 
    "link": "http://arxiv.org/pdf/1005.1195v1", 
    "arxiv-id": "1005.1195v1"
},{
    "category": "cs.DC", 
    "author": "Lakshmi Rajamani", 
    "title": "A Real Time Optimistic Strategy to achieve Concurrency Control in Mobile   Environments Using On-demand Multicasting", 
    "publish": "2010-05-11T08:17:36Z", 
    "summary": "In mobile database environments, multiple users may access similar data items\nirrespective of their physical location leading to concurrent access anomalies.\nAs disconnections and mobility are the common characteristics in mobile\nenvironment, performing concurrent access to a particular data item leads to\ninconsistency. Most of the approaches use locking mechanisms to achieve\nconcurrency control. However this leads to increase in blocking and abort rate.\nIn this paper an optimistic concurrency control strategy using on-demand\nmulticasting is proposed for mobile database environments which guarantees\nconsistency and introduces application-specific conflict detection and\nresolution strategies. The simulation results specify increase in system\nthroughput by reducing the transaction abort rates as compared to the other\noptimistic strategies proposed in literature.", 
    "link": "http://arxiv.org/pdf/1005.1747v1", 
    "arxiv-id": "1005.1747v1"
},{
    "category": "cs.DC", 
    "author": "Akshay Atrey", 
    "title": "An Efficient and Secure Routing Protocol for Mobile Ad-Hoc Networks", 
    "publish": "2010-05-11T08:30:10Z", 
    "summary": "Efficiency and simplicity of random algorithms have made them a lucrative\nalternative for solving complex problems in the domain of communication\nnetworks. This paper presents a random algorithm for handling the routing\nproblem in Mobile Ad hoc Networks [MANETS].The performance of most existing\nrouting protocols for MANETS degrades in terms of packet delay and congestion\ncaused as the number of mobile nodes increases beyond a certain level or their\nspeed passes a certain level. As the network becomes more and more dynamic,\ncongestion in network increases due to control packets generated by the routing\nprotocols in the process of route discovery and route maintenance. Most of this\ncongestion is due to flooding mechanism used in protocols like AODV and DSDV\nfor the purpose of route discovery and route maintenance or for route discovery\nas in the case of DSR protocol. This paper introduces the concept of random\nrouting algorithm that neither maintains a routing table nor floods the entire\nnetwork as done by various known protocols thereby reducing the load on network\nin terms of number of control packets in a highly dynamic scenario. This paper\ncalculates the expected run time of the designed random algorithm.", 
    "link": "http://arxiv.org/pdf/1005.1751v1", 
    "arxiv-id": "1005.1751v1"
},{
    "category": "cs.DC", 
    "author": "Vijayshree Tiwari", 
    "title": "Cloud Computing: Exploring the scope", 
    "publish": "2010-05-11T18:23:38Z", 
    "summary": "Cloud computing refers to a paradigm shift to overall IT solutions while\nraising the accessibility, scalability and effectiveness through its enabling\ntechnologies. However, migrated cloud platforms and services cost benefits as\nwell as performances are neither clear nor summarized. Globalization and the\nrecessionary economic times have not only raised the bar of a better IT\ndelivery models but also have given access to technology enabled services via\ninternet. Cloud computing has vast potential in terms of lean Retail\nmethodologies that can minimize the operational cost by using the third party\nbased IT capabilities, as a service. It will not only increase the ROI but will\nalso help in lowering the total cost of ownership. In this paper we have tried\nto compare the cloud computing cost benefits with the actual premise cost which\nan organization incurs normally. However, in spite of the cost benefits, many\nIT professional believe that the latest model i.e. \"cloud computing\" has risks\nand security concerns. This report demonstrates how to answer the following\nquestions: (1) Idea behind cloud computing. (2) Monetary cost benefits of using\ncloud with respect to traditional premise computing. (3) What are the various\nsecurity issues? We have tried to find out the cost benefit by comparing the\nMicrosoft Azure cloud cost with the prevalent premise cost.", 
    "link": "http://arxiv.org/pdf/1005.1904v3", 
    "arxiv-id": "1005.1904v3"
},{
    "category": "cs.DC", 
    "author": "Nandini Mukherjee", 
    "title": "A Multi-agent Framework for Performance Tuning in Distributed   Environment", 
    "publish": "2010-05-12T09:39:18Z", 
    "summary": "This paper presents the overall design of a multi-agent framework for tuning\nthe performance of an application executing in a distributed environment. The\nmulti-agent framework provides services like resource brokering, analyzing\nperformance monitoring data, local tuning and also rescheduling in case of any\nperformance problem on a specific resource provider. The paper also briefly\ndescribes the implementation of some part of the framework. In particular, job\nmigration on the basis of performance monitoring data is particularly\nhighlighted in this paper.", 
    "link": "http://arxiv.org/pdf/1005.2027v1", 
    "arxiv-id": "1005.2027v1"
},{
    "category": "cs.DC", 
    "author": "Nandini Mukherjee", 
    "title": "An Integrated Framework for Performance Analysis and Tuning in Grid   Environment", 
    "publish": "2010-05-12T10:13:59Z", 
    "summary": "In a heterogeneous, dynamic environment, like Grid, post-mortem analysis is\nof no use and data needs to be collected and analysed in real time. Novel\ntechniques are also required for dynamically tuning the application's\nperformance and resource brokering in order to maintain the desired QoS. The\nobjective of this paper is to propose an integrated framework for performance\nanalysis and tuning of the application, and rescheduling the application, if\nnecessary, to some other resources in order to adapt to the changing resource\nusage scenario in a dynamic environment.", 
    "link": "http://arxiv.org/pdf/1005.2037v1", 
    "arxiv-id": "1005.2037v1"
},{
    "category": "cs.DC", 
    "author": "Fabian Kuhn", 
    "title": "Deploying Wireless Networks with Beeps", 
    "publish": "2010-05-14T16:32:02Z", 
    "summary": "We present the \\emph{discrete beeping} communication model, which assumes\nnodes have minimal knowledge about their environment and severely limited\ncommunication capabilities. Specifically, nodes have no information regarding\nthe local or global structure of the network, don't have access to synchronized\nclocks and are woken up by an adversary. Moreover, instead on communicating\nthrough messages they rely solely on carrier sensing to exchange information.\nWe study the problem of \\emph{interval coloring}, a variant of vertex coloring\nspecially suited for the studied beeping model. Given a set of resources, the\ngoal of interval coloring is to assign every node a large contiguous fraction\nof the resources, such that neighboring nodes share no resources. To highlight\nthe importance of the discreteness of the model, we contrast it against a\ncontinuous variant described in [17]. We present an O(1$ time algorithm that\nterminates with probability 1 and assigns an interval of size\n$\\Omega(T/\\Delta)$ that repeats every $T$ time units to every node of the\nnetwork. This improves an $O(\\log n)$ time algorithm with the same guarantees\npresented in \\cite{infocom09}, and accentuates the unrealistic assumptions of\nthe continuous model. Under the more realistic discrete model, we present a Las\nVegas algorithm that solves $\\Omega(T/\\Delta)$-interval coloring in $O(\\log n)$\ntime with high probability and describe how to adapt the algorithm for dynamic\nnetworks where nodes may join or leave. For constant degree graphs we prove a\nlower bound of $\\Omega(\\log n)$ on the time required to solve interval coloring\nfor this model against randomized algorithms. This lower bound implies that our\nalgorithm is asymptotically optimal for constant degree graphs.", 
    "link": "http://arxiv.org/pdf/1005.2567v1", 
    "arxiv-id": "1005.2567v1"
},{
    "category": "cs.DC", 
    "author": "Ruhollah Tavakoli", 
    "title": "Improvement Cache Efficiency of Explicit Finite Element Procedure and   its Application to Parallel Casting Solidification Simulation", 
    "publish": "2010-05-18T11:17:34Z", 
    "summary": "A simple method for improving cache efficiency of serial and parallel\nexplicit finite procedure with application to casting solidification simulation\nover three-dimensional complex geometries is presented. The method is based on\ndivision of the global data to smaller blocks and treating each block\nindependently from others at each time step. A novel parallel finite element\nalgorithm for non-overlapped element-base decomposed domain is presented for\nimplementation of serial and parallel version of the presented method. Effect\nof mesh reordering on the efficiency is also investigated. A simple algorithm\nis presented for high quality decomposition of decoupled global mesh. Our\nresult shows 10-20 \\% performance improvement by mesh reordering and 1.2-2.2\nspeedup with application of the presented cache efficient algorithm (for serial\nand parallel versions). Also the presented parallel solver (without\ncache-efficient feature) shows nearly linear speedup on the traditional\nEthernet networked Linux cluster.", 
    "link": "http://arxiv.org/pdf/1005.3158v1", 
    "arxiv-id": "1005.3158v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Bounding the Impact of Unbounded Attacks in Stabilization", 
    "publish": "2010-05-19T06:34:03Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. Combining these two properties proved\ndifficult: it is impossible to contain the spatial impact of Byzantine nodes in\na self-stabilizing context for global tasks such as tree orientation and tree\nconstruction. We present and illustrate a new concept of Byzantine containment\nin stabilization. Our property, called Strong Stabilization enables to contain\nthe impact of Byzantine nodes if they actually perform too many Byzantine\nactions. We derive impossibility results for strong stabilization and present\nstrongly stabilizing protocols for tree orientation and tree construction that\nare optimal with respect to the number of Byzantine nodes that can be tolerated\nin a self-stabilizing context.", 
    "link": "http://arxiv.org/pdf/1005.3367v2", 
    "arxiv-id": "1005.3367v2"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "On Byzantine Containment Properties of the $min+1$ Protocol", 
    "publish": "2010-05-28T06:28:10Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a breadth-first spanning tree in this context. Combining these two\nproperties proves difficult: we demonstrate that it is impossible to contain\nthe impact of Byzantine nodes in a strictly or strongly stabilizing manner. We\nthen adopt the weaker scheme of topology-aware strict stabilization and we\npresent a similar weakening of strong stabilization. We prove that the\nclassical $min+1$ protocol has optimal Byzantine containment properties with\nrespect to these criteria.", 
    "link": "http://arxiv.org/pdf/1005.5223v1", 
    "arxiv-id": "1005.5223v1"
},{
    "category": "cs.DC", 
    "author": "S. C. Mehrotra", 
    "title": "Dynamic management of transactions in distributed real-time processing   system", 
    "publish": "2010-05-29T07:57:30Z", 
    "summary": "Managing the transactions in real time distributed computing system is not\neasy, as it has heterogeneously networked computers to solve a single problem.\nIf a transaction runs across some different sites, it may commit at some sites\nand may failure at another site, leading to an inconsistent transaction. The\ncomplexity is increase in real time applications by placing deadlines on the\nresponse time of the database system and transactions processing. Such a system\nneeds to process Transactions before these deadlines expired. A series of\nsimulation study have been performed to analyze the performance under different\ntransaction management under conditions such as different workloads,\ndistribution methods, execution mode-distribution and parallel etc. The\nscheduling of data accesses are done in order to meet their deadlines and to\nminimize the number of transactions that missed deadlines. A new concept is\nintroduced to manage the transactions in dynamic ways rather than setting\ncomputing parameters in static ways. With this approach, the system gives a\nsignificant improvement in performance.", 
    "link": "http://arxiv.org/pdf/1005.5435v1", 
    "arxiv-id": "1005.5435v1"
},{
    "category": "cs.DC", 
    "author": "Parveen Kumar", 
    "title": "A Low Overhead Minimum Process Global Snapshop Collection Algorithm for   Mobile Distributed System", 
    "publish": "2010-05-29T08:29:45Z", 
    "summary": "Coordinated checkpointing is an effective fault tolerant technique in\ndistributed system as it avoids the domino effect and require minimum storage\nrequirement. Most of the earlier coordinated checkpoint algorithms block their\ncomputation during checkpointing and forces minimum-process or non-blocking but\nforces all nodes to takes checkpoint even though many of them may not be\nnecessary or non-blocking minimum-process but takes useless checkpoints or\nreduced useless checkpoint but has higher synchronization message overhead or\nhas high checkpoint request propagation time. Hence in mobile distributed\nsystems there is a great need of minimizing the number of communication message\nand checkpointing overhead as it raise new issues such as mobility, low\nbandwidth of wireless channels, frequently disconnections, limited battery\npower and lack of reliable stable storage on mobile nodes. In this paper, we\npropose a minimum-process coordinated checkpointing algorithm for mobile\ndistributed system where no useless checkpoints are taken, no blocking of\nprocesses takes place and enforces a minimum-number of processes to take\ncheckpoints. Our algorithm imposes low memory and computation overheads on MH's\nand low communication overheads on wireless channels. It avoids awakening of an\nMH if it is not required to take its checkpoint and has reduced latency time as\neach process involved in a global checkpoint can forward its own decision\ndirectly to the checkpoint initiator.", 
    "link": "http://arxiv.org/pdf/1005.5440v1", 
    "arxiv-id": "1005.5440v1"
},{
    "category": "cs.DC", 
    "author": "D. Sasireka", 
    "title": "Implementation of a Cloud Data Server (CDS) for Providing Secure Service   in E-Business", 
    "publish": "2010-05-31T07:08:34Z", 
    "summary": "Cloud Data Servers is the novel approach for providing secure service to\ne-business .Millions of users are surfing the Cloud for various purposes,\ntherefore they need highly safe and persistent services. Usually hackers target\nparticular Operating Systems or a Particular Controller. Inspiteof several\nongoing researches Conventional Web Servers and its Intrusion Detection System\nmight not be able to detect such attacks. So we implement a Cloud Data Server\nwith Session Controller Architecture using Redundancy and Disconnected Data\nAccess Mechanism. In this paper, we generate the hash code using MD5 algorithm.\nWith the help of which we can circumvent even the attacks, which are undefined\nby traditional Systems .we implement Cloud Data Sever using Java and Hash Code\nbackup Management using My SQL. Here we Implement AES Algorithm for providing\nmore Security for the hash Code. The CDS using the Virtual Controller controls\nand monitors the Connections and modifications of the page so as to prevent\nmalicious users from hacking the website. In the proposed approach an activity\nanalyzer takes care of intimating the administrator about possible intrusions\nand the counter measures required to tackle them. The efficiency ratio of our\napproach is 98.21% compared with similar approaches.", 
    "link": "http://arxiv.org/pdf/1005.5606v1", 
    "arxiv-id": "1005.5606v1"
},{
    "category": "cs.DC", 
    "author": "Christian Lavault", 
    "title": "Reliable Self-Stabilizing Communication for Quasi Rendezvous", 
    "publish": "2010-05-31T09:09:25Z", 
    "summary": "The paper presents three self-stabilizing protocols for basic fair and\nreliable link communication primitives. We assume a link-register communication\nmodel under read/write atomicity, where every process can read from but cannot\nwrite into its neighbours' registers. The first primitive guarantees that any\nprocess writes a new value in its register(s) only after all its neighbours\nhave read the previous value, whatever the initial scheduling of processes'\nactions. The second primitive implements a \"weak rendezvous\" communication\nmechanism by using an alternating bit protocol: whenever a process\nconsecutively writes n values (possibly the same ones) in a register, each\nneighbour is guaranteed to read each value from the register at least once. On\nthe basis of the previous protocol, the third primitive implements a \"quasi\nrendezvous\": in words, this primitive ensures furthermore that there exists\nexactly one reading between two writing operations All protocols are\nself-stabilizing and run in asynchronous arbitrary networks. The goal of the\npaper is in handling each primitive by a separate procedure, which can be used\nas a \"black box\" in more involved self-stabilizing protocols.", 
    "link": "http://arxiv.org/pdf/1005.5630v1", 
    "arxiv-id": "1005.5630v1"
},{
    "category": "cs.DC", 
    "author": "Heinz Kredel", 
    "title": "Parallel and distributed Gr\u00f6bner bases computation in JAS", 
    "publish": "2010-07-30T20:38:38Z", 
    "summary": "This paper considers parallel Gr\\\"obner bases algorithms on distributed\nmemory parallel computers with multi-core compute nodes. We summarize three\ndifferent Gr\\\"obner bases implementations: shared memory parallel, pure\ndistributed memory parallel and distributed memory combined with shared memory\nparallelism. The last algorithm, called distributed hybrid, uses only one\ncontrol communication channel between the master node and the worker nodes and\nkeeps polynomials in shared memory on a node. The polynomials are transported\nasynchronous to the control-flow of the algorithm in a separate distributed\ndata structure. The implementation is generic and works for all implemented\n(exact) fields. We present new performance measurements and discuss the\nperformance of the algorithms.", 
    "link": "http://arxiv.org/pdf/1008.0011v1", 
    "arxiv-id": "1008.0011v1"
},{
    "category": "cs.DC", 
    "author": "Anwitaman Datta", 
    "title": "Self-repairing Homomorphic Codes for Distributed Storage Systems", 
    "publish": "2010-07-31T07:37:26Z", 
    "summary": "Erasure codes provide a storage efficient alternative to replication based\nredundancy in (networked) storage systems. They however entail high\ncommunication overhead for maintenance, when some of the encoded fragments are\nlost and need to be replenished. Such overheads arise from the fundamental need\nto recreate (or keep separately) first a copy of the whole object before any\nindividual encoded fragment can be generated and replenished. There has been\nrecently intense interest to explore alternatives, most prominent ones being\nregenerating codes (RGC) and hierarchical codes (HC). We propose as an\nalternative a new family of codes to improve the maintenance process, which we\ncall self-repairing codes (SRC), with the following salient features: (a)\nencoded fragments can be repaired directly from other subsets of encoded\nfragments without having to reconstruct first the original data, ensuring that\n(b) a fragment is repaired from a fixed number of encoded fragments, the number\ndepending only on how many encoded blocks are missing and independent of which\nspecific blocks are missing. These properties allow for not only low\ncommunication overhead to recreate a missing fragment, but also independent\nreconstruction of different missing fragments in parallel, possibly in\ndifferent parts of the network. We analyze the static resilience of SRCs with\nrespect to traditional erasure codes, and observe that SRCs incur marginally\nlarger storage overhead in order to achieve the aforementioned properties. The\nsalient SRC properties naturally translate to low communication overheads for\nreconstruction of lost fragments, and allow reconstruction with lower latency\nby facilitating repairs in parallel. These desirable properties make\nself-repairing codes a good and practical candidate for networked distributed\nstorage systems.", 
    "link": "http://arxiv.org/pdf/1008.0064v1", 
    "arxiv-id": "1008.0064v1"
},{
    "category": "cs.DC", 
    "author": "Cho-Yu Jason Chiang", 
    "title": "On Optimal Deadlock Detection Scheduling", 
    "publish": "2010-08-03T03:46:56Z", 
    "summary": "Deadlock detection scheduling is an important, yet often overlooked problem\nthat can significantly affect the overall performance of deadlock handling.\nExcessive initiation of deadlock detection increases overall message usage,\nresulting in degraded system performance in the absence of deadlocks; while\ninsufficient initiation of deadlock detection increases the deadlock\npersistence time, resulting in an increased deadlock resolution cost in the\npresence of deadlocks. The investigation of this performance tradeoff, however,\nis missing in the literature. This paper studies the impact of deadlock\ndetection scheduling on the overall performance of deadlock handling. In\nparticular, we show that there exists an optimal deadlock detection frequency\nthat yields the minimum long-run mean average cost, which is determined by the\nmessage complexities of the deadlock detection and resolution algorithms being\nused, as well as the rate of deadlock formation, denoted as $\\lambda$. For the\nbest known deadlock detection and resolution algorithms, we show that the\nasymptotically optimal frequency of deadlock detection scheduling that\nminimizes the overall message overhead is ${\\cal O}((\\lambda n)^{1/3})$, when\nthe total number $n$ of processes is sufficiently large. Furthermore, we show\nthat in general fully distributed (uncoordinated) deadlock detection scheduling\ncannot be performed as efficiently as centralized (coordinated) deadlock\ndetection scheduling.", 
    "link": "http://arxiv.org/pdf/1008.0451v1", 
    "arxiv-id": "1008.0451v1"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "The Cloud Adoption Toolkit: Supporting Cloud Adoption Decisions in the   Enterprise", 
    "publish": "2010-08-11T12:56:32Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper describes the challenges that\ndecision makers face when assessing the feasibility of the adoption of cloud\ncomputing in their organisations, and describes our Cloud Adoption Toolkit,\nwhich has been developed to support this process. The toolkit provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. Cost Modeling is the most mature tool in the toolkit, and this\npaper shows its effectiveness by demonstrating how practitioners can use it to\nexamine the costs of deploying their IT systems on the cloud. The Cost Modeling\ntool is evaluated using a case study of an organization that is considering the\nmigration of some of its IT systems to the cloud. The case study shows that\nrunning systems on the cloud using a traditional \"always on\" approach can be\nless cost effective, and the elastic nature of the cloud has to be used to\nreduce costs. Therefore, decision makers have to be able to model the\nvariations in resource usage and their systems deployment options to obtain\naccurate cost estimates.", 
    "link": "http://arxiv.org/pdf/1008.1900v1", 
    "arxiv-id": "1008.1900v1"
},{
    "category": "cs.DC", 
    "author": "Simon Portegies Zwart", 
    "title": "A Light-Weight Communication Library for Distributed Computing", 
    "publish": "2010-08-16T20:14:11Z", 
    "summary": "We present MPWide, a platform independent communication library for\nperforming message passing between computers. Our library allows coupling of\nseveral local MPI applications through a long distance network and is\nspecifically optimized for such communications. The implementation is\ndeliberately kept light-weight, platform independent and the library can be\ninstalled and used without administrative privileges. The only requirements are\na C++ compiler and at least one open port to a wide area network on each site.\nIn this paper we present the library, describe the user interface, present\nperformance tests and apply MPWide in a large scale cosmological N-body\nsimulation on a network of two computers, one in Amsterdam and the other in\nTokyo.", 
    "link": "http://arxiv.org/pdf/1008.2767v1", 
    "arxiv-id": "1008.2767v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Deterministic Consensus Algorithm with Linear Per-Bit Complexity", 
    "publish": "2010-08-26T17:29:31Z", 
    "summary": "In this report, building on the deterministic multi-valued one-to-many\nByzantine agreement (broadcast) algorithm in our recent technical report [2],\nwe introduce a deterministic multi-valued all-to-all Byzantine agreement\nalgorithm (consensus), with linear complexity per bit agreed upon. The\ndiscussion in this note is not self-contained, and relies heavily on the\nmaterial in [2] - please refer to [2] for the necessary background.", 
    "link": "http://arxiv.org/pdf/1008.4551v1", 
    "arxiv-id": "1008.4551v1"
},{
    "category": "cs.DC", 
    "author": "Costas Busch", 
    "title": "A Competitive Analysis for Balanced Transactional Memory Workloads", 
    "publish": "2010-08-31T23:54:57Z", 
    "summary": "We consider transactional memory contention management in the context of\nbalanced workloads, where if a transaction is writing, the number of write\noperations it performs is a constant fraction of its total reads and writes. We\nexplore the theoretical performance boundaries of contention management in\nbalanced workloads from the worst-case perspective by presenting and analyzing\ntwo new contention management algorithms. The first algorithm Clairvoyant is\nO(\\surd s)-competitive, where s is the number of shared resources. This\nalgorithm depends on explicitly knowing the conflict graph. The second\nalgorithm Non-Clairvoyant is O(\\surd s \\cdot log n)-competitive, with high\nprobability, which is only a O(log n) factor worse, but does not require\nknowledge of the conflict graph, where n is the number of transactions. Both of\nthese algorithms are greedy. We also prove that the performance of Clairvoyant\nis tight since there is no contention management algorithm that is better than\nO((\\surd s)^(1-\\epsilon))-competitive for any constant \\epsilon > 0, unless\nNP\\subseteq ZPP. To our knowledge, these results are significant improvements\nover the best previously known O(s) competitive ratio bound.", 
    "link": "http://arxiv.org/pdf/1009.0056v1", 
    "arxiv-id": "1009.0056v1"
},{
    "category": "cs.DC", 
    "author": "Deepak Gour", 
    "title": "Mapping Cloud Computing onto Useful e-Governance", 
    "publish": "2010-09-13T07:36:47Z", 
    "summary": "Most of the services viewed in context to grid and cloud computing are mostly\nconfined to services that are available for intellectual purposes. The grid or\ncloud computing are large scale distributed systems. The essence of large scale\ndistribution can only be realized if the services are rendered to common man.\nThe only organization which has exposure to almost every single resident is the\nrespective governments in every country. As the size of population increases so\nthe need for a larger purview arises. The problem of having a large purview can\nbe solved by means of large scale grid for online services. The government\nservices can be rendered through fully customized Service-oriented Clouds. In\nthis paper we are presenting tight similarities between generic government\nfunctioning and the service oriented grid/cloud approach. Also, we will discuss\nthe major issues in establishing services oriented grids for governmental\norganization.", 
    "link": "http://arxiv.org/pdf/1009.2314v1", 
    "arxiv-id": "1009.2314v1"
},{
    "category": "cs.DC", 
    "author": "Zeeshan Ahmed", 
    "title": "A Middleware road towards Web (Grid) Services", 
    "publish": "2010-09-21T10:29:50Z", 
    "summary": "Middleware technologies is a very big field, containing a strong already done\nresearch as well as the currently running research to confirm already done\nresearch's results and the to have some new solution by theoretical as well as\nthe experimental (practical) way. This document has been produced by Zeeshan\nAhmed (Student: Connectivity Software Technologies Blekinge Institute of\nTechnologies). This describes the research already done in the field of\nmiddleware technologies including Web Services, Grid Computing, Grid Services\nand Open Grid Service Infrastructure & Architecture. This document concludes\nwith the overview of Web (Grid) Service, Chain of Web (Grid) Services and the\nnecessary security issue.", 
    "link": "http://arxiv.org/pdf/1009.4048v1", 
    "arxiv-id": "1009.4048v1"
},{
    "category": "cs.DC", 
    "author": "Max Pagel", 
    "title": "Hallway Monitoring: Distributed Data Processing with Wireless Sensor   Networks", 
    "publish": "2010-09-24T15:42:30Z", 
    "summary": "We present a sensor network testbed that monitors a hallway. It consists of\n120 load sensors and 29 passive infrared sensors (PIRs), connected to 30\nwireless sensor nodes. There are also 29 LEDs and speakers installed, operating\nas actuators, and enabling a direct interaction between the testbed and\npassers-by. Beyond that, the network is heterogeneous, consisting of three\ndifferent circuit boards---each with its specific responsibility. The design of\nthe load sensors is of extremely low cost compared to industrial solutions and\neasily transferred to other settings. The network is used for in-network data\nprocessing algorithms, offering possibilities to develop, for instance,\ndistributed target-tracking algorithms. Special features of our installation\nare highly correlated sensor data and the availability of miscellaneous sensor\ntypes.", 
    "link": "http://arxiv.org/pdf/1009.4870v1", 
    "arxiv-id": "1009.4870v1"
},{
    "category": "cs.DC", 
    "author": "Somayeh Mohamadi", 
    "title": "The Impact of Data Replicatino on Job Scheduling Performance in   Hierarchical data Grid", 
    "publish": "2010-10-04T12:25:04Z", 
    "summary": "In data-intensive applications data transfer is a primary cause of job\nexecution delay. Data access time depends on bandwidth. The major bottleneck to\nsupporting fast data access in Grids is the high latencies of Wide Area\nNetworks and Internet. Effective scheduling can reduce the amount of data\ntransferred across the internet by dispatching a job to where the needed data\nare present. Another solution is to use a data replication mechanism. Objective\nof dynamic replica strategies is reducing file access time which leads to\nreducing job runtime. In this paper we develop a job scheduling policy and a\ndynamic data replication strategy, called HRS (Hierarchical Replication\nStrategy), to improve the data access efficiencies. We study our approach and\nevaluate it through simulation. The results show that our algorithm has\nimproved 12% over the current strategies.", 
    "link": "http://arxiv.org/pdf/1010.0562v1", 
    "arxiv-id": "1010.0562v1"
},{
    "category": "cs.DC", 
    "author": "Partha Sarathi Mandal", 
    "title": "Reconstruction of Aggregation Tree in spite of Faulty Nodes in Wireless   Sensor Networks", 
    "publish": "2010-10-05T17:54:58Z", 
    "summary": "Recent advances in wireless sensor networks (WSNs) have led to many new\npromissing applications. However data communication between nodes consumes a\nlarge portion of the total energy of WSNs. Consequently efficient data\naggregation technique can help greatly to reduce power consumption. Data\naggregation has emerged as a basic approach in WSNs in order to reduce the\nnumber of transmissions of sensor nodes over {\\it aggregation tree} and hence\nminimizing the overall power consumption in the network. If a sensor node fails\nduring data aggregation then the aggregation tree is disconnected. Hence the\nWSNs rely on in-network aggregation for efficiency but a single faulty node can\nseverely influence the outcome by contributing an arbitrary partial aggregate\nvalue.\n  In this paper we have presented a distributed algorithm that reconstruct the\naggregation tree from the initial aggregation tree excluding the faulty sensor\nnode. This is a synchronous model that is completed in several rounds. Our\nproposed scheme can handle multiple number of faulty nodes as well.", 
    "link": "http://arxiv.org/pdf/1010.0958v1", 
    "arxiv-id": "1010.0958v1"
},{
    "category": "cs.DC", 
    "author": "YingYi Bu", 
    "title": "Astronomy in the Cloud: Using MapReduce for Image Coaddition", 
    "publish": "2010-10-05T20:35:53Z", 
    "summary": "In the coming decade, astronomical surveys of the sky will generate tens of\nterabytes of images and detect hundreds of millions of sources every night. The\nstudy of these sources will involve computation challenges such as anomaly\ndetection and classification, and moving object tracking. Since such studies\nbenefit from the highest quality data, methods such as image coaddition\n(stacking) will be a critical preprocessing step prior to scientific\ninvestigation. With a requirement that these images be analyzed on a nightly\nbasis to identify moving sources or transient objects, these data streams\npresent many computational challenges. Given the quantity of data involved, the\ncomputational load of these problems can only be addressed by distributing the\nworkload over a large number of nodes. However, the high data throughput\ndemanded by these applications may present scalability challenges for certain\nstorage architectures. One scalable data-processing method that has emerged in\nrecent years is MapReduce, and in this paper we focus on its popular\nopen-source implementation called Hadoop. In the Hadoop framework, the data is\npartitioned among storage attached directly to worker nodes, and the processing\nworkload is scheduled in parallel on the nodes that contain the required input\ndata. A further motivation for using Hadoop is that it allows us to exploit\ncloud computing resources, e.g., Amazon's EC2. We report on our experience\nimplementing a scalable image-processing pipeline for the SDSS imaging database\nusing Hadoop. This multi-terabyte imaging dataset provides a good testbed for\nalgorithm development since its scope and structure approximate future surveys.\nFirst, we describe MapReduce and how we adapted image coaddition to the\nMapReduce framework. Then we describe a number of optimizations to our basic\napproach and report experimental results comparing their performance.", 
    "link": "http://arxiv.org/pdf/1010.1015v1", 
    "arxiv-id": "1010.1015v1"
},{
    "category": "cs.DC", 
    "author": "Rafail Ostrovsky", 
    "title": "Deterministic and Energy-Optimal Wireless Synchronization", 
    "publish": "2010-10-06T10:18:02Z", 
    "summary": "We consider the problem of clock synchronization in a wireless setting where\nprocessors must power-down their radios in order to save energy. Energy\nefficiency is a central goal in wireless networks, especially if energy\nresources are severely limited. In the current setting, the problem is to\nsynchronize clocks of $m$ processors that wake up in arbitrary time points,\nsuch that the maximum difference between wake up times is bounded by a positive\ninteger $n$, where time intervals are appropriately discretized. Currently, the\nbest-known results for synchronization for single-hop networks of $m$\nprocessors is a randomized algorithm due to \\cite{BKO09} of O(\\sqrt {n /m}\n\\cdot poly-log(n)) awake times per processor and a lower bound of\nOmega(\\sqrt{n/m}) of the number of awake times needed per processor\n\\cite{BKO09}. The main open question left in their work is to close the\npoly-log gap between the upper and the lower bound and to de-randomize their\nprobabilistic construction and eliminate error probability. This is exactly\nwhat we do in this paper.\n  That is, we show a {deterministic} algorithm with radio use of Theta(\\sqrt {n\n/m}) that never fails. We stress that our upper bound exactly matches the lower\nbound proven in \\cite{BKO09}, up to a small multiplicative constant. Therefore,\nour algorithm is {optimal} in terms of energy efficiency and completely\nresolves a long sequence of works in this area. In order to achieve these\nresults we devise a novel {adaptive} technique that determines the times when\ndevices power their radios on and off. In addition, we prove several lower\nbounds on the energy efficiency of algorithms for {multi-hop networks}.\nSpecifically, we show that any algorithm for multi-hop networks must have radio\nuse of Omega(\\sqrt n) per processor.", 
    "link": "http://arxiv.org/pdf/1010.1112v1", 
    "arxiv-id": "1010.1112v1"
},{
    "category": "cs.DC", 
    "author": "Afroza Nahar", 
    "title": "Modified Bully Algorithm using Election Commission", 
    "publish": "2010-10-09T06:00:33Z", 
    "summary": "Electing leader is a vital issue not only in distributed computing but also\nin communication network [1, 2, 3, 4, 5], centralized mutual exclusion\nalgorithm [6, 7], centralized control IPC, etc. A leader is required to make\nsynchronization between different processes. And different election algorithms\nare used to elect a coordinator among the available processes in the system\nsuch a way that there will be only one coordinator at any time. Bully election\nalgorithm is one of the classical and well-known approaches in coordinator\nelection process. This paper will present a modified version of bully election\nalgorithm using a new concept called election commission. This approach will\nnot only reduce redundant elections but also minimize total number of elections\nand hence it will minimize message passing, network traffic, and complexity of\nthe existing system.", 
    "link": "http://arxiv.org/pdf/1010.1812v1", 
    "arxiv-id": "1010.1812v1"
},{
    "category": "cs.DC", 
    "author": "Julien Langou", 
    "title": "A Critical Path Approach to Analyzing Parallelism of Algorithmic   Variants. Application to Cholesky Inversion", 
    "publish": "2010-10-11T05:34:40Z", 
    "summary": "Algorithms come with multiple variants which are obtained by changing the\nmathematical approach from which the algorithm is derived. These variants offer\na wide spectrum of performance when implemented on a multicore platform and we\nseek to understand these differences in performances from a theoretical point\nof view. To that aim, we derive and present the critical path lengths of each\nalgorithmic variant for our application problem which enables us to determine a\nlower bound on the time to solution. This metric provides an intuitive grasp of\nthe performance of a variant and we present numerical experiments to validate\nthe tightness of our lower bounds on practical applications. Our case study is\nthe Cholesky inversion and its use in computing the inverse of a symmetric\npositive definite matrix.", 
    "link": "http://arxiv.org/pdf/1010.2000v1", 
    "arxiv-id": "1010.2000v1"
},{
    "category": "cs.DC", 
    "author": "Michael Elkin", 
    "title": "Distributed Deterministic Edge Coloring using Bounded Neighborhood   Independence", 
    "publish": "2010-10-12T17:51:01Z", 
    "summary": "We study the {edge-coloring} problem in the message-passing model of\ndistributed computing. This is one of the most fundamental and well-studied\nproblems in this area. Currently, the best-known deterministic algorithms for\n(2Delta -1)-edge-coloring requires O(Delta) + log-star n time \\cite{PR01},\nwhere Delta is the maximum degree of the input graph. Also, recent results of\n\\cite{BE10} for vertex-coloring imply that one can get an\nO(Delta)-edge-coloring in O(Delta^{epsilon} \\cdot \\log n) time, and an\nO(Delta^{1 + epsilon})-edge-coloring in O(log Delta log n) time, for an\narbitrarily small constant epsilon > 0.\n  In this paper we devise a drastically faster deterministic edge-coloring\nalgorithm. Specifically, our algorithm computes an O(Delta)-edge-coloring in\nO(Delta^{epsilon}) + log-star n time, and an O(Delta^{1 +\nepsilon})-edge-coloring in O(log Delta) + log-star n time. This result improves\nthe previous state-of-the-art {exponentially} in a wide range of Delta,\nspecifically, for 2^{Omega(\\log-star n)} \\leq Delta \\leq polylog(n). In\naddition, for small values of Delta our deterministic algorithm outperforms all\nthe existing {randomized} algorithms for this problem.\n  On our way to these results we study the {vertex-coloring} problem on the\nfamily of graphs with bounded {neighborhood independence}. This is a large\nfamily, which strictly includes line graphs of r-hypergraphs for any r = O(1),\nand graphs of bounded growth. We devise a very fast deterministic algorithm for\nvertex-coloring graphs with bounded neighborhood independence. This algorithm\ndirectly gives rise to our edge-coloring algorithms, which apply to {general}\ngraphs.\n  Our main technical contribution is a subroutine that computes an\nO(Delta/p)-defective p-vertex coloring of graphs with bounded neighborhood\nindependence in O(p^2) + \\log-star n time, for a parameter p, 1 \\leq p \\leq\nDelta.", 
    "link": "http://arxiv.org/pdf/1010.2454v1", 
    "arxiv-id": "1010.2454v1"
},{
    "category": "cs.DC", 
    "author": "Eric Madelaine", 
    "title": "Behavioural Models for Group Communications", 
    "publish": "2010-10-14T05:16:17Z", 
    "summary": "Group communication is becoming a more and more popular infrastructure for\nefficient distributed applications. It consists in representing locally a group\nof remote objects as a single object accessed in a single step; communications\nare then broadcasted to all members. This paper provides models for automatic\nverification of group-based applications, typically for detecting deadlocks or\nchecking message ordering. We show how to encode group communication, together\nwith different forms of synchronisation for group results. The proposed models\nare parametric such that, for example, different group sizes or group members\ncould be experimented with the minimum modification of the original model.", 
    "link": "http://arxiv.org/pdf/1010.2824v1", 
    "arxiv-id": "1010.2824v1"
},{
    "category": "cs.DC", 
    "author": "Antoine Beugnard", 
    "title": "A Reusable Component for Communication and Data Synchronization in   Mobile Distributed Interactive Applications", 
    "publish": "2010-10-14T05:16:43Z", 
    "summary": "In Distributed Interactive Applications (DIA) such as multiplayer games,\nwhere many participants are involved in a same game session and communicate\nthrough a network, they may have an inconsistent view of the virtual world\nbecause of the communication delays across the network. This issue becomes even\nmore challenging when communicating through a cellular network while executing\nthe DIA client on a mobile terminal. Consistency maintenance algorithms may be\nused to obtain a uniform view of the virtual world. These algorithms are very\ncomplex and hard to program and therefore, the implementation and the future\nevolution of the application logic code become difficult. To solve this\nproblem, we propose an approach where the consistency concerns are handled\nseparately by a distributed component called a Synchronization Medium, which is\nresponsible for the communication management as well as the consistency\nmaintenance. We present the detailed architecture of the Synchronization Medium\nand the generic interfaces it offers to DIAs. We evaluate our approach both\nqualitatively and quantitatively. We first demonstrate that the Synchronization\nMedium is a reusable component through the development of two game\napplications, a car racing game and a space war game. A performance evaluation\nthen shows that the overhead introduced by the Synchronization Medium remains\nacceptable.", 
    "link": "http://arxiv.org/pdf/1010.2828v1", 
    "arxiv-id": "1010.2828v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Service Level Agreement (SLA) in Utility Computing Systems", 
    "publish": "2010-10-14T11:33:28Z", 
    "summary": "In recent years, extensive research has been conducted in the area of Service\nLevel Agreement (SLA) for utility computing systems. An SLA is a formal\ncontract used to guarantee that consumers' service quality expectation can be\nachieved. In utility computing systems, the level of customer satisfaction is\ncrucial, making SLAs significantly important in these environments. Fundamental\nissue is the management of SLAs, including SLA autonomy management or trade off\namong multiple Quality of Service (QoS) parameters. Many SLA languages and\nframeworks have been developed as solutions; however, there is no overall\nclassification for these extensive works. Therefore, the aim of this chapter is\nto present a comprehensive survey of how SLAs are created, managed and used in\nutility computing environment. We discuss existing use cases from Grid and\nCloud computing systems to identify the level of SLA realization in\nstate-of-art systems and emerging challenges for future research.", 
    "link": "http://arxiv.org/pdf/1010.2881v1", 
    "arxiv-id": "1010.2881v1"
},{
    "category": "cs.DC", 
    "author": "Erhard Rahm", 
    "title": "Parallel Sorted Neighborhood Blocking with MapReduce", 
    "publish": "2010-10-15T00:28:44Z", 
    "summary": "Cloud infrastructures enable the efficient parallel execution of\ndata-intensive tasks such as entity resolution on large datasets. We\ninvestigate challenges and possible solutions of using the MapReduce\nprogramming model for parallel entity resolution. In particular, we propose and\nevaluate two MapReduce-based implementations for Sorted Neighborhood blocking\nthat either use multiple MapReduce jobs or apply a tailored data replication.", 
    "link": "http://arxiv.org/pdf/1010.3053v1", 
    "arxiv-id": "1010.3053v1"
},{
    "category": "cs.DC", 
    "author": "Adam Pilbeam", 
    "title": "A Survey of Virtualization Technologies With Performance Testing", 
    "publish": "2010-10-15T17:51:50Z", 
    "summary": "Virtualization has rapidly become a go-to technology for increasing\nefficiency in the data center. With virtualization technologies providing\ntremendous flexibility, even disparate architectures may be deployed on a\nsingle machine without interference. Awareness of limitations and requirements\nof physical hosts to be used for virtualization is important. This paper\nreviews the present virtualization methods, virtual computing software, and\nprovides a brief analysis of the performance issues inherent to each. In the\nend we present testing results of KVM-QEMU on two current Multi-Core CPU\nArchitectures and System Configurations.", 
    "link": "http://arxiv.org/pdf/1010.3233v1", 
    "arxiv-id": "1010.3233v1"
},{
    "category": "cs.DC", 
    "author": "Chadi Kari", 
    "title": "A Paradigm for Channel Assignment and Data Migration in Distributed   Systems", 
    "publish": "2010-10-19T19:37:09Z", 
    "summary": "In this manuscript, we consider the problems of channel assignment in\nwireless networks and data migration in heterogeneous storage systems. We show\nthat a soft edge coloring approach to both problems gives rigorous\napproximation guarantees. In the channel assignment problem arising in wireless\nnetworks a pair of edges incident to a vertex are said to be conflicting if the\nchannels assigned to them are the same. Our goal is to assign channels (color\nedges) so that the number of conflicts is minimized. The problem is NP-hard by\na reduction from Edge coloring and we present two combinatorial algorithms for\nthis case. The first algorithm is based on a distributed greedy method and\ngives a solution with at most $2(1-\\frac{1}{k})|E|$ more conflicts than the\noptimal solution.The approximation ratio if the second algorithm is $1 +\n\\frac{|V|}{|E|}$, which gives a ($1 + o(1)$)-factor for dense graphs and is the\nbest possible unless P = NP. We also consider the data migration problem in\nheterogeneous storage systems. In such systems, data layouts may need to be\nreconfigured over time for load balancing or in the event of system\nfailure/upgrades. It is critical to migrate data to their target locations as\nquickly as possible to obtain the best performance of the system. Most of the\nprevious results on data migration assume that each storage node can perform\nonly one data transfer at a time. However, storage devices tend to have\nheterogeneous capabilities as devices may be added over time due to storage\ndemand increase. We develop algorithms to minimize the data migration time. We\nshow that it is possible to find an optimal migration schedule when all $c_v$'s\nare even. Furthermore, though the problem is NP-hard in general, we give an\nefficient soft edge coloring algorithm that offers a rigorous $(1 +\no(1))$-approximation guarantee.", 
    "link": "http://arxiv.org/pdf/1010.4018v1", 
    "arxiv-id": "1010.4018v1"
},{
    "category": "cs.DC", 
    "author": "Jean-Luc Dekeyser", 
    "title": "Parallel Sparse Matrix Solver on the GPU Applied to Simulation of   Electrical Machines", 
    "publish": "2010-10-22T08:46:04Z", 
    "summary": "Nowadays, several industrial applications are being ported to parallel\narchitectures. In fact, these platforms allow acquire more performance for\nsystem modelling and simulation. In the electric machines area, there are many\nproblems which need speed-up on their solution. This paper examines the\nparallelism of sparse matrix solver on the graphics processors. More\nspecifically, we implement the conjugate gradient technique with input matrix\nstored in CSR, and Symmetric CSR and CSC formats. This method is one of the\nmost efficient iterative methods available for solving the finite-element basis\nfunctions of Maxwell's equations. The GPU (Graphics Processing Unit), which is\nused for its implementation, provides mechanisms to parallel the algorithm.\nThus, it increases significantly the computation speed in relation to serial\ncode on CPU based systems.", 
    "link": "http://arxiv.org/pdf/1010.4639v1", 
    "arxiv-id": "1010.4639v1"
},{
    "category": "cs.DC", 
    "author": "Sangyoon Oh", 
    "title": "Towards Constraint-based High Performance Cloud System in the Process of   Cloud Computing Adoption in an Organization", 
    "publish": "2010-10-24T12:08:12Z", 
    "summary": "Cloud computing is penetrating into various domains and environments, from\ntheoretical computer science to economy, from marketing hype to educational\ncurriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently\ndeveloping state of cloud computing leaves several issues to address and also\naffects cloud computing adoption by organizations. In this paper, we explain\nhow the transition into the cloud can occur in an organization and describe the\nmechanism for transforming legacy infrastructure into a virtual\ninfrastructure-based cloud. We describe the state of the art of infrastructural\ncloud, which is essential in the decision making on cloud adoption, and\nhighlight the challenges that can limit the scale and speed of the adoption. We\nthen suggest a strategic framework for designing a high performance cloud\nsystem. This framework is applicable when transformation cloudbased deployment\nmodel collides with some constraints. We give an example of the implementation\nof the framework in a design of a budget-constrained high availability cloud\nsystem.", 
    "link": "http://arxiv.org/pdf/1010.4952v1", 
    "arxiv-id": "1010.4952v1"
},{
    "category": "cs.DC", 
    "author": "Subhash Kak", 
    "title": "On the Mesh Array for Matrix Multiplication", 
    "publish": "2010-10-26T15:10:37Z", 
    "summary": "This article presents new properties of the mesh array for matrix\nmultiplication. In contrast to the standard array that requires 3n-2 steps to\ncomplete its computation, the mesh array requires only 2n-1 steps. Symmetries\nof the mesh array computed values are presented which enhance the efficiency of\nthe array for specific applications. In multiplying symmetric matrices, the\nresults are obtained in 3n/2+1 steps. The mesh array is examined for its\napplication as a scrambling system.", 
    "link": "http://arxiv.org/pdf/1010.5421v1", 
    "arxiv-id": "1010.5421v1"
},{
    "category": "cs.DC", 
    "author": "Igor Sfiligoi", 
    "title": "Flexible Session Management in a Distributed Environment", 
    "publish": "2010-11-02T19:42:04Z", 
    "summary": "Many secure communication libraries used by distributed systems, such as SSL,\nTLS, and Kerberos, fail to make a clear distinction between the authentication,\nsession, and communication layers. In this paper we introduce CEDAR, the secure\ncommunication library used by the Condor High Throughput Computing software,\nand present the advantages to a distributed computing system resulting from\nCEDAR's separation of these layers. Regardless of the authentication method\nused, CEDAR establishes a secure session key, which has the flexibility to be\nused for multiple capabilities. We demonstrate how a layered approach to\nsecurity sessions can avoid round-trips and latency inherent in network\nauthentication. The creation of a distinct session management layer allows for\noptimizations to improve scalability by way of delegating sessions to other\ncomponents in the system. This session delegation creates a chain of trust that\nreduces the overhead of establishing secure connections and enables centralized\nenforcement of system-wide security policies. Additionally, secure channels\nbased upon UDP datagrams are often overlooked by existing libraries; we show\nhow CEDAR's structure accommodates this as well. As an example of the utility\nof this work, we show how the use of delegated security sessions and other\ntechniques inherent in CEDAR's architecture enables US CMS to meet their\nscalability requirements in deploying Condor over large-scale, wide-area grid\nsystems.", 
    "link": "http://arxiv.org/pdf/1011.0715v1", 
    "arxiv-id": "1011.0715v1"
},{
    "category": "cs.DC", 
    "author": "Christian Walder", 
    "title": "Rank k Cholesky Up/Down-dating on the GPU: gpucholmodV0.2", 
    "publish": "2010-11-04T14:39:26Z", 
    "summary": "In this note we briefly describe our Cholesky modification algorithm for\nstreaming multiprocessor architectures. Our implementation is available in C++\nwith Matlab binding, using CUDA to utilise the graphics processing unit (GPU).\nLimited speed ups are possible due to the bandwidth bound nature of the\nproblem. Furthermore, a complex dependency pattern must be obeyed, requiring\nmultiple kernels to be launched. Nonetheless, this makes for an interesting\nproblem, and our approach can reduce the computation time by a factor of around\n7 for matrices of size 5000 by 5000 and k=16, in comparison with the LINPACK\nsuite running on a CPU of comparable vintage. Much larger problems can be\nhandled however due to the O(n) scaling in required GPU memory of our method.", 
    "link": "http://arxiv.org/pdf/1011.1173v1", 
    "arxiv-id": "1011.1173v1"
},{
    "category": "cs.DC", 
    "author": "Michael G. Rabbat", 
    "title": "Multiscale Gossip for Efficient Decentralized Averaging in Wireless   Packet Networks", 
    "publish": "2010-11-09T23:50:10Z", 
    "summary": "This paper describes and analyzes a hierarchical gossip algorithm for solving\nthe distributed average consensus problem in wireless sensor networks. The\nnetwork is recursively partitioned into subnetworks. Initially, nodes at the\nfinest scale gossip to compute local averages. Then, using geographic routing\nto enable gossip between nodes that are not directly connected, these local\naverages are progressively fused up the hierarchy until the global average is\ncomputed. We show that the proposed hierarchical scheme with $k$ levels of\nhierarchy is competitive with state-of-the-art randomized gossip algorithms, in\nterms of message complexity, achieving $\\epsilon$-accuracy with high\nprobability after $O\\big(n \\log \\log n \\log \\frac{kn}{\\epsilon} \\big)$\nmessages. Key to our analysis is the way in which the network is recursively\npartitioned. We find that the optimal scaling law is achieved when subnetworks\nat scale $j$ contain $O(n^{(2/3)^j})$ nodes; then the message complexity at any\nindividual scale is $O(n \\log \\frac{kn}{\\epsilon})$, and the total number of\nscales in the hierarchy grows slowly, as $\\Theta(\\log \\log n)$. Another\nimportant consequence of hierarchical construction is that the longest distance\nover which messages are exchanged is $O(n^{1/3})$ hops (at the highest scale),\nand most messages (at lower scales) travel shorter distances. In networks that\nuse link-level acknowledgements, this results in less congestion and resource\nusage by reducing message retransmissions. Simulations illustrate that the\nproposed scheme is more message-efficient than existing state-of-the-art\nrandomized gossip algorithms based on averaging along paths.", 
    "link": "http://arxiv.org/pdf/1011.2235v3", 
    "arxiv-id": "1011.2235v3"
},{
    "category": "cs.DC", 
    "author": "Matthieu Roy", 
    "title": "A framework for proving the self-organization of dynamic systems", 
    "publish": "2010-11-10T08:37:53Z", 
    "summary": "This paper aims at providing a rigorous definition of self- organization, one\nof the most desired properties for dynamic systems (e.g., peer-to-peer systems,\nsensor networks, cooperative robotics, or ad-hoc networks). We characterize\ndifferent classes of self-organization through liveness and safety properties\nthat both capture information re- garding the system entropy. We illustrate\nthese classes through study cases. The first ones are two representative P2P\noverlays (CAN and Pas- try) and the others are specific implementations of\n\\Omega (the leader oracle) and one-shot query abstractions for dynamic\nsettings. Our study aims at understanding the limits and respective power of\nexisting self-organized protocols and lays the basis of designing robust\nalgorithm for dynamic systems.", 
    "link": "http://arxiv.org/pdf/1011.2312v1", 
    "arxiv-id": "1011.2312v1"
},{
    "category": "cs.DC", 
    "author": "Devan Sohier", 
    "title": "A Distributed Clustering Algorithm for Dynamic Networks", 
    "publish": "2010-11-12T15:35:10Z", 
    "summary": "We propose an algorithm that builds and maintains clusters over a network\nsubject to mobility. This algorithm is fully decentralized and makes all the\ndifferent clusters grow concurrently. The algorithm uses circulating tokens\nthat collect data and move according to a random walk traversal scheme. Their\ntask consists in (i) creating a cluster with the nodes it discovers and (ii)\nmanaging the cluster expansion; all decisions affecting the cluster are taken\nonly by a node that owns the token. The size of each cluster is maintained\nhigher than $m$ nodes ($m$ is a parameter of the algorithm). The obtained\nclustering is locally optimal in the sense that, with only a local view of each\nclusters, it computes the largest possible number of clusters (\\emph{ie} the\nsizes of the clusters are as close to $m$ as possible). This algorithm is\ndesigned as a decentralized control algorithm for large scale networks and is\nmobility-adaptive: after a series of topological changes, the algorithm\nconverges to a clustering. This recomputation only affects nodes in clusters in\nwhich topological changes happened, and in adjacent clusters.", 
    "link": "http://arxiv.org/pdf/1011.2953v1", 
    "arxiv-id": "1011.2953v1"
},{
    "category": "cs.DC", 
    "author": "Meng Shao", 
    "title": "A High-confidence Cyber-Physical Alarm System: Design and Implementation", 
    "publish": "2010-11-13T03:23:15Z", 
    "summary": "Most traditional alarm systems cannot address security threats in a\nsatisfactory manner. To alleviate this problem, we developed a high-confidence\ncyber-physical alarm system (CPAS), a new kind of alarm systems. This system\nestablishes the connection of the Internet (i.e. TCP/IP) through GPRS/CDMA/3G.\nIt achieves mutual communication control among terminal equipments, human\nmachine interfaces and users by using the existing mobile communication\nnetwork. The CPAS will enable the transformation in alarm mode from traditional\none-way alarm to two-way alarm. The system has been successfully applied in\npractice. The results show that the CPAS could avoid false alarms and satisfy\nresidents' security needs.", 
    "link": "http://arxiv.org/pdf/1011.3094v1", 
    "arxiv-id": "1011.3094v1"
},{
    "category": "cs.DC", 
    "author": "James Aspnes", 
    "title": "Slightly smaller splitter networks", 
    "publish": "2010-11-14T00:52:14Z", 
    "summary": "The classic renaming protocol of Moir and Anderson (1995) uses a network of\nTheta(n^2) splitters to assign unique names to n processes with unbounded\ninitial names. We show how to reduce this bound to Theta(n^{3/2}) splitters.", 
    "link": "http://arxiv.org/pdf/1011.3170v1", 
    "arxiv-id": "1011.3170v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Stabilizing data-link over non-FIFO channels with optimal   fault-resilience", 
    "publish": "2010-11-16T10:04:34Z", 
    "summary": "Self-stabilizing systems have the ability to converge to a correct behavior\nwhen started in any configuration. Most of the work done so far in the\nself-stabilization area assumed either communication via shared memory or via\nFIFO channels. This paper is the first to lay the bases for the design of\nself-stabilizing message passing algorithms over unreliable non-FIFO channels.\nWe propose a fault-send-deliver optimal stabilizing data-link layer that\nemulates a reliable FIFO communication channel over unreliable capacity bounded\nnon-FIFO channels.", 
    "link": "http://arxiv.org/pdf/1011.3632v2", 
    "arxiv-id": "1011.3632v2"
},{
    "category": "cs.DC", 
    "author": "Rong Zheng", 
    "title": "Progressive Decoding for Data Availability and Reliability in   Distributed Networked Storage", 
    "publish": "2010-11-18T04:47:28Z", 
    "summary": "To harness the ever growing capacity and decreasing cost of storage,\nproviding an abstraction of dependable storage in the presence of crash-stop\nand Byzantine failures is compulsory. We propose a decentralized Reed Solomon\ncoding mechanism with minimum communication overhead. Using a progressive data\nretrieval scheme, a data collector contacts only the necessary number of\nstorage nodes needed to guarantee data integrity. The scheme gracefully adapts\nthe cost of successful data retrieval to the number of storage node failures.\nMoreover, by leveraging the Welch-Berlekamp algorithm, it avoids unnecessary\ncomputations. Compared to the state-of-the-art decoding scheme, the\nimplementation and evaluation results show that our progressive data retrieval\nscheme has up to 35 times better computation performance for low Byzantine node\nrates. Additionally, the communication cost in data retrieval is derived\nanalytically and corroborated by Monte-Carlo simulation results. Our\nimplementation is flexible in that the level of redundancy it provides is\nindependent of the number of data generating nodes, a requirement for\ndistributed storage systems", 
    "link": "http://arxiv.org/pdf/1011.4135v1", 
    "arxiv-id": "1011.4135v1"
},{
    "category": "cs.DC", 
    "author": "Madhu Kumar SD", 
    "title": "Optimal Placement Algorithms for Virtual Machines", 
    "publish": "2010-11-23T11:26:48Z", 
    "summary": "Cloud computing provides a computing platform for the users to meet their\ndemands in an efficient, cost-effective way. Virtualization technologies are\nused in the clouds to aid the efficient usage of hardware. Virtual machines\n(VMs) are utilized to satisfy the user needs and are placed on physical\nmachines (PMs) of the cloud for effective usage of hardware resources and\nelectricity in the cloud. Optimizing the number of PMs used helps in cutting\ndown the power consumption by a substantial amount.\n  In this paper, we present an optimal technique to map virtual machines to\nphysical machines (nodes) such that the number of required nodes is minimized.\nWe provide two approaches based on linear programming and quadratic programming\ntechniques that significantly improve over the existing theoretical bounds and\nefficiently solve the problem of virtual machine (VM) placement in data\ncenters.", 
    "link": "http://arxiv.org/pdf/1011.5064v1", 
    "arxiv-id": "1011.5064v1"
},{
    "category": "cs.DC", 
    "author": "Roger Wattenhofer", 
    "title": "Local Computation: Lower and Upper Bounds", 
    "publish": "2010-11-24T19:56:31Z", 
    "summary": "The question of what can be computed, and how efficiently, are at the core of\ncomputer science. Not surprisingly, in distributed systems and networking\nresearch, an equally fundamental question is what can be computed in a\n\\emph{distributed} fashion. More precisely, if nodes of a network must base\ntheir decision on information in their local neighborhood only, how well can\nthey compute or approximate a global (optimization) problem? In this paper we\ngive the first poly-logarithmic lower bound on such local computation for\n(optimization) problems including minimum vertex cover, minimum (connected)\ndominating set, maximum matching, maximal independent set, and maximal\nmatching. In addition we present a new distributed algorithm for solving\ngeneral covering and packing linear programs. For some problems this algorithm\nis tight with the lower bounds, for others it is a distributed approximation\nscheme. Together, our lower and upper bounds establish the local computability\nand approximability of a large class of problems, characterizing how much local\ninformation is required to solve these tasks.", 
    "link": "http://arxiv.org/pdf/1011.5470v2", 
    "arxiv-id": "1011.5470v2"
},{
    "category": "cs.DC", 
    "author": "Anderson David", 
    "title": "Correlated Resource Models of Internet End Hosts", 
    "publish": "2010-11-25T08:41:38Z", 
    "summary": "Understanding and modelling resources of Internet end hosts is essential for\nthe design of desktop software and Internet-distributed applications. In this\npaper we develop a correlated resource model of Internet end hosts based on\nreal trace data taken from the SETI@home project. This data covers a 5-year\nperiod with statistics for 2.7 million hosts. The resource model is based on\nstatistical analysis of host computational power, memory, and storage as well\nas how these resources change over time and the correlations between them. We\nfind that resources with few discrete values (core count, memory) are well\nmodeled by exponential laws governing the change of relative resource\nquantities over time. Resources with a continuous range of values are well\nmodeled with either correlated normal distributions (processor speed for\ninteger operations and floating point operations) or log-normal distributions\n(available disk space). We validate and show the utility of the models by\napplying them to a resource allocation problem for Internet-distributed\napplications, and demonstrate their value over other models. We also make our\ntrace data and tool for automatically generating realistic Internet end hosts\npublicly available.", 
    "link": "http://arxiv.org/pdf/1011.5568v1", 
    "arxiv-id": "1011.5568v1"
},{
    "category": "cs.DC", 
    "author": "Ricardo Gon\u00e7alves", 
    "title": "Dotted Version Vectors: Logical Clocks for Optimistic Replication", 
    "publish": "2010-11-26T14:45:53Z", 
    "summary": "In cloud computing environments, a large number of users access data stored\nin highly available storage systems. To provide good performance to\ngeographically disperse users and allow operation even in the presence of\nfailures or network partitions, these systems often rely on optimistic\nreplication solutions that guarantee only eventual consistency. In this\nscenario, it is important to be able to accurately and efficiently identify\nupdates executed concurrently. In this paper, first we review, and expose\nproblems with current approaches to causality tracking in optimistic\nreplication: these either lose information about causality or do not scale, as\nthey require replicas to maintain information that grows linearly with the\nnumber of clients or updates. Then, we propose a novel solution that fully\ncaptures causality while being very concise in that it maintains information\nthat grows linearly only with the number of servers that register updates for a\ngiven data element, bounded by the degree of replication.", 
    "link": "http://arxiv.org/pdf/1011.5808v1", 
    "arxiv-id": "1011.5808v1"
},{
    "category": "cs.DC", 
    "author": "Paulo S\u00e9rgio Almeida", 
    "title": "Dependability in Aggregation by Averaging", 
    "publish": "2010-11-30T16:10:49Z", 
    "summary": "Aggregation is an important building block of modern distributed\napplications, allowing the determination of meaningful properties (e.g. network\nsize, total storage capacity, average load, majorities, etc.) that are used to\ndirect the execution of the system. However, the majority of the existing\naggregation algorithms exhibit relevant dependability issues, when prospecting\ntheir use in real application environments. In this paper, we reveal some\ndependability issues of aggregation algorithms based on iterative averaging\ntechniques, giving some directions to solve them. This class of algorithms is\nconsidered robust (when compared to common tree-based approaches), being\nindependent from the used routing topology and providing an aggregation result\nat all nodes. However, their robustness is strongly challenged and their\ncorrectness often compromised, when changing the assumptions of their working\nenvironment to more realistic ones. The correctness of this class of algorithms\nrelies on the maintenance of a fundamental invariant, commonly designated as\n\"mass conservation\". We will argue that this main invariant is often broken in\npractical settings, and that additional mechanisms and modifications are\nrequired to maintain it, incurring in some degradation of the algorithms\nperformance. In particular, we discuss the behavior of three representative\nalgorithms Push-Sum Protocol, Push-Pull Gossip protocol and Distributed Random\nGrouping under asynchronous and faulty (with message loss and node crashes)\nenvironments. More specifically, we propose and evaluate two new versions of\nthe Push-Pull Gossip protocol, which solve its message interleaving problem\n(evidenced even in a synchronous operation mode).", 
    "link": "http://arxiv.org/pdf/1011.6596v1", 
    "arxiv-id": "1011.6596v1"
},{
    "category": "cs.DC", 
    "author": "Claudia-Lavinia Ignat", 
    "title": "A Log Auditing Approach for Trust Management in Peer-to-Peer   Collaboration", 
    "publish": "2010-12-06T11:14:22Z", 
    "summary": "Nowadays we are faced with an increasing popularity of social software\nincluding wikis, blogs, micro-blogs and online social networks such as Facebook\nand MySpace. Unfortunately, the mostly used social services are centralized and\npersonal information is stored at a single vendor. This results in potential\nprivacy problems as users do not have much control over how their private data\nis disseminated. To overcome this limitation, some recent approaches envisioned\nreplacing the single authority centralization of services by a peer-to-peer\ntrust-based approach where users can decide with whom they want to share their\nprivate data. In this peer-to-peer collaboration it is very difficult to ensure\nthat after data is shared with other peers, these peers will not misbehave and\nviolate data privacy. In this paper we propose a mechanism that addresses the\nissue of data privacy violation due to data disclosure to malicious peers. In\nour approach trust values between users are adjusted according to their\nprevious activities on the shared data. Users share their private data by\nspecifying some obligations the receivers must follow. We log modifications\ndone by users on the shared data as well as the obligations that must be\nfollowed when data is shared. By a log-auditing mechanism we detect users that\nmisbehaved and we adjust their associated trust values by using any existing\ndecentralized trust model.", 
    "link": "http://arxiv.org/pdf/1012.1131v1", 
    "arxiv-id": "1012.1131v1"
},{
    "category": "cs.DC", 
    "author": "Jan Vacata", 
    "title": "New Row-grouped CSR format for storing the sparse matrices on GPU with   implementation in CUDA", 
    "publish": "2010-12-10T14:04:33Z", 
    "summary": "In this article we present a new format for storing sparse matrices. The\nformat is designed to perform well mainly on the GPU devices. We present its\nimplementation in CUDA. The performance has been tested on 1,600 different\ntypes of matrices and we compare our format with the Hybrid format. We give\ndetailed comparison of both formats and show their strong and weak parts.", 
    "link": "http://arxiv.org/pdf/1012.2270v1", 
    "arxiv-id": "1012.2270v1"
},{
    "category": "cs.DC", 
    "author": "A. B. Mutiara", 
    "title": "Performance Evaluation of Parallel Message Passing and Thread   Programming Model on Multicore Architectures", 
    "publish": "2010-12-10T14:17:19Z", 
    "summary": "The current trend of multicore architectures on shared memory systems\nunderscores the need of parallelism. While there are some programming model to\nexpress parallelism, thread programming model has become a standard to support\nthese system such as OpenMP, and POSIX threads. MPI (Message Passing Interface)\nwhich remains the dominant model used in high-performance computing today faces\nthis challenge.\n  Previous version of MPI which is MPI-1 has no shared memory concept, and\nCurrent MPI version 2 which is MPI-2 has a limited support for shared memory\nsystems. In this research, MPI-2 version of MPI will be compared with OpenMP to\nsee how well does MPI perform on multicore / SMP (Symmetric Multiprocessor)\nmachines.\n  Comparison between OpenMP for thread programming model and MPI for message\npassing programming model will be conducted on multicore shared memory machine\narchitectures to see who has a better performance in terms of speed and\nthroughput. Application used to assess the scalability of the evaluated\nparallel programming solutions is matrix multiplication with customizable\nmatrix dimension.\n  Many research done on a large scale parallel computing which using high scale\nbenchmark such as NSA Parallel Benchmark (NPB) for their testing standarization\n[1]. This research will be conducted on a small scale parallel computing that\nemphasize more on the performance evaluation between MPI and OpenMPI parallel\nprogramming model using self created benchmark.", 
    "link": "http://arxiv.org/pdf/1012.2273v1", 
    "arxiv-id": "1012.2273v1"
},{
    "category": "cs.DC", 
    "author": "L. T. Handoko", 
    "title": "openPC : a toolkit for public cluster with full ownership", 
    "publish": "2010-12-12T00:15:44Z", 
    "summary": "The openPC is a set of open source tools that realizes a parallel machine and\ndistributed computing environment divisible into several independent blocks of\nnodes, and each of them is remotely but fully in any means accessible for users\nwith a full ownership policy. The openPC components address fundamental issues\nrelating to security, resource access, resource allocation, compatibilities\nwith heterogeneous middlewares, user-friendly and integrated web-based\ninterfaces, hardware control and monitoring systems. These components have been\ndeployed successfully to the LIPI Public Cluster which is open for public use.\nIn this paper, the unique characteristics of openPC due to its rare\nrequirements are introduced, its components and a brief performance analysis\nare discussed.", 
    "link": "http://arxiv.org/pdf/1012.2499v1", 
    "arxiv-id": "1012.2499v1"
},{
    "category": "cs.DC", 
    "author": "Joey Paquet", 
    "title": "Towards Refactoring the DMF to Support Jini and JMS DMS in GIPSY", 
    "publish": "2010-12-13T20:58:58Z", 
    "summary": "In this paper we report on our re-engineering effort to refactor and unify\ntwo somewhat disjoint Java distributed middleware technologies -- Jini and JMS\n-- used in the implementation of the Demand Migration System (DMS). In doing\nso, we refactor their parent Demand Migration Framework (DMF), within the\nGeneral Intensional Programming System (GIPSY). The complex Java-based GIPSY\nproject is used to investigate on the intensional and hybrid programming\nparadigms.", 
    "link": "http://arxiv.org/pdf/1012.2860v4", 
    "arxiv-id": "1012.2860v4"
},{
    "category": "cs.DC", 
    "author": "Sangyoon Oh", 
    "title": "An Architectural Design for Brokered Collaborative Content Delivery   System", 
    "publish": "2010-12-15T14:31:05Z", 
    "summary": "Advances in web technologies have driven massive content uploads and requests\nthat can be identified by the increased usage of multimedia web and social web\nservices. This situation enforces the content providers to scale their\ninfrastructure in order to cope with the extra provisioning of network traffic,\nstorage and other resources. Since the complexity and cost factors in scaling\nthe infrastructure exist, we propose a novel solution for providing and\ndelivering contents to clients by introducing a brokered collaborative content\ndelivery system. The architectural design of this system leverages content\nredundancy and content distribution mechanisms in other content providers to\ndeliver contents to the clients. With the recent emergence of cloud computing,\nwe show that this system can also be adopted to run on the cloud. In this\npaper, we focus on a brokering scheme to mediate user requests to the most\nappropriate content provider based on a ranking system. The architecture\nprovides a novel Global Rank Value (GRV) concept in estimating content provider\ncapability and transforming the QoS requirement of a content request. A\nfairness model that will bring this design to be attractive to the current\ncontent delivery regime is also introduced. Through simulation, we show that\nusing fair provider selection, contents can be provisioned by a better pool of\nqualified providers thus leveraging the collaboration and preventing potential\nQoS violation that may occur when the size of pool is smaller.", 
    "link": "http://arxiv.org/pdf/1012.3347v2", 
    "arxiv-id": "1012.3347v2"
},{
    "category": "cs.DC", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "title": "Work-stealing for mixed-mode parallelism by deterministic team-building", 
    "publish": "2010-12-22T16:43:12Z", 
    "summary": "We show how to extend classical work-stealing to deal also with data parallel\ntasks that can require any number of threads r >= 1 for their execution. We\nexplain in detail the so introduced idea of work-stealing with deterministic\nteam-building which in a natural way generalizes classical work-stealing. A\nprototype C++ implementation of the generalized work-stealing algorithm has\nbeen given and is briefly described. Building on this, a serious, well-known\ncontender for a best parallel Quicksort algorithm has been implemented, which\nnaturally relies on both task and data parallelism. For instance, sorting\n2^27-1 randomly generated integers we could improve the speed-up from 5.1 to\n8.7 on a 32-core Intel Nehalem EX system, being consistently better than the\ntuned, task-parallel Cilk++ system.", 
    "link": "http://arxiv.org/pdf/1012.5030v1", 
    "arxiv-id": "1012.5030v1"
},{
    "category": "cs.DC", 
    "author": "Rong Zheng", 
    "title": "Maximum Lifetime for Data Regeneration in Wireless Sensor Networks", 
    "publish": "2010-12-28T20:36:09Z", 
    "summary": "Robust distributed storage systems dedicated to wireless sensor networks\nutilize several nodes to redundantly store sensed data so that when some\nstorage nodes fail, the sensed data can still be reconstructed. For the same\nlevel of redundancy, erasure coding based approaches are known to require less\ndata storage space than replication methods.\n  To maintain the same level of redundancy when one storage node fails, erasure\ncoded data can be restored onto some other storage node by having this node\ndownload respective pieces from other live storage nodes. Previous works showed\nthat the benefits in using erasure coding for robust storage over replication\nare made unappealing by the complication in regenerating lost data. More recent\nwork has, however, shown that the bandwidth for erasure coded data can be\nfurther reduced by proposing Regenerating Coding, making erasure codes again\ndesirable for robust data storage.\n  But none of these works on regenerating coding consider how these codes will\nperform for data regeneration in wireless sensor networks. We therefore propose\nan analytical model to quantify the network lifetime gains of regenerating\ncoding over classical schemes. We also propose a distributed algorithm, TROY,\nthat determines which nodes and routes to use for data regeneration. Our\nanalytical studies show that for certain topologies, TROY achieves maximum\nnetwork lifetime. Our evaluation studies in real sensor network traces show\nthat TROY achieves near optimal lifetime and performs better than baseline\nalgorithms.", 
    "link": "http://arxiv.org/pdf/1012.5834v5", 
    "arxiv-id": "1012.5834v5"
},{
    "category": "cs.DC", 
    "author": "Madhuri Bhavsar", 
    "title": "QOS based user driven scheduler for grid environment", 
    "publish": "2011-02-02T17:36:58Z", 
    "summary": "As grids are in essence heterogeneous, dynamic, shared and distributed\nenvironments, managing these kinds of platforms efficiently is extremely\ncomplex. A promising scalable approach to deal with these intricacies is the\ndesign of self-managing of autonomic applications. Autonomic applications adapt\ntheir execution accordingly by considering knowledge about their own behaviour\nand environmental conditions.QoS based User Driven scheduling for grid that\nprovides the self-optimizing ability in autonomic applications. Computational\ngrids to provide a user to solve large scale problem by spreading a single\nlarge computation across multiple machines of physical location. QoS based User\nDriven scheduler for grid also provides reliability of the grid systems and\nincrease the performance of the grid to reducing the execution time of job by\napplying scheduling policies defined by the user. The main aim of this paper is\nto distribute the computational load among the available grid nodes and to\ndeveloped a QoS based scheduling algorithm for grid and making grid more\nreliable.Grid computing system is different from conventional distributed\ncomputing systems by its focus on large scale resource sharing, where\nprocessors and communication have significant inuence on Grid computing\nreliability. Reliability capabilities initiated by end users from within\napplications they submit to the grid for execution. Reliability of\ninfrastructure and management services that perform essential functions\nnecessary for grid systems to operate, such as resource allocation and\nscheduling.", 
    "link": "http://arxiv.org/pdf/1102.0516v1", 
    "arxiv-id": "1102.0516v1"
},{
    "category": "cs.DC", 
    "author": "K. Indumathi", 
    "title": "Probability Based Adaptive Invoked Clustering Algorithm in MANETs", 
    "publish": "2011-02-09T00:25:31Z", 
    "summary": "A mobile ad hoc network (MANET), is a self-configuring network of mobile\ndevices connected by wireless links. In order to achieve stable clusters, the\ncluster-heads maintaining the cluster should be stable with minimum overhead of\ncluster re-elections. In this paper we propose a Probability Based Adaptive\nInvoked Weighted Clustering Algorithm (PAIWCA) which can enhance the stability\nof the clusters by taking battery power of the nodes into considerations for\nthe clustering formation and electing stable cluster-heads using cluster head\nprobability of a node. In this simulation study a comparison was conducted to\nmeasure the performance of our algorithm with maximal weighted independent set\n(MWIS) in terms of the number of clusters formed, the connectivity of the\nnetwork, dominant set updates,throughput of the overall network and packet\ndelivery ratio. The result shows that our algorithm performs better than\nexisting one and is also tunable to different kinds of network conditions.", 
    "link": "http://arxiv.org/pdf/1102.1754v1", 
    "arxiv-id": "1102.1754v1"
},{
    "category": "cs.DC", 
    "author": "Sachin Bhardwaj", 
    "title": "Analytical Study of Object Components for Distributed and Ubiquitous   Computing Environment", 
    "publish": "2011-02-10T14:39:49Z", 
    "summary": "The Distributed object computing is a paradigm that allows objects to be\ndistributed across a heterogeneous network, and allows each of the components\nto interoperate as a unified whole. A new generation of distributed\napplications, such as telemedicine and e-commerce applications, are being\ndeployed in heterogeneous and ubiquitous computing environments. The objective\nof this paper is to explore an applicability of a component based services in\nubiquitous computational environment. While the fundamental structure of\nvarious distributed object components is similar, there are differences that\ncan profoundly impact an application developer or the administrator of a\ndistributed simulation exercise and to implement in Ubiquitous Computing\nEnvironment.", 
    "link": "http://arxiv.org/pdf/1102.2131v2", 
    "arxiv-id": "1102.2131v2"
},{
    "category": "cs.DC", 
    "author": "Madhu Kumar S D", 
    "title": "Power Efficient Resource Allocation for Clouds Using Ant Colony   Framework", 
    "publish": "2011-02-13T15:56:29Z", 
    "summary": "Cloud computing is one of the rapidly improving technologies. It provides\nscalable resources needed for the ap- plications hosted on it. As cloud-based\nservices become more dynamic, resource provisioning becomes more challenging.\nThe QoS constrained resource allocation problem is considered in this paper, in\nwhich customers are willing to host their applications on the provider's cloud\nwith a given SLA requirements for performance such as throughput and response\ntime. Since, the data centers hosting the applications consume huge amounts of\nenergy and cause huge operational costs, solutions that reduce energy\nconsumption as well as operational costs are gaining importance. In this work,\nwe propose an energy efficient mechanism that allocates the cloud resources to\nthe applications without violating the given service level agreements(SLA)\nusing Ant colony framework.", 
    "link": "http://arxiv.org/pdf/1102.2608v1", 
    "arxiv-id": "1102.2608v1"
},{
    "category": "cs.DC", 
    "author": "Sanjeev Sharma", 
    "title": "An Improved Multiple Faults Reassignment based Recovery in Cluster   Computing", 
    "publish": "2011-02-13T16:50:30Z", 
    "summary": "In case of multiple node failures performance becomes very low as compare to\nsingle node failure. Failures of nodes in cluster computing can be tolerated by\nmultiple fault tolerant computing. Existing recovery schemes are efficient for\nsingle fault but not with multiple faults. Recovery scheme proposed in this\npaper having two phases; sequentially phase, concurrent phase. In sequentially\nphase, loads of all working nodes are uniformly and evenly distributed by\nproposed dynamic rank based and load distribution algorithm. In concurrent\nphase, loads of all failure nodes as well as new job arrival are assigned\nequally to all available nodes by just finding the least loaded node among the\nseveral nodes by failure nodes job allocation algorithm. Sequential and\nconcurrent executions of algorithms improve the performance as well better\nresource utilization. Dynamic rank based algorithm for load redistribution\nworks as a sequential restoration algorithm and reassignment algorithm for\ndistribution of failure nodes to least loaded computing nodes works as a\nconcurrent recovery reassignment algorithm. Since load is evenly and uniformly\ndistributed among all available working nodes with less number of iterations,\nlow iterative time and communication overheads hence performance is improved.\nDynamic ranking algorithm is low overhead, high convergence algorithm for\nreassignment of tasks uniformly among all available nodes. Reassignments of\nfailure nodes are done by a low overhead efficient failure job allocation\nalgorithm. Test results to show effectiveness of the proposed scheme are\npresented.", 
    "link": "http://arxiv.org/pdf/1102.2616v1", 
    "arxiv-id": "1102.2616v1"
},{
    "category": "cs.DC", 
    "author": "Mike Kenyon", 
    "title": "Establishing Applicability of SSDs to LHC Tier-2 Hardware Configuration", 
    "publish": "2011-02-15T16:05:04Z", 
    "summary": "Solid State Disk technologies are increasingly replacing high-speed hard\ndisks as the storage technology in high-random-I/O environments. There are\nseveral potentially I/O bound services within the typical LHC Tier-2 - in the\nback-end, with the trend towards many-core architectures continuing, worker\nnodes running many single-threaded jobs and storage nodes delivering many\nsimultaneous files can both exhibit I/O limited efficiency. We estimate the\neffectiveness of affordable SSDs in the context of worker nodes, on a large\nTier-2 production setup using both low level tools and real LHC I/O intensive\ndata analysis jobs comparing and contrasting with high performance spinning\ndisk based solutions. We consider the applicability of each solution in the\ncontext of its price/performance metrics, with an eye on the pragmatic issues\nfacing Tier-2 provision and upgrades", 
    "link": "http://arxiv.org/pdf/1102.3114v1", 
    "arxiv-id": "1102.3114v1"
},{
    "category": "cs.DC", 
    "author": "Mikhail Posypkin", 
    "title": "Parallel algorithms for SAT in application to inversion problems of some   discrete functions", 
    "publish": "2011-02-17T11:28:21Z", 
    "summary": "In this article we consider the inversion problem for polynomially computable\ndiscrete functions. These functions describe behavior of many discrete systems\nand are used in model checking, hardware verification, cryptanalysis, computer\nbiology and other domains. Quite often it is necessary to invert these\nfunctions, i.e. to find an unknown preimage if an image and algorithm of\nfunction computation are given. In general case this problem is computationally\nintractable. However, many of it's special cases are very important in\npractical applications. Thus development of algorithms that are applicable to\nthese special cases is of importance. The practical applicability of such\nalgorithms can be validated by their ability to solve the problems that are\nconsidered to be computationally hard (for example cryptanalysis problems). In\nthis article we propose the technology of solving the inversion problem for\npolynomially computable discrete functions. This technology was implemented in\ndistributed computing environments (parallel clusters and Grid-systems). It is\nbased on reducing the inversion problem for the considered function to some SAT\nproblem. We describe a general approach to coarse-grained parallelization for\nobtained SAT problems. Efficiency of each parallelization scheme is determined\nby the means of a special predictive function. The proposed technology was\nvalidated by successful solving of cryptanalysis problems for some keystream\ngenerators. The main practical result of this work is a complete cryptanalysis\nof keystream generator A5/1 which was performed in a Grid system specially\nbuilt for this task.", 
    "link": "http://arxiv.org/pdf/1102.3563v1", 
    "arxiv-id": "1102.3563v1"
},{
    "category": "cs.DC", 
    "author": "Ulrich Schmid", 
    "title": "Solving k-Set Agreement with Stable Skeleton Graphs", 
    "publish": "2011-02-22T07:41:38Z", 
    "summary": "In this paper we consider the k-set agreement problem in distributed\nmessage-passing systems using a round-based approach: Both synchrony of\ncommunication and failures are captured just by means of the messages that\narrive within a round, resulting in round-by-round communication graphs that\ncan be characterized by simple communication predicates. We introduce the weak\ncommunication predicate PSources(k) and show that it is tight for k-set\nagreement, in the following sense: We (i) prove that there is no algorithm for\nsolving (k-1)-set agreement in systems characterized by PSources(k), and (ii)\npresent a novel distributed algorithm that achieves k-set agreement in runs\nwhere PSources(k) holds. Our algorithm uses local approximations of the stable\nskeleton graph, which reflects the underlying perpetual synchrony of a run. We\nprove that this approximation is correct in all runs, regardless of the\ncommunication predicate, and show that graph-theoretic properties of the stable\nskeleton graph can be used to solve k-set agreement if PSources(k) holds.", 
    "link": "http://arxiv.org/pdf/1102.4423v1", 
    "arxiv-id": "1102.4423v1"
},{
    "category": "cs.DC", 
    "author": "Sergio Rajsbaum", 
    "title": "An Equivariance Theorem with Applications to Renaming (Preliminary   Version)", 
    "publish": "2011-02-24T10:24:43Z", 
    "summary": "In the renaming problem, each process in a distributed system is issued a\nunique name from a large name space, and the processes must coordinate with one\nanother to choose unique names from a much smaller name space. We show that\nlower bounds on the solvability of renaming in an asynchronous distributed\nsystem can be formulated as a purely topological question about the existence\nof an equivariant chain map from a topological disk to a topological annulus.\nProving the non-existence of such a map implies the non-existence of a\ndistributed renaming algorithm in several related models of computation.", 
    "link": "http://arxiv.org/pdf/1102.4946v1", 
    "arxiv-id": "1102.4946v1"
},{
    "category": "cs.DC", 
    "author": "Stanimire Tomov", 
    "title": "Fully Empirical Autotuned QR Factorization For Multicore Architectures", 
    "publish": "2011-02-25T20:21:32Z", 
    "summary": "Tuning numerical libraries has become more difficult over time, as systems\nget more sophisticated. In particular, modern multicore machines make the\nbehaviour of algorithms hard to forecast and model. In this paper, we tackle\nthe issue of tuning a dense QR factorization on multicore architectures. We\nshow that it is hard to rely on a model, which motivates us to design a fully\nempirical approach. We exhibit few strong empirical properties that enable us\nto efficiently prune the search space. Our method is automatic, fast and\nreliable. The tuning process is indeed fully performed at install time in less\nthan one and ten minutes on five out of seven platforms. We achieve an average\nperformance varying from 97% to 100% of the optimum performance depending on\nthe platform. This work is a basis for autotuning the PLASMA library and\nenabling easy performance portability across hardware systems.", 
    "link": "http://arxiv.org/pdf/1102.5328v1", 
    "arxiv-id": "1102.5328v1"
},{
    "category": "cs.DC", 
    "author": "Ravi Sundaram", 
    "title": "Scheduler Vulnerabilities and Attacks in Cloud Computing", 
    "publish": "2011-03-03T19:09:47Z", 
    "summary": "In hardware virtualization a hypervisor provides multiple Virtual Machines\n(VMs) on a single physical system, each executing a separate operating system\ninstance. The hypervisor schedules execution of these VMs much as the scheduler\nin an operating system does, balancing factors such as fairness and I/O\nperformance. As in an operating system, the scheduler may be vulnerable to\nmalicious behavior on the part of users seeking to deny service to others or\nmaximize their own resource usage.\n  Recently, publically available cloud computing services such as Amazon EC2\nhave used virtualization to provide customers with virtual machines running on\nthe provider's hardware, typically charging by wall clock time rather than\nresources consumed. Under this business model, manipulation of the scheduler\nmay allow theft of service at the expense of other customers, rather than\nmerely reallocating resources within the same administrative domain.\n  We describe a flaw in the Xen scheduler allowing virtual machines to consume\nalmost all CPU time, in preference to other users, and demonstrate kernel-based\nand user-space versions of the attack. We show results demonstrating the\nvulnerability in the lab, consuming as much as 98% of CPU time regardless of\nfair share, as well as on Amazon EC2, where Xen modifications protect other\nusers but still allow theft of service. In case of EC2, following the\nresponsible disclosure model, we have reported this vulnerability to Amazon;\nthey have since implemented a fix that we have tested and verified (See\nAppendix B). We provide a novel analysis of the necessary conditions for such\nattacks, and describe scheduler modifications to eliminate the vulnerability.\n  We present experimental results demonstrating the effectiveness of these\ndefenses while imposing negligible overhead.", 
    "link": "http://arxiv.org/pdf/1103.0759v1", 
    "arxiv-id": "1103.0759v1"
},{
    "category": "cs.DC", 
    "author": "Ms. Archana B. Saxena", 
    "title": "Framework to Solve Load Balancing Problem in Heterogeneous Web Servers", 
    "publish": "2011-03-07T07:40:22Z", 
    "summary": "For popular websites most important concern is to handle incoming load\ndynamically among web servers, so that they can respond to their client without\nany wait or failure. Different websites use different strategies to distribute\nload among web servers but most of the schemes concentrate on only one factor\nthat is number of requests, but none of the schemes consider the point that\ndifferent type of requests will require different level of processing efforts\nto answer, status record of all the web servers that are associated with one\ndomain name and mechanism to handle a situation when one of the servers is not\nworking. Therefore, there is a fundamental need to develop strategy for dynamic\nload allocation on web side. In this paper, an effort has been made to\nintroduce a cluster based frame work to solve load distribution problem. This\nframework aims to distribute load among clusters on the basis of their\noperational capabilities. Moreover, the experimental results are shown with the\nhelp of example, algorithm and analysis of the algorithm.", 
    "link": "http://arxiv.org/pdf/1103.1207v1", 
    "arxiv-id": "1103.1207v1"
},{
    "category": "cs.DC", 
    "author": "Srivatsan Ravi", 
    "title": "On the Cost of Concurrency in Transactional Memory", 
    "publish": "2011-03-07T15:37:44Z", 
    "summary": "The crux of software transactional memory (STM) is to combine an easy-to-use\nprogramming interface with an efficient utilization of the concurrent-computing\nabilities provided by modern machines. But does this combination come with an\ninherent cost? We evaluate the cost of concurrency by measuring the amount of\nexpensive synchronization that must be employed in an STM implementation that\nensures positive concurrency, i.e., allows for concurrent transaction\nprocessing in some executions. We focus on two popular progress conditions that\nprovide positive concurrency: progressiveness and permissiveness. We show that\nin permissive STMs, providing a very high degree of concurrency, a transaction\nperforms a linear number of expensive synchronization patterns with respect to\nits read-set size. In contrast, progressive STMs provide a very small degree of\nconcurrency but, as we demonstrate, can be implemented using at most one\nexpensive synchronization pattern per transaction. However, we show that even\nin progressive STMs, a transaction has to \"protect\" (e.g., by using locks or\nstrong synchronization primitives) a linear amount of data with respect to its\nwrite-set size. Our results suggest that looking for high degrees of\nconcurrency in STM implementations may bring a considerable synchronization\ncost.", 
    "link": "http://arxiv.org/pdf/1103.1302v9", 
    "arxiv-id": "1103.1302v9"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Aneka Cloud Application Platform and Its Integration with Windows Azure", 
    "publish": "2011-03-14T06:38:11Z", 
    "summary": "Aneka is an Application Platform-as-a-Service (Aneka PaaS) for Cloud\nComputing. It acts as a framework for building customized applications and\ndeploying them on either public or private Clouds. One of the key features of\nAneka is its support for provisioning resources on different public Cloud\nproviders such as Amazon EC2, Windows Azure and GoGrid. In this chapter, we\nwill present Aneka platform and its integration with one of the public Cloud\ninfrastructures, Windows Azure, which enables the usage of Windows Azure\nCompute Service as a resource provider of Aneka PaaS. The integration of the\ntwo platforms will allow users to leverage the power of Windows Azure Platform\nfor Aneka Cloud Computing, employing a large number of compute instances to run\ntheir applications in parallel. Furthermore, customers of the Windows Azure\nplatform can benefit from the integration with Aneka PaaS by embracing the\nadvanced features of Aneka in terms of multiple programming models, scheduling\nand management services, application execution services, accounting and pricing\nservices and dynamic provisioning services. Finally, in addition to the Windows\nAzure Platform we will illustrate in this chapter the integration of Aneka PaaS\nwith other public Cloud platforms such as Amazon EC2 and GoGrid, and virtual\nmachine management platforms such as Xen Server. The new support of\nprovisioning resources on Windows Azure once again proves the adaptability,\nextensibility and flexibility of Aneka.", 
    "link": "http://arxiv.org/pdf/1103.2590v1", 
    "arxiv-id": "1103.2590v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Self-Stabilization, Byzantine Containment, and Maximizable Metrics:   Necessary Conditions", 
    "publish": "2011-03-17T20:37:17Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties leads to some impossibility results. In this paper, we provide two\nnecessary conditions to construct maximum metric tree in presence of transients\nand (permanent) Byzantine faults.", 
    "link": "http://arxiv.org/pdf/1103.3515v1", 
    "arxiv-id": "1103.3515v1"
},{
    "category": "cs.DC", 
    "author": "Ulrich Schmid", 
    "title": "Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems", 
    "publish": "2011-03-18T17:31:41Z", 
    "summary": "Despite of being quite similar agreement problems, consensus and general\nk-set agreement require surprisingly different techniques for proving the\nimpossibility in asynchronous systems with crash failures: Rather than\nrelatively simple bivalence arguments as in the impossibility proof for\nconsensus (= 1-set agreement) in the presence of a single crash failure, known\nproofs for the impossibility of k-set agreement in systems with at least k>1\ncrash failures use algebraic topology or a variant of Sperner's Lemma. In this\npaper, we present a generic theorem for proving the impossibility of k-set\nagreement in various message passing settings, which is based on a simple\nreduction to the consensus impossibility in a certain subsystem. We demonstrate\nthe broad applicability of our result by exploring the\npossibility/impossibility border of k-set agreement in several message-passing\nsystem models: (i) asynchronous systems with crash failures, (ii) partially\nsynchronous processes with (initial) crash failures, and (iii) asynchronous\nsystems augmented with failure detectors. In (i) and (ii), the impossibility\npart is just an instantiation of our main theorem, whereas the possibility of\nachieving k-set agreement in (ii) follows by generalizing the consensus\nalgorithm for initial crashes by Fisher, Lynch and Patterson. In (iii),\napplying our technique yields the exact border for the parameter k where k-set\nagreement is solvable with the failure detector class (Sigma_k,Omega_k), for\n(1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be\nnecessary for solving k-set agreement, this result yields new insights on the\nquest for the weakest failure detector.", 
    "link": "http://arxiv.org/pdf/1103.3671v2", 
    "arxiv-id": "1103.3671v2"
},{
    "category": "cs.DC", 
    "author": "Philipp Woelfel", 
    "title": "Linearizable Implementations Do Not Suffice for Randomized Distributed   Computation", 
    "publish": "2011-03-24T07:54:19Z", 
    "summary": "Linearizability is the gold standard among algorithm designers for deducing\nthe correctness of a distributed algorithm using implemented shared objects\nfrom the correctness of the corresponding algorithm using atomic versions of\nthe same objects. We show that linearizability does not suffice for this\npurpose when processes can exploit randomization, and we discuss the existence\nof alternative correctness conditions.", 
    "link": "http://arxiv.org/pdf/1103.4690v2", 
    "arxiv-id": "1103.4690v2"
},{
    "category": "cs.DC", 
    "author": "Md. Mahbubul Alam Joarder", 
    "title": "Load Balancing in a Networked Environment through Homogenization", 
    "publish": "2011-03-29T19:56:06Z", 
    "summary": "Distributed processing across a networked environment suffers from\nunpredictable behavior of speedup due to heterogeneous nature of the hardware\nand software in the remote machines. It is challenging to get a better\nperformance from a distributed system by distributing task in an intelligent\nmanner such that the heterogeneous nature of the system do not have any effect\non the speedup ratio. This paper introduces homogenization, a technique that\ndistributes and balances the workload in such a manner that the user gets the\nhighest speedup possible from a distributed environment. Along with providing\nbetter performance, homogenization is totally transparent to the user and\nrequires no interaction with the system.", 
    "link": "http://arxiv.org/pdf/1103.5743v1", 
    "arxiv-id": "1103.5743v1"
},{
    "category": "cs.DC", 
    "author": "Debzani Deb", 
    "title": "Triangular Dynamic Architecture for Distributed Computing in a LAN   Environment", 
    "publish": "2011-03-29T20:05:50Z", 
    "summary": "A computationally intensive large job, granulized to concurrent pieces and\noperating in a dynamic environment should reduce the total processing time.\nHowever, distributing jobs across a networked environment is a tedious and\ndifficult task. Job distribution in a Local Area Network based on Triangular\nDynamic Architecture (TDA) is a mechanism that establishes a dynamic\nenvironment for job distribution, load balancing and distributed processing\nwith minimum interaction from the user. This paper introduces TDA and discusses\nits architecture and shows the benefits gained by utilizing such architecture\nin a distributed computing environment.", 
    "link": "http://arxiv.org/pdf/1103.5760v1", 
    "arxiv-id": "1103.5760v1"
},{
    "category": "cs.DC", 
    "author": "Md. Mahbubul Alam Joarder", 
    "title": "Agent Based Processing of Global Evaluation Function", 
    "publish": "2011-03-29T20:15:48Z", 
    "summary": "Load balancing across a networked environment is a monotonous job. Moreover,\nif the job to be distributed is a constraint satisfying one, the distribution\nof load demands core intelligence. This paper proposes parallel processing\nthrough Global Evaluation Function by means of randomly initialized agents for\nsolving Constraint Satisfaction Problems. A potential issue about the number of\nagents in a machine under the invocation of distribution is discussed here for\nsecuring the maximum benefit from Global Evaluation and parallel processing.\nThe proposed system is compared with typical solution that shows an exclusive\noutcome supporting the nobility of parallel implementation of Global Evaluation\nFunction with certain number of agents in each invoked machine.", 
    "link": "http://arxiv.org/pdf/1103.5764v1", 
    "arxiv-id": "1103.5764v1"
},{
    "category": "cs.DC", 
    "author": "Philipp Woelfel", 
    "title": "The Space Complexity of Long-lived and One-Shot Timestamp   Implementations", 
    "publish": "2011-03-29T23:43:21Z", 
    "summary": "This paper is concerned with the problem of implementing an unbounded\ntimestamp object from multi-writer atomic registers, in an asynchronous\ndistributed system of n processors with distinct identifiers where timestamps\nare taken from an arbitrary universe. Ellen, Fatourou and Ruppert (2008) showed\nthat sqrt{n}/2-O(1) registers are required for any obstruction-free\nimplementation of long-lived timestamp systems from atomic registers (meaning\nprocessors can repeatedly get timestamps). We improve this existing lower bound\nin two ways. First we establish a lower bound of n/6 - O(1) registers for the\nobstruction-free long-lived timestamp problem. Previous such linear lower\nbounds were only known for constrained versions of the timestamp problem. This\nbound is asymptotically tight; Ellen, Fatourou and Ruppert (2008) constructed a\nwait-free algorithm that uses n-1 registers. Second we show that sqrt{n} - O(1)\nregisters are required for any obstruction-free implementation of one-shot\ntimestamp systems(meaning each processor can get a timestamp at most once). We\nshow that this bound is also asymptotically tight by providing a wait-free\none-shot timestamp system that uses fewer than 2 sqrt{n} registers, thus\nestablishing a space complexity gap between one-shot and long-lived timestamp\nsystems.", 
    "link": "http://arxiv.org/pdf/1103.5794v2", 
    "arxiv-id": "1103.5794v2"
},{
    "category": "cs.DC", 
    "author": "Lei Wang", 
    "title": "Automatic Performance Debugging of SPMD-style Parallel Programs", 
    "publish": "2011-03-31T05:38:39Z", 
    "summary": "The simple program and multiple data (SPMD) programming model is widely used\nfor both high performance computing and Cloud computing. In this paper, we\ndesign and implement an innovative system, AutoAnalyzer, that automates the\nprocess of debugging performance problems of SPMD-style parallel programs,\nincluding data collection, performance behavior analysis, locating bottlenecks,\nand uncovering their root causes. AutoAnalyzer is unique in terms of two\nfeatures: first, without any apriori knowledge, it automatically locates\nbottlenecks and uncovers their root causes for performance optimization;\nsecond, it is lightweight in terms of the size of performance data to be\ncollected and analyzed. Our contributions are three-fold: first, we propose two\neffective clustering algorithms to investigate the existence of performance\nbottlenecks that cause process behavior dissimilarity or code region behavior\ndisparity, respectively; meanwhile, we present two searching algorithms to\nlocate bottlenecks; second, on a basis of the rough set theory, we propose an\ninnovative approach to automatically uncovering root causes of bottlenecks;\nthird, on the cluster systems with two different configurations, we use two\nproduction applications, written in Fortran 77, and one open source\ncode-MPIBZIP2 (http://compression.ca/mpibzip2/), written in C++, to verify the\neffectiveness and correctness of our methods. For three applications, we also\npropose an experimental approach to investigating the effects of different\nmetrics on locating bottlenecks.", 
    "link": "http://arxiv.org/pdf/1103.6087v1", 
    "arxiv-id": "1103.6087v1"
},{
    "category": "cs.DC", 
    "author": "Karin Strauss", 
    "title": "The Impact of Memory Models on Software Reliability in Multiprocessors", 
    "publish": "2011-03-31T07:43:58Z", 
    "summary": "The memory consistency model is a fundamental system property characterizing\na multiprocessor. The relative merits of strict versus relaxed memory models\nhave been widely debated in terms of their impact on performance, hardware\ncomplexity and programmability. This paper adds a new dimension to this\ndiscussion: the impact of memory models on software reliability. By allowing\nsome instructions to reorder, weak memory models may expand the window between\ncritical memory operations. This can increase the chance of an undesirable\nthread-interleaving, thus allowing an otherwise-unlikely concurrency bug to\nmanifest. To explore this phenomenon, we define and study a probabilistic model\nof shared-memory parallel programs that takes into account such reordering. We\nuse this model to formally derive bounds on the \\emph{vulnerability} to\nconcurrency bugs of different memory models. Our results show that for 2 (or a\nsmall constant number of) concurrent threads, weaker memory models do indeed\nhave a higher likelihood of allowing bugs. On the other hand, we show that as\nthe number of parallel threads increases, the gap between the different memory\nmodels becomes proportionally insignificant. This suggests the\ncounter-intuitive rule that \\emph{as the number of parallel threads in the\nsystem increases, the importance of using a strict memory model diminishes};\nwhich potentially has major implications on the choice of memory consistency\nmodels in future multi-core systems.", 
    "link": "http://arxiv.org/pdf/1103.6114v2", 
    "arxiv-id": "1103.6114v2"
},{
    "category": "cs.DC", 
    "author": "Boris D. Lubachevsky", 
    "title": "Why The Results of Parallel and Serial Monte Carlo Simulations May   Differ", 
    "publish": "2011-04-01T15:26:43Z", 
    "summary": "Parallel Monte Carlo simulations often expose faults in random number\ngenerators", 
    "link": "http://arxiv.org/pdf/1104.0198v1", 
    "arxiv-id": "1104.0198v1"
},{
    "category": "cs.DC", 
    "author": "\u00dcmit V. \u00c7ataly\u00fcrek", 
    "title": "Load-Balancing Spatially Located Computations using Rectangular   Partitions", 
    "publish": "2011-04-13T18:08:57Z", 
    "summary": "Distributing spatially located heterogeneous workloads is an important\nproblem in parallel scientific computing. We investigate the problem of\npartitioning such workloads (represented as a matrix of non-negative integers)\ninto rectangles, such that the load of the most loaded rectangle (processor) is\nminimized. Since finding the optimal arbitrary rectangle-based partition is an\nNP-hard problem, we investigate particular classes of solutions: rectilinear,\njagged and hierarchical. We present a new class of solutions called m-way\njagged partitions, propose new optimal algorithms for m-way jagged partitions\nand hierarchical partitions, propose new heuristic algorithms, and provide\nworst case performance analyses for some existing and new heuristics. Moreover,\nthe algorithms are tested in simulation on a wide set of instances. Results\nshow that two of the algorithms we introduce lead to a much better load balance\nthan the state-of-the-art algorithms. We also show how to design a two-phase\nalgorithm that reaches different time/quality tradeoff.", 
    "link": "http://arxiv.org/pdf/1104.2566v1", 
    "arxiv-id": "1104.2566v1"
},{
    "category": "cs.DC", 
    "author": "Juha Koivisto", 
    "title": "Extending and Implementing the Self-adaptive Virtual Processor for   Distributed Memory Architectures", 
    "publish": "2011-04-19T20:44:19Z", 
    "summary": "Many-core architectures of the future are likely to have distributed memory\norganizations and need fine grained concurrency management to be used\neffectively. The Self-adaptive Virtual Processor (SVP) is an abstract\nconcurrent programming model which can provide this, but the model and its\ncurrent implementations assume a single address space shared memory. We\ninvestigate and extend SVP to handle distributed environments, and discuss a\nprototype SVP implementation which transparently supports execution on\nheterogeneous distributed memory clusters over TCP/IP connections, while\nretaining the original SVP programming model.", 
    "link": "http://arxiv.org/pdf/1104.3876v1", 
    "arxiv-id": "1104.3876v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Communication Optimalement Stabilisante sur Canaux non Fiables et non   FIFO", 
    "publish": "2011-04-20T06:08:34Z", 
    "summary": "A self-stabilizing protocol has the capacity to recover a legitimate behavior\nwhatever is its initial state. The majority of works in self-stabilization\nassume a shared memory model or a communication using reliable and FIFO\nchannels. In this article, we interest in self-stabilizing systems using\nbounded but non reliable and non FIFO channels. We propose a stabilizing\ncommunication protocol with optimal fault resilience. In more details, this\nprotocol simulates a reliable and FIFO channel and ensures a minimal number of\nlooses, duplications, creations, and re-ordering of messages.", 
    "link": "http://arxiv.org/pdf/1104.3947v1", 
    "arxiv-id": "1104.3947v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Auto-Stabilisation et Confinement de Fautes Malicieuses : Optimalit\u00e9   du Protocole min+1", 
    "publish": "2011-04-20T13:45:26Z", 
    "summary": "A self-stabilizing is naturally resilient to transients faults (that is,\nfaults of finite duration). Recently, a new class of protocol appears. These\nprotocols are self-stabilizing and are moreover resilient to a limited number\nof permanent faults. In this article, we interest in self-stabilizing protocols\nthat tolerate very hard permanent faults: Byzantine faults. We introduce two\nnew scheme of Byzantine containment in self-stabilizing systems. We show that,\nfor the problem of BFS spanning tree construction, the well known\nself-stabilizing protocol min+1 provides without significant modification the\nbest Byzantine containment with respect to these new schemes.", 
    "link": "http://arxiv.org/pdf/1104.4022v1", 
    "arxiv-id": "1104.4022v1"
},{
    "category": "cs.DC", 
    "author": "Karthik Sukumar", 
    "title": "Platforms for Building and Deploying Applications for Cloud Computing", 
    "publish": "2011-04-22T02:51:54Z", 
    "summary": "Cloud computing is rapidly emerging as a new paradigm for delivering IT\nservices as utlity-oriented services on subscription-basis. The rapid\ndevelopment of applications and their deployment in Cloud computing\nenvironments in efficient manner is a complex task. In this article, we give a\nbrief introduction to Cloud computing technology and Platform as a Service, we\nexamine the offerings in this category, and provide the basis for helping\nreaders to understand basic application platform opportunities in Cloud by\ntechnologies such as Microsoft Azure, Sales Force, Google App, and Aneka for\nCloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application\nPlatform (CAP) leveraging these concepts and allowing an easy development of\nCloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers\nfacilities for quickly developing Cloud applications and a modular platform\nwhere additional services can be easily integrated to extend the system\ncapabilities, thus being at pace with the rapidly evolution of Cloud computing.", 
    "link": "http://arxiv.org/pdf/1104.4379v1", 
    "arxiv-id": "1104.4379v1"
},{
    "category": "cs.DC", 
    "author": "Yves Robert", 
    "title": "Tiled QR factorization algorithms", 
    "publish": "2011-04-22T16:45:02Z", 
    "summary": "This work revisits existing algorithms for the QR factorization of\nrectangular matrices composed of p-by-q tiles, where p >= q. Within this\nframework, we study the critical paths and performance of algorithms such as\nSameh and Kuck, Modi and Clarke, Greedy, and those found within PLASMA.\nAlthough neither Modi and Clarke nor Greedy is optimal, both are shown to be\nasymptotically optimal for all matrices of size p = q^2 f(q), where f is any\nfunction such that \\lim_{+\\infty} f= 0. This novel and important complexity\nresult applies to all matrices where p and q are proportional, p = \\lambda q,\nwith \\lambda >= 1, thereby encompassing many important situations in practice\n(least squares). We provide an extensive set of experiments that show the\nsuperiority of the new algorithms for tall matrices.", 
    "link": "http://arxiv.org/pdf/1104.4475v1", 
    "arxiv-id": "1104.4475v1"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Maximum Metric Spanning Tree made Byzantine Tolerant", 
    "publish": "2011-04-28T12:14:53Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. This paper focus on systems that are both\nself-stabilizing and Byzantine tolerant. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties is known to induce many impossibility results. In this paper, we\nprovide first two impossibility results about the construction of maximum\nmetric tree in presence of transients and (permanent) Byzantine faults. Then,\nwe provide a new self-stabilizing protocol that provides optimal containment of\nan arbitrary number of Byzantine faults.", 
    "link": "http://arxiv.org/pdf/1104.5368v1", 
    "arxiv-id": "1104.5368v1"
},{
    "category": "cs.DC", 
    "author": "Lucas Gerin", 
    "title": "On the algebraic numbers computable by some generalized Ehrenfest urns", 
    "publish": "2011-04-29T15:00:08Z", 
    "summary": "This article deals with some stochastic population protocols, motivated by\ntheoretical aspects of distributed computing. We modelize the problem by a\nlarge urn of black and white balls from which at every time unit a fixed number\nof balls are drawn and their colors are changed according to the number of\nblack balls among them. When the time and the number of balls both tend to\ninfinity the proportion of black balls converges to an algebraic number. We\nprove that, surprisingly enough, not every algebraic number can be \"computed\"\nthis way.", 
    "link": "http://arxiv.org/pdf/1104.5643v2", 
    "arxiv-id": "1104.5643v2"
},{
    "category": "cs.DC", 
    "author": "S\u00e9bastien Tixeuil", 
    "title": "Asynchronous mobile robot gathering from symmetric configurations   without global multiplicity detection", 
    "publish": "2011-04-29T15:20:45Z", 
    "summary": "We consider a set of k autonomous robots that are endowed with visibility\nsensors (but that are otherwise unable to communicate) and motion actuators.\nThose robots must collaborate to reach a sin- gle vertex that is unknown\nbeforehand, and to remain there hereafter. Previous works on gathering in\nring-shaped networks suggest that there exists a tradeoff between the size of\nthe set of potential initial configurations, and the power of the sensing\ncapabilities of the robots (i.e. the larger the initial configuration set, the\nmost powerful the sensor needs to be). We prove that there is no such trade\noff. We propose a gathering protocol for an odd number of robots in a\nring-shaped network that allows symmetric but not periodic configurations as\ninitial configurations, yet uses only local weak multiplicity detection. Robots\nare assumed to be anonymous and oblivious, and the execution model is the\nnon-atomic CORDA model with asynchronous fair scheduling. Our protocol allows\nthe largest set of initial configurations (with respect to impossibility\nresults) yet uses the weakest multiplicity detector to date. The time\ncomplexity of our protocol is O(n2), where n denotes the size of the ring.\nCompared to previous work that also uses local weak multiplicity detection, we\ndo not have the constraint that k < n/2 (here, we simply have 2 < k < n - 3).", 
    "link": "http://arxiv.org/pdf/1104.5660v1", 
    "arxiv-id": "1104.5660v1"
},{
    "category": "cs.DC", 
    "author": "Pradeep Teregowda", 
    "title": "Decision Support Tools for Cloud Migration in the Enterprise", 
    "publish": "2011-05-01T07:48:42Z", 
    "summary": "This paper describes two tools that aim to support decision making during the\nmigration of IT systems to the cloud. The first is a modeling tool that\nproduces cost estimates of using public IaaS clouds. The tool enables IT\narchitects to model their applications, data and infrastructure requirements in\naddition to their computational resource usage patterns. The tool can be used\nto compare the cost of different cloud providers, deployment options and usage\nscenarios. The second tool is a spreadsheet that outlines the benefits and\nrisks of using IaaS clouds from an enterprise perspective; this tool provides a\nstarting point for risk assessment. Two case studies were used to evaluate the\ntools. The tools were useful as they informed decision makers about the costs,\nbenefits and risks of using the cloud.", 
    "link": "http://arxiv.org/pdf/1105.0149v1", 
    "arxiv-id": "1105.0149v1"
},{
    "category": "cs.DC", 
    "author": "Yang D. Li", 
    "title": "A Formal Model of Anonymous Systems", 
    "publish": "2011-05-02T10:44:12Z", 
    "summary": "We put forward a formal model of anonymous systems. And we concentrate on the\nanonymous failure detectors in our model. In particular, we give three examples\nof anonymous failure detectors and show that they can be used to solve the\nconsensus problem and that they are equivalent to their classic counterparts.\nMoreover, we show some relationship among them and provide a simple\nclassification of anonymous failure detectors.", 
    "link": "http://arxiv.org/pdf/1105.0296v1", 
    "arxiv-id": "1105.0296v1"
},{
    "category": "cs.DC", 
    "author": "Stefano Ferretti", 
    "title": "On the Degree Distribution of Faulty Peer-to-Peer Overlays", 
    "publish": "2011-05-03T11:02:37Z", 
    "summary": "This paper presents an analytical framework to model fault-tolerance in\nunstructured peer-to-peer overlays, represented as complex networks. We define\na distributed protocol peers execute for managing the overlay and reacting to\nnode faults. Based on the protocol, evolution equations are defined and\nmanipulated by resorting to generating functions. Obtained outcomes provide\ninsights on the nodes' degree probability distribution. From the study of the\ndegree distribution, it is possible to estimate other important metrics of the\npeer-to-peer overlay, such as the diameter of the network. We study different\nnetworks, characterized by three specific desired degree distributions, i.e.\nnets with nodes having a fixed desired degree, random graphs and scale-free\nnetworks. All these networks are assessed via the analytical tool and\nsimulation as well. Results show that the approach can be factually employed to\ndynamically tune the average attachment rate at peers so that they maintain\ntheir own desired degree and, in general, the desired network topology.", 
    "link": "http://arxiv.org/pdf/1105.0545v4", 
    "arxiv-id": "1105.0545v4"
},{
    "category": "cs.DC", 
    "author": "Anil K. Ghosh", 
    "title": "Secure Position Verification for Wireless Sensor Networks in Noisy   Channels", 
    "publish": "2011-05-03T19:59:13Z", 
    "summary": "Position verification in wireless sensor networks (WSNs) is quite tricky in\npresence of attackers (malicious sensor nodes), who try to break the\nverification protocol by reporting their incorrect positions (locations) during\nthe verification stage. In the literature of WSNs, most of the existing methods\nof position verification have used trusted verifiers, which are in fact\nvulnerable to attacks by malicious nodes. They also depend on some distance\nestimation techniques, which are not accurate in noisy channels (mediums). In\nthis article, we propose a secure position verification scheme for WSNs in\nnoisy channels without relying on any trusted entities. Our verification scheme\ndetects and filters out all malicious nodes from the network with very high\nprobability.", 
    "link": "http://arxiv.org/pdf/1105.0668v2", 
    "arxiv-id": "1105.0668v2"
},{
    "category": "cs.DC", 
    "author": "Pascal Frossard", 
    "title": "Progressive quantization in distributed average consensus", 
    "publish": "2011-05-05T13:56:48Z", 
    "summary": "We consider the problem of distributed average consensus in a sensor network\nwhere sensors exchange quantized information with their neighbors. We propose a\nnovel quantization scheme that exploits the increasing correlation between the\nvalues exchanged by the sensors throughout the iterations of the consensus\nalgorithm. A low complexity, uniform quantizer is implemented in each sensor,\nand refined quantization is achieved by progressively reducing the quantization\nintervals during the convergence of the consensus algorithm. We propose a\nrecurrence relation for computing the quantization parameters that depend on\nthe network topology and the communication rate. We further show that the\nrecurrence relation can lead to a simple exponential model for the size of the\nquantization step size over the iterations, whose parameters can be computed a\npriori. Finally, simulation results demonstrate the effectiveness of the\nprogressive quantization scheme that leads to the consensus solution even at\nlow communication rate.", 
    "link": "http://arxiv.org/pdf/1105.1074v2", 
    "arxiv-id": "1105.1074v2"
},{
    "category": "cs.DC", 
    "author": "Peter Pietrzyk", 
    "title": "A Distributed Approximation Algorithm for the Metric Uncapacitated   Facility Location Problem in the Congest Model", 
    "publish": "2011-05-06T09:36:03Z", 
    "summary": "We present a randomized distributed approximation algorithm for the metric\nuncapacitated facility location problem. The algorithm is executed on a\nbipartite graph in the Congest model yielding a (1.861 + epsilon) approximation\nfactor, where epsilon is an arbitrary small positive constant. It needs\nO(n^{3/4}log_{1+epsilon}^2(n) communication rounds with high probability (n\ndenoting the number of facilities and clients). To the best of our knowledge,\nour algorithm currently has the best approximation factor for the facility\nlocation problem in a distributed setting. It is based on a greedy sequential\napproximation algorithm by Jain et al. (J. ACM 50(6), pages: 795-824, 2003).\nThe main difficulty in executing this sequential algorithm lies in dealing with\nsituations, where multiple facilities are eligible for opening, but (in order\nto preserve the approximation factor of the sequential algorithm) only a subset\nof them can actually be opened. Note that while the presented runtime bound of\nour algorithm is \"with high probability\", the approximation factor is not \"in\nexpectation\" but always guaranteed to be (1.861 + epsilon). Thus, our main\ncontribution is a sublinear time selection mechanism that, while increasing the\napproximation factor by an arbitrary small additive term, allows us to decide\nwhich of the eligible facilities to open.", 
    "link": "http://arxiv.org/pdf/1105.1248v1", 
    "arxiv-id": "1105.1248v1"
},{
    "category": "cs.DC", 
    "author": "Sharad Mehrotra", 
    "title": "Secure Data Processing in a Hybrid Cloud", 
    "publish": "2011-05-10T15:49:38Z", 
    "summary": "Cloud computing has made it possible for a user to be able to select a\ncomputing service precisely when needed. However, certain factors such as\nsecurity of data and regulatory issues will impact a user's choice of using\nsuch a service. A solution to these problems is the use of a hybrid cloud that\ncombines a user's local computing capabilities (for mission- or\norganization-critical tasks) with a public cloud (for less influential tasks).\nWe foresee three challenges that must be overcome before the adoption of a\nhybrid cloud approach: 1) data design: How to partition relations in a hybrid\ncloud? The solution to this problem must account for the sensitivity of\nattributes in a relation as well as the workload of a user; 2) data security:\nHow to protect a user's data in a public cloud with encryption while enabling\nquery processing over this encrypted data? and 3) query processing: How to\nexecute queries efficiently over both, encrypted and unencrypted data? This\npaper addresses these challenges and incorporates their solutions into an\nadd-on tool for a Hadoop and Hive based cloud computing infrastructure.", 
    "link": "http://arxiv.org/pdf/1105.1982v1", 
    "arxiv-id": "1105.1982v1"
},{
    "category": "cs.DC", 
    "author": "Larbi Esmahi", 
    "title": "A Cloud-based Approach for Context Information Provisioning", 
    "publish": "2011-05-10T18:02:56Z", 
    "summary": "As a result of the phenomenal proliferation of modern mobile Internet-enabled\ndevices and the widespread utilization of wireless and cellular data networks,\nmobile users are increasingly requiring services tailored to their current\ncontext. High-level context information is typically obtained from context\nservices that aggregate raw context information sensed by various sensors and\nmobile devices. Given the massive amount of sensed data, traditional context\nservices are lacking the necessary resources to store and process these data,\nas well as to disseminate high-level context information to a variety of\npotential context consumers. In this paper, we propose a novel framework for\ncontext information provisioning, which relies on deploying context services on\nthe cloud and using context brokers to mediate between context consumers and\ncontext services using a publish/subscribe model. Moreover, we describe a\nmulti-attributes decision algorithm for the selection of potential context\nservices that can fulfill context consumers' requests for context information.\nThe algorithm calculates the score of each context service, per context\ninformation type, based on the quality-of-service (QoS) and quality-of-context\ninformation (QoC) requirements expressed by the context consumer. One of the\nbenefits of the approach is that context providers can scale up and down, in\nterms of cloud resources they use, depending on current demand for context\ninformation. Besides, the selection algorithm allows ranking context services\nby matching their QoS and QoC offers against the QoS and QoC requirements of\nthe context consumer.", 
    "link": "http://arxiv.org/pdf/1105.2213v1", 
    "arxiv-id": "1105.2213v1"
},{
    "category": "cs.DC", 
    "author": "Gabriele D'Angelo", 
    "title": "Parallel and Distributed Simulation from Many Cores to the Public Cloud   (Extended Version)", 
    "publish": "2011-05-11T20:01:11Z", 
    "summary": "In this tutorial paper, we will firstly review some basic simulation concepts\nand then introduce the parallel and distributed simulation techniques in view\nof some new challenges of today and tomorrow. More in particular, in the last\nyears there has been a wide diffusion of many cores architectures and we can\nexpect this trend to continue. On the other hand, the success of cloud\ncomputing is strongly promoting the everything as a service paradigm. Is\nparallel and distributed simulation ready for these new challenges? The current\napproaches present many limitations in terms of usability and adaptivity: there\nis a strong need for new evaluation metrics and for revising the currently\nimplemented mechanisms. In the last part of the paper, we propose a new\napproach based on multi-agent systems for the simulation of complex systems. It\nis possible to implement advanced techniques such as the migration of simulated\nentities in order to build mechanisms that are both adaptive and very easy to\nuse. Adaptive mechanisms are able to significantly reduce the communication\ncost in the parallel/distributed architectures, to implement load-balance\ntechniques and to cope with execution environments that are both variable and\ndynamic. Finally, such mechanisms will be used to build simulations on top of\nunreliable cloud services.", 
    "link": "http://arxiv.org/pdf/1105.2301v4", 
    "arxiv-id": "1105.2301v4"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "Workload Classification & Software Energy Measurement for Efficient   Scheduling on Private Cloud Platforms", 
    "publish": "2011-05-12T22:00:36Z", 
    "summary": "At present there are a number of barriers to creating an energy efficient\nworkload scheduler for a Private Cloud based data center. Firstly, the\nrelationship between different workloads and power consumption must be\ninvestigated. Secondly, current hardware-based solutions to providing energy\nusage statistics are unsuitable in warehouse scale data centers where low cost\nand scalability are desirable properties. In this paper we discuss the effect\nof different workloads on server power consumption in a Private Cloud platform.\nWe display a noticeable difference in energy consumption when servers are given\ntasks that dominate various resources (CPU, Memory, Hard Disk and Network). We\nthen use this insight to develop CloudMonitor, a software utility that is\ncapable of >95% accurate power predictions from monitoring resource consumption\nof workloads, after a \"training phase\" in which a dynamic power model is\ndeveloped.", 
    "link": "http://arxiv.org/pdf/1105.2584v1", 
    "arxiv-id": "1105.2584v1"
},{
    "category": "cs.DC", 
    "author": "Jean-Luc Dekeyser", 
    "title": "A Modeling Approach based on UML/MARTE for GPU Architecture", 
    "publish": "2011-05-23T07:54:02Z", 
    "summary": "Nowadays, the High Performance Computing is part of the context of embedded\nsystems. Graphics Processing Units (GPUs) are more and more used in\nacceleration of the most part of algorithms and applications. Over the past\nyears, not many efforts have been done to describe abstractions of applications\nin relation to their target architectures. Thus, when developers need to\nassociate applications and GPUs, for example, they find difficulty and prefer\nusing API for these architectures. This paper presents a metamodel extension\nfor MARTE profile and a model for GPU architectures. The main goal is to\nspecify the task and data allocation in the memory hierarchy of these\narchitectures. The results show that this approach will help to generate code\nfor GPUs based on model transformations using Model Driven Engineering (MDE).", 
    "link": "http://arxiv.org/pdf/1105.4424v1", 
    "arxiv-id": "1105.4424v1"
},{
    "category": "cs.DC", 
    "author": "Anissa Lamani", 
    "title": "Robot Networks with Homonyms: The Case of Patterns Formation", 
    "publish": "2011-05-29T19:23:54Z", 
    "summary": "In this paper, we consider the problem of formation of a series of geometric\npatterns [4] by a network of oblivious mobile robots that communicate only\nthrough vision. So far, the problem has been studied in models where robots are\neither assumed to have distinct identifiers or to be completely anonymous. To\ngeneralize these results and to better understand how anonymity affects the\ncomputational power of robots, we study the problem in a new model, introduced\nrecently in [5], in which n robots may share up to 1 <= h <= n different\nidentifiers. We present necessary and sufficient conditions, relating\nsymmetricity and homonymy, that makes the problem solvable. We also show that\nin the case where h = n, making the identifiers of robots invisible does not\nlimit their computational power. This contradicts a result of [4]. To present\nour algorithms, we use a function that computes the Weber point for many\nregular and symmetric configurations. This function is interesting in its own\nright, since the problem of finding Weber points has been solved up to now for\nonly few other patterns.", 
    "link": "http://arxiv.org/pdf/1105.5817v1", 
    "arxiv-id": "1105.5817v1"
},{
    "category": "cs.DC", 
    "author": "Alaa Ismail Elnashar", 
    "title": "Parallel Performance of MPI Sorting Algorithms on Dual-Core Processor   Windows-Based Systems", 
    "publish": "2011-05-30T16:53:36Z", 
    "summary": "Message Passing Interface (MPI) is widely used to implement parallel\nprograms. Although Windowsbased architectures provide the facilities of\nparallel execution and multi-threading, little attention has been focused on\nusing MPI on these platforms. In this paper we use the dual core Window-based\nplatform to study the effect of parallel processes number and also the number\nof cores on the performance of three MPI parallel implementations for some\nsorting algorithms.", 
    "link": "http://arxiv.org/pdf/1105.6040v1", 
    "arxiv-id": "1105.6040v1"
},{
    "category": "cs.DC", 
    "author": "Jason H. Li", 
    "title": "Distributed Stochastic Power Control in Ad-hoc Networks: A Nonconvex   Case", 
    "publish": "2011-06-03T19:50:22Z", 
    "summary": "Utility-based power allocation in wireless ad-hoc networks is inherently\nnonconvex because of the global coupling induced by the co-channel\ninterference. To tackle this challenge, we first show that the globally optimal\npoint lies on the boundary of the feasible region, which is utilized as a basis\nto transform the utility maximization problem into an equivalent max-min\nproblem with more structure. By using extended duality theory, penalty\nmultipliers are introduced for penalizing the constraint violations, and the\nminimum weighted utility maximization problem is then decomposed into\nsubproblems for individual users to devise a distributed stochastic power\ncontrol algorithm, where each user stochastically adjusts its target utility to\nimprove the total utility by simulated annealing. The proposed distributed\npower control algorithm can guarantee global optimality at the cost of slow\nconvergence due to simulated annealing involved in the global optimization. The\ngeometric cooling scheme and suitable penalty parameters are used to improve\nthe convergence rate. Next, by integrating the stochastic power control\napproach with the back-pressure algorithm, we develop a joint scheduling and\npower allocation policy to stabilize the queueing systems. Finally, we\ngeneralize the above distributed power control algorithms to multicast\ncommunications, and show their global optimality for multicast traffic.", 
    "link": "http://arxiv.org/pdf/1106.0736v1", 
    "arxiv-id": "1106.0736v1"
},{
    "category": "cs.DC", 
    "author": "Herodotos Herodotou", 
    "title": "Hadoop Performance Models", 
    "publish": "2011-06-06T00:02:32Z", 
    "summary": "Hadoop MapReduce is now a popular choice for performing large-scale data\nanalytics. This technical report describes a detailed set of mathematical\nperformance models for describing the execution of a MapReduce job on Hadoop.\nThe models describe dataflow and cost information at the fine granularity of\nphases within the map and reduce tasks of a job execution. The models can be\nused to estimate the performance of MapReduce jobs as well as to find the\noptimal configuration settings to use when running the jobs.", 
    "link": "http://arxiv.org/pdf/1106.0940v1", 
    "arxiv-id": "1106.0940v1"
},{
    "category": "cs.DC", 
    "author": "Y. Venkataramani", 
    "title": "Traffic Performance Analysis of Manet Routing Protocol", 
    "publish": "2011-06-07T09:09:17Z", 
    "summary": "The primary objective of this research work is to study and investigate the\nperformance measures of Gossip Routing protocol and Energy Efficient and\nReliable Adaptive Gossip routing protocols. We use TCP and CBR based traffic\nmodels to analyze the performance of above mentioned protocols based on the\nparameters of Packet Delivery Ratio, Average End-to-End Delay and Throughput.\nWe will investigate the effect of change in the simulation time and Number of\nnodes for the MANET routing protocols. For Simulation, we have used ns-2\nsimulator.", 
    "link": "http://arxiv.org/pdf/1106.1286v1", 
    "arxiv-id": "1106.1286v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Byzantine Broadcast in Point-to-Point Networks using Local Linear Coding", 
    "publish": "2011-06-09T16:07:57Z", 
    "summary": "The goal of Byzantine Broadcast (BB) is to allow a set of fault-free nodes to\nagree on information that a source node wants to broadcast to them, in the\npresence of Byzantine faulty nodes. We consider design of efficient algorithms\nfor BB in {\\em synchronous} point-to-point networks, where the rate of\ntransmission over each communication link is limited by its \"link capacity\".\nThe throughput of a particular BB algorithm is defined as the average number of\nbits that can be reliably broadcast to all fault-free nodes per unit time using\nthe algorithm without violating the link capacity constraints. The {\\em\ncapacity} of BB in a given network is then defined as the supremum of all\nachievable BB throughputs in the given network, over all possible BB\nalgorithms.\n  We develop NAB -- a Network-Aware Byzantine broadcast algorithm -- for\narbitrary point-to-point networks consisting of $n$ nodes, wherein the number\nof faulty nodes is at most $f$, $f<n/3$, and the network connectivity is at\nleast $2f+1$. We also prove an upper bound on the capacity of Byzantine\nbroadcast, and conclude that NAB can achieve throughput at least 1/3 of the\ncapacity. When the network satisfies an additional condition, NAB can achieve\nthroughput at least 1/2 of the capacity.\n  To the best of our knowledge, NAB is the first algorithm that can achieve a\nconstant fraction of capacity of Byzantine Broadcast (BB) in arbitrary\npoint-to-point networks.", 
    "link": "http://arxiv.org/pdf/1106.1845v3", 
    "arxiv-id": "1106.1845v3"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures", 
    "publish": "2011-06-09T16:13:54Z", 
    "summary": "In this report, we investigate the multi-valued Byzantine consensus problem.\nWe introduce two algorithms: the first one achieves traditional validity\nrequirement for consensus, and the second one achieves a stronger \"q-validity\"\nrequirement. Both algorithms are more efficient than the ones introduces in our\nrecent PODC 2011 paper titled \"Error-Free Multi-Valued Consensus with Byzantine\nFailures\".", 
    "link": "http://arxiv.org/pdf/1106.1846v1", 
    "arxiv-id": "1106.1846v1"
},{
    "category": "cs.DC", 
    "author": "Benny Sudakov", 
    "title": "Oblivious Collaboration", 
    "publish": "2011-06-10T14:11:33Z", 
    "summary": "Communication is a crucial ingredient in every kind of collaborative work.\nBut what is the least possible amount of communication required for a given\ntask? We formalize this question by introducing a new framework for distributed\ncomputation, called {\\em oblivious protocols}.\n  We investigate the power of this model by considering two concrete examples,\nthe {\\em musical chairs} task $MC(n,m)$ and the well-known {\\em Renaming}\nproblem. The $MC(n,m)$ game is played by $n$ players (processors) with $m$\nchairs. Players can {\\em occupy} chairs, and the game terminates as soon as\neach player occupies a unique chair. Thus we say that player $P$ is {\\em in\nconflict} if some other player $Q$ is occupying the same chair, i.e.,\ntermination means there are no conflicts. By known results from distributed\ncomputing, if $m \\le 2n-2$, no strategy of the players can guarantee\ntermination. However, there is a protocol with $m = 2n-1$ chairs that always\nterminates. Here we consider an oblivious protocol where in every time step the\nonly communication is this: an adversarial {\\em scheduler} chooses an arbitrary\nnonempty set of players, and for each of them provides only one bit of\ninformation, specifying whether the player is currently in conflict or not. A\nplayer notified not to be in conflict halts and never changes its chair,\nwhereas a player notified to be in conflict changes its chair according to its\ndeterministic program. Remarkably, even with this minimal communication\ntermination can be guaranteed with only $m=2n-1$ chairs. Likewise, we obtain an\noblivious protocol for the Renaming problem whose name-space is small as that\nof the optimal nonoblivious distributed protocol.\n  Other aspects suggest themselves, such as the efficiency (program length) of\nour protocols. We make substantial progress here as well, though many\ninteresting questions remain open.", 
    "link": "http://arxiv.org/pdf/1106.2065v1", 
    "arxiv-id": "1106.2065v1"
},{
    "category": "cs.DC", 
    "author": "Ziv Bar-Joseph", 
    "title": "MIS on the fly", 
    "publish": "2011-06-10T17:28:35Z", 
    "summary": "Humans are very good at optimizing solutions for specific problems.\nBiological processes, on the other hand, have evolved to handle multiple\nconstrained distributed environments and so they are robust and adaptable.\nInspired by observations made in a biological system we have recently presented\na simple new randomized distributed MIS algorithm \\cite{ZScience}. Here we\nextend these results by removing a number of strong assumptions that we made,\nmaking the algorithms more practical. Specifically we present an $O(\\log^2 n)$\nrounds synchronous randomized MIS algorithm which uses only 1 bit unary\nmessages (a beeping signal with collision detection), allows for asynchronous\nwake up, does not assume any knowledge of the network topology, and assumes\nonly a loose bound on the network size. We also present an extension with no\ncollision detection in which the round complexity increases to $(\\log^3 n)$.\nFinally, we show that our algorithm is optimal under some restriction, by\npresenting a tight lower bound of $\\Omega(\\log^2 n)$ on the number of rounds\nrequired to construct a MIS for a restricted model.", 
    "link": "http://arxiv.org/pdf/1106.2126v1", 
    "arxiv-id": "1106.2126v1"
},{
    "category": "cs.DC", 
    "author": "Joseph Peters", 
    "title": "Consensus vs Broadcast in Communication Networks with Arbitrary Mobile   Omission Faults", 
    "publish": "2011-06-17T21:16:51Z", 
    "summary": "We compare the solvability of the Consensus and Broadcast problems in\nsynchronous communication networks in which the delivery of messages is not\nreliable. The failure model is the mobile omission faults model. During each\nround, some messages can be lost and the set of possible simultaneous losses is\nthe same for each round. We investigate these problems for the first time for\narbitrary sets of possible failures. Previously, these sets were defined by\nbounding the numbers of failures.\n  In this setting, we present a new necessary condition for the solvability of\nConsensus that unifies previous impossibility results in this area. This\ncondition is expressed using Broadcastability properties. As a very important\napplication, we show that when the sets of omissions that can occur are defined\nby bounding the numbers of failures, counted in any way (locally, globally,\netc.), then the Consensus problem is actually equivalent to the Broadcast\nproblem.", 
    "link": "http://arxiv.org/pdf/1106.3579v2", 
    "arxiv-id": "1106.3579v2"
},{
    "category": "cs.DC", 
    "author": "Guangming Tan", 
    "title": "A New and Efficient Algorithm-Based Fault Tolerance Scheme for A Million   Way Parallelism", 
    "publish": "2011-06-21T14:24:43Z", 
    "summary": "Fault tolerance overhead of high performance computing (HPC) applications is\nbecoming critical to the efficient utilization of HPC systems at large scale.\nHPC applications typically tolerate fail-stop failures by checkpointing.\nAnother promising method is in the algorithm level, called algorithmic\nrecovery. These two methods can achieve high efficiency when the system scale\nis not very large, but will both lose their effectiveness when systems approach\nthe scale of Exaflops, where the number of processors including in system is\nexpected to achieve one million. This paper develops a new and efficient\nalgorithm-based fault tolerance scheme for HPC applications. When failure\noccurs during the execution, we do not stop to wait for the recovery of\ncorrupted data, but replace them with the corresponding redundant data and\ncontinue the execution. A background accelerated recovery method is also\nproposed to rebuild redundancy to tolerate multiple times of failures during\nthe execution. To demonstrate the feasibility of our new scheme, we have\nincorporated it to the High Performance Linpack. Theoretical analysis\ndemonstrates that our new fault tolerance scheme can still be effective even\nwhen the system scale achieves the Exaflops. Experiment using SiCortex SC5832\nverifies the feasibility of the scheme, and indicates that the advantage of our\nscheme can be observable even in a small scale.", 
    "link": "http://arxiv.org/pdf/1106.4213v1", 
    "arxiv-id": "1106.4213v1"
},{
    "category": "cs.DC", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "title": "Dynamic Fractional Resource Scheduling vs. Batch Scheduling", 
    "publish": "2011-06-24T14:54:51Z", 
    "summary": "We propose a novel job scheduling approach for homogeneous cluster computing\nplatforms. Its key feature is the use of virtual machine technology to share\nfractional node resources in a precise and controlled manner. Other VM-based\nscheduling approaches have focused primarily on technical issues or on\nextensions to existing batch scheduling systems, while we take a more\naggressive approach and seek to find heuristics that maximize an objective\nmetric correlated with job performance. We derive absolute performance bounds\nand develop algorithms for the online, non-clairvoyant version of our\nscheduling problem. We further evaluate these algorithms in simulation against\nboth synthetic and real-world HPC workloads and compare our algorithms to\nstandard batch scheduling approaches. We find that our approach improves over\nbatch scheduling by orders of magnitude in terms of job stretch, while leading\nto comparable or better resource utilization. Our results demonstrate that\nvirtualization technology coupled with lightweight online scheduling strategies\ncan afford dramatic improvements in performance for executing HPC workloads.", 
    "link": "http://arxiv.org/pdf/1106.4985v1", 
    "arxiv-id": "1106.4985v1"
},{
    "category": "cs.DC", 
    "author": "Corina Stratan", 
    "title": "MONARC Simulation Framework", 
    "publish": "2011-06-25T19:40:21Z", 
    "summary": "This paper discusses the latest generation of the MONARC (MOdels of Networked\nAnalysis at Regional Centers) simulation framework, as a design and modelling\ntool for large scale distributed systems applied to HEP experiments. A\nprocess-oriented approach for discrete event simulation is well-suited for\ndescribing concurrent running programs, as well as the stochastic arrival\npatterns that characterize how such systems are used. The simulation engine is\nbased on Threaded Objects (or Active Objects), which offer great flexibility in\nsimulating the complex behavior of distributed data processing programs. The\nengine provides an appropriate scheduling mechanism for the Active objects with\nsupport for interrupts. This approach offers a natural way of describing\ncomplex running programs that are data dependent and which concurrently compete\nfor shared resources as well as large numbers of concurrent data transfers on\nshared resources. The framework provides a complete set of basic components\n(processing nodes, data servers, network components) together with dynamically\nloadable decision units (scheduling or data replication modules) for easily\nbuilding complex Computing Model simulations. Examples of simulating complex\ndata processing systems are presented, and the way the framework is used to\ncompare different decision making algorithms or to optimize the overall Grid\narchitecture and/or the policies that govern the Grid's use.", 
    "link": "http://arxiv.org/pdf/1106.5158v1", 
    "arxiv-id": "1106.5158v1"
},{
    "category": "cs.DC", 
    "author": "Lucian Musat", 
    "title": "LISA (Localhost Information Service Agent)", 
    "publish": "2011-06-25T20:49:17Z", 
    "summary": "Grid computing has gained an increasing importance in the last years,\nespecially in the academic environments, offering the possibility to rapidly\nsolve complex scientific problems. The monitoring of the Grid jobs has a vital\nimportance for analyzing the system's performance, for providing the users an\nappropriate feed-back, and for obtaining historical data which may be used for\nperformance prediction. Several monitoring systems have been developed, with\ndifferent strategies to collect and store the information. We shall present\nhere a solution based on MonALISA, a distributed service for monitoring,\ncontrol and global optimization of complex systems, and LISA, a component\napplication of MonALISA which can help in optimizing other applications by\nmeans of monitoring services. The advantages of this system are, among others,\nflexibility, dynamic configuration, high communication performance.", 
    "link": "http://arxiv.org/pdf/1106.5168v1", 
    "arxiv-id": "1106.5168v1"
},{
    "category": "cs.DC", 
    "author": "Allison Lewko", 
    "title": "The Contest Between Simplicity and Efficiency in Asynchronous Byzantine   Agreement", 
    "publish": "2011-06-25T20:53:20Z", 
    "summary": "In the wake of the decisive impossibility result of Fischer, Lynch, and\nPaterson for deterministic consensus protocols in the aynchronous model with\njust one failure, Ben-Or and Bracha demonstrated that the problem could be\nsolved with randomness, even for Byzantine failures. Both protocols are natural\nand intuitive to verify, and Bracha's achieves optimal resilience. However, the\nexpected running time of these protocols is exponential in general. Recently,\nKapron, Kempe, King, Saia, and Sanwalani presented the first efficient\nByzantine agreement algorithm in the asynchronous, full information model,\nrunning in polylogarithmic time. Their algorithm is Monte Carlo and drastically\ndeparts from the simple structure of Ben-Or and Bracha's Las Vegas algorithms.\n  In this paper, we begin an investigation of the question: to what extent is\nthis departure necessary? Might there be a much simpler and intuitive Las Vegas\nprotocol that runs in expected polynomial time? We will show that the\nexponential running time of Ben-Or and Bracha's algorithms is no mere accident\nof their specific details, but rather an unavoidable consequence of their\ngeneral symmetry and round structure. We define a natural class of \"fully\nsymmetric round protocols\" for solving Byzantine agreement in an asynchronous\nsetting and show that any such protocol can be forced to run in expected\nexponential time by an adversary in the full information model. We assume the\nadversary controls $t$ Byzantine processors for $t = cn$, where $c$ is an\narbitrary positive constant $< 1/3$. We view our result as a step toward\nidentifying the level of complexity required for a polynomial-time algorithm in\nthis setting, and also as a guide in the search for new efficient algorithms.", 
    "link": "http://arxiv.org/pdf/1106.5170v2", 
    "arxiv-id": "1106.5170v2"
},{
    "category": "cs.DC", 
    "author": "Iosif C. Legrand", 
    "title": "A Distributed Agent Based System to Control and Coordinate Large Scale   Data Transfers", 
    "publish": "2011-06-25T20:54:12Z", 
    "summary": "We present a distributed agent based system used to monitor, configure and\ncontrol complex, large scale data transfers in the Wide Area Network. The\nLocalhost Information Service Agent (LISA) is a lightweight dynamic service\nthat provides complete system and applications monitoring, is capable to\ndynamically configure system parameters and can help in optimizing distributed\napplications.\n  As part of the MonALISA (Monitoring Agents in A Large Integrated Services\nArchitecture) system, LISA is an end host agent capable to collect any type of\nmonitoring information, to distribute them, and to take actions based on local\nor global decision units. The system has been used for the Bandwidth Challenge\nat Supercomputing 2006 to coordinate global large scale data transfers using\nFast Data Transfer (FDT) application between hundreds of servers distributed on\nmajor Grid sites involved in processing High Energy Physics data for the future\nLarge Hadron Collider experiments.", 
    "link": "http://arxiv.org/pdf/1106.5171v1", 
    "arxiv-id": "1106.5171v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "DistHash: A robust P2P DHT-based system for replicated objects", 
    "publish": "2011-06-27T05:49:21Z", 
    "summary": "Over the Internet today, computing and communications environments are\nsignificantly more complex and chaotic than classical distributed systems,\nlacking any centralized organization or hierarchical control. There has been\nmuch interest in emerging Peer-to-Peer (P2P) network overlays because they\nprovide a good substrate for creating large-scale data sharing, content\ndistribution and application-level multicast applications. In this paper we\npresent DistHash, a P2P overlay network designed to share large sets of\nreplicated distributed objects in the context of large-scale highly dynamic\ninfrastructures. We present original solutions to achieve optimal message\nrouting in hop-count and throughput, provide an adequate consistency approach\namong replicas, as well as provide a fault-tolerant substrate.", 
    "link": "http://arxiv.org/pdf/1106.5299v1", 
    "arxiv-id": "1106.5299v1"
},{
    "category": "cs.DC", 
    "author": "Vladimir Cretu", 
    "title": "Towards an IO intensive Grid application instrumentation in MedioGRID", 
    "publish": "2011-06-27T06:03:28Z", 
    "summary": "Obtaining high performance in IO intensive applications requires systems that\nsupport reliable fast transfer, data replication, and caching. In this paper we\npresent an architecture designed for supporting IO intensive applications in\nMedioGRID, a system for real-time processing of satellite images, operating in\na Grid environment. The solution ensures that applications which are processing\ngeographical data have uniform access to data and is based on continuous\nmonitoring of the data transfers using MonALISA and its extensions. The\nMedioGRID architecture is also built on Globus, Condor and PBS and based on\nthis middleware we aim to extract information about the running systems. The\nresults obtained in testing MedioGRID system for large data transfers show that\nmonitoring system provides a very good view of system evolution.", 
    "link": "http://arxiv.org/pdf/1106.5302v1", 
    "arxiv-id": "1106.5302v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "Intelligent strategies for DAG scheduling optimization in Grid   environments", 
    "publish": "2011-06-27T06:03:42Z", 
    "summary": "The paper presents a solution to the dynamic DAG scheduling problem in Grid\nenvironments. It presents a distributed, scalable, efficient and fault-tolerant\nalgorithm for optimizing tasks assignment. The scheduler algorithm for tasks\nwith dependencies uses a heuristic model to optimize the total cost of tasks\nexecution. Also, a method based on genetic algorithms is proposed to optimize\nthe procedure of resources assignment. The experiments used the MonALISA\nmonitoring environment and its extensions. The results demonstrate very good\nbehavior in comparison with other scheduling approaches for this kind of DAG\nscheduling algorithms.", 
    "link": "http://arxiv.org/pdf/1106.5303v1", 
    "arxiv-id": "1106.5303v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "Resource CoAllocation for Scheduling Tasks with Dependencies, in Grid", 
    "publish": "2011-06-27T06:24:40Z", 
    "summary": "Scheduling applications on wide-area distributed systems is useful for\nobtaining quick and reliable results in an efficient manner. Optimized\nscheduling algorithms are fundamentally important in order to achieve optimized\nresources utilization. The existing and potential applications include many\nfields of activity like satellite image processing and medicine. The paper\nproposes a scheduling algorithm for tasks with dependencies in Grid\nenvironments. CoAllocation represents a strategy that provides a schedule for\ntask with dependencies, having as main purpose the efficiency of the schedule,\nin terms of load balancing and minimum time for the execution of the tasks.", 
    "link": "http://arxiv.org/pdf/1106.5309v1", 
    "arxiv-id": "1106.5309v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "Advance Reservation of Resources for Task Execution in Grid Environments", 
    "publish": "2011-06-27T06:24:50Z", 
    "summary": "The paper proposes a solution for the Grid scheduling problem, addressing in\nparticular the requirement of high performance an efficient algorithm must\nfulfill. Advance Reservation engages a distributed, dynamic, fault-tolerant and\nefficient strategy which reserves resources for future task execution. The\npaper presents the main features of the strategy, the functioning mechanism the\nstrategy is based on and the methods used for evaluating the algorithm.", 
    "link": "http://arxiv.org/pdf/1106.5310v1", 
    "arxiv-id": "1106.5310v1"
},{
    "category": "cs.DC", 
    "author": "Dave Cliff", 
    "title": "Hybrid complex network topologies are preferred for   component-subscription in large-scale data-centres", 
    "publish": "2011-06-27T17:16:41Z", 
    "summary": "We report on experiments exploring the interplay between the topology of the\ncomplex network of dependent components in a large-scale data-centre, and the\nrobustness and scaling properties of that data-centre. In a previous paper [1]\nwe used the SPECI large-scale data-centre simulator [2] to compare the\nrobustness and scaling characteristics of data-centres whose dependent\ncomponents are connected via Strogatz-Watts small-world (SW) networks [3],\nversus those organized as Barabasi-Albert scale-free (SF) networks [4], and\nfound significant differences. In this paper, we present results from using the\nKlemm-Eguiliz (KE) construction method [5] to generate complex network\ntopologies for data-centre component dependencies. The KE model has a control\nparameter {\\mu}\\in[0,1]\\inR that determines whether the networks generated are\nSW (0<{\\mu}<<1) or SF ({\\mu}=1) or a \"hybrid\" network topology part-way between\nSW and SF (0<{\\mu}<1). We find that the best scores for system-level\nperformance metrics of the simulated data-centres are given by \"hybrid\" values\nof {\\mu} significantly different from pure-SW or pure-SF.", 
    "link": "http://arxiv.org/pdf/1106.5451v1", 
    "arxiv-id": "1106.5451v1"
},{
    "category": "cs.DC", 
    "author": "Ilango Sriram", 
    "title": "Modelling Resilience in Cloud-Scale Data Centres", 
    "publish": "2011-06-27T17:31:49Z", 
    "summary": "The trend for cloud computing has initiated a race towards data centres (DC)\nof an ever-increasing size. The largest DCs now contain many hundreds of\nthousands of virtual machine (VM) services. Given the finite lifespan of\nhardware, such large DCs are subject to frequent hardware failure events that\ncan lead to disruption of service. To counter this, multiple redundant copies\nof task threads may be distributed around a DC to ensure that individual\nhardware failures do not cause entire jobs to fail. Here, we present results\ndemonstrating the resilience of different job scheduling algorithms in a\nsimulated DC with hardware failure. We use a simple model of jobs distributed\nacross a hardware network to demonstrate the relationship between resilience\nand additional communication costs of different scheduling methods.", 
    "link": "http://arxiv.org/pdf/1106.5457v1", 
    "arxiv-id": "1106.5457v1"
},{
    "category": "cs.DC", 
    "author": "Dave Cliff", 
    "title": "SPECI-2: An open-source framework for predictive simulation of   cloud-scale data-centres", 
    "publish": "2011-06-27T18:01:03Z", 
    "summary": "We introduce Version 2 of SPECI, a system for predictive simulation modeling\nof large-scale data-centres, i.e. warehouse-sized facilities containing\nhundreds of thousands of servers, as used to provide cloud services.", 
    "link": "http://arxiv.org/pdf/1106.5465v1", 
    "arxiv-id": "1106.5465v1"
},{
    "category": "cs.DC", 
    "author": "Ciprian Dobre", 
    "title": "A distributed service for on demand end to end optical circuits", 
    "publish": "2011-06-28T06:01:49Z", 
    "summary": "In this paper we present a system for monitoring and controlling dynamic\nnetwork circuits inside the USLHCNet network. This distributed service system\nprovides in near real-time complete topological information for all the\ncircuits, resource allocation and usage, accounting, detects automatically\nfailures in the links and network equipment, generate alarms and has the\nfunctionality to take automatic actions. The system is developed based on the\nMonALISA framework, which provides a robust monitoring and controlling service\noriented architecture, with no single points of failure.", 
    "link": "http://arxiv.org/pdf/1106.5570v1", 
    "arxiv-id": "1106.5570v1"
},{
    "category": "cs.DC", 
    "author": "Catalin Leordeanu", 
    "title": "Models and Techniques for Ensuring Reliability, Safety, Availability and   Security of Large Scale Distributed Systems", 
    "publish": "2011-06-28T06:53:59Z", 
    "summary": "17th International Conference on Control Systems and Computer Science (CSCS\n17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 401-406, ISSN: 2066-4451.", 
    "link": "http://arxiv.org/pdf/1106.5576v1", 
    "arxiv-id": "1106.5576v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "An Architectural Model for a Grid based Workflow Management Platform in   Scientific Applications", 
    "publish": "2011-06-29T06:03:11Z", 
    "summary": "With recent increasing computational and data requirements of scientific\napplications, the use of large clustered systems as well as distributed\nresources is inevitable. Although executing large applications in these\nenvironments brings increased performance, the automation of the process\nbecomes more and more challenging. While the use of complex workflow management\nsystems has been a viable solution for this automation process in business\noriented environments, the open source engines available for scientific\napplications lack some functionalities or are too difficult to use for\nnon-specialists. In this work we propose an architectural model for a grid\nbased workflow management platform providing features like an intuitive way to\ndescribe workflows, efficient data handling mechanisms and flexible fault\ntolerance support. Our integrated solution introduces a workflow engine\ncomponent based on ActiveBPEL extended with additional functionalities and a\nscheduling component providing efficient mapping between tasks and available\nresources.", 
    "link": "http://arxiv.org/pdf/1106.5846v1", 
    "arxiv-id": "1106.5846v1"
},{
    "category": "cs.DC", 
    "author": "Gerhard Wellein", 
    "title": "Hybrid-parallel sparse matrix-vector multiplication with explicit   communication overlap on current multicore-based systems", 
    "publish": "2011-06-29T11:25:50Z", 
    "summary": "We evaluate optimized parallel sparse matrix-vector operations for several\nrepresentative application areas on widespread multicore-based cluster\nconfigurations. First the single-socket baseline performance is analyzed and\nmodeled with respect to basic architectural properties of standard multicore\nchips. Beyond the single node, the performance of parallel sparse matrix-vector\noperations is often limited by communication overhead. Starting from the\nobservation that nonblocking MPI is not able to hide communication cost using\nstandard MPI implementations, we demonstrate that explicit overlap of\ncommunication and computation can be achieved by using a dedicated\ncommunication thread, which may run on a virtual core. Moreover we identify\nperformance benefits of hybrid MPI/OpenMP programming due to improved load\nbalancing even without explicit communication overlap. We compare performance\nresults for pure MPI, the widely used \"vector-like\" hybrid programming\nstrategies, and explicit overlap on a modern multicore-based cluster and a Cray\nXE6 system.", 
    "link": "http://arxiv.org/pdf/1106.5908v1", 
    "arxiv-id": "1106.5908v1"
},{
    "category": "cs.DC", 
    "author": "Iosif C. Legrand", 
    "title": "Simulation Framework for Modeling Large-Scale Distributed Systems", 
    "publish": "2011-06-30T06:37:34Z", 
    "summary": "Simulation has become the evaluation method of choice for many areas of\ndistributing computing research. However, most existing simulation packages\nhave several limitations on the size and complexity of the system being\nmodeled. Fine grained simulation of complex systems such as Grids requires high\ncomputational effort which can only be obtained by using an underlying\ndistributed architecture. We are proposing a new distributed simulation system\nthat has the advantage of being able to model very complex distributed systems\nwhile hiding the computational effort from the end-user.", 
    "link": "http://arxiv.org/pdf/1106.6122v1", 
    "arxiv-id": "1106.6122v1"
},{
    "category": "cs.DC", 
    "author": "Adi Suissa", 
    "title": "A Dynamic Elimination-Combining Stack Algorithm", 
    "publish": "2011-06-30T17:12:14Z", 
    "summary": "Two key synchronization paradigms for the construction of scalable concurrent\ndata-structures are software combining and elimination. Elimination-based\nconcurrent data-structures allow operations with reverse semantics (such as\npush and pop stack operations) to \"collide\" and exchange values without having\nto access a central location. Software combining, on the other hand, is\neffective when colliding operations have identical semantics: when a pair of\nthreads performing operations with identical semantics collide, the task of\nperforming the combined set of operations is delegated to one of the threads\nand the other thread waits for its operation(s) to be performed. Applying this\nmechanism iteratively can reduce memory contention and increase throughput. The\nmost highly scalable prior concurrent stack algorithm is the\nelimination-backoff stack. The elimination-backoff stack provides high\nparallelism for symmetric workloads in which the numbers of push and pop\noperations are roughly equal, but its performance deteriorates when workloads\nare asymmetric. We present DECS, a novel Dynamic Elimination-Combining Stack\nalgorithm, that scales well for all workload types. While maintaining the\nsimplicity and low-overhead of the elimination-bakcoff stack, DECS manages to\nbenefit from collisions of both identical- and reverse-semantics operations.\nOur empirical evaluation shows that DECS scales significantly better than both\nblocking and non-blocking best prior stack algorithms.", 
    "link": "http://arxiv.org/pdf/1106.6304v1", 
    "arxiv-id": "1106.6304v1"
},{
    "category": "cs.DC", 
    "author": "Yvonnick Le Menach", 
    "title": "Automatic Multi-GPU Code Generation applied to Simulation of Electrical   Machines", 
    "publish": "2011-07-04T06:13:51Z", 
    "summary": "The electrical and electronic engineering has used parallel programming to\nsolve its large scale complex problems for performance reasons. However, as\nparallel programming requires a non-trivial distribution of tasks and data,\ndevelopers find it hard to implement their applications effectively. Thus, in\norder to reduce design complexity, we propose an approach to generate code for\nhybrid architectures (e.g. CPU + GPU) using OpenCL, an open standard for\nparallel programming of heterogeneous systems. This approach is based on Model\nDriven Engineering (MDE) and the MARTE profile, standard proposed by Object\nManagement Group (OMG). The aim is to provide resources to non-specialists in\nparallel programming to implement their applications. Moreover, thanks to model\nreuse capacity, we can add/change functionalities or the target architecture.\nConsequently, this approach helps industries to achieve their time-to-market\nconstraints and confirms by experimental tests, performance improvements using\nmulti-GPU environments.", 
    "link": "http://arxiv.org/pdf/1107.0538v1", 
    "arxiv-id": "1107.0538v1"
},{
    "category": "cs.DC", 
    "author": "Dohan Kim", 
    "title": "Priority-based task reassignments in hierarchical 2D mesh-connected   systems using tableaux", 
    "publish": "2011-07-10T15:48:34Z", 
    "summary": "Task reassignments in 2D mesh-connected systems (2D-MSs) have been researched\nfor several decades. We propose a hierarchical 2D mesh-connected system\n(2D-HMS) in order to exploit the regular nature of a 2D-MS. In our approach\npriority-based task assignments and reassignments in a 2D-HMS are represented\nby tableaux and their algorithms. We show how task relocations for a\npriority-based task reassignment in a 2D-HMS are reduced to a jeu de taquin\nslide.", 
    "link": "http://arxiv.org/pdf/1107.1866v4", 
    "arxiv-id": "1107.1866v4"
},{
    "category": "cs.DC", 
    "author": "Aggelos Kiayias", 
    "title": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness", 
    "publish": "2011-07-15T04:24:38Z", 
    "summary": "We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$.", 
    "link": "http://arxiv.org/pdf/1107.2990v2", 
    "arxiv-id": "1107.2990v2"
},{
    "category": "cs.DC", 
    "author": "Anwitaman Datta", 
    "title": "Homomorphic Self-repairing Codes for Agile Maintenance of Distributed   Storage Systems", 
    "publish": "2011-07-15T18:46:33Z", 
    "summary": "Distributed data storage systems are essential to deal with the need to store\nmassive volumes of data. In order to make such a system fault-tolerant, some\nform of redundancy becomes crucial, incurring various overheads - most\nprominently in terms of storage space and maintenance bandwidth requirements.\nErasure codes, originally designed for communication over lossy channels,\nprovide a storage efficient alternative to replication based redundancy,\nhowever entailing high communication overhead for maintenance, when some of the\nencoded fragments need to be replenished in news ones after failure of some\nstorage devices. We propose as an alternative a new family of erasure codes\ncalled self-repairing codes (SRC) taking into account the peculiarities of\ndistributed storage systems, specifically the maintenance process. SRC has the\nfollowing salient features: (a) encoded fragments can be repaired directly from\nother subsets of encoded fragments by downloading less data than the size of\nthe complete object, ensuring that (b) a fragment is repaired from a fixed\nnumber of encoded fragments, the number depending only on how many encoded\nblocks are missing and independent of which specific blocks are missing. This\npaper lays the foundations by defining the novel self-repairing codes,\nelaborating why the defined characteristics are desirable for distributed\nstorage systems. Then homomorphic self-repairing codes (HSRC) are proposed as a\nconcrete instance, whose various aspects and properties are studied and\ncompared - quantitatively or qualitatively with respect to other codes\nincluding traditional erasure codes as well as other recent codes designed\nspecifically for storage applications.", 
    "link": "http://arxiv.org/pdf/1107.3129v1", 
    "arxiv-id": "1107.3129v1"
},{
    "category": "cs.DC", 
    "author": "Denis Trystram", 
    "title": "Decentralized List Scheduling", 
    "publish": "2011-07-19T15:13:23Z", 
    "summary": "Classical list scheduling is a very popular and efficient technique for\nscheduling jobs in parallel and distributed platforms. It is inherently\ncentralized. However, with the increasing number of processors, the cost for\nmanaging a single centralized list becomes too prohibitive. A suitable approach\nto reduce the contention is to distribute the list among the computational\nunits: each processor has only a local view of the work to execute. Thus, the\nscheduler is no longer greedy and standard performance guarantees are lost.\n  The objective of this work is to study the extra cost that must be paid when\nthe list is distributed among the computational units. We first present a\ngeneral methodology for computing the expected makespan based on the analysis\nof an adequate potential function which represents the load unbalance between\nthe local lists. We obtain an equation on the evolution of the potential by\ncomputing its expected decrease in one step of the schedule. Our main theorem\nshows how to solve such equations to bound the makespan. Then, we apply this\nmethod to several scheduling problems, namely, for unit independent tasks, for\nweighted independent tasks and for tasks with precendence constraints. More\nprecisely, we prove that the time for scheduling a global workload W composed\nof independent unit tasks on m processors is equal to W/m plus an additional\nterm proportional to log_2 W. We provide a lower bound which shows that this is\noptimal up to a constant. This result is extended to the case of weighted\nindependent tasks. In the last setting, precedence task graphs, our analysis\nleads to an improvement on the bound of Arora et al. We finally provide some\nexperiments using a simulator. The distribution of the makespan is shown to fit\nexisting probability laws. The additive term is shown by simulation to be\naround 3 \\log_2 W confirming the tightness of our analysis.", 
    "link": "http://arxiv.org/pdf/1107.3734v1", 
    "arxiv-id": "1107.3734v1"
},{
    "category": "cs.DC", 
    "author": "Vincent Villain", 
    "title": "Snap-Stabilizing Message Forwarding Algorithm on Tree Topologies", 
    "publish": "2011-07-29T16:37:44Z", 
    "summary": "In this paper, we consider the message forwarding problem that consists in\nmanaging the network resources that are used to forward messages. Previous\nworks on this problem provide solutions that either use a significant number of\nbuffers (that is n buffers per processor, where n is the number of processors\nin the network) making the solution not scalable or, they reserve all the\nbuffers from the sender to the receiver to forward only one message %while\nusing D buffers (where D refers to the diameter of the network) . The only\nsolution that uses a constant number of buffers per link was introduced in [1].\nHowever the solution works only on a chain networks. In this paper, we propose\na snap-stabilizing algorithm for the message forwarding problem that uses the\nsame complexity on the number of buffers as [1] and works on tree topologies.", 
    "link": "http://arxiv.org/pdf/1107.6014v1", 
    "arxiv-id": "1107.6014v1"
},{
    "category": "cs.DC", 
    "author": "Gian Luigi Ferrari", 
    "title": "Predicting global usages of resources endowed with local policies", 
    "publish": "2011-08-01T03:58:16Z", 
    "summary": "The effective usages of computational resources are a primary concern of\nup-to-date distributed applications. In this paper, we present a methodology to\nreason about resource usages (acquisition, release, revision, ...), and\ntherefore the proposed approach enables to predict bad usages of resources.\nKeeping in mind the interplay between local and global information occurring in\nthe application-resource interactions, we model resources as entities with\nlocal policies and global properties governing the overall interactions.\nFormally, our model takes the shape of an extension of pi-calculus with\nprimitives to manage resources. We develop a Control Flow Analysis computing a\nstatic approximation of process behaviour and therefore of the resource usages.", 
    "link": "http://arxiv.org/pdf/1108.0231v1", 
    "arxiv-id": "1108.0231v1"
},{
    "category": "cs.DC", 
    "author": "Daniel Lombrana Gonzalez", 
    "title": "Proposal for improvement in the transfer and execution of multiple   instances of a virtual image", 
    "publish": "2011-08-02T15:45:12Z", 
    "summary": "Virtualization technology allows currently any application run any\napplication complex and expensive computational (the scientific applications\nare a good example) on heterogeneous distributed systems, which make regular\nuse of Grid and Cloud technologies, enabling significant savings in computing\ntime. This model is particularly interesting for the mass execution of\nscientific simulations and calculations, allowing parallel execution of\napplications using the same execution environment (unchanged) used by the\nscientist as usual. However, the use and distribution of large virtual images\ncan be a problem (up to tens of GBytes), which is aggravated when attempting a\nmass mailing on a large number of distributed computers. This work has as main\nobjective to present an analysis of how implementation and a proposal for the\nimprovement (reduction in size) of the virtual images pretending reduce\ndistribution time in distributed systems. This analysis is done very specific\nrequirements that need an operating system (guest OS) on some aspects of its\nexecution.", 
    "link": "http://arxiv.org/pdf/1108.0599v1", 
    "arxiv-id": "1108.0599v1"
},{
    "category": "cs.DC", 
    "author": "Nattakan Puttarak", 
    "title": "Network Localization on Unit Disk Graphs", 
    "publish": "2011-08-04T01:22:53Z", 
    "summary": "We study the problem of cooperative localization of a large network of nodes\nin integer-coordinated unit disk graphs, a simplified but useful version of\ngeneral random graph. Exploiting the property that the radius $r$ sets clear\ncut on the connectivity of two nodes, we propose an essential philosophy that\n\"no connectivity is also useful information just like the information being\nconnected\" in unit disk graphs. Exercising this philosophy, we show that the\nconventional network localization problem can be re-formulated to significantly\nreduce the search space, and that global rigidity, a necessary and sufficient\ncondition for the existence of unique solution in general graphs, is no longer\nnecessary. While the problem is still NP-hard, we show that a (depth-first)\ntree-search algorithm with memory O(N) ($N$ is the network size) can be\ndeveloped, and for practical setups, the search complexity and speed is very\nmanageable, and is magnitudes less than the conventional problem, especially\nwhen the graph is sparse or when only very limited anchor nodes are available.", 
    "link": "http://arxiv.org/pdf/1108.0972v1", 
    "arxiv-id": "1108.0972v1"
},{
    "category": "cs.DC", 
    "author": "Meenakshi Sati", 
    "title": "Accurate location estimation of moving object with energy constraint &   adaptive update algorithms to save data", 
    "publish": "2011-08-05T12:56:43Z", 
    "summary": "In research paper \"Accurate estimation of the target location of object with\nenergy constraint & Adaptive Update Algorithms to Save Data\" one of the central\nissues in sensor networks is track the location, of moving object which have\noverhead of saving data, an accurate estimation of the target location of\nobject with energy constraint .We do not have any mechanism which control and\nmaintain data .The wireless communication bandwidth is also very limited. Some\nfield which is using this technique are flood and typhoon detection, forest\nfire detection, temperature and humidity and ones we have these information use\nthese information back to a central air conditioning and ventilation system. In\nthis research paper, we propose protocol based on the prediction and adaptive\nbased algorithm which is using less sensor node reduced by an accurate\nestimation of the target location. we are using minimum three sensor node to\nget the accurate position .We can extend it upto four or five to find more\naccurate location but we have energy constraint so we are using three with\naccurate estimation of location help us to reduce sensor node..We show that our\ntracking method performs well in terms of energy saving regardless of mobility\npattern of the mobile target .We extends the life time of network with less\nsensor node. Once a new object is detected, a mobile agent will be initiated to\ntrack the roaming path of the object. The agent is mobile since it will choose\nthe sensor closest to the object to stay. The agent may invite some nearby\nslave sensors to cooperatively position the object and inhibit other irrelevant\n(i.e., farther) sensors from tracking the object. As a result, the\ncommunication and sensing overheads are greatly reduced.", 
    "link": "http://arxiv.org/pdf/1108.1321v1", 
    "arxiv-id": "1108.1321v1"
},{
    "category": "cs.DC", 
    "author": "Nibedita Adhikari", 
    "title": "On a New Multicomputer Interconnection Topology for Massively Parallel   Systems", 
    "publish": "2011-08-06T08:03:43Z", 
    "summary": "This paper introduces a new interconnection network topology called Balanced\nVarietal Hypercube (BVH), suitable for massively parallel systems. The proposed\ntopology being a hybrid structure retains almost all the attractive properties\nof Balanced Hypercube and Varietal Hypercube. The topology, various parameters,\nrouting and broadcasting of Balanced Varietal Hypercube are presented. The\nperformance of the Balanced Varietal Hypercube is compared with other networks.\nIn terms of diameter, cost and average distance and reliability the proposed\nnetwork is found to be better than the Hypercube, Balanced Hypercube and\nVarietal Hypercube. Also it is more reliable and cost-effective than Hypercube\nand Balanced Hypercube.", 
    "link": "http://arxiv.org/pdf/1108.1462v1", 
    "arxiv-id": "1108.1462v1"
},{
    "category": "cs.DC", 
    "author": "Erhard Rahm", 
    "title": "Load Balancing for MapReduce-based Entity Resolution", 
    "publish": "2011-08-08T08:48:57Z", 
    "summary": "The effectiveness and scalability of MapReduce-based implementations of\ncomplex data-intensive tasks depend on an even redistribution of data between\nmap and reduce tasks. In the presence of skewed data, sophisticated\nredistribution approaches thus become necessary to achieve load balancing among\nall reduce tasks to be executed in parallel. For the complex problem of entity\nresolution, we propose and evaluate two approaches for such skew handling and\nload balancing. The approaches support blocking techniques to reduce the search\nspace of entity resolution, utilize a preprocessing MapReduce job to analyze\nthe data distribution, and distribute the entities of large blocks among\nmultiple reduce tasks. The evaluation on a real cloud infrastructure shows the\nvalue and effectiveness of the proposed load balancing approaches.", 
    "link": "http://arxiv.org/pdf/1108.1631v1", 
    "arxiv-id": "1108.1631v1"
},{
    "category": "cs.DC", 
    "author": "Ernst Biersack", 
    "title": "HybridNN: Supporting Network Location Service on Generalized Delay   Metrics", 
    "publish": "2011-08-09T13:57:18Z", 
    "summary": "Distributed Nearest Neighbor Search (DNNS) locates service nodes that have\nshortest interactive delay towards requesting hosts. DNNS provides an important\nservice for large-scale latency sensitive networked applications, such as VoIP,\nonline network games, or interactive network services on the cloud. Existing\nwork assumes the delay to be symmetric, which does not generalize to\napplications that are sensitive to one-way delays, such as the multimedia video\ndelivery from the servers to the hosts. We propose a relaxed inframetric model\nfor the network delay space that does not assume the triangle inequality and\ndelay symmetry to hold. We prove that the DNNS requests can be completed\nefficiently if the delay space exhibits modest inframetric dimensions, which we\ncan observe empirically. Finally, we propose a DNNS method named HybridNN\n(\\textit{Hybrid} \\textit{N}earest \\textit{N}eighbor search) based on the\ninframetric model for fast and accurate DNNS. For DNNS requests, HybridNN\nchooses closest neighbors accurately via the inframetric modelling, and\nscalably by combining delay predictions with direct probes to a pruned set of\nneighbors. Simulation results show that HybridNN locates nearly optimally the\nnearest neighbor. Experiments on PlanetLab show that HybridNN can provide\naccurate nearest neighbors that are close to optimal with modest query overhead\nand maintenance traffic.", 
    "link": "http://arxiv.org/pdf/1108.1928v1", 
    "arxiv-id": "1108.1928v1"
},{
    "category": "cs.DC", 
    "author": "Mohiuddin Ahmed", 
    "title": "On the Performance of MPI-OpenMP on a 12 nodes Multi-core Cluster", 
    "publish": "2011-08-16T15:26:44Z", 
    "summary": "With the increasing number of Quad-Core-based clusters and the introduction\nof compute nodes designed with large memory capacity shared by multiple cores,\nnew problems related to scalability arise. In this paper, we analyze the\noverall performance of a cluster built with nodes having a dual Quad-Core\nProcessor on each node. Some benchmark results are presented and some\nobservations are mentioned when handling such processors on a benchmark test. A\nQuad-Core-based cluster's complexity arises from the fact that both local\ncommunication and network communications between the running processes need to\nbe addressed. The potentials of an MPI-OpenMP approach are pinpointed because\nof its reduced communication overhead. At the end, we come to a conclusion that\nan MPI-OpenMP solution should be considered in such clusters since optimizing\nnetwork communications between nodes is as important as optimizing local\ncommunications between processors in a multi-core cluster.", 
    "link": "http://arxiv.org/pdf/1108.3268v1", 
    "arxiv-id": "1108.3268v1"
},{
    "category": "cs.DC", 
    "author": "Ursula Goltz", 
    "title": "Synchrony vs. Causality in Asynchronous Petri Nets", 
    "publish": "2011-08-23T01:24:08Z", 
    "summary": "Given a synchronous system, we study the question whether the behaviour of\nthat system can be exhibited by a (non-trivially) distributed and hence\nasynchronous implementation. In this paper we show, by counterexample, that\nsynchronous systems cannot in general be implemented in an asynchronous fashion\nwithout either introducing an infinite implementation or changing the causal\nstructure of the system behaviour.", 
    "link": "http://arxiv.org/pdf/1108.4471v1", 
    "arxiv-id": "1108.4471v1"
},{
    "category": "cs.DC", 
    "author": "Gabriele D'Angelo", 
    "title": "Auction-Based Resource Allocation in Digital Ecosystems", 
    "publish": "2011-09-02T10:08:38Z", 
    "summary": "The proliferation of portable devices (PDAs, smartphones, digital multimedia\nplayers, and so forth) allows mobile users to carry around a pool of computing,\nstorage and communication resources. Sharing these resources with other users\n(\"Digital Organisms\" -- DOs) opens the door to novel interesting scenarios,\nwhere people trade resources to allow the execution, anytime and anywhere, of\napplications that require a mix of capabilities. In this paper we present a\nfully distributed approach for resource sharing among multiple devices owned by\ndifferent mobile users. Our scheme enables DOs to trade computing/networking\nfacilities through an auction-based mechanism, without the need of a central\ncontrol. We use a set of numerical experiments to compare our approach with an\noptimal (centralized) allocation strategy that, given the set of resource\ndemands and offers, maximizes the number of matches. Results confirm the\neffectiveness of our approach since it produces a fair allocation of resources\nwith low computational cost, providing DOs with the means to form an altruistic\ndigital ecosystem.", 
    "link": "http://arxiv.org/pdf/1109.0397v2", 
    "arxiv-id": "1109.0397v2"
},{
    "category": "cs.DC", 
    "author": "Robert Louis Cloud", 
    "title": "Problems in Modern High Performance Parallel I/O Systems", 
    "publish": "2011-09-04T19:28:43Z", 
    "summary": "In the past couple of decades, the computational abilities of supercomput-\ners have increased tremendously. Leadership scale supercomputers now are\ncapable of petaflops. Likewise, the problem size targeted by applications\nrunning on such computers has also scaled. These large applications have I/O\nthroughput requirements on the order of tens of gigabytes per second. For a\nvariety of reasons, the I/O subsystems of such computers have not kept pace\nwith the computational increases, and the time required for I/O in an\napplication has become one of the dominant bottlenecks. Also troublesome is the\nfact that scientific applications do not attain near the peak theoretical\nbandwidth of the I/O subsystems. In addressing the two prior issues, one must\nalso question the nature of the data itself; one can ask whether contem- porary\npractices of data dumping and analysis are optimal and whether they will\ncontinue to be applicable as computers continue to scale. These three topics,\nthe I/O subsystem, the nature of scientific data output, and future possible\noptimizations are discussed in this report.", 
    "link": "http://arxiv.org/pdf/1109.0742v1", 
    "arxiv-id": "1109.0742v1"
},{
    "category": "cs.DC", 
    "author": "Subhas Chandra Pal", 
    "title": "An Empirical Study and Analysis of the Dynamic Load Balancing Techniques   Used in Parallel Computing Systems", 
    "publish": "2011-09-08T08:04:06Z", 
    "summary": "A parallel computer system is a collection of processing elements that\ncommunicate and cooperate to solve large computational problems efficiently. To\nachieve this, at first the large computational problem is partitioned into\nseveral tasks with different work-loads and then are assigned to the different\nprocessing elements for computation. Distribution of the work load is known as\nLoad Balancing. An appropriate distribution of work-loads across the various\nprocessing elements is very important as disproportional workloads can\neliminate the performance benefit of parallelizing the job. Hence, load\nbalancing on parallel systems is a critical and challenging activity. Load\nbalancing algorithms can be broadly categorized as static or dynamic. Static\nload balancing algorithms distribute the tasks to processing elements at\ncompile time, while dynamic algorithms bind tasks to processing elements at run\ntime. This paper explains only the different dynamic load balancing techniques\nin brief used in parallel systems and concluding with the comparative\nperformance analysis result of these algorithms.", 
    "link": "http://arxiv.org/pdf/1109.1650v1", 
    "arxiv-id": "1109.1650v1"
},{
    "category": "cs.DC", 
    "author": "C. Pandu Rangan", 
    "title": "On the Fault Tolerance and Hamiltonicity of the Optical Transpose   Interconnection System of Non-Hamiltonian Base Graphs", 
    "publish": "2011-09-08T12:50:38Z", 
    "summary": "Hamiltonicity is an important property in parallel and distributed\ncomputation. Existence of Hamiltonian cycle allows efficient emulation of\ndistributed algorithms on a network wherever such algorithm exists for\nlinear-array and ring, and can ensure deadlock freedom in some routing\nalgorithms in hierarchical interconnection networks. Hamiltonicity can also be\nused for construction of independent spanning tree and leads to designing fault\ntolerant protocols. Optical Transpose Interconnection Systems or OTIS (also\nreferred to as two-level swapped network) is a widely studied interconnection\nnetwork topology which is popular due to high degree of scalability,\nregularity, modularity and package ability. Surprisingly, to our knowledge,\nonly one strong result is known regarding Hamiltonicity of OTIS - showing that\nOTIS graph built of Hamiltonian base graphs are Hamiltonian. In this work we\nconsider Hamiltonicity of OTIS networks, built on Non-Hamiltonian base and\nanswer some important questions. First, we prove that Hamiltonicity of base\ngraph is not a necessary condition for the OTIS to be Hamiltonian. We present\nan infinite family of Hamiltonian OTIS graphs composed on Non-Hamiltonian base\ngraphs. We further show that, it is not sufficient for the base graph to have\nHamiltonian path for the OTIS constructed on it to be Hamiltonian. We give\nconstructive proof of Hamiltonicity for a large family of Butterfly-OTIS. This\nproof leads to an alternate efficient algorithm for independent spanning trees\nconstruction on this class of OTIS graphs. Our algorithm is linear in the\nnumber of vertices as opposed to the generalized algorithm, which is linear in\nthe number of edges of the graph.", 
    "link": "http://arxiv.org/pdf/1109.1706v1", 
    "arxiv-id": "1109.1706v1"
},{
    "category": "cs.DC", 
    "author": "David Clarke", 
    "title": "Design and implementation of self-adaptable parallel algorithms for   scientific computing on highly heterogeneous HPC platforms", 
    "publish": "2011-09-14T13:25:35Z", 
    "summary": "Traditional heterogeneous parallel algorithms, designed for heterogeneous\nclusters of workstations, are based on the assumption that the absolute speed\nof the processors does not depend on the size of the computational task. This\nassumption proved inaccurate for modern and perspective highly heterogeneous\nHPC platforms. New class of algorithms based on the functional performance\nmodel (FPM), representing the speed of the processor by a function of problem\nsize, has been recently proposed. These algorithms cannot be however employed\nin self-adaptable applications because of very high cost of construction of the\nfunctional performance model. The paper presents a new class of parallel\nalgorithms for highly heterogeneous HPC platforms. Like traditional FPM-based\nalgorithms, these algorithms assume that the speed of the processors is\ncharacterized by speed functions rather than speed constants. Unlike the\ntraditional algorithms, they do not assume the speed functions to be given.\nInstead, they estimate the speed functions of the processors for different\nproblem sizes during their execution. These algorithms do not construct the\nfull speed function for each processor but rather build and use their partial\nestimates sufficient for optimal distribution of computations with a given\naccuracy. The low execution cost of distribution of computations between\nheterogeneous processors in these algorithms make them suitable for employment\nin self-adaptable applications. Experiments with parallel matrix multiplication\napplications based on this approach are performed on local and global\nheterogeneous computational clusters. The results show that the execution time\nof optimal matrix distribution between processors is significantly less, by\norders of magnitude, than the total execution time of the optimized\napplication.", 
    "link": "http://arxiv.org/pdf/1109.3074v1", 
    "arxiv-id": "1109.3074v1"
},{
    "category": "cs.DC", 
    "author": "Devan Sohier", 
    "title": "Universal adaptive self-stabilizing traversal scheme: random walk and   reloading wave", 
    "publish": "2011-09-16T09:38:20Z", 
    "summary": "In this paper, we investigate random walk based token circulation in dynamic\nenvironments subject to failures. We describe hypotheses on the dynamic\nenvironment that allow random walks to meet the important property that the\ntoken visits any node infinitely often. The randomness of this scheme allows it\nto work on any topology, and require no adaptation after a topological change,\nwhich is a desirable property for applications to dynamic systems. For random\nwalks to be a traversal scheme and to answer the concurrence problem, one needs\nto guarantee that exactly one token circulates in the system. In the presence\nof transient failures, configurations with multiple tokens or with no token can\noccur. The meeting property of random walks solves the cases with multiple\ntokens. The reloading wave mechanism we propose, together with timeouts, allows\nto detect and solve cases with no token. This traversal scheme is\nself-stabilizing, and universal, meaning that it needs no assumption on the\nsystem topology. We describe conditions on the dynamicity (with a local\ndetection criterion) under which the algorithm is tolerant to dynamic\nreconfigurations. We conclude by a study on the time between two visits of the\ntoken to a node, which we use to tune the parameters of the reloading wave\nmechanism according to some system characteristics.", 
    "link": "http://arxiv.org/pdf/1109.3561v1", 
    "arxiv-id": "1109.3561v1"
},{
    "category": "cs.DC", 
    "author": "Basilis Mamalis", 
    "title": "Lowest-ID with Adaptive ID Reassignment: A Novel Mobile Ad-Hoc Networks   Clustering Algorithm", 
    "publish": "2011-09-19T11:11:57Z", 
    "summary": "Clustering is a promising approach for building hierarchies and simplifying\nthe routing process in mobile ad-hoc network environments. The main objective\nof clustering is to identify suitable node representatives, i.e. cluster heads\n(CHs), to store routing and topology information and maximize clusters\nstability. Traditional clustering algorithms suggest CH election exclusively\nbased on node IDs or location information and involve frequent broadcasting of\ncontrol packets, even when network topology remains unchanged. More recent\nworks take into account additional metrics (such as energy and mobility) and\noptimize initial clustering. However, in many situations (e.g. in relatively\nstatic topologies) re-clustering procedure is hardly ever invoked; hence\ninitially elected CHs soon reach battery exhaustion. Herein, we introduce an\nefficient distributed clustering algorithm that uses both mobility and energy\nmetrics to provide stable cluster formations. CHs are initially elected based\non the time and cost-efficient lowest-ID method. During clustering maintenance\nphase though, node IDs are re-assigned according to nodes mobility and energy\nstatus, ensuring that nodes with low-mobility and sufficient energy supply are\nassigned low IDs and, hence, are elected as CHs. Our algorithm also reduces\ncontrol traffic volume since broadcast period is adjusted according to nodes\nmobility pattern: we employ infrequent broadcasting for relative static network\ntopologies, and increase broadcast frequency for highly mobile network\nconfigurations. Simulation results verify that energy consumption is uniformly\ndistributed among network nodes and that signaling overhead is significantly\ndecreased.", 
    "link": "http://arxiv.org/pdf/1109.3997v1", 
    "arxiv-id": "1109.3997v1"
},{
    "category": "cs.DC", 
    "author": "V\u00edtor Santos Costa", 
    "title": "Couillard: Parallel Programming via Coarse-Grained Data-Flow Compilation", 
    "publish": "2011-09-22T19:44:19Z", 
    "summary": "Data-flow is a natural approach to parallelism. However, describing\ndependencies and control between fine-grained data-flow tasks can be complex\nand present unwanted overheads. TALM (TALM is an Architecture and Language for\nMulti-threading) introduces a user-defined coarse-grained parallel data-flow\nmodel, where programmers identify code blocks, called super-instructions, to be\nrun in parallel and connect them in a data-flow graph. TALM has been\nimplemented as a hybrid Von Neumann/data-flow execution system: the\n\\emph{Trebuchet}. We have observed that TALM's usefulness largely depends on\nhow programmers specify and connect super-instructions. Thus, we present\n\\emph{Couillard}, a full compiler that creates, based on an annotated\nC-program, a data-flow graph and C-code corresponding to each\nsuper-instruction. We show that our toolchain allows one to benefit from\ndata-flow execution and explore sophisticated parallel programming techniques,\nwith small effort. To evaluate our system we have executed a set of real\napplications on a large multi-core machine. Comparison with popular parallel\nprogramming methods shows competitive speedups, while providing an easier\nparallel programing approach.", 
    "link": "http://arxiv.org/pdf/1109.4925v1", 
    "arxiv-id": "1109.4925v1"
},{
    "category": "cs.DC", 
    "author": "Johannes Gehrke", 
    "title": "Nerio: Leader Election and Edict Ordering", 
    "publish": "2011-09-23T15:27:43Z", 
    "summary": "Coordination in a distributed system is facilitated if there is a unique\nprocess, the leader, to manage the other processes. The leader creates edicts\nand sends them to other processes for execution or forwarding to other\nprocesses. The leader may fail, and when this occurs a leader election protocol\nselects a replacement. This paper describes Nerio, a class of such leader\nelection protocols.", 
    "link": "http://arxiv.org/pdf/1109.5111v2", 
    "arxiv-id": "1109.5111v2"
},{
    "category": "cs.DC", 
    "author": "Wojciech Golab", 
    "title": "A Complexity Separation Between the Cache-Coherent and Distributed   Shared Memory Models", 
    "publish": "2011-09-23T18:39:18Z", 
    "summary": "We consider asynchronous multiprocessor systems where processes communicate\nby accessing shared memory. Exchange of information among processes in such a\nmultiprocessor necessitates costly memory accesses called \\emph{remote memory\nreferences} (RMRs), which generate communication on the interconnect joining\nprocessors and main memory. In this paper we compare two popular shared memory\narchitecture models, namely the \\emph{cache-coherent} (CC) and\n\\emph{distributed shared memory} (DSM) models, in terms of their power for\nsolving synchronization problems efficiently with respect to RMRs. The\nparticular problem we consider entails one process sending a \"signal\" to a\nsubset of other processes. We show that a variant of this problem can be solved\nvery efficiently with respect to RMRs in the CC model, but not so in the DSM\nmodel, even when we consider amortized RMR complexity.\n  To our knowledge, this is the first separation in terms of amortized RMR\ncomplexity between the CC and DSM models. It is also the first separation in\nterms of RMR complexity (for asynchronous systems) that does not rely in any\nway on wait-freedom---the requirement that a process makes progress in a\nbounded number of its own steps.", 
    "link": "http://arxiv.org/pdf/1109.5153v1", 
    "arxiv-id": "1109.5153v1"
},{
    "category": "cs.DC", 
    "author": "Thomas Sterling", 
    "title": "Improving the scalability of parallel N-body applications with an event   driven constraint based execution model", 
    "publish": "2011-09-23T20:14:24Z", 
    "summary": "The scalability and efficiency of graph applications are significantly\nconstrained by conventional systems and their supporting programming models.\nTechnology trends like multicore, manycore, and heterogeneous system\narchitectures are introducing further challenges and possibilities for emerging\napplication domains such as graph applications. This paper explores the space\nof effective parallel execution of ephemeral graphs that are dynamically\ngenerated using the Barnes-Hut algorithm to exemplify dynamic workloads. The\nworkloads are expressed using the semantics of an Exascale computing execution\nmodel called ParalleX. For comparison, results using conventional execution\nmodel semantics are also presented. We find improved load balancing during\nruntime and automatic parallelism discovery improving efficiency using the\nadvanced semantics for Exascale computing.", 
    "link": "http://arxiv.org/pdf/1109.5190v1", 
    "arxiv-id": "1109.5190v1"
},{
    "category": "cs.DC", 
    "author": "Thomas Sterling", 
    "title": "An Application Driven Analysis of the ParalleX Execution Model", 
    "publish": "2011-09-23T21:00:36Z", 
    "summary": "Exascale systems, expected to emerge by the end of the next decade, will\nrequire the exploitation of billion-way parallelism at multiple hierarchical\nlevels in order to achieve the desired sustained performance. The task of\nassessing future machine performance is approached by identifying the factors\nwhich currently challenge the scalability of parallel applications. It is\nsuggested that the root cause of these challenges is the incoherent coupling\nbetween the current enabling technologies, such as Non-Uniform Memory Access of\npresent multicore nodes equipped with optional hardware accelerators and the\ndecades older execution model, i.e., the Communicating Sequential Processes\n(CSP) model best exemplified by the message passing interface (MPI) application\nprogramming interface. A new execution model, ParalleX, is introduced as an\nalternative to the CSP model. In this paper, an overview of the ParalleX\nexecution model is presented along with details about a ParalleX-compliant\nruntime system implementation called High Performance ParalleX (HPX). Scaling\nand performance results for an adaptive mesh refinement numerical relativity\napplication developed using HPX are discussed. The performance results of this\nHPX-based application are compared with a counterpart MPI-based mesh refinement\ncode. The overheads associated with HPX are explored and hardware solutions are\nintroduced for accelerating the runtime system.", 
    "link": "http://arxiv.org/pdf/1109.5201v1", 
    "arxiv-id": "1109.5201v1"
},{
    "category": "cs.DC", 
    "author": "Tony Q. S. Quek", 
    "title": "Cooperative and Distributed Localization for Wireless Sensor Networks in   Multipath Environments", 
    "publish": "2011-09-27T04:38:33Z", 
    "summary": "We consider the problem of sensor localization in a wireless network in a\nmultipath environment, where time and angle of arrival information are\navailable at each sensor. We propose a distributed algorithm based on belief\npropagation, which allows sensors to cooperatively self-localize with respect\nto one single anchor in a multihop network. The algorithm has low overhead and\nis scalable. Simulations show that although the network is loopy, the proposed\nalgorithm converges, and achieves good localization accuracy.", 
    "link": "http://arxiv.org/pdf/1109.5770v1", 
    "arxiv-id": "1109.5770v1"
},{
    "category": "cs.DC", 
    "author": "Thomas Sterling", 
    "title": "Adaptive Mesh Refinement for Astrophysics Applications with ParalleX", 
    "publish": "2011-10-06T01:27:28Z", 
    "summary": "Several applications in astrophysics require adequately resolving many\nphysical and temporal scales which vary over several orders of magnitude.\nAdaptive mesh refinement techniques address this problem effectively but often\nresult in constrained strong scaling performance. The ParalleX execution model\nis an experimental execution model that aims to expose new forms of program\nparallelism and eliminate any global barriers present in a scaling-impaired\napplication such as adaptive mesh refinement. We present two astrophysics\napplications using the ParalleX execution model: a tabulated equation of state\ncomponent for neutron star evolutions and a cosmology model evolution.\nPerformance and strong scaling results from both simulations are presented. The\ntabulated equation of state data are distributed with transparent access over\nthe nodes of the cluster. This allows seamless overlapping of computation with\nthe latencies introduced by the remote access to the table. Because of the\nexpected size increases to the equation of state table, this type of table\npartitioning for neutron star simulations is essential while the implementation\nis greatly simplified by ParalleX semantics.", 
    "link": "http://arxiv.org/pdf/1110.1131v1", 
    "arxiv-id": "1110.1131v1"
},{
    "category": "cs.DC", 
    "author": "and Yves Robert", 
    "title": "Hierarchical QR factorization algorithms for multi-core cluster systems", 
    "publish": "2011-10-07T14:51:08Z", 
    "summary": "This paper describes a new QR factorization algorithm which is especially\ndesigned for massively parallel platforms combining parallel distributed\nmulti-core nodes. These platforms make the present and the foreseeable future\nof high-performance computing. Our new QR factorization algorithm falls in the\ncategory of the tile algorithms which naturally enables good data locality for\nthe sequential kernels executed by the cores (high sequential performance), low\nnumber of messages in a parallel distributed setting (small latency term), and\nfine granularity (high parallelism).", 
    "link": "http://arxiv.org/pdf/1110.1553v1", 
    "arxiv-id": "1110.1553v1"
},{
    "category": "cs.DC", 
    "author": "Orhan Dagdeviren", 
    "title": "Cluster-Based Load Balancing Algorithms for Grids", 
    "publish": "2011-10-10T10:32:29Z", 
    "summary": "E-science applications may require huge amounts of data and high processing\npower where grid infrastructures are very suitable for meeting these\nrequirements. The load distribution in a grid may vary leading to the\nbottlenecks and overloaded sites. We describe a hierarchical dynamic load\nbalancing protocol for Grids. The Grid consists of clusters and each cluster is\nrepresented by a coordinator. Each coordinator first attempts to balance the\nload in its cluster and if this fails, communicates with the other coordinators\nto perform transfer or reception of load. This process is repeated\nperiodically. We analyze the correctness, performance and scalability of the\nproposed protocol and show from the simulation results that our algorithm\nbalances the load by decreasing the number of high loaded nodes in a grid\nenvironment.", 
    "link": "http://arxiv.org/pdf/1110.1991v1", 
    "arxiv-id": "1110.1991v1"
},{
    "category": "cs.DC", 
    "author": "Valery I. Belokon", 
    "title": "Rigorous Calculation of the Partition Function for the Finite Number of   Ising Spins", 
    "publish": "2011-10-12T03:34:51Z", 
    "summary": "The high-performance scalable parallel algorithm for rigorous calculation of\npartition function of lattice systems with finite number Ising spins was\ndeveloped. The parallel calculations run by C++ code with using of Message\nPassing Interface and massive parallel instructions. The algorithm can be used\nfor the research of the interacting spin systems in the Ising models of 2D and\n3D. The processing power and scalability is analyzed for different parallel and\ndistributed systems. Different methods of the speed up measuring allow obtain\nthe super-linear speeding up for the small number of processes. Program code\ncould be useful also for research by exact method of different Ising spin\nsystems, e.g. system with competition interactions.", 
    "link": "http://arxiv.org/pdf/1110.2561v1", 
    "arxiv-id": "1110.2561v1"
},{
    "category": "cs.DC", 
    "author": "Vivek Kale", 
    "title": "Hybrid static/dynamic scheduling for already optimized dense matrix   factorization", 
    "publish": "2011-10-12T15:09:45Z", 
    "summary": "We present the use of a hybrid static/dynamic scheduling strategy of the task\ndependency graph for direct methods used in dense numerical linear algebra.\nThis strategy provides a balance of data locality, load balance, and low\ndequeue overhead. We show that the usage of this scheduling in communication\navoiding dense factorization leads to significant performance gains. On a 48\ncore AMD Opteron NUMA machine, our experiments show that we can achieve up to\n64% improvement over a version of CALU that uses fully dynamic scheduling, and\nup to 30% improvement over the version of CALU that uses fully static\nscheduling. On a 16-core Intel Xeon machine, our hybrid static/dynamic\nscheduling approach is up to 8% faster than the version of CALU that uses a\nfully static scheduling or fully dynamic scheduling. Our algorithm leads to\nspeedups over the corresponding routines for computing LU factorization in well\nknown libraries. On the 48 core AMD NUMA machine, our best implementation is up\nto 110% faster than MKL, while on the 16 core Intel Xeon machine, it is up to\n82% faster than MKL. Our approach also shows significant speedups compared with\nPLASMA on both of these systems.", 
    "link": "http://arxiv.org/pdf/1110.2677v1", 
    "arxiv-id": "1110.2677v1"
},{
    "category": "cs.DC", 
    "author": "Mema Roussopoulos", 
    "title": "On the Practicality of `Practical' Byzantine Fault Tolerance", 
    "publish": "2011-10-21T18:02:22Z", 
    "summary": "Byzantine Fault Tolerant (BFT) systems are considered by the systems research\ncommunity to be state of the art with regards to providing reliability in\ndistributed systems. BFT systems provide safety and liveness guarantees with\nreasonable assumptions, amongst a set of nodes where at most f nodes display\narbitrarily incorrect behaviors, known as Byzantine faults. Despite this, BFT\nsystems are still rarely used in practice. In this paper we describe our\nexperience, from an application developer's perspective, trying to leverage the\npublicly available and highly-tuned PBFT middleware (by Castro and Liskov), to\nprovide provable reliability guarantees for an electronic voting application\nwith high security and robustness needs. We describe several obstacles we\nencountered and drawbacks we identified in the PBFT approach. These include\nsome that we tackled, such as lack of support for dynamic client management and\nleaving state management completely up to the application. Others still\nremaining include the lack of robust handling of non-determinism, lack of\nsupport for web-based applications, lack of support for stronger cryptographic\nprimitives, and others. We find that, while many of the obstacles could be\novercome with a revised BFT middleware implementation that is tuned\nspecifically for the needs of the particular application, they require\nsignificant engineering effort and time and their performance implications for\nthe end-application are unclear. An application developer is thus unlikely to\nbe willing to invest the time and effort to do so to leverage the BFT approach.\nWe conclude that the research community needs to focus on the usability of BFT\nalgorithms for real world applications, from the end-developer perspective, in\naddition to continuing to improve the BFT middleware performance, robustness\nand deployment layouts.", 
    "link": "http://arxiv.org/pdf/1110.4854v1", 
    "arxiv-id": "1110.4854v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Reliable Provisioning of Spot Instances for Compute-intensive   Applications", 
    "publish": "2011-10-27T03:12:07Z", 
    "summary": "Cloud computing providers are now offering their unused resources for leasing\nin the spot market, which has been considered the first step towards a\nfull-fledged market economy for computational resources. Spot instances are\nvirtual machines (VMs) available at lower prices than their standard on-demand\ncounterparts. These VMs will run for as long as the current price is lower than\nthe maximum bid price users are willing to pay per hour. Spot instances have\nbeen increasingly used for executing compute-intensive applications. In spite\nof an apparent economical advantage, due to an intermittent nature of biddable\nresources, application execution times may be prolonged or they may not finish\nat all. This paper proposes a resource allocation strategy that addresses the\nproblem of running compute-intensive jobs on a pool of intermittent virtual\nmachines, while also aiming to run applications in a fast and economical way.\nTo mitigate potential unavailability periods, a multifaceted fault-aware\nresource provisioning policy is proposed. Our solution employs price and\nruntime estimation mechanisms, as well as three fault tolerance techniques,\nnamely checkpointing, task duplication and migration. We evaluate our\nstrategies using trace-driven simulations, which take as input real price\nvariation traces, as well as an application trace from the Parallel Workload\nArchive. Our results demonstrate the effectiveness of executing applications on\nspot instances, respecting QoS constraints, despite occasional failures.", 
    "link": "http://arxiv.org/pdf/1110.5969v1", 
    "arxiv-id": "1110.5969v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Provisioning Spot Market Cloud Resources to Create Cost-effective   Virtual Clusters", 
    "publish": "2011-10-27T03:22:25Z", 
    "summary": "Infrastructure-as-a-Service providers are offering their unused resources in\nthe form of variable-priced virtual machines (VMs), known as \"spot instances\",\nat prices significantly lower than their standard fixed-priced resources. To\nlease spot instances, users specify a maximum price they are willing to pay per\nhour and VMs will run only when the current price is lower than the user's bid.\nThis paper proposes a resource allocation policy that addresses the problem of\nrunning deadline-constrained compute-intensive jobs on a pool of composed\nsolely of spot instances, while exploiting variations in price and performance\nto run applications in a fast and economical way. Our policy relies on job\nruntime estimations to decide what are the best types of VMs to run each job\nand when jobs should run. Several estimation methods are evaluated and\ncompared, using trace-based simulations, which take real price variation traces\nobtained from Amazon Web Services as input, as well as an application trace\nfrom the Parallel Workload Archive. Results demonstrate the effectiveness of\nrunning computational jobs on spot instances, at a fraction (up to 60% lower)\nof the price that would normally cost on fixed priced resources.", 
    "link": "http://arxiv.org/pdf/1110.5972v1", 
    "arxiv-id": "1110.5972v1"
},{
    "category": "cs.DC", 
    "author": "Agnieszka \u0141upi\u0144ska", 
    "title": "Parallel implematation of flow and matching algorithms", 
    "publish": "2011-10-28T01:44:45Z", 
    "summary": "In our work we present two parallel algorithms and their lock-free\nimplementations using a popular GPU environment Nvidia CUDA. The first\nalgorithm is the push-relabel method for the flow problem in grid graphs. The\nsecond is the cost scaling algorithm for the assignment problem in complete\nbipartite graphs.", 
    "link": "http://arxiv.org/pdf/1110.6231v1", 
    "arxiv-id": "1110.6231v1"
},{
    "category": "cs.DC", 
    "author": "Lars Kotthoff", 
    "title": "Reliability of Computational Experiments on Virtualised Hardware", 
    "publish": "2011-10-28T10:21:23Z", 
    "summary": "We present preliminary results of an investigation into the suitability of\nvirtualised hardware -- in particular clouds -- for running computational\nexperiments. Our main concern was that the reported CPU time would not be\nreliable and reproducible. The results demonstrate that while this is true in\ncases where many virtual machines are running on the same physical hardware,\nthere is no inherent variation introduced by using virtualised hardware\ncompared to non-virtualised hardware.", 
    "link": "http://arxiv.org/pdf/1110.6288v1", 
    "arxiv-id": "1110.6288v1"
},{
    "category": "cs.DC", 
    "author": "George Kesidis", 
    "title": "Game Theoretic Iterative Partitioning for Dynamic Load Balancing in   Distributed Network Simulation", 
    "publish": "2011-11-03T15:21:25Z", 
    "summary": "High fidelity simulation of large-sized complex networks can be realized on a\ndistributed computing platform that leverages the combined resources of\nmultiple processors or machines. In a discrete event driven simulation, the\nassignment of logical processes (LPs) to machines is a critical step that\naffects the computational and communication burden on the machines, which in\nturn affects the simulation execution time of the experiment. We study a\nnetwork partitioning game wherein each node (LP) acts as a selfish player. We\nderive two local node-level cost frameworks which are feasible in the sense\nthat the aggregate state information required to be exchanged between the\nmachines is independent of the size of the simulated network model. For both\ncost frameworks, we prove the existence of stable Nash equilibria in pure\nstrategies. Using iterative partition improvements, we propose game theoretic\npartitioning algorithms based on the two cost criteria and show that each\ndescends in a global cost. To exploit the distributed nature of the system, the\nalgorithm is distributed, with each node's decision based on its local\ninformation and on a few global quantities which can be communicated\nmachine-to-machine. We demonstrate the performance of our partitioning\nalgorithm on an optimistic discrete event driven simulation platform that\nmodels an actual parallel simulator.", 
    "link": "http://arxiv.org/pdf/1111.0875v2", 
    "arxiv-id": "1111.0875v2"
},{
    "category": "cs.DC", 
    "author": "Gerhard Wellein", 
    "title": "Domain decomposition and locality optimization for large-scale lattice   Boltzmann simulations", 
    "publish": "2011-11-04T13:52:36Z", 
    "summary": "We present a simple, parallel and distributed algorithm for setting up and\npartitioning a sparse representation of a regular discretized simulation\ndomain. This method is scalable for a large number of processes even for\ncomplex geometries and ensures load balance between the domains, reasonable\ncommunication interfaces, and good data locality within the domain. Applying\nthis scheme to a list-based lattice Boltzmann flow solver can achieve similar\nor even higher flow solver performance than widely used standard graph\npartition based tools such as METIS and PT-SCOTCH.", 
    "link": "http://arxiv.org/pdf/1111.1129v1", 
    "arxiv-id": "1111.1129v1"
},{
    "category": "cs.DC", 
    "author": "Bertrand Meyer", 
    "title": "Record-replay debugging for the SCOOP concurrency model", 
    "publish": "2011-11-04T16:15:14Z", 
    "summary": "To support developers in writing reliable and efficient concurrent programs,\nnovel concurrent programming abstractions have been proposed in recent years.\nProgramming with such abstractions requires new analysis tools because the\nexecution semantics often differs considerably from established models. We\npresent a record-replay technique for programs written in SCOOP, an\nobject-oriented programming model for concurrency. The resulting tool enables\ndevelopers to reproduce the nondeterministic execution of a concurrent program,\na necessary prerequisite for debugging and testing.", 
    "link": "http://arxiv.org/pdf/1111.1170v3", 
    "arxiv-id": "1111.1170v3"
},{
    "category": "cs.DC", 
    "author": "Oleg Titov", 
    "title": "Choosing the best resource by method of mamdani", 
    "publish": "2011-11-06T20:35:30Z", 
    "summary": "A method for selecting the best service for the storage of information by\nMamdani.", 
    "link": "http://arxiv.org/pdf/1111.2237v1", 
    "arxiv-id": "1111.2237v1"
},{
    "category": "cs.DC", 
    "author": "C. Dinesh", 
    "title": "Secured Data Consistency and Storage Way in Untrusted Cloud using Server   Management Algorithm", 
    "publish": "2011-11-10T08:05:38Z", 
    "summary": "It is very challenging part to keep safely all required data that are needed\nin many applications for user in cloud. Storing our data in cloud may not be\nfully trustworthy. Since client doesn't have copy of all stored data, he has to\ndepend on Cloud Service Provider. But dynamic data operations, Read-Solomon and\nverification token construction methods don't tell us about total storage\ncapacity of server allocated space before and after the data addition in cloud.\nSo we have to introduce a new proposed system of efficient storage measurement\nand space comparison algorithm with time management for measuring the total\nallocated storage area before and after the data insertion in cloud. So by\nusing our proposed scheme, the value or weight of stored data before and after\nis measured by client with specified time in cloud storage area with accuracy.\nAnd here we also have proposed the multi-server restore point in server failure\ncondition. If there occurs any server failure, by using this scheme the data\ncan be recovered automatically in cloud server. Our proposed scheme efficiently\nchecks space for the in-outsourced data to maintain integrity. Here the TPA\nnecessarily doesn't have the delegation to audit user's data.", 
    "link": "http://arxiv.org/pdf/1111.2412v1", 
    "arxiv-id": "1111.2412v1"
},{
    "category": "cs.DC", 
    "author": "C. Dinesh", 
    "title": "Data Integrity and Dynamic Storage Way in Cloud Computing", 
    "publish": "2011-11-10T08:23:47Z", 
    "summary": "It is not an easy task to securely maintain all essential data where it has\nthe need in many applications for clients in cloud. To maintain our data in\ncloud, it may not be fully trustworthy because client doesn't have copy of all\nstored data. But any authors don't tell us data integrity through its user and\nCSP level by comparison before and after the data update in cloud. So we have\nto establish new proposed system for this using our data reading protocol\nalgorithm to check the integrity of data before and after the data insertion in\ncloud. Here the security of data before and after is checked by client with the\nhelp of CSP using our \"effective automatic data reading protocol from user as\nwell as cloud level into the cloud\" with truthfulness. Also we have proposed\nthe multi-server data comparison algorithm with the calculation of overall data\nin each update before its outsourced level for server restore access point for\nfuture data recovery from cloud data server. Our proposed scheme efficiently\nchecks integrity in efficient manner so that data integrity as well as security\ncan be maintained in all cases by considering drawbacks of existing methods.", 
    "link": "http://arxiv.org/pdf/1111.2418v1", 
    "arxiv-id": "1111.2418v1"
},{
    "category": "cs.DC", 
    "author": "Jian Lu", 
    "title": "Design of a Sliding Window over Asynchronous Event Streams", 
    "publish": "2011-11-13T15:20:49Z", 
    "summary": "The proliferation of sensing and monitoring applications motivates adoption\nof the event stream model of computation. Though sliding windows are widely\nused to facilitate effective event stream processing, it is greatly challenged\nwhen the event sources are distributed and asynchronous. To address this\nchallenge, we first show that the snapshots of the asynchronous event streams\nwithin the sliding window form a convex distributive lattice (denoted by\nLat-Win). Then we propose an algorithm to maintain Lat-Win at runtime. The\nLat-Win maintenance algorithm is implemented and evaluated on the open-source\ncontext-aware middleware we developed. The evaluation results first show the\nnecessity of adopting sliding windows over asynchronous event streams. Then\nthey show the performance of detecting specified predicates within Lat-Win,\neven when faced with dynamic changes in the computing environment.", 
    "link": "http://arxiv.org/pdf/1111.3022v1", 
    "arxiv-id": "1111.3022v1"
},{
    "category": "cs.DC", 
    "author": "Partha Sarathi Mandal", 
    "title": "Fixing Data Anomalies with Prediction Based Algorithm in Wireless Sensor   Networks", 
    "publish": "2011-11-14T19:43:04Z", 
    "summary": "Data inconsistencies are present in the data collected over a large wireless\nsensor network (WSN), usually deployed for any kind of monitoring applications.\nBefore passing this data to some WSN applications for decision making, it is\nnecessary to ensure that the data received are clean and accurate. In this\npaper, we have used a statistical tool to examine the past data to fit in a\nhighly sophisticated prediction model i.e., ARIMA for a given sensor node and\nwith this, the model corrects the data using forecast value if any data anomaly\nexists there. Another scheme is also proposed for detecting data anomaly at\nsink among the aggregated data in the data are received from a particular\nsensor node. The effectiveness of our methods are validated by data collected\nover a real WSN application consisting of Crossbow IRIS Motes\n\\cite{Crossbow:2009}.", 
    "link": "http://arxiv.org/pdf/1111.3334v1", 
    "arxiv-id": "1111.3334v1"
},{
    "category": "cs.DC", 
    "author": "Mohsen Sharifi", 
    "title": "A Low-Energy Fast Cyber Foraging Mechanism for Mobile Devices", 
    "publish": "2011-11-18T21:37:36Z", 
    "summary": "The ever increasing demands for using resource-constrained mobile devices for\nrunning more resource intensive applications nowadays has initiated the\ndevelopment of cyber foraging solutions that offload parts or whole\ncomputational intensive tasks to more powerful surrogate stationary computers\nand run them on behalf of mobile devices as required. The choice of proper mix\nof mobile devices and surrogates has remained an unresolved challenge though.\nIn this paper, we propose a new decision-making mechanism for cyber foraging\nsystems to select the best locations to run an application, based on context\nmetrics such as the specifications of surrogates, the specifications of mobile\ndevices, application specification, and communication network specification.\nExperimental results show faster response time and lower energy consumption of\nbenched applications compared to when applications run wholly on mobile devices\nand when applications are offloaded to surrogates blindly for execution.", 
    "link": "http://arxiv.org/pdf/1111.4499v1", 
    "arxiv-id": "1111.4499v1"
},{
    "category": "cs.DC", 
    "author": "Pascal Frossard", 
    "title": "Distributed Signal Processing via Chebyshev Polynomial Approximation", 
    "publish": "2011-11-22T16:15:32Z", 
    "summary": "Unions of graph multiplier operators are an important class of linear\noperators for processing signals defined on graphs. We present a novel method\nto efficiently distribute the application of these operators. The proposed\nmethod features approximations of the graph multipliers by shifted Chebyshev\npolynomials, whose recurrence relations make them readily amenable to\ndistributed computation. We demonstrate how the proposed method can be applied\nto distributed processing tasks such as smoothing, denoising, inverse\nfiltering, and semi-supervised classification, and show that the communication\nrequirements of the method scale gracefully with the size of the network.", 
    "link": "http://arxiv.org/pdf/1111.5239v2", 
    "arxiv-id": "1111.5239v2"
},{
    "category": "cs.DC", 
    "author": "Xiaofan Yang", 
    "title": "Hamiltonian Connectivity of Twisted Hypercube-Like Networks under the   Large Fault Model", 
    "publish": "2011-11-23T03:05:45Z", 
    "summary": "Twisted hypercube-like networks (THLNs) are an important class of\ninterconnection networks for parallel computing systems, which include most\npopular variants of the hypercubes, such as crossed cubes, M\\\"obius cubes,\ntwisted cubes and locally twisted cubes. This paper deals with the\nfault-tolerant hamiltonian connectivity of THLNs under the large fault model.\nLet $G$ be an $n$-dimensional THLN and $F \\subseteq V(G)\\bigcup E(G)$, where $n\n\\geq 7$ and $|F| \\leq 2n - 10$. We prove that for any two nodes $u,v \\in V(G -\nF)$ satisfying a simple necessary condition on neighbors of $u$ and $v$, there\nexists a hamiltonian or near-hamiltonian path between $u$ and $v$ in $G-F$. The\nresult extends further the fault-tolerant graph embedding capability of THLNs.", 
    "link": "http://arxiv.org/pdf/1111.5391v1", 
    "arxiv-id": "1111.5391v1"
},{
    "category": "cs.DC", 
    "author": "Wim H. Hesselink", 
    "title": "Partial mutual exclusion for infinitely many processes", 
    "publish": "2011-11-24T14:17:05Z", 
    "summary": "Partial mutual exclusion is the drinking philosophers problem for complete\ngraphs. It is the problem that a process may enter a critical section CS of its\ncode only when some finite set nbh of other processes are not in their critical\nsections. For each execution of CS, the set nbh can be given by the\nenvironment. We present a starvation free solution of this problem in a setting\nwith infinitely many processes, each with finite memory, that communicate by\nasynchronous messages. The solution has the property of first-come\nfirst-served, in so far as this can be guaranteed by asynchronous messages. For\nevery execution of CS and every process in nbh, between three and six messages\nare needed. The correctness of the solution is argued with invariants and\ntemporal logic. It has been verified with the proof assistant PVS.", 
    "link": "http://arxiv.org/pdf/1111.5775v2", 
    "arxiv-id": "1111.5775v2"
},{
    "category": "cs.DC", 
    "author": "Ilja Honkonen", 
    "title": "Improving the Load Balancing Performance of Vlasiator", 
    "publish": "2011-11-28T00:50:25Z", 
    "summary": "This whitepaper describes the load-balancing performance issues that are\nobserved and tackled during the petascaling of the Vlasiator codes. Vlasiator\nis a Vlasov-hybrid simulation code developed in Finnish Meteorological\nInstitute (FMI). Vlasiator models the communications associated with the\nspatial grid operated on as a hypergraph and partitions the grid using the\nparallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning\nframework. The result of partitioning determines the distribution of grid cells\nto processors. It is observed that the partitioning phase takes a substantial\npercentage of the overall computation time. Alternative\n(graph-partitioning-based) schemes that perform almost as well as the\nhypergraph partitioning scheme and that require less preprocessing overhead and\nbetter balance are proposed and investigated. A comparison in terms of effect\non running time, preprocessing overhead and load-balancing quality of Zoltan's\nPHG, ParMeTiS, and PT-SCOTCH are presented. Test results on J\\\"uelich\nBlueGene/P cluster are presented.", 
    "link": "http://arxiv.org/pdf/1111.6324v1", 
    "arxiv-id": "1111.6324v1"
},{
    "category": "cs.DC", 
    "author": "Zhifeng Sun", 
    "title": "Information Spreading in Dynamic Networks", 
    "publish": "2011-12-02T04:56:12Z", 
    "summary": "We study the fundamental problem of information spreading (also known as\ngossip) in dynamic networks. In gossip, or more generally, $k$-gossip, there\nare $k$ pieces of information (or tokens) that are initially present in some\nnodes and the problem is to disseminate the $k$ tokens to all nodes. The goal\nis to accomplish the task in as few rounds of distributed computation as\npossible. The problem is especially challenging in dynamic networks where the\nnetwork topology can change from round to round and can be controlled by an\non-line adversary.\n  The focus of this paper is on the power of token-forwarding algorithms, which\ndo not manipulate tokens in any way other than storing and forwarding them. We\nfirst consider a worst-case adversarial model first studied by Kuhn, Lynch, and\nOshman~\\cite{kuhn+lo:dynamic} in which the communication links for each round\nare chosen by an adversary, and nodes do not know who their neighbors for the\ncurrent round are before they broadcast their messages. Our main result is an\n$\\Omega(nk/\\log n)$ lower bound on the number of rounds needed for any\ndeterministic token-forwarding algorithm to solve $k$-gossip. This resolves an\nopen problem raised in~\\cite{kuhn+lo:dynamic}, improving their lower bound of\n$\\Omega(n \\log k)$, and matching their upper bound of $O(nk)$ to within a\nlogarithmic factor.\n  We next show that token-forwarding algorithms can achieve subquadratic time\nin the offline version of the problem where the adversary has to commit all the\ntopology changes in advance at the beginning of the computation, and present\ntwo polynomial-time offline token-forwarding algorithms. Our results are a step\ntowards understanding the power and limitation of token-forwarding algorithms\nin dynamic networks.", 
    "link": "http://arxiv.org/pdf/1112.0384v1", 
    "arxiv-id": "1112.0384v1"
},{
    "category": "cs.DC", 
    "author": "Karl Henrik Johansson", 
    "title": "Consensus over Random Graph Processes: Network Borel-Cantelli Lemmas for   Almost Sure Convergence", 
    "publish": "2011-12-06T16:33:04Z", 
    "summary": "Distributed consensus computation over random graph processes is considered.\nThe random graph process is defined as a sequence of random variables which\ntake values from the set of all possible digraphs over the node set. At each\ntime step, every node updates its state based on a Bernoulli trial, independent\nin time and among different nodes: either averaging among the neighbor set\ngenerated by the random graph, or sticking with its current state.\nConnectivity-independence and arc-independence are introduced to capture the\nfundamental influence of the random graphs on the consensus convergence.\nNecessary and/or sufficient conditions are presented on the success\nprobabilities of the Bernoulli trials for the network to reach a global almost\nsure consensus, with some sharp threshold established revealing a consensus\nzero-one law. Convergence rates are established by lower and upper bounds of\nthe $\\epsilon$-computation time. We also generalize the concepts of\nconnectivity/arc independence to their analogues from the $*$-mixing point of\nview, so that our results apply to a very wide class of graphical models,\nincluding the majority of random graph models in the literature, e.g.,\nErd\\H{o}s-R\\'{e}nyi, gossiping, and Markovian random graphs. We show that under\n$*$-mixing, our convergence analysis continues to hold and the corresponding\nalmost sure consensus conditions are established. Finally, we further\ninvestigate almost sure finite-time convergence of random gossiping algorithms,\nand prove that the Bernoulli trials play a key role in ensuring finite-time\nconvergence. These results add to the understanding of the interplay between\nrandom graphs, random computations, and convergence probability for distributed\ninformation processing.", 
    "link": "http://arxiv.org/pdf/1112.1336v5", 
    "arxiv-id": "1112.1336v5"
},{
    "category": "cs.DC", 
    "author": "Stefan Tai", 
    "title": "(MC2)2: A Generic Decision-Making Framework and its Application to Cloud   Computing", 
    "publish": "2011-12-08T14:46:08Z", 
    "summary": "Cloud computing is a disruptive technology, representing a new model for\ninformation technology (IT) solution engineering and management that promises\nto introduce significant cost savings and other benefits. The adoption of Cloud\ncomputing requires a detailed comparison of infrastructure alternatives, taking\na number of aspects into careful consideration. Existing methods of evaluation,\nhowever, limit decision making to the relative costs of cloud computing, but do\nnot take a broader range of criteria into account. In this paper, we introduce\na generic, multi-criteria-based decision framework and an application for Cloud\nComputing, the Multi-Criteria Comparison Method for Cloud Computing ((MC2)2).\nThe framework and method allow organizations to determine what infrastructure\nbest suits their needs by evaluating and ranking infrastructure alternatives\nusing multiple criteria. Therefore, (MC2)2 offers a way to differentiate\ninfrastructures not only by costs, but also in terms of benefits, opportunities\nand risks. (MC2)2 can be adapted to facilitate a wide array of decision-making\nscenarios within the domain of information technology infrastructures,\ndepending on the criteria selected to support the framework.", 
    "link": "http://arxiv.org/pdf/1112.1851v2", 
    "arxiv-id": "1112.1851v2"
},{
    "category": "cs.DC", 
    "author": "Thinn Thu Naing", 
    "title": "PC-Cluster based Storage System Architecture for Cloud Storage", 
    "publish": "2011-12-09T06:58:13Z", 
    "summary": "Design and architecture of cloud storage system plays a vital role in cloud\ncomputing infrastructure in order to improve the storage capacity as well as\ncost effectiveness. Usually cloud storage system provides users to efficient\nstorage space with elasticity feature. One of the challenges of cloud storage\nsystem is difficult to balance the providing huge elastic capacity of storage\nand investment of expensive cost for it. In order to solve this issue in the\ncloud storage infrastructure, low cost PC cluster based storage server is\nconfigured to be activated for large amount of data to provide cloud users.\nMoreover, one of the contributions of this system is proposed an analytical\nmodel using M/M/1 queuing network model, which is modeled on intended\narchitecture to provide better response time, utilization of storage as well as\npending time when the system is running. According to the analytical result on\nexperimental testing, the storage can be utilized more than 90% of storage\nspace. In this paper, two parts have been described such as (i) design and\narchitecture of PC cluster based cloud storage system. On this system, related\nto cloud applications, services configurations are explained in detailed. (ii)\nAnalytical model has been enhanced to be increased the storage utilization on\nthe target architecture.", 
    "link": "http://arxiv.org/pdf/1112.2025v1", 
    "arxiv-id": "1112.2025v1"
},{
    "category": "cs.DC", 
    "author": "Amir Asif", 
    "title": "Distributed Particle Filter Implementation with Intermittent/Irregular   Consensus Convergence", 
    "publish": "2011-12-12T03:23:47Z", 
    "summary": "Motivated by non-linear, non-Gaussian, distributed multi-sensor/agent\nnavigation and tracking applications, we propose a multi-rate consensus/fusion\nbased framework for distributed implementation of the particle filter (CF/DPF).\nThe CF/DPF framework is based on running localized particle filters to estimate\nthe overall state vector at each observation node. Separate fusion filters are\ndesigned to consistently assimilate the local filtering distributions into the\nglobal posterior by compensating for the common past information between\nneighbouring nodes. The CF/DPF offers two distinct advantages over its\ncounterparts. First, the CF/DPF framework is suitable for scenarios where\nnetwork connectivity is intermittent and consensus can not be reached between\ntwo consecutive observations. Second, the CF/DPF is not limited to the Gaussian\napproximation for the global posterior density. A third contribution of the\npaper is the derivation of the exact expression for computing the posterior\nCramer-Rao lower bound (PCRLB) for the distributed architecture based on a\nrecursive procedure involving the local Fisher information matrices (FIM) of\nthe distributed estimators. The performance of the CF/DPF algorithm closely\nfollows the centralized particle filter approaching the PCRLB at the signal to\nnoise ratios that we tested.", 
    "link": "http://arxiv.org/pdf/1112.2431v2", 
    "arxiv-id": "1112.2431v2"
},{
    "category": "cs.DC", 
    "author": "Safaai Bin Deris", 
    "title": "Web Services Non-Functional Classification to Enhance Discovery Speed", 
    "publish": "2011-12-16T07:39:02Z", 
    "summary": "Recently, the use and deployment of web services has dramatically increased.\nThis is due to the easiness, interoperability, and flexibility that web\nservices offer to the software systems, which other software structures don't\nsupport or support poorly. Web services discovery became more important and\nresearch conducted in this area became more critical. With the increasing\nnumber of published and publicly available web services, speed in web service\ndiscovery process is becoming an issue which cannot be neglected. This paper\nproposes a generic non-functional based web services classification algorithm.\nClassification algorithm depends on information supplied by web service\nprovider at the registration time. Authors have proved mathematically and\nexperimentally the usefulness and efficiency of proposed algorithm.", 
    "link": "http://arxiv.org/pdf/1112.3725v1", 
    "arxiv-id": "1112.3725v1"
},{
    "category": "cs.DC", 
    "author": "Rajiv Ranjan", 
    "title": "CloudGenius: Automated Decision Support for Migrating Multi-Component   Enterprise Applications to Clouds", 
    "publish": "2011-12-16T16:24:34Z", 
    "summary": "One of the key problems in migrating multi-component enterprise applications\nto Clouds is selecting the best mix of VM images and Cloud infrastructure\nservices. A migration process has to ensure that Quality of Service (QoS)\nrequirements are met, while satisfying conflicting selection criteria, e.g.\nthroughput and cost. When selecting Cloud services, application engineers must\nconsider heterogeneous sets of criteria and complex dependencies across\nmultiple layers impossible to resolve manually. To overcome this challenge, we\npresent the generic recommender framework CloudGenius and an implementation\nthat leverage well known multi-criteria decision making technique Analytic\nHierarchy Process to automate the selection process based on a model, factors,\nand QoS requirements related to enterprise applications. In particular, we\nintroduce a structured migration process for multi-component enterprise\napplications, clearly identify the most important criteria relevant to the\nselection problem and present a multi-criteria-based selection algorithm.\nExperiments with the software prototype CumulusGenius show time complexities.", 
    "link": "http://arxiv.org/pdf/1112.3880v2", 
    "arxiv-id": "1112.3880v2"
},{
    "category": "cs.DC", 
    "author": "Meni Rosenfeld", 
    "title": "Analysis of Bitcoin Pooled Mining Reward Systems", 
    "publish": "2011-12-21T10:40:38Z", 
    "summary": "In this paper we describe the various scoring systems used to calculate\nrewards of participants in Bitcoin pooled mining, explain the problems each\nwere designed to solve and analyze their respective advantages and\ndisadvantages.", 
    "link": "http://arxiv.org/pdf/1112.4980v1", 
    "arxiv-id": "1112.4980v1"
},{
    "category": "cs.DC", 
    "author": "Abhiram Ranade", 
    "title": "Scheduling Light-trails in WDM Rings", 
    "publish": "2011-12-29T08:56:42Z", 
    "summary": "We consider the problem of scheduling communication on optical WDM\n(wavelength division multiplexing) networks using the light-trails technology.\nWe seek to design scheduling algorithms such that the given transmission\nrequests can be scheduled using minimum number of wavelengths (optical\nchannels). We provide algorithms and close lower bounds for two versions of the\nproblem on an $n$ processor linear array/ring network. In the {\\em stationary}\nversion, the pattern of transmissions (given) is assumed to not change over\ntime. For this, a simple lower bound is $c$, the congestion or the maximum\ntotal traffic required to pass through any link. We give an algorithm that\nschedules the transmissions using $O(c+\\log{n})$ wavelengths. We also show a\npattern for which $\\Omega(c+\\log{n}/\\log\\log{n})$ wavelengths are needed. In\nthe {\\em on-line} version, the transmissions arrive and depart dynamically, and\nmust be scheduled without upsetting the previously scheduled transmissions. For\nthis case we give an on-line algorithm which has competitive ratio\n$\\Theta(\\log{n})$. We show that this is optimal in the sense that every on-line\nalgorithm must have competitive ratio $\\Omega(\\log{n})$. We also give an\nalgorithm that appears to do well in simulation (for the classes of traffic we\nconsider), but which has competitive ratio between $\\Omega(\\log^2n/\\log\n\\log{n})$ and $O(\\log^2n)$. We present detailed simulations of both our\nalgorithms.", 
    "link": "http://arxiv.org/pdf/1112.6254v1", 
    "arxiv-id": "1112.6254v1"
},{
    "category": "cs.DC", 
    "author": "Ljupco Krstevski", 
    "title": "On the Performance of Exhaustive Search with Cooperating agents", 
    "publish": "2012-01-01T16:03:49Z", 
    "summary": "Despite the occurrence of elegant algorithms for solving complex problem,\nexhaustive search has retained its significance since many real-life problems\nexhibit no regular structure and exhaustive search is the only possible\nsolution. The advent of high-performance computing either via multicore\nprocessors or distributed processors emphasizes the possibility for exhaustive\nsearch by multiple search agents. Here we analyse the performance of exhaustive\nsearch when it is conducted by multiple search agents. Several strategies for\ncooperation between the search agents are evaluated. We discover that the\nperformance of the search improves with the increase in the level of\ncooperation. Same search performance can be achieved with homogeneous and\nheterogeneous search agents provided that the length of subregions allocated to\nindividual search regions follow the differences in the speeds of heterogeneous\nsearch agents.", 
    "link": "http://arxiv.org/pdf/1201.0360v1", 
    "arxiv-id": "1201.0360v1"
},{
    "category": "cs.DC", 
    "author": "Jian Tao", 
    "title": "A Massive Data Parallel Computational Framework for Petascale/Exascale   Hybrid Computer Systems", 
    "publish": "2012-01-10T17:20:17Z", 
    "summary": "Heterogeneous systems are becoming more common on High Performance Computing\n(HPC) systems. Even using tools like CUDA and OpenCL it is a non-trivial task\nto obtain optimal performance on the GPU. Approaches to simplifying this task\ninclude Merge (a library based framework for heterogeneous multi-core systems),\nZippy (a framework for parallel execution of codes on multiple GPUs), BSGP (a\nnew programming language for general purpose computation on the GPU) and\nCUDA-lite (an enhancement to CUDA that transforms code based on annotations).\nIn addition, efforts are underway to improve compiler tools for automatic\nparallelization and optimization of affine loop nests for GPUs and for\nautomatic translation of OpenMP parallelized codes to CUDA.\n  In this paper we present an alternative approach: a new computational\nframework for the development of massively data parallel scientific codes\napplications suitable for use on such petascale/exascale hybrid systems built\nupon the highly scalable Cactus framework. As the first non-trivial\ndemonstration of its usefulness, we successfully developed a new 3D CFD code\nthat achieves improved performance.", 
    "link": "http://arxiv.org/pdf/1201.2118v1", 
    "arxiv-id": "1201.2118v1"
},{
    "category": "cs.DC", 
    "author": "S. Ramachandram", 
    "title": "Purging of untrustworthy recommendations from a grid", 
    "publish": "2012-01-10T17:46:16Z", 
    "summary": "In grid computing, trust has massive significance. There is lot of research\nto propose various models in providing trusted resource sharing mechanisms. The\ntrust is a belief or perception that various researchers have tried to\ncorrelate with some computational model. Trust on any entity can be direct or\nindirect. Direct trust is the impact of either first impression over the entity\nor acquired during some direct interaction. Indirect trust is the trust may be\ndue to either reputation gained or recommendations received from various\nrecommenders of a particular domain in a grid or any other domain outside that\ngrid or outside that grid itself. Unfortunately, malicious indirect trust leads\nto the misuse of valuable resources of the grid. This paper proposes the\nmechanism of identifying and purging the untrustworthy recommendations in the\ngrid environment. Through the obtained results, we show the way of purging of\nuntrustworthy entities.", 
    "link": "http://arxiv.org/pdf/1201.2125v1", 
    "arxiv-id": "1201.2125v1"
},{
    "category": "cs.DC", 
    "author": "Pasquale Cataldi", 
    "title": "Adaptive Redundancy Management for Durable P2P Backup", 
    "publish": "2012-01-11T17:56:56Z", 
    "summary": "We design and analyze the performance of a redundancy management mechanism\nfor Peer-to-Peer backup applications. Armed with the realization that a backup\nsystem has peculiar requirements -- namely, data is read over the network only\nduring restore processes caused by data loss -- redundancy management targets\ndata durability rather than attempting to make each piece of information\navailabile at any time.\n  In our approach each peer determines, in an on-line manner, an amount of\nredundancy sufficient to counter the effects of peer deaths, while preserving\nacceptable data restore times. Our experiments, based on trace-driven\nsimulations, indicate that our mechanism can reduce the redundancy by a factor\nbetween two and three with respect to redundancy policies aiming for data\navailability. These results imply an according increase in storage capacity and\ndecrease in time to complete backups, at the expense of longer times required\nto restore data. We believe this is a very reasonable price to pay, given the\nnature of the application.\n  We complete our work with a discussion on practical issues, and their\nsolutions, related to which encoding technique is more suitable to support our\nscheme.", 
    "link": "http://arxiv.org/pdf/1201.2360v2", 
    "arxiv-id": "1201.2360v2"
},{
    "category": "cs.DC", 
    "author": "Brian Vinter", 
    "title": "Managing Communication Latency-Hiding at Runtime for Parallel   Programming Languages and Libraries", 
    "publish": "2012-01-18T14:43:43Z", 
    "summary": "This work introduces a runtime model for managing communication with support\nfor latency-hiding. The model enables non-computer science researchers to\nexploit communication latency-hiding techniques seamlessly. For compiled\nlanguages, it is often possible to create efficient schedules for\ncommunication, but this is not the case for interpreted languages. By\nmaintaining data dependencies between scheduled operations, it is possible to\naggressively initiate communication and lazily evaluate tasks to allow maximal\ntime for the communication to finish before entering a wait state. We implement\na heuristic of this model in DistNumPy, an auto-parallelizing version of\nnumerical Python that allows sequential NumPy programs to run on distributed\nmemory architectures. Furthermore, we present performance comparisons for eight\nbenchmarks with and without automatic latency-hiding. The results shows that\nour model reduces the time spent on waiting for communication as much as 27\ntimes, from a maximum of 54% to only 2% of the total execution time, in a\nstencil application.", 
    "link": "http://arxiv.org/pdf/1201.3804v1", 
    "arxiv-id": "1201.3804v1"
},{
    "category": "cs.DC", 
    "author": "Guanfeng Liang", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs", 
    "publish": "2012-01-19T22:05:08Z", 
    "summary": "In this paper, we explore the problem of iterative approximate Byzantine\nconsensus in arbitrary directed graphs. In particular, we prove a necessary and\nsufficient condition for the existence of iterative byzantine consensus\nalgorithms. Additionally, we use our sufficient condition to examine whether\nsuch algorithms exist for some specific graphs.", 
    "link": "http://arxiv.org/pdf/1201.4183v2", 
    "arxiv-id": "1201.4183v2"
},{
    "category": "cs.DC", 
    "author": "Shir Peled", 
    "title": "\"Tri, Tri again\": Finding Triangles and Small Subgraphs in a Distributed   Setting", 
    "publish": "2012-01-31T19:02:49Z", 
    "summary": "Let G = (V,E) be an n-vertex graph and M_d a d-vertex graph, for some\nconstant d. Is M_d a subgraph of G? We consider this problem in a model where\nall n processes are connected to all other processes, and each message contains\nup to O(log n) bits. A simple deterministic algorithm that requires\nO(n^((d-2)/d) / log n) communication rounds is presented. For the special case\nthat M_d is a triangle, we present a probabilistic algorithm that requires an\nexpected O(ceil(n^(1/3) / (t^(2/3) + 1))) rounds of communication, where t is\nthe number of triangles in the graph, and O(min{n^(1/3) log^(2/3) n / (t^(2/3)\n+ 1), n^(1/3)}) with high probability.\n  We also present deterministic algorithms specially suited for sparse graphs.\nIn any graph of maximum degree Delta, we can test for arbitrary subgraphs of\ndiameter D in O(ceil(Delta^(D+1) / n)) rounds. For triangles, we devise an\nalgorithm featuring a round complexity of O(A^2 / n + log_(2+n/A^2) n), where A\ndenotes the arboricity of G.", 
    "link": "http://arxiv.org/pdf/1201.6652v3", 
    "arxiv-id": "1201.6652v3"
},{
    "category": "cs.DC", 
    "author": "Durg Singh Chauhan", 
    "title": "Case Tool: Fast Interconnections with New 3-Disjoint Paths MIN   Simulation Module", 
    "publish": "2012-02-03T07:03:49Z", 
    "summary": "Multi-stage interconnection networks (MIN) can be designed to achieve fault\ntolerance and collision solving by providing a set of disjoint paths. In this\npaper, we are discussing the new simulator added to the tool designed for\ndeveloping fault tolerant MINs. The designed tool is one of its own kind and\nwill help the user in developing 2 and 3-disjoint path networks. The java\ntechnology has been used to design the tool and have been tested on different\nsoftware platform.", 
    "link": "http://arxiv.org/pdf/1202.0616v1", 
    "arxiv-id": "1202.0616v1"
},{
    "category": "cs.DC", 
    "author": "Alexander Schill", 
    "title": "\u03c0-Control: A Personal Cloud Control Centre", 
    "publish": "2012-02-05T15:25:02Z", 
    "summary": "Consumption of online services and cloud computing offerings is on the rise,\nlargely due to compelling advantages over traditional local applications. From\na user perspective, these include zero-maintenance of software, the always-on\nnature of such services, mashups of different applications and the networking\neffect with other users. Associated disadvantages are known, but effective\nmeans and tools to limit their effect are not yet well-established and not yet\ngenerally available to service users. We propose (1) a user-centric model of\ncloud elements beyond the conventional <SPI>aaS layers, including activities\nacross trust zones, and (2) a personal control console for all individual and\ncollaborative user activities in the cloud.", 
    "link": "http://arxiv.org/pdf/1202.0970v1", 
    "arxiv-id": "1202.0970v1"
},{
    "category": "cs.DC", 
    "author": "Mahesh Chandra Govil", 
    "title": "On Stability Problems of Omega and 3-Disjoint Paths Omega Multi-stage   Interconnection Networks", 
    "publish": "2012-02-06T07:39:06Z", 
    "summary": "The research paper emphasizes that the Stable Matching problems are the same\nas the problems of stable configurations of Multi-stage Interconnection\nNetworks (MIN). We have discusses the Stability Problems of Existing Regular\nOmega Multi-stage Interconnection Network (OMIN) and Proposed 3-Disjoint Paths\nOmega Multi-stage Interconnection Network (3DON) using the approaches and\nsolutions provided by the Stable Matching Problem. Specifically, Stable\nMarriage Problem is used as an example of Stable Matching. On application of\nthe concept of the Stable Marriage over the MINs states that OMIN is highly\nstable in comparison to 3DON.", 
    "link": "http://arxiv.org/pdf/1202.1062v1", 
    "arxiv-id": "1202.1062v1"
},{
    "category": "cs.DC", 
    "author": "Roger Wattenhofer", 
    "title": "Stone Age Distributed Computing", 
    "publish": "2012-02-06T16:20:06Z", 
    "summary": "The traditional models of distributed computing focus mainly on networks of\ncomputer-like devices that can exchange large messages with their neighbors and\nperform arbitrary local computations. Recently, there is a trend to apply\ndistributed computing methods to networks of sub-microprocessor devices, e.g.,\nbiological cellular networks or networks of nano-devices. However, the\nsuitability of the traditional distributed computing models to these types of\nnetworks is questionable: do tiny bio/nano nodes \"compute\" and/or \"communicate\"\nessentially the same as a computer? In this paper, we introduce a new model\nthat depicts a network of randomized finite state machines operating in an\nasynchronous environment. Although the computation and communication\ncapabilities of each individual device in the new model are, by design, much\nweaker than those of a computer, we show that some of the most important and\nextensively studied distributed computing problems can still be solved\nefficiently.", 
    "link": "http://arxiv.org/pdf/1202.1186v2", 
    "arxiv-id": "1202.1186v2"
},{
    "category": "cs.DC", 
    "author": "Johannes Schneider", 
    "title": "The Locality of Distributed Symmetry Breaking", 
    "publish": "2012-02-09T13:46:11Z", 
    "summary": "Symmetry breaking problems are among the most well studied in the field of\ndistributed computing and yet the most fundamental questions about their\ncomplexity remain open. In this paper we work in the LOCAL model (where the\ninput graph and underlying distributed network are identical) and study the\nrandomized complexity of four fundamental symmetry breaking problems on graphs:\ncomputing MISs (maximal independent sets), maximal matchings, vertex colorings,\nand ruling sets. A small sample of our results includes\n  - An MIS algorithm running in $O(\\log^2\\Delta + 2^{O(\\sqrt{\\log\\log n})})$\ntime, where $\\Delta$ is the maximum degree. This is the first MIS algorithm to\nimprove on the 1986 algorithms of Luby and Alon, Babai, and Itai, when $\\log n\n\\ll \\Delta \\ll 2^{\\sqrt{\\log n}}$, and comes close to the $\\Omega(\\log \\Delta)$\nlower bound of Kuhn, Moscibroda, and Wattenhofer.\n  - A maximal matching algorithm running in $O(\\log\\Delta + \\log^4\\log n)$\ntime. This is the first significant improvement to the 1986 algorithm of\nIsraeli and Itai. Moreover, its dependence on $\\Delta$ is provably optimal.\n  - A method for reducing symmetry breaking problems in low\narboricity/degeneracy graphs to low degree graphs. (Roughly speaking, the\narboricity or degeneracy of a graph bounds the density of any subgraph.)\nCorollaries of this reduction include an $O(\\sqrt{\\log n})$-time maximal\nmatching algorithm for graphs with arboricity up to $2^{\\sqrt{\\log n}}$ and an\n$O(\\log^{2/3} n)$-time MIS algorithm for graphs with arboricity up to $2^{(\\log\nn)^{1/3}}$.\n  Each of our algorithms is based on a simple, but powerful technique for\nreducing a randomized symmetry breaking task to a corresponding deterministic\none on a poly$(\\log n)$-size graph.", 
    "link": "http://arxiv.org/pdf/1202.1983v3", 
    "arxiv-id": "1202.1983v3"
},{
    "category": "cs.DC", 
    "author": "Dana Petcu", 
    "title": "DEPAS: A Decentralized Probabilistic Algorithm for Auto-Scaling", 
    "publish": "2012-02-12T09:26:40Z", 
    "summary": "The dynamic provisioning of virtualized resources offered by cloud computing\ninfrastructures allows applications deployed in a cloud environment to\nautomatically increase and decrease the amount of used resources. This\ncapability is called auto-scaling and its main purpose is to automatically\nadjust the scale of the system that is running the application to satisfy the\nvarying workload with minimum resource utilization. The need for auto-scaling\nis particularly important during workload peaks, in which applications may need\nto scale up to extremely large-scale systems.\n  Both the research community and the main cloud providers have already\ndeveloped auto-scaling solutions. However, most research solutions are\ncentralized and not suitable for managing large-scale systems, moreover cloud\nproviders' solutions are bound to the limitations of a specific provider in\nterms of resource prices, availability, reliability, and connectivity.\n  In this paper we propose DEPAS, a decentralized probabilistic auto-scaling\nalgorithm integrated into a P2P architecture that is cloud provider\nindependent, thus allowing the auto-scaling of services over multiple cloud\ninfrastructures at the same time. Our simulations, which are based on real\nservice traces, show that our approach is capable of: (i) keeping the overall\nutilization of all the instantiated cloud resources in a target range, (ii)\nmaintaining service response times close to the ones obtained using optimal\ncentralized auto-scaling approaches.", 
    "link": "http://arxiv.org/pdf/1202.2509v1", 
    "arxiv-id": "1202.2509v1"
},{
    "category": "cs.DC", 
    "author": "Valentin Cristea", 
    "title": "A Simulation Model for Evaluating Distributed Systems Dependability", 
    "publish": "2012-02-12T18:17:10Z", 
    "summary": "In this paper we present a new simulation model designed to evaluate the\ndependability in distributed systems. This model extends the MONARC simulation\nmodel with new capabilities for capturing reliability, safety, availability,\nsecurity, and maintainability requirements. The model has been implemented as\nan extension of the multithreaded, process oriented simulator MONARC, which\nallows the realistic simulation of a wide-range of distributed system\ntechnologies, with respect to their specific components and characteristics.\nThe extended simulation model includes the necessary components to inject\nvarious failure events, and provides the mechanisms to evaluate different\nstrategies for replication, redundancy procedures, and security enforcement\nmechanisms, as well. The results obtained in simulation experiments presented\nin this paper probe that the use of discrete-event simulators, such as MONARC,\nin the design and development of distributed systems is appealing due to their\nefficiency and scalability.", 
    "link": "http://arxiv.org/pdf/1202.2551v1", 
    "arxiv-id": "1202.2551v1"
},{
    "category": "cs.DC", 
    "author": "Dana Petcu", 
    "title": "Theoretical Analysis and Tuning of Decentralized Probabilistic   Auto-Scaling", 
    "publish": "2012-02-14T10:21:51Z", 
    "summary": "A major impediment towards the industrial adoption of decentralized\ndistributed systems comes from the difficulty to theoretically prove that these\nsystems exhibit the required behavior. In this paper, we use probability theory\nto analyze a decentralized auto-scaling algorithm in which each node\nprobabilistically decides to scale in or out. We prove that, in the context of\ndynamic workloads, the average load of the system is maintained within a\nvariation interval with a given probability, provided that the number of nodes\nand the variation interval length are higher than certain bounds. The paper\nalso proposes numerical algorithms for approximating these minimum bounds.", 
    "link": "http://arxiv.org/pdf/1202.2981v1", 
    "arxiv-id": "1202.2981v1"
},{
    "category": "cs.DC", 
    "author": "Anne-Marie Kermarrec", 
    "title": "On Dynamic Distributed Computing", 
    "publish": "2012-02-14T16:50:12Z", 
    "summary": "This paper shows for the first time that distributed computing can be both\nreliable and efficient in an environment that is both highly dynamic and\nhostile. More specifically, we show how to maintain clusters of size $O(\\log\nN)$, each containing more than two thirds of honest nodes with high\nprobability, within a system whose size can vary \\textit{polynomially} with\nrespect to its initial size. Furthermore, the communication cost induced by\neach node arrival or departure is polylogarithmic with respect to $N$, the\nmaximal size of the system. Our clustering can be achieved despite the presence\nof a Byzantine adversary controlling a fraction $\\bad \\leq \\{1}{3}-\\epsilon$ of\nthe nodes, for some fixed constant $\\epsilon > 0$, independent of $N$. So far,\nsuch a clustering could only be performed for systems who size can vary\nconstantly and it was not clear whether that was at all possible for polynomial\nvariances.", 
    "link": "http://arxiv.org/pdf/1202.3084v2", 
    "arxiv-id": "1202.3084v2"
},{
    "category": "cs.DC", 
    "author": "Matei Ripeanu", 
    "title": "GPUs as Storage System Accelerators", 
    "publish": "2012-02-16T19:08:29Z", 
    "summary": "Massively multicore processors, such as Graphics Processing Units (GPUs),\nprovide, at a comparable price, a one order of magnitude higher peak\nperformance than traditional CPUs. This drop in the cost of computation, as any\norder-of-magnitude drop in the cost per unit of performance for a class of\nsystem components, triggers the opportunity to redesign systems and to explore\nnew ways to engineer them to recalibrate the cost-to-performance relation. This\nproject explores the feasibility of harnessing GPUs' computational power to\nimprove the performance, reliability, or security of distributed storage\nsystems. In this context, we present the design of a storage system prototype\nthat uses GPU offloading to accelerate a number of computationally intensive\nprimitives based on hashing, and introduce techniques to efficiently leverage\nthe processing power of GPUs. We evaluate the performance of this prototype\nunder two configurations: as a content addressable storage system that\nfacilitates online similarity detection between successive versions of the same\nfile and as a traditional system that uses hashing to preserve data integrity.\nFurther, we evaluate the impact of offloading to the GPU on competing\napplications' performance. Our results show that this technique can bring\ntangible performance gains without negatively impacting the performance of\nconcurrently running applications.", 
    "link": "http://arxiv.org/pdf/1202.3669v2", 
    "arxiv-id": "1202.3669v2"
},{
    "category": "cs.DC", 
    "author": "Justin M. Wozniak", 
    "title": "Many-Task Computing and Blue Waters", 
    "publish": "2012-02-17T16:01:53Z", 
    "summary": "This report discusses many-task computing (MTC) generically and in the\ncontext of the proposed Blue Waters systems, which is planned to be the largest\nNSF-funded supercomputer when it begins production use in 2012. The aim of this\nreport is to inform the BW project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies.\nMany MTC applications do not neatly fit the stereotypes of high-performance\ncomputing (HPC) or high-throughput computing (HTC) applications. Like HTC\napplications, by definition MTC applications are structured as graphs of\ndiscrete tasks, with explicit input and output dependencies forming the graph\nedges. However, MTC applications have significant features that distinguish\nthem from typical HTC applications. In particular, different engineering\nconstraints for hardware and software must be met in order to support these\napplications. HTC applications have traditionally run on platforms such as\ngrids and clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time to\nsolution, may be communication intensive or data intensive, and may comprise\nvery short tasks. Therefore, hardware and software for MTC must be engineered\nto support the additional communication and I/O and must minimize task dispatch\noverheads. The hardware of large-scale HPC systems, with its high degree of\nparallelism and support for intensive communication, is well suited for MTC\napplications. However, HPC systems often lack a dynamic resource-provisioning\nfeature, are not ideal for task communication via the file system, and have an\nI/O system that is not optimized for MTC-style applications. Hence, additional\nsoftware support is likely to be required to gain full benefit from the HPC\nhardware.", 
    "link": "http://arxiv.org/pdf/1202.3943v1", 
    "arxiv-id": "1202.3943v1"
},{
    "category": "cs.DC", 
    "author": "Amit Bawaskar", 
    "title": "GPGPU Processing in CUDA Architecture", 
    "publish": "2012-02-20T15:16:40Z", 
    "summary": "The future of computation is the Graphical Processing Unit, i.e. the GPU. The\npromise that the graphics cards have shown in the field of image processing and\naccelerated rendering of 3D scenes, and the computational capability that these\nGPUs possess, they are developing into great parallel computing units. It is\nquite simple to program a graphics processor to perform general parallel tasks.\nBut after understanding the various architectural aspects of the graphics\nprocessor, it can be used to perform other taxing tasks as well. In this paper,\nwe will show how CUDA can fully utilize the tremendous power of these GPUs.\nCUDA is NVIDIA's parallel computing architecture. It enables dramatic increases\nin computing performance, by harnessing the power of the GPU. This paper talks\nabout CUDA and its architecture. It takes us through a comparison of CUDA C/C++\nwith other parallel programming languages like OpenCL and DirectCompute. The\npaper also lists out the common myths about CUDA and how the future seems to be\npromising for CUDA.", 
    "link": "http://arxiv.org/pdf/1202.4347v1", 
    "arxiv-id": "1202.4347v1"
},{
    "category": "cs.DC", 
    "author": "Johannes Reich", 
    "title": "Processes, Roles and Their Interactions", 
    "publish": "2012-02-21T01:41:49Z", 
    "summary": "Taking an interaction network oriented perspective in informatics raises the\nchallenge to describe deterministic finite systems which take part in networks\nof nondeterministic interactions. The traditional approach to describe\nprocesses as stepwise executable activities which are not based on the\nordinarily nondeterministic interaction shows strong centralization tendencies.\nAs suggested in this article, viewing processes and their interactions as\ncomplementary can circumvent these centralization tendencies.\n  The description of both, processes and their interactions is based on the\nsame building blocks, namely finite input output automata (or transducers).\nProcesses are viewed as finite systems that take part in multiple, ordinarily\nnondeterministic interactions. The interactions between processes are described\nas protocols.\n  The effects of communication between processes as well as the necessary\ncoordination of different interactions within a processes are both based on the\nrestriction of the transition relation of product automata. The channel based\nouter coupling represents the causal relation between the output and the input\nof different systems. The coordination condition based inner coupling\nrepresents the causal relation between the input and output of a single system.\n  All steps are illustrated with the example of a network of resource\nadministration processes which is supposed to provide requesting user processes\nexclusive access to a single resource.", 
    "link": "http://arxiv.org/pdf/1202.4508v1", 
    "arxiv-id": "1202.4508v1"
},{
    "category": "cs.DC", 
    "author": "Santiago Zazo", 
    "title": "Belief Consensus Algorithms for Fast Distributed Target Tracking in   Wireless Sensor Networks", 
    "publish": "2012-02-23T18:32:59Z", 
    "summary": "In distributed target tracking for wireless sensor networks, agreement on the\ntarget state can be achieved by the construction and maintenance of a\ncommunication path, in order to exchange information regarding local likelihood\nfunctions. Such an approach lacks robustness to failures and is not easily\napplicable to ad-hoc networks. To address this, several methods have been\nproposed that allow agreement on the global likelihood through fully\ndistributed belief consensus (BC) algorithms, operating on local likelihoods in\ndistributed particle filtering (DPF). However, a unified comparison of the\nconvergence speed and communication cost has not been performed. In this paper,\nwe provide such a comparison and propose a novel BC algorithm based on belief\npropagation (BP). According to our study, DPF based on metropolis belief\nconsensus (MBC) is the fastest in loopy graphs, while DPF based on BP consensus\nis the fastest in tree graphs. Moreover, we found that BC-based DPF methods\nhave lower communication overhead than data flooding when the network is\nsufficiently sparse.", 
    "link": "http://arxiv.org/pdf/1202.5261v4", 
    "arxiv-id": "1202.5261v4"
},{
    "category": "cs.DC", 
    "author": "Richard McClatchey", 
    "title": "Risk-Driven Compliant Access Controls for Clouds", 
    "publish": "2012-02-24T15:49:39Z", 
    "summary": "There is widespread agreement that cloud computing have proven cost cutting\nand agility benefits. However, security and regulatory compliance issues are\ncontinuing to challenge the wide acceptance of such technology both from social\nand commercial stakeholders. An important facture behind this is the fact that\nclouds and in particular public clouds are usually deployed and used within\nbroad geographical or even international domains. This implies that the\nexchange of private and other protected data within the cloud environment would\nbe governed by multiple jurisdictions. These jurisdictions have a great degree\nof harmonisation; however, they present possible conflicts that are hard to\nnegotiate at run time. So far, important efforts were played in order to deal\nwith regulatory compliance management for large distributed systems. However,\nmeasurable solutions are required for the context of cloud. In this position\npaper, we are suggesting an approach that starts with a conceptual model of\nexplicit regulatory requirements for exchanging private data on a\nmultijurisdictional environment and build on it in order to define metrics for\nnon-compliance or, in other terms, risks to compliance. These metrics will be\nintegrated within usual data access-control policies and will be checked at\npolicy analysis time before a decision to allow/deny the data access is made.", 
    "link": "http://arxiv.org/pdf/1202.5482v2", 
    "arxiv-id": "1202.5482v2"
},{
    "category": "cs.DC", 
    "author": "Ashiq Anjum", 
    "title": "An Architecture for Integrated Intelligence in Urban Management using   Cloud Computing", 
    "publish": "2012-02-24T15:52:56Z", 
    "summary": "With the emergence of new methodologies and technologies it has now become\npossible to manage large amounts of environmental sensing data and apply new\nintegrated computing models to acquire information intelligence. This paper\nadvocates the application of cloud capacity to support the information,\ncommunication and decision making needs of a wide variety of stakeholders in\nthe complex business of the management of urban and regional development. The\ncomplexity lies in the interactions and impacts embodied in the concept of the\nurban-ecosystem at various governance levels. This highlights the need for more\neffective integrated environmental management systems. This paper offers a\nuser-orientated approach based on requirements for an effective management of\nthe urban-ecosystem and the potential contributions that can be supported by\nthe cloud computing community. Furthermore, the commonality of the influence of\nthe drivers of change at the urban level offers the opportunity for the cloud\ncomputing community to develop generic solutions that can serve the needs of\nhundreds of cities from Europe and indeed globally.", 
    "link": "http://arxiv.org/pdf/1202.5483v1", 
    "arxiv-id": "1202.5483v1"
},{
    "category": "cs.DC", 
    "author": "Richard McClatchey", 
    "title": "A Fault Tolerant, Dynamic and Low Latency BDII Architecture for Grids", 
    "publish": "2012-02-24T17:51:35Z", 
    "summary": "The current BDII model relies on information gathering from agents that run\non each core node of a Grid. This information is then published into a Grid\nwide information resource known as Top BDII. The Top level BDIIs are updated\ntypically in cycles of a few minutes each. A new BDDI architecture is proposed\nand described in this paper based on the hypothesis that only a few attribute\nvalues change in each BDDI information cycle and consequently it may not be\nnecessary to update each parameter in a cycle. It has been demonstrated that\nsignificant performance gains can be achieved by exchanging only the\ninformation about records that changed during a cycle. Our investigations have\nled us to implement a low latency and fault tolerant BDII system that involves\nonly minimal data transfer and facilitates secure transactions in a Grid\nenvironment.", 
    "link": "http://arxiv.org/pdf/1202.5512v1", 
    "arxiv-id": "1202.5512v1"
},{
    "category": "cs.DC", 
    "author": "Richard McClatchey", 
    "title": "Context-Aware Service Utilisation in the Clouds and Energy Conservation", 
    "publish": "2012-02-24T18:21:23Z", 
    "summary": "Ubiquitous computing environments are characterised by smart, interconnected\nartefacts embedded in our physical world that are projected to provide useful\nservices to human inhabitants unobtrusively. Mobile devices are becoming the\nprimary tools of human interaction with these embedded artefacts and\nutilisation of services available in smart computing environments such as\nclouds. Advancements in capabilities of mobile devices allow a number of user\nand environment related context consumers to be hosted on these devices.\nWithout a coordinating component, these context consumers and providers are a\npotential burden on device resources; specifically the effect of uncoordinated\ncomputation and communication with cloud-enabled services can negatively impact\nthe battery life. Therefore energy conservation is a major concern in realising\nthe collaboration and utilisation of mobile device based context-aware\napplications and cloud based services. This paper presents the concept of a\ncontext-brokering component to aid in coordination and communication of context\ninformation between mobile devices and services deployed in a cloud\ninfrastructure. A prototype context broker is experimentally analysed for\neffects on energy conservation when accessing and coordinating with cloud\nservices on a smart device, with results signifying reduction in energy\nconsumption.", 
    "link": "http://arxiv.org/pdf/1202.5519v1", 
    "arxiv-id": "1202.5519v1"
},{
    "category": "cs.DC", 
    "author": "Guanfeng Liang", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -   Part II: Synchronous and Asynchronous Systems", 
    "publish": "2012-02-28T00:01:48Z", 
    "summary": "This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms.", 
    "link": "http://arxiv.org/pdf/1202.6094v2", 
    "arxiv-id": "1202.6094v2"
},{
    "category": "cs.DC", 
    "author": "Chunjie Luo", 
    "title": "High Volume Computing: Identifying and Characterizing Throughput   Oriented Workloads in Data Centers", 
    "publish": "2012-02-28T06:37:31Z", 
    "summary": "For the first time, this paper systematically identifies three categories of\nthroughput oriented workloads in data centers: services, data processing\napplications, and interactive real-time applications, whose targets are to\nincrease the volume of throughput in terms of processed requests or data, or\nsupported maximum number of simultaneous subscribers, respectively, and we coin\na new term high volume computing (in short HVC) to describe those workloads and\ndata center computer systems designed for them. We characterize and compare HVC\nwith other computing paradigms, e.g., high throughput computing,\nwarehouse-scale computing, and cloud computing, in terms of levels, workloads,\nmetrics, coupling degree, data scales, and number of jobs or service instances.\nWe also preliminarily report our ongoing work on the metrics and benchmarks for\nHVC systems, which is the foundation of designing innovative data center\ncomputer systems for HVC workloads.", 
    "link": "http://arxiv.org/pdf/1202.6134v2", 
    "arxiv-id": "1202.6134v2"
},{
    "category": "cs.DC", 
    "author": "Maxwell Young", 
    "title": "Resource-Competitive Communication", 
    "publish": "2012-02-29T06:28:53Z", 
    "summary": "Consider the general scenario where Alice wishes to transmit a message m to\nBob. These two players share a communication channel; however, there exists an\nadversary, Carol, who aims to prevent the transmission of m by blocking this\nchannel. There are costs to send, receive or block m on the channel, and we\nask: How much do Alice and Bob need to spend relative to the adversary Carol in\norder to guarantee transmission of m?\n  We show that in a time-slotted network with constant costs to send, receive\nand block m in a slot, if Carol spends a total of B slots trying to block m,\nthen both Alice and Bob must be active for only O(B^{\\varphi - 1} +\n1)=O(B^{.62}+1) slots in expectation to transmit m, where \\varphi = (1 +\n\\sqrt{5})/2 is the golden ratio. Surprisingly, this result holds even if (1) B\nis unknown to either player; (2) Carol knows the algorithms of both players,\nbut not their random bits; and (3) Carol can attack using total knowledge of\npast actions of both players.\n  In the spirit of competitive analysis, approximation guarantees, and\ngame-theoretic treatments, our approach represents another notion of relative\nperformance that we call resource competitiveness. This new metric measures the\nworst-case performance of an algorithm relative to any adversarial strategy and\npertains to scenarios where all network devices are resource-constrained. Here,\nwe apply the resource-competitive results above to two concrete problems.\nFirst, we consider jamming attacks in WSNs and address the fundamental task of\npropagating m from a single device to all others in the presence of faults.\nSecond, we examine how to mitigate application-level DDoS attacks in a wired\nclient-server scenario.", 
    "link": "http://arxiv.org/pdf/1202.6456v1", 
    "arxiv-id": "1202.6456v1"
},{
    "category": "cs.DC", 
    "author": "Pierre de Leusse", 
    "title": "Securing business operations in an SOA", 
    "publish": "2012-03-02T11:51:20Z", 
    "summary": "Service-oriented infrastructures pose new challenges in a number of areas,\nnotably with regard to security and dependability. BT has developed a\ncombination of innovative security solutions and governance frameworks that can\naddress these challenges. They include advances in identity federation;\ndistributed usage and access management; context-aware secure messaging,\nrouting and transformation; and (security) policy governance for\nservice-oriented architectures. This paper discusses these developments and the\nsteps being taken to validate their functionality and performance.", 
    "link": "http://arxiv.org/pdf/1203.0429v1", 
    "arxiv-id": "1203.0429v1"
},{
    "category": "cs.DC", 
    "author": "Krzysztof Zielinski", 
    "title": "Toward Governance of Cross-Cloud Application Deployment", 
    "publish": "2012-03-02T12:03:02Z", 
    "summary": "In this article, the authors introduce the main ideas around the governance\nof cross-Cloud application deployment and their related concepts. It is argued\nthat, due to the increasing complexity and nature of the Cloud market, an\nintermediary specialized in brokering the deployment of different components of\na same application onto different Cloud products could both facilitate said\ndeployment and in some cases improve its quality in terms of cost, security &\nreliability and QoS. In order to fulfill these objectives, the authors propose\na high level architecture that relies on their previous work on governance of\npolicy & rule driven distributed systems. This architecture aims at supplying\nfive main functions of 1) translation of Service Level Agreements (SLAs) and\npricing into a common shared DSL, 2) correlation of analytical data (e.g.\nmonitoring, metering), 3) combination of Cloud products, 4) information from\nthird parties regarding different aspects of Quality of Service (QoS) and 5)\ncross-Cloud application deployment specification and governance.", 
    "link": "http://arxiv.org/pdf/1203.0432v1", 
    "arxiv-id": "1203.0432v1"
},{
    "category": "cs.DC", 
    "author": "Krzysztof Zielinski", 
    "title": "A common interface for multi-rule-engine distributed systems", 
    "publish": "2012-03-02T12:07:57Z", 
    "summary": "The rule technological landscape is becoming ever more complex, with an\nextended number of specifications and products. It is therefore becoming\nincreasingly difficult to integrate rule-driven components and manage\ninteroperability in multi-rule engine environments. The described work presents\nthe possibility to provide a common interface for rule-driven components in a\ndistributed system. The authors' approach leverages on a set of discovery\nprotocol, rule interchange and user interface to alleviate the environment's\ncomplexity.", 
    "link": "http://arxiv.org/pdf/1203.0435v1", 
    "arxiv-id": "1203.0435v1"
},{
    "category": "cs.DC", 
    "author": "Andreas Maierhofer", 
    "title": "Secure & Rapid Composition of Infrastructure Services in the Cloud", 
    "publish": "2012-03-02T12:25:09Z", 
    "summary": "A fundamental ambition of grid and distributed systems is to be capable of\nsustaining evolution and allowing for adaptability ((F. Losavio et al., 2002),\n(S. Radhakrishnan, 2005)). Furthermore, as the complexity and sophistication of\ntheses structures increases, so does the need for adaptability of each\ncomponent. One of the primary benefits of service oriented architecture (SOA)\nis the ability to compose applications, processes or more complex services from\nother services which increases the capacity for adaptation. This document\nproposes a novel infrastructure composition model that aims at increasing the\nadaptability of the capabilities exposed through it by dynamically managing\ntheir non functional requirements.", 
    "link": "http://arxiv.org/pdf/1203.0443v1", 
    "arxiv-id": "1203.0443v1"
},{
    "category": "cs.DC", 
    "author": "Javid Taheri", 
    "title": "On Modeling Dependency between MapReduce Configuration Parameters and   Total Execution Time", 
    "publish": "2012-03-03T13:18:51Z", 
    "summary": "In this paper, we propose an analytical method to model the dependency\nbetween configuration parameters and total execution time of Map-Reduce\napplications. Our approach has three key phases: profiling, modeling, and\nprediction. In profiling, an application is run several times with different\nsets of MapReduce configuration parameters to profile the execution time of the\napplication on a given platform. Then in modeling, the relation between these\nparameters and total execution time is modeled by multivariate linear\nregression. Among the possible configuration parameters, two main parameters\nhave been used in this study: the number of Mappers, and the number of\nReducers. For evaluation, two standard applications (WordCount, and Exim\nMainlog parsing) are utilized to evaluate our technique on a 4-node MapReduce\nplatform.", 
    "link": "http://arxiv.org/pdf/1203.0651v1", 
    "arxiv-id": "1203.0651v1"
},{
    "category": "cs.DC", 
    "author": "Jundong Yang", 
    "title": "Resource Availability-Aware Advance Reservation for Parallel Jobs with   Deadlines", 
    "publish": "2012-03-04T14:11:56Z", 
    "summary": "Advance reservation is important to guarantee the quality of services of jobs\nby allowing exclusive access to resources over a defined time interval on\nresources. It is a challenge for the scheduler to organize available resources\nefficiently and to allocate them for parallel AR jobs with deadline constraint\nappropriately. This paper provides a slot-based data structure to organize\navailable resources of multiprocessor systems in a way that enables efficient\nsearch and update operations, and formulates a suite of scheduling policies to\nallocate resources for dynamically arriving AR requests. The performance of the\nscheduling algorithms were investigated by simulations with different job sizes\nand durations, system loads and scheduling flexibilities. Simulation results\nshow that job sizes and durations, system load and the flexibility of\nscheduling will impact the performance metrics of all the scheduling\nalgorithms, and the PE-Worst-Fit algorithm becomes the best algorithm for the\nscheduler with the highest acceptance rate of AR requests, and the jobs with\nthe First-Fit algorithm experience the lowest average slowdown. The data\nstructure and scheduling policies can be used to organize and allocate\nresources for parallel AR jobs with deadline constraint in large-scale\ncomputing systems.", 
    "link": "http://arxiv.org/pdf/1203.0740v1", 
    "arxiv-id": "1203.0740v1"
},{
    "category": "cs.DC", 
    "author": "Sergio Maffioletti", 
    "title": "Batch-oriented software appliances", 
    "publish": "2012-03-07T13:45:24Z", 
    "summary": "This paper presents AppPot, a system for creating Linux software appliances.\nAppPot can be run as a regular batch or grid job and executed in user space,\nand requires no special virtualization support in the infrastructure.\n  The main design goal of AppPot is to bring the benefits of a\nvirtualization-based IaaS cloud to existing batch-oriented computing\ninfrastructures.\n  In particular, AppPot addresses the application deployment and configuration\non large heterogeneous computing infrastructures: users are enabled to prepare\ntheir own customized virtual appliance for providing a safe execution\nenvironment for their applications. These appliances can then be executed on\nvirtually any computing infrastructure being in a private or public cloud as\nwell as any batch-controlled computing clusters the user may have access to.\n  We give an overview of AppPot and its features, the technology that makes it\npossible, and report on experiences running it in production use within the\nSwiss National Grid infrastructure SMSCG.", 
    "link": "http://arxiv.org/pdf/1203.1466v1", 
    "arxiv-id": "1203.1466v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Matrix Representation of Iterative Approximate Byzantine Consensus in   Directed Graphs", 
    "publish": "2012-03-08T19:00:19Z", 
    "summary": "This paper presents a proof of correctness of an iterative approximate\nByzantine consensus (IABC) algorithm for directed graphs. The iterative\nalgorithm allows fault- free nodes to reach approximate conensus despite the\npresence of up to f Byzantine faults. Necessary conditions on the underlying\nnetwork graph for the existence of a correct IABC algorithm were shown in our\nrecent work [15, 16]. [15] also analyzed a specific IABC algorithm and showed\nthat it performs correctly in any network graph that satisfies the necessary\ncondition, proving that the necessary condition is also sufficient. In this\npaper, we present an alternate proof of correctness of the IABC algorithm,\nusing a familiar technique based on transition matrices [9, 3, 17, 19].\n  The key contribution of this paper is to exploit the following observation:\nfor a given evolution of the state vector corresponding to the state of the\nfault-free nodes, many alternate state transition matrices may be chosen to\nmodel that evolution cor- rectly. For a given state evolution, we identify one\napproach to suitably \"design\" the transition matrices so that the standard\ntools for proving convergence can be applied to the Byzantine fault-tolerant\nalgorithm as well. In particular, the transition matrix for each iteration is\ndesigned such that each row of the matrix contains a large enough number of\nelements that are bounded away from 0.", 
    "link": "http://arxiv.org/pdf/1203.1888v1", 
    "arxiv-id": "1203.1888v1"
},{
    "category": "cs.DC", 
    "author": "Matthew Felice Pace", 
    "title": "BSP vs MapReduce", 
    "publish": "2012-03-09T13:42:03Z", 
    "summary": "The MapReduce framework has been generating a lot of interest in a wide range\nof areas. It has been widely adopted in industry and has been used to solve a\nnumber of non-trivial problems in academia. Putting MapReduce on strong\ntheoretical foundations is crucial in understanding its capabilities. This work\nlinks MapReduce to the BSP model of computation, underlining the relevance of\nBSP to modern parallel algorithm design and defining a subclass of BSP\nalgorithms that can be efficiently implemented in MapReduce.", 
    "link": "http://arxiv.org/pdf/1203.2081v2", 
    "arxiv-id": "1203.2081v2"
},{
    "category": "cs.DC", 
    "author": "Tristan Glatard", 
    "title": "Technical support for Life Sciences communities on a production grid   infrastructure", 
    "publish": "2012-03-11T19:22:51Z", 
    "summary": "Production operation of large distributed computing infrastructures (DCI)\nstill requires a lot of human intervention to reach acceptable quality of\nservice. This may be achievable for scientific communities with solid IT\nsupport, but it remains a show-stopper for others. Some application execution\nenvironments are used to hide runtime technical issues from end users. But they\nmostly aim at fault-tolerance rather than incident resolution, and their\noperation still requires substantial manpower. A longer-term support activity\nis thus needed to ensure sustained quality of service for Virtual Organisations\n(VO). This paper describes how the biomed VO has addressed this challenge by\nsetting up a technical support team. Its organisation, tooling, daily tasks,\nand procedures are described. Results are shown in terms of resource usage by\nend users, amount of reported incidents, and developed software tools. Based on\nour experience, we suggest ways to measure the impact of the technical support,\nperspectives to decrease its human cost and make it more community-specific.", 
    "link": "http://arxiv.org/pdf/1203.2366v1", 
    "arxiv-id": "1203.2366v1"
},{
    "category": "cs.DC", 
    "author": "C\u00e9dric Tedeschi", 
    "title": "A Protocol for the Atomic Capture of Multiple Molecules at Large Scale", 
    "publish": "2012-03-14T07:30:11Z", 
    "summary": "With the rise of service-oriented computing, applications are more and more\nbased on coordination of autonomous services. Envisioned over largely\ndistributed and highly dynamic platforms, expressing this coordination calls\nfor alternative programming models. The chemical programming paradigm, which\nmodels applications as chemical solutions where molecules representing digital\nentities involved in the computation, react together to produce a result, has\nbeen recently shown to provide the needed abstractions for autonomic\ncoordination of services. However, the execution of such programs over large\nscale platforms raises several problems hindering this paradigm to be actually\nleveraged. Among them, the atomic capture of molecules participating in concur-\nrent reactions is one of the most significant. In this paper, we propose a\nprotocol for the atomic capture of these molecules distributed and evolving\nover a large scale platform. As the density of possible reactions is crucial\nfor the liveness and efficiency of such a capture, the protocol proposed is\nmade up of two sub-protocols, each of them aimed at addressing different levels\nof densities of potential reactions in the solution. While the decision to\nchoose one or the other is local to each node participating in a program's\nexecution, a global coherent behaviour is obtained. Proof of liveness, as well\nas intensive simulation results showing the efficiency and limited overhead of\nthe protocol are given.", 
    "link": "http://arxiv.org/pdf/1203.3013v1", 
    "arxiv-id": "1203.3013v1"
},{
    "category": "cs.DC", 
    "author": "Ekaterina S. Kudryashova", 
    "title": "Generalized Asynchronous Systems", 
    "publish": "2012-03-14T14:37:25Z", 
    "summary": "The paper is devoted to a mathematical model of concurrency the special case\nof which is asynchronous system. Distributed asynchronous automata are\nintroduced here. It is proved that the Petri nets and transition systems with\nindependence can be considered like the distributed asynchronous automata. Time\ndistributed asynchronous automata are defined in standard way by the map which\nassigns time intervals to events. It is proved that the time distributed\nasynchronous automata are generalized the time Petri nets and asynchronous\nsystems.", 
    "link": "http://arxiv.org/pdf/1203.3098v1", 
    "arxiv-id": "1203.3098v1"
},{
    "category": "cs.DC", 
    "author": "Nini Zhu", 
    "title": "The Byzantine Brides Problem", 
    "publish": "2012-03-15T21:26:34Z", 
    "summary": "We investigate the hardness of establishing as many stable marriages (that\nis, marriages that last forever) in a population whose memory is placed in some\narbitrary state with respect to the considered problem, and where traitors try\nto jeopardize the whole process by behaving in a harmful manner. On the\nnegative side, we demonstrate that no solution that is completely insensitive\nto traitors can exist, and we propose a protocol for the problem that is\noptimal with respect to the traitor containment radius.", 
    "link": "http://arxiv.org/pdf/1203.3575v1", 
    "arxiv-id": "1203.3575v1"
},{
    "category": "cs.DC", 
    "author": "Dana Petcu", 
    "title": "Decentralized Probabilistic Auto-Scaling for Heterogeneous Systems", 
    "publish": "2012-03-17T18:48:23Z", 
    "summary": "The DEPAS (Decentralized Probabilistic Auto-Scaling) algorithm assumes an\noverlay network of computing nodes where each node probabilistically decides to\nshut down, allocate one or more other nodes or do nothing. DEPAS was\nformulated, tested, and theoretically analyzed for the simplified case of\nhomogenous systems. In this paper, we extend DEPAS to heterogeneous systems.", 
    "link": "http://arxiv.org/pdf/1203.3885v1", 
    "arxiv-id": "1203.3885v1"
},{
    "category": "cs.DC", 
    "author": "Faiez Gargouri", 
    "title": "DiscopFlow: A new Tool for Discovering Organizational Structures and   Interaction Protocols in WorkFlow", 
    "publish": "2012-03-19T20:53:03Z", 
    "summary": "This work deals with Workflow Mining (WM) a very active and promising\nresearch area. First, in this paper we give a critical and comparative study of\nthree representative WM systems of this area: the ProM, InWolve and\nWorkflowMiner systems. The comparison is made according to quality criteria\nthat we have defined such as the capacity to filter and convert a Workflow log,\nthe capacity to discover workflow perspectives and the capacity to support\nMulti-Analysis of processes. The major drawback of these systems is the non\npossibility to deal with organizational perspective discovering issue. We mean\nby organizational perspective, the organizational structures (federation,\ncoalition, market or hierarchy) and interaction protocols (contract net,\nauction or vote). This paper defends the idea that organizational dimension in\nMulti-Agent System is an appropriate approach to support the discovering of\nthis organizational perspective. Second, the paper proposes a Workflow log\nmeta-model which extends the classical one by considering the interactions\namong actors thanks to the FIPA-ACL Performatives. Third, it describes in\ndetails our DiscopFlow tool which validates our contribution.", 
    "link": "http://arxiv.org/pdf/1203.4257v1", 
    "arxiv-id": "1203.4257v1"
},{
    "category": "cs.DC", 
    "author": "Nasrin Jaberi", 
    "title": "Thesis Report: Resource Utilization Provisioning in MapReduce", 
    "publish": "2012-03-20T10:06:24Z", 
    "summary": "In this thesis report, we have a survey on state-of-the-art methods for\nmodelling resource utilization of MapReduce applications regard to its\nconfiguration parameters. After implementation of one of the algorithms in\nliterature, we tried to find that if CPU usage modelling of a MapReduce\napplication can be used to predict CPU usage of another MapReduce application.", 
    "link": "http://arxiv.org/pdf/1203.4367v1", 
    "arxiv-id": "1203.4367v1"
},{
    "category": "cs.DC", 
    "author": "Srivatsan Ravi", 
    "title": "Optimism for Boosting Concurrency", 
    "publish": "2012-03-21T14:44:40Z", 
    "summary": "Modern concurrent programming benefits from a large variety of\nsynchronization techniques. These include conventional pessimistic locking, as\nwell as optimistic techniques based on conditional synchronization primitives\nor transactional memory. Yet, it is unclear which of these approaches better\nleverage the concurrency inherent to multi-cores.\n  In this paper, we compare the level of concurrency one can obtain by\nconverting a sequential program into a concurrent one using optimistic or\npessimistic techniques. To establish fair comparison of such implementations,\nwe introduce a new correctness criterion for concurrent programs, defined\nindependently of the synchronization techniques they use.\n  We treat a program's concurrency as its ability to accept a concurrent\nschedule, a metric inspired by the theories of both databases and transactional\nmemory. We show that pessimistic locking can provide strictly higher\nconcurrency than transactions for some applications whereas transactions can\nprovide strictly higher concurrency than pessimistic locks for others. Finally,\nwe show that combining the benefits of the two synchronization techniques can\nprovide strictly more concurrency than any of them individually. We propose a\nlist-based set algorithm that is optimal in the sense that it accepts all\ncorrect concurrent schedules. As we show via experimentation, the optimality in\nterms of concurrency is reflected by scalability gains.", 
    "link": "http://arxiv.org/pdf/1203.4751v8", 
    "arxiv-id": "1203.4751v8"
},{
    "category": "cs.DC", 
    "author": "Luis Cabellos", 
    "title": "Advanced Programming Platform for efficient use of Data Parallel   Hardware", 
    "publish": "2012-03-22T09:54:58Z", 
    "summary": "Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown.", 
    "link": "http://arxiv.org/pdf/1203.4938v2", 
    "arxiv-id": "1203.4938v2"
},{
    "category": "cs.DC", 
    "author": "Colm O. Dunlaing", 
    "title": "CUDA implementation of Wagener's 2D convex hull PRAM algorithm", 
    "publish": "2012-03-22T14:30:25Z", 
    "summary": "This paper describes a CUDA implementation of Wagener's PRAM convex hull\nalgorithm in two dimensions. It is presented in Knuth's literate programming\nstyle.", 
    "link": "http://arxiv.org/pdf/1203.5004v2", 
    "arxiv-id": "1203.5004v2"
},{
    "category": "cs.DC", 
    "author": "Javid Taheri", 
    "title": "Multiple Frequency Selection in DVFS-Enabled Processors to Minimize   Energy Consumption", 
    "publish": "2012-03-23T02:42:38Z", 
    "summary": "In this chapter we focus on slack reclamation and propose a new slack\nreclamation technique, Multiple Frequency Selection DVFS (MFS-DVFS). The key\nidea is to execute each task with a linear combination of more than one\nfrequency such that this combination results in using the lowest energy by\ncovering the whole slack time of the task. We have tested our algorithm with\nboth random and real-world application task graphs and compared with the\nresults in previous researches in [9] and [12-13]. The experimental results\nshow that our approach can achieve energy almost identical to the optimum\nenergy saving.", 
    "link": "http://arxiv.org/pdf/1203.5160v2", 
    "arxiv-id": "1203.5160v2"
},{
    "category": "cs.DC", 
    "author": "Christian Vecchiola", 
    "title": "Market-Oriented Cloud Computing and the Cloudbus Toolkit", 
    "publish": "2012-03-23T08:50:57Z", 
    "summary": "Cloud computing has penetrated the Information Technology industry deep\nenough to influence major companies to adopt it into their mainstream business.\nA strong thrust on the use of virtualization technology to realize\nInfrastructure-as-a-Service (IaaS) has led enterprises to leverage\nsubscription-oriented computing capabilities of public Clouds for hosting their\napplication services. In parallel, research in academia has been investigating\ntransversal aspects such as security, software frameworks, quality of service,\nand standardization. We believe that the complete realization of the Cloud\ncomputing vision will lead to the introduction of a virtual market where Cloud\nbrokers, on behalf of end users, are in charge of selecting and composing the\nservices advertised by different Cloud vendors. In order to make this happen,\nexisting solutions and technologies have to be redesigned and extended from a\nmarket-oriented perspective and integrated together, giving rise to what we\nterm Market-Oriented Cloud Computing.\n  In this paper, we will assess the current status of Cloud computing by\nproviding a reference model, discuss the challenges that researchers and IT\npractitioners are facing and will encounter in the near future, and present the\napproach for solving them from the perspective of the Cloudbus toolkit, which\ncomprises of a set of technologies geared towards the realization of Market\nOriented Cloud Computing vision. We provide experimental results demonstrating\nmarket-oriented resource provisioning and brokering within a Cloud and across\nmultiple distributed resources. We also include an application illustrating the\nhosting of ECG analysis as SaaS on Amazon IaaS (EC2 and S3) services.", 
    "link": "http://arxiv.org/pdf/1203.5196v1", 
    "arxiv-id": "1203.5196v1"
},{
    "category": "cs.DC", 
    "author": "Eli Gafni", 
    "title": "Asynchrony from Synchrony", 
    "publish": "2012-03-27T22:26:02Z", 
    "summary": "We consider synchronous dynamic networks which like radio networks may have\nasymmetric communication links, and are affected by communication rather than\nprocessor failures. In this paper we investigate the minimal message\nsurvivability in a per round basis that allows for the minimal global\ncooperation, i.e., allows to solve any task that is wait-free read-write\nsolvable. The paper completely characterizes this survivability requirement.\nMessage survivability is formalized by considering adversaries that have a\nlimited power to remove messages in a round. Removal of a message on a link in\none direction does not necessarily imply the removal of the message on that\nlink in the other direction. Surprisingly there exist a single strongest\nadversary which solves any wait-free read/write task. Any different adversary\nthat solves any wait-free read/write task is weaker, and any stronger adversary\nwill not solve any wait-free read/write task. ABD \\cite{ABD} who considered\nprocessor failure, arrived at an adversary that is $n/2$ resilient,\nconsequently can solve tasks, such as $n/2$-set-consensus, which are not\nread/write wait-free solvable. With message adversaries, we arrive at an\nadversary which has exactly the read-write wait-free power. Furthermore, this\nadversary allows for a considerably simpler (simplest that we know of) proof\nthat the protocol complex of any read/write wait-free task is a subdivided\nsimplex, finally making this proof accessible for students with no\nalgebraic-topology prerequisites, and alternatively dispensing with the\nassumption that the Immediate Snapshot complex is a subdivided simplex.", 
    "link": "http://arxiv.org/pdf/1203.6096v1", 
    "arxiv-id": "1203.6096v1"
},{
    "category": "cs.DC", 
    "author": "Martin Raussen", 
    "title": "Trace Spaces: an Efficient New Technique for State-Space Reduction", 
    "publish": "2012-04-02T14:24:18Z", 
    "summary": "State-space reduction techniques, used primarily in model-checkers, all rely\non the idea that some actions are independent, hence could be taken in any\n(respective) order while put in parallel, without changing the semantics. It is\nthus not necessary to consider all execution paths in the interleaving\nsemantics of a concurrent program, but rather some equivalence classes. The\npurpose of this paper is to describe a new algorithm to compute such\nequivalence classes, and a representative per class, which is based on ideas\noriginating in algebraic topology. We introduce a geometric semantics of\nconcurrent languages, where programs are interpreted as directed topological\nspaces, and study its properties in order to devise an algorithm for computing\ndihomotopy classes of execution paths. In particular, our algorithm is able to\ncompute a control-flow graph for concurrent programs, possibly containing\nloops, which is \"as reduced as possible\" in the sense that it generates traces\nmodulo equivalence. A preliminary implementation was achieved, showing\npromising results towards efficient methods to analyze concurrent programs,\nwith very promising results compared to partial-order reduction techniques.", 
    "link": "http://arxiv.org/pdf/1204.0414v1", 
    "arxiv-id": "1204.0414v1"
},{
    "category": "cs.DC", 
    "author": "Ulrich Schmid", 
    "title": "Agreement in Directed Dynamic Networks", 
    "publish": "2012-04-03T10:00:45Z", 
    "summary": "We study distributed computation in synchronous dynamic networks where an\nomniscient adversary controls the unidirectional communication links. Its\nbehavior is modeled as a sequence of directed graphs representing the active\n(i.e. timely) communication links per round. We prove that consensus is\nimpossible under some natural weak connectivity assumptions, and introduce\nvertex-stable root components as a means for circumventing this impossibility.\nEssentially, we assume that there is a short period of time during which an\narbitrary part of the network remains strongly connected, while its\ninterconnect topology may keep changing continuously. We present a consensus\nalgorithm that works under this assumption, and prove its correctness. Our\nalgorithm maintains a local estimate of the communication graphs, and applies\ntechniques for detecting stable network properties and univalent system\nconfigurations. Our possibility results are complemented by several\nimpossibility results and lower bounds for consensus and other distributed\ncomputing problems like leader election, revealing that our algorithm is\nasymptotically optimal.", 
    "link": "http://arxiv.org/pdf/1204.0641v5", 
    "arxiv-id": "1204.0641v5"
},{
    "category": "cs.DC", 
    "author": "Gaurav Sharma", 
    "title": "Reliable Resource Selection in Grid Environment", 
    "publish": "2012-04-06T17:09:53Z", 
    "summary": "The primary concern in area of computational grid is security and resources.\nMost of the existing grids address this problem by authenticating the users,\nhosts and their interactions in an appropriate manner. A secured system is\ncompulsory for the efficient utilization of grid services. The high degree of\nstrangeness has been identified as the problem factors in the secured selection\nof grid. Without the assurance of a higher degree of trust relationship,\ncompetent resource selection and utilization cannot be achieved. In this paper\nwe proposed an approach which is providing reliability and reputation aware\nsecurity for resource selection in grid environment. In this approach, the\nself-protection capability and reputation weightage is utilized to obtain the\nReliability Factor (RF) value. Therefore jobs are allocated to the resources\nthat posses higher RF values. Extensive experimental evaluation shows that as\nhigher trust and reliable nodes are selected the chances of failure decreased\ndrastically.", 
    "link": "http://arxiv.org/pdf/1204.1516v1", 
    "arxiv-id": "1204.1516v1"
},{
    "category": "cs.DC", 
    "author": "Boualem Benatallah", 
    "title": "Programming Cloud Resource Orchestration Framework: Operations and   Research Challenges", 
    "publish": "2012-04-10T16:06:46Z", 
    "summary": "The emergence of cloud computing over the past five years is potentially one\nof the breakthrough advances in the history of computing. It delivers hardware\nand software resources as virtualization-enabled services and in which\nadministrators are free from the burden of worrying about the low level\nimplementation or system administration details. Although cloud computing\noffers considerable opportunities for the users (e.g. application developers,\ngovernments, new startups, administrators, consultants, scientists, business\nanalyst, etc.) such as no up-front investment, lowering operating cost, and\ninfinite scalability, it has many unique research challenges that need to be\ncarefully addressed in the future. In this paper, we present a survey on key\ncloud computing concepts, resource abstractions, and programming operations for\norchestrating resources and associated research challenges, wherever\napplicable.", 
    "link": "http://arxiv.org/pdf/1204.2204v3", 
    "arxiv-id": "1204.2204v3"
},{
    "category": "cs.DC", 
    "author": "Nicola Santoro", 
    "title": "Building Fastest Broadcast Trees in Periodically-Varying Graphs", 
    "publish": "2012-04-13T17:25:28Z", 
    "summary": "Delay-tolerant networks (DTNs) are characterized by a possible absence of\nend-to-end communication routes at any instant. Still, connectivity can\ngenerally be established over time and space. The optimality of a temporal path\n(journey) in this context can be defined in several terms, including\ntopological (e.g. {\\em shortest} in hops) and temporal (e.g. {\\em fastest,\nforemost}). The combinatorial problem of computing shortest, foremost, and\nfastest journeys {\\em given full knowledge} of the network schedule was\naddressed a decade ago (Bui-Xuan {\\it et al.}, 2003). A recent line of research\nhas focused on the distributed version of this problem, where foremost,\nshortest or fastest {\\em broadcast} are performed without knowing the schedule\nbeforehand. In this paper we show how to build {\\em fastest} broadcast trees\n(i.e., trees that minimize the global duration of the broadcast, however late\nthe departure is) in Time-Varying Graphs where intermittent edges are available\nperiodically (it is known that the problem is infeasible in the general case\neven if various parameters of the graph are know). We address the general case\nwhere contacts between nodes can have arbitrary durations and thus fastest\nroutes may consist of a mixture of {\\em continuous} and {\\em discontinuous}\nsegments (a more complex scenario than when contacts are {\\em punctual} and\nthus routes are only discontinuous). Using the abstraction of \\tclocks to\ncompute the temporal distances, we solve the fastest broadcast problem by first\nlearning, at the emitter, what is its time of {\\em minimum temporal\neccentricity} (i.e. the fastest time to reach all the other nodes), and second\nby building a {\\em foremost} broadcast tree relative to this particular\nemission date.", 
    "link": "http://arxiv.org/pdf/1204.3058v1", 
    "arxiv-id": "1204.3058v1"
},{
    "category": "cs.DC", 
    "author": "Lu\u00eds Rodrigues", 
    "title": "Asynchrony and Collusion in the N-party BAR Transfer Problem", 
    "publish": "2012-04-18T11:01:42Z", 
    "summary": "The problem of reliably transferring data from a set of $N_P$ producers to a\nset of $N_C$ consumers in the BAR model, named N-party BAR Transfer (NBART), is\nan important building block for volunteer computing systems. An algorithm to\nsolve this problem in synchronous systems, which provides a Nash equilibrium,\nhas been presented in previous work. In this paper, we propose an NBART\nalgorithm for asynchronous systems. Furthermore, we also address the\npossibility of collusion among the Rational processes. Our game theoretic\nanalysis shows that the proposed algorithm tolerates certain degree of\narbitrary collusion, while still fulfilling the NBART properties.", 
    "link": "http://arxiv.org/pdf/1204.4044v1", 
    "arxiv-id": "1204.4044v1"
},{
    "category": "cs.DC", 
    "author": "Colin W. Glass", 
    "title": "Hybrid MPI/StarSs - a case study", 
    "publish": "2012-04-18T14:06:14Z", 
    "summary": "Hybrid parallel programming models combining distributed and shared memory\nparadigms are well established in high-performance computing. The classical\nprototype of hybrid programming in HPC is MPI/OpenMP, but many other\ncombinations are being investigated. Recently, the data-dependency driven, task\nparallel model for shared memory parallelisation named StarSs has been\nsuggested for usage in combination with MPI. In this paper we apply hybrid\nMPI/StarSs to a Lattice-Boltzmann code. In particular, we present the hybrid\nprogramming model, the benefits we expect, the challenges in porting, and\nfinally a comparison of the performance of MPI/StarSs hybrid, MPI/OpenMP hybrid\nand the original MPI-only versions of the same code.", 
    "link": "http://arxiv.org/pdf/1204.4086v1", 
    "arxiv-id": "1204.4086v1"
},{
    "category": "cs.DC", 
    "author": "Nini Zhu", 
    "title": "Mariages et Trahisons", 
    "publish": "2012-04-20T08:59:01Z", 
    "summary": "A self-stabilizing protocol tolerates by definition transient faults (faults\nof finite duration). Recently, a new class of self-stabilizing protocols that\nare able to tolerate a given number of permanent faults. In this paper, we\nfocus on self-stabilizing protocols able to tolerate Byzantine faults, that is\nfaults that introduce an arbitrary behaviour. We focus on strict-stabilization\nin which the system have to contain the effects of Byzantine faults.\nSpecificaly, we study the possibility to construct in a self-stabilizing way a\nmaximal matching in a network where an arbitrary number of process may become\nByzantine.", 
    "link": "http://arxiv.org/pdf/1204.4565v1", 
    "arxiv-id": "1204.4565v1"
},{
    "category": "cs.DC", 
    "author": "Massimo Torquati", 
    "title": "FastFlow tutorial", 
    "publish": "2012-04-24T15:17:53Z", 
    "summary": "FastFlow is a structured parallel programming framework targeting shared\nmemory multicores. Its layered design and the optimized implementation of the\ncommunication mechanisms used to implement the FastFlow streaming networks\nprovided to the application programmer as algorithmic skeletons support the\ndevelopment of efficient fine grain parallel applications. FastFlow is\navailable (open source) at SourceForge\n(http://sourceforge.net/projects/mc-fastflow/). This work introduces FastFlow\nprogramming techniques and points out the different ways used to parallelize\nexisting C/C++ code using FastFlow as a software accelerator. In short: this is\na kind of tutorial on FastFlow.", 
    "link": "http://arxiv.org/pdf/1204.5402v1", 
    "arxiv-id": "1204.5402v1"
},{
    "category": "cs.DC", 
    "author": "Wim H. Hesselink", 
    "title": "A distributed resource allocation algorithm for many processes", 
    "publish": "2012-04-27T11:14:54Z", 
    "summary": "Resource allocation is the problem that a process may enter a critical\nsection CS of its code only when its resource requirements are not in conflict\nwith those of other processes in their critical sections. For each execution of\nCS, these requirements are given anew. In the resource requirements, levels can\nbe distinguished, such as e.g. read access or write access. We allow infinitely\nmany processes that communicate by reliable asynchronous messages and have\nfinite memory. A simple starvation-free solution is presented. Processes only\nwait for one another when they have conflicting resource requirements. The\ncorrectness of the solution is argued with invariants and temporal logic. It\nhas been verified with the proof assistant PVS.", 
    "link": "http://arxiv.org/pdf/1204.6170v2", 
    "arxiv-id": "1204.6170v2"
},{
    "category": "cs.DC", 
    "author": "Federico Calzolari", 
    "title": "The Anatomy of a Grid portal", 
    "publish": "2012-04-30T13:32:48Z", 
    "summary": "In this paper we introduce a new way to deal with Grid portals referring to\nour implementation. L-GRID is a light portal to access the EGEE/EGI Grid\ninfrastructure via Web, allowing users to submit their jobs from a common Web\nbrowser in a few minutes, without any knowledge about the Grid infrastructure.\nIt provides the control over the complete lifecycle of a Grid Job, from its\nsubmission and status monitoring, to the output retrieval. The system,\nimplemented as client-server architecture, is based on the Globus Grid\nmiddleware. The client side application is based on a java applet; the server\nrelies on a Globus User Interface. There is no need of user registration on the\nserver side, and the user needs only his own X.509 personal certificate. The\nsystem is user-friendly, secure (it uses SSL protocol, mechanism for dynamic\ndelegation and identity creation in public key infrastructures), highly\ncustomizable, open source, and easy to install. The X.509 personal certificate\ndoes not get out from the local machine. It allows to reduce the time spent for\nthe job submission, granting at the same time a higher efficiency and a better\nsecurity level in proxy delegation and management.", 
    "link": "http://arxiv.org/pdf/1204.6628v1", 
    "arxiv-id": "1204.6628v1"
},{
    "category": "cs.DC", 
    "author": "Silvia Volpe", 
    "title": "A new job migration algorithm to improve data center efficiency", 
    "publish": "2012-04-30T13:44:03Z", 
    "summary": "The under exploitation of the available resources risks to be one of the main\nproblems for a computing center. The growing demand of computational power\nnecessarily entails more complex approaches in the management of the computing\nresources, with particular attention to the batch queue system scheduler. In a\nheterogeneous batch queue system, available for both serial single core\nprocesses and parallel multi core jobs, it may happen that one or more\ncomputational nodes composing the cluster are not fully occupied, running a\nnumber of jobs lower than their actual capability. A typical case is\nrepresented by more single core jobs running each one over a different multi\ncore server, while more parallel jobs - requiring all the available cores of a\nhost - are queued. A job rearrangement executed at runtime is able to free\nextra resources, in order to host new processes. We present an efficient method\nto improve the computing resources exploitation.", 
    "link": "http://arxiv.org/pdf/1204.6631v1", 
    "arxiv-id": "1204.6631v1"
},{
    "category": "cs.DC", 
    "author": "Ivona Brandic", 
    "title": "Energy Efficient Service Delivery in Clouds in Compliance with the Kyoto   Protocol", 
    "publish": "2012-04-30T16:28:19Z", 
    "summary": "Cloud computing is revolutionizing the ICT landscape by providing scalable\nand efficient computing resources on demand. The ICT industry - especially data\ncenters, are responsible for considerable amounts of CO2 emissions and will\nvery soon be faced with legislative restrictions, such as the Kyoto protocol,\ndefining caps at different organizational levels (country, industry branch\netc.) A lot has been done around energy efficient data centers, yet there is\nvery little work done in defining flexible models considering CO2. In this\npaper we present a first attempt of modeling data centers in compliance with\nthe Kyoto protocol. We discuss a novel approach for trading credits for\nemission reductions across data centers to comply with their constraints. CO2\ncaps can be integrated with Service Level Agreements and juxtaposed to other\ncomputing commodities (e.g. computational power, storage), setting a foundation\nfor implementing next-generation schedulers and pricing models that support\nKyoto-compliant CO2 trading schemes.", 
    "link": "http://arxiv.org/pdf/1204.6691v1", 
    "arxiv-id": "1204.6691v1"
},{
    "category": "cs.DC", 
    "author": "Abdullah Gani", 
    "title": "Mobile Cloud Computing: A Review on Smartphone Augmentation Approaches", 
    "publish": "2012-05-02T15:04:04Z", 
    "summary": "Smartphones have recently gained significant popularity in heavy mobile\nprocessing while users are increasing their expectations toward rich computing\nexperience. However, resource limitations and current mobile computing\nadvancements hinder this vision. Therefore, resource-intensive application\nexecution remains a challenging task in mobile computing that necessitates\ndevice augmentation. In this article, smartphone augmentation approaches are\nreviewed and classified in two main groups, namely hardware and software.\nGenerating high-end hardware is a subset of hardware augmentation approaches,\nwhereas conserving local resource and reducing resource requirements approaches\nare grouped under software augmentation methods. Our study advocates that\nconsreving smartphones' native resources, which is mainly done via task\noffloading, is more appropriate for already-developed applications than new\nones, due to costly re-development process. Cloud computing has recently\nobtained momentous ground as one of the major cornerstone technologies in\naugmenting smartphones. We present sample execution model for intensive mobile\napplications and devised taxonomy of augmentation approaches. For better\ncomprehension, the results of this study are summarized in a table.", 
    "link": "http://arxiv.org/pdf/1205.0451v2", 
    "arxiv-id": "1205.0451v2"
},{
    "category": "cs.DC", 
    "author": "Shaojie Tang", 
    "title": "On Exploiting Hotspot and Entropy for Data Forwarding in Delay Tolerant   Networks", 
    "publish": "2012-05-03T09:02:44Z", 
    "summary": "Performance of data forwarding in Delay Tolerant Networks (DTNs) benefits\nconsiderably if one can make use of human mobility in terms of social\nstructures. However, it is difficult and time-consuming to calculate the\ncentrality and similarity of nodes by using solutions for traditional social\nnetworks, this is mainly because of the transient node contact and the\nintermittently connected environment. In this work, we are interested in the\nfollowing question: Can we explore some other stable social attributes to\nquantify the centrality and similarity of nodes? Taking GPS traces of human\nwalks from the real world, we find that there exist two known phenomena. One is\npublic hotspot, the other is personal hotspot. Motivated by this observation,\nwe present Hoten (hotspot and entropy), a novel routing metric to improve\nrouting performance in DTNs. First, we use the relative entropy between the\npublic hotspots and the personal hotspots to compute the centrality of nodes.\nThen we utilize the inverse symmetrized entropy of the personal hotspots\nbetween two nodes to compute the similarity between them. Third, we exploit the\nentropy of personal hotspots of a node to estimate its personality. Besides, we\npropose a method to ascertain the optimized size of hotspot. Finally, we\ncompare our routing strategy with other state-of-the-art routing schemes\nthrough extensive trace-driven simulations, the results show that Hoten largely\noutperforms other solutions, especially in terms of combined overhead/packet\ndelivery ratio and the average number of hops per message.", 
    "link": "http://arxiv.org/pdf/1205.0652v2", 
    "arxiv-id": "1205.0652v2"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "CloudMonitor: Profiling Power Usage", 
    "publish": "2012-05-11T15:00:53Z", 
    "summary": "In Cloud Computing platforms the addition of hardware monitoring devices to\ngather power usage data can be impractical or uneconomical due to the large\nnumber of machines to be metered. CloudMonitor, a monitoring tool that can\ngenerate power models for software-based power estimation, can provide insights\nto the energy costs of deployments without additional hardware. Accurate power\nusage data leads to the possibility of Cloud providers creating a separate\ntariff for power and therefore incentivizing software developers to create\nenergy-efficient applications.", 
    "link": "http://arxiv.org/pdf/1205.2546v1", 
    "arxiv-id": "1205.2546v1"
},{
    "category": "cs.DC", 
    "author": "Rashid Hafeez Khokhar", 
    "title": "Tripod of Requirements in Horizontal Heterogeneous Mobile Cloud   Computing", 
    "publish": "2012-05-15T03:29:23Z", 
    "summary": "Recent trend of mobile computing is emerging toward executing\nresource-intensive applications in mobile devices regardless of underlying\nresource restrictions (e.g. limited processor and energy) that necessitate\nimminent technologies. Prosperity of cloud computing in stationary computers\nbreeds Mobile Cloud Computing (MCC) technology that aims to augment computing\nand storage capabilities of mobile devices besides conserving energy. However,\nMCC is more heterogeneous and unreliable (due to wireless connectivity) compare\nto cloud computing. Problems like variations in OS, data fragmentation, and\nsecurity and privacy discourage and decelerate implementation and pervasiveness\nof MCC. In this paper, we describe MCC as a horizontal heterogeneous ecosystem\nand identify thirteen critical metrics and approaches that influence on\nmobile-cloud solutions and success of MCC. We divide them into three major\nclasses, namely ubiquity, trust, and energy efficiency and devise a tripod of\nrequirements in MCC. Our proposed tripod shows that success of MCC is\nachievable by reducing mobility challenges (e.g. seamless connectivity,\nfragmentation), increasing trust, and enhancing energy efficiency.", 
    "link": "http://arxiv.org/pdf/1205.3247v1", 
    "arxiv-id": "1205.3247v1"
},{
    "category": "cs.DC", 
    "author": "Emina Soljanin", 
    "title": "Toward Sustainable Networking: Storage Area Networks with Network Coding", 
    "publish": "2012-05-16T20:15:49Z", 
    "summary": "This manuscript provides a model to characterize the energy savings of\nnetwork coded storage (NCS) in storage area networks (SANs). We consider\nblocking probability of drives as our measure of performance. A mapping\ntechnique to analyze SANs as independent M/G/K/K queues is presented, and\nblocking probabilities for uncoded storage schemes and NCS are derived and\ncompared. We show that coding operates differently than the amalgamation of\nfile chunks and energy savings are shown to scale well with striping number. We\nillustrate that for enterprise-level SANs energy savings of 20-50% can be\nrealized.", 
    "link": "http://arxiv.org/pdf/1205.3797v1", 
    "arxiv-id": "1205.3797v1"
},{
    "category": "cs.DC", 
    "author": "Alex Pothen", 
    "title": "Graph Coloring Algorithms for Muti-core and Massively Multithreaded   Architectures", 
    "publish": "2012-05-16T20:59:48Z", 
    "summary": "We explore the interplay between architectures and algorithm design in the\ncontext of shared-memory platforms and a specific graph problem of central\nimportance in scientific and high-performance computing, distance-1 graph\ncoloring. We introduce two different kinds of multithreaded heuristic\nalgorithms for the stated, NP-hard, problem. The first algorithm relies on\nspeculation and iteration, and is suitable for any shared-memory system. The\nsecond algorithm uses dataflow principles, and is targeted at the\nnon-conventional, massively multithreaded Cray XMT system. We study the\nperformance of the algorithms on the Cray XMT and two multi-core systems, Sun\nNiagara 2 and Intel Nehalem. Together, the three systems represent a spectrum\nof multithreading capabilities and memory structure. As testbed, we use\nsynthetically generated large-scale graphs carefully chosen to cover a wide\nrange of input types. The results show that the algorithms have scalable\nruntime performance and use nearly the same number of colors as the underlying\nserial algorithm, which in turn is effective in practice. The study provides\ninsight into the design of high performance algorithms for irregular problems\non many-core architectures.", 
    "link": "http://arxiv.org/pdf/1205.3809v1", 
    "arxiv-id": "1205.3809v1"
},{
    "category": "cs.DC", 
    "author": "John Feo", 
    "title": "Parallel implementation of fast randomized algorithms for the   decomposition of low rank matrices", 
    "publish": "2012-05-16T23:41:33Z", 
    "summary": "We analyze the parallel performance of randomized interpolative decomposition\nby decomposing low rank complex-valued Gaussian random matrices up to 64 GB. We\nchose a Cray XMT supercomputer as it provides an almost ideal PRAM model\npermitting quick investigation of parallel algorithms without obfuscation from\nhardware idiosyncrasies. We obtain that on non-square matrices performance\nbecomes very good, with overall runtime over 70 times faster on 128 processors.\nWe also verify that numerically discovered error bounds still hold on matrices\nnearly two orders of magnitude larger than those previously tested.", 
    "link": "http://arxiv.org/pdf/1205.3830v2", 
    "arxiv-id": "1205.3830v2"
},{
    "category": "cs.DC", 
    "author": "Amos Korman", 
    "title": "Memory Lower Bounds for Randomized Collaborative Search and Applications   to Biology", 
    "publish": "2012-05-21T09:52:57Z", 
    "summary": "Initial knowledge regarding group size can be crucial for collective\nperformance. We study this relation in the context of the {\\em Ants Nearby\nTreasure Search (ANTS)} problem \\cite{FKLS}, which models natural cooperative\nforaging behavior such as that performed by ants around their nest. In this\nproblem, $k$ (probabilistic) agents, initially placed at some central location,\ncollectively search for a treasure on the two-dimensional grid. The treasure is\nplaced at a target location by an adversary and the goal is to find it as fast\nas possible as a function of both $k$ and $D$, where $D$ is the (unknown)\ndistance between the central location and the target. It is easy to see that\n$T=\\Omega(D+D^2/k)$ time units are necessary for finding the treasure.\nRecently, it has been established that $O(T)$ time is sufficient if the agents\nknow their total number $k$ (or a constant approximation of it), and enough\nmemory bits are available at their disposal \\cite{FKLS}. In this paper, we\nestablish lower bounds on the agent memory size required for achieving certain\nrunning time performances. To the best our knowledge, these bounds are the\nfirst non-trivial lower bounds for the memory size of probabilistic searchers.\nFor example, for every given positive constant $\\epsilon$, terminating the\nsearch by time $O(\\log^{1-\\epsilon} k \\cdot T)$ requires agents to use\n$\\Omega(\\log\\log k)$ memory bits. Such distributed computing bounds may provide\na novel, strong tool for the investigation of complex biological systems.", 
    "link": "http://arxiv.org/pdf/1205.4545v1", 
    "arxiv-id": "1205.4545v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Iterative Approximate Byzantine Consensus under a Generalized Fault   Model", 
    "publish": "2012-05-22T05:43:53Z", 
    "summary": "In this work, we consider a generalized fault model that can be used to\nrepresent a wide range of failure scenarios, including correlated failures and\nnon-uniform node reliabilities. This fault model is general in the sense that\nfault models studied in prior related work, such as f -total and f -local\nmodels, are special cases of the generalized fault model. Under the generalized\nfault model, we explore iterative approximate Byzantine consensus (IABC)\nalgorithms in arbitrary directed networks. We prove a necessary and sufficient\ncondition for the existence of IABC algorithms. The use of the generalized\nfault model helps to gain a better understanding of IABC algorithms.", 
    "link": "http://arxiv.org/pdf/1205.4809v1", 
    "arxiv-id": "1205.4809v1"
},{
    "category": "cs.DC", 
    "author": "Lei Liu", 
    "title": "Hybrid Parallel Bidirectional Sieve based on SMP Cluster", 
    "publish": "2012-05-22T11:27:10Z", 
    "summary": "In this article, hybrid parallel bidirectional sieve method is implemented by\nSMP Cluster, the individual computational units joined together by the\ncommunication network, are usually shared-memory systems with one or more\nmulticore processor. To high-efficiency optimization, we propose average divide\ndata into nodes, generating double-ended queues (deque) for sieve method that\nare able to exploit dual-cores simultaneously start sifting out primes from the\nhead and tail.And each node create a FIFO queue as dynamic data buffer to ache\ntemporary data from another nodes send to. The approach obtains huge speedup\nand efficiency on SMP Cluster.", 
    "link": "http://arxiv.org/pdf/1205.4883v1", 
    "arxiv-id": "1205.4883v1"
},{
    "category": "cs.DC", 
    "author": "Thomas Sterling", 
    "title": "Neutron Star Evolutions using Tabulated Equations of State with a New   Execution Model", 
    "publish": "2012-05-22T20:46:11Z", 
    "summary": "The addition of nuclear and neutrino physics to general relativistic fluid\ncodes allows for a more realistic description of hot nuclear matter in neutron\nstar and black hole systems. This additional microphysics requires that each\nprocessor have access to large tables of data, such as equations of state, and\nin large simulations the memory required to store these tables locally can\nbecome excessive unless an alternative execution model is used. In this work we\npresent relativistic fluid evolutions of a neutron star obtained using a\nmessage driven multi-threaded execution model known as ParalleX. These neutron\nstar simulations would require substantial memory overhead dedicated entirely\nto the equation of state table if using a more traditional execution model. We\nintroduce a ParalleX component based on Futures for accessing large tables of\ndata, including out-of-core sized tables, which does not require substantial\nmemory overhead and effectively hides any increased network latency.", 
    "link": "http://arxiv.org/pdf/1205.5055v1", 
    "arxiv-id": "1205.5055v1"
},{
    "category": "cs.DC", 
    "author": "Andrzej Pelc", 
    "title": "Leader Election for Anonymous Asynchronous Agents in Arbitrary Networks", 
    "publish": "2012-05-29T02:27:35Z", 
    "summary": "We study the problem of leader election among mobile agents operating in an\narbitrary network modeled as an undirected graph. Nodes of the network are\nunlabeled and all agents are identical. Hence the only way to elect a leader\namong agents is by exploiting asymmetries in their initial positions in the\ngraph. Agents do not know the graph or their positions in it, hence they must\ngain this knowledge by navigating in the graph and share it with other agents\nto accomplish leader election. This can be done using meetings of agents, which\nis difficult because of their asynchronous nature: an adversary has total\ncontrol over the speed of agents. When can a leader be elected in this\nadversarial scenario and how to do it? We give a complete answer to this\nquestion by characterizing all initial configurations for which leader election\nis possible and by constructing an algorithm that accomplishes leader election\nfor all configurations for which this can be done.", 
    "link": "http://arxiv.org/pdf/1205.6249v1", 
    "arxiv-id": "1205.6249v1"
},{
    "category": "cs.DC", 
    "author": "Yun Wang", 
    "title": "Reaching Approximate Byzantine Consensus in Partially-Connected Mobile   Networks", 
    "publish": "2012-06-01T06:14:29Z", 
    "summary": "We consider the problem of approximate consensus in mobile networks\ncontaining Byzantine nodes. We assume that each correct node can communicate\nonly with its neighbors and has no knowledge of the global topology. As all\nnodes have moving ability, the topology is dynamic. The number of Byzantine\nnodes is bounded by f and known by all correct nodes. We first introduce an\napproximate Byzantine consensus protocol which is based on the linear iteration\nmethod. As nodes are allowed to collect information during several consecutive\nrounds, moving gives them the opportunity to gather more values. We propose a\nnovel sufficient and necessary condition to guarantee the final convergence of\nthe consensus protocol. The requirement expressed by our condition is not\n\"universal\": in each phase it affects only a single correct node. More\nprecisely, at least one correct node among those that propose either the\nminimum or the maximum value which is present in the network, has to receive\nenough messages (quantity constraint) with either higher or lower values\n(quality constraint). Of course, nodes' motion should not prevent this\nrequirement to be fulfilled. Our conclusion shows that the proposed condition\ncan be satisfied if the total number of nodes is greater than 3f+1.", 
    "link": "http://arxiv.org/pdf/1206.0089v1", 
    "arxiv-id": "1206.0089v1"
},{
    "category": "cs.DC", 
    "author": "Takahashi Toru", 
    "title": "Pipelining the Fast Multipole Method over a Runtime System", 
    "publish": "2012-06-01T08:05:39Z", 
    "summary": "Fast Multipole Methods (FMM) are a fundamental operation for the simulation\nof many physical problems. The high performance design of such methods usually\nrequires to carefully tune the algorithm for both the targeted physics and the\nhardware. In this paper, we propose a new approach that achieves high\nperformance across architectures. Our method consists of expressing the FMM\nalgorithm as a task flow and employing a state-of-the-art runtime system,\nStarPU, in order to process the tasks on the different processing units. We\ncarefully design the task flow, the mathematical operators, their Central\nProcessing Unit (CPU) and Graphics Processing Unit (GPU) implementations, as\nwell as scheduling schemes. We compute potentials and forces of 200 million\nparticles in 48.7 seconds on a homogeneous 160 cores SGI Altix UV 100 and of 38\nmillion particles in 13.34 seconds on a heterogeneous 12 cores Intel Nehalem\nprocessor enhanced with 3 Nvidia M2090 Fermi GPUs.", 
    "link": "http://arxiv.org/pdf/1206.0115v1", 
    "arxiv-id": "1206.0115v1"
},{
    "category": "cs.DC", 
    "author": "P Jayarekha", 
    "title": "Pre-allocation Strategies of Computational Resources in Cloud Computing   using Adaptive Resonance Theory-2", 
    "publish": "2012-06-03T04:42:23Z", 
    "summary": "One of the major challenges of cloud computing is the management of\nrequest-response coupling and optimal allocation strategies of computational\nresources for the various types of service requests. In the normal situations\nthe intelligence required to classify the nature and order of the request using\nstandard methods is insufficient because the arrival of request is at a random\nfashion and it is meant for multiple resources with different priority order\nand variety. Hence, it becomes absolutely essential that we identify the trends\nof different request streams in every category by auto classifications and\norganize preallocation strategies in a predictive way. It calls for designs of\nintelligent modes of interaction between the client request and cloud computing\nresource manager. This paper discusses about the corresponding scheme using\nAdaptive Resonance Theory-2.", 
    "link": "http://arxiv.org/pdf/1206.0419v1", 
    "arxiv-id": "1206.0419v1"
},{
    "category": "cs.DC", 
    "author": "Azizah Abdul Rahman", 
    "title": "Virtualization Implementation Model for Cost Effective & Efficient Data   Centers", 
    "publish": "2012-04-07T05:19:32Z", 
    "summary": "Data centers form a key part of the infrastructure upon which a variety of\ninformation technology services are built. They provide the capabilities of\ncentralized repository for storage, management, networking and dissemination of\ndata. With the rapid increase in the capacity and size of data centers, there\nis a continuous increase in the demand for energy consumption. These data\ncenters not only consume a tremendous amount of energy but are riddled with IT\ninefficiencies. Data center are plagued with thousands of servers as major\ncomponents. These servers consume huge energy without performing useful work.\nIn an average server environment, 30% of the servers are \"dead\" only consuming\nenergy, without being properly utilized. This paper proposes a five step model\nusing an emerging technology called virtualization to achieve energy efficient\ndata centers. The proposed model helps Data Center managers to properly\nimplement virtualization technology in their data centers to make them green\nand energy efficient so as to ensure that IT infrastructure contributes as\nlittle as possible to the emission of greenhouse gases, and helps to regain\npower and cooling capacity, recapture resilience and dramatically reducing\nenergy costs and total cost of ownership.", 
    "link": "http://arxiv.org/pdf/1206.0988v1", 
    "arxiv-id": "1206.0988v1"
},{
    "category": "cs.DC", 
    "author": "Karl Henrik Johansson", 
    "title": "Finite-time Convergent Gossiping", 
    "publish": "2012-06-05T17:07:34Z", 
    "summary": "Gossip algorithms are widely used in modern distributed systems, with\napplications ranging from sensor networks and peer-to-peer networks to mobile\nvehicle networks and social networks. A tremendous research effort has been\ndevoted to analyzing and improving the asymptotic rate of convergence for\ngossip algorithms. In this work we study finite-time convergence of\ndeterministic gossiping. We show that there exists a symmetric gossip algorithm\nthat converges in finite time if and only if the number of network nodes is a\npower of two, while there always exists an asymmetric gossip algorithm with\nfinite-time convergence, independent of the number of nodes. For $n=2^m$ nodes,\nwe prove that a fastest convergence can be reached in $nm=n\\log_2 n$ node\nupdates via symmetric gossiping. On the other hand, under asymmetric gossip\namong $n=2^m+r$ nodes with $0\\leq r<2^m$, it takes at least $mn+2r$ node\nupdates for achieving finite-time convergence. It is also shown that the\nexistence of finite-time convergent gossiping often imposes strong structural\nrequirements on the underlying interaction graph. Finally, we apply our results\nto gossip algorithms in quantum networks, where the goal is to control the\nstate of a quantum system via pairwise interactions. We show that finite-time\nconvergence is never possible for such systems.", 
    "link": "http://arxiv.org/pdf/1206.0992v6", 
    "arxiv-id": "1206.0992v6"
},{
    "category": "cs.DC", 
    "author": "Abdullah Gani", 
    "title": "Research On Mobile Cloud Computing: Review, Trend, And Perspectives", 
    "publish": "2012-06-06T03:53:39Z", 
    "summary": "Mobile Cloud Computing (MCC) which combines mobile computing and cloud\ncomputing, has become one of the industry buzz words and a major discussion\nthread in the IT world since 2009. As MCC is still at the early stage of\ndevelopment, it is necessary to grasp a thorough understanding of the\ntechnology in order to point out the direction of future research. With the\nlatter aim, this paper presents a review on the background and principle of\nMCC, characteristics, recent research work, and future research trends. A brief\naccount on the background of MCC: from mobile computing to cloud computing is\npresented and then followed with a discussion on characteristics and recent\nresearch work. It then analyses the features and infrastructure of mobile cloud\ncomputing. The rest of the paper analyses the challenges of mobile cloud\ncomputing, summary of some research projects related to this area, and points\nout promising future research directions.", 
    "link": "http://arxiv.org/pdf/1206.1118v1", 
    "arxiv-id": "1206.1118v1"
},{
    "category": "cs.DC", 
    "author": "Paul G. Spirakis", 
    "title": "Causality, Influence, and Computation in Possibly Disconnected Dynamic   Networks", 
    "publish": "2012-06-06T18:10:34Z", 
    "summary": "In this work, we study the propagation of influence and computation in\ndynamic distributed systems. We focus on broadcasting models under a worst-case\ndynamicity assumption which have received much attention recently. We drop for\nthe first time in worst-case dynamic networks the common instantaneous\nconnectivity assumption and require a minimal temporal connectivity. Our\ntemporal connectivity constraint only requires that another causal influence\noccurs within every time-window of some given length. We establish that there\nare dynamic graphs with always disconnected instances with equivalent temporal\nconnectivity to those with always connected instances. We present a termination\ncriterion and also establish the computational equivalence with instantaneous\nconnectivity networks. We then consider another model of dynamic networks in\nwhich each node has an underlying communication neighborhood and the\nrequirement is that each node covers its local neighborhood within any\ntime-window of some given length. We discuss several properties and provide a\nprotocol for counting, that is for determining the number of nodes in the\nnetwork.", 
    "link": "http://arxiv.org/pdf/1206.1290v1", 
    "arxiv-id": "1206.1290v1"
},{
    "category": "cs.DC", 
    "author": "Anwitaman Datta", 
    "title": "PriSM: A Private Social Mesh for Leveraging Social Networking at   Workplace", 
    "publish": "2012-06-08T02:44:05Z", 
    "summary": "In this work we describe the PriSM framework for decentralized deployment of\na federation of autonomous social networks (ASN). The individual ASNs are\ncentrally managed by organizations according to their institutional needs,\nwhile cross-ASN interactions are facilitated subject to security and\nconfidentiality requirements specified by administrators and users of the ASNs.\nSuch decentralized deployment, possibly either on private or public clouds,\nprovides control and ownership of information/flow to individual organizations.\nLack of such complete control (if third party online social networking services\nwere to be used) has so far been a great barrier in taking full advantage of\nthe novel communication mechanisms at workplace that have however become\ncommonplace for personal usage with the advent of Web 2.0 platforms and online\nsocial networks. PriSM provides a practical solution for organizations to\nharness the advantages of online social networking both in\nintra/inter-organizational settings without sacrificing autonomy, security and\nconfidentiality needs.", 
    "link": "http://arxiv.org/pdf/1206.1653v2", 
    "arxiv-id": "1206.1653v2"
},{
    "category": "cs.DC", 
    "author": "Rajeev K. Shakya", 
    "title": "TTMA: Traffic-adaptive Time-division Multiple Access Protocol Wireless   Sensor Networks", 
    "publish": "2012-06-09T02:06:59Z", 
    "summary": "This paper has been withdrawn by arXiv. arXiv admin note: author list\ntruncated due to disputed authorship and content. This submission repeats large\nportions of text from this http URL by other authors. Duty cycle mode in WSN\nimproves energy-efficiency, but also introduces packet delivery latency.\nSeveral duty-cycle based MAC schemes have been proposed to reduce latency, but\nthroughput is limited by duty-cycled scheduling performance. In this paper, a\nTraffic-adaptive Time-division Multiple Access (TTMA), a distributed TDMA-based\nMAC protocol is introduced to improves the throughput by traffic-adaptive\ntime-slot scheduling that increases the channel utilisation efficiency. The\nproposed time-slot scheduling method first avoids time-slots assigned to nodes\nwith no traffic through fast traffic notification. It then achieves better\nchannel utilisation among nodes having traffic through an ordered schedule\nnegotiation scheme. By decomposing traffic notification and data transmission\nscheduling into two phases leads each phase to be simple and efficient. The\nperformance evaluation shows that the two-phase design significantly improves\nthe throughput and outperforms the time division multiple access (TDMA) control\nwith slot stealing.", 
    "link": "http://arxiv.org/pdf/1206.1899v3", 
    "arxiv-id": "1206.1899v3"
},{
    "category": "cs.DC", 
    "author": "Nasrin Jaberi", 
    "title": "Energy-Aware Scheduling using Dynamic Voltage-Frequency Scaling", 
    "publish": "2012-06-10T00:13:47Z", 
    "summary": "The energy consumption issue in distributed computing systems has become\nquite critical due to environmental concerns. In response to this, many\nenergy-aware scheduling algorithms have been developed primarily by using the\ndynamic voltage-frequency scaling (DVFS) capability incorporated in recent\ncommodity processors. The majority of these algorithms involve two passes:\nschedule generation and slack reclamation. The latter is typically achieved by\nlowering processor frequency for tasks with slacks. In this article, we study\nthe latest papers in this area and develop them. This study has been evaluated\nbased on results obtained from experiments with 1,500 randomly generated task\ngraphs.", 
    "link": "http://arxiv.org/pdf/1206.1984v1", 
    "arxiv-id": "1206.1984v1"
},{
    "category": "cs.DC", 
    "author": "Anwitaman Datta", 
    "title": "An Empirical Study of the Repair Performance of Novel Coding Schemes for   Networked Distributed Storage Systems", 
    "publish": "2012-06-11T12:55:53Z", 
    "summary": "Erasure coding techniques are getting integrated in networked distributed\nstorage systems as a way to provide fault-tolerance at the cost of less storage\noverhead than traditional replication. Redundancy is maintained over time\nthrough repair mechanisms, which may entail large network resource overheads.\nIn recent years, several novel codes tailor-made for distributed storage have\nbeen proposed to optimize storage overhead and repair, such as Regenerating\nCodes that minimize the per repair traffic, or Self-Repairing Codes which\nminimize the number of nodes contacted per repair. Existing studies of these\ncoding techniques are however predominantly theoretical, under the simplifying\nassumption that only one object is stored. They ignore many practical issues\nthat real systems must address, such as data placement, de/correlation of\nmultiple stored objects, or the competition for limited network resources when\nmultiple objects are repaired simultaneously. This paper empirically studies\nthe repair performance of these novel storage centric codes with respect to\nclassical erasure codes by simulating realistic scenarios and exploring the\ninterplay of code parameters, failure characteristics and data placement with\nrespect to the trade-offs of bandwidth usage and speed of repairs.", 
    "link": "http://arxiv.org/pdf/1206.2187v1", 
    "arxiv-id": "1206.2187v1"
},{
    "category": "cs.DC", 
    "author": "Moreno Marzolla", 
    "title": "Time Warp on the Go (Updated Version)", 
    "publish": "2012-06-13T11:48:48Z", 
    "summary": "In this paper we deal with the impact of multi and many-core processor\narchitectures on simulation. Despite the fact that modern CPUs have an\nincreasingly large number of cores, most softwares are still unable to take\nadvantage of them. In the last years, many tools, programming languages and\ngeneral methodologies have been proposed to help building scalable applications\nfor multi-core architectures, but those solutions are somewhat limited.\nParallel and distributed simulation is an interesting application area in which\nefficient and scalable multi-core implementations would be desirable. In this\npaper we investigate the use of the Go Programming Language to implement\noptimistic parallel simulations based on the Time Warp mechanism. Specifically,\nwe describe the design, implementation and evaluation of a new parallel\nsimulator. The scalability of the simulator is studied when in presence of a\nmodern multi-core CPU and the effects of the Hyper-Threading technology on\noptimistic simulation are analyzed.", 
    "link": "http://arxiv.org/pdf/1206.2772v3", 
    "arxiv-id": "1206.2772v3"
},{
    "category": "cs.DC", 
    "author": "Moreno Marzolla", 
    "title": "Parallel Discrete Event Simulation with Erlang", 
    "publish": "2012-06-13T12:12:21Z", 
    "summary": "Discrete Event Simulation (DES) is a widely used technique in which the state\nof the simulator is updated by events happening at discrete points in time\n(hence the name). DES is used to model and analyze many kinds of systems,\nincluding computer architectures, communication networks, street traffic, and\nothers. Parallel and Distributed Simulation (PADS) aims at improving the\nefficiency of DES by partitioning the simulation model across multiple\nprocessing elements, in order to enabling larger and/or more detailed studies\nto be carried out. The interest on PADS is increasing since the widespread\navailability of multicore processors and affordable high performance computing\nclusters. However, designing parallel simulation models requires considerable\nexpertise, the result being that PADS techniques are not as widespread as they\ncould be. In this paper we describe ErlangTW, a parallel simulation middleware\nbased on the Time Warp synchronization protocol. ErlangTW is entirely written\nin Erlang, a concurrent, functional programming language specifically targeted\nat building distributed systems. We argue that writing parallel simulation\nmodels in Erlang is considerably easier than using conventional programming\nlanguages. Moreover, ErlangTW allows simulation models to be executed either on\nsingle-core, multicore and distributed computing architectures. We describe the\ndesign and prototype implementation of ErlangTW, and report some preliminary\nperformance results on multicore and distributed architectures using the well\nknown PHOLD benchmark.", 
    "link": "http://arxiv.org/pdf/1206.2775v3", 
    "arxiv-id": "1206.2775v3"
},{
    "category": "cs.DC", 
    "author": "Alexandre van Kempen", 
    "title": "Clustered Network Coding for Maintenance in Practical Storage Systems", 
    "publish": "2012-06-19T11:01:42Z", 
    "summary": "Classical erasure codes, e.g. Reed-Solomon codes, have been acknowledged as\nan efficient alternative to plain replication to reduce the storage overhead in\nreliable distributed storage systems. Yet, such codes experience high overhead\nduring the maintenance process. In this paper we propose a novel erasure-coded\nframework especially tailored for networked storage systems. Our approach\nrelies on the use of random codes coupled with a clustered placement strategy,\nenabling the maintenance of a failed machine at the granularity of multiple\nfiles. Our repair protocol leverages network coding techniques to reduce by\nhalf the amount of data transferred during maintenance, as several files can be\nrepaired simultaneously. This approach, as formally proven and demonstrated by\nour evaluation on a public experimental testbed, enables to dramatically\ndecrease the bandwidth overhead during the maintenance process, as well as the\ntime to repair a failure. In addition, the implementation is made as simple as\npossible, aiming at a deployment into practical systems.", 
    "link": "http://arxiv.org/pdf/1206.4175v1", 
    "arxiv-id": "1206.4175v1"
},{
    "category": "cs.DC", 
    "author": "Kah Phooi Seng", 
    "title": "Performance Evaluation of Ant-Based Routing Protocols for Wireless   Sensor Networks", 
    "publish": "2012-06-26T09:27:57Z", 
    "summary": "High efficient routing is an important issue in the design of limited energy\nresource Wireless Sensor Networks (WSNs). Due to the characteristic of the\nenvironment at which the sensor node is to operate, coupled with severe\nresources; on-board energy, transmission power, processing capability, and\nstorage limitations, prompt for careful resource management and new routing\nprotocol so as to counteract the differences and challenges. To this end, we\npresent an Improved Energy-Efficient Ant-Based Routing (IEEABR) Algorithm in\nwireless sensor networks. Compared to the state-of-the-art Ant-Based routing\nprotocols; Basic Ant-Based Routing (BABR) Algorithm, Sensor-driven and\nCost-aware ant routing (SC), Flooded Forward ant routing (FF), Flooded\nPiggybacked ant routing (FP), and Energy-Efficient Ant-Based Routing (EEABR),\nthe proposed IEEABR approach has advantages in terms of reduced energy usage\nwhich can effectively balance the WSN node's power consumption, and high energy\nefficiency. The performance evaluations for the algorithms on a real\napplication are conducted in a well known WSN MATLAB-based simulator (RMASE)\nusing both static and dynamic scenario.", 
    "link": "http://arxiv.org/pdf/1206.5938v1", 
    "arxiv-id": "1206.5938v1"
},{
    "category": "cs.DC", 
    "author": "A. Anasuya Threse Innocent", 
    "title": "Cloud Infrastructure Service Management - A Review", 
    "publish": "2012-05-30T09:45:54Z", 
    "summary": "The new era of computing called Cloud Computing allows the user to access the\ncloud services dynamically over the Internet wherever and whenever needed.\nCloud consists of data and resources; and the cloud services include the\ndelivery of software, infrastructure, applications, and storage over the\nInternet based on user demand through Internet. In short, cloud computing is a\nbusiness and economic model allowing the users to utilize high-end computing\nand storage virtually with minimal infrastructure on their end. Cloud has three\nservice models namely, Cloud Software-as-a-Service (SaaS), Cloud\nPlatform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS).\nThis paper talks in depth of cloud infrastructure service management.", 
    "link": "http://arxiv.org/pdf/1206.6016v1", 
    "arxiv-id": "1206.6016v1"
},{
    "category": "cs.DC", 
    "author": "Jue Hong", 
    "title": "An Optimal Fully Distributed Algorithm to Minimize the Resource   Consumption of Cloud Applications", 
    "publish": "2012-06-27T09:02:05Z", 
    "summary": "According to the pay-per-use model adopted in clouds, the more the resources\nconsumed by an application running in a cloud computing environment, the\ngreater the amount of money the owner of the corresponding application will be\ncharged. Therefore, applying intelligent solutions to minimize the resource\nconsumption is of great importance. Because centralized solutions are deemed\nunsuitable for large-distributed systems or large-scale applications, we\npropose a fully distributed algorithm (called DRA) to overcome the scalability\nissues. Specifically, DRA migrates the inter-communicating components of an\napplication, such as processes or virtual machines, close to each other to\nminimize the total resource consumption. The migration decisions are made in a\ndynamic way and based only on local information. We prove that DRA achieves\nconvergence and results always in the optimal solution.", 
    "link": "http://arxiv.org/pdf/1206.6207v1", 
    "arxiv-id": "1206.6207v1"
},{
    "category": "cs.DC", 
    "author": "Abdullah Gani", 
    "title": "Virtual Machine Migration: A Resource Intensive Outsourcing Mechanism   for Mobile Cloud Computing", 
    "publish": "2012-06-27T10:23:14Z", 
    "summary": "In Mobile Cloud Computing (MCC), Virtual Machine (VM) migration based process\noffloading is a dominant approach to enhance Smart Mobile Devices (SMDs). A\nchallenging aspect of VM deployment is the additional computing resources usage\nin the deployment and management of VM which obliges computing resources for VM\ncreation and configuration. The management of VM comprises computing resources\nexploitation in the monitoring of VM in entire lifecycle and physical resources\nmanagement for VM on SMDs. Therefore, VM migration based application offloading\nrequires additional computing resource. Consequently computing resources demand\nand execution time of the application increases respectively. In this paper, we\nempirically review the impact of VM deployment and management on the execution\ntime of application in diverse scenarios. We investigate VM deployment and\nmanagement for application processing in simulation environment by employing\nCloudSim: a simulation toolkit that provides an extensible simulation framework\nto model VM deployment and management for application processing in cloud\ninfrastructure. The significance of this work is to ensure that VM deployment\nand management necessitates additional computing resources on SMD for\napplication offloading. We evaluate VM deployment and management in application\nprocessing by analyzing Key Performance Parameters (KPPs) in different\nscenarios; such as VM deployment, the execution time of applications, and total\nexecution time of the simulation. We use KPPs to assess deviations in the\nresults of diverse experimental scenarios. The empirical analysis concludes\nthat VM deployment and management oblige additional resources on computing host\nwhich make it a heavyweight approach for process offloading on smart mobile\ndevice.", 
    "link": "http://arxiv.org/pdf/1206.6225v3", 
    "arxiv-id": "1206.6225v3"
},{
    "category": "cs.DC", 
    "author": "Marimuthu Palaniswami", 
    "title": "Internet of Things (IoT): A Vision, Architectural Elements, and Future   Directions", 
    "publish": "2012-07-01T13:10:15Z", 
    "summary": "Ubiquitous sensing enabled by Wireless Sensor Network (WSN) technologies cuts\nacross many areas of modern day living. This offers the ability to measure,\ninfer and understand environmental indicators, from delicate ecologies and\nnatural resources to urban environments. The proliferation of these devices in\na communicating-actuating network creates the Internet of Things (IoT),\nwherein, sensors and actuators blend seamlessly with the environment around us,\nand the information is shared across platforms in order to develop a common\noperating picture (COP). Fuelled by the recent adaptation of a variety of\nenabling device technologies such as RFID tags and readers, near field\ncommunication (NFC) devices and embedded sensor and actuator nodes, the IoT has\nstepped out of its infancy and is the the next revolutionary technology in\ntransforming the Internet into a fully integrated Future Internet. As we move\nfrom www (static pages web) to web2 (social networking web) to web3 (ubiquitous\ncomputing web), the need for data-on-demand using sophisticated intuitive\nqueries increases significantly. This paper presents a cloud centric vision for\nworldwide implementation of Internet of Things. The key enabling technologies\nand application domains that are likely to drive IoT research in the near\nfuture are discussed. A cloud implementation using Aneka, which is based on\ninteraction of private and public clouds is presented. We conclude our IoT\nvision by expanding on the need for convergence of WSN, the Internet and\ndistributed computing directed at technological research community.", 
    "link": "http://arxiv.org/pdf/1207.0203v1", 
    "arxiv-id": "1207.0203v1"
},{
    "category": "cs.DC", 
    "author": "Dariusz R. Kowalski", 
    "title": "Distributed backbone structure for deterministic algorithms in the SINR   model of wireless networks", 
    "publish": "2012-07-03T08:10:21Z", 
    "summary": "The Signal-to-Interference-and-Noise-Ratio (SINR) physical model is one of\nthe legitimate models of wireless networks. Despite of the vast amount of study\ndone in design and analysis of centralized algorithms supporting wireless\ncommunication under the SINR physical model, little is known about distributed\nalgorithms in this model, especially deterministic ones. In this work we\nconstruct, in a deterministic distributed way, a backbone structure on the top\nof a given wireless network, which can be used for transforming many algorithms\ndesigned in a simpler model of ad hoc broadcast networks without interference\ninto the SINR physical model with uniform power of stations, without increasing\ntheir asymptotic time complexity. The time cost of the backbone data structure\nconstruction is only O(Delta polylog n) rounds, where Delta is roughly the\ninverse of network density and n is the number of nodes in the whole network.\nThe core of the construction is a novel combinatorial structure called\nSINR-selector, which is introduced and constructed in this paper. We\ndemonstrate the power of the backbone data structure by using it for obtaining\nefficient O(D+Delta polylog n)-round and O(D+k+Delta polylog n)-round\ndeterministic distributed solutions for leader election and multi-broadcast,\nrespectively, where D is the network diameter and k is the number of messages\nto be disseminated.", 
    "link": "http://arxiv.org/pdf/1207.0602v2", 
    "arxiv-id": "1207.0602v2"
},{
    "category": "cs.DC", 
    "author": "L. S. S. Reddy", 
    "title": "Survey on Improved Scheduling in Hadoop MapReduce in Cloud Environments", 
    "publish": "2012-07-03T19:01:26Z", 
    "summary": "Cloud Computing is emerging as a new computational paradigm shift.\nHadoop-MapReduce has become a powerful Computation Model for processing large\ndata on distributed commodity hardware clusters such as Clouds. In all Hadoop\nimplementations, the default FIFO scheduler is available where jobs are\nscheduled in FIFO order with support for other priority based schedulers also.\nIn this paper we study various scheduler improvements possible with Hadoop and\nalso provided some guidelines on how to improve the scheduling in Hadoop in\nCloud Environments.", 
    "link": "http://arxiv.org/pdf/1207.0780v1", 
    "arxiv-id": "1207.0780v1"
},{
    "category": "cs.DC", 
    "author": "L. S. S. Reddy", 
    "title": "Performance Issues of Heterogeneous Hadoop Clusters in Cloud Computing", 
    "publish": "2012-07-04T04:18:03Z", 
    "summary": "Nowadays most of the cloud applications process large amount of data to\nprovide the desired results. Data volumes to be processed by cloud applications\nare growing much faster than computing power. This growth demands new\nstrategies for processing and analyzing information. Dealing with large data\nvolumes requires two things: 1) Inexpensive, reliable storage 2) New tools for\nanalyzing unstructured and structured data. Hadoop is a powerful open source\nsoftware platform that addresses both of these problems. The current Hadoop\nimplementation assumes that computing nodes in a cluster are homogeneous in\nnature. Hadoop lacks performance in heterogeneous clusters where the nodes have\ndifferent computing capacity. In this paper we address the issues that affect\nthe performance of hadoop in heterogeneous clusters and also provided some\nguidelines on how to overcome these bottlenecks", 
    "link": "http://arxiv.org/pdf/1207.0894v1", 
    "arxiv-id": "1207.0894v1"
},{
    "category": "cs.DC", 
    "author": "Franck Petit", 
    "title": "Optimization in a Self-Stabilizing Service Discovery Framework for Large   Scale Systems", 
    "publish": "2012-07-05T19:38:00Z", 
    "summary": "Ability to find and get services is a key requirement in the development of\nlarge-scale distributed sys- tems. We consider dynamic and unstable\nenvironments, namely Peer-to-Peer (P2P) systems. In previous work, we designed\na service discovery solution called Distributed Lexicographic Placement Table\n(DLPT), based on a hierar- chical overlay structure. A self-stabilizing version\nwas given using the Propagation of Information with Feedback (PIF) paradigm. In\nthis paper, we introduce the self-stabilizing COPIF (for Collaborative PIF)\nscheme. An algo- rithm is provided with its correctness proof. We use this\napproach to improve a distributed P2P framework designed for the services\ndiscovery. Significantly efficient experimental results are presented.", 
    "link": "http://arxiv.org/pdf/1207.1337v1", 
    "arxiv-id": "1207.1337v1"
},{
    "category": "cs.DC", 
    "author": "\u0141ukasz Miros\u0142aw", 
    "title": "Complete PISO and SIMPLE solvers on Graphics Processing Units", 
    "publish": "2012-07-06T10:02:26Z", 
    "summary": "We implemented the pressure-implicit with splitting of operators (PISO) and\nsemi-implicit method for pressure-linked equations (SIMPLE) solvers of the\nNavier-Stokes equations on Fermi-class graphics processing units (GPUs) using\nthe CUDA technology. We also introduced a new format of sparse matrices\noptimized for performing elementary CFD operations, like gradient or divergence\ndiscretization, on GPUs. We verified the validity of the implementation on\nseveral standard, steady and unsteady problems. Computational effciency of the\nGPU implementation was examined by comparing its double precision run times\nwith those of essentially the same algorithms implemented in OpenFOAM. The\nresults show that a GPU (Tesla C2070) can outperform a server-class 6-core,\n12-thread CPU (Intel Xeon X5670) by a factor of 4.2.", 
    "link": "http://arxiv.org/pdf/1207.1571v1", 
    "arxiv-id": "1207.1571v1"
},{
    "category": "cs.DC", 
    "author": "Christoph Lenzen", 
    "title": "Optimal Deterministic Routing and Sorting on the Congested Clique", 
    "publish": "2012-07-08T07:49:22Z", 
    "summary": "Consider a clique of n nodes, where in each synchronous round each pair of\nnodes can exchange O(log n) bits. We provide deterministic constant-time\nsolutions for two problems in this model. The first is a routing problem where\neach node is source and destination of n messages of size O(log n). The second\nis a sorting problem where each node i is given n keys of size O(log n) and\nneeds to receive the ith batch of n keys according to the global order of the\nkeys. The latter result also implies deterministic constant-round solutions for\nrelated problems such as selection or determining modes.", 
    "link": "http://arxiv.org/pdf/1207.1852v4", 
    "arxiv-id": "1207.1852v4"
},{
    "category": "cs.DC", 
    "author": "Ankit Nischal", 
    "title": "Efficient Resource Allocation in Resource provisioning policies over   Resource Cloud Communication Paradigm", 
    "publish": "2012-07-11T16:37:51Z", 
    "summary": "Optimal resource utilization for executing tasks within the cloud is one of\nthe biggest challenges. In executing the task over a cloud, the resource\nprovisioner is responsible for providing the resources to create virtual\nmachines. To utilize the resources optimally, the resource provisioner has to\ntake care of the process of allocating resources to Virtual Machine Manager\n(VMM). In this paper, an efficient way to utilize the resources, within the\ncloud, to create virtual machines has been proposed considering optimum cost\nbased on performance factor. This performance factor depends upon the overall\ncost of the resource, communication channel cost, reliability and popularity\nfactor. We have proposed a framework for communication between resource owner\nand cloud using Resource Cloud Communication Paradigm (RCCP). We extend the\nCloudSim[2] adding provisioner policies and Efficient Resource Allocation (ERA)\nalgorithm in VMM allocation policy as a decision support for resource\nprovisioner.", 
    "link": "http://arxiv.org/pdf/1207.2704v1", 
    "arxiv-id": "1207.2704v1"
},{
    "category": "cs.DC", 
    "author": "Kamaljit Kaur", 
    "title": "Secure Cloud Communication for Effective Cost Management System through   MSBE", 
    "publish": "2012-07-11T16:51:01Z", 
    "summary": "In Cloud Computing Architecture, Brokers are responsible to provide services\nto the end users. An Effective Cost Management System (ECMS) which works over\nSecure Cloud Communication Paradigm (SCCP) helps in finding a communication\nlink with overall minimum cost of links. We propose an improved Broker Cloud\nCommunication Paradigm (BCCP) with integration of security issues. Two\nalgorithms are included, first is Secure Optimized Route Cost Finder (S-ORCF)\nto find optimum route between broker and cloud on the behalf of cost factor and\nsecond is Secure Optimized Route Management (S-ORM) to maintain optimum route.\nThese algorithms proposed with cryptographic integrity of the secure route\ndiscovery process in efficient routing approaches between broker and cloud.\nThere is lack in Dynamic Source Routing Approach to verify whether any\nintermediate node has been deleted, inserted or modified with no valid\nauthentication. We use symmetric cryptographic primitives, which is made\npossible due to multisource broadcast encryption scheme. This paper outlines\nthe use of secure route discovery protocol (SRDP)that employs such a security\nparadigm in cloud computing.", 
    "link": "http://arxiv.org/pdf/1207.2706v1", 
    "arxiv-id": "1207.2706v1"
},{
    "category": "cs.DC", 
    "author": "Sonika Setia", 
    "title": "Effective Cost Mechanism for Cloudlet Retransmission and Prioritized VM   Scheduling Mechanism over Broker Virtual Machine Communication Framework", 
    "publish": "2012-07-11T16:57:18Z", 
    "summary": "In current scenario cloud computing is most widely increasing platform for\ntask execution. Lot of research is going on to cut down the cost and execution\ntime. In this paper, we propose an efficient algorithm to have an effective and\nfast execution of task assigned by the user. We proposed an effective\ncommunication framework between broker and virtual machine for assigning the\ntask and fetching the results in optimum time and cost using Broker Virtual\nMachine Communication Framework (BVCF). We implement it over cloudsim under VM\nscheduling policies by modification based on Virtual Machine Cost. Scheduling\nover Virtual Machine as well as over Cloudlets and Retransmission of Cloudlets\nare the basic building blocks of the proposed work on which the whole\narchitecture is dependent. Execution of cloudlets is being analyzed over Round\nRobin and FCFS scheduling policy.", 
    "link": "http://arxiv.org/pdf/1207.2708v1", 
    "arxiv-id": "1207.2708v1"
},{
    "category": "cs.DC", 
    "author": "Ghobad Zarrinchian", 
    "title": "A Novel Process Mapping Strategy in Clustered Environments", 
    "publish": "2012-07-12T08:44:48Z", 
    "summary": "Nowadays the number of available processing cores within computing nodes\nwhich are used in recent clustered environments, are growing up with a rapid\nrate. Despite this trend, the number of available network interfaces in such\ncomputing nodes has almost been remained unchanged. This issue can lead to high\nusage of network interface in many workloads, especially in heavy-communicating\nworkloads. As a result, network interface may raise as a performance bottleneck\nand can drastically degrade the performance. The goal of this paper is to\nintroduce a new process mapping strategy in multi-core clusters aimed at\nreducing network interface contention and improving inter-node communication\nperformance of parallel applications. Performance evaluation of the new mapping\nalgorithm in synthetic and real workloads indicates that the new strategy can\nachieve 5% to 90% performance improvement in heavy communicating workloads,\ncompared to other well-known methods.", 
    "link": "http://arxiv.org/pdf/1207.2878v1", 
    "arxiv-id": "1207.2878v1"
},{
    "category": "cs.DC", 
    "author": "Kumud Singh", 
    "title": "Implementation of Private Cloud using Eucalyptus and an open source   Operating System", 
    "publish": "2012-07-11T15:57:29Z", 
    "summary": "Cloud computing is bringing a revolution in computing environment replacing\ntraditional software installations, licensing issues into complete on-demand\nservices through internet. Microsoft office 365 a cloud based office\napplication is available to clients online hence no need to buy and install the\nsoftware. On Facebook a social networking website, users upload videos which\nuses cloud provider's storage service so less hardware cost for\nclients.Virtualization technology has great contribution in advent of cloud\ncomputing. Paper describes implementation of Private Cloud using open source\noperating system Ubuntu 10.04 server edition, installation of Ubuntu Enterprise\nCloud with Eucalyptus 1.6.2 and providing CentOS 5.3 operating system through\ncloud.", 
    "link": "http://arxiv.org/pdf/1207.3037v1", 
    "arxiv-id": "1207.3037v1"
},{
    "category": "cs.DC", 
    "author": "Sriram Pemmaraju", 
    "title": "Super-Fast 3-Ruling Sets", 
    "publish": "2012-07-12T21:02:17Z", 
    "summary": "A $t$-ruling set of a graph $G = (V, E)$ is a vertex-subset $S \\subseteq V$\nthat is independent and satisfies the property that every vertex $v \\in V$ is\nat a distance of at most $t$ from some vertex in $S$. A \\textit{maximal\nindependent set (MIS)} is a 1-ruling set. The problem of computing an MIS on a\nnetwork is a fundamental problem in distributed algorithms and the fastest\nalgorithm for this problem is the $O(\\log n)$-round algorithm due to Luby\n(SICOMP 1986) and Alon et al. (J. Algorithms 1986) from more than 25 years ago.\nSince then the problem has resisted all efforts to yield to a sub-logarithmic\nalgorithm. There has been recent progress on this problem, most importantly an\n$O(\\log \\Delta \\cdot \\sqrt{\\log n})$-round algorithm on graphs with $n$\nvertices and maximum degree $\\Delta$, due to Barenboim et al. (Barenboim,\nElkin, Pettie, and Schneider, April 2012, arxiv 1202.1983; to appear FOCS\n2012).\n  We approach the MIS problem from a different angle and ask if O(1)-ruling\nsets can be computed much more efficiently than an MIS? As an answer to this\nquestion, we show how to compute a 2-ruling set of an $n$-vertex graph in\n$O((\\log n)^{3/4})$ rounds. We also show that the above result can be improved\nfor special classes of graphs such as graphs with high girth, trees, and graphs\nof bounded arboricity.\n  Our main technique involves randomized sparsification that rapidly reduces\nthe graph degree while ensuring that every deleted vertex is close to some\nvertex that remains. This technique may have further applications in other\ncontexts, e.g., in designing sub-logarithmic distributed approximation\nalgorithms. Our results raise intriguing questions about how quickly an MIS (or\n1-ruling sets) can be computed, given that 2-ruling sets can be computed in\nsub-logarithmic rounds.", 
    "link": "http://arxiv.org/pdf/1207.3099v1", 
    "arxiv-id": "1207.3099v1"
},{
    "category": "cs.DC", 
    "author": "Ankit Singla", 
    "title": "On the Resilience of Routing Tables", 
    "publish": "2012-07-16T17:17:36Z", 
    "summary": "Many modern network designs incorporate \"failover\" paths into routers'\nforwarding tables. We initiate the theoretical study of the conditions under\nwhich such resilient routing tables can guarantee delivery of packets.", 
    "link": "http://arxiv.org/pdf/1207.3732v2", 
    "arxiv-id": "1207.3732v2"
},{
    "category": "cs.DC", 
    "author": "Tianyi Zang", 
    "title": "Survey and comparison for Open and closed sources in cloud computing", 
    "publish": "2012-07-23T18:39:49Z", 
    "summary": "Cloud computing is a new technology widely studied in recent years. Now there\nare many cloud platforms both in industry and in academic circle. How to\nunderstand and use these platforms is a big issue. A detailed comparison has\nbeen presented in this paper focused on the aspects such as the architecture,\ncharacteristics, application and so on. To know the differences between open\nsource and close source in cloud environment we mention some examples for\nSoftware-as-a-Service, Platform-as-a-Service, and Infrastructure-as-a-Service.\nWe made comparison between them. Before conclusion we demonstrate some\nconvergences and differences between open and closed platform, but we realized\nopen source should be the best option.", 
    "link": "http://arxiv.org/pdf/1207.5480v1", 
    "arxiv-id": "1207.5480v1"
},{
    "category": "cs.DC", 
    "author": "Michael G. Rabbat", 
    "title": "The Impact of Communication Delays on Distributed Consensus Algorithms", 
    "publish": "2012-07-24T22:15:10Z", 
    "summary": "We study the effect of communication delays on distributed consensus\nalgorithms. Two ways to model delays on a network are presented. The first\nmodel assumes that each link delivers messages with a fixed (constant) amount\nof delay, and the second model is more realistic, allowing for i.i.d.\ntime-varying bounded delays. In contrast to previous work studying the effects\nof delays on consensus algorithms, the models studied here allow for a node to\nreceive multiple messages from the same neighbor in one iteration. The analysis\nof the fixed delay model shows that convergence to a consensus is guaranteed\nand the rate of convergence is reduced by no more than a factor O(B^2) where B\nis the maximum delay on any link. For the time-varying delay model we also give\na convergence proof which, for row-stochastic consensus protocols, is not a\ntrivial consequence of ergodic matrix products. In both delay models, the\nconsensus value is no longer the average, even if the original protocol was an\naveraging protocol. For this reason, we propose the use of a different\nconsensus algorithm called Push-Sum [Kempe et al. 2003]. We model delays in the\nPush-Sum framework and show that convergence to the average consensus is\nguaranteed. This suggests that Push-Sum might be a better choice from a\npractical standpoint.", 
    "link": "http://arxiv.org/pdf/1207.5839v1", 
    "arxiv-id": "1207.5839v1"
},{
    "category": "cs.DC", 
    "author": "Christine Morin", 
    "title": "Using Open Standards for Interoperability - Issues, Solutions, and   Challenges facing Cloud Computing", 
    "publish": "2012-07-25T10:54:08Z", 
    "summary": "Virtualization offers several benefits for optimal resource utilization over\ntraditional non-virtualized server farms. With improvements in internetworking\ntechnologies and increase in network bandwidth speeds, a new era of computing\nhas been ushered in, that of grids and clouds. With several commercial cloud\nproviders coming up, each with their own APIs, application description formats,\nand varying support for SLAs, vendor lock-in has become a serious issue for end\nusers. This article attempts to describe the problem, issues, possible\nsolutions and challenges in achieving cloud interoperability. These issues will\nbe analyzed in the ambit of the European project Contrail that is trying to\nadopt open standards with available virtualization solutions to enhance users'\ntrust in the clouds by attempting to prevent vendor lock-ins, supporting and\nenforcing SLAs together with adequate data protection for sensitive data.", 
    "link": "http://arxiv.org/pdf/1207.5949v1", 
    "arxiv-id": "1207.5949v1"
},{
    "category": "cs.DC", 
    "author": "Maurizio Naldi", 
    "title": "Analysis of cloud storage prices", 
    "publish": "2012-07-25T14:45:55Z", 
    "summary": "Cloud storage is fast securing its role as a major repository for both\nconsumers and business customers. Many companies now offer storage solutions,\nsometimes for free for limited amounts of capacity. We have surveyed the\npricing plans of a selection of major cloud providers and compared them using\nthe unit price as the means of comparison. All the providers, excepting Amazon,\nadopt a bundling pricing scheme; Amazon follows instead a block-declining\npricing policy. We compare the pricing plans through a double approach: a\npointwise comparison for each value of capacity, and an overall comparison\nusing a two-part tariff approximation and a Pareto-dominance criterion. Under\nboth approaches, most providers appear to offer pricing plans that are more\nexpensive and can be excluded from a procurement selection in favour of a\nlimited number of dominant providers.", 
    "link": "http://arxiv.org/pdf/1207.6011v1", 
    "arxiv-id": "1207.6011v1"
},{
    "category": "cs.DC", 
    "author": "Shantenu Jha", 
    "title": "P*: A Model of Pilot-Abstractions", 
    "publish": "2012-07-27T20:08:12Z", 
    "summary": "Pilot-Jobs support effective distributed resource utilization, and are\narguably one of the most widely-used distributed computing abstractions - as\nmeasured by the number and types of applications that use them, as well as the\nnumber of production distributed cyberinfrastructures that support them. In\nspite of broad uptake, there does not exist a well-defined, unifying conceptual\nmodel of Pilot-Jobs which can be used to define, compare and contrast different\nimplementations. Often Pilot-Job implementations are strongly coupled to the\ndistributed cyber-infrastructure they were originally designed for. These\nfactors present a barrier to extensibility and interoperability. This pa- per\nis an attempt to (i) provide a minimal but complete model (P*) of Pilot-Jobs,\n(ii) establish the generality of the P* Model by mapping various existing and\nwell known Pilot-Job frameworks such as Condor and DIANE to P*, (iii) derive an\ninteroperable and extensible API for the P* Model (Pilot-API), (iv) validate\nthe implementation of the Pilot-API by concurrently using multiple distinct\nPilot-Job frameworks on distinct production distributed cyberinfrastructures,\nand (v) apply the P* Model to Pilot-Data.", 
    "link": "http://arxiv.org/pdf/1207.6644v1", 
    "arxiv-id": "1207.6644v1"
},{
    "category": "cs.DC", 
    "author": "Grzegorz Stachowiak", 
    "title": "Distributed Broadcasting in Wireless Networks under the SINR Model", 
    "publish": "2012-07-28T21:21:21Z", 
    "summary": "In the advent of large-scale multi-hop wireless technologies, such as MANET,\nVANET, iThings, it is of utmost importance to devise efficient distributed\nprotocols to maintain network architecture and provide basic communication\ntools. One of such fundamental communication tasks is broadcast, also known as\na 1-to-all communication. We propose several new efficient distributed\nalgorithms and evaluate their time performance both theoretically and by\nsimulations. First randomized algorithm accomplishes broadcast in O(D+log(1/d))\nrounds with probability at least 1-d on any uniform-power network of n nodes\nand diameter D, when equipped with local estimate of network density.\nAdditionally, we evaluate average performance of this protocols by simulations\non two classes of generated networks - uniform and social - and compare the\nresults with performance of exponential backoff heuristic. Ours is the first\nprovably efficient and well-scalable distributed solution for the (global)\nbroadcast task. The second randomized protocol developed in this paper does not\nrely on the estimate of local density, and achieves only slightly higher time\nperformance O((D+log(1/d))log n). Finally, we provide a deterministic algorithm\nachieving similar time O(D log^2 n), supported by theoretical analysis.", 
    "link": "http://arxiv.org/pdf/1207.6732v2", 
    "arxiv-id": "1207.6732v2"
},{
    "category": "cs.DC", 
    "author": "Frederique Oggier", 
    "title": "RapidRAID: Pipelined Erasure Codes for Fast Data Archival in Distributed   Storage Systems", 
    "publish": "2012-07-29T04:27:44Z", 
    "summary": "To achieve reliability in distributed storage systems, data has usually been\nreplicated across different nodes. However the increasing volume of data to be\nstored has motivated the introduction of erasure codes, a storage efficient\nalternative to replication, particularly suited for archival in data centers,\nwhere old datasets (rarely accessed) can be erasure encoded, while replicas are\nmaintained only for the latest data. Many recent works consider the design of\nnew storage-centric erasure codes for improved repairability. In contrast, this\npaper addresses the migration from replication to encoding: traditionally\nerasure coding is an atomic operation in that a single node with the whole\nobject encodes and uploads all the encoded pieces. Although large datasets can\nbe concurrently archived by distributing individual object encodings among\ndifferent nodes, the network and computing capacity of individual nodes\nconstrain the archival process due to such atomicity.\n  We propose a new pipelined coding strategy that distributes the network and\ncomputing load of single-object encodings among different nodes, which also\nspeeds up multiple object archival. We further present RapidRAID codes, an\nexplicit family of pipelined erasure codes which provides fast archival without\ncompromising either data reliability or storage overheads. Finally, we provide\na real implementation of RapidRAID codes and benchmark its performance using\nboth a cluster of 50 nodes and a set of Amazon EC2 instances. Experiments show\nthat RapidRAID codes reduce a single object's coding time by up to 90%, while\nwhen multiple objects are encoded concurrently, the reduction is up to 20%.", 
    "link": "http://arxiv.org/pdf/1207.6744v2", 
    "arxiv-id": "1207.6744v2"
},{
    "category": "cs.DC", 
    "author": "Ramesh K. Sitaraman", 
    "title": "Optimizing MapReduce for Highly Distributed Environments", 
    "publish": "2012-07-30T19:42:31Z", 
    "summary": "MapReduce, the popular programming paradigm for large-scale data processing,\nhas traditionally been deployed over tightly-coupled clusters where the data is\nalready locally available. The assumption that the data and compute resources\nare available in a single central location, however, no longer holds for many\nemerging applications in commercial, scientific and social networking domains,\nwhere the data is generated in a geographically distributed manner. Further,\nthe computational resources needed for carrying out the data analysis may be\ndistributed across multiple data centers or community resources such as Grids.\nIn this paper, we develop a modeling framework to capture MapReduce execution\nin a highly distributed environment comprising distributed data sources and\ndistributed computational resources. This framework is flexible enough to\ncapture several design choices and performance optimizations for MapReduce\nexecution. We propose a model-driven optimization that has two key features:\n(i) it is end-to-end as opposed to myopic optimizations that may only make\nlocally optimal but globally suboptimal decisions, and (ii) it can control\nmultiple MapReduce phases to achieve low runtime, as opposed to single-phase\noptimizations that may control only individual phases. Our model results show\nthat our optimization can provide nearly 82% and 64% reduction in execution\ntime over myopic and single-phase optimizations, respectively. We have modified\nHadoop to implement our model outputs, and using three different MapReduce\napplications over an 8-node emulated PlanetLab testbed, we show that our\noptimized Hadoop execution plan achieves 31-41% reduction in runtime over a\nvanilla Hadoop execution. Our model-driven optimization also provides several\ninsights into the choice of techniques and execution parameters based on\napplication and platform characteristics.", 
    "link": "http://arxiv.org/pdf/1207.7055v1", 
    "arxiv-id": "1207.7055v1"
},{
    "category": "cs.DC", 
    "author": "Lei Xu", 
    "title": "Feedback from nature: an optimal distributed algorithm for maximal   independent set selection", 
    "publish": "2012-11-01T17:41:34Z", 
    "summary": "Maximal Independent Set selection is a fundamental problem in distributed\ncomputing. A novel probabilistic algorithm for this problem has recently been\nproposed by Afek et al, inspired by the study of the way that developing cells\nin the fly become specialised. The algorithm they propose is simple and robust,\nbut not as efficient as previous approaches: the expected time complexity is\nO(log^2 n). Here we first show that the approach of Afek et al cannot achieve\nbetter efficiency than this across all networks, no matter how the probability\nvalues are chosen. However, we then propose a new algorithm that incorporates\nanother important feature of the biological system: adapting the probabilities\nused at each node based on local feedback from neighbouring nodes. Our new\nalgorithm retains all the advantages of simplicity and robustness, but also\nachieves the optimal efficiency of O(log n) expected time.", 
    "link": "http://arxiv.org/pdf/1211.0235v1", 
    "arxiv-id": "1211.0235v1"
},{
    "category": "cs.DC", 
    "author": "Nandini Mukherjee", 
    "title": "Application-centric Resource Provisioning for Amazon EC2 Spot Instances", 
    "publish": "2012-11-06T15:58:55Z", 
    "summary": "In late 2009, Amazon introduced spot instances to offer their unused\nresources at lower cost with reduced reliability. Amazon's spot instances allow\ncustomers to bid on unused Amazon EC2 capacity and run those instances for as\nlong as their bid exceeds the current spot price. The spot price changes\nperiodically based on supply and demand, and customers whose bids exceed it\ngain access to the available spot instances. Customers may expect their\nservices at lower cost with spot instances compared to on-demand or reserved.\nHowever the reliability is compromised since the instances(IaaS) providing the\nservice(SaaS) may become unavailable at any time without any notice to the\ncustomer. Checkpointing and migration schemes are of great use to cope with\nsuch situation. In this paper we study various checkpointing schemes that can\nbe used with spot instances. Also we device some algorithms for checkpointing\nscheme on top of application-centric resource provisioning framework that\nincrease the reliability while reducing the cost significantly.", 
    "link": "http://arxiv.org/pdf/1211.1279v1", 
    "arxiv-id": "1211.1279v1"
},{
    "category": "cs.DC", 
    "author": "Vipul A. Shah", 
    "title": "Advance Reservation based DAG Application Scheduling Simulator for Grid   Environment", 
    "publish": "2012-11-07T04:21:58Z", 
    "summary": "In the last decade, scheduling of Directed Acyclic Graph (DAG) application in\nthe context of Grid environment has attracted attention of many researchers.\nHowever, deployment of Grid environment requires skills, efforts, budget, and\ntime. Although various simulation toolkits or frameworks are available for\nsimulating Grid environment, either they support different possible studies in\nGrid computing area or takes lot of efforts in molding them to make them\nsuitable for scheduling of DAG application. In this paper, we describe design\nand implementation of GridSim based ready to use application scheduler for\nscheduling of DAG application in Grid environment. The proposed application\nscheduler supports supplying DAG application and configuration of Grid\nresources through GUI. We also describe implementation of Min-Min static\nscheduling algorithm for scheduling of DAG application to validate the proposed\nscheduler. Our proposed DAG application scheduling simulator is useful, easy,\nand time-saver.", 
    "link": "http://arxiv.org/pdf/1211.1447v1", 
    "arxiv-id": "1211.1447v1"
},{
    "category": "cs.DC", 
    "author": "Andrew Lumsdaine", 
    "title": "Extending Task Parallelism for Frequent Pattern Mining", 
    "publish": "2012-11-07T20:18:30Z", 
    "summary": "Algorithms for frequent pattern mining, a popular informatics application,\nhave unique requirements that are not met by any of the existing parallel\ntools. In particular, such applications operate on extremely large data sets\nand have irregular memory access patterns. For efficient parallelization of\nsuch applications, it is necessary to support dynamic load balancing along with\nscheduling mechanisms that allow users to exploit data locality. Given these\nrequirements, task parallelism is the most promising of the available parallel\nprogramming models. However, existing solutions for task parallelism schedule\ntasks implicitly and hence, custom scheduling policies that can exploit data\nlocality cannot be easily employed. In this paper we demonstrate and\ncharacterize the speedup obtained in a frequent pattern mining application\nusing a custom clustered scheduling policy in place of the popular Cilk-style\npolicy. We present PFunc, a novel task parallel library whose customizable task\nscheduling and task priorities facilitated the implementation of our clustered\nscheduling policy.", 
    "link": "http://arxiv.org/pdf/1211.1658v1", 
    "arxiv-id": "1211.1658v1"
},{
    "category": "cs.DC", 
    "author": "Mohamed Firdhous", 
    "title": "Implementation of Security in Distributed Systems - A Comparative Study", 
    "publish": "2012-11-09T02:52:32Z", 
    "summary": "This paper presents a comparative study of distributed systems and the\nsecurity issues associated with those systems. Four commonly used distributed\nsystems were considered for detailed analysis in terms of technologies\ninvolved, security issues faced by them and solution proposed to circumvent\nthose issues. Finally the security issues and the solutions were summarized and\ncompared with each other.", 
    "link": "http://arxiv.org/pdf/1211.2032v1", 
    "arxiv-id": "1211.2032v1"
},{
    "category": "cs.DC", 
    "author": "S. R. Sathe", 
    "title": "Comparison of OpenMP & OpenCL Parallel Processing Technologies", 
    "publish": "2012-11-09T04:27:32Z", 
    "summary": "This paper presents a comparison of OpenMP and OpenCL based on the parallel\nimplementation of algorithms from various fields of computer applications. The\nfocus of our study is on the performance of benchmark comparing OpenMP and\nOpenCL. We observed that OpenCL programming model is a good option for mapping\nthreads on different processing cores. Balancing all available cores and\nallocating sufficient amount of work among all computing units, can lead to\nimproved performance. In our simulation, we used Fedora operating system; a\nsystem with Intel Xeon Dual core processor having thread count 24 coupled with\nNVIDIA Quadro FX 3800 as graphical processing unit.", 
    "link": "http://arxiv.org/pdf/1211.2038v1", 
    "arxiv-id": "1211.2038v1"
},{
    "category": "cs.DC", 
    "author": "Jay J. Wylie", 
    "title": "Toward a Principled Framework for Benchmarking Consistency", 
    "publish": "2012-11-19T02:59:53Z", 
    "summary": "Large-scale key-value storage systems sacrifice consistency in the interest\nof dependability (i.e., partition tolerance and availability), as well as\nperformance (i.e., latency). Such systems provide eventual\nconsistency,which---to this point---has been difficult to quantify in real\nsystems. Given the many implementations and deployments of\neventually-consistent systems (e.g., NoSQL systems), attempts have been made to\nmeasure this consistency empirically, but they suffer from important drawbacks.\nFor example, state-of-the art consistency benchmarks exercise the system only\nin restricted ways and disrupt the workload, which limits their accuracy.\n  In this paper, we take the position that a consistency benchmark should paint\na comprehensive picture of the relationship between the storage system under\nconsideration, the workload, the pattern of failures, and the consistency\nobserved by clients. To illustrate our point, we first survey prior efforts to\nquantify eventual consistency. We then present a benchmarking technique that\novercomes the shortcomings of existing techniques to measure the consistency\nobserved by clients as they execute the workload under consideration. This\nmethod is versatile and minimally disruptive to the system under test. As a\nproof of concept, we demonstrate this tool on Cassandra.", 
    "link": "http://arxiv.org/pdf/1211.4290v2", 
    "arxiv-id": "1211.4290v2"
},{
    "category": "cs.DC", 
    "author": "Nicolas Gren\u00e8che", 
    "title": "Int\u00e9gration des intergiciels de grilles de PC dans le nuage SlapOS :   le cas de BOINC", 
    "publish": "2012-11-27T23:05:10Z", 
    "summary": "In this article we describe the problems and solutions related to the\nintegration of desktop grid middleware in a cloud, in this case the open source\nSlapOS cloud. We focus on the issues about recipes that describe the\nintegration and the problem of the confinement of execution. They constitute\ntwo aspects of service-oriented architecture and Cloud Computing. These two\nissues solved with SlapOS are not in relation to what is traditionally done in\nthe clouds because we do not rely on virtual machines and, there is no data\ncenter (as defined in cloud). Moreover, we show that from the initial\ndeployment model we take into account not only Web applications, B2B\napplications... but also applications from the field of grids; here desktop\ngrid middleware which is a case study.", 
    "link": "http://arxiv.org/pdf/1211.6473v1", 
    "arxiv-id": "1211.6473v1"
},{
    "category": "cs.DC", 
    "author": "Kamesh Munagala", 
    "title": "Complexity Measures for Map-Reduce, and Comparison to Parallel Computing", 
    "publish": "2012-11-28T06:03:24Z", 
    "summary": "The programming paradigm Map-Reduce and its main open-source implementation,\nHadoop, have had an enormous impact on large scale data processing. Our goal in\nthis expository writeup is two-fold: first, we want to present some complexity\nmeasures that allow us to talk about Map-Reduce algorithms formally, and\nsecond, we want to point out why this model is actually different from other\nmodels of parallel programming, most notably the PRAM (Parallel Random Access\nMemory) model. We are looking for complexity measures that are detailed enough\nto make fine-grained distinction between different algorithms, but which also\nabstract away many of the implementation details.", 
    "link": "http://arxiv.org/pdf/1211.6526v1", 
    "arxiv-id": "1211.6526v1"
},{
    "category": "cs.DC", 
    "author": "Kapil Phatnani", 
    "title": "VUPIC: Virtual Machine Usage Based Placement in IaaS Cloud", 
    "publish": "2012-12-01T08:51:48Z", 
    "summary": "Efficient resource allocation is one of the critical performance challenges\nin an Infrastructure as a Service (IaaS) cloud. Virtual machine (VM) placement\nand migration decision making methods are integral parts of these resource\nallocation mechanisms. We present a novel virtual machine placement algorithm\nwhich takes performance isolation amongst VMs and their continuous resource\nusage into account while taking placement decisions. Performance isolation is a\nform of resource contention between virtual machines interested in basic low\nlevel hardware resources (CPU, memory, storage, and networks bandwidth).\nResource contention amongst multiple co-hosted neighbouring VMs form the basis\nof the presented novel approach. Experiments are conducted to show the various\ncategories of applications and effect of performance isolation and resource\ncontention amongst them. A per-VM 3-dimensional Resource Utilization Vector\n(RUV) has been continuously calculated and used for placement decisions while\ntaking conflicting resource interests of VMs into account. Experiments using\nthe novel placement algorithm: VUPIC, show effective improvements in VM\nperformance as well as overall resource utilization of the cloud.", 
    "link": "http://arxiv.org/pdf/1212.0085v1", 
    "arxiv-id": "1212.0085v1"
},{
    "category": "cs.DC", 
    "author": "Surya Nepal", 
    "title": "An Ontology based System for Cloud Infrastructure Services Discovery", 
    "publish": "2012-12-01T20:39:22Z", 
    "summary": "The Cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice. As a result, Cloud service identification and discovery\nremains a hard problem due to different service descriptions, non standardised\nnaming conventions and heterogeneous types and features of Cloud services. In\nthis paper, we present an OWL based ontology, the Cloud Computing Ontology\n(CoCoOn) that defines functional and non functional concepts, attributes and\nrelations of infrastructure services. We also present a system...", 
    "link": "http://arxiv.org/pdf/1212.0156v1", 
    "arxiv-id": "1212.0156v1"
},{
    "category": "cs.DC", 
    "author": "Krzysztof Rzadca", 
    "title": "Network delay-aware load balancing in selfish and cooperative   distributed systems", 
    "publish": "2012-12-03T15:52:18Z", 
    "summary": "We consider a request processing system composed of organizations and their\nservers connected by the Internet.\n  The latency a user observes is a sum of communication delays and the time\nneeded to handle the request on a server. The handling time depends on the\nserver congestion, i.e. the total number of requests a server must handle. We\nanalyze the problem of balancing the load in a network of servers in order to\nminimize the total observed latency. We consider both cooperative and selfish\norganizations (each organization aiming to minimize the latency of the\nlocally-produced requests). The problem can be generalized to the task\nscheduling in a distributed cloud; or to content delivery in an\norganizationally-distributed CDNs.\n  In a cooperative network, we show that the problem is polynomially solvable.\nWe also present a distributed algorithm iteratively balancing the load. We show\nhow to estimate the distance between the current solution and the optimum based\non the amount of load exchanged by the algorithm. During the experimental\nevaluation, we show that the distributed algorithm is efficient, therefore it\ncan be used in networks with dynamically changing loads.\n  In a network of selfish organizations, we prove that the price of anarchy\n(the worst-case loss of performance due to selfishness) is low when the network\nis homogeneous and the servers are loaded (the request handling time is high\ncompared to the communication delay). After relaxing these assumptions, we\nassess the loss of performance caused by the selfishness experimentally,\nshowing that it remains low.\n  Our results indicate that a network of servers handling requests can be\nefficiently managed by a distributed algorithm. Additionally, even if the\nnetwork is organizationally distributed, with individual organizations\noptimizing performance of their requests, the network remains efficient.", 
    "link": "http://arxiv.org/pdf/1212.0421v1", 
    "arxiv-id": "1212.0421v1"
},{
    "category": "cs.DC", 
    "author": "Krzysztof Rzadca", 
    "title": "Exploring heterogeneity of unreliable machines for p2p backup", 
    "publish": "2012-12-03T16:09:02Z", 
    "summary": "P2P architecture is a viable option for enterprise backup. In contrast to\ndedicated backup servers, nowadays a standard solution, making backups directly\non organization's workstations should be cheaper (as existing hardware is\nused), more efficient (as there is no single bottleneck server) and more\nreliable (as the machines are geographically dispersed).\n  We present the architecture of a p2p backup system that uses pairwise\nreplication contracts between a data owner and a replicator. In contrast to\nstandard p2p storage systems using directly a DHT, the contracts allow our\nsystem to optimize replicas' placement depending on a specific optimization\nstrategy, and so to take advantage of the heterogeneity of the machines and the\nnetwork. Such optimization is particularly appealing in the context of backup:\nreplicas can be geographically dispersed, the load sent over the network can be\nminimized, or the optimization goal can be to minimize the backup/restore time.\nHowever, managing the contracts, keeping them consistent and adjusting them in\nresponse to dynamically changing environment is challenging.\n  We built a scientific prototype and ran the experiments on 150 workstations\nin the university's computer laboratories and, separately, on 50 PlanetLab\nnodes. We found out that the main factor affecting the quality of the system is\nthe availability of the machines. Yet, our main conclusion is that it is\npossible to build an efficient and reliable backup system on highly unreliable\nmachines (our computers had just 13% average availability).", 
    "link": "http://arxiv.org/pdf/1212.0427v2", 
    "arxiv-id": "1212.0427v2"
},{
    "category": "cs.DC", 
    "author": "M. R. Doomun", 
    "title": "Integrated Green Cloud Computing Architecture", 
    "publish": "2012-12-06T10:57:59Z", 
    "summary": "Arbitrary usage of cloud computing, either private or public, can lead to\nuneconomical energy consumption in data processing, storage and communication.\nHence, green cloud computing solutions aim not only to save energy but also\nreduce operational costs and carbon footprints on the environment. In this\npaper, an Integrated Green Cloud Architecture (IGCA) is proposed that comprises\nof a client-oriented Green Cloud Middleware to assist managers in better\noverseeing and configuring their overall access to cloud services in the\ngreenest or most energy-efficient way. Decision making, whether to use local\nmachine processing, private or public clouds, is smartly handled by the\nmiddleware using predefined system specifications such as service level\nagreement (SLA), Quality of service (QoS), equipment specifications and job\ndescription provided by IT department. Analytical model is used to show the\nfeasibility to achieve efficient energy consumption while choosing between\nlocal, private and public Cloud service provider (CSP).", 
    "link": "http://arxiv.org/pdf/1212.1284v1", 
    "arxiv-id": "1212.1284v1"
},{
    "category": "cs.DC", 
    "author": "Aaditya Prakash", 
    "title": "Measures of Fault Tolerance in Distributed Simulated Annealing", 
    "publish": "2012-12-13T20:00:40Z", 
    "summary": "In this paper, we examine the different measures of Fault Tolerance in a\nDistributed Simulated Annealing process. Optimization by Simulated Annealing on\na distributed system is prone to various sources of failure. We analyse\nsimulated annealing algorithm, its architecture in distributed platform and\npotential sources of failures. We examine the behaviour of tolerant distributed\nsystem for optimization task. We present possible methods to overcome the\nfailures and achieve fault tolerance for the distributed simulated annealing\nprocess. We also examine the implementation of Simulated Annealing in MapReduce\nsystem and possible ways to prevent failures in reaching the global optima.\nThis paper will be beneficial to those who are interested in implementing a\nlarge scale distributed simulated annealing optimization problem of industrial\nor academic interest. We recommend hybrid tolerance technique to optimize the\ntrade-off between efficiency and availability.", 
    "link": "http://arxiv.org/pdf/1212.3295v2", 
    "arxiv-id": "1212.3295v2"
},{
    "category": "cs.DC", 
    "author": "Joey Paquet", 
    "title": "An Interactive Graph-Based Automation Assistant: A Case Study to Manage   the GIPSY's Distributed Multi-tier Run-Time System", 
    "publish": "2012-12-17T20:09:55Z", 
    "summary": "The GIPSY system provides a framework for a distributed multi-tier\ndemand-driven evaluation of heterogeneous programs, in which certain tiers can\ngenerate demands, while others can respond to demands to work on them. They are\nconnected through a virtual network that can be flexibly reconfigured at\nrun-time. Although the demand generator components were originally designed\nspecifically for the eductive (demand-driven) evaluation of Lucid intensional\nprograms, the GIPSY's run-time's flexible framework design enables it to\nperform the execution of various kinds of programs that can be evaluated using\nthe demand-driven computational model. Management of the GISPY networks has\nbecome a tedious (although scripted) task that took manual command-line console\nto do, which does not scale for large experiments. Therefore a new component\nhas been designed and developed to allow users to represent, visualize, and\ninteractively create, configure and seamlessly manage such a network as a\ngraph. Consequently, this work presents a Graphical GMT Manager, an interactive\ngraph-based assistant component for the GIPSY network creation and\nconfiguration management. Besides allowing the management of the nodes and\ntiers (mapped to hosts where store, workers, and generators reside), it lets\nthe user to visually control the network parameters and the interconnection\nbetween computational nodes at run-time. In this paper we motivate and present\nthe key features of this newly implemented graph-based component. We give the\ngraph representation details, mapping of the graph nodes to tiers, tier groups,\nand specific commands. We provide the requirements and design specification of\nthe tool and its implementation. Then we detail and discuss some experimental\nresults.", 
    "link": "http://arxiv.org/pdf/1212.4123v4", 
    "arxiv-id": "1212.4123v4"
},{
    "category": "cs.DC", 
    "author": "Piero Spinnato", 
    "title": "Resource management on a VM based computer cluster for scientific   computing", 
    "publish": "2012-12-19T13:31:04Z", 
    "summary": "In the last ten years host virtualization has brought a revolution in the way\nalmost every activity related to information technology is thought of and\nperformed. The use of virtualization for HPC and HTC computing, while eagerly\ndesired, has probably been one of the last steps of this revolution, the\nperformance loss due to the hardware abstraction layer being the cause that\nslowed down a process that has been much faster in other fields. Nowadays the\nwidespread diffusion of virtualization and of new virtualization techniques\nseem to have helped breaking this last barrier and virtual host computing\ninfrastructures for HPC and HTC are found in many data centers. In this\ndocument the approach adopted at the INFN \"Laboratori Nazionali del Gran Sasso\"\nfor providing computational resources via a virtual host based computing\nfacility is described. Particular evidence is given to the storage layout, to\nthe middleware architecture and to resource allocation strategies, as these are\nissues for which a personalized solution was adopted. Other aspects may be\ncovered in the future within other documents.", 
    "link": "http://arxiv.org/pdf/1212.4658v1", 
    "arxiv-id": "1212.4658v1"
},{
    "category": "cs.DC", 
    "author": "Kiran U. Shanbag", 
    "title": "Map / Reduce Deisgn and Implementation of Apriori Alogirthm for handling   voluminous data-sets", 
    "publish": "2012-12-19T15:04:12Z", 
    "summary": "Apriori is one of the key algorithms to generate frequent itemsets. Analyzing\nfrequent itemset is a crucial step in analysing structured data and in finding\nassociation relationship between items. This stands as an elementary foundation\nto supervised learning, which encompasses classifier and feature extraction\nmethods. Applying this algorithm is crucial to understand the behaviour of\nstructured data. Most of the structured data in scientific domain are\nvoluminous. Processing such kind of data requires state of the art computing\nmachines. Setting up such an infrastructure is expensive. Hence a distributed\nenvironment such as a clustered setup is employed for tackling such scenarios.\nApache Hadoop distribution is one of the cluster frameworks in distributed\nenvironment that helps by distributing voluminous data across a number of nodes\nin the framework. This paper focuses on map/reduce design and implementation of\nApriori algorithm for structured data analysis.", 
    "link": "http://arxiv.org/pdf/1212.4692v1", 
    "arxiv-id": "1212.4692v1"
},{
    "category": "cs.DC", 
    "author": "Tian Niu", 
    "title": "Interoperability and Standardization of Intercloud Cloud Computing", 
    "publish": "2012-12-24T19:24:35Z", 
    "summary": "Cloud computing is getting mature, and the interoperability and\nstandardization of the clouds is still waiting to be solved. This paper\ndiscussed the interoperability among clouds about message transmission, data\ntransmission and virtual machine transfer. Starting from IEEE Pioneering Cloud\nComputing Initiative, this paper discussed about standardization of the cloud\ncomputing, especially intercloud cloud computing. This paper also discussed the\nstandardization from the market-oriented view.", 
    "link": "http://arxiv.org/pdf/1212.5956v1", 
    "arxiv-id": "1212.5956v1"
},{
    "category": "cs.DC", 
    "author": "Ran Wolff", 
    "title": "Local Thresholding on Distributed Hash Tables", 
    "publish": "2013-01-14T13:56:17Z", 
    "summary": "We present a binary routing tree protocol for distributed hash table\noverlays. Using this protocol each peer can independently route messages to its\nparent and two descendants on the fly without any maintenance, global context,\nand synchronization. The protocol is then extended to support tree change\nnotification with similar efficiency. The resulting tree is almost perfectly\ndense and balanced, and has O(1) stretch if the distributed hash table is\nsymmetric Chord. We use the tree routing protocol to overcome the main\nimpediment for implementation of local thresholding algorithms in peer-to-peer\nsystems -- their requirement for cycle free routing. Direct comparison of a\ngossip-based algorithm and a corresponding local thresholding algorithm on a\nmajority voting problem reveals that the latter obtains superior accuracy using\na fraction of the communication overhead.", 
    "link": "http://arxiv.org/pdf/1301.2976v1", 
    "arxiv-id": "1301.2976v1"
},{
    "category": "cs.DC", 
    "author": "Mark Lewko", 
    "title": "On the Complexity of Asynchronous Agreement Against Powerful Adversaries", 
    "publish": "2013-01-15T04:39:01Z", 
    "summary": "We introduce new techniques for proving lower bounds on the running time of\nrandomized algorithms for asynchronous agreement against powerful adversaries.\nIn particular, we define a \\emph{strongly adaptive adversary} that is\ncomputationally unbounded and has a limited ability to corrupt a dynamic subset\nof processors by erasing their memories. We demonstrate that the randomized\nagreement algorithms designed by Ben-Or and Bracha to tolerate crash or\nByzantine failures in the asynchronous setting extend to defeat a strongly\nadaptive adversary. These algorithms have essentially perfect correctness and\ntermination, but at the expense of exponential running time. In the case of the\nstrongly adaptive adversary, we show that this dismally slow running time is\n\\emph{inherent}: we prove that any algorithm with essentially perfect\ncorrectness and termination against the strongly adaptive adversary must have\nexponential running time. We additionally interpret this result as yielding an\nenhanced understanding of the tools needed to simultaneously achieving perfect\ncorrectness and termination as well as fast running time for randomized\nalgorithms tolerating crash or Byzantine failures.", 
    "link": "http://arxiv.org/pdf/1301.3223v4", 
    "arxiv-id": "1301.3223v4"
},{
    "category": "cs.DC", 
    "author": "Srinidhi Varadarajan", 
    "title": "Regional Consistency: Programmability and Performance for   Non-Cache-Coherent Systems", 
    "publish": "2013-01-18T20:40:42Z", 
    "summary": "Parallel programmers face the often irreconcilable goals of programmability\nand performance. HPC systems use distributed memory for scalability, thereby\nsacrificing the programmability advantages of shared memory programming models.\nFurthermore, the rapid adoption of heterogeneous architectures, often with\nnon-cache-coherent memory systems, has further increased the challenge of\nsupporting shared memory programming models. Our primary objective is to define\na memory consistency model that presents the familiar thread-based shared\nmemory programming model, but allows good application performance on\nnon-cache-coherent systems, including distributed memory clusters and\naccelerator-based systems. We propose regional consistency (RegC), a new\nconsistency model that achieves this objective. Results on up to 256 processors\nfor representative benchmarks demonstrate the potential of RegC in the context\nof our prototype distributed shared memory system.", 
    "link": "http://arxiv.org/pdf/1301.4490v1", 
    "arxiv-id": "1301.4490v1"
},{
    "category": "cs.DC", 
    "author": "Olivier Cessenat", 
    "title": "Sophie, an FDTD code on the way to multicore, getting rid of the memory   bandwidth bottleneck better using cache", 
    "publish": "2013-01-19T08:13:27Z", 
    "summary": "FDTD codes, such as Sophie developed at CEA/DAM, no longer take advantage of\nthe processor's increased computing power, especially recently with the raising\nmulticore technology. This is rooted in the fact that low order numerical\nschemes need an important memory bandwidth to bring and store the computed\nfields. The aim of this article is to present a programming method at the\nsoftware's architecture level that improves the memory access pattern in order\nto reuse data in cache instead of constantly accessing RAM memory. We will\nexhibit a more than two computing time improvement in practical applications.\nThe target audience of this article is made of computing scientists and of\nelectrical engineers that develop simulation codes with no specific knowledge\nin computer science or electronics.", 
    "link": "http://arxiv.org/pdf/1301.4539v1", 
    "arxiv-id": "1301.4539v1"
},{
    "category": "cs.DC", 
    "author": "Shinichi Honiden", 
    "title": "A Scalable Distributed Architecture for Network- and QoS-aware Service   Composition", 
    "publish": "2013-01-21T12:25:16Z", 
    "summary": "Service-Oriented Computing (SOC) enables the composition of loosely coupled\nservice agents provided with varying Quality of Service (QoS) levels,\neffectively forming a multiagent system (MAS). Selecting a (near-)optimal set\nof services for a composition in terms of QoS is crucial when many functionally\nequivalent services are available. As the number of distributed services,\nespecially in the cloud, is rising rapidly, the impact of the network on the\nQoS keeps increasing. Despite this and opposed to most MAS approaches, current\nservice approaches depend on a centralized architecture which cannot adapt to\nthe network. Thus, we propose a scalable distributed architecture composed of a\nflexible number of distributed control nodes. Our architecture requires no\nchanges to existing services and adapts from a centralized to a completely\ndistributed realization by adding control nodes as needed. Also, we propose an\nextended QoS aggregation algorithm that allows to accurately estimate network\nQoS. Finally, we evaluate the benefits and optimality of our architecture in a\ndistributed environment.", 
    "link": "http://arxiv.org/pdf/1301.4839v1", 
    "arxiv-id": "1301.4839v1"
},{
    "category": "cs.DC", 
    "author": "Matei Ripeanu", 
    "title": "The Case for Cross-Layer Optimizations in Storage: A Workflow-Optimized   Storage System", 
    "publish": "2013-01-26T00:53:12Z", 
    "summary": "This paper proposes using file system custom metadata as a bidirectional\ncommunication channel between applications and the storage system. This channel\ncan be used to pass hints that enable cross-layer optimizations, an option\nhindered today by the ossified file-system interface. We study this approach in\ncontext of storage system support for large-scale workflow execution systems:\nOur workflow optimized storage system (WOSS), exploits application hints to\nprovide per-file optimized operations, and exposes data location to enable\nlocation-aware scheduling.\n  This paper argues that an incremental adoption path for adopting cross-layer\noptimizations in storage systems exists, presents the system architecture for a\nworkflow-optimized storage system and its integration with a workflow runtime\nengine, and evaluates the proposed approach using synthetic as well as real\napplications workloads.", 
    "link": "http://arxiv.org/pdf/1301.6195v1", 
    "arxiv-id": "1301.6195v1"
},{
    "category": "cs.DC", 
    "author": "Shantenu Jha", 
    "title": "Pilot-Data: An Abstraction for Distributed Data", 
    "publish": "2013-01-26T10:06:13Z", 
    "summary": "Scientific problems that depend on processing large amounts of data require\novercoming challenges in multiple areas: managing large-scale data\ndistribution, controlling co-placement and scheduling of data with compute\nresources, and storing, transferring, and managing large volumes of data.\nAlthough there exist multiple approaches to addressing each of these\nchallenges, an integrative approach is missing; furthermore, extending existing\nfunctionality or enabling interoperable capabilities remains difficult at best.\nWe propose the concept of Pilot-Data to address the fundamental challenges of\nco-placement and scheduling of data and compute in heterogeneous and\ndistributed environments with interoperability and extensibility as first-order\nconcerns. Pilot-Data is an extension of the Pilot-Job abstraction for\nsupporting the management of data in conjunction with compute tasks. Pilot-Data\nseparates logical data units from physical storage, thereby providing the basis\nfor efficient compute/data placement and scheduling. In this paper, we discuss\nthe design and implementation of the Pilot-Data prototype, demonstrate its use\nby data-intensive applications on multiple production distributed\ncyberinfrastructure and illustrate the advantages arising from flexible\nexecution modes enabled by Pilot-Data. Our experiments utilize an\nimplementation of Pilot-Data in conjunction with a scalable Pilot-Job (BigJob)\nto establish the application performance that can be enabled by the use of\nPilot-Data. We demonstrate how the concept of Pilot-Data also provides the\nbasis upon which to build tools and support capabilities like affinity which in\nturn can be used for advanced data-compute co-placement and scheduling.", 
    "link": "http://arxiv.org/pdf/1301.6228v3", 
    "arxiv-id": "1301.6228v3"
},{
    "category": "cs.DC", 
    "author": "Srivatsan Ravi", 
    "title": "Safety of Deferred Update in Transactional Memory", 
    "publish": "2013-01-26T23:35:25Z", 
    "summary": "Transactional memory allows the user to declare sequences of instructions as\nspeculative \\emph{transactions} that can either \\emph{commit} or \\emph{abort}.\nIf a transaction commits, it appears to be executed sequentially, so that the\ncommitted transactions constitute a correct sequential execution. If a\ntransaction aborts, none of its instructions can affect other transactions.\n  The popular criterion of \\emph{opacity} requires that the views of aborted\ntransactions must also be consistent with the global sequential order\nconstituted by committed ones. This is believed to be important, since\ninconsistencies observed by an aborted transaction may cause a fatal\nirrecoverable error or waste of the system in an infinite loop. Intuitively, an\nopaque implementation must ensure that no intermediate view a transaction\nobtains before it commits or aborts can be affected by a transaction that has\nnot started committing yet, so called \\emph{deferred-update} semantics.\n  In this paper, we intend to grasp this intuition formally. We propose a\nvariant of opacity that explicitly requires the sequential order to respect the\ndeferred-update semantics. We show that our criterion is a safety property,\ni.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures\nthat a serialization of a history implies serializations of its prefixes.\nFinally, we show that our property is equivalent to opacity if we assume that\nno two transactions commit identical values on the same variable, and present a\ncounter-example for scenarios when the \"unique-write\" assumption does not hold.", 
    "link": "http://arxiv.org/pdf/1301.6297v3", 
    "arxiv-id": "1301.6297v3"
},{
    "category": "cs.DC", 
    "author": "Umit V. Catalyurek", 
    "title": "GPU accelerated maximum cardinality matching algorithms for bipartite   graphs", 
    "publish": "2013-03-06T16:38:37Z", 
    "summary": "We design, implement, and evaluate GPU-based algorithms for the maximum\ncardinality matching problem in bipartite graphs. Such algorithms have a\nvariety of applications in computer science, scientific computing,\nbioinformatics, and other areas. To the best of our knowledge, ours is the\nfirst study which focuses on GPU implementation of the maximum cardinality\nmatching algorithms. We compare the proposed algorithms with serial and\nmulticore implementations from the literature on a large set of real-life\nproblems where in majority of the cases one of our GPU-accelerated algorithms\nis demonstrated to be faster than both the sequential and multicore\nimplementations.", 
    "link": "http://arxiv.org/pdf/1303.1379v1", 
    "arxiv-id": "1303.1379v1"
},{
    "category": "cs.DC", 
    "author": "Bernadette Charron-Bost", 
    "title": "Orientation and Connectivity Based Criteria for Asymptotic Consensus", 
    "publish": "2013-03-08T16:22:46Z", 
    "summary": "In this article, we establish orientation and connectivity based criteria for\nthe agreement algorithm to achieve asymptotic consensus in the context of\ntime-varying topology and communication delays. These criteria unify and extend\nmany earlier convergence results on the agreement algorithm for deterministic\nand discrete-time multiagent systems.", 
    "link": "http://arxiv.org/pdf/1303.2043v1", 
    "arxiv-id": "1303.2043v1"
},{
    "category": "cs.DC", 
    "author": "Shubham Gupta", 
    "title": "CPU and/or GPU: Revisiting the GPU Vs. CPU Myth", 
    "publish": "2013-03-09T05:35:31Z", 
    "summary": "Parallel computing using accelerators has gained widespread research\nattention in the past few years. In particular, using GPUs for general purpose\ncomputing has brought forth several success stories with respect to time taken,\ncost, power, and other metrics. However, accelerator based computing has\nsignifi- cantly relegated the role of CPUs in computation. As CPUs evolve and\nalso offer matching computational resources, it is important to also include\nCPUs in the computation. We call this the hybrid computing model. Indeed, most\ncomputer systems of the present age offer a degree of heterogeneity and\ntherefore such a model is quite natural.\n  We reevaluate the claim of a recent paper by Lee et al.(ISCA 2010). We argue\nthat the right question arising out of Lee et al. (ISCA 2010) should be how to\nuse a CPU+GPU platform efficiently, instead of whether one should use a CPU or\na GPU exclusively. To this end, we experiment with a set of 13 diverse\nworkloads ranging from databases, image processing, sparse matrix kernels, and\ngraphs. We experiment with two different hybrid platforms: one consisting of a\n6-core Intel i7-980X CPU and an NVidia Tesla T10 GPU, and another consisting of\nan Intel E7400 dual core CPU with an NVidia GT520 GPU. On both these platforms,\nwe show that hybrid solutions offer good advantage over CPU or GPU alone\nsolutions. On both these platforms, we also show that our solutions are 90%\nresource efficient on average.\n  Our work therefore suggests that hybrid computing can offer tremendous\nadvantages at not only research-scale platforms but also the more realistic\nscale systems with significant performance gains and resource efficiency to the\nlarge scale user community.", 
    "link": "http://arxiv.org/pdf/1303.2171v1", 
    "arxiv-id": "1303.2171v1"
},{
    "category": "cs.DC", 
    "author": "Russell Power", 
    "title": "Making Systems More Robust with Flexible RPC Lookup", 
    "publish": "2013-03-11T19:05:37Z", 
    "summary": "Modern distributed systems use names everywhere. Lockservices such as Chubby\nand ZooKeeper provide an effective mechanism for mapping from application names\nto server instances, but proper usage of them requires a large amount of\nerror-prone boiler-plate code.\n  Application programmers often try to write wrappers to abstract away this\nlogic, but it turns out there is a more general and easier way of handling the\nissue. We show that by extending the existing name resolution capabilities of\nRPC libraries, we can remove the need for such annoying boiler-plate code while\nat the same time making our services more robust.", 
    "link": "http://arxiv.org/pdf/1303.2619v1", 
    "arxiv-id": "1303.2619v1"
},{
    "category": "cs.DC", 
    "author": "Niloufar Shafiei", 
    "title": "Non-blocking Patricia Tries with Replace Operations", 
    "publish": "2013-03-14T22:14:36Z", 
    "summary": "This paper presents a non-blocking Patricia trie implementation for an\nasynchronous shared-memory system using Compare&Swap. The trie implements a\nlinearizable set and supports three update operations: insert adds an element,\ndelete removes an element and replace replaces one element by another. The\nreplace operation is interesting because it changes two different locations of\ntree atomically. If all update operations modify different parts of the trie,\nthey run completely concurrently. The implementation also supports a wait-free\nfind operation, which only reads shared memory and never changes the data\nstructure. Empirically, we compare our algorithms to some existing set\nimplementations.", 
    "link": "http://arxiv.org/pdf/1303.3626v1", 
    "arxiv-id": "1303.3626v1"
},{
    "category": "cs.DC", 
    "author": "N. Sadagopan", 
    "title": "Parallel Search with Extended Fibonacci Primitive", 
    "publish": "2013-03-18T09:03:56Z", 
    "summary": "Search pattern experienced by the processor to search an element in secondary\nstorage devices follows a random sequence. Formally, it is a random walk and\nits modeling is crucial in studying performance metrics like memory access\ntime. In this paper, we first model the random walk using extended Fibonacci\nseries. Our simulation is done on a parallel computing model (PRAM) with EREW\nstrategy. Three search primitives are proposed under parallel computing model\nand each primitive is thoroughly tested on an array of size $10^7$ with the\nsize of random walk being $10^4$. Our findings reveal that search primitive\nwith pointer jumping is better than the other two primitives. Our key\ncontribution lies in modeling random walk as an extended Fibonacci series\ngenerator and simulating the same with various search primitives.", 
    "link": "http://arxiv.org/pdf/1303.4191v4", 
    "arxiv-id": "1303.4191v4"
},{
    "category": "cs.DC", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "title": "Perfectly load-balanced, optimal, stable, parallel merge", 
    "publish": "2013-03-18T16:40:58Z", 
    "summary": "We present a simple, work-optimal and synchronization-free solution to the\nproblem of stably merging in parallel two given, ordered arrays of m and n\nelements into an ordered array of m+n elements. The main contribution is a new,\nsimple, fast and direct algorithm that determines, for any prefix of the stably\nmerged output sequence, the exact prefixes of each of the two input sequences\nneeded to produce this output prefix. More precisely, for any given index\n(rank) in the resulting, but not yet constructed output array representing an\noutput prefix, the algorithm computes the indices (co-ranks) in each of the two\ninput arrays representing the required input prefixes without having to merge\nthe input arrays. The co-ranking algorithm takes O(log min(m,n)) time steps.\nThe algorithm is used to devise a perfectly load-balanced, stable, parallel\nmerge algorithm where each of p processing elements has exactly the same number\nof input elements to merge. Compared to other approaches to the parallel merge\nproblem, our algorithm is considerably simpler and can be faster up to a factor\nof two. Compared to previous algorithms for solving the co-ranking problem, the\nalgorithm given here is direct and maintains stability in the presence of\nrepeated elements at no extra space or time cost. When the number of processing\nelements p does not exceed (m+n)/log min(m,n), the parallel merge algorithm has\noptimal speedup. It is easy to implement on both shared and distributed memory\nparallel systems.", 
    "link": "http://arxiv.org/pdf/1303.4312v2", 
    "arxiv-id": "1303.4312v2"
},{
    "category": "cs.DC", 
    "author": "Bingsheng He", 
    "title": "Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and   Scheduling", 
    "publish": "2013-03-21T04:50:48Z", 
    "summary": "Graphics processors, or GPUs, have recently been widely used as accelerators\nin the shared environments such as clusters and clouds. In such shared\nenvironments, many kernels are submitted to GPUs from different users, and\nthroughput is an important metric for performance and total ownership cost.\nDespite the recently improved runtime support for concurrent GPU kernel\nexecutions, the GPU can be severely underutilized, resulting in suboptimal\nthroughput. In this paper, we propose Kernelet, a runtime system with dynamic\nslicing and scheduling techniques to improve the throughput of concurrent\nkernel executions on the GPU. With slicing, Kernelet divides a GPU kernel into\nmultiple sub-kernels (namely slices). Each slice has tunable occupancy to allow\nco-scheduling with other slices and to fully utilize the GPU resources. We\ndevelop a novel and effective Markov chain based performance model to guide the\nscheduling decision. Our experimental results demonstrate up to 31.1% and 23.4%\nperformance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively.", 
    "link": "http://arxiv.org/pdf/1303.5164v1", 
    "arxiv-id": "1303.5164v1"
},{
    "category": "cs.DC", 
    "author": "James Southern", 
    "title": "Achieving Efficient Strong Scaling with PETSc using Hybrid MPI/OpenMP   Optimisation", 
    "publish": "2013-03-21T14:56:02Z", 
    "summary": "The increasing number of processing elements and decreas- ing memory to core\nratio in modern high-performance platforms makes efficient strong scaling a key\nrequirement for numerical algorithms. In order to achieve efficient scalability\non massively parallel systems scientific software must evolve across the entire\nstack to exploit the multiple levels of parallelism exposed in modern\narchitectures. In this paper we demonstrate the use of hybrid MPI/OpenMP\nparallelisation to optimise parallel sparse matrix-vector multiplication in\nPETSc, a widely used scientific library for the scalable solution of partial\ndifferential equations. Using large matrices generated by Fluidity, an open\nsource CFD application code which uses PETSc as its linear solver engine, we\nevaluate the effect of explicit communication overlap using task-based\nparallelism and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. We demonstrate a significant speedup\nover the pure-MPI mode and efficient strong scaling of sparse matrix-vector\nmultiplication on Fujitsu PRIMEHPC FX10 and Cray XE6 systems.", 
    "link": "http://arxiv.org/pdf/1303.5275v1", 
    "arxiv-id": "1303.5275v1"
},{
    "category": "cs.DC", 
    "author": "Amal Khabou", 
    "title": "Multilevel communication optimal LU and QR factorizations for   hierarchical platforms", 
    "publish": "2013-03-23T11:40:50Z", 
    "summary": "This study focuses on the performance of two classical dense linear algebra\nalgorithms, the LU and the QR factorizations, on multilevel hierarchical\nplatforms. We first introduce a new model called Hierarchical Cluster Platform\n(HCP), encapsulating the characteristics of such platforms. The focus is set on\nreducing the communication requirements of studied algorithms at each level of\nthe hierarchy. Lower bounds on communications are therefore extended with\nrespect to the HCP model. We then introduce multilevel LU and QR algorithms\ntailored for those platforms, and provide a detailed performance analysis. We\nalso provide a set of numerical experiments and performance predictions\ndemonstrating the need for such algorithms on large platforms.", 
    "link": "http://arxiv.org/pdf/1303.5837v1", 
    "arxiv-id": "1303.5837v1"
},{
    "category": "cs.DC", 
    "author": "Vijay K. Garg", 
    "title": "Fault Tolerance in Distributed Systems using Fused State Machines", 
    "publish": "2013-03-23T23:03:34Z", 
    "summary": "Replication is a standard technique for fault tolerance in distributed\nsystems modeled as deterministic finite state machines (DFSMs or machines). To\ncorrect f crash or f/2 Byzantine faults among n different machines, replication\nrequires nf additional backup machines. We present a solution called fusion\nthat requires just f additional backup machines. First, we build a framework\nfor fault tolerance in DFSMs based on the notion of Hamming distances. We\nintroduce the concept of an (f,m)-fusion, which is a set of m backup machines\nthat can correct f crash faults or f/2 Byzantine faults among a given set of\nmachines. Second, we present an algorithm to generate an (f,f)-fusion for a\ngiven set of machines. We ensure that our backups are efficient in terms of the\nsize of their state and event sets. Our evaluation of fusion on the widely used\nMCNC'91 benchmarks for DFSMs show that the average state space savings in\nfusion (over replication) is 38% (range 0-99%). To demonstrate the practical\nuse of fusion, we describe its potential application to the MapReduce\nframework. Using a simple case study, we compare replication and fusion as\napplied to this framework. While a pure replication-based solution requires 1.8\nmillion map tasks, our fusion-based solution requires only 1.4 million map\ntasks with minimal overhead during normal operation or recovery. Hence, fusion\nresults in considerable savings in state space and other resources such as the\npower needed to run the backup tasks.", 
    "link": "http://arxiv.org/pdf/1303.5891v1", 
    "arxiv-id": "1303.5891v1"
},{
    "category": "cs.DC", 
    "author": "Ulf Leser", 
    "title": "Parallelization in Scientific Workflow Management Systems", 
    "publish": "2013-03-28T18:20:17Z", 
    "summary": "Over the last two decades, scientific workflow management systems (SWfMS)\nhave emerged as a means to facilitate the design, execution, and monitoring of\nreusable scientific data processing pipelines. At the same time, the amounts of\ndata generated in various areas of science outpaced enhancements in\ncomputational power and storage capabilities. This is especially true for the\nlife sciences, where new technologies increased the sequencing throughput from\nkilobytes to terabytes per day. This trend requires current SWfMS to adapt:\nNative support for parallel workflow execution must be provided to increase\nperformance; dynamically scalable \"pay-per-use\" compute infrastructures have to\nbe integrated to diminish hardware costs; adaptive scheduling of workflows in\ndistributed compute environments is required to optimize resource utilization.\nIn this survey we give an overview of parallelization techniques for SWfMS,\nboth in theory and in their realization in concrete systems. We find that\ncurrent systems leave considerable room for improvement and we propose key\nadvancements to the landscape of SWfMS.", 
    "link": "http://arxiv.org/pdf/1303.7195v1", 
    "arxiv-id": "1303.7195v1"
},{
    "category": "cs.DC", 
    "author": "Albert Y. Zomaya", 
    "title": "Data-Intensive Workload Consolidation on Hadoop Distributed File System", 
    "publish": "2013-03-28T23:15:36Z", 
    "summary": "Workload consolidation, sharing physical resources among multiple workloads,\nis a promising technique to save cost and energy in cluster computing systems.\nThis paper highlights a few challenges of workload consolidation for Hadoop as\none of the current state-of-the-art data-intensive cluster computing system.\nThrough a systematic step-by-step procedure, we investigate challenges for\nefficient server consolidation in Hadoop environments. To this end, we first\ninvestigate the inter-relationship between last level cache (LLC) contention\nand throughput degradation for consolidated workloads on a single physical\nserver employing Hadoop distributed file system (HDFS). We then investigate the\ngeneral case of consolidation on multiple physical servers so that their\nthroughput never falls below a desired/predefined utilization level. We use our\nempirical results to model consolidation as a classic two-dimensional bin\npacking problem and then design a computationally efficient greedy algorithm to\nachieve minimum throughput degradation on multiple servers. Results are very\npromising and show that our greedy approach is able to achieve near optimal\nsolution in all experimented cases.", 
    "link": "http://arxiv.org/pdf/1303.7270v1", 
    "arxiv-id": "1303.7270v1"
},{
    "category": "cs.DC", 
    "author": "M. K. Kaushik", 
    "title": "Queuing Methodology Based Power Efficient Routing Protocol for Reliable   Data Communications in Manets", 
    "publish": "2013-03-29T06:01:08Z", 
    "summary": "A mobile ad hoc network (MANET) is a wireless network that uses multi-hop\npeer-to- peer routing instead of static network infrastructure to provide\nnetwork connectivity. MANETs have applications in rapidly deployed and dynamic\nmilitary and civilian systems. The network topology in a MANET usually changes\nwith time. Therefore, there are new challenges for routing protocols in MANETs\nsince traditional routing protocols may not be suitable for MANETs. In recent\nyears, a variety of new routing protocols targeted specifically at this\nenvironment have been developed, but little performance information on each\nprotocol and no realistic performance comparison between them is available.\nThis paper presents the results of a detailed packet-level simulation comparing\nthree multi-hop wireless ad hoc network routing protocols that cover a range of\ndesign choices: DSR, NFPQR, and clustered NFPQR. By applying queuing\nmethodology to the introduced routing protocol the reliability and throughput\nof the network is increased.", 
    "link": "http://arxiv.org/pdf/1303.7300v1", 
    "arxiv-id": "1303.7300v1"
},{
    "category": "cs.DC", 
    "author": "Russell Power", 
    "title": "Using Memory-Protection to Simplify Zero-copy Operations", 
    "publish": "2013-03-29T20:03:47Z", 
    "summary": "High performance networks (e.g. Infiniband) rely on zero-copy operations for\nperformance. Zero-copy operations, as the name implies, avoid copying buffers\nfor sending and receiving data. Instead, hardware devices directly read and\nwrite to application specified areas of memory. Since these networks can send\nand receive at nearly the same speed as the memory bus inside machines,\nzero-copy operations are necessary to achieve peak performance for many\napplications.\n  Unfortunately, programming with zero-copy APIs is a *giant pain*. Users must\ncarefully avoid using buffers that may be accessed by a device. Typically this\neither results in spaghetti code (where every access to a buffer is checked\nbefore usage), or blocking operations (which pretty much defeat the whole point\nof zero-copy).\n  We show that by abusing memory protection hardware, we can offer the best of\nboth worlds: a simple zero-copy mechanism which allows for non-blocking send\nand receives while protecting against incorrect accesses.", 
    "link": "http://arxiv.org/pdf/1304.0012v1", 
    "arxiv-id": "1304.0012v1"
},{
    "category": "cs.DC", 
    "author": "Peter Hahn", 
    "title": "Improving Lower Bounds for the Quadratic Assignment Problem by applying   a Distributed Dual Ascent Algorithm", 
    "publish": "2013-04-01T00:14:45Z", 
    "summary": "The application of the Reformulation Linearization Technique (RLT) to the\nQuadratic Assignment Problem (QAP) leads to a tight linear relaxation with huge\ndimensions that is hard to solve. Previous works found in the literature show\nthat these relaxations combined with branch-and-bound algorithms belong to the\nstate-of-the-art of exact methods for the QAP. For the level 3 RLT (RLT3),\nusing this relaxation is prohibitive in conventional machines for instances\nwith more than 22 locations due to memory limitations. This paper presents a\ndistributed version of a dual ascent algorithm for the RLT3 QAP relaxation that\napproximately solves it for instances with up to 30 locations for the first\ntime. Although, basically, the distributed algorithm has been implemented on\ntop of its sequential conterpart, some changes, which improved not only the\nparallel performance but also the quality of solutions, were proposed here.\nWhen compared to other lower bounding methods found in the literature, our\nalgorithm generates the best known lower bounds for 26 out of the 28 tested\ninstances, reaching the optimal solution in 18 of them.", 
    "link": "http://arxiv.org/pdf/1304.0267v1", 
    "arxiv-id": "1304.0267v1"
},{
    "category": "cs.DC", 
    "author": "Stefano Ferretti", 
    "title": "Resilience of Dynamic Overlays through Local Interactions", 
    "publish": "2013-04-09T14:43:02Z", 
    "summary": "This paper presents a self-organizing protocol for dynamic (unstructured P2P)\noverlay networks, which allows to react to the variability of node arrivals and\ndepartures. Through local interactions, the protocol avoids that the departure\nof nodes causes a partitioning of the overlay. We show that it is sufficient to\nhave knowledge about 1st and 2nd neighbours, plus a simple interaction P2P\nprotocol, to make unstructured networks resilient to node faults. A simulation\nassessment over different kinds of overlay networks demonstrates the viability\nof the proposal.", 
    "link": "http://arxiv.org/pdf/1304.2617v1", 
    "arxiv-id": "1304.2617v1"
},{
    "category": "cs.DC", 
    "author": "Manish Parashar", 
    "title": "Cross-layer Application-aware Power/Energy Management for Extreme Scale   Science", 
    "publish": "2013-04-10T04:06:30Z", 
    "summary": "High Performance Computing (HPC) has evolved over the past decades into\nincreasingly complex and powerful systems. Current HPC systems consume several\nMWs of power, enough to power small towns, and are in fact soon approaching the\nlimits of the power available to them. Estimates are with the given current\ntechnology, achieving exascale will require hundreds of MW, which is not\nfeasible from multiple perspectives. Architecture and technology researchers\nare aggressively addressing this; however as past history is shown, innovation\nat these levels are not sufficient and have to be accompanied with innovations\nat higher levels (algorithms, programming, runtime, OS) to achieve the multiple\norders of magnitude reduction - i.e., a comprehensive cross-layer and\napplication-aware strategy is required. Furthermore, energy/power-efficiency\nhas to be addressed in combination with quality of solutions, performance and\nreliability and other objectives and appropriate tradeoffs are required.", 
    "link": "http://arxiv.org/pdf/1304.2840v1", 
    "arxiv-id": "1304.2840v1"
},{
    "category": "cs.DC", 
    "author": "Zheng Xuefeng", 
    "title": "Cloud Computing: a Prologue", 
    "publish": "2013-04-10T14:45:47Z", 
    "summary": "An emerging internet based super computing model is represented by cloud\ncomputing. Cloud computing is the convergence and evolution of several concepts\nfrom virtualization, distributed storage, grid, and automation management to\nenable a more flexible approach for deploying and scaling applications.\nHowever, cloud computing moves the application software and databases to the\nlarge data centers, where the management of the data and services may not be\nfully trustworthy. The concept of cloud computing on the basis of the various\ndefinitions available in the industry and the characteristics of cloud\ncomputing are being analyzed in this paper. The paper also describes the main\ncloud service providers and their products followed by primary cloud computing\noperating systems.", 
    "link": "http://arxiv.org/pdf/1304.2981v2", 
    "arxiv-id": "1304.2981v2"
},{
    "category": "cs.DC", 
    "author": "Zheng Xuefeng", 
    "title": "Cloud Computing Research Challenges", 
    "publish": "2013-04-11T06:10:12Z", 
    "summary": "In recent times cloud computing has appeared as a new model for hosting and\nconveying services over the Internet. This model is striking to business\nvendors as it eradicates the requirement for users to plan in advance, and it\npermits the organization to start from low level and then add more resources\nonly if there is an increase in the service demand. Even though cloud computing\npresents greater opportunities not only to information technology industry, but\nevery organization involved in utilizing the computing in one way or the other,\nit is still in infancy with many problems to be fixed. The paper discusses\nresearch challenges in cloud computing.", 
    "link": "http://arxiv.org/pdf/1304.3203v1", 
    "arxiv-id": "1304.3203v1"
},{
    "category": "cs.DC", 
    "author": "Vipin Tyagi", 
    "title": "Validated Real Time Middle Ware For Distributed Cyber Physical Systems   Using HMM", 
    "publish": "2013-04-11T19:09:42Z", 
    "summary": "Distributed Cyber Physical Systems designed for different scenario must be\ncapable enough to perform in an efficient manner in every situation. Earlier\napproaches, such as CORBA, has performed but with different time constraints.\nTherefore, there was the need to design reconfigurable, robust, validated and\nconsistent real time middle ware systems with end-to-end timing. In the\nDCPS-HMM we have proposed the processor efficiency and data validation which\nmay proof crucial in implementing various distributed systems such as credit\ncard systems or file transfer through network.", 
    "link": "http://arxiv.org/pdf/1304.3396v1", 
    "arxiv-id": "1304.3396v1"
},{
    "category": "cs.DC", 
    "author": "Harshad B. Prajapati", 
    "title": "Reallocation and Allocation of Virtual Machines in Cloud Computing", 
    "publish": "2013-04-15T05:02:16Z", 
    "summary": "Cloud computing has given the new face to the distributed field. Two main\nissues are discussed in this paper, (I) the process of finding the efficient\nvirtual machine by using the concept of load balancing algorithm. (II)\nReallocation of the Virtual Machines i.e. migration of the Virtual Machines\nwhen cloud provider is not available with the required Virtual Machines. We\nhave discussed about the different load balancing algorithms which are used for\ndeciding the efficient Virtual Machine for the allocation to the client on\ndemand. While in the second issue is concern we have discuss about different\nmodules available for the migration of Virtual Machines from one source machine\nto the other target machine. At last discussion about the different simulators\navailable for the cloud are carried out in this paper.", 
    "link": "http://arxiv.org/pdf/1304.3978v1", 
    "arxiv-id": "1304.3978v1"
},{
    "category": "cs.DC", 
    "author": "Harshad. B. Prajapati", 
    "title": "Scheduling of Dependent Tasks Application using Random Search Technique", 
    "publish": "2013-04-15T05:04:31Z", 
    "summary": "Since beginning of Grid computing, scheduling of dependent tasks application\nhas attracted attention of researchers due to NP-Complete nature of the\nproblem. In Grid environment, scheduling is deciding about assignment of tasks\nto available resources. Scheduling in Grid is challenging when the tasks have\ndependencies and resources are heterogeneous. The main objective in scheduling\nof dependent tasks is minimizing make-span. Due to NP-complete nature of\nscheduling problem, exact solutions cannot generate schedule efficiently.\nTherefore, researchers apply heuristic or random search techniques to get\noptimal or near to optimal solution of such problems. In this paper, we show\nhow Genetic Algorithm can be used to solve dependent task scheduling problem.\nWe describe how initial population can be generated using random assignment and\nheight based approaches. We also present design of crossover and mutation\noperators to enable scheduling of dependent tasks application without violating\ndependency constraints. For implementation of GA based scheduling, we explore\nand analyze SimGrid and GridSim simulation toolkits. From results, we found\nthat SimGrid is suitable, as it has support of SimDag API for DAG applications.\nWe found that GA based approach can generate schedule for dependent tasks\napplication in reasonable time while trying to minimize make-span.", 
    "link": "http://arxiv.org/pdf/1304.3980v1", 
    "arxiv-id": "1304.3980v1"
},{
    "category": "cs.DC", 
    "author": "Neeraj Mittal", 
    "title": "Distributed Abstraction Algorithm for Online Predicate Detection", 
    "publish": "2013-04-16T03:56:24Z", 
    "summary": "Analyzing a distributed computation is a hard problem in general due to the\ncombinatorial explosion in the size of the state-space with the number of\nprocesses in the system. By abstracting the computation, unnecessary\nexplorations can be avoided. Computation slicing is an approach for abstracting\ndis- tributed computations with respect to a given predicate. We focus on\nregular predicates, a family of predicates that covers a large number of\ncommonly used predicates for runtime verification. The existing algorithms for\ncomputation slicing are centralized in nature in which a single process is\nresponsible for computing the slice in either offline or online manner. In this\npaper, we present a distributed online algorithm for computing the slice of a\ndistributed computation with respect to a regular predicate. Our algorithm\ndistributes the work and storage requirements across the system, thus reducing\nthe space and computation complexities per process. In addition, for\nconjunctive predicates, our algorithm also reduces the message load per\nprocess.", 
    "link": "http://arxiv.org/pdf/1304.4326v3", 
    "arxiv-id": "1304.4326v3"
},{
    "category": "cs.DC", 
    "author": "Rachid Guerraoui", 
    "title": "Sp\u00e9culation et auto-stabilisation", 
    "publish": "2013-04-25T19:54:00Z", 
    "summary": "Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits a correct behaviour.\nSpeculation consists in guaranteeing that the system satisfies its requirements\nfor any execution but exhibits significantly better performances for a subset\nof executions that are more probable. A speculative protocol is in this sense\nsupposed to be both robust and efficient in practice. We introduce the notion\nof speculative stabilization which we illustrate through the mutual exclusion\nproblem. We then present a novel speculatively stabilizing mutual exclusion\nprotocol. Our protocol is self-stabilizing for any asynchronous execution. We\nprove that its stabilization time for synchronous executions is diam(g)/2 steps\n(where diam(g) denotes the diameter of the system). This complexity result is\nof independent interest. The celebrated mutual exclusion protocol of Dijkstra\nstabilizes in n steps (where n is the number of processes) in synchronous\nexecutions and the question whether the stabilization time could be strictly\nsmaller than the diameter has been open since then (almost 40 years). We show\nthat this is indeed possible for any underlying topology. We also provide a\nlower bound proof that shows that our new stabilization time of diam(g)/2 steps\nis optimal for synchronous executions, even if asynchronous stabilization is\nnot required.", 
    "link": "http://arxiv.org/pdf/1304.6994v1", 
    "arxiv-id": "1304.6994v1"
},{
    "category": "cs.DC", 
    "author": "M. Sergio Campobasso", 
    "title": "Optimised hybrid parallelisation of a CFD code on Many Core   architectures", 
    "publish": "2013-04-29T13:22:38Z", 
    "summary": "COSA is a novel CFD system based on the compressible Navier-Stokes model for\nunsteady aerodynamics and aeroelasticity of fixed structures, rotary wings and\nturbomachinery blades. It includes a steady, time domain, and harmonic balance\nflow solver.\n  COSA has primarily been parallelised using MPI, but there is also a hybrid\nparallelisation that adds OpenMP functionality to the MPI parallelisation to\nenable larger number of cores to be utilised for a given simulation as the MPI\nparallelisation is limited to the number of geometric partitions (or blocks) in\nthe simulation, or to exploit multi-threaded hardware where appropriate. This\npaper outlines the work undertaken to optimise these two parallelisation\nstrategies, improving the efficiency of both and therefore reducing the\ncomputational time required to compute simulations. We also analyse the power\nconsumption of the code on a range of leading HPC systems to further understand\nthe performance of the code.", 
    "link": "http://arxiv.org/pdf/1304.7654v1", 
    "arxiv-id": "1304.7654v1"
},{
    "category": "cs.DC", 
    "author": "Eli Upfal", 
    "title": "Storage and Search in Dynamic Peer-to-Peer Networks", 
    "publish": "2013-05-06T09:04:39Z", 
    "summary": "We study robust and efficient distributed algorithms for searching, storing,\nand maintaining data in dynamic Peer-to-Peer (P2P) networks. P2P networks are\nhighly dynamic networks that experience heavy node churn (i.e., nodes join and\nleave the network continuously over time). Our goal is to guarantee, despite\nhigh node churn rate, that a large number of nodes in the network can store,\nretrieve, and maintain a large number of data items. Our main contributions are\nfast randomized distributed algorithms that guarantee the above with high\nprobability (whp) even under high adversarial churn:\n  1. A randomized distributed search algorithm that (whp) guarantees that\nsearches from as many as $n - o(n)$ nodes ($n$ is the stable network size)\nsucceed in ${O}(\\log n)$-rounds despite ${O}(n/\\log^{1+\\delta} n)$ churn, for\nany small constant $\\delta > 0$, per round. We assume that the churn is\ncontrolled by an oblivious adversary (that has complete knowledge and control\nof what nodes join and leave and at what time, but is oblivious to the random\nchoices made by the algorithm).\n  2. A storage and maintenance algorithm that guarantees (whp) data items can\nbe efficiently stored (with only $\\Theta(\\log{n})$ copies of each data item)\nand maintained in a dynamic P2P network with churn rate up to\n${O}(n/\\log^{1+\\delta} n)$ per round. Our search algorithm together with our\nstorage and maintenance algorithm guarantees that as many as $n - o(n)$ nodes\ncan efficiently store, maintain, and search even under ${O}(n/\\log^{1+\\delta}\nn)$ churn per round. Our algorithms require only polylogarithmic in $n$ bits to\nbe processed and sent (per round) by each node.\n  To the best of our knowledge, our algorithms are the first-known,\nfully-distributed storage and search algorithms that provably work under highly\ndynamic settings (i.e., high churn rates per step).", 
    "link": "http://arxiv.org/pdf/1305.1121v1", 
    "arxiv-id": "1305.1121v1"
},{
    "category": "cs.DC", 
    "author": "L. Matyska", 
    "title": "Optimizing CUDA Code By Kernel Fusion---Application on BLAS", 
    "publish": "2013-05-06T13:32:22Z", 
    "summary": "Modern GPUs are able to perform significantly more arithmetic operations than\ntransfers of a single word to or from global memory. Hence, many GPU kernels\nare limited by memory bandwidth and cannot exploit the arithmetic power of\nGPUs. However, the memory locality can be often improved by kernel fusion when\na sequence of kernels is executed and some kernels in this sequence share data.\n  In this paper, we show how kernels performing map, reduce or their nested\ncombinations can be fused automatically by our source-to-source compiler. To\ndemonstrate the usability of the compiler, we have implemented several BLAS-1\nand BLAS-2 routines and show how the performance of their sequences can be\nimproved by fusions.\n  Compared to similar sequences using CUBLAS, our compiler is able to generate\ncode that is up to 2.61x faster for the examples tested.", 
    "link": "http://arxiv.org/pdf/1305.1183v2", 
    "arxiv-id": "1305.1183v2"
},{
    "category": "cs.DC", 
    "author": "Adam Barker", 
    "title": "An Architecture for Decentralised Orchestration of Web Service Workflows", 
    "publish": "2013-05-08T15:07:08Z", 
    "summary": "Service-oriented workflows are typically executed using a centralised\norchestration approach that presents significant scalability challenges. These\nchallenges include the consumption of network bandwidth, degradation of\nperformance, and single-points of failure. We provide a decentralised\norchestration architecture that attempts to address these challenges. Our\narchitecture adopts a design model that permits the computation to be moved\n\"closer\" to services in a workflow. This is achieved by partitioning workflows\nspecified using our simple dataflow language into smaller fragments, which may\nbe sent to remote locations for execution.", 
    "link": "http://arxiv.org/pdf/1305.1842v1", 
    "arxiv-id": "1305.1842v1"
},{
    "category": "cs.DC", 
    "author": "Bholanathsingh Surajbali", 
    "title": "Experiences of Using a Hybrid Cloud to Construct an Environmental   Virtual Observatory", 
    "publish": "2013-05-10T12:08:43Z", 
    "summary": "Environmental science is often fragmented: data is collected using mismatched\nformats and conventions, and models are misaligned and run in isolation. Cloud\ncomputing offers a lot of potential in the way of resolving such issues by\nsupporting data from different sources and at various scales, by facilitating\nthe integration of models to create more sophisticated software services, and\nby providing a sustainable source of suitable computational and storage\nresources. In this paper, we highlight some of our experiences in building the\nEnvironmental Virtual Observatory pilot (EVOp), a tailored cloud-based\ninfrastructure and associated web-based tools designed to enable users from\ndifferent backgrounds to access data concerning different environmental issues.\nWe review our architecture design, the current deployment and prototypes. We\nalso reflect on lessons learned. We believe that such experiences are of\nbenefit to other scientific communities looking to assemble virtual\nobservatories or similar virtual research environments.", 
    "link": "http://arxiv.org/pdf/1305.2319v1", 
    "arxiv-id": "1305.2319v1"
},{
    "category": "cs.DC", 
    "author": "Dan C. Marinescu", 
    "title": "Clustering Algorithms for Scale-free Networks and Applications to Cloud   Resource Management", 
    "publish": "2013-05-14T06:19:43Z", 
    "summary": "In this paper we introduce algorithms for the construction of scale-free\nnetworks and for clustering around the nerve centers, nodes with a high\nconnectivity in a scale-free networks. We argue that such overlay networks\ncould support self-organization in a complex system like a cloud computing\ninfrastructure and allow the implementation of optimal resource management\npolicies.", 
    "link": "http://arxiv.org/pdf/1305.3031v1", 
    "arxiv-id": "1305.3031v1"
},{
    "category": "cs.DC", 
    "author": "Arry Yanuar", 
    "title": "Performance Analysis of Embarassingly Parallel Application on Cluster   Computer Environment: A Case Study of Virtual Screening with Autodock Vina   1.1 on Hastinapura Cluster", 
    "publish": "2013-05-14T11:57:20Z", 
    "summary": "IT based scientific research requires high computational resources. The\nlimitation on funding and infrastructure led the high performance computing era\nfrom supercomputer to cluster and grid computing technology. Parallel\napplication running well on cluster computer as well as supercomputer, one of\nthe type is embarrassingly parallel application. Many scientist loves EP\nbecause it doesn't need any sophisticated technique but gives amazing\nperformance. This paper discuss the bioinformatics research that used\nembarrassingly application and show its performance on cluster computer.", 
    "link": "http://arxiv.org/pdf/1305.3123v1", 
    "arxiv-id": "1305.3123v1"
},{
    "category": "cs.DC", 
    "author": "Sylvie Dela\u00ebt", 
    "title": "Self-Stabilizing Paxos", 
    "publish": "2013-05-18T13:20:11Z", 
    "summary": "We present the first self-stabilizing consensus and replicated state machine\nfor asynchronous message passing systems. The scheme does not require that all\nparticipants make a certain number of steps prior to reaching a practically\ninfinite execution where the replicated state machine exhibits the desired\nbehavior. In other words, the system reaches a configuration from which it\noperates according to the specified requirements of the replicated\nstate-machine, for a long enough execution regarding all practical\nconsiderations.", 
    "link": "http://arxiv.org/pdf/1305.4263v1", 
    "arxiv-id": "1305.4263v1"
},{
    "category": "cs.DC", 
    "author": "Punith Kumar G", 
    "title": "Performance Analysis of Parallel Pollard's Rho Algorithm", 
    "publish": "2013-05-19T14:44:57Z", 
    "summary": "Integer factorization is one of the vital algorithms discussed as a part of\nanalysis of any black-box cipher suites where the cipher algorithm is based on\nnumber theory. The origin of the problem is from Discrete Logarithmic Problem\nwhich appears under the analysis of the crypto-graphic algorithms as seen by a\ncrypt-analyst. The integer factorization algorithm poses a potential in\ncomputational science too, obtaining the factors of a very large number is\nchallenging with a limited computing infrastructure. This paper analyses the\nPollards Rho heuristic with a varying input size to evaluate the performance\nunder a multi-core environment and also to estimate the threshold for each\ncomputing infrastructure.", 
    "link": "http://arxiv.org/pdf/1305.4365v1", 
    "arxiv-id": "1305.4365v1"
},{
    "category": "cs.DC", 
    "author": "Rapha\u00ebl Jolly", 
    "title": "Parallelizing Stream with Future", 
    "publish": "2013-05-19T15:00:14Z", 
    "summary": "Stream is re-interpreted in terms of a Lazy monad. Future is substituted for\nLazy in the obtained construct, resulting in possible parallelization of any\nalgorithm expressible as a Stream computation. The principle is tested against\ntwo example algorithms. Performance is evaluated, and a way to improve it\nbriefly discussed.", 
    "link": "http://arxiv.org/pdf/1305.4367v1", 
    "arxiv-id": "1305.4367v1"
},{
    "category": "cs.DC", 
    "author": "Lukasz Swierczewski", 
    "title": "3DES ECB Optimized for Massively Parallel CUDA GPU Architecture", 
    "publish": "2013-05-19T16:02:42Z", 
    "summary": "Modern computers have graphics cards with much higher theoretical efficiency\nthan conventional CPU. The paper presents application possibilities GPU CUDA\nacceleration for encryption of data using the new architecture tailored to the\n3DES algorithm, characterized by increased security compared to the normal DES.\nThe algorithm used in ECB mode (Electronic Codebook), in which 64-bit data\nblocks are encrypted independently by stream processors (CUDA cores).", 
    "link": "http://arxiv.org/pdf/1305.4376v1", 
    "arxiv-id": "1305.4376v1"
},{
    "category": "cs.DC", 
    "author": "Marko Vukolic", 
    "title": "Asynchronous BFT Storage with 2t+1 Data Replicas", 
    "publish": "2013-05-21T16:06:23Z", 
    "summary": "The cost of Byzantine Fault Tolerant (BFT) storage is the main concern\npreventing its adoption in practice. This cost stems from the need to maintain\nat least 3t+1 replicas in different storage servers in the asynchronous model,\nso that t Byzantine replica faults can be tolerated. In this paper, we present\nMDStore, the first fully asynchronous read/write BFT storage protocol that\nreduces the number of data replicas to as few as 2t+1, maintaining 3t+1\nreplicas of metadata at (possibly) different servers. At the heart of MDStore\nstore is its metadata service that is built upon a new abstraction we call\ntimestamped storage. Timestamped storage both allows for conditional writes\n(facilitating the implementation of a metadata service) and has consensus\nnumber one (making it implementable wait-free in an asynchronous system despite\nfaults). In addition to its low data replication factor, MDStore offers very\nstrong guarantees implementing multi-writer multi-reader atomic wait-free\nsemantics and tolerating any number of Byzantine readers and crash-faulty\nwriters. We further show that MDStore data replication overhead is optimal;\nnamely, we prove a lower bound of 2t+1 on the number of data replicas that\napplies even to crash-tolerant storage with a fault-free metadata service\noracle. Finally, we prove that separating data from metadata for reducing the\ncost of BFT storage is not possible without cryptographic assumptions. However,\nour MDStore protocol uses only lightweight cryptographic hash functions.", 
    "link": "http://arxiv.org/pdf/1305.4868v3", 
    "arxiv-id": "1305.4868v3"
},{
    "category": "cs.DC", 
    "author": "Ben Liang", 
    "title": "To Reserve or Not to Reserve: Optimal Online Multi-Instance Acquisition   in IaaS Clouds", 
    "publish": "2013-05-24T02:58:50Z", 
    "summary": "Infrastructure-as-a-Service (IaaS) clouds offer diverse instance purchasing\noptions. A user can either run instances on demand and pay only for what it\nuses, or it can prepay to reserve instances for a long period, during which a\nusage discount is entitled. An important problem facing a user is how these two\ninstance options can be dynamically combined to serve time-varying demands at\nminimum cost. Existing strategies in the literature, however, require either\nexact knowledge or the distribution of demands in the long-term future, which\nsignificantly limits their use in practice. Unlike existing works, we propose\ntwo practical online algorithms, one deterministic and another randomized, that\ndynamically combine the two instance options online without any knowledge of\nthe future. We show that the proposed deterministic (resp., randomized)\nalgorithm incurs no more than 2-alpha (resp., e/(e-1+alpha)) times the minimum\ncost obtained by an optimal offline algorithm that knows the exact future a\npriori, where alpha is the entitled discount after reservation. Our online\nalgorithms achieve the best possible competitive ratios in both the\ndeterministic and randomized cases, and can be easily extended to cases when\nshort-term predictions are reliable. Simulations driven by a large volume of\nreal-world traces show that significant cost savings can be achieved with\nprevalent IaaS prices.", 
    "link": "http://arxiv.org/pdf/1305.5608v1", 
    "arxiv-id": "1305.5608v1"
},{
    "category": "cs.DC", 
    "author": "Ilya Mirsky", 
    "title": "Lightweight Contention Management for Efficient Compare-and-Swap   Operations", 
    "publish": "2013-05-24T17:05:51Z", 
    "summary": "Many concurrent data-structure implementations use the well-known\ncompare-and-swap (CAS) operation, supported in hardware by most modern\nmultiprocessor architectures for inter-thread synchronization. A key weakness\nof the CAS operation is the degradation in its performance in the presence of\nmemory contention.\n  In this work we study the following question: can software-based contention\nmanagement improve the efficiency of hardware-provided CAS operations? Our\nperformance evaluation establishes that lightweight contention management\nsupport can greatly improve performance under medium and high contention levels\nwhile typically incurring only small overhead when contention is low.", 
    "link": "http://arxiv.org/pdf/1305.5800v1", 
    "arxiv-id": "1305.5800v1"
},{
    "category": "cs.DC", 
    "author": "Dr S C Pradhan", 
    "title": "Building Internal Cloud at NIC : A Preview", 
    "publish": "2013-05-27T06:35:01Z", 
    "summary": "The most of computing environments in the IT support organization like NIC\nare designed to run in centralized datacentre. The centralized infrastructure\nof various development projects are used to deploy their services on it and\nconnecting remotely to that datacentre from all the stations of organization.\nCurrently these servers are mostly underutilized due to the static and\nconventional approaches used for accessing and utilizing of these resources.\nThe cloud patterns is much needful for optimizing resource utilization and\nreducing the investments on unnecessary costs. So, we build up and prototyped a\nprivate cloud system called nIC(NIC Internal Cloud) to leverage the benefits of\ncloud environment. For this system we adopted the combination of various\ntechniques from open source software community. The user-base of nIC consists\ndevelopers, web and database admins, service providers and desktop users from\nvarious projects in NIC. We can optimize the resource usage by customizing the\nuser based template services on these virtualized infrastructure. It will also\nincreases the flexibility of the managing and maintenance of the operations\nlike archiving, disaster recovery and scaling of resources. The open-source\napproach is further decreases the enterprise costs. In this paper, we describe\nthe design and analysis of implementing issues on internal cloud environments\nin NIC and similar organizations.", 
    "link": "http://arxiv.org/pdf/1305.6123v1", 
    "arxiv-id": "1305.6123v1"
},{
    "category": "cs.DC", 
    "author": "Sergiu Stelian Iliescu", 
    "title": "Dynamic Management Techniques for Increasing Energy Efficiency within a   Data Center", 
    "publish": "2013-05-27T13:06:36Z", 
    "summary": "In ours days data centers provide the global community an indispensable\nservice: nearly unlimited access to almost any kind of information we can\nimagine by supporting most Internet services such as: Web hosting and\nE-commerce services. Because of their capacity and their work, data centers\nhave various impacts on the environment, but those related with the electricity\nuse are by far the most important. In this paper, we present several power and\nenergy management techniques for data centers and we will focus our attention\non techniques that are explicitly tailored to servers and their workloads.", 
    "link": "http://arxiv.org/pdf/1305.6203v1", 
    "arxiv-id": "1305.6203v1"
},{
    "category": "cs.DC", 
    "author": "Philippas Tsigas", 
    "title": "Configurable Strategies for Work-stealing", 
    "publish": "2013-05-28T12:59:25Z", 
    "summary": "Work-stealing systems are typically oblivious to the nature of the tasks they\nare scheduling. For instance, they do not know or take into account how long a\ntask will take to execute or how many subtasks it will spawn. Moreover, the\nactual task execution order is typically determined by the underlying task\nstorage data structure, and cannot be changed. There are thus possibilities for\noptimizing task parallel executions by providing information on specific tasks\nand their preferred execution order to the scheduling system.\n  We introduce scheduling strategies to enable applications to dynamically\nprovide hints to the task-scheduling system on the nature of specific tasks.\nScheduling strategies can be used to independently control both local task\nexecution order as well as steal order. In contrast to conventional scheduling\npolicies that are normally global in scope, strategies allow the scheduler to\napply optimizations on individual tasks. This flexibility greatly improves\ncomposability as it allows the scheduler to apply different, specific\nscheduling choices for different parts of applications simultaneously. We\npresent a number of benchmarks that highlight diverse, beneficial effects that\ncan be achieved with scheduling strategies. Some benchmarks (branch-and-bound,\nsingle-source shortest path) show that prioritization of tasks can reduce the\ntotal amount of work compared to standard work-stealing execution order. For\nother benchmarks (triangle strip generation) qualitatively better results can\nbe achieved in shorter time. Other optimizations, such as dynamic merging of\ntasks or stealing of half the work, instead of half the tasks, are also shown\nto improve performance. Composability is demonstrated by examples that combine\ndifferent strategies, both within the same kernel (prefix sum) as well as when\nscheduling multiple kernels (prefix sum and unbalanced tree search).", 
    "link": "http://arxiv.org/pdf/1305.6474v1", 
    "arxiv-id": "1305.6474v1"
},{
    "category": "cs.DC", 
    "author": "Sathya Peri", 
    "title": "A TimeStamp based Multi-version STM Protocol that satisfies Opacity and   Multi-Version Permissiveness", 
    "publish": "2013-05-28T20:41:06Z", 
    "summary": "Software Transactional Memory Systems (STM) are a promising alternative to\nlock based systems for concurrency control in shared memory systems. In\nmultiversion STM systems, each write on a transaction object produces a new\nversion of that object. The advantage obtained by storing multiple versions is\nthat one can ensure that read operations do not fail. Opacity is a commonly\nused correctness criterion for STM systems. Multi-Version permissive STM system\nnever aborts a read-only transaction. Although many multi-version STM systems\nhave been proposed, to the best of our knowledge none of them have been\nformally proved to satisfy opacity. In this paper we present a time-stamp based\nmultiversion STM system that satisfies opacity and mv-permissiveness. We\nformally prove the correctness of the proposed STM system. We also present\ngarbage collection procedure which deletes unwanted versions of the transaction\nobjects and formally prove it correctness.", 
    "link": "http://arxiv.org/pdf/1305.6624v1", 
    "arxiv-id": "1305.6624v1"
},{
    "category": "cs.DC", 
    "author": "Adam Barker", 
    "title": "Monitoring Large-Scale Cloud Systems with Layered Gossip Protocols", 
    "publish": "2013-05-31T14:11:15Z", 
    "summary": "Monitoring is an essential aspect of maintaining and developing computer\nsystems that increases in difficulty proportional to the size of the system.\nThe need for robust monitoring tools has become more evident with the advent of\ncloud computing. Infrastructure as a Service (IaaS) clouds allow end users to\ndeploy vast numbers of virtual machines as part of dynamic and transient\narchitectures. Current monitoring solutions, including many of those in the\nopen-source domain rely on outdated concepts including manual deployment and\nconfiguration, centralised data collection and adapt poorly to membership\nchurn.\n  In this paper we propose the development of a cloud monitoring suite to\nprovide scalable and robust lookup, data collection and analysis services for\nlarge-scale cloud systems. In lieu of centrally managed monitoring we propose a\nmulti-tier architecture using a layered gossip protocol to aggregate monitoring\ninformation and facilitate lookup, information collection and the\nidentification of redundant capacity. This allows for a resource aware data\ncollection and storage architecture that operates over the system being\nmonitored. This in turn enables monitoring to be done in-situ without the need\nfor significant additional infrastructure to facilitate monitoring services. We\nevaluate this approach against alternative monitoring paradigms and demonstrate\nhow our solution is well adapted to usage in a cloud-computing context.", 
    "link": "http://arxiv.org/pdf/1305.7403v1", 
    "arxiv-id": "1305.7403v1"
},{
    "category": "cs.DC", 
    "author": "Stefan Schmid", 
    "title": "A Distributed SDN Control Plane for Consistent Policy Updates", 
    "publish": "2013-05-31T14:45:31Z", 
    "summary": "Software-defined networking (SDN) is a novel paradigm that out-sources the\ncontrol of packet-forwarding switches to a set of software controllers. The\nmost fundamental task of these controllers is the correct implementation of the\n\\emph{network policy}, i.e., the intended network behavior. In essence, such a\npolicy specifies the rules by which packets must be forwarded across the\nnetwork.\n  This paper studies a distributed SDN control plane that enables\n\\emph{concurrent} and \\emph{robust} policy implementation. We introduce a\nformal model describing the interaction between the data plane and a\ndistributed control plane (consisting of a collection of fault-prone\ncontrollers). Then we formulate the problem of \\emph{consistent} composition of\nconcurrent network policy updates (short: the \\emph{CPC Problem}). To\nanticipate scenarios in which some conflicting policy updates must be rejected,\nwe enable the composition via a natural \\emph{transactional} interface with\nall-or-nothing semantics.\n  We show that the ability of an $f$-resilient distributed control plane to\nprocess concurrent policy updates depends on the tag complexity, i. e., the\nnumber of policy labels (a.k.a. \\emph{tags}) available to the controllers, and\ndescribe a CPC protocol with optimal tag complexity $f+2$.", 
    "link": "http://arxiv.org/pdf/1305.7429v3", 
    "arxiv-id": "1305.7429v3"
},{
    "category": "cs.DC", 
    "author": "Jalal Kawash", 
    "title": "Partition Consistency: A Case Study in Modeling Systems with Weak Memory   Consistency and Proving Correctness of their Implementations", 
    "publish": "2013-06-01T04:35:02Z", 
    "summary": "Multiprocess systems, including grid systems, multiprocessors and multicore\ncomputers, incorporate a variety of specialized hardware and software\nmechanisms, which speed computation, but result in complex memory behavior. As\na consequence, the possible outcomes of a concurrent program can be unexpected.\nA memory consistency model is a description of the behaviour of such a system.\nAbstract memory consistency models aim to capture the concrete implementations\nand architectures. Therefore, formal specification of the implementation or\narchitecture is necessary, and proofs of correspondence between the abstract\nand the concrete models are required.\n  This paper provides a case study of this process. We specify a new model,\npartition consistency, that generalizes many existing consistency models. A\nconcrete message-passing network model is also specified. Implementations of\npartition consistency on this network model are then presented and proved\ncorrect. A middle level of abstraction is utilized to facilitate the proofs.\nAll three levels of abstraction are specified using the same framework. The\npaper aims to illustrate a general methodology and techniques for specifying\nmemory consistency models and proving the correctness of their implementations.", 
    "link": "http://arxiv.org/pdf/1306.0077v1", 
    "arxiv-id": "1306.0077v1"
},{
    "category": "cs.DC", 
    "author": "Wojciech Indyk", 
    "title": "Parallel Processing of Large Graphs", 
    "publish": "2013-06-03T08:44:32Z", 
    "summary": "More and more large data collections are gathered worldwide in various IT\nsystems. Many of them possess the networked nature and need to be processed and\nanalysed as graph structures. Due to their size they require very often usage\nof parallel paradigm for efficient computation. Three parallel techniques have\nbeen compared in the paper: MapReduce, its map-side join extension and Bulk\nSynchronous Parallel (BSP). They are implemented for two different graph\nproblems: calculation of single source shortest paths (SSSP) and collective\nclassification of graph nodes by means of relational influence propagation\n(RIP). The methods and algorithms are applied to several network datasets\ndiffering in size and structural profile, originating from three domains:\ntelecommunication, multimedia and microblog. The results revealed that\niterative graph processing with the BSP implementation always and\nsignificantly, even up to 10 times outperforms MapReduce, especially for\nalgorithms with many iterations and sparse communication. Also MapReduce\nextension based on map-side join usually noticeably presents better efficiency,\nalthough not as much as BSP. Nevertheless, MapReduce still remains the good\nalternative for enormous networks, whose data structures do not fit in local\nmemories.", 
    "link": "http://arxiv.org/pdf/1306.0326v1", 
    "arxiv-id": "1306.0326v1"
},{
    "category": "cs.DC", 
    "author": "Islam Elgedawy", 
    "title": "DCaaS: Data Consistency as a Service for Managing Data Uncertainty on   the Clouds", 
    "publish": "2013-06-03T14:46:08Z", 
    "summary": "Ensuring data correctness over partitioned distributed database systems is a\nclassical problem. Classical solutions proposed to solve this problem are\nmainly adopting locking or blocking techniques. These techniques are not\nsuitable for cloud environments as they produce terrible response times; due to\nthe long latency and faultiness of wide area network connections among cloud\ndatacenters. One way to improve performance is to restrict access of\nusers-bases to specific datacenters and avoid data sharing between datacenters.\nHowever, conflicts might appear when data is replicated between datacenters;\nnevertheless change propagation timeliness is not guaranteed. Such problems\ncreated data uncertainty on cloud environments. Managing data uncertainty is\none of the main obstacles for supporting global distributed transactions on the\nclouds. To overcome this problem, this paper proposes an quota-based approach\nfor managing data uncertainty on the clouds that guarantees global data\ncorrectness without global locking or blocking. To decouple service developers\nfrom the hassles of managing data uncertainty, we propose to use a new platform\nservice (i.e. Data Consistency as a Service (DCaaS)) to encapsulate the\nproposed approach. DCaaS service also ensures SaaS services cloud portability,\nas it works as a cloud adapter between SaaS service instances. Experiments show\nthat proposed approach realized by the DCaaS service provides much better\nresponse time when compared with classical locking and blocking techniques.", 
    "link": "http://arxiv.org/pdf/1306.0441v1", 
    "arxiv-id": "1306.0441v1"
},{
    "category": "cs.DC", 
    "author": "Ibrahim Abd El-Salam", 
    "title": "Adaptive Fixed Priority End-To-End Imprecise Scheduling In Distributed   Real Time Systems", 
    "publish": "2013-06-03T15:07:27Z", 
    "summary": "In end-to-end distributed real time systems, a task may be executed\nsequentially on different processors. The end-toend task response time must not\nexceed the end-to-end task deadline to consider the task a schedulable task. In\ntransient over load periods, deadlines may be missed or processors may\nsaturate. The imprecise computation technique is a way to overcome the\nmentioned problems by trading off precision and timeliness. We developed an\nimprecise integrated framework for scheduling fixed priority end-to-end tasks\nin distributed real time systems by extending an existing integrated framework\nfor the same problem. We devised a new priority assignment scheme called global\nmandatory relevance scheme to meet the concept of imprecise computation. We\ndevised an algorithm for processor utilization adjustment, this algorithm\ndecreases the processor load when the processor utilization is greater than\none. Also we extended the schedulability analysis algorithms presented in the\nold framework to allow adaptive priority assignment and to meet imprecise\ncomputation concept. Simulation results showed that our new framework is more\ndependable and predictable than the existing framework over transient overload\nperiods.", 
    "link": "http://arxiv.org/pdf/1306.0448v1", 
    "arxiv-id": "1306.0448v1"
},{
    "category": "cs.DC", 
    "author": "Ambika Prasad Mohanty", 
    "title": "Scalable Distributed Job Processing with Dynamic Load Balancing", 
    "publish": "2013-06-06T06:32:54Z", 
    "summary": "We present here a cost effective framework for a robust scalable and\ndistributed job processing system that adapts to the dynamic computing needs\neasily with efficient load balancing for heterogeneous systems. The design is\nsuch that each of the components are self contained and do not depend on each\nother. Yet, they are still interconnected through an enterprise message bus so\nas to ensure safe, secure and reliable communication based on transactional\nfeatures to avoid duplication as well as data loss. The load balancing,\nfault-tolerance and failover recovery are built into the system through a\nmechanism of health check facility and a queue based load balancing. The system\nhas a centralized repository with central monitors to keep track of the\nprogress of various job executions as well as status of processors in\nreal-time. The basic requirement of assigning a priority and processing as per\npriority is built into the framework. The most important aspect of the\nframework is that it avoids the need for job migration by computing the target\nprocessors based on the current load and the various cost factors. The\nframework will have the capability to scale horizontally as well as vertically\nto achieve the required performance, thus effectively minimizing the total cost\nof ownership.", 
    "link": "http://arxiv.org/pdf/1306.1303v1", 
    "arxiv-id": "1306.1303v1"
},{
    "category": "cs.DC", 
    "author": "Adam Barker", 
    "title": "A Cloud Computing Survey: Developments and Future Trends in   Infrastructure as a Service Computing", 
    "publish": "2013-06-06T12:41:57Z", 
    "summary": "Cloud computing is a recent paradigm based around the notion of delivery of\nresources via a service model over the Internet. Despite being a new paradigm\nof computation, cloud computing owes its origins to a number of previous\nparadigms. The term cloud computing is well defined and no longer merits\nrigorous taxonomies to furnish a definition. Instead this survey paper\nconsiders the past, present and future of cloud computing. As an evolution of\nprevious paradigms, we consider the predecessors to cloud computing and what\nsignificance they still hold to cloud services. Additionally we examine the\ntechnologies which comprise cloud computing and how the challenges and future\ndevelopments of these technologies will influence the field. Finally we examine\nthe challenges that limit the growth, application and development of cloud\ncomputing and suggest directions required to overcome these challenges in order\nto further the success of cloud computing.", 
    "link": "http://arxiv.org/pdf/1306.1394v1", 
    "arxiv-id": "1306.1394v1"
},{
    "category": "cs.DC", 
    "author": "Tarandeep Kaur", 
    "title": "Delivering IT as A Utility- A Systematic Review", 
    "publish": "2013-06-07T07:37:31Z", 
    "summary": "Utility Computing has facilitated the creation of new markets that has made\nit possible to realize the long held dream of delivering IT as a Utility. Even\nthough utility computing is in its nascent stage today, the proponents of\nutility computing envisage that it will become a commodity business in the\nupcoming time and utility service providers will meet all the IT requests of\nthe companies. This paper takes a cross-sectional view at the emergence of\nutility computing along with different requirements needed to realize utility\nmodel. It also surveys the current trends in utility computing highlighting\ndiverse architecture models aligned towards delivering IT as a utility.\nDifferent resource management systems for proficient allocation of resources\nhave been listed together with various resource scheduling and pricing\nstrategies used by them. Further, a review of generic key perspectives closely\nrelated to the concept of delivering IT as a Utility has been taken citing the\ncontenders for the future enhancements in this technology in the form of Grid\nand Cloud Computing.", 
    "link": "http://arxiv.org/pdf/1306.1639v1", 
    "arxiv-id": "1306.1639v1"
},{
    "category": "cs.DC", 
    "author": "Christian Scheideler", 
    "title": "A DeterministicWorst-Case Message Complexity Optimal Solution for   Resource Discovery", 
    "publish": "2013-06-07T11:23:40Z", 
    "summary": "We consider the problem of resource discovery in distributed systems. In\nparticular we give an algorithm, such that each node in a network discovers the\naddress of any other node in the network. We model the knowledge of the nodes\nas a virtual overlay network given by a directed graph such that complete\nknowledge of all nodes corresponds to a complete graph in the overlay network.\nAlthough there are several solutions for resource discovery, our solution is\nthe first that achieves worst-case optimal work for each node, i.e. the number\nof addresses (O(n)) or bits (O(n log n)) a node receives or sends coincides\nwith the lower bound, while ensuring only a linear runtime (O(n)) on the number\nof rounds.", 
    "link": "http://arxiv.org/pdf/1306.1692v1", 
    "arxiv-id": "1306.1692v1"
},{
    "category": "cs.DC", 
    "author": "Elli Zavou", 
    "title": "Online Parallel Scheduling of Non-uniform Tasks: Trading Failures for   Energy", 
    "publish": "2013-06-08T00:34:57Z", 
    "summary": "Consider a system in which tasks of different execution times arrive\ncontinuously and have to be executed by a set of processors that are prone to\ncrashes and restarts. In this paper we model and study the impact of\nparallelism and failures on the competitiveness of such an online system. In a\nfault-free environment, a simple Longest-in-System scheduling policy, enhanced\nby a redundancy-avoidance mechanism, guarantees optimality in a long-term\nexecution. In the presence of failures though, scheduling becomes a much more\nchallenging task. In particular, no parallel deterministic algorithm can be\ncompetitive against an offline optimal solution, even with one single processor\nand tasks of only two different execution times. We find that when additional\nenergy is provided to the system in the form of processor speedup, the\nsituation changes. Specifically, we identify thresholds on the speedup under\nwhich such competitiveness cannot be achieved by any deterministic algorithm,\nand above which competitive algorithms exist. Finally, we propose algorithms\nthat achieve small bounded competitive ratios when the speedup is over the\nthreshold.", 
    "link": "http://arxiv.org/pdf/1306.1861v1", 
    "arxiv-id": "1306.1861v1"
},{
    "category": "cs.DC", 
    "author": "Elarbi Badidi", 
    "title": "A Framework for Software-as-a-Service Selection and Provisioning", 
    "publish": "2013-06-08T07:20:49Z", 
    "summary": "As cloud computing is increasingly transforming the information technology\nlandscape, organizations and businesses are exhibiting strong interest in\nSoftware-as-a-Service (SaaS) offerings that can help them increase business\nagility and reduce their operational costs. They increasingly demand services\nthat can meet their functional and non-functional requirements. Given the\nplethora and the variety of SaaS offerings, we propose, in this paper, a\nframework for SaaS provisioning, which relies on brokered Service Level\nagreements (SLAs), between service consumers and SaaS providers. The Cloud\nService Broker (CSB) helps service consumers find the right SaaS providers that\ncan fulfil their functional and non-functional requirements. The proposed\nselection algorithm ranks potential SaaS providers by matching their offerings\nagainst the requirements of the service consumer using an aggregate utility\nfunction. Furthermore, the CSB is in charge of conducting SLA negotiation with\nselected SaaS providers, on behalf of service consumers, and performing SLA\ncompliance monitoring.", 
    "link": "http://arxiv.org/pdf/1306.1888v1", 
    "arxiv-id": "1306.1888v1"
},{
    "category": "cs.DC", 
    "author": "Michael Hosein", 
    "title": "A Light-Weight Distributed System for the processing of Replicated   Counter-like Objects", 
    "publish": "2013-06-08T15:05:18Z", 
    "summary": "In order to increase availability in a distributed system some or all of the\ndata items are replicated and stored at separate sites. This is an issue of key\nconcern especially since there is such a proliferation of wireless technologies\nand mobile users. However, the concurrent processing of transactions at\nseparate sites can generate inconsistencies in the stored information. We have\nbuilt a distributed service that manages updates to widely deployed\ncounter-like replicas. There are many heavy-weight distributed systems\ntargeting large information critical applications. Our system is intentionally,\nrelatively lightweight and useful for the somewhat reduced information critical\napplications. The service is built on our distributed concurrency control\nscheme which combines optimism and pessimism in the processing of transactions.\nThe service allows a transaction to be processed immediately (optimistically)\nat any individual replica as long as the transaction satisfies a cost bound.\nAll transactions are also processed in a concurrent pessimistic manner to\nensure mutual consistency.", 
    "link": "http://arxiv.org/pdf/1306.1928v1", 
    "arxiv-id": "1306.1928v1"
},{
    "category": "cs.DC", 
    "author": "Marc Shapiro", 
    "title": "Non-Monotonic Snapshot Isolation", 
    "publish": "2013-06-17T15:45:13Z", 
    "summary": "Many distributed applications require transactions. However, transactional\nprotocols that require strong synchronization are costly in large scale\nenvironments. Two properties help with scalability of a transactional system:\ngenuine partial replication (GPR), which leverages the intrinsic parallelism of\na workload, and snapshot isolation (SI), which decreases the need for\nsynchronization. We show that, under standard assumptions (data store accesses\nare not known in advance, and transactions may access arbitrary objects in the\ndata store), it is impossible to have both SI and GPR. To circumvent this\nimpossibility, we propose a weaker consistency criterion, called Non-monotonic\nSnapshot Isolation (NMSI). NMSI retains the most important properties of SI,\ni.e., read-only transactions always commit, and two write-conflicting updates\ndo not both commit. We present a GPR protocol that ensures NMSI, and has lower\nmessage cost (i.e., it contacts fewer replicas and/or commits faster) than\nprevious approaches.", 
    "link": "http://arxiv.org/pdf/1306.3906v1", 
    "arxiv-id": "1306.3906v1"
},{
    "category": "cs.DC", 
    "author": "Alexey Lastovetsky", 
    "title": "Hierarchical Parallel Matrix Multiplication on Large-Scale Distributed   Memory Platforms", 
    "publish": "2013-06-18T12:17:36Z", 
    "summary": "Matrix multiplication is a very important computation kernel both in its own\nright as a building block of many scientific applications and as a popular\nrepresentative for other scientific applications. Cannon algorithm which dates\nback to 1969 was the first efficient algorithm for parallel matrix\nmultiplication providing theoretically optimal communication cost. However this\nalgorithm requires a square number of processors. In the mid 1990s, the SUMMA\nalgorithm was introduced. SUMMA overcomes the shortcomings of Cannon algorithm\nas it can be used on a non-square number of processors as well. Since then the\nnumber of processors in HPC platforms has increased by two orders of magnitude\nmaking the contribution of communication in the overall execution time more\nsignificant. Therefore, the state of the art parallel matrix multiplication\nalgorithms should be revisited to reduce the communication cost further. This\npaper introduces a new parallel matrix multiplication algorithm, Hierarchical\nSUMMA (HSUMMA), which is a redesign of SUMMA. Our algorithm reduces the\ncommunication cost of SUMMA by introducing a two-level virtual hierarchy into\nthe two-dimensional arrangement of processors. Experiments on an IBM BlueGene-P\ndemonstrate the reduction of communication cost up to 2.08 times on 2048 cores\nand up to 5.89 times on 16384 cores.", 
    "link": "http://arxiv.org/pdf/1306.4161v1", 
    "arxiv-id": "1306.4161v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "Cloud-Based Augmentation for Mobile Devices: Motivation, Taxonomies, and   Open Challenges", 
    "publish": "2013-06-20T18:41:00Z", 
    "summary": "Recently, Cloud-based Mobile Augmentation (CMA) approaches have gained\nremarkable ground from academia and industry. CMA is the state-of-the-art\nmobile augmentation model that employs resource-rich clouds to increase,\nenhance, and optimize computing capabilities of mobile devices aiming at\nexecution of resource-intensive mobile applications. Augmented mobile devices\nenvision to perform extensive computations and to store big data beyond their\nintrinsic capabilities with least footprint and vulnerability. Researchers\nutilize varied cloud-based computing resources (e.g., distant clouds and nearby\nmobile nodes) to meet various computing requirements of mobile users. However,\nemploying cloud-based computing resources is not a straightforward panacea.\nComprehending critical factors that impact on augmentation process and optimum\nselection of cloud-based resource types are some challenges that hinder CMA\nadaptability. This paper comprehensively surveys the mobile augmentation domain\nand presents taxonomy of CMA approaches. The objectives of this study is to\nhighlight the effects of remote resources on the quality and reliability of\naugmentation processes and discuss the challenges and opportunities of\nemploying varied cloud-based resources in augmenting mobile devices. We present\naugmentation definition, motivation, and taxonomy of augmentation types,\nincluding traditional and cloud-based. We critically analyze the\nstate-of-the-art CMA approaches and classify them into four groups of distant\nfixed, proximate fixed, proximate mobile, and hybrid to present a taxonomy.\nVital decision making and performance limitation factors that influence on the\nadoption of CMA approaches are introduced and an exemplary decision making\nflowchart for future CMA approaches are presented. Impacts of CMA approaches on\nmobile computing is discussed and open challenges are presented as the future\nresearch directions.", 
    "link": "http://arxiv.org/pdf/1306.4956v1", 
    "arxiv-id": "1306.4956v1"
},{
    "category": "cs.DC", 
    "author": "Patrizio Dazzi", 
    "title": "A Tool for Programming Embarrassingly Task Parallel Applications on CoW   and NoW", 
    "publish": "2013-06-24T21:05:44Z", 
    "summary": "Embarrassingly parallel problems can be split in parts that are characterized\nby a really low (or sometime absent) exchange of information during their\ncomputation in parallel. As a consequence they can be effectively computed in\nparallel exploiting commodity hardware, hence without particularly\nsophisticated interconnection networks. Basically, this means Clusters,\nNetworks of Workstations and Desktops as well as Computational Clouds. Despite\nthe simplicity of this computational model, it can be exploited to compute a\nquite large range of problems. This paper describes JJPF, a tool for developing\ntask parallel applications based on Java and Jini that showed to be an\neffective and efficient solution in environment like Clusters and Networks of\nWorkstations and Desktops.", 
    "link": "http://arxiv.org/pdf/1306.5782v1", 
    "arxiv-id": "1306.5782v1"
},{
    "category": "cs.DC", 
    "author": "Matteo Dell'Amico", 
    "title": "A Simulator for Data-Intensive Job Scheduling", 
    "publish": "2013-06-25T16:34:05Z", 
    "summary": "Despite the fact that size-based schedulers can give excellent results in\nterms of both average response times and fairness, data-intensive computing\nexecution engines generally do not employ size-based schedulers, mainly because\nof the fact that job size is not known a priori.\n  In this work, we perform a simulation-based analysis of the performance of\nsize-based schedulers when they are employed with the workload of typical\ndata-intensive schedules and with approximated size estimations. We show\nresults that are very promising: even when size estimation is very imprecise,\nresponse times of size-based schedulers can be definitely smaller than those of\nsimple scheduling techniques such as processor sharing or FIFO.", 
    "link": "http://arxiv.org/pdf/1306.6023v2", 
    "arxiv-id": "1306.6023v2"
},{
    "category": "cs.DC", 
    "author": "Lukasz Swierczewski", 
    "title": "Akceleracja obliczen algebry liniowej z wykorzystaniem masywnie   rownoleglych, wielordzeniowych procesorow GPU", 
    "publish": "2013-06-26T10:14:37Z", 
    "summary": "The paper presents the aspect of use of modern graphics accelerators\nsupporting CUDA technology for high-performance computing in the field of\nlinear algebra. Fully programmable graphic cards have been available for\nseveral years for both ordinary users and research units. They provide the\ncapability of performing virtually any computing with high performance, which\nis often beyond the reach of conventional CPUs. GPU architecture, also in case\nof classical problems of linear algebra which is the basis for many\ncalculations, can bring many benefits to the developer. Performance increase,\nobserved during matrix multiplication on nVidia Tesla C2050, was more than\nthousandfold compared to ordinary CPU, resulting in drastic reduction of\nlatency for some of the results, thus the cost of obtaining them.", 
    "link": "http://arxiv.org/pdf/1306.6192v1", 
    "arxiv-id": "1306.6192v1"
},{
    "category": "cs.DC", 
    "author": "Vipin Tyagi", 
    "title": "Query Centric CPS (QCPS) Approach for Multiple Heterogeneous Systems", 
    "publish": "2013-06-27T03:13:09Z", 
    "summary": "In modern scenario we need to have mechanisms which can provide better\ninteraction with physical world by an efficient and more effective\ncommunication and computation approach for multiple heterogeneous sensor\nnetworks. Previous work provides efficient communication approach between\nsensor nodes and a query centric approach for multiple collaborative\nheterogeneous sensor networks. Even there is energy issues involved in wireless\nsensor network operation. In this paper we have proposed Query centric Cyber\nPhysical System (QCPS)model to implement query centric user request using Cyber\nPhysical System (CPS). CPS takes both communication and computation in parallel\nto provide better interaction with physical world. This feature of CPS reduces\nsystem cost and makes it more energy efficient. This paper provides an\nefficient query processing approach for multiple heterogeneous sensor networks\nusing cyber physical system.This approach results in reduction of communication\nand computation cost as sensor network communicates using centroid of\nrespective grids which reduces cost of communication while involvement of CPS\nreduces the computation cost.", 
    "link": "http://arxiv.org/pdf/1306.6397v1", 
    "arxiv-id": "1306.6397v1"
},{
    "category": "cs.DC", 
    "author": "Cheng Liu", 
    "title": "Monetary Cost Optimizations for Hosting Workflow-as-a-Service in IaaS   Clouds", 
    "publish": "2013-06-27T05:50:26Z", 
    "summary": "Recently, we have witnessed workflows from science and other data-intensive\napplications emerging on Infrastructure-asa-Service (IaaS) clouds, and many\nworkflow service providers offering workflow as a service (WaaS). The major\nconcern of WaaS providers is to minimize the monetary cost of executing\nworkflows in the IaaS cloud. While there have been previous studies on this\nconcern, most of them assume static task execution time and static pricing\nscheme, and have the QoS notion of satisfying a deterministic deadline.\nHowever, cloud environment is dynamic, with performance dynamics caused by the\ninterference from concurrent executions and price dynamics like spot prices\noffered by Amazon EC2. Therefore, we argue that WaaS providers should have the\nnotion of offering probabilistic performance guarantees for individual\nworkflows on IaaS clouds. We develop a probabilistic scheduling framework\ncalled Dyna to minimize the monetary cost while offering probabilistic deadline\nguarantees. The framework includes an A*-based instance configuration method\nfor performance dynamics, and a hybrid instance configuration refinement for\nutilizing spot instances. Experimental results with three real-world scientific\nworkflow applications on Amazon EC2 demonstrate (1) the accuracy of our\nframework on satisfying the probabilistic deadline guarantees required by the\nusers; (2) the effectiveness of our framework on reducing monetary cost in\ncomparison with the existing approaches.", 
    "link": "http://arxiv.org/pdf/1306.6410v2", 
    "arxiv-id": "1306.6410v2"
},{
    "category": "cs.DC", 
    "author": "Rishideep Singh", 
    "title": "Earthquake Disaster based Efficient Resource Utilization Technique in   IaaS Cloud", 
    "publish": "2013-06-27T18:23:06Z", 
    "summary": "Cloud Computing is an emerging area. The main aim of the initial\nsearch-and-rescue period after strong earthquakes is to reduce the whole number\nof mortalities. One main trouble rising in this period is to and the greatest\nassignment of available resources to functioning zones. For this issue a\ndynamic optimization model is presented. The model uses thorough descriptions\nof the operational zones and of the available resources to determine the\nresource performance and efficiency for different workloads related to the\nresponse. A suitable solution method for the model is offered as well. In this\npaper, Earthquake Disaster Based Resource Scheduling (EDBRS) Framework has been\nproposed. The allocation of resources to cloud workloads based on urgency\n(emergency during Earthquake Disaster). Based on this criterion, the resource\nscheduling algorithm has been proposed. The performance of the proposed\nalgorithm has been assessed with the existing common scheduling algorithms\nthrough the CloudSim. The experimental results show that the proposed algorithm\noutperforms the existing algorithms by reducing execution cost and time of\ncloud consumer workloads submitted to the cloud.", 
    "link": "http://arxiv.org/pdf/1306.6597v3", 
    "arxiv-id": "1306.6597v3"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Byzantine Convex Consensus: Preliminary Version", 
    "publish": "2013-07-03T15:49:13Z", 
    "summary": "Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [3, 7]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [8, 12]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [8, 12] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In this paper, we generalize the problem to allow the decision to be a convex\npolytope in the d-dimensional space, such that the decided polytope is within\nthe convex hull of the input vectors at the fault-free nodes. We name this\nproblem as Byzantine convex consensus (BCC), and present an asynchronous\napproximate BCC algorithm with optimal fault tolerance. Ideally, the goal here\nis to agree on a convex polytope that is as large as possible. While we do not\nclaim that our algorithm satisfies this goal, we show a bound on the output\nconvex polytope chosen by our algorithm.", 
    "link": "http://arxiv.org/pdf/1307.1051v1", 
    "arxiv-id": "1307.1051v1"
},{
    "category": "cs.DC", 
    "author": "Piero Vicini", 
    "title": "A heterogeneous many-core platform for experiments on scalable custom   interconnects and management of fault and critical events, applied to   many-process applications: Vol. II, 2012 technical report", 
    "publish": "2013-07-04T10:55:43Z", 
    "summary": "This is the second of a planned collection of four yearly volumes describing\nthe deployment of a heterogeneous many-core platform for experiments on\nscalable custom interconnects and management of fault and critical events,\napplied to many-process applications. This volume covers several topics, among\nwhich: 1- a system for awareness of faults and critical events (named LO|FA|MO)\non experimental heterogeneous many-core hardware platforms; 2- the integration\nand test of the experimental hardware heterogeneous many-core platform QUoNG,\nbased on the APEnet+ custom interconnect; 3- the design of a\nSoftware-Programmable Distributed Network Processor architecture (DNP) using\nASIP technology; 4- the initial stages of design of a new DNP generation onto a\n28nm FPGA. These developments were performed in the framework of the EURETILE\nEuropean Project under the Grant Agreement no. 247846.", 
    "link": "http://arxiv.org/pdf/1307.1270v1", 
    "arxiv-id": "1307.1270v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Byzantine Convex Consensus: An Optimal Algorithm", 
    "publish": "2013-07-04T14:01:57Z", 
    "summary": "Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [4, 8]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [9, 13]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [9, 13] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In our recent work [arXiv:/1307.1051], we proposed a generalization of the\nconsensus problem, namely Byzantine convex consensus (BCC), which allows the\ndecision to be a convex polytope in the d-dimensional space, such that the\ndecided polytope is within the convex hull of the input vectors at the\nfault-free nodes. We also presented an asynchronous approximate BCC algorithm.\n  In this paper, we propose a new BCC algorithm with optimal fault-tolerance\nthat also agrees on a convex polytope that is as large as possible under\nadversarial conditions. Our prior work [arXiv:/1307.1051] does not guarantee\nthe optimality of the output polytope.", 
    "link": "http://arxiv.org/pdf/1307.1332v2", 
    "arxiv-id": "1307.1332v2"
},{
    "category": "cs.DC", 
    "author": "Aaradhana Deshmukh", 
    "title": "Perform wordcount Map-Reduce Job in Single Node Apache Hadoop cluster   and compress data using Lempel-Ziv-Oberhumer (LZO) algorithm", 
    "publish": "2013-07-05T04:10:34Z", 
    "summary": "Applications like Yahoo, Facebook, Twitter have huge data which has to be\nstored and retrieved as per client access. This huge data storage requires huge\ndatabase leading to increase in physical storage and becomes complex for\nanalysis required in business growth. This storage capacity can be reduced and\ndistributed processing of huge data can be done using Apache Hadoop which uses\nMap-reduce algorithm and combines the repeating data so that entire data is\nstored in reduced format. The paper describes performing a wordcount Map-Reduce\nJob in Single Node Apache Hadoop cluster and compress data using\nLempel-Ziv-Oberhumer (LZO) algorithm.", 
    "link": "http://arxiv.org/pdf/1307.1517v1", 
    "arxiv-id": "1307.1517v1"
},{
    "category": "cs.DC", 
    "author": "Bingsheng He", 
    "title": "Revisiting Co-Processing for Hash Joins on the Coupled CPU-GPU   Architecture", 
    "publish": "2013-07-08T06:23:12Z", 
    "summary": "Query co-processing on graphics processors (GPUs) has become an effective\nmeans to improve the performance of main memory databases. However, the\nrelatively low bandwidth and high latency of the PCI-e bus are usually\nbottleneck issues for co-processing. Recently, coupled CPU-GPU architectures\nhave received a lot of attention, e.g. AMD APUs with the CPU and the GPU\nintegrated into a single chip. That opens up new opportunities for optimizing\nquery co-processing. In this paper, we experimentally revisit hash joins, one\nof the most important join algorithms for main memory databases, on a coupled\nCPU-GPU architecture. Particularly, we study the fine-grained co-processing\nmechanisms on hash joins with and without partitioning. The co-processing\noutlines an interesting design space. We extend existing cost models to\nautomatically guide decisions on the design space. Our experimental results on\na recent AMD APU show that (1) the coupled architecture enables fine-grained\nco-processing and cache reuses, which are inefficient on discrete CPU-GPU\narchitectures; (2) the cost model can automatically guide the design and tuning\nknobs in the design space; (3) fine-grained co-processing achieves up to 53%,\n35% and 28% performance improvement over CPU-only, GPU-only and conventional\nCPU-GPU co-processing, respectively. We believe that the insights and\nimplications from this study are initial yet important for further research on\nquery co-processing on coupled CPU-GPU architectures.", 
    "link": "http://arxiv.org/pdf/1307.1955v1", 
    "arxiv-id": "1307.1955v1"
},{
    "category": "cs.DC", 
    "author": "Rajkumar Buyya", 
    "title": "A Taxonomy of Performance Prediction Systems in the Parallel and   Distributed Computing Grids", 
    "publish": "2013-07-09T09:34:25Z", 
    "summary": "As Grids are loosely-coupled congregations of geographically distributed\nheterogeneous resources, the efficient utilization of the resources requires\nthe support of a sound Performance Prediction System (PPS). The performance\nprediction of grid resources is helpful for both Resource Management Systems\nand grid users to make optimized resource usage decisions. There have been many\nPPS projects that span over several grid resources in several dimensions. In\nthis paper the taxonomy for describing the PPS architecture is discussed. The\ntaxonomy is used to categorize and identify approaches which are followed in\nthe implementation of the existing PPSs for Grids. The taxonomy and the survey\nresults are used to identify approaches and issues that have not been fully\nexplored in research.", 
    "link": "http://arxiv.org/pdf/1307.2380v2", 
    "arxiv-id": "1307.2380v2"
},{
    "category": "cs.DC", 
    "author": "Nitin H. Vaidya", 
    "title": "Iterative Byzantine Vector Consensus in Incomplete Graphs", 
    "publish": "2013-07-09T14:53:05Z", 
    "summary": "This work addresses Byzantine vector consensus (BVC), wherein the input at\neach process is a d-dimensional vector of reals, and each process is expected\nto decide on a decision vector that is in the convex hull of the input vectors\nat the fault-free processes [3, 8]. The input vector at each process may also\nbe viewed as a point in the d-dimensional Euclidean space R^d, where d > 0 is a\nfinite integer. Recent work [3, 8] has addressed Byzantine vector consensus in\nsystems that can be modeled by a complete graph. This paper considers Byzantine\nvector consensus in incomplete graphs. In particular, we address a particular\nclass of iterative algorithms in incomplete graphs, and prove a necessary\ncondition, and a sufficient condition, for the graphs to be able to solve the\nvector consensus problem iteratively. We present an iterative Byzantine vector\nconsensus algorithm, and prove it correct under the sufficient condition. The\nnecessary condition presented in this paper for vector consensus does not match\nwith the sufficient condition for d > 1; thus, a weaker condition may\npotentially suffice for Byzantine vector consensus.", 
    "link": "http://arxiv.org/pdf/1307.2483v1", 
    "arxiv-id": "1307.2483v1"
},{
    "category": "cs.DC", 
    "author": "Carlos Baquero", 
    "title": "Scalable Eventually Consistent Counters over Unreliable Networks", 
    "publish": "2013-07-11T18:28:06Z", 
    "summary": "Counters are an important abstraction in distributed computing, and play a\ncentral role in large scale geo-replicated systems, counting events such as web\npage impressions or social network \"likes\". Classic distributed counters,\nstrongly consistent, cannot be made both available and partition-tolerant, due\nto the CAP Theorem, being unsuitable to large scale scenarios. This paper\ndefines Eventually Consistent Distributed Counters (ECDC) and presents an\nimplementation of the concept, Handoff Counters, that is scalable and works\nover unreliable networks. By giving up the sequencer aspect of classic\ndistributed counters, ECDC implementations can be made AP in the CAP design\nspace, while retaining the essence of counting. Handoff Counters are the first\nCRDT (Conflict-free Replicated Data Type) based mechanism that overcomes the\nidentity explosion problem in naive CRDTs, such as G-Counters (where state size\nis linear in the number of independent actors that ever incremented the\ncounter), by managing identities towards avoiding global propagation and\ngarbage collecting temporary entries. The approach used in Handoff Counters is\nnot restricted to counters, being more generally applicable to other data types\nwith associative and commutative operations.", 
    "link": "http://arxiv.org/pdf/1307.3207v1", 
    "arxiv-id": "1307.3207v1"
},{
    "category": "cs.DC", 
    "author": "Dan C. Marinescu", 
    "title": "Energy-aware Application Scaling on a Cloud", 
    "publish": "2013-07-12T02:43:46Z", 
    "summary": "Cloud elasticity - the ability to use as much resources as needed at any\ngiven time - and low cost - a user pays only for the resources it consumes -\nrepresent solid incentives for many organizations to migrate some of their\ncomputational activities to a public cloud. As the interest in cloud computing\ngrows, so does the size of the cloud computing centers and their energy\nfootprint. The realization that power consumption of cloud computing centers is\nsignificant and it is expected to increase substantially in the future\nmotivates our interest in scheduling and scaling algorithms which minimize\npower consumption. We propose energy-aware application scaling and resource\nmanagement algorithms. Though targeting primarily the Infrastructure as a\nService (IaaS), the system models and the algorithms we propose can be applied\nto the other cloud delivery models and to private clouds.", 
    "link": "http://arxiv.org/pdf/1307.3306v1", 
    "arxiv-id": "1307.3306v1"
},{
    "category": "cs.DC", 
    "author": "Christian Scheideler", 
    "title": "Ameba-inspired Self-organizing Particle Systems", 
    "publish": "2013-07-16T12:41:05Z", 
    "summary": "Particle systems are physical systems of simple computational particles that\ncan bond to neighboring particles and use these bonds to move from one spot to\nanother (non-occupied) spot. These particle systems are supposed to be able to\nself-organize in order to adapt to a desired shape without any central control.\nSelf-organizing particle systems have many interesting applications like\ncoating objects for monitoring and repair purposes and the formation of\nnano-scale devices for surgery and molecular-scale electronic structures. While\nthere has been quite a lot of systems work in this area, especially in the\ncontext of modular self-reconfigurable robotic systems, only very little\ntheoretical work has been done in this area so far. We attempt to bridge this\ngap by proposing a model inspired by the behavior of ameba that allows rigorous\nalgorithmic research on self-organizing particle systems.", 
    "link": "http://arxiv.org/pdf/1307.4259v1", 
    "arxiv-id": "1307.4259v1"
},{
    "category": "cs.DC", 
    "author": "James Southern", 
    "title": "Benchmarking mixed-mode PETSc performance on high-performance   architectures", 
    "publish": "2013-07-17T10:36:35Z", 
    "summary": "The trend towards highly parallel multi-processing is ubiquitous in all\nmodern computer architectures, ranging from handheld devices to large-scale HPC\nsystems; yet many applications are struggling to fully utilise the multiple\nlevels of parallelism exposed in modern high-performance platforms. In order to\nrealise the full potential of recent hardware advances, a mixed-mode between\nshared-memory programming techniques and inter-node message passing can be\nadopted which provides high-levels of parallelism with minimal overheads. For\nscientific applications this entails that not only the simulation code itself,\nbut the whole software stack needs to evolve. In this paper, we evaluate the\nmixed-mode performance of PETSc, a widely used scientific library for the\nscalable solution of partial differential equations. We describe the addition\nof OpenMP threaded functionality to the library, focusing on sparse\nmatrix-vector multiplication. We highlight key challenges in achieving good\nparallel performance, such as explicit communication overlap using task-based\nparallelism, and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. Using a set of matrices extracted from\nFluidity, a CFD application code which uses the library as its linear solver\nengine, we then benchmark the parallel performance of mixed-mode PETSc across\nmultiple nodes on several modern HPC architectures. We evaluate the parallel\nscalability on Uniform Memory Access (UMA) systems, such as the Fujitsu\nPRIMEHPC FX10 and IBM BlueGene/Q, as well as a Non-Uniform Memory Access (NUMA)\nCray XE6 platform. A detailed comparison is performed which highlights the\ncharacteristics of each particular architecture, before demonstrating efficient\nstrong scalability of sparse matrix-vector multiplication with significant\nspeedups over the pure-MPI mode.", 
    "link": "http://arxiv.org/pdf/1307.4567v1", 
    "arxiv-id": "1307.4567v1"
},{
    "category": "cs.DC", 
    "author": "Hari Sundar", 
    "title": "A Nested Partitioning Scheme for Parallel Heterogeneous Clusters", 
    "publish": "2013-07-17T19:17:32Z", 
    "summary": "Modern supercomputers are increasingly requiring the presence of accelerators\nand co-processors. However, it has not been easy to achieve good performance on\nsuch heterogeneous clusters. The key challenge has been to ensure good load\nbalance and that neither the CPU nor the accelerator is left idle. Traditional\napproaches have offloaded entire computations to the accelerator, resulting in\nan idle CPU, or have opted for task-level parallelism requiring large data\ntransfers between the CPU and the accelerator. True work-parallelism has been\nhard as the Accelerators cannot directly communicate with other CPUs (besides\nthe host) and Accelerators. In this work, we present a new nested partition\nscheme to overcome this problem. By partitioning the work assignment on a given\nnode asymmetrically into boundary and interior work, and assigning the interior\nto the accelerator, we are able to achieve excellent efficiency while ensure\nproper utilization of both the CPU and Accelerator resources. The problem used\nfor evaluating the new partition is an $hp$ discontinuous Galerkin spectral\nelement method for a coupled elastic--acoustic wave propagation problem.", 
    "link": "http://arxiv.org/pdf/1307.4731v1", 
    "arxiv-id": "1307.4731v1"
},{
    "category": "cs.DC", 
    "author": "A. B. Premkumar", 
    "title": "Distributed Computation of the Conditional PCRLB for Quantized   Decentralized Particle Filters", 
    "publish": "2013-07-20T16:16:11Z", 
    "summary": "The conditional posterior Cramer-Rao lower bound (PCRLB) is an effective\nsensor resource management criteria for large, geographically distributed\nsensor networks. Existing algorithms for distributed computation of the PCRLB\n(dPCRLB) are based on raw observations leading to significant communication\noverhead to the estimation mechanism. This letter derives distributed\ncomputational techniques for determining the conditional dPCRLB for quantized,\ndecentralized sensor networks (CQ/dPCRLB). Analytical expressions for the\nCQ/dPCRLB are derived, which are particularly useful for particle filter-based\nestimators. The CQ/dPCRLB is compared for accuracy with its centralized\ncounterpart through Monte-Carlo simulations.", 
    "link": "http://arxiv.org/pdf/1307.5435v1", 
    "arxiv-id": "1307.5435v1"
},{
    "category": "cs.DC", 
    "author": "Gal Amram", 
    "title": "On the Mailbox Problem", 
    "publish": "2013-07-22T08:27:31Z", 
    "summary": "The Mailbox Problem was described and solved by Aguilera, Gafni, and Lamport\nin their 2010 DC paper with an algorithm that uses two flag registers that\ncarry 14 values each. An interesting problem that they ask is whether there is\na mailbox algorithm with smaller flag values. We give a positive answer by\ndescribing a mailbox algorithm with 6 and 4 values in the two flag registers.", 
    "link": "http://arxiv.org/pdf/1307.5619v1", 
    "arxiv-id": "1307.5619v1"
},{
    "category": "cs.DC", 
    "author": "Christian Scheideler", 
    "title": "CONE-DHT: A distributed self-stabilizing algorithm for a heterogeneous   storage system", 
    "publish": "2013-07-25T14:01:26Z", 
    "summary": "We consider the problem of managing a dynamic heterogeneous storage system in\na distributed way so that the amount of data assigned to a host in that system\nis related to its capacity. Two central problems have to be solved for this:\n(1) organizing the hosts in an overlay network with low degree and diameter so\nthat one can efficiently check the correct distribution of the data and route\nbetween any two hosts, and (2) distributing the data among the hosts so that\nthe distribution respects the capacities of the hosts and can easily be adapted\nas the set of hosts or their capacities change. We present distributed\nprotocols for these problems that are self-stabilizing and that do not need any\nglobal knowledge about the system such as the number of nodes or the overall\ncapacity of the system. Prior to this work no solution was known satisfying\nthese properties.", 
    "link": "http://arxiv.org/pdf/1307.6747v1", 
    "arxiv-id": "1307.6747v1"
},{
    "category": "cs.DC", 
    "author": "Ivona Brandi\u0107", 
    "title": "Take a break: cloud scheduling optimized for real-time electricity   pricing", 
    "publish": "2013-07-26T13:54:20Z", 
    "summary": "Cloud computing revolutionised the industry with its elastic, on-demand\napproach to computational resources, but has lead to a tremendous impact on the\nenvironment. Data centers constitute 1.1-1.5% of total electricity usage in the\nworld. Taking a more informed view of the electrical grid by analysing\nreal-time electricity prices, we set the foundations of a grid-conscious cloud.\nWe propose a scheduling algorithm that predicts electricity price peaks and\nthrottles energy consumption by pausing virtual machines. We evaluate the\napproach on the OpenStack cloud manager through an empirical approach and show\nreductions in energy consumption and costs. Finally, we define green instances\nin which cloud providers can offer such services to their customers under\nbetter pricing options.", 
    "link": "http://arxiv.org/pdf/1307.7037v1", 
    "arxiv-id": "1307.7037v1"
},{
    "category": "cs.DC", 
    "author": "Christoph Lenzen", 
    "title": "Node-Initiated Byzantine Consensus Without a Common Clock", 
    "publish": "2013-07-30T13:41:29Z", 
    "summary": "The majority of the literature on consensus assumes that protocols are\njointly started at all nodes of the distributed system. We show how to remove\nthis problematic assumption in semi-synchronous systems, where messages delays\nand relative drifts of local clocks may vary arbitrarily within known bounds.\nOur framework is self-stabilizing and efficient both in terms of communication\nand time; more concretely, compared to a synchronous start in a synchronous\nmodel of a non-self-stabilizing protocol, we achieve a constant-factor increase\nin the time and communicated bits to complete an instance, plus an additive\ncommunication overhead of O(n log n) broadcasted bits per time unit and node.\nThe latter can be further reduced, at an additive increase in time complexity.", 
    "link": "http://arxiv.org/pdf/1307.7976v2", 
    "arxiv-id": "1307.7976v2"
},{
    "category": "cs.DC", 
    "author": "Sachin Tripathi", 
    "title": "Addressing Security Challenges in Cloud Computing", 
    "publish": "2013-07-31T05:53:00Z", 
    "summary": "Cloud computing is a new computing paradigm which allows sharing of resources\non remote server such as hardware, network, storage using internet and provides\nthe way through which application, computing power, computing infrastructure\ncan be delivered to the user as a service. Cloud computing unique attribute\npromise cost effective Information Technology Solution (IT Solution) to the\nuser. All computing needs are provided by the Cloud Service Provider (CSP) and\nthey can be increased or decreased dynamically as required by the user. As data\nand Application are located at the server and may be beyond geographical\nboundary, this leads a number of concern from the user prospective. The\nobjective of this paper is to explore the key issues of cloud computing which\nis delaying its adoption.", 
    "link": "http://arxiv.org/pdf/1307.8228v1", 
    "arxiv-id": "1307.8228v1"
},{
    "category": "cs.DC", 
    "author": "Sathya Peri", 
    "title": "Multi-Version Conflict Notion", 
    "publish": "2013-07-31T09:17:23Z", 
    "summary": "This paper introduces the useful notion of Multi-Version Conflict notion.", 
    "link": "http://arxiv.org/pdf/1307.8256v1", 
    "arxiv-id": "1307.8256v1"
},{
    "category": "cs.DC", 
    "author": "Ben Liang", 
    "title": "Dominant Resource Fairness in Cloud Computing Systems with Heterogeneous   Servers", 
    "publish": "2013-08-01T03:08:22Z", 
    "summary": "We study the multi-resource allocation problem in cloud computing systems\nwhere the resource pool is constructed from a large number of heterogeneous\nservers, representing different points in the configuration space of resources\nsuch as processing, memory, and storage. We design a multi-resource allocation\nmechanism, called DRFH, that generalizes the notion of Dominant Resource\nFairness (DRF) from a single server to multiple heterogeneous servers. DRFH\nprovides a number of highly desirable properties. With DRFH, no user prefers\nthe allocation of another user; no one can improve its allocation without\ndecreasing that of the others; and more importantly, no user has an incentive\nto lie about its resource demand. As a direct application, we design a simple\nheuristic that implements DRFH in real-world systems. Large-scale simulations\ndriven by Google cluster traces show that DRFH significantly outperforms the\ntraditional slot-based scheduler, leading to much higher resource utilization\nwith substantially shorter job completion times.", 
    "link": "http://arxiv.org/pdf/1308.0083v1", 
    "arxiv-id": "1308.0083v1"
},{
    "category": "cs.DC", 
    "author": "Ivo F. Sbalzarini", 
    "title": "Balancing indivisible real-valued loads in arbitrary networks", 
    "publish": "2013-08-01T10:32:33Z", 
    "summary": "In parallel computing, a problem is divided into a set of smaller tasks that\nare distributed across multiple processing elements. Balancing the load of the\nprocessing elements is key to achieving good performance and scalability. If\nthe computational costs of the individual tasks vary over time in an\nunpredictable way, dynamic load balancing aims at migrating them between\nprocessing elements so as to maintain load balance. During dynamic load\nbalancing, the tasks amount to indivisible work packets with a real-valued\ncost. For this case of indivisible, real- valued loads, we analyze the\nbalancing circuit model, a local dynamic load-balancing scheme that does not\nrequire global communication. We extend previous analyses to the present case\nand provide a probabilistic bound for the achievable load balance. Based on an\nanalogy with the offline balls-into-bins problem, we further propose a novel\nalgorithm for dynamic balancing of indivisible, real-valued loads. We benchmark\nthe proposed algorithm in numerical experiments and compare it with the\nclassical greedy algorithm, both in terms of solution quality and communication\ncost. We find that the increased communication cost of the proposed algorithm\nis compensated by a higher solution quality, leading on average to about an\norder of magnitude gain in overall performance.", 
    "link": "http://arxiv.org/pdf/1308.0148v1", 
    "arxiv-id": "1308.0148v1"
},{
    "category": "cs.DC", 
    "author": "M A El-dosuky", 
    "title": "Visualization of Job Scheduling in Grid Computers", 
    "publish": "2013-08-02T18:09:23Z", 
    "summary": "One of the hot problems in grid computing is job scheduling. It is known that\nthe job scheduling is NP-complete, and thus the use of heuristics is the de\nfacto approach to deal with this practice in its difficulty. The proposed is an\nimagination to fish swarm, job dispatcher and Visualization gridsim to execute\nsome jobs.", 
    "link": "http://arxiv.org/pdf/1308.0568v1", 
    "arxiv-id": "1308.0568v1"
},{
    "category": "cs.DC", 
    "author": "Odej Kao", 
    "title": "Nephele Streaming: Stream Processing Under QoS Constraints At Scale", 
    "publish": "2013-08-05T16:15:58Z", 
    "summary": "The ability to process large numbers of continuous data streams in a\nnear-real-time fashion has become a crucial prerequisite for many scientific\nand industrial use cases in recent years. While the individual data streams are\nusually trivial to process, their aggregated data volumes easily exceed the\nscalability of traditional stream processing systems. At the same time,\nmassively-parallel data processing systems like MapReduce or Dryad currently\nenjoy a tremendous popularity for data-intensive applications and have proven\nto scale to large numbers of nodes. Many of these systems also provide\nstreaming capabilities. However, unlike traditional stream processors, these\nsystems have disregarded QoS requirements of prospective stream processing\napplications so far. In this paper we address this gap. First, we analyze\ncommon design principles of today's parallel data processing frameworks and\nidentify those principles that provide degrees of freedom in trading off the\nQoS goals latency and throughput. Second, we propose a highly distributed\nscheme which allows these frameworks to detect violations of user-defined QoS\nconstraints and optimize the job execution without manual interaction. As a\nproof of concept, we implemented our approach for our massively-parallel data\nprocessing framework Nephele and evaluated its effectiveness through a\ncomparison with Hadoop Online. For an example streaming application from the\nmultimedia domain running on a cluster of 200 nodes, our approach improves the\nprocessing latency by a factor of at least 13 while preserving high data\nthroughput when needed.", 
    "link": "http://arxiv.org/pdf/1308.1031v1", 
    "arxiv-id": "1308.1031v1"
},{
    "category": "cs.DC", 
    "author": "Shanmugapriyaa", 
    "title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service", 
    "publish": "2013-08-05T06:11:12Z", 
    "summary": "Enterprises are driving towards less cost, more availability, agility,\nmanaged risk - all of which is accelerated towards Cloud Computing. Cloud is\nnot a particular product, but a way of delivering IT services that are\nconsumable on demand, elastic to scale up and down as needed, and follow a\npay-for-usage model. Out of the three common types of cloud computing service\nmodels, Infrastructure as a Service (IaaS) is a service model that provides\nservers, computing power, network bandwidth and Storage capacity, as a service\nto their subscribers. Cloud can relate to many things but without the\nfundamental storage pieces, which is provided as a service namely Cloud\nStorage, none of the other applications is possible. This paper introduces\nCloud Storage, which covers the key technologies in cloud computing and Cloud\nStorage, management insights about cloud computing, different types of cloud\nservices, driving forces of cloud computing and cloud storage, advantages and\nchallenges of cloud storage and concludes by pinpointing few challenges to be\naddressed by the cloud storage providers.", 
    "link": "http://arxiv.org/pdf/1308.1303v1", 
    "arxiv-id": "1308.1303v1"
},{
    "category": "cs.DC", 
    "author": "Erik Schnetter", 
    "title": "Performance and Optimization Abstractions for Large Scale Heterogeneous   Systems in the Cactus/Chemora Framework", 
    "publish": "2013-08-06T16:40:47Z", 
    "summary": "We describe a set of lower-level abstractions to improve performance on\nmodern large scale heterogeneous systems. These provide portable access to\nsystem- and hardware-dependent features, automatically apply dynamic\noptimizations at run time, and target stencil-based codes used in finite\ndifferencing, finite volume, or block-structured adaptive mesh refinement\ncodes.\n  These abstractions include a novel data structure to manage refinement\ninformation for block-structured adaptive mesh refinement, an iterator\nmechanism to efficiently traverse multi-dimensional arrays in stencil-based\ncodes, and a portable API and implementation for explicit SIMD vectorization.\n  These abstractions can either be employed manually, or be targeted by\nautomated code generation, or be used via support libraries by compilers during\ncode generation. The implementations described below are available in the\nCactus framework, and are used e.g. in the Einstein Toolkit for relativistic\nastrophysics simulations.", 
    "link": "http://arxiv.org/pdf/1308.1343v1", 
    "arxiv-id": "1308.1343v1"
},{
    "category": "cs.DC", 
    "author": "Luiz E. Buzato", 
    "title": "The Performance of Paxos and Fast Paxos", 
    "publish": "2013-08-06T17:44:13Z", 
    "summary": "Paxos and Fast Paxos are optimal consensus algorithms that are simple and\nelegant, while suitable for efficient implementation. In this paper, we compare\nthe performance of both algorithms in failure-free and failure-prone runs using\nTreplica, a general replication toolkit that implements these algorithms in a\nmodular and efficient manner. We have found that Paxos outperforms Fast Paxos\nfor small number of replicas and that collisions are not the cause of this\nperformance difference.", 
    "link": "http://arxiv.org/pdf/1308.1358v1", 
    "arxiv-id": "1308.1358v1"
},{
    "category": "cs.DC", 
    "author": "Vipin Tyagi", 
    "title": "Linear Network Coding on Multi-Mesh of Trees (MMT) using All to All   Broadcast (AAB)", 
    "publish": "2013-08-08T06:21:21Z", 
    "summary": "We introduce linear network coding on parallel architecture for multi-source\nfinite acyclic network. In this problem, different messages in diverse time\nperiods are broadcast and every nonsource node in the network decodes and\nencodes the message based on further communication.We wish to minimize the\ncommunication steps and time complexity involved in transfer of data from\nnode-to-node during parallel communication.We have used Multi-Mesh of Trees\n(MMT) topology for implementing network coding. To envisage our result, we use\nall-to-all broadcast as communication algorithm.", 
    "link": "http://arxiv.org/pdf/1308.1763v1", 
    "arxiv-id": "1308.1763v1"
},{
    "category": "cs.DC", 
    "author": "Kuldeep Goswami", 
    "title": "A Survey of Current Trends in Distributed, Grid and Cloud Computing", 
    "publish": "2013-08-08T10:23:09Z", 
    "summary": "Through the 1990s to 2012 the internet changed the world of computing\ndrastically. It started its journey with parallel computing after it advanced\nto distributed computing and further to grid computing. And in present scenario\nit creates a new world which is pronounced as a Cloud Computing [1]. These all\nthree terms have different meanings. Cloud computing is based on backward\ncomputing schemes like cluster computing, distributed computing, grid computing\nand utility computing. The basic concept of cloud computing is virtualization.\nIt provides virtual hardware and software resources to various requesting\nprograms. This paper gives a detailed description about cluster computing, grid\ncomputing and cloud computing and gives an insight of some implementations of\nthe same. We try to list the inspirations for the advent of all these\ntechnologies. We also account for some present scenario faults of grid\ncomputing and also discuss new cloud computing projects which are being managed\nby the Government of India for learning. The paper also reviews the existing\nwork and covers (analytically), to some extent, some innovative ideas that can\nbe implemented.", 
    "link": "http://arxiv.org/pdf/1308.1806v1", 
    "arxiv-id": "1308.1806v1"
},{
    "category": "cs.DC", 
    "author": "Paul H. J. Kelly", 
    "title": "A thread-parallel algorithm for anisotropic mesh adaptation", 
    "publish": "2013-08-12T07:45:15Z", 
    "summary": "Anisotropic mesh adaptation is a powerful way to directly minimise the\ncomputational cost of mesh based simulation. It is particularly important for\nmulti-scale problems where the required number of floating-point operations can\nbe reduced by orders of magnitude relative to more traditional static mesh\napproaches.\n  Increasingly, finite element and finite volume codes are being optimised for\nmodern multi-core architectures. Typically, decomposition methods implemented\nthrough the Message Passing Interface (MPI) are applied for inter-node\nparallelisation, while a threaded programming model, such as OpenMP, is used\nfor intra-node parallelisation. Inter-node parallelism for mesh adaptivity has\nbeen successfully implemented by a number of groups. However, thread-level\nparallelism is significantly more challenging because the underlying data\nstructures are extensively modified during mesh adaptation and a greater degree\nof parallelism must be realised.\n  In this paper we describe a new thread-parallel algorithm for anisotropic\nmesh adaptation algorithms. For each of the mesh optimisation phases\n(refinement, coarsening, swapping and smoothing) we describe how independent\nsets of tasks are defined. We show how a deferred updates strategy can be used\nto update the mesh data structures in parallel and without data contention. We\nshow that despite the complex nature of mesh adaptation and inherent load\nimbalances in the mesh adaptivity, a parallel efficiency of 60% is achieved on\nan 8 core Intel Xeon Sandybridge, and a 40% parallel efficiency is achieved\nusing 16 cores in a 2 socket Intel Xeon Sandybridge ccNUMA system.", 
    "link": "http://arxiv.org/pdf/1308.2480v1", 
    "arxiv-id": "1308.2480v1"
},{
    "category": "cs.DC", 
    "author": "Sriram V. Pemmaraju", 
    "title": "A Super-Fast Distributed Algorithm for Bipartite Metric Facility   Location", 
    "publish": "2013-08-12T20:45:17Z", 
    "summary": "The \\textit{facility location} problem consists of a set of\n\\textit{facilities} $\\mathcal{F}$, a set of \\textit{clients} $\\mathcal{C}$, an\n\\textit{opening cost} $f_i$ associated with each facility $x_i$, and a\n\\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client\n$y_j$. The goal is to find a subset of facilities to \\textit{open}, and to\nconnect each client to an open facility, so as to minimize the total facility\nopening costs plus connection costs. This paper presents the first\nexpected-sub-logarithmic-round distributed O(1)-approximation algorithm in the\n$\\mathcal{CONGEST}$ model for the \\textit{metric} facility location problem on\nthe complete bipartite network with parts $\\mathcal{F}$ and $\\mathcal{C}$. Our\nalgorithm has an expected running time of $O((\\log \\log n)^3)$ rounds, where $n\n= |\\mathcal{F}| + |\\mathcal{C}|$. This result can be viewed as a continuation\nof our recent work (ICALP 2012) in which we presented the first\nsub-logarithmic-round distributed O(1)-approximation algorithm for metric\nfacility location on a \\textit{clique} network. The bipartite setting presents\nseveral new challenges not present in the problem on a clique network. We\npresent two new techniques to overcome these challenges. (i) In order to deal\nwith the problem of not being able to choose appropriate probabilities (due to\nlack of adequate knowledge), we design an algorithm that performs a random walk\nover a probability space and analyze the progress our algorithm makes as the\nrandom walk proceeds. (ii) In order to deal with a problem of quickly\ndisseminating a collection of messages, possibly containing many duplicates,\nover the bipartite network, we design a probabilistic hashing scheme that\ndelivers all of the messages in expected-$O(\\log \\log n)$ rounds.", 
    "link": "http://arxiv.org/pdf/1308.2694v1", 
    "arxiv-id": "1308.2694v1"
},{
    "category": "cs.DC", 
    "author": "Marco Serafini", 
    "title": "On Barriers and the Gap between Active and Passive Replication (Full   Version)", 
    "publish": "2013-08-13T21:09:27Z", 
    "summary": "Active replication is commonly built on top of the atomic broadcast\nprimitive. Passive replication, which has been recently used in the popular\nZooKeeper coordination system, can be naturally built on top of the\nprimary-order atomic broadcast primitive. Passive replication differs from\nactive replication in that it requires processes to cross a barrier before they\nbecome primaries and start broadcasting messages. In this paper, we propose a\nbarrier function tau that explains and encapsulates the differences between\nexisting primary-order atomic broadcast algorithms, namely semi-passive\nreplication and Zookeeper atomic broadcast (Zab), as well as the differences\nbetween Paxos and Zab. We also show that implementing primary-order atomic\nbroadcast on top of a generic consensus primitive and tau inherently results in\nhigher time complexity than atomic broadcast, as witnessed by existing\nalgorithms. We overcome this problem by presenting an alternative,\nprimary-order atomic broadcast implementation that builds on top of a generic\nconsensus primitive and uses consensus itself to form a barrier. This algorithm\nis modular and matches the time complexity of existing tau-based algorithms.", 
    "link": "http://arxiv.org/pdf/1308.2979v5", 
    "arxiv-id": "1308.2979v5"
},{
    "category": "cs.DC", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "title": "On the State and Importance of Reproducible Experimental Research in   Parallel Computing", 
    "publish": "2013-08-16T15:11:35Z", 
    "summary": "Computer science is also an experimental science. This is particularly the\ncase for parallel computing, which is in a total state of flux, and where\nexperiments are necessary to substantiate, complement, and challenge\ntheoretical modeling and analysis. Here, experimental work is as important as\nare advances in theory, that are indeed often driven by the experimental\nfindings. In parallel computing, scientific contributions presented in research\narticles are therefore often based on experimental data, with a substantial\npart devoted to presenting and discussing the experimental findings. As in all\nof experimental science, experiments must be presented in a way that makes\nreproduction by other researchers possible, in principle. Despite appearance to\nthe contrary, we contend that reproducibility plays a small role, and is\ntypically not achieved. As can be found, articles often do not have a\nsufficiently detailed description of their experiments, and do not make\navailable the software used to obtain the claimed results. As a consequence,\nparallel computational results are most often impossible to reproduce, often\nquestionable, and therefore of little or no scientific value. We believe that\nthe description of how to reproduce findings should play an important part in\nevery serious, experiment-based parallel computing research article. We aim to\ninitiate a discussion of the reproducibility issue in parallel computing, and\nelaborate on the importance of reproducible research for (1) better and sounder\ntechnical/scientific papers, (2) a sounder and more efficient review process\nand (3) more effective collective work. This paper expresses our current view\non the subject and should be read as a position statement for discussion and\nfuture work. We do not consider the related (but no less important) issue of\nthe quality of the experimental design.", 
    "link": "http://arxiv.org/pdf/1308.3648v1", 
    "arxiv-id": "1308.3648v1"
},{
    "category": "cs.DC", 
    "author": "Carlos Queiroz", 
    "title": "Patience-aware Scheduling for Cloud Services: Freeing Users from the   Chains of Boredom", 
    "publish": "2013-08-19T20:30:38Z", 
    "summary": "Scheduling of service requests in Cloud computing has traditionally focused\non the reduction of pre-service wait, generally termed as waiting time. Under\ncertain conditions such as peak load, however, it is not always possible to\ngive reasonable response times to all users. This work explores the fact that\ndifferent users may have their own levels of tolerance or patience with\nresponse delays. We introduce scheduling strategies that produce better\nassignment plans by prioritising requests from users who expect to receive the\nresults earlier and by postponing servicing jobs from those who are more\ntolerant to response delays. Our analytical results show that the behaviour of\nusers' patience plays a key role in the evaluation of scheduling techniques,\nand our computational evaluation demonstrates that, under peak load, the new\nalgorithms typically provide better user experience than the traditional FIFO\nstrategy.", 
    "link": "http://arxiv.org/pdf/1308.4166v1", 
    "arxiv-id": "1308.4166v1"
},{
    "category": "cs.DC", 
    "author": "Frederico Durao", 
    "title": "A Systematic Mapping Study on Cloud Computing", 
    "publish": "2013-08-20T02:17:27Z", 
    "summary": "Cloud Computing emerges from the global economic crisis as an option to use\ncomputing resources from a more rational point of view. In other words, a\ncheaper way to have IT resources. However, issues as security and privacy, SLA\n(Service Layer Agreement), resource sharing, and billing has left open\nquestions about the real gains of that model. This study aims to investigate\nstate-of-the-art in Cloud Computing, identify gaps, challenges, synthesize\navailable evidences both its use and development, and provides relevant\ninformation, clarifying open questions and common discussed issues about that\nmodel through literature. The good practices of systematic map- ping study\nmethodology were adopted in order to reach those objectives. Al- though Cloud\nComputing is based on a business model with over 50 years of existence,\nevidences found in this study indicate that Cloud Computing still presents\nlimitations that prevent the full use of the proposal on-demand.", 
    "link": "http://arxiv.org/pdf/1308.4208v1", 
    "arxiv-id": "1308.4208v1"
},{
    "category": "cs.DC", 
    "author": "V. Sangeetha", 
    "title": "Data Grid Concepts for Data Security in Distributed Computing", 
    "publish": "2013-08-28T04:51:08Z", 
    "summary": "Data grid is a distributed computing architecture that integrates a large\nnumber of data and computing resources into a single virtual data management\nsystem. It enables the sharing and coordinated use of data from various\nresources and provides various services to fit the needs of high performance\ndistributed and data-intensive computing. Here data partitioning and dynamic\nreplication in data grid are considered. In which security and access\nperformance of a system are efficient. There are several important requirements\nfor data grids, including information survivability, security, and access\nperformance. More specifically, the investigation is the problem of optimal\nallocation of sensitive data objects that are partitioned by using secret\nsharing scheme or erasure coding scheme and replicated. DATA PARTITIONING is\nknown as the single data can be divided into multiple objects. REPLICATION is\nknown as process of sharing information. storing same data in multiple systems.\nReplication techniques are frequently used to improve data availability. Single\npoint failure does not affect this system. Where the data will be secured.", 
    "link": "http://arxiv.org/pdf/1308.6058v1", 
    "arxiv-id": "1308.6058v1"
},{
    "category": "cs.DC", 
    "author": "Krishnendu Mukhopadhyaya", 
    "title": "Localizability of Wireless Sensor Networks: Beyond Wheel Extension", 
    "publish": "2013-08-29T13:37:44Z", 
    "summary": "A network is called localizable if the positions of all the nodes of the\nnetwork can be computed uniquely. If a network is localizable and embedded in\nplane with generic configuration, the positions of the nodes may be computed\nuniquely in finite time. Therefore, identifying localizable networks is an\nimportant function. If the complete information about the network is available\nat a single place, localizability can be tested in polynomial time. In a\ndistributed environment, networks with trilateration orderings (popular in real\napplications) and wheel extensions (a specific class of localizable networks)\nembedded in plane can be identified by existing techniques. We propose a\ndistributed technique which efficiently identifies a larger class of\nlocalizable networks. This class covers both trilateration and wheel\nextensions. In reality, exact distance is almost impossible or costly. The\nproposed algorithm based only on connectivity information. It requires no\ndistance information.", 
    "link": "http://arxiv.org/pdf/1308.6464v1", 
    "arxiv-id": "1308.6464v1"
},{
    "category": "cs.DC", 
    "author": "Ryan Taylor", 
    "title": "Dynamic web cache publishing for IaaS clouds using Shoal", 
    "publish": "2013-10-31T22:43:01Z", 
    "summary": "We have developed a highly scalable application, called Shoal, for tracking\nand utilizing a distributed set of HTTP web caches. Squid servers advertise\ntheir existence to the Shoal server via AMQP messaging by running Shoal Agent.\nThe Shoal server provides a simple REST interface that allows clients to\ndetermine their closest Squid cache. Our goal is to dynamically instantiate\nSquid caches on IaaS clouds in response to client demand. Shoal provides the\nVMs on IaaS clouds with the location of the nearest dynamically instantiated\nSquid Cache. In this paper, we describe the design and performance of Shoal.", 
    "link": "http://arxiv.org/pdf/1311.0058v1", 
    "arxiv-id": "1311.0058v1"
},{
    "category": "cs.DC", 
    "author": "Sverker Holmgren", 
    "title": "Dynamic autotuning of adaptive fast multipole methods on hybrid   multicore CPU & GPU systems", 
    "publish": "2013-11-05T10:31:57Z", 
    "summary": "We discuss an implementation of adaptive fast multipole methods targeting\nhybrid multicore CPU- and GPU-systems. From previous experiences with the\ncomputational profile of our version of the fast multipole algorithm, suitable\nparts are off-loaded to the GPU, while the remaining parts are threaded and\nexecuted concurrently by the CPU. The parameters defining the algorithm affects\nthe performance and by measuring this effect we are able to dynamically balance\nthe algorithm towards optimal performance. Our setup uses the dynamic nature of\nthe computations and is therefore of general character.", 
    "link": "http://arxiv.org/pdf/1311.1006v3", 
    "arxiv-id": "1311.1006v3"
},{
    "category": "cs.DC", 
    "author": "Marco Luise", 
    "title": "A Chemistry-Inspired Framework for Achieving Consensus in Wireless   Sensor Networks", 
    "publish": "2013-08-27T08:51:45Z", 
    "summary": "The aim of this paper is to show how simple interaction mechanisms, inspired\nby chemical systems, can provide the basic tools to design and analyze a\nmathematical model for achieving consensus in wireless sensor networks,\ncharacterized by balanced directed graphs. The convergence and stability of the\nmodel are first proven by using new mathematical tools, which are borrowed\ndirectly from chemical theory, and then validated by means of simulation\nresults, for different network topologies and number of sensors. The underlying\nchemical theory is also used to derive simple interaction rules that may\naccount for practical issues, such as the estimation of the number of neighbors\nand the robustness against perturbations. Finally, the proposed chemical\nsolution is validated under real-world conditions by means of a four-node\nhardware implementation where the exchange of information among nodes takes\nplace in a distributed manner (with no need for any admission control and\nsynchronism procedure), simply relying on the transmission of a pulse whose\nrate is proportional to the state of each sensor.", 
    "link": "http://arxiv.org/pdf/1311.1084v1", 
    "arxiv-id": "1311.1084v1"
},{
    "category": "cs.DC", 
    "author": "Saurabh Jha", 
    "title": "A Parallel Simulated Annealing Approach for the Mirrored Traveling   Tournament Problem", 
    "publish": "2013-11-08T06:16:43Z", 
    "summary": "The Traveling Tournament Problem (TTP) is a benchmark problem in sports\nscheduling and has been extensively studied in recent years. The Mirrored\nTraveling Tournament Problem (mTTP) is variation of the TTP that represents\ncertain types of sports scheduling problems where the main objective is to\nminimize the total distance traveled by all the participating teams. In this\npaper we test a parallel simulated annealing approach for solving the mTTP\nusing OpenMP on shared memory systems and we found that this approach is\nsuperior especially with respect to the number of solution instances that are\nprobed per second. We also see that there is significant speed up of 1.5x -\n2.2x in terms of number of solutions explored per unit time.", 
    "link": "http://arxiv.org/pdf/1311.1884v1", 
    "arxiv-id": "1311.1884v1"
},{
    "category": "cs.DC", 
    "author": "Ram\u00f3n Beivide", 
    "title": "Symmetric Interconnection Networks from Cubic Crystal Lattices", 
    "publish": "2013-11-08T16:57:34Z", 
    "summary": "Torus networks of moderate degree have been widely used in the supercomputer\nindustry. Tori are superb when used for executing applications that require\nnear-neighbor communications. Nevertheless, they are not so good when dealing\nwith global communications. Hence, typical 3D implementations have evolved to\n5D networks, among other reasons, to reduce network distances. Most of these\nbig systems are mixed-radix tori which are not the best option for minimizing\ndistances and efficiently using network resources. This paper is focused on\nimproving the topological properties of these networks.\n  By using integral matrices to deal with Cayley graphs over Abelian groups, we\nhave been able to propose and analyze a family of high-dimensional grid-based\ninterconnection networks. As they are built over $n$-dimensional grids that\ninduce a regular tiling of the space, these topologies have been denoted\n\\textsl{lattice graphs}. We will focus on cubic crystal lattices for modeling\nsymmetric 3D networks. Other higher dimensional networks can be composed over\nthese graphs, as illustrated in this research. Easy network partitioning can\nalso take advantage of this network composition operation. Minimal routing\nalgorithms are also provided for these new topologies. Finally, some practical\nissues such as implementability and preliminary performance evaluations have\nbeen addressed.", 
    "link": "http://arxiv.org/pdf/1311.2019v1", 
    "arxiv-id": "1311.2019v1"
},{
    "category": "cs.DC", 
    "author": "V. Nicolaou", 
    "title": "Micro-CernVM: Slashing the Cost of Building and Deploying Virtual   Machines", 
    "publish": "2013-11-11T12:22:07Z", 
    "summary": "The traditional virtual machine building and and deployment process is\ncentered around the virtual machine hard disk image. The packages comprising\nthe VM operating system are carefully selected, hard disk images are built for\na variety of different hypervisors, and images have to be distributed and\ndecompressed in order to instantiate a virtual machine. Within the HEP\ncommunity, the CernVM File System has been established in order to decouple the\ndistribution from the experiment software from the building and distribution of\nthe VM hard disk images.\n  We show how to get rid of such pre-built hard disk images altogether. Due to\nthe high requirements on POSIX compliance imposed by HEP application software,\nCernVM-FS can also be used to host and boot a Linux operating system. This\nallows the use of a tiny bootable CD image that comprises only a Linux kernel\nwhile the rest of the operating system is provided on demand by CernVM-FS. This\napproach speeds up the initial instantiation time and reduces virtual machine\nimage sizes by an order of magnitude. Furthermore, security updates can be\ndistributed instantaneously through CernVM-FS. By leveraging the fact that\nCernVM-FS is a versioning file system, a historic analysis environment can be\neasily re-spawned by selecting the corresponding CernVM-FS file system\nsnapshot.", 
    "link": "http://arxiv.org/pdf/1311.2426v1", 
    "arxiv-id": "1311.2426v1"
},{
    "category": "cs.DC", 
    "author": "Dr. T. P. Singh", 
    "title": "The Distributed Computing Paradigms: P2P, Grid, Cluster, Cloud, and   Jungle", 
    "publish": "2013-11-13T10:26:58Z", 
    "summary": "The distributed computing is done on many systems to solve a large scale\nproblem. The growing of high-speed broadband networks in developed and\ndeveloping countries, the continual increase in computing power, and the rapid\ngrowth of the Internet have changed the way. In it the society manages\ninformation and information services. Historically, the state of computing has\ngone through a series of platform and environmental changes. Distributed\ncomputing holds great assurance for using computer systems effectively. As a\nresult, supercomputer sites and data centers have changed from providing high\nperformance floating point computing capabilities to concurrently servicing\nhuge number of requests from billions of users. The distributed computing\nsystem uses multiple computers to solve large-scale problems over the Internet.\nIt becomes data-intensive and network-centric. The applications of distributed\ncomputing have become increasingly wide-spread. In distributed computing, the\nmain stress is on the large scale resource sharing and always goes for the best\nperformance. In this article, we have reviewed the work done in the area of\ndistributed computing paradigms. The main stress is on the evolving area of\ncloud computing.", 
    "link": "http://arxiv.org/pdf/1311.3070v1", 
    "arxiv-id": "1311.3070v1"
},{
    "category": "cs.DC", 
    "author": "Nir Shavit", 
    "title": "Are Lock-Free Concurrent Algorithms Practically Wait-Free?", 
    "publish": "2013-11-13T16:39:40Z", 
    "summary": "Lock-free concurrent algorithms guarantee that some concurrent operation will\nalways make progress in a finite number of steps. Yet programmers prefer to\ntreat concurrent code as if it were wait-free, guaranteeing that all operations\nalways make progress. Unfortunately, designing wait-free algorithms is\ngenerally a very complex task, and the resulting algorithms are not always\nefficient. While obtaining efficient wait-free algorithms has been a long-time\ngoal for the theory community, most non-blocking commercial code is only\nlock-free.\n  This paper suggests a simple solution to this problem. We show that, for a\nlarge class of lock- free algorithms, under scheduling conditions which\napproximate those found in commercial hardware architectures, lock-free\nalgorithms behave as if they are wait-free. In other words, programmers can\nkeep on designing simple lock-free algorithms instead of complex wait-free\nones, and in practice, they will get wait-free progress.\n  Our main contribution is a new way of analyzing a general class of lock-free\nalgorithms under a stochastic scheduler. Our analysis relates the individual\nperformance of processes with the global performance of the system using Markov\nchain lifting between a complex per-process chain and a simpler system progress\nchain. We show that lock-free algorithms are not only wait-free with\nprobability 1, but that in fact a general subset of lock-free algorithms can be\nclosely bounded in terms of the average number of steps required until an\noperation completes.\n  To the best of our knowledge, this is the first attempt to analyze progress\nconditions, typically stated in relation to a worst case adversary, in a\nstochastic model capturing their expected asymptotic behavior.", 
    "link": "http://arxiv.org/pdf/1311.3200v2", 
    "arxiv-id": "1311.3200v2"
},{
    "category": "cs.DC", 
    "author": "Amos Korman", 
    "title": "Breathe before Speaking: Efficient Information Dissemination Despite   Noisy, Limited and Anonymous Communication", 
    "publish": "2013-11-14T09:10:15Z", 
    "summary": "Distributed computing models typically assume reliable communication between\nprocessors. While such assumptions often hold for engineered networks, e.g.,\ndue to underlying error correction protocols, their relevance to biological\nsystems, wherein messages are often distorted before reaching their\ndestination, is quite limited. In this study we take a first step towards\nreducing this gap by rigorously analyzing a model of communication in large\nanonymous populations composed of simple agents which interact through short\nand highly unreliable messages.\n  We focus on the broadcast problem and the majority-consensus problem. Both\nare fundamental information dissemination problems in distributed computing, in\nwhich the goal of agents is to converge to some prescribed desired opinion. We\ninitiate the study of these problems in the presence of communication noise.\nOur model for communication is extremely weak and follows the push gossip\ncommunication paradigm: In each round each agent that wishes to send\ninformation delivers a message to a random anonymous agent. This communication\nis further restricted to contain only one bit (essentially representing an\nopinion). Lastly, the system is assumed to be so noisy that the bit in each\nmessage sent is flipped independently with probability $1/2-\\epsilon$, for some\nsmall $\\epsilon >0$.\n  Even in this severely restricted, stochastic and noisy setting we give\nnatural protocols that solve the noisy broadcast and the noisy\nmajority-consensus problems efficiently. Our protocols run in $O(\\log n /\n\\epsilon^2)$ rounds and use $O(n \\log n / \\epsilon^2)$ messages/bits in total,\nwhere $n$ is the number of agents. These bounds are asymptotically optimal and,\nin fact, are as fast and message efficient as if each agent would have been\nsimultaneously informed directly by an agent that knows the prescribed desired\nopinion.", 
    "link": "http://arxiv.org/pdf/1311.3425v3", 
    "arxiv-id": "1311.3425v3"
},{
    "category": "cs.DC", 
    "author": "Qihui Wu", 
    "title": "Big Data Analytics in Future Internet of Things", 
    "publish": "2013-11-17T03:03:36Z", 
    "summary": "Current research on Internet of Things (IoT) mainly focuses on how to enable\ngeneral objects to see, hear, and smell the physical world for themselves, and\nmake them connected to share the observations. In this paper, we argue that\nonly connected is not enough, beyond that, general objects should have the\ncapability to learn, think, and understand both the physical world by\nthemselves. On the other hand, the future IoT will be highly populated by large\nnumbers of heterogeneous networked embedded devices, which are generating\nmassive or big data in an explosive fashion. Although there is a consensus\namong almost everyone on the great importance of big data analytics in IoT, to\ndate, limited results, especially the mathematical foundations, are obtained.\nThese practical needs impels us to propose a systematic tutorial on the\ndevelopment of effective algorithms for big data analytics in future IoT, which\nare grouped into four classes: 1) heterogeneous data processing, 2) nonlinear\ndata processing, 3) high-dimensional data processing, and 4) distributed and\nparallel data processing. We envision that the presented research is offered as\na mere baby step in a potentially fruitful research direction. We hope that\nthis article, with interdisciplinary perspectives, will stimulate more\ninterests in research and development of practical and effective algorithms for\nspecific IoT applications, to enable smart resource allocation, automatic\nnetwork operation, and intelligent service provisioning.", 
    "link": "http://arxiv.org/pdf/1311.4112v1", 
    "arxiv-id": "1311.4112v1"
},{
    "category": "cs.DC", 
    "author": "Bernd Burgstaller", 
    "title": "Dynamic Partitioning-based JPEG Decompression on Heterogeneous Multicore   Architectures", 
    "publish": "2013-11-21T03:49:00Z", 
    "summary": "With the emergence of social networks and improvements in computational\nphotography, billions of JPEG images are shared and viewed on a daily basis.\nDesktops, tablets and smartphones constitute the vast majority of hardware\nplatforms used for displaying JPEG images. Despite the fact that these\nplatforms are heterogeneous multicores, no approach exists yet that is capable\nof joining forces of a system's CPU and GPU for JPEG decoding. In this paper we\nintroduce a novel JPEG decoding scheme for heterogeneous architectures\nconsisting of a CPU and an OpenCL-programmable GPU. We employ an offline\nprofiling step to determine the performance of a system's CPU and GPU with\nrespect to JPEG decoding. For a given JPEG image, our performance model uses\n(1) the CPU and GPU performance characteristics, (2) the image entropy and (3)\nthe width and height of the image to balance the JPEG decoding workload on the\nunderlying hardware. Our run-time partitioning and scheduling scheme exploits\ntask, data and pipeline parallelism by scheduling the non-parallelizable\nentropy decoding task on the CPU, whereas inverse cosine transformations\n(IDCTs), color conversions and upsampling are conducted on both the CPU and the\nGPU. Our kernels have been optimized for GPU memory hierarchies. We have\nimplemented the proposed method in the context of the libjpeg-turbo library,\nwhich is an industrial-strength JPEG encoding and decoding engine.\nLibjpeg-turbo's hand-optimized SIMD routines for ARM and x86 constitute a\ncompetitive yardstick for the comparison to the proposed approach.\nRetro-fitting our method with libjpeg-turbo provided insights on the\nsoftware-engineering aspects of re-engineering legacy code for heterogeneous\nmulticores.", 
    "link": "http://arxiv.org/pdf/1311.5304v3", 
    "arxiv-id": "1311.5304v3"
},{
    "category": "cs.DC", 
    "author": "M. Zoll", 
    "title": "The IceProd Framework: Distributed Data Processing for the IceCube   Neutrino Observatory", 
    "publish": "2013-11-22T21:16:58Z", 
    "summary": "IceCube is a one-gigaton instrument located at the geographic South Pole,\ndesigned to detect cosmic neutrinos, iden- tify the particle nature of dark\nmatter, and study high-energy neutrinos themselves. Simulation of the IceCube\ndetector and processing of data require a significant amount of computational\nresources. IceProd is a distributed management system based on Python, XML-RPC\nand GridFTP. It is driven by a central database in order to coordinate and\nadmin- ister production of simulations and processing of data produced by the\nIceCube detector. IceProd runs as a separate layer on top of other middleware\nand can take advantage of a variety of computing resources, including grids and\nbatch systems such as CREAM, Condor, and PBS. This is accomplished by a set of\ndedicated daemons that process job submission in a coordinated fashion through\nthe use of middleware plugins that serve to abstract the details of job\nsubmission and job management from the framework.", 
    "link": "http://arxiv.org/pdf/1311.5904v3", 
    "arxiv-id": "1311.5904v3"
},{
    "category": "cs.DC", 
    "author": "Viktor Prasanna", 
    "title": "GoFFish: A Sub-Graph Centric Framework for Large-Scale Graph Analytics", 
    "publish": "2013-11-23T02:53:58Z", 
    "summary": "Large scale graph processing is a major research area for Big Data\nexploration. Vertex centric programming models like Pregel are gaining traction\ndue to their simple abstraction that allows for scalable execution on\ndistributed systems naturally. However, there are limitations to this approach\nwhich cause vertex centric algorithms to under-perform due to poor compute to\ncommunication overhead ratio and slow convergence of iterative superstep. In\nthis paper we introduce GoFFish a scalable sub-graph centric framework\nco-designed with a distributed persistent graph storage for large scale graph\nanalytics on commodity clusters. We introduce a sub-graph centric programming\nabstraction that combines the scalability of a vertex centric approach with the\nflexibility of shared memory sub-graph computation. We map Connected\nComponents, SSSP and PageRank algorithms to this model to illustrate its\nflexibility. Further, we empirically analyze GoFFish using several real world\ngraphs and demonstrate its significant performance improvement, orders of\nmagnitude in some cases, compared to Apache Giraph, the leading open source\nvertex centric implementation.", 
    "link": "http://arxiv.org/pdf/1311.5949v1", 
    "arxiv-id": "1311.5949v1"
},{
    "category": "cs.DC", 
    "author": "Fernando Pedone", 
    "title": "Rethinking State-Machine Replication for Parallelism", 
    "publish": "2013-11-24T22:45:59Z", 
    "summary": "State-machine replication, a fundamental approach to designing fault-tolerant\nservices, requires commands to be executed in the same order by all replicas.\nMoreover, command execution must be deterministic: each replica must produce\nthe same output upon executing the same sequence of commands. These\nrequirements usually result in single-threaded replicas, which hinders service\nperformance. This paper introduces Parallel State-Machine Replication (P-SMR),\na new approach to parallelism in state-machine replication. P-SMR scales better\nthan previous proposals since no component plays a centralizing role in the\nexecution of independent commands---those that can be executed concurrently, as\ndefined by the service. The paper introduces P-SMR, describes a \"commodified\narchitecture\" to implement it, and compares its performance to other proposals\nusing a key-value store and a networked file system.", 
    "link": "http://arxiv.org/pdf/1311.6183v1", 
    "arxiv-id": "1311.6183v1"
},{
    "category": "cs.DC", 
    "author": "Frank Mueller", 
    "title": "Evaluating the Impact of SDC on the GMRES Iterative Solver", 
    "publish": "2013-11-25T22:19:39Z", 
    "summary": "Increasing parallelism and transistor density, along with increasingly\ntighter energy and peak power constraints, may force exposure of occasionally\nincorrect computation or storage to application codes. Silent data corruption\n(SDC) will likely be infrequent, yet one SDC suffices to make numerical\nalgorithms like iterative linear solvers cease progress towards the correct\nanswer. Thus, we focus on resilience of the iterative linear solver GMRES to a\nsingle transient SDC. We derive inexpensive checks to detect the effects of an\nSDC in GMRES that work for a more general SDC model than presuming a bit flip.\nOur experiments show that when GMRES is used as the inner solver of an\ninner-outer iteration, it can \"run through\" SDC of almost any magnitude in the\ncomputationally intensive orthogonalization phase. That is, it gets the right\nanswer using faulty data without any required roll back. Those SDCs which it\ncannot run through, get caught by our detection scheme.", 
    "link": "http://arxiv.org/pdf/1311.6505v1", 
    "arxiv-id": "1311.6505v1"
},{
    "category": "cs.DC", 
    "author": "Yoram Moses", 
    "title": "Good, Better, Best! - Unbeatable Protocols for Consensus and Set   Consensus", 
    "publish": "2013-11-27T09:13:43Z", 
    "summary": "While the very first consensus protocols for the synchronous model were\ndesigned to match the worst-case lower bound, deciding in exactly t+1 rounds in\nall runs, it was soon realized that they could be strictly improved upon by\nearly stopping protocols. These dominate the first ones, by always deciding in\nat most t+1 rounds, but often much faster. A protocol is unbeatable if it can't\nbe strictly dominated. Unbeatability is often a much more suitable notion of\noptimality for distributed protocols than worst-case performance. Using a\nknowledge-based analysis, this paper studies unbeatability for both consensus\nand k-set consensus. We present unbeatable solutions to non-uniform consensus\nand k-set consensus, and uniform consensus in synchronous message-passing\ncontexts with crash failures.\n  The k-set consensus problem is much more technically challenging than\nconsensus, and its analysis has triggered the development of the topological\napproach to distributed computing. Worst-case lower bounds for this problem\nhave required either techniques based on algebraic topology, or reduction-based\nproofs. Our proof of unbeatability is purely combinatorial, and is a direct,\nalbeit nontrivial, generalization of the one for consensus. We also present an\nalternative topological unbeatability proof that allows to understand the\nconnection between the connectivity of protocol complexes and the decision time\nof processes.\n  For the synchronous model, only solutions to the uniform variant of k-set\nconsensus have been offered. Based on our unbeatable protocols for uniform\nconsensus and for non-uniform k-set consensus, we present a uniform k-set\nconsensus protocol that strictly dominates all known solutions to this problem\nin the synchronous model.", 
    "link": "http://arxiv.org/pdf/1311.6902v1", 
    "arxiv-id": "1311.6902v1"
},{
    "category": "cs.DC", 
    "author": "Nagesh N. Jadhav", 
    "title": "QoS Based Framework for Effective Web Services in Cloud Computing", 
    "publish": "2013-11-28T05:16:09Z", 
    "summary": "Enhancements in technology always follow Consumer requirements. Consumer\nrequires best of service with least possible mismatch and on time. Numerous\napplications available today are based on Web Services and Cloud Computing.\nRecently, there exist many Web Services with similar functional\ncharacteristics. Choosing a right Service from group of similar Web Service is\na complicated task for Service Consumer. In that case, Service Consumer can\ndiscover the required Web Service using non functional attributes of the Web\nServices such as QoS. Proposed layered architecture and Web Service Cloud\ni.e.WS Cloud computing Framework synthesizes the Non functional attributes that\nincludes reliability, availability, response time, latency etc. The Service\nConsumer is projected to provide the QoS requirements as part of Service\ndiscovery query. This framework will discover and filter the Web Services form\nthe cloud and rank them according to Service Consumer preferences to facilitate\nService on time.", 
    "link": "http://arxiv.org/pdf/1311.7210v1", 
    "arxiv-id": "1311.7210v1"
},{
    "category": "cs.DC", 
    "author": "Jussi Kangasharju", 
    "title": "LiteLab: Efficient Large-scale Network Experiments", 
    "publish": "2013-11-28T20:56:09Z", 
    "summary": "Large-scale network experiments is a challenging problem. Simulations,\nemulations, and real-world testbeds all have their advantages and\ndisadvantages. In this paper we present LiteLab, a light-weight platform\nspecialized for large-scale networking experiments. We cover in detail its\ndesign, key features, and architecture. We also perform an extensive evaluation\nof LiteLab's performance and accuracy and show that it is able to both simulate\nnetwork parameters with high accuracy, and also able to scale up to very large\nnetworks. LiteLab is flexible, easy to deploy, and allows researchers to\nperform large-scale network experiments with a short development cycle. We have\nused LiteLab for many different kinds of network experiments and are planning\nto make it available for others to use as well.", 
    "link": "http://arxiv.org/pdf/1311.7422v1", 
    "arxiv-id": "1311.7422v1"
},{
    "category": "cs.DC", 
    "author": "Jussi Kangasharju", 
    "title": "Experimenting with BitTorrent on a Cluster: A Good or a Bad Idea?", 
    "publish": "2013-11-28T22:34:11Z", 
    "summary": "Evaluation of large-scale network systems and applications is usually done in\none of three ways: simulations, real deployment on Internet, or on an emulated\nnetwork testbed such as a cluster. Simulations can study very large systems but\noften abstract out many practical details, whereas real world tests are often\nquite small, on the order of a few hundred nodes at most, but have very\nrealistic conditions. Clusters and other dedicated testbeds offer a middle\nground between the two: large systems with real application code. They also\ntypically allow configuring the testbed to enable repeatable experiments. In\nthis paper we explore how to run large BitTorrent experiments in a cluster\nsetup. We have chosen BitTorrent because the source code is available and it\nhas been a popular target for research. Our contribution is twofold. First, we\nshow how to tweak and configure the BitTorrent client to allow for a maximum\nnumber of clients to be run on a single machine, without running into any\nphysical limits of the machine. Second, our results show that the behavior of\nBitTorrent can be very sensitive to the configuration and we revisit some\nexisting BitTorrent research and consider the implications of our findings on\npreviously published results. As we show in this paper, BitTorrent can change\nits behavior in subtle ways which are sometimes ignored in published works.", 
    "link": "http://arxiv.org/pdf/1311.7435v1", 
    "arxiv-id": "1311.7435v1"
},{
    "category": "cs.DC", 
    "author": "Yue Zhang", 
    "title": "Constructing Gazetteers from Volunteered Big Geo-Data Based on Hadoop", 
    "publish": "2013-11-29T19:52:42Z", 
    "summary": "Traditional gazetteers are built and maintained by authoritative mapping\nagencies. In the age of Big Data, it is possible to construct gazetteers in a\ndata-driven approach by mining rich volunteered geographic information (VGI)\nfrom the Web. In this research, we build a scalable distributed platform and a\nhigh-performance geoprocessing workflow based on the Hadoop ecosystem to\nharvest crowd-sourced gazetteer entries. Using experiments based on geotagged\ndatasets in Flickr, we find that the MapReduce-based workflow running on the\nspatially enabled Hadoop cluster can reduce the processing time compared with\ntraditional desktop-based operations by an order of magnitude. We demonstrate\nhow to use such a novel spatial-computing infrastructure to facilitate\ngazetteer research. In addition, we introduce a provenance-based trust model\nfor quality assurance. This work offers new insights on enriching future\ngazetteers with the use of Hadoop clusters, and makes contributions in\nconnecting GIS to the cloud computing environment for the next frontier of Big\nGeo-Data analytics.", 
    "link": "http://arxiv.org/pdf/1311.7676v2", 
    "arxiv-id": "1311.7676v2"
},{
    "category": "cs.DC", 
    "author": "Ning Xie", 
    "title": "A Many-core Machine Model for Designing Algorithms with Minimum   Parallelism Overheads", 
    "publish": "2014-02-03T00:22:59Z", 
    "summary": "We present a model of multithreaded computation, combining fork-join and\nsingle-instruction-multiple-data parallelisms, with an emphasis on estimating\nparallelism overheads of programs written for modern many-core architectures.\nWe establish a Graham-Brent theorem for this model so as to estimate execution\ntime of programs running on a given number of streaming multiprocessors. We\nevaluate the benefits of our model with four fundamental algorithms from\nscientific computing. In each case, our model is used to minimize parallelism\noverheads by determining an appropriate value range for a given program\nparameter; moreover experimentation confirms the model's prediction.", 
    "link": "http://arxiv.org/pdf/1402.0264v1", 
    "arxiv-id": "1402.0264v1"
},{
    "category": "cs.DC", 
    "author": "Mohammed Bakri Bashir", 
    "title": "On-Demand Grid Provisioning Using Cloud Infrastructures and Related   Virtualization Tools: A Survey and Taxonomy", 
    "publish": "2014-02-04T11:27:56Z", 
    "summary": "Recent researches have shown that grid resources can be accessed by client\non-demand, with the help of virtualization technology in the Cloud. The virtual\nmachines hosted by the hypervisors are being utilized to build the grid network\nwithin the cloud environment. The aim of this study is to survey some concepts\nused for the on-demand grid provisioning using Infrastructure as a Service\nCloud and the taxonomy of its related components. This paper, discusses the\ndifferent approaches for on-demand grid using infrastructural Cloud, the issues\nit tries to address and the implementation tools. The paper also, proposed an\nextended classification for the virtualization technology used and a new\nclassification for the Grid-Cloud integration which was based on the\narchitecture, communication flow and the user demand for the Grid resources.\nThis survey, tools and taxonomies presented here will contribute as a guide in\nthe design of future architectures for further researches.", 
    "link": "http://arxiv.org/pdf/1402.0696v1", 
    "arxiv-id": "1402.0696v1"
},{
    "category": "cs.DC", 
    "author": "Dejan Milojicic", 
    "title": "Backtracking algorithms for service selection", 
    "publish": "2014-02-06T10:38:26Z", 
    "summary": "In this paper, we explore the automation of services' compositions. We focus\non the service selection problem. In the formulation that we consider, the\nproblem's inputs are constituted by a behavioral composition whose abstract\nservices must be bound to concrete ones. The objective is to find the binding\nthat optimizes the {\\it utility} of the composition under some services level\nagreements. We propose a complete solution. Firstly, we show that the service\nselection problem can be mapped onto a Constraint Satisfaction Problem (CSP).\nThe benefit of this mapping is that the large know-how in the resolution of the\nCSP can be used for the service selection problem. Among the existing\ntechniques for solving CSP, we consider the backtracking. Our second\ncontribution is to propose various backtracking-based algorithms for the\nservice selection problem. The proposed variants are inspired by existing\nheuristics for the CSP. We analyze the runtime gain of our framework over an\nintuitive resolution based on exhaustive search. Our last contribution is an\nexperimental evaluation in which we demonstrate that there is an effective gain\nin using backtracking instead of some comparable approaches. The experiments\nalso show that our proposal can be used for finding in real time, optimal\nsolutions on small and medium services' compositions.", 
    "link": "http://arxiv.org/pdf/1402.1309v1", 
    "arxiv-id": "1402.1309v1"
},{
    "category": "cs.DC", 
    "author": "Prince Jain", 
    "title": "An EMUSIM Technique and its Components in Cloud Computing- A Review", 
    "publish": "2014-02-09T10:10:48Z", 
    "summary": "Recent efforts to design and develop Cloud technologies focus on defining\nnovel methods, policies and mechanisms for efficiently managing Cloud\ninfrastructures. One key challenge potential Cloud customers have before\nrenting resources is to know how their services will behave in a set of\nresources and the costs involved when growing and shrinking their resource\npool. Most of the studies in this area rely on simulation-based experiments,\nwhich consider simplified modeling of applications and computing environment.\nIn order to better predict service's behavior on Cloud platforms, an integrated\narchitecture that is based on both simulation and emulation. The proposed\narchitecture, named EMUSIM, automatically extracts information from application\nbehavior via emulation and then uses this information to generate the\ncorresponding simulation model. This paper presents brief overview of the\nEMUSIM technique and its components. The work in this paper focuses on\narchitecture and operation details of Automated Emulation Framework (AEF),\nQAppDeployer and proposes Cloud Sim Application for Simulation techniques.", 
    "link": "http://arxiv.org/pdf/1402.1932v1", 
    "arxiv-id": "1402.1932v1"
},{
    "category": "cs.DC", 
    "author": "Krzysztof Rzadca", 
    "title": "We Are Impatient: Algorithms for Geographically Distributed Load   Balancing with (Almost) Arbitrary Load Functions", 
    "publish": "2014-02-10T10:33:52Z", 
    "summary": "In geographically-distributed systems, communication latencies are\nnon-negligible. The perceived processing time of a request is thus composed of\nthe time needed to route the request to the server and the true processing\ntime. Once a request reaches a target server, the processing time depends on\nthe total load of that server; this dependency is described by a load function.\nWe consider a broad class of load functions; we just require that they are\nconvex and two times differentiable. In particular our model can be applied to\nheterogeneous systems in which every server has a different load function. This\napproach allows us not only to generalize results for queuing theory and for\nbatches of requests, but also to use empirically-derived load functions,\nmeasured in a system under stress-testing. The optimal assignment of requests\nto servers is communication-balanced, i.e. for any pair of non\nperfectly-balanced servers, the reduction of processing time resulting from\nmoving a single request from the overloaded to underloaded server is smaller\nthan the additional communication latency. We present a centralized and a\ndecentralized algorithm for optimal load balancing. We prove bounds on the\nalgorithms' convergence. To the best of our knowledge these bounds were not\nknown even for the special cases studied previously (queuing theory and batches\nof requests). Both algorithms are any-time algorithms. In the decentralized\nalgorithm, each server balances the load with a randomly chosen peer. Such\nalgorithm is very robust to failures. We prove that the decentralized algorithm\nperforms locally-optimal steps. Our work extends the currently known results by\nconsidering a broad class of load functions and by establishing theoretical\nbounds on the algorithms' convergence. These results are applicable for servers\nwhose characteristics under load cannot be described by a standard mathematical\nmodels.", 
    "link": "http://arxiv.org/pdf/1402.2090v1", 
    "arxiv-id": "1402.2090v1"
},{
    "category": "cs.DC", 
    "author": "Pietro Michiardi", 
    "title": "OS-Assisted Task Preemption for Hadoop", 
    "publish": "2014-02-10T11:14:19Z", 
    "summary": "This work introduces a new task preemption primitive for Hadoop, that allows\ntasks to be suspended and resumed exploiting existing memory management\nmechanisms readily available in modern operating systems. Our technique fills\nthe gap that exists between the two extremes cases of killing tasks (which\nwaste work) or waiting for their completion (which introduces latency):\nexperimental results indicate superior performance and very small overheads\nwhen compared to existing alternatives.", 
    "link": "http://arxiv.org/pdf/1402.2107v1", 
    "arxiv-id": "1402.2107v1"
},{
    "category": "cs.DC", 
    "author": "K. Saravanan", 
    "title": "Optimizing the Cost for Resource Subscription Policy in IaaS Cloud", 
    "publish": "2014-02-11T14:11:18Z", 
    "summary": "Cloud computing allow the users to efficiently and dynamically provision\ncomputing resource to meet their IT needs. Cloud Provider offers two\nsubscription plan to the customer namely reservation and on-demand. The\nreservation plan is typically cheaper than on-demand plan. If the actual\ncomputing demand is known in advance reserving the resource would be\nstraightforward. The challenge is how to make properly resource provisioning\nand how the customers efficiently purchase the provisioning options under\nreservation and on-demand. To address this issue, two-phase algorithm are\nproposed to minimize service provision cost in both reservation and on-demand\nplan. To reserve the correct and optimal amount of resources during\nreservation, proposed a mathematical formulae in the first phase. To predict\nresource demand, use kalman filter in the second phase. The evaluation result\nshows that the two-phase algorithm can significantly reduce the provision cost\nand the prediction is of reasonable accuracy.", 
    "link": "http://arxiv.org/pdf/1402.2491v1", 
    "arxiv-id": "1402.2491v1"
},{
    "category": "cs.DC", 
    "author": "Jukka Suomela", 
    "title": "Linial's Lower Bound Made Easy", 
    "publish": "2014-02-11T16:33:07Z", 
    "summary": "Linial's seminal result shows that any deterministic distributed algorithm\nthat finds a $3$-colouring of an $n$-cycle requires at least $\\log^*(n)/2 - 1$\ncommunication rounds. We give a new simpler proof of this theorem.", 
    "link": "http://arxiv.org/pdf/1402.2552v1", 
    "arxiv-id": "1402.2552v1"
},{
    "category": "cs.DC", 
    "author": "Georgios Zois", 
    "title": "Energy Efficient Scheduling of MapReduce Jobs", 
    "publish": "2014-02-12T13:16:16Z", 
    "summary": "MapReduce is emerged as a prominent programming model for data-intensive\ncomputation. In this work, we study power-aware MapReduce scheduling in the\nspeed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on\nthe minimization of the total weighted completion time of a set of MapReduce\njobs under a given budget of energy. Using a linear programming relaxation of\nour problem, we derive a polynomial time constant-factor approximation\nalgorithm. We also propose a convex programming formulation that we combine\nwith standard list scheduling policies, and we evaluate their performance using\nsimulations.", 
    "link": "http://arxiv.org/pdf/1402.2810v1", 
    "arxiv-id": "1402.2810v1"
},{
    "category": "cs.DC", 
    "author": "Inderveer Chana", 
    "title": "Formal Specification Language Based IaaS Cloud Workload Regression   Analysis", 
    "publish": "2014-02-13T05:24:37Z", 
    "summary": "Cloud Computing is an emerging area for accessing computing resources. In\ngeneral, Cloud service providers offer services that can be clustered into\nthree categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload\nanalysis. The efficient Cloud workload resource mapping technique is proposed.\nThis paper aims to provide a means of understanding and investigating IaaS\nCloud workloads and the resources. In this paper, regression analysis is used\nto analyze the Cloud workloads and identifies the relationship between Cloud\nworkloads and available resources. The effective organization of dynamic nature\nresources can be done with the help of Cloud workloads. Till Cloud workload is\nconsidered a vital talent, the Cloud resources cannot be consumed in an\neffective style. The proposed technique has been validated by Z Formal\nspecification language. This approach is effective in minimizing the cost and\nsubmission burst time of Cloud workloads.", 
    "link": "http://arxiv.org/pdf/1402.3034v1", 
    "arxiv-id": "1402.3034v1"
},{
    "category": "cs.DC", 
    "author": "Michael L. Nelson", 
    "title": "Real-Time Notification for Resource Synchronization", 
    "publish": "2014-02-11T21:04:54Z", 
    "summary": "Web applications frequently leverage resources made available by remote web\nservers. As resources are created, updated, deleted, or moved, these\napplications face challenges to remain in lockstep with the server's change\ndynamics. Several approaches exist to help meet this challenge for use cases\nwhere \"good enough\" synchronization is acceptable. But when strict resource\ncoverage or low synchronization latency is required, commonly accepted\nWeb-based solutions remain elusive. This paper details characteristics of an\napproach that aims at decreasing synchronization latency while maintaining\ndesired levels of accuracy. The approach builds on pushing change notifications\nand pulling changed resources and it is explored with an experiment based on a\nDBpedia Live instance.", 
    "link": "http://arxiv.org/pdf/1402.3305v1", 
    "arxiv-id": "1402.3305v1"
},{
    "category": "cs.DC", 
    "author": "Nicola Zannone", 
    "title": "Flow-based reputation with uncertainty: Evidence-Based Subjective Logic", 
    "publish": "2014-02-13T21:49:52Z", 
    "summary": "The concept of reputation is widely used as a measure of trustworthiness\nbased on ratings from members in a community. The adoption of reputation\nsystems, however, relies on their ability to capture the actual trustworthiness\nof a target. Several reputation models for aggregating trust information have\nbeen proposed in the literature. The choice of model has an impact on the\nreliability of the aggregated trust information as well as on the procedure\nused to compute reputations. Two prominent models are flow-based reputation\n(e.g., EigenTrust, PageRank) and Subjective Logic based reputation. Flow-based\nmodels provide an automated method to aggregate trust information, but they are\nnot able to express the level of uncertainty in the information. In contrast,\nSubjective Logic extends probabilistic models with an explicit notion of\nuncertainty, but the calculation of reputation depends on the structure of the\ntrust network and often requires information to be discarded. These are severe\ndrawbacks.\n  In this work, we observe that the `opinion discounting' operation in\nSubjective Logic has a number of basic problems. We resolve these problems by\nproviding a new discounting operator that describes the flow of evidence from\none party to another. The adoption of our discounting rule results in a\nconsistent Subjective Logic algebra that is entirely based on the handling of\nevidence. We show that the new algebra enables the construction of an automated\nreputation assessment procedure for arbitrary trust networks, where the\ncalculation no longer depends on the structure of the network, and does not\nneed to throw away any information. Thus, we obtain the best of both worlds:\nflow-based reputation and consistent handling of uncertainties.", 
    "link": "http://arxiv.org/pdf/1402.3319v2", 
    "arxiv-id": "1402.3319v2"
},{
    "category": "cs.DC", 
    "author": "M H Habaebi", 
    "title": "A Framework for Developing Real-Time OLAP algorithm using Multi-core   processing and GPU: Heterogeneous Computing", 
    "publish": "2014-02-16T10:11:57Z", 
    "summary": "The overwhelmingly increasing amount of stored data has spurred researchers\nseeking different methods in order to optimally take advantage of it which\nmostly have faced a response time problem as a result of this enormous size of\ndata. Most of solutions have suggested materialization as a favourite solution.\nHowever, such a solution cannot attain Real- Time answers anyhow. In this paper\nwe propose a framework illustrating the barriers and suggested solutions in the\nway of achieving Real-Time OLAP answers that are significantly used in decision\nsupport systems and data warehouses.", 
    "link": "http://arxiv.org/pdf/1402.3781v1", 
    "arxiv-id": "1402.3781v1"
},{
    "category": "cs.DC", 
    "author": "Natalya Litvinenko", 
    "title": "Using of GPUs for cluster analysis of large data by K-means method", 
    "publish": "2014-02-16T11:36:39Z", 
    "summary": "This problem was solved within the framework of the grant project \"Solving of\nproblems of cluster analysis with application of parallel algorithms and cloud\ntechnologies\" in the Institute of Mathematics and Mathematical Modelling in\nAlmaty. The problem of cluster analysis for the large amount of data is very\nimportant in different areas of science - genetics, biology, sociology etc. At\nthe same time, such statistical known packages as STATISTICA, STADIA, SYSTAT\nand others do not allow to solve large problems. The new algorithm that uses\nthe high processing power of GPUs for solving clustering problems by the\nK-means method was developed. This algorithm is implemented as a C++\napplication in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce\n660. The developed software package for solving clustering problems by the\nmethod of K - means with using GPUs allows us to handle up to 2 million records\nwith number of features up to 25. The gain in the computing time is in factor\n5. We plan to increase this factor up to 20-30 after improving the algorithms.", 
    "link": "http://arxiv.org/pdf/1402.3788v1", 
    "arxiv-id": "1402.3788v1"
},{
    "category": "cs.DC", 
    "author": "Natalya Litvinenko", 
    "title": "Parallel algorithms for problems of cluster analysis with very large   amount of data", 
    "publish": "2014-02-16T11:53:30Z", 
    "summary": "In this paper we solve on GPUs massive problems with large amount of data,\nwhich are not appropriate for solution with the SIMD technology. For the given\nproblem we consider a three-level parallelization. The multithreading of CPU is\nused at the top level and graphic processors for massive computing. For solving\nproblems of cluster analysis on GPUs the nearest neighbor method (NNM) is\ndeveloped. This algorithm allows us to handle up to 2 millions records with\nnumber of features up to 25. Since sequential and parallel algorithms are\nfundamentally different, it is difficult to compare the computation times.\nHowever, some comparisons are made. The gain in the computing time is about 10\ntimes. We plan to increase this factor up to 50-100 after fine tuning of\nalgorithms.", 
    "link": "http://arxiv.org/pdf/1402.3789v1", 
    "arxiv-id": "1402.3789v1"
},{
    "category": "cs.DC", 
    "author": "Peter Koro\u0161ec", 
    "title": "A GRASS GIS parallel module for radio-propagation predictions", 
    "publish": "2014-02-17T14:10:12Z", 
    "summary": "Geographical information systems are ideal candidates for the application of\nparallel programming techniques, mainly because they usually handle large data\nsets. To help us deal with complex calculations over such data sets, we\ninvestigated the performance constraints of a classic master-worker parallel\nparadigm over a message-passing communication model. To this end, we present a\nnew approach that employs an external database in order to improve the\ncalculation/communication overlap, thus reducing the idle times for the worker\nprocesses. The presented approach is implemented as part of a parallel\nradio-coverage prediction tool for the GRASS environment. The prediction\ncalculation employs digital elevation models and land-usage data in order to\nanalyze the radio coverage of a geographical area. We provide an extended\nanalysis of the experimental results, which are based on real data from an LTE\nnetwork currently deployed in Slovenia. Based on the results of the\nexperiments, which were performed on a computer cluster, the new approach\nexhibits better scalability than the traditional master-worker approach. We\nsuccessfully tackled real-world data sets, while greatly reducing the\nprocessing time and saturating the hardware utilization.", 
    "link": "http://arxiv.org/pdf/1402.4010v1", 
    "arxiv-id": "1402.4010v1"
},{
    "category": "cs.DC", 
    "author": "Sang-Mook Lee", 
    "title": "Effects of Easy Hybrid Parallelization with CUDA for   Numerical-Atomic-Orbital Density Functional Theory Calculation", 
    "publish": "2014-02-18T08:25:47Z", 
    "summary": "We modified a MPI-friendly density functional theory (DFT) source code within\nhybrid parallelization including CUDA. Our objective is to find out how simple\nconversions within the hybrid parallelization with mid-range GPUs affect DFT\ncode not originally suitable to CUDA. We settled several rules of hybrid\nparallelization for numerical-atomic-orbital (NAO) DFT codes. The test was\nperformed on a magnetite material system with OpenMX code by utilizing a\nhardware system containing 2 Xeon E5606 CPUs and 2 Quadro 4000 GPUs. 3-way\nhybrid routines obtained a speedup of 7.55 while 2-way hybrid speedup by 10.94.\nGPUs with CUDA complement the efficiency of OpenMP and compensate CPUs'\nexcessive competition within MPI.", 
    "link": "http://arxiv.org/pdf/1402.4247v1", 
    "arxiv-id": "1402.4247v1"
},{
    "category": "cs.DC", 
    "author": "Robert Green", 
    "title": "On Cloud-based Oversubscription", 
    "publish": "2014-02-19T18:25:54Z", 
    "summary": "Rising trends in the number of customers turning to the cloud for their\ncomputing needs has made effective resource allocation imperative for cloud\nservice providers. In order to maximize profits and reduce waste, providers\nhave started to explore the role of oversubscribing cloud resources. However,\nthe benefits of cloud-based oversubscription are not without inherent risks.\nThis paper attempts to unveil the incentives, risks, and techniques behind\noversubscription in a cloud infrastructure. Additionally, an overview of the\ncurrent research that has been completed on this highly relevant topic is\nreviewed, and suggestions are made regarding potential avenues for future work.", 
    "link": "http://arxiv.org/pdf/1402.4758v2", 
    "arxiv-id": "1402.4758v2"
},{
    "category": "cs.DC", 
    "author": "Marko Vukolic", 
    "title": "Erasure-Coded Byzantine Storage with Separate Metadata", 
    "publish": "2014-02-20T10:53:07Z", 
    "summary": "Although many distributed storage protocols have been introduced, a solution\nthat combines the strongest properties in terms of availability, consistency,\nfault-tolerance, storage complexity and the supported level of concurrency, has\nbeen elusive for a long time. Combining these properties is difficult,\nespecially if the resulting solution is required to be efficient and incur low\ncost. We present AWE, the first erasure-coded distributed implementation of a\nmulti-writer multi-reader read/write storage object that is, at the same time:\n(1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (i.e., with data\nnodes storing a bounded number of values) and (5) Byzantine fault-tolerant\n(BFT) using the optimal number of nodes. Furthermore, AWE is efficient since it\ndoes not use public-key cryptography and requires data nodes that support only\nreads and writes, further reducing the cost of deployment and ownership of a\ndistributed storage solution. Notably, AWE stores metadata separately from\n$k$-out-of-$n$ erasure-coded fragments. This enables AWE to be the first BFT\nprotocol that uses as few as $2t+k$ data nodes to tolerate $t$ Byzantine nodes,\nfor any $k \\ge 1$.", 
    "link": "http://arxiv.org/pdf/1402.4958v1", 
    "arxiv-id": "1402.4958v1"
},{
    "category": "cs.DC", 
    "author": "Hong Tian", 
    "title": "Performance Impact of Data Layout on the GPU-accelerated IDW   Interpolation", 
    "publish": "2014-02-20T12:44:09Z", 
    "summary": "This paper focuses on evaluating the performance impact of different data\nlayouts on the GPU-accelerated IDW interpolation. First, we redesign and\nimprove our previous GPU implementation that was performed by exploiting the\nfeature CUDA Dynamic Parallel (CDP). And then, we implement three versions of\nGPU implementations, i.e., the naive version, the tiled version, and the\nimproved CDP version, based on five layouts including the Structure of Arrays\n(SoA), the Array of Sturcutes (AoS), the Array of aligned Sturcutes (AoaS), the\nStructure of Arrays of aligned Structures (SoAoS), and the Hybrid layout.\nExperimental results show that: the layouts AoS and AoaS achieve better\nperformance than the layout SoA for both the naive version and tiled version,\nwhile the layout SoA is the best choice for the improved CDP version. We also\nobserve that: for the two combined data layouts (the SoAoS and the Hybrid),\nthere are no notable performance gains when compared to other three basic\nlayouts. We recommend that: in practical applications, the layout AoaS is the\nbest choice since the tiled version is the fastest one among the three versions\nof GPU implementations, especially on single precision.", 
    "link": "http://arxiv.org/pdf/1402.4986v1", 
    "arxiv-id": "1402.4986v1"
},{
    "category": "cs.DC", 
    "author": "Ankur Sahai", 
    "title": "VM Power Prediction in Distributed Systems for Maximizing Renewable   Energy Usage", 
    "publish": "2014-02-23T17:56:00Z", 
    "summary": "In the context of GreenPAD project it is important to predict the energy\nconsumption of individual (and mixture of) VMs / workload for optimal\nscheduling (running those VMs which require higher energy when there is more\ngreen energy available and vice-versa) in order to maximize green energy\nutilization.\n  For this we execute the following experiments on an Openstack cloud testbed\nconsisting of Fujitsu servers: VM energy measurement for different\nconfigurations (flavor + workload) and VM energy prediction for a new\nconfiguration. The automation framework for running these experiments uses bash\nscripts which call tools like 'stress' (simulating workloads), 'collected'\n(resource usage) and 'IPMI' (power measurement).\n  We propose a linear model for predicting the power usage of the VMs based on\nregression. We first collect the resource usage (using collected) and the\nassociated power usage (using IPMI) for different VM configurations and use\nthis to build a (multi-) regression model (between resource usage and VM energy\nconsumption). Then we use the information about the resource usage patterns of\nthe new workload to predict the power usage. For predicting power for mix of\nworkloads we execute (build a regression model based on) experiments with\nrandom workloads. We observe the highest energy usage for CPU-intensive\nworkloads followed by memory-intensive workloads.", 
    "link": "http://arxiv.org/pdf/1402.5642v1", 
    "arxiv-id": "1402.5642v1"
},{
    "category": "cs.DC", 
    "author": "Denis Trystram", 
    "title": "Scheduling data flow program in xkaapi: A new affinity based Algorithm   for Heterogeneous Architectures", 
    "publish": "2014-02-26T16:37:01Z", 
    "summary": "Efficient implementations of parallel applications on heterogeneous hybrid\narchitectures require a careful balance between computations and communications\nwith accelerator devices. Even if most of the communication time can be\noverlapped by computations, it is essential to reduce the total volume of\ncommunicated data. The literature therefore abounds with ad-hoc methods to\nreach that balance, but that are architecture and application dependent. We\npropose here a generic mechanism to automatically optimize the scheduling\nbetween CPUs and GPUs, and compare two strategies within this mechanism: the\nclassical Heterogeneous Earliest Finish Time (HEFT) algorithm and our new,\nparametrized, Distributed Affinity Dual Approximation algorithm (DADA), which\nconsists in grouping the tasks by affinity before running a fast dual\napproximation. We ran experiments on a heterogeneous parallel machine with six\nCPU cores and eight NVIDIA Fermi GPUs. Three standard dense linear algebra\nkernels from the PLASMA library have been ported on top of the Xkaapi runtime.\nWe report their performances. It results that HEFT and DADA perform well for\nvarious experimental conditions, but that DADA performs better for larger\nsystems and number of GPUs, and, in most cases, generates much lower data\ntransfers than HEFT to achieve the same performance.", 
    "link": "http://arxiv.org/pdf/1402.6601v2", 
    "arxiv-id": "1402.6601v2"
},{
    "category": "cs.DC", 
    "author": "Kieran Greer", 
    "title": "Recent Developments with the licas System 2", 
    "publish": "2014-03-04T11:54:01Z", 
    "summary": "The licas (lightweight Internet-based communication for autonomic services)\nsystem is a Java-based open source framework for building service-based\nnetworks, similar to what you would use a Cloud or SOA platform for. The\nframework comes with a server for running the services on, mechanisms for\nadding services to the server, mechanisms for linking services with each other,\nand mechanisms for allowing the services to communicate with each other. The\ngeneral architecture of the system is now fairly well set, where this paper\ndescribes recent developments that have focused on making the framework more\nrobust and additional features for easier programming.", 
    "link": "http://arxiv.org/pdf/1403.0750v2", 
    "arxiv-id": "1403.0750v2"
},{
    "category": "cs.DC", 
    "author": "Kieran Greer", 
    "title": "Recent Developments with the licas System", 
    "publish": "2014-03-04T11:58:32Z", 
    "summary": "This paper describes recent developments with the licas (lightweight\nInternet-based communication for autonomic services) software package. In\nparticular, it describes how the architecture and functionality have changed\nfrom the first version release. The autonomous nature of the system is focused\non, which requires independent behaviour and metadata descriptions of each\nservice. The system has now also been ported to the Java mobile environment.\nThen some open questions or problems will be discussed in the areas of metadata\nconsistency, security and trust. Finally, some solutions to these problems will\nalso be suggested.", 
    "link": "http://arxiv.org/pdf/1403.0753v1", 
    "arxiv-id": "1403.0753v1"
},{
    "category": "cs.DC", 
    "author": "Kieran Greer", 
    "title": "Evaluating Dynamic Linking through the Query Process using the Licas   Test Platform", 
    "publish": "2014-03-04T12:33:38Z", 
    "summary": "A novel linking mechanism has been described previously [4] that can be used\nto autonomously link sources that provide related answers to queries executed\nover an information network. The test query platform has now been re-written\nresulting in essentially a new test platform using the same basic query\nmechanism, but with a slightly different algorithm. This paper describes recent\ntest results on the same query test process that supports the original findings\nand also shows the effectiveness of the linking mechanism in a new set of test\nscenarios.", 
    "link": "http://arxiv.org/pdf/1403.0762v1", 
    "arxiv-id": "1403.0762v1"
},{
    "category": "cs.DC", 
    "author": "T. Warburton", 
    "title": "OCCA: A unified approach to multi-threading languages", 
    "publish": "2014-03-04T22:30:49Z", 
    "summary": "The inability to predict lasting languages and architectures led us to\ndevelop OCCA, a C++ library focused on host-device interaction. Using run-time\ncompilation and macro expansions, the result is a novel single kernel language\nthat expands to multiple threading languages. Currently, OCCA supports device\nkernel expansions for the OpenMP, OpenCL, and CUDA platforms. Computational\nresults using finite difference, spectral element and discontinuous Galerkin\nmethods show OCCA delivers portable high performance in different architectures\nand platforms.", 
    "link": "http://arxiv.org/pdf/1403.0968v1", 
    "arxiv-id": "1403.0968v1"
},{
    "category": "cs.DC", 
    "author": "Chenyang Lu", 
    "title": "CloudPowerCap: Integrating Power Budget and Resource Management across a   Virtualized Server Cluster", 
    "publish": "2014-03-05T22:44:31Z", 
    "summary": "In many datacenters, server racks are highly underutilized. Rack slots are\nleft empty to keep the sum of the server nameplate maximum power below the\npower provisioned to the rack. And the servers that are placed in the rack\ncannot make full use of available rack power. The root cause of this rack\nunderutilization is that the server nameplate power is often much higher than\ncan be reached in practice. To address rack underutilization, server vendors\nare shipping support for per-host power caps, which provide a server-enforced\nlimit on the amount of power that the server can draw. Using this feature,\ndatacenter operators can set power caps on the hosts in the rack to ensure that\nthe sum of those caps does not exceed the rack's provisioned power. While this\napproach improves rack utilization, it burdens the operator with managing the\nrack power budget across the hosts and does not lend itself to flexible\nallocation of power to handle workload usage spikes or to respond to changes in\nthe amount of powered-on server capacity in the rack. In this paper we present\nCloudPowerCap, a practical and scalable solution for power budget management in\na virtualized environment. CloudPowerCap manages the power budget for a cluster\nof virtualized servers, dynamically adjusting the per-host power caps for hosts\nin the cluster. We show how CloudPowerCap can provide better use of power than\nper-host static settings, while respecting virtual machine resource\nentitlements and constraints.", 
    "link": "http://arxiv.org/pdf/1403.1289v2", 
    "arxiv-id": "1403.1289v2"
},{
    "category": "cs.DC", 
    "author": "R. G. Ragel", 
    "title": "String Matching with Multicore CPUs: Performing Better with the   Aho-Corasick Algorithm", 
    "publish": "2014-03-06T00:53:36Z", 
    "summary": "Multiple string matching is known as locating all the occurrences of a given\nnumber of patterns in an arbitrary string. It is used in bio-computing\napplications where the algorithms are commonly used for retrieval of\ninformation such as sequence analysis and gene/protein identification.\nExtremely large amount of data in the form of strings has to be processed in\nsuch bio-computing applications. Therefore, improving the performance of\nmultiple string matching algorithms is always desirable. Multicore\narchitectures are capable of providing better performance by parallelizing the\nmultiple string matching algorithms. The Aho-Corasick algorithm is the one that\nis commonly used in exact multiple string matching algorithms. The focus of\nthis paper is the acceleration of Aho-Corasick algorithm through a multicore\nCPU based software implementation. Through our implementation and evaluation of\nresults, we prove that our method performs better compared to the state of the\nart.", 
    "link": "http://arxiv.org/pdf/1403.1305v1", 
    "arxiv-id": "1403.1305v1"
},{
    "category": "cs.DC", 
    "author": "Geoffrey C. Fox", 
    "title": "A Tale of Two Data-Intensive Paradigms: Applications, Abstractions, and   Architectures", 
    "publish": "2014-03-06T18:48:55Z", 
    "summary": "Scientific problems that depend on processing large amounts of data require\novercoming challenges in multiple areas: managing large-scale data\ndistribution, co-placement and scheduling of data with compute resources, and\nstoring and transferring large volumes of data. We analyze the ecosystems of\nthe two prominent paradigms for data-intensive applications, hereafter referred\nto as the high-performance computing and the Apache-Hadoop paradigm. We propose\na basis, common terminology and functional factors upon which to analyze the\ntwo approaches of both paradigms. We discuss the concept of \"Big Data Ogres\"\nand their facets as means of understanding and characterizing the most common\napplication workloads found across the two paradigms. We then discuss the\nsalient features of the two paradigms, and compare and contrast the two\napproaches. Specifically, we examine common implementation/approaches of these\nparadigms, shed light upon the reasons for their current \"architecture\" and\ndiscuss some typical workloads that utilize them. In spite of the significant\nsoftware distinctions, we believe there is architectural similarity. We discuss\nthe potential integration of different implementations, across the different\nlevels and components. Our comparison progresses from a fully qualitative\nexamination of the two paradigms, to a semi-quantitative methodology. We use a\nsimple and broadly used Ogre (K-means clustering), characterize its performance\non a range of representative platforms, covering several implementations from\nboth paradigms. Our experiments provide an insight into the relative strengths\nof the two paradigms. We propose that the set of Ogres will serve as a\nbenchmark to evaluate the two paradigms along different dimensions.", 
    "link": "http://arxiv.org/pdf/1403.1528v2", 
    "arxiv-id": "1403.1528v2"
},{
    "category": "cs.DC", 
    "author": "Ruhi Gupta", 
    "title": "Implementation of an efficient RBAC in Cloud Computing using .NET   environment", 
    "publish": "2014-03-09T09:59:32Z", 
    "summary": "Cloud Computing is flourishing day by day and it will continue in developing\nphase until computers and internet era is in existence. While dealing with\ncloud computing, a number of security and traffic related issues are\nconfronted. Load Balancing is one of the answers to these issues. RBAC deals\nwith such an answer. The proposed technique involves the hybrid of FCFS with\nRBAC technique. RBAC will assign roles to the clients and clients with a\nparticular role can only access the particular document. Hence identity\nmanagement and access management are fully implemented using this technique.", 
    "link": "http://arxiv.org/pdf/1403.2043v1", 
    "arxiv-id": "1403.2043v1"
},{
    "category": "cs.DC", 
    "author": "Mohamed Benyettou", 
    "title": "A study of risk management in cloud computing bank", 
    "publish": "2014-03-10T09:30:11Z", 
    "summary": "Cloud computing apparently helps in reducing costs and providing the\nscheduling optimal level. In practice however it may confront the problem of\nunavailability of resources. Taking into consideration the cloud computing bank\nwith its somehow commercial nature, the resources unavailability, such as\nliquidity risk, remains. In this paper, an attempt to show through a solution\nso far applied in economy, how would it be possible to predict such a liquidity\nrisk in cloud computing bank. The proposed solution can especially be adapted\nto stock management. To reduce the risk we will also make use of a method\ninspired from physics based on the fluids mechanics; it is an application of\nBernoulli's theorem called Torricelli. The resource bank will be considered as\na reservoir of liquid, and the availability of resources then will depend on\nthe liquid flow velocity and the replacement.", 
    "link": "http://arxiv.org/pdf/1403.2187v1", 
    "arxiv-id": "1403.2187v1"
},{
    "category": "cs.DC", 
    "author": "Nandini Mukherjee", 
    "title": "Heuristic-based Optimal Resource Provisioning in Application-centric   Cloud", 
    "publish": "2014-03-11T09:07:16Z", 
    "summary": "Cloud Service Providers (CSPs) adapt different pricing models for their\noffered services. Some of the models are suitable for short term requirement\nwhile others may be suitable for the Cloud Service User's (CSU) long term\nrequirement. In this paper, we look at the problem of finding the amount of\nresources to be reserved to satisfy the CSU's long term demands with the aim of\nminimizing the total cost. Finding the optimal resource requirement to satisfy\nthe the CSU's demand for resources needs sufficient research effort. Various\nalgorithms were discussed in the last couple of years for finding the optimal\nresource requirement but most of them are based on IPP which is NP in nature.\nIn this paper, we derive some heuristic-based polynomial time algorithms to\nfind some near optimal solution to the problem. We show that the cost for CSU\nusing our approach is comparable to the solution obtained using optimal Integer\nProgramming Problem(IPP).", 
    "link": "http://arxiv.org/pdf/1403.2508v1", 
    "arxiv-id": "1403.2508v1"
},{
    "category": "cs.DC", 
    "author": "Aubin Jarry", 
    "title": "The Four Principles of Geographic Routing", 
    "publish": "2014-03-12T16:26:59Z", 
    "summary": "Geographic routing consists in using the position information of nodes to\nassist in the routing process, and has been a widely studied subject in sensor\nnetworks. One of the outstanding challenges facing geographic routing has been\nits applicability. Authors either make some broad assumptions on an idealized\nversion of wireless networks which are often unverifiable, or they use costly\nmethods to planarize the communication graph.\n  The overarching questions that drive us are the following. When, and how\nshould we use geographic routing? Is there a criterion to tell whether a\ncommunication network is fit for geographic routing? When exactly does\ngeographic routing make sense?\n  In this paper we formulate the four principles that define geographic routing\nand explore their topological consequences. Given a localized communication\nnetwork, we then define and compute its geographic eccentricity, which measures\nits fitness for geographic routing. Finally we propose a distributed algorithm\nthat either enables geographic routing on the network or proves that its\ngeographic eccentricity is too high.", 
    "link": "http://arxiv.org/pdf/1403.3007v3", 
    "arxiv-id": "1403.3007v3"
},{
    "category": "cs.DC", 
    "author": "G. Sahoo", 
    "title": "Cloud Computing Simulation Using CloudSim", 
    "publish": "2014-02-21T16:07:01Z", 
    "summary": "As we know that Cloud Computing is a new paradigm in IT. It has many\nadvantages and disadvantages. But in future it will spread in the whole world.\nMany researches are going on for securing the cloud services. Simulation is the\nact of imitating or pretending. It is a situation in which a particular set of\ncondition is created artificially in order to study that could exit in reality.\nWe need only a simple Operating System with some memory to startup our\nComputer. All our resources will be available in the cloud.", 
    "link": "http://arxiv.org/pdf/1403.3253v1", 
    "arxiv-id": "1403.3253v1"
},{
    "category": "cs.DC", 
    "author": "L M Patnaik", 
    "title": "Improved Bully Election Algorithm for Distributed Systems", 
    "publish": "2014-02-28T10:10:39Z", 
    "summary": "Electing a leader is a classical problem in distributed computing system.\nSynchronization between processes often requires one process acting as a\ncoordinator. If an elected leader node fails, the other nodes of the system\nneed to elect another leader without much wasting of time. The bully algorithm\nis a classical approach for electing a leader in a synchronous distributed\ncomputing system, which is used to determine the process with highest priority\nnumber as the coordinator. In this paper, we have discussed the limitations of\nBully algorithm and proposed a simple and efficient method for the Bully\nalgorithm which reduces the number of messages during the election. Our\nanalytical simulation shows that, our proposed algorithm is more efficient than\nthe Bully algorithm with fewer messages passing and fewer stages.", 
    "link": "http://arxiv.org/pdf/1403.3255v1", 
    "arxiv-id": "1403.3255v1"
},{
    "category": "cs.DC", 
    "author": "Saeed Jalili", 
    "title": "Beyond Batch Processing: Towards Real-Time and Streaming Big Data", 
    "publish": "2014-03-13T19:22:13Z", 
    "summary": "Today, big data is generated from many sources and there is a huge demand for\nstoring, managing, processing, and querying on big data. The MapReduce model\nand its counterpart open source implementation Hadoop, has proven itself as the\nde facto solution to big data processing. Hadoop is inherently designed for\nbatch and high throughput processing jobs. Although Hadoop is very suitable for\nbatch jobs but there is an increasing demand for non-batch processes on big\ndata like: interactive jobs, real-time queries, and big data streams. Since\nHadoop is not proper for these non-batch workloads, new solutions are proposed\nto these new challenges. In this article, we discuss two categories of these\nsolutions: real-time processing, and stream processing for big data. For each\ncategory, we discuss paradigms, strengths and differences to Hadoop. We also\nintroduce some practical systems and frameworks for each category. Finally,\nsome simple experiments are done to show effectiveness of some solutions\ncompared to available Hadoop-based solutions.", 
    "link": "http://arxiv.org/pdf/1403.3375v2", 
    "arxiv-id": "1403.3375v2"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Asynchronous Convex Consensus in the Presence of Crash Faults", 
    "publish": "2014-03-13T23:01:39Z", 
    "summary": "This paper defines a new consensus problem, convex consensus. Similar to\nvector consensus [13, 20, 19], the input at each process is a d-dimensional\nvector of reals (or, equivalently, a point in the d-dimensional Euclidean\nspace). However, for convex consensus, the output at each process is a convex\npolytope contained within the convex hull of the inputs at the fault-free\nprocesses. We explore the convex consensus problem under crash faults with\nincorrect inputs, and present an asynchronous approximate convex consensus\nalgorithm with optimal fault tolerance that reaches consensus on an optimal\noutput polytope. Convex consensus can be used to solve other related problems.\nFor instance, a solution for convex consensus trivially yields a solution for\nvector consensus. More importantly, convex consensus can potentially be used to\nsolve other more interesting problems, such as convex function optimization [5,\n4].", 
    "link": "http://arxiv.org/pdf/1403.3455v2", 
    "arxiv-id": "1403.3455v2"
},{
    "category": "cs.DC", 
    "author": "Naftaly Minsky", 
    "title": "Dependable Management of Untrusted Distributed Systems", 
    "publish": "2014-03-18T02:35:30Z", 
    "summary": "The conventional approach to the online management of distributed\nsystems---represented by such standards as SNMP for network management, and\nWSDM for systems based on service oriented computing (SOC)---relies on the\ncomponents of the managed system to cooperate in the management process, by\nproviding the managers with the means to monitor their state and activities,\nand to control their behavior. Unfortunately, the trust thus placed in the\ncooperation of the managed components is unwarranted for many types of\nsystems---such as systems based on SOA---making the conventional management of\nsuch systems unreliable and insecure.\n  This paper introduces a radically new approach to the management of\ndistributed systems, called governance-based management (GBM), which is based\non a middleware that can govern the exchange of messages between system\ncomponents. GBM has a substantial ability to manage distributed systems, in a\nreliable and secure manner, even without any trustworthy cooperation of the\nmanaged components.\n  And it can fully incorporate the conventional management techniques wherever\nsuch cooperation can be trusted. GBM also supports a reflexive mode of\nmanagement, which manages the management process itself, making it safer.\nHowever, GBM is still a work in progress, as it raises several open problems\nthat needs to be addressed before this management technique can be put to\npractice.", 
    "link": "http://arxiv.org/pdf/1403.4321v1", 
    "arxiv-id": "1403.4321v1"
},{
    "category": "cs.DC", 
    "author": "Meilin Liu", 
    "title": "A Non-Cooperative Game Model for Reliability-Based Task Scheduling in   Cloud Computing", 
    "publish": "2014-03-20T00:48:42Z", 
    "summary": "Cloud computing is a newly emerging distributed system which is evolved from\nGrid computing. Task scheduling is the core research of cloud computing which\nstudies how to allocate the tasks among the physical nodes, so that the tasks\ncan get a balanced allocation or each task's execution cost decreases to the\nminimum, or the overall system performance is optimal. Unlike task scheduling\nbased on time or cost before, aiming at the special reliability requirements in\ncloud computing, we propose a non-cooperative game model for reliability-based\ntask scheduling approach. This model takes the steady-state availability that\ncomputing nodes provide as the target, takes the task slicing strategy of the\nschedulers as the game strategy, then finds the Nash equilibrium solution. And\nalso, we design a task scheduling algorithm based on this model. The\nexperiments can be seen that our task scheduling algorithm is better than the\nso-called balanced scheduling algorithm.", 
    "link": "http://arxiv.org/pdf/1403.5012v2", 
    "arxiv-id": "1403.5012v2"
},{
    "category": "cs.DC", 
    "author": "Mahshwari Tripathi", 
    "title": "A Novel Quorum Protocol", 
    "publish": "2014-03-20T13:20:07Z", 
    "summary": "One of the traditional mechanisms used in distributed systems for maintaining\nthe consistency of replicated data is voting.\n  A problem involved in voting mechanisms is the size of the Quorums needed on\neach access to the data. In this paper, we present a novel and efficient\ndistributed algorithm for managing replicated data. We impose a logical wheel\nstructure on the set of copies of an object. The protocol ensures minimum read\nquorum size of one, by reading one copy of an object while guaranteeing\nfault-tolerance of write operations.Wheel structure has a wider application\narea as it can be imposed in a network with any number of nodes.", 
    "link": "http://arxiv.org/pdf/1403.5128v1", 
    "arxiv-id": "1403.5128v1"
},{
    "category": "cs.DC", 
    "author": "K. C. Shet", 
    "title": "Design Architecture-Based on Web Server and Application Cluster in Cloud   Environment", 
    "publish": "2014-03-21T08:02:57Z", 
    "summary": "Cloud has been a computational and storage solution for many data centric\norganizations. The problem today those organizations are facing from the cloud\nis in data searching in an efficient manner. A framework is required to\ndistribute the work of searching and fetching from thousands of computers. The\ndata in HDFS is scattered and needs lots of time to retrieve. The major idea is\nto design a web server in the map phase using the jetty web server which will\ngive a fast and efficient way of searching data in MapReduce paradigm. For real\ntime processing on Hadoop, a searchable mechanism is implemented in HDFS by\ncreating a multilevel index in web server with multi-level index keys. The web\nserver uses to handle traffic throughput. By web clustering technology we can\nimprove the application performance. To keep the work down, the load balancer\nshould automatically be able to distribute load to the newly added nodes in the\nserver.", 
    "link": "http://arxiv.org/pdf/1403.5392v1", 
    "arxiv-id": "1403.5392v1"
},{
    "category": "cs.DC", 
    "author": "Eli Gafni", 
    "title": "Group Mutual Exclusion in Linear Time and Space", 
    "publish": "2014-03-22T03:07:20Z", 
    "summary": "We present two algorithms for the Group Mutual Exclusion (GME) Problem that\nsatisfy the properties of Mutual Exclusion, Starvation Freedom, Bounded Exit,\nConcurrent Entry and First Come First Served. Both our algorithms use only\nsimple read and write instructions, have O(N) Shared Space complexity and O(N)\nRemote Memory Reference (RMR) complexity in the Cache Coherency (CC) model. Our\nfirst algorithm is developed by generalizing the well-known Lamport's Bakery\nAlgorithm for the classical mutual exclusion problem, while preserving its\nsimplicity and elegance. However, it uses unbounded shared registers. Our\nsecond algorithm uses only bounded registers and is developed by generalizing\nTaubenfeld's Black and White Bakery Algorithm to solve the classical mutual\nexclusion problem using only bounded shared registers. We show that contrary to\ncommon perception our algorithms are the first to achieve these properties with\nthese combination of complexities.", 
    "link": "http://arxiv.org/pdf/1403.5605v2", 
    "arxiv-id": "1403.5605v2"
},{
    "category": "cs.DC", 
    "author": "Alberto Montresor", 
    "title": "Distributed Edge Partitioning for Graph Processing", 
    "publish": "2014-03-25T09:38:12Z", 
    "summary": "The availability of larger and larger graph datasets, growing exponentially\nover the years, has created several new algorithmic challenges to be addressed.\nSequential approaches have become unfeasible, while interest on parallel and\ndistributed algorithms has greatly increased.\n  Appropriately partitioning the graph as a preprocessing step can improve the\ndegree of parallelism of its analysis. A number of heuristic algorithms have\nbeen developed to solve this problem, but many of them subdivide the graph on\nits vertex set, thus obtaining a vertex-partitioned graph.\n  Aim of this paper is to explore a completely different approach based on edge\npartitioning, in which edges, rather than vertices, are partitioned into\ndisjoint subsets. Contribution of this paper is twofold: first, we introduce a\ngraph processing framework based on edge partitioning, that is flexible enough\nto be applied to several different graph problems. Second, we show the\nfeasibility of these ideas by presenting a distributed edge partitioning\nalgorithm called d-fep.\n  Our framework is thoroughly evaluated, using both simulations and an Hadoop\nimplementation running on the Amazon EC2 cloud. The experiments show that d-fep\nis efficient, scalable and obtains consistently good partitions. The resulting\nedge-partitioned graph can be exploited to obtain more efficient\nimplementations of graph analysis algorithms.", 
    "link": "http://arxiv.org/pdf/1403.6270v1", 
    "arxiv-id": "1403.6270v1"
},{
    "category": "cs.DC", 
    "author": "Atul Mishra", 
    "title": "A Comparative Study of Load Balancing Algorithms in Cloud Computing   Environment", 
    "publish": "2014-03-27T05:07:28Z", 
    "summary": "Cloud Computing is a new trend emerging in IT environment with huge\nrequirements of infrastructure and resources. Load Balancing is an important\naspect of cloud computing environment. Efficient load balancing scheme ensures\nefficient resource utilization by provisioning of resources to cloud users on\ndemand basis in pay as you say manner. Load Balancing may even support\nprioritizing users by applying appropriate scheduling criteria. This paper\npresents various load balancing schemes in different cloud environment based on\nrequirements specified in Service Level Agreement (SLA).", 
    "link": "http://arxiv.org/pdf/1403.6918v1", 
    "arxiv-id": "1403.6918v1"
},{
    "category": "cs.DC", 
    "author": "R. G. Ragel", 
    "title": "Accelerating string matching for bio-computing applications on   multi-core CPUs", 
    "publish": "2014-03-28T07:44:23Z", 
    "summary": "Huge amount of data in the form of strings are being handled in bio-computing\napplications and searching algorithms are quite frequently used in them. Many\nmethods utilizing on both software and hardware are being proposed to\naccelerate processing of such data. The typical hardware-based acceleration\ntechniques either require special hardware such as general purpose graphics\nprocessing units (GPGPUs) or need building a new hardware such as an FPGA based\ndesign. On the other hard, software-based acceleration techniques are easier\nsince they only require some changes in the software code or the software\narchitecture. Typical software-based techniques make use of computers connected\nover a network, also known as a network grid to accelerate the processing. In\nthis paper, we test the hypothesis that multi-core architectures should provide\nbetter performance in this kind of computation, but still it would depend on\nthe algorithm selected as well as the programming model being utilized. We\npresent the acceleration of a string-searching algorithm on a multi-core CPU\nvia a POSIX thread based implementation. Our implementation on an 8-core\nprocessor (that supports 16-threads) resulted in 9x throughput improvement\ncompared to a single thread implementation.", 
    "link": "http://arxiv.org/pdf/1403.7294v1", 
    "arxiv-id": "1403.7294v1"
},{
    "category": "cs.DC", 
    "author": "Adrian M. Peter", 
    "title": "Parallel Hierarchical Affinity Propagation with MapReduce", 
    "publish": "2014-03-28T14:37:29Z", 
    "summary": "The accelerated evolution and explosion of the Internet and social media is\ngenerating voluminous quantities of data (on zettabyte scales). Paramount\namongst the desires to manipulate and extract actionable intelligence from vast\nbig data volumes is the need for scalable, performance-conscious analytics\nalgorithms. To directly address this need, we propose a novel MapReduce\nimplementation of the exemplar-based clustering algorithm known as Affinity\nPropagation. Our parallelization strategy extends to the multilevel\nHierarchical Affinity Propagation algorithm and enables tiered aggregation of\nunstructured data with minimal free parameters, in principle requiring only a\nsimilarity measure between data points. We detail the linear run-time\ncomplexity of our approach, overcoming the limiting quadratic complexity of the\noriginal algorithm. Experimental validation of our clustering methodology on a\nvariety of synthetic and real data sets (e.g. images and point data)\ndemonstrates our competitiveness against other state-of-the-art MapReduce\nclustering techniques.", 
    "link": "http://arxiv.org/pdf/1403.7394v1", 
    "arxiv-id": "1403.7394v1"
},{
    "category": "cs.DC", 
    "author": "Camille Coti", 
    "title": "POSH: Paris OpenSHMEM: A High-Performance OpenSHMEM Implementation for   Shared Memory Systems", 
    "publish": "2014-03-30T17:43:21Z", 
    "summary": "In this paper we present the design and implementation of POSH, an\nOpen-Source implementation of the OpenSHMEM standard. We present a model for\nits communications, and prove some properties on the memory model defined in\nthe OpenSHMEM specification. We present some performance measurements of the\ncommunication library featured by POSH and compare them with an existing\none-sided communication library. POSH can be downloaded from\n\\url{http://www.lipn.fr/~coti/POSH}. % 9 - 67", 
    "link": "http://arxiv.org/pdf/1403.7791v1", 
    "arxiv-id": "1403.7791v1"
},{
    "category": "cs.DC", 
    "author": "Bryan Ford", 
    "title": "Crux: Locality-Preserving Distributed Systems", 
    "publish": "2014-05-04T00:35:25Z", 
    "summary": "Distributed systems achieve scalability by balancing load across many\nmachines, but wide-area distribution can introduce worst-case response\nlatencies proportional to the network's delay diameter. Crux is a general\nframework to build locality-preserving distributed systems, by transforming\nsome existing scalable distributed algorithm A into a new algorithm A' that\nguarantees for any two clients u and v interacting via service requests to A',\nthese interactions exhibit worst-case response latencies proportional to the\nnetwork delay between u and v. Locality-preserving PlanetLab deployments of a\nmemcached distributed cache, a bamboo distributed hash table, and a redis\npublish/subscribe service indicate that Crux is effective and applicable to a\nvariety of existing distributed algorithms. Crux achieves several orders of\nmagnitude latency improvement for localized interactions at the cost of\nincreasing per-node overheads, as each physical node must participate in\nmultiple instances of algorithm A.", 
    "link": "http://arxiv.org/pdf/1405.0637v1", 
    "arxiv-id": "1405.0637v1"
},{
    "category": "cs.DC", 
    "author": "Calvin Newport", 
    "title": "Consensus with an Abstract MAC Layer", 
    "publish": "2014-05-06T17:48:03Z", 
    "summary": "In this paper, we study distributed consensus in the radio network setting.\nWe produce new upper and lower bounds for this problem in an abstract MAC layer\nmodel that captures the key guarantees provided by most wireless MAC layers. In\nmore detail, we first generalize the well-known impossibility of deterministic\nconsensus with a single crash failure [FLP 1895] from the asynchronous message\npassing model to our wireless setting. Proceeding under the assumption of no\nfaults, we then investigate the amount of network knowledge required to solve\nconsensus in our model---an important question given that these networks are\noften deployed in an ad hoc manner. We prove consensus is impossible without\nunique ids or without knowledge of network size (in multihop topologies). We\nalso prove a lower bound on optimal time complexity. We then match these lower\nbounds with a pair of new deterministic consensus algorithms---one for single\nhop topologies and one for multihop topologies---providing a comprehensive\ncharacterization of the consensus problem in the wireless setting. From a\ntheoretical perspective, our results shed new insight into the role of network\ninformation and the power of MAC layer abstractions in solving distributed\nconsensus. From a practical perspective, given the level of abstraction used by\nour model, our upper bounds can be easily implemented in real wireless devices\non existing MAC layers while preserving their correctness\nguarantees---facilitating the development of wireless distributed systems.", 
    "link": "http://arxiv.org/pdf/1405.1382v1", 
    "arxiv-id": "1405.1382v1"
},{
    "category": "cs.DC", 
    "author": "Calvin Newport", 
    "title": "Multi-Message Broadcast with Abstract MAC Layers and Unreliable Links", 
    "publish": "2014-05-07T17:20:13Z", 
    "summary": "We study the multi-message broadcast problem using abstract MAC layer models\nof wireless networks. These models capture the key guarantees of existing MAC\nlayers while abstracting away low-level details such as signal propagation and\ncontention. We begin by studying upper and lower bounds for this problem in a\n{\\em standard abstract MAC layer model}---identifying an interesting dependence\nbetween the structure of unreliable links and achievable time complexity. In\nmore detail, given a restriction that devices connected directly by an\nunreliable link are not too far from each other in the reliable link topology,\nwe can (almost) match the efficiency of the reliable case. For the related\nrestriction, however, that two devices connected by an unreliable link are not\ntoo far from each other in geographic distance, we prove a new lower bound that\nshows that this efficiency is impossible. We then investigate how much extra\npower must be added to the model to enable a new order of magnitude of\nefficiency. In more detail, we consider an {\\em enhanced abstract MAC layer\nmodel} and present a new multi-message broadcast algorithm that (under certain\nnatural assumptions) solves the problem in this model faster than any known\nsolutions in an abstract MAC layer setting.", 
    "link": "http://arxiv.org/pdf/1405.1671v1", 
    "arxiv-id": "1405.1671v1"
},{
    "category": "cs.DC", 
    "author": "Tsvetomira Radeva", 
    "title": "Trade-offs between Selection Complexity and Performance when Searching   the Plane without Communication", 
    "publish": "2014-05-07T18:27:20Z", 
    "summary": "We consider the ANTS problem [Feinerman et al.] in which a group of agents\ncollaboratively search for a target in a two-dimensional plane. Because this\nproblem is inspired by the behavior of biological species, we argue that in\naddition to studying the {\\em time complexity} of solutions it is also\nimportant to study the {\\em selection complexity}, a measure of how likely a\ngiven algorithmic strategy is to arise in nature due to selective pressures. In\nmore detail, we propose a new selection complexity metric $\\chi$, defined for\nalgorithm ${\\cal A}$ such that $\\chi({\\cal A}) = b + \\log \\ell$, where $b$ is\nthe number of memory bits used by each agent and $\\ell$ bounds the fineness of\navailable probabilities (agents use probabilities of at least $1/2^\\ell$). In\nthis paper, we study the trade-off between the standard performance metric of\nspeed-up, which measures how the expected time to find the target improves with\n$n$, and our new selection metric.\n  In particular, consider $n$ agents searching for a treasure located at\n(unknown) distance $D$ from the origin (where $n$ is sub-exponential in $D$).\nFor this problem, we identify $\\log \\log D$ as a crucial threshold for our\nselection complexity metric. We first prove a new upper bound that achieves a\nnear-optimal speed-up of $(D^2/n +D) \\cdot 2^{O(\\ell)}$ for $\\chi({\\cal A})\n\\leq 3 \\log \\log D + O(1)$. In particular, for $\\ell \\in O(1)$, the speed-up is\nasymptotically optimal. By comparison, the existing results for this problem\n[Feinerman et al.] that achieve similar speed-up require $\\chi({\\cal A}) =\n\\Omega(\\log D)$. We then show that this threshold is tight by describing a\nlower bound showing that if $\\chi({\\cal A}) < \\log \\log D - \\omega(1)$, then\nwith high probability the target is not found within $D^{2-o(1)}$ moves per\nagent. Hence, there is a sizable gap to the straightforward $\\Omega(D^2/n + D)$\nlower bound in this setting.", 
    "link": "http://arxiv.org/pdf/1405.1688v1", 
    "arxiv-id": "1405.1688v1"
},{
    "category": "cs.DC", 
    "author": "Neda Abdolvand", 
    "title": "Towards Cloud Computing: A SWOT Analysis on its Adoption in SMEs", 
    "publish": "2014-05-08T14:04:43Z", 
    "summary": "Over the past few years, emergence of cloud computing has notably made an\nevolution in the IT industry by putting forward an \"everything as a service\"\nidea .Cloud Computing is of growing interest to companies throughout the world,\nbut there are many barriers associated with its adoption which should be\neliminated. This paper aims to investigate Cloud Computing and discusses the\ndrivers and inhibitors of its adoption. Moreover, an attempt has been made to\nidentify the key stakeholders of Cloud Computing and outline the current\nsecurity challenges. A SWOT analysis which consists of strengths, weaknesses,\nopportunities and threats has also carried out in which Cloud Computing\nadoption for SMEs (Small and Medium-sized Enterprises) is evaluated. Finally,\nthe paper concludes with some further research areas in the field of Cloud\nComputing.", 
    "link": "http://arxiv.org/pdf/1405.1932v1", 
    "arxiv-id": "1405.1932v1"
},{
    "category": "cs.DC", 
    "author": "Boaz Patt-Shamir", 
    "title": "Improved Distributed Steiner Forest Construction", 
    "publish": "2014-05-08T16:42:29Z", 
    "summary": "We present new distributed algorithms for constructing a Steiner Forest in\nthe CONGEST model. Our deterministic algorithm finds, for any given constant\n$\\epsilon>0$, a $(2+\\epsilon)$-approximation in\n$\\tilde{O}(sk+\\sqrt{\\min(st,n)})$ rounds, where $s$ is the shortest path\ndiameter, $t$ is the number of terminals, $k$ is the number of terminal\ncomponents in the input, and $n$ is the number of nodes. Our randomized\nalgorithm finds, with high probability, an $O(\\log n)$- approximation in time\n$\\tilde{O}(k+\\min(s,\\sqrt n)+D)$, where $D$ is the unweighted diameter of the\nnetwork. We also prove a matching lower bound of\n$\\tilde{\\Omega}(k+\\min(s,\\sqrt{n})+D)$ on the running time of any distributed\napproximation algorithm for the Steiner Forest problem. Previous algorithms\nwere randomized, and obtained either an $O(\\log n)$-approximation in\n$\\tilde{O}(sk)$ time, or an $O(1/\\epsilon)$-approximation in\n$\\tilde{O}((\\sqrt{n}+t)^{1+\\epsilon}+D)$ time.", 
    "link": "http://arxiv.org/pdf/1405.2011v1", 
    "arxiv-id": "1405.2011v1"
},{
    "category": "cs.DC", 
    "author": "Samuel Thibault", 
    "title": "Taking advantage of hybrid systems for sparse direct solvers via   task-based runtimes", 
    "publish": "2014-05-12T06:28:03Z", 
    "summary": "The ongoing hardware evolution exhibits an escalation in the number, as well\nas in the heterogeneity, of computing resources. The pressure to maintain\nreasonable levels of performance and portability forces application developers\nto leave the traditional programming paradigms and explore alternative\nsolutions. PaStiX is a parallel sparse direct solver, based on a dynamic\nscheduler for modern hierarchical manycore architectures. In this paper, we\nstudy the benefits and limits of replacing the highly specialized internal\nscheduler of the PaStiX solver with two generic runtime systems: PaRSEC and\nStarPU. The tasks graph of the factorization step is made available to the two\nruntimes, providing them the opportunity to process and optimize its traversal\nin order to maximize the algorithm efficiency for the targeted hardware\nplatform. A comparative study of the performance of the PaStiX solver on top of\nits native internal scheduler, PaRSEC, and StarPU frameworks, on different\nexecution environments, is performed. The analysis highlights that these\ngeneric task-based runtimes achieve comparable results to the\napplication-optimized embedded scheduler on homogeneous platforms. Furthermore,\nthey are able to significantly speed up the solver on heterogeneous\nenvironments by taking advantage of the accelerators while hiding the\ncomplexity of their efficient manipulation from the programmer.", 
    "link": "http://arxiv.org/pdf/1405.2636v1", 
    "arxiv-id": "1405.2636v1"
},{
    "category": "cs.DC", 
    "author": "T. Charles Clancy", 
    "title": "On the Latency and Energy Efficiency of Erasure-Coded Cloud Storage   Systems", 
    "publish": "2014-05-12T16:51:53Z", 
    "summary": "The increase in data storage and power consumption at data-centers has made\nit imperative to design energy efficient Distributed Storage Systems (DSS). The\nenergy efficiency of DSS is strongly influenced not only by the volume of data,\nfrequency of data access and redundancy in data storage, but also by the\nheterogeneity exhibited by the DSS in these dimensions. To this end, we propose\nand analyze the energy efficiency of a heterogeneous distributed storage system\nin which $n$ storage servers (disks) store the data of $R$ distinct classes.\nData of class $i$ is encoded using a $(n,k_{i})$ erasure code and the (random)\ndata retrieval requests can also vary across classes. We show that the energy\nefficiency of such systems is closely related to the average latency and hence\nmotivates us to study the energy efficiency via the lens of average latency.\nThrough this connection, we show that erasure coding serves the dual purpose of\nreducing latency and increasing energy efficiency. We present a queuing\ntheoretic analysis of the proposed model and establish upper and lower bounds\non the average latency for each data class under various scheduling policies.\nThrough extensive simulations, we present qualitative insights which reveal the\nimpact of coding rate, number of servers, service distribution and number of\nredundant requests on the average latency and energy efficiency of the DSS.", 
    "link": "http://arxiv.org/pdf/1405.2833v2", 
    "arxiv-id": "1405.2833v2"
},{
    "category": "cs.DC", 
    "author": "Wolfgang Karl", 
    "title": "Evaluating the Self-Optimization Process of the Adaptive Memory   Management Architecture Self-aware Memory", 
    "publish": "2014-05-12T16:40:42Z", 
    "summary": "With the continuously increasing integration level, manycore processor\nsystems are likely to be the coming system structure not only in HPC but also\nfor desktop or mobile systems. Nowadays manycore processors like Tilera TILE,\nKALRAY MPPA or Intel SCC combine a rising number of cores in a tiled\narchitecture and are mainly designed for high performance applications with\nfocus on direct inter-core communication. The current architectures have\nlimitations by central or sparse components like memory controllers, memory I/O\nor inflexible memory management.\n  In the future highly dynamic workloads with multiple concurrently running\napplications, changing I/O characteristics and a not predictable memory usage\nhave to be utilized on these manycore systems. Consequently the memory\nmanagement has to become more flexible and distributed in nature and adaptive\nmechanisms and system structures are needed. With Self-aware Memory (SaM), a\ndecentralized, scalable and autonomous self-optimizing memory architecture is\ndeveloped. This adaptive memory management can achieve higher flexibility and\nan easy usage of memory.\n  In this paper the concept of an ongoing decentralized self-optimization is\nintroduced and the evaluation of its various parameters is presented. The\nresults show that the overhead of the decentralized optimization process is\namortized by the optimized runtime using the appropriate parameter settings.", 
    "link": "http://arxiv.org/pdf/1405.2910v1", 
    "arxiv-id": "1405.2910v1"
},{
    "category": "cs.DC", 
    "author": "Mehdi B. Tahoori", 
    "title": "Towards Cross-layer Reliability Analysis of Transient and Permanent   Faults", 
    "publish": "2014-05-12T16:42:09Z", 
    "summary": "Due to the increasing complexity of Multi-Processor Systems on Chip (MPSoCs),\nsystem-level design methodologies have got a lot of attention in recent years.\nHowever, the significant gap between the system-level reliability analysis and\nthe level where the actual faults occur necessitates a cross-layer approach in\nwhich the sufficient data about the effects of faults at low levels are passed\nto the system level. So far, the cross-layer reliability analysis techniques\nfocus on a specific type of faults, e.g., either permanent or transient faults.\nIn this work, we aim at proposing a cross-layer reliability analysis which\nconsiders different fault types concurrently and connects reliability analysis\ntechniques at different levels of abstraction using adapters.", 
    "link": "http://arxiv.org/pdf/1405.2914v1", 
    "arxiv-id": "1405.2914v1"
},{
    "category": "cs.DC", 
    "author": "Lu Li", 
    "title": "Optimized Composition: Generating Efficient Code for Heterogeneous   Systems from Multi-Variant Components, Skeletons and Containers", 
    "publish": "2014-05-12T16:42:26Z", 
    "summary": "In this survey paper, we review recent work on frameworks for the high-level,\nportable programming of heterogeneous multi-/manycore systems (especially,\nGPU-based systems) using high-level constructs such as annotated user-level\nsoftware components, skeletons (i.e., predefined generic components) and\ncontainers, and discuss the optimization problems that need to be considered in\nselecting among multiple implementation variants, generating code and providing\nruntime support for efficient execution on such systems.", 
    "link": "http://arxiv.org/pdf/1405.2915v1", 
    "arxiv-id": "1405.2915v1"
},{
    "category": "cs.DC", 
    "author": "Christophe Morvan", 
    "title": "A Grammatical Approach to Data-centric Case Management in a Distributed   Collaborative Environment", 
    "publish": "2014-05-13T16:43:49Z", 
    "summary": "This paper presents a purely declarative approach to artifact-centric case\nmanagement systems, and a decentralization scheme for this model. Each case is\npresented as a tree-like structure; nodes bear information that combines data\nand computations. Each node belongs to a given stakeholder, and semantic rules\ngovern the evolution of the tree structure, as well as how data values derive\nfrom information stemming from the context of the node. Stakeholders\ncommunicate through asynchronous message passing without shared memory,\nenabling convenient distribution.", 
    "link": "http://arxiv.org/pdf/1405.3223v1", 
    "arxiv-id": "1405.3223v1"
},{
    "category": "cs.DC", 
    "author": "Imtiaz Ali Korejo", 
    "title": "A Case Study on Job Scheduling Policy for Workload Characterization and   Power Efficiency", 
    "publish": "2014-05-14T08:47:48Z", 
    "summary": "With the increasing popularity of cloud computing, datacenters are becoming\nmore important than ever before. A typical datacenter typically consists of a\nlarge number of homogeneous or heterogeneous servers connected by networks.\nUnfortunately, these servers and network equipment are often under-utilized and\npower hungry. To improve the utilization of hardware resources and make them\npower efficiency in datacenters, workload characterization and analysis is at\nthe foundation. In this paper, we characterize and analyze the job arriving\nrate, arriving time, job length, power consumption, and temperature dissipation\nin a real world datacenter by using statistical methods. From the\ncharacterization, we find unique features in the workload can be used to\noptimize the resource utilization and power consumption of datacenters", 
    "link": "http://arxiv.org/pdf/1405.3411v1", 
    "arxiv-id": "1405.3411v1"
},{
    "category": "cs.DC", 
    "author": "Justin Shi", 
    "title": "Seeking the Principles of Sustainable Software Engineering", 
    "publish": "2014-05-18T06:29:02Z", 
    "summary": "Like other engineering disciplines, software engineering should also have\nprinciples to guide the construction of sustainable computer applications.\nTangible properties include a) unlimited scalability, b) maximal\nreproducibility, and c) optimizable energy efficiency. In practice, we expect a\nsustainable scientific application should be written once and execute many\ntimes on multiple different processing platforms of different scales with\noptimized performance and energy efficiency. For more than two decades,\nexplicit parallel programming/processing paradigms only focused on performance.\nPractices showed that the rigid program-data binding prohibited dynamic runtime\nresource optimization and fault isolation, making it difficult to reproduce\napplications in scale. This paper reports our practice and experiences in\nsearch of the first principles of sustainable software engineering for compute\nand data intensive applications. Specifically, we report our practice and\nexperiences using implicit parallel programming/processing paradigms.", 
    "link": "http://arxiv.org/pdf/1405.4464v3", 
    "arxiv-id": "1405.4464v3"
},{
    "category": "cs.DC", 
    "author": "Yunji Wang", 
    "title": "A novel energy-efficient resource allocation algorithm based on immune   clonal optimization for green cloud computing", 
    "publish": "2014-05-19T06:52:59Z", 
    "summary": "Cloud computing is a style of computing in which dynamically scalable and\nother virtualized resources are provided as a service over the Internet. The\nenergy consumption and makespan associated with the resources allocated should\nbe taken into account. This paper proposes an improved clonal selection\nalgorithm based on time cost and energy consumption models in cloud computing\nenvironment. We have analyzed the performance of our approach using the\nCloudSim toolkit. The experimental results show that our approach has immense\npotential as it offers significant improvement in the aspects of response time\nand makespan, demonstrates high potential for the improvement in energy\nefficiency of the data center, and can effectively meet the service level\nagreement requested by the users.", 
    "link": "http://arxiv.org/pdf/1405.4618v1", 
    "arxiv-id": "1405.4618v1"
},{
    "category": "cs.DC", 
    "author": "Eli Gafni", 
    "title": "Set Consensus: Captured by a Set of Runs with Ramifications", 
    "publish": "2014-05-20T16:22:02Z", 
    "summary": "Are (set)-consensus objects necessary? This paper answer is negative.\n  We show that the availability of consensus objects can be replaced by\nrestricting the set of runs we consider. In particular we concentrate of the\nset of runs of the Immediate-Snapshot-Model (IIS), and given the object we\nidentify this restricted subset of IIS runs.\n  We further show that given an $(m,k)$-set consensus, an object that provides\n$k$-set consensus among $m$ processors, in a system of $n$, $n>m$ processors,\nwe do not need to use the precise power of the objects but rather their\neffective cumulative set consensus power. E.g. when $n=3, m=2,$ and $k=1$ and\nall the 3 processors are active then we only use 2-set consensus among the 3\nprocessors, as if 2-processors consensus is not available. We do this until at\nleast one of the 3 processors obtains an output. We show that this suggests a\nnew direction in the design of algorithms when consensus objects are involved.", 
    "link": "http://arxiv.org/pdf/1405.5145v1", 
    "arxiv-id": "1405.5145v1"
},{
    "category": "cs.DC", 
    "author": "Nova Ahmed", 
    "title": "Efficient and Reliable Hybrid Cloud Architechture for Big Data", 
    "publish": "2014-01-09T12:46:41Z", 
    "summary": "The objective of our paper is to propose a Cloud computing framework which is\nfeasible and necessary for handling huge data. In our prototype system we\nconsidered national ID database structure of Bangladesh which is prepared by\nelection commission of Bangladesh. Using this database we propose an\ninteractive graphical user interface for Bangladeshi People Search (BDPS) that\nuse a hybrid structure of cloud computing handled by apache Hadoop where\ndatabase is implemented by HiveQL. The infrastructure divides into two parts:\nlocally hosted cloud which is based on Eucalyptus and the remote cloud which is\nimplemented on well-known Amazon Web Service (AWS). Some common problems of\nBangladesh aspect which includes data traffic congestion, server time out and\nserver down issue is also discussed.", 
    "link": "http://arxiv.org/pdf/1405.5200v1", 
    "arxiv-id": "1405.5200v1"
},{
    "category": "cs.DC", 
    "author": "Nir Shavit", 
    "title": "Inherent Limitations of Hybrid Transactional Memory", 
    "publish": "2014-05-22T09:43:07Z", 
    "summary": "Several Hybrid Transactional Memory (HyTM) schemes have recently been\nproposed to complement the fast, but best-effort, nature of Hardware\nTransactional Memory (HTM) with a slow, reliable software backup. However, the\nfundamental limitations of building a HyTM with nontrivial concurrency between\nhardware and software transactions are still not well understood.\n  In this paper, we propose a general model for HyTM implementations, which\ncaptures the ability of hardware transactions to buffer memory accesses, and\nallows us to formally quantify and analyze the amount of overhead\n(instrumentation) of a HyTM scheme. We prove the following: (1) it is\nimpossible to build a strictly serializable HyTM implementation that has both\nuninstrumented reads and writes, even for weak progress guarantees, and (2)\nunder reasonable assumptions, in any opaque progressive HyTM, a hardware\ntransaction must incur instrumentation costs linear in the size of its data\nset. We further provide two upper bound implementations whose instrumentation\ncosts are optimal with respect to their progress guarantees. In sum, this paper\ncaptures for the first time an inherent trade-off between the degree of\nconcurrency a HyTM provides between hardware and software transactions, and the\namount of instrumentation overhead the implementation must incur.", 
    "link": "http://arxiv.org/pdf/1405.5689v3", 
    "arxiv-id": "1405.5689v3"
},{
    "category": "cs.DC", 
    "author": "Hamed Barangi", 
    "title": "A Novel Self-Recognition Method for Autonomic Grid Networks Case Study:   Advisor Labor Law Software Application", 
    "publish": "2014-02-10T01:28:22Z", 
    "summary": "Recently, Grid Computing Systems have provided wide integrated use of\nresources. Grid computing systems provide the ability to share, select and\naggregate distributed resources as computers, storage systems or other devices\nin an integrated way. Grid computing systems have solved many problems in\nscience, engineering and commerce fields. In this paper we introduce a\nself-recognition algorithm for grid network and introduced this algorithm to\nhave exclusive management control on the autonomic grid networks. This\nalgorithm is base on binomial heap to allocate and recognition any node in the\ngrid. We try to using this algorithm in advisor labor law software application\nas case study and shown in this application how to use this method for any\nadvisor application on the network. By this implementation model shown this\nmethod can get better answer to any question as a best labor law advisor.", 
    "link": "http://arxiv.org/pdf/1405.6178v1", 
    "arxiv-id": "1405.6178v1"
},{
    "category": "cs.DC", 
    "author": "David Keyes", 
    "title": "A Performance Model for the Communication in Fast Multipole Methods on   HPC Platforms", 
    "publish": "2014-05-25T07:28:33Z", 
    "summary": "Exascale systems are predicted to have approximately one billion cores,\nassuming Gigahertz cores. Limitations on affordable network topologies for\ndistributed memory systems of such massive scale bring new challenges to the\ncurrent parallel programing model. Currently, there are many efforts to\nevaluate the hardware and software bottlenecks of exascale designs. There is\ntherefore an urgent need to model application performance and to understand\nwhat changes need to be made to ensure extrapolated scalability. The fast\nmultipole method (FMM) was originally developed for accelerating N-body\nproblems in astrophysics and molecular dynamics, but has recently been extended\nto a wider range of problems, including preconditioners for sparse linear\nsolvers. It's high arithmetic intensity combined with its linear complexity and\nasynchronous communication patterns makes it a promising algorithm for exascale\nsystems. In this paper, we discuss the challenges for FMM on current parallel\ncomputers and future exascale architectures, with a focus on inter-node\ncommunication. We develop a performance model that considers the communication\npatterns of the FMM, and observe a good match between our model and the actual\ncommunication time, when latency, bandwidth, network topology, and multi-core\npenalties are all taken into account. To our knowledge, this is the first\nformal characterization of inter-node communication in FMM, which validates the\nmodel against actual measurements of communication time.", 
    "link": "http://arxiv.org/pdf/1405.6362v1", 
    "arxiv-id": "1405.6362v1"
},{
    "category": "cs.DC", 
    "author": "Pawan Luthra", 
    "title": "Review of Linpack and Cloudsim on VMM", 
    "publish": "2014-05-26T07:50:16Z", 
    "summary": "Virtualization is a framework of dividing the resources of a computer into\nmultiple execution environments which offers a lot of benefits including\nflexibility, security, ease to configuration and reduction of cost but at the\nsame time it also brings a certain degree of performance overhead. Furthermore,\nVirtual Machine Monitor (VMM) is the core component of virtual machine (VM)\nsystem and its effectiveness greatly impacts the performance of the whole\nsystem. This review paper will try to describe the basic knowledge about\nvarious virtual machine monitors such as VMware and VirtualBox. It also\ndiscussed and explores the benchmark LINPACK and CloudSim available for cloud\ncomputing. This benchmark and CloudSim can be used to measure the performance\nof two different virtual machine monitors in terms of processing speed, time,\nbandwidth, quality and response of the cloud computing network.", 
    "link": "http://arxiv.org/pdf/1405.6490v1", 
    "arxiv-id": "1405.6490v1"
},{
    "category": "cs.DC", 
    "author": "David Keyes", 
    "title": "Asynchronous Execution of the Fast Multipole Method Using Charm++", 
    "publish": "2014-05-29T08:09:35Z", 
    "summary": "Fast multipole methods (FMM) on distributed mem- ory have traditionally used\na bulk-synchronous model of com- municating the local essential tree (LET) and\noverlapping it with computation of the local data. This could be perceived as\nan extreme case of data aggregation, where the whole LET is communicated at\nonce. Charm++ allows a much finer control over the granularity of\ncommunication, and has a asynchronous execution model that fits well with the\nstructure of our FMM code. Unlike previous work on asynchronous fast N-body\nmethods such as ChaNGa and PEPC, the present work performs a direct comparison\nagainst the traditional bulk-synchronous approach and the asynchronous approach\nusing Charm++. Furthermore, the serial performance of our FMM code is over an\norder of magnitude better than these previous codes, so it is much more\nchallenging to hide the overhead of Charm++.", 
    "link": "http://arxiv.org/pdf/1405.7487v1", 
    "arxiv-id": "1405.7487v1"
},{
    "category": "cs.DC", 
    "author": "Joel Saltz", 
    "title": "Region Templates: Data Representation and Management for Large-Scale   Image Analysis", 
    "publish": "2014-05-30T19:22:46Z", 
    "summary": "Distributed memory machines equipped with CPUs and GPUs (hybrid computing\nnodes) are hard to program because of the multiple layers of memory and\nheterogeneous computing configurations. In this paper, we introduce a region\ntemplate abstraction for the efficient management of common data types used in\nanalysis of large datasets of high resolution images on clusters of hybrid\ncomputing nodes. The region template provides a generic container template for\ncommon data structures, such as points, arrays, regions, and object sets,\nwithin a spatial and temporal bounding box. The region template abstraction\nenables different data management strategies and data I/O implementations,\nwhile providing a homogeneous, unified interface to the application for data\nstorage and retrieval. The execution of region templates applications is\ncoordinated by a runtime system that supports efficient execution in hybrid\nmachines. Region templates applications are represented as hierarchical\ndataflow in which each computing stage may be represented as another dataflow\nof finer-grain tasks. A number of optimizations for hybrid machines are\navailable in our runtime system, including performance-aware scheduling for\nmaximizing utilization of computing devices and techniques to reduce impact of\ndata transfers between CPUs and GPUs. An experimental evaluation on a\nstate-of-the-art hybrid cluster using a microscopy imaging study shows that\nthis abstraction adds negligible overhead (about 3%) and achieves good\nscalability.", 
    "link": "http://arxiv.org/pdf/1405.7958v1", 
    "arxiv-id": "1405.7958v1"
},{
    "category": "cs.DC", 
    "author": "Wing Cheong Lau", 
    "title": "Optimization for Speculative Execution of Multiple Jobs in a   MapReduce-like Cluster", 
    "publish": "2014-06-03T07:44:33Z", 
    "summary": "Nowadays, a computing cluster in a typical data center can easily consist of\nhundreds of thousands of commodity servers, making component/ machine failures\nthe norm rather than exception. A parallel processing job can be delayed\nsubstantially as long as one of its many tasks is being assigned to a failing\nmachine. To tackle this so-called straggler problem, most parallel processing\nframeworks such as MapReduce have adopted various strategies under which the\nsystem may speculatively launch additional copies of the same task if its\nprogress is abnormally slow or simply because extra idling resource is\navailable. In this paper, we focus on the design of speculative execution\nschemes for a parallel processing cluster under different loading conditions.\nFor the lightly loaded case, we analyze and propose two optimization-based\nschemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing\nthe job utility and the Straggler Detection Algorithm (SDA) which minimizes the\noverall resource consumption of a job. We also derive the workload threshold\nunder which SCA or SDA should be used for speculative execution. Our simulation\nresults show both SCA and SDA can reduce the job flowtime by nearly 60%\ncomparing to the speculative execution strategy of Microsoft Mantri. For the\nheavily loaded case, we propose the Enhanced Speculative Execution (ESE)\nalgorithm which is an extension of the Microsoft Mantri scheme. We show that\nthe ESE algorithm can beat the Mantri baseline scheme by 18% in terms of job\nflowtime while consuming the same amount of resource.", 
    "link": "http://arxiv.org/pdf/1406.0609v3", 
    "arxiv-id": "1406.0609v3"
},{
    "category": "cs.DC", 
    "author": "Maleq Khan", 
    "title": "Parallel Algorithms for Generating Random Networks with Given Degree   Sequences", 
    "publish": "2014-06-04T21:29:38Z", 
    "summary": "Random networks are widely used for modeling and analyzing complex processes.\nMany mathematical models have been proposed to capture diverse real-world\nnetworks. One of the most important aspects of these models is degree\ndistribution. Chung--Lu (CL) model is a random network model, which can produce\nnetworks with any given arbitrary degree distribution. The complex systems we\ndeal with nowadays are growing larger and more diverse than ever. Generating\nrandom networks with any given degree distribution consisting of billions of\nnodes and edges or more has become a necessity, which requires efficient and\nparallel algorithms. We present an MPI-based distributed memory parallel\nalgorithm for generating massive random networks using CL model, which takes\n$O(\\frac{m+n}{P}+P)$ time with high probability and $O(n)$ space per processor,\nwhere $n$, $m$, and $P$ are the number of nodes, edges and processors,\nrespectively. The time efficiency is achieved by using a novel load-balancing\nalgorithm. Our algorithms scale very well to a large number of processors and\ncan generate massive power--law networks with one billion nodes and $250$\nbillion edges in one minute using $1024$ processors.", 
    "link": "http://arxiv.org/pdf/1406.1215v3", 
    "arxiv-id": "1406.1215v3"
},{
    "category": "cs.DC", 
    "author": "Yitzhak Birk", 
    "title": "Merge Path - A Visually Intuitive Approach to Parallel Merging", 
    "publish": "2014-06-10T17:04:23Z", 
    "summary": "Merging two sorted arrays is a prominent building block for sorting and other\nfunctions. Its efficient parallelization requires balancing the load among\ncompute cores, minimizing the extra work brought about by parallelization, and\nminimizing inter-thread synchronization requirements. Efficient use of memory\nis also important.\n  We present a novel, visually intuitive approach to partitioning two input\nsorted arrays into pairs of contiguous sequences of elements, one from each\narray, such that 1) each pair comprises any desired total number of elements,\nand 2) the elements of each pair form a contiguous sequence in the output\nmerged sorted array. While the resulting partition and the computational\ncomplexity are similar to those of certain previous algorithms, our approach is\ndifferent, extremely intuitive, and offers interesting insights. Based on this,\nwe present a synchronization-free, cache-efficient merging (and sorting)\nalgorithm.\n  While we use a shared memory architecture as the basis, our algorithm is\neasily adaptable to additional architectures. In fact, our approach is even\nrelevant to cache-efficient sequential sorting. The algorithms are presented,\nalong with important cache-related insights.", 
    "link": "http://arxiv.org/pdf/1406.2628v2", 
    "arxiv-id": "1406.2628v2"
},{
    "category": "cs.DC", 
    "author": "Bertrand Le Cun", 
    "title": "Partitionnement D\u00e9terministe pour R\u00e9soudre les Probl\u00e8mes de   Programmation Par Contraintes en utilisant le Framework Parall\u00e8le Bobpp", 
    "publish": "2014-06-11T09:50:52Z", 
    "summary": "This paper presents a deterministic parallelization to explore a Constraint\nProgramming search space. This work is an answer to an industrial project named\nPAJERO, which is in need of a parallel constraint solver which always responds\nwith the same solution whether using sequential or parallel machines. It is\nwell known that parallel tree search changes the order in which the exploration\nof solution space is done. In the context where the first solution found is\nreturned, using a different number of cores may change the returned solution.\nIn the literature, several non deterministic strategies have been proposed to\nparallelize the exploration of Constraint Programming search space. Most of\nthem are based on the Work Stealing technique used to partition the Constraint\nProgramming search space on demand and during the execution of the search\nalgorithm. Our study focuses on the determinism of the parallel search versus\nthe sequential one. We consider that the sequential search algorithm is\ndeterministic, then propose an elegant solution introducing a total order on\nthe nodes in which the parallel algorithm always gives the same solution as the\nsequential one regardless of the number of cores used. To evaluate this\ndeterministic strategy, we ran tests using the Google OR-Tools Constraint\nProgramming solver on top of our parallel Bobpp framework. The performances are\nillustrated by solving Constraint Programming problems modeled in FlatZinc\nformat.", 
    "link": "http://arxiv.org/pdf/1406.2844v1", 
    "arxiv-id": "1406.2844v1"
},{
    "category": "cs.DC", 
    "author": "Grzegorz Stachowiak", 
    "title": "On the Impact of Geometry on Ad Hoc Communication in Wireless Networks", 
    "publish": "2014-06-11T10:27:14Z", 
    "summary": "In this work we address the question how important is the knowledge of\ngeometric location and network density to the efficiency of (distributed)\nwireless communication in ad hoc networks. We study fundamental communication\ntask of broadcast and develop well-scalable, randomized algorithms that do not\nrely on GPS information, and which efficiency formulas do not depend on how\ndense the geometric network is. We consider two settings: with and without\nspontaneous wake-up of nodes. In the former setting, in which all nodes start\nthe protocol at the same time, our algorithm accomplishes broadcast in $O(D\\log\nn + \\log^2 n)$ rounds under the SINR model, with high probability (whp), where\n$D$ is the diameter of the communication graph and $n$ is the number of\nstations. In the latter setting, in which only the source node containing the\noriginal message is active in the beginning, we develop a slightly slower\nalgorithm working in $O(D\\log^2 n)$ rounds whp. Both algorithms are based on a\nnovel distributed coloring method, which is of independent interest and\npotential applicability to other communication tasks under the SINR wireless\nmodel.", 
    "link": "http://arxiv.org/pdf/1406.2852v2", 
    "arxiv-id": "1406.2852v2"
},{
    "category": "cs.DC", 
    "author": "Jay Lofstead", 
    "title": "Distributed Versioned Object Storage -- Alternatives at the OSD layer   (Poster Extended Abstract)", 
    "publish": "2014-06-14T08:02:02Z", 
    "summary": "The ability to store multiple versions of a data item is a powerful primitive\nthat has had a wide variety of uses: relational databases, transactional\nmemory, version control systems, to name a few. However, each implementation\nuses a very particular form of versioning that is customized to the domain in\nquestion and hidden away from the user. In our going project, we are reviewing\nand analyzing multiple uses of versioning in distinct domains, with the goal of\nidentifying the basic components required to provide a generic distributed\nmultiversioning object storage service, and define how these can be customized\nin order to serve distinct needs. With this primitive, new services can\nleverage multiversioning to ease development and provide specific consistency\nguarantees that address particular use cases. This work presents early results\nthat quantify the trade-offs in implementing versioning at the local storage\nlayer.", 
    "link": "http://arxiv.org/pdf/1406.3699v1", 
    "arxiv-id": "1406.3699v1"
},{
    "category": "cs.DC", 
    "author": "Zhiyong Liu", 
    "title": "OS4M: Achieving Global Load Balance of MapReduce Workload by Scheduling   at the Operation Level", 
    "publish": "2014-06-16T04:16:41Z", 
    "summary": "The efficiency of MapReduce is closely related to its load balance. Existing\nworks on MapReduce load balance focus on coarse-grained scheduling. This study\nconcerns fine-grained scheduling on MapReduce operations, with each operation\nrepresenting one invocation of the Map or Reduce function. By default,\nMapReduce adopts the hash-based method to schedule Reduce operations, which\noften leads to poor load balance. In addition, the copy phase of Reduce tasks\noverlaps with Map tasks, which significantly hinders the progress of Map tasks\ndue to I/O contention. Moreover, the three phases of Reduce tasks run in\nsequence, while consuming different resources, thereby under-utilizing\nresources. To overcome these problems, we introduce a set of mechanisms named\nOS4M (Operation Scheduling for MapReduce) to improve MapReduce's performance.\nOS4M achieves load balance by collecting statistics of all Map operations, and\ncalculates a globally optimal schedule to distribute Reduce operations. With\nOS4M, the copy phase of Reduce tasks no longer overlaps with Map tasks, and the\nthree phases of Reduce tasks are pipelined based on their operation loads. OS4M\nhas been transparently incorporated into MapReduce. Evaluations on standard\nbenchmarks show that OS4M's job duration can be shortened by up to 42%,\ncompared with a baseline of Hadoop.", 
    "link": "http://arxiv.org/pdf/1406.3901v1", 
    "arxiv-id": "1406.3901v1"
},{
    "category": "cs.DC", 
    "author": "Pablo S\u00e1nchez Espeso", 
    "title": "Fast Trace Generation of Many-Core Embedded Systems with Native   Simulation", 
    "publish": "2014-06-18T18:59:45Z", 
    "summary": "Embedded Software development and optimization are complex tasks. Late\navailably of hardware platforms, their usual low visibility and\ncontrollability, and their limiting resource constraints makes early\nperformance estimation an attractive option instead of using the final\nexecution platform. With early performance estimation, software development can\nprogress although the real hardware is not yet available or it is too complex\nto interact with. In this paper, we present how the native simulation framework\nSCoPE is extended to generate OTF trace files. Those trace files can be later\nvisualized with trace visualization tools, which recently were only used to\noptimize HPC workloads in order to iterate in the development process.", 
    "link": "http://arxiv.org/pdf/1406.4840v1", 
    "arxiv-id": "1406.4840v1"
},{
    "category": "cs.DC", 
    "author": "Ian Sommerville", 
    "title": "Academic Cloud Computing Research: Five Pitfalls and Five Opportunities", 
    "publish": "2014-06-19T08:08:14Z", 
    "summary": "This discussion paper argues that there are five fundamental pitfalls, which\ncan restrict academics from conducting cloud computing research at the\ninfrastructure level, which is currently where the vast majority of academic\nresearch lies. Instead academics should be conducting higher risk research, in\norder to gain understanding and open up entirely new areas.\n  We call for a renewed mindset and argue that academic research should focus\nless upon physical infrastructure and embrace the abstractions provided by\nclouds through five opportunities: user driven research, new programming\nmodels, PaaS environments, and improved tools to support elasticity and\nlarge-scale debugging. The objective of this paper is to foster discussion, and\nto define a roadmap forward, which will allow academia to make longer-term\nimpacts to the cloud computing community.", 
    "link": "http://arxiv.org/pdf/1406.4974v1", 
    "arxiv-id": "1406.4974v1"
},{
    "category": "cs.DC", 
    "author": "Syed Akther Hossain", 
    "title": "An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud   Computing Platform", 
    "publish": "2014-06-22T20:40:07Z", 
    "summary": "Cloud Computing is becoming a viable computing solution for services oriented\ncomputing. Several open-source cloud solutions are available to these supports.\nOpen-source software stacks offer a huge amount of customizability without huge\nlicensing fees. As a result, open source software are widely used for designing\ncloud, and private clouds are being built increasingly in the open source way.\nNumerous contributions have been made by the open-source community related to\nprivate-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private\ncloud management software. However, little has been done to systematically\ninvestigate the performance evaluation of this open-source cloud solution in\nthe existing literature. The performance evaluation aids new and existing\nresearch, industry and international projects when selecting OpenNebula\nsoftware to their work. The objective of this paper is to evaluate the\nload-balancing performance of the OpenNebula cloud management software. For the\nperformance evaluation, the OpenNebula cloud management software is installed\nand configured as a prototype implementation and tested on the DIU Cloud Lab.\nIn this paper, two set of experiments are conducted to identify the load\nbalancing performance of the OpenNebula cloud management platform- (1) Delete\nand Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping\nPhysical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.", 
    "link": "http://arxiv.org/pdf/1406.5759v1", 
    "arxiv-id": "1406.5759v1"
},{
    "category": "cs.DC", 
    "author": "Syed Akther Hossain", 
    "title": "Virtual Memory Streaming Technique for Virtual Machines (VMs) for Rapid   Scaling and High Performance in Cloud Environment", 
    "publish": "2014-06-22T20:44:10Z", 
    "summary": "This paper addresses the impact of Virtual Memory Streaming (VMS) technique\nin provisioning virtual machines (VMs) in cloud environment. VMS is a scaling\nvirtualization technology that allows different virtual machines rapid scale,\nhigh performance, and increase hardware utilization. Traditional hypervisors do\nnot support true no-downtime live migration, and its lack of memory\noversubscription can hurt the economics of a private cloud deployment by\nlimiting the number of VMs on each host. VMS brings together several advanced\nhypervisor memory management techniques including granular page sharing,\ndynamic memory footprint management, live migration, read caching, and a unique\nvirtual machine cloning capability. An architecture model is described,\ntogether with a proof-of-concept implementation, that VMS dynamically scaling\nof virtualized infrastructure with true live migration and cloning of VMs. This\npaper argues that VMS for Cloud allows requiring significantly reduced server\nmemory and reducing the time for virtualized resource scaling by instantly\nadding more virtual machines.", 
    "link": "http://arxiv.org/pdf/1406.5760v1", 
    "arxiv-id": "1406.5760v1"
},{
    "category": "cs.DC", 
    "author": "Viktor Prasanna", 
    "title": "Scalable Analytics over Distributed Time-series Graphs using GoFFish", 
    "publish": "2014-06-23T16:48:03Z", 
    "summary": "Graphs are a key form of Big Data, and performing scalable analytics over\nthem is invaluable to many domains. As our ability to collect data grows, there\nis an emerging class of inter-connected data which accumulates or varies over\ntime, and on which novel analytics - both over the network structure and across\nthe time-variant attribute values - is necessary. We introduce the notion of\ntime-series graph analytics and propose Gopher, a scalable programming\nabstraction to develop algorithms and analytics on such datasets. Our\nabstraction leverages a sub-graph centric programming model and extends it to\nthe temporal dimension using an iterative BSP (Bulk Synchronous Parallel)\napproach. Gopher is co-designed with GoFS, a distributed storage specialized\nfor time-series graphs, as part of the GoFFish distributed analytics platform.\nWe examine storage optimizations for GoFS, design patterns in Gopher to\nleverage the distributed data layout, and evaluate the GoFFish platform using\ntime-series graph data and applications on a commodity cluster.", 
    "link": "http://arxiv.org/pdf/1406.5975v1", 
    "arxiv-id": "1406.5975v1"
},{
    "category": "cs.DC", 
    "author": "Alok Kumbhare", 
    "title": "Floe: A Continuous Dataflow Framework for Dynamic Cloud Applications", 
    "publish": "2014-06-23T16:56:48Z", 
    "summary": "Applications in cyber-physical systems are increasingly coupled with online\ninstruments to perform long running, continuous data processing. Such \"always\non\" dataflow applications are dynamic, where they need to change the\napplications logic and performance at runtime, in response to external\noperational needs. Floe is a continuous dataflow framework that is designed to\nbe adaptive for dynamic applications on Cloud infrastructure. It offers\nadvanced dataflow patterns like BSP and MapReduce for flexible and holistic\ncomposition of streams and files, and supports dynamic recomposition at runtime\nwith minimal impact on the execution. Adaptive resource allocation strategies\nallow our framework to effectively use elastic Cloud resources to meet varying\ndata rates. We illustrate the design patterns of Floe by running an integration\npipeline and a tweet clustering application from the Smart Power Grids domain\non a private Eucalyptus Cloud. The responsiveness of our resource adaptation is\nvalidated through simulations for periodic, bursty and random workloads.", 
    "link": "http://arxiv.org/pdf/1406.5977v1", 
    "arxiv-id": "1406.5977v1"
},{
    "category": "cs.DC", 
    "author": "Ajay Agarwal", 
    "title": "An Efficient Read Dominant Data Replication Protocol under Serial   Isolation using Quorum Consensus Approach", 
    "publish": "2014-06-28T17:40:19Z", 
    "summary": "In distributed systems, data replication provides better availability, higher\nread capacity, improved access efficiency and lower bandwidth requirements in\nthe system. In this paper, we propose a significantly efficient approach of the\ndata replication for serial isolation by using newly proposed Circular quorum\nsystems. This paper has three major contributions. First, we have proposed the\nCircular quorum systems that generalize the various existing quorum systems,\nsuch as Read-one-write-all (ROWA) quorum systems, Majority quorum systems, Grid\nquorum systems, Diamond quorum systems, D-Space quorum systems,\nMulti-dimensional-grid quorum systems and Generalized-grid quorum systems.\nSecond, Circular quorum systems not only generalizes but also improves the\nperformance over existing quorum systems of their category. Third, we proposed\na highly available Circular quorum consensus protocol for data replication\nunder serial isolation level that uses a suitable Circular quorum system for\nread dominant scenario.", 
    "link": "http://arxiv.org/pdf/1406.7423v1", 
    "arxiv-id": "1406.7423v1"
},{
    "category": "cs.DC", 
    "author": "Nimal Nissanke", 
    "title": "Towards a Generic Application Partitioning and Retraction Framework for   Pervasive Environments", 
    "publish": "2014-06-29T17:08:17Z", 
    "summary": "Current mobile context-aware applications for pervasive environments have\nbeen designed to consume information from computational nodes or devices in\ntheir surroundings or environments. As the hardware industry continues making\nmuch smaller, compact and cheap hardware, the vision of having plenty of very\nsmall powerful digital networking nodes in, for e.g., the living room or\nbedroom, is not so far. Designing software that can make optimal use of all\nthese computational nodes when needed is still challenging; since software will\nnot only consume information from these nodes but parts of the software can be\nhosted on these different nodes. In this paper we propose the BubbleCodes\nFramework which is a generic application partitioning and retraction framework\nfor next generation context-aware applications that will have the capabilities\nto partition and retract themselves on multiple computational nodes in a\npervasive environment.", 
    "link": "http://arxiv.org/pdf/1406.7524v1", 
    "arxiv-id": "1406.7524v1"
},{
    "category": "cs.DC", 
    "author": "Beno\u00eet Garbinato", 
    "title": "Building global and scalable systems with Atomic Multicast", 
    "publish": "2014-06-29T19:27:44Z", 
    "summary": "The rise of worldwide Internet-scale services demands large distributed\nsystems. Indeed, when handling several millions of users, it is common to\noperate thousands of servers spread across the globe. Here, replication plays a\ncentral role, as it contributes to improve the user experience by hiding\nfailures and by providing acceptable latency. In this paper, we claim that\natomic multicast, with strong and well-defined properties, is the appropriate\nabstraction to efficiently design and implement globally scalable distributed\nsystems. We substantiate our claim with the design of two modern online\nservices atop atomic multicast, a strongly consistent key-value store and a\ndistributed log. In addition to presenting the design of these services, we\nexperimentally assess their performance in a geographically distributed\ndeployment.", 
    "link": "http://arxiv.org/pdf/1406.7540v1", 
    "arxiv-id": "1406.7540v1"
},{
    "category": "cs.DC", 
    "author": "Alexander A. Shvartsman", 
    "title": "Technical Report: Dealing with Undependable Workers in Decentralized   Network Supercomputing", 
    "publish": "2014-07-02T02:29:12Z", 
    "summary": "Internet supercomputing is an approach to solving partitionable,\ncomputation-intensive problems by harnessing the power of a vast number of\ninterconnected computers. This paper presents a new algorithm for the problem\nof using network supercomputing to perform a large collection of independent\ntasks, while dealing with undependable processors. The adversary may cause the\nprocessors to return bogus results for tasks with certain probabilities, and\nmay cause a subset $F$ of the initial set of processors $P$ to crash. The\nadversary is constrained in two ways. First, for the set of non-crashed\nprocessors $P-F$, the \\emph{average} probability of a processor returning a\nbogus result is inferior to $\\frac{1}{2}$. Second, the adversary may crash a\nsubset of processors $F$, provided the size of $P-F$ is bounded from below. We\nconsider two models: the first bounds the size of $P-F$ by a fractional\npolynomial, the second bounds this size by a poly-logarithm. Both models yield\nadversaries that are much stronger than previously studied. Our randomized\nsynchronous algorithm is formulated for $n$ processors and $t$ tasks, with\n$n\\le t$, where depending on the number of crashes each live processor is able\nto terminate dynamically with the knowledge that the problem is solved with\nhigh probability. For the adversary constrained by a fractional polynomial, the\nround complexity of the algorithm is\n$O(\\frac{t}{n^\\varepsilon}\\log{n}\\log{\\log{n}})$, its work is $O(t\\log{n}\n\\log{\\log{n}})$ and message complexity is $O(n\\log{n}\\log{\\log{n}})$. For the\npoly-log constrained adversary, the round complexity is $O(t)$, work is $O(t\nn^{\\varepsilon})$, %$O(t \\, poly \\log{n})$, and message complexity is\n$O(n^{1+\\varepsilon})$ %$O(n \\, poly \\log{n})$. All bounds are shown to hold\nwith high probability.", 
    "link": "http://arxiv.org/pdf/1407.0442v1", 
    "arxiv-id": "1407.0442v1"
},{
    "category": "cs.DC", 
    "author": "Alexander A. Shvartsman", 
    "title": "Technical Report: Estimating Reliability of Workers for Cooperative   Distributed Computing", 
    "publish": "2014-07-02T02:41:00Z", 
    "summary": "Internet supercomputing is an approach to solving partitionable,\ncomputation-intensive problems by harnessing the power of a vast number of\ninterconnected computers. For the problem of using network supercomputing to\nperform a large collection of independent tasks, prior work introduced a\ndecentralized approach and provided randomized synchronous algorithms that\nperform all tasks correctly with high probability, while dealing with\nmisbehaving or crash-prone processors. The main weaknesses of existing\nalgorithms is that they assume either that the \\emph{average} probability of a\nnon-crashed processor returning incorrect results is inferior to $\\frac{1}{2}$,\nor that the probability of returning incorrect results is known to \\emph{each}\nprocessor. Here we present a randomized synchronous distributed algorithm that\ntightly estimates the probability of each processor returning correct results.\nStarting with the set $P$ of $n$ processors, let $F$ be the set of processors\nthat crash. Our algorithm estimates the probability $p_i$ of returning a\ncorrect result for each processor $i \\in P-F$, making the estimates available\nto all these processors. The estimation is based on the $(\\epsilon,\n\\delta)$-approximation, where each estimated probability $\\tilde{p_i}$ of $p_i$\nobeys the bound ${\\sf Pr}[p_i(1-\\epsilon) \\leq \\tilde{p_i} \\leq\np_i(1+\\epsilon)] > 1 - \\delta$, for any constants $\\delta >0$ and $\\epsilon >0$\nchosen by the user. An important aspect of this algorithm is that each\nprocessor terminates without global coordination. We assess the efficiency of\nthe algorithm in three adversarial models as follows. For the model where the\nnumber of non-crashed processors $|P-F|$ is linearly bounded the time\ncomplexity $T(n)$ of the algorithm is $\\Theta(\\log{n})$, work complexity $W(n)$\nis $\\Theta(n\\log{n})$, and message complexity $M(n)$ is $\\Theta(n\\log^2n)$.", 
    "link": "http://arxiv.org/pdf/1407.0696v1", 
    "arxiv-id": "1407.0696v1"
},{
    "category": "cs.DC", 
    "author": "Ajay Agarwal", 
    "title": "HT-Paxos: High Throughput State-Machine Replication Protocol for Large   Clustered Data Centers", 
    "publish": "2014-07-04T13:58:50Z", 
    "summary": "Paxos is a prominent theory of state machine replication. Recent data\nintensive Systems those implement state machine replication generally require\nhigh throughput. Earlier versions of Paxos as few of them are classical Paxos,\nfast Paxos and generalized Paxos have a major focus on fault tolerance and\nlatency but lacking in terms of throughput and scalability. A major reason for\nthis is the heavyweight leader. Through offloading the leader, we can further\nincrease throughput of the system. Ring Paxos, Multi Ring Paxos and S-Paxos are\nfew prominent attempts in this direction for clustered data centers. In this\npaper, we are proposing HT-Paxos, a variant of Paxos that one is the best\nsuitable for any large clustered data center. HT-Paxos further offloads the\nleader very significantly and hence increases the throughput and scalability of\nthe system. While at the same time, among high throughput state-machine\nreplication protocols, HT-Paxos provides reasonably low latency and response\ntime.", 
    "link": "http://arxiv.org/pdf/1407.1237v1", 
    "arxiv-id": "1407.1237v1"
},{
    "category": "cs.DC", 
    "author": "R. Silvestri", 
    "title": "Plurality Consensus in the Gossip Model", 
    "publish": "2014-07-09T17:13:35Z", 
    "summary": "We study Plurality Consensus in the Gossip Model over a network of $n$\nanonymous agents. Each agent supports an initial opinion or color. We assume\nthat at the onset, the number of agents supporting the plurality color exceeds\nthat of the agents supporting any other color by a sufficiently-large bias. The\ngoal is to provide a protocol that, with high probability, brings the system\ninto the configuration in which all agents support the (initial) plurality\ncolor. We consider the Undecided-State Dynamics, a well-known protocol which\nuses just one more state (the undecided one) than those necessary to store\ncolors. We show that the speed of convergence of this protocol depends on the\ninitial color configuration as a whole, not just on the gap between the\nplurality and the second largest color community. This dependence is best\ncaptured by a novel notion we introduce, namely, the monochromatic distance\n${md}(\\bar{\\mathbf{c}})$ which measures the distance of the initial color\nconfiguration $\\bar{ \\mathbf {c}}$ from the closest monochromatic one. In the\ncomplete graph, we prove that, for a wide range of the input parameters, this\ndynamics converges within $O({md}(\\bar {\\mathbf {c}}) \\log {n})$ rounds. We\nprove that this upper bound is almost tight in the strong sense: Starting from\nany color configuration $\\bar {\\mathbf {c}}$, the convergence time is\n$\\Omega({md}(\\bar {\\mathbf {c}}))$. Finally, we adapt the Undecided-State\nDynamics to obtain a fast, random walk-based protocol for plurality consensus\non regular expanders. This protocol converges in $O({md}(\\bar {\\mathbf {c}})\n\\mathrm{polylog}(n))$ rounds using only $\\mathrm{polylog}(n)$ local memory. A\nkey-ingredient to achieve the above bounds is a new analysis of the maximum\nnode congestion that results from performing $n$ parallel random walks on\nregular expanders. All our bounds hold with high probability.", 
    "link": "http://arxiv.org/pdf/1407.2565v2", 
    "arxiv-id": "1407.2565v2"
},{
    "category": "cs.DC", 
    "author": "Vijay Gadepally", 
    "title": "Parallel MATLAB Techniques", 
    "publish": "2014-07-09T20:54:42Z", 
    "summary": "In this chapter, we show why parallel MATLAB is useful, provide a comparison\nof the different parallel MATLAB choices, and describe a number of applications\nin Signal and Image Processing: Audio Signal Processing, Synthetic Aperture\nRadar (SAR) Processing and Superconducting Quantum Interference Filters\n(SQIFs). Each of these applications have been parallelized using different\nmethods (Task parallel and Data parallel techniques). The applications\npresented may be considered representative of type of problems faced by signal\nand image processing researchers. This chapter will also strive to serve as a\nguide to new signal and image processing parallel programmers, by suggesting a\nparallelization strategy that can be employed when developing a general\nparallel algorithm. The objective of this chapter is to help signal and image\nprocessing algorithm developers understand the advantages of using parallel\nMATLAB to tackle larger problems while staying within the powerful environment\nof MATLAB.", 
    "link": "http://arxiv.org/pdf/1407.2636v1", 
    "arxiv-id": "1407.2636v1"
},{
    "category": "cs.DC", 
    "author": "Josef Widder", 
    "title": "Solvability-Based Comparison of Failure Detectors", 
    "publish": "2014-07-11T20:17:12Z", 
    "summary": "Failure detectors are oracles that have been introduced to provide processes\nin asynchronous systems with information about faults. This information can\nthen be used to solve problems otherwise unsolvable in asynchronous systems. A\nnatural question is on the \"minimum amount of information\" a failure detector\nhas to provide for a given problem. This question is classically addressed\nusing a relation that states that a failure detector D is stronger (that is,\nprovides \"more, or better, information\") than a failure detector D' if D can be\nused to implement D'. It has recently been shown that this classic\nimplementability relation has some drawbacks. To overcome this, different\nrelations have been defined, one of which states that a failure detector D is\nstronger than D' if D can solve all the time-free problems solvable by D'. In\nthis paper we compare the implementability-based hierarchy of failure detectors\nto the hierarchy based on solvability. This is done by introducing a new proof\ntechnique for establishing the solvability relation. We apply this technique to\nknown failure detectors from the literature and demonstrate significant\ndifferences between the hierarchies.", 
    "link": "http://arxiv.org/pdf/1407.3286v1", 
    "arxiv-id": "1407.3286v1"
},{
    "category": "cs.DC", 
    "author": "Grigori Fursin", 
    "title": "Collective Tuning Initiative", 
    "publish": "2014-07-13T17:13:17Z", 
    "summary": "Computing systems rarely deliver best possible performance due to ever\nincreasing hardware and software complexity and limitations of the current\noptimization technology. Additional code and architecture optimizations are\noften required to improve execution time, size, power consumption, reliability\nand other important characteristics of computing systems. However, it is often\na tedious, repetitive, isolated and time consuming process. In order to\nautomate, simplify and systematize program optimization and architecture\ndesign, we are developing open-source modular plugin-based Collective Tuning\nInfrastructure (CTI, http://cTuning.org) that can distribute optimization\nprocess and leverage optimization experience of multiple users. CTI provides a\nnovel fully integrated, collaborative, \"one button\" approach to improve\nexisting underperfoming computing systems ranging from embedded architectures\nto high-performance servers based on systematic iterative compilation,\nstatistical collective optimization and machine learning. Our experimental\nresults show that it is possible to reduce execution time (and code size) of\nsome programs from SPEC2006 and EEMBC among others by more than a factor of 2\nautomatically. It can also reduce development and testing time considerably.\nTogether with the first production quality machine learning enabled interactive\nresearch compiler (MILEPOST GCC) this infrastructure opens up many research\nopportunities to study and develop future realistic self-tuning and\nself-organizing adaptive intelligent computing systems based on systematic\nstatistical performance evaluation and benchmarking. Finally, using common\noptimization repository is intended to improve the quality and reproducibility\nof the research on architecture and code optimization.", 
    "link": "http://arxiv.org/pdf/1407.3487v1", 
    "arxiv-id": "1407.3487v1"
},{
    "category": "cs.DC", 
    "author": "Vipul A. Shah", 
    "title": "Scheduling in Grid Computing Environment", 
    "publish": "2014-07-15T04:28:09Z", 
    "summary": "Scheduling in Grid computing has been active area of research since its\nbeginning. However, beginners find very difficult to understand related\nconcepts due to a large learning curve of Grid computing. Thus, there is a need\nof concise understanding of scheduling in Grid computing area. This paper\nstrives to present concise understanding of scheduling and related\nunderstanding of Grid computing system. The paper describes overall picture of\nGrid computing and discusses important sub-systems that enable Grid computing\npossible. Moreover, the paper also discusses concepts of resource scheduling\nand application scheduling and also presents classification of scheduling\nalgorithms. Furthermore, the paper also presents methodology used for\nevaluating scheduling algorithms including both real system and simulation\nbased approaches. The presented work on scheduling in Grid containing concise\nunderstandings of scheduling system, scheduling algorithm, and scheduling\nmethodology would be very useful to users and researchers", 
    "link": "http://arxiv.org/pdf/1407.3879v1", 
    "arxiv-id": "1407.3879v1"
},{
    "category": "cs.DC", 
    "author": "Vipul A. Shah", 
    "title": "Experimental Study of Remote Job Submission and Execution on LRM through   Grid Computing Mechanisms", 
    "publish": "2014-07-15T04:53:12Z", 
    "summary": "Remote job submission and execution is fundamental requirement of distributed\ncomputing done using Cluster computing. However, Cluster computing limits usage\nwithin a single organization. Grid computing environment can allow use of\nresources for remote job execution that are available in other organizations.\nThis paper discusses concepts of batch-job execution using LRM and using Grid.\nThe paper discusses two ways of preparing test Grid computing environment that\nwe use for experimental testing of concepts. This paper presents experimental\ntesting of remote job submission and execution mechanisms through LRM specific\nway and Grid computing ways. Moreover, the paper also discusses various\nproblems faced while working with Grid computing environment and discusses\ntheir trouble-shootings. The understanding and experimental testing presented\nin this paper would become very useful to researchers who are new to the field\nof job management in Grid.", 
    "link": "http://arxiv.org/pdf/1407.3881v1", 
    "arxiv-id": "1407.3881v1"
},{
    "category": "cs.DC", 
    "author": "Vivek Sarkar", 
    "title": "ADHA: Automatic Data layout framework for Heterogeneous Architectures", 
    "publish": "2014-07-18T00:09:04Z", 
    "summary": "Data layouts play a crucial role in determining the performance of a given\napplication running on a given architecture. Existing parallel programming\nframeworks for both multicore and heterogeneous systems leave the onus of\nselecting a data layout to the programmer. Therefore, shifting the burden of\ndata layout selection to optimizing compilers can greatly enhance programmer\nproductivity and application performance. In this work, we introduce {\\ADHA}: a\ntwo-level hierarchal formulation of the data layout problem for modern\nheterogeneous architectures. We have created a reference implementation of ADHA\nin the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows\nsignificant performance benefits of up to 6.92$\\times$ compared to manually\nspecified layouts for two benchmark programs running on a CPU+GPU heterogeneous\nplatform.", 
    "link": "http://arxiv.org/pdf/1407.4859v1", 
    "arxiv-id": "1407.4859v1"
},{
    "category": "cs.DC", 
    "author": "Raluca Mariana Dragoescu", 
    "title": "Integrating R and Hadoop for Big Data Analysis", 
    "publish": "2014-07-18T08:17:55Z", 
    "summary": "Analyzing and working with big data could be very diffi cult using classical\nmeans like relational database management systems or desktop software packages\nfor statistics and visualization. Instead, big data requires large clusters\nwith hundreds or even thousands of computing nodes. Offi cial statistics is\nincreasingly considering big data for deriving new statistics because big data\nsources could produce more relevant and timely statistics than traditional\nsources. One of the software tools successfully and wide spread used for\nstorage and processing of big data sets on clusters of commodity hardware is\nHadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS),\na resource-management platform and implements a version of the MapReduce\nprogramming model for large scale data processing. In this paper we investigate\nthe possibilities of integrating Hadoop with R which is a popular software used\nfor statistical computing and data visualization. We present three ways of\nintegrating them: R with Streaming, Rhipe and RHadoop and we emphasize the\nadvantages and disadvantages of each solution.", 
    "link": "http://arxiv.org/pdf/1407.4908v1", 
    "arxiv-id": "1407.4908v1"
},{
    "category": "cs.DC", 
    "author": "T. R. Gopalakrishnan Nair", 
    "title": "Model based design of super schedulers managing catastrophic scenario in   hard real time systems", 
    "publish": "2014-07-21T07:52:42Z", 
    "summary": "The conventional design of real-time approaches depends heavily on the normal\nperformance of systems and it often becomes incapacitated in dealing with\ncatastrophic scenarios effectively. There are several investigations carried\nout to effectively tackle large scale catastrophe of a plant and how real-time\nsystems must reorganize itself to respond optimally to changing scenarios to\nreduce catastrophe and aid human intervention. The study presented here is in\nthis direction and the model accommodates catastrophe generated tasks while it\ntries to minimize the total number of deadline miss, by dynamically scheduling\nthe unusual pattern of tasks. The problem is NP hard. We prove the methods for\nan optimal scheduling algorithm. We also derive a model to maintain the\nstability of the processes. Moreover, we study the problem of minimizing the\nnumber of processors required for scheduling with a set of periodic and\nsporadic hard real time tasks with primary/backup mechanism to achieve fault\ntolerance. EDF scheduling algorithms are used on each processor to manage\nscenario changes. Finally we present a simulation of super scheduler with\nsmall, medium and large real time tasks pattern for catastrophe management.", 
    "link": "http://arxiv.org/pdf/1407.5404v1", 
    "arxiv-id": "1407.5404v1"
},{
    "category": "cs.DC", 
    "author": "Qin Li", 
    "title": "A macro-level model for investigating the effect of directional bias on   network coverage", 
    "publish": "2014-07-22T07:14:16Z", 
    "summary": "Random walks have been proposed as a simple method of efficiently searching,\nor disseminating information throughout, communication and sensor networks. In\nnature, animals (such as ants) tend to follow correlated random walks, i.e.,\nrandom walks that are biased towards their current heading. In this paper, we\ninvestigate whether or not complementing random walks with directional bias can\ndecrease the expected discovery and coverage times in networks.\n  To do so, we develop a macro-level model of a directionally biased random\nwalk based on Markov chains. By focussing on regular, connected networks, the\nmodel allows us to efficiently calculate expected coverage times for different\nnetwork sizes and biases. Our analysis shows that directional bias can\nsignificantly reduce coverage time, but only when the bias is below a certain\nvalue which is dependent on the network size.", 
    "link": "http://arxiv.org/pdf/1407.5762v2", 
    "arxiv-id": "1407.5762v2"
},{
    "category": "cs.DC", 
    "author": "Robert Obryk", 
    "title": "Write-and-f-array: implementation and an application", 
    "publish": "2014-07-23T09:47:57Z", 
    "summary": "We introduce a new shared memory object: the write-and-f-array, provide its\nwait-free implementation and use it to construct an improved wait-free\nimplementation of the fetch-and-add object. The write-and-f-array generalizes\nsingle-writer write-and-snapshot object in a similar way that the f-array\ngeneralizes the multi-writer snapshot object. More specifically, a\nwrite-and-f-array is parameterized by an associative operator $f$ and is\nconceptually an array with two atomic operations:\n  - write-and-f modifies a single array's element and returns the result of\napplying $f$ to all the elements,\n  - read returns the result of applying $f$ to all the array's elements.\n  We provide a wait-free implementation of an $N$-element write-and-f-array\nwith $O(N \\log N)$ memory complexity, $O(\\log^3 N)$ step complexity of the\nwrite-and-f operation and $O(1)$ step complexity of the read operation. The\nimplementation uses CAS objects and requires their size to be $\\Omega(\\log M)$,\nwhere $M$ is the total number of write-and-f operations executed. We also show,\nhow it can be modified to achieve $O(\\log^2 N)$ step complexity of write-and-f,\nwhile increasing the memory complexity to $O(N \\log^2 N)$.\n  The write-and-f-array can be applied to create a fetch-and-add object for $P$\nprocesses with $O(P \\log P)$ memory complexity and $O(\\log^3 P)$ step\ncomplexity of the fetch-and-add operation. This is the first implementation of\nfetch-and-add with polylogarithmic step complexity and subquadratic memory\ncomplexity that can be implemented without CAS or LL/SC objects of unrealistic\nsize.", 
    "link": "http://arxiv.org/pdf/1407.6153v1", 
    "arxiv-id": "1407.6153v1"
},{
    "category": "cs.DC", 
    "author": "B. Raveendra Babu", 
    "title": "A Fuzzy Differential Evolution Algorithm for Job Scheduling on   Computational Grids", 
    "publish": "2014-07-23T18:08:25Z", 
    "summary": "Grid computing is the recently growing area of computing that share data,\nstorage, computing across geographically dispersed area. This paper proposes a\nnovel fuzzy approach using Differential Evolution (DE) for scheduling jobs on\ncomputational grids. The fuzzy based DE generates an optimal plan to complete\nthe jobs within a minimum period of time. We evaluate the performance of the\nproposed fuzzy based DE algorithm with Genetic Algorithm (GA), Simulated\nAnnealing (SA), Differential Evolution and fuzzy PSO. Experimental results have\nshown that the new algorithm produces more optimal solutions for the job\nscheduling problems compared to other algorithms.", 
    "link": "http://arxiv.org/pdf/1407.6317v1", 
    "arxiv-id": "1407.6317v1"
},{
    "category": "cs.DC", 
    "author": "Mazin Nasser", 
    "title": "Parallelize Bubble Sort Algorithm Using OpenMP", 
    "publish": "2014-07-24T14:47:48Z", 
    "summary": "Sorting has been a profound area for the algorithmic researchers and many\nresources are invested to suggest more works for sorting algorithms. For this\npurpose, many existing sorting algorithms were observed in terms of the\nefficiency of the algorithmic complexity. In this paper we implemented the\nbubble sort algorithm using multithreading (OpenMP). The proposed work tested\non two standard datasets (text file) with different size . The main idea of the\nproposed algorithm is distributing the elements of the input datasets into many\nadditional temporary sub-arrays according to a number of characters in each\nword. The sizes of each of these sub-arrays are decided depending on a number\nof elements with the same number of characters in the input array. We\nimplemented OpenMP using Intel core i7-3610QM ,(8 CPUs),using two approaches\n(vectors of string and array 3D) . Finally, we get the data structure effects\non the performance of the algorithm for that we choice the second approach.", 
    "link": "http://arxiv.org/pdf/1407.6603v1", 
    "arxiv-id": "1407.6603v1"
},{
    "category": "cs.DC", 
    "author": "\u00dcmit V. \u00c7ataly\u00fcrek", 
    "title": "On Distributed Graph Coloring with Iterative Recoloring", 
    "publish": "2014-07-24T22:02:53Z", 
    "summary": "Identifying the sets of operations that can be executed simultaneously is an\nimportant problem appearing in many parallel applications. By modeling the\noperations and their interactions as a graph, one can identify the independent\noperations by solving a graph coloring problem. Many efficient sequential\nalgorithms are known for this NP-Complete problem, but they are typically\nunsuitable when the operations and their interactions are distributed in the\nmemory of large parallel computers. On top of an existing distributed-memory\ngraph coloring algorithm, we investigate two compatible techniques in this\npaper for fast and scalable distributed-memory graph coloring. First, we\nintroduce an improvement for the distributed post-processing operation, called\nrecoloring, which drastically improves the number of colors. We propose a novel\nand efficient communication scheme for recoloring which enables it to scale\ngracefully. Recoloring must be seeded with an existing coloring of the graph.\nOur second contribution is to introduce a randomized color selection strategy\nfor initial coloring which quickly produces solutions of modest quality. We\nextensively evaluate the impact of our new techniques on existing distributed\nalgorithms and show the time-quality tradeoffs. We show that combining an\ninitial randomized coloring with multiple recoloring iterations yields better\nquality solutions with the smaller runtime at large scale.", 
    "link": "http://arxiv.org/pdf/1407.6745v1", 
    "arxiv-id": "1407.6745v1"
},{
    "category": "cs.DC", 
    "author": "Srivatsan Ravi", 
    "title": "On Partial Wait-Freedom in Transactional Memory", 
    "publish": "2014-07-25T12:52:59Z", 
    "summary": "Transactional memory (TM) is a convenient synchronization tool that allows\nconcurrent threads to declare sequences of instructions on shared data as\nspeculative \\emph{transactions} with \"all-or-nothing\" semantics. It is known\nthat dynamic transactional memory cannot provide \\emph{wait-free} progress in\nthe sense that every transaction commits in a finite number of its own steps.\nIn this paper, we explore the costs of providing wait-freedom to only a\n\\emph{subset} of transactions. Since most transactional workloads are believed\nto be read-dominated, we require that read-only transactions commit in the\nwait-free manner, while updating transactions are guaranteed to commit only if\nthey run in the absence of concurrency. We show that this kind of partial\nwait-freedom, combined with attractive requirements like read invisibility or\ndisjoint-access parallelism, incurs considerable complexity costs.", 
    "link": "http://arxiv.org/pdf/1407.6876v2", 
    "arxiv-id": "1407.6876v2"
},{
    "category": "cs.DC", 
    "author": "Zaid Abdi Alkareem Alyasseri", 
    "title": "Survey of Parallel Computing with MATLAB", 
    "publish": "2014-07-25T12:59:26Z", 
    "summary": "Matlab is one of the most widely used mathematical computing environments in\ntechnical computing. It has an interactive environment which provides high\nperformance computing (HPC) procedures and easy to use. Parallel computing with\nMatlab has been an interested area for scientists of parallel computing\nresearches for a number of years. Where there are many attempts to parallel\nMatlab. In this paper, we present most of the past,present attempts of parallel\nMatlab such as MatlabMPI, bcMPI, pMatlab, Star-P and PCT. Finally, we expect\nthe future attempts.", 
    "link": "http://arxiv.org/pdf/1407.6878v1", 
    "arxiv-id": "1407.6878v1"
},{
    "category": "cs.DC", 
    "author": "Bradley S. Rees", 
    "title": "Accelerating Fast Fourier Transforms Using Hadoop and CUDA", 
    "publish": "2014-07-25T14:25:35Z", 
    "summary": "There has been considerable research into improving Fast Fourier Transform\n(FFT) performance through parallelization and optimization for specialized\nhardware. However, even with those advancements, processing of very large\nfiles, over 1TB in size, still remains prohibitively slow. Analysts performing\nsignal processing are forced to wait hours or days for results, which results\nin a disruption of their workflow and a decrease in productivity. In this paper\nwe present a unique approach that not only parallelizes the workload over\nmulti-cores, but distributes the problem over a cluster of graphics processing\nunit (GPU)-equipped servers. By utilizing Hadoop and CUDA, we can take\nadvantage of inexpensive servers while still exceeding the processing power of\na dedicated supercomputer, as demonstrated in our result using Amazon EC2.", 
    "link": "http://arxiv.org/pdf/1407.6915v1", 
    "arxiv-id": "1407.6915v1"
},{
    "category": "cs.DC", 
    "author": "Jordi Carrabina", 
    "title": "OMP2HMPP: HMPP Source Code Generation from Programs with Pragma   Extensions", 
    "publish": "2014-07-25T15:14:08Z", 
    "summary": "High-performance computing are based more and more in heterogeneous\narchitectures and GPGPUs have become one of the main integrated blocks in\nthese, as the recently emerged Mali GPU in embedded systems or the NVIDIA GPUs\nin HPC servers. In both GPGPUs, programming could become a hurdle that can\nlimit their adoption, since the programmer has to learn the hardware\ncapabilities and the language to work with these. We present OMP2HMPP, a tool\nthat, automatically trans-lates a high-level C source code(OpenMP) code into\nHMPP. The generated version rarely will differs from a hand-coded HMPP version,\nand will provide an important speedup, near 113%, that could be later improved\nby hand-coded CUDA. The generated code could be transported either to HPC\nservers and to embedded GPUs, due to the commonalities between them.", 
    "link": "http://arxiv.org/pdf/1407.6932v1", 
    "arxiv-id": "1407.6932v1"
},{
    "category": "cs.DC", 
    "author": "Shadi Ibrahim", 
    "title": "A Taxonomy and Survey on eScience as a Service in the Cloud", 
    "publish": "2014-07-28T09:14:35Z", 
    "summary": "Cloud computing has recently evolved as a popular computing infrastructure\nfor many applications. Scientific computing, which was mainly hosted in private\nclusters and grids, has started to migrate development and deployment to the\npublic cloud environment. eScience as a service becomes an emerging and\npromising direction for science computing. We review recent efforts in\ndeveloping and deploying scientific computing applications in the cloud. In\nparticular, we introduce a taxonomy specifically designed for scientific\ncomputing in the cloud, and further review the taxonomy with four major kinds\nof science applications, including life sciences, physics sciences, social and\nhumanities sciences, and climate and earth sciences. Our major finding is that,\ndespite existing efforts in developing cloud-based eScience, eScience still has\na long way to go to fully unlock the power of cloud computing paradigm.\nTherefore, we present the challenges and opportunities in the future\ndevelopment of cloud-based eScience services, and call for collaborations and\ninnovations from both the scientific and computer system communities to address\nthose challenges.", 
    "link": "http://arxiv.org/pdf/1407.7360v1", 
    "arxiv-id": "1407.7360v1"
},{
    "category": "cs.DC", 
    "author": "Christoph Lenzen", 
    "title": "The 1-2-3-Toolkit for Building Your Own Balls-into-Bins Algorithm", 
    "publish": "2014-07-31T14:27:10Z", 
    "summary": "In this work, we examine a generic class of simple distributed\nballs-into-bins algorithms. Exploiting the strong concentration bounds that\napply to balls-into-bins games, we provide an iterative method to compute\naccurate estimates of the remaining balls and the load distribution after each\nround. Each algorithm is classified by (i) the load that bins accept in a given\nround, (ii) the number of messages each ball sends in a given round, and (iii)\nwhether each such message is given a rank expressing the sender's inclination\nto commit to the receiving bin (if feasible). This novel ranking mechanism\nresults in notable improvements, in particular in the number of balls that may\ncommit to a bin in the first round of the algorithm. Simulations independently\nverify the correctness of the results and confirm that our approximation is\nhighly accurate even for a moderate number of $10^6$ balls and bins.", 
    "link": "http://arxiv.org/pdf/1407.8433v1", 
    "arxiv-id": "1407.8433v1"
},{
    "category": "cs.DC", 
    "author": "Achim Streit", 
    "title": "Energy-Aware Cloud Management through Progressive SLA Specification", 
    "publish": "2014-09-01T08:35:08Z", 
    "summary": "Novel energy-aware cloud management methods dynamically reallocate\ncomputation across geographically distributed data centers to leverage regional\nelectricity price and temperature differences. As a result, a managed VM may\nsuffer occasional downtimes. Current cloud providers only offer high\navailability VMs, without enough flexibility to apply such energy-aware\nmanagement. In this paper we show how to analyse past traces of dynamic cloud\nmanagement actions based on electricity prices and temperatures to estimate VM\navailability and price values. We propose a novel SLA specification approach\nfor offering VMs with different availability and price values guaranteed over\nmultiple SLAs to enable flexible energy-aware cloud management. We determine\nthe optimal number of such SLAs as well as their availability and price\nguaranteed values. We evaluate our approach in a user SLA selection simulation\nusing Wikipedia and Grid'5000 workloads. The results show higher customer\nconversion and 39% average energy savings per VM.", 
    "link": "http://arxiv.org/pdf/1409.0325v1", 
    "arxiv-id": "1409.0325v1"
},{
    "category": "cs.DC", 
    "author": "Gabriel L. Muller", 
    "title": "HTML5 WebSocket protocol and its application to distributed computing", 
    "publish": "2014-09-11T09:36:46Z", 
    "summary": "HTML5 WebSocket protocol brings real time communication in web browsers to a\nnew level. Daily, new products are designed to stay permanently connected to\nthe web. WebSocket is the technology enabling this revolution. WebSockets are\nsupported by all current browsers, but it is still a new technology in constant\nevolution.\n  WebSockets are slowly replacing older client-server communication\ntechnologies. As opposed to comet-like technologies WebSockets' remarkable\nperformances is a result of the protocol's fully duplex nature and because it\ndoesn't rely on HTTP communications.\n  To begin with this paper studies the WebSocket protocol and different\nWebSocket servers implementations. This first theoretic part focuses more\ndeeply on heterogeneous implementations and OpenCL. The second part is a\nbenchmark of a new promising library.\n  The real-time engine used for testing purposes is SocketCluster.\nSocketCluster provides a highly scalable WebSocket server that makes use of all\navailable cpu cores on an instance. The scope of this work is reduced to\nvertical scaling of SocketCluster.", 
    "link": "http://arxiv.org/pdf/1409.3367v1", 
    "arxiv-id": "1409.3367v1"
},{
    "category": "cs.DC", 
    "author": "Dushyant Vaghela", 
    "title": "An Advanced Approach On Load Balancing in Grid Computing", 
    "publish": "2014-09-12T05:40:34Z", 
    "summary": "With the rapid development in wide area networks and low cost, powerful\ncomputational resources, grid computing has gained its popularity. With the\nadvent of grid computing, space limitations of conventional distributed systems\ncan be overcome and underutilized computing resources at different locations\naround the world can be put to distributed jobs. Workload and resource\nmanagement is the main key grid services at the service level of grid\ninfrastructures, out of which load balancing in the main concern for grid\ndevelopers. It has been found that load is the major problem which server\nfaces, especially when the number of users increases. A lot of research is\nbeing done in the area of load management. This paper presents the various\nmechanisms of load balancing in grid computing so that the readers will get an\nidea of which algorithm would be suitable in different situations. Keywords:\nwide area network, distributed computing, load balancing.", 
    "link": "http://arxiv.org/pdf/1409.3651v1", 
    "arxiv-id": "1409.3651v1"
},{
    "category": "cs.DC", 
    "author": "Evgeny Nikulchev", 
    "title": "Virtual Laboratories in Cloud Infrastructure of Educational Institutions", 
    "publish": "2014-09-14T17:53:49Z", 
    "summary": "Modern educational institutions widely used virtual laboratories and cloud\ntechnologies. In practice must deal with security, processing speed and other\ntasks. The paper describes the experience of the construction of an\nexperimental stand cloud computing and network management. Models and control\nprinciples set forth herein.", 
    "link": "http://arxiv.org/pdf/1409.4082v1", 
    "arxiv-id": "1409.4082v1"
},{
    "category": "cs.DC", 
    "author": "Simon Payain", 
    "title": "Laboratory Test Bench for Research Network and Cloud Computing", 
    "publish": "2014-09-14T18:00:13Z", 
    "summary": "At present moment, there is a great interest in development of information\nsystems operating in cloud infrastructures. Generally, many of tasks remain\nunresolved such as tasks of optimization of large databases in a hybrid cloud\ninfrastructure, quality of service (QoS) at different levels of cloud services,\ndynamic control of distribution of cloud resources in application systems and\nmany others. Research and development of new solutions can be limited in case\nof using emulators or international commercial cloud services, due to the\nclosed architecture and limited opportunities for experimentation. Article\nprovides answers to questions on the establishment of a pilot cloud practically\n\"at home\" with the ability to adjust the width of the emulation channel and\ndelays in data transmission. It also describes architecture and configuration\nof the experimental setup. The proposed modular structure can be expanded by\navailable computing power.", 
    "link": "http://arxiv.org/pdf/1409.4626v1", 
    "arxiv-id": "1409.4626v1"
},{
    "category": "cs.DC", 
    "author": "Alexander A. Schwarzmann", 
    "title": "Doing-it-All with Bounded Work and Communication", 
    "publish": "2014-09-16T17:39:27Z", 
    "summary": "We consider the Do-All problem, where $p$ cooperating processors need to\ncomplete $t$ similar and independent tasks in an adversarial setting. Here we\ndeal with a synchronous message passing system with processors that are subject\nto crash failures. Efficiency of algorithms in this setting is measured in\nterms of work complexity (also known as total available processor steps) and\ncommunication complexity (total number of point-to-point messages). When work\nand communication are considered to be comparable resources, then the overall\nefficiency is meaningfully expressed in terms of effort defined as work +\ncommunication. We develop and analyze a constructive algorithm that has work\n${\\cal O}( t + p \\log p\\, (\\sqrt{p\\log p}+\\sqrt{t\\log t}\\, ) )$ and a\nnonconstructive algorithm that has work ${\\cal O}(t +p \\log^2 p)$. The latter\nresult is close to the lower bound $\\Omega(t + p \\log p/ \\log \\log p)$ on work.\nThe effort of each of these algorithms is proportional to its work when the\nnumber of crashes is bounded above by $c\\,p$, for some positive constant $c <\n1$. We also present a nonconstructive algorithm that has effort ${\\cal O}(t + p\n^{1.77})$.", 
    "link": "http://arxiv.org/pdf/1409.4711v2", 
    "arxiv-id": "1409.4711v2"
},{
    "category": "cs.DC", 
    "author": "Holger Machens", 
    "title": "Sandboxing for Software Transactional Memory with Deferred Updates", 
    "publish": "2014-09-18T14:26:22Z", 
    "summary": "Software transactional memory implementations which allow transactions to\nwork on inconsistent states of shared data, risk to cause application visible\nerrors such as memory access violations or endless loops. Hence, many\nimplementations rely on repeated incremental validation of every read of the\ntransaction to always guarantee for a consistent view of shared data. Because\nthis eager validation technique generates significant processing costs several\nproposals have been published to establish a sandbox for transactions, which\ntransparently prevents or suppresses those errors and thereby allows to reduce\nthe frequency of in-flight validations.\n  The most comprehensive sandboxing concept of transactions in software\ntransactional memory based on deferred updates and considering unmanaged\nlanguages, integrates multiple techniques such as signal interposition,\nout-of-band validation and static and dynamic instrumentation. The latter\ncomprises the insertion of a validation barrier in front of every direct write\nwhich addresses the execution stack of the thread and potentially results from\nunvalidated reads.\n  This paper basically results from a review of this sandboxing approach, which\nrevealed some improvements for sandboxing on C/C++. Based on knowledge about\nthe runtime environment and the compiler an error model has been developed to\nidentify critical paths to application visible errors. This analysis lead to a\nconcept for stack protection with less frequent validation, an alternative\nout-of-band validation technique and revealed additional risks of so-called\nwaivered regions without instrumentation inside transactions.", 
    "link": "http://arxiv.org/pdf/1409.5313v2", 
    "arxiv-id": "1409.5313v2"
},{
    "category": "cs.DC", 
    "author": "Kazi Sakib", 
    "title": "Watchword-Oriented and Time-Stamped Algorithms for Tamper-Proof Cloud   Provenance Cognition", 
    "publish": "2014-09-19T08:33:15Z", 
    "summary": "Provenance is derivative journal information about the origin and activities\nof system data and processes. For a highly dynamic system like the cloud,\nprovenance can be accurately detected and securely used in cloud digital\nforensic investigation activities. This paper proposes watchword oriented\nprovenance cognition algorithm for the cloud environment. Additionally\ntime-stamp based buffer verifying algorithm is proposed for securing the access\nto the detected cloud provenance. Performance analysis of the novel algorithms\nproposed here yields a desirable detection rate of 89.33% and miss rate of\n8.66%. The securing algorithm successfully rejects 64% of malicious requests,\nyielding a cumulative frequency of 21.43 for MR.", 
    "link": "http://arxiv.org/pdf/1409.5546v1", 
    "arxiv-id": "1409.5546v1"
},{
    "category": "cs.DC", 
    "author": "Kazi Sakib", 
    "title": "Active-Threaded Algorithms for Provenance Cognition in the Cloud   preserving Low Overhead and Fault Tolerance", 
    "publish": "2014-09-19T08:47:51Z", 
    "summary": "Provenance is the derivation history of information about the origin of data\nand processes. For a highly dynamic system such as the cloud, provenance must\nbe effectively detected to be used as proves to ensure accountability during\ndigital forensic investigations. This paper proposes active-threaded provenance\ncognition algorithms that ensure effective and high speed detection of\nprovenance information in the activity layer of the cloud. The algorithms also\nsupport encapsulation of the provenance information on specific targets.\nPerformance evaluation of the proposed algorithms reveal mean delay of 8.198\nseconds that is below the pre-defined benchmark of 10 seconds. Standard\ndeviation and cumulative frequencies for delays are found to be 1.434 and 45.1%\nrespectively.", 
    "link": "http://arxiv.org/pdf/1409.5552v1", 
    "arxiv-id": "1409.5552v1"
},{
    "category": "cs.DC", 
    "author": "Philipp Hoenisch", 
    "title": "Elastic Business Process Management: State of the Art and Open   Challenges for BPM in the Cloud", 
    "publish": "2014-09-19T16:36:49Z", 
    "summary": "With the advent of cloud computing, organizations are nowadays able to react\nrapidly to changing demands for computational resources. Not only individual\napplications can be hosted on virtual cloud infrastructures, but also complete\nbusiness processes. This allows the realization of so-called elastic processes,\ni.e., processes which are carried out using elastic cloud resources. Despite\nthe manifold benefits of elastic processes, there is still a lack of solutions\nsupporting them.\n  In this paper, we identify the state of the art of elastic Business Process\nManagement with a focus on infrastructural challenges. We conceptualize an\narchitecture for an elastic Business Process Management System and discuss\nexisting work on scheduling, resource allocation, monitoring, decentralized\ncoordination, and state management for elastic processes. Furthermore, we\npresent two representative elastic Business Process Management Systems which\nare intended to counter these challenges. Based on our findings, we identify\nopen issues and outline possible research directions for the realization of\nelastic processes and elastic Business Process Management.", 
    "link": "http://arxiv.org/pdf/1409.5715v2", 
    "arxiv-id": "1409.5715v2"
},{
    "category": "cs.DC", 
    "author": "Aashiha Priyadarshni. L", 
    "title": "Heterogeneous Multi core processors for improving the efficiency of   Market basket analysis algorithm in data mining", 
    "publish": "2014-09-23T17:44:59Z", 
    "summary": "Heterogeneous multi core processors can offer diverse computing capabilities.\nThe efficiency of Market Basket Analysis Algorithm can be improved with\nheterogeneous multi core processors. Market basket analysis algorithm utilises\napriori algorithm and is one of the popular data mining algorithms which can\nutilise Map/Reduce framework to perform analysis. The algorithm generates\nassociation rules based on transactional data and Map/Reduce motivates to\nredesign and convert the existing sequential algorithms for efficiency. Hadoop\nis the parallel programming platform built on Hadoop Distributed File\nSystems(HDFS) for Map/Reduce computation that process data as (key, value)\npairs. In Hadoop map/reduce, the sequential jobs are parallelised and the Job\nTracker assigns parallel tasks to the Task Tracker. Based on single threaded or\nmultithreaded parallel tasks in the task tracker, execution is carried out in\nthe appropriate cores. For this, a new scheduler called MB Scheduler can be\ndeveloped. Switching between the cores can be made static or dynamic. The use\nof heterogeneous multi core processors optimizes processing capabilities and\npower requirements for a processor and improves the performance of the system.", 
    "link": "http://arxiv.org/pdf/1409.6679v1", 
    "arxiv-id": "1409.6679v1"
},{
    "category": "cs.DC", 
    "author": "Adam Barker", 
    "title": "Workflow Partitioning and Deployment on the Cloud using Orchestra", 
    "publish": "2014-09-29T12:39:13Z", 
    "summary": "Orchestrating service-oriented workflows is typically based on a design model\nthat routes both data and control through a single point - the centralised\nworkflow engine. This causes scalability problems that include the unnecessary\nconsumption of the network bandwidth, high latency in transmitting data between\nthe services, and performance bottlenecks. These problems are highly prominent\nwhen orchestrating workflows that are composed from services dispersed across\ndistant geographical locations. This paper presents a novel workflow\npartitioning approach, which attempts to improve the scalability of\norchestrating large-scale workflows. It permits the workflow computation to be\nmoved towards the services providing the data in order to garner optimal\nperformance results. This is achieved by decomposing the workflow into smaller\nsub workflows for parallel execution, and determining the most appropriate\nnetwork locations to which these sub workflows are transmitted and subsequently\nexecuted. This paper demonstrates the efficiency of our approach using a set of\nexperimental workflows that are orchestrated over Amazon EC2 and across several\ngeographic network regions.", 
    "link": "http://arxiv.org/pdf/1409.8098v1", 
    "arxiv-id": "1409.8098v1"
},{
    "category": "cs.DC", 
    "author": "Robbert van Renesse", 
    "title": "Cache Serializability: Reducing Inconsistency in Edge Transactions", 
    "publish": "2014-09-29T20:53:34Z", 
    "summary": "Read-only caches are widely used in cloud infrastructures to reduce access\nlatency and load on backend databases. Operators view coherent caches as\nimpractical at genuinely large scale and many client-facing caches are updated\nin an asynchronous manner with best-effort pipelines. Existing solutions that\nsupport cache consistency are inapplicable to this scenario since they require\na round trip to the database on every cache transaction.\n  Existing incoherent cache technologies are oblivious to transactional data\naccess, even if the backend database supports transactions. We propose T-Cache,\na novel caching policy for read-only transactions in which inconsistency is\ntolerable (won't cause safety violations) but undesirable (has a cost). T-Cache\nimproves cache consistency despite asynchronous and unreliable communication\nbetween the cache and the database. We define cache-serializability, a variant\nof serializability that is suitable for incoherent caches, and prove that with\nunbounded resources T-Cache implements this new specification. With limited\nresources, T-Cache allows the system manager to choose a trade-off between\nperformance and consistency.\n  Our evaluation shows that T-Cache detects many inconsistencies with only\nnominal overhead. We use synthetic workloads to demonstrate the efficacy of\nT-Cache when data accesses are clustered and its adaptive reaction to workload\nchanges. With workloads based on the real-world topologies, T-Cache detects\n43-70% of the inconsistencies and increases the rate of consistent transactions\nby 33-58%.", 
    "link": "http://arxiv.org/pdf/1409.8324v3", 
    "arxiv-id": "1409.8324v3"
},{
    "category": "cs.DC", 
    "author": "Adrian Vladu", 
    "title": "How to Elect a Leader Faster than a Tournament", 
    "publish": "2014-11-04T18:57:28Z", 
    "summary": "The problem of electing a leader from among $n$ contenders is one of the\nfundamental questions in distributed computing. In its simplest formulation,\nthe task is as follows: given $n$ processors, all participants must eventually\nreturn a win or lose indication, such that a single contender may win. Despite\na considerable amount of work on leader election, the following question is\nstill open: can we elect a leader in an asynchronous fault-prone system faster\nthan just running a $\\Theta(\\log n)$-time tournament, against a strong adaptive\nadversary?\n  In this paper, we answer this question in the affirmative, improving on a\ndecades-old upper bound. We introduce two new algorithmic ideas to reduce the\ntime complexity of electing a leader to $O(\\log^* n)$, using $O(n^2)$\npoint-to-point messages. A non-trivial application of our algorithm is a new\nupper bound for the tight renaming problem, assigning $n$ items to the $n$\nparticipants in expected $O(\\log^2 n)$ time and $O(n^2)$ messages. We\ncomplement our results with lower bound of $\\Omega(n^2)$ messages for solving\nthese two problems, closing the question of their message complexity.", 
    "link": "http://arxiv.org/pdf/1411.1001v2", 
    "arxiv-id": "1411.1001v2"
},{
    "category": "cs.DC", 
    "author": "Adam Barker", 
    "title": "BigExcel: A Web-Based Framework for Exploring Big Data in Social   Sciences", 
    "publish": "2014-11-05T10:22:27Z", 
    "summary": "This paper argues that there are three fundamental challenges that need to be\novercome in order to foster the adoption of big data technologies in\nnon-computer science related disciplines: addressing issues of accessibility of\nsuch technologies for non-computer scientists, supporting the ad hoc\nexploration of large data sets with minimal effort and the availability of\nlightweight web-based frameworks for quick and easy analytics. In this paper,\nwe address the above three challenges through the development of 'BigExcel', a\nthree tier web-based framework for exploring big data to facilitate the\nmanagement of user interactions with large data sets, the construction of\nqueries to explore the data set and the management of the infrastructure. The\nfeasibility of BigExcel is demonstrated through two Yahoo Sandbox datasets. The\nfirst dataset is the Yahoo Buzz Score data set we use for quantitatively\npredicting trending technologies and the second is the Yahoo n-gram corpus we\nuse for qualitatively inferring the coverage of important events. A\ndemonstration of the BigExcel framework and source code is available at\nhttp://bigdata.cs.st-andrews.ac.uk/projects/bigexcel-exploring-big-data-for-social-sciences/.", 
    "link": "http://arxiv.org/pdf/1411.1215v1", 
    "arxiv-id": "1411.1215v1"
},{
    "category": "cs.DC", 
    "author": "Sasikala Gowtham", 
    "title": "An Experimental Evaluation of Performance of A Hadoop Cluster on Replica   Management", 
    "publish": "2014-11-07T14:29:07Z", 
    "summary": "Hadoop is an open source implementation of the MapReduce Framework in the\nrealm of distributed processing. A Hadoop cluster is a unique type of\ncomputational cluster designed for storing and analyzing large data sets across\ncluster of workstations. To handle massive scale data, Hadoop exploits the\nHadoop Distributed File System termed as HDFS. The HDFS similar to most\ndistributed file systems share a familiar problem on data sharing and\navailability among compute nodes, often which leads to decrease in performance.\nThis paper is an experimental evaluation of Hadoop's computing performance\nwhich is made by designing a rack aware cluster that utilizes the Hadoop's\ndefault block placement policy to improve data availability. Additionally, an\nadaptive data replication scheme that relies on access count prediction using\nLangrange's interpolation is adapted to fit the scenario. To prove, experiments\nwere conducted on a rack aware cluster setup which significantly reduced the\ntask completion time, but once the volume of the data being processed increases\nthere is a considerable cutback in computational speeds due to update cost.\nFurther the threshold level for balance between the update cost and replication\nfactor is identified and presented graphically.", 
    "link": "http://arxiv.org/pdf/1411.1931v1", 
    "arxiv-id": "1411.1931v1"
},{
    "category": "cs.DC", 
    "author": "Manuel P\u00f6ter", 
    "title": "Pheet meets C++11", 
    "publish": "2014-11-07T15:40:48Z", 
    "summary": "Pheet is a C++ task-scheduling framework that allows for easy customization\nof internal data-structures. The implementation was started before the C++11\nstandard was committed and therefore did not use the new standardized memory\nmodel but compiler/platform specific intrinsics for atomic memory operations.\nThis not only makes the implementation harder to port to other compilers or\narchitectures but also suffers from the fact that prior C++ versions did not\nspecify any memory model.\n  In this report I discuss the porting of one of the internal Pheet data\nstructures to the new memory model and provide reasoning about the correctness\nbased on the semantics of the memory consistency model. Using two benchmarks\nfrom the Pheet benchmark suite I compare the performance of the original\nagainst the new implementation which shows a significant speedup under certain\nconditions on one of the two test machines.", 
    "link": "http://arxiv.org/pdf/1411.1951v1", 
    "arxiv-id": "1411.1951v1"
},{
    "category": "cs.DC", 
    "author": "Christine Morin", 
    "title": "Checkpointing as a Service in Heterogeneous Cloud Environments", 
    "publish": "2014-11-07T15:59:47Z", 
    "summary": "A non-invasive, cloud-agnostic approach is demonstrated for extending\nexisting cloud platforms to include checkpoint-restart capability. Most cloud\nplatforms currently rely on each application to provide its own fault\ntolerance. A uniform mechanism within the cloud itself serves two purposes: (a)\ndirect support for long-running jobs, which would otherwise require a custom\nfault-tolerant mechanism for each application; and (b) the administrative\ncapability to manage an over-subscribed cloud by temporarily swapping out jobs\nwhen higher priority jobs arrive. An advantage of this uniform approach is that\nit also supports parallel and distributed computations, over both TCP and\nInfiniBand, thus allowing traditional HPC applications to take advantage of an\nexisting cloud infrastructure. Additionally, an integrated health-monitoring\nmechanism detects when long-running jobs either fail or incur exceptionally low\nperformance, perhaps due to resource starvation, and proactively suspends the\njob. The cloud-agnostic feature is demonstrated by applying the implementation\nto two very different cloud platforms: Snooze and OpenStack. The use of a\ncloud-agnostic architecture also enables, for the first time, migration of\napplications from one cloud platform to another.", 
    "link": "http://arxiv.org/pdf/1411.1958v2", 
    "arxiv-id": "1411.1958v2"
},{
    "category": "cs.DC", 
    "author": "Michael Walfish", 
    "title": "Yesquel: scalable SQL storage for Web applications", 
    "publish": "2014-11-08T20:32:17Z", 
    "summary": "Based on a brief history of the storage systems for Web applications, we\nmotivate the need for a new storage system. We then describe the architecture\nof such a system, called Yesquel. Yesquel supports the SQL query language and\noffers performance similar to NOSQL storage systems.", 
    "link": "http://arxiv.org/pdf/1411.2160v1", 
    "arxiv-id": "1411.2160v1"
},{
    "category": "cs.DC", 
    "author": "Juergen Cito", 
    "title": "Patterns in the Chaos - a Study of Performance Variation and   Predictability in Public IaaS Clouds", 
    "publish": "2014-11-10T13:58:58Z", 
    "summary": "Benchmarking the performance of public cloud providers is a common research\ntopic. Previous research has already extensively evaluated the performance of\ndifferent cloud platforms for different use cases, and under different\nconstraints and experiment setups. In this paper, we present a principled,\nlarge-scale literature review to collect and codify existing research regarding\nthe predictability of performance in public Infrastructure-as-a-Service (IaaS)\nclouds. We formulate 15 hypotheses relating to the nature of performance\nvariations in IaaS systems, to the factors of influence of performance\nvariations, and how to compare different instance types. In a second step, we\nconduct extensive real-life experimentation on Amazon EC2 and Google Compute\nEngine to empirically validate those hypotheses. At the time of our research,\nperformance in EC2 was substantially less predictable than in GCE. Further, we\nshow that hardware heterogeneity is in practice less prevalent than anticipated\nby earlier research, while multi-tenancy has a dramatic impact on performance\nand predictability.", 
    "link": "http://arxiv.org/pdf/1411.2429v2", 
    "arxiv-id": "1411.2429v2"
},{
    "category": "cs.DC", 
    "author": "Qiang Zeng", 
    "title": "Hybrid Ant Colony Algorithm Clonal Selection in the Application of the   Cloud's Resource Scheduling", 
    "publish": "2014-11-10T18:42:11Z", 
    "summary": "In this paper, thinking over characteristics of ant colony optimization\nAlgorithm, taking into account the characteristics of cloud computing, combined\nwith clonal selection algorithm (CSA) global optimum advantage of the\nconvergence of the clonal selection algorithm (CSA) into every ACO iteration,\nspeeding up the convergence rate, and the introduction of reverse mutation\nstrategy, ant colony optimization algorithm avoids local optimum. Depth study\nof the cloud environment ant colony clonal selection algorithm resource\nscheduling policy, clonal selection algorithm converges to solve optimization\nproblems when sufficient condition for global optimal solution based on clonal\nselection algorithm for various applications such as BCA and CLONALG algorithm,\nusing these sufficient condition to meet and simulation platform CloudSim\nachieve a simulation by extending the cloud. Experimental results show that\nthis task can be shortened fusion algorithm running time cloud environment,\nimprove resource utilization. Demonstrate the effectiveness of the method.", 
    "link": "http://arxiv.org/pdf/1411.2528v1", 
    "arxiv-id": "1411.2528v1"
},{
    "category": "cs.DC", 
    "author": "Zizhong Chen", 
    "title": "Algorithmic Energy Saving for Parallel Cholesky, LU, and QR   Factorizations", 
    "publish": "2014-11-10T18:59:19Z", 
    "summary": "The pressing demands of improving energy efficiency for high performance\nscientific computing have motivated a large body of software-controlled hard-\nware solutions using Dynamic Voltage and Frequency Scaling (DVFS) that\nstrategically switch processors to low-power states, when the peak processor\nperformance is not necessary. Although OS level solutions have demonstrated the\neffectiveness of saving energy in a black-box fashion, for applications with\nvariable execution characteristics, the optimal energy efficiency can be\nblundered away due to defective prediction mechanism and untapped load\nimbalance. In this paper, we propose TX, a library level race-to-halt DVFS\nscheduling approach that analyzes Task Dependency Set of each task in parallel\nCholesky, LU, and QR factorizations to achieve substantial energy savings OS\nlevel solutions cannot fulfill. Partially giving up the generality of OS level\nsolutions per requiring library level source modification, TX lever- ages\nalgorithmic characteristics of the applications to gain greater energy savings.\nExperimental results on two power-aware clusters indicate that TX can save up\nto 17.8% more energy than state-of-the-art OS level solutions with negligible\n3.5% on average performance loss.", 
    "link": "http://arxiv.org/pdf/1411.2536v2", 
    "arxiv-id": "1411.2536v2"
},{
    "category": "cs.DC", 
    "author": "Umesh Bellur", 
    "title": "Novel Power and Completion Time Models for Virtualized Environments", 
    "publish": "2014-11-12T15:16:51Z", 
    "summary": "Power consumption costs takes upto half of operational expenses of\ndatacenters making power management a critical concern. Advances in processor\ntechnology provide fine-grained control over operating frequency and voltage of\nprocessors and this control can be used to tradeoff power for performance.\nAlthough many power and performance models exist, they have a significant error\nmargin while predicting the performance of memory or file-intensive tasks and\nHPC applications. Our investigations reveal that the prediction error is due in\npart to the fact that they do not take frequency AND CPU variations account,\nrather they just depend on the CPU by itself.\n  In this paper, we empirically derive power and completion time models using\nlinear regression with CPU utilization and operating frequency as parameters.\nWe validate our power model on several Intel and AMD processors by predicting\nwithin 2-7% of measured power. We validate our completion time model using five\nkernels of NASA Parallel Benchmark suite and five CPU, memory and\nfile-intensive benchmarks on four heterogeneous systems and predicting within\n1-6% of observed performance. We then show how these models can be employed to\nrealize as much as 15% savings in power while delivering 44% better performance\nfor applications deployed in a virtualized environment.", 
    "link": "http://arxiv.org/pdf/1411.3201v1", 
    "arxiv-id": "1411.3201v1"
},{
    "category": "cs.DC", 
    "author": "Subhash Kak", 
    "title": "Efficiency of Matrix Multiplication on the Cross-Wired Mesh Array", 
    "publish": "2014-11-12T18:35:49Z", 
    "summary": "This note looks at the efficiency of the cross-wired mesh array in the\ncontext of matrix multiplication. It is shown that in case of repeated\noperations, the average number of steps to multiply sets of nxn matrices on a\n2D cross-wired mesh array approaches n.", 
    "link": "http://arxiv.org/pdf/1411.3273v1", 
    "arxiv-id": "1411.3273v1"
},{
    "category": "cs.DC", 
    "author": "Olivia Das", 
    "title": "Performance-oriented Cloud Provisioning: Taxonomy and Survey", 
    "publish": "2014-11-19T00:04:52Z", 
    "summary": "Cloud computing is being viewed as the technology of today and the future.\nThrough this paradigm, the customers gain access to shared computing resources\nlocated in remote data centers that are hosted by cloud providers (CP). This\ntechnology allows for provisioning of various resources such as virtual\nmachines (VM), physical machines, processors, memory, network, storage and\nsoftware as per the needs of customers. Application providers (AP), who are\ncustomers of the CP, deploy applications on the cloud infrastructure and then\nthese applications are used by the end-users. To meet the fluctuating\napplication workload demands, dynamic provisioning is essential and this\narticle provides a detailed literature survey of dynamic provisioning within\ncloud systems with focus on application performance. The well-known types of\nprovisioning and the associated problems are clearly and pictorially explained\nand the provisioning terminology is clarified. A very detailed and general\ncloud provisioning classification is presented, which views provisioning from\ndifferent perspectives, aiding in understanding the process inside-out. Cloud\ndynamic provisioning is explained by considering resources, stakeholders,\ntechniques, technologies, algorithms, problems, goals and more.", 
    "link": "http://arxiv.org/pdf/1411.5077v1", 
    "arxiv-id": "1411.5077v1"
},{
    "category": "cs.DC", 
    "author": "Nitin Vaidya", 
    "title": "Reaching Approximate Byzantine Consensus with Multi-hop Communication", 
    "publish": "2014-11-19T16:32:24Z", 
    "summary": "We address the problem of reaching consensus in the presence of Byzantine\nfaults. In particular, we are interested in investigating the impact of\nmessages relay on the network connectivity for a correct iterative approximate\nByzantine consensus algorithm to exist. The network is modeled by a simple\ndirected graph. We assume a node can send messages to another node that is up\nto $l$ hops away via forwarding by the intermediate nodes on the routes, where\n$l\\in \\mathbb{N}$ is a natural number. We characterize the necessary and\nsufficient topological conditions on the network structure. The tight\nconditions we found are consistent with the tight conditions identified for\n$l=1$, where only local communication is allowed, and are strictly weaker for\n$l>1$. Let $l^*$ denote the length of a longest path in the given network. For\n$l\\ge l^*$ and undirected graphs, our conditions hold if and only if $n\\ge\n3f+1$ and the node-connectivity of the given graph is at least $2f+1$ , where\n$n$ is the total number of nodes and $f$ is the maximal number of Byzantine\nnodes; and for $l\\ge l^*$ and directed graphs, our conditions is equivalent to\nthe tight condition found for exact Byzantine consensus.\n  Our sufficiency is shown by constructing a correct algorithm, wherein the\ntrim function is constructed based on investigating a newly introduced minimal\nmessages cover property. The trim function proposed also works over\nmulti-graphs.", 
    "link": "http://arxiv.org/pdf/1411.5282v2", 
    "arxiv-id": "1411.5282v2"
},{
    "category": "cs.DC", 
    "author": "Mazin Nasser", 
    "title": "Parallelize Bubble and Merge Sort Algorithms Using Message Passing   Interface (MPI)", 
    "publish": "2014-11-19T16:35:16Z", 
    "summary": "Sorting has been a profound area for the algorithmic researchers and many\nresources are invested to suggest more works for sorting algorithms. For this\npurpose, many existing sorting algorithms were observed in terms of the\nefficiency of the algorithmic complexity. In this paper we implemented the\nbubble and merge sort algorithms using Message Passing Interface (MPI)\napproach. The proposed work tested on two standard datasets (text file) with\ndifferent size. The main idea of the proposed algorithm is distributing the\nelements of the input datasets into many additional temporary sub-arrays\naccording to a number of characters in each word. The sizes of each of these\nsub-arrays are decided depending on a number of elements with the same number\nof characters in the input array. We implemented MPI using Intel core i7-3610QM\n,(8 CPUs),using two approaches (vectors of string and array 3D) . Finally, we\nget the data structure effects on the performance of the algorithm for that we\nchoice the second approach.", 
    "link": "http://arxiv.org/pdf/1411.5283v1", 
    "arxiv-id": "1411.5283v1"
},{
    "category": "cs.DC", 
    "author": "Vasudeva Varma", 
    "title": "Energy and SLA aware VM Scheduling", 
    "publish": "2014-11-22T11:21:01Z", 
    "summary": "With the advancement of Cloud Computing over the past few years, there has\nbeen a massive shift from traditional data centers to cloud enabled data\ncenters. The enterprises with cloud data centers are focusing their attention\non energy savings through effective utilization of resources. In this work, we\npropose algorithms which try to minimize the energy consumption in the data\ncenter duly maintaining the SLA guarantees. The algorithms try to utilize least\nnumber of physical machines in the data center by dynamically rebalancing the\nphysical machines based on their resource utilization. The algorithms also\nperform an optimal consolidation of virtual machines on a physical machine,\nminimizing SLA violations. In extensive simulation, our algorithms achieve\nsavings of about 21% in terms of energy consumption and in terms of maintaining\nthe SLAs, it performs 60% better than Single Threshold algorithm.", 
    "link": "http://arxiv.org/pdf/1411.6114v1", 
    "arxiv-id": "1411.6114v1"
},{
    "category": "cs.DC", 
    "author": "Fran\u00e7ois Ta\u00efani", 
    "title": "Fisheye Consistency: Keeping Data in Synch in a Georeplicated World", 
    "publish": "2014-11-24T15:12:39Z", 
    "summary": "Over the last thirty years, numerous consistency conditions for replicated\ndata have been proposed and implemented. Popular examples of such conditions\ninclude linearizability (or atomicity), sequential consistency, causal\nconsistency, and eventual consistency. These consistency conditions are usually\ndefined independently from the computing entities (nodes) that manipulate the\nreplicated data; i.e., they do not take into account how computing entities\nmight be linked to one another, or geographically distributed. To address this\nlack, as a first contribution, this paper introduces the notion of proximity\ngraph between computing nodes. If two nodes are connected in this graph, their\noperations must satisfy a strong consistency condition, while the operations\ninvoked by other nodes are allowed to satisfy a weaker condition. The second\ncontribution is the use of such a graph to provide a generic approach to the\nhybridization of data consistency conditions into the same system. We\nillustrate this approach on sequential consistency and causal consistency, and\npresent a model in which all data operations are causally consistent, while\noperations by neighboring processes in the proximity graph are sequentially\nconsistent. The third contribution of the paper is the design and the proof of\na distributed algorithm based on this proximity graph, which combines\nsequential consistency and causal consistency (the resulting condition is\ncalled fisheye consistency). In doing so the paper not only extends the domain\nof consistency conditions, but provides a generic provably correct solution of\ndirect relevance to modern georeplicated systems.", 
    "link": "http://arxiv.org/pdf/1411.6478v2", 
    "arxiv-id": "1411.6478v2"
},{
    "category": "cs.DC", 
    "author": "Inderveer Chana", 
    "title": "Metrics based Workload Analysis Technique for IaaS Cloud", 
    "publish": "2014-11-25T07:27:49Z", 
    "summary": "The Dynamic Scalability of resources, a problem in Infrastructure as a\nService (IaaS) has been the hotspot for research and industry communities. The\nheterogeneous and dynamic nature of the Cloud workloads depends on the Quality\nof Service (QoS) allocation of appropriate workloads to appropriate resources.\nA workload is an abstraction of work that instance or set of instances that are\ngoing to perform. Running a web service or being a Hadoop data node is valid\nworkloads. The efficient management of dynamic nature resources can be done\nwith the help of workloads. Until workload is considered a fundamental\ncapability, the Cloud resources cannot be utilized in an efficient manner. In\nthis paper, different workloads have been identified and categorized along with\ntheir characteristics and constraints. The metrics based on Quality of Service\n(QoS) requirements have been identified for each workload and have been\nanalyzed for creating better application design.", 
    "link": "http://arxiv.org/pdf/1411.6753v1", 
    "arxiv-id": "1411.6753v1"
},{
    "category": "cs.DC", 
    "author": "Pranav Gadekar", 
    "title": "Addressing NameNode Scalability Issue in Hadoop Distributed File System   using Cache Approach", 
    "publish": "2014-11-25T09:10:29Z", 
    "summary": "Hadoop is a distributed batch processing infrastructure which is currently\nbeing used for big data management. The foundation of Hadoop consists of Hadoop\nDistributed File System or HDFS. HDFS presents a client server architecture\ncomprised of a NameNode and many DataNodes. The NameNode stores the metadata\nfor the DataNodes and DataNode stores application data. The NameNode holds file\nsystem metadata in memory, and thus the limit to the number of files in a file\nsystem is governed by the amount of memory on the NameNode. Thus when the\nmemory on NameNode is full there is no further chance of increasing the cluster\ncapacity. In this paper we have used the concept of cache memory for handling\nthe issue of NameNode scalability. The focus of this paper is to highlight our\napproach that tries to enhance the current architecture and ensure that\nNameNode does not reach its threshold value soon.", 
    "link": "http://arxiv.org/pdf/1411.6775v1", 
    "arxiv-id": "1411.6775v1"
},{
    "category": "cs.DC", 
    "author": "Oussama Tahan", 
    "title": "Towards Efficient OpenMP Strategies for Non-Uniform Architectures", 
    "publish": "2014-11-26T08:15:52Z", 
    "summary": "Parallel processing is considered as todays and future trend for improving\nperformance of computers. Computing devices ranging from small embedded systems\nto big clusters of computers rely on parallelizing applications to reduce\nexecution time. Many of current computing systems rely on Non-Uniform Memory\nAccess (NUMA) based processors architectures. In these architectures, analyzing\nand considering the non-uniformity is of high importance for improving\nscalability of systems. In this paper, we analyze and develop a NUMA based\napproach for the OpenMP parallel programming model. Our technique applies a\nsmart threads allocation method and an advanced tasks scheduling strategy for\nreducing remote memory accesses and consequently their extra time consumption.\nWe implemented our approach within the NANOS runtime system. A set of tests was\nconducted using the BOTS benchmarks and results showed the capacity of our\ntechnique in improving the performance of OpenMP applications especially those\ndealing with a large amount of data.", 
    "link": "http://arxiv.org/pdf/1411.7131v1", 
    "arxiv-id": "1411.7131v1"
},{
    "category": "cs.DC", 
    "author": "A B M Moniruzzaman", 
    "title": "Analysis of Memory Ballooning Technique for Dynamic Memory Management of   Virtual Machines (VMs)", 
    "publish": "2014-11-26T19:33:13Z", 
    "summary": "Memory ballooning is dynamic memory management technique for virtual machines\n(VMs). Ballooning is a part of memory reclamation technique operations used by\na hypervisor to allow the physical host system to retrieve unused memory from\ncertain guest virtual machines (VMs) and share it with others. Memory\nballooning allows the total amount ofRAM required by guest VMs to exceed the\namount ofphysical RAM available on the host. Memory overcommitment enables a\nhigher consolidation ratio in a hypervisor. Using memory overcommitment, users\ncan consolidate VMs on a physical machine such that physical resources are\nutilized in an optimal manner while delivering good performance. Hence memory\nreclamation is an integral component ofmemory overcommitment. In this paper, we\naddress that the basic cause of memory that ballooning is memory overcommitment\nfrom using memory-intensive virtual machines. We compared to others reclamation\ntechnique and identify Cost Associate with Memory Ballooning in state of Memory\nOvercommitment. The objective of this paper is to analyse memory ballooning\ntechnique for dynamic memory management of VMs. For this analysis, VMware based\nvirtualization software e.g ESXi Server, vCenter Server, vSphere Client are\ninstalled and configured on the Centre for Innovation and Technology (CIT) Lab,\nDIU; for monitor and analyze VM performance for memory ballooning technique.\nThe performance ofmemory ballooning technique is evaluated with two different\ntest cases. The purpose is to help users understand, how this technique impact\nthe performance. Finally, we presents the throughput ofheavy workload with\ndifferent memory limits when using ballooning or swapping; and analyse VM\nperformance issue for this technique.", 
    "link": "http://arxiv.org/pdf/1411.7344v1", 
    "arxiv-id": "1411.7344v1"
},{
    "category": "cs.DC", 
    "author": "H. L. Phalachandra", 
    "title": "Efficient Support of Big Data Storage Systems on the Cloud", 
    "publish": "2014-11-27T09:25:54Z", 
    "summary": "Due to its advantages over traditional data centers, there has been a rapid\ngrowth in the usage of cloud infrastructures. These include public clouds\n(e.g., Amazon EC2), or private clouds, such as clouds deployed using OpenStack.\nA common factor in many of the well known infrastructures, for example\nOpenStack and CloudStack, is that networked storage is used for storage of\npersistent data. However, traditional Big Data systems, including Hadoop, store\ndata in commodity local storage for reasons of high performance and low cost.\nWe present an architecture for supporting Hadoop on Openstack using local\nstorage. Subsequently, we use benchmarks on Openstack and Amazon to show that\nfor supporting Hadoop, local storage has better performance and lower cost. We\nconclude that cloud systems should support local storage for persistent data\n(in addition to networked storage) so as to provide efficient support for\nHadoop and other Big Data systems", 
    "link": "http://arxiv.org/pdf/1411.7507v1", 
    "arxiv-id": "1411.7507v1"
},{
    "category": "cs.DC", 
    "author": "Debajyoti Mukhopadhyay", 
    "title": "Analyzing Web Application Log Files to Find Hit Count Through the   Utilization of Hadoop MapReduce in Cloud Computing Environment", 
    "publish": "2014-11-27T16:42:44Z", 
    "summary": "MapReduce has been widely applied in various fields of data and compute\nintensive applications and also it is important programming model for cloud\ncomputing. Hadoop is an open-source implementation of MapReduce which operates\non terabytes of data using commodity hardware. We have applied this Hadoop\nMapReduce programming model for analyzing web log files so that we could get\nhit count of specific web application. This system uses Hadoop file system to\nstore log file and results are evaluated using Map and Reduce function.\nExperimental results show hit count for each field in log file. Also due to\nMapReduce runtime parallelization response time is reduced.", 
    "link": "http://arxiv.org/pdf/1411.7639v1", 
    "arxiv-id": "1411.7639v1"
},{
    "category": "cs.DC", 
    "author": "Md. Sadekur Rahman", 
    "title": "A High Availability Clusters Model Combined with Load Balancing and   Shared Storage Technologies for Web Servers", 
    "publish": "2014-11-27T17:29:28Z", 
    "summary": "This paper designs and implements a high availability clusters and\nincorporated with load balance infrastructure of web servers. The paper\ndescribed system can provide full facilities to the website hosting provider\nand large business organizations. This system can provide continuous service\nthough any system components fail uncertainly with the help of Linux Virtual\nServer (LVS) loadbalancing cluster technology and combined with virtualization\nas well as shared storage technology to achieve the three-tier architecture of\nWeb server clusters. This technology not only improves availability, but also\naffects the security and performance of the application services being\nrequested. Benefits of the system include node failover overcome; network\nfailover overcome; storage limitation overcome and load distribution.", 
    "link": "http://arxiv.org/pdf/1411.7658v1", 
    "arxiv-id": "1411.7658v1"
},{
    "category": "cs.DC", 
    "author": "Daniel Kroening", 
    "title": "Faster linearizability checking via $P$-compositionality", 
    "publish": "2015-04-01T12:46:38Z", 
    "summary": "Linearizability is a well-established consistency and correctness criterion\nfor concurrent data types. An important feature of linearizability is Herlihy\nand Wing's locality principle, which says that a concurrent system is\nlinearizable if and only if all of its constituent parts (so-called objects)\nare linearizable. This paper presents $P$-compositionality, which generalizes\nthe idea behind the locality principle to operations on the same concurrent\ndata type. We implement $P$-compositionality in a novel linearizability\nchecker. Our experiments with over nine implementations of concurrent sets,\nincluding Intel's TBB library, show that our linearizability checker is one\norder of magnitude faster and/or more space efficient than the state-of-the-art\nalgorithm.", 
    "link": "http://arxiv.org/pdf/1504.00204v1", 
    "arxiv-id": "1504.00204v1"
},{
    "category": "cs.DC", 
    "author": "Christos Markou", 
    "title": "Using IKAROS to provide Scalable I/O bandwidth", 
    "publish": "2015-04-01T17:58:23Z", 
    "summary": "We present IKAROS as a utility that permit us to form scalable storage\nplatforms. IKAROS enable us to create ad-hoc nearby storage formations and use\na huge number of I/O nodes in order to increase the available bandwidth. We\nmeasure the performance and scalability of IKAROS versus the IBMs General\nParallel File System (GPFS) under a variety of conditions. The measurements are\nbased on benchmark programs that allow us to vary block sizes and to measure\naggregate throughput rates.", 
    "link": "http://arxiv.org/pdf/1504.00316v2", 
    "arxiv-id": "1504.00316v2"
},{
    "category": "cs.DC", 
    "author": "Onur Mutlu", 
    "title": "The Blacklisting Memory Scheduler: Balancing Performance, Fairness and   Complexity", 
    "publish": "2015-04-01T21:06:52Z", 
    "summary": "In a multicore system, applications running on different cores interfere at\nmain memory. This inter-application interference degrades overall system\nperformance and unfairly slows down applications. Prior works have developed\napplication-aware memory schedulers to tackle this problem. State-of-the-art\napplication-aware memory schedulers prioritize requests of applications that\nare vulnerable to interference, by ranking individual applications based on\ntheir memory access characteristics and enforcing a total rank order.\n  In this paper, we observe that state-of-the-art application-aware memory\nschedulers have two major shortcomings. First, such schedulers trade off\nhardware complexity in order to achieve high performance or fairness, since\nranking applications with a total order leads to high hardware complexity.\nSecond, ranking can unfairly slow down applications that are at the bottom of\nthe ranking stack. To overcome these shortcomings, we propose the Blacklisting\nMemory Scheduler (BLISS), which achieves high system performance and fairness\nwhile incurring low hardware complexity, based on two observations. First, we\nfind that, to mitigate interference, it is sufficient to separate applications\ninto only two groups. Second, we show that this grouping can be efficiently\nperformed by simply counting the number of consecutive requests served from\neach application.\n  We evaluate BLISS across a wide variety of workloads/system configurations\nand compare its performance and hardware complexity, with five state-of-the-art\nmemory schedulers. Our evaluations show that BLISS achieves 5% better system\nperformance and 25% better fairness than the best-performing previous scheduler\nwhile greatly reducing critical path latency and hardware area cost of the\nmemory scheduler (by 79% and 43%, respectively), thereby achieving a good\ntrade-off between performance, fairness and hardware complexity.", 
    "link": "http://arxiv.org/pdf/1504.00390v1", 
    "arxiv-id": "1504.00390v1"
},{
    "category": "cs.DC", 
    "author": "Marco Serafini", 
    "title": "The Power of Both Choices: Practical Load Balancing for Distributed   Stream Processing Engines", 
    "publish": "2015-04-03T09:24:22Z", 
    "summary": "We study the problem of load balancing in distributed stream processing\nengines, which is exacerbated in the presence of skew. We introduce Partial Key\nGrouping (PKG), a new stream partitioning scheme that adapts the classical\n\"power of two choices\" to a distributed streaming setting by leveraging two\nnovel techniques: key splitting and local load estimation. In so doing, it\nachieves better load balancing than key grouping while being more scalable than\nshuffle grouping. We test PKG on several large datasets, both real-world and\nsynthetic. Compared to standard hashing, PKG reduces the load imbalance by up\nto several orders of magnitude, and often achieves nearly-perfect load balance.\nThis result translates into an improvement of up to 60% in throughput and up to\n45% in latency when deployed on a real Storm cluster.", 
    "link": "http://arxiv.org/pdf/1504.00788v1", 
    "arxiv-id": "1504.00788v1"
},{
    "category": "cs.DC", 
    "author": "Henning Meyerhenke", 
    "title": "Graphs, Matrices, and the GraphBLAS: Seven Good Reasons", 
    "publish": "2015-04-04T19:11:38Z", 
    "summary": "The analysis of graphs has become increasingly important to a wide range of\napplications. Graph analysis presents a number of unique challenges in the\nareas of (1) software complexity, (2) data complexity, (3) security, (4)\nmathematical complexity, (5) theoretical analysis, (6) serial performance, and\n(7) parallel performance. Implementing graph algorithms using matrix-based\napproaches provides a number of promising solutions to these challenges. The\nGraphBLAS standard (istc- bigdata.org/GraphBlas) is being developed to bring\nthe potential of matrix based graph algorithms to the broadest possible\naudience. The GraphBLAS mathematically defines a core set of matrix-based graph\noperations that can be used to implement a wide class of graph algorithms in a\nwide range of programming environments. This paper provides an introduction to\nthe GraphBLAS and describes how the GraphBLAS can be used to address many of\nthe challenges associated with analysis of graphs.", 
    "link": "http://arxiv.org/pdf/1504.01039v1", 
    "arxiv-id": "1504.01039v1"
},{
    "category": "cs.DC", 
    "author": "Adam Strzelecki", 
    "title": "Benchmarking the cost of thread divergence in CUDA", 
    "publish": "2015-04-07T15:53:48Z", 
    "summary": "All modern processors include a set of vector instructions. While this gives\na tremendous boost to the performance, it requires a vectorized code that can\ntake advantage of such instructions. As an ideal vectorization is hard to\nachieve in practice, one has to decide when different instructions may be\napplied to different elements of the vector operand. This is especially\nimportant in implicit vectorization as in NVIDIA CUDA Single Instruction\nMultiple Threads (SIMT) model, where the vectorization details are hidden from\nthe programmer. In order to assess the costs incurred by incompletely\nvectorized code, we have developed a micro-benchmark that measures the\ncharacteristics of the CUDA thread divergence model on different architectures\nfocusing on the loops performance.", 
    "link": "http://arxiv.org/pdf/1504.01650v1", 
    "arxiv-id": "1504.01650v1"
},{
    "category": "cs.DC", 
    "author": "Lizhe Wang", 
    "title": "A Cloud Infrastructure Service Recommendation System for Optimizing   Real-time QoS Provisioning Constraints", 
    "publish": "2015-04-08T04:32:18Z", 
    "summary": "Proliferation of cloud computing has revolutionized hosting and delivery of\nInternet-based application services. However, with the constant launch of new\ncloud services and capabilities almost every month by both big (e.g., Amazon\nWeb Service, Microsoft Azure) and small companies (e.g. Rackspace, Ninefold),\ndecision makers (e.g. application developers, CIOs) are likely to be\noverwhelmed by choices available. The decision making problem is further\ncomplicated due to heterogeneous service configurations and application\nprovisioning Quality of Service (QoS) constraints. To address this hard\nchallenge, in our previous work we developed a semi-automated, extensible, and\nontology-based approach to infrastructure service discovery and selection based\non only design time constraints (e.g., renting cost, datacentre location,\nservice feature, etc.). In this paper, we extend our approach to include the\nreal-time (run-time) QoS (endto- end message latency, end-to-end message\nthroughput) in the decision making process. Hosting of next generation\napplications in domain of on-line interactive gaming, large scale sensor\nanalytics, and real-time mobile applications on cloud services necessitates\noptimization of such real-time QoS constraints for meeting Service Level\nAgreements (SLAs). To this end, we present a real-time QoS aware multi-criteria\ndecision making technique that builds over well known Analytics Hierarchy\nProcess (AHP) method. The proposed technique is applicable to selecting\nInfrastructure as a Service (IaaS) cloud offers, and it allows users to define\nmultiple design-time and real-time QoS constraints or requirements. These\nrequirements are then matched against our knowledge base to compute possible\nbest fit combinations of cloud services at IaaS layer. We conducted extensive\nexperiments to prove the feasibility of our approach.", 
    "link": "http://arxiv.org/pdf/1504.01828v1", 
    "arxiv-id": "1504.01828v1"
},{
    "category": "cs.DC", 
    "author": "Lei Wang", 
    "title": "BigDataBench-MT: A Benchmark Tool for Generating Realistic Mixed Data   Center Workloads", 
    "publish": "2015-04-09T07:15:24Z", 
    "summary": "Long-running service workloads (e.g. web search engine) and short-term data\nanalysis workloads (e.g. Hadoop MapReduce jobs) co-locate in today's data\ncenters. Developing realistic benchmarks to reflect such practical scenario of\nmixed workload is a key problem to produce trustworthy results when evaluating\nand comparing data center systems. This requires using actual workloads as well\nas guaranteeing their submissions to follow patterns hidden in real-world\ntraces. However, existing benchmarks either generate actual workloads based on\nprobability models, or replay real-world workload traces using basic I/O\noperations. To fill this gap, we propose a benchmark tool that is a first step\ntowards generating a mix of actual service and data analysis workloads on the\nbasis of real workload traces. Our tool includes a combiner that enables the\nreplaying of actual workloads according to the workload traces, and a\nmulti-tenant generator that flexibly scales the workloads up and down according\nto users' requirements. Based on this, our demo illustrates the workload\ncustomization and generation process using a visual interface. The proposed\ntool, called BigDataBench-MT, is a multi-tenant version of our comprehensive\nbenchmark suite BigDataBench and it is publicly available from\nhttp://prof.ict.ac.cn/BigDataBench/multi-tenancyversion/.", 
    "link": "http://arxiv.org/pdf/1504.02205v3", 
    "arxiv-id": "1504.02205v3"
},{
    "category": "cs.DC", 
    "author": "Wim Vanderbauwhede", 
    "title": "Model Coupling between the Weather Research and Forecasting Model and   the DPRI Large Eddy Simulator for Urban Flows on GPU-accelerated Multicore   Systems", 
    "publish": "2015-04-09T11:22:46Z", 
    "summary": "In this report we present a novel approach to model coupling for\nshared-memory multicore systems hosting OpenCL-compliant accelerators, which we\ncall The Glasgow Model Coupling Framework (GMCF). We discuss the implementation\nof a prototype of GMCF and its application to coupling the Weather Research and\nForecasting Model and an OpenCL-accelerated version of the Large Eddy Simulator\nfor Urban Flows (LES) developed at DPRI.\n  The first stage of this work concerned the OpenCL port of the LES. The\nmethodology used for the OpenCL port is a combination of automated analysis and\ncode generation and rule-based manual parallelization. For the evaluation, the\nnon-OpenCL LES code was compiled using gfortran, fort and pgfortran}, in each\ncase with auto-parallelization and auto-vectorization. The OpenCL-accelerated\nversion of the LES achieves a 7 times speed-up on a NVIDIA GeForce GTX 480\nGPGPU, compared to the fastest possible compilation of the original code\nrunning on a 12-core Intel Xeon E5-2640.\n  In the second stage of this work, we built the Glasgow Model Coupling\nFramework and successfully used it to couple an OpenMP-parallelized WRF\ninstance with an OpenCL LES instance which runs the LES code on the GPGPI. The\nsystem requires only very minimal changes to the original code. The report\ndiscusses the rationale, aims, approach and implementation details of this\nwork.", 
    "link": "http://arxiv.org/pdf/1504.02264v1", 
    "arxiv-id": "1504.02264v1"
},{
    "category": "cs.DC", 
    "author": "Danny Dolev", 
    "title": "Byzantine Agreement with Optimal Early Stopping, Optimal Resilience and   Polynomial Complexity", 
    "publish": "2015-04-10T04:04:31Z", 
    "summary": "We provide the first protocol that solves Byzantine agreement with optimal\nearly stopping ($\\min\\{f+2,t+1\\}$ rounds) and optimal resilience ($n>3t$) using\npolynomial message size and computation.\n  All previous approaches obtained sub-optimal results and used resolve rules\nthat looked only at the immediate children in the EIG (\\emph{Exponential\nInformation Gathering}) tree. At the heart of our solution are new resolve\nrules that look at multiple layers of the EIG tree.", 
    "link": "http://arxiv.org/pdf/1504.02547v2", 
    "arxiv-id": "1504.02547v2"
}]