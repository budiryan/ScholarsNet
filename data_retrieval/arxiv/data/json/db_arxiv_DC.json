[{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9809125v1", 
    "title": "Distributed Computation, the Twisted Isomorphism, and Auto-Poiesis", 
    "arxiv-id": "cs/9809125v1", 
    "author": "Michael Manthey", 
    "publish": "1998-09-14T14:42:26Z", 
    "summary": "This paper presents a synchronization-based, multi-process computational\nmodel of anticipatory systems called the Phase Web. It describes a\nself-organizing paradigm that explicitly recognizes and exploits the existence\nof a boundary between inside and outside, accepts and exploits intentionality,\nand uses explicit self-reference to describe eg. auto-poiesis. The model\nexplicitly connects computation to a discrete Clifford algebraic formalization\nthat is in turn extended into homology and co-homology, wherein the recursive\nnature of objects and boundaries becomes apparent and itself subject to\nhierarchical recursion. Topsy, a computer program embodying the Phase Web, is\navailable at www.cs.auc.dk/topsy."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9810019v1", 
    "title": "Gryphon: An Information Flow Based Approach to Message Brokering", 
    "arxiv-id": "cs/9810019v1", 
    "author": "Michael Ward", 
    "publish": "1998-10-21T18:43:47Z", 
    "summary": "Gryphon is a distributed computing paradigm for message brokering, which is\nthe transferring of information in the form of streams of events from\ninformation providers to information consumers. This extended abstract outlines\nthe major problems in message brokering and Gryphon's approach to solving them."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9909013v1", 
    "title": "Self-stabilizing mutual exclusion on a ring, even if K=N", 
    "arxiv-id": "cs/9909013v1", 
    "author": "Jaap-Henk Hoepman", 
    "publish": "1999-09-21T13:39:03Z", 
    "summary": "We show that, contrary to common belief, Dijkstra's self-stabilizing mutual\nexclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of\nstates per node is one less than the number of nodes on the ring."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/9909015v1", 
    "title": "A decision-theoretic approach to reliable message delivery", 
    "arxiv-id": "cs/9909015v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "1999-09-21T20:51:37Z", 
    "summary": "We argue that the tools of decision theory need to be taken more seriously in\nthe specification and analysis of systems. We illustrate this by considering a\nsimple problem involving reliable communication, showing how considerations of\nutility and probability can be used to decide when it is worth sending\nheartbeat messages and, if they are sent, how often they should be sent."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0002008v1", 
    "title": "On Automata with Boundary", 
    "arxiv-id": "cs/0002008v1", 
    "author": "R. F. C. Walters", 
    "publish": "2000-02-16T02:45:39Z", 
    "summary": "We present a theory of automata with boundary for designing, modelling and\nanalysing distributed systems. Notions of behaviour, design and simulation\nappropriate to the theory are defined. The problem of model checking for\ndeadlock detection is discussed, and an algorithm for state space reduction in\nexhaustive search, based on the theory presented here, is described. Three\nexamples of the application of the theory are given, one in the course of the\ndevelopment of the ideas and two as illustrative examples of the use of the\ntheory."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0003054v1", 
    "title": "A Problem-Specific Fault-Tolerance Mechanism for Asynchronous,   Distributed Systems", 
    "arxiv-id": "cs/0003054v1", 
    "author": "Ian Foster", 
    "publish": "2000-03-12T00:19:24Z", 
    "summary": "The idle computers on a local area, campus area, or even wide area network\nrepresent a significant computational resource---one that is, however, also\nunreliable, heterogeneous, and opportunistic. This type of resource has been\nused effectively for embarrassingly parallel problems but not for more tightly\ncoupled problems. We describe an algorithm that allows branch-and-bound\nproblems to be solved in such environments. In designing this algorithm, we\nfaced two challenges: (1) scalability, to effectively exploit the variably\nsized pools of resources available, and (2) fault tolerance, to ensure the\nreliability of services. We achieve scalability through a fully decentralized\nalgorithm, by using a membership protocol for managing dynamically available\nresources. However, this fully decentralized design makes achieving reliability\neven more challenging. We guarantee fault tolerance in the sense that the loss\nof up to all but one resource will not affect the quality of the solution. For\npropagating information efficiently, we use epidemic communication for both the\nmembership protocol and the fault-tolerance mechanism. We have developed a\nsimulation framework that allows us to evaluate design alternatives. Results\nobtained in this framework suggest that our techniques can execute scalably and\nreliably."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0004013v1", 
    "title": "Sorting Integers on the AP1000", 
    "arxiv-id": "cs/0004013v1", 
    "author": "Andrew Lynes", 
    "publish": "2000-04-23T04:32:18Z", 
    "summary": "Sorting is one of the classic problems of computer science. Whilst well\nunderstood on sequential machines, the diversity of architectures amongst\nparallel systems means that algorithms do not perform uniformly on all\nplatforms. This document describes the implementation of a radix based\nalgorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer,\nwhich was constructed as an entry in the Joint Symposium on Parallel Processing\n(JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also\ngiven to a full radix sort conducted in parallel across the machine."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0006004v1", 
    "title": "A Note on \"Optimal Static Load Balancing in Distributed Computer   Systems\"", 
    "arxiv-id": "cs/0006004v1", 
    "author": "S. A. Mondal", 
    "publish": "2000-06-02T03:14:07Z", 
    "summary": "The problem of minimizing mean response time of generic jobs submitted to a\nheterogenous distributed computer systems is considered in this paper. A static\nload balancing strategy, in which decision of redistribution of loads does not\ndepend on the state of the system, is used for this purpose. The article is\nclosely related to a previous article on the same topic. The present article\npoints out number of inconsistencies in the previous article, provides a new\nformulation, and discusses the impact of new findings, based on the improved\nformulation, on the results of the previous article."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0006008v1", 
    "title": "Performing work efficiently in the presence of faults", 
    "arxiv-id": "cs/0006008v1", 
    "author": "O. Waarts", 
    "publish": "2000-06-02T18:35:55Z", 
    "summary": "We consider a system of t synchronous processes that communicate only by\nsending messages to one another, and that together must perform $n$ independent\nunits of work. Processes may fail by crashing; we want to guarantee that in\nevery execution of the protocol in which at least one process survives, all n\nunits of work will be performed. We consider three parameters: the number of\nmessages sent, the total number of units of work performed (including\nmultiplicities), and time. We present three protocols for solving the problem.\nAll three are work-optimal, doing O(n+t) work. The first has moderate costs in\nthe remaining two parameters, sending O(t\\sqrt{t}) messages, and taking O(n+t)\ntime. This protocol can be easily modified to run in any completely\nasynchronous system equipped with a failure detection mechanism. The second\nsends only O(t log{t}) messages, but its running time is large (exponential in\nn and t). The third is essentially time-optimal in the (usual) case in which\nthere are no failures, and its time complexity degrades gracefully as the\nnumber of failures increases."
},{
    "category": "cs.DC", 
    "doi": "10.1016/0020-0190(94)90044-2", 
    "link": "http://arxiv.org/pdf/cs/0007015v1", 
    "title": "Phase Clocks for Transient Fault Repair", 
    "arxiv-id": "cs/0007015v1", 
    "author": "Ted Herman", 
    "publish": "2000-07-10T15:59:03Z", 
    "summary": "Phase clocks are synchronization tools that implement a form of logical time\nin distributed systems. For systems tolerating transient faults by self-repair\nof damaged data, phase clocks can enable reasoning about the progress of\ndistributed repair procedures. This paper presents a phase clock algorithm\nsuited to the model of transient memory faults in asynchronous systems with\nread/write registers. The algorithm is self-stabilizing and guarantees accuracy\nof phase clocks within O(k) time following an initial state that is k-faulty.\nComposition theorems show how the algorithm can be used for the timing of\ndistributed procedures that repair system outputs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0009021v1", 
    "title": "Nimrod/G: An Architecture of a Resource Management and Scheduling System   in a Global Computational Grid", 
    "arxiv-id": "cs/0009021v1", 
    "author": "Jon Giddy", 
    "publish": "2000-09-22T10:44:16Z", 
    "summary": "The availability of powerful microprocessors and high-speed networks as\ncommodity components has enabled high performance computing on distributed\nsystems (wide-area cluster computing). In this environment, as the resources\nare usually distributed geographically at various levels (department,\nenterprise, or worldwide) there is a great challenge in integrating,\ncoordinating and presenting them as a single resource to the user; thus forming\na computational grid. Another challenge comes from the distributed ownership of\nresources with each resource having its own access policy, cost, and mechanism.\n  The proposed Nimrod/G grid-enabled resource management and scheduling system\nbuilds on our earlier work on Nimrod and follows a modular and component-based\narchitecture enabling extensibility, portability, ease of development, and\ninteroperability of independently developed components. It uses the Globus\ntoolkit services and can be easily extended to operate with any other emerging\ngrid middleware services. It focuses on the management and scheduling of\ncomputations over dynamic resources scattered geographically across the\nInternet at department, enterprise, or global level with particular emphasis on\ndeveloping scheduling schemes based on the concept of computational economy for\na real test bed, namely, the Globus testbed (GUSTO)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0012024v4", 
    "title": "Byzantine Agreement with Faulty Majority using Bounded Broadcast", 
    "arxiv-id": "cs/0012024v4", 
    "author": "David Metcalf", 
    "publish": "2000-12-26T23:05:58Z", 
    "summary": "Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely\nused building block of reliable distributed protocols. It simulates broadcast\ndespite the presence of faulty parties within the network, traditionally using\nonly private unicast links. Under such conditions, Byzantine Agreement requires\nmore than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed\na Byzantine Agreement protocol for any compliant majority based on an\nadditional primitive allowing transmission to any two parties simultaneously.\nThey proposed a problem of generalizing these results to wider channels and\nfewer compliant parties. We prove that 2f < kh condition is necessary and\nsufficient for implementing broadcast with h compliant and f faulty parties\nusing k-cast channels."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0102016v1", 
    "title": "A Scientific Data Management System for Irregular Applications", 
    "arxiv-id": "cs/0102016v1", 
    "author": "Alok Choudhary", 
    "publish": "2001-02-20T20:17:22Z", 
    "summary": "Many scientific applications are I/O intensive and generate or access large\ndata sets, spanning hundreds or thousands of \"files.\" Management, storage,\nefficient access, and analysis of this data present an extremely challenging\ntask. We have developed a software system, called Scientific Data Manager\n(SDM), that uses a combination of parallel file I/O and database support for\nhigh-performance scientific data management. SDM provides a high-level API to\nthe user and internally, uses a parallel file system to store real data and a\ndatabase to store application-related metadata. In this paper, we describe how\nwe designed and implemented SDM to support irregular applications. SDM can\nefficiently handle the reading and writing of data in an irregular mesh as well\nas the distribution of index values. We describe the SDM user interface and how\nwe implemented it to achieve high performance. SDM makes extensive use of\nMPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a\nhistory file to optimize the cost of the index distribution using the metadata\nstored in the database. We present performance results with two irregular\napplications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,\non the SGI Origin2000 at Argonne National Laboratory."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0102017v1", 
    "title": "Components and Interfaces of a Process Management System for Parallel   Programs", 
    "arxiv-id": "cs/0102017v1", 
    "author": "Ewing Lusk", 
    "publish": "2001-02-21T16:04:21Z", 
    "summary": "Parallel jobs are different from sequential jobs and require a different type\nof process management. We present here a process management system for parallel\nprograms such as those written using MPI. A primary goal of the system, which\nwe call MPD (for multipurpose daemon), is to be scalable. By this we mean that\nstartup of interactive parallel jobs comprising thousands of processes is\nquick, that signals can be quickly delivered to processes, and that stdin,\nstdout, and stderr are managed intuitively. Our primary target is parallel\nmachines made up of clusters of SMPs, but the system is also useful in more\ntightly integrated environments. We describe how MPD enables much faster\nstartup and better runtime management of parallel jobs. We show how close\ncontrol of stdio can support the easy implementation of a number of convenient\nsystem utilities, even a parallel debugger. We describe a simple but general\ninterface that can be used to separate any process manager from a parallel\nlibrary, which we use to keep MPD separate from MPICH."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0104002v1", 
    "title": "Replica Selection in the Globus Data Grid", 
    "arxiv-id": "cs/0104002v1", 
    "author": "Ian Foster", 
    "publish": "2001-04-02T18:19:06Z", 
    "summary": "The Globus Data Grid architecture provides a scalable infrastructure for the\nmanagement of storage resources and data that are distributed across Grid\nenvironments. These services are designed to support a variety of scientific\napplications, ranging from high-energy physics to computational genomics, that\nrequire access to large amounts of data (terabytes or even petabytes) with\nvaried quality of service requirements. By layering on a set of core services,\nsuch as data transport, security, and replica cataloging, one can construct\nvarious higher-level services. In this paper, we discuss the design and\nimplementation of a high-level replica selection service that uses information\nregarding replica location and user preferences to guide selection from among\nstorage replica alternatives. We first present a basic replica selection\nservice design, then show how dynamic information collected using Globus\ninformation service capabilities concerning storage system properties can help\nimprove and optimize the selection process. We demonstrate the use of Condor's\nClassAds resource description and matchmaking mechanism as an efficient tool\nfor representing and matching storage resource capabilities and policies\nagainst application requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0105013v1", 
    "title": "Dijkstra's Self-Stabilizing Algorithm in Unsupportive Environments", 
    "arxiv-id": "cs/0105013v1", 
    "author": "Ted Herman", 
    "publish": "2001-05-07T14:29:59Z", 
    "summary": "The first self-stabilizing algorithm [Dij73] assumed the existence of a\ncentral daemon, that activates one processor at time to change state as a\nfunction of its own state and the state of a neighbor. Subsequent research has\nreconsidered this algorithm without the assumption of a central daemon, and\nunder different forms of communication, such as the model of link registers. In\nall of these investigations, one common feature is the atomicity of\ncommunication, whether by shared variables or read/write registers. This paper\nweakens the atomicity assumptions for the communication model, proposing\nversions of [Dij73] that tolerate various weaker forms of atomicity. First, a\nsolution for the case of regular registers is presented. Then the case of safe\nregisters is considered, with both negative and positive results presented. The\npaper also presents an implementation of [Dij73] based on registers that have\nprobabilistically correct behavior, which requires a notion of weak\nstabilization."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0105034v1", 
    "title": "On the Area of Hypercube Layouts", 
    "arxiv-id": "cs/0105034v1", 
    "author": "Lee Guan", 
    "publish": "2001-05-29T21:59:18Z", 
    "summary": "This paper precisely analyzes the wire density and required area in standard\nlayout styles for the hypercube. The most natural, regular layout of a\nhypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses\nfloor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of\ntracks per row can be reduced by 1 with a less regular design.) This paper also\ngives a simple formula for the wire density at any cut position and a full\ncharacterization of all places where the wire density is maximized (which does\nnot occur at the bisection)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106020v1", 
    "title": "Economic Models for Management of Resources in Grid Computing", 
    "arxiv-id": "cs/0106020v1", 
    "author": "David Abrams", 
    "publish": "2001-06-11T07:46:56Z", 
    "summary": "The accelerated development in Grid and peer-to-peer computing has positioned\nthem as promising next generation computing platforms. They enable the creation\nof Virtual Enterprises (VE) for sharing resources distributed across the world.\nHowever, resource management, application development and usage models in these\nenvironments is a complex undertaking. This is due to the geographic\ndistribution of resources that are owned by different organizations. The\nresource owners of each of these resources have different usage or access\npolicies and cost models, and varying loads and availability. In order to\naddress complex resource management issues, we have proposed a computational\neconomy framework for resource allocation and for regulating supply and demand\nin Grid computing environments. The framework provides mechanisms for\noptimizing resource provider and consumer objective functions through trading\nand brokering services. In a real world market, there exist various economic\nmodels for setting the price for goods based on supply-and-demand and their\nvalue to the user. They include commodity market, posted price, tenders and\nauctions. In this paper, we discuss the use of these models for interaction\nbetween Grid components in deciding resource value and the necessary\ninfrastructure to realize them. In addition to normal services offered by Grid\ncomputing systems, we need an infrastructure to support interaction protocols,\nallocation mechanisms, currency, secure banking, and enforcement services.\nFurthermore, we demonstrate the usage of some of these economic models in\nresource brokering through Nimrod/G deadline and cost-based scheduling for two\ndifferent optimization strategies on the World Wide Grid (WWG) testbed."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106038v1", 
    "title": "Simple and Effective Distributed Computing with a Scheduling Service", 
    "arxiv-id": "cs/0106038v1", 
    "author": "David M. Mackie", 
    "publish": "2001-06-15T17:09:49Z", 
    "summary": "High-throughput computing projects require the solution of large numbers of\nproblems. In many cases, these problems can be solved on desktop PCs, or can be\nbroken down into independent \"PC-solvable\" sub-problems. In such cases, the\nprojects are high-performance computing projects, but only because of the sheer\nnumber of the needed calculations. We briefly describe our efforts to increase\nthe throughput of one such project. We then explain how to easily set up a\ndistributed computing facility composed of standard networked PCs running\nWindows 95, 98, 2000, or NT. The facility requires no special software or\nhardware, involves little or no re-coding of application software, and operates\nalmost invisibly to the owners of the PCs. Depending on the number and quality\nof PCs recruited, performance can rival that of supercomputers."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0106056v2", 
    "title": "Randomized Two-Process Wait-Free Test-and-Set", 
    "arxiv-id": "cs/0106056v2", 
    "author": "Paul Vitanyi", 
    "publish": "2001-06-28T15:45:35Z", 
    "summary": "We present the first explicit, and currently simplest, randomized algorithm\nfor 2-process wait-free test-and-set. It is implemented with two 4-valued\nsingle writer single reader atomic variables. A test-and-set takes at most 11\nexpected elementary steps, while a reset takes exactly 1 elementary step. Based\non a finite-state analysis, the proofs of correctness and expected length are\ncompressed into one table."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0107034v1", 
    "title": "NEOS Server 4.0 Administrative Guide", 
    "arxiv-id": "cs/0107034v1", 
    "author": "Elizabeth D. Dolan", 
    "publish": "2001-07-26T15:19:00Z", 
    "summary": "The NEOS Server 4.0 provides a general Internet-based client/server as a link\nbetween users and software applications. The administrative guide covers the\nfundamental principals behind the operation of the NEOS Server, installation\nand trouble-shooting of the Server software, and implementation details of\npotential interest to a NEOS Server administrator. The guide also discusses\nmaking new software applications available through the Server, including areas\nof concern to remote solver administrators such as maintaining security,\nproviding usage instructions, and enforcing reasonable restrictions on jobs.\nThe administrative guide is intended both as an introduction to the NEOS Server\nand as a reference for use when running the Server."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108001v1", 
    "title": "The Cactus Worm: Experiments with Dynamic Resource Discovery and   Allocation in a Grid Environment", 
    "arxiv-id": "cs/0108001v1", 
    "author": "John Shalf", 
    "publish": "2001-08-01T23:14:34Z", 
    "summary": "The ability to harness heterogeneous, dynamically available \"Grid\" resources\nis attractive to typically resource-starved computational scientists and\nengineers, as in principle it can increase, by significant factors, the number\nof cycles that can be delivered to applications. However, new adaptive\napplication structures and dynamic runtime system mechanisms are required if we\nare to operate effectively in Grid environments. In order to explore some of\nthese issues in a practical setting, we are developing an experimental\nframework, called Cactus, that incorporates both adaptive application\nstructures for dealing with changing resource characteristics and adaptive\nresource selection mechanisms that allow applications to change their resource\nallocations (e.g., via migration) when performance falls outside specified\nlimits. We describe here the adaptive resource selection mechanisms and\ndescribe how they are used to achieve automatic application migration to\n\"better\" resources following performance degradation. Our results provide\ninsights into the architectural structures required to support adaptive\nresource selection. In addition, we suggest that this \"Cactus Worm\" is an\ninteresting challenge problem for Grid computing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108002v1", 
    "title": "Bounded Concurrent Timestamp Systems Using Vector Clocks", 
    "arxiv-id": "cs/0108002v1", 
    "author": "Paul Vitanyi", 
    "publish": "2001-08-02T17:13:48Z", 
    "summary": "Shared registers are basic objects used as communication mediums in\nasynchronous concurrent computation. A concurrent timestamp system is a higher\ntyped communication object, and has been shown to be a powerful tool to solve\nmany concurrency control problems. It has turned out to be possible to\nconstruct such higher typed objects from primitive lower typed ones. The next\nstep is to find efficient constructions. We propose a very efficient wait-free\nconstruction of bounded concurrent timestamp systems from 1-writer multireader\nregisters. This finalizes, corrects, and extends, a preliminary bounded\nmultiwriter construction proposed by the second author in 1986. That work\npartially initiated the current interest in wait-free concurrent objects, and\nintroduced a notion of discrete vector clocks in distributed algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0108019v1", 
    "title": "Scalable Unix Commands for Parallel Processors: A High-Performance   Implementation", 
    "arxiv-id": "cs/0108019v1", 
    "author": "W. Gropp", 
    "publish": "2001-08-27T15:54:41Z", 
    "summary": "We describe a family of MPI applications we call the Parallel Unix Commands.\nThese commands are natural parallel versions of common Unix user commands such\nas ls, ps, and find, together with a few similar commands particular to the\nparallel environment. We describe the design and implementation of these\nprograms and present some performance results on a 256-node Linux cluster. The\nParallel Unix Commands are open source and freely available."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0109017v1", 
    "title": "Learning from the Success of MPI", 
    "arxiv-id": "cs/0109017v1", 
    "author": "William D. Gropp", 
    "publish": "2001-09-13T21:14:21Z", 
    "summary": "The Message Passing Interface (MPI) has been extremely successful as a\nportable way to program high-performance parallel computers. This success has\noccurred in spite of the view of many that message passing is difficult and\nthat other approaches, including automatic parallelization and directive-based\nparallelism, are easier to use. This paper argues that MPI has succeeded\nbecause it addresses all of the important issues in providing a parallel\nprogramming model."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0110058v1", 
    "title": "Teaching Parallel Programming Using Both High-Level and Low-Level   Languages", 
    "arxiv-id": "cs/0110058v1", 
    "author": "Yi Pan", 
    "publish": "2001-10-29T19:00:15Z", 
    "summary": "We discuss the use of both MPI and OpenMP in the teaching of senior\nundergraduate and junior graduate classes in parallel programming. We briefly\nintroduce the OpenMP standard and discuss why we have chosen to use it in\nparallel programming classes. Advantages of using OpenMP over message passing\nmethods are discussed. We also include a brief enumeration of some of the\ndrawbacks of using OpenMP and how these drawbacks are being addressed by\nsupplementing OpenMP with additional MPI codes and projects. Several projects\ngiven in my class are also described in this paper."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111028v1", 
    "title": "The ESRF TANGO control system status", 
    "arxiv-id": "cs/0111028v1", 
    "author": "P. Verdier", 
    "publish": "2001-11-09T14:29:16Z", 
    "summary": "TANGO is an object oriented control system toolkit based on CORBA presently\nunder development at the ESRF. IN this paper, the TANGO philosophy is briefly\npresented. All the existing tools developed around TANGO will also be\npresented. This include a code genrator, a WEB interface to TANGO objects, an\nadministration tool and an interface to LabView. Finally, an xample of a TANGO\ndevice server for OPC device is given."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111031v1", 
    "title": "Large-Scale Corba-Distributed Software Framework for Nif Controls", 
    "arxiv-id": "cs/0111031v1", 
    "author": "John P. Woodruff", 
    "publish": "2001-11-09T16:40:09Z", 
    "summary": "The Integrated Computer Control System (ICCS) is based on a scalable software\nframework that is distributed over some 325 computers throughout the NIF\nfacility. The framework provides templates and services at multiple levels of\nabstraction for the construction of software applications that communicate via\nCORBA (Common Object Request Broker Architecture). Various forms of\nobject-oriented software design patterns are implemented as templates to be\nextended by application software. Developers extend the framework base classes\nto model the numerous physical control points, thereby sharing the\nfunctionality defined by the base classes. About 56,000 software objects each\nindividually addressed through CORBA are to be created in the complete ICCS.\nMost objects have a persistent state that is initialized at system start-up and\nstored in a database. Additional framework services are provided by centralized\nserver programs that implement events, alerts, reservations, message logging,\ndatabase/file persistence, name services, and process management. The ICCS\nsoftware framework approach allows for efficient construction of a software\nsystem that supports a large number of distributed control points representing\na complex control application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111033v1", 
    "title": "Modernising the ESRF control system with GNU/Linux", 
    "arxiv-id": "cs/0111033v1", 
    "author": "W. D. Klotz", 
    "publish": "2001-11-09T20:08:18Z", 
    "summary": "he ESRF control system is in the process of being modernised. The present\ncontrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,\nMotif and C. The new control system will be based on compact PCI, 100 MHz\nEthernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main\nfrontend operating system will be GNU/Linux running on Intel/x86 and\nMotorola/68k. Linux will also be used on handheld devices for mobile control.\nThis poster describes how GNU/Linux is being used to modernise the control\nsystem and what problems have been encountered so far"
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111047v1", 
    "title": "Virtual Laboratory: Enabling On-Demand Drug Design with the World Wide   Grid", 
    "arxiv-id": "cs/0111047v1", 
    "author": "David Abramson", 
    "publish": "2001-11-17T07:20:58Z", 
    "summary": "Computational Grids are emerging as a popular paradigm for solving\nlarge-scale compute and data intensive problems in science, engineering, and\ncommerce. However, application composition, resource management and scheduling\nin these environments is a complex undertaking. In this paper, we illustrate\nthe creation of a virtual laboratory environment by leveraging existing Grid\ntechnologies to enable molecular modeling for drug design on distributed\nresources. It involves screening millions of molecules of chemical compounds\nagainst a protein target, chemical database (CDB) to identify those with\npotential use for drug design. We have grid-enabled the molecular docking\nprocess by composing it as a parameter sweep application using the Nimrod-G\ntools. We then developed new tools for remote access to molecules in CDB small\nmolecule database. The Nimrod-G resource broker along with molecule CDB data\nbroker is used for scheduling and on-demand processing of jobs on distributed\ngrid resources. The results demonstrate the ease of use and suitability of the\nNimrod-G and virtual laboratory tools."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0111048v1", 
    "title": "A Computational Economy for Grid Computing and its Implementation in the   Nimrod-G Resource Brok", 
    "arxiv-id": "cs/0111048v1", 
    "author": "Jonathan Giddy", 
    "publish": "2001-11-17T07:32:44Z", 
    "summary": "Computational Grids, coupling geographically distributed resources such as\nPCs, workstations, clusters, and scientific instruments, have emerged as a next\ngeneration computing platform for solving large-scale problems in science,\nengineering, and commerce. However, application development, resource\nmanagement, and scheduling in these environments continue to be a complex\nundertaking. In this article, we discuss our efforts in developing a resource\nmanagement system for scheduling computations on resources distributed across\nthe world with varying quality of service. Our service-oriented grid computing\nsystem called Nimrod-G manages all operations associated with remote execution\nincluding resource discovery, trading, scheduling based on economic principles\nand a user defined quality of service requirement. The Nimrod-G resource broker\nis implemented by leveraging existing technologies such as Globus, and provides\nnew services that are essential for constructing industrial-strength Grids. We\ndiscuss results of preliminary experiments on scheduling some parametric\ncomputations using the Nimrod-G resource broker on a world-wide grid testbed\nthat spans five continents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0202003v3", 
    "title": "Simple Optimal Wait-free Multireader Registers", 
    "arxiv-id": "cs/0202003v3", 
    "author": "Paul Vitanyi", 
    "publish": "2002-02-04T17:29:15Z", 
    "summary": "Multireader shared registers are basic objects used as communication medium\nin asynchronous concurrent computation. We propose a surprisingly simple and\nnatural scheme to obtain several wait-free constructions of bounded 1-writer\nmultireader registers from atomic 1-writer 1-reader registers, that is easier\nto prove correct than any previous construction. Our main construction is the\nfirst symmetric pure timestamp one that is optimal with respect to the\nworst-case local use of control bits; the other one is optimal with respect to\nglobal use of control bits; both are optimal in time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0203019v1", 
    "title": "GridSim: A Toolkit for the Modeling and Simulation of Distributed   Resource Management and Scheduling for Grid Computing", 
    "arxiv-id": "cs/0203019v1", 
    "author": "Manzur Murshed", 
    "publish": "2002-03-14T03:44:18Z", 
    "summary": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular\nparadigms for next generation parallel and distributed computing. The\nmanagement of resources and scheduling of applications in such large-scale\ndistributed systems is a complex undertaking. In order to prove the\neffectiveness of resource brokers and associated scheduling algorithms, their\nperformance needs to be evaluated under different scenarios such as varying\nnumber of resources and users with different requirements. In a grid\nenvironment, it is hard and even impossible to perform scheduler performance\nevaluation in a repeatable and controllable manner as resources and users are\ndistributed across multiple organizations with their own policies. To overcome\nthis limitation, we have developed a Java-based discrete-event grid simulation\ntoolkit called GridSim. The toolkit supports modeling and simulation of\nheterogeneous grid resources (both time- and space-shared), users and\napplication models. It provides primitives for creation of application tasks,\nmapping of tasks to resources, and their management. To demonstrate suitability\nof the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker\nand evaluated the performance of deadline and budget constrained cost- and\ntime-minimization scheduling algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0203020v1", 
    "title": "A Deadline and Budget Constrained Cost-Time Optimisation Algorithm for   Scheduling Task Farming Applications on Global Grids", 
    "arxiv-id": "cs/0203020v1", 
    "author": "Manzur Murshed", 
    "publish": "2002-03-14T03:50:19Z", 
    "summary": "Computational Grids and peer-to-peer (P2P) networks enable the sharing,\nselection, and aggregation of geographically distributed resources for solving\nlarge-scale problems in science, engineering, and commerce. The management and\ncomposition of resources and services for scheduling applications, however,\nbecomes a complex undertaking. We have proposed a computational economy\nframework for regulating the supply and demand for resources and allocating\nthem for applications based on the users quality of services requirements. The\nframework requires economy driven deadline and budget constrained (DBC)\nscheduling algorithms for allocating resources to application jobs in such a\nway that the users requirements are met. In this paper, we propose a new\nscheduling algorithm, called DBC cost-time optimisation, which extends the DBC\ncost-optimisation algorithm to optimise for time, keeping the cost of\ncomputation at the minimum. The superiority of this new scheduling algorithm,\nin achieving lower job completion time, is demonstrated by simulating the\nWorld-Wide Grid and scheduling task-farming applications for different deadline\nand budget scenarios using both this new and the cost optimisation scheduling\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0204048v1", 
    "title": "Economic-based Distributed Resource Management and Scheduling for Grid   Computing", 
    "arxiv-id": "cs/0204048v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2002-04-22T20:06:42Z", 
    "summary": "Computational Grids, emerging as an infrastructure for next generation\ncomputing, enable the sharing, selection, and aggregation of geographically\ndistributed resources for solving large-scale problems in science, engineering,\nand commerce. As the resources in the Grid are heterogeneous and geographically\ndistributed with varying availability and a variety of usage and cost policies\nfor diverse users at different times and, priorities as well as goals that vary\nwith time. The management of resources and application scheduling in such a\nlarge and distributed environment is a complex task. This thesis proposes a\ndistributed computational economy as an effective metaphor for the management\nof resources and application scheduling. It proposes an architectural framework\nthat supports resource trading and quality of services based scheduling. It\nenables the regulation of supply and demand for resources and provides an\nincentive for resource owners for participating in the Grid and motives the\nusers to trade-off between the deadline, budget, and the required level of\nquality of service. The thesis demonstrates the capability of economic-based\nsystems for peer-to-peer distributed computing by developing users'\nquality-of-service requirements driven scheduling strategies and algorithms. It\ndemonstrates their effectiveness by performing scheduling experiments on the\nWorld-Wide Grid for solving parameter sweep applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0205021v1", 
    "title": "An Overview of a Grid Architecture for Scientific Computing", 
    "arxiv-id": "cs/0205021v1", 
    "author": "O. Smirnova", 
    "publish": "2002-05-14T19:22:00Z", 
    "summary": "This document gives an overview of a Grid testbed architecture proposal for\nthe NorduGrid project. The aim of the project is to establish an inter-Nordic\ntestbed facility for implementation of wide area computing and data handling.\nThe architecture is supposed to define a Grid system suitable for solving data\nintensive problems at the Large Hadron Collider at CERN. We present the various\narchitecture components needed for such a system. After that we go on to give a\ndescription of the dynamics by showing the task flow."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0205023v2", 
    "title": "Performance evaluation of the GridFTP within the NorduGrid project", 
    "arxiv-id": "cs/0205023v2", 
    "author": "A. Waananen", 
    "publish": "2002-05-14T19:55:37Z", 
    "summary": "This report presents results of the tests measuring the performance of\nmulti-threaded file transfers, using the GridFTP implementation of the Globus\nproject over the NorduGrid network resources. Point to point WAN tests, carried\nout between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. It\nwas found that multiple threaded download via the high performance GridFTP\nprotocol can significantly improve file transfer performance, and can serve as\na reliable data"
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0206038v1", 
    "title": "A Multilevel Approach to Topology-Aware Collective Operations in   Computational Grids", 
    "arxiv-id": "cs/0206038v1", 
    "author": "E. Lusk", 
    "publish": "2002-06-24T18:28:21Z", 
    "summary": "The efficient implementation of collective communiction operations has\nreceived much attention. Initial efforts produced \"optimal\" trees based on\nnetwork communication models that assumed equal point-to-point latencies\nbetween any two processes. This assumption is violated in most practical\nsettings, however, particularly in heterogeneous systems such as clusters of\nSMPs and wide-area \"computational Grids,\" with the result that collective\noperations perform suboptimally. In response, more recent work has focused on\ncreating topology-aware trees for collective operations that minimize\ncommunication across slower channels (e.g., a wide-area network). While these\nefforts have significant communication benefits, they all limit their view of\nthe network to only two layers. We present a strategy based upon a multilayer\nview of the network. By creating multilevel topology-aware trees we take\nadvantage of communication cost differences at every level in the network. We\nused this strategy to implement topology-aware versions of several MPI\ncollective operations in MPICH-G2, the Globus Toolkit[tm]-enabled version of\nthe popular MPICH implementation of the MPI standard. Using information about\ntopology provided by MPICH-G2, we construct these multilevel topology-aware\ntrees automatically during execution. We present results demonstrating the\nadvantages of our multilevel approach by comparing it to the default\n(topology-unaware) implementation provided by MPICH and a topology-aware\ntwo-layer implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0206040v2", 
    "title": "MPICH-G2: A Grid-Enabled Implementation of the Message Passing Interface", 
    "arxiv-id": "cs/0206040v2", 
    "author": "I. Foster", 
    "publish": "2002-06-25T19:55:45Z", 
    "summary": "Application development for distributed computing \"Grids\" can benefit from\ntools that variously hide or enable application-level management of critical\naspects of the heterogeneous environment. As part of an investigation of these\nissues, we have developed MPICH-G2, a Grid-enabled implementation of the\nMessage Passing Interface (MPI) that allows a user to run MPI programs across\nmultiple computers, at the same or different sites, using the same commands\nthat would be used on a parallel computer. This library extends the Argonne\nMPICH implementation of MPI to use services provided by the Globus Toolkit for\nauthentication, authorization, resource allocation, executable staging, and\nI/O, as well as for process creation, monitoring, and control. Various\nperformance-critical operations, including startup and collective operations,\nare configured to exploit network topology information. The library also\nexploits MPI constructs for performance management; for example, the MPI\ncommunicator construct is used for application-level discovery of, and\nadaptation to, both network topology and network quality-of-service mechanisms.\nWe describe the MPICH-G2 design and implementation, present performance\nresults, and review application experiences, including record-setting\ndistributed simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0207036v1", 
    "title": "System Description for a Scalable, Fault-Tolerant, Distributed Garbage   Collector", 
    "arxiv-id": "cs/0207036v1", 
    "author": "T. Terriberry", 
    "publish": "2002-07-10T00:53:23Z", 
    "summary": "We describe an efficient and fault-tolerant algorithm for distributed cyclic\ngarbage collection. The algorithm imposes few requirements on the local\nmachines and allows for flexibility in the choice of local collector and\ndistributed acyclic garbage collector to use with it. We have emphasized\nreducing the number and size of network messages without sacrificing the\npromptness of collection throughout the algorithm. Our proposed collector is a\nvariant of back tracing to avoid extensive synchronization between machines. We\nhave added an explicit forward tracing stage to the standard back tracing stage\nand designed a tuned heuristic to reduce the total amount of work done by the\ncollector. Of particular note is the development of fault-tolerant cooperation\nbetween traces and a heuristic that aggressively reduces the set of suspect\nobjects."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPC.2000.846563", 
    "link": "http://arxiv.org/pdf/cs/0207096v1", 
    "title": "Noncontiguous I/O through PVFS", 
    "arxiv-id": "cs/0207096v1", 
    "author": "William Gropp", 
    "publish": "2002-07-29T16:20:25Z", 
    "summary": "With the tremendous advances in processor and memory technology, I/O has\nrisen to become the bottleneck in high-performance computing for many\napplications. The development of parallel file systems has helped to ease the\nperformance gap, but I/O still remains an area needing significant performance\nimprovement. Research has found that noncontiguous I/O access patterns in\nscientific applications combined with current file system methods to perform\nthese accesses lead to unacceptable performance for large data sets. To enhance\nperformance of noncontiguous I/O we have created list I/O, a native version of\nnoncontiguous I/O. We have used the Parallel Virtual File System (PVFS) to\nimplement our ideas. Our research and experimentation shows that list I/O\noutperforms current noncontiguous I/O access methods in most I/O situations and\ncan substantially enhance the performance of real-world scientific\napplications."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0208021v1", 
    "title": "Implicit Simulations using Messaging Protocols", 
    "arxiv-id": "cs/0208021v1", 
    "author": "G. A. Kohring", 
    "publish": "2002-08-14T09:11:20Z", 
    "summary": "A novel algorithm for performing parallel, distributed computer simulations\non the Internet using IP control messages is introduced. The algorithm employs\ncarefully constructed ICMP packets which enable the required computations to be\ncompleted as part of the standard IP communication protocol. After providing a\ndetailed description of the algorithm, experimental applications in the areas\nof stochastic neural networks and deterministic cellular automata are\ndiscussed. As an example of the algorithms potential power, a simulation of a\ndeterministic cellular automaton involving 10^5 Internet connected devices was\nperformed."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0208027v1", 
    "title": "A Unified Theory of Shared Memory Consistency", 
    "arxiv-id": "cs/0208027v1", 
    "author": "Gary J. Nutt", 
    "publish": "2002-08-19T19:09:04Z", 
    "summary": "Memory consistency models have been developed to specify what values may be\nreturned by a read given that, in a distributed system, memory operations may\nonly be partially ordered. Before this work, consistency models were defined\nindependently. Each model followed a set of rules which was separate from the\nrules of every other model. In our work we have defined a set of four\nconsistency properties. Any subset of the four properties yields a set of rules\nwhich constitute a consistency model. Every consistency model previously\ndescribed in the literature can be defined based on our four properties.\nTherefore, we present these properties as a unfied theory of shared memory\nconsistency."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0210002v1", 
    "title": "GridBank: A Grid Accounting Services Architecture (GASA) for Distributed   Systems Sharing and Integration", 
    "arxiv-id": "cs/0210002v1", 
    "author": "Alexander Barmouta Rajkumar Buyya", 
    "publish": "2002-10-01T10:41:52Z", 
    "summary": "Computational Grids are emerging as new infrastructure for Internet-based\nparallel and distributed computing. They enable the sharing, exchange,\ndiscovery, and aggregation of resources distributed across multiple\nadministrative domains, organizations and enterprises. To accomplish this,\nGrids need infrastructure that supports various services: security, uniform\naccess, resource management, scheduling, application composition, computational\neconomy, and accountability. Many Grid projects have developed technologies\nthat provide many of these services with an exception of accountability. To\novercome this limitation, we propose a new infrastructure called Grid Bank that\nprovides services for accounting. This paper presents requirements of Grid\naccountability and different models within which it can operate and proposes\nGrid Bank Services Architecture that meets them. The paper highlights\nimplementation issues with detailed discussion on format for various\nrecords/database that the GridBank need to maintain. It also presents protocols\nfor interaction between GridBank and various components within Grid computing\nenvironments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0301033v1", 
    "title": "Computational Grids in Action: The Natinal Fusion Collaboratory", 
    "arxiv-id": "cs/0301033v1", 
    "author": "D. McCune", 
    "publish": "2003-01-28T20:11:16Z", 
    "summary": "The National Fusion Collaboratory (NFC) project was created to advance\nscientific understanding and innovation in magnetic fusion research by enabling\nmore efficient use of existing experimental facilities through more effective\nintegration of experiment, theory, and modeling. To achieve this objective, NFC\nintroduced the concept of \"network services\", which build on top of\ncomputational Grids, and provide Fusion codes, together with their maintenance\nand hardware resources as a service to the community. This mode of operation\nrequires the development of new authorization and enforcement capabilities. In\naddition, the nature of Fusion experiments places strident quality of service\nrequirements on codes run during the experimental cycle. In this paper, we\ndescribe Grid computing requirements of the Fusion community, and present our\nfirst experiments in meeting those requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0301035v1", 
    "title": "On the Complexity of Buffer Allocation in Message Passing Systems", 
    "arxiv-id": "cs/0301035v1", 
    "author": "Alan Wagner", 
    "publish": "2003-01-31T01:21:21Z", 
    "summary": "Message passing programs commonly use buffers to avoid unnecessary\nsynchronizations and to improve performance by overlapping communication with\ncomputation. Unfortunately, using buffers makes the program no longer portable,\npotentially unable to complete on systems without a sufficient number of\nbuffers. Effective buffer use entails that the minimum number needed for a safe\nexecution be allocated.\n  We explore a variety of problems related to buffer allocation for safe and\nefficient execution of message passing programs. We show that determining the\nminimum number of buffers or verifying a buffer assignment are intractable\nproblems. However, we give a polynomial time algorithm to determine the minimum\nnumber of buffers needed to allow for asynchronous execution. We extend these\nresults to several different buffering schemes, which in some cases make the\nproblems tractable."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302006v1", 
    "title": "Grid Market Directory: A Web Services based Grid Service Publication   Directory", 
    "arxiv-id": "cs/0302006v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T03:31:02Z", 
    "summary": "As Grids are emerging as the next generation service-oriented computing\nplatforms, they need to support Grid economy that helps in the management of\nsupply and demand for resources and offers an economic incentive for Grid\nresource providers. To enable this Grid economy, a market-like Grid environment\nincluding an infrastructure that supports the publication of services and their\ndiscovery is needed. As part of the Gridbus project, we proposed and have\ndeveloped a Grid Market Directory (GMD) that serves as a registry for\nhigh-level service publication and discovery in Virtual Organisations."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302007v1", 
    "title": "G-Monitor: Gridbus web portal for monitoring and steering application   execution on global grids", 
    "arxiv-id": "cs/0302007v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T03:36:29Z", 
    "summary": "Grids are experiencing a rapid growth in their application and along with\nthis there is a requirement for a portal which is easy to use and scalable. We\nhave responded to this requirement by developing an easy to use, scalable,\nweb-based portal called G-Monitor. This paper proposes a generic architecture\nfor a web portal into a grid environment and discusses our implementation and\nits application."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302008v1", 
    "title": "Visual Environment for Rapid Composition of Parameter-Sweep Applications   for Distributed Processing on Global Grids", 
    "arxiv-id": "cs/0302008v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-06T04:06:02Z", 
    "summary": "Computational Grids are emerging as a platform for next-generation parallel\nand distributed computing. Large-scale parametric studies and parameter sweep\napplications find a natural place in the Grid?s distribution model. There is\nlittle or no communication between jobs. The task of parallelizing and\ndistributing existing applications is conceptually trivial. These properties of\nparametric studies make it an ideal place to start developing integrated\ndevelopment environments (IDEs) for rapidly Grid-enabling applications.\nHowever, the availability of IDEs for scientists to Grid-enable their\napplications, without the need of developing them as parallel applications\nexplicitly, is still lacking. This paper presents a Java based IDE called\nVisual Parametric Tool (VPT), developed as part of the Gridbus project, for\nrapid creation of parameter sweep (data parallel/SPMD) applications. It\nsupports automatic creation of parameter script and parameterisation of the\ninput data files, which is compatible with the Nimrod-G parameter specification\nlanguage. The usefulness of VPT is demonstrated by a case study on composition\nof molecular docking application as a parameter sweep application. Such\napplications can be deployed on clusters using the Nimrod/enFuzion system and\non global Grids using the Nimrod-G grid resource broker."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302018v1", 
    "title": "Guided Google: A Meta Search Engine and its Implementation using the   Google Distributed Web Services", 
    "arxiv-id": "cs/0302018v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-02-13T04:49:26Z", 
    "summary": "With the advent of the Internet, search engines have begun sprouting like\nmushrooms after a rainfall. Only in recent years, have developers become more\ninnovative, and came up with guided searching facilities online. The goals of\nthese applications are to help ease and guide the searching efforts of a novice\nweb user toward their desired objectives. A number of implementations of such\nservices are emerging. This paper proposes a guided meta-search engine, called\n\"Guided Google\", as it serves as an interface to the actual Google.com search\nengine, using the Google Web Services."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0302019v1", 
    "title": "Economic and On Demand Brain Activity Analysis on Global Grids", 
    "arxiv-id": "cs/0302019v1", 
    "author": "D. Abramson", 
    "publish": "2003-02-13T04:53:10Z", 
    "summary": "The lack of computational power within an organization for analyzing\nscientific data, and the distribution of knowledge (by scientists) and\ntechnologies (advanced scientific devices) are two major problems commonly\nobserved in scientific disciplines. One such scientific discipline is brain\nscience. The analysis of brain activity data gathered from the MEG\n(Magnetoencephalography) instrument is an important research topic in medical\nscience since it helps doctors in identifying symptoms of diseases. The data\nneeds to be analyzed exhaustively to efficiently diagnose and analyze brain\nfunctions and requires access to large-scale computational resources. The\npotential platform for solving such resource intensive applications is the\nGrid. This paper describes a MEG data analysis system developed by us,\nleveraging Grid technologies, primarily Nimrod-G, Gridbus, and Globus. This\npaper explains the application of economy-based grid scheduling algorithms to\nthe problem domain for on-demand processing of analysis jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0303005v1", 
    "title": "Fair Solution to the Reader-Writer-Problem with Semaphores only", 
    "arxiv-id": "cs/0303005v1", 
    "author": "H. Ballhausen", 
    "publish": "2003-03-08T17:15:00Z", 
    "summary": "The reader-writer-problem is a standard problem in concurrent programming. A\nresource is shared by several processes which need either inclusive reading or\nexclusive writing access. The known solutions to this problem typically involve\na number of global counters and queues. Here a very simple algorithm is\npresented which needs only two semaphores for synchronisation and no other\nglobal objects. The approach yields a fair solution without starving."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0304037v1", 
    "title": "Using Regression Techniques to Predict Large Data Transfers", 
    "arxiv-id": "cs/0304037v1", 
    "author": "Jennifer M. Schopf", 
    "publish": "2003-04-23T20:36:09Z", 
    "summary": "The recent proliferation of Data Grids and the increasingly common practice\nof using resources as distributed data stores provide a convenient environment\nfor communities of researchers to share, replicate, and manage access to copies\nof large datasets. This has led to the question of which replica can be\naccessed most efficiently. In such environments, fetching data from one of the\nseveral replica locations requires accurate predictions of end-to-end transfer\ntimes. The answer to this question can depend on many factors, including\nphysical characteristics of the resources and the load behavior on the CPUs,\nnetworks, and storage devices that are part of the end-to-end data path linking\npossible sources and sinks. Our approach combines end-to-end application\nthroughput observations with network and disk load variations and captures\nwhole-system performance and variations in load patterns. Our predictions\ncharacterize the effect of load variations of several shared devices (network\nand disk) on file transfer times. We develop a suite of univariate and\nmultivariate predictors that can use multiple data sources to improve the\naccuracy of the predictions as well as address Data Grid variations\n(availability of data and sporadic nature of transfers). We ran a large set of\ndata transfer experiments using GridFTP and observed performance predictions\nwithin 15% error for our testbed sites, which is quite promising for a\npragmatic system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305050v2", 
    "title": "Towards automation of computing fabrics using tools from the fabric   management workpackage of the EU DataGrid project", 
    "arxiv-id": "cs/0305050v2", 
    "author": "Olof Barring", 
    "publish": "2003-05-28T16:25:45Z", 
    "summary": "The EU DataGrid project workpackage 4 has as an objective to provide the\nnecessary tools for automating the management of medium size to very large\ncomputing fabrics. At the end of the second project year subsystems for\ncentralized configuration management (presented at LISA'02) and\nperformance/exception monitoring have been delivered. This will soon be\naugmented with a subsystem for node installation and service configuration,\nwhich is based on existing widely used standards where available (e.g. rpm,\nkickstart, init.d scripts) and clean interfaces to OS dependent components\n(e.g. base installation and service management). The three subsystems together\nallow for centralized management of very large computer farms. Finally, a fault\ntolerance system is being developed for tying together the above subsystems to\nform a complete framework for automated enterprise computing management by\n3Q03. All software developed is open source covered by the EU DataGrid project\nlicense agreements. This article describes the architecture behind the designed\nfabric management system and the status of the different developments. It also\ncovers the experience with an existing tool for automated configuration and\ninstallation that have been adapted and used from the beginning to manage the\nEU DataGrid testbed, which is now used for LHC data challenges."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305058v2", 
    "title": "ATLAS and CMS applications on the WorldGrid testbed", 
    "arxiv-id": "cs/0305058v2", 
    "author": "L. Vaccarossa", 
    "publish": "2003-05-30T09:18:55Z", 
    "summary": "WorldGrid is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. It has been developed in the context of the DataTAG and iVDGL\nprojects, and successfully demonstrated during the WorldGrid demos at IST2002\n(Copenhagen) and SC2002 (Baltimore). Two HEP experiments, ATLAS and CMS,\nsuccessful exploited the WorldGrid testbed for executing jobs simulating the\nresponse of their detectors to physics eve nts produced by real collisions\nexpected at the LHC accelerator starting from 2007. This data intensive\nactivity has been run since many years on local dedicated computing farms\nconsisting of hundreds of nodes and Terabytes of disk and tape storage. Within\nthe WorldGrid testbed, for the first time HEP simulation jobs were submitted\nand run indifferently on US and European resources, despite of their underlying\ndifferent Grid implementations, and produced data which could be retrieved and\nfurther analysed on the submitting machine, or simply stored on the remote\nresources and registered on a Replica Catalogue which made them available to\nthe Grid for further processing. In this contribution we describe the job\nsubmission from Europe for both ATLAS and CMS applications, performed through\nthe GENIUS portal operating on top of an EDG User Interface submitting to an\nEDG Resource Broker, pointing out the chosen interoperability solutions which\nmade US and European resources equivalent from the applications point of view,\nthe data management in the WorldGrid environment, and the CMS specific\nproduction tools which were interfaced to the GENIUS portal."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305059v1", 
    "title": "EU DataGRID testbed management and support at CERN", 
    "arxiv-id": "cs/0305059v1", 
    "author": "M. W. Schulz", 
    "publish": "2003-05-30T10:07:12Z", 
    "summary": "In this paper we report on the first two years of running the CERN testbed\nsite for the EU DataGRID project. The site consists of about 120 dual-processor\nPCs distributed over several testbeds used for different purposes: software\ndevelopment, system integration, and application tests. Activities at the site\nincluded test productions of MonteCarlo data for LHC experiments, tutorials and\ndemonstrations of GRID technologies, and support for individual users analysis.\nThis paper focuses on node installation and configuration techniques, service\nmanagement, user support in a gridified environment, and includes\nconsiderations on scalability and security issues and comparisons with\n\"traditional\" production systems, as seen from the administrator point of view."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305061v1", 
    "title": "A Secure Infrastructure For System Console and Reset Access", 
    "arxiv-id": "cs/0305061v1", 
    "author": "Markus Schulz", 
    "publish": "2003-05-30T11:42:37Z", 
    "summary": "During the last years large farms have been built using commodity hardware.\nThis hardware lacks components for remote and automated administration.\nProducts that can be retrofitted to these systems are either costly or\ninherently insecure. We present a system based on serial ports and simple\nmachine controlled relays. We report on experience gained by setting up a\n50-machine test environment as well as current work in progress in the area."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305062v2", 
    "title": "DIAMOnDS - DIstributed Agents for MObile & Dynamic Services", 
    "arxiv-id": "cs/0305062v2", 
    "author": "Harvey Newman", 
    "publish": "2003-05-30T11:48:57Z", 
    "summary": "Distributed Services Architecture with support for mobile agents between\nservices, offer significantly improved communication and computational\nflexibility. The uses of agents allow execution of complex operations that\ninvolve large amounts of data to be processed effectively using distributed\nresources. The prototype system Distributed Agents for Mobile and Dynamic\nServices (DIAMOnDS), allows a service to send agents on its behalf, to other\nservices, to perform data manipulation and processing. Agents have been\nimplemented as mobile services that are discovered using the Jini Lookup\nmechanism and used by other services for task management and communication.\nAgents provide proxies for interaction with other services as well as specific\nGUI to monitor and control the agent activity. Thus agents acting on behalf of\none service cooperate with other services to carry out a job, providing\ninter-operation of loosely coupled services in a semi-autonomous way. Remote\nfile system access functionality has been incorporated by the agent framework\nand allows services to dynamically share and browse the file system resources\nof hosts, running the services. Generic database access functionality has been\nimplemented in the mobile agent framework that allows performing complex data\nmining and processing operations efficiently in distributed system. A basic\ndata searching agent is also implemented that performs a query based search in\na file system. The testing of the framework was carried out on WAN by moving\nConnectivity Test agents between AgentStations in CERN, Switzerland and NUST,\nPakistan."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305063v2", 
    "title": "McRunjob: A High Energy Physics Workflow Planner for Grid Production   Processing", 
    "arxiv-id": "cs/0305063v2", 
    "author": "Iain Bertram", 
    "publish": "2003-05-30T19:53:14Z", 
    "summary": "McRunjob is a powerful grid workflow manager used to manage the generation of\nlarge numbers of production processing jobs in High Energy Physics. In use at\nboth the DZero and CMS experiments, McRunjob has been used to manage large\nMonte Carlo production processing since 1999 and is being extended to uses in\nregular production processing for analysis and reconstruction. Described at\nCHEP 2001, McRunjob converts core metadata into jobs submittable in a variety\nof environments. The powerful core metadata description language includes\nmethods for converting the metadata into persistent forms, job descriptions,\nmulti-step workflows, and data provenance information. The language features\nallow for structure in the metadata by including full expressions, namespaces,\nfunctional dependencies, site specific parameters in a grid environment, and\nontological definitions. It also has simple control structures for\nparallelization of large jobs. McRunjob features a modular design which allows\nfor easy expansion to new job description languages or new application level\ntasks."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305065v1", 
    "title": "A Generic Multi-node State Monitoring Subsystem", 
    "arxiv-id": "cs/0305065v1", 
    "author": "Rainer Bartoldus", 
    "publish": "2003-05-30T17:43:03Z", 
    "summary": "The BaBar online data acquisition (DAQ) system includes approximately fifty\nUnix systems that collectively implement the level-three trigger. These systems\nall run the same code. Each of these systems has its own state, and this state\nis expected to change in response to changes in the overall DAQ system. A\nspecialized subsystem has been developed to initiate processing on this\ncollection of systems, and to monitor them both for error conditions and to\nensure that they all follow the same state trajectory within a specifiable\nperiod of time. This subsystem receives start commands from the main DAQ run\ncontrol system, and reports major coherent state changes, as well as error\nconditions, back to the run control system. This state monitoring subsystem has\nthe novel feature that it does not know anything about the state machines that\nit is monitoring, and hence does not introduce any fundamentally new state\nmachine into the overall system. This feature makes it trivially applicable to\nother multi-node subsystems. Indeed it has already found a second application\nbeyond the level-three trigger, within the BaBar experiment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0305066v2", 
    "title": "The CMS Integration Grid Testbed", 
    "arxiv-id": "cs/0305066v2", 
    "author": "Todd Tannenbaum", 
    "publish": "2003-05-30T19:45:44Z", 
    "summary": "The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2\nhardware at the following sites: the California Institute of Technology, Fermi\nNational Accelerator Laboratory, the University of California at San Diego, and\nthe University of Florida at Gainesville. The IGT runs jobs using the Globus\nToolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is\nmanaged using VO management scripts from the European Data Grid (EDG). Gridwide\nmonitoring is accomplished using local tools such as Ganglia interfaced into\nthe Globus Metadata Directory Service (MDS) and the agent based Mona Lisa.\nDomain specific software is packaged and installed using the Distrib ution\nAfter Release (DAR) tool of CMS, while middleware under the auspices of the\nVirtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us\ntwo month span in Fall of 2002, over 1 million official CMS GEANT based Monte\nCarlo events were generated and returned to CERN for analysis while being\ndemonstrated at SC2002. In this paper, we describe the process that led to one\nof the world's first continuously available, functioning grids."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306001v2", 
    "title": "Clarens Client and Server Applications", 
    "arxiv-id": "cs/0306001v2", 
    "author": "Frank van Lingen", 
    "publish": "2003-05-30T20:25:50Z", 
    "summary": "Several applications have been implemented with access via the Clarens web\nservice infrastructure, including virtual organization management, JetMET\nphysics data analysis using relational databases, and Storage Resource Broker\n(SRB) access. This functionality is accessible transparently from Python\nscripts, the Root analysis framework and from Java applications and browser\napplets."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306002v2", 
    "title": "The Clarens web services architecture", 
    "arxiv-id": "cs/0306002v2", 
    "author": "Frank van Lingen", 
    "publish": "2003-05-30T20:34:05Z", 
    "summary": "Clarens is a uniquely flexible web services infrastructure providing a\nunified access protocol to a diverse set of functions useful to the HEP\ncommunity. It uses the standard HTTP protocol combined with application layer,\ncertificate based authentication to provide single sign-on to individuals,\norganizations and hosts, with fine-grained access control to services, files\nand virtual organization (VO) management. This contribution describes the\nserver functionality, while client applications are described in a subsequent\ntalk."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306003v2", 
    "title": "R-GMA: First results after deployment", 
    "arxiv-id": "cs/0306003v2", 
    "author": "Xiaomei Zhu", 
    "publish": "2003-05-30T20:39:27Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which is being\ndeveloped within the European DataGrid Project as an Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each VO had one large relational database. We provide a number of different\nProducer types with different characteristics; for example some support\nstreaming of information. We also provide combined Consumer/Producers, which\nare able to combine information and republish it. At the heart of the system is\nthe mediator, which for any query is able to find and connect to the best\nProducers to do the job. We are able to invoke MDS info-provider scripts and\npublish the resulting information via R-GMA in addition to having some of our\nown sensors. APIs are available which allow the user to deploy monitoring and\ninformation services for any application that may be needed in the future. We\nhave used it both for information about the grid (primarily to find what\nservices are available at any one time) and for application monitoring. R-GMA\nhas been deployed in Grid testbeds, we describe the results and experiences of\nthis deployment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306004v2", 
    "title": "Managing Dynamic User Communities in a Grid of Autonomous Resources", 
    "arxiv-id": "cs/0306004v2", 
    "author": "K. Lhorentey", 
    "publish": "2003-05-30T21:04:49Z", 
    "summary": "One of the fundamental concepts in Grid computing is the creation of Virtual\nOrganizations (VO's): a set of resource consumers and providers that join\nforces to solve a common problem. Typical examples of Virtual Organizations\ninclude collaborations formed around the Large Hadron Collider (LHC)\nexperiments. To date, Grid computing has been applied on a relatively small\nscale, linking dozens of users to a dozen resources, and management of these\nVO's was a largely manual operation. With the advance of large collaboration,\nlinking more than 10000 users with a 1000 sites in 150 counties, a\ncomprehensive, automated management system is required. It should be simple\nenough not to deter users, while at the same time ensuring local site autonomy.\nThe VO Management Service (VOMS), developed by the EU DataGrid and DataTAG\nprojects[1, 2], is a secured system for managing authorization for users and\nresources in virtual organizations. It extends the existing Grid Security\nInfrastructure[3] architecture with embedded VO affiliation assertions that can\nbe independently verified by all VO members and resource providers. Within the\nEU DataGrid project, Grid services for job submission, file- and database\naccess are being equipped with fine- grained authorization systems that take VO\nmembership into account. These also give resource owners the ability to ensure\nsite security and enforce local access policies. This paper will describe the\nEU DataGrid security architecture, the VO membership service and the local site\nenforcement mechanisms Local Centre Authorization Service (LCAS), Local\nCredential Mapping Service(LCMAPS) and the Java Trust and Authorization\nManager."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306007v1", 
    "title": "The first deployment of workload management services on the EU DataGrid   Testbed: feedback on design and implementation", 
    "arxiv-id": "cs/0306007v1", 
    "author": "A. Werbrouck", 
    "publish": "2003-05-31T19:39:54Z", 
    "summary": "Application users have now been experiencing for about a year with the\nstandardized resource brokering services provided by the 'workload management'\npackage of the EU DataGrid project (WP1). Understanding, shaping and pushing\nthe limits of the system has provided valuable feedback on both its design and\nimplementation. A digest of the lessons, and \"better practices\", that were\nlearned, and that were applied towards the second major release of the\nsoftware, is given."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306008v3", 
    "title": "The new BaBar Data Reconstruction Control System", 
    "arxiv-id": "cs/0306008v3", 
    "author": "T. M. Pulliam", 
    "publish": "2003-05-31T20:48:53Z", 
    "summary": "The BaBar experiment is characterized by extremely high luminosity, a complex\ndetector, and a huge data volume, with increasing requirements each year. To\nfulfill these requirements a new control system has been designed and developed\nfor the offline data reconstruction system. The new control system described in\nthis paper provides the performance and flexibility needed to manage a large\nnumber of small computing farms, and takes full benefit of OO design. The\ninfrastructure is well isolated from the processing layer, it is generic and\nflexible, based on a light framework providing message passing and cooperative\nmultitasking. The system is actively distributed, enforces the separation\nbetween different processing tiers by using different naming domains, and glues\nthem together by dedicated brokers. It provides a powerful Finite State Machine\nframework to describe custom processing models in a simple regular language.\nThis paper describes this new control system, currently in use at SLAC and\nPadova on ~450 CPUs organized in 12 farms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306011v1", 
    "title": "Grid Data Management in Action: Experience in Running and Supporting   Data Management Services in the EU DataGrid Project", 
    "arxiv-id": "cs/0306011v1", 
    "author": "Paul Millar", 
    "publish": "2003-06-02T08:50:48Z", 
    "summary": "In the first phase of the EU DataGrid (EDG) project, a Data Management System\nhas been implemented and provided for deployment. The components of the current\nEDG Testbed are: a prototype of a Replica Manager Service built around the\nbasic services provided by Globus, a centralised Replica Catalogue to store\ninformation about physical locations of files, and the Grid Data Mirroring\nPackage (GDMP) that is widely used in various HEP collaborations in Europe and\nthe US for data mirroring. During this year these services have been refined\nand made more robust so that they are fit to be used in a pre-production\nenvironment. Application users have been using this first release of the Data\nManagement Services for more than a year. In the paper we present the\ncomponents and their interaction, our implementation and experience as well as\nthe feedback received from our user communities. We have resolved not only\nissues regarding integration with other EDG service components but also many of\nthe interoperability issues with components of our partner projects in Europe\nand the U.S. The paper concludes with the basic lessons learned during this\noperation. These conclusions provide the motivation for the architecture of the\nnext generation of Data Management Services that will be deployed in EDG during\n2003."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306018v1", 
    "title": "A monitoring tool for a GRID operation center", 
    "arxiv-id": "cs/0306018v1", 
    "author": "G. Tortone", 
    "publish": "2003-06-03T17:22:07Z", 
    "summary": "WorldGRID is an intercontinental testbed spanning Europe and the US\nintegrating architecturally different Grid implementations based on the Globus\ntoolkit. The WorldGRID testbed has been successfully demonstrated during the\nWorldGRID demos at SuperComputing 2002 (Baltimore) and IST2002 (Copenhagen)\nwhere real HEP application jobs were transparently submitted from US and Europe\nusing \"native\" mechanisms and run where resources were available, independently\nof their location. To monitor the behavior and performance of such testbed and\nspot problems as soon as they arise, DataTAG has developed the EDT-Monitor tool\nbased on the Nagios package that allows for Virtual Organization centric views\nof the Grid through dynamic geographical maps. The tool has been used to spot\nseveral problems during the WorldGRID operations, such as malfunctioning\nResource Brokers or Information Servers, sites not correctly configured, job\ndispatching problems, etc. In this paper we give an overview of the package,\nits features and scalability solutions and we report on the experience acquired\nand the benefit that a GRID operation center would gain from such a tool."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306027v1", 
    "title": "HEP Applications Evaluation of the EDG Testbed and Middleware", 
    "arxiv-id": "cs/0306027v1", 
    "author": "S. Burke", 
    "publish": "2003-06-05T15:35:20Z", 
    "summary": "Workpackage 8 of the European Datagrid project was formed in January 2001\nwith representatives from the four LHC experiments, and with experiment\nindependent people from five of the six main EDG partners. In September 2002\nWP8 was strengthened by the addition of effort from BaBar and D0. The original\nmandate of WP8 was, following the definition of short- and long-term\nrequirements, to port experiment software to the EDG middleware and testbed\nenvironment. A major additional activity has been testing the basic\nfunctionality and performance of this environment. This paper reviews\nexperiences and evaluations in the areas of job submission, data management,\nmass storage handling, information systems and monitoring. It also comments on\nthe problems of remote debugging, the portability of code, and scaling problems\nwith increasing numbers of jobs, sites and nodes. Reference is made to the\npioneeering work of Atlas and CMS in integrating the use of the EDG Testbed\ninto their data challenges. A forward look is made to essential software\ndevelopments within EDG and to the necessary cooperation between EDG and LCG\nfor the LCG prototype due in mid 2003."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306029v1", 
    "title": "A Software Data Transport Framework for Trigger Applications on Clusters", 
    "arxiv-id": "cs/0306029v1", 
    "author": "Heinz Tilsner", 
    "publish": "2003-06-06T07:02:25Z", 
    "summary": "In the future ALICE heavy ion experiment at CERN's Large Hadron Collider\ninput data rates of up to 25 GB/s have to be handled by the High Level Trigger\n(HLT) system, which has to scale them down to at most 1.25 GB/s before being\nwritten to permanent storage. The HLT system that is being designed to cope\nwith these data rates consists of a large PC cluster, up to the order of a 1000\nnodes, connected by a fast network. For the software that will run on these\nnodes a flexible data transport and distribution software framework has been\ndeveloped. This framework consists of a set of separate components, that can be\nconnected via a common interface, allowing to construct different\nconfigurations for the HLT, that are even changeable at runtime. To ensure a\nfault-tolerant operation of the HLT, the framework includes a basic fail-over\nmechanism that will be further expanded in the future, utilizing the runtime\nreconnection feature of the framework's component interface. First performance\ntests show very promising results for the software, indicating that it can\nachieve an event rate for the data transport sufficiently high to satisfy\nALICE's requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306030v1", 
    "title": "Grid-based access control for Unix environments, Filesystems and Web   Sites", 
    "arxiv-id": "cs/0306030v1", 
    "author": "A. McNab", 
    "publish": "2003-06-06T10:45:43Z", 
    "summary": "The EU DataGrid has deployed a grid testbed at approximately 20 sites across\nEurope, with several hundred registered users. This paper describes\nauthorisation systems produced by GridPP and currently used on the EU DataGrid\nTestbed, including local Unix pool accounts and fine-grained access control\nwith Access Control Lists and Grid-aware filesystems, fileservers and web\ndevelopement environments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306045v1", 
    "title": "The WorldGrid transatlantic testbed: a successful example of Grid   interoperability across EU and U.S. domains", 
    "arxiv-id": "cs/0306045v1", 
    "author": "Marco Verlato", 
    "publish": "2003-06-11T16:47:17Z", 
    "summary": "The European DataTAG project has taken a major step towards making the\nconcept of a worldwide computing Grid a reality. In collaboration with the\ncompanion U.S. project iVDGL, DataTAG has realized an intercontinental testbed\nspanning Europe and the U.S. integrating architecturally different Grid\nimplementations based on the Globus toolkit. The WorldGrid testbed has been\nsuccessfully demonstrated at SuperComputing 2002 and IST2002 where real HEP\napplication jobs were transparently submitted from U.S. and Europe using native\nmechanisms and run where resources were available, independently of their\nlocation. In this paper we describe the architecture of the WorldGrid testbed,\nthe problems encountered and the solutions taken in realizing such a testbed.\nWith our work we present an important step towards interoperability of Grid\nmiddleware developed and deployed in Europe and the U.S.. Some of the solutions\ndeveloped in WorldGrid will be adopted by the LHC Computing Grid first service.\nTo the best of our knowledge, this is the first large-scale testbed that\ncombines middleware components and makes them work together."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306048v1", 
    "title": "Parallel netCDF: A Scientific High-Performance I/O Interface", 
    "arxiv-id": "cs/0306048v1", 
    "author": "Rob Latham", 
    "publish": "2003-06-11T20:25:52Z", 
    "summary": "Dataset storage, exchange, and access play a critical role in scientific\napplications. For such purposes netCDF serves as a portable and efficient file\nformat and programming interface, which is popular in numerous scientific\napplication domains. However, the original interface does not provide an\nefficient mechanism for parallel data storage and access. In this work, we\npresent a new parallel interface for writing and reading netCDF datasets. This\ninterface is derived with minimum changes from the serial netCDF interface but\ndefines semantics for parallel access and is tailored for high performance. The\nunderlying parallel I/O is achieved through MPI-IO, allowing for dramatic\nperformance gains through the use of collective I/O optimizations. We compare\nthe implementation strategies with HDF5 and analyze both. Our tests indicate\nprogramming convenience and significant I/O performance improvement with this\nparallel netCDF interface."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306051v2", 
    "title": "A data Grid testbed environment in Gigabit WAN with HPSS", 
    "arxiv-id": "cs/0306051v2", 
    "author": "Shigeo Yashiro", 
    "publish": "2003-06-12T08:48:16Z", 
    "summary": "For data analysis of large-scale experiments such as LHC Atlas and other\nJapanese high energy and nuclear physics projects, we have constructed a Grid\ntest bed at ICEPP and KEK. These institutes are connected to national\nscientific gigabit network backbone called SuperSINET. In our test bed, we have\ninstalled NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK\nas a large scale data store. Atlas simulation data at ICEPP has been\ntransferred and accessed using SuperSINET. We have tested various performances\nand characteristics of HPSS through this high speed WAN. The measurement\nincludes comparison between computing and storage resources are tightly coupled\nwith low latency LAN and long distant WAN."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306052v1", 
    "title": "ATLAS Data Challenge 1", 
    "arxiv-id": "cs/0306052v1", 
    "author": "Gilbert Poulard", 
    "publish": "2003-06-12T12:59:39Z", 
    "summary": "In 2002 the ATLAS experiment started a series of Data Challenges (DC) of\nwhich the goals are the validation of the Computing Model, of the complete\nsoftware suite, of the data model, and to ensure the correctness of the\ntechnical choices to be made. A major feature of the first Data Challenge (DC1)\nwas the preparation and the deployment of the software required for the\nproduction of large event samples for the High Level Trigger (HLT) and physics\ncommunities, and the production of those samples as a world-wide distributed\nactivity. The first phase of DC1 was run during summer 2002, and involved 39\ninstitutes in 18 countries. More than 10 million physics events and 30 million\nsingle particle events were fully simulated. Over a period of about 40 calendar\ndays 71000 CPU-days were used producing 30 Tbytes of data in about 35000\npartitions. In the second phase the next processing step was performed with the\nparticipation of 56 institutes in 21 countries (~ 4000 processors used in\nparallel). The basic elements of the ATLAS Monte Carlo production system are\ndescribed. We also present how the software suite was validated and the\nparticipating sites were certified. These productions were already partly\nperformed by using different flavours of Grid middleware at ~ 20 sites."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306055v1", 
    "title": "BlueOx: A Java Framework for Distributed Data Analysis", 
    "arxiv-id": "cs/0306055v1", 
    "author": "David Bengali", 
    "publish": "2003-06-12T15:53:17Z", 
    "summary": "High energy physics experiments including those at the Tevatron and the\nupcoming LHC require analysis of large data sets which are best handled by\ndistributed computation. We present the design and development of a distributed\ndata analysis framework based on Java. Analysis jobs run through three phases:\ndiscovery of data sets available, brokering/assignment of data sets to analysis\nservers, and job execution. Each phase is represented by a set of abstract\ninterfaces. These interfaces allow different techniques to be used without\nmodification to the framework. For example, the communications interface has\nbeen implemented by both a packet protocol and a SOAP-based scheme. User\nauthentication can be provided either through simple passwords or through a GSI\ncertificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a\nhypothetical high-energy linear collider experiment have been interfaced with\nthe framework."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306058v1", 
    "title": "Installing, Running and Maintaining Large Linux Clusters at CERN", 
    "arxiv-id": "cs/0306058v1", 
    "author": "Tim Smith", 
    "publish": "2003-06-12T18:45:34Z", 
    "summary": "Having built up Linux clusters to more than 1000 nodes over the past five\nyears, we already have practical experience confronting some of the LHC scale\ncomputing challenges: scalability, automation, hardware diversity, security,\nand rolling OS upgrades. This paper describes the tools and processes we have\nimplemented, working in close collaboration with the EDG project [1],\nespecially with the WP4 subtask, to improve the manageability of our clusters,\nin particular in the areas of system installation, configuration, and\nmonitoring. In addition to the purely technical issues, providing shared\ninteractive and batch services which can adapt to meet the diverse and changing\nrequirements of our users is a significant challenge. We describe the\ndevelopments and tuning that we have introduced on our LSF based systems to\nmaximise both responsiveness to users and overall system utilisation. Finally,\nthis paper will describe the problems we are facing in enlarging our\nheterogeneous Linux clusters, the progress we have made in dealing with the\ncurrent issues and the steps we are taking to gridify the clusters"
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306060v1", 
    "title": "DIRAC - Distributed Infrastructure with Remote Agent Control", 
    "arxiv-id": "cs/0306060v1", 
    "author": "A. Washbrook", 
    "publish": "2003-06-12T23:54:24Z", 
    "summary": "This paper describes DIRAC, the LHCb Monte Carlo production system. DIRAC has\na client/server architecture based on: Compute elements distributed among the\ncollaborating institutes; Databases for production management, bookkeeping (the\nmetadata catalogue) and software configuration; Monitoring and cataloguing\nservices for updating and accessing the databases. Locally installed software\nagents implemented in Python monitor the local batch queue, interrogate the\nproduction database for any outstanding production requests using the XML-RPC\nprotocol and initiate the job submission. The agent checks and, if necessary,\ninstalls any required software automatically. After the job has processed the\nevents, the agent transfers the output data and updates the metadata catalogue.\nDIRAC has been successfully installed at 18 collaborating institutes, including\nthe DataGRID, and has been used in recent Physics Data Challenges. In the near\nto medium term future we must use a mixed environment with different types of\ngrid middleware or no middleware. We describe how this flexibility has been\nachieved and how ubiquitously available grid middleware would improve DIRAC."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306063v1", 
    "title": "A Model for Grid User Management", 
    "arxiv-id": "cs/0306063v1", 
    "author": "Tomasz Wlodek", 
    "publish": "2003-06-13T17:01:45Z", 
    "summary": "Registration and management of users in a large scale Grid computing\nenvironment presents new challenges that are not well addressed by existing\nprotocols. Within a single Virtual Organization (VO), thousands of users will\npotentially need access to hundreds of computing sites, and the traditional\nmodel where users register for local accounts at each site will present\nsignificant scaling problems. However, computing sites must maintain control\nover access to the site and site policies generally require individual local\naccounts for every user. We present here a model that allows users to register\nonce with a VO and yet still provides all of the computing sites the\ninformation they require with the required level of trust. We have developed\ntools to allow sites to automate the management of local accounts and the\nmappings between Grid identities and local accounts."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306064v2", 
    "title": "Exploiting peer group concept for adaptive and highly available services", 
    "arxiv-id": "cs/0306064v2", 
    "author": "Arshad Ali", 
    "publish": "2003-06-13T13:38:01Z", 
    "summary": "This paper presents a prototype for redundant, highly available and fault\ntolerant peer to peer framework for data management. Peer to peer computing is\ngaining importance due to its flexible organization, lack of central authority,\ndistribution of functionality to participating nodes and ability to utilize\nunused computational resources. Emergence of GRID computing has provided much\nneeded infrastructure and administrative domain for peer to peer computing. The\ncomponents of this framework exploit peer group concept to scope service and\ninformation search, arrange services and information in a coherent manner,\nprovide selective redundancy and ensure availability in face of failure and\nhigh load conditions. A prototype system has been implemented using JXTA peer\nto peer technology and XML is used for service description and interfaces,\nallowing peers to communicate with services implemented in various platforms\nincluding web services and JINI services. It utilizes code mobility to achieve\nrole interchange among services and ensure dynamic group membership. Security\nis ensured by using Public Key Infrastructure (PKI) to implement group level\nsecurity policies for membership and service access."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306067v1", 
    "title": "The AliEn system, status and perspectives", 
    "arxiv-id": "cs/0306067v1", 
    "author": "A. J. Peters", 
    "publish": "2003-06-13T15:43:15Z", 
    "summary": "AliEn is a production environment that implements several components of the\nGrid paradigm needed to simulate, reconstruct and analyse HEP data in a\ndistributed way. The system is built around Open Source components, uses the\nWeb Services model and standard network protocols to implement the computing\nplatform that is currently being used to produce and analyse Monte Carlo data\nat over 30 sites on four continents. The aim of this paper is to present the\ncurrent AliEn architecture and outline its future developments in the light of\nemerging standards."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306068v1", 
    "title": "AliEn Resource Brokers", 
    "arxiv-id": "cs/0306068v1", 
    "author": "Andreas J. Peters", 
    "publish": "2003-06-13T16:00:45Z", 
    "summary": "AliEn (ALICE Environment) is a lightweight GRID framework developed by the\nAlice Collaboration. When the experiment starts running, it will collect data\nat a rate of approximately 2 PB per year, producing O(109) files per year. All\nthese files, including all simulated events generated during the preparation\nphase of the experiment, must be accounted and reliably tracked in the GRID\nenvironment. The backbone of AliEn is a distributed file catalogue, which\nassociates universal logical file name to physical file names for each dataset\nand provides transparent access to datasets independently of physical location.\nThe file replication and transport is carried out under the control of the File\nTransport Broker. In addition, the file catalogue maintains information about\nevery job running in the system. The jobs are distributed by the Job Resource\nBroker that is implemented using a simplified pull (as opposed to traditional\npush) architecture. This paper describes the Job and File Transport Resource\nBrokers and shows that a similar architecture can be applied to solve both\nproblems."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306069v1", 
    "title": "Distributed Offline Data Reconstruction in BaBar", 
    "arxiv-id": "cs/0306069v1", 
    "author": "Alvise Dorigo", 
    "publish": "2003-06-13T16:16:44Z", 
    "summary": "The BaBar experiment at SLAC is in its fourth year of running. The data\nprocessing system has been continuously evolving to meet the challenges of\nhigher luminosity running and the increasing bulk of data to re-process each\nyear. To meet these goals a two-pass processing architecture has been adopted,\nwhere 'rolling calibrations' are quickly calculated on a small fraction of the\nevents in the first pass and the bulk data reconstruction done in the second.\nThis allows for quick detector feedback in the first pass and allows for the\nparallelization of the second pass over two or more separate farms. This\ntwo-pass system allows also for distribution of processing farms off-site. The\nfirst such site has been setup at INFN Padova. The challenges met here were\nmany. The software was ported to a full Linux-based, commodity hardware system.\nThe raw dataset, 90 TB, was imported from SLAC utilizing a 155 Mbps network\nlink. A system for quality control and export of the processed data back to\nSLAC was developed. Between SLAC and Padova we are currently running three\npass-one farms, with 32 CPUs each, and nine pass-two farms with 64 to 80 CPUs\neach. The pass-two farms can process between 2 and 4 million events per day.\nDetails about the implementation and performance of the system will be\npresented."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306071v1", 
    "title": "AliEnFS - a Linux File System for the AliEn Grid Services", 
    "arxiv-id": "cs/0306071v1", 
    "author": "P. Buncic", 
    "publish": "2003-06-13T18:18:59Z", 
    "summary": "Among the services offered by the AliEn (ALICE Environment\nhttp://alien.cern.ch) Grid framework there is a virtual file catalogue to allow\ntransparent access to distributed data-sets using various file transfer\nprotocols. $alienfs$ (AliEn File System) integrates the AliEn file catalogue as\na new file system type into the Linux kernel using LUFS, a hybrid user space\nfile system framework (Open Source http://lufs.sourceforge.net). LUFS uses a\nspecial kernel interface level called VFS (Virtual File System Switch) to\ncommunicate via a generalised file system interface to the AliEn file system\ndaemon. The AliEn framework is used for authentication, catalogue browsing,\nfile registration and read/write transfer operations. A C++ API implements the\ngeneric file system operations. The goal of AliEnFS is to allow users easy\ninteractive access to a worldwide distributed virtual file system using\nfamiliar shell commands (f.e. cp,ls,rm ...) The paper discusses general aspects\nof Grid File Systems, the AliEn implementation and present and future\ndevelopments for the AliEn Grid File System."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306072v1", 
    "title": "The EU DataGrid Workload Management System: towards the second major   release", 
    "arxiv-id": "cs/0306072v1", 
    "author": "A. Werbrouck", 
    "publish": "2003-06-13T18:57:35Z", 
    "summary": "In the first phase of the European DataGrid project, the 'workload\nmanagement' package (WP1) implemented a working prototype, providing users with\nan environment allowing to define and submit jobs to the Grid, and able to find\nand use the ``best'' resources for these jobs. Application users have now been\nexperiencing for about a year now with this first release of the workload\nmanagement system. The experiences acquired, the feedback received by the user\nand the need to plug new components implementing new functionalities, triggered\nan update of the existing architecture. A description of this revised and\ncomplemented workload management system is given."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306074v1", 
    "title": "Understanding and Coping with Hardware and Software Failures in a Very   Large Trigger Farm", 
    "arxiv-id": "cs/0306074v1", 
    "author": "Jim Kowalkowski", 
    "publish": "2003-06-13T21:24:05Z", 
    "summary": "When thousands of processors are involved in performing event filtering on a\ntrigger farm, there is likely to be a large number of failures within the\nsoftware and hardware systems. BTeV, a proton/antiproton collider experiment at\nFermi National Accelerator Laboratory, has designed a trigger, which includes\nseveral thousand processors. If fault conditions are not given proper\ntreatment, it is conceivable that this trigger system will experience failures\nat a high enough rate to have a negative impact on its effectiveness. The RTES\n(Real Time Embedded Systems) collaboration is a group of physicists, engineers,\nand computer scientists working to address the problem of reliability in\nlarge-scale clusters with real-time constraints such as this. Resulting\ninfrastructure must be highly scalable, verifiable, extensible by users, and\ndynamically changeable."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306075v1", 
    "title": "Data Management for Physics Analysis in Phenix (BNL, RHIC)", 
    "arxiv-id": "cs/0306075v1", 
    "author": "Qiu Zhiping", 
    "publish": "2003-06-13T21:24:55Z", 
    "summary": "Every year the PHENIX collaboration deals with increasing volume of data (now\nabout 1/4 PB/year). Apparently the more data the more questions how to process\nall the data in most efficient way. In recent past many developments in HEP\ncomputing were dedicated to the production environment. Now we need more tools\nto help to obtain physics results from the analysis of distributed simulated\nand experimental data. Developments in Grid architectures gave many examples\nhow distributed computing facilities can be organized to meet physics analysis\nneeds. We feel that our main task in this area is to try to use already\ndeveloped systems or system components in PHENIX environment.\n  We are concentrating here on the followed problems: file/replica catalog\nwhich keep names of our files, data moving over WAN, job submission in\nmulticluster environment.\n  PHENIX is a running experiment and this fact narrowed our ability to test new\nsoftware on the collaboration computer facilities. We are experimenting with\nsystem prototypes at State University of New York at Stony Brook (SUNYSB) where\nwe run midrange computing cluster for physics analysis. The talk is dedicated\nto discuss some experience with Grid software and achieved results."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306084v1", 
    "title": "BaBar Web job submission with Globus authentication and AFS access", 
    "arxiv-id": "cs/0306084v1", 
    "author": "T. Adye", 
    "publish": "2003-06-14T02:39:07Z", 
    "summary": "We present two versions of a grid job submission system produced for the\nBaBar experiment. Both use globus job submission to process data spread across\nvarious sites, producing output which can be combined for analysis. The\nproblems encountered with authorisation and authentication, data location, job\nsubmission, and the input and output sandboxes are described, as are the\nsolutions. The total system is still some way short of the aims of enterprises\nsuch as the EDG, but represent a significant step along the way."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306092v1", 
    "title": "Building A High Performance Parallel File System Using Grid Datafarm and   ROOT I/O", 
    "arxiv-id": "cs/0306092v1", 
    "author": "A. Dell'Acqua", 
    "publish": "2003-06-14T16:29:16Z", 
    "summary": "Sheer amount of petabyte scale data foreseen in the LHC experiments require a\ncareful consideration of the persistency design and the system design in the\nworld-wide distributed computing. Event parallelism of the HENP data analysis\nenables us to take maximum advantage of the high performance cluster computing\nand networking when we keep the parallelism both in the data processing phase,\nin the data management phase, and in the data transfer phase. A modular\narchitecture of FADS/ Goofy, a versatile detector simulation framework for\nGeant4, enables an easy choice of plug-in facilities for persistency\ntechnologies such as Objectivity/DB and ROOT I/O. The framework is designed to\nwork naturally with the parallel file system of Grid Datafarm (Gfarm).\nFADS/Goofy is proven to generate 10^6 Geant4-simulated Atlas Mockup events\nusing a 512 CPU PC cluster. The data in ROOT I/O files is replicated using\nGfarm file system. The histogram information is collected from the distributed\nROOT files. During the data replication it has been demonstrated to achieve\nmore than 2.3 Gbps data transfer rate between the PC clusters over seven\nparticipating PC clusters in the United States and in Japan."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306093v1", 
    "title": "Grid-Brick Event Processing Framework in GEPS", 
    "arxiv-id": "cs/0306093v1", 
    "author": "Jaime E. Villate", 
    "publish": "2003-06-14T22:33:35Z", 
    "summary": "Experiments like ATLAS at LHC involve a scale of computing and data\nmanagement that greatly exceeds the capability of existing systems, making it\nnecessary to resort to Grid-based Parallel Event Processing Systems (GEPS).\nTraditional Grid systems concentrate the data in central data servers which\nhave to be accessed by many nodes each time an analysis or processing job\nstarts. These systems require very powerful central data servers and make\nlittle use of the distributed disk space that is available in commodity\ncomputers. The Grid-Brick system, which is described in this paper, follows a\ndifferent approach. The data storage is split among all grid nodes having each\none a piece of the whole information. Users submit queries and the system will\ndistribute the tasks through all the nodes and retrieve the result, merging\nthem together in the Job Submit Server. The main advantage of using this system\nis the huge scalability it provides, while its biggest disadvantage appears in\nthe case of failure of one of the nodes. A workaround for this problem involves\ndata replication or backup."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306096v1", 
    "title": "MonALISA : A Distributed Monitoring Service Architecture", 
    "arxiv-id": "cs/0306096v1", 
    "author": "C. Cirstoiu", 
    "publish": "2003-06-16T08:33:44Z", 
    "summary": "The MonALISA (Monitoring Agents in A Large Integrated Services Architecture)\nsystem provides a distributed monitoring service. MonALISA is based on a\nscalable Dynamic Distributed Services Architecture which is designed to meet\nthe needs of physics collaborations for monitoring global Grid systems, and is\nimplemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the\nsystem derives from the use of multithreaded Station Servers to host a variety\nof loosely coupled self-describing dynamic services, the ability of each\nservice to register itself and then to be discovered and used by any other\nservices, or clients that require such information, and the ability of all\nservices and clients subscribing to a set of events (state changes) in the\nsystem to be notified automatically. The framework integrates several existing\nmonitoring tools and procedures to collect parameters describing computational\nnodes, applications and network performance. It has built-in SNMP support and\nnetwork-performance monitoring algorithms that enable it to monitor end-to-end\nnetwork performance as well as the performance and state of site facilities in\na Grid. MonALISA is currently running around the clock on the US CMS test Grid\nas well as an increasing number of other sites. It is also being used to\nmonitor the performance and optimize the interconnections among the reflectors\nin the VRVS system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306100v1", 
    "title": "Site Authorization Service (SAZ)", 
    "arxiv-id": "cs/0306100v1", 
    "author": "Dane Skow", 
    "publish": "2003-06-16T16:07:24Z", 
    "summary": "In this paper we present a methodology to provide an additional level of\ncentralized control for the grid resources. This centralized control is applied\nto site-wide distribution of various grids and thus providing an upper hand in\nthe maintenance."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306110v1", 
    "title": "Run Control and Monitor System for the CMS Experiment", 
    "arxiv-id": "cs/0306110v1", 
    "author": "L. Zangrando", 
    "publish": "2003-06-18T16:34:11Z", 
    "summary": "The Run Control and Monitor System (RCMS) of the CMS experiment is the set of\nhardware and software components responsible for controlling and monitoring the\nexperiment during data-taking. It provides users with a \"virtual counting\nroom\", enabling them to operate the experiment and to monitor detector status\nand data quality from any point in the world. This paper describes the\narchitecture of the RCMS with particular emphasis on its scalability through a\ndistributed collection of nodes arranged in a tree-based hierarchy. The current\nimplementation of the architecture in a prototype RCMS used in test beam\nsetups, detector validations and DAQ demonstrators is documented. A discussion\nof the key technologies used, including Web Services, and the results of tests\nperformed with a 128-node system are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306111v1", 
    "title": "Sharing a conceptual model of grid resources and services", 
    "arxiv-id": "cs/0306111v1", 
    "author": "Cristina Vistoli", 
    "publish": "2003-06-18T14:55:40Z", 
    "summary": "Grid technologies aim at enabling a coordinated resource-sharing and\nproblem-solving capabilities over local and wide area networks and span\nlocations, organizations, machine architectures and software boundaries. The\nheterogeneity of involved resources and the need for interoperability among\ndifferent grid middlewares require the sharing of a common information model.\nAbstractions of different flavors of resources and services and conceptual\nschemas of domain specific entities require a collaboration effort in order to\nenable a coherent information services cooperation.\n  With this paper, we present the result of our experience in grid resources\nand services modelling carried out within the Grid Laboratory Uniform\nEnvironment (GLUE) effort, a joint US and EU High Energy Physics projects\ncollaboration towards grid interoperability. The first implementation-neutral\nagreement on services such as batch computing and storage manager, resources\nsuch as the hierarchy cluster, sub-cluster, host and the storage library are\npresented. Design guidelines and operational results are depicted together with\nopen issues and future evolutions."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306112v1", 
    "title": "Adapting SAM for CDF", 
    "arxiv-id": "cs/0306112v1", 
    "author": "R. St. Denis", 
    "publish": "2003-06-18T16:28:02Z", 
    "summary": "The CDF and D0 experiments probe the high-energy frontier and as they do so\nhave accumulated hundreds of Terabytes of data on the way to petabytes of data\nover the next two years. The experiments have made a commitment to use the\ndeveloping Grid based on the SAM system to handle these data. The D0 SAM has\nbeen extended for use in CDF as common patterns of design emerged to meet the\nsimilar requirements of these experiments. The process by which the merger was\nachieved is explained with particular emphasis on lessons learned concerning\nthe database design patterns plus realization of the use cases."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306115v1", 
    "title": "D0 Regional Analysis Center Concepts", 
    "arxiv-id": "cs/0306115v1", 
    "author": "representing the D0 Remote Analysis Task Force", 
    "publish": "2003-06-19T19:58:07Z", 
    "summary": "The D0 experiment is facing many exciting challenges providing a computing\nenvironment for its worldwide collaboration. Transparent access to data for\nprocessing and analysis has been enabled through deployment of its SAM system\nto collaborating sites and additional functionality will be provided soon with\nSAMGrid components. In order to maximize access to global storage,\ncomputational and intellectual resources, and to enable the system to scale to\nthe large demands soon to be realized, several strategic sites have been\nidentified as Regional Analysis Centers (RAC's). These sites play an expanded\nrole within the system. The philosophy and function of these centers is\ndiscussed and details of their composition and operation are outlined. The plan\nfor future additional centers is also addressed."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0306133v1", 
    "title": "GRAPPA: Grid Access Portal for Physics Applications", 
    "arxiv-id": "cs/0306133v1", 
    "author": "R. Bramley", 
    "publish": "2003-06-26T17:09:07Z", 
    "summary": "Grappa is a Grid portal effort designed to provide physicists convenient\naccess to Grid tools and services. The ATLAS analysis and control framework,\nAthena, was used as the target application. Grappa provides basic Grid\nfunctionality such as resource configuration, credential testing, job\nsubmission, job monitoring, results monitoring, and preliminary integration\nwith the ATLAS replica catalog system, MAGDA. Grappa uses Jython to combine the\nease of scripting with the power of java-based toolkits. This provides a\npowerful framework for accessing diverse Grid resources with uniform\ninterfaces. The initial prototype system was based on the XCAT Science Portal\ndeveloped at the Indiana University Extreme Computing Lab and was demonstrated\nby running Monte Carlo production on the U.S. ATLAS test-bed. The portal also\ncommunicated with a European resource broker on WorldGrid as part of the joint\niVDGL-DataTAG interoperability project for the IST2002 and SC2002\ndemonstrations. The current prototype replaces the XCAT Science Portal with an\nxbooks jetspeed portlet for managing user scripts."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307007v2", 
    "title": "Management of Grid Jobs and Information within SAMGrid", 
    "arxiv-id": "cs/0307007v2", 
    "author": "T. Rockwell", 
    "publish": "2003-07-03T20:26:13Z", 
    "summary": "We describe some of the key aspects of the SAMGrid system, used by the D0 and\nCDF experiments at Fermilab. Having sustained success of the data handling part\nof SAMGrid, we have developed new services for job and information services.\nOur job management is rooted in \\CondorG and uses enhancements that are general\napplicability for HEP grids. Our information system is based on a uniform\nframework for configuration management based on XML data representation and\nprocessing."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307019v1", 
    "title": "Lattice QCD Production on Commodity Clusters at Fermilab", 
    "arxiv-id": "cs/0307019v1", 
    "author": "J. Simone", 
    "publish": "2003-07-08T15:36:56Z", 
    "summary": "We describe the construction and results to date of Fermilab's three\nMyrinet-networked lattice QCD production clusters (an 80-node dual Pentium III\ncluster, a 48-node dual Xeon cluster, and a 128-node dual Xeon cluster). We\nexamine a number of aspects of performance of the MILC lattice QCD code running\non these clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307021v1", 
    "title": "Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at   Fermilab", 
    "arxiv-id": "cs/0307021v1", 
    "author": "S. Epsteyn", 
    "publish": "2003-07-08T16:58:57Z", 
    "summary": "Fermilab operates several clusters for lattice gauge computing. Minimal\nmanpower is available to manage these clusters. We have written a number of\ntools and developed techniques to cope with this task. We describe our tools\nwhich use the IPMI facilities of our systems for hardware management tasks such\nas remote power control, remote system resets, and health monitoring. We\ndiscuss our techniques involving network booting for installation and upgrades\nof the operating system on these computers, and for reloading BIOS and other\nfirmware. Finally, we discuss our tools for parallel command processing and\ntheir use in monitoring and administrating the PBS batch queue system used on\nour clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307052v1", 
    "title": "Gridscape: A Tool for the Creation of Interactive and Dynamic Grid   Testbed Web Portals", 
    "arxiv-id": "cs/0307052v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2003-07-22T12:27:28Z", 
    "summary": "The notion of grid computing has gained an increasing popularity recently as\na realistic solution to many of our large-scale data storage and processing\nneeds. It enables the sharing, selection and aggregation of resources\ngeographically distributed across collaborative organisations. Now more and\nmore people are beginning to embrace grid computing and thus are seeing the\nneed to set up their own grids and grid testbeds. With this comes the need to\nhave some means to enable them to view and monitor the status of the resources\nin these testbeds (eg. Web based Grid portal). Generally developers invest a\nsubstantial amount of time and effort developing custom monitoring software. To\novercome this limitation, this paper proposes Gridscape ? a tool that enables\nthe rapid creation of interactive and dynamic testbed portals (without any\nprogramming effort). Gridscape primarily aims to provide a solution for those\nusers who need to be able to create a grid testbed portal but don?t necessarily\nhave the time or resources to build a system of their own from scratch."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0307066v1", 
    "title": "Augernome & XtremWeb: Monte Carlos computation on a global computing   platform", 
    "arxiv-id": "cs/0307066v1", 
    "author": "Franck Cappello", 
    "publish": "2003-07-29T14:12:07Z", 
    "summary": "In this paper, we present XtremWeb, a Global Computing platform used to\ngenerate monte carlos showers in Auger, an HEP experiment to study the highest\nenergy cosmic rays at Mallargue-Mendoza, Argentina.\n  XtremWeb main goal, as a Global Computing platform, is to compute distributed\napplications using idle time of widely interconnected machines. It is\nespecially dedicated to -but not limited to- multi-parameters applications such\nas monte carlos computations; its security mechanisms ensuring not only hosts\nintegrity but also results certification and its fault tolerant features,\nencouraged us to test it and, finally, to deploy it as to support our CPU needs\nto simulate showers.\n  We first introduce Auger computing needs and how Global Computing could help.\nWe then detail XtremWeb architecture and goals. The fourth and last part\npresents the profits we have gained to choose this platform. We conclude on\nwhat could be done next."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0308024v1", 
    "title": "Relational Grid Monitoring Architecture (R-GMA)", 
    "arxiv-id": "cs/0308024v1", 
    "author": "Antony J Wilson", 
    "publish": "2003-08-15T23:53:49Z", 
    "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which has been\ndeveloped within the European DataGrid Project as a Grid Information and\nMonitoring System. Is is based on the GMA from GGF, which is a simple\nConsumer-Producer model. The special strength of this implementation comes from\nthe power of the relational model. We offer a global view of the information as\nif each Virtual Organisation had one large relational database. We provide a\nnumber of different Producer types with different characteristics; for example\nsome support streaming of information. We also provide combined\nConsumer/Producers, which are able to combine information and republish it. At\nthe heart of the system is the mediator, which for any query is able to find\nand connect to the best Producers for the job. We have developed components to\nallow a measure of inter-working between MDS and R-GMA. We have used it both\nfor information about the grid (primarily to find out about what services are\navailable at any one time) and for application monitoring. R-GMA has been\ndeployed in various testbeds; we describe some preliminary results and\nexperiences of this deployment."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S012918310300436X", 
    "link": "http://arxiv.org/pdf/cs/0309026v1", 
    "title": "A thought experiment on Quantum Mechanics and Distributed Failure   Detection", 
    "arxiv-id": "cs/0309026v1", 
    "author": "Mark C. Little", 
    "publish": "2003-09-15T10:43:47Z", 
    "summary": "One of the biggest problems in current distributed systems is that presented\nby one machine attempting to determine the liveness of another in a timely\nmanner. Unfortunately, the symptoms exhibited by a failed machine can also be\nthe result of other causes, e.g., an overloaded machine or network which drops\nmessages, making it impossible to detect a machine failure with cetainty until\nthat machine recovers. This is a well understood problem and one which has led\nto a large body of research into failure suspectors: since it is not possible\nto detect a failure, the best one can do is suspect a failure and program\naccordingly. However, one machine's suspicions may not be the same as\nanother's; therefore, these algorithms spend a considerable effort in ensuring\na consistent view among all available machines of who is suspects of being\nfailed. This paper describes a thought experiment on how quantum mechanics may\nbe used to provide a failure detector that is guaranteed to give both accurate\nand instantaneous information about the liveness of machines, no matter the\ndistances involved."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0166-218X(03)00368-8", 
    "link": "http://arxiv.org/pdf/cs/0309040v1", 
    "title": "A distributed algorithm to find k-dominating sets", 
    "arxiv-id": "cs/0309040v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T01:14:43Z", 
    "summary": "We consider a connected undirected graph $G(n,m)$ with $n$ nodes and $m$\nedges. A $k$-dominating set $D$ in $G$ is a set of nodes having the property\nthat every node in $G$ is at most $k$ edges away from at least one node in $D$.\nFinding a $k$-dominating set of minimum size is NP-hard. We give a new\nsynchronous distributed algorithm to find a $k$-dominating set in $G$ of size\nno greater than $\\lfloor n/(k+1)\\rfloor$. Our algorithm requires $O(k\\log^*n)$\ntime and $O(m\\log k+n\\log k\\log^*n)$ messages to run. It has the same time\ncomplexity as the best currently known algorithm, but improves on that\nalgorithm's message complexity and is, in addition, conceptually simpler."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0309042v1", 
    "title": "On reducing the complexity of matrix clocks", 
    "arxiv-id": "cs/0309042v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T12:57:59Z", 
    "summary": "Matrix clocks are a generalization of the notion of vector clocks that allows\nthe local representation of causal precedence to reach into an asynchronous\ndistributed computation's past with depth $x$, where $x\\ge 1$ is an integer.\nMaintaining matrix clocks correctly in a system of $n$ nodes requires that\neverymessage be accompanied by $O(n^x)$ numbers, which reflects an exponential\ndependency of the complexity of matrix clocks upon the desired depth $x$. We\nintroduce a novel type of matrix clock, one that requires only $nx$ numbers to\nbe attached to each message while maintaining what for many applications may be\nthe most significant portion of the information that the original matrix clock\ncarries. In order to illustrate the new clock's applicability, we demonstrate\nits use in the monitoring of certain resource-sharing computations."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0309049v1", 
    "title": "Control and Debugging of Distributed Programs Using Fiddle", 
    "arxiv-id": "cs/0309049v1", 
    "author": "Vitor Moreira", 
    "publish": "2003-09-26T11:49:10Z", 
    "summary": "The main goal of Fiddle, a distributed debugging engine, is to provide a\nflexible platform for developing debugging tools. Fiddle provides a layered set\nof interfaces with a minimal set of debugging functionalities, for the\ninspection and control of distributed and multi-threaded applications.\n  This paper illustrates how Fiddle is used to support integrated testing and\ndebugging. The approach described is based on a tool, called Deipa, that\ninterprets sequences of commands read from an input file, generated by an\nindependent testing tool. Deipa acts as a Fiddle client, in order to enforce\nspecific execution paths in a distributed PVM program. Other Fiddle clients may\nbe used along with Deipa for the fine debugging at process level. Fiddle and\nDeipa functionalities and architectures are described, and a working example\nshows a step-by-step application of these tools."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0310029v1", 
    "title": "Optimizing Noncontiguous Accesses in MPI-IO", 
    "arxiv-id": "cs/0310029v1", 
    "author": "Ewing Lusk", 
    "publish": "2003-10-15T19:35:00Z", 
    "summary": "The I/O access patterns of many parallel applications consist of accesses to\na large number of small, noncontiguous pieces of data. If an application's I/O\nneeds are met by making many small, distinct I/O requests, however, the I/O\nperformance degrades drastically. To avoid this problem, MPI-IO allows users to\naccess noncontiguous data with a single I/O function call, unlike in Unix I/O.\nIn this paper, we explain how critical this feature of MPI-IO is for high\nperformance and how it enables implementations to perform optimizations. We\nfirst provide a classification of the different ways of expressing an\napplication's I/O needs in MPI-IO--we classify them into four levels, called\nlevel~0 through level~3. We demonstrate that, for applications with\nnoncontiguous access patterns, the I/O performance improves dramatically if\nusers write their applications to make level-3 requests (noncontiguous,\ncollective) rather than level-0 requests (Unix style). We then describe how our\nMPI-IO implementation, ROMIO, delivers high performance for noncontiguous\nrequests. We explain in detail the two key optimizations ROMIO performs: data\nsieving for noncontiguous requests from one process and collective I/O for\nnoncontiguous requests from multiple processes. We describe how we have\nimplemented these optimizations portably on multiple machines and file systems,\ncontrolled their memory requirements, and also achieved high performance. We\ndemonstrate the performance and portability with performance results for three\napplications--an astrophysics-application template (DIST3D), the NAS BTIO\nbenchmark, and an unstructured code (UNSTRUC)--on five different parallel\nmachines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0310030v1", 
    "title": "A Particular Bug Trap: Execution Replay Using Virtual Machines", 
    "arxiv-id": "cs/0310030v1", 
    "author": "Oliver Oppitz", 
    "publish": "2003-10-15T20:54:14Z", 
    "summary": "Execution-replay (ER) is well known in the literature but has been restricted\nto special system architectures for many years. Improved hardware resources and\nthe maturity of virtual machine technology promise to make ER useful for a\nbroader range of development projects.\n  This paper describes an approach to create a practical, generic ER\ninfrastructure for desktop PC systems using virtual machine technology. In the\ncreated VM environment arbitrary application programs will run and be replayed\nunmodified, neither instrumentation nor recompilation are required."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311009v1", 
    "title": "OGSA/Globus Evaluation for Data Intensive Applications", 
    "arxiv-id": "cs/0311009v1", 
    "author": "C. Wang", 
    "publish": "2003-11-10T11:12:26Z", 
    "summary": "We present an architecture of Globus Toolkit 3 based testbed intended for\nevaluation of applicability of the Open Grid Service Architecture (OGSA) for\nData Intensive Applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311010v1", 
    "title": "Problem of Application Job Monitoring in GRID Systems", 
    "arxiv-id": "cs/0311010v1", 
    "author": "A. Kryukov", 
    "publish": "2003-11-10T11:39:04Z", 
    "summary": "We present a new approach to monitoring of the execution process of an\napplication job in the GRID environment. The main point of the approach is use\nof GRID ervices to access monitoring information with the security level\navailable in GRID."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0311021v1", 
    "title": "LCG-1 Deployment and usage experience", 
    "arxiv-id": "cs/0311021v1", 
    "author": "L. Shamardin", 
    "publish": "2003-11-17T13:19:31Z", 
    "summary": "LCG-1 is the second release of the software framework for the LHC Computing\nGrid project. In our work we describe the installation process, arising\nproblems and their solutions, and configuration tuning details of the complete\nLCG-1 site, including all LCG elements required for the self-sufficient site."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0312022v1", 
    "title": "GridEmail: A Case for Economically Regulated Internet-based   Interpersonal Communications", 
    "arxiv-id": "cs/0312022v1", 
    "author": "Baikunth Nath", 
    "publish": "2003-12-12T11:42:17Z", 
    "summary": "Email has emerged as a dominant form of electronic communication between\npeople. Spam is a major problem for email users, with estimates of up to 56% of\nemail falling into that category. Control of Spam is being attempted with\ntechnical and legislative methods. In this paper we look at email and spam from\na supply-demand perspective. We propose Gridemail, an email system based on an\neconomy of communicating parties, where participants? motivations are\nrepresented as pricing policies and profiles. This system is expected to help\npeople regulate their personal communications to suit their conditions, and\nhelp in removing unwanted messages."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0312049v1", 
    "title": "Using virtual processors for SPMD parallel programs", 
    "arxiv-id": "cs/0312049v1", 
    "author": "Gianluca Argentini", 
    "publish": "2003-12-21T14:37:24Z", 
    "summary": "In this paper I describe some results on the use of virtual processors\ntechnology for parallelize some SPMD computational programs. The tested\ntechnology is the INTEL Hyper Threading on real processors, and the programs\nare MATLAB scripts for floating points computation. The conclusions of the work\nconcern on the utility and limits of the used approach. The main result is that\nusing virtual processors is a good technique for improving parallel programs\nnot only for memory-based computations, but in the case of massive disk-storage\noperations too."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0401027v1", 
    "title": "ClassdescMP: Easy MPI programming in C++", 
    "arxiv-id": "cs/0401027v1", 
    "author": "Duraid Madina", 
    "publish": "2004-01-27T03:55:27Z", 
    "summary": "ClassdescMP is a distributed memory parallel programming system for use with\nC++ and MPI. It uses the Classdesc reflection system to ease the task of\nbuilding complicated messages to be sent between processes. It doesn't hide the\nunderlying MPI API, so it is an augmentation of MPI capabilities. Users can\nstill call standard MPI function calls if needed for performance reasons."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402012v1", 
    "title": "A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and   Failure Detectors", 
    "arxiv-id": "cs/0402012v1", 
    "author": "Aleta Ricciardi", 
    "publish": "2004-02-05T17:15:08Z", 
    "summary": "It is shown that, in a precise sense, if there is no bound on the number of\nfaulty processes in a system with unreliable but fair communication, Uniform\nDistributed Coordination (UDC) can be attained if and only if a system has\nperfect failure detectors. This result is generalized to the case where there\nis a bound t on the number of faulty processes. It is shown that a certain type\nof generalized failure detector is necessary and sufficient for achieving UDC\nin a context with at most t faulty processes. Reasoning about processes'\nknowledge as to which other processes are faulty plays a key role in the\nanalysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402017v1", 
    "title": "Alchemi: A .NET-based Grid Computing Framework and its Integration into   Global Grids", 
    "arxiv-id": "cs/0402017v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-02-10T09:18:07Z", 
    "summary": "Computational grids that couple geographically distributed resources are\nbecoming the de-facto computing platform for solving large-scale problems in\nscience, engineering, and commerce. Software to enable grid computing has been\nprimarily written for Unix-class operating systems, thus severely limiting the\nability to effectively utilize the computing resources of the vast majority of\ndesktop computers i.e. those running variants of the Microsoft Windows\noperating system. Addressing Windows-based grid computing is particularly\nimportant from the software industry's viewpoint where interest in grids is\nemerging rapidly. Microsoft's .NET Framework has become near-ubiquitous for\nimplementing commercial distributed systems for Windows-based platforms,\npositioning it as the ideal platform for grid computing in this context. In\nthis paper we present Alchemi, a .NET-based grid computing framework that\nprovides the runtime machinery and programming environment required to\nconstruct desktop grids and develop grid applications. It allows flexible\napplication composition by supporting an object-oriented grid application\nprogramming model in addition to a grid job model. Cross-platform support is\nprovided via a web services interface and a flexible execution model supports\ndedicated and non-dedicated (voluntary) execution by grid nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/S0167-8191(03)00066-8", 
    "link": "http://arxiv.org/pdf/cs/0402018v1", 
    "title": "P2P Networks for Content Sharing", 
    "arxiv-id": "cs/0402018v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2004-02-10T14:24:48Z", 
    "summary": "Peer-to-peer (P2P) technologies have been widely used for content sharing,\npopularly called \"file-swapping\" networks. This chapter gives a broad overview\nof content sharing P2P technologies. It starts with the fundamental concept of\nP2P computing followed by the analysis of network topologies used in\npeer-to-peer systems. Next, three milestone peer-to-peer technologies: Napster,\nGnutella, and Fasttrack are explored in details, and they are finally concluded\nwith the comparison table in the last section."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0403015v2", 
    "title": "Belle Computing System", 
    "arxiv-id": "cs/0403015v2", 
    "author": "Masahiko Yokoyama", 
    "publish": "2004-03-11T09:20:20Z", 
    "summary": "We describe the present status of the computing system in the Belle\nexperiment at the KEKB $e^+e^-$ asymmetric-energy collider. So far, we have\nlogged more than 160 fb$^{-1}$ of data, corresponding to the world's largest\ndata sample of 170M $B\\bar{B}$ pairs at the $\\Upsilon(4S)$ energy region. A\nlarge amount of event data has to be processed to produce an analysis event\nsample in a timely fashion. In addition, Monte Carlo events have to be created\nto control systematic errors accurately. This requires stable and efficient\nusage of computing resources. Here we review our computing model and then\ndescribe how we efficiently proceed DST/MC productions in our system."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404014v1", 
    "title": "A Modular and Fault-Tolerant Data Transport Framework", 
    "arxiv-id": "cs/0404014v1", 
    "author": "Timm M. Steinbeck", 
    "publish": "2004-04-06T13:35:55Z", 
    "summary": "The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to\nreduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output\nbefore the data is written to permanent storage. To cope with these data rates\na large PC cluster system is being designed to scale to several 1000 nodes,\nconnected by a fast network. For the software that will run on these nodes a\nflexible data transport and distribution software framework, described in this\nthesis, has been developed. The framework consists of a set of separate\ncomponents, that can be connected via a common interface. This allows to\nconstruct different configurations for the HLT, that are even changeable at\nruntime. To ensure a fault-tolerant operation of the HLT, the framework\nincludes a basic fail-over mechanism that allows to replace whole nodes after a\nfailure. The mechanism will be further expanded in the future, utilizing the\nruntime reconnection feature of the framework's component interface. To connect\ncluster nodes a communication class library is used that abstracts from the\nactual network technology and protocol used to retain flexibility in the\nhardware choice. It contains already two working prototype versions for the TCP\nprotocol as well as SCI network adapters. Extensions can be added to the\nlibrary without modifications to other parts of the framework. Extensive tests\nand measurements have been performed with the framework. Their results as well\nas conclusions drawn from them are also presented in this thesis. Performance\ntests show very promising results for the system, indicating that it can\nfulfill ALICE's requirements concerning the data transport."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404015v2", 
    "title": "The study of distributed computing algorithms by multithread   applications", 
    "arxiv-id": "cs/0404015v2", 
    "author": "Ahmet A. Husainov", 
    "publish": "2004-04-07T01:27:32Z", 
    "summary": "The material in this note is used as an introduction to distributed\nalgorithms in a four year course on software and automatic control system in\nthe computer technology department of the Komsomolsk-on-Amur state technical\nuniversity. All our the program examples are written in Borland C/C++ 5.02 for\nWindows 95/98/2000/NT/XP, and hence suit to compile and execute by Visual\nC/C++. We consider the following approaches of the distributed computing: the\nconversion of recursive algorithms to multithread applications, a realization\nof the pairing algorithm, the building of wave systems by Petri nets and object\noriented programming."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0404027v1", 
    "title": "The Gridbus Toolkit for Service Oriented Grid and Utility Computing: An   Overview and Status Report", 
    "arxiv-id": "cs/0404027v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-04-13T09:16:54Z", 
    "summary": "Grids aim at exploiting synergies that result from cooperation of autonomous\ndistributed entities. The synergies that result from grid cooperation include\nthe sharing, exchange, selection, and aggregation of geographically distributed\nresources such as computers, data bases, software, and scientific instruments\nfor solving large-scale problems in science, engineering, and commerce. For\nthis cooperation to be sustainable, participants need to have economic\nincentive. Therefore, \"incentive\" mechanisms should be considered as one of key\ndesign parameters of Grid architectures. In this article, we present an\noverview and status of an open source Grid toolkit, called Gridbus, whose\narchitecture is fundamentally driven by the requirements of Grid economy.\nGridbus technologies provide services for both computational and data grids\nthat power the emerging eScience and eBusiness applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0405023v1", 
    "title": "A Grid Service Broker for Scheduling Distributed Data-Oriented   Applications on Global Grids", 
    "arxiv-id": "cs/0405023v1", 
    "author": "Lyle Winton", 
    "publish": "2004-05-06T10:15:26Z", 
    "summary": "The next generation of scientific experiments and studies, popularly called\nas e-Science, is carried out by large collaborations of researchers distributed\naround the world engaged in analysis of huge collections of data generated by\nscientific instruments. Grid computing has emerged as an enabler for e-Science\nas it permits the creation of virtual organizations that bring together\ncommunities with common objectives. Within a community, data collections are\nstored or replicated on distributed resources to enhance storage capability or\nefficiency of access. In such an environment, scientists need to have the\nability to carry out their studies by transparently accessing distributed data\nand computational resources. In this paper, we propose and develop a Grid\nbroker that mediates access to distributed resources by (a) discovering\nsuitable data sources for a given analysis scenario, (b) suitable computational\nresources, (c) optimally mapping analysis jobs to resources, (d) deploying and\nmonitoring job execution on selected resources, (e) accessing data from local\nor remote data source during job execution and (f) collating and presenting\nresults. The broker supports a declarative and dynamic parametric programming\nmodel for creating grid applications. We have used this model in grid-enabling\na high energy physics analysis application (Belle Analysis Software Framework).\nThe broker has been used in deploying Belle experiment data analysis jobs on a\ngrid testbed, called Belle Analysis Data Grid, having resources distributed\nacross Australia interconnected through GrangeNet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0405086v1", 
    "title": "A New Dynamical Domain Decomposition Method for Parallel Molecular   Dynamics Simulation on Grid", 
    "arxiv-id": "cs/0405086v1", 
    "author": "Shinji Shimojo", 
    "publish": "2004-05-24T07:29:13Z", 
    "summary": "We develop a new Lagrangian material particle -- dynamical domain\ndecomposition method (MPD^3) for large scale parallel molecular dynamics (MD)\nsimulation of nonstationary heterogeneous systems on a heterogeneous computing\nnet. MPD^3 is based on Voronoi decomposition of simulated matter. The map of\nVoronoi polygons is known as the Dirichlet tessellation and used for grid\ngeneration in computational fluid dynamics. From the hydrodynamics point of\nview the moving Voronoi polygon looks as a material particle (MP). MPs can\nexchange particles and information. To balance heterogeneous computing\nconditions the MP centers should be dependent on timing data. We propose a\nsimple and efficient iterative algorithm which based on definition of the\ntiming-dependent balancing displacement of MP center for next simulation step.\n  The MPD^3 program was tested in various computing environments and physical\nproblems. We have demonstrated that MPD^3 is a high-adaptive decomposition\nalgorithm for MD simulation. It was shown that the well-balanced decomposition\ncan result from dynamical Voronoi polygon tessellation. One would expect the\nsimilar approach can be successfully applied for other particle methods like\nMonte Carlo, particle-in-cell, and smooth-particle-hydrodynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407001v1", 
    "title": "Global Grids and Software Toolkits: A Study of Four Grid Middleware   Technologies", 
    "arxiv-id": "cs/0407001v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2004-07-01T11:54:06Z", 
    "summary": "Grid is an infrastructure that involves the integrated and collaborative use\nof computers, networks, databases and scientific instruments owned and managed\nby multiple organizations. Grid applications often involve large amounts of\ndata and/or computing resources that require secure resource sharing across\norganizational boundaries. This makes Grid application management and\ndeployment a complex undertaking. Grid middlewares provide users with seamless\ncomputing ability and uniform access to resources in the heterogeneous Grid\nenvironment. Several software toolkits and systems have been developed, most of\nwhich are results of academic research projects, all over the world. This\nchapter will focus on four of these middlewares--UNICORE, Globus, Legion and\nGridbus. It also presents our implementation of a resource broker for UNICORE\nas this functionality was not supported in it. A comparison of these systems on\nthe basis of the architecture, implementation model and several other features\nis included."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407012v1", 
    "title": "A Taxonomy and Survey of Grid Resource Planning and Reservation Systems   for Grid Enabled Analysis Environment", 
    "arxiv-id": "cs/0407012v1", 
    "author": "Conrad Steenberg", 
    "publish": "2004-07-05T15:48:43Z", 
    "summary": "The concept of coupling geographically distributed resources for solving\nlarge scale problems is becoming increasingly popular forming what is popularly\ncalled grid computing. Management of resources in the Grid environment becomes\ncomplex as the resources are geographically distributed, heterogeneous in\nnature and owned by different individuals and organizations each having their\nown resource management policies and different access and cost models. There\nhave been many projects that have designed and implemented the resource\nmanagement systems with a variety of architectures and services. In this paper\nwe have presented the general requirements that a Resource Management system\nshould satisfy. The taxonomy has also been defined based on which survey of\nresource management systems in different existing Grid projects has been\nconducted to identify the key areas where these systems lack the desired\nfunctionality."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407013v1", 
    "title": "Distributed Analysis and Load Balancing System for Grid Enabled Analysis   on Hand-held devices using Multi-Agents Systems", 
    "arxiv-id": "cs/0407013v1", 
    "author": "Ian Willers", 
    "publish": "2004-07-05T16:08:36Z", 
    "summary": "Handheld devices, while growing rapidly, are inherently constrained and lack\nthe capability of executing resource hungry applications. This paper presents\nthe design and implementation of distributed analysis and load-balancing system\nfor hand-held devices using multi-agents system. This system enables low\nresource mobile handheld devices to act as potential clients for Grid enabled\napplications and analysis environments. We propose a system, in which mobile\nagents will transport, schedule, execute and return results for heavy\ncomputational jobs submitted by handheld devices. Moreover, in this way, our\nsystem provides high throughput computing environment for hand-held devices."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407014v2", 
    "title": "A Grid-enabled Interface to Condor for Interactive Analysis on Handheld   and Resource-limited Devices", 
    "arxiv-id": "cs/0407014v2", 
    "author": "Ian Willers", 
    "publish": "2004-07-05T19:40:00Z", 
    "summary": "This paper was withdrawn by the authors."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0407017v1", 
    "title": "A Low Cost Distributed Computing Approach to Pulsar Searches at a Small   College", 
    "arxiv-id": "cs/0407017v1", 
    "author": "Reid Sherman", 
    "publish": "2004-07-07T18:55:20Z", 
    "summary": "We describe a distributed processing cluster of inexpensive Linux machines\ndeveloped jointly by the Astronomy and Computer Science departments at\nHaverford College which has been successfully used to search a large volume of\ndata from a recent radio pulsar survey. Analysis of radio pulsar surveys\nrequires significant computational resources to handle the demanding data\nstorage and processing needs. One goal of this project was to explore issues\nencountered when processing a large amount of pulsar survey data with limited\ncomputational resources. This cluster, which was developed and activated in\nonly a few weeks by supervised undergraduate summer research students, used\nexisting decommissioned computers, the campus network, and a script-based,\nclient-oriented, self-scheduled data distribution approach to process the data.\nThis setup provided simplicity, efficiency, and \"on-the-fly\" scalability at low\ncost. The entire 570 GB data set from the pulsar survey was processed at\nHaverford over the course of a ten-week summer period using this cluster. We\nconclude that this cluster can serve as a useful computational model in cases\nwhere data processing must be carried out on a limited budget. We have also\nconstructed a DVD archive of the raw survey data in order to investigate the\nfeasibility of using DVD as an inexpensive and easily accessible raw data\nstorage format for pulsar surveys. DVD-based storage has not been widely\nexplored in the pulsar community, but it has several advantages. The DVD\narchive we have constructed is reliable, portable, inexpensive, and can be\neasily read by any standard modern machine."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408032v2", 
    "title": "Performance Characterisation of Intra-Cluster Collective Communications", 
    "arxiv-id": "cs/0408032v2", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T06:31:10Z", 
    "summary": "Although recent works try to improve collective communication in grid systems\nby separating intra and inter-cluster communication, the optimisation of\ncommunications focus only on inter-cluster communications. We believe, instead,\nthat the overall performance of the application may be improved if\nintra-cluster collective communications performance is known in advance. Hence,\nit is important to have an accurate model of the intra-cluster collective\ncommunications, which provides the necessary evidences to tune and to predict\ntheir performance correctly. In this paper we present our experience on\nmodelling such communication strategies. We describe and compare different\nimplementation strategies with their communication models, evaluating the\nmodels' accuracy and describing the practical challenges that can be found when\nmodelling collective communications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408033v3", 
    "title": "Identifying Logical Homogeneous Clusters for Efficient Wide-area   Communications", 
    "arxiv-id": "cs/0408033v3", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T07:39:57Z", 
    "summary": "Recently, many works focus on the implementation of collective communication\noperations adapted to wide area computational systems, like computational Grids\nor global-computing. Due to the inherently heterogeneity of such environments,\nmost works separate \"clusters\" in different hierarchy levels. to better model\nthe communication. However, in our opinion, such works do not give enough\nattention to the delimitation of such clusters, as they normally use the\nlocality or the IP subnet from the machines to delimit a cluster without\nverifying the \"homogeneity\" of such clusters. In this paper, we describe a\nstrategy to gather network information from different local-area networks and\nto construct \"logical homogeneous clusters\", better suited to the performance\nmodelling."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0408034v3", 
    "title": "Fast Tuning of Intra-Cluster Collective Communications", 
    "arxiv-id": "cs/0408034v3", 
    "author": "Gregory Mounie", 
    "publish": "2004-08-14T07:40:33Z", 
    "summary": "Recent works try to optimise collective communication in grid systems\nfocusing mostly on the optimisation of communications among different clusters.\nWe believe that intra-cluster collective communications should also be\noptimised, as a way to improve the overall efficiency and to allow the\nconstruction of multi-level collective operations. Indeed, inside homogeneous\nclusters, a simple optimisation approach rely on the comparison from different\nimplementation strategies, through their communication models. In this paper we\nevaluate this approach, comparing different implementation strategies with\ntheir predicted performances. As a result, we are able to choose the\ncommunication strategy that better adapts to each network environment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410009v1", 
    "title": "Diffusive Load Balancing of Loosely-Synchronous Parallel Programs over   Peer-to-Peer Networks", 
    "arxiv-id": "cs/0410009v1", 
    "author": "Aaron Harwood", 
    "publish": "2004-10-05T08:23:45Z", 
    "summary": "The use of under-utilized Internet resources is widely recognized as a viable\nform of high performance computing. Sustained processing power of roughly 40T\nFLOPS using 4 million volunteered Internet hosts has been reported for\nembarrassingly parallel problems. At the same time, peer-to-peer (P2P) file\nsharing networks, with more than 50 million participants, have demonstrated the\ncapacity for scale in distributed systems. This paper contributes a study of\nload balancing techniques for a general class of loosely-synchronous parallel\nalgorithms when executed over a P2P network. We show that decentralized,\ndiffusive load balancing can be effective at balancing load and is facilitated\nby the dynamic properties of P2P. While a moderate degree of dynamicity can\nbenefit load balancing, significant dynamicity hinders the parallel program\nperformance due to the need for increased load migration. To the best of our\nknowledge this study provides new insight into the performance of\nloosely-synchronous parallel programs over the Internet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410016v1", 
    "title": "HEP@Home - A distributed computing system based on BOINC", 
    "arxiv-id": "cs/0410016v1", 
    "author": "Pedro Andrade", 
    "publish": "2004-10-07T12:42:10Z", 
    "summary": "Project SETI@HOME has proven to be one of the biggest successes of\ndistributed computing during the last years. With a quite simple approach SETI\nmanages to process large volumes of data using a vast amount of distributed\ncomputer power.\n  To extend the generic usage of this kind of distributed computing tools,\nBOINC is being developed. In this paper we propose HEP@HOME, a BOINC version\ntailored to the specific requirements of the High Energy Physics (HEP)\ncommunity.\n  The HEP@HOME will be able to process large amounts of data using virtually\nunlimited computing power, as BOINC does, and it should be able to work\naccording to HEP specifications. In HEP the amounts of data to be analyzed or\nreconstructed are of central importance. Therefore, one of the design\nprinciples of this tool is to avoid data transfer. This will allow scientists\nto run their analysis applications and taking advantage of a large number of\nCPUs. This tool also satisfies other important requirements in HEP, namely,\nsecurity, fault-tolerance and monitoring."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0410074v2", 
    "title": "ReCord: A Distributed Hash Table with Recursive Structure", 
    "arxiv-id": "cs/0410074v2", 
    "author": "Wen-Jing Hsu", 
    "publish": "2004-10-29T02:56:21Z", 
    "summary": "We propose a simple distributed hash table called ReCord, which is a\ngeneralized version of Randomized-Chord and offers improved tradeoffs in\nperformance and topology maintenance over existing P2P systems. ReCord is\nscalable and can be easily implemented as an overlay network, and offers a good\ntradeoff between the node degree and query latency. For instance, an $n$-node\nReCord with $O(\\log n)$ node degree has an expected latency of $\\Theta(\\log n)$\nhops. Alternatively, it can also offer $\\Theta(\\frac{\\log n}{\\log \\log n})$\nhops latency at a higher cost of $O(\\frac{\\log^2 n}{\\log\n  \\log n})$ node degree. Meanwhile, simulations of the dynamic behaviors of\nReCord are studied."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0411045v1", 
    "title": "Usage Policy-based CPU Sharing in VOs", 
    "arxiv-id": "cs/0411045v1", 
    "author": "Ian Foster", 
    "publish": "2004-11-12T22:48:00Z", 
    "summary": "Resource sharing within Grid collaborations usually implies specific sharing\nmechanisms at participating sites. Challenging policy issues can arise within\nvirtual organizations (VOs) that integrate participants and resources spanning\nmultiple physical institutions. Resource owners may wish to grant to one or\nmore VOs the right to use certain resources subject to local policy and service\nlevel agreements, and each VO may then wish to use those resources subject to\nVO policy. Thus, we must address the question of what usage policies (UPs)\nshould be considered for resource sharing in VOs. As a first step in addressing\nthis question, we develop and evaluate different UP scenarios within a\nspecialized context that mimics scientific Grids within which the resources to\nbe shared are computers. We also present a UP architecture and define roles and\nfunctions for scheduling resources in such grid environments while satisfying\nresource owner policies."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0411046v2", 
    "title": "Balanced Overlay Networks (BON): Decentralized Load Balancing via   Self-Organized Random Networks", 
    "arxiv-id": "cs/0411046v2", 
    "author": "Vwani P. Roychowdhury", 
    "publish": "2004-11-15T20:05:00Z", 
    "summary": "We present a novel framework, called balanced overlay networks (BON), that\nprovides scalable, decentralized load balancing for distributed computing using\nlarge-scale pools of heterogeneous computers. Fundamentally, BON encodes the\ninformation about each node's available computational resources in the\nstructure of the links connecting the nodes in the network. This distributed\nencoding is self-organized, with each node managing its in-degree and local\nconnectivity via random-walk sampling. Assignment of incoming jobs to nodes\nwith the most free resources is also accomplished by sampling the nodes via\nshort random walks. Extensive simulations show that the resulting highly\ndynamic and self-organized graph structure can efficiently balance\ncomputational load throughout large-scale networks. These simulations cover a\nwide spectrum of cases, including significant heterogeneity in available\ncomputing resources and high burstiness in incoming load. We provide analytical\nresults that prove BON's scalability for truly large-scale networks: in\nparticular we show that under certain ideal conditions, the network structure\nconverges to Erdos-Renyi (ER) random graphs; our simulation results, however,\nshow that the algorithm does much better, and the structures seem to approach\nthe ideal case of d-regular random graphs. We also make a connection between\nhighly-loaded BONs and the well-known ball-bin randomized load balancing\nframework."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412115v1", 
    "title": "Reductions in Distributed Computing Part I: Consensus and Atomic   Commitment Tasks", 
    "arxiv-id": "cs/0412115v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2004-12-29T19:50:21Z", 
    "summary": "We introduce several notions of reduction in distributed computing, and\ninvestigate reduction properties of two fundamental agreement tasks, namely\nConsensus and Atomic Commitment.\n  We first propose the notion of reduction \"a la Karp'', an analog for\ndistributed computing of the classical Karp reduction. We then define a weaker\nreduction which is the analog of Cook reduction. These two reductions are\ncalled K-reduction and C-reduction, respectively.\n  We also introduce the notion of C*-reduction which has no counterpart in\nclassical (namely, non distributed) systems, and which naturally arises when\ndealing with symmetric tasks.\n  We establish various reducibility and irreducibility theorems with respect to\nthese three reductions. Our main result is an incomparability statement for\nConsensus and Atomic Commitment tasks: we show that they are incomparable with\nrespect to the C-reduction, except when the resiliency degree is 1, in which\ncase Atomic Commitment is strictly harder than Consensus. A side consequence of\nthese results is that our notion of C-reduction is strictly weaker than the one\nof K-reduction, even for unsolvable tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412116v1", 
    "title": "Reductions in Distributed Computing Part II: k-Threshold Agreement Tasks", 
    "arxiv-id": "cs/0412116v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2004-12-29T20:51:58Z", 
    "summary": "We extend the results of Part I by considering a new class of agreement\ntasks, the so-called k-Threshold Agreement tasks (previously introduced by\nCharron-Bost and Le Fessant). These tasks naturally interpolate between Atomic\nCommitment and Consensus. Moreover, they constitute a valuable tool to derive\nirreducibility results between Consensus tasks only. In particular, they allow\nus to show that (A) for a fixed set of processes, the higher the resiliency\ndegree is, the harder the Consensus task is, and (B) for a fixed resiliency\ndegree, the smaller the set of processes is, the harder the Consensus task is.\n  The proofs of these results lead us to consider new oracle-based reductions,\ninvolving a weaker variant of the C-reduction introduced in Part I. We also\ndiscuss the relationship between our results and previous ones relating\nf-resiliency and wait-freedom."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0412121v1", 
    "title": "A Distributed Economics-based Infrastructure for Utility Computing", 
    "arxiv-id": "cs/0412121v1", 
    "author": "William Yurcik", 
    "publish": "2004-12-31T19:16:38Z", 
    "summary": "Existing attempts at utility computing revolve around two approaches. The\nfirst consists of proprietary solutions involving renting time on dedicated\nutility computing machines. The second requires the use of heavy, monolithic\napplications that are difficult to deploy, maintain, and use.\n  We propose a distributed, community-oriented approach to utility computing.\nOur approach provides an infrastructure built on Web Services in which modular\ncomponents are combined to create a seemingly simple, yet powerful system. The\ncommunity-oriented nature generates an economic environment which results in\nfair transactions between consumers and providers of computing cycles while\nsimultaneously encouraging improvements in the infrastructure of the\ncomputational grid itself."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0501002v1", 
    "title": "A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel   Systems", 
    "arxiv-id": "cs/0501002v1", 
    "author": "Michael Treaster", 
    "publish": "2005-01-01T03:32:51Z", 
    "summary": "Supercomputing systems today often come in the form of large numbers of\ncommodity systems linked together into a computing cluster. These systems, like\nany distributed system, can have large numbers of independent hardware\ncomponents cooperating or collaborating on a computation. Unfortunately, any of\nthis vast number of components can fail at any time, resulting in potentially\nerroneous output. In order to improve the robustness of supercomputing\napplications in the presence of failures, many techniques have been developed\nto provide resilience to these kinds of system faults. This survey provides an\noverview of these various fault-tolerance techniques."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502018v1", 
    "title": "When Database Systems Meet the Grid", 
    "arxiv-id": "cs/0502018v1", 
    "author": "James Annis", 
    "publish": "2005-02-03T21:43:50Z", 
    "summary": "We illustrate the benefits of combining database systems and Grid\ntechnologies for data-intensive applications. Using a cluster of SQL servers,\nwe reimplemented an existing Grid application that finds galaxy clusters in a\nlarge astronomical database. The SQL implementation runs an order of magnitude\nfaster than the earlier Tcl-C-file-based implementation. We discuss why and how\nGrid applications can take advantage of database systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502051v1", 
    "title": "A Semantic Grid-based E-Learning Framework (SELF)", 
    "arxiv-id": "cs/0502051v1", 
    "author": "Farooq Ahmad", 
    "publish": "2005-02-09T17:22:21Z", 
    "summary": "E-learning can be loosely defined as a wide set of applications and\nprocesses, which uses available electronic media (and tools) to deliver\nvocational education and training. With its increasing recognition as an\nubiquitous mode of instruction and interaction in the academic as well as\ncorporate world, the need for a scaleable and realistic model is becoming\nimportant. In this paper we introduce SELF; a Semantic grid-based E-Learning\nFramework. SELF aims to identify the key-enablers in a practical grid-based\nE-learning environment and to minimize technological reworking by proposing a\nwell-defined interaction plan among currently available tools and technologies.\nWe define a dichotomy with E-learning specific application layers on top and\nsemantic grid-based support layers underneath. We also map the latest open and\nfreeware technologies with various components in SELF."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502069v1", 
    "title": "Koordinatenfreies Lokationsbewusstsein (Localization without   Coordinates)", 
    "arxiv-id": "cs/0502069v1", 
    "author": "Dennis Pfisterer", 
    "publish": "2005-02-15T16:35:13Z", 
    "summary": "Localization is one of the fundamental issues in sensor networks. It is\nalmost always assumed that it must be solved by assigning coordinates to the\nnodes. This article discusses positioning algorithms from a theoretical,\npractical and simulative point of view, and identifies difficulties and\nlimitations. Ideas for more abstract means of location awareness are presented\nand the resulting possible improvements for applications are shown. Nodes with\ncertain topological or environmental properties are clustered, and the\nneighborhood structure of the clusters is modeled as a graph. Eines der\nfundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer\ndie Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon\nausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In\ndiesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein\nkritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen\naufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine\nabstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird\ngezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf\neiner graphenbasierten Modellierung des Netzes: Knoten mit bestimmten\ntopologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und\nClusternachbarschaften dann als Graphen modelliert."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0502093v1", 
    "title": "Online Permutation Routing in Partitioned Optical Passive Star Networks", 
    "arxiv-id": "cs/0502093v1", 
    "author": "Romeo Rizzi", 
    "publish": "2005-02-26T00:07:43Z", 
    "summary": "This paper establishes the state of the art in both deterministic and\nrandomized online permutation routing in the POPS network. Indeed, we show that\nany permutation can be routed online on a POPS network either with\n$O(\\frac{d}{g}\\log g)$ deterministic slots, or, with high probability, with\n$5c\\lceil d/g\\rceil+o(d/g)+O(\\log\\log g)$ randomized slots, where constant\n$c=\\exp (1+e^{-1})\\approx 3.927$. When $d=\\Theta(g)$, that we claim to be the\n\"interesting\" case, the randomized algorithm is exponentially faster than any\nother algorithm in the literature, both deterministic and randomized ones. This\nis true in practice as well. Indeed, experiments show that it outperforms its\nrivals even starting from as small a network as a POPS(2,2), and the gap grows\nexponentially with the size of the network. We can also show that, under proper\nhypothesis, no deterministic algorithm can asymptotically match its\nperformance."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503013v1", 
    "title": "Pr\u00e9diction de Performances pour les Communications Collectives", 
    "arxiv-id": "cs/0503013v1", 
    "author": "Gr\u00e9gory Mouni\u00e9", 
    "publish": "2005-03-04T17:09:23Z", 
    "summary": "Des travaux r\\'{e}cents visent l'optimisation des op\\'{e}rations de\ncommunication collective dans les environnements de type grille de calcul. La\nsolution la plus r\\'{e}pandue est la s\\'{e}paration des communications internes\net externes \\`{a} chaque grappe, mais cela n'exclut pas le d\\'{e}coupage des\ncommunications en plusieurs couches, pratique efficace d\\'{e}montr\\'{e}e par\nKaronis et al. [10]. Dans les deux cas, la pr\\'{e}diction des performances est\nun facteur essentiel, soit pour le r\\'{e}glage fin des param\\`{e}tres de\ncommunication, soit pour le calcul de la distribution et de la hi\\'{e}rarchie\ndes communications. Pour cela, il est tr\\`{e}s important d'avoir des\nmod\\`{e}les pr\\'{e}cis des communications collectives, lesquels seront\nutilis\\'{e}s pour pr\\'{e}dire ces performances. Cet article d\\'{e}crit notre\nexp\\'{e}rience sur la mod\\'{e}lisation des op\\'{e}rations de communication\ncollective. Nous pr\\'{e}sentons des mod\\`{e}les de communication pour\ndiff\\'{e}rents patrons de communication collective comme un vers plusieurs, un\nvers plusieurs personnalis\\'{e} et plusieurs vers plusieurs. Pour \\'{e}valuer\nla pr\\'{e}cision des mod\\`{e}les, nous comparons les pr\\'{e}dictions obtenues\navec les r\\'{e}sultats des exp\\'{e}rimentations effectu\\'{e}es sur deux\nenvironnements r\\'{e}seaux diff\\'{e}rents, Fast Ethernet et Myrinet."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503025v2", 
    "title": "A Taxonomy of Workflow Management Systems for Grid Computing", 
    "arxiv-id": "cs/0503025v2", 
    "author": "Rajkumar Buyya", 
    "publish": "2005-03-11T01:13:07Z", 
    "summary": "With the advent of Grid and application technologies, scientists and\nengineers are building more and more complex applications to manage and process\nlarge data sets, and execute scientific experiments on distributed resources.\nSuch application scenarios require means for composing and executing complex\nworkflows. Therefore, many efforts have been made towards the development of\nworkflow management systems for Grid computing. In this paper, we propose a\ntaxonomy that characterizes and classifies various approaches for building and\nexecuting workflows on Grids. We also survey several representative Grid\nworkflow systems developed by various projects world-wide to demonstrate the\ncomprehensiveness of the taxonomy. The taxonomy not only highlights the design\nand engineering similarities and differences of state-of-the-art in Grid\nworkflow systems, but also identifies the areas that need further research."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0503045v1", 
    "title": "Contextual Constraint Modeling in Grid Application Workflows", 
    "arxiv-id": "cs/0503045v1", 
    "author": "Peter Love", 
    "publish": "2005-03-19T01:28:48Z", 
    "summary": "This paper introduces a new mechanism for specifying constraints in\ndistributed workflows. By introducing constraints in a contextual form, it is\nshown how different people and groups within collaborative communities can\ncooperatively constrain workflows. A comparison with existing state-of-the-art\nworkflow systems is made. These ideas are explored in practice with an\nillustrative example from High Energy Physics."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504033v1", 
    "title": "Resource Management Services for a Grid Analysis Environment", 
    "arxiv-id": "cs/0504033v1", 
    "author": "Muhammad Adeel Zafar", 
    "publish": "2005-04-10T11:59:25Z", 
    "summary": "Selecting optimal resources for submitting jobs on a computational Grid or\naccessing data from a data grid is one of the most important tasks of any Grid\nmiddleware. Most modern Grid software today satisfies this responsibility and\ngives a best-effort performance to solve this problem. Almost all decisions\nregarding scheduling and data access are made by the software automatically,\ngiving users little or no control over the entire process. To solve this\nproblem, a more interactive set of services and middleware is desired that\nprovides users more information about Grid weather, and gives them more control\nover the decision making process. This paper presents a set of services that\nhave been developed to provide more interactive resource management\ncapabilities within the Grid Analysis Environment (GAE) being developed\ncollaboratively by Caltech, NUST and several other institutes. These include a\nsteering service, a job monitoring service and an estimator service that have\nbeen designed and written using a common Grid-enabled Web Services framework\nnamed Clarens. The paper also presents a performance analysis of the developed\nservices to show that they have indeed resulted in a more interactive and\npowerful system for user-centric Grid-enabled physics analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504034v1", 
    "title": "Heterogeneous Relational Databases for a Grid-enabled Analysis   Environment", 
    "arxiv-id": "cs/0504034v1", 
    "author": "Ian Willers", 
    "publish": "2005-04-10T12:05:03Z", 
    "summary": "Grid based systems require a database access mechanism that can provide\nseamless homogeneous access to the requested data through a virtual data access\nsystem, i.e. a system which can take care of tracking the data that is stored\nin geographically distributed heterogeneous databases. This system should\nprovide an integrated view of the data that is stored in the different\nrepositories by using a virtual data access mechanism, i.e. a mechanism which\ncan hide the heterogeneity of the backend databases from the client\napplications. This paper focuses on accessing data stored in disparate\nrelational databases through a web service interface, and exploits the features\nof a Data Warehouse and Data Marts. We present a middleware that enables\napplications to access data stored in geographically distributed relational\ndatabases without being aware of their physical locations and underlying\nschema. A web service interface is provided to enable applications to access\nthis middleware in a language and platform independent way. A prototype\nimplementation was created based on Clarens [4], Unity [7] and POOL [8]. This\nability to access the data stored in the distributed relational databases\ntransparently is likely to be a very powerful one for Grid users, especially\nthe scientific community wishing to collate and analyze data distributed over\nthe Grid."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0504044v1", 
    "title": "JClarens: A Java Framework for Developing and Deploying Web Services for   Grid Computing", 
    "arxiv-id": "cs/0504044v1", 
    "author": "Jang Uk In", 
    "publish": "2005-04-11T21:45:07Z", 
    "summary": "High Energy Physics (HEP) and other scientific communities have adopted\nService Oriented Architectures (SOA) as part of a larger Grid computing effort.\nThis effort involves the integration of many legacy applications and\nprogramming libraries into a SOA framework. The Grid Analysis Environment (GAE)\nis such a service oriented architecture based on the Clarens Grid Services\nFramework and is being developed as part of the Compact Muon Solenoid (CMS)\nexperiment at the Large Hadron Collider (LHC) at European Laboratory for\nParticle Physics (CERN). Clarens provides a set of authorization, access\ncontrol, and discovery services, as well as XMLRPC and SOAP access to all\ndeployed services. Two implementations of the Clarens Web Services Framework\n(Python and Java) offer integration possibilities for a wide range of\nprogramming languages. This paper describes the Java implementation of the\nClarens Web Services Framework called JClarens. and several web services of\ninterest to the scientific and Grid community that have been deployed using\nJClarens."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506001v1", 
    "title": "SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters", 
    "arxiv-id": "cs/0506001v1", 
    "author": "Sean Keller", 
    "publish": "2005-05-31T21:05:03Z", 
    "summary": "Modern high-performance computing relies heavily on the use of commodity\nprocessors arranged together in clusters. These clusters consist of individual\nnodes (typically off-the-shelf single or dual processor machines) connected\ntogether with a high speed interconnect. Using cluster computation has many\nbenefits, but also carries the liability of being failure prone due to the\nsheer number of components involved. Many effective solutions have been\nproposed to aid failure recovery in clusters, their one significant downside\nbeing the failure models they support. Most of the work in the area has focused\non detecting and correcting fail-stop errors. We propose a system that will\nalso detect more general error models, such as Byzantine errors, thus allowing\nexisting failure recovery methods to handle them correctly."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506006v1", 
    "title": "A batch scheduler with high level components", 
    "arxiv-id": "cs/0506006v1", 
    "author": "Olivier Richard", 
    "publish": "2005-06-02T13:04:14Z", 
    "summary": "In this article we present the design choices and the evaluation of a batch\nscheduler for large clusters, named OAR. This batch scheduler is based upon an\noriginal design that emphasizes on low software complexity by using high level\ntools. The global architecture is built upon the scripting language Perl and\nthe relational database engine Mysql. The goal of the project OAR is to prove\nthat it is possible today to build a complex system for ressource management\nusing such tools without sacrificing efficiency and scalability. Currently, our\nsystem offers most of the important features implemented by other batch\nschedulers such as priority scheduling (by queues), reservations, backfilling\nand some global computing support. Despite the use of high level tools, our\nexperiments show that our system has performances close to other systems.\nFurthermore, OAR is currently exploited for the management of 700 nodes (a\nmetropolitan GRID) and has shown good efficiency and robustness."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.nima.2004.07.058", 
    "link": "http://arxiv.org/pdf/cs/0506097v1", 
    "title": "A Flexible Thread Scheduler for Hierarchical Multiprocessor Machines", 
    "arxiv-id": "cs/0506097v1", 
    "author": "Samuel Thibault", 
    "publish": "2005-06-27T14:32:50Z", 
    "summary": "With the current trend of multiprocessor machines towards more and more\nhierarchical architectures, exploiting the full computational power requires\ncareful distribution of execution threads and data so as to limit expensive\nremote memory accesses. Existing multi-threaded libraries provide only limited\nfacilities to let applications express distribution indications, so that\nprogrammers end up with explicitly distributing tasks according to the\nunderlying architecture, which is difficult and not portable. In this article,\nwe present: (1) a model for dynamically expressing the structure of the\ncomputation; (2) a scheduler interpreting this model so as to make judicious\nhierarchical distribution decisions; (3) an implementation within the Marcel\nuser-level thread library. We experimented our proposal on a scientific\napplication running on a ccNUMA Bull NovaScale with 16 Intel Itanium II\nprocessors; results show a 30% gain compared to a classical scheduler, and are\nsimilar to what a handmade scheduler achieves in a non-portable way."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0508052v1", 
    "title": "Energy Optimal Data Propagation in Wireless Sensor Networks", 
    "arxiv-id": "cs/0508052v1", 
    "author": "Jose Rolim", 
    "publish": "2005-08-10T15:54:13Z", 
    "summary": "We propose an algorithm which produces a randomized strategy reaching optimal\ndata propagation in wireless sensor networks (WSN).In [6] and [8], an energy\nbalanced solution is sought using an approximation algorithm. Our algorithm\nimproves by (a) when an energy-balanced solution does not exist, it still finds\nan optimal solution (whereas previous algorithms did not consider this case and\nprovide no useful solution) (b) instead of being an approximation algorithm, it\nfinds the exact solution in one pass. We also provide a rigorous proof of the\noptimality of our solution."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0510081v1", 
    "title": "Virtual Environments for multiphysics code validation on Computing Grids", 
    "arxiv-id": "cs/0510081v1", 
    "author": "Vittorio Selmin", 
    "publish": "2005-10-26T12:26:57Z", 
    "summary": "We advocate in this paper the use of grid-based infrastructures that are\ndesigned for seamless approaches to the numerical expert users, i.e., the\nmultiphysics applications designers. It relies on sophisticated computing\nenvironments based on computing grids, connecting heterogeneous computing\nresources: mainframes, PC-clusters and workstations running multiphysics codes\nand utility software, e.g., visualization tools. The approach is based on\nconcepts defined by the HEAVEN* consortium. HEAVEN is a European scientific\nconsortium including industrial partners from the aerospace, telecommunication\nand software industries, as well as academic research institutes. Currently,\nthe HEAVEN consortium works on a project that aims to create advanced services\nplatforms. It is intended to enable \"virtual private grids\" supporting various\nenvironments for users manipulating a suitable high-level interface. This will\nbecome the basis for future generalized services allowing the integration of\nvarious services without the need to deploy specific grid infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0511083v1", 
    "title": "Gradient Based Routing in Wireless Sensor Networks: a Mixed Strategy", 
    "arxiv-id": "cs/0511083v1", 
    "author": "Jose Rolim", 
    "publish": "2005-11-23T18:13:18Z", 
    "summary": "We show how recent theoretical advances for data-propagation in Wireless\nSensor Networks (WSNs) can be combined to improve gradient-based routing (GBR)\nin Wireless Sensor Networks. We propose a mixed-strategy of direct transmission\nand multi-hop propagation of data which improves the lifespan of WSNs by\nreaching better energy-load-balancing amongst sensor nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0512005v1", 
    "title": "On Ants, Bacteria and Dynamic Environments", 
    "arxiv-id": "cs/0512005v1", 
    "author": "Agostinho C. Rosa", 
    "publish": "2005-12-01T04:52:00Z", 
    "summary": "Wasps, bees, ants and termites all make effective use of their environment\nand resources by displaying collective swarm intelligence. Termite colonies -\nfor instance - build nests with a complexity far beyond the comprehension of\nthe individual termite, while ant colonies dynamically allocate labor to\nvarious vital tasks such as foraging or defense without any central\ndecision-making ability. Recent research suggests that microbial life can be\neven richer: highly social, intricately networked, and teeming with\ninteractions, as found in bacteria. What strikes from these observations is\nthat both ant colonies and bacteria have similar natural mechanisms based on\nStigmergy and Self-Organization in order to emerge coherent and sophisticated\npatterns of global behaviour. Keeping in mind the above characteristics we will\npresent a simple model to tackle the collective adaptation of a social swarm\nbased on real ant colony behaviors (SSA algorithm) for tracking extrema in\ndynamic environments and highly multimodal complex functions described in the\nwell-know De Jong test suite. Then, for the purpose of comparison, a recent\nmodel of artificial bacterial foraging (BFOA algorithm) based on similar\nstigmergic features is described and analyzed. Final results indicate that the\nSSA collective intelligence is able to cope and quickly adapt to unforeseen\nsituations even when over the same cooperative foraging period, the community\nis requested to deal with two different and contradictory purposes, while\noutperforming BFOA in adaptive speed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0601078v1", 
    "title": "Exploring high performance distributed file storage using LDPC codes", 
    "arxiv-id": "cs/0601078v1", 
    "author": "Nuno Santos", 
    "publish": "2006-01-17T13:30:47Z", 
    "summary": "We explore the feasibility of implementing a reliable, high performance,\ndistributed storage system on a commodity computing cluster. Files are\ndistributed across storage nodes using erasure coding with small Low-Density\nParity-Check (LDPC) codes which provide high reliability while keeping the\nstorage and performance overhead small. We present performance measurements\ndone on a prototype system comprising 50 nodes which are self organised using a\npeer-to-peer overlay."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0601111v1", 
    "title": "Localization in Wireless Sensor Grids", 
    "arxiv-id": "cs/0601111v1", 
    "author": "Ted Herman", 
    "publish": "2006-01-25T23:48:12Z", 
    "summary": "This work reports experiences on using radio ranging to position sensors in a\ngrid topology. The implementation is simple, efficient, and could be\npractically distributed. The paper describes an implementation and experimental\nresult based on RSSI distance estimation. Novel techniques such as fuzzy\nmembership functions and table lookup are used to obtain more accurate result\nand simplify the computation. An 86% accuracy is achieved in the experiment in\nspite of inaccurate RSSI distance estimates with errors up to 60%."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0602013v1", 
    "title": "An Optimal Distributed Edge-Biconnectivity Algorithm", 
    "arxiv-id": "cs/0602013v1", 
    "author": "David Pritchard", 
    "publish": "2006-02-05T20:47:23Z", 
    "summary": "We describe a synchronous distributed algorithm which identifies the\nedge-biconnected components of a connected network. It requires a leader, and\nuses messages of size O(log |V|). The main idea is to preorder a BFS spanning\ntree, and then to efficiently compute least common ancestors so as to mark\ncycle edges. This algorithm takes O(Diam) time and uses O(|E|) messages.\nFurthermore, we show that no correct singly-initiated edge-biconnectivity\nalgorithm can beat either bound on any graph by more than a constant factor. We\nalso describe a near-optimal local algorithm for edge-biconnectivity."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0602026v1", 
    "title": "Bulk Scheduling with DIANA Scheduler", 
    "arxiv-id": "cs/0602026v1", 
    "author": "Ian Willers", 
    "publish": "2006-02-07T16:47:16Z", 
    "summary": "Results from and progress on the development of a Data Intensive and Network\nAware (DIANA) Scheduling engine, primarily for data intensive sciences such as\nphysics analysis, are described. Scientific analysis tasks can involve\nthousands of computing, data handling, and network resources and the size of\nthe input and output files and the amount of overall storage space allocated to\na user necessarily can have significant bearing on the scheduling of data\nintensive applications. If the input or output files must be retrieved from a\nremote location, then the time required transferring the files must also be\ntaken into consideration when scheduling compute resources for the given\napplication. The central problem in this study is the coordinated management of\ncomputation and data at multiple locations and not simply data movement.\nHowever, this can be a very costly operation and efficient scheduling can be a\nchallenge if compute and data resources are mapped without network cost. We\nhave implemented an adaptive algorithm within the DIANA Scheduler which takes\ninto account data location and size, network performance and computation\ncapability to make efficient global scheduling decisions. DIANA is a\nperformance-aware as well as an economy-guided Meta Scheduler. It iteratively\nallocates each job to the site that is likely to produce the best performance\nas well as optimizing the global queue for any remaining pending jobs.\nTherefore it is equally suitable whether a single job is being submitted or\nbulk scheduling is being performed. Results suggest that considerable\nperformance improvements are to be gained by adopting the DIANA scheduling\napproach."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0603035v2", 
    "title": "Final Results from and Exploitation Plans for MammoGrid", 
    "arxiv-id": "cs/0603035v2", 
    "author": "Ruth Warren", 
    "publish": "2006-03-09T13:49:14Z", 
    "summary": "The MammoGrid project has delivered the first deployed instance of a\nhealthgrid for clinical mammography that spans national boundaries. During the\nlast year, the final MammoGrid prototype has undergone a series of rigorous\ntests undertaken by radiologists in the UK and Italy and this paper draws\nconclusions from those tests for the benefit of the Healthgrid community. In\naddition, lessons learned during the lifetime of the project are detailed and\nrecommendations drawn for future health applications using grids. Following the\ncompletion of the project, plans have been put in place for the\ncommercialisation of the MammoGrid system and this is also reported in this\narticle. Particular emphasis is placed on the issues surrounding the transition\nfrom collaborative research project to a marketable product. This paper\nconcludes by highlighting some of the potential areas of future development and\nresearch."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2006.10.007", 
    "link": "http://arxiv.org/pdf/cs/0603036v2", 
    "title": "Health-e-Child : An Integrated Biomedical Platform for Grid-Based   Paediatric Applications", 
    "arxiv-id": "cs/0603036v2", 
    "author": "ZHOU", 
    "publish": "2006-03-09T13:54:15Z", 
    "summary": "There is a compelling demand for the integration and exploitation of\nheterogeneous biomedical information for improved clinical practice, medical\nresearch, and personalised healthcare across the EU. The Health-e-Child project\naims at developing an integrated healthcare platform for European Paediatrics,\nproviding seamless integration of traditional and emerging sources of\nbiomedical information. The long-term goal of the project is to provide\nuninhibited access to universal biomedical knowledge repositories for\npersonalised and preventive healthcare, large-scale information-based\nbiomedical research and training, and informed policy making. The project focus\nwill be on individualised disease prevention, screening, early diagnosis,\ntherapy and follow-up of paediatric heart diseases, inflammatory diseases, and\nbrain tumours. The project will build a Grid-enabled European network of\nleading clinical centres that will share and annotate biomedical data, validate\nsystems clinically, and diffuse clinical excellence across Europe by setting up\nnew technologies, clinical workflows, and standards. This paper outlines the\ndesign approach being adopted in Health-e-Child to enable the delivery of an\nintegrated biomedical information platform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0603112v1", 
    "title": "A General Framework for Scalability and Performance Analysis of DHT   Routing Systems", 
    "arxiv-id": "cs/0603112v1", 
    "author": "Vwani P. Roychowdhury", 
    "publish": "2006-03-28T22:54:37Z", 
    "summary": "In recent years, many DHT-based P2P systems have been proposed, analyzed, and\ncertain deployments have reached a global scale with nearly one million nodes.\nOne is thus faced with the question of which particular DHT system to choose,\nand whether some are inherently more robust and scalable.\n  Toward developing such a comparative framework, we present the reachable\ncomponent method (RCM) for analyzing the performance of different DHT routing\nsystems subject to random failures. We apply RCM to five DHT systems and obtain\nanalytical expressions that characterize their routability as a continuous\nfunction of system size and node failure probability. An important consequence\nis that in the large-network limit, the routability of certain DHT systems go\nto zero for any non-zero probability of node failure. These DHT routing\nalgorithms are therefore unscalable, while some others, including Kademlia,\nwhich powers the popular eDonkey P2P system, are found to be scalable."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0604109v1", 
    "title": "CMS Software Distribution on the LCG and OSG Grids", 
    "arxiv-id": "cs/0604109v1", 
    "author": "for the CMS Collaboration", 
    "publish": "2006-04-27T16:14:32Z", 
    "summary": "The efficient exploitation of worldwide distributed storage and computing\nresources available in the grids require a robust, transparent and fast\ndeployment of experiment specific software. The approach followed by the CMS\nexperiment at CERN in order to enable Monte-Carlo simulations, data analysis\nand software development in an international collaboration is presented. The\ncurrent status and future improvement plans are described."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605053v1", 
    "title": "Gridscape II: A Customisable and Pluggable Grid Monitoring Portal and   its Integration with Google Maps", 
    "arxiv-id": "cs/0605053v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-12T09:41:28Z", 
    "summary": "Grid computing has emerged as an effective means of facilitating the sharing\nof distributed heterogeneous resources, enabling collaboration in large scale\nenvironments. However, the nature of Grid systems, coupled with the\noverabundance and fragmentation of information, makes it difficult to monitor\nresources, services, and computations in order to plan and make decisions. In\nthis paper we present Gridscape II, a customisable portal component that can be\nused on its own or plugged in to compliment existing Grid portals. Gridscape II\nmanages the gathering of information from arbitrary, heterogeneous and\ndistributed sources and presents them together seamlessly within a single\ninterface. It also leverages the Google Maps API in order to provide a highly\ninteractive user interface. Gridscape II is simple and easy to use, providing a\nsolution to those users who do not wish to invest heavily in developing their\nown monitoring portal from scratch, and also for those users who want something\nthat is easy to customise and extend for their specific needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605056v1", 
    "title": "Utility Computing and Global Grids", 
    "arxiv-id": "cs/0605056v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-12T20:36:45Z", 
    "summary": "This chapter focuses on the use of Grid technologies to achieve utility\ncomputing. An overview of how Grids can support utility computing is first\npresented through the architecture of Utility Grids. Then, utility-based\nresource allocation is described in detail at each level of the architecture.\nFinally, some industrial solutions for utility computing are discussed."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DSN.2006.4", 
    "link": "http://arxiv.org/pdf/cs/0605057v1", 
    "title": "SLA-Based Coordinated Superscheduling Scheme and Performance for   Computational Grids", 
    "arxiv-id": "cs/0605057v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-15T13:41:27Z", 
    "summary": "The Service Level Agreement~(SLA) based grid superscheduling approach\npromotes coordinated resource sharing. Superscheduling is facilitated between\nadministratively and topologically distributed grid sites by grid schedulers\nsuch as Resource brokers. In this work, we present a market-based SLA\ncoordination mechanism. We based our SLA model on a well known \\emph{contract\nnet protocol}.\n  The key advantages of our approach are that it allows:~(i) resource owners to\nhave finer degree of control over the resource allocation that was previously\nnot possible through traditional mechanism; and (ii) superschedulers to bid for\nSLA contracts in the contract net with focus on completing the job within the\nuser specified deadline. In this work, we use simulation to show the\neffectiveness of our proposed approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0605060v1", 
    "title": "A Case for Cooperative and Incentive-Based Coupling of Distributed   Clusters", 
    "arxiv-id": "cs/0605060v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2006-05-15T10:21:22Z", 
    "summary": "Research interest in Grid computing has grown significantly over the past\nfive years. Management of distributed resources is one of the key issues in\nGrid computing. Central to management of resources is the effectiveness of\nresource allocation as it determines the overall utility of the system. The\ncurrent approaches to superscheduling in a grid environment are non-coordinated\nsince application level schedulers or brokers make scheduling decisions\nindependently of the others in the system. Clearly, this can exacerbate the\nload sharing and utilization problems of distributed resources due to\nsuboptimal schedules that are likely to occur. To overcome these limitations,\nwe propose a mechanism for coordinated sharing of distributed clusters based on\ncomputational economy. The resulting environment, called\n\\emph{Grid-Federation}, allows the transparent use of resources from the\nfederation when local resources are insufficient to meet its users'\nrequirements. The use of computational economy methodology in coordinating\nresource allocation not only facilitates the QoS based scheduling, but also\nenhances utility delivered by resources."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0605141v1", 
    "title": "General Compact Labeling Schemes for Dynamic Trees", 
    "arxiv-id": "cs/0605141v1", 
    "author": "Amos Korman", 
    "publish": "2006-05-30T13:54:26Z", 
    "summary": "Let $F$ be a function on pairs of vertices. An {\\em $F$- labeling scheme} is\ncomposed of a {\\em marker} algorithm for labeling the vertices of a graph with\nshort labels, coupled with a {\\em decoder} algorithm allowing one to compute\n$F(u,v)$ of any two vertices $u$ and $v$ directly from their labels. As\napplications for labeling schemes concern mainly large and dynamically changing\nnetworks, it is of interest to study {\\em distributed dynamic} labeling\nschemes. This paper investigates labeling schemes for dynamic trees.\n  This paper presents a general method for constructing labeling schemes for\ndynamic trees. Our method is based on extending an existing {\\em static} tree\nlabeling scheme to the dynamic setting. This approach fits many natural\nfunctions on trees, such as ancestry relation, routing (in both the adversary\nand the designer port models), nearest common ancestor etc.. Our resulting\ndynamic schemes incur overheads (over the static scheme) on the label size and\non the communication complexity. Informally, for any function $k(n)$ and any\nstatic $F$-labeling scheme on trees, we present an $F$-labeling scheme on\ndynamic trees incurring multiplicative overhead factors\n  (over the static scheme) of $O(\\log_{k(n)} n)$ on the label size and\n$O(k(n)\\log_{k(n)} n)$ on the amortized message complexity. In particular, by\nsetting $k(n)=n^{\\epsilon}$ for any $0<\\epsilon<1$, we obtain dynamic labeling\nschemes with asymptotically optimal label sizes and sublinear amortized message\ncomplexity for all the above mentioned functions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606028v1", 
    "title": "Affine Transformations of Loop Nests for Parallel Execution and   Distribution of Data over Processors", 
    "arxiv-id": "cs/0606028v1", 
    "author": "N. A. Likhoded", 
    "publish": "2006-06-07T12:59:49Z", 
    "summary": "The paper is devoted to the problem of mapping affine loop nests onto\ndistributed memory parallel computers. A method to find affine transformations\nof loop nests for parallel execution and distribution of data over processors\nis presented. The method tends to minimize the number of communications between\nprocessors and to improve locality of data within one processor. A problem of\ndetermination of data exchange sequence is investigated. Conditions to\ndetermine the ability to arrange broadcast is presented."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606047v1", 
    "title": "Asynchronous iterative computations with Web information retrieval   structures: The PageRank case", 
    "arxiv-id": "cs/0606047v1", 
    "author": "Daniel B. Szyld", 
    "publish": "2006-06-11T09:36:26Z", 
    "summary": "There are several ideas being used today for Web information retrieval, and\nspecifically in Web search engines. The PageRank algorithm is one of those that\nintroduce a content-neutral ranking function over Web pages. This ranking is\napplied to the set of pages returned by the Google search engine in response to\nposting a search query. PageRank is based in part on two simple common sense\nconcepts: (i)A page is important if many important pages include links to it.\n(ii)A page containing many links has reduced impact on the importance of the\npages it links to. In this paper we focus on asynchronous iterative schemes to\ncompute PageRank over large sets of Web pages. The elimination of the\nsynchronizing phases is expected to be advantageous on heterogeneous platforms.\nThe motivation for a possible move to such large scale distributed platforms\nlies in the size of matrices representing Web structure. In orders of\nmagnitude: $10^{10}$ pages with $10^{11}$ nonzero elements and $10^{12}$ bytes\njust to store a small percentage of the Web (the already crawled); distributed\nmemory machines are necessary for such computations. The present research is\npart of our general objective, to explore the potential of asynchronous\ncomputational models as an underlying framework for very large scale\ncomputations over the Grid. The area of ``internet algorithmics'' appears to\noffer many occasions for computations of unprecedent dimensionality that would\nbe good candidates for this framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLUSTR.2005.347038", 
    "link": "http://arxiv.org/pdf/cs/0606095v1", 
    "title": "A verification algorithm for Declarative Concurrent Programming", 
    "arxiv-id": "cs/0606095v1", 
    "author": "Jean Krivine", 
    "publish": "2006-06-22T13:23:15Z", 
    "summary": "A verification method for distributed systems based on decoupling forward and\nbackward behaviour is proposed. This method uses an event structure based\nalgorithm that, given a CCS process, constructs its causal compression relative\nto a choice of observable actions. Verifying the original process equipped with\ndistributed backtracking on non-observable actions, is equivalent to verifying\nits relative compression which in general is much smaller. We call this method\nDeclarative Concurrent Programming (DCP). DCP technique compares well with\ndirect bisimulation based methods. Benchmarks for the classic dining\nphilosophers problem show that causal compression is rather efficient both\ntime- and space-wise. State of the art verification tools can successfully\nhandle more than 15 agents, whereas they can handle no more than 5 following\nthe traditional direct method; an altogether spectacular improvement, since in\nthis example the specification size is exponential in the number of agents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GCC.2006.49", 
    "link": "http://arxiv.org/pdf/cs/0608046v1", 
    "title": "From Grid Middleware to a Grid Operating System", 
    "arxiv-id": "cs/0608046v1", 
    "author": "Athar Mohsin", 
    "publish": "2006-08-08T17:41:26Z", 
    "summary": "Grid computing has made substantial advances during the last decade. Grid\nmiddleware such as Globus has contributed greatly in making this possible.\nThere are, however, significant barriers to the adoption of Grid computing in\nother fields, most notably day-to-day user computing environments. We will\ndemonstrate in this paper that this is primarily due to the limitations of the\nexisting Grid middleware which does not take into account the needs of everyday\nscientific and business users. In this paper we will formally advocate a Grid\nOperating System and propose an architecture to migrate Grid computing into a\nGrid operating system which we believe would help remove most of the technical\nbarriers to the adoption of Grid computing and make it relevant to the\nday-to-day user. We believe this proposed transition to a Grid operating system\nwill drive more pervasive Grid computing research and application development\nand deployment in future."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CBMS.2006.109", 
    "link": "http://arxiv.org/pdf/cs/0608047v1", 
    "title": "Lessons Learned from MammoGrid for Integrated Biomedical Solutions", 
    "arxiv-id": "cs/0608047v1", 
    "author": "A. E. Solomonides", 
    "publish": "2006-08-08T17:49:28Z", 
    "summary": "This paper presents an overview of the MammoGrid project and some of its\nachievements. In terms of the global grid project, and European research in\nparticular, the project has successfully demonstrated the capacity of a\ngrid-based system to support effective collaboration between physicians,\nincluding handling and querying image databases, as well as using grid\nservices, such as image standardization and Computer-Aided Detection (CADe) of\nsuspect or indicative features. In terms of scientific results, in radiology,\nthere have been significant epidemiological findings in the assessment of\nbreast density as a risk factor, but the results for CADe are less clear-cut.\nFinally, the foundations of a technology transfer process to establish a\nworking MammoGrid plus system in Spain through the company Maat GKnowledge and\nthe collaboration of CIEMAT and hospitals in Extremadura."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608048v1", 
    "title": "Bulk Scheduling with the DIANA Scheduler", 
    "arxiv-id": "cs/0608048v1", 
    "author": "Ian Willers", 
    "publish": "2006-08-08T17:53:15Z", 
    "summary": "Results from the research and development of a Data Intensive and Network\nAware (DIANA) scheduling engine, to be used primarily for data intensive\nsciences such as physics analysis, are described. In Grid analyses, tasks can\ninvolve thousands of computing, data handling, and network resources. The\ncentral problem in the scheduling of these resources is the coordinated\nmanagement of computation and data at multiple locations and not just data\nreplication or movement. However, this can prove to be a rather costly\noperation and efficient sing can be a challenge if compute and data resources\nare mapped without considering network costs. We have implemented an adaptive\nalgorithm within the so-called DIANA Scheduler which takes into account data\nlocation and size, network performance and computation capability in order to\nenable efficient global scheduling. DIANA is a performance-aware and\neconomy-guided Meta Scheduler. It iteratively allocates each job to the site\nthat is most likely to produce the best performance as well as optimizing the\nglobal queue for any remaining jobs. Therefore it is equally suitable whether a\nsingle job is being submitted or bulk scheduling is being performed. Results\nindicate that considerable performance improvements can be gained by adopting\nthe DIANA scheduling approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608092v2", 
    "title": "Self-Stabilizing Byzantine Pulse Synchronization", 
    "arxiv-id": "cs/0608092v2", 
    "author": "Danny Dolev", 
    "publish": "2006-08-24T16:07:07Z", 
    "summary": "The ``Pulse Synchronization'' problem can be loosely described as targeting\nto invoke a recurring distributed event as simultaneously as possible at the\ndifferent nodes and with a frequency that is as regular as possible. This\ntarget becomes surprisingly subtle and difficult to achieve when facing both\ntransient and permanent failures. In this paper we present an algorithm for\npulse synchronization that self-stabilizes while at the same time tolerating a\npermanent presence of Byzantine faults. The Byzantine nodes might incessantly\ntry to de-synchronize the correct nodes. Transient failures might throw the\nsystem into an arbitrary state in which correct nodes have no common notion\nwhat-so-ever, such as time or round numbers, and can thus not infer anything\nfrom their own local states upon the state of other correct nodes. The\npresented algorithm grants nodes the ability to infer that eventually all\ncorrect nodes will invoke their pulses within a very short time interval of\neach other and will do so regularly.\n  Pulse synchronization has previously been shown to be a powerful tool for\ndesigning general self-stabilizing Byzantine algorithms and is hitherto the\nonly method that provides for the general design of efficient practical\nprotocols in the confluence of these two fault models. The difficulty, in\ngeneral, to design any algorithm in this fault model may be indicated by the\nremarkably few algorithms resilient to both fault models. The few published\nself-stabilizing Byzantine algorithms are typically complicated and sometimes\nconverge from an arbitrary initial state only after exponential or super\nexponential time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608096v1", 
    "title": "Linear-time Self-stabilizing Byzantine Clock Synchronization", 
    "arxiv-id": "cs/0608096v1", 
    "author": "Hanna Parnas", 
    "publish": "2006-08-25T03:11:28Z", 
    "summary": "Clock synchronization is a very fundamental task in distributed system. It\nthus makes sense to require an underlying clock synchronization mechanism to be\nhighly fault-tolerant. A self-stabilizing algorithm seeks to attain\nsynchronization once lost; a Byzantine algorithm assumes synchronization is\nnever lost and focuses on containing the influence of the permanent presence of\nfaulty nodes. There are efficient self-stabilizing solutions for clock\nsynchronization as well as efficient solutions that are resilient to Byzantine\nfaults. In contrast, to the best of our knowledge there is no practical\nsolution that is self-stabilizing while tolerating the permanent presence of\nByzantine nodes. We present the first linear-time self-stabilizing Byzantine\nclock synchronization algorithm. Our deterministic clock synchronization\nalgorithm is based on the observation that all clock synchronization algorithms\nrequire events for exchanging clock values and re-synchronizing the clocks to\nwithin safe bounds. These events usually need to happen synchronously at the\ndifferent nodes. In classic Byzantine algorithms this is fulfilled or aided by\nhaving the clocks initially close to each other and thus the actual clock\nvalues can be used for synchronizing the events. This implies that clock values\ncannot differ arbitrarily, which necessarily renders these solutions to be\nnon-stabilizing. Our scheme suggests using an underlying distributed pulse\nsynchronization module that is uncorrelated to the clock values."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608112v1", 
    "title": "Entity Based Peer-to-Peer in a Data Grid Environment", 
    "arxiv-id": "cs/0608112v1", 
    "author": "M-T. Kechadi", 
    "publish": "2006-08-29T11:58:50Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, the grid characterisation,\nprogramming environments, etc. In this paper we present a new Grid system\ndedicated to deal with data issues, called DGET (Data Grid Environment and\nTools). DGET is characterized by its peer-to-peer communication system and\nentity-based architecture, therefore, taking advantage of the main\nfunctionality of both systems; P2P and Grid. DGET is currently under\ndevelopment and a prototype implementing the main components is in its first\nphase of testing. In this paper we limit our description to the system\narchitectural features and to the main differences with other systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608113v1", 
    "title": "A Java Based Architecture of P2P-Grid Middleware", 
    "arxiv-id": "cs/0608113v1", 
    "author": "T. Kechadi", 
    "publish": "2006-08-29T13:11:05Z", 
    "summary": "During the last decade there has been a huge interest in Grid technologies,\nand numerous Grid projects have been initiated with various visions of the\nGrid. While all these visions have the same goal of resource sharing, they\ndiffer in the functionality that a Grid supports, characterization, programming\nenvironments, etc. We present a new Grid system dedicated to dealing with data\nissues, called DGET (Data Grid Environment and Tools). DGET is characterized by\nits peerto- peer communication system and entity-based architecture, therefore,\ntaking advantage of the main functionality of both systems; P2P and Grid. DGET\nis currently under development and a prototype implementing the main components\nis in its first phase of testing. In this paper we limit our description to the\nsystem architectural features and to the main differences with other systems.\nKeywords: Grid Computing, Peer to Peer, Peer to Peer Grid"
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608114v1", 
    "title": "Reliable multicast fault tolerant MPI in the Grid environment", 
    "arxiv-id": "cs/0608114v1", 
    "author": "Serge Petiton", 
    "publish": "2006-08-29T13:14:29Z", 
    "summary": "Grid environments have recently been developed with low stretch and overheads\nthat increase with the logarithm of the number of nodes in the system. Getting\nand sending data to/from a large numbers of nodes is gaining importance due to\nan increasing number of independent data providers and the heterogeneity of the\nnetwork/Grid. One of the key challenges is to achieve a balance between low\nbandwidth consumption and good reliability. In this paper we present an\nimplementation of a reliable multicast protocol over a fault tolerant MPI:\nMPICHV2. It can provide one way to solve the problem of transferring large\nchunks of data between applications running on a grid with limited network\nlinks. We first show that we can achieve similar performance as the MPICH-P4\nimplementation by using multicast with data compression in a cluster. Next, we\nprovide a theoretical cluster organization and GRID network architecture to\nharness the performance provided by using multicast. Finally, we present the\nconclusion and future work."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608116v1", 
    "title": "Transparent Migration of Multi-Threaded Applications on a Java Based   Grid", 
    "arxiv-id": "cs/0608116v1", 
    "author": "T. Kechadi", 
    "publish": "2006-08-29T13:29:29Z", 
    "summary": "Grid computing has enabled pooling a very large number of heterogeneous\nresource administered by different security domains. Applications are\ndynamically deployed on the resources available at the time. Dynamic nature of\nthe resources and applications requirements makes needs the grid middleware to\nsupport the ability of migrating a running application to a different resource.\nEspecially, Grid applications are typically long running and thus stoping them\nand starting them from scratch is not a feasible option. This paper presents an\noverview of migration support in a java based grid middleware called DGET.\nMigration support in DGET includes multi-threaded migration and asynchronous\nmigration as well."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0608118v1", 
    "title": "TreeP: A Tree-Based P2P Network Architecture", 
    "arxiv-id": "cs/0608118v1", 
    "author": "A. Ottewill", 
    "publish": "2006-08-29T15:08:42Z", 
    "summary": "In this paper we proposed a hierarchical P2P network based on a dynamic\npartitioning on a 1-D space. This hierarchy is created and maintained\ndynamically and provides a gridmiddleware (like DGET) a P2P basic functionality\nfor resource discovery and load-balancing.This network architecture is called\nTreeP (Tree based P2P network architecture) and is based on atessellation of a\n1-D space. We show that this topology exploits in an efficient way\ntheheterogeneity feature of the network while limiting the overhead introduced\nby the overlaymaintenance. Experimental results show that this topology is\nhighly resilient to a large number ofnetwork failures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0609027v1", 
    "title": "A Case for Peering of Content Delivery Networks", 
    "arxiv-id": "cs/0609027v1", 
    "author": "Zahir Tari", 
    "publish": "2006-09-06T23:41:32Z", 
    "summary": "The proliferation of Content Delivery Networks (CDN) reveals that existing\ncontent networks are owned and operated by individual companies. As a\nconsequence, closed delivery networks are evolved which do not cooperate with\nother CDNs and in practice, islands of CDNs are formed. Moreover, the logical\nseparation between contents and services in this context results in two content\nnetworking domains. But present trends in content networks and content\nnetworking capabilities give rise to the interest in interconnecting content\nnetworks. Finding ways for distinct content networks to coordinate and\ncooperate with other content networks is necessary for better overall service.\nIn addition to that, meeting the QoS requirements of users according to the\nnegotiated Service Level Agreements between the user and the content network is\na burning issue in this perspective. In this article, we present an open,\nscalable and Service-Oriented Architecture based system to assist the creation\nof open Content and Service Delivery Networks (CSDN) that scale and support\nsharing of resources with other CSDNs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0609163v1", 
    "title": "Labeling Schemes with Queries", 
    "arxiv-id": "cs/0609163v1", 
    "author": "Shay Kutten", 
    "publish": "2006-09-29T12:31:35Z", 
    "summary": "We study the question of ``how robust are the known lower bounds of labeling\nschemes when one increases the number of consulted labels''. Let $f$ be a\nfunction on pairs of vertices. An $f$-labeling scheme for a family of graphs\n$\\cF$ labels the vertices of all graphs in $\\cF$ such that for every graph\n$G\\in\\cF$ and every two vertices $u,v\\in G$, the value $f(u,v)$ can be inferred\nby merely inspecting the labels of $u$ and $v$.\n  This paper introduces a natural generalization: the notion of $f$-labeling\nschemes with queries, in which the value $f(u,v)$ can be inferred by inspecting\nnot only the labels of $u$ and $v$ but possibly the labels of some additional\nvertices. We show that inspecting the label of a single additional vertex (one\n{\\em query}) enables us to reduce the label size of many labeling schemes\nsignificantly."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0610131v1", 
    "title": "Scheduling and data redistribution strategies on star platforms", 
    "arxiv-id": "cs/0610131v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2006-10-23T07:45:01Z", 
    "summary": "In this work we are interested in the problem of scheduling and\nredistributing data on master-slave platforms. We consider the case were the\nworkers possess initial loads, some of which having to be redistributed in\norder to balance their completion times. We examine two different scenarios.\nThe first model assumes that the data consists of independent and identical\ntasks. We prove the NP-completeness in the strong sense for the general case,\nand we present two optimal algorithms for special platform types. Furthermore\nwe propose three heuristics for the general case. Simulations consolidate the\ntheoretical results. The second data model is based on Divisible Load Theory.\nThis problem can be solved in polynomial time by a combination of linear\nprogramming and simple analytical manipulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNS.2006.886047", 
    "link": "http://arxiv.org/pdf/cs/0611034v1", 
    "title": "Strategies for Replica Placement in Tree Networks", 
    "arxiv-id": "cs/0611034v1", 
    "author": "Veronika Rehn", 
    "publish": "2006-11-08T08:16:55Z", 
    "summary": "In this paper, we discuss and compare several policies to place replicas in\ntree networks, subject to server capacity and QoS constraints. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. The standard approach in the literature is to enforce that\nall requests of a client be served by the closest server in the tree. We\nintroduce and study two new policies. In the first policy, all requests from a\ngiven client are still processed by the same server, but this server can be\nlocated anywhere in the path from the client to the root. In the second policy,\nthe requests of a given client can be processed by multiple servers. One major\ncontribution of this paper is to assess the impact of these new policies on the\ntotal replication cost. Another important goal is to assess the impact of\nserver heterogeneity, both from a theoretical and a practical perspective. In\nthis paper, we establish several new complexity results, and provide several\nefficient polynomial heuristics for NP-complete instances of the problem. These\nheuristics are compared to an absolute lower bound provided by the formulation\nof the problem in terms of the solution of an integer linear program."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11768869_8", 
    "link": "http://arxiv.org/pdf/cs/0611139v1", 
    "title": "Static Safety for an Actor Dedicated Process Calculus by Abstract   Interpretation", 
    "arxiv-id": "cs/0611139v1", 
    "author": "Xavier Thirioux", 
    "publish": "2006-11-28T07:48:18Z", 
    "summary": "The actor model eases the definition of concurrent programs with non uniform\nbehaviors. Static analysis of such a model was previously done in a data-flow\noriented way, with type systems. This approach was based on constraint set\nresolution and was not able to deal with precise properties for communications\nof behaviors. We present here a new approach, control-flow oriented, based on\nthe abstract interpretation framework, able to deal with communication of\nbehaviors. Within our new analyses, we are able to verify most of the previous\nproperties we observed as well as new ones, principally based on occurrence\ncounting."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0611165v1", 
    "title": "Partially ordered distributed computations on asynchronous   point-to-point networks", 
    "arxiv-id": "cs/0611165v1", 
    "author": "Valmir C. Barbosa", 
    "publish": "2006-11-30T13:01:36Z", 
    "summary": "Asynchronous executions of a distributed algorithm differ from each other due\nto the nondeterminism in the order in which the messages exchanged are handled.\nIn many situations of interest, the asynchronous executions induced by\nrestricting nondeterminism are more efficient, in an application-specific\nsense, than the others. In this work, we define partially ordered executions of\na distributed algorithm as the executions satisfying some restricted orders of\ntheir actions in two different frameworks, those of the so-called event- and\npulse-driven computations. The aim of these restrictions is to characterize\nasynchronous executions that are likely to be more efficient for some important\nclasses of applications. Also, an asynchronous algorithm that ensures the\noccurrence of partially ordered executions is given for each case. Two of the\napplications that we believe may benefit from the restricted nondeterminism are\nbacktrack search, in the event-driven case, and iterative algorithms for\nsystems of linear equations, in the pulse-driven case."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612013v2", 
    "title": "Economy-based Content Replication for Peering Content Delivery Networks", 
    "arxiv-id": "cs/0612013v2", 
    "author": "Kris Bubendorfer", 
    "publish": "2006-12-04T06:29:16Z", 
    "summary": "Existing Content Delivery Networks (CDNs) exhibit the nature of closed\ndelivery networks which do not cooperate with other CDNs and in practice,\nislands of CDNs are formed. The logical separation between contents and\nservices in this context results in two content networking domains. In addition\nto that, meeting the Quality of Service requirements of users according to\nnegotiated Service Level Agreement is crucial for a CDN. Present trends in\ncontent networks and content networking capabilities give rise to the interest\nin interconnecting content networks. Hence, in this paper, we present an open,\nscalable, and Service-Oriented Architecture (SOA)-based system that assist the\ncreation of open Content and Service Delivery Networks (CSDNs), which scale and\nsupports sharing of resources through peering with other CSDNs. To encourage\nresource sharing and peering arrangements between different CDN providers at\nglobal level, we propose using market-based models by introducing an\neconomy-based strategy for content replication."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612025v1", 
    "title": "Registers", 
    "arxiv-id": "cs/0612025v1", 
    "author": "Paul M. B. Vitanyi", 
    "publish": "2006-12-05T16:51:25Z", 
    "summary": "Entry in: Encyclopedia of Algorithms, Ming-Yang Kao, Ed., Springer, To\nappear.\n  Synonyms: Wait-free registers, wait-free shared variables, asynchronous\ncommunication hardware. Problem Definition: Consider a system of asynchronous\nprocesses that communicate among themselves by only executing read and write\noperations on a set of shared variables (also known as shared registers). The\nsystem has no global clock or other synchronization primitives."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2008.09.011", 
    "link": "http://arxiv.org/pdf/cs/0612035v1", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "arxiv-id": "cs/0612035v1", 
    "author": "Michel Raynal", 
    "publish": "2006-12-06T13:57:54Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services that are\ncapable of dealing with resource assignment and management in a large-scale,\nheterogeneous and unreliable environment. One such service, the slicing\nservice, has been proposed to allow for an automatic partitioning of P2P\nnetworks into groups (slices) that represent a controllable amount of some\nresource and that are also relatively homogeneous with respect to that\nresource, in the face of churn and other failures. In this report we propose\ntwo algorithms to solve the distributed slicing problem. The first algorithm\nimproves upon an existing algorithm that is based on gossip-based sorting of a\nset of uniform random numbers. We speed up convergence via a heuristic for\ngossip peer selection. The second algorithm is based on a different approach:\nstatistical approximation of the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms relies on\ntheir gossip-based models. We present theoretical and experimental results to\nprove the viability of these algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0612125v2", 
    "title": "Heterogeneous Strong Computation Migration", 
    "arxiv-id": "cs/0612125v2", 
    "author": "Bruno Schulze", 
    "publish": "2006-12-22T18:27:25Z", 
    "summary": "The continuous increase in performance requirements, for both scientific\ncomputation and industry, motivates the need of a powerful computing\ninfrastructure. The Grid appeared as a solution for inexpensive execution of\nheavy applications in a parallel and distributed manner. It allows combining\nresources independently of their physical location and architecture to form a\nglobal resource pool available to all grid users. However, grid environments\nare highly unstable and unpredictable. Adaptability is a crucial issue in this\ncontext, in order to guarantee an appropriate quality of service to users.\nMigration is a technique frequently used for achieving adaptation. The\nobjective of this report is to survey the problem of strong migration in\nheterogeneous environments like the grids', the related implementation issues\nand the current solutions."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0701015v2", 
    "title": "Asynchronous Implementation of Failure Detectors with partial   connectivity and unknown participants", 
    "arxiv-id": "cs/0701015v2", 
    "author": "Fabiola Greve", 
    "publish": "2007-01-03T12:52:58Z", 
    "summary": "We consider the problem of failure detection in dynamic networks such as\nMANETs. Unreliable failure detectors are classical mechanisms which provide\ninformation about process failures. However, most of current implementations\nconsider that the network is fully connected and that the initial number of\nnodes of the system is known. This assumption is not applicable to dynamic\nenvironments. Furthermore, such implementations are usually timer-based while\nin dynamic networks there is no upper bound for communication delays since\nnodes can move. This paper presents an asynchronous implementation of a failure\ndetector for unknown and mobile networks. Our approach does not rely on timers\nand neither the composition nor the number of nodes in the system are known. We\nprove that our algorithm can implement failure detectors of class <>S when\nbehavioral properties and connectivity conditions are satisfied by the\nunderlying system."
},{
    "category": "cs.DC", 
    "doi": "10.1002/cpe.1287", 
    "link": "http://arxiv.org/pdf/cs/0701064v1", 
    "title": "Causing Communication Closure: Safe Program Composition with Reliable   Non-FIFO Channels", 
    "arxiv-id": "cs/0701064v1", 
    "author": "Yoram Moses", 
    "publish": "2007-01-09T12:18:14Z", 
    "summary": "A semantic framework for analyzing safe composition of distributed programs\nis presented. Its applicability is illustrated by a study of program\ncomposition when communication is reliable but not necessarily FIFO\\@. In this\nmodel, special care must be taken to ensure that messages do not accidentally\novertake one another in the composed program. We show that barriers do not\nexist in this model. Indeed, no program that sends or receives messages can\nautomatically be composed with arbitrary programs without jeopardizing their\nintended behavior. Safety of composition becomes context-sensitive and new\ntools are needed for ensuring it. A notion of \\emph{sealing} is defined, where\nif a program $P$ is immediately followed by a program $Q$ that seals $P$ then\n$P$ will be communication-closed--it will execute as if it runs in isolation.\nThe investigation of sealing in this model reveals a novel connection between\nLamport causality and safe composition. A characterization of sealable programs\nis given, as well as efficient algorithms for testing if $Q$ seals $P$ and for\nconstructing a seal for a significant class of programs. It is shown that every\nsealable program that is open to interference on $O(n^2)$ channels can be\nsealed using O(n) messages."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0701134v2", 
    "title": "Byzantine Fault Tolerance for Nondeterministic Applications", 
    "arxiv-id": "cs/0701134v2", 
    "author": "Wenbing Zhao", 
    "publish": "2007-01-21T20:44:52Z", 
    "summary": "All practical applications contain some degree of nondeterminism. When such\napplications are replicated to achieve Byzantine fault tolerance (BFT), their\nnondeterministic operations must be controlled to ensure replica consistency.\nTo the best of our knowledge, only the most simplistic types of replica\nnondeterminism have been dealt with. Furthermore, there lacks a systematic\napproach to handling common types of nondeterminism. In this paper, we propose\na classification of common types of replica nondeterminism with respect to the\nrequirement of achieving Byzantine fault tolerance, and describe the design and\nimplementation of the core mechanisms necessary to handle such nondeterminism\nwithin a Byzantine fault tolerance framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0701179v1", 
    "title": "Scatter of Weak Robots", 
    "arxiv-id": "cs/0701179v1", 
    "author": "Franck Petit", 
    "publish": "2007-01-27T06:43:04Z", 
    "summary": "In this paper, we first formalize the problem to be solved, i.e., the Scatter\nProblem (SP). We then show that SP cannot be deterministically solved. Next, we\npropose a randomized algorithm for this problem. The proposed solution is\ntrivially self-stabilizing. We then show how to design a self-stabilizing\nversion of any deterministic solution for the Pattern Formation and the\nGathering problems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702066v1", 
    "title": "Comments on \"Design and performance evaluation of load distribution   strategies for multiple loads on heterogeneous linear daisy chain networks''", 
    "arxiv-id": "cs/0702066v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-02-10T17:43:35Z", 
    "summary": "Min, Veeravalli, and Barlas proposed strategies to minimize the overall\nexecution time of one or several divisible loads on a heterogeneous linear\nnetwork, using one or more installments. We show on a very simple example that\nthe proposed approach does not always produce a solution and that, when it\ndoes, the solution is often suboptimal. We also show how to find an optimal\nscheduling for any instance, once the number of installments per load is given.\nFinally, we formally prove that under a linear cost model, as in the original\npaper, an optimal schedule has an infinite number of installments. Such a cost\nmodel can therefore not be sed to design practical multi-installment\nstrategies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702076v2", 
    "title": "A First Step Towards Automatically Building Network Representations", 
    "arxiv-id": "cs/0702076v2", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-02-13T16:35:04Z", 
    "summary": "To fully harness Grids, users or middlewares must have some knowledge on the\ntopology of the platform interconnection network. As such knowledge is usually\nnot available, one must uses tools which automatically build a topological\nnetwork model through some measurements. In this article, we define a\nmethodology to assess the quality of these network model building tools, and we\napply this methodology to representatives of the main classes of model builders\nand to two new algorithms. We show that none of the main existing techniques\nbuild models that enable to accurately predict the running time of simple\napplication kernels for actual platforms. However some of the new algorithms we\npropose give excellent results in a wide range of situations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0702114v1", 
    "title": "Nearest Neighbor Network Traversal", 
    "arxiv-id": "cs/0702114v1", 
    "author": "David Pritchard", 
    "publish": "2007-02-20T03:54:12Z", 
    "summary": "A mobile agent in a network wants to visit every node of an n-node network,\nusing a small number of steps. We investigate the performance of the following\n``nearest neighbor'' heuristic: always go to the nearest unvisited node. If the\nnetwork graph never changes, then from (Rosenkrantz, Stearns and Lewis, 1977)\nand (Hurkens and Woeginger, 2004) it follows that Theta(n log n) steps are\nnecessary and sufficient in the worst case. We give a simpler proof of the\nupper bound and an example that improves the best known lower bound.\n  We investigate how the performance of this heuristic changes when it is\ndistributively implemented in a network. Even if network edges are allow to\nfail over time, we show that the nearest neighbor strategy never runs for more\nthan O(n^2) iterations. We also show that any strategy can be forced to take at\nleast n(n-1)/2 steps before all nodes are visited, if the edges of the network\nare deleted in an adversarial way."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703094v1", 
    "title": "Geographic Routing Around Obstacles in Wireless Sensor Networks", 
    "arxiv-id": "cs/0703094v1", 
    "author": "Sotiris Nikolesteas", 
    "publish": "2007-03-20T12:17:57Z", 
    "summary": "Geographic routing is becoming the protocol of choice for many sensor network\napplications. The current state of the art is unsatisfactory: some algorithms\nare very efficient, however they require a preliminary planarization of the\ncommunication graph. Planarization induces overhead and is not realistic in\nmany scenarios. On the otherhand, georouting algorithms which do not rely on\nplanarization have fairly low success rates and either fail to route messages\naround all but the simplest obstacles or have a high topology control overhead\n(e.g. contour detection algorithms). To overcome these limitations, we propose\nGRIC, the first lightweight and efficient on demand (i.e. all-to-all)\ngeographic routing algorithm which does not require planarization and has\nalmost 100% delivery rates (when no obstacles are added). Furthermore, the\nexcellent behavior of our algorithm is maintained even in the presence of large\nconvex obstacles. The case of hard concave obstacles is also studied; such\nobstacles are hard instances for which performance diminishes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703112v1", 
    "title": "User-level DSM System for Modern High-Performance Interconnection   Networks", 
    "arxiv-id": "cs/0703112v1", 
    "author": "Srinidhi Varadarajan", 
    "publish": "2007-03-22T19:15:00Z", 
    "summary": "In this paper, we introduce a new user-level DSM system which has the ability\nto directly interact with underlying interconnection networks. The DSM system\nprovides the application programmer a flexible API to program parallel\napplications either using shared memory semantics over physically distributed\nmemory or to use an efficient remote memory demand paging technique. We also\nintroduce a new time slice based memory consistency protocol which is used by\nthe DSM system. We present preliminary results from our implementation on a\nsmall Opteron Linux cluster interconnected over Myrinet."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703117v1", 
    "title": "Self-adaptive Gossip Policies for Distributed Population-based   Algorithms", 
    "arxiv-id": "cs/0703117v1", 
    "author": "J. J. Merelo", 
    "publish": "2007-03-23T11:29:10Z", 
    "summary": "Gossipping has demonstrate to be an efficient mechanism for spreading\ninformation among P2P networks. Within the context of P2P computing, we propose\nthe so-called Evolvable Agent Model for distributed population-based algorithms\nwhich uses gossipping as communication policy, and represents every individual\nas a self-scheduled single thread. The model avoids obsolete nodes in the\npopulation by defining a self-adaptive refresh rate which depends on the\nlatency and bandwidth of the network. Such a mechanism balances the migration\nrate to the congestion of the links pursuing global population coherence. We\nperform an experimental evaluation of this model on a real parallel system and\nobserve how solution quality and algorithm speed scale with the number of\nprocessors with this seamless approach."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703122v1", 
    "title": "Rapid Almost-Complete Broadcasting in Faulty Networks", 
    "arxiv-id": "cs/0703122v1", 
    "author": "Richard Kr\u00e1lovi\u010d", 
    "publish": "2007-03-23T22:38:06Z", 
    "summary": "This paper studies the problem of broadcasting in synchronous point-to-point\nnetworks, where one initiator owns a piece of information that has to be\ntransmitted to all other vertices as fast as possible. The model of fractional\ndynamic faults with threshold is considered: in every step either a fixed\nnumber $T$, or a fraction $\\alpha$, of sent messages can be lost depending on\nwhich quantity is larger.\n  As the main result we show that in complete graphs and hypercubes it is\npossible to inform all but a constant number of vertices, exhibiting only a\nlogarithmic slowdown, i.e. in time $O(D\\log n)$ where $D$ is the diameter of\nthe network and $n$ is the number of vertices.\n  Moreover, for complete graphs under some additional conditions (sense of\ndirection, or $\\alpha<0.55$) the remaining constant number of vertices can be\ninformed in the same time, i.e. $O(\\log n)$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/cs/0703137v1", 
    "title": "ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous   Applications in a Parallel Environment", 
    "arxiv-id": "cs/0703137v1", 
    "author": "Calvin J. Ribbens", 
    "publish": "2007-03-27T23:27:54Z", 
    "summary": "Applications in science and engineering often require huge computational\nresources for solving problems within a reasonable time frame. Parallel\nsupercomputers provide the computational infrastructure for solving such\nproblems. A traditional application scheduler running on a parallel cluster\nonly supports static scheduling where the number of processors allocated to an\napplication remains fixed throughout the lifetime of execution of the job. Due\nto the unpredictability in job arrival times and varying resource requirements,\nstatic scheduling can result in idle system resources thereby decreasing the\noverall system throughput. In this paper we present a prototype framework\ncalled ReSHAPE, which supports dynamic resizing of parallel MPI applications\nexecuted on distributed memory platforms. The framework includes a scheduler\nthat supports resizing of applications, an API to enable applications to\ninteract with the scheduler, and a library that makes resizing viable.\nApplications executed using the ReSHAPE scheduler framework can expand to take\nadvantage of additional free processors or can shrink to accommodate a high\npriority application, without getting suspended. In our research, we have\nmainly focused on structured applications that have two-dimensional data arrays\ndistributed across a two-dimensional processor grid. The resize library\nincludes algorithms for processor selection and processor mapping. Experimental\nresults show that the ReSHAPE framework can improve individual job turn-around\ntime and overall system throughput."
},{
    "category": "cs.DC", 
    "doi": "10.1109/DASC.2007.11", 
    "link": "http://arxiv.org/pdf/0704.1827v1", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids", 
    "arxiv-id": "0704.1827v1", 
    "author": "Gerald Krafft", 
    "publish": "2007-04-06T15:59:27Z", 
    "summary": "This paper analyses the possibilities of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1051/epjap:1998151", 
    "link": "http://arxiv.org/pdf/0704.2344v1", 
    "title": "Parallel computing for the finite element method", 
    "arxiv-id": "0704.2344v1", 
    "author": "Alain Nicolas", 
    "publish": "2007-04-18T13:20:25Z", 
    "summary": "A finite element method is presented to compute time harmonic microwave\nfields in three dimensional configurations. Nodal-based finite elements have\nbeen coupled with an absorbing boundary condition to solve open boundary\nproblems. This paper describes how the modeling of large devices has been made\npossible using parallel computation, New algorithms are then proposed to\nimplement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and\non a CRAY C98. Analysis of the computation efficiency is performed using simple\nproblems. The electromagnetic scattering of a plane wave by a perfect electric\nconducting airplane is finally given as example."
},{
    "category": "cs.DC", 
    "doi": "10.1051/epjap:1998151", 
    "link": "http://arxiv.org/pdf/0704.2355v1", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3", 
    "arxiv-id": "0704.2355v1", 
    "author": "Luigi Santocanale", 
    "publish": "2007-04-18T14:39:09Z", 
    "summary": "We address the problem of &#64257;nding nice labellings for event structures\nof degree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0704.3890v1", 
    "title": "An algorithm for clock synchronization with the gradient property in   sensor networks", 
    "arxiv-id": "0704.3890v1", 
    "author": "Valmir C. Barbosa", 
    "publish": "2007-04-30T19:59:14Z", 
    "summary": "We introduce a distributed algorithm for clock synchronization in sensor\nnetworks. Our algorithm assumes that nodes in the network only know their\nimmediate neighborhoods and an upper bound on the network's diameter.\nClock-synchronization messages are only sent as part of the communication,\nassumed reasonably frequent, that already takes place among nodes. The\nalgorithm has the gradient property of [2], achieving an O(1) worst-case skew\nbetween the logical clocks of neighbors. As in the case of [3,8], the\nalgorithm's actions are such that no constant lower bound exists on the rate at\nwhich logical clocks progress in time, and for this reason the lower bound of\n[2,5] that forbids constant skew between neighbors does not apply."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0706.2069v1", 
    "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors:   the BubbleSched Framework", 
    "arxiv-id": "0706.2069v1", 
    "author": "Pierre-Andr\u00e9 Wacrenier", 
    "publish": "2007-06-14T09:35:30Z", 
    "summary": "Exploiting full computational power of current more and more hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. Unfortunately, most\noperating systems only provide a poor scheduling API that does not allow\napplications to transmit valuable scheduling hints to the system. In a previous\npaper, we showed that using a bubble-based thread scheduler can significantly\nimprove applications' performance in a portable way. However, since\nmultithreaded applications have various scheduling requirements, there is no\nuniversal scheduler that could meet all these needs. In this paper, we present\na framework that allows scheduling experts to implement and experiment with\ncustomized thread schedulers. It provides a powerful API for dynamically\ndistributing bubbles among the machine in a high-level, portable, and efficient\nway. Several examples show how experts can then develop, debug and tune their\nown portable bubble schedulers."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2008.11.001", 
    "link": "http://arxiv.org/pdf/0706.2146v1", 
    "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel   Computations", 
    "arxiv-id": "0706.2146v1", 
    "author": "Calvin J. Ribbens", 
    "publish": "2007-06-14T15:54:10Z", 
    "summary": "Traditional parallel schedulers running on cluster supercomputers support\nonly static scheduling, where the number of processors allocated to an\napplication remains fixed throughout the execution of the job. This results in\nunder-utilization of idle system resources thereby decreasing overall system\nthroughput. In our research, we have developed a prototype framework called\nReSHAPE, which supports dynamic resizing of parallel MPI applications executing\non distributed memory platforms. The resizing library in ReSHAPE includes\nsupport for releasing and acquiring processors and efficiently redistributing\napplication state to a new set of processors. In this paper, we derive an\nalgorithm for redistributing two-dimensional block-cyclic arrays from $P$ to\n$Q$ processors, organized as 2-D processor grids. The algorithm ensures a\ncontention-free communication schedule for data redistribution if $P_r \\leq\nQ_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row\nand column shifts on the communication schedule to minimize node contention."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3008v1", 
    "title": "A Generic Deployment Framework for Grid Computing and Distributed   Applications", 
    "arxiv-id": "0706.3008v1", 
    "author": "Philippe Merle", 
    "publish": "2007-06-20T15:17:47Z", 
    "summary": "Deployment of distributed applications on large systems, and especially on\ngrid infrastructures, becomes a more and more complex task. Grid users spend a\nlot of time to prepare, install and configure middleware and application\nbinaries on nodes, and eventually start their applications. The problem is that\nthe deployment process is composed of many heterogeneous tasks that have to be\norchestrated in a specific correct order. As a consequence, the automatization\nof the deployment process is currently very difficult to reach. To address this\nproblem, we propose in this paper a generic deployment framework allowing to\nautomatize the execution of heterogeneous tasks composing the whole deployment\nprocess. Our approach is based on a reification as software components of all\nrequired deployment mechanisms or existing tools. Grid users only have to\ndescribe the configuration to deploy in a simple natural language instead of\nprogramming or scripting how the deployment process is executed. As a toy\nexample, this framework is used to deploy CORBA component-based applications\nand OpenCCM middleware on one thousand nodes of the French Grid5000\ninfrastructure."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3350v3", 
    "title": "Optimal Replica Placement in Tree Networks with QoS and Bandwidth   Constraints and the Closest Allocation Policy", 
    "arxiv-id": "0706.3350v3", 
    "author": "Veronika Rehn-Sonigo", 
    "publish": "2007-06-22T15:01:35Z", 
    "summary": "This paper deals with the replica placement problem on fully homogeneous tree\nnetworks known as the Replica Placement optimization problem. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. We investigate the latter problem using the Closest access\npolicy when adding QoS and bandwidth constraints. We propose an optimal\nalgorithm in two passes using dynamic programming."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.3546v2", 
    "title": "stdchk: A Checkpoint Storage System for Desktop Grid Computing", 
    "arxiv-id": "0706.3546v2", 
    "author": "Abdullah Gharaibeh", 
    "publish": "2007-06-25T01:24:46Z", 
    "summary": "Checkpointing is an indispensable technique to provide fault tolerance for\nlong-running high-throughput applications like those running on desktop grids.\nThis paper argues that a dedicated checkpoint storage system, optimized to\noperate in these environments, can offer multiple benefits: reduce the load on\na traditional file system, offer high-performance through specialization, and,\nfinally, optimize data management by taking into account checkpoint application\nsemantics. Such a storage system can present a unifying abstraction to\ncheckpoint operations, while hiding the fact that there are no dedicated\nresources to store the checkpoint data. We prototype stdchk, a checkpoint\nstorage system that uses scavenged disk space from participating desktops to\nbuild a low-cost storage system, offering a traditional file system interface\nfor easy integration with applications. This paper presents the stdchk\narchitecture, key performance optimizations, support for incremental\ncheckpointing, and increased data availability. Our evaluation confirms that\nthe stdchk approach is viable in a desktop grid setting and offers a low cost\nstorage system with desirable performance characteristics: high write\nthroughput and reduced storage space and network effort to save checkpoint\nimages."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4009v2", 
    "title": "Multi-criteria scheduling of pipeline workflows", 
    "arxiv-id": "0706.4009v2", 
    "author": "Yves Robert", 
    "publish": "2007-06-27T13:43:16Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonist criteria should be optimized, such as throughput and latency (or a\ncombination). In this paper, we study the complexity of the bi-criteria mapping\nproblem for pipeline graphs on communication homogeneous platforms. In\nparticular, we assess the complexity of the well-known chains-to-chains problem\nfor different-speed processors, which turns out to be NP-hard. We provide\nseveral efficient polynomial bi-criteria heuristics, and their relative\nperformance is evaluated through extensive simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4015v1", 
    "title": "Self-Stabilizing Wavelets and r-Hops Coordination", 
    "arxiv-id": "0706.4015v1", 
    "author": "Franck Petit", 
    "publish": "2007-06-27T12:53:06Z", 
    "summary": "We introduce a simple tool called the wavelet (or, r-wavelet) scheme.\nWavelets deals with coordination among processes which are at most r hops away\nof each other. We present a selfstabilizing solution for this scheme. Our\nsolution requires no underlying structure and works in arbritrary anonymous\nnetworks, i.e., no process identifier is required. Moreover, our solution works\nunder any (even unfair) daemon. Next, we use the wavelet scheme to design\nself-stabilizing layer clocks. We show that they provide an efficient device in\nthe design of local coordination problems at distance r, i.e., r-barrier\nsynchronization and r-local resource allocation (LRA) such as r-local mutual\nexclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some\nsolutions to the r-LRA problem (e.g., r-LME) also provide transformers to\ntransform algorithms written assuming any r-central daemon into algorithms\nworking with any distributed daemon."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4038v2", 
    "title": "Scheduling multiple divisible loads on a linear processor network", 
    "arxiv-id": "0706.4038v2", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2007-06-27T14:43:13Z", 
    "summary": "Min, Veeravalli, and Barlas have recently proposed strategies to minimize the\noverall execution time of one or several divisible loads on a heterogeneous\nlinear network, using one or more installments. We show on a very simple\nexample that their approach does not always produce a solution and that, when\nit does, the solution is often suboptimal. We also show how to find an optimal\nschedule for any instance, once the number of installments per load is given.\nThen, we formally state that any optimal schedule has an infinite number of\ninstallments under a linear cost model as the one assumed in the original\npapers. Therefore, such a cost model cannot be used to design practical\nmulti-installment strategies. Finally, through extensive simulations we\nconfirmed that the best solution is always produced by the linear programming\napproach, while solutions of the original papers can be far away from the\noptimal."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0706.4298v1", 
    "title": "Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous   Anonymous Networks", 
    "arxiv-id": "0706.4298v1", 
    "author": "Christian Boulinier", 
    "publish": "2007-06-28T18:51:36Z", 
    "summary": "How to pass from local to global scales in anonymous networks? How to\norganize a selfstabilizing propagation of information with feedback. From the\nAngluin impossibility results, we cannot elect a leader in a general anonymous\nnetwork. Thus, it is impossible to build a rooted spanning tree. Many problems\ncan only be solved by probabilistic methods. In this paper we show how to use\nUnison to design a self-stabilizing barrier synchronization in an anonymous\nnetwork. We show that the commuication structure of this barrier\nsynchronization designs a self-stabilizing wave-stream, or pipelining wave, in\nanonymous networks. We introduce two variants of Wave: the strong waves and the\nwavelets. A strong wave can be used to solve the idempotent r-operator\nparametrized computation problem. A wavelet deals with k-distance computation.\nWe show how to use Unison to design a self-stabilizing wave stream, a\nself-stabilizing strong wave stream and a self-stabilizing wavelet stream."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0365v1", 
    "title": "Performance Analysis of Publish/Subscribe Systems", 
    "arxiv-id": "0707.0365v1", 
    "author": "Mohamed Jemni", 
    "publish": "2007-07-03T09:02:45Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. Its technology consists\nmainly in exploiting resources, geographically dispersed, to treat complex\napplications needing big power of calculation and/or important storage\ncapacity. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfigurations, decentralisation and performance\nbecomes more and more essential. Since such properties are exhibited by P2P\nsystems, the convergence of grid computing and P2P computing seems natural. In\nthis context, this paper evaluates the scalability and performance of P2P tools\nfor discovering and registering services. Three protocols are used for this\npurpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of\ntheses protocols related to two criteria: the elapsed time for registrations\nservices and the needed time to discover new services. Our aim is to analyse\nthese results in order to choose the best protocol we can use in order to\ncreate a decentralised middleware for desktop grid."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0740v1", 
    "title": "A Multi Interface Grid Discovery System", 
    "arxiv-id": "0707.0740v1", 
    "author": "Ian Willers", 
    "publish": "2007-07-05T09:22:45Z", 
    "summary": "Discovery Systems (DS) can be considered as entry points for global loosely\ncoupled distributed systems. An efficient Discovery System in essence increases\nthe performance, reliability and decision making capability of distributed\nsystems. With the rapid increase in scale of distributed applications, existing\nsolutions for discovery systems are fast becoming either obsolete or incapable\nof handling such complexity. They are particularly ineffective when handling\nservice lifetimes and providing up-to-date information, poor at enabling\ndynamic service access and they can also impose unwanted restrictions on\ninterfaces to widely available information repositories. In this paper we\npresent essential the design characteristics, an implementation and a\nperformance analysis for a discovery system capable of overcoming these\ndeficiencies in large, globally distributed environments."
},{
    "category": "cs.DC", 
    "doi": "10.1007/11914952_26", 
    "link": "http://arxiv.org/pdf/0707.0742v1", 
    "title": "Mobile Computing in Physics Analysis - An Indicator for eScience", 
    "arxiv-id": "0707.0742v1", 
    "author": "I. Willers", 
    "publish": "2007-07-05T09:32:29Z", 
    "summary": "This paper presents the design and implementation of a Grid-enabled physics\nanalysis environment for handheld and other resource-limited computing devices\nas one example of the use of mobile devices in eScience. Handheld devices offer\ngreat potential because they provide ubiquitous access to data and\nround-the-clock connectivity over wireless links. Our solution aims to provide\nusers of handheld devices the capability to launch heavy computational tasks on\ncomputational and data Grids, monitor the jobs status during execution, and\nretrieve results after job completion. Users carry their jobs on their handheld\ndevices in the form of executables (and associated libraries). Users can\ntransparently view the status of their jobs and get back their outputs without\nhaving to know where they are being executed. In this way, our system is able\nto act as a high-throughput computing environment where devices ranging from\npowerful desktop machines to small handhelds can employ the power of the Grid.\nThe results shown in this paper are readily applicable to the wider eScience\ncommunity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0743v1", 
    "title": "DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling", 
    "arxiv-id": "0707.0743v1", 
    "author": "O. Alvi", 
    "publish": "2007-07-05T09:36:18Z", 
    "summary": "The use of meta-schedulers for resource management in large-scale distributed\nsystems often leads to a hierarchy of schedulers. In this paper, we discuss why\nexisting meta-scheduling hierarchies are sometimes not sufficient for Grid\nsystems due to their inability to re-organise jobs already scheduled locally.\nSuch a job re-organisation is required to adapt to evolving loads which are\ncommon in heavily used Grid infrastructures. We propose a peer-to-peer\nscheduling model and evaluate it using case studies and mathematical modelling.\nWe detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and\nits queue management system for coping with the load distribution and for\nsupporting bulk job scheduling. We demonstrate that such a system is beneficial\nfor dynamic, distributed and self-organizing resource management and can assist\nin optimizing load or job distribution in complex Grid infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0748v1", 
    "title": "Experiences of Engineering Grid-Based Medical Software", 
    "arxiv-id": "0707.0748v1", 
    "author": "T. Solomonides", 
    "publish": "2007-07-05T10:06:41Z", 
    "summary": "Objectives: Grid-based technologies are emerging as potential solutions for\nmanaging and collaborating distributed resources in the biomedical domain. Few\nexamples exist, however, of successful implementations of Grid-enabled medical\nsystems and even fewer have been deployed for evaluation in practice. The\nobjective of this paper is to evaluate the use in clinical practice of a\nGrid-based imaging prototype and to establish directions for engineering future\nmedical Grid developments and their subsequent deployment. Method: The\nMammoGrid project has deployed a prototype system for clinicians using the Grid\nas its information infrastructure. To assist in the specification of the system\nrequirements (and for the first time in healthgrid applications), use-case\nmodelling has been carried out in close collaboration with clinicians and\nradiologists who had no prior experience of this modelling technique. A\ncritical qualitative and, where possible, quantitative analysis of the\nMammoGrid prototype is presented leading to a set of recommendations from the\ndelivery of the first deployed Grid-based medical imaging application. Results:\nWe report critically on the application of software engineering techniques in\nthe specification and implementation of the MammoGrid project and show that\nuse-case modelling is a suitable vehicle for representing medical requirements\nand for communicating effectively with the clinical community. This paper also\ndiscusses the practical advantages and limitations of applying the Grid to\nreal-life clinical applications and presents the consequent lessons learned."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0762v1", 
    "title": "PhantomOS: A Next Generation Grid Operating System", 
    "arxiv-id": "0707.0762v1", 
    "author": "Peter Bloodsworth", 
    "publish": "2007-07-05T11:14:45Z", 
    "summary": "Grid Computing has made substantial advances in the past decade; these are\nprimarily due to the adoption of standardized Grid middleware. However Grid\ncomputing has not yet become pervasive because of some barriers that we believe\nhave been caused by the adoption of middleware centric approaches. These\nbarriers include: scant support for major types of applications such as\ninteractive applications; lack of flexible, autonomic and scalable Grid\narchitectures; lack of plug-and-play Grid computing and, most importantly, no\nstraightforward way to setup and administer Grids. PhantomOS is a project which\naims to address many of these barriers. Its goal is the creation of a user\nfriendly pervasive Grid computing platform that facilitates the rapid\ndeployment and easy maintenance of Grids whilst providing support for major\ntypes of applications on Grids of almost any topology. In this paper we present\nthe detailed system architecture and an overview of its implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.0862v1", 
    "title": "Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments", 
    "arxiv-id": "0707.0862v1", 
    "author": "Michael Thomas", 
    "publish": "2007-07-05T19:46:51Z", 
    "summary": "In Grids scheduling decisions are often made on the basis of jobs being\neither data or computation intensive: in data intensive situations jobs may be\npushed to the data and in computation intensive situations data may be pulled\nto the jobs. This kind of scheduling, in which there is no consideration of\nnetwork characteristics, can lead to performance degradation in a Grid\nenvironment and may result in large processing queues and job execution delays\ndue to site overloads. In this paper we describe a Data Intensive and Network\nAware (DIANA) meta-scheduling approach, which takes into account data,\nprocessing power and network characteristics when making scheduling decisions\nacross multiple sites. Through a practical implementation on a Grid testbed, we\ndemonstrate that queue and execution times of data-intensive jobs can be\nsignificantly improved when we introduce our proposed DIANA scheduler. The\nbasic scheduling decisions are dictated by a weighting factor for each\npotential target location which is a calculated function of network\ncharacteristics, processing cycles and data location and size. The job\nscheduler provides a global ranking of the computing resources and then selects\nan optimal one on the basis of this overall access and execution cost. The\nDIANA approach considers the Grid as a combination of active network elements\nand takes network characteristics as a first class criterion in the scheduling\ndecision matrix along with computation and data. The scheduler can then make\ninformed decisions by taking into account the changing state of the network,\nlocality and size of the data and the pool of available processing cycles."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0707.1607v1", 
    "title": "Cactus Framework: Black Holes to Gamma Ray Bursts", 
    "arxiv-id": "0707.1607v1", 
    "author": "John Shalf", 
    "publish": "2007-07-11T13:01:50Z", 
    "summary": "Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of\ncosmological origin. They are among the most scientifically interesting\nastrophysical systems, and the riddle concerning their central engines and\nemission mechanisms is one of the most complex and challenging problems of\nastrophysics today. In this article we outline our petascale approach to the\nGRB problem and discuss the computational toolkits and numerical codes that are\ncurrently in use and that will be scaled up to run on emerging petaflop scale\ncomputing platforms in the near future.\n  Petascale computing will require additional ingredients over conventional\nparallelism. We consider some of the challenges which will be caused by future\npetascale architectures, and discuss our plans for the future development of\nthe Cactus framework and its applications to meet these challenges in order to\nprofit from these new architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.0608v2", 
    "title": "Resource Allocation in Public Cluster with Extended Optimization   Algorithm", 
    "arxiv-id": "0708.0608v2", 
    "author": "L. T. Handoko", 
    "publish": "2007-08-04T05:15:05Z", 
    "summary": "We introduce an optimization algorithm for resource allocation in the LIPI\nPublic Cluster to optimize its usage according to incoming requests from users.\nThe tool is an extended and modified genetic algorithm developed to match\nspecific natures of public cluster. We present a detail analysis of\noptimization, and compare the results with the exact calculation. We show that\nit would be very useful and could realize an automatic decision making system\nfor public clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.3446v2", 
    "title": "Multi and Independent Block Approach in Public Cluster", 
    "arxiv-id": "0708.3446v2", 
    "author": "L. T. Handoko", 
    "publish": "2007-08-25T19:46:52Z", 
    "summary": "We present extended multi block approach in the LIPI Public Cluster. The\nmulti block approach enables a cluster to be divided into several independent\nblocks which run jobs owned by different users simultaneously. Previously, we\nhave maintained the blocks using single master node for all blocks due to\nefficiency and resource limitations. Following recent advancements and\nexpansion of node\\'s number, we have modified the multi block approach with\nmultiple master nodes, each of them is responsible for a single block. We argue\nthat this approach improves the overall performance significantly, for\nespecially data intensive computational works."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0708.3734v1", 
    "title": "Searching for a dangerous host: randomized vs. deterministic", 
    "arxiv-id": "0708.3734v1", 
    "author": "Rossano Venturini", 
    "publish": "2007-08-28T09:24:13Z", 
    "summary": "A Black Hole is an harmful host in a network that destroys incoming agents\nwithout leaving any trace of such event. The problem of locating the black hole\nin a network through a team of agent coordinated by a common protocol is\nusually referred in literature as the Black Hole Search problem (or BHS for\nbrevity) and it is a consolidated research topic in the area of distributed\nalgorithms. The aim of this paper is to extend the results for BHS by\nconsidering more general (and hence harder) classes of dangerous host. In\nparticular we introduce rB-hole as a probabilistic generalization of the Black\nHole, in which the destruction of an incoming agent is a purely random event\nhappening with some fixed probability (like flipping a biased coin). The main\nresult we present is that if we tolerate an arbitrarily small error probability\nin the result then the rB-hole Search problem, or RBS, is not harder than the\nusual BHS. We establish this result in two different communication model,\nspecifically both in presence or absence of whiteboards non-located at the\nhomebase. The core of our methods is a general reduction tool for transforming\nalgorithms for the black hole into algorithms for the rB-hole."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.2635v1", 
    "title": "Non-Blocking Signature of very large SOAP Messages", 
    "arxiv-id": "0709.2635v1", 
    "author": "L. Lo Iacono", 
    "publish": "2007-09-17T13:50:42Z", 
    "summary": "Data transfer and staging services are common components in Grid-based, or\nmore generally, in service-oriented applications. Security mechanisms play a\ncentral role in such services, especially when they are deployed in sensitive\napplication fields like e-health. The adoption of WS-Security and related\nstandards to SOAP-based transfer services is, however, problematic as a\nstraightforward adoption of SOAP with MTOM introduces considerable\ninefficiencies in the signature generation process when large data sets are\ninvolved. This paper proposes a non-blocking, signature generation approach\nenabling a stream-like processing with considerable performance enhancements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3689v1", 
    "title": "Static Deadlock Detection in MPI Synchronization Communication", 
    "arxiv-id": "0709.3689v1", 
    "author": "Fan Zhi-Hua", 
    "publish": "2007-09-24T05:33:29Z", 
    "summary": "It is very common to use dynamic methods to detect deadlocks in MPI programs\nfor the reason that static methods have some restrictions. To guarantee high\nreliability of some important MPI-based application software, a model of MPI\nsynchronization communication is abstracted and a type of static method is\ndevised to examine deadlocks in such modes. The model has three forms with\ndifferent complexity: sequential model, single-loop model and nested-loop\nmodel. Sequential model is a base for all models. Single-loop model must be\ntreated with a special type of equation group and nested-loop model extends the\nmethods for the other two models. A standard Java-based software framework\noriginated from these methods is constructed for determining whether MPI\nprograms are free from synchronization communication deadlocks. Our practice\nshows the software framework is better than those tools using dynamic methods\nbecause it can dig out all synchronization communication deadlocks before an\nMPI-based program goes into running."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3692v2", 
    "title": "Deadlock Detection in Basic Models of MPI Synchronization Communication   Programs", 
    "arxiv-id": "0709.3692v2", 
    "author": "Zhi-hua Fan", 
    "publish": "2007-09-24T06:02:29Z", 
    "summary": "A model of MPI synchronization communication programs is presented and its\nthree basic simplified models are also defined. A series of theorems and\nmethods for deciding whether deadlocks will occur among the three models are\ngiven and proved strictly. These theories and methods for simple models'\ndeadlock detection are the necessary base for real MPI program deadlock\ndetection. The methods are based on a static analysis through programs and with\nruntime detection in necessary cases and they are able to determine before\ncompiling whether it will be deadlocked for two of the three basic models. For\nanother model, some deadlock cases can be found before compiling and others at\nruntime. Our theorems can be used to prove the correctness of currently popular\nMPI program deadlock detection algorithms. Our methods may decrease codes that\nthose algorithms need to change to MPI source or profiling interface and may\ndetects deadlocks ahead of program execution, thus the overheads can be reduced\ngreatly."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3693v1", 
    "title": "Algorithm of Static Deadlock Detection in MPI Synchronization   Communication Sequential Model", 
    "arxiv-id": "0709.3693v1", 
    "author": "Fan Zhi-Hua", 
    "publish": "2007-09-24T06:06:34Z", 
    "summary": "Detecting deadlocks in MPI synchronization communication programs is very\ndifficult and need building program models. All complex models are based on\nsequential models. The sequential model is mapped into a set of character\nstrings and its deadlock detection problem is translated into an equivalent\nmulti-queue string matching problem. An algorithm is devised and implemented to\nstatically detect deadlocks in sequential models of MPI synchronization\ncommunication programs. The time and space complexity of the algorithm is O(n)\nwhere n is the amount of message in model. The algorithm is better than usual\ncircle-detection methods and can adapt well to dynamic message stream."
},{
    "category": "cs.DC", 
    "doi": "10.1109/E-SCIENCE.2006.261173", 
    "link": "http://arxiv.org/pdf/0709.3826v1", 
    "title": "Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids", 
    "arxiv-id": "0709.3826v1", 
    "author": "Gerald Krafft", 
    "publish": "2007-09-24T19:25:27Z", 
    "summary": "This paper analyses the requirements of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the most promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.1499v1", 
    "title": "Approximating max-min linear programs with local algorithms", 
    "arxiv-id": "0710.1499v1", 
    "author": "Jukka Suomela", 
    "publish": "2007-10-08T09:46:47Z", 
    "summary": "A local algorithm is a distributed algorithm where each node must operate\nsolely based on the information that was available at system startup within a\nconstant-size neighbourhood of the node. We study the applicability of local\nalgorithms to max-min LPs where the objective is to maximise $\\min_k \\sum_v\nc_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$\nfor each $v$. Here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $V_i =\n\\{v : a_{iv} > 0 \\}$, $V_k = \\{v : c_{kv}>0 \\}$, $I_v = \\{i : a_{iv} > 0 \\}$\nand $K_v = \\{k : c_{kv} > 0 \\}$ have bounded size. In the distributed setting,\neach agent $v$ is responsible for choosing the value of $x_v$, and the\ncommunication network is a hypergraph $\\mathcal{H}$ where the sets $V_k$ and\n$V_i$ constitute the hyperedges. We present inapproximability results for a\nwide range of structural assumptions; for example, even if $|V_i|$ and $|V_k|$\nare bounded by some constants larger than 2, there is no local approximation\nscheme. To contrast the negative results, we present a local approximation\nalgorithm which achieves good approximation ratios if we can bound the relative\ngrowth of the vertex neighbourhoods in $\\mathcal{H}$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.1784v1", 
    "title": "Designing a commutative replicated data type", 
    "arxiv-id": "0710.1784v1", 
    "author": "Nuno Pregui\u00e7a", 
    "publish": "2007-10-09T14:38:50Z", 
    "summary": "Commuting operations greatly simplify consistency in distributed systems.\nThis paper focuses on designing for commutativity, a topic neglected\npreviously. We show that the replicas of \\emph{any} data type for which\nconcurrent operations commute converges to a correct value, under some simple\nand standard assumptions. We also show that such a data type supports\ntransactions with very low cost. We identify a number of approaches and\ntechniques to ensure commutativity. We re-use some existing ideas\n(non-destructive updates coupled with invariant identification), but propose a\nmuch more efficient implementation. Furthermore, we propose a new technique,\nbackground consensus. We illustrate these ideas with a shared edit buffer data\ntype."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0710.5348v1", 
    "title": "Towards Grid Monitoring and deployment in Jade, using ProActive", 
    "arxiv-id": "0710.5348v1", 
    "author": "Virginie Legrand Contes", 
    "publish": "2007-10-29T10:44:35Z", 
    "summary": "This document describes our current effort to gridify Jade, a java-based\nenvironment for the autonomic management of clustered J2EE application servers,\ndeveloped in the INRIA SARDES research team. Towards this objective, we use the\njava ProActive grid technology. We first present some of the challenges to turn\nsuch an autonomic management system initially dedicated to distributed\napplications running on clusters of machines, into one that can provide\nself-management capabilities to large-scale systems, i.e. deployed on grid\ninfrastructures. This leads us to a brief state of the art on grid monitoring\nsystems. Then, we recall the architecture of Jade, and consequently propose to\nreorganize it in a potentially more scalable way. Practical experiments pertain\nto the use of the grid deployment feature offered by ProActive to easily\nconduct the deployment of the Jade system or its revised version on any sort of\ngrid."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0314v1", 
    "title": "Resource and Application Models for Advanced Grid Schedulers", 
    "arxiv-id": "0711.0314v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:03:46Z", 
    "summary": "As Grid computing is becoming an inevitable future, managing, scheduling and\nmonitoring dynamic, heterogeneous resources will present new challenges.\nSolutions will have to be agile and adaptive, support self-organization and\nautonomous management, while maintaining optimal resource utilisation.\nPresented in this paper are basic principles and architectural concepts for\nefficient resource allocation in heterogeneous Grid environment."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0315v1", 
    "title": "Measuring and Monitoring Grid Resource Utilisation", 
    "arxiv-id": "0711.0315v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:12:27Z", 
    "summary": "Effective resource utilisation monitoring and highly granular yet adaptive\nmeasurements are prerequisites for a more efficient Grid scheduler. We present\na suite of measurement applications able to monitor per-process resource\nutilisation, and a customisable tool for emulating observed utilisation models."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0316v1", 
    "title": "A Study of Grid Applications: Scheduling Perspective", 
    "arxiv-id": "0711.0316v1", 
    "author": "Lionel Sacks", 
    "publish": "2007-11-02T14:15:45Z", 
    "summary": "As the Grid evolves from a high performance cluster middleware to a\nmultipurpose utility computing framework, a good understanding of Grid\napplications, their statistics and utilisation patterns is required. This study\nlooks at job execution times and resource utilisations in a Grid environment,\nand their significance in cluster and network dimensioning, local level\nscheduling and resource management."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0325v1", 
    "title": "Self-Organising management of Grid environments", 
    "arxiv-id": "0711.0325v1", 
    "author": "Paul McKee", 
    "publish": "2007-11-02T15:26:48Z", 
    "summary": "This paper presents basic concepts, architectural principles and algorithms\nfor efficient resource and security management in cluster computing\nenvironments and the Grid. The work presented in this paper is funded by\nBTExacT and the EPSRC project SO-GRM (GR/S21939)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0326v1", 
    "title": "Enabling Adaptive Grid Scheduling and Resource Management", 
    "arxiv-id": "0711.0326v1", 
    "author": "Ognjen Prnjat", 
    "publish": "2007-11-02T15:30:53Z", 
    "summary": "Wider adoption of the Grid concept has led to an increasing amount of\nfederated computational, storage and visualisation resources being available to\nscientists and researchers. Distributed and heterogeneous nature of these\nresources renders most of the legacy cluster monitoring and management\napproaches inappropriate, and poses new challenges in workflow scheduling on\nsuch systems. Effective resource utilisation monitoring and highly granular yet\nadaptive measurements are prerequisites for a more efficient Grid scheduler. We\npresent a suite of measurement applications able to monitor per-process\nresource utilisation, and a customisable tool for emulating observed\nutilisation models. We also outline our future work on a predictive and\nprobabilistic Grid scheduler. The research is undertaken as part of UK\ne-Science EPSRC sponsored project SO-GRM (Self-Organising Grid Resource\nManagement) in cooperation with BT."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.0327v1", 
    "title": "Managing Uncertainty: A Case for Probabilistic Grid Scheduling", 
    "arxiv-id": "0711.0327v1", 
    "author": "Ognjen Prnjat", 
    "publish": "2007-11-02T15:36:36Z", 
    "summary": "The Grid technology is evolving into a global, service-orientated\narchitecture, a universal platform for delivering future high demand\ncomputational services. Strong adoption of the Grid and the utility computing\nconcept is leading to an increasing number of Grid installations running a wide\nrange of applications of different size and complexity. In this paper we\naddress the problem of elivering deadline/economy based scheduling in a\nheterogeneous application environment using statistical properties of job\nhistorical executions and its associated meta-data. This approach is motivated\nby a study of six-month computational load generated by Grid applications in a\nmulti-purpose Grid cluster serving a community of twenty e-Science projects.\nThe observed job statistics, resource utilisation and user behaviour is\ndiscussed in the context of management approaches and models most suitable for\nsupporting a probabilistic and autonomous scheduling architecture."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.1231v3", 
    "title": "Optimizing Latency and Reliability of Pipeline Workflow Applications", 
    "arxiv-id": "0711.1231v3", 
    "author": "Yves Robert", 
    "publish": "2007-11-08T14:45:12Z", 
    "summary": "Mapping applications onto heterogeneous platforms is a difficult challenge,\neven for simple application patterns such as pipeline graphs. The problem is\neven more complex when processors are subject to failure during the execution\nof the application. In this paper, we study the complexity of a bi-criteria\nmapping which aims at optimizing the latency (i.e., the response time) and the\nreliability (i.e., the probability that the computation will be successful) of\nthe application. Latency is minimized by using faster processors, while\nreliability is increased by replicating computations on a set of processors.\nHowever, replication increases latency (additional communications, slower\nprocessors). The application fails to be executed only if all the processors\nfail during execution. While simple polynomial algorithms can be found for\nfully homogeneous platforms, the problem becomes NP-hard when tackling\nheterogeneous platforms. This is yet another illustration of the additional\ncomplexity added by heterogeneity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.1786v1", 
    "title": "A Mobile Computing Architecture for Numerical Simulation", 
    "arxiv-id": "0711.1786v1", 
    "author": "Fabrice Mourlin", 
    "publish": "2007-11-12T14:39:34Z", 
    "summary": "The domain of numerical simulation is a place where the parallelization of\nnumerical code is common. The definition of a numerical context means the\nconfiguration of resources such as memory, processor load and communication\ngraph, with an evolving feature: the resources availability. A feature is often\nmissing: the adaptability. It is not predictable and the adaptable aspect is\nessential. Without calling into question these implementations of these codes,\nwe create an adaptive use of these implementations. Because the execution has\nto be driven by the availability of main resources, the components of a numeric\ncomputation have to react when their context changes. This paper offers a new\narchitecture, a mobile computing architecture, based on mobile agents and\nJavaSpace. At the end of this paper, we apply our architecture to several case\nstudies and obtain our first results."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.2062v1", 
    "title": "Autoregressive Time Series Forecasting of Computational Demand", 
    "arxiv-id": "0711.2062v1", 
    "author": "Thomas Sandholm", 
    "publish": "2007-11-14T00:57:13Z", 
    "summary": "We study the predictive power of autoregressive moving average models when\nforecasting demand in two shared computational networks, PlanetLab and Tycoon.\nDemand in these networks is very volatile, and predictive techniques to plan\nusage in advance can improve the performance obtained drastically.\n  Our key finding is that a random walk predictor performs best for\none-step-ahead forecasts, whereas ARIMA(1,1,0) and adaptive exponential\nsmoothing models perform better for two and three-step-ahead forecasts. A Monte\nCarlo bootstrap test is proposed to evaluate the continuous prediction\nperformance of different models with arbitrary confidence and statistical\nsignificance levels. Although the prediction results differ between the Tycoon\nand PlanetLab networks, we observe very similar overall statistical properties,\nsuch as volatility dynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0711.3949v1", 
    "title": "An Adaptive Checkpointing Scheme for Peer-to-Peer Based Volunteer   Computing Work Flows", 
    "arxiv-id": "0711.3949v1", 
    "author": "Aaron Harwood", 
    "publish": "2007-11-26T06:41:23Z", 
    "summary": "Volunteer Computing, sometimes called Public Resource Computing, is an\nemerging computational model that is very suitable for work-pooled parallel\nprocessing. As more complex grid applications make use of work flows in their\ndesign and deployment it is reasonable to consider the impact of work flow\ndeployment over a Volunteer Computing infrastructure. In this case, the inter\nwork flow I/O can lead to a significant increase in I/O demands at the work\npool server. A possible solution is the use of a Peer-to- Peer based parallel\ncomputing architecture to off-load this I/O demand to the workers; where the\nworkers can fulfill some aspects of work flow coordination and I/O checking,\netc. However, achieving robustness in such a large scale system is a\nchallenging hurdle towards the decentralized execution of work flows and\ngeneral parallel processes. To increase robustness, we propose and show the\nmerits of using an adaptive checkpoint scheme that efficiently checkpoints the\nstatus of the parallel processes according to the estimation of relevant\nnetwork and peer parameters. Our scheme uses statistical data observed during\nruntime to dynamically make checkpoint decisions in a completely de-\ncentralized manner. The results of simulation show support for our proposed\napproach in terms of reduced required runtime."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0712.3980v1", 
    "title": "Distributed Slicing in Dynamic Systems", 
    "arxiv-id": "0712.3980v1", 
    "author": "Michel Raynal", 
    "publish": "2007-12-26T13:55:47Z", 
    "summary": "Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services capable of\ndealing with resource assignment and management in a large-scale, heterogeneous\nand unreliable environment. The slicing service, has been proposed to allow for\nan automatic partitioning of P2P networks into groups (slices) that represent a\ncontrollable amount of some resource and that are also relatively homogeneous\nwith respect to that resource. In this paper we propose two gossip-based\nalgorithms to solve the distributed slicing problem. The first algorithm speeds\nup an existing algorithm sorting a set of uniform random numbers. The second\nalgorithm statistically approximates the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms rely on\ntheir gossip-based models. These algorithms are proved viable theoretically and\nexperimentally."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1210v1", 
    "title": "Increasing GP Computing Power via Volunteer Computing", 
    "arxiv-id": "0801.1210v1", 
    "author": "K. Sharman", 
    "publish": "2008-01-08T11:36:35Z", 
    "summary": "This paper describes how it is possible to increase GP Computing Power via\nVolunteer Computing (VC) using the BOINC framework. Two experiments using\nwell-known GP tools -Lil-gp & ECJ- are performed in order to demonstrate the\nbenefit of using VC in terms of computing power and speed up. Finally we\npresent an extension of the model where any GP tool or framework can be used\ninside BOINC regardless of its programming language, complexity or required\noperating system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1419v1", 
    "title": "Core Persistence in Peer-to-Peer Systems: Relating Size to Lifetime", 
    "arxiv-id": "0801.1419v1", 
    "author": "Bruno Sericola", 
    "publish": "2008-01-09T12:41:15Z", 
    "summary": "Distributed systems are now both very large and highly dynamic. Peer to peer\noverlay networks have been proved efficient to cope with this new deal that\ntraditional approaches can no longer accommodate. While the challenge of\norganizing peers in an overlay network has generated a lot of interest leading\nto a large number of solutions, maintaining critical data in such a network\nremains an open issue. In this paper, we are interested in defining the portion\nof nodes and frequency one has to probe, given the churn observed in the\nsystem, in order to achieve a given probability of maintaining the persistence\nof some critical data. More specifically, we provide a clear result relating\nthe size and the frequency of the probing set along with its proof as well as\nan analysis of the way of leveraging such an information in a large scale\ndynamic distributed system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.1772v1", 
    "title": "Bi-criteria Pipeline Mappings for Parallel Image Processing", 
    "arxiv-id": "0801.1772v1", 
    "author": "Yves Robert", 
    "publish": "2008-01-11T14:48:43Z", 
    "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonistic criteria should be optimized, such as throughput and latency (or a\ncombination). Typical applications include digital image processing, where\nimages are processed in steady-state mode. In this paper, we study the mapping\nof a particular image processing application, the JPEG encoding. Mapping\npipelined JPEG encoding onto parallel platforms is useful for instance for\nencoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete,\nwe concentrate on the evaluation and performance of polynomial heuristics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0801.4150v1", 
    "title": "e-Science perspectives in Venezuela", 
    "arxiv-id": "0801.4150v1", 
    "author": "M. Uzcategui", 
    "publish": "2008-01-27T20:22:29Z", 
    "summary": "We describe the e-Science strategy in Venezuela, in particular initiatives by\nthe Centro Nacional de Calculo Cientifico Universidad de Los Andes (CECALCULA),\nMerida, the Universidad de Los Andes (ULA), Merida, and the Instituto\nVenezolano de Investigaciones Cientificas (IVIC), Caracas. We present the plans\nfor the Venezuelan Academic Grid and the current status of Grid ULA supported\nby Internet2. We show different web-based scientific applications that are\nbeing developed in quantum chemistry, atomic physics, structural damage\nanalysis, biomedicine and bioclimate within the framework of the\nE-Infrastructure shared between Europe and Latin America (EELA)"
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0802.0550v1", 
    "title": "Energy Aware Self-Organizing Density Management in Wireless Sensor   Networks", 
    "arxiv-id": "0802.0550v1", 
    "author": "Marin Bertier", 
    "publish": "2008-02-05T07:03:28Z", 
    "summary": "Energy consumption is the most important factor that determines sensor node\nlifetime. The optimization of wireless sensor network lifetime targets not only\nthe reduction of energy consumption of a single sensor node but also the\nextension of the entire network lifetime. We propose a simple and adaptive\nenergy-conserving topology management scheme, called SAND (Self-Organizing\nActive Node Density). SAND is fully decentralized and relies on a distributed\nprobing approach and on the redundancy resolution of sensors for energy\noptimizations, while preserving the data forwarding and sensing capabilities of\nthe network. We present the SAND's algorithm, its analysis of convergence, and\nsimulation results. Simulation results show that, though slightly increasing\npath lengths from sensor to sink nodes, the proposed scheme improves\nsignificantly the network lifetime for different neighborhood densities\ndegrees, while preserving both sensing and routing fidelity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.0048v4", 
    "title": "A Bit-Compatible Shared Memory Parallelization for ILU(k)   Preconditioning and a Bit-Compatible Generalization to Distributed Memory", 
    "arxiv-id": "0803.0048v4", 
    "author": "Gene Cooperman", 
    "publish": "2008-03-01T07:21:27Z", 
    "summary": "ILU(k) is a commonly used preconditioner for iterative linear solvers for\nsparse, non-symmetric systems. It is often preferred for the sake of its\nstability. We present TPILU(k), the first efficiently parallelized ILU(k)\npreconditioner that maintains this important stability property. Even better,\nTPILU(k) preconditioning produces an answer that is bit-compatible with the\nsequential ILU(k) preconditioning. In terms of performance, the TPILU(k)\npreconditioning is shown to run faster whenever more cores are made available\nto it --- while continuing to be as stable as sequential ILU(k). This is in\ncontrast to some competing methods that may become unstable if the degree of\nthread parallelism is raised too far. Where Block Jacobi ILU(k) fails in an\napplication, it can be replaced by TPILU(k) in order to maintain good\nperformance, while also achieving full stability. As a further optimization,\nTPILU(k) offers an optional level-based incomplete inverse method as a fast\napproximation for the original ILU(k) preconditioned matrix. Although this\nenhancement is not bit-compatible with classical ILU(k), it is bit-compatible\nwith the output from the single-threaded version of the same algorithm. In\nexperiments on a 16-core computer, the enhanced TPILU(k)-based iterative linear\nsolver performed up to 9 times faster. As we approach an era of many-core\ncomputing, the ability to efficiently take advantage of many cores will become\never more important. TPILU(k) also demonstrates good performance on cluster or\nGrid. For example, the new algorithm achieves 50 times speedup with 80 nodes\nfor general sparse matrices of dimension 160,000 that are diagonally dominant."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.0241v2", 
    "title": "Self-Stabilizing Pulse Synchronization Inspired by Biological Pacemaker   Networks", 
    "arxiv-id": "0803.0241v2", 
    "author": "Hanna Parnas", 
    "publish": "2008-03-03T13:46:45Z", 
    "summary": "We define the ``Pulse Synchronization'' problem that requires nodes to\nachieve tight synchronization of regular pulse events, in the settings of\ndistributed computing systems. Pulse-coupled synchronization is a phenomenon\ndisplayed by a large variety of biological systems, typically overcoming a high\nlevel of noise. Inspired by such biological models, a robust and\nself-stabilizing Byzantine pulse synchronization algorithm for distributed\ncomputer systems is presented. The algorithm attains near optimal\nsynchronization tightness while tolerating up to a third of the nodes\nexhibiting Byzantine behavior concurrently. Pulse synchronization has been\npreviously shown to be a powerful building block for designing algorithms in\nthis severe fault model. We have previously shown how to stabilize general\nByzantine algorithms, using pulse synchronization. To the best of our knowledge\nthere is no other scheme to do this without the use of synchronized pulses."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.1520v1", 
    "title": "Integrity-Enhancing Replica Coordination for Byzantine Fault Tolerant   Systems", 
    "arxiv-id": "0803.1520v1", 
    "author": "Wenbing Zhao", 
    "publish": "2008-03-11T04:20:06Z", 
    "summary": "Strong replica consistency is often achieved by writing deterministic\napplications, or by using a variety of mechanisms to render replicas\ndeterministic. There exists a large body of work on how to render replicas\ndeterministic under the benign fault model. However, when replicas can be\nsubject to malicious faults, most of the previous work is no longer effective.\nFurthermore, the determinism of the replicas is often considered harmful from\nthe security perspective and for many applications, their integrity strongly\ndepends on the randomness of some of their internal operations. This calls for\nnew approaches towards achieving replica consistency while preserving the\nreplica randomness. In this paper, we present two such approaches. One is based\non Byzantine agreement and the other on threshold coin-tossing. Each approach\nhas its strength and weaknesses. We compare the performance of the two\napproaches and outline their respective best use scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.1521v1", 
    "title": "Proactive Service Migration for Long-Running Byzantine Fault Tolerant   Systems", 
    "arxiv-id": "0803.1521v1", 
    "author": "Wenbing Zhao", 
    "publish": "2008-03-11T04:34:04Z", 
    "summary": "In this paper, we describe a novel proactive recovery scheme based on service\nmigration for long-running Byzantine fault tolerant systems. Proactive recovery\nis an essential method for ensuring long term reliability of fault tolerant\nsystems that are under continuous threats from malicious adversaries. The\nprimary benefit of our proactive recovery scheme is a reduced vulnerability\nwindow. This is achieved by removing the time-consuming reboot step from the\ncritical path of proactive recovery. Our migration-based proactive recovery is\ncoordinated among the replicas, therefore, it can automatically adjust to\ndifferent system loads and avoid the problem of excessive concurrent proactive\nrecoveries that may occur in previous work with fixed watchdog timeouts.\nMoreover, the fast proactive recovery also significantly improves the system\navailability in the presence of faults."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.2219v1", 
    "title": "Lighweight Target Tracking Using Passive Traces in Sensor Networks", 
    "arxiv-id": "0803.2219v1", 
    "author": "Jose Rolim", 
    "publish": "2008-03-14T18:01:17Z", 
    "summary": "We study the important problem of tracking moving targets in wireless sensor\nnetworks. We try to overcome the limitations of standard state of the art\ntracking methods based on continuous location tracking, i.e. the high energy\ndissipation and communication overhead imposed by the active participation of\nsensors in the tracking process and the low scalability, especially in sparse\nnetworks. Instead, our approach uses sensors in a passive way: they just record\nand judiciously spread information about observed target presence in their\nvicinity; this information is then used by the (powerful) tracking agent to\nlocate the target by just following the traces left at sensors. Our protocol is\ngreedy, local, distributed, energy efficient and very successful, in the sense\nthat (as shown by extensive simulations) the tracking agent manages to quickly\nlocate and follow the target; also, we achieve good trade-offs between the\nenergy dissipation and latency."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0803.3642v3", 
    "title": "Distributed Averaging in the presence of a Sparse Cut", 
    "arxiv-id": "0803.3642v3", 
    "author": "Hariharan Narayanan", 
    "publish": "2008-03-25T22:04:50Z", 
    "summary": "We consider the question of averaging on a graph that has one sparse cut\nseparating two subgraphs that are internally well connected.\n  While there has been a large body of work devoted to algorithms for\ndistributed averaging, nearly all algorithms involve only {\\it convex} updates.\nIn this paper, we suggest that {\\it non-convex} updates can lead to significant\nimprovements. We do so by exhibiting a decentralized algorithm for graphs with\none sparse cut that uses non-convex averages and has an averaging time that can\nbe significantly smaller than the averaging time of known distributed\nalgorithms, such as those of \\cite{tsitsiklis, Boyd}. We use stochastic\ndominance to prove this result in a way that may be of independent interest."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0804.1607v2", 
    "title": "Distributed and Recursive Parameter Estimation in Parametrized Linear   State-Space Models", 
    "arxiv-id": "0804.1607v2", 
    "author": "A. Nedic", 
    "publish": "2008-04-10T03:47:05Z", 
    "summary": "We consider a network of sensors deployed to sense a spatio-temporal field\nand estimate a parameter of interest. We are interested in the case where the\ntemporal process sensed by each sensor can be modeled as a state-space process\nthat is perturbed by random noise and parametrized by an unknown parameter. To\nestimate the unknown parameter from the measurements that the sensors\nsequentially collect, we propose a distributed and recursive estimation\nalgorithm, which we refer to as the incremental recursive prediction error\nalgorithm. This algorithm has the distributed property of incremental gradient\nalgorithms and the on-line property of recursive prediction error algorithms.\nWe study the convergence behavior of the algorithm and provide sufficient\nconditions for its convergence. Our convergence result is rather general and\ncontains as special cases the known convergence results for the incremental\nversions of the least-mean square algorithm. Finally, we use the algorithm\ndeveloped in this paper to identify the source of a gas-leak (diffusing source)\nin a closed warehouse and also report numerical simulations to verify\nconvergence."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2008.4536235", 
    "link": "http://arxiv.org/pdf/0804.4590v1", 
    "title": "\u00c9tude de performance des syst\u00e8mes de d\u00e9couverte de ressources", 
    "arxiv-id": "0804.4590v1", 
    "author": "Mohamed Jemni", 
    "publish": "2008-04-29T11:51:19Z", 
    "summary": "The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. This technology consists\nmainly in exploiting PC resources, geographically dispersed, to treat time\nconsuming applications and/or important storage capacity requiring\napplications. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfiguration, decentralization and performance\nbecomes more and more essential. In this context, this paper evaluates the\nscalability and performance of P2P tools for registering and discovering\nservices (Publish/Subscribe systems). Three protocols are used in this purpose:\nBonjour, Avahi and Pastry. We have studied the behaviour of these protocols\nrelated to two criteria: the elapsed time for registrations services and the\nneeded time to discover new services."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0804.4815v1", 
    "title": "Tight local approximation results for max-min linear programs", 
    "arxiv-id": "0804.4815v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-04-30T12:54:34Z", 
    "summary": "In a bipartite max-min LP, we are given a bipartite graph $\\myG = (V \\cup I\n\\cup K, E)$, where each agent $v \\in V$ is adjacent to exactly one constraint\n$i \\in I$ and exactly one objective $k \\in K$. Each agent $v$ controls a\nvariable $x_v$. For each $i \\in I$ we have a nonnegative linear constraint on\nthe variables of adjacent agents. For each $k \\in K$ we have a nonnegative\nlinear objective function of the variables of adjacent agents. The task is to\nmaximise the minimum of the objective functions. We study local algorithms\nwhere each agent $v$ must choose $x_v$ based on input within its\nconstant-radius neighbourhood in $\\myG$. We show that for every $\\epsilon>0$\nthere exists a local algorithm achieving the approximation ratio ${\\Delta_I (1\n- 1/\\Delta_K)} + \\epsilon$. We also show that this result is the best possible\n-- no local algorithm can achieve the approximation ratio ${\\Delta_I (1 -\n1/\\Delta_K)}$. Here $\\Delta_I$ is the maximum degree of a vertex $i \\in I$, and\n$\\Delta_K$ is the maximum degree of a vertex $k \\in K$. As a methodological\ncontribution, we introduce the technique of graph unfolding for the design of\nlocal approximation algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.0444v2", 
    "title": "Two-enqueuer queue in Common2", 
    "arxiv-id": "0805.0444v2", 
    "author": "David Eisenstat", 
    "publish": "2008-05-04T21:08:50Z", 
    "summary": "The question of whether all shared objects with consensus number 2 belong to\nCommon2, the set of objects that can be implemented in a wait-free manner by\nany type of consensus number 2, was first posed by Herlihy. In the absence of\ngeneral results, several researchers have obtained implementations for\nrestricted-concurrency versions of FIFO queues. We present the first Common2\nalgorithm for a queue with two enqueuers and any number of dequeuers."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.2068v1", 
    "title": "Fork Sequential Consistency is Blocking", 
    "arxiv-id": "0805.2068v1", 
    "author": "Alexander Shraer", 
    "publish": "2008-05-14T14:23:53Z", 
    "summary": "We consider an untrusted server storing shared data on behalf of clients. We\nshow that no storage access protocol can on the one hand preserve sequential\nconsistency and wait-freedom when the server is correct, and on the other hand\nalways preserve fork sequential consistency."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0805.3196v1", 
    "title": "Coupling Component Systems towards Systems of Systems", 
    "arxiv-id": "0805.3196v1", 
    "author": "Jean-Ren\u00e9 Ruault", 
    "publish": "2008-05-21T05:02:22Z", 
    "summary": "Systems of systems (SoS) are a hot topic in our \"fully connected global\nworld\". Our aim is not to provide another definition of what SoS are, but\nrather to focus on the adequacy of reusing standard system architecting\ntechniques within this approach in order to improve performance, fault\ndetection and safety issues in large-scale coupled systems that definitely\nqualify as SoS, whatever the definition is. A key issue will be to secure the\navailability of the services provided by the SoS despite the evolution of the\nvarious systems composing the SoS. We will also tackle contracting issues and\nresponsibility transfers, as they should be addressed to ensure the expected\nbehavior of the SoS whilst the various independently contracted systems evolve\nasynchronously."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-92862-1_2", 
    "link": "http://arxiv.org/pdf/0806.0282v1", 
    "title": "Local approximation algorithms for a class of 0/1 max-min linear   programs", 
    "arxiv-id": "0806.0282v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-06-02T14:27:46Z", 
    "summary": "We study the applicability of distributed, local algorithms to 0/1 max-min\nLPs where the objective is to maximise ${\\min_k \\sum_v c_{kv} x_v}$ subject to\n${\\sum_v a_{iv} x_v \\le 1}$ for each $i$ and ${x_v \\ge 0}$ for each $v$. Here\n$c_{kv} \\in \\{0,1\\}$, $a_{iv} \\in \\{0,1\\}$, and the support sets ${V_i = \\{v :\na_{iv} > 0 \\}}$ and ${V_k = \\{v : c_{kv}>0 \\}}$ have bounded size; in\nparticular, we study the case $|V_k| \\le 2$. Each agent $v$ is responsible for\nchoosing the value of $x_v$ based on information within its constant-size\nneighbourhood; the communication network is the hypergraph where the sets $V_k$\nand $V_i$ constitute the hyperedges. We present a local approximation algorithm\nwhich achieves an approximation ratio arbitrarily close to the theoretical\nlower bound presented in prior work."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.2395v1", 
    "title": "Ad-hoc Limited Scale-Free Models for Unstructured Peer-to-Peer Networks", 
    "arxiv-id": "0806.2395v1", 
    "author": "Murat Yuksel", 
    "publish": "2008-06-14T19:01:50Z", 
    "summary": "Several protocol efficiency metrics (e.g., scalability, search success rate,\nrouting reachability and stability) depend on the capability of preserving\nstructure even over the churn caused by the ad-hoc nodes joining or leaving the\nnetwork. Preserving the structure becomes more prohibitive due to the\ndistributed and potentially uncooperative nature of such networks, as in the\npeer-to-peer (P2P) networks. Thus, most practical solutions involve\nunstructured approaches while attempting to maintain the structure at various\nlevels of protocol stack. The primary focus of this paper is to investigate\nconstruction and maintenance of scale-free topologies in a distributed manner\nwithout requiring global topology information at the time when nodes join or\nleave. We consider the uncooperative behavior of peers by limiting the number\nof neighbors to a pre-defined hard cutoff value (i.e., no peer is a major hub),\nand the ad-hoc behavior of peers by rewiring the neighbors of nodes leaving the\nnetwork. We also investigate the effect of these hard cutoffs and rewiring of\nad-hoc nodes on the P2P search efficiency."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.3152v2", 
    "title": "TRANS-Net: an Efficient Peer-to-Peer Overlay Network Based on a Full   Transposition Graph", 
    "arxiv-id": "0806.3152v2", 
    "author": "Athanasios K. Tsakalidis", 
    "publish": "2008-06-19T08:28:28Z", 
    "summary": "In this paper we propose a new practical P2P system based on a full\ntransposition network topology named TRANS-Net. Full transposition networks\nachieve higher fault-tolerance and lower congestion among the class of\ntransposition networks. TRANS-Net provides an efficient lookup service i.e. k\nhops with high probability, where k satisfies Theta(log_n m) less than k less\nthan Theta(log_2 m), where m denotes the number of system nodes and n is a\nsystem parameter related to the maximum number that m can take (up to n!).\nExperiments show that the look-up performance achieves the lower limit of the\ncomplexity relation. TRANS-Net also preserves data locality and provides\nefficient look-up performance for complex queries such as multi-dimensional\nqueries."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.3215v1", 
    "title": "MOHCS: Towards Mining Overlapping Highly Connected Subgraphs", 
    "arxiv-id": "0806.3215v1", 
    "author": "David K. Y. Chiu", 
    "publish": "2008-06-19T15:13:38Z", 
    "summary": "Many networks in real-life typically contain parts in which some nodes are\nmore highly connected to each other than the other nodes of the network. The\ncollection of such nodes are usually called clusters, communities, cohesive\ngroups or modules. In graph terminology, it is called highly connected graph.\nIn this paper, we first prove some properties related to highly connected\ngraph. Based on these properties, we then redefine the highly connected\nsubgraph which results in an algorithm that determines whether a given graph is\nhighly connected in linear time. Then we present a computationally efficient\nalgorithm, called MOHCS, for mining overlapping highly connected subgraphs. We\nhave evaluated experimentally the performance of MOHCS using real and synthetic\ndata sets from computer-generated graph and yeast protein network. Our results\nshow that MOHCS is effective and reliable in finding overlapping highly\nconnected subgraphs. Keywords-component; Highly connected subgraph, clustering\nalgorithms, minimum cut, minimum degree"
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0806.4221v1", 
    "title": "Localized Spanners for Wireless Networks", 
    "arxiv-id": "0806.4221v1", 
    "author": "Sriram V. Pemmaraju", 
    "publish": "2008-06-26T02:09:17Z", 
    "summary": "We present a new efficient localized algorithm to construct, for any given\nquasi-unit disk graph G=(V,E) and any e > 0, a (1+e)-spanner for G of maximum\ndegree O(1) and total weight O(w(MST)), where w(MST) denotes the weight of a\nminimum spanning tree for V. We further show that similar localized techniques\ncan be used to construct, for a given unit disk graph G = (V, E), a planar\nCdel(1+e)(1+pi/2)-spanner for G of maximum degree O(1) and total weight\nO(w(MST)). Here Cdel denotes the stretch factor of the unit Delaunay\ntriangulation for V. Both constructions can be completed in O(1) communication\nrounds, and require each node to know its own coordinates."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0807.1720v1", 
    "title": "Resource Allocation Strategies for In-Network Stream Processing", 
    "arxiv-id": "0807.1720v1", 
    "author": "Yves Robert", 
    "publish": "2008-07-10T19:14:14Z", 
    "summary": "In this paper we consider the operator mapping problem for in-network stream\nprocessing applications. In-network stream processing consists in applying a\ntree of operators in steady-state to multiple data objects that are continually\nupdated at various locations on a network. Examples of in-network stream\nprocessing include the processing of data in a sensor network, or of continuous\nqueries on distributed relational databases. We study the operator mapping\nproblem in a ``constructive'' scenario, i.e., a scenario in which one builds a\nplatform dedicated to the application buy purchasing processing servers with\nvarious costs and capabilities. The objective is to minimize the cost of the\nplatform while ensuring that the application achieves a minimum steady-state\nthroughput. The first contribution of this paper is the formalization of a set\nof relevant operator-placement problems as linear programs, and a proof that\neven simple versions of the problem are NP-complete. Our second contribution is\nthe design of several polynomial time heuristics, which are evaluated via\nextensive simulations and compared to theoretical bounds for optimal solutions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0807.4609v1", 
    "title": "Analisis Kinerja Sistem Cluster Terhadapa Aplikasi Simulasi Dinamika   Molekular NAMD Memanfaatkan Pustaka CHARM++", 
    "arxiv-id": "0807.4609v1", 
    "author": "A. B. Mutiara", 
    "publish": "2008-07-29T09:18:21Z", 
    "summary": "Tingkat kompleksitas dari program simulasi dinamika molekular membutuhkan\nmesin pemroses dengan kemampuan yang sangat besar. Mesin-mesin paralel terbukti\nmemiliki potensi untuk menjawab tantangan komputasi ini. Untuk memanfaatkan\npotensi ini secara maksimal, diperlukan suatu program paralel dengan tingkat\nefisiensi, efektifitas, skalabilitas, dan ekstensibilitas yang maksimal pula.\nProgram NAMD yang dibahas pada penulisan ini dianggap mampu untuk memenuhi\nsemua kriteria yang diinginkan. Program ini dirancang dengan\nmengimplementasikan pustaka Charm++ untuk pembagian tugas perhitungan secara\nparalel. NAMD memiliki sistem automatic load balancing secara periodik yang\ncerdas, sehingga dapat memaksimalkan penggunaan kemampuan mesin yang tersedia.\nProgram ini juga dirancang secara modular, sehingga dapat dimodifikasi dan\nditambah dengan sangat mudah. NAMD menggunakan banyak kombinasi algoritma\nperhitungan dan tehnik-tehnik numerik lainnya dalam melakukan tugasnya. NAMD\n2.5 mengimplementasikan semua tehnik dan persamaan perhitungan yang digunakan\ndalam dunia simulasi dinamika molekular saat ini. NAMD dapat berjalan diatas\nberbagai mesin paralel termasuk arsitektur cluster, dengan hasil speedup yang\nmengejutkan. Tulisan ini akan menjelaskan dan membuktikan kemampuan NAMD secara\nparalel diatas lima buah mesin cluster. Penulisan ini juga akan memaparkan\nkinerja NAMD pada beberapa."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0808.1505v1", 
    "title": "An Almost-Surely Terminating Polynomial Protocol for Asynchronous   Byzantine Agreement with Optimal Resilience", 
    "arxiv-id": "0808.1505v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2008-08-11T12:22:12Z", 
    "summary": "Consider an asynchronous system with private channels and $n$ processes, up\nto $t$ of which may be faulty. We settle a longstanding open question by\nproviding a Byzantine agreement protocol that simultaneously achieves three\nproperties:\n  1. (optimal) resilience: it works as long as $n>3t$\n  2. (almost-sure) termination: with probability one, all nonfaulty processes\nterminate\n  3. (polynomial) efficiency: the expected computation time, memory\nconsumption, message size, and number of messages sent are all polynomial in\n$n$.\n  Earlier protocols have achieved only two of these three properties. In\nparticular, the protocol of Bracha is not polynomially efficient, the protocol\nof Feldman and Micali is not optimally resilient, and the protocol of Canetti\nand Rabin does not have almost-sure termination. Our protocol utilizes a new\nprimitive called shunning (asynchronous) verifiable secret sharing (SVSS),\nwhich ensures, roughly speaking, that either a secret is successfully shared or\na new faulty process is ignored from this point onwards by some nonfaulty\nprocess."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0808.1802v1", 
    "title": "Compute and Storage Clouds Using Wide Area High Performance Networks", 
    "arxiv-id": "0808.1802v1", 
    "author": "Wanzhi Zhang", 
    "publish": "2008-08-13T09:48:37Z", 
    "summary": "We describe a cloud based infrastructure that we have developed that is\noptimized for wide area, high performance networks and designed to support data\nmining applications. The infrastructure consists of a storage cloud called\nSector and a compute cloud called Sphere. We describe two applications that we\nhave built using the cloud and some experimental studies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0808.3019v1", 
    "title": "Data Mining Using High Performance Data Clouds: Experimental Studies   Using Sector and Sphere", 
    "arxiv-id": "0808.3019v1", 
    "author": "Yunhong Gu", 
    "publish": "2008-08-22T01:24:06Z", 
    "summary": "We describe the design and implementation of a high performance cloud that we\nhave used to archive, analyze and mine large distributed data sets. By a cloud,\nwe mean an infrastructure that provides resources and/or services over the\nInternet. A storage cloud provides storage services, while a compute cloud\nprovides compute services. We describe the design of the Sector storage cloud\nand how it provides the storage services required by the Sphere compute cloud.\nWe also describe the programming paradigm supported by the Sphere compute\ncloud. Sector and Sphere are designed for analyzing large data sets using\ncomputer clusters connected with wide area high performance networks (for\nexample, 10+ Gb/s). We describe a distributed data mining application that we\nhave developed using Sector and Sphere. Finally, we describe some experimental\nstudies comparing Sector/Sphere to Hadoop."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0808.3535v1", 
    "title": "Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for   Data Intensive Applications", 
    "arxiv-id": "0808.3535v1", 
    "author": "Alex Szalay", 
    "publish": "2008-08-26T15:19:44Z", 
    "summary": "Data intensive applications often involve the analysis of large datasets that\nrequire large amounts of compute and storage resources. While dedicated compute\nand/or storage farms offer good task/data throughput, they suffer low resource\nutilization problem under varying workloads conditions. If we instead move such\ndata to distributed computing resources, then we incur expensive data transfer\ncost. In this paper, we propose a data diffusion approach that combines dynamic\nresource provisioning, on-demand data replication and caching, and data\nlocality-aware scheduling to achieve improved resource efficiency under varying\nworkloads. We define an abstract \"data diffusion model\" that takes into\nconsideration the workload characteristics, data accessing cost, application\nthroughput and resource utilization; we validate the model using a real-world\nlarge-scale astronomy application. Our results show that data diffusion can\nincrease the performance index by as much as 34X, and improve application\nresponse time by over 506X, while achieving near-optimal throughputs and\nexecution times."
},{
    "category": "cs.DC", 
    "doi": "10.1109/P2P.2008.16", 
    "link": "http://arxiv.org/pdf/0808.3536v1", 
    "title": "Enabling Loosely-Coupled Serial Job Execution on the IBM BlueGene/P   Supercomputer and the SiCortex SC5832", 
    "arxiv-id": "0808.3536v1", 
    "author": "Ian Foster", 
    "publish": "2008-08-26T16:59:41Z", 
    "summary": "Our work addresses the enabling of the execution of highly parallel\ncomputations composed of loosely coupled serial jobs with no modifications to\nthe respective applications, on large-scale systems. This approach allows\nnew-and potentially far larger-classes of application to leverage systems such\nas the IBM Blue Gene/P supercomputer and similar emerging petascale\narchitectures. We present here the challenges of I/O performance encountered in\nmaking this model practical, and show results using both micro-benchmarks and\nreal applications on two large-scale systems, the BG/P and the SiCortex SC5832.\nOur preliminary benchmarks show that we can scale to 4096 processors on the\nBlue Gene/P and 5832 processors on the SiCortex with high efficiency, and can\nachieve thousands of tasks/sec sustained execution rates for parallel workloads\nof ordinary serial applications. We measured applications from two domains,\neconomic energy modeling and molecular dynamics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SC.2008.5219768", 
    "link": "http://arxiv.org/pdf/0808.3540v2", 
    "title": "Towards Loosely-Coupled Programming on Petascale Systems", 
    "arxiv-id": "0808.3540v2", 
    "author": "Ben Clifford", 
    "publish": "2008-08-26T16:48:14Z", 
    "summary": "We have extended the Falkon lightweight task execution framework to make\nloosely coupled programming on petascale systems a practical and useful\nprogramming model. This work studies and measures the performance factors\ninvolved in applying this approach to enable the use of petascale systems by a\nbroader user community, and with greater ease. Our work enables the execution\nof highly parallel computations composed of loosely coupled serial jobs with no\nmodifications to the respective applications. This approach allows a new-and\npotentially far larger-class of applications to leverage petascale systems,\nsuch as the IBM Blue Gene/P supercomputer. We present the challenges of I/O\nperformance encountered in making this model practical, and show results using\nboth microbenchmarks and real applications from two domains: economic energy\nmodeling and molecular dynamics. Our benchmarks show that we can scale up to\n160K processor-cores with high efficiency, and can achieve sustained execution\nrates of thousands of tasks per second."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1383519.1383521", 
    "link": "http://arxiv.org/pdf/0808.3546v1", 
    "title": "Accelerating Large-scale Data Exploration through Data Diffusion", 
    "arxiv-id": "0808.3546v1", 
    "author": "Alex Szalay", 
    "publish": "2008-08-26T16:02:50Z", 
    "summary": "Data-intensive applications often require exploratory analysis of large\ndatasets. If analysis is performed on distributed resources, data locality can\nbe crucial to high throughput and performance. We propose a \"data diffusion\"\napproach that acquires compute and storage resources dynamically, replicates\ndata in response to demand, and schedules computations close to data. As demand\nincreases, more resources are acquired, thus allowing faster response to\nsubsequent requests that refer to the same data; when demand drops, resources\nare released. This approach can provide the benefits of dedicated hardware\nwithout the associated high costs, depending on workload and resource\ncharacteristics. The approach is reminiscent of cooperative caching,\nweb-caching, and peer-to-peer storage systems, but addresses different\napplication demands. Other data-aware scheduling approaches assume dedicated\nresources, which can be expensive and/or inefficient if load varies\nsignificantly. To explore the feasibility of the data diffusion approach, we\nhave extended the Falkon resource provisioning and task scheduling system to\nsupport data caching and data-aware scheduling. Performance results from both\nmicro-benchmarks and a large scale astronomy application demonstrate that our\napproach improves performance relative to alternative approaches, as well as\nprovides improved scalability as aggregated I/O bandwidth scales linearly with\nthe number of data cache nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0808.3558v1", 
    "title": "Market-Oriented Cloud Computing: Vision, Hype, and Reality for   Delivering IT Services as Computing Utilities", 
    "arxiv-id": "0808.3558v1", 
    "author": "Srikumar Venugopal", 
    "publish": "2008-08-26T17:16:11Z", 
    "summary": "This keynote paper: presents a 21st century vision of computing; identifies\nvarious computing paradigms promising to deliver the vision of computing\nutilities; defines Cloud computing and provides the architecture for creating\nmarket-oriented Clouds by leveraging technologies such as VMs; provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; presents some representative Cloud platforms\nespecially those developed in industries along with our current work towards\nrealising market-oriented resource allocation of Clouds by leveraging the 3rd\ngeneration Aneka enterprise Grid technology; reveals our early thoughts on\ninterconnecting Clouds for dynamically creating an atmospheric computing\nenvironment along with pointers to future community research; and concludes\nwith the need for convergence of competing IT paradigms for delivering our 21st\ncentury vision."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0808.3693v1", 
    "title": "Providing Virtual Execution Environments: A Twofold Illustration", 
    "arxiv-id": "0808.3693v1", 
    "author": "J. M. Dana", 
    "publish": "2008-08-27T12:48:39Z", 
    "summary": "Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2008.172", 
    "link": "http://arxiv.org/pdf/0809.1181v2", 
    "title": "Sector and Sphere: Towards Simplified Storage and Processing of Large   Scale Distributed Data", 
    "arxiv-id": "0809.1181v2", 
    "author": "Robert L Grossman", 
    "publish": "2008-09-06T18:37:51Z", 
    "summary": "Cloud computing has demonstrated that processing very large datasets over\ncommodity clusters can be done simply given the right programming model and\ninfrastructure. In this paper, we describe the design and implementation of the\nSector storage cloud and the Sphere compute cloud. In contrast to existing\nstorage and compute clouds, Sector can manage data not only within a data\ncenter, but also across geographically distributed data centers. Similarly, the\nSphere compute cloud supports User Defined Functions (UDF) over data both\nwithin a data center and across data centers. As a special case, MapReduce\nstyle programming can be implemented in Sphere by using a Map UDF followed by a\nReduce UDF. We describe some experimental studies comparing Sector/Sphere and\nHadoop using the Terasort Benchmark. In these studies, Sector is about twice as\nfast as Hadoop. Sector/Sphere is open source."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1583991.1584058", 
    "link": "http://arxiv.org/pdf/0809.1489v1", 
    "title": "An optimal local approximation algorithm for max-min linear programs", 
    "arxiv-id": "0809.1489v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-09-09T06:11:31Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\napproximating max-min LPs. The objective is to maximise $\\omega$ subject to $Ax\n\\le 1$, $Cx \\ge \\omega 1$, and $x \\ge 0$ for nonnegative matrices $A$ and $C$.\nThe approximation ratio of our algorithm is the best possible for any local\nalgorithm; there is a matching unconditional lower bound."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1583991.1584058", 
    "link": "http://arxiv.org/pdf/0809.4107v1", 
    "title": "Modelling interdependencies between the electricity and information   infrastructures", 
    "arxiv-id": "0809.4107v1", 
    "author": "Mohamed Kaaniche", 
    "publish": "2008-09-24T07:26:09Z", 
    "summary": "The aim of this paper is to provide qualitative models characterizing\ninterdependencies related failures of two critical infrastructures: the\nelectricity infrastructure and the associated information infrastructure. The\ninterdependencies of these two infrastructures are increasing due to a growing\nconnection of the power grid networks to the global information infrastructure,\nas a consequence of market deregulation and opening. These interdependencies\nincrease the risk of failures. We focus on cascading, escalating and\ncommon-cause failures, which correspond to the main causes of failures due to\ninterdependencies. We address failures in the electricity infrastructure, in\ncombination with accidental failures in the information infrastructure, then we\nshow briefly how malicious attacks in the information infrastructure can be\naddressed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2175v1", 
    "title": "A simple local 3-approximation algorithm for vertex cover", 
    "arxiv-id": "0810.2175v1", 
    "author": "Jukka Suomela", 
    "publish": "2008-10-13T12:45:15Z", 
    "summary": "We present a local algorithm (constant-time distributed algorithm) for\nfinding a 3-approximate vertex cover in bounded-degree graphs. The algorithm is\ndeterministic, and no auxiliary information besides port numbering is required."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2226v1", 
    "title": "Enabling Lock-Free Concurrent Fine-Grain Access to Massive Distributed   Data: Application to Supernovae Detection", 
    "arxiv-id": "0810.2226v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2008-10-13T13:07:18Z", 
    "summary": "We consider the problem of efficiently managing massive data in a large-scale\ndistributed environment. We consider data strings of size in the order of\nTerabytes, shared and accessed by concurrent clients. On each individual\naccess, a segment of a string, of the order of Megabytes, is read or modified.\nOur goal is to provide the clients with efficient fine-grain access the data\nstring as concurrently as possible, without locking the string itself. This\nissue is crucial in the context of applications in the field of astronomy,\ndatabases, data mining and multimedia. We illustrate these requiremens with the\ncase of an application for searching supernovae. Our solution relies on\ndistributed, RAM-based data storage, while leveraging a DHT-based, parallel\nmetadata management scheme. The proposed architecture and algorithms have been\nvalidated through a software prototype and evaluated in a cluster environment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.2227v1", 
    "title": "Distributed Management of Massive Data: an Efficient Fine-Grain Data   Access Scheme", 
    "arxiv-id": "0810.2227v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2008-10-13T13:08:14Z", 
    "summary": "This paper addresses the problem of efficiently storing and accessing massive\ndata blocks in a large-scale distributed environment, while providing efficient\nfine-grain access to data subsets. This issue is crucial in the context of\napplications in the field of databases, data mining and multimedia. We propose\na data sharing service based on distributed, RAM-based storage of data, while\nleveraging a DHT-based, natively parallel metadata management scheme. As\nopposed to the most commonly used grid storage infrastructures that provide\nmechanisms for explicit data localization and transfer, we provide a\ntransparent access model, where data are accessed through global identifiers.\nOur proposal has been validated through a prototype implementation whose\npreliminary evaluation provides promising results."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.3715v1", 
    "title": "Distributed Estimation over Wireless Sensor Networks with Packet Losses", 
    "arxiv-id": "0810.3715v1", 
    "author": "Alberto Sangiovanni-Vincentelli", 
    "publish": "2008-10-21T01:00:26Z", 
    "summary": "A distributed adaptive algorithm to estimate a time-varying signal, measured\nby a wireless sensor network, is designed and analyzed. One of the major\nfeatures of the algorithm is that no central coordination among the nodes needs\nto be assumed. The measurements taken by the nodes of the network are affected\nby noise, and the communication among the nodes is subject to packet losses.\nNodes exchange local estimates and measurements with neighboring nodes. Each\nnode of the network locally computes adaptive weights that minimize the\nestimation error variance. Decentralized conditions on the weights, needed for\nthe convergence of the estimation error throughout the overall network, are\npresented. A Lipschitz optimization problem is posed to guarantee stability and\nthe minimization of the variance. An efficient strategy to distribute the\ncomputation of the optimal solution is investigated. A theoretical performance\nanalysis of the distributed algorithm is carried out both in the presence of\nperfect and lossy links. Numerical simulations illustrate performance for\nvarious network topologies and packet loss probabilities."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.ipl.2009.02.017", 
    "link": "http://arxiv.org/pdf/0810.3836v3", 
    "title": "Best-effort Group Service in Dynamic Networks", 
    "arxiv-id": "0810.3836v3", 
    "author": "Franck Petit", 
    "publish": "2008-10-21T13:58:50Z", 
    "summary": "We propose a group membership service for dynamic ad hoc networks. It\nmaintains as long as possible the existing groups and ensures that each group\ndiameter is always smaller than a constant, fixed according to the application\nusing the groups. The proposed protocol is self-stabilizing and works in\ndynamic distributed systems. Moreover, it ensures a kind of continuity in the\nservice offer to the application while the system is converging, except if too\nstrong topology changes happen. Such a best effort behavior allows applications\nto rely on the groups while the stabilization has not been reached, which is\nvery useful in dynamic ad hoc networks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.4440v1", 
    "title": "Randomization Adaptive Self-Stabilization", 
    "arxiv-id": "0810.4440v1", 
    "author": "Nir Tzachar", 
    "publish": "2008-10-24T12:15:40Z", 
    "summary": "We present a scheme to convert self-stabilizing algorithms that use\nrandomization during and following convergence to self-stabilizing algorithms\nthat use randomization only during convergence. We thus reduce the number of\nrandom bits from an infinite number to a bounded number. The scheme is\napplicable to the cases in which there exits a local predicate for each node,\nsuch that global consistency is implied by the union of the local predicates.\nWe demonstrate our scheme over the token circulation algorithm of Herman and\nthe recent constant time Byzantine self-stabilizing clock synchronization\nalgorithm by Ben-Or, Dolev and Hoch. The application of our scheme results in\nthe first constant time Byzantine self-stabilizing clock synchronization\nalgorithm that uses a bounded number of random bits."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5439v1", 
    "title": "The Multi-Core Era - Trends and Challenges", 
    "arxiv-id": "0810.5439v1", 
    "author": "Peter Tr\u00f6ger", 
    "publish": "2008-10-30T08:34:48Z", 
    "summary": "Since the very beginning of hardware development, computer processors were\ninvented with ever-increasing clock frequencies and sophisticated in-build\noptimization strategies. Due to physical limitations, this 'free lunch' of\nspeedup has come to an end.\n  The following article gives a summary and bibliography for recent trends and\nchallenges in CMP architectures. It discusses how 40 years of parallel\ncomputing research need to be considered in the upcoming multi-core era. We\nargue that future research must be driven from two sides - a better expression\nof hardware structures, and a domain-specific understanding of software\nparallelism."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5596v1", 
    "title": "Programming languages with algorithmically parallelizing problem", 
    "arxiv-id": "0810.5596v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-10-31T00:25:15Z", 
    "summary": "The study consists of two parts. Objective of the first part is modern\nlanguage constructions responsible for algorithmically insolvability of\nparallelizing problem. Second part contains several ways to modify the\nconstructions to make the problem algorithmically solvable"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5732v1", 
    "title": "Practical language based on systems of definitions", 
    "arxiv-id": "0810.5732v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-10-31T16:42:45Z", 
    "summary": "The article suggests a description of a system of tables with a set of\nspecial lists absorbing a semantics of data and reflects a fullness of data. It\nshows how their parallel processing can be constructed based on the\ndescriptions. The approach also might be used for definition intermediate\ntargets for data mining and unstructured data processing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0810.5758v1", 
    "title": "Non procedural language for parallel programs", 
    "arxiv-id": "0810.5758v1", 
    "author": "Renat Nuriyev", 
    "publish": "2008-10-31T18:44:38Z", 
    "summary": "Probably building non procedural languages is the most prospective way for\nparallel programming just because non procedural means no fixed way for\nexecution. The article consists of 3 parts. In first part we consider formal\nsystems for definition a named datasets and studying an expression power of\ndifferent subclasses. In the second part we consider a complexity of algorithms\nof building sets by the definitions. In third part we consider a fullness and\nflexibility of the class of program based data set definitions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_57", 
    "link": "http://arxiv.org/pdf/0811.1504v1", 
    "title": "Parallel execution of portfolio optimization", 
    "arxiv-id": "0811.1504v1", 
    "author": "R. Nuriyev", 
    "publish": "2008-11-10T15:52:25Z", 
    "summary": "Analysis of asset liability management (ALM) strategies especially for long\nterm horizon is a crucial issue for banks, funds and insurance companies.\n  Modern economic models, investment strategies and optimization criteria make\nALM studies computationally very intensive task. It attracts attention to\nmultiprocessor system and especially to the cheapest one: multi core PCs and PC\nclusters.\n  In this article we are analyzing problem of parallel organization of\nportfolio optimization, results of using clusters for optimization and the most\nefficient cluster architecture for these kinds of tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-89335-6_9", 
    "link": "http://arxiv.org/pdf/0811.3176v1", 
    "title": "Self-stabilizing Numerical Iterative Computation", 
    "arxiv-id": "0811.3176v1", 
    "author": "Danny Dolev", 
    "publish": "2008-11-19T19:11:46Z", 
    "summary": "Many challenging tasks in sensor networks, including sensor calibration,\nranking of nodes, monitoring, event region detection, collaborative filtering,\ncollaborative signal processing, {\\em etc.}, can be formulated as a problem of\nsolving a linear system of equations. Several recent works propose different\ndistributed algorithms for solving these problems, usually by using linear\niterative numerical methods.\n  In this work, we extend the settings of the above approaches, by adding\nanother dimension to the problem. Specifically, we are interested in {\\em\nself-stabilizing} algorithms, that continuously run and converge to a solution\nfrom any initial state. This aspect of the problem is highly important due to\nthe dynamic nature of the network and the frequent changes in the measured\nenvironment.\n  In this paper, we link together algorithms from two different domains. On the\none hand, we use the rich linear algebra literature of linear iterative methods\nfor solving systems of linear equations, which are naturally distributed with\nrapid convergence properties. On the other hand, we are interested in\nself-stabilizing algorithms, where the input to the computation is constantly\nchanging, and we would like the algorithms to converge from any initial state.\nWe propose a simple novel method called \\syncAlg as a self-stabilizing variant\nof the linear iterative methods. We prove that under mild conditions the\nself-stabilizing algorithm converges to a desired result. We further extend\nthese results to handle the asynchronous case.\n  As a case study, we discuss the sensor calibration problem and provide\nsimulation results to support the applicability of our approach."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-540-89335-6_9", 
    "link": "http://arxiv.org/pdf/0812.0736v1", 
    "title": "Fully distributed and fault tolerant task management based on diffusions", 
    "arxiv-id": "0812.0736v1", 
    "author": "Cyril Rabat", 
    "publish": "2008-12-03T14:58:19Z", 
    "summary": "The task management is a critical component for the computational grids. The\naim is to assign tasks on nodes according to a global scheduling policy and a\nview of local resources of nodes. A peer-to-peer approach for the task\nmanagement involves a better scalability for the grid and a higher fault\ntolerance. But some mechanisms have to be proposed to avoid the computation of\nreplicated tasks that can reduce the efficiency and increase the load of nodes.\nIn the same way, these mechanisms have to limit the number of exchanged\nmessages to avoid the overload of the network.\n  In a previous paper, we have proposed two methods for the task management\ncalled active and passive. These methods are based on a random walk: they are\nfully distributed and fault tolerant. Each node owns a local tasks states set\nupdated thanks to a random walk and each node is in charge of the local\nassignment. Here, we propose three methods to improve the efficiency of the\nactive method. These new methods are based on a circulating word. The nodes\nlocal tasks states sets are updated thanks to periodical diffusions along trees\nbuilt from the circulating word. Particularly, we show that these methods\nincrease the efficiency of the active method: they produce less replicated\ntasks. These three methods are also fully distributed and fault tolerant. On\nthe other way, the circulating word can be exploited for other applications\nlike the resources management or the nodes synchronization."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GCE.2008.4738445", 
    "link": "http://arxiv.org/pdf/0901.0131v1", 
    "title": "Cloud Computing and Grid Computing 360-Degree Compared", 
    "arxiv-id": "0901.0131v1", 
    "author": "Shiyong Lu", 
    "publish": "2008-12-31T19:13:05Z", 
    "summary": "Cloud Computing has become another buzzword after Web 2.0. However, there are\ndozens of different definitions for Cloud Computing and there seems to be no\nconsensus on what a Cloud is. On the other hand, Cloud Computing is not a\ncompletely new concept; it has intricate connection to the relatively new but\nthirteen-year established Grid Computing paradigm, and other relevant\ntechnologies such as utility computing, cluster computing, and distributed\nsystems in general. This paper strives to compare and contrast Cloud Computing\nwith Grid Computing from various angles and give insights into the essential\ncharacteristics of both."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.0134v1", 
    "title": "Design and Evaluation of a Collective IO Model for Loosely Coupled   Petascale Programming", 
    "arxiv-id": "0901.0134v1", 
    "author": "Michael Wilde", 
    "publish": "2008-12-31T19:35:07Z", 
    "summary": "Loosely coupled programming is a powerful paradigm for rapidly creating\nhigher-level applications from scientific programs on petascale systems,\ntypically using scripting languages. This paradigm is a form of many-task\ncomputing (MTC) which focuses on the passing of data between programs as\nordinary files rather than messages. While it has the significant benefits of\ndecoupling producer and consumer and allowing existing application programs to\nbe executed in parallel with no recoding, its typical implementation using\nshared file systems places a high performance burden on the overall system and\non the user who will analyze and consume the downstream data. Previous efforts\nhave achieved great speedups with loosely coupled programs, but have done so\nwith careful manual tuning of all shared file system access. In this work, we\nevaluate a prototype collective IO model for file-based MTC. The model enables\nefficient and easy distribution of input data files to computing nodes and\ngathering of output results from them. It eliminates the need for such manual\ntuning and makes the programming of large-scale clusters using a loosely\ncoupled model easier. Our approach, inspired by in-memory approaches to\ncollective operations for parallel programming, builds on fast local file\nsystems to provide high-speed local file caches for parallel scripts, uses a\nbroadcast approach to handle distribution of common input data, and uses\nefficient scatter/gather and caching techniques for input and output. We\ndescribe the design of the prototype model, its implementation on the Blue\nGene/P supercomputer, and present preliminary measurements of its performance\non synthetic benchmarks and on a large-scale molecular dynamics application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.1307v1", 
    "title": "Using Graphics Processors for Parallelizing Hash-based Data Carving", 
    "arxiv-id": "0901.1307v1", 
    "author": "David Defour", 
    "publish": "2009-01-09T20:15:26Z", 
    "summary": "The ability to detect fragments of deleted image files and to reconstruct\nthese image files from all available fragments on disk is a key activity in the\nfield of digital forensics. Although reconstruction of image files from the\nfile fragments on disk can be accomplished by simply comparing the content of\nsectors on disk with the content of known files, this brute-force approach can\nbe time consuming. This paper presents results from research into the use of\nGraphics Processing Units (GPUs) in detecting specific image file byte patterns\nin disk clusters. Unique identifying pattern for each disk sector is compared\nagainst patterns in known images. A pattern match indicates the potential\npresence of an image and flags the disk sector for further in-depth examination\nto confirm the match. The GPU-based implementation outperforms the software\nimplementation by a significant margin."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MTAGS.2008.4777908", 
    "link": "http://arxiv.org/pdf/0901.3384v1", 
    "title": "A Boundary Approximation Algorithm for Distributed Sensor Networks", 
    "arxiv-id": "0901.3384v1", 
    "author": "Marko A. Rodriguez", 
    "publish": "2009-01-22T00:59:01Z", 
    "summary": "We present an algorithm for boundary approximation in locally-linked sensor\nnetworks that communicate with a remote monitoring station. Delaunay\ntriangulations and Voronoi diagrams are used to generate a sensor communication\nnetwork and define boundary segments between sensors, respectively. The\nproposed algorithm reduces remote station communication by approximating\nboundaries via a decentralized computation executed within the sensor network.\nMoreover, the algorithm identifies boundaries based on differences between\nneighboring sensor readings, and not absolute sensor values. An analysis of the\nbandwidth consumption of the algorithm is presented and compared to two naive\napproaches. The proposed algorithm reduces the amount of remote communication\n(compared to the naive approaches) and becomes increasingly useful in networks\nwith more nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0902.2685v2", 
    "title": "Ganga: a tool for computational-task management and easy access to Grid   resources", 
    "arxiv-id": "0902.2685v2", 
    "author": "M. Williams", 
    "publish": "2009-02-16T13:31:44Z", 
    "summary": "In this paper, we present the computational task-management tool Ganga, which\nallows for the specification, submission, bookkeeping and post-processing of\ncomputational tasks on a wide set of distributed resources. Ganga has been\ndeveloped to solve a problem increasingly common in scientific projects, which\nis that researchers must regularly switch between different processing systems,\neach with its own command set, to complete their computational tasks. Ganga\nprovides a homogeneous environment for processing data on heterogeneous\nresources. We give examples from High Energy Physics, demonstrating how an\nanalysis can be developed on a local system and then transparently moved to a\nGrid system for processing of all available data. Ganga has an API that can be\nused via an interactive interface, in scripts, or through a GUI. Specific\nknowledge about types of tasks or computational resources is provided at\nrun-time through a plugin system, making new developments easy to integrate. We\ngive an overview of the Ganga architecture, give examples of current use, and\ndemonstrate how Ganga can be used in many different areas of science."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0902.4730v1", 
    "title": "Minimal Economic Distributed Computing", 
    "arxiv-id": "0902.4730v1", 
    "author": "Jim Shank", 
    "publish": "2009-02-27T00:00:55Z", 
    "summary": "In an ideal distributed computing infrastructure, users would be able to use\ndiverse distributed computing resources in a simple coherent way, with\nguaranteed security and efficient use of shared resources in accordance with\nthe wishes of the owners of the resources. Our strategy for approaching this\nideal is to first find the simplest structure within which these goals can\nplausibly be achieved. This structure, we find, is given by a particular\nrecursive distributive lattice freely constructed from a presumed partially\nordered set of all data in the infrastructure. Minor syntactic adjustments to\nthe resulting algebra yields a simple language resembling a UNIX shell, a\nconcept of execution and an interprocess protocol. Persons, organizations and\nservers within the system express their interests explicitly via a hierarchical\ncurrency. The currency provides a common framework for treating authentication,\naccess control and resource sharing as economic problems while also introducing\na new dimension for improving the infrastructure over time by designing system\ncomponents which compete with each other to earn the currency. We explain these\nresults, discuss experience with an implementation called egg and point out\nareas where more research is needed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0903.0710v1", 
    "title": "Resource Allocation for Multiple Concurrent In-Network Stream-Processing   Applications", 
    "arxiv-id": "0903.0710v1", 
    "author": "Yves Robert", 
    "publish": "2009-03-04T08:15:00Z", 
    "summary": "This paper investigates the operator mapping problem for in-network\nstream-processing applications. In-network stream-processing amounts to\napplying one or more trees of operators in steady-state, to multiple data\nobjects that are continuously updated at different locations in the network.\nThe goal is to compute some final data at some desired rate. Different operator\ntrees may share common subtrees. Therefore, it may be possible to reuse some\nintermediate results in different application trees. The first contribution of\nthis work is to provide complexity results for different instances of the basic\nproblem, as well as integer linear program formulations of various problem\ninstances. The second second contribution is the design of several\npolynomial-time heuristics. One of the primary objectives of these heuristics\nis to reuse intermediate results shared by multiple applications. Our\nquantitative comparisons of these heuristics in simulation demonstrates the\nimportance of choosing appropriate processors for operator mapping. It also\nallow us to identify a heuristic that achieves good results in practice."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0903.0730v1", 
    "title": "Grid Technologies", 
    "arxiv-id": "0903.0730v1", 
    "author": "Delia Sabina Stinga", 
    "publish": "2009-03-04T11:08:20Z", 
    "summary": "This paper contains the most important aspects of computing grids. Grid\ncomputing allows high performance distributed systems to act as a single\ncomputer. An overview of grids structure and techniques is given in order to\nunderstand the way grids work."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2009.06.016", 
    "link": "http://arxiv.org/pdf/0903.1386v1", 
    "title": "Multi-Objective Problem Solving With Offspring on Enterprise Clouds", 
    "arxiv-id": "0903.1386v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-03-08T04:37:33Z", 
    "summary": "In this paper, we present a distributed implementation of a network based\nmulti-objective evolutionary algorithm, called EMO, by using Offspring. Network\nbased evolutionary algorithms have proven to be effective for multi-objective\nproblem solving. They feature a network of connections between individuals that\ndrives the evolution of the algorithm. Unfortunately, they require large\npopulations to be effective and a distributed implementation can leverage the\ncomputation time. Most of the existing frameworks are limited to providing\nsolutions that are basic or specific to a given algorithm. Our Offspring\nframework is a plug-in based software environment that allows rapid deployment\nand execution of evolutionary algorithms on distributed computing environments\nsuch as Enterprise Clouds. Its features and benefits are presented by\ndescribing the distributed implementation of EMO."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.1388v1", 
    "title": "Jeeva: Enterprise Grid-enabled Web Portal for Protein Secondary   Structure Prediction", 
    "arxiv-id": "0903.1388v1", 
    "author": "Marimuthu Palaniswami", 
    "publish": "2009-03-08T04:50:53Z", 
    "summary": "This paper presents a Grid portal for protein secondary structure prediction\ndeveloped by using services of Aneka, a .NET-based enterprise Grid technology.\nThe portal is used by research scientists to discover new prediction structures\nin a parallel manner. An SVM (Support Vector Machine)-based prediction\nalgorithm is used with 64 sample protein sequences as a case study to\ndemonstrate the potential of enterprise Grids."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.3462v1", 
    "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3 (Extended   Version)", 
    "arxiv-id": "0903.3462v1", 
    "author": "Luigi Santocanale", 
    "publish": "2009-03-20T07:54:05Z", 
    "summary": "We address the problem of finding nice labellings for event structures of\ndegree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0903.4961v2", 
    "title": "Global Clock, Physical Time Order and Pending Period Analysis in   Multiprocessor Systems", 
    "arxiv-id": "0903.4961v2", 
    "author": "Weiwu Hu", 
    "publish": "2009-03-29T10:34:34Z", 
    "summary": "In multiprocessor systems, various problems are treated with Lamport's\nlogical clock and the resultant logical time orders between operations.\nHowever, one often needs to face the high complexities caused by the lack of\nlogical time order information in practice. In this paper, we utilize the\n\\emph{global clock} to infuse the so-called \\emph{pending period} to each\noperation in a multiprocessor system, where the pending period is a time\ninterval that contains the performed time of the operation. Further, we define\nthe \\emph{physical time order} for any two operations with disjoint pending\nperiods. The physical time order is obeyed by any real execution in\nmultiprocessor systems due to that it is part of the truly happened operation\norders restricted by global clock, and it is then proven to be independent and\nconsistent with traditional logical time orders. The above novel yet\nfundamental concepts enables new effective approaches for analyzing\nmultiprocessor systems, which are named \\emph{pending period analysis} as a\nwhole. As a consequence of pending period analysis, many important problems of\nmultiprocessor systems can be tackled effectively. As a significant application\nexample, complete memory consistency verification, which was known as an\nNP-hard problem, can be solved with the complexity of $O(n^2)$ (where $n$ is\nthe number of operations). Moreover, the two event ordering problems, which\nwere proven to be Co-NP-Hard and NP-hard respectively, can both be solved with\nthe time complexity of O(n) if restricted by pending period information."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0904.4181v1", 
    "title": "Java Technology : a Strategic Solution for Interactive Distributed   Applications", 
    "arxiv-id": "0904.4181v1", 
    "author": "Michel Salomon", 
    "publish": "2009-04-27T15:16:32Z", 
    "summary": "In a world demanding the best performance from financial investments,\ndistributed applications occupy the first place among the proposed solutions.\nThis particularity is due to their distributed architecture which is able to\nacheives high performance. Currently, many research works aim to develop tools\nthat facilitate the implementation of such applications. The urgent need for\nsuch applications in all areas pushes researchers to accelerate this process.\nHowever, the lack of standardization results in the absence of strategic\ndecisions taken by computer science community. In this article, we argue that\nJava technology represents an elegant compromise ahead of the list of the\ncurrently available solutions. In fact, by promoting the independence of\nhardware and software, Java technology makes it possible to overcome pitfalls\nthat are inherent to the creation of distributed applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0905.1113v1", 
    "title": "BlobSeer: How to Enable Efficient Versioning for Large Object Storage   under Heavy Access Concurrency", 
    "arxiv-id": "0905.1113v1", 
    "author": "Luc Boug\u00e9", 
    "publish": "2009-05-07T19:37:48Z", 
    "summary": "To accommodate the needs of large-scale distributed P2P systems, scalable\ndata management strategies are required, allowing applications to efficiently\ncope with continuously growing, highly dis tributed data. This paper addresses\nthe problem of efficiently stor ing and accessing very large binary data\nobjects (blobs). It proposesan efficient versioning scheme allowing a large\nnumber of clients to concurrently read, write and append data to huge blobs\nthat are fragmented and distributed at a very large scale. Scalability under\nheavy concurrency is achieved thanks to an original metadata scheme, based on a\ndistributed segment tree built on top of a Distributed Hash Table (DHT). Our\napproach has been implemented and experimented within our BlobSeer prototype on\nthe Grid'5000 testbed, using up to 175 nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0905.1786v1", 
    "title": "Une CNS pour l'acheminement de messages instantan\u00e9ment stabilisant", 
    "arxiv-id": "0905.1786v1", 
    "author": "Vincent Villain", 
    "publish": "2009-05-12T08:28:47Z", 
    "summary": "A snap-stabilizing algorithm ensures that it always behaves according to its\nspecifications whenever it starts from an arbitrary configuration. In this\npaper, we interest in the message forwarding problem in a message-switched\nnetwork. We must manage network ressources in order to deliver messages to any\nprocessor of the network. In this goal, we need information given by a routing\nalgorithm. But, due to the context of stabilization, this information can be\ninitially corrupted. It is why the existence of snap-stabilizing algorithms for\nthis task (proved in [CDV09]) implies that we can ask the system to begin\nforwarding messages even if routing tables are initially corrupted. In this\npaper, we generalize the previous result given a necessary and sufficient\ncondition to solve the forwarding problem in a snap-stabilizing way."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0905.4147v1", 
    "title": "Distributed Discovery of Large Near-Cliques", 
    "arxiv-id": "0905.4147v1", 
    "author": "Boaz Patt-Shamir", 
    "publish": "2009-05-26T09:51:35Z", 
    "summary": "Given an undirected graph and $0\\le\\epsilon\\le1$, a set of nodes is called\n$\\epsilon$-near clique if all but an $\\epsilon$ fraction of the pairs of nodes\nin the set have a link between them. In this paper we present a fast\nsynchronous network algorithm that uses small messages and finds a near-clique.\nSpecifically, we present a constant-time algorithm that finds, with constant\nprobability of success, a linear size $\\epsilon$-near clique if there exists an\n$\\epsilon^3$-near clique of linear size in the graph. The algorithm uses\nmessages of $O(\\log n)$ bits. The failure probability can be reduced to\n$n^{-\\Omega(1)}$ in $O(\\log n)$ time, and the algorithm also works if the graph\ncontains a clique of size $\\Omega(n/\\log^{\\alpha}\\log n)$ for some $\\alpha \\in\n(0,1)$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0906.0281v1", 
    "title": "Microcontroller based distributed and networked control system for   public cluster", 
    "arxiv-id": "0906.0281v1", 
    "author": "L. T. Handoko", 
    "publish": "2009-06-01T13:25:21Z", 
    "summary": "We present the architecture and application of the distributed control in\npublic cluster, a parallel machine which is open for public access. Following\nthe nature of public cluster, the integrated distributed control system is\nfully accessible through network using a user-friendly web interface. The\nsystem is intended mainly to control the power of each node in a block of\nparallel computers provided to certain users. This is especially important to\nextend the life-time of related hardwares, and to reduce the whole running and\nmaintainance costs. The system consists of two parts : the master- and\nnode-controllers, and both are connected each other through RS-485 interface.\nEach node-controller is assigned with a unique address to distinguish each of\nthem. We also discuss briefly the implementation of the system at the LIPI\nPublic Cluster."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0906.1328v1", 
    "title": "Multidimensional Analysis of System Logs in Large-scale Cluster Systems", 
    "arxiv-id": "0906.1328v1", 
    "author": "Dan Meng", 
    "publish": "2009-06-07T06:03:14Z", 
    "summary": "It is effective to improve the reliability and availability of large-scale\ncluster systems through the analysis of failures. Existed failure analysis\nmethods understand and analyze failures from one or few dimension. The analysis\nresults are partial and with less precision because of the limitation of data\nsource. This paper presents multidimensional analysis based on graph mining to\nanalyze multi-source system logs, which is a promising failure analysis method\nto get more complete and precise failure knowledge."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0906.1346v6", 
    "title": "Phoenix Cloud: Consolidating Different Computing Loads on Shared Cluster   System for Large Organization", 
    "arxiv-id": "0906.1346v6", 
    "author": "Dan Meng", 
    "publish": "2009-06-07T10:15:28Z", 
    "summary": "Different departments of a large organization often run dedicated cluster\nsystems for different computing loads, like HPC (high performance computing)\njobs or Web service applications. In this paper, we have designed and\nimplemented a cloud management system software Phoenix Cloud to consolidate\nheterogeneous workloads from different departments affiliated to the same\norganization on the shared cluster system. We have also proposed cooperative\nresource provisioning and management policies for a large organization and its\naffiliated departments, running HPC jobs and Web service applications, to share\nthe consolidated cluster system. The experiments show that in comparison with\nthe case that each department operates its dedicated cluster system, Phoenix\nCloud significantly decreases the scale of the required cluster system for a\nlarge organization, improves the benefit of the scientific computing\ndepartment, and at the same time provisions enough resources to the other\ndepartment running Web services with varying loads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0906.1947v1", 
    "title": "Ideal Stabilization", 
    "arxiv-id": "0906.1947v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2009-06-10T14:31:18Z", 
    "summary": "We define and explore the concept of ideal stabilization. The program is\nideally stabilizing if its every state is legitimate. Ideal stabilization\nallows the specification designer to prescribe with arbitrary degree of\nprecision not only the fault-free program behavior but also its recovery\noperation. Specifications may or may not mention all possible states. We\nidentify approaches to designing ideal stabilization to both kinds of\nspecifications. For the first kind, we state the necessary condition for an\nideally stabilizing solution. On the basis of this condition we prove that\nthere is no ideally stabilizing solution to the leader election problem. We\nillustrate the utility of the concept by providing examples of well-known\nprograms and proving them ideally stabilizing. Specifically, we prove ideal\nstabilization of the conflict manager, the alternator, the propagation of\ninformation with feedback and the alternating bit protocol."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ADCOM.2008.4760440", 
    "link": "http://arxiv.org/pdf/0906.2143v1", 
    "title": "Dependable Distributed Computing for the International Telecommunication   Union Regional Radio Conference RRC06", 
    "arxiv-id": "0906.2143v1", 
    "author": "A. Muraru", 
    "publish": "2009-06-11T16:04:12Z", 
    "summary": "The International Telecommunication Union (ITU) Regional Radio Conference\n(RRC06) established in 2006 a new frequency plan for the introduction of\ndigital broadcasting in European, African, Arab, CIS countries and Iran. The\npreparation of the plan involved complex calculations under short deadline and\nrequired dependable and efficient computing capability. The ITU designed and\ndeployed in-situ a dedicated PC farm, in parallel to the European Organization\nfor Nuclear Research (CERN) which provided and supported a system based on the\nEGEE Grid. The planning cycle at the RRC06 required a periodic execution in the\norder of 200,000 short jobs, using several hundreds of CPU hours, in a period\nof less than 12 hours. The nature of the problem required dynamic\nworkload-balancing and low-latency access to the computing resources. We\npresent the strategy and key technical choices that delivered a reliable\nservice to the RRC06."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062069", 
    "link": "http://arxiv.org/pdf/0906.2914v2", 
    "title": "Efficient Multi-site Data Movement Using Constraint Programming for Data   Hungry Science", 
    "arxiv-id": "0906.2914v2", 
    "author": "Michal \u0160umbera", 
    "publish": "2009-06-16T12:33:25Z", 
    "summary": "For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data\nsets that have been increasing in size as a function of time. In order to\noptimize all available resources (geographically spread) and minimize the\nprocessing time, it is necessary to face also the question of efficient data\ntransfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to\nthe truly distributed task scheduling we present the technique using a\nConstraint Programming (CP) approach. The CP technique schedules data transfers\nfrom multiple resources considering all available paths of diverse\ncharacteristic (capacity, sharing and storage) having minimum user's waiting\ntime as an objective. We introduce a model for planning data transfers to a\nsingle destination (data transfer) as well as its extension for an optimal data\nset spreading strategy (data placement). Several enhancements for a solver of\nthe CP model will be shown, leading to a faster schedule computation time using\nsymmetry breaking, branch cutting, well studied principles from job-shop\nscheduling field and several heuristics. Finally, we will present the design\nand implementation of a corner-stone application aimed at moving datasets\naccording to the schedule. Results will include comparison of performance and\ntrade-off between CP techniques and a Peer-2-Peer model from simulation\nframework as well as the real case scenario taken from a practical usage of a\nCP scheduler."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062069", 
    "link": "http://arxiv.org/pdf/0906.3424v1", 
    "title": "Decentralized Traffic Management Strategies for Sensor-Enabled Cars", 
    "arxiv-id": "0906.3424v1", 
    "author": "Kotagiri Ramamohanarao", 
    "publish": "2009-06-18T12:16:06Z", 
    "summary": "Traffic Congestions and accidents are major concerns in today's\ntransportation systems. This thesis investigates how to optimize traffic flow\non highways, in particular for merging situations such as intersections where a\nramp leads onto the highway. In our work, cars are equipped with sensors that\ncan detect distance to neighboring cars, and communicate their velocity and\nacceleration readings with one another. Sensor-enabled cars can locally\nexchange sensed information about the traffic and adapt their behavior much\nearlier than regular cars.\n  We propose proactive algorithms for merging different streams of\nsensor-enabled cars into a single stream. A proactive merging algorithm\ndecouples the decision point from the actual merging point. Sensor-enabled cars\nallow us to decide where and when a car merges before it arrives at the actual\nmerging point. This leads to a significant improvement in traffic flow as\nvelocities can be adjusted appropriately. We compare proactive merging\nalgorithms against the conventional priority-based merging algorithm in a\ncontrolled simulation environment. Experiment results show that proactive\nmerging algorithms outperform the priority-based merging algorithm in terms of\nflow and delay."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0906.4302v1", 
    "title": "A Peer to Peer Protocol for Online Dispute Resolution over Storage   Consumption", 
    "arxiv-id": "0906.4302v1", 
    "author": "Carlos Molina-Jimenez", 
    "publish": "2009-06-23T16:30:51Z", 
    "summary": "In bilateral accounting of resource consumption both the consumer and\nprovider independently measure the amount of resources consumed by the\nconsumer. The problem here is that potential disparities between the provider's\nand consumer's accountings, might lead to conflicts between the two parties\nthat need to be resolved. We argue that with the proper mechanisms available,\nmost of these conflicts can be solved online, as opposite to in court\nresolution; the design of such mechanisms is still a research topic; to help\ncover the gap, in this paper we propose a peer--to--peer protocol for online\ndispute resolution over storage consumption. The protocol is peer--to--peer and\ntakes into consideration the possible causes (e.g, transmission delays,\nunsynchronized metric collectors, etc.) of the disparity between the provider's\nand consumer's accountings to make, if possible, the two results converge."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0906.4680v1", 
    "title": "Reconfiguration of Distributed Information Fusion System ? A case study", 
    "arxiv-id": "0906.4680v1", 
    "author": "Olivier Passalacqua", 
    "publish": "2009-06-25T12:35:57Z", 
    "summary": "Information Fusion Systems are now widely used in different fusion contexts,\nlike scientific processing, sensor networks, video and image processing. One of\nthe current trends in this area is to cope with distributed systems. In this\ncontext, we have defined and implemented a Dynamic Distributed Information\nFusion System runtime model. It allows us to cope with dynamic execution\nsupports while trying to maintain the functionalities of a given Dynamic\nDistributed Information Fusion System. The paper presents our system, the\nreconfiguration problems we are faced with and our solutions."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.2.1", 
    "link": "http://arxiv.org/pdf/0907.0929v1", 
    "title": "CRDTs: Consistency without concurrency control", 
    "arxiv-id": "0907.0929v1", 
    "author": "Marc Shapiro", 
    "publish": "2009-07-06T08:01:05Z", 
    "summary": "A CRDT is a data type whose operations commute when they are concurrent.\nReplicas of a CRDT eventually converge without any complex concurrency control.\nAs an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer\ncalled Treedoc. We outline the design, implementation and performance of\nTreedoc. We discuss how the CRDT concept can be generalised, and its\nlimitations."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.1375v1", 
    "title": "PT-Scotch: A tool for efficient parallel graph ordering", 
    "arxiv-id": "0907.1375v1", 
    "author": "Fran\u00e7ois Pellegrini", 
    "publish": "2009-07-08T15:11:00Z", 
    "summary": "The parallel ordering of large graphs is a difficult problem, because on the\none hand minimum degree algorithms do not parallelize well, and on the other\nhand the obtainment of high quality orderings with the nested dissection\nalgorithm requires efficient graph bipartitioning heuristics, the best\nsequential implementations of which are also hard to parallelize. This paper\npresents a set of algorithms, implemented in the PT-Scotch software package,\nwhich allows one to order large graphs in parallel, yielding orderings the\nquality of which is only slightly worse than the one of state-of-the-art\nsequential algorithms. Our implementation uses the classical nested dissection\napproach but relies on several novel features to solve the parallel graph\nbipartitioning problem. Thanks to these improvements, PT-Scotch produces\nconsistently better orderings than ParMeTiS on large numbers of processors."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.2949v2", 
    "title": "Distributed anonymous function computation in information fusion and   multiagent systems", 
    "arxiv-id": "0907.2949v2", 
    "author": "John N. Tsitsiklis", 
    "publish": "2009-07-16T22:42:40Z", 
    "summary": "We propose a model for deterministic distributed function computation by a\nnetwork of identical and anonymous nodes, with bounded computation and storage\ncapabilities that do not scale with the network size. Our goal is to\ncharacterize the class of functions that can be computed within this model. In\nour main result, we exhibit a class of non-computable functions, and prove that\nevery function outside this class can at least be approximated. The problem of\ncomputing averages in a distributed manner plays a central role in our\ndevelopment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.3118v1", 
    "title": "On the Convergence of Population Protocols When Population Goes to   Infinity", 
    "arxiv-id": "0907.3118v1", 
    "author": "Xavier Koegler", 
    "publish": "2009-07-17T17:22:12Z", 
    "summary": "Population protocols have been introduced as a model of sensor networks\nconsisting of very limited mobile agents with no control over their own\nmovement. A population protocol corresponds to a collection of anonymous\nagents, modeled by finite automata, that interact with one another to carry out\ncomputations, by updating their states, using some rules. Their computational\npower has been investigated under several hypotheses but always when restricted\nto finite size populations. In particular, predicates stably computable in the\noriginal model have been characterized as those definable in Presburger\narithmetic. We study mathematically the convergence of population protocols\nwhen the size of the population goes to infinity. We do so by giving general\nresults, that we illustrate through the example of a particular population\nprotocol for which we even obtain an asymptotic development. This example shows\nin particular that these protocols seem to have a rather different\ncomputational power when a huge population hypothesis is considered."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0907.4810v1", 
    "title": "The Open Cloud Testbed: A Wide Area Testbed for Cloud Computing   Utilizing High Performance Network Services", 
    "arxiv-id": "0907.4810v1", 
    "author": "Joe Mambratti", 
    "publish": "2009-07-28T00:54:23Z", 
    "summary": "Recently, a number of cloud platforms and services have been developed for\ndata intensive computing, including Hadoop, Sector, CloudStore (formerly KFS),\nHBase, and Thrift. In order to benchmark the performance of these systems, to\ninvestigate their interoperability, and to experiment with new services based\non flexible compute node and network provisioning capabilities, we have\ndesigned and implemented a large scale testbed called the Open Cloud Testbed\n(OCT). Currently the OCT has 120 nodes in four data centers: Baltimore, Chicago\n(two locations), and San Diego. In contrast to other cloud testbeds, which are\nin small geographic areas and which are based on commodity Internet services,\nthe OCT is a wide area testbed and the four data centers are connected with a\nhigh performance 10Gb/s network, based on a foundation of dedicated lightpaths.\nThis testbed can address the requirements of extremely large data streams that\nchallenge other types of distributed infrastructure. We have also developed\nseveral utilities to support the development of cloud computing systems and\nservices, including novel node and network provisioning services, a monitoring\nsystem, and a RPC system. In this paper, we describe the OCT architecture and\nmonitoring system. We also describe some benchmarks that we developed and some\ninteroperability studies we performed using these benchmarks."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2007.12.001", 
    "link": "http://arxiv.org/pdf/0908.0160v1", 
    "title": "Self-stabilizing Byzantine Agreement", 
    "arxiv-id": "0908.0160v1", 
    "author": "Danny Dolev", 
    "publish": "2009-08-02T21:09:20Z", 
    "summary": "Byzantine agreement algorithms typically assume implicit initial state\nconsistency and synchronization among the correct nodes and then operate in\ncoordinated rounds of information exchange to reach agreement based on the\ninput values. The implicit initial assumptions enable correct nodes to infer\nabout the progression of the algorithm at other nodes from their local state.\nThis paper considers a more severe fault model than permanent Byzantine\nfailures, one in which the system can in addition be subject to severe\ntransient failures that can temporarily throw the system out of its assumption\nboundaries. When the system eventually returns to behave according to the\npresumed assumptions it may be in an arbitrary state in which any\nsynchronization among the nodes might be lost, and each node may be at an\narbitrary state. We present a self-stabilizing Byzantine agreement algorithm\nthat reaches agreement among the correct nodes in an optimal ration of faulty\nto correct, by using only the assumption of eventually bounded message\ntransmission delay. In the process of solving the problem, two additional\nimportant and challenging building blocks were developed: a unique\nself-stabilizing protocol for assigning consistent relative times to protocol\ninitialization and a Reliable Broadcast primitive that progresses at the speed\nof actual message delivery time."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_25", 
    "link": "http://arxiv.org/pdf/0908.1797v2", 
    "title": "Separation of Circulating Tokens", 
    "arxiv-id": "0908.1797v2", 
    "author": "Ted Herman", 
    "publish": "2009-08-12T20:52:52Z", 
    "summary": "Self-stabilizing distributed control is often modeled by token abstractions.\nA system with a single token may implement mutual exclusion; a system with\nmultiple tokens may ensure that immediate neighbors do not simultaneously enjoy\na privilege. For a cyber-physical system, tokens may represent physical objects\nwhose movement is controlled. The problem studied in this paper is to ensure\nthat a synchronous system with m circulating tokens has at least d distance\nbetween tokens. This problem is first considered in a ring where d is given\nwhilst m and the ring size n are unknown. The protocol solving this problem can\nbe uniform, with all processes running the same program, or it can be\nnon-uniform, with some processes acting only as token relays. The protocol for\nthis first problem is simple, and can be expressed with Petri net formalism. A\nsecond problem is to maximize d when m is given, and n is unknown. For the\nsecond problem, the paper presents a non-uniform protocol with a single\ncorrective process."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_25", 
    "link": "http://arxiv.org/pdf/0908.2222v1", 
    "title": "Transaction-Oriented Simulation In Ad Hoc Grids: Design and Experience", 
    "arxiv-id": "0908.2222v1", 
    "author": "Vladimir Getov", 
    "publish": "2009-08-16T08:30:48Z", 
    "summary": "In this paper we analyse the requirements of performing parallel\ntransaction-oriented simulations within loosely coupled systems like ad hoc\ngrids. We focus especially on the space-parallel approach to parallel\nsimulation and on discrete event synchronisation algorithms that are suitable\nfor transaction-oriented simulation and the target environment of ad hoc grids.\nTo demonstrate our findings, a Java-based parallel simulator for the\ntransaction-oriented language GPSS/H is implemented on the basis of the most\npromising shock-resistant Time Warp (SRTW) synchronisation algorithm and using\nthe grid framework ProActive. The analysis of our parallel simulator, based on\nexperiments using the Grid5000 platform, shows that the SRTW algorithm can\nsuccessfully reduce the number of rolled back transaction moves but it also\nreveals circumstances in which the SRTW algorithm can be outperformed by the\nnormal Time Warp algorithm. Finally, possible improvements to the SRTW\nalgorithm are proposed in order to avoid such problems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0908.2295v1", 
    "title": "An Optimal Self-Stabilizing Firing Squad", 
    "arxiv-id": "0908.2295v1", 
    "author": "Yoram Moses", 
    "publish": "2009-08-17T07:39:06Z", 
    "summary": "Consider a fully connected network where up to $t$ processes may crash, and\nall processes start in an arbitrary memory state. The self-stabilizing firing\nsquad problem consists of eventually guaranteeing simultaneous response to an\nexternal input. This is modeled by requiring that the non-crashed processes\n\"fire\" simultaneously if some correct process received an external \"GO\" input,\nand that they only fire as a response to some process receiving such an input.\nThis paper presents FireAlg, the first self-stabilizing firing squad algorithm.\n  The FireAlg algorithm is optimal in two respects: (a) Once the algorithm is\nin a safe state, it fires in response to a GO input as fast as any other\nalgorithm does, and (b) Starting from an arbitrary state, it converges to a\nsafe state as fast as any other algorithm does."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0908.2958v1", 
    "title": "Design and Implementation of a Distributed Middleware for Parallel   Execution of Legacy Enterprise Applications", 
    "arxiv-id": "0908.2958v1", 
    "author": "Que Thu Dung Nguyen", 
    "publish": "2009-08-20T16:26:21Z", 
    "summary": "A typical enterprise uses a local area network of computers to perform its\nbusiness. During the off-working hours, the computational capacities of these\nnetworked computers are underused or unused. In order to utilize this\ncomputational capacity an application has to be recoded to exploit concurrency\ninherent in a computation which is clearly not possible for legacy applications\nwithout any source code. This thesis presents the design an implementation of a\ndistributed middleware which can automatically execute a legacy application on\nmultiple networked computers by parallelizing it. This middleware runs multiple\ncopies of the binary executable code in parallel on different hosts in the\nnetwork. It wraps up the binary executable code of the legacy application in\norder to capture the kernel level data access system calls and perform them\ndistributively over multiple computers in a safe and conflict free manner. The\nmiddleware also incorporates a dynamic scheduling technique to execute the\ntarget application in minimum time by scavenging the available CPU cycles of\nthe hosts in the network. This dynamic scheduling also supports the CPU\navailability of the hosts to change over time and properly reschedule the\nreplicas performing the computation to minimize the execution time. A prototype\nimplementation of this middleware has been developed as a proof of concept of\nthe design. This implementation has been evaluated with a few typical case\nstudies and the test results confirm that the middleware works as expected."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-05118-0_20", 
    "link": "http://arxiv.org/pdf/0909.1146v1", 
    "title": "Energy-Efficient Scheduling of HPC Applications in Cloud Computing   Environments", 
    "arxiv-id": "0909.1146v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-09-07T06:13:40Z", 
    "summary": "The use of High Performance Computing (HPC) in commercial and consumer IT\napplications is becoming popular. They need the ability to gain rapid and\nscalable access to high-end computing capabilities. Cloud computing promises to\ndeliver such a computing infrastructure using data centers so that HPC users\ncan access applications and data from a Cloud anywhere in the world on demand\nand pay based on what they use. However, the growing demand drastically\nincreases the energy consumption of data centers, which has become a critical\nissue. High energy consumption not only translates to high energy cost, which\nwill reduce the profit margin of Cloud providers, but also high carbon\nemissions which is not environmentally sustainable. Hence, energy-efficient\nsolutions are required that can address the high increase in the energy\nconsumption from the perspective of not only Cloud provider but also from the\nenvironment. To address this issue we propose near-optimal scheduling policies\nthat exploits heterogeneity across multiple data centers for a Cloud provider.\nWe consider a number of energy efficiency factors such as energy cost, carbon\nemission rate, workload, and CPU power efficiency which changes across\ndifferent data center depending on their location, architectural design, and\nmanagement system. Our carbon/energy based scheduling policies are able to\nachieve on average up to 30% of energy savings in comparison to profit based\nscheduling policies leading to higher profit and less carbon emissions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-1-4419-6794-7_8", 
    "link": "http://arxiv.org/pdf/0909.1517v1", 
    "title": "Autonomic management of multiple non-functional concerns in behavioural   skeletons", 
    "arxiv-id": "0909.1517v1", 
    "author": "Peter Kilpatrick", 
    "publish": "2009-09-08T17:02:58Z", 
    "summary": "We introduce and address the problem of concurrent autonomic management of\ndifferent non-functional concerns in parallel applications build as a\nhierarchical composition of behavioural skeletons. We first define the problems\narising when multiple concerns are dealt with by independent managers, then we\npropose a methodology supporting coordinated management, and finally we discuss\nhow autonomic management of multiple concerns may be implemented in a typical\nuse case. The paper concludes with an outline of the challenges involved in\nrealizing the proposed methodology on distributed target architectures such as\nclusters and grids. Being based on the behavioural skeleton concept proposed in\nthe CoreGRID GCM, it is anticipated that the methodology will be readily\nintegrated into the current reference implementation of GCM based on Java\nProActive and running on top of major grid middleware systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-1-4419-6794-7_8", 
    "link": "http://arxiv.org/pdf/0909.1788v1", 
    "title": "Building on Quicksand", 
    "arxiv-id": "0909.1788v1", 
    "author": "David Campbell", 
    "publish": "2009-09-09T18:10:57Z", 
    "summary": "Reliable systems have always been built out of unreliable components. Early\non, the reliable components were small such as mirrored disks or ECC (Error\nCorrecting Codes) in core memory. These systems were designed such that\nfailures of these small components were transparent to the application. Later,\nthe size of the unreliable components grew larger and semantic challenges crept\ninto the application when failures occurred.\n  As the granularity of the unreliable component grows, the latency to\ncommunicate with a backup becomes unpalatable. This leads to a more relaxed\nmodel for fault tolerance. The primary system will acknowledge the work request\nand its actions without waiting to ensure that the backup is notified of the\nwork. This improves the responsiveness of the system.\n  There are two implications of asynchronous state capture: 1) Everything\npromised by the primary is probabilistic. There is always a chance that an\nuntimely failure shortly after the promise results in a backup proceeding\nwithout knowledge of the commitment. Hence, nothing is guaranteed! 2)\nApplications must ensure eventual consistency. Since work may be stuck in the\nprimary after a failure and reappear later, the processing order for work\ncannot be guaranteed.\n  Platform designers are struggling to make this easier for their applications.\nEmerging patterns of eventual consistency and probabilistic execution may soon\nyield a way for applications to express requirements for a \"looser\" form of\nconsistency while providing availability in the face of ever larger failures.\n  This paper recounts portions of the evolution of these trends, attempts to\nshow the patterns that span these changes, and talks about future directions as\nwe continue to \"build on quicksand\"."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/0909.5177v3", 
    "title": "Transform-based Distributed Data Gathering", 
    "arxiv-id": "0909.5177v3", 
    "author": "Antonio Ortega", 
    "publish": "2009-09-28T19:57:13Z", 
    "summary": "A general class of unidirectional transforms is presented that can be\ncomputed in a distributed manner along an arbitrary routing tree. Additionally,\nwe provide a set of conditions under which these transforms are invertible.\nThese transforms can be computed as data is routed towards the collection (or\nsink) node in the tree and exploit data correlation between nodes in the tree.\nMoreover, when used in wireless sensor networks, these transforms can also\nleverage data received at nodes via broadcast wireless communications. Various\nconstructions of unidirectional transforms are also provided for use in data\ngathering in wireless sensor networks. New wavelet transforms are also proposed\nwhich provide significant improvements over existing unidirectional transforms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/0910.0317v1", 
    "title": "A New Fuzzy Approach for Dynamic Load Balancing Algorithm", 
    "arxiv-id": "0910.0317v1", 
    "author": "M. Iqbal b. Saripan", 
    "publish": "2009-10-02T03:32:09Z", 
    "summary": "Load balancing is the process of improving the Performance of a parallel and\ndistributed system through is distribution of load among the processors [1-2].\nMost of the previous work in load balancing and distributed decision making in\ngeneral, do not effectively take into account the uncertainty and inconsistency\nin state information but in fuzzy logic, we have advantage of using crisps\ninputs. In this paper, we present a new approach for implementing dynamic load\nbalancing algorithm with fuzzy logic, which can face to uncertainty and\ninconsistency of previous algorithms, further more our algorithm shows better\nresponse time than round robin and randomize algorithm respectively 30.84\npercent and 45.45 percent."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2047640", 
    "link": "http://arxiv.org/pdf/0910.1852v1", 
    "title": "DAMQ-Based Schemes for Efficiently Using the Buffer Spaces of a NoC   Router", 
    "arxiv-id": "0910.1852v1", 
    "author": "Ahmad Khademzadeh", 
    "publish": "2009-10-09T20:24:57Z", 
    "summary": "In this paper we present high performance dynamically allocated multi-queue\n(DAMQ) buffer schemes for fault tolerance systems on chip applications that\nrequire an interconnection network. Two or four virtual channels shared the\nsame buffer space. On the message switching layer, we make improvement to boost\nsystem performance when there are faults involved in the components\ncommunication. The proposed schemes are when a node or a physical channel is\ndeemed as faulty, the previous hop node will terminate the buffer occupancy of\nmessages destined to the failed link. The buffer usage decisions are made at\nswitching layer without interactions with higher abstract layer, thus buffer\nspace will be released to messages destined to other healthy nodes quickly.\nTherefore, the buffer space will be efficiently used in case fault occurs at\nsome nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_4", 
    "link": "http://arxiv.org/pdf/0910.1974v1", 
    "title": "Cloudbus Toolkit for Market-Oriented Cloud Computing", 
    "arxiv-id": "0910.1974v1", 
    "author": "Christian Vecchiola", 
    "publish": "2009-10-11T06:26:29Z", 
    "summary": "This keynote paper: (1) presents the 21st century vision of computing and\nidentifies various IT paradigms promising to deliver computing as a utility;\n(2) defines the architecture for creating market-oriented Clouds and computing\natmosphere by leveraging technologies such as virtual machines; (3) provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; (4) presents the work carried out as part of\nour new Cloud Computing initiative, called Cloudbus: (i) Aneka, a Platform as a\nService software system containing SDK (Software Development Kit) for\nconstruction of Cloud applications and deployment on private or public Clouds,\nin addition to supporting market-oriented resource management; (ii)\ninternetworking of Clouds for dynamic creation of federated computing\nenvironments for scaling of elastic applications; (iii) creation of 3rd party\nCloud brokering services for building content delivery networks and e-Science\napplications and their deployment on capabilities of IaaS providers such as\nAmazon along with Grid mashups; (iv) CloudSim supporting modelling and\nsimulation of Clouds for performance studies; (v) Energy Efficient Resource\nAllocation Mechanisms and Techniques for creation and management of Green\nClouds; and (vi) pathways for future research."
},{
    "category": "cs.DC", 
    "doi": "10.1109/I-SPAN.2009.150", 
    "link": "http://arxiv.org/pdf/0910.1979v1", 
    "title": "High-Performance Cloud Computing: A View of Scientific Applications", 
    "arxiv-id": "0910.1979v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2009-10-11T07:42:44Z", 
    "summary": "Scientific computing often requires the availability of a massive number of\ncomputers for performing large scale experiments. Traditionally, these needs\nhave been addressed by using high-performance computing solutions and installed\nfacilities such as clusters and super computers, which are difficult to setup,\nmaintain, and operate. Cloud computing provides scientists with a completely\nnew model of utilizing the computing infrastructure. Compute resources, storage\nresources, as well as applications, can be dynamically provisioned (and\nintegrated within the existing infrastructure) on a pay per use basis. These\nresources can be released when they are no more needed. Such services are often\noffered within the context of a Service Level Agreement (SLA), which ensure the\ndesired Quality of Service (QoS). Aneka, an enterprise Cloud computing\nsolution, harnesses the power of compute resources by relying on private and\npublic Clouds and delivers to users the desired QoS. Its flexible and service\nbased infrastructure supports multiple programming paradigms that make Aneka\naddress a variety of different scenarios: from finance applications to\ncomputational science. As examples of scientific computing in the Cloud, we\npresent a preliminary case study on using Aneka for the classification of gene\nexpression data and the execution of fMRI brain imaging workflow."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/5/052014", 
    "link": "http://arxiv.org/pdf/0910.4507v1", 
    "title": "ScotGrid: Providing an Effective Distributed Tier-2 in the LHC Era", 
    "arxiv-id": "0910.4507v1", 
    "author": "Graeme Stewart", 
    "publish": "2009-10-23T13:02:19Z", 
    "summary": "ScotGrid is a distributed Tier-2 centre in the UK with sites in Durham,\nEdinburgh and Glasgow. ScotGrid has undergone a huge expansion in hardware in\nanticipation of the LHC and now provides more than 4MSI2K and 500TB to the LHC\nVOs. Scaling up to this level of provision has brought many challenges to the\nTier-2 and we show in this paper how we have adopted new methods of organising\nthe centres, from fabric management and monitoring to remote management of\nsites to management and operational procedures, to meet these challenges. We\ndescribe how we have coped with different operational models at the sites,\nwhere Glagsow and Durham sites are managed \"in house\" but resources at\nEdinburgh are managed as a central university resource. This required the\nadoption of a different fabric management model at Edinburgh and a special\nengagement with the cluster managers. Challenges arose from the different job\nmodels of local and grid submission that required special attention to resolve.\nWe show how ScotGrid has successfully provided an infrastructure for ATLAS and\nLHCb Monte Carlo production. Special attention has been paid to ensuring that\nuser analysis functions efficiently, which has required optimisation of local\nstorage and networking to cope with the demands of user analysis. Finally,\nalthough these Tier-2 resources are pledged to the whole VO, we have\nestablished close links with our local physics user communities as being the\nbest way to ensure that the Tier-2 functions effectively as a part of the LHC\ngrid computing framework.."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/6/062066", 
    "link": "http://arxiv.org/pdf/0910.4510v1", 
    "title": "Optimised access to user analysis data using the gLite DPM", 
    "arxiv-id": "0910.4510v1", 
    "author": "Graeme Stewart", 
    "publish": "2009-10-23T13:29:44Z", 
    "summary": "The ScotGrid distributed Tier-2 now provides more that 4MSI2K and 500TB for\nLHC computing, which is spread across three sites at Durham, Edinburgh and\nGlasgow. Tier-2 sites have a dual role to play in the computing models of the\nLHC VOs. Firstly, their CPU resources are used for the generation of Monte\nCarlo event data. Secondly, the end user analysis data is distributed across\nthe grid to the site's storage system and held on disk ready for processing by\nphysicists' analysis jobs. In this paper we show how we have designed the\nScotGrid storage and data management resources in order to optimise access by\nphysicists to LHC data. Within ScotGrid, all sites use the gLite DPM storage\nmanager middleware. Using the EGEE grid to submit real ATLAS analysis code to\nprocess VO data stored on the ScotGrid sites, we present an analysis of the\nperformance of the architecture at one site, and procedures that may be\nundertaken to improve such. The results will be presented from the point of\nview of the end user (in terms of number of events processed/second) and from\nthe point of view of the site, which wishes to minimise load and the impact\nthat analysis activity has on other users of the system."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_35", 
    "link": "http://arxiv.org/pdf/0910.4568v1", 
    "title": "SPECI, a simulation tool exploring cloud-scale data centres", 
    "arxiv-id": "0910.4568v1", 
    "author": "Ilango Sriram", 
    "publish": "2009-10-23T19:05:29Z", 
    "summary": "There is a rapid increase in the size of data centres (DCs) used to provide\ncloud computing services. It is commonly agreed that not all properties in the\nmiddleware that manages DCs will scale linearly with the number of components.\nFurther, \"normal failure\" complicates the assessment of the per-formance of a\nDC. However, unlike in other engineering domains, there are no well established\ntools that allow the prediction of the performance and behav-iour of future\ngenerations of DCs. SPECI, Simulation Program for Elastic Cloud\nInfrastructures, is a simulation tool which allows exploration of aspects of\nscaling as well as performance properties of future DCs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_35", 
    "link": "http://arxiv.org/pdf/0911.0910v1", 
    "title": "Domain Decomposition Based High Performance Parallel Computing", 
    "arxiv-id": "0911.0910v1", 
    "author": "Siddhartha Khaitan", 
    "publish": "2009-11-04T18:56:03Z", 
    "summary": "The study deals with the parallelization of finite element based\nNavier-Stokes codes using domain decomposition and state-ofart sparse direct\nsolvers. There has been significant improvement in the performance of sparse\ndirect solvers. Parallel sparse direct solvers are not found to exhibit good\nscalability. Hence, the parallelization of sparse direct solvers is done using\ndomain decomposition techniques. A highly efficient sparse direct solver\nPARDISO is used in this study. The scalability of both Newton and modified\nNewton algorithms are tested."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.3945v1", 
    "title": "A Semantic Grid Oriented to E-Tourism", 
    "arxiv-id": "0911.3945v1", 
    "author": "Xiao Ming Zhang", 
    "publish": "2009-11-20T01:51:30Z", 
    "summary": "With increasing complexity of tourism business models and tasks, there is a\nclear need of the next generation e-Tourism infrastructure to support flexible\nautomation, integration, computation, storage, and collaboration. Currently\nseveral enabling technologies such as semantic Web, Web service, agent and grid\ncomputing have been applied in the different e-Tourism applications, however\nthere is no a unified framework to be able to integrate all of them. So this\npaper presents a promising e-Tourism framework based on emerging semantic grid,\nin which a number of key design issues are discussed including architecture,\nontologies structure, semantic reconciliation, service and resource discovery,\nrole based authorization and intelligent agent. The paper finally provides the\nimplementation of the framework."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.5438v1", 
    "title": "Building and Installing a Hadoop/MapReduce Cluster from Commodity   Components", 
    "arxiv-id": "0911.5438v1", 
    "author": "Gary Berosik", 
    "publish": "2009-11-28T23:50:28Z", 
    "summary": "This tutorial presents a recipe for the construction of a compute cluster for\nprocessing large volumes of data, using cheap, easily available personal\ncomputer hardware (Intel/AMD based PCs) and freely available open source\nsoftware (Ubuntu Linux, Apache Hadoop)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0911.5593v1", 
    "title": "Checkpointing vs. Migration for Post-Petascale Machines", 
    "arxiv-id": "0911.5593v1", 
    "author": "Yves Robert", 
    "publish": "2009-11-30T09:39:15Z", 
    "summary": "We craft a few scenarios for the execution of sequential and parallel jobs on\nfuture generation machines. Checkpointing or migration, which technique to\nchoose?"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/0912.1835v1", 
    "title": "High Availability Cluster System for Local Disaster Recovery with Markov   Modeling Approach", 
    "arxiv-id": "0912.1835v1", 
    "author": "T. Thein", 
    "publish": "2009-12-09T18:50:52Z", 
    "summary": "The need for high availability (HA) and disaster recovery (DR) in IT\nenvironment is more stringent than most of the other sectors of enterprises.\nMany businesses require the availability of business-critical applications 24\nhours a day, seven days a week, and can afford no data loss in the event of a\ndisaster. It is vital that the IT infrastructure is resilient with regard to\ndisruption, even site failures, and that business operations can continue\nwithout significant impact. As a result, DR has gained great importance in IT.\nClustering of multiple industries standard servers together to allow workload\nsharing and fail-over capabilities is a low cost approach. In this paper, we\npresent the availability model through Semi-Markov Process (SMP) and also\nanalyze the difference in downtime of the SMP model and the approximate\nContinuous Time Markov Chain (CTMC) model. To acquire system availability, we\nperform numerical analysis and SHARPE tool evaluation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.0421v1", 
    "title": "MapReduce for Integer Factorization", 
    "arxiv-id": "1001.0421v1", 
    "author": "Javier Tordable", 
    "publish": "2010-01-04T00:15:58Z", 
    "summary": "Integer factorization is a very hard computational problem. Currently no\nefficient algorithm for integer factorization is publicly known. However, this\nis an important problem on which it relies the security of many real world\ncryptographic systems.\n  I present an implementation of a fast factorization algorithm on MapReduce.\nMapReduce is a programming model for high performance applications developed\noriginally at Google. The quadratic sieve algorithm is split into the different\nMapReduce phases and compared against a standard implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.2785v2", 
    "title": "Termination Detection of Local Computations", 
    "arxiv-id": "1001.2785v2", 
    "author": "Gerard Tel", 
    "publish": "2010-01-18T20:12:54Z", 
    "summary": "Contrary to the sequential world, the processes involved in a distributed\nsystem do not necessarily know when a computation is globally finished. This\npaper investigates the problem of the detection of the termination of local\ncomputations. We define four types of termination detection: no detection,\ndetection of the local termination, detection by a distributed observer,\ndetection of the global termination. We give a complete characterisation\n(except in the local termination detection case where a partial one is given)\nfor each of this termination detection and show that they define a strict\nhierarchy. These results emphasise the difference between computability of a\ndistributed task and termination detection. Furthermore, these\ncharacterisations encompass all standard criteria that are usually formulated :\ntopological restriction (tree, rings, or triangu- lated networks ...),\ntopological knowledge (size, diameter ...), and local knowledge to distinguish\nnodes (identities, sense of direction). These results are now presented as\ncorollaries of generalising theorems. As a very special and important case, the\ntechniques are also applied to the election problem. Though given in the model\nof local computations, these results can give qualitative insight for similar\nresults in other standard models. The necessary conditions involve graphs\ncovering and quasi-covering; the sufficient conditions (constructive local\ncomputations) are based upon an enumeration algorithm of Mazurkiewicz and a\nstable properties detection algorithm of Szymanski, Shi and Prywes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3259v1", 
    "title": "Research Agenda in Cloud Technologies", 
    "arxiv-id": "1001.3259v1", 
    "author": "Ali Khajeh-Hosseini", 
    "publish": "2010-01-19T15:53:41Z", 
    "summary": "Cloud computing is the latest effort in delivering computing resources as a\nservice. It represents a shift away from computing as a product that is\npurchased, to computing as a service that is delivered to consumers over the\ninternet from large-scale data centres - or \"clouds\". Whilst cloud computing is\ngaining growing popularity in the IT industry, academia appeared to be lagging\nbehind the rapid developments in this field. This paper is the first systematic\nreview of peer-reviewed academic research published in this field, and aims to\nprovide an overview of the swiftly developing advances in the technical\nfoundations of cloud computing and their research efforts. Structured along the\ntechnical aspects on the cloud agenda, we discuss lessons from related\ntechnologies; advances in the introduction of protocols, interfaces, and\nstandards; techniques for modelling and building clouds; and new use-cases\narising through cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3718v1", 
    "title": "Severity Prediction of Drought in A Large Geographical Area Using   Distributed Wireless Sensor Networks", 
    "arxiv-id": "1001.3718v1", 
    "author": "T. R. Gopalakrsihnan Nair", 
    "publish": "2010-01-21T04:47:58Z", 
    "summary": "In this paper, the severity prediction of drought through the implementation\nof modern sensor networks is discussed. We describe how to design a drought\nprediction system using wireless sensor networks. This paper will describe a\nterrestrial interconnected wireless sensor network paradigm for the prediction\nof severity of drought over a vast area of 10,000 sq km. The communication\narchitecture for sensor network is outlined and the protocols developed for\neach layer is explored. The data integration model and sensor data analysis at\nthe central computer is explained. The advantages and limitations are discussed\nalong with the use of wireless standards. They are analyzed for its relevance.\nFinally a conclusion is presented along with open research issues."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.3824v1", 
    "title": "Performance and Fault Tolerance in the StoreTorrent Parallel Filesystem", 
    "arxiv-id": "1001.3824v1", 
    "author": "Federico D. Sacerdoti", 
    "publish": "2010-01-21T15:17:30Z", 
    "summary": "With a goal of supporting the timely and cost-effective analysis of Terabyte\ndatasets on commodity components, we present and evaluate StoreTorrent, a\nsimple distributed filesystem with integrated fault tolerance for efficient\nhandling of small data records. Our contributions include an application-OS\npipelining technique and metadata structure to increase small write and read\nperformance by a factor of 1-10, and the use of peer-to-peer communication of\nreplica-location indexes to avoid transferring data during parallel analysis\neven in a degraded state. We evaluated StoreTorrent, PVFS, and Gluster\nfilesystems using 70 storage nodes and 560 parallel clients on an 8-core/node\nEthernet cluster with directly attached SATA disks. StoreTorrent performed\nparallel small writes at an aggregate rate of 1.69 GB/s, and supported reads\nover the network at 8.47 GB/s. We ported a parallel analysis task and\ndemonstrate it achieved parallel reads at the full aggregate speed of the\nstorage node local filesystems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1001.4197v1", 
    "title": "Optimization of Multiple Vehicle Routing Problems using Approximation   Algorithms", 
    "arxiv-id": "1001.4197v1", 
    "author": "P. Parthiban", 
    "publish": "2010-01-23T19:28:13Z", 
    "summary": "This paper deals with generating of an optimized route for multiple Vehicle\nrouting Problems (mVRP). We used a methodology of clustering the given cities\ndepending upon the number of vehicles and each cluster is allotted to a\nvehicle. k- Means clustering algorithm has been used for easy clustering of the\ncities. In this way the mVRP has been converted into VRP which is simple in\ncomputation compared to mVRP. After clustering, an optimized route is generated\nfor each vehicle in its allotted cluster. Once the clustering had been done and\nafter the cities were allocated to the various vehicles, each cluster/tour was\ntaken as an individual Vehicle Routing problem and the steps of Genetic\nAlgorithm were applied to the cluster and iterated to obtain the most optimal\nvalue of the distance after convergence takes place. After the application of\nthe various heuristic techniques, it was found that the Genetic algorithm gave\na better result and a more optimal tour for mVRPs in short computational time\nthan other Algorithms due to the extensive search and constructive nature of\nthe algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-10665-1_44", 
    "link": "http://arxiv.org/pdf/1002.0125v1", 
    "title": "Local algorithms in (weakly) coloured graphs", 
    "arxiv-id": "1002.0125v1", 
    "author": "Jara Uitto", 
    "publish": "2010-01-31T13:46:27Z", 
    "summary": "A local algorithm is a distributed algorithm that completes after a constant\nnumber of synchronous communication rounds. We present local approximation\nalgorithms for the minimum dominating set problem and the maximum matching\nproblem in 2-coloured and weakly 2-coloured graphs. In a weakly 2-coloured\ngraph, both problems admit a local algorithm with the approximation factor\n$(\\Delta+1)/2$, where $\\Delta$ is the maximum degree of the graph. We also give\na matching lower bound proving that there is no local algorithm with a better\napproximation factor for either of these problems. Furthermore, we show that\nthe stronger assumption of a 2-colouring does not help in the case of the\ndominating set problem, but there is a local approximation scheme for the\nmaximum matching problem in 2-coloured graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3492v1", 
    "title": "Cloud Migration: A Case Study of Migrating an Enterprise IT System to   IaaS", 
    "arxiv-id": "1002.3492v1", 
    "author": "Ian Sommerville", 
    "publish": "2010-02-18T11:25:49Z", 
    "summary": "This case study illustrates the potential benefits and risks associated with\nthe migration of an IT system in the oil & gas industry from an in-house data\ncenter to Amazon EC2 from a broad variety of stakeholder perspectives across\nthe enterprise, thus transcending the typical, yet narrow, financial and\ntechnical analysis offered by providers. Our results show that the system\ninfrastructure in the case study would have cost 37% less over 5 years on EC2,\nand using cloud computing could have potentially eliminated 21% of the support\ncalls for this system. These findings seem significant enough to call for a\nmigration of the system to the cloud but our stakeholder impact analysis\nrevealed that there are significant risks associated with this. Whilst the\nbenefits of using the cloud are attractive, we argue that it is important that\nenterprise decision-makers consider the overall organizational implications of\nthe changes brought about with cloud computing to avoid implementing local\noptimizations at the cost of organization-wide performance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3602v2", 
    "title": "Mobile Wireless Localization through Cooperation", 
    "arxiv-id": "1002.3602v2", 
    "author": "Jing Li", 
    "publish": "2010-02-18T20:38:03Z", 
    "summary": "This paper considers N mobile nodes that move together in the vicinity of\neach other, whose initial poses as well as subsequent movements must be\naccurately tracked in real time with the assist of M(>=3) reference nodes. By\nengaging the neighboring mobile nodes in a simple but effective cooperation,\nand by exploiting both the time-of-arrival (TOA) information (between mobile\nnodes and reference nodes) and the received-signal-strength (RSS) information\n(between mobile nodes), an effective new localization strategy, termed\ncooperative TOA and RSS (COTAR), is developed. An optimal maximum likelihood\ndetector is first formulated, followed by the derivation of a low-complexity\niterative approach that can practically achieve the Cramer-Rao lower bound.\nInstead of using simplified channel models as in many previous studies, a\nsophisticated and realistic channel model is used, which can effectively\naccount for the critical fact that the direct path is not necessarily the\nstrongest path. Extensive simulations are conducted in static and mobile\nsettings, and various practical issues and system parameters are evaluated. It\nis shown that COTAR significantly outperforms the existing strategies,\nachieving a localization accuracy of only a few tenths of a meter in clear\nenvironments and a couple of meters in heavily obstructed environments."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3629v1", 
    "title": "Generalized Adaptive Network Coded Cooperation (GANCC): A Unified   Framework for Network Coding and Channel Coding", 
    "arxiv-id": "1002.3629v1", 
    "author": "Jing Li", 
    "publish": "2010-02-18T22:28:03Z", 
    "summary": "This paper considers distributed coding for multi-source single-sink data\ncollection wireless networks. A unified framework for network coding and\nchannel coding, termed \"generalized adaptive network coded cooperation\"\n(GANCC), is proposed. Key ingredients of GANCC include: matching code graphs\nwith the dynamic network graphs on-the-fly, and integrating channel coding with\nnetwork coding through circulant low-density parity-check codes. Several code\nconstructing methods and several families of sparse-graph codes are proposed,\nand information theoretical analysis is performed. It is shown that GANCC is\nsimple to operate, adaptive in real time, distributed in nature, and capable of\nproviding remarkable coding gains even with a very limited number of\ncooperating users."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.3757v1", 
    "title": "Fast Flooding over Manhattan", 
    "arxiv-id": "1002.3757v1", 
    "author": "Riccardo Silvestri", 
    "publish": "2010-02-19T15:02:50Z", 
    "summary": "We consider a Mobile Ad-hoc NETwork (MANET) formed by n agents that move at\nspeed V according to the Manhattan Random-Way Point model over a square region\nof side length L. The resulting stationary (agent) spatial probability\ndistribution is far to be uniform: the average density over the \"central zone\"\nis asymptotically higher than that over the \"suburb\". Agents exchange data iff\nthey are at distance at most R within each other.\n  We study the flooding time of this MANET: the number of time steps required\nto broadcast a message from one source agent to all agents of the network in\nthe stationary phase. We prove the first asymptotical upper bound on the\nflooding time. This bound holds with high probability, it is a decreasing\nfunction of R and V, and it is tight for a wide and relevant range of the\nnetwork parameters (i.e. L, R and V).\n  A consequence of our result is that flooding over the sparse and\nhighly-disconnected suburb can be as fast as flooding over the dense and\nconnected central zone. Rather surprisingly, this property holds even when R is\nexponentially below the connectivity threshold of the MANET and the speed V is\nvery low."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4003v1", 
    "title": "A Cluster-based Approach for Outlier Detection in Dynamic Data Streams   (KORM: k-median OutlieR Miner)", 
    "arxiv-id": "1002.4003v1", 
    "author": "Priti Bansal", 
    "publish": "2010-02-21T19:39:35Z", 
    "summary": "Outlier detection in data streams has gained wide importance presently due to\nthe increasing cases of fraud in various applications of data streams. The\ntechniques for outlier detection have been divided into either statistics\nbased, distance based, density based or deviation based. Till now, most of the\nwork in the field of fraud detection was distance based but it is incompetent\nfrom computational point of view. In this paper we introduced a new clustering\nbased approach, which divides the stream in chunks and clusters each chunk\nusing kmedian into variable number of clusters. Instead of storing complete\ndata stream chunk in memory, we replace it with the weighted medians found\nafter mining a data stream chunk and pass that information along with the newly\narrived data chunk to the next phase. The weighted medians found in each phase\nare tested for outlierness and after a given number of phases, it is either\ndeclared as a real outlier or an inlier. Our technique is theoretically better\nthan the k-means as it does not fix the number of clusters to k rather gives a\nrange to it and provides a more stable and better solution which runs in\npoly-logarithmic space."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4561v1", 
    "title": "Breaking the O(n^2) Bit Barrier: Scalable Byzantine agreement with an   Adaptive Adversary", 
    "arxiv-id": "1002.4561v1", 
    "author": "Jared Saia", 
    "publish": "2010-02-24T15:19:55Z", 
    "summary": "We describe an algorithm for Byzantine agreement that is scalable in the\nsense that each processor sends only $\\tilde{O}(\\sqrt{n})$ bits, where $n$ is\nthe total number of processors. Our algorithm succeeds with high probability\nagainst an \\emph{adaptive adversary}, which can take over processors at any\ntime during the protocol, up to the point of taking over arbitrarily close to a\n1/3 fraction. We assume synchronous communication but a \\emph{rushing}\nadversary. Moreover, our algorithm works in the presence of flooding:\nprocessors controlled by the adversary can send out any number of messages. We\nassume the existence of private channels between all pairs of processors but\nmake no other cryptographic assumptions. Finally, our algorithm has latency\nthat is polylogarithmic in $n$. To the best of our knowledge, ours is the first\nalgorithm to solve Byzantine agreement against an adaptive adversary, while\nrequiring $o(n^{2})$ total bits of communication."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1002.4738v1", 
    "title": "An Approach to Ad hoc Cloud Computing", 
    "arxiv-id": "1002.4738v1", 
    "author": "Alvaro Fernandes", 
    "publish": "2010-02-25T10:19:37Z", 
    "summary": "We consider how underused computing resources within an enterprise may be\nharnessed to improve utilization and create an elastic computing\ninfrastructure. Most current cloud provision involves a data center model, in\nwhich clusters of machines are dedicated to running cloud infrastructure\nsoftware. We propose an additional model, the ad hoc cloud, in which\ninfrastructure software is distributed over resources harvested from machines\nalready in existence within an enterprise. In contrast to the data center cloud\nmodel, resource levels are not established a priori, nor are resources\ndedicated exclusively to the cloud while in use. A participating machine is not\ndedicated to the cloud, but has some other primary purpose such as running\ninteractive processes for a particular user. We outline the major\nimplementation challenges and one approach to tackling them."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2010.37", 
    "link": "http://arxiv.org/pdf/1003.0951v2", 
    "title": "LogMaster: Mining Event Correlations in Logs of Large scale Cluster   Systems", 
    "arxiv-id": "1003.0951v2", 
    "author": "Wei Zhou", 
    "publish": "2010-03-04T02:47:07Z", 
    "summary": "This paper presents a methodology and a system, named LogMaster, for mining\ncorrelations of events that have multiple attributions, i.e., node ID,\napplication ID, event type, and event severity, in logs of large-scale cluster\nsystems. Different from traditional transactional data, e.g., supermarket\npurchases, system logs have their unique characteristic, and hence we propose\nseveral innovative approaches to mine their correlations. We present a simple\nmetrics to measure correlations of events that may happen interleavedly. On the\nbasis of the measurement of correlations, we propose two approaches to mine\nevent correlations; meanwhile, we propose an innovative abstraction: event\ncorrelation graphs (ECGs) to represent event correlations, and present an ECGs\nbased algorithm for predicting events. For two system logs of a production\nHadoop-based cloud computing system at Research Institution of China Mobile and\na production HPC cluster system at Los Alamos National Lab (LANL), we evaluate\nour approaches in three scenarios: (a) predicting all events on the basis of\nboth failure and non-failure events; (b) predicting only failure events on the\nbasis of both failure and non-failure events; (c) predicting failure events\nafter removing non-failure events."
},{
    "category": "cs.DC", 
    "doi": "10.4203/ccp.101.22", 
    "link": "http://arxiv.org/pdf/1003.0952v3", 
    "title": "Parallel structurally-symmetric sparse matrix-vector products on   multi-core processors", 
    "arxiv-id": "1003.0952v3", 
    "author": "Fernando L. B. Ribeiro", 
    "publish": "2010-03-04T03:25:41Z", 
    "summary": "We consider the problem of developing an efficient multi-threaded\nimplementation of the matrix-vector multiplication algorithm for sparse\nmatrices with structural symmetry. Matrices are stored using the compressed\nsparse row-column format (CSRC), designed for profiting from the symmetric\nnon-zero pattern observed in global finite element matrices. Unlike classical\ncompressed storage formats, performing the sparse matrix-vector product using\nthe CSRC requires thread-safe access to the destination vector. To avoid race\nconditions, we have implemented two partitioning strategies. In the first one,\neach thread allocates an array for storing its contributions, which are later\ncombined in an accumulation step. We analyze how to perform this accumulation\nin four different ways. The second strategy employs a coloring algorithm for\ngrouping rows that can be concurrently processed by threads. Our results\nindicate that, although incurring an increase in the working set size, the\nformer approach leads to the best performance improvements for most matrices."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-13284-1_11", 
    "link": "http://arxiv.org/pdf/1003.1058v2", 
    "title": "Algorithms For Extracting Timeliness Graphs", 
    "arxiv-id": "1003.1058v2", 
    "author": "Mikel Larrea", 
    "publish": "2010-03-04T14:47:17Z", 
    "summary": "We consider asynchronous message-passing systems in which some links are\ntimely and processes may crash. Each run defines a timeliness graph among\ncorrect processes: (p; q) is an edge of the timeliness graph if the link from p\nto q is timely (that is, there is bound on communication delays from p to q).\nThe main goal of this paper is to approximate this timeliness graph by graphs\nhaving some properties (such as being trees, rings, ...). Given a family S of\ngraphs, for runs such that the timeliness graph contains at least one graph in\nS then using an extraction algorithm, each correct process has to converge to\nthe same graph in S that is, in a precise sense, an approximation of the\ntimeliness graph of the run. For example, if the timeliness graph contains a\nring, then using an extraction algorithm, all correct processes eventually\nconverge to the same ring and in this ring all nodes will be correct processes\nand all links will be timely. We first present a general extraction algorithm\nand then a more specific extraction algorithm that is communication efficient\n(i.e., eventually all the messages of the extraction algorithm use only links\nof the extracted graph)."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1291v1", 
    "title": "The Grid[Way] Job Template Manager, a tool for parameter sweeping", 
    "arxiv-id": "1003.1291v1", 
    "author": "Ignacio M. Llorente", 
    "publish": "2010-03-05T15:34:09Z", 
    "summary": "Parameter sweeping is a widely used algorithmic technique in computational\nscience. It is specially suited for high-throughput computing since the jobs\nevaluating the parameter space are loosely coupled or independent.\n  A tool that integrates the modeling of a parameter study with the control of\njobs in a distributed architecture is presented. The main task is to facilitate\nthe creation and deletion of job templates, which are the elements describing\nthe jobs to be run. Extra functionality relies upon the GridWay Metascheduler,\nacting as the middleware layer for job submission and control. It supports\ninteresting features like multi-dimensional sweeping space, wildcarding of\nparameters, functional evaluation of ranges, value-skipping and job template\nautomatic indexation.\n  The use of this tool increases the reliability of the parameter sweep study\nthanks to the systematic bookkeping of job templates and respective job\nstatuses. Furthermore, it simplifies the porting of the target application to\nthe grid reducing the required amount of time and effort."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1395v2", 
    "title": "Start-phase control of distributed systems written in Erlang/OTP", 
    "arxiv-id": "1003.1395v2", 
    "author": "Antal T\u00e1trai", 
    "publish": "2010-03-06T15:47:21Z", 
    "summary": "This paper presents a realization for the reliable and fast startup of\ndistributed systems written in Erlang. The traditional startup provided by the\nErlang/OTP library is sequential, parallelization usually requires unsafe and\nad-hoc solutions. The proposed method calls only for slight modifications in\nthe Erlang/OTP stdlib by applying a system dependency graph. It makes the\nstartup safe, quick, and it is equally easy to use in newly developed and\nlegacy systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1397v1", 
    "title": "Using Coloured Petri Nets for design of parallel raytracing environment", 
    "arxiv-id": "1003.1397v1", 
    "author": "Branislav Sobota", 
    "publish": "2010-03-06T16:24:29Z", 
    "summary": "This paper deals with the parallel raytracing part of virtual-reality system\nPROLAND, developed at the home institution of authors. It describes an actual\nimplementation of the raytracing part and introduces a Coloured Petri Nets\nmodel of the implementation. The model is used for an evaluation of the\nimplementation by means of simulation-based performance analysis and also forms\nthe basis for future improvements of its parallelization strategy."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2010.12.041", 
    "link": "http://arxiv.org/pdf/1003.1608v1", 
    "title": "Deterministic Distributed Vertex Coloring in Polylogarithmic Time", 
    "arxiv-id": "1003.1608v1", 
    "author": "Michael Elkin", 
    "publish": "2010-03-08T12:00:09Z", 
    "summary": "Consider an n-vertex graph G = (V,E) of maximum degree Delta, and suppose\nthat each vertex v \\in V hosts a processor. The processors are allowed to\ncommunicate only with their neighbors in G. The communication is synchronous,\ni.e., it proceeds in discrete rounds. In the distributed vertex coloring\nproblem the objective is to color G with Delta + 1, or slightly more than Delta\n+ 1, colors using as few rounds of communication as possible. (The number of\nrounds of communication will be henceforth referred to as running time.)\nEfficient randomized algorithms for this problem are known for more than twenty\nyears \\cite{L86, ABI86}. Specifically, these algorithms produce a (Delta +\n1)-coloring within O(log n) time, with high probability. On the other hand, the\nbest known deterministic algorithm that requires polylogarithmic time employs\nO(Delta^2) colors. This algorithm was devised in a seminal FOCS'87 paper by\nLinial \\cite{L87}. Its running time is O(log^* n). In the same paper Linial\nasked whether one can color with significantly less than Delta^2 colors in\ndeterministic polylogarithmic time. By now this question of Linial became one\nof the most central long-standing open questions in this area. In this paper we\nanswer this question in the affirmative, and devise a deterministic algorithm\nthat employs \\Delta^{1 +o(1)} colors, and runs in polylogarithmic time.\nSpecifically, the running time of our algorithm is O(f(Delta) log Delta log n),\nfor an arbitrarily slow-growing function f(Delta) = \\omega(1). We can also\nproduce O(Delta^{1 + \\eta})-coloring in O(log Delta log n)-time, for an\narbitrarily small constant \\eta > 0, and O(Delta)-coloring in\nO(Delta^{\\epsilon} log n) time, for an arbitrarily small constant \\epsilon > 0."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3305v1", 
    "title": "Towards trusted volunteer grid environments", 
    "arxiv-id": "1003.3305v1", 
    "author": "Tunisia)", 
    "publish": "2010-03-17T06:28:40Z", 
    "summary": "Intensive experiences show and confirm that grid environments can be\nconsidered as the most promising way to solve several kinds of problems\nrelating either to cooperative work especially where involved collaborators are\ndispersed geographically or to some very greedy applications which require\nenough power of computing or/and storage. Such environments can be classified\ninto two categories; first, dedicated grids where the federated computers are\nsolely devoted to a specific work through its end. Second, Volunteer grids\nwhere federated computers are not completely devoted to a specific work but\ninstead they can be randomly and intermittently used, at the same time, for any\nother purpose or they can be connected or disconnected at will by their owners\nwithout any prior notification. Each category of grids includes surely several\nadvantages and disadvantages; nevertheless, we think that volunteer grids are\nvery promising and more convenient especially to build a general multipurpose\ndistributed scalable environment. Unfortunately, the big challenge of such\nenvironments is, however, security and trust. Indeed, owing to the fact that\nevery federated computer in such an environment can randomly be used at the\nsame time by several users or can be disconnected suddenly, several security\nproblems will automatically arise. In this paper, we propose a novel solution\nbased on identity federation, agent technology and the dynamic enforcement of\naccess control policies that lead to the design and implementation of trusted\nvolunteer grid environments."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3684v1", 
    "title": "Parallel Generation of Massive Scale-Free Graphs", 
    "arxiv-id": "1003.3684v1", 
    "author": "Keith Henderson", 
    "publish": "2010-03-18T22:11:14Z", 
    "summary": "One of the biggest huddles faced by researchers studying algorithms for\nmassive graphs is the lack of large input graphs that are essential for the\ndevelopment and test of the graph algorithms. This paper proposes two efficient\nand highly scalable parallel graph generation algorithms that can produce\nmassive realistic graphs to address this issue. The algorithms, designed to\nachieve high degree of parallelism by minimizing inter-processor\ncommunications, are two of the fastest graph generators which are capable of\ngenerating scale-free graphs with billions of vertices and edges. The synthetic\ngraphs generated by the proposed methods possess the most common properties of\nreal complex networks such as power-law degree distribution, small-worldness,\nand communities-within-communities. Scalability was tested on a large cluster\nat Lawrence Livermore National Laboratory. In the experiment, we were able to\ngenerate a graph with 1 billion vertices and 5 billion edges in less than 13\nseconds. To the best of our knowledge, this is the largest synthetic scale-free\ngraph reported in the literature."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3866v2", 
    "title": "The Cloud Adoption Toolkit: Addressing the Challenges of Cloud Adoption   in Enterprise", 
    "arxiv-id": "1003.3866v2", 
    "author": "Ian Sommerville", 
    "publish": "2010-03-19T19:40:41Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper: i) describes the challenges that\ndecision makers face when attempting to determine the feasibility of the\nadoption of cloud computing in their organisations; ii) illustrates a lack of\nexisting work to address the feasibility challenges of cloud adoption in the\nenterprise; iii) introduces the Cloud Adoption Toolkit that provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. The paper adopts a position paper methodology such that case\nstudy evidence is provided, where available, to support claims. We conclude\nthat the Cloud Adoption Toolkit, whilst still under development, shows signs\nthat it is a useful tool for decision makers as it helps address the\nfeasibility challenges of cloud adoption in the enterprise."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.3920v1", 
    "title": "InterCloud: Utility-Oriented Federation of Cloud Computing Environments   for Scaling of Application Services", 
    "arxiv-id": "1003.3920v1", 
    "author": "Rodrigo N. Calheiros", 
    "publish": "2010-03-20T10:54:43Z", 
    "summary": "Cloud computing providers have setup several data centers at different\ngeographical locations over the Internet in order to optimally serve needs of\ntheir customers around the world. However, existing systems do not support\nmechanisms and policies for dynamically coordinating load distribution among\ndifferent Cloud-based data centers in order to determine optimal location for\nhosting application services to achieve reasonable QoS levels. Further, the\nCloud computing providers are unable to predict geographic distribution of\nusers consuming their services, hence the load coordination must happen\nautomatically, and distribution of services must change in response to changes\nin the load. To counter this problem, we advocate creation of federated Cloud\ncomputing environment (InterCloud) that facilitates just-in-time,\nopportunistic, and scalable provisioning of application services, consistently\nachieving QoS targets under variable workload, resource and network conditions.\nThe overall goal is to create a computing environment that supports dynamic\nexpansion or contraction of capabilities (VMs, services, storage, and database)\nfor handling sudden variations in service demands.\n  This paper presents vision, challenges, and architectural elements of\nInterCloud for utility-oriented federation of Cloud computing environments. The\nproposed InterCloud environment supports scaling of applications across\nmultiple vendor clouds. We have validated our approach by conducting a set of\nrigorous performance evaluation study using the CloudSim toolkit. The results\ndemonstrate that federated Cloud computing model has immense potential as it\noffers significant performance gains as regards to response time and cost\nsaving under dynamic workload scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.5342v1", 
    "title": "Improving Waiting Time of Tasks Scheduled Under Preemptive Round Robin   Using Changeable Time Quantum", 
    "arxiv-id": "1003.5342v1", 
    "author": "Samih Mohemmed Mostafa", 
    "publish": "2010-03-28T06:34:20Z", 
    "summary": "Minimizing waiting time for tasks waiting in the queue for execution is one\nof the important scheduling cri-teria which took a wide area in scheduling\npreemptive tasks. In this paper we present Changeable Time Quan-tum (CTQ)\napproach combined with the round-robin algorithm, we try to adjust the time\nquantum according to the burst times of the tasks in the ready queue. There are\ntwo important benefits of using (CTQ) approach: minimizing the average waiting\ntime of the tasks, consequently minimizing the average turnaround time, and\nkeeping the number of context switches as low as possible, consequently\nminimizing the scheduling overhead. In this paper, we consider the scheduling\nproblem for preemptive tasks, where the time costs of these tasks are known a\npriori. Our experimental results demonstrate that CTQ can provide much lower\nscheduling overhead and better scheduling criteria."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2207", 
    "link": "http://arxiv.org/pdf/1003.5794v3", 
    "title": "Scalable Group Management in Large-Scale Virtualized Clusters", 
    "arxiv-id": "1003.5794v3", 
    "author": "Jianfeng Zhan", 
    "publish": "2010-03-30T11:22:55Z", 
    "summary": "To save cost, recently more and more users choose to provision virtual\nmachine resources in cluster systems, especially in data centres. Maintaining a\nconsistent member view is the foundation of reliable cluster managements, and\nit also raises several challenge issues for large scale cluster systems\ndeployed with virtual machines (which we call virtualized clusters). In this\npaper, we introduce our experiences in design and implementation of scalable\nmember view management on large-scale virtual clusters. Our research\ncontributions are three-fold: 1) we propose a scalable and reliable management\ninfrastructure that combines a peer-to-peer structure and a hierarchy structure\nto maintain a consistent member view in virtual clusters; 2) we present a\nlight-weighted group membership algorithm that can reach the consistent member\nview within a single round of message exchange; and 3) we design and implement\na scalable membership service that can provision virtual machines and maintain\na consistent member view in virtual clusters. Our work is verified on Dawning\n5000A, which ranked No.10 of Top 500 super computers in November, 2008."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICECCS.2010.21", 
    "link": "http://arxiv.org/pdf/1004.0728v1", 
    "title": "Effects of component-subscription network topology on large-scale data   centre performance scaling", 
    "arxiv-id": "1004.0728v1", 
    "author": "Dave Cliff", 
    "publish": "2010-04-05T22:08:46Z", 
    "summary": "Modern large-scale date centres, such as those used for cloud computing\nservice provision, are becoming ever-larger as the operators of those data\ncentres seek to maximise the benefits from economies of scale. With these\nincreases in size comes a growth in system complexity, which is usually\nproblematic. There is an increased desire for automated \"self-star\"\nconfiguration, management, and failure-recovery of the data-centre\ninfrastructure, but many traditional techniques scale much worse than linearly\nas the number of nodes to be managed increases. As the number of nodes in a\nmedian-sized data-centre looks set to increase by two or three orders of\nmagnitude in coming decades, it seems reasonable to attempt to explore and\nunderstand the scaling properties of the data-centre middleware before such\ndata-centres are constructed. In [1] we presented SPECI, a simulator that\npredicts aspects of large-scale data-centre middleware performance,\nconcentrating on the influence of status changes such as policy updates or\nroutine node failures. [...]. In [1] we used a first-approximation assumption\nthat such subscriptions are distributed wholly at random across the data\ncentre. In this present paper, we explore the effects of introducing more\nrealistic constraints to the structure of the internal network of\nsubscriptions. We contrast the original results [...] exploring the effects of\nmaking the data-centre's subscription network have a regular lattice-like\nstructure, and also semi-random network structures resulting from parameterised\nnetwork generation functions that create \"small-world\" and \"scale-free\"\nnetworks. We show that for distributed middleware topologies, the structure and\ndistribution of tasks carried out in the data centre can significantly\ninfluence the performance overhead imposed by the middleware."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICECCS.2010.21", 
    "link": "http://arxiv.org/pdf/1004.1276v1", 
    "title": "In Cloud, Can Scientific Communities Benefit from the Economies of   Scale?", 
    "arxiv-id": "1004.1276v1", 
    "author": "Yi Liang", 
    "publish": "2010-04-08T08:07:08Z", 
    "summary": "The basic idea behind Cloud computing is that resource providers offer\nelastic resources to end users. In this paper, we intend to answer one key\nquestion to the success of Cloud computing: in Cloud, can small or medium-scale\nscientific computing communities benefit from the economies of scale? Our\nresearch contributions are three-fold: first, we propose an enhanced scientific\npublic cloud model (ESP) that encourages small- or medium-scale organizations\nto rent elastic resources from a public cloud provider; second, on a basis of\nthe ESP model, we design and implement the DawningCloud system that can\nconsolidate heterogeneous scientific workloads on a Cloud site; third, we\npropose an innovative emulation methodology and perform a comprehensive\nevaluation. We found that for two typical workloads: high throughput computing\n(HTC) and many task computing (MTC), DawningCloud saves the resource\nconsumption maximally by 44.5% (HTC) and 72.6% (MTC) for service providers, and\nsaves the total resource consumption maximally by 47.3% for a resource provider\nwith respect to the previous two public Cloud solutions. To this end, we\nconclude that for typical workloads: HTC and MTC, DawningCloud can enable\nscientific communities to benefit from the economies of scale of public Clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1324v1", 
    "title": "Distributed Fault-Tolerant Avionic Systems - A Real-Time Perspective", 
    "arxiv-id": "1004.1324v1", 
    "author": "Neil Audsley", 
    "publish": "2010-04-08T13:03:11Z", 
    "summary": "This paper examines the problem of introducing advanced forms of\nfault-tolerance via reconfiguration into safety-critical avionic systems. This\nis required to enable increased availability after fault occurrence in\ndistributed integrated avionic systems(compared to static federated systems).\nThe approach taken is to identify a migration path from current architectures\nto those that incorporate re-configuration to a lesser or greater degree. Other\nchallenges identified include change of the development process; incremental\nand flexible timing and safety analyses; configurable kernels applicable for\nsafety-critical systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1746v1", 
    "title": "Internet ware cloud computing :Challenges", 
    "arxiv-id": "1004.1746v1", 
    "author": "Mrityunjay Singh", 
    "publish": "2010-04-10T22:17:18Z", 
    "summary": "After decades of engineering development and infrastructural investment,\nInternet connections have become commodity product in many countries, and\nInternet scale \"cloud computing\" has started to compete with traditional\nsoftware business through its technological advantages and economy of scale.\nCloud computing is a promising enabling technology of Internet ware Cloud\nComputing is termed as the next big thing in the modern corporate world. Apart\nfrom the present day software and technologies, cloud computing will have a\ngrowing impact on enterprise IT and business activities in many large\norganizations. This paper provides an insight to cloud computing, its impacts\nand discusses various issues that business organizations face while\nimplementing cloud computing. Further, it recommends various strategies that\norganizations need to adopt while migrating to cloud computing. The purpose of\nthis paper is to develop an understanding of cloud computing in the modern\nworld and its impact on organizations and businesses. Initially the paper\nprovides a brief description of the cloud computing model introduction and its\npurposes. Further it discusses various technical and non-technical issues that\nneed to be overcome in order for the benefits of cloud computing to be realized\nin corporate businesses and organizations. It then provides various\nrecommendations and strategies that businesses need to work on before stepping\ninto new technologies."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.1864v2", 
    "title": "The Low Latency Fault Tolerance System", 
    "arxiv-id": "1004.1864v2", 
    "author": "L. E. Moser", 
    "publish": "2010-04-12T01:25:49Z", 
    "summary": "The Low Latency Fault Tolerance (LLFT) system provides fault tolerance for\ndistributed applications, using the leader-follower replication technique. The\nLLFT system provides application-transparent replication, with strong replica\nconsistency, for applications that involve multiple interacting processes or\nthreads. The LLFT system comprises a Low Latency Messaging Protocol, a\nLeader-Determined Membership Protocol, and a Virtual Determinizer Framework.\nThe Low Latency Messaging Protocol provides reliable, totally ordered message\ndelivery by employing a direct group-to-group multicast, where the message\nordering is determined by the primary replica in the group. The\nLeader-Determined Membership Protocol provides reconfiguration and recovery\nwhen a replica becomes faulty and when a replica joins or leaves a group, where\nthe membership of the group is determined by the primary replica. The Virtual\nDeterminizer Framework captures the ordering information at the primary replica\nand enforces the same ordering at the backup replicas for major sources of\nnon-determinism, including multi-threading, time-related operations and socket\ncommunication. The LLFT system achieves low latency message delivery during\nnormal operation and low latency reconfiguration and recovery when a fault\noccurs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.2308v1", 
    "title": "Addressing the P2P Bootstrap Problem for Small Networks", 
    "arxiv-id": "1004.2308v1", 
    "author": "Renato Figueiredo", 
    "publish": "2010-04-14T03:14:49Z", 
    "summary": "P2P overlays provide a framework for building distributed applications\nconsisting of few to many resources with features including self-configuration,\nscalability, and resilience to node failures. Such systems have been\nsuccessfully adopted in large-scale services for content delivery networks,\nfile sharing, and data storage. In small-scale systems, they can be useful to\naddress privacy concerns and for network applications that lack dedicated\nservers. The bootstrap problem, finding an existing peer in the overlay,\nremains a challenge to enabling these services for small-scale P2P systems. In\nlarge networks, the solution to the bootstrap problem has been the use of\ndedicated services, though creating and maintaining these systems requires\nexpertise and resources, which constrain their usefulness and make them\nunappealing for small-scale systems. This paper surveys and summarizes\nrequirements that allow peers potentially constrained by network connectivity\nto bootstrap small-scale overlays through the use of existing public overlays.\nIn order to support bootstrapping, a public overlay must support the following\nrequirements: a method for reflection in order to obtain publicly reachable\naddresses, so peers behind network address translators and firewalls can\nreceive incoming connection requests; communication relaying to share public\naddresses and communicate when direct communication is not feasible; and\nrendezvous for discovering remote peers, when the overlay lacks stable\nmembership. After presenting a survey of various public overlays, we identify\ntwo overlays that match the requirements: XMPP overlays, such as Google Talk\nand Live Journal Talk, and Brunet, a structured overlay based upon Symphony. We\npresent qualitative experiences with prototypes that demonstrate the ability to\nbootstrap small-scale private structured overlays from public Brunet or XMPP\ninfrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.2772v2", 
    "title": "Boosting Multi-Core Reachability Performance with Shared Hash Tables", 
    "arxiv-id": "1004.2772v2", 
    "author": "Michael Weber", 
    "publish": "2010-04-16T07:53:38Z", 
    "summary": "This paper focuses on data structures for multi-core reachability, which is a\nkey component in model checking algorithms and other verification methods. A\ncornerstone of an efficient solution is the storage of visited states. In\nrelated work, static partitioning of the state space was combined with\nthread-local storage and resulted in reasonable speedups, but left open whether\nimprovements are possible. In this paper, we present a scaling solution for\nshared state storage which is based on a lockless hash table implementation.\nThe solution is specifically designed for the cache architecture of modern\nCPUs. Because model checking algorithms impose loose requirements on the hash\ntable operations, their design can be streamlined substantially compared to\nrelated work on lockless hash tables. Still, an implementation of the hash\ntable presented here has dozens of sensitive performance parameters (bucket\nsize, cache line size, data layout, probing sequence, etc.). We analyzed their\nimpact and compared the resulting speedups with related tools. Our\nimplementation outperforms two state-of-the-art multi-core model checkers (SPIN\nand DiVinE) by a substantial margin, while placing fewer constraints on the\nload balancing and search algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.3395v1", 
    "title": "Passively Mobile Communicating Logarithmic Space Machines", 
    "arxiv-id": "1004.3395v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2010-04-20T10:03:01Z", 
    "summary": "We propose a new theoretical model for passively mobile Wireless Sensor\nNetworks. We call it the PALOMA model, standing for PAssively mobile\nLOgarithmic space MAchines. The main modification w.r.t. the Population\nProtocol model is that agents now, instead of being automata, are Turing\nMachines whose memory is logarithmic in the population size n. Note that the\nnew model is still easily implementable with current technology. We focus on\ncomplete communication graphs. We define the complexity class PLM, consisting\nof all symmetric predicates on input assignments that are stably computable by\nthe PALOMA model. We assume that the agents are initially identical.\nSurprisingly, it turns out that the PALOMA model can assign unique consecutive\nids to the agents and inform them of the population size! This allows us to\ngive a direct simulation of a Deterministic Turing Machine of O(nlogn) space,\nthus, establishing that any symmetric predicate in SPACE(nlogn) also belongs to\nPLM. We next prove that the PALOMA model can simulate the Community Protocol\nmodel, thus, improving the previous lower bound to all symmetric predicates in\nNSPACE(nlogn). Going one step further, we generalize the simulation of the\ndeterministic TM to prove that the PALOMA model can simulate a Nondeterministic\nTM of O(nlogn) space. Although providing the same lower bound, the important\nremark here is that the bound is now obtained in a direct manner, in the sense\nthat it does not depend on the simulation of a TM by a Pointer Machine.\nFinally, by showing that a Nondeterministic TM of O(nlogn) space decides any\nlanguage stably computable by the PALOMA model, we end up with an exact\ncharacterization for PLM: it is precisely the class of all symmetric predicates\nin NSPACE(nlogn)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.3566v1", 
    "title": "An Economic-based Resource Management and Scheduling for Grid Computing   Applications", 
    "arxiv-id": "1004.3566v1", 
    "author": "C. Chellappan", 
    "publish": "2010-04-20T20:32:31Z", 
    "summary": "Resource management and scheduling plays a crucial role in achieving high\nutilization of resources in grid computing environments. Due to heterogeneity\nof resources, scheduling an application is significantly complicated and\nchallenging task in grid system. Most of the researches in this area are mainly\nfocused on to improve the performance of the grid system. There were some\nallocation model has been proposed based on divisible load theory with\ndifferent type of workloads and a single originating processor. In this paper\nwe introduce a new resource allocation model with multiple load originating\nprocessors as an economic model. Solutions for an optimal allocation of\nfraction of loads to nodes obtained to minimize the cost of the grid users via\nlinear programming approach. It is found that the resource allocation model can\nefficiently and effectively allocate workloads to proper resources.\nExperimental results showed that the proposed model obtained the better\nsolution in terms of cost and time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/AERO.1998.682155", 
    "link": "http://arxiv.org/pdf/1004.4559v1", 
    "title": "The Accuracy of Tree-based Counting in Dynamic Networks", 
    "arxiv-id": "1004.4559v1", 
    "author": "Fetahi Wuhib", 
    "publish": "2010-04-26T15:51:23Z", 
    "summary": "Tree-based protocols are ubiquitous in distributed systems. They are\nflexible, they perform generally well, and, in static conditions, their\nanalysis is mostly simple. Under churn, however, node joins and failures can\nhave complex global effects on the tree overlays, making analysis surprisingly\nsubtle. To our knowledge, few prior analytic results for performance estimation\nof tree based protocols under churn are currently known. We study a simple\nBellman-Ford-like protocol which performs network size estimation over a\ntree-shaped overlay. A continuous time Markov model is constructed which allows\nkey protocol characteristics to be estimated, including the expected number of\nnodes at a given (perceived) distance to the root and, for each such node, the\nexpected (perceived) size of the subnetwork rooted at that node. We validate\nthe model by simulation, using a range of network sizes, node degrees, and\nchurn-to-protocol rates, with convincing results."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1004.4701v3", 
    "title": "Relating L-Resilience and Wait-Freedom via Hitting Sets", 
    "arxiv-id": "1004.4701v3", 
    "author": "Petr Kuznetsov", 
    "publish": "2010-04-27T03:29:43Z", 
    "summary": "The condition of t-resilience stipulates that an n-process program is only\nobliged to make progress when at least n-t processes are correct. Put another\nway, the live sets, the collection of process sets such that progress is\nrequired if all the processes in one of these sets are correct, are all sets\nwith at least n-t processes.\n  We show that the ability of arbitrary collection of live sets L to solve\ndistributed tasks is tightly related to the minimum hitting set of L, a minimum\ncardinality subset of processes that has a non-empty intersection with every\nlive set. Thus, finding the computing power of L is NP-complete.\n  For the special case of colorless tasks that allow participating processes to\nadopt input or output values of each other, we use a simple simulation to show\nthat a task can be solved L-resiliently if and only if it can be solved\n(h-1)-resiliently, where h is the size of the minimum hitting set of L.\n  For general tasks, we characterize L-resilient solvability of tasks with\nrespect to a limited notion of weak solvability: in every execution where all\nprocesses in some set in L are correct, outputs must be produced for every\nprocess in some (possibly different) participating set in L. Given a task T, we\nconstruct another task T_L such that T is solvable weakly L-resiliently if and\nonly if T_L is solvable weakly wait-free."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1004.5256v1", 
    "title": "Construction auto-stabilisante d'arbre couvrant en d\u00e9pit d'actions   malicieuses", 
    "arxiv-id": "1004.5256v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-04-29T12:02:13Z", 
    "summary": "A self-stabilizing protocol provides by definition a tolerance to transient\nfailures. Recently, a new class of self-stabilizing protocols appears. These\nprotocols provides also a tolerance to a given number of permanent failures. In\nthis article, we are interested in self-stabilizing protocols that deal with\nByzantines failures. We prove that, for some problems which not allow strict\nstabilization (see [Nesterenko,Arora,2002]), there exist solutions that\ntolerates Byzantine faults if we define a new criteria of tolerance."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1005.0925v1", 
    "title": "Improving Overhead Computation and pre-processing Time for Grid   Scheduling System", 
    "arxiv-id": "1005.0925v1", 
    "author": "Abdul Hanan Abdullah", 
    "publish": "2010-05-06T08:30:16Z", 
    "summary": "Computational Grid is enormous environments with heterogeneous resources and\nstable infrastructures among other Internet-based computing systems. However,\nthe managing of resources in such systems has its special problems. Scheduler\nsystems need to get last information about participant nodes from information\ncenters for the purpose of firmly job scheduling. In this paper, we focus on\nonline updating resource information centers with processed and provided data\nbased on the assumed hierarchical model. A hybrid knowledge extraction method\nhas been used to classifying grid nodes based on prediction of jobs' features.\nAn affirmative point of this research is that scheduler systems don't waste\nextra time for getting up-to-date information of grid nodes. The experimental\nresult shows the advantages of our approach compared to other conservative\nmethods, especially due to its ability to predict the behavior of nodes based\non comprehensive data tables on each node."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-17679-1_17", 
    "link": "http://arxiv.org/pdf/1005.1195v1", 
    "title": "The Impact of Topology on Byzantine Containment in Stabilization", 
    "arxiv-id": "1005.1195v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-07T12:33:41Z", 
    "summary": "Self-stabilization is an versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed system that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties prove difficult: we demonstrate that it is impossible to contain the\nimpact of Byzantine nodes in a self-stabilizing context for maximum metric tree\nconstruction (strict stabilization). We propose a weaker containment scheme\ncalled topology-aware strict stabilization, and present a protocol for\ncomputing maximum metric trees that is optimal for this scheme with respect to\nimpossibility result."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2010.2212", 
    "link": "http://arxiv.org/pdf/1005.1747v1", 
    "title": "A Real Time Optimistic Strategy to achieve Concurrency Control in Mobile   Environments Using On-demand Multicasting", 
    "arxiv-id": "1005.1747v1", 
    "author": "Lakshmi Rajamani", 
    "publish": "2010-05-11T08:17:36Z", 
    "summary": "In mobile database environments, multiple users may access similar data items\nirrespective of their physical location leading to concurrent access anomalies.\nAs disconnections and mobility are the common characteristics in mobile\nenvironment, performing concurrent access to a particular data item leads to\ninconsistency. Most of the approaches use locking mechanisms to achieve\nconcurrency control. However this leads to increase in blocking and abort rate.\nIn this paper an optimistic concurrency control strategy using on-demand\nmulticasting is proposed for mobile database environments which guarantees\nconsistency and introduces application-specific conflict detection and\nresolution strategies. The simulation results specify increase in system\nthroughput by reducing the transaction abort rates as compared to the other\noptimistic strategies proposed in literature."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.1751v1", 
    "title": "An Efficient and Secure Routing Protocol for Mobile Ad-Hoc Networks", 
    "arxiv-id": "1005.1751v1", 
    "author": "Akshay Atrey", 
    "publish": "2010-05-11T08:30:10Z", 
    "summary": "Efficiency and simplicity of random algorithms have made them a lucrative\nalternative for solving complex problems in the domain of communication\nnetworks. This paper presents a random algorithm for handling the routing\nproblem in Mobile Ad hoc Networks [MANETS].The performance of most existing\nrouting protocols for MANETS degrades in terms of packet delay and congestion\ncaused as the number of mobile nodes increases beyond a certain level or their\nspeed passes a certain level. As the network becomes more and more dynamic,\ncongestion in network increases due to control packets generated by the routing\nprotocols in the process of route discovery and route maintenance. Most of this\ncongestion is due to flooding mechanism used in protocols like AODV and DSDV\nfor the purpose of route discovery and route maintenance or for route discovery\nas in the case of DSR protocol. This paper introduces the concept of random\nrouting algorithm that neither maintains a routing table nor floods the entire\nnetwork as done by various known protocols thereby reducing the load on network\nin terms of number of control packets in a highly dynamic scenario. This paper\ncalculates the expected run time of the designed random algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.1904v3", 
    "title": "Cloud Computing: Exploring the scope", 
    "arxiv-id": "1005.1904v3", 
    "author": "Vijayshree Tiwari", 
    "publish": "2010-05-11T18:23:38Z", 
    "summary": "Cloud computing refers to a paradigm shift to overall IT solutions while\nraising the accessibility, scalability and effectiveness through its enabling\ntechnologies. However, migrated cloud platforms and services cost benefits as\nwell as performances are neither clear nor summarized. Globalization and the\nrecessionary economic times have not only raised the bar of a better IT\ndelivery models but also have given access to technology enabled services via\ninternet. Cloud computing has vast potential in terms of lean Retail\nmethodologies that can minimize the operational cost by using the third party\nbased IT capabilities, as a service. It will not only increase the ROI but will\nalso help in lowering the total cost of ownership. In this paper we have tried\nto compare the cloud computing cost benefits with the actual premise cost which\nan organization incurs normally. However, in spite of the cost benefits, many\nIT professional believe that the latest model i.e. \"cloud computing\" has risks\nand security concerns. This report demonstrates how to answer the following\nquestions: (1) Idea behind cloud computing. (2) Monetary cost benefits of using\ncloud with respect to traditional premise computing. (3) What are the various\nsecurity issues? We have tried to find out the cost benefit by comparing the\nMicrosoft Azure cloud cost with the prevalent premise cost."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2027v1", 
    "title": "A Multi-agent Framework for Performance Tuning in Distributed   Environment", 
    "arxiv-id": "1005.2027v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-05-12T09:39:18Z", 
    "summary": "This paper presents the overall design of a multi-agent framework for tuning\nthe performance of an application executing in a distributed environment. The\nmulti-agent framework provides services like resource brokering, analyzing\nperformance monitoring data, local tuning and also rescheduling in case of any\nperformance problem on a specific resource provider. The paper also briefly\ndescribes the implementation of some part of the framework. In particular, job\nmigration on the basis of performance monitoring data is particularly\nhighlighted in this paper."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2037v1", 
    "title": "An Integrated Framework for Performance Analysis and Tuning in Grid   Environment", 
    "arxiv-id": "1005.2037v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-05-12T10:13:59Z", 
    "summary": "In a heterogeneous, dynamic environment, like Grid, post-mortem analysis is\nof no use and data needs to be collected and analysed in real time. Novel\ntechniques are also required for dynamically tuning the application's\nperformance and resource brokering in order to maintain the desired QoS. The\nobjective of this paper is to propose an integrated framework for performance\nanalysis and tuning of the application, and rescheduling the application, if\nnecessary, to some other resources in order to adapt to the changing resource\nusage scenario in a dynamic environment."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.2567v1", 
    "title": "Deploying Wireless Networks with Beeps", 
    "arxiv-id": "1005.2567v1", 
    "author": "Fabian Kuhn", 
    "publish": "2010-05-14T16:32:02Z", 
    "summary": "We present the \\emph{discrete beeping} communication model, which assumes\nnodes have minimal knowledge about their environment and severely limited\ncommunication capabilities. Specifically, nodes have no information regarding\nthe local or global structure of the network, don't have access to synchronized\nclocks and are woken up by an adversary. Moreover, instead on communicating\nthrough messages they rely solely on carrier sensing to exchange information.\nWe study the problem of \\emph{interval coloring}, a variant of vertex coloring\nspecially suited for the studied beeping model. Given a set of resources, the\ngoal of interval coloring is to assign every node a large contiguous fraction\nof the resources, such that neighboring nodes share no resources. To highlight\nthe importance of the discreteness of the model, we contrast it against a\ncontinuous variant described in [17]. We present an O(1$ time algorithm that\nterminates with probability 1 and assigns an interval of size\n$\\Omega(T/\\Delta)$ that repeats every $T$ time units to every node of the\nnetwork. This improves an $O(\\log n)$ time algorithm with the same guarantees\npresented in \\cite{infocom09}, and accentuates the unrealistic assumptions of\nthe continuous model. Under the more realistic discrete model, we present a Las\nVegas algorithm that solves $\\Omega(T/\\Delta)$-interval coloring in $O(\\log n)$\ntime with high probability and describe how to adapt the algorithm for dynamic\nnetworks where nodes may join or leave. For constant degree graphs we prove a\nlower bound of $\\Omega(\\log n)$ on the time required to solve interval coloring\nfor this model against randomized algorithms. This lower bound implies that our\nalgorithm is asymptotically optimal for constant degree graphs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.3158v1", 
    "title": "Improvement Cache Efficiency of Explicit Finite Element Procedure and   its Application to Parallel Casting Solidification Simulation", 
    "arxiv-id": "1005.3158v1", 
    "author": "Ruhollah Tavakoli", 
    "publish": "2010-05-18T11:17:34Z", 
    "summary": "A simple method for improving cache efficiency of serial and parallel\nexplicit finite procedure with application to casting solidification simulation\nover three-dimensional complex geometries is presented. The method is based on\ndivision of the global data to smaller blocks and treating each block\nindependently from others at each time step. A novel parallel finite element\nalgorithm for non-overlapped element-base decomposed domain is presented for\nimplementation of serial and parallel version of the presented method. Effect\nof mesh reordering on the efficiency is also investigated. A simple algorithm\nis presented for high quality decomposition of decoupled global mesh. Our\nresult shows 10-20 \\% performance improvement by mesh reordering and 1.2-2.2\nspeedup with application of the presented cache efficient algorithm (for serial\nand parallel versions). Also the presented parallel solver (without\ncache-efficient feature) shows nearly linear speedup on the traditional\nEthernet networked Linux cluster."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.3367v2", 
    "title": "Bounding the Impact of Unbounded Attacks in Stabilization", 
    "arxiv-id": "1005.3367v2", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-19T06:34:03Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. Combining these two properties proved\ndifficult: it is impossible to contain the spatial impact of Byzantine nodes in\na self-stabilizing context for global tasks such as tree orientation and tree\nconstruction. We present and illustrate a new concept of Byzantine containment\nin stabilization. Our property, called Strong Stabilization enables to contain\nthe impact of Byzantine nodes if they actually perform too many Byzantine\nactions. We derive impossibility results for strong stabilization and present\nstrongly stabilizing protocols for tree orientation and tree construction that\nare optimal with respect to the number of Byzantine nodes that can be tolerated\nin a self-stabilizing context."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcnc.2010.2303", 
    "link": "http://arxiv.org/pdf/1005.5223v1", 
    "title": "On Byzantine Containment Properties of the $min+1$ Protocol", 
    "arxiv-id": "1005.5223v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-05-28T06:28:10Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a breadth-first spanning tree in this context. Combining these two\nproperties proves difficult: we demonstrate that it is impossible to contain\nthe impact of Byzantine nodes in a strictly or strongly stabilizing manner. We\nthen adopt the weaker scheme of topology-aware strict stabilization and we\npresent a similar weakening of strong stabilization. We prove that the\nclassical $min+1$ protocol has optimal Byzantine containment properties with\nrespect to these criteria."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2210", 
    "link": "http://arxiv.org/pdf/1005.5435v1", 
    "title": "Dynamic management of transactions in distributed real-time processing   system", 
    "arxiv-id": "1005.5435v1", 
    "author": "S. C. Mehrotra", 
    "publish": "2010-05-29T07:57:30Z", 
    "summary": "Managing the transactions in real time distributed computing system is not\neasy, as it has heterogeneously networked computers to solve a single problem.\nIf a transaction runs across some different sites, it may commit at some sites\nand may failure at another site, leading to an inconsistent transaction. The\ncomplexity is increase in real time applications by placing deadlines on the\nresponse time of the database system and transactions processing. Such a system\nneeds to process Transactions before these deadlines expired. A series of\nsimulation study have been performed to analyze the performance under different\ntransaction management under conditions such as different workloads,\ndistribution methods, execution mode-distribution and parallel etc. The\nscheduling of data accesses are done in order to meet their deadlines and to\nminimize the number of transactions that missed deadlines. A new concept is\nintroduced to manage the transactions in dynamic ways rather than setting\ncomputing parameters in static ways. With this approach, the system gives a\nsignificant improvement in performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijma.2010.2202", 
    "link": "http://arxiv.org/pdf/1005.5440v1", 
    "title": "A Low Overhead Minimum Process Global Snapshop Collection Algorithm for   Mobile Distributed System", 
    "arxiv-id": "1005.5440v1", 
    "author": "Parveen Kumar", 
    "publish": "2010-05-29T08:29:45Z", 
    "summary": "Coordinated checkpointing is an effective fault tolerant technique in\ndistributed system as it avoids the domino effect and require minimum storage\nrequirement. Most of the earlier coordinated checkpoint algorithms block their\ncomputation during checkpointing and forces minimum-process or non-blocking but\nforces all nodes to takes checkpoint even though many of them may not be\nnecessary or non-blocking minimum-process but takes useless checkpoints or\nreduced useless checkpoint but has higher synchronization message overhead or\nhas high checkpoint request propagation time. Hence in mobile distributed\nsystems there is a great need of minimizing the number of communication message\nand checkpointing overhead as it raise new issues such as mobility, low\nbandwidth of wireless channels, frequently disconnections, limited battery\npower and lack of reliable stable storage on mobile nodes. In this paper, we\npropose a minimum-process coordinated checkpointing algorithm for mobile\ndistributed system where no useless checkpoints are taken, no blocking of\nprocesses takes place and enforces a minimum-number of processes to take\ncheckpoints. Our algorithm imposes low memory and computation overheads on MH's\nand low communication overheads on wireless channels. It avoids awakening of an\nMH if it is not required to take its checkpoint and has reduced latency time as\neach process involved in a global checkpoint can forward its own decision\ndirectly to the checkpoint initiator."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1005.5606v1", 
    "title": "Implementation of a Cloud Data Server (CDS) for Providing Secure Service   in E-Business", 
    "arxiv-id": "1005.5606v1", 
    "author": "D. Sasireka", 
    "publish": "2010-05-31T07:08:34Z", 
    "summary": "Cloud Data Servers is the novel approach for providing secure service to\ne-business .Millions of users are surfing the Cloud for various purposes,\ntherefore they need highly safe and persistent services. Usually hackers target\nparticular Operating Systems or a Particular Controller. Inspiteof several\nongoing researches Conventional Web Servers and its Intrusion Detection System\nmight not be able to detect such attacks. So we implement a Cloud Data Server\nwith Session Controller Architecture using Redundancy and Disconnected Data\nAccess Mechanism. In this paper, we generate the hash code using MD5 algorithm.\nWith the help of which we can circumvent even the attacks, which are undefined\nby traditional Systems .we implement Cloud Data Sever using Java and Hash Code\nbackup Management using My SQL. Here we Implement AES Algorithm for providing\nmore Security for the hash Code. The CDS using the Virtual Controller controls\nand monitors the Connections and modifications of the page so as to prevent\nmalicious users from hacking the website. In the proposed approach an activity\nanalyzer takes care of intimating the administrator about possible intrusions\nand the counter measures required to tackle them. The efficiency ratio of our\napproach is 98.21% compared with similar approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1005.5630v1", 
    "title": "Reliable Self-Stabilizing Communication for Quasi Rendezvous", 
    "arxiv-id": "1005.5630v1", 
    "author": "Christian Lavault", 
    "publish": "2010-05-31T09:09:25Z", 
    "summary": "The paper presents three self-stabilizing protocols for basic fair and\nreliable link communication primitives. We assume a link-register communication\nmodel under read/write atomicity, where every process can read from but cannot\nwrite into its neighbours' registers. The first primitive guarantees that any\nprocess writes a new value in its register(s) only after all its neighbours\nhave read the previous value, whatever the initial scheduling of processes'\nactions. The second primitive implements a \"weak rendezvous\" communication\nmechanism by using an alternating bit protocol: whenever a process\nconsecutively writes n values (possibly the same ones) in a register, each\nneighbour is guaranteed to read each value from the register at least once. On\nthe basis of the previous protocol, the third primitive implements a \"quasi\nrendezvous\": in words, this primitive ensures furthermore that there exists\nexactly one reading between two writing operations All protocols are\nself-stabilizing and run in asynchronous arbitrary networks. The goal of the\npaper is in handling each primitive by a separate procedure, which can be used\nas a \"black box\" in more involved self-stabilizing protocols."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1006.0308v1", 
    "title": "Energy-Efficient Management of Data Center Resources for Cloud   Computing: A Vision, Architectural Elements, and Open Challenges", 
    "arxiv-id": "1006.0308v1", 
    "author": "Jemal Abawajy", 
    "publish": "2010-06-02T06:45:07Z", 
    "summary": "Cloud computing is offering utility-oriented IT services to users worldwide.\nBased on a pay-as-you-go model, it enables hosting of pervasive applications\nfrom consumer, scientific, and business domains. However, data centers hosting\nCloud applications consume huge amounts of energy, contributing to high\noperational costs and carbon footprints to the environment. Therefore, we need\nGreen Cloud computing solutions that can not only save energy for the\nenvironment but also reduce operational costs. This paper presents vision,\nchallenges, and architectural elements for energy-efficient management of Cloud\ncomputing environments. We focus on the development of dynamic resource\nprovisioning and allocation algorithms that consider the synergy between\nvarious data center infrastructures (i.e., the hardware, power units, cooling\nand software), and holistically work to boost data center energy efficiency and\nperformance. In particular, this paper proposes (a) architectural principles\nfor energy-efficient management of Clouds; (b) energy-efficient resource\nallocation policies and scheduling algorithms considering quality-of-service\nexpectations, and devices power usage characteristics; and (c) a novel software\ntechnology for energy-efficient management of Clouds. We have validated our\napproach by conducting a set of rigorous performance evaluation study using the\nCloudSim toolkit. The results demonstrate that Cloud computing model has\nimmense potential as it offers significant performance gains as regards to\nresponse time and cost saving under dynamic workload scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdms.2010.2203", 
    "link": "http://arxiv.org/pdf/1006.1074v1", 
    "title": "Youpi, a Web-based Astronomical Image Processing Pipeline", 
    "arxiv-id": "1006.1074v1", 
    "author": "G. S\u00e9mah", 
    "publish": "2010-06-05T22:27:14Z", 
    "summary": "Youpi stands for \"YOUpi is your processing PIpeline\". It is a portable, easy\nto use web application providing high level functionalities to perform data\nreduction on scientific FITS images. It is built on top of open source\nprocessing tools that are released to the community by Terapix, in order to\norganize your data on a computer cluster, to manage your processing jobs in\nreal time and to facilitate teamwork by allowing fine-grain sharing of results\nand data. On the server side, Youpi is written in the Python programming\nlanguage and uses the Django web framework. On the client side, Ajax techniques\nare used along with the Prototype and script.aculo.us Javascript librairies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2301", 
    "link": "http://arxiv.org/pdf/1006.1177v1", 
    "title": "Efficient Resource Matching in Heterogeneous Grid Using Resource Vector", 
    "arxiv-id": "1006.1177v1", 
    "author": "George L Barnes", 
    "publish": "2010-06-07T06:22:05Z", 
    "summary": "In this paper, a method for efficient scheduling to obtain optimum job\nthroughput in a distributed campus grid environment is presented; Traditional\njob schedulers determine job scheduling using user and job resource attributes.\nUser attributes are related to current usage, historical usage, user priority\nand project access. Job resource attributes mainly comprise of soft\nrequirements (compilers, libraries) and hard requirements like memory, storage\nand interconnect. A job scheduler dispatches jobs to a resource if a job's hard\nand soft requirements are met by a resource. In current scenario during\nexecution of a job, if a resource becomes unavailable, schedulers are presented\nwith limited options, namely re-queuing job or migrating job to a different\nresource. Both options are expensive in terms of data and compute time. These\nsituations can be avoided, if the often ignored factor, availability time of a\nresource in a grid environment is considered. We propose resource rank\napproach, in which jobs are dispatched to a resource which has the highest rank\namong all resources that match the job's requirement. The results show that our\napproach can increase throughput of many serial / monolithic jobs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1191v1", 
    "title": "Survey on the Event Orderings Semantics Used for Distributed System", 
    "arxiv-id": "1006.1191v1", 
    "author": "Suhaidi Hassan", 
    "publish": "2010-06-07T07:26:00Z", 
    "summary": "Event ordering in distributed system (DS) is disputable and proactive subject\nin DS particularly with the emergence of multimedia synchronization. According\nto the literature, different type of event ordering is used for different DS\nmode such as asynchronous or synchronous. Recently, there are several novel\nimplementation of these types introduced to fulfill the demand for establishing\na certain order according to a specific criterion in DS with lighter\ncomplexity."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1401v2", 
    "title": "PhoenixCloud: Provisioning Resources for Heterogeneous Workloads in   Cloud Computing", 
    "arxiv-id": "1006.1401v2", 
    "author": "Xiutao Zang", 
    "publish": "2010-06-08T00:42:20Z", 
    "summary": "As more and more service providers choose Cloud platforms, which is provided\nby third party resource providers, resource providers needs to provision\nresources for heterogeneous workloads in different Cloud scenarios. Taking into\naccount the dramatic differences of heterogeneous workloads, can we\ncoordinately provision resources for heterogeneous workloads in Cloud\ncomputing? In this paper we focus on this important issue, which is\ninvestigated by few previous work. Our contributions are threefold: (1) we\nrespectively propose a coordinated resource provisioning solution for\nheterogeneous workloads in two typical Cloud scenarios: first, a large\norganization operates a private Cloud for two heterogeneous workloads; second,\na large organization or two service providers running heterogeneous workloads\nrevert to a public Cloud; (2) we build an agile system PhoenixCloud that\nenables a resource provider to create coordinated runtime environments on\ndemand for heterogeneous workloads when they are consolidated on a Cloud site;\nand (3) A comprehensive evaluation has been performed in experiments. For two\ntypical heterogeneous workload traces: parallel batch jobs and Web services,\nour experiments show that: a) in a private Cloud scenario, when the throughput\nis almost same like that of a dedicated cluster system, our solution decreases\nthe configuration size of a cluster by about 40%; b) in a public Cloud\nscenario, our solution decreases not only the total resource consumption, but\nalso the peak resource consumption maximally to 31% with respect to that of EC2\n+RightScale solution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.1926v1", 
    "title": "Roomy: A System for Space Limited Computations", 
    "arxiv-id": "1006.1926v1", 
    "author": "Daniel Kunkle", 
    "publish": "2010-06-09T23:12:33Z", 
    "summary": "There are numerous examples of problems in symbolic algebra in which the\nrequired storage grows far beyond the limitations even of the distributed RAM\nof a cluster. Often this limitation determines how large a problem one can\nsolve in practice. Roomy provides a minimally invasive system to modify the\ncode for such a computation, in order to use the local disks of a cluster or a\nSAN as a transparent extension of RAM.\n  Roomy is implemented as a C/C++ library. It provides some simple data\nstructures (arrays, unordered lists, and hash tables). Some typical programming\nconstructs that one might employ in Roomy are: map, reduce, duplicate\nelimination, chain reduction, pair reduction, and breadth-first search. All\naspects of parallelism and remote I/O are hidden within the Roomy library."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.2650v1", 
    "title": "A Study on Performance Analysis Tools for Applications Running on Large   Distributed Systems", 
    "arxiv-id": "1006.2650v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2010-06-14T09:39:45Z", 
    "summary": "The evolution of distributed architectures and programming paradigms for\nperformance-oriented program development, challenge the state-of-the-art\ntechnology for performance tools. The area of high performance computing is\nrapidly expanding from single parallel systems to clusters and grids of\nheterogeneous sequential and parallel systems. Performance analysis and tuning\napplications is becoming crucial because it is hardly possible to otherwise\nachieve the optimum performance of any application. The objective of this paper\nis to study the state-of-the-art technology of the existing performance tools\nfor distributed systems. The paper surveys some representative tools from\ndifferent aspects in order to highlight the approaches and technologies used by\nthem."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3432v1", 
    "title": "Snap-Stabilizing Linear Message Forwarding", 
    "arxiv-id": "1006.3432v1", 
    "author": "Vincent Villain", 
    "publish": "2010-06-17T11:22:27Z", 
    "summary": "In this paper, we present the first snap-stabilizing message forwarding\nprotocol that uses a number of buffers per node being inde- pendent of any\nglobal parameter, that is 4 buffers per link. The protocol works on a linear\nchain of nodes, that is possibly an overlay on a large- scale and dynamic\nsystem, e.g., Peer-to-Peer systems, Grids. . . Provided that the topology\nremains a linear chain and that nodes join and leave \"neatly\", the protocol\ntolerates topology changes. We expect that this protocol will be the base to\nget similar results on more general topologies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3452v1", 
    "title": "Generating a Family of Byzantine Tolerant Protocol Implementations Using   a Meta-Model Architecture", 
    "arxiv-id": "1006.3452v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-17T12:46:19Z", 
    "summary": "We describe an approach to modelling a Byzantine tolerant distributed\nalgorithm as a family of related finite state machines, generated from a single\nmeta-model. Various artefacts are generated from each state machine, including\ndiagrams and source-level protocol implementations. The approach allows a state\nmachine formulation to be applied to problems for which it would not otherwise\nbe suitable, increasing confidence in correctness."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3465v1", 
    "title": "Hosting Byzantine Fault Tolerant Services on a Chord Ring", 
    "arxiv-id": "1006.3465v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-17T13:40:59Z", 
    "summary": "In this paper we demonstrate how stateful Byzantine Fault Tolerant services\nmay be hosted on a Chord ring. The strategy presented is fourfold: firstly a\nreplication scheme that dissociates the maintenance of replicated service state\nfrom ring recovery is developed. Secondly, clients of the ring based services\nare made replication aware. Thirdly, a consensus protocol is introduced that\nsupports the serialization of updates. Finally Byzantine fault tolerant\nreplication protocols are developed that ensure the integrity of service data\nhosted on the ring."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3724v1", 
    "title": "A Peer-to-Peer Middleware Framework for Resilient Persistent Programming", 
    "arxiv-id": "1006.3724v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T15:35:50Z", 
    "summary": "The persistent programming systems of the 1980s offered a programming model\nthat integrated computation and long-term storage. In these systems, reliable\napplications could be engineered without requiring the programmer to write\ntranslation code to manage the transfer of data to and from non-volatile\nstorage. More importantly, it simplified the programmer's conceptual model of\nan application, and avoided the many coherency problems that result from\nmultiple cached copies of the same information. Although technically\ninnovative, persistent languages were not widely adopted, perhaps due in part\nto their closed-world model. Each persistent store was located on a single\nhost, and there were no flexible mechanisms for communication or transfer of\ndata between separate stores. Here we re-open the work on persistence and\ncombine it with modern peer-to-peer techniques in order to provide support for\northogonal persistence in resilient and potentially long-running distributed\napplications. Our vision is of an infrastructure within which an application\ncan be developed and distributed with minimal modification, whereupon the\napplication becomes resilient to certain failure modes. If a node, or the\nconnection to it, fails during execution of the application, the objects are\nre-instantiated from distributed replicas, without their reference holders\nbeing aware of the failure. Furthermore, we believe that this can be achieved\nwithin a spectrum of application programmer intervention, ranging from minimal\nto totally prescriptive, as desired. The same mechanisms encompass an\northogonally persistent programming model. We outline our approach to\nimplementing this vision, and describe current progress."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3728v1", 
    "title": "RAFDA: A Policy-Aware Middleware Supporting the Flexible Separation of   Application Logic from Distribution", 
    "arxiv-id": "1006.3728v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T15:39:22Z", 
    "summary": "Middleware technologies often limit the way in which object classes may be\nused in distributed applications due to the fixed distribution policies that\nthey impose. These policies permeate applications developed using existing\nmiddleware systems and force an unnatural encoding of application level\nsemantics. For example, the application programmer has no direct control over\ninter-address-space parameter passing semantics. Semantics are fixed by the\ndistribution topology of the application, which is dictated early in the design\ncycle. This creates applications that are brittle with respect to changes in\ndistribution. This paper explores technology that provides control over the\nextent to which inter-address-space communication is exposed to programmers, in\norder to aid the creation, maintenance and evolution of distributed\napplications. The described system permits arbitrary objects in an application\nto be dynamically exposed for remote access, allowing applications to be\nwritten without concern for distribution. Programmers can conceal or expose the\ndistributed nature of applications as required, permitting object placement and\ndistribution boundaries to be decided late in the design cycle and even\ndynamically. Inter-address-space parameter passing semantics may also be\ndecided independently of object implementation and at varying times in the\ndesign cycle, again possibly as late as run-time. Furthermore, transmission\npolicy may be defined on a per-class, per-method or per-parameter basis,\nmaximizing plasticity. This flexibility is of utility in the development of new\ndistributed applications, and the creation of management and monitoring\ninfrastructures for existing applications."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3732v1", 
    "title": "Towards Adaptable and Adaptive Policy-Free Middleware", 
    "arxiv-id": "1006.3732v1", 
    "author": "Greg Bigwood", 
    "publish": "2010-06-18T15:47:35Z", 
    "summary": "We believe that to fully support adaptive distributed applications,\nmiddleware must itself be adaptable, adaptive and policy-free. In this paper we\npresent a new language-independent adaptable and adaptive policy framework\nsuitable for integration in a wide variety of middleware systems. This\nframework facilitates the construction of adaptive distributed applications.\nThe framework addresses adaptability through its ability to represent a wide\nrange of specific middleware policies. Adaptiveness is supported by a rich\ncontextual model, through which an application programmer may control precisely\nhow policies should be selected for any particular interaction with the\nmiddleware. A contextual pattern mechanism facilitates the succinct expression\nof both coarse- and fine-grain policy contexts. Policies may be specified and\naltered dynamically, and may themselves take account of dynamic conditions. The\nframework contains no hard-wired policies; instead, all policies can be\nconfigured."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3739v1", 
    "title": "Promoting Component Reuse by Separating Transmission Policy from   Implementation", 
    "arxiv-id": "1006.3739v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-18T16:12:58Z", 
    "summary": "In this paper we present a methodology and set of tools which assist the\nconstruction of applications from components, by separating the issues of\ntransmission policy from component definition and implementation. This promotes\na greater degree of software reuse than is possible using traditional\nmiddleware environments. Whilst component technologies are usually presented as\na mechanism for promoting reuse, reuse is often limited due to design choices\nthat permeate component implementation. The programmer has no direct control\nover inter-address-space parameter passing semantics: it is fixed by the\ndistributed application's structure, based on the remote accessibility of the\ncomponents. Using traditional middleware tools and environments, the\napplication designer may be forced to use an unnatural encoding of application\nlevel semantics since application parameter passing semantics are tightly\ncoupled with the component deployment topology. This paper describes how\ninter-address-space parameter passing semantics may be decided independently of\ncomponent implementation. Transmission policy may be dynamically defined on a\nper-class, per-method or per-parameter basis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3742v1", 
    "title": "RAFDA: Middleware Supporting the Separation of Application Logic from   Distribution Policy", 
    "arxiv-id": "1006.3742v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-18T16:22:10Z", 
    "summary": "Middleware technologies often limit the way in which object classes may be\nused in distributed applications due to the fixed distribution policies imposed\nby the Middleware system. These policies permeate the applications developed\nusing them and force an unnatural encoding of application level semantics. For\nexample, the application programmer has no direct control over\ninter-address-space parameter passing semantics since it is fixed by the\napplication's distribution topology which is dictated early in the design cycle\nby the Middleware. This creates applications that are brittle with respect to\nchanges in the way in which the applications are distributed. This paper\nexplores technology permitting arbitrary objects in an application to be\ndynamically exposed for remote access. Using this, the application can be\nwritten without concern for its distribution with object placement and\ndistribution boundaries decided late in the design cycle and even dynamically.\nInter-address-space parameter passing semantics may also be decided\nindependently of object implementation and at varying times in the design\ncycle, again, possibly as late as run-time. Furthermore, transmission policy\nmay be defined on a per-class, per-method or per-parameter basis maximizing\nplasticity. This flexibility is of utility in the development of new\ndistributed applications and the creation of management and monitoring\ninfrastructures for existing applications."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijcsit.2010.2311", 
    "link": "http://arxiv.org/pdf/1006.3909v2", 
    "title": "Constructing Two Edge-Disjoint Hamiltonian Cycles and Two Equal   Node-Disjoint Cycles in Twisted Cubes", 
    "arxiv-id": "1006.3909v2", 
    "author": "Ruo-Wei Hung", 
    "publish": "2010-06-20T05:37:07Z", 
    "summary": "The hypercube is one of the most popular interconnection networks since it\nhas simple structure and is easy to implement. The $n$-dimensional twisted\ncube, denoted by $TQ_n$, an important variation of the hypercube, possesses\nsome properties superior to the hypercube. Recently, some interesting\nproperties of $TQ_n$ were investigated. In this paper, we construct two\nedge-disjoint Hamiltonian cycles in $TQ_n$ for any odd integer $n\\geqslant 5$.\nThe presence of two edge-disjoint Hamiltonian cycles provides an advantage when\nimplementing two algorithms that require a ring structure by allowing message\ntraffic to be spread evenly across the twisted cube. Furthermore, we construct\ntwo equal node-disjoint cycles in $TQ_n$ for any odd integer $n\\geqslant 3$, in\nwhich these two cycles contain the same number of nodes and every node appears\nin one cycle exactly once. In other words, we decompose a twisted cube into two\ncomponents with the same size such that each component contains a Hamiltonian\ncycle."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.3919v1", 
    "title": "Convergence-Optimal Quantizer Design of Distributed Contraction-based   Iterative Algorithms with Quantized Message Passing", 
    "arxiv-id": "1006.3919v1", 
    "author": "Vincent K. N. Lau", 
    "publish": "2010-06-20T08:32:16Z", 
    "summary": "In this paper, we study the convergence behavior of distributed iterative\nalgorithms with quantized message passing. We first introduce general iterative\nfunction evaluation algorithms for solving fixed point problems distributively.\nWe then analyze the convergence of the distributed algorithms, e.g. Jacobi\nscheme and Gauss-Seidel scheme, under the quantized message passing. Based on\nthe closed-form convergence performance derived, we propose two quantizer\ndesigns, namely the time invariant convergence-optimal quantizer (TICOQ) and\nthe time varying convergence-optimal quantizer (TVCOQ), to minimize the effect\nof the quantization error on the convergence. We also study the tradeoff\nbetween the convergence error and message passing overhead for both TICOQ and\nTVCOQ. As an example, we apply the TICOQ and TVCOQ designs to the iterative\nwaterfilling algorithm of MIMO interference game."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4504v1", 
    "title": "Exposing Application Components as Web Services", 
    "arxiv-id": "1006.4504v1", 
    "author": "Stuart Norcross", 
    "publish": "2010-06-23T12:59:19Z", 
    "summary": "This paper explores technology permitting arbitrary application components to\nbe exposed for remote access from other software. Using this, the application\nand its constituent components can be written without concern for its\ndistribution. Software running in different address spaces, on different\nmachines, can perform operations on the remotely accessible components. This is\nof utility in the creation of distributed applications and in permitting tools\nsuch as debuggers, component browsers, observers or remote probes access to\napplication components. Current middleware systems do not allow arbitrary\nexposure of application components: instead, the programmer is forced to decide\nstatically which classes of component will support remote accessibility. In the\nwork described here, arbitrary components of any class can be dynamically\nexposed via Web Services. Traditional Web Services are extended with a remote\nreference scheme. This extension permits application components to be invoked\nusing either the traditional pass-by-value semantics supported by Web Services\nor pass-by-reference semantics. The latter permits the preservation of local\ncall semantics across address space boundaries."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4549v1", 
    "title": "A Flexible and Secure Deployment Framework for Distributed Applications", 
    "arxiv-id": "1006.4549v1", 
    "author": "Juan-Carlos Diaz y Carballo", 
    "publish": "2010-06-23T15:07:26Z", 
    "summary": "This paper describes an implemented system which is designed to support the\ndeployment of applications offering distributed services, comprising a number\nof distributed components. This is achieved by creating high level placement\nand topology descriptions which drive tools that deploy applications consisting\nof components running on multiple hosts. The system addresses issues of\nheterogeneity by providing abstractions over host-specific attributes yielding\na homogeneous run-time environment into which components may be deployed. The\nrun-time environments provide secure binding mechanisms that permit deployed\ncomponents to bind to stored data and services on the hosts on which they are\nrunning."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4572v1", 
    "title": "A Framework for Constraint-Based Deployment and Autonomic Management of   Distributed Applications", 
    "arxiv-id": "1006.4572v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-23T15:28:42Z", 
    "summary": "We propose a framework for deployment and subsequent autonomic management of\ncomponent-based distributed applications. An initial deployment goal is\nspecified using a declarative constraint language, expressing constraints over\naspects such as component-host mappings and component interconnection topology.\nA constraint solver is used to find a configuration that satisfies the goal,\nand the configuration is deployed automatically. The deployed application is\ninstrumented to allow subsequent autonomic management. If, during execution,\nthe manager detects that the original goal is no longer being met, the\nsatisfy/deploy process can be repeated automatically in order to generate a\nrevised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2010.2055861", 
    "link": "http://arxiv.org/pdf/1006.4708v1", 
    "title": "A Survey on Peer-to-Peer and DHT", 
    "arxiv-id": "1006.4708v1", 
    "author": "Siamak Sarmady", 
    "publish": "2010-06-24T07:56:26Z", 
    "summary": "Distributed systems with different levels of dependence to central services\nhave been designed and used during recent years. Pure peer-to-peer systems\namong distributed systems have no dependence on a central resource. DHT is one\nof the main techniques behind these systems resulting into failure tolerant\nsystems which are also able to isolate continuous changes to the network to a\nsmall section of it and therefore making it possible to scale up such networks\nto millions of nodes. This survey takes a look at P2P in general and DHT\nalgorithms and implementations in more detail."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4730v1", 
    "title": "A Framework for Constraint-Based Deployment and Autonomic Management of   Distributed Applications (Extended Abstract)", 
    "arxiv-id": "1006.4730v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-24T09:44:38Z", 
    "summary": "We propose a framework for the deployment and subsequent autonomic management\nof component-based distributed applications. An initial deployment goal is\nspecified using a declarative constraint language, expressing constraints over\naspects such as component-host mappings and component interconnection topology.\nA constraint solver is used to find a configuration that satisfies the goal,\nand the configuration is deployed automatically. The deployed application is\ninstrumented to allow subsequent autonomic management. If, during execution,\nthe manager detects that the original goal is no longer being met, the\nsatisfy/deploy process can be repeated automatically in order to generate a\nrevised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4733v1", 
    "title": "A Middleware Framework for Constraint-Based Deployment and Autonomic   Management of Distributed Applications", 
    "arxiv-id": "1006.4733v1", 
    "author": "Andrew McCarthy", 
    "publish": "2010-06-24T09:53:35Z", 
    "summary": "We propose a middleware framework for deployment and subsequent autonomic\nmanagement of component-based distributed applications. An initial deployment\ngoal is specified using a declarative constraint language, expressing\nconstraints over aspects such as component-host mappings and component\ninterconnection topology. A constraint solver is used to find a configuration\nthat satisfies the goal, and the configuration is deployed automatically. The\ndeployed application is instrumented to allow subsequent autonomic management.\nIf, during execution, the manager detects that the original goal is no longer\nbeing met, the satisfy/deploy process can be repeated automatically in order to\ngenerate a revised deployment that does meet the goal."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICAC.2004.3", 
    "link": "http://arxiv.org/pdf/1006.4746v1", 
    "title": "Active Architecture for Pervasive Contextual Services", 
    "arxiv-id": "1006.4746v1", 
    "author": "Paddy Nixon", 
    "publish": "2010-06-24T11:03:00Z", 
    "summary": "Pervasive services may be defined as services that are available \"to any\nclient (anytime, anywhere)\". Here we focus on the software and network\ninfrastructure required to support pervasive contextual services operating over\na wide area. One of the key requirements is a matching service capable of\nas-similating and filtering information from various sources and determining\nmatches relevant to those services. We consider some of the challenges in\nengineering a globally distributed matching service that is scalable,\nmanageable, and able to evolve incrementally as usage patterns, data formats,\nservices, network topologies and deployment technologies change. We outline an\napproach based on the use of a peer-to-peer architecture to distribute user\nevents and data, and to support the deployment and evolution of the\ninfrastructure itself."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.4827v1", 
    "title": "Architectural Support for Global Smart Spaces", 
    "arxiv-id": "1006.4827v1", 
    "author": "Andy Wilson", 
    "publish": "2010-06-24T16:27:03Z", 
    "summary": "A GLObal Smart Space (GLOSS) provides support for interaction amongst people,\nartefacts and places while taking account of both context and movement on a\nglobal scale. Crucial to the definition of a GLOSS is the provision of a set of\nlocation-aware services that detect, convey, store and exploit location\ninformation. We use one of these services, hearsay, to illustrate the\nimplementation dimensions of a GLOSS. The focus of the paper is on both local\nand global software architecture to support the implementation of such\nservices. The local architecture is based on XML pipe-lines and is used to\nconstruct location-aware components. The global architecture is based on a\nhybrid peer-to-peer routing scheme and provides the local architectures with\nthe means to communicate in the global context."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.4900v2", 
    "title": "Optimal Degree Distributions for Uniform Small World Rings", 
    "arxiv-id": "1006.4900v2", 
    "author": "James D. Fix", 
    "publish": "2010-06-25T02:14:57Z", 
    "summary": "Motivated by Kleinberg's (2000) and subsequent work, we consider the\nperformance of greedy routing on a directed ring of $n$ nodes augmented with\nlong-range contacts. In this model, each node $u$ is given an additional $D_u$\nedges, a degree chosen from a specified probability distribution. Each such\nedge from $u$ is linked to a random node at distance $r$ ahead in the ring with\nprobability proportional to $1/r$, a \"harmonic\" distance distribution of\ncontacts. Aspnes et al. (2002) have shown an $O(\\log^2 n / \\ell)$ bound on the\nexpected length of greedy routes in the case when each node is assigned exactly\n$\\ell$ contacts and, as a consequence of recent work by Dietzfelbinger and\nWoelfel (2009), this bound is known to be tight. In this paper, we generalize\nAspnes' upper bound to show that any degree distribution with mean $\\ell$ and\nmaximum value $O(\\log n)$ has greedy routes of expected length $O(\\log^2n /\n\\ell)$, implying that any harmonic ring in this family is asymptotically\noptimal. Furthermore, for a more general family of rings, we show that a fixed\ndegree distribution is optimal. More precisely, if each random contact is\nchosen at distance $r$ with a probability that decreases with $r$, then among\ndegree distributions with mean $\\ell$, greedy routing time is smallest when\nevery node is assigned $\\floor{\\ell}$ or $\\ceiling{\\ell}$ contacts."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5309v1", 
    "title": "Data Partitioning for Parallel Entity Matching", 
    "arxiv-id": "1006.5309v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-06-28T10:25:53Z", 
    "summary": "Entity matching is an important and difficult step for integrating web data.\nTo reduce the typically high execution time for matching we investigate how we\ncan perform entity matching in parallel on a distributed infrastructure. We\npropose different strategies to partition the input data and generate multiple\nmatch tasks that can be independently executed. One of our strategies supports\nboth, blocking to reduce the search space for matching and parallel matching to\nimprove efficiency. Special attention is given to the number and size of data\npartitions as they impact the overall communication overhead and memory\nrequirements of individual match tasks. We have developed a service-based\ndistributed infrastructure for the parallel execution of match workflows. We\nevaluate our approach in detail for different match strategies for matching\nreal-world product data of different web shops. We also consider caching of\nin-put entities and affinity-based scheduling of match tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5376v1", 
    "title": "Resource Allocation using Virtual Clusters", 
    "arxiv-id": "1006.5376v1", 
    "author": "Henri Casanova", 
    "publish": "2010-06-28T15:28:43Z", 
    "summary": "In this report we demonstrate the potential utility of resource allocation\nmanagement systems that use virtual machine technology for sharing parallel\ncomputing resources among competing jobs. We formalize the resource allocation\nproblem with a number of underlying assumptions, determine its complexity,\npropose several heuristic algorithms to find near-optimal solutions, and\nevaluate these algorithms in simulation. We find that among our algorithms one\nis very efficient and also leads to the best resource allocations. We then\ndescribe how our approach can be made more general by removing several of the\nunderlying assumptions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5572v1", 
    "title": "A Multi-Core Processor Platform for Open Embedded Systems", 
    "arxiv-id": "1006.5572v1", 
    "author": "Hiroaki Inoue", 
    "publish": "2010-06-29T11:35:55Z", 
    "summary": "Recent proliferation of embedded systems has generated a bold new paradigm,\nknown as open embedded systems. While traditional embedded systems provide only\nclosed base applications (natively-installed software) to users, open embedded\nsystems allow the users to freely execute open applications\n(additionally-installed software) in order to meet various user requirements,\nsuch as user personalization and device coordination. Key to the success of\nplatforms required for open embedded systems is the achievement of both the\nscalable extension of base applications and the secure execution of open\napplications. Most existing platforms, however, have focused on either scalable\nor secure execution, limiting their applicability. This dissertation presents a\nnew secure platform using multi-core processors, which achieves both\nscalability and security. Four techniques feature the new platform: (1)\nseamless communication, by which legacy applications designed for a single\nprocessor make it possible to be executed on multiple processors without any\nsoftware modifications; (2) secure processor partitioning with hardware\nsupport, by which Operating Systems (OSs) required for base and open\napplications are securely executed on separate processors; (3) asymmetric\nvirtualization, by which many OSs over the number of processors are securely\nexecuted under secure processor partitioning; and (4) secure dynamic\npartitioning, by which the number of processors allocated to individual OSs\nmakes it possible to be dynamically changed under secure processor\npartitioning."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5643v1", 
    "title": "A Reflective Approach to Providing Flexibility in Application   Distribution", 
    "arxiv-id": "1006.5643v1", 
    "author": "Alan Dearle", 
    "publish": "2010-06-29T14:57:44Z", 
    "summary": "Current middleware systems suffer from drawbacks. Often one is forced to make\ndecisions early in the design process about which classes may participate in\ninter-machine communication. Further, application level and middleware specific\nsemantics cannot be separated forcing an unnatural design. The RAFDA project\nproposes to adress these deficiencies by creating an adaptive, reflective\nframework that enables the transformation of non-distributed applications into\nisomorphic applications whose distribution architecture is flexible. This paper\ndescribes the code transformation techniques that have been developed as part\nof the project. The system enables the distribution of a program according to a\nflexible configuration without user intervention. Proxy objects can then be\nsubstituted, permitting cross-address space communication. The distributed\nprogram can adapt to its environment by dynamically altering its distribution\nboundaries."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5661v1", 
    "title": "Working Document on Gloss Ontology", 
    "arxiv-id": "1006.5661v1", 
    "author": "Evangelos Zirintsis", 
    "publish": "2010-06-29T15:59:22Z", 
    "summary": "This document describes the Gloss Ontology. The ontology and associated class\nmodel are organised into several packages. Section 2 describes each package in\ndetail, while Section 3 contains a summary of the whole ontology."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5940v1", 
    "title": "An Information Flow Architecture for Global Smart Spaces", 
    "arxiv-id": "1006.5940v1", 
    "author": "Juan-Carlos Diaz y Carballo", 
    "publish": "2010-06-29T14:32:01Z", 
    "summary": "In this paper we describe an architecture which: Permits the deployment and\nexecution of components in appropriate geographical locations. Provides\nsecurity mechanisms that prevent misuse of the architecture. Supports a\nprogramming model that is familiar to application programmers. Permits\ninstalled components to share data. Permits the deployed components to\ncommunicate via communication channels. Provides evolution mechanisms\npermitting the dynamic rearrangement of inter-connection topologies the\ncomponents that they connect. Supports the specification and deployment of\ndistributed component deployments."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1006.5941v1", 
    "title": "Second Set of Spaces", 
    "arxiv-id": "1006.5941v1", 
    "author": "Steven Harris", 
    "publish": "2010-06-29T15:53:53Z", 
    "summary": "This document describes the Gloss infrastructure supporting implementation of\nlocation-aware services. The document is in two parts. The first part describes\nsoftware architecture for the smart space. As described in D8, a local\narchitecture provides a framework for constructing Gloss applications, termed\nassemblies, that run on individual physical nodes, whereas a global\narchitecture defines an overlay network for linking individual assemblies. The\nsecond part outlines the hardware installation for local sensing. This\ndescribes the first phase of the installation in Strathclyde University."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1008.0011v1", 
    "title": "Parallel and distributed Gr\u00f6bner bases computation in JAS", 
    "arxiv-id": "1008.0011v1", 
    "author": "Heinz Kredel", 
    "publish": "2010-07-30T20:38:38Z", 
    "summary": "This paper considers parallel Gr\\\"obner bases algorithms on distributed\nmemory parallel computers with multi-core compute nodes. We summarize three\ndifferent Gr\\\"obner bases implementations: shared memory parallel, pure\ndistributed memory parallel and distributed memory combined with shared memory\nparallelism. The last algorithm, called distributed hybrid, uses only one\ncontrol communication channel between the master node and the worker nodes and\nkeeps polynomials in shared memory on a node. The polynomials are transported\nasynchronous to the control-flow of the algorithm in a separate distributed\ndata structure. The implementation is generic and works for all implemented\n(exact) fields. We present new performance measurements and discuss the\nperformance of the algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1008.0064v1", 
    "title": "Self-repairing Homomorphic Codes for Distributed Storage Systems", 
    "arxiv-id": "1008.0064v1", 
    "author": "Anwitaman Datta", 
    "publish": "2010-07-31T07:37:26Z", 
    "summary": "Erasure codes provide a storage efficient alternative to replication based\nredundancy in (networked) storage systems. They however entail high\ncommunication overhead for maintenance, when some of the encoded fragments are\nlost and need to be replenished. Such overheads arise from the fundamental need\nto recreate (or keep separately) first a copy of the whole object before any\nindividual encoded fragment can be generated and replenished. There has been\nrecently intense interest to explore alternatives, most prominent ones being\nregenerating codes (RGC) and hierarchical codes (HC). We propose as an\nalternative a new family of codes to improve the maintenance process, which we\ncall self-repairing codes (SRC), with the following salient features: (a)\nencoded fragments can be repaired directly from other subsets of encoded\nfragments without having to reconstruct first the original data, ensuring that\n(b) a fragment is repaired from a fixed number of encoded fragments, the number\ndepending only on how many encoded blocks are missing and independent of which\nspecific blocks are missing. These properties allow for not only low\ncommunication overhead to recreate a missing fragment, but also independent\nreconstruction of different missing fragments in parallel, possibly in\ndifferent parts of the network. We analyze the static resilience of SRCs with\nrespect to traditional erasure codes, and observe that SRCs incur marginally\nlarger storage overhead in order to achieve the aforementioned properties. The\nsalient SRC properties naturally translate to low communication overheads for\nreconstruction of lost fragments, and allow reconstruction with lower latency\nby facilitating repairs in parallel. These desirable properties make\nself-repairing codes a good and practical candidate for networked distributed\nstorage systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1008.0451v1", 
    "title": "On Optimal Deadlock Detection Scheduling", 
    "arxiv-id": "1008.0451v1", 
    "author": "Cho-Yu Jason Chiang", 
    "publish": "2010-08-03T03:46:56Z", 
    "summary": "Deadlock detection scheduling is an important, yet often overlooked problem\nthat can significantly affect the overall performance of deadlock handling.\nExcessive initiation of deadlock detection increases overall message usage,\nresulting in degraded system performance in the absence of deadlocks; while\ninsufficient initiation of deadlock detection increases the deadlock\npersistence time, resulting in an increased deadlock resolution cost in the\npresence of deadlocks. The investigation of this performance tradeoff, however,\nis missing in the literature. This paper studies the impact of deadlock\ndetection scheduling on the overall performance of deadlock handling. In\nparticular, we show that there exists an optimal deadlock detection frequency\nthat yields the minimum long-run mean average cost, which is determined by the\nmessage complexities of the deadlock detection and resolution algorithms being\nused, as well as the rate of deadlock formation, denoted as $\\lambda$. For the\nbest known deadlock detection and resolution algorithms, we show that the\nasymptotically optimal frequency of deadlock detection scheduling that\nminimizes the overall message overhead is ${\\cal O}((\\lambda n)^{1/3})$, when\nthe total number $n$ of processes is sufficiently large. Furthermore, we show\nthat in general fully distributed (uncoordinated) deadlock detection scheduling\ncannot be performed as efficiently as centralized (coordinated) deadlock\ndetection scheduling."
},{
    "category": "cs.DC", 
    "doi": "10.1007/3-540-36389-0_11", 
    "link": "http://arxiv.org/pdf/1008.1900v1", 
    "title": "The Cloud Adoption Toolkit: Supporting Cloud Adoption Decisions in the   Enterprise", 
    "arxiv-id": "1008.1900v1", 
    "author": "Ian Sommerville", 
    "publish": "2010-08-11T12:56:32Z", 
    "summary": "Cloud computing promises a radical shift in the provisioning of computing\nresource within the enterprise. This paper describes the challenges that\ndecision makers face when assessing the feasibility of the adoption of cloud\ncomputing in their organisations, and describes our Cloud Adoption Toolkit,\nwhich has been developed to support this process. The toolkit provides a\nframework to support decision makers in identifying their concerns, and\nmatching these concerns to appropriate tools/techniques that can be used to\naddress them. Cost Modeling is the most mature tool in the toolkit, and this\npaper shows its effectiveness by demonstrating how practitioners can use it to\nexamine the costs of deploying their IT systems on the cloud. The Cost Modeling\ntool is evaluated using a case study of an organization that is considering the\nmigration of some of its IT systems to the cloud. The case study shows that\nrunning systems on the cloud using a traditional \"always on\" approach can be\nless cost effective, and the elastic nature of the cloud has to be used to\nreduce costs. Therefore, decision makers have to be able to model the\nvariations in resource usage and their systems deployment options to obtain\naccurate cost estimates."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1008.2767v1", 
    "title": "A Light-Weight Communication Library for Distributed Computing", 
    "arxiv-id": "1008.2767v1", 
    "author": "Simon Portegies Zwart", 
    "publish": "2010-08-16T20:14:11Z", 
    "summary": "We present MPWide, a platform independent communication library for\nperforming message passing between computers. Our library allows coupling of\nseveral local MPI applications through a long distance network and is\nspecifically optimized for such communications. The implementation is\ndeliberately kept light-weight, platform independent and the library can be\ninstalled and used without administrative privileges. The only requirements are\na C++ compiler and at least one open port to a wide area network on each site.\nIn this paper we present the library, describe the user interface, present\nperformance tests and apply MPWide in a large scale cosmological N-body\nsimulation on a network of two computers, one in Amsterdam and the other in\nTokyo."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1008.4551v1", 
    "title": "Deterministic Consensus Algorithm with Linear Per-Bit Complexity", 
    "arxiv-id": "1008.4551v1", 
    "author": "Nitin Vaidya", 
    "publish": "2010-08-26T17:29:31Z", 
    "summary": "In this report, building on the deterministic multi-valued one-to-many\nByzantine agreement (broadcast) algorithm in our recent technical report [2],\nwe introduce a deterministic multi-valued all-to-all Byzantine agreement\nalgorithm (consensus), with linear complexity per bit agreed upon. The\ndiscussion in this note is not self-contained, and relies heavily on the\nmaterial in [2] - please refer to [2] for the necessary background."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1009.0056v1", 
    "title": "A Competitive Analysis for Balanced Transactional Memory Workloads", 
    "arxiv-id": "1009.0056v1", 
    "author": "Costas Busch", 
    "publish": "2010-08-31T23:54:57Z", 
    "summary": "We consider transactional memory contention management in the context of\nbalanced workloads, where if a transaction is writing, the number of write\noperations it performs is a constant fraction of its total reads and writes. We\nexplore the theoretical performance boundaries of contention management in\nbalanced workloads from the worst-case perspective by presenting and analyzing\ntwo new contention management algorithms. The first algorithm Clairvoyant is\nO(\\surd s)-competitive, where s is the number of shared resources. This\nalgorithm depends on explicitly knowing the conflict graph. The second\nalgorithm Non-Clairvoyant is O(\\surd s \\cdot log n)-competitive, with high\nprobability, which is only a O(log n) factor worse, but does not require\nknowledge of the conflict graph, where n is the number of transactions. Both of\nthese algorithms are greedy. We also prove that the performance of Clairvoyant\nis tight since there is no contention management algorithm that is better than\nO((\\surd s)^(1-\\epsilon))-competitive for any constant \\epsilon > 0, unless\nNP\\subseteq ZPP. To our knowledge, these results are significant improvements\nover the best previously known O(s) competitive ratio bound."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1009.2314v1", 
    "title": "Mapping Cloud Computing onto Useful e-Governance", 
    "arxiv-id": "1009.2314v1", 
    "author": "Deepak Gour", 
    "publish": "2010-09-13T07:36:47Z", 
    "summary": "Most of the services viewed in context to grid and cloud computing are mostly\nconfined to services that are available for intellectual purposes. The grid or\ncloud computing are large scale distributed systems. The essence of large scale\ndistribution can only be realized if the services are rendered to common man.\nThe only organization which has exposure to almost every single resident is the\nrespective governments in every country. As the size of population increases so\nthe need for a larger purview arises. The problem of having a large purview can\nbe solved by means of large scale grid for online services. The government\nservices can be rendered through fully customized Service-oriented Clouds. In\nthis paper we are presenting tight similarities between generic government\nfunctioning and the service oriented grid/cloud approach. Also, we will discuss\nthe major issues in establishing services oriented grids for governmental\norganization."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1009.4048v1", 
    "title": "A Middleware road towards Web (Grid) Services", 
    "arxiv-id": "1009.4048v1", 
    "author": "Zeeshan Ahmed", 
    "publish": "2010-09-21T10:29:50Z", 
    "summary": "Middleware technologies is a very big field, containing a strong already done\nresearch as well as the currently running research to confirm already done\nresearch's results and the to have some new solution by theoretical as well as\nthe experimental (practical) way. This document has been produced by Zeeshan\nAhmed (Student: Connectivity Software Technologies Blekinge Institute of\nTechnologies). This describes the research already done in the field of\nmiddleware technologies including Web Services, Grid Computing, Grid Services\nand Open Grid Service Infrastructure & Architecture. This document concludes\nwith the overview of Web (Grid) Service, Chain of Web (Grid) Services and the\nnecessary security issue."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1749-4699/3/1/015002", 
    "link": "http://arxiv.org/pdf/1009.4870v1", 
    "title": "Hallway Monitoring: Distributed Data Processing with Wireless Sensor   Networks", 
    "arxiv-id": "1009.4870v1", 
    "author": "Max Pagel", 
    "publish": "2010-09-24T15:42:30Z", 
    "summary": "We present a sensor network testbed that monitors a hallway. It consists of\n120 load sensors and 29 passive infrared sensors (PIRs), connected to 30\nwireless sensor nodes. There are also 29 LEDs and speakers installed, operating\nas actuators, and enabling a direct interaction between the testbed and\npassers-by. Beyond that, the network is heterogeneous, consisting of three\ndifferent circuit boards---each with its specific responsibility. The design of\nthe load sensors is of extremely low cost compared to industrial solutions and\neasily transferred to other settings. The network is used for in-network data\nprocessing algorithms, offering possibilities to develop, for instance,\ndistributed target-tracking algorithms. Special features of our installation\nare highly correlated sensor data and the availability of miscellaneous sensor\ntypes."
},{
    "category": "cs.DC", 
    "doi": "10.5121/jgraphoc.2010.2302", 
    "link": "http://arxiv.org/pdf/1010.0562v1", 
    "title": "The Impact of Data Replicatino on Job Scheduling Performance in   Hierarchical data Grid", 
    "arxiv-id": "1010.0562v1", 
    "author": "Somayeh Mohamadi", 
    "publish": "2010-10-04T12:25:04Z", 
    "summary": "In data-intensive applications data transfer is a primary cause of job\nexecution delay. Data access time depends on bandwidth. The major bottleneck to\nsupporting fast data access in Grids is the high latencies of Wide Area\nNetworks and Internet. Effective scheduling can reduce the amount of data\ntransferred across the internet by dispatching a job to where the needed data\nare present. Another solution is to use a data replication mechanism. Objective\nof dynamic replica strategies is reducing file access time which leads to\nreducing job runtime. In this paper we develop a job scheduling policy and a\ndynamic data replication strategy, called HRS (Hierarchical Replication\nStrategy), to improve the data access efficiencies. We study our approach and\nevaluate it through simulation. The results show that our algorithm has\nimproved 12% over the current strategies."
},{
    "category": "cs.DC", 
    "doi": "10.5121/jgraphoc.2010.2302", 
    "link": "http://arxiv.org/pdf/1010.0958v1", 
    "title": "Reconstruction of Aggregation Tree in spite of Faulty Nodes in Wireless   Sensor Networks", 
    "arxiv-id": "1010.0958v1", 
    "author": "Partha Sarathi Mandal", 
    "publish": "2010-10-05T17:54:58Z", 
    "summary": "Recent advances in wireless sensor networks (WSNs) have led to many new\npromissing applications. However data communication between nodes consumes a\nlarge portion of the total energy of WSNs. Consequently efficient data\naggregation technique can help greatly to reduce power consumption. Data\naggregation has emerged as a basic approach in WSNs in order to reduce the\nnumber of transmissions of sensor nodes over {\\it aggregation tree} and hence\nminimizing the overall power consumption in the network. If a sensor node fails\nduring data aggregation then the aggregation tree is disconnected. Hence the\nWSNs rely on in-network aggregation for efficiency but a single faulty node can\nseverely influence the outcome by contributing an arbitrary partial aggregate\nvalue.\n  In this paper we have presented a distributed algorithm that reconstruct the\naggregation tree from the initial aggregation tree excluding the faulty sensor\nnode. This is a synchronous model that is completed in several rounds. Our\nproposed scheme can handle multiple number of faulty nodes as well."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1015v1", 
    "title": "Astronomy in the Cloud: Using MapReduce for Image Coaddition", 
    "arxiv-id": "1010.1015v1", 
    "author": "YingYi Bu", 
    "publish": "2010-10-05T20:35:53Z", 
    "summary": "In the coming decade, astronomical surveys of the sky will generate tens of\nterabytes of images and detect hundreds of millions of sources every night. The\nstudy of these sources will involve computation challenges such as anomaly\ndetection and classification, and moving object tracking. Since such studies\nbenefit from the highest quality data, methods such as image coaddition\n(stacking) will be a critical preprocessing step prior to scientific\ninvestigation. With a requirement that these images be analyzed on a nightly\nbasis to identify moving sources or transient objects, these data streams\npresent many computational challenges. Given the quantity of data involved, the\ncomputational load of these problems can only be addressed by distributing the\nworkload over a large number of nodes. However, the high data throughput\ndemanded by these applications may present scalability challenges for certain\nstorage architectures. One scalable data-processing method that has emerged in\nrecent years is MapReduce, and in this paper we focus on its popular\nopen-source implementation called Hadoop. In the Hadoop framework, the data is\npartitioned among storage attached directly to worker nodes, and the processing\nworkload is scheduled in parallel on the nodes that contain the required input\ndata. A further motivation for using Hadoop is that it allows us to exploit\ncloud computing resources, e.g., Amazon's EC2. We report on our experience\nimplementing a scalable image-processing pipeline for the SDSS imaging database\nusing Hadoop. This multi-terabyte imaging dataset provides a good testbed for\nalgorithm development since its scope and structure approximate future surveys.\nFirst, we describe MapReduce and how we adapted image coaddition to the\nMapReduce framework. Then we describe a number of optimizations to our basic\napproach and report experimental results comparing their performance."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1112v1", 
    "title": "Deterministic and Energy-Optimal Wireless Synchronization", 
    "arxiv-id": "1010.1112v1", 
    "author": "Rafail Ostrovsky", 
    "publish": "2010-10-06T10:18:02Z", 
    "summary": "We consider the problem of clock synchronization in a wireless setting where\nprocessors must power-down their radios in order to save energy. Energy\nefficiency is a central goal in wireless networks, especially if energy\nresources are severely limited. In the current setting, the problem is to\nsynchronize clocks of $m$ processors that wake up in arbitrary time points,\nsuch that the maximum difference between wake up times is bounded by a positive\ninteger $n$, where time intervals are appropriately discretized. Currently, the\nbest-known results for synchronization for single-hop networks of $m$\nprocessors is a randomized algorithm due to \\cite{BKO09} of O(\\sqrt {n /m}\n\\cdot poly-log(n)) awake times per processor and a lower bound of\nOmega(\\sqrt{n/m}) of the number of awake times needed per processor\n\\cite{BKO09}. The main open question left in their work is to close the\npoly-log gap between the upper and the lower bound and to de-randomize their\nprobabilistic construction and eliminate error probability. This is exactly\nwhat we do in this paper.\n  That is, we show a {deterministic} algorithm with radio use of Theta(\\sqrt {n\n/m}) that never fails. We stress that our upper bound exactly matches the lower\nbound proven in \\cite{BKO09}, up to a small multiplicative constant. Therefore,\nour algorithm is {optimal} in terms of energy efficiency and completely\nresolves a long sequence of works in this area. In order to achieve these\nresults we devise a novel {adaptive} technique that determines the times when\ndevices power their radios on and off. In addition, we prove several lower\nbounds on the energy efficiency of algorithms for {multi-hop networks}.\nSpecifically, we show that any algorithm for multi-hop networks must have radio\nuse of Omega(\\sqrt n) per processor."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.1812v1", 
    "title": "Modified Bully Algorithm using Election Commission", 
    "arxiv-id": "1010.1812v1", 
    "author": "Afroza Nahar", 
    "publish": "2010-10-09T06:00:33Z", 
    "summary": "Electing leader is a vital issue not only in distributed computing but also\nin communication network [1, 2, 3, 4, 5], centralized mutual exclusion\nalgorithm [6, 7], centralized control IPC, etc. A leader is required to make\nsynchronization between different processes. And different election algorithms\nare used to elect a coordinator among the available processes in the system\nsuch a way that there will be only one coordinator at any time. Bully election\nalgorithm is one of the classical and well-known approaches in coordinator\nelection process. This paper will present a modified version of bully election\nalgorithm using a new concept called election commission. This approach will\nnot only reduce redundant elections but also minimize total number of elections\nand hence it will minimize message passing, network traffic, and complexity of\nthe existing system."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.2000v1", 
    "title": "A Critical Path Approach to Analyzing Parallelism of Algorithmic   Variants. Application to Cholesky Inversion", 
    "arxiv-id": "1010.2000v1", 
    "author": "Julien Langou", 
    "publish": "2010-10-11T05:34:40Z", 
    "summary": "Algorithms come with multiple variants which are obtained by changing the\nmathematical approach from which the algorithm is derived. These variants offer\na wide spectrum of performance when implemented on a multicore platform and we\nseek to understand these differences in performances from a theoretical point\nof view. To that aim, we derive and present the critical path lengths of each\nalgorithmic variant for our application problem which enables us to determine a\nlower bound on the time to solution. This metric provides an intuitive grasp of\nthe performance of a variant and we present numerical experiments to validate\nthe tightness of our lower bounds on practical applications. Our case study is\nthe Cholesky inversion and its use in computing the inverse of a symmetric\npositive definite matrix."
},{
    "category": "cs.DC", 
    "doi": "10.1086/658877", 
    "link": "http://arxiv.org/pdf/1010.2454v1", 
    "title": "Distributed Deterministic Edge Coloring using Bounded Neighborhood   Independence", 
    "arxiv-id": "1010.2454v1", 
    "author": "Michael Elkin", 
    "publish": "2010-10-12T17:51:01Z", 
    "summary": "We study the {edge-coloring} problem in the message-passing model of\ndistributed computing. This is one of the most fundamental and well-studied\nproblems in this area. Currently, the best-known deterministic algorithms for\n(2Delta -1)-edge-coloring requires O(Delta) + log-star n time \\cite{PR01},\nwhere Delta is the maximum degree of the input graph. Also, recent results of\n\\cite{BE10} for vertex-coloring imply that one can get an\nO(Delta)-edge-coloring in O(Delta^{epsilon} \\cdot \\log n) time, and an\nO(Delta^{1 + epsilon})-edge-coloring in O(log Delta log n) time, for an\narbitrarily small constant epsilon > 0.\n  In this paper we devise a drastically faster deterministic edge-coloring\nalgorithm. Specifically, our algorithm computes an O(Delta)-edge-coloring in\nO(Delta^{epsilon}) + log-star n time, and an O(Delta^{1 +\nepsilon})-edge-coloring in O(log Delta) + log-star n time. This result improves\nthe previous state-of-the-art {exponentially} in a wide range of Delta,\nspecifically, for 2^{Omega(\\log-star n)} \\leq Delta \\leq polylog(n). In\naddition, for small values of Delta our deterministic algorithm outperforms all\nthe existing {randomized} algorithms for this problem.\n  On our way to these results we study the {vertex-coloring} problem on the\nfamily of graphs with bounded {neighborhood independence}. This is a large\nfamily, which strictly includes line graphs of r-hypergraphs for any r = O(1),\nand graphs of bounded growth. We devise a very fast deterministic algorithm for\nvertex-coloring graphs with bounded neighborhood independence. This algorithm\ndirectly gives rise to our edge-coloring algorithms, which apply to {general}\ngraphs.\n  Our main technical contribution is a subroutine that computes an\nO(Delta/p)-defective p-vertex coloring of graphs with bounded neighborhood\nindependence in O(p^2) + \\log-star n time, for a parameter p, 1 \\leq p \\leq\nDelta."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.4", 
    "link": "http://arxiv.org/pdf/1010.2824v1", 
    "title": "Behavioural Models for Group Communications", 
    "arxiv-id": "1010.2824v1", 
    "author": "Eric Madelaine", 
    "publish": "2010-10-14T05:16:17Z", 
    "summary": "Group communication is becoming a more and more popular infrastructure for\nefficient distributed applications. It consists in representing locally a group\nof remote objects as a single object accessed in a single step; communications\nare then broadcasted to all members. This paper provides models for automatic\nverification of group-based applications, typically for detecting deadlocks or\nchecking message ordering. We show how to encode group communication, together\nwith different forms of synchronisation for group results. The proposed models\nare parametric such that, for example, different group sizes or group members\ncould be experimented with the minimum modification of the original model."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.2828v1", 
    "title": "A Reusable Component for Communication and Data Synchronization in   Mobile Distributed Interactive Applications", 
    "arxiv-id": "1010.2828v1", 
    "author": "Antoine Beugnard", 
    "publish": "2010-10-14T05:16:43Z", 
    "summary": "In Distributed Interactive Applications (DIA) such as multiplayer games,\nwhere many participants are involved in a same game session and communicate\nthrough a network, they may have an inconsistent view of the virtual world\nbecause of the communication delays across the network. This issue becomes even\nmore challenging when communicating through a cellular network while executing\nthe DIA client on a mobile terminal. Consistency maintenance algorithms may be\nused to obtain a uniform view of the virtual world. These algorithms are very\ncomplex and hard to program and therefore, the implementation and the future\nevolution of the application logic code become difficult. To solve this\nproblem, we propose an approach where the consistency concerns are handled\nseparately by a distributed component called a Synchronization Medium, which is\nresponsible for the communication management as well as the consistency\nmaintenance. We present the detailed architecture of the Synchronization Medium\nand the generic interfaces it offers to DIAs. We evaluate our approach both\nqualitatively and quantitatively. We first demonstrate that the Synchronization\nMedium is a reusable component through the development of two game\napplications, a car racing game and a space war game. A performance evaluation\nthen shows that the overhead introduced by the Synchronization Medium remains\nacceptable."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.2881v1", 
    "title": "Service Level Agreement (SLA) in Utility Computing Systems", 
    "arxiv-id": "1010.2881v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2010-10-14T11:33:28Z", 
    "summary": "In recent years, extensive research has been conducted in the area of Service\nLevel Agreement (SLA) for utility computing systems. An SLA is a formal\ncontract used to guarantee that consumers' service quality expectation can be\nachieved. In utility computing systems, the level of customer satisfaction is\ncrucial, making SLAs significantly important in these environments. Fundamental\nissue is the management of SLAs, including SLA autonomy management or trade off\namong multiple Quality of Service (QoS) parameters. Many SLA languages and\nframeworks have been developed as solutions; however, there is no overall\nclassification for these extensive works. Therefore, the aim of this chapter is\nto present a comprehensive survey of how SLAs are created, managed and used in\nutility computing environment. We discuss existing use cases from Grid and\nCloud computing systems to identify the level of SLA realization in\nstate-of-art systems and emerging challenges for future research."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.3053v1", 
    "title": "Parallel Sorted Neighborhood Blocking with MapReduce", 
    "arxiv-id": "1010.3053v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-10-15T00:28:44Z", 
    "summary": "Cloud infrastructures enable the efficient parallel execution of\ndata-intensive tasks such as entity resolution on large datasets. We\ninvestigate challenges and possible solutions of using the MapReduce\nprogramming model for parallel entity resolution. In particular, we propose and\nevaluate two MapReduce-based implementations for Sorted Neighborhood blocking\nthat either use multiple MapReduce jobs or apply a tailored data replication."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.3233v1", 
    "title": "A Survey of Virtualization Technologies With Performance Testing", 
    "arxiv-id": "1010.3233v1", 
    "author": "Adam Pilbeam", 
    "publish": "2010-10-15T17:51:50Z", 
    "summary": "Virtualization has rapidly become a go-to technology for increasing\nefficiency in the data center. With virtualization technologies providing\ntremendous flexibility, even disparate architectures may be deployed on a\nsingle machine without interference. Awareness of limitations and requirements\nof physical hosts to be used for virtualization is important. This paper\nreviews the present virtualization methods, virtual computing software, and\nprovides a brief analysis of the performance issues inherent to each. In the\nend we present testing results of KVM-QEMU on two current Multi-Core CPU\nArchitectures and System Configurations."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4018v1", 
    "title": "A Paradigm for Channel Assignment and Data Migration in Distributed   Systems", 
    "arxiv-id": "1010.4018v1", 
    "author": "Chadi Kari", 
    "publish": "2010-10-19T19:37:09Z", 
    "summary": "In this manuscript, we consider the problems of channel assignment in\nwireless networks and data migration in heterogeneous storage systems. We show\nthat a soft edge coloring approach to both problems gives rigorous\napproximation guarantees. In the channel assignment problem arising in wireless\nnetworks a pair of edges incident to a vertex are said to be conflicting if the\nchannels assigned to them are the same. Our goal is to assign channels (color\nedges) so that the number of conflicts is minimized. The problem is NP-hard by\na reduction from Edge coloring and we present two combinatorial algorithms for\nthis case. The first algorithm is based on a distributed greedy method and\ngives a solution with at most $2(1-\\frac{1}{k})|E|$ more conflicts than the\noptimal solution.The approximation ratio if the second algorithm is $1 +\n\\frac{|V|}{|E|}$, which gives a ($1 + o(1)$)-factor for dense graphs and is the\nbest possible unless P = NP. We also consider the data migration problem in\nheterogeneous storage systems. In such systems, data layouts may need to be\nreconfigured over time for load balancing or in the event of system\nfailure/upgrades. It is critical to migrate data to their target locations as\nquickly as possible to obtain the best performance of the system. Most of the\nprevious results on data migration assume that each storage node can perform\nonly one data transfer at a time. However, storage devices tend to have\nheterogeneous capabilities as devices may be added over time due to storage\ndemand increase. We develop algorithms to minimize the data migration time. We\nshow that it is possible to find an optimal migration schedule when all $c_v$'s\nare even. Furthermore, though the problem is NP-hard in general, we give an\nefficient soft edge coloring algorithm that offers a rigorous $(1 +\no(1))$-approximation guarantee."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4639v1", 
    "title": "Parallel Sparse Matrix Solver on the GPU Applied to Simulation of   Electrical Machines", 
    "arxiv-id": "1010.4639v1", 
    "author": "Jean-Luc Dekeyser", 
    "publish": "2010-10-22T08:46:04Z", 
    "summary": "Nowadays, several industrial applications are being ported to parallel\narchitectures. In fact, these platforms allow acquire more performance for\nsystem modelling and simulation. In the electric machines area, there are many\nproblems which need speed-up on their solution. This paper examines the\nparallelism of sparse matrix solver on the graphics processors. More\nspecifically, we implement the conjugate gradient technique with input matrix\nstored in CSR, and Symmetric CSR and CSC formats. This method is one of the\nmost efficient iterative methods available for solving the finite-element basis\nfunctions of Maxwell's equations. The GPU (Graphics Processing Unit), which is\nused for its implementation, provides mechanisms to parallel the algorithm.\nThus, it increases significantly the computation speed in relation to serial\ncode on CPU based systems."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.4952v1", 
    "title": "Towards Constraint-based High Performance Cloud System in the Process of   Cloud Computing Adoption in an Organization", 
    "arxiv-id": "1010.4952v1", 
    "author": "Sangyoon Oh", 
    "publish": "2010-10-24T12:08:12Z", 
    "summary": "Cloud computing is penetrating into various domains and environments, from\ntheoretical computer science to economy, from marketing hype to educational\ncurriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently\ndeveloping state of cloud computing leaves several issues to address and also\naffects cloud computing adoption by organizations. In this paper, we explain\nhow the transition into the cloud can occur in an organization and describe the\nmechanism for transforming legacy infrastructure into a virtual\ninfrastructure-based cloud. We describe the state of the art of infrastructural\ncloud, which is essential in the decision making on cloud adoption, and\nhighlight the challenges that can limit the scale and speed of the adoption. We\nthen suggest a strategic framework for designing a high performance cloud\nsystem. This framework is applicable when transformation cloudbased deployment\nmodel collides with some constraints. We give an example of the implementation\nof the framework in a design of a budget-constrained high availability cloud\nsystem."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.37.7", 
    "link": "http://arxiv.org/pdf/1010.5421v1", 
    "title": "On the Mesh Array for Matrix Multiplication", 
    "arxiv-id": "1010.5421v1", 
    "author": "Subhash Kak", 
    "publish": "2010-10-26T15:10:37Z", 
    "summary": "This article presents new properties of the mesh array for matrix\nmultiplication. In contrast to the standard array that requires 3n-2 steps to\ncomplete its computation, the mesh array requires only 2n-1 steps. Symmetries\nof the mesh array computed values are presented which enhance the efficiency of\nthe array for specific applications. In multiplying symmetric matrices, the\nresults are obtained in 3n/2+1 steps. The mesh array is examined for its\napplication as a scrambling system."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.0715v1", 
    "title": "Flexible Session Management in a Distributed Environment", 
    "arxiv-id": "1011.0715v1", 
    "author": "Igor Sfiligoi", 
    "publish": "2010-11-02T19:42:04Z", 
    "summary": "Many secure communication libraries used by distributed systems, such as SSL,\nTLS, and Kerberos, fail to make a clear distinction between the authentication,\nsession, and communication layers. In this paper we introduce CEDAR, the secure\ncommunication library used by the Condor High Throughput Computing software,\nand present the advantages to a distributed computing system resulting from\nCEDAR's separation of these layers. Regardless of the authentication method\nused, CEDAR establishes a secure session key, which has the flexibility to be\nused for multiple capabilities. We demonstrate how a layered approach to\nsecurity sessions can avoid round-trips and latency inherent in network\nauthentication. The creation of a distinct session management layer allows for\noptimizations to improve scalability by way of delegating sessions to other\ncomponents in the system. This session delegation creates a chain of trust that\nreduces the overhead of establishing secure connections and enables centralized\nenforcement of system-wide security policies. Additionally, secure channels\nbased upon UDP datagrams are often overlooked by existing libraries; we show\nhow CEDAR's structure accommodates this as well. As an example of the utility\nof this work, we show how the use of delegated security sessions and other\ntechniques inherent in CEDAR's architecture enables US CMS to meet their\nscalability requirements in deploying Condor over large-scale, wide-area grid\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.1173v1", 
    "title": "Rank k Cholesky Up/Down-dating on the GPU: gpucholmodV0.2", 
    "arxiv-id": "1011.1173v1", 
    "author": "Christian Walder", 
    "publish": "2010-11-04T14:39:26Z", 
    "summary": "In this note we briefly describe our Cholesky modification algorithm for\nstreaming multiprocessor architectures. Our implementation is available in C++\nwith Matlab binding, using CUDA to utilise the graphics processing unit (GPU).\nLimited speed ups are possible due to the bandwidth bound nature of the\nproblem. Furthermore, a complex dependency pattern must be obeyed, requiring\nmultiple kernels to be launched. Nonetheless, this makes for an interesting\nproblem, and our approach can reduce the computation time by a factor of around\n7 for matrices of size 5000 by 5000 and k=16, in comparison with the LINPACK\nsuite running on a CPU of comparable vintage. Much larger problems can be\nhandled however due to the O(n) scaling in required GPU memory of our method."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2235v3", 
    "title": "Multiscale Gossip for Efficient Decentralized Averaging in Wireless   Packet Networks", 
    "arxiv-id": "1011.2235v3", 
    "author": "Michael G. Rabbat", 
    "publish": "2010-11-09T23:50:10Z", 
    "summary": "This paper describes and analyzes a hierarchical gossip algorithm for solving\nthe distributed average consensus problem in wireless sensor networks. The\nnetwork is recursively partitioned into subnetworks. Initially, nodes at the\nfinest scale gossip to compute local averages. Then, using geographic routing\nto enable gossip between nodes that are not directly connected, these local\naverages are progressively fused up the hierarchy until the global average is\ncomputed. We show that the proposed hierarchical scheme with $k$ levels of\nhierarchy is competitive with state-of-the-art randomized gossip algorithms, in\nterms of message complexity, achieving $\\epsilon$-accuracy with high\nprobability after $O\\big(n \\log \\log n \\log \\frac{kn}{\\epsilon} \\big)$\nmessages. Key to our analysis is the way in which the network is recursively\npartitioned. We find that the optimal scaling law is achieved when subnetworks\nat scale $j$ contain $O(n^{(2/3)^j})$ nodes; then the message complexity at any\nindividual scale is $O(n \\log \\frac{kn}{\\epsilon})$, and the total number of\nscales in the hierarchy grows slowly, as $\\Theta(\\log \\log n)$. Another\nimportant consequence of hierarchical construction is that the longest distance\nover which messages are exchanged is $O(n^{1/3})$ hops (at the highest scale),\nand most messages (at lower scales) travel shorter distances. In networks that\nuse link-level acknowledgements, this results in less congestion and resource\nusage by reducing message retransmissions. Simulations illustrate that the\nproposed scheme is more message-efficient than existing state-of-the-art\nrandomized gossip algorithms based on averaging along paths."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2312v1", 
    "title": "A framework for proving the self-organization of dynamic systems", 
    "arxiv-id": "1011.2312v1", 
    "author": "Matthieu Roy", 
    "publish": "2010-11-10T08:37:53Z", 
    "summary": "This paper aims at providing a rigorous definition of self- organization, one\nof the most desired properties for dynamic systems (e.g., peer-to-peer systems,\nsensor networks, cooperative robotics, or ad-hoc networks). We characterize\ndifferent classes of self-organization through liveness and safety properties\nthat both capture information re- garding the system entropy. We illustrate\nthese classes through study cases. The first ones are two representative P2P\noverlays (CAN and Pas- try) and the others are specific implementations of\n\\Omega (the leader oracle) and one-shot query abstractions for dynamic\nsettings. Our study aims at understanding the limits and respective power of\nexisting self-organized protocols and lays the basis of designing robust\nalgorithm for dynamic systems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/219/4/042017", 
    "link": "http://arxiv.org/pdf/1011.2953v1", 
    "title": "A Distributed Clustering Algorithm for Dynamic Networks", 
    "arxiv-id": "1011.2953v1", 
    "author": "Devan Sohier", 
    "publish": "2010-11-12T15:35:10Z", 
    "summary": "We propose an algorithm that builds and maintains clusters over a network\nsubject to mobility. This algorithm is fully decentralized and makes all the\ndifferent clusters grow concurrently. The algorithm uses circulating tokens\nthat collect data and move according to a random walk traversal scheme. Their\ntask consists in (i) creating a cluster with the nodes it discovers and (ii)\nmanaging the cluster expansion; all decisions affecting the cluster are taken\nonly by a node that owns the token. The size of each cluster is maintained\nhigher than $m$ nodes ($m$ is a parameter of the algorithm). The obtained\nclustering is locally optimal in the sense that, with only a local view of each\nclusters, it computes the largest possible number of clusters (\\emph{ie} the\nsizes of the clusters are as close to $m$ as possible). This algorithm is\ndesigned as a decentralized control algorithm for large scale networks and is\nmobility-adaptive: after a series of topological changes, the algorithm\nconverges to a clustering. This recomputation only affects nodes in clusters in\nwhich topological changes happened, and in adjacent clusters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3094v1", 
    "title": "A High-confidence Cyber-Physical Alarm System: Design and Implementation", 
    "arxiv-id": "1011.3094v1", 
    "author": "Meng Shao", 
    "publish": "2010-11-13T03:23:15Z", 
    "summary": "Most traditional alarm systems cannot address security threats in a\nsatisfactory manner. To alleviate this problem, we developed a high-confidence\ncyber-physical alarm system (CPAS), a new kind of alarm systems. This system\nestablishes the connection of the Internet (i.e. TCP/IP) through GPRS/CDMA/3G.\nIt achieves mutual communication control among terminal equipments, human\nmachine interfaces and users by using the existing mobile communication\nnetwork. The CPAS will enable the transformation in alarm mode from traditional\none-way alarm to two-way alarm. The system has been successfully applied in\npractice. The results show that the CPAS could avoid false alarms and satisfy\nresidents' security needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3170v1", 
    "title": "Slightly smaller splitter networks", 
    "arxiv-id": "1011.3170v1", 
    "author": "James Aspnes", 
    "publish": "2010-11-14T00:52:14Z", 
    "summary": "The classic renaming protocol of Moir and Anderson (1995) uses a network of\nTheta(n^2) splitters to assign unique names to n processes with unbounded\ninitial names. We show how to reduce this bound to Theta(n^{3/2}) splitters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.3632v2", 
    "title": "Stabilizing data-link over non-FIFO channels with optimal   fault-resilience", 
    "arxiv-id": "1011.3632v2", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2010-11-16T10:04:34Z", 
    "summary": "Self-stabilizing systems have the ability to converge to a correct behavior\nwhen started in any configuration. Most of the work done so far in the\nself-stabilization area assumed either communication via shared memory or via\nFIFO channels. This paper is the first to lay the bases for the design of\nself-stabilizing message passing algorithms over unreliable non-FIFO channels.\nWe propose a fault-send-deliver optimal stabilizing data-link layer that\nemulates a reliable FIFO communication channel over unreliable capacity bounded\nnon-FIFO channels."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.4135v1", 
    "title": "Progressive Decoding for Data Availability and Reliability in   Distributed Networked Storage", 
    "arxiv-id": "1011.4135v1", 
    "author": "Rong Zheng", 
    "publish": "2010-11-18T04:47:28Z", 
    "summary": "To harness the ever growing capacity and decreasing cost of storage,\nproviding an abstraction of dependable storage in the presence of crash-stop\nand Byzantine failures is compulsory. We propose a decentralized Reed Solomon\ncoding mechanism with minimum communication overhead. Using a progressive data\nretrieval scheme, a data collector contacts only the necessary number of\nstorage nodes needed to guarantee data integrity. The scheme gracefully adapts\nthe cost of successful data retrieval to the number of storage node failures.\nMoreover, by leveraging the Welch-Berlekamp algorithm, it avoids unnecessary\ncomputations. Compared to the state-of-the-art decoding scheme, the\nimplementation and evaluation results show that our progressive data retrieval\nscheme has up to 35 times better computation performance for low Byzantine node\nrates. Additionally, the communication cost in data retrieval is derived\nanalytically and corroborated by Monte-Carlo simulation results. Our\nimplementation is flexible in that the level of redundancy it provides is\nindependent of the number of data generating nodes, a requirement for\ndistributed storage systems"
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5064v1", 
    "title": "Optimal Placement Algorithms for Virtual Machines", 
    "arxiv-id": "1011.5064v1", 
    "author": "Madhu Kumar SD", 
    "publish": "2010-11-23T11:26:48Z", 
    "summary": "Cloud computing provides a computing platform for the users to meet their\ndemands in an efficient, cost-effective way. Virtualization technologies are\nused in the clouds to aid the efficient usage of hardware. Virtual machines\n(VMs) are utilized to satisfy the user needs and are placed on physical\nmachines (PMs) of the cloud for effective usage of hardware resources and\nelectricity in the cloud. Optimizing the number of PMs used helps in cutting\ndown the power consumption by a substantial amount.\n  In this paper, we present an optimal technique to map virtual machines to\nphysical machines (nodes) such that the number of required nodes is minimized.\nWe provide two approaches based on linear programming and quadratic programming\ntechniques that significantly improve over the existing theoretical bounds and\nefficiently solve the problem of virtual machine (VM) placement in data\ncenters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5470v2", 
    "title": "Local Computation: Lower and Upper Bounds", 
    "arxiv-id": "1011.5470v2", 
    "author": "Roger Wattenhofer", 
    "publish": "2010-11-24T19:56:31Z", 
    "summary": "The question of what can be computed, and how efficiently, are at the core of\ncomputer science. Not surprisingly, in distributed systems and networking\nresearch, an equally fundamental question is what can be computed in a\n\\emph{distributed} fashion. More precisely, if nodes of a network must base\ntheir decision on information in their local neighborhood only, how well can\nthey compute or approximate a global (optimization) problem? In this paper we\ngive the first poly-logarithmic lower bound on such local computation for\n(optimization) problems including minimum vertex cover, minimum (connected)\ndominating set, maximum matching, maximal independent set, and maximal\nmatching. In addition we present a new distributed algorithm for solving\ngeneral covering and packing linear programs. For some problems this algorithm\nis tight with the lower bounds, for others it is a distributed approximation\nscheme. Together, our lower and upper bounds establish the local computability\nand approximability of a large class of problems, characterizing how much local\ninformation is required to solve these tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5568v1", 
    "title": "Correlated Resource Models of Internet End Hosts", 
    "arxiv-id": "1011.5568v1", 
    "author": "Anderson David", 
    "publish": "2010-11-25T08:41:38Z", 
    "summary": "Understanding and modelling resources of Internet end hosts is essential for\nthe design of desktop software and Internet-distributed applications. In this\npaper we develop a correlated resource model of Internet end hosts based on\nreal trace data taken from the SETI@home project. This data covers a 5-year\nperiod with statistics for 2.7 million hosts. The resource model is based on\nstatistical analysis of host computational power, memory, and storage as well\nas how these resources change over time and the correlations between them. We\nfind that resources with few discrete values (core count, memory) are well\nmodeled by exponential laws governing the change of relative resource\nquantities over time. Resources with a continuous range of values are well\nmodeled with either correlated normal distributions (processor speed for\ninteger operations and floating point operations) or log-normal distributions\n(available disk space). We validate and show the utility of the models by\napplying them to a resource allocation problem for Internet-distributed\napplications, and demonstrate their value over other models. We also make our\ntrace data and tool for automatically generating realistic Internet end hosts\npublicly available."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.5808v1", 
    "title": "Dotted Version Vectors: Logical Clocks for Optimistic Replication", 
    "arxiv-id": "1011.5808v1", 
    "author": "Ricardo Gon\u00e7alves", 
    "publish": "2010-11-26T14:45:53Z", 
    "summary": "In cloud computing environments, a large number of users access data stored\nin highly available storage systems. To provide good performance to\ngeographically disperse users and allow operation even in the presence of\nfailures or network partitions, these systems often rely on optimistic\nreplication solutions that guarantee only eventual consistency. In this\nscenario, it is important to be able to accurately and efficiently identify\nupdates executed concurrently. In this paper, first we review, and expose\nproblems with current approaches to causality tracking in optimistic\nreplication: these either lose information about causality or do not scale, as\nthey require replicas to maintain information that grows linearly with the\nnumber of clients or updates. Then, we propose a novel solution that fully\ncaptures causality while being very concise in that it maintains information\nthat grows linearly only with the number of servers that register updates for a\ngiven data element, bounded by the degree of replication."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1011.6596v1", 
    "title": "Dependability in Aggregation by Averaging", 
    "arxiv-id": "1011.6596v1", 
    "author": "Paulo S\u00e9rgio Almeida", 
    "publish": "2010-11-30T16:10:49Z", 
    "summary": "Aggregation is an important building block of modern distributed\napplications, allowing the determination of meaningful properties (e.g. network\nsize, total storage capacity, average load, majorities, etc.) that are used to\ndirect the execution of the system. However, the majority of the existing\naggregation algorithms exhibit relevant dependability issues, when prospecting\ntheir use in real application environments. In this paper, we reveal some\ndependability issues of aggregation algorithms based on iterative averaging\ntechniques, giving some directions to solve them. This class of algorithms is\nconsidered robust (when compared to common tree-based approaches), being\nindependent from the used routing topology and providing an aggregation result\nat all nodes. However, their robustness is strongly challenged and their\ncorrectness often compromised, when changing the assumptions of their working\nenvironment to more realistic ones. The correctness of this class of algorithms\nrelies on the maintenance of a fundamental invariant, commonly designated as\n\"mass conservation\". We will argue that this main invariant is often broken in\npractical settings, and that additional mechanisms and modifications are\nrequired to maintain it, incurring in some degradation of the algorithms\nperformance. In particular, we discuss the behavior of three representative\nalgorithms Push-Sum Protocol, Push-Pull Gossip protocol and Distributed Random\nGrouping under asynchronous and faulty (with message loss and node crashes)\nenvironments. More specifically, we propose and evaluate two new versions of\nthe Push-Pull Gossip protocol, which solve its message interleaving problem\n(evidenced even in a synchronous operation mode)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1012.1131v1", 
    "title": "A Log Auditing Approach for Trust Management in Peer-to-Peer   Collaboration", 
    "arxiv-id": "1012.1131v1", 
    "author": "Claudia-Lavinia Ignat", 
    "publish": "2010-12-06T11:14:22Z", 
    "summary": "Nowadays we are faced with an increasing popularity of social software\nincluding wikis, blogs, micro-blogs and online social networks such as Facebook\nand MySpace. Unfortunately, the mostly used social services are centralized and\npersonal information is stored at a single vendor. This results in potential\nprivacy problems as users do not have much control over how their private data\nis disseminated. To overcome this limitation, some recent approaches envisioned\nreplacing the single authority centralization of services by a peer-to-peer\ntrust-based approach where users can decide with whom they want to share their\nprivate data. In this peer-to-peer collaboration it is very difficult to ensure\nthat after data is shared with other peers, these peers will not misbehave and\nviolate data privacy. In this paper we propose a mechanism that addresses the\nissue of data privacy violation due to data disclosure to malicious peers. In\nour approach trust values between users are adjusted according to their\nprevious activities on the shared data. Users share their private data by\nspecifying some obligations the receivers must follow. We log modifications\ndone by users on the shared data as well as the obligations that must be\nfollowed when data is shared. By a log-auditing mechanism we detect users that\nmisbehaved and we adjust their associated trust values by using any existing\ndecentralized trust model."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1012.2270v1", 
    "title": "New Row-grouped CSR format for storing the sparse matrices on GPU with   implementation in CUDA", 
    "arxiv-id": "1012.2270v1", 
    "author": "Jan Vacata", 
    "publish": "2010-12-10T14:04:33Z", 
    "summary": "In this article we present a new format for storing sparse matrices. The\nformat is designed to perform well mainly on the GPU devices. We present its\nimplementation in CUDA. The performance has been tested on 1,600 different\ntypes of matrices and we compare our format with the Hybrid format. We give\ndetailed comparison of both formats and show their strong and weak parts."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GreenCom-CPSCom.2010.75", 
    "link": "http://arxiv.org/pdf/1012.2273v1", 
    "title": "Performance Evaluation of Parallel Message Passing and Thread   Programming Model on Multicore Architectures", 
    "arxiv-id": "1012.2273v1", 
    "author": "A. B. Mutiara", 
    "publish": "2010-12-10T14:17:19Z", 
    "summary": "The current trend of multicore architectures on shared memory systems\nunderscores the need of parallelism. While there are some programming model to\nexpress parallelism, thread programming model has become a standard to support\nthese system such as OpenMP, and POSIX threads. MPI (Message Passing Interface)\nwhich remains the dominant model used in high-performance computing today faces\nthis challenge.\n  Previous version of MPI which is MPI-1 has no shared memory concept, and\nCurrent MPI version 2 which is MPI-2 has a limited support for shared memory\nsystems. In this research, MPI-2 version of MPI will be compared with OpenMP to\nsee how well does MPI perform on multicore / SMP (Symmetric Multiprocessor)\nmachines.\n  Comparison between OpenMP for thread programming model and MPI for message\npassing programming model will be conducted on multicore shared memory machine\narchitectures to see who has a better performance in terms of speed and\nthroughput. Application used to assess the scalability of the evaluated\nparallel programming solutions is matrix multiplication with customizable\nmatrix dimension.\n  Many research done on a large scale parallel computing which using high scale\nbenchmark such as NSA Parallel Benchmark (NPB) for their testing standarization\n[1]. This research will be conducted on a small scale parallel computing that\nemphasize more on the performance evaluation between MPI and OpenMPI parallel\nprogramming model using self created benchmark."
},{
    "category": "cs.DC", 
    "doi": "10.7763/IJCTE.2010.V2.273", 
    "link": "http://arxiv.org/pdf/1012.2499v1", 
    "title": "openPC : a toolkit for public cluster with full ownership", 
    "arxiv-id": "1012.2499v1", 
    "author": "L. T. Handoko", 
    "publish": "2010-12-12T00:15:44Z", 
    "summary": "The openPC is a set of open source tools that realizes a parallel machine and\ndistributed computing environment divisible into several independent blocks of\nnodes, and each of them is remotely but fully in any means accessible for users\nwith a full ownership policy. The openPC components address fundamental issues\nrelating to security, resource access, resource allocation, compatibilities\nwith heterogeneous middlewares, user-friendly and integrated web-based\ninterfaces, hardware control and monitoring systems. These components have been\ndeployed successfully to the LIPI Public Cluster which is open for public use.\nIn this paper, the unique characteristics of openPC due to its rare\nrequirements are introduced, its components and a brief performance analysis\nare discussed."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.2860v4", 
    "title": "Towards Refactoring the DMF to Support Jini and JMS DMS in GIPSY", 
    "arxiv-id": "1012.2860v4", 
    "author": "Joey Paquet", 
    "publish": "2010-12-13T20:58:58Z", 
    "summary": "In this paper we report on our re-engineering effort to refactor and unify\ntwo somewhat disjoint Java distributed middleware technologies -- Jini and JMS\n-- used in the implementation of the Demand Migration System (DMS). In doing\nso, we refactor their parent Demand Migration Framework (DMF), within the\nGeneral Intensional Programming System (GIPSY). The complex Java-based GIPSY\nproject is used to investigate on the intensional and hybrid programming\nparadigms."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.3347v2", 
    "title": "An Architectural Design for Brokered Collaborative Content Delivery   System", 
    "arxiv-id": "1012.3347v2", 
    "author": "Sangyoon Oh", 
    "publish": "2010-12-15T14:31:05Z", 
    "summary": "Advances in web technologies have driven massive content uploads and requests\nthat can be identified by the increased usage of multimedia web and social web\nservices. This situation enforces the content providers to scale their\ninfrastructure in order to cope with the extra provisioning of network traffic,\nstorage and other resources. Since the complexity and cost factors in scaling\nthe infrastructure exist, we propose a novel solution for providing and\ndelivering contents to clients by introducing a brokered collaborative content\ndelivery system. The architectural design of this system leverages content\nredundancy and content distribution mechanisms in other content providers to\ndeliver contents to the clients. With the recent emergence of cloud computing,\nwe show that this system can also be adopted to run on the cloud. In this\npaper, we focus on a brokering scheme to mediate user requests to the most\nappropriate content provider based on a ranking system. The architecture\nprovides a novel Global Rank Value (GRV) concept in estimating content provider\ncapability and transforming the QoS requirement of a content request. A\nfairness model that will bring this design to be attractive to the current\ncontent delivery regime is also introduced. Through simulation, we show that\nusing fair provider selection, contents can be provisioned by a better pool of\nqualified providers thus leveraging the collaboration and preventing potential\nQoS violation that may occur when the size of pool is smaller."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.5030v1", 
    "title": "Work-stealing for mixed-mode parallelism by deterministic team-building", 
    "arxiv-id": "1012.5030v1", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2010-12-22T16:43:12Z", 
    "summary": "We show how to extend classical work-stealing to deal also with data parallel\ntasks that can require any number of threads r >= 1 for their execution. We\nexplain in detail the so introduced idea of work-stealing with deterministic\nteam-building which in a natural way generalizes classical work-stealing. A\nprototype C++ implementation of the generalized work-stealing algorithm has\nbeen given and is briefly described. Building on this, a serious, well-known\ncontender for a best parallel Quicksort algorithm has been implemented, which\nnaturally relies on both task and data parallelism. For instance, sorting\n2^27-1 randomly generated integers we could improve the speed-up from 5.1 to\n8.7 on a 32-core Intel Nehalem EX system, being consistently better than the\ntuned, task-parallel Cilk++ system."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1012.5834v5", 
    "title": "Maximum Lifetime for Data Regeneration in Wireless Sensor Networks", 
    "arxiv-id": "1012.5834v5", 
    "author": "Rong Zheng", 
    "publish": "2010-12-28T20:36:09Z", 
    "summary": "Robust distributed storage systems dedicated to wireless sensor networks\nutilize several nodes to redundantly store sensed data so that when some\nstorage nodes fail, the sensed data can still be reconstructed. For the same\nlevel of redundancy, erasure coding based approaches are known to require less\ndata storage space than replication methods.\n  To maintain the same level of redundancy when one storage node fails, erasure\ncoded data can be restored onto some other storage node by having this node\ndownload respective pieces from other live storage nodes. Previous works showed\nthat the benefits in using erasure coding for robust storage over replication\nare made unappealing by the complication in regenerating lost data. More recent\nwork has, however, shown that the bandwidth for erasure coded data can be\nfurther reduced by proposing Regenerating Coding, making erasure codes again\ndesirable for robust data storage.\n  But none of these works on regenerating coding consider how these codes will\nperform for data regeneration in wireless sensor networks. We therefore propose\nan analytical model to quantify the network lifetime gains of regenerating\ncoding over classical schemes. We also propose a distributed algorithm, TROY,\nthat determines which nodes and routes to use for data regeneration. Our\nanalytical studies show that for certain topologies, TROY achieves maximum\nnetwork lifetime. Our evaluation studies in real sensor network traces show\nthat TROY achieves near optimal lifetime and performs better than baseline\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.0093v1", 
    "title": "Optimizing ccNUMA locality for task-parallel execution under OpenMP and   TBB on multicore-based systems", 
    "arxiv-id": "1101.0093v1", 
    "author": "Georg Hager", 
    "publish": "2010-12-30T14:55:02Z", 
    "summary": "Task parallelism as employed by the OpenMP task construct or some Intel\nThreading Building Blocks (TBB) components, although ideal for tackling\nirregular problems or typical producer/consumer schemes, bears some potential\nfor performance bottlenecks if locality of data access is important, which is\ntypically the case for memory-bound code on ccNUMA systems. We present a thin\nsoftware layer ameliorates adverse effects of dynamic task distribution by\nsorting tasks into locality queues, each of which is preferably processed by\nthreads that belong to the same locality domain. Dynamic scheduling is fully\npreserved inside each domain, and is preferred over possible load imbalance\neven if nonlocal access is required, making this strategy well-suited for\ntypical multicore-mutisocket systems. The effectiveness of the approach is\ndemonstrated by using a blocked six-point stencil solver as a toy model."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.0357v1", 
    "title": "Data Intensive High Energy Physics Analysis in a Distributed Cloud", 
    "arxiv-id": "1101.0357v1", 
    "author": "W. Podaima", 
    "publish": "2011-01-01T17:44:04Z", 
    "summary": "We show that distributed Infrastructure-as-a-Service (IaaS) compute clouds\ncan be effectively used for the analysis of high energy physics data. We have\ndesigned a distributed cloud system that works with any application using large\ninput data sets requiring a high throughput computing environment. The system\nuses IaaS-enabled science and commercial clusters in Canada and the United\nStates. We describe the process in which a user prepares an analysis virtual\nmachine (VM) and submits batch jobs to a central scheduler. The system boots\nthe user-specific VM on one of the IaaS clouds, runs the jobs and returns the\noutput to the user. The user application accesses a central database for\ncalibration data during the execution of the application. Similarly, the data\nis located in a central location and streamed by the running application. The\nsystem can easily run one hundred simultaneous jobs in an efficient manner and\nshould scale to many hundreds and possibly thousands of user jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.0664v1", 
    "title": "Computer Simulation Center in Internet", 
    "arxiv-id": "1101.0664v1", 
    "author": "E. V. Vorozhtsov", 
    "publish": "2011-01-04T06:58:07Z", 
    "summary": "The general description of infrastructure and content of SciShop.ru computer\nsimulation center is given. This resource is a new form of knowledge generation\nand remote education using modern Cloud Computing technologies."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.1680v1", 
    "title": "Safe Register Token Transfer in a Ring", 
    "arxiv-id": "1101.1680v1", 
    "author": "Ted Herman", 
    "publish": "2011-01-09T22:24:44Z", 
    "summary": "A token ring is an arrangement of N processors that take turns engaging in an\nactivity which must be controlled. A token confers the right to engage in the\ncontrolled activity. Processors communicate with neighbors in the ring to\nobtain and release a token. The communication mechanism investigated in this\npaper is the safe register abstraction, which may arbitrarily corrupt a value\nthat a processor reads when the operation reading a register is concurrent with\nan write operation on that register by a neighboring processor. The main\nresults are simple protocols for quasi-atomic communication, constructed from\nsafe registers. A quasi-atomic register behaves atomically except that a\nspecial undefined value may be returned in the case of concurrent read and\nwrite operations. Under certain conditions that constrain the number of writes\nand registers, quasi-atomic protocols are adequate substitutes for atomic\nprotocols. The paper demonstrates how quasi-atomic protocols can be used to\nimplement a self-stabilizing token ring, either by using two safe registers\nbetween neighboring processors or by using O(lg N) safe registers between\nneighbors, which lowers read complexity."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.1846v1", 
    "title": "Using graphics processing units to generate random numbers", 
    "arxiv-id": "1101.1846v1", 
    "author": "B. Ozell", 
    "publish": "2011-01-10T15:27:48Z", 
    "summary": "The future of high-performance computing is aligning itself towards the\nefficient use of highly parallel computing environments. One application where\nthe use of massive parallelism comes instinctively is Monte Carlo simulations,\nwhere a large number of independent events have to be simulated. At the core of\nthe Monte Carlo simulation lies the Random Number Generator (RNG). In this\npaper, the massively parallel implementation of a collection of pseudo-random\nnumber generators on a graphics processing unit (GPU) is presented. The results\nof the GPU implementation, in terms of samples/s, effective bandwidth and\noperations per second, are presented. A comparison with other implementations\non different hardware platforms, in terms of samples/s, power efficiency and\ncost-benefit, is also presented. Random numbers generation throughput of up to\n~18MSamples/s have been achieved on the graphics hardware used. Efficient\nhardware utilization, in terms of operations per second, has reached ~98% of\nthe possible integer operation throughput."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.1902v1", 
    "title": "Sorting, Searching, and Simulation in the MapReduce Framework", 
    "arxiv-id": "1101.1902v1", 
    "author": "Qin Zhang", 
    "publish": "2011-01-10T17:46:40Z", 
    "summary": "In this paper, we study the MapReduce framework from an algorithmic\nstandpoint and demonstrate the usefulness of our approach by designing and\nanalyzing efficient MapReduce algorithms for fundamental sorting, searching,\nand simulation problems. This study is motivated by a goal of ultimately\nputting the MapReduce framework on an equal theoretical footing with the\nwell-known PRAM and BSP parallel models, which would benefit both the theory\nand practice of MapReduce algorithms. We describe efficient MapReduce\nalgorithms for sorting, multi-searching, and simulations of parallel algorithms\nspecified in the BSP and CRCW PRAM models. We also provide some applications of\nthese results to problems in parallel computational geometry for the MapReduce\nframework, which result in efficient MapReduce algorithms for sorting, 2- and\n3-dimensional convex hulls, and fixed-dimensional linear programming. For the\ncase when mappers and reducers have a memory/message-I/O size of\n$M=\\Theta(N^\\epsilon)$, for a small constant $\\epsilon>0$, all of our MapReduce\nalgorithms for these applications run in a constant number of rounds."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.1932v1", 
    "title": "Efficient tilings of de Bruijn and Kautz graphs", 
    "arxiv-id": "1101.1932v1", 
    "author": "Lawrence C. Stewart", 
    "publish": "2011-01-10T19:34:56Z", 
    "summary": "Kautz and de Bruijn graphs have a high degree of connectivity which makes\nthem ideal candidates for massively parallel computer network topologies. In\norder to realize a practical computer architecture based on these graphs, it is\nuseful to have a means of constructing a large-scale system from smaller,\nsimpler modules. In this paper we consider the mathematical problem of\nuniformly tiling a de Bruijn or Kautz graph. This can be viewed as a\ngeneralization of the graph bisection problem. We focus on the problem of graph\ntilings by a set of identical subgraphs. Tiles should contain a maximal number\nof internal edges so as to minimize the number of edges connecting distinct\ntiles. We find necessary and sufficient conditions for the construction of\ntilings. We derive a simple lower bound on the number of edges which must leave\neach tile, and construct a class of tilings whose number of edges leaving each\ntile agrees asymptotically in form with the lower bound to within a constant\nfactor. These tilings make possible the construction of large-scale computing\nsystems based on de Bruijn and Kautz graph topologies."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2347583.2347588", 
    "link": "http://arxiv.org/pdf/1101.2573v1", 
    "title": "An Overview of Portable Distributed Techniques", 
    "arxiv-id": "1101.2573v1", 
    "author": "Nirved Pandey", 
    "publish": "2011-01-13T14:50:33Z", 
    "summary": "In this paper, we reviewed of several portable parallel programming paradigms\nfor use in a distributed programming environment. The Techniques reviewed here\nare portable. These are mainly distributing computing using MPI pure java\nbased, MPI native java based (JNI) and PVM. We will discuss architecture and\nutilities of each technique based on our literature review. We explored these\nportable distributed techniques in four important characteristics scalability,\nfault tolerance, load balancing and performance. We have identified the various\nfactors and issues for improving these four important characteristics."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4116v3", 
    "title": "GridCertLib: a Single Sign-on Solution for Grid Web Applications and   Portals", 
    "arxiv-id": "1101.4116v3", 
    "author": "Valery Tschopp", 
    "publish": "2011-01-21T11:56:07Z", 
    "summary": "This paper describes the design and implementation of GridCertLib, a Java\nlibrary leveraging a Shibboleth-based authentication infrastructure and the\nSLCS online certificate signing service, to provide short-lived X.509\ncertificates and Grid proxies. The main use case envisioned for GridCertLib, is\nto provide seamless and secure access to Grid/X.509 certificates and proxies in\nweb applications and portals: when a user logs in to the portal using\nShibboleth authentication, GridCertLib can automatically obtain a Grid/X.509\ncertificate from the SLCS service and generate a VOMS proxy from it. We give an\noverview of the architecture of GridCertLib and briefly describe its\nprogramming model. Its application to some deployment scenarios is outlined, as\nwell as a report on practical experience integrating GridCertLib into portals\nfor Bioinformatics and Computational Chemistry applications, based on the\npopular P-GRADE and Django softwares."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4193v2", 
    "title": "A Model for Coherent Distributed Memory For Race Condition Detection", 
    "arxiv-id": "1101.4193v2", 
    "author": "Camille Coti", 
    "publish": "2011-01-21T17:47:24Z", 
    "summary": "We present a new model for distributed shared memory systems, based on remote\ndata accesses. Such features are offered by network interface cards that allow\none-sided operations, remote direct memory access and OS bypass. This model\nleads to new interpretations of distributed algorithms allowing us to propose\nan innovative detection technique of race conditions only based on logical\nclocks. Indeed, the presence of (data) races in a parallel program makes it\nhard to reason about and is usually considered as a bug."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1101.4474v1", 
    "title": "Thermal Analysis of Climate Regions using Remote Sensing and Grid   Computing", 
    "arxiv-id": "1101.4474v1", 
    "author": "Carmen Maftei", 
    "publish": "2011-01-24T09:32:03Z", 
    "summary": "The analysis of climate regions is very important for designers and\narchitects, because the increase in density and built up spaces and reduction\nin open spaces and green lands induce the increase of heat, especially in an\nurban area, deteriorating the environment and causing health problems. This\nstudy analyzes the Land Surface Temperature (LST) differences in the region of\nDobrogea, Romania, and compares with the land use and land cover types using TM\nand ETM+ data of 1989 and 2000. As the analysis is performed on large data\nsets, we used Grid Computing to implement a service for using on Computational\nGrids with a Web-based client interface, which will be greatly useful and\nconvenient for those who are studying the ground thermal environment and heat\nisland effects by using Landsat TM/ETM+ bands, and have typical workstations,\nwith no special computing and storing resources for computationally intensive\nsatellite image processing and no license for a commercial image processing\ntool. Based on the satellite imagery, the paper also addresses a Supervised\nClassification algorithm and the computation of two indices of great value in\nwater resources management, Normalized Difference Vegetation Index (NDVI),\nrespectively Land Surface Emissivity (LSE)."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.0516v1", 
    "title": "QOS based user driven scheduler for grid environment", 
    "arxiv-id": "1102.0516v1", 
    "author": "Madhuri Bhavsar", 
    "publish": "2011-02-02T17:36:58Z", 
    "summary": "As grids are in essence heterogeneous, dynamic, shared and distributed\nenvironments, managing these kinds of platforms efficiently is extremely\ncomplex. A promising scalable approach to deal with these intricacies is the\ndesign of self-managing of autonomic applications. Autonomic applications adapt\ntheir execution accordingly by considering knowledge about their own behaviour\nand environmental conditions.QoS based User Driven scheduling for grid that\nprovides the self-optimizing ability in autonomic applications. Computational\ngrids to provide a user to solve large scale problem by spreading a single\nlarge computation across multiple machines of physical location. QoS based User\nDriven scheduler for grid also provides reliability of the grid systems and\nincrease the performance of the grid to reducing the execution time of job by\napplying scheduling policies defined by the user. The main aim of this paper is\nto distribute the computational load among the available grid nodes and to\ndeveloped a QoS based scheduling algorithm for grid and making grid more\nreliable.Grid computing system is different from conventional distributed\ncomputing systems by its focus on large scale resource sharing, where\nprocessors and communication have significant inuence on Grid computing\nreliability. Reliability capabilities initiated by end users from within\napplications they submit to the grid for execution. Reliability of\ninfrastructure and management services that perform essential functions\nnecessary for grid systems to operate, such as resource allocation and\nscheduling."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.1754v1", 
    "title": "Probability Based Adaptive Invoked Clustering Algorithm in MANETs", 
    "arxiv-id": "1102.1754v1", 
    "author": "K. Indumathi", 
    "publish": "2011-02-09T00:25:31Z", 
    "summary": "A mobile ad hoc network (MANET), is a self-configuring network of mobile\ndevices connected by wireless links. In order to achieve stable clusters, the\ncluster-heads maintaining the cluster should be stable with minimum overhead of\ncluster re-elections. In this paper we propose a Probability Based Adaptive\nInvoked Weighted Clustering Algorithm (PAIWCA) which can enhance the stability\nof the clusters by taking battery power of the nodes into considerations for\nthe clustering formation and electing stable cluster-heads using cluster head\nprobability of a node. In this simulation study a comparison was conducted to\nmeasure the performance of our algorithm with maximal weighted independent set\n(MWIS) in terms of the number of clusters formed, the connectivity of the\nnetwork, dominant set updates,throughput of the overall network and packet\ndelivery ratio. The result shows that our algorithm performs better than\nexisting one and is also tunable to different kinds of network conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2131v2", 
    "title": "Analytical Study of Object Components for Distributed and Ubiquitous   Computing Environment", 
    "arxiv-id": "1102.2131v2", 
    "author": "Sachin Bhardwaj", 
    "publish": "2011-02-10T14:39:49Z", 
    "summary": "The Distributed object computing is a paradigm that allows objects to be\ndistributed across a heterogeneous network, and allows each of the components\nto interoperate as a unified whole. A new generation of distributed\napplications, such as telemedicine and e-commerce applications, are being\ndeployed in heterogeneous and ubiquitous computing environments. The objective\nof this paper is to explore an applicability of a component based services in\nubiquitous computational environment. While the fundamental structure of\nvarious distributed object components is similar, there are differences that\ncan profoundly impact an application developer or the administrator of a\ndistributed simulation exercise and to implement in Ubiquitous Computing\nEnvironment."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2608v1", 
    "title": "Power Efficient Resource Allocation for Clouds Using Ant Colony   Framework", 
    "arxiv-id": "1102.2608v1", 
    "author": "Madhu Kumar S D", 
    "publish": "2011-02-13T15:56:29Z", 
    "summary": "Cloud computing is one of the rapidly improving technologies. It provides\nscalable resources needed for the ap- plications hosted on it. As cloud-based\nservices become more dynamic, resource provisioning becomes more challenging.\nThe QoS constrained resource allocation problem is considered in this paper, in\nwhich customers are willing to host their applications on the provider's cloud\nwith a given SLA requirements for performance such as throughput and response\ntime. Since, the data centers hosting the applications consume huge amounts of\nenergy and cause huge operational costs, solutions that reduce energy\nconsumption as well as operational costs are gaining importance. In this work,\nwe propose an energy efficient mechanism that allocates the cloud resources to\nthe applications without violating the given service level agreements(SLA)\nusing Ant colony framework."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10723-011-9195-y", 
    "link": "http://arxiv.org/pdf/1102.2616v1", 
    "title": "An Improved Multiple Faults Reassignment based Recovery in Cluster   Computing", 
    "arxiv-id": "1102.2616v1", 
    "author": "Sanjeev Sharma", 
    "publish": "2011-02-13T16:50:30Z", 
    "summary": "In case of multiple node failures performance becomes very low as compare to\nsingle node failure. Failures of nodes in cluster computing can be tolerated by\nmultiple fault tolerant computing. Existing recovery schemes are efficient for\nsingle fault but not with multiple faults. Recovery scheme proposed in this\npaper having two phases; sequentially phase, concurrent phase. In sequentially\nphase, loads of all working nodes are uniformly and evenly distributed by\nproposed dynamic rank based and load distribution algorithm. In concurrent\nphase, loads of all failure nodes as well as new job arrival are assigned\nequally to all available nodes by just finding the least loaded node among the\nseveral nodes by failure nodes job allocation algorithm. Sequential and\nconcurrent executions of algorithms improve the performance as well better\nresource utilization. Dynamic rank based algorithm for load redistribution\nworks as a sequential restoration algorithm and reassignment algorithm for\ndistribution of failure nodes to least loaded computing nodes works as a\nconcurrent recovery reassignment algorithm. Since load is evenly and uniformly\ndistributed among all available working nodes with less number of iterations,\nlow iterative time and communication overheads hence performance is improved.\nDynamic ranking algorithm is low overhead, high convergence algorithm for\nreassignment of tasks uniformly among all available nodes. Reassignments of\nfailure nodes are done by a low overhead efficient failure job allocation\nalgorithm. Test results to show effectiveness of the proposed scheme are\npresented."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/5/052019", 
    "link": "http://arxiv.org/pdf/1102.3114v1", 
    "title": "Establishing Applicability of SSDs to LHC Tier-2 Hardware Configuration", 
    "arxiv-id": "1102.3114v1", 
    "author": "Mike Kenyon", 
    "publish": "2011-02-15T16:05:04Z", 
    "summary": "Solid State Disk technologies are increasingly replacing high-speed hard\ndisks as the storage technology in high-random-I/O environments. There are\nseveral potentially I/O bound services within the typical LHC Tier-2 - in the\nback-end, with the trend towards many-core architectures continuing, worker\nnodes running many single-threaded jobs and storage nodes delivering many\nsimultaneous files can both exhibit I/O limited efficiency. We estimate the\neffectiveness of affordable SSDs in the context of worker nodes, on a large\nTier-2 production setup using both low level tools and real LHC I/O intensive\ndata analysis jobs comparing and contrasting with high performance spinning\ndisk based solutions. We consider the applicability of each solution in the\ncontext of its price/performance metrics, with an eye on the pragmatic issues\nfacing Tier-2 provision and upgrades"
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/331/5/052019", 
    "link": "http://arxiv.org/pdf/1102.3563v1", 
    "title": "Parallel algorithms for SAT in application to inversion problems of some   discrete functions", 
    "arxiv-id": "1102.3563v1", 
    "author": "Mikhail Posypkin", 
    "publish": "2011-02-17T11:28:21Z", 
    "summary": "In this article we consider the inversion problem for polynomially computable\ndiscrete functions. These functions describe behavior of many discrete systems\nand are used in model checking, hardware verification, cryptanalysis, computer\nbiology and other domains. Quite often it is necessary to invert these\nfunctions, i.e. to find an unknown preimage if an image and algorithm of\nfunction computation are given. In general case this problem is computationally\nintractable. However, many of it's special cases are very important in\npractical applications. Thus development of algorithms that are applicable to\nthese special cases is of importance. The practical applicability of such\nalgorithms can be validated by their ability to solve the problems that are\nconsidered to be computationally hard (for example cryptanalysis problems). In\nthis article we propose the technology of solving the inversion problem for\npolynomially computable discrete functions. This technology was implemented in\ndistributed computing environments (parallel clusters and Grid-systems). It is\nbased on reducing the inversion problem for the considered function to some SAT\nproblem. We describe a general approach to coarse-grained parallelization for\nobtained SAT problems. Efficiency of each parallelization scheme is determined\nby the means of a special predictive function. The proposed technology was\nvalidated by successful solving of cryptanalysis problems for some keystream\ngenerators. The main practical result of this work is a complete cryptanalysis\nof keystream generator A5/1 which was performed in a Grid system specially\nbuilt for this task."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.4423v1", 
    "title": "Solving k-Set Agreement with Stable Skeleton Graphs", 
    "arxiv-id": "1102.4423v1", 
    "author": "Ulrich Schmid", 
    "publish": "2011-02-22T07:41:38Z", 
    "summary": "In this paper we consider the k-set agreement problem in distributed\nmessage-passing systems using a round-based approach: Both synchrony of\ncommunication and failures are captured just by means of the messages that\narrive within a round, resulting in round-by-round communication graphs that\ncan be characterized by simple communication predicates. We introduce the weak\ncommunication predicate PSources(k) and show that it is tight for k-set\nagreement, in the following sense: We (i) prove that there is no algorithm for\nsolving (k-1)-set agreement in systems characterized by PSources(k), and (ii)\npresent a novel distributed algorithm that achieves k-set agreement in runs\nwhere PSources(k) holds. Our algorithm uses local approximations of the stable\nskeleton graph, which reflects the underlying perpetual synchrony of a run. We\nprove that this approximation is correct in all runs, regardless of the\ncommunication predicate, and show that graph-theoretic properties of the stable\nskeleton graph can be used to solve k-set agreement if PSources(k) holds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.4946v1", 
    "title": "An Equivariance Theorem with Applications to Renaming (Preliminary   Version)", 
    "arxiv-id": "1102.4946v1", 
    "author": "Sergio Rajsbaum", 
    "publish": "2011-02-24T10:24:43Z", 
    "summary": "In the renaming problem, each process in a distributed system is issued a\nunique name from a large name space, and the processes must coordinate with one\nanother to choose unique names from a much smaller name space. We show that\nlower bounds on the solvability of renaming in an asynchronous distributed\nsystem can be formulated as a purely topological question about the existence\nof an equivariant chain map from a topological disk to a topological annulus.\nProving the non-existence of such a map implies the non-existence of a\ndistributed renaming algorithm in several related models of computation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1102.5328v1", 
    "title": "Fully Empirical Autotuned QR Factorization For Multicore Architectures", 
    "arxiv-id": "1102.5328v1", 
    "author": "Stanimire Tomov", 
    "publish": "2011-02-25T20:21:32Z", 
    "summary": "Tuning numerical libraries has become more difficult over time, as systems\nget more sophisticated. In particular, modern multicore machines make the\nbehaviour of algorithms hard to forecast and model. In this paper, we tackle\nthe issue of tuning a dense QR factorization on multicore architectures. We\nshow that it is hard to rely on a model, which motivates us to design a fully\nempirical approach. We exhibit few strong empirical properties that enable us\nto efficiently prune the search space. Our method is automatic, fast and\nreliable. The tuning process is indeed fully performed at install time in less\nthan one and ten minutes on five out of seven platforms. We achieve an average\nperformance varying from 97% to 100% of the optimum performance depending on\nthe platform. This work is a basis for autotuning the PLASMA library and\nenabling easy performance portability across hardware systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.0759v1", 
    "title": "Scheduler Vulnerabilities and Attacks in Cloud Computing", 
    "arxiv-id": "1103.0759v1", 
    "author": "Ravi Sundaram", 
    "publish": "2011-03-03T19:09:47Z", 
    "summary": "In hardware virtualization a hypervisor provides multiple Virtual Machines\n(VMs) on a single physical system, each executing a separate operating system\ninstance. The hypervisor schedules execution of these VMs much as the scheduler\nin an operating system does, balancing factors such as fairness and I/O\nperformance. As in an operating system, the scheduler may be vulnerable to\nmalicious behavior on the part of users seeking to deny service to others or\nmaximize their own resource usage.\n  Recently, publically available cloud computing services such as Amazon EC2\nhave used virtualization to provide customers with virtual machines running on\nthe provider's hardware, typically charging by wall clock time rather than\nresources consumed. Under this business model, manipulation of the scheduler\nmay allow theft of service at the expense of other customers, rather than\nmerely reallocating resources within the same administrative domain.\n  We describe a flaw in the Xen scheduler allowing virtual machines to consume\nalmost all CPU time, in preference to other users, and demonstrate kernel-based\nand user-space versions of the attack. We show results demonstrating the\nvulnerability in the lab, consuming as much as 98% of CPU time regardless of\nfair share, as well as on Amazon EC2, where Xen modifications protect other\nusers but still allow theft of service. In case of EC2, following the\nresponsible disclosure model, we have reported this vulnerability to Amazon;\nthey have since implemented a fix that we have tested and verified (See\nAppendix B). We provide a novel analysis of the necessary conditions for such\nattacks, and describe scheduler modifications to eliminate the vulnerability.\n  We present experimental results demonstrating the effectiveness of these\ndefenses while imposing negligible overhead."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.1207v1", 
    "title": "Framework to Solve Load Balancing Problem in Heterogeneous Web Servers", 
    "arxiv-id": "1103.1207v1", 
    "author": "Ms. Archana B. Saxena", 
    "publish": "2011-03-07T07:40:22Z", 
    "summary": "For popular websites most important concern is to handle incoming load\ndynamically among web servers, so that they can respond to their client without\nany wait or failure. Different websites use different strategies to distribute\nload among web servers but most of the schemes concentrate on only one factor\nthat is number of requests, but none of the schemes consider the point that\ndifferent type of requests will require different level of processing efforts\nto answer, status record of all the web servers that are associated with one\ndomain name and mechanism to handle a situation when one of the servers is not\nworking. Therefore, there is a fundamental need to develop strategy for dynamic\nload allocation on web side. In this paper, an effort has been made to\nintroduce a cluster based frame work to solve load distribution problem. This\nframework aims to distribute load among clusters on the basis of their\noperational capabilities. Moreover, the experimental results are shown with the\nhelp of example, algorithm and analysis of the algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.1302v9", 
    "title": "On the Cost of Concurrency in Transactional Memory", 
    "arxiv-id": "1103.1302v9", 
    "author": "Srivatsan Ravi", 
    "publish": "2011-03-07T15:37:44Z", 
    "summary": "The crux of software transactional memory (STM) is to combine an easy-to-use\nprogramming interface with an efficient utilization of the concurrent-computing\nabilities provided by modern machines. But does this combination come with an\ninherent cost? We evaluate the cost of concurrency by measuring the amount of\nexpensive synchronization that must be employed in an STM implementation that\nensures positive concurrency, i.e., allows for concurrent transaction\nprocessing in some executions. We focus on two popular progress conditions that\nprovide positive concurrency: progressiveness and permissiveness. We show that\nin permissive STMs, providing a very high degree of concurrency, a transaction\nperforms a linear number of expensive synchronization patterns with respect to\nits read-set size. In contrast, progressive STMs provide a very small degree of\nconcurrency but, as we demonstrate, can be implemented using at most one\nexpensive synchronization pattern per transaction. However, we show that even\nin progressive STMs, a transaction has to \"protect\" (e.g., by using locks or\nstrong synchronization primitives) a linear amount of data with respect to its\nwrite-set size. Our results suggest that looking for high degrees of\nconcurrency in STM implementations may bring a considerable synchronization\ncost."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.2590v1", 
    "title": "Aneka Cloud Application Platform and Its Integration with Windows Azure", 
    "arxiv-id": "1103.2590v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-03-14T06:38:11Z", 
    "summary": "Aneka is an Application Platform-as-a-Service (Aneka PaaS) for Cloud\nComputing. It acts as a framework for building customized applications and\ndeploying them on either public or private Clouds. One of the key features of\nAneka is its support for provisioning resources on different public Cloud\nproviders such as Amazon EC2, Windows Azure and GoGrid. In this chapter, we\nwill present Aneka platform and its integration with one of the public Cloud\ninfrastructures, Windows Azure, which enables the usage of Windows Azure\nCompute Service as a resource provider of Aneka PaaS. The integration of the\ntwo platforms will allow users to leverage the power of Windows Azure Platform\nfor Aneka Cloud Computing, employing a large number of compute instances to run\ntheir applications in parallel. Furthermore, customers of the Windows Azure\nplatform can benefit from the integration with Aneka PaaS by embracing the\nadvanced features of Aneka in terms of multiple programming models, scheduling\nand management services, application execution services, accounting and pricing\nservices and dynamic provisioning services. Finally, in addition to the Windows\nAzure Platform we will illustrate in this chapter the integration of Aneka PaaS\nwith other public Cloud platforms such as Amazon EC2 and GoGrid, and virtual\nmachine management platforms such as Xen Server. The new support of\nprovisioning resources on Windows Azure once again proves the adaptability,\nextensibility and flexibility of Aneka."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.3515v1", 
    "title": "Self-Stabilization, Byzantine Containment, and Maximizable Metrics:   Necessary Conditions", 
    "arxiv-id": "1103.3515v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-03-17T20:37:17Z", 
    "summary": "Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties leads to some impossibility results. In this paper, we provide two\nnecessary conditions to construct maximum metric tree in presence of transients\nand (permanent) Byzantine faults."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.3671v2", 
    "title": "Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems", 
    "arxiv-id": "1103.3671v2", 
    "author": "Ulrich Schmid", 
    "publish": "2011-03-18T17:31:41Z", 
    "summary": "Despite of being quite similar agreement problems, consensus and general\nk-set agreement require surprisingly different techniques for proving the\nimpossibility in asynchronous systems with crash failures: Rather than\nrelatively simple bivalence arguments as in the impossibility proof for\nconsensus (= 1-set agreement) in the presence of a single crash failure, known\nproofs for the impossibility of k-set agreement in systems with at least k>1\ncrash failures use algebraic topology or a variant of Sperner's Lemma. In this\npaper, we present a generic theorem for proving the impossibility of k-set\nagreement in various message passing settings, which is based on a simple\nreduction to the consensus impossibility in a certain subsystem. We demonstrate\nthe broad applicability of our result by exploring the\npossibility/impossibility border of k-set agreement in several message-passing\nsystem models: (i) asynchronous systems with crash failures, (ii) partially\nsynchronous processes with (initial) crash failures, and (iii) asynchronous\nsystems augmented with failure detectors. In (i) and (ii), the impossibility\npart is just an instantiation of our main theorem, whereas the possibility of\nachieving k-set agreement in (ii) follows by generalizing the consensus\nalgorithm for initial crashes by Fisher, Lynch and Patterson. In (iii),\napplying our technique yields the exact border for the parameter k where k-set\nagreement is solvable with the failure detector class (Sigma_k,Omega_k), for\n(1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be\nnecessary for solving k-set agreement, this result yields new insights on the\nquest for the weakest failure detector."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.4690v2", 
    "title": "Linearizable Implementations Do Not Suffice for Randomized Distributed   Computation", 
    "arxiv-id": "1103.4690v2", 
    "author": "Philipp Woelfel", 
    "publish": "2011-03-24T07:54:19Z", 
    "summary": "Linearizability is the gold standard among algorithm designers for deducing\nthe correctness of a distributed algorithm using implemented shared objects\nfrom the correctness of the corresponding algorithm using atomic versions of\nthe same objects. We show that linearizability does not suffice for this\npurpose when processes can exploit randomization, and we discuss the existence\nof alternative correctness conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5743v1", 
    "title": "Load Balancing in a Networked Environment through Homogenization", 
    "arxiv-id": "1103.5743v1", 
    "author": "Md. Mahbubul Alam Joarder", 
    "publish": "2011-03-29T19:56:06Z", 
    "summary": "Distributed processing across a networked environment suffers from\nunpredictable behavior of speedup due to heterogeneous nature of the hardware\nand software in the remote machines. It is challenging to get a better\nperformance from a distributed system by distributing task in an intelligent\nmanner such that the heterogeneous nature of the system do not have any effect\non the speedup ratio. This paper introduces homogenization, a technique that\ndistributes and balances the workload in such a manner that the user gets the\nhighest speedup possible from a distributed environment. Along with providing\nbetter performance, homogenization is totally transparent to the user and\nrequires no interaction with the system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5760v1", 
    "title": "Triangular Dynamic Architecture for Distributed Computing in a LAN   Environment", 
    "arxiv-id": "1103.5760v1", 
    "author": "Debzani Deb", 
    "publish": "2011-03-29T20:05:50Z", 
    "summary": "A computationally intensive large job, granulized to concurrent pieces and\noperating in a dynamic environment should reduce the total processing time.\nHowever, distributing jobs across a networked environment is a tedious and\ndifficult task. Job distribution in a Local Area Network based on Triangular\nDynamic Architecture (TDA) is a mechanism that establishes a dynamic\nenvironment for job distribution, load balancing and distributed processing\nwith minimum interaction from the user. This paper introduces TDA and discusses\nits architecture and shows the benefits gained by utilizing such architecture\nin a distributed computing environment."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5764v1", 
    "title": "Agent Based Processing of Global Evaluation Function", 
    "arxiv-id": "1103.5764v1", 
    "author": "Md. Mahbubul Alam Joarder", 
    "publish": "2011-03-29T20:15:48Z", 
    "summary": "Load balancing across a networked environment is a monotonous job. Moreover,\nif the job to be distributed is a constraint satisfying one, the distribution\nof load demands core intelligence. This paper proposes parallel processing\nthrough Global Evaluation Function by means of randomly initialized agents for\nsolving Constraint Satisfaction Problems. A potential issue about the number of\nagents in a machine under the invocation of distribution is discussed here for\nsecuring the maximum benefit from Global Evaluation and parallel processing.\nThe proposed system is compared with typical solution that shows an exclusive\noutcome supporting the nobility of parallel implementation of Global Evaluation\nFunction with certain number of agents in each invoked machine."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.5794v2", 
    "title": "The Space Complexity of Long-lived and One-Shot Timestamp   Implementations", 
    "arxiv-id": "1103.5794v2", 
    "author": "Philipp Woelfel", 
    "publish": "2011-03-29T23:43:21Z", 
    "summary": "This paper is concerned with the problem of implementing an unbounded\ntimestamp object from multi-writer atomic registers, in an asynchronous\ndistributed system of n processors with distinct identifiers where timestamps\nare taken from an arbitrary universe. Ellen, Fatourou and Ruppert (2008) showed\nthat sqrt{n}/2-O(1) registers are required for any obstruction-free\nimplementation of long-lived timestamp systems from atomic registers (meaning\nprocessors can repeatedly get timestamps). We improve this existing lower bound\nin two ways. First we establish a lower bound of n/6 - O(1) registers for the\nobstruction-free long-lived timestamp problem. Previous such linear lower\nbounds were only known for constrained versions of the timestamp problem. This\nbound is asymptotically tight; Ellen, Fatourou and Ruppert (2008) constructed a\nwait-free algorithm that uses n-1 registers. Second we show that sqrt{n} - O(1)\nregisters are required for any obstruction-free implementation of one-shot\ntimestamp systems(meaning each processor can get a timestamp at most once). We\nshow that this bound is also asymptotically tight by providing a wait-free\none-shot timestamp system that uses fewer than 2 sqrt{n} registers, thus\nestablishing a space complexity gap between one-shot and long-lived timestamp\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.6087v1", 
    "title": "Automatic Performance Debugging of SPMD-style Parallel Programs", 
    "arxiv-id": "1103.6087v1", 
    "author": "Lei Wang", 
    "publish": "2011-03-31T05:38:39Z", 
    "summary": "The simple program and multiple data (SPMD) programming model is widely used\nfor both high performance computing and Cloud computing. In this paper, we\ndesign and implement an innovative system, AutoAnalyzer, that automates the\nprocess of debugging performance problems of SPMD-style parallel programs,\nincluding data collection, performance behavior analysis, locating bottlenecks,\nand uncovering their root causes. AutoAnalyzer is unique in terms of two\nfeatures: first, without any apriori knowledge, it automatically locates\nbottlenecks and uncovers their root causes for performance optimization;\nsecond, it is lightweight in terms of the size of performance data to be\ncollected and analyzed. Our contributions are three-fold: first, we propose two\neffective clustering algorithms to investigate the existence of performance\nbottlenecks that cause process behavior dissimilarity or code region behavior\ndisparity, respectively; meanwhile, we present two searching algorithms to\nlocate bottlenecks; second, on a basis of the rough set theory, we propose an\ninnovative approach to automatically uncovering root causes of bottlenecks;\nthird, on the cluster systems with two different configurations, we use two\nproduction applications, written in Fortran 77, and one open source\ncode-MPIBZIP2 (http://compression.ca/mpibzip2/), written in C++, to verify the\neffectiveness and correctness of our methods. For three applications, we also\npropose an experimental approach to investigating the effects of different\nmetrics on locating bottlenecks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2011.301", 
    "link": "http://arxiv.org/pdf/1103.6114v2", 
    "title": "The Impact of Memory Models on Software Reliability in Multiprocessors", 
    "arxiv-id": "1103.6114v2", 
    "author": "Karin Strauss", 
    "publish": "2011-03-31T07:43:58Z", 
    "summary": "The memory consistency model is a fundamental system property characterizing\na multiprocessor. The relative merits of strict versus relaxed memory models\nhave been widely debated in terms of their impact on performance, hardware\ncomplexity and programmability. This paper adds a new dimension to this\ndiscussion: the impact of memory models on software reliability. By allowing\nsome instructions to reorder, weak memory models may expand the window between\ncritical memory operations. This can increase the chance of an undesirable\nthread-interleaving, thus allowing an otherwise-unlikely concurrency bug to\nmanifest. To explore this phenomenon, we define and study a probabilistic model\nof shared-memory parallel programs that takes into account such reordering. We\nuse this model to formally derive bounds on the \\emph{vulnerability} to\nconcurrency bugs of different memory models. Our results show that for 2 (or a\nsmall constant number of) concurrent threads, weaker memory models do indeed\nhave a higher likelihood of allowing bugs. On the other hand, we show that as\nthe number of parallel threads increases, the gap between the different memory\nmodels becomes proportionally insignificant. This suggests the\ncounter-intuitive rule that \\emph{as the number of parallel threads in the\nsystem increases, the importance of using a strict memory model diminishes};\nwhich potentially has major implications on the choice of memory consistency\nmodels in future multi-core systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0149v1", 
    "title": "Decision Support Tools for Cloud Migration in the Enterprise", 
    "arxiv-id": "1105.0149v1", 
    "author": "Pradeep Teregowda", 
    "publish": "2011-05-01T07:48:42Z", 
    "summary": "This paper describes two tools that aim to support decision making during the\nmigration of IT systems to the cloud. The first is a modeling tool that\nproduces cost estimates of using public IaaS clouds. The tool enables IT\narchitects to model their applications, data and infrastructure requirements in\naddition to their computational resource usage patterns. The tool can be used\nto compare the cost of different cloud providers, deployment options and usage\nscenarios. The second tool is a spreadsheet that outlines the benefits and\nrisks of using IaaS clouds from an enterprise perspective; this tool provides a\nstarting point for risk assessment. Two case studies were used to evaluate the\ntools. The tools were useful as they informed decision makers about the costs,\nbenefits and risks of using the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0296v1", 
    "title": "A Formal Model of Anonymous Systems", 
    "arxiv-id": "1105.0296v1", 
    "author": "Yang D. Li", 
    "publish": "2011-05-02T10:44:12Z", 
    "summary": "We put forward a formal model of anonymous systems. And we concentrate on the\nanonymous failure detectors in our model. In particular, we give three examples\nof anonymous failure detectors and show that they can be used to solve the\nconsensus problem and that they are equivalent to their classic counterparts.\nMoreover, we show some relationship among them and provide a simple\nclassification of anonymous failure detectors."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0545v4", 
    "title": "On the Degree Distribution of Faulty Peer-to-Peer Overlays", 
    "arxiv-id": "1105.0545v4", 
    "author": "Stefano Ferretti", 
    "publish": "2011-05-03T11:02:37Z", 
    "summary": "This paper presents an analytical framework to model fault-tolerance in\nunstructured peer-to-peer overlays, represented as complex networks. We define\na distributed protocol peers execute for managing the overlay and reacting to\nnode faults. Based on the protocol, evolution equations are defined and\nmanipulated by resorting to generating functions. Obtained outcomes provide\ninsights on the nodes' degree probability distribution. From the study of the\ndegree distribution, it is possible to estimate other important metrics of the\npeer-to-peer overlay, such as the diameter of the network. We study different\nnetworks, characterized by three specific desired degree distributions, i.e.\nnets with nodes having a fixed desired degree, random graphs and scale-free\nnetworks. All these networks are assessed via the analytical tool and\nsimulation as well. Results show that the approach can be factually employed to\ndynamically tune the average attachment rate at peers so that they maintain\ntheir own desired degree and, in general, the desired network topology."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.0668v2", 
    "title": "Secure Position Verification for Wireless Sensor Networks in Noisy   Channels", 
    "arxiv-id": "1105.0668v2", 
    "author": "Anil K. Ghosh", 
    "publish": "2011-05-03T19:59:13Z", 
    "summary": "Position verification in wireless sensor networks (WSNs) is quite tricky in\npresence of attackers (malicious sensor nodes), who try to break the\nverification protocol by reporting their incorrect positions (locations) during\nthe verification stage. In the literature of WSNs, most of the existing methods\nof position verification have used trusted verifiers, which are in fact\nvulnerable to attacks by malicious nodes. They also depend on some distance\nestimation techniques, which are not accurate in noisy channels (mediums). In\nthis article, we propose a secure position verification scheme for WSNs in\nnoisy channels without relying on any trusted entities. Our verification scheme\ndetects and filters out all malicious nodes from the network with very high\nprobability."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1074v2", 
    "title": "Progressive quantization in distributed average consensus", 
    "arxiv-id": "1105.1074v2", 
    "author": "Pascal Frossard", 
    "publish": "2011-05-05T13:56:48Z", 
    "summary": "We consider the problem of distributed average consensus in a sensor network\nwhere sensors exchange quantized information with their neighbors. We propose a\nnovel quantization scheme that exploits the increasing correlation between the\nvalues exchanged by the sensors throughout the iterations of the consensus\nalgorithm. A low complexity, uniform quantizer is implemented in each sensor,\nand refined quantization is achieved by progressively reducing the quantization\nintervals during the convergence of the consensus algorithm. We propose a\nrecurrence relation for computing the quantization parameters that depend on\nthe network topology and the communication rate. We further show that the\nrecurrence relation can lead to a simple exponential model for the size of the\nquantization step size over the iterations, whose parameters can be computed a\npriori. Finally, simulation results demonstrate the effectiveness of the\nprogressive quantization scheme that leads to the consensus solution even at\nlow communication rate."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1248v1", 
    "title": "A Distributed Approximation Algorithm for the Metric Uncapacitated   Facility Location Problem in the Congest Model", 
    "arxiv-id": "1105.1248v1", 
    "author": "Peter Pietrzyk", 
    "publish": "2011-05-06T09:36:03Z", 
    "summary": "We present a randomized distributed approximation algorithm for the metric\nuncapacitated facility location problem. The algorithm is executed on a\nbipartite graph in the Congest model yielding a (1.861 + epsilon) approximation\nfactor, where epsilon is an arbitrary small positive constant. It needs\nO(n^{3/4}log_{1+epsilon}^2(n) communication rounds with high probability (n\ndenoting the number of facilities and clients). To the best of our knowledge,\nour algorithm currently has the best approximation factor for the facility\nlocation problem in a distributed setting. It is based on a greedy sequential\napproximation algorithm by Jain et al. (J. ACM 50(6), pages: 795-824, 2003).\nThe main difficulty in executing this sequential algorithm lies in dealing with\nsituations, where multiple facilities are eligible for opening, but (in order\nto preserve the approximation factor of the sequential algorithm) only a subset\nof them can actually be opened. Note that while the presented runtime bound of\nour algorithm is \"with high probability\", the approximation factor is not \"in\nexpectation\" but always guaranteed to be (1.861 + epsilon). Thus, our main\ncontribution is a sublinear time selection mechanism that, while increasing the\napproximation factor by an arbitrary small additive term, allows us to decide\nwhich of the eligible facilities to open."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.1982v1", 
    "title": "Secure Data Processing in a Hybrid Cloud", 
    "arxiv-id": "1105.1982v1", 
    "author": "Sharad Mehrotra", 
    "publish": "2011-05-10T15:49:38Z", 
    "summary": "Cloud computing has made it possible for a user to be able to select a\ncomputing service precisely when needed. However, certain factors such as\nsecurity of data and regulatory issues will impact a user's choice of using\nsuch a service. A solution to these problems is the use of a hybrid cloud that\ncombines a user's local computing capabilities (for mission- or\norganization-critical tasks) with a public cloud (for less influential tasks).\nWe foresee three challenges that must be overcome before the adoption of a\nhybrid cloud approach: 1) data design: How to partition relations in a hybrid\ncloud? The solution to this problem must account for the sensitivity of\nattributes in a relation as well as the workload of a user; 2) data security:\nHow to protect a user's data in a public cloud with encryption while enabling\nquery processing over this encrypted data? and 3) query processing: How to\nexecute queries efficiently over both, encrypted and unencrypted data? This\npaper addresses these challenges and incorporates their solutions into an\nadd-on tool for a Hadoop and Hive based cloud computing infrastructure."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2011.59", 
    "link": "http://arxiv.org/pdf/1105.2213v1", 
    "title": "A Cloud-based Approach for Context Information Provisioning", 
    "arxiv-id": "1105.2213v1", 
    "author": "Larbi Esmahi", 
    "publish": "2011-05-10T18:02:56Z", 
    "summary": "As a result of the phenomenal proliferation of modern mobile Internet-enabled\ndevices and the widespread utilization of wireless and cellular data networks,\nmobile users are increasingly requiring services tailored to their current\ncontext. High-level context information is typically obtained from context\nservices that aggregate raw context information sensed by various sensors and\nmobile devices. Given the massive amount of sensed data, traditional context\nservices are lacking the necessary resources to store and process these data,\nas well as to disseminate high-level context information to a variety of\npotential context consumers. In this paper, we propose a novel framework for\ncontext information provisioning, which relies on deploying context services on\nthe cloud and using context brokers to mediate between context consumers and\ncontext services using a publish/subscribe model. Moreover, we describe a\nmulti-attributes decision algorithm for the selection of potential context\nservices that can fulfill context consumers' requests for context information.\nThe algorithm calculates the score of each context service, per context\ninformation type, based on the quality-of-service (QoS) and quality-of-context\ninformation (QoC) requirements expressed by the context consumer. One of the\nbenefits of the approach is that context providers can scale up and down, in\nterms of cloud resources they use, depending on current demand for context\ninformation. Besides, the selection algorithm allows ranking context services\nby matching their QoS and QoC offers against the QoS and QoC requirements of\nthe context consumer."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.2301v4", 
    "title": "Parallel and Distributed Simulation from Many Cores to the Public Cloud   (Extended Version)", 
    "arxiv-id": "1105.2301v4", 
    "author": "Gabriele D'Angelo", 
    "publish": "2011-05-11T20:01:11Z", 
    "summary": "In this tutorial paper, we will firstly review some basic simulation concepts\nand then introduce the parallel and distributed simulation techniques in view\nof some new challenges of today and tomorrow. More in particular, in the last\nyears there has been a wide diffusion of many cores architectures and we can\nexpect this trend to continue. On the other hand, the success of cloud\ncomputing is strongly promoting the everything as a service paradigm. Is\nparallel and distributed simulation ready for these new challenges? The current\napproaches present many limitations in terms of usability and adaptivity: there\nis a strong need for new evaluation metrics and for revising the currently\nimplemented mechanisms. In the last part of the paper, we propose a new\napproach based on multi-agent systems for the simulation of complex systems. It\nis possible to implement advanced techniques such as the migration of simulated\nentities in order to build mechanisms that are both adaptive and very easy to\nuse. Adaptive mechanisms are able to significantly reduce the communication\ncost in the parallel/distributed architectures, to implement load-balance\ntechniques and to cope with execution environments that are both variable and\ndynamic. Finally, such mechanisms will be used to build simulations on top of\nunreliable cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.2584v1", 
    "title": "Workload Classification & Software Energy Measurement for Efficient   Scheduling on Private Cloud Platforms", 
    "arxiv-id": "1105.2584v1", 
    "author": "Ian Sommerville", 
    "publish": "2011-05-12T22:00:36Z", 
    "summary": "At present there are a number of barriers to creating an energy efficient\nworkload scheduler for a Private Cloud based data center. Firstly, the\nrelationship between different workloads and power consumption must be\ninvestigated. Secondly, current hardware-based solutions to providing energy\nusage statistics are unsuitable in warehouse scale data centers where low cost\nand scalability are desirable properties. In this paper we discuss the effect\nof different workloads on server power consumption in a Private Cloud platform.\nWe display a noticeable difference in energy consumption when servers are given\ntasks that dominate various resources (CPU, Memory, Hard Disk and Network). We\nthen use this insight to develop CloudMonitor, a software utility that is\ncapable of >95% accurate power predictions from monitoring resource consumption\nof workloads, after a \"training phase\" in which a dynamic power model is\ndeveloped."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.4424v1", 
    "title": "A Modeling Approach based on UML/MARTE for GPU Architecture", 
    "arxiv-id": "1105.4424v1", 
    "author": "Jean-Luc Dekeyser", 
    "publish": "2011-05-23T07:54:02Z", 
    "summary": "Nowadays, the High Performance Computing is part of the context of embedded\nsystems. Graphics Processing Units (GPUs) are more and more used in\nacceleration of the most part of algorithms and applications. Over the past\nyears, not many efforts have been done to describe abstractions of applications\nin relation to their target architectures. Thus, when developers need to\nassociate applications and GPUs, for example, they find difficulty and prefer\nusing API for these architectures. This paper presents a metamodel extension\nfor MARTE profile and a model for GPU architectures. The main goal is to\nspecify the task and data allocation in the memory hierarchy of these\narchitectures. The results show that this approach will help to generate code\nfor GPUs based on model transformations using Model Driven Engineering (MDE)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCSim.2011.5999802", 
    "link": "http://arxiv.org/pdf/1105.5817v1", 
    "title": "Robot Networks with Homonyms: The Case of Patterns Formation", 
    "arxiv-id": "1105.5817v1", 
    "author": "Anissa Lamani", 
    "publish": "2011-05-29T19:23:54Z", 
    "summary": "In this paper, we consider the problem of formation of a series of geometric\npatterns [4] by a network of oblivious mobile robots that communicate only\nthrough vision. So far, the problem has been studied in models where robots are\neither assumed to have distinct identifiers or to be completely anonymous. To\ngeneralize these results and to better understand how anonymity affects the\ncomputational power of robots, we study the problem in a new model, introduced\nrecently in [5], in which n robots may share up to 1 <= h <= n different\nidentifiers. We present necessary and sufficient conditions, relating\nsymmetricity and homonymy, that makes the problem solvable. We also show that\nin the case where h = n, making the identifiers of robots invisible does not\nlimit their computational power. This contradicts a result of [4]. To present\nour algorithms, we use a function that computes the Weber point for many\nregular and symmetric configurations. This function is interesting in its own\nright, since the problem of finding Weber points has been solved up to now for\nonly few other patterns."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2301", 
    "link": "http://arxiv.org/pdf/1105.6040v1", 
    "title": "Parallel Performance of MPI Sorting Algorithms on Dual-Core Processor   Windows-Based Systems", 
    "arxiv-id": "1105.6040v1", 
    "author": "Alaa Ismail Elnashar", 
    "publish": "2011-05-30T16:53:36Z", 
    "summary": "Message Passing Interface (MPI) is widely used to implement parallel\nprograms. Although Windowsbased architectures provide the facilities of\nparallel execution and multi-threading, little attention has been focused on\nusing MPI on these platforms. In this paper we use the dual core Window-based\nplatform to study the effect of parallel processes number and also the number\nof cores on the performance of three MPI parallel implementations for some\nsorting algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1186/1687-1499-2012-231", 
    "link": "http://arxiv.org/pdf/1106.0736v1", 
    "title": "Distributed Stochastic Power Control in Ad-hoc Networks: A Nonconvex   Case", 
    "arxiv-id": "1106.0736v1", 
    "author": "Jason H. Li", 
    "publish": "2011-06-03T19:50:22Z", 
    "summary": "Utility-based power allocation in wireless ad-hoc networks is inherently\nnonconvex because of the global coupling induced by the co-channel\ninterference. To tackle this challenge, we first show that the globally optimal\npoint lies on the boundary of the feasible region, which is utilized as a basis\nto transform the utility maximization problem into an equivalent max-min\nproblem with more structure. By using extended duality theory, penalty\nmultipliers are introduced for penalizing the constraint violations, and the\nminimum weighted utility maximization problem is then decomposed into\nsubproblems for individual users to devise a distributed stochastic power\ncontrol algorithm, where each user stochastically adjusts its target utility to\nimprove the total utility by simulated annealing. The proposed distributed\npower control algorithm can guarantee global optimality at the cost of slow\nconvergence due to simulated annealing involved in the global optimization. The\ngeometric cooling scheme and suitable penalty parameters are used to improve\nthe convergence rate. Next, by integrating the stochastic power control\napproach with the back-pressure algorithm, we develop a joint scheduling and\npower allocation policy to stabilize the queueing systems. Finally, we\ngeneralize the above distributed power control algorithms to multicast\ncommunications, and show their global optimality for multicast traffic."
},{
    "category": "cs.DC", 
    "doi": "10.1186/1687-1499-2012-231", 
    "link": "http://arxiv.org/pdf/1106.0940v1", 
    "title": "Hadoop Performance Models", 
    "arxiv-id": "1106.0940v1", 
    "author": "Herodotos Herodotou", 
    "publish": "2011-06-06T00:02:32Z", 
    "summary": "Hadoop MapReduce is now a popular choice for performing large-scale data\nanalytics. This technical report describes a detailed set of mathematical\nperformance models for describing the execution of a MapReduce job on Hadoop.\nThe models describe dataflow and cost information at the fine granularity of\nphases within the map and reduce tasks of a job execution. The models can be\nused to estimate the performance of MapReduce jobs as well as to find the\noptimal configuration settings to use when running the jobs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1286v1", 
    "title": "Traffic Performance Analysis of Manet Routing Protocol", 
    "arxiv-id": "1106.1286v1", 
    "author": "Y. Venkataramani", 
    "publish": "2011-06-07T09:09:17Z", 
    "summary": "The primary objective of this research work is to study and investigate the\nperformance measures of Gossip Routing protocol and Energy Efficient and\nReliable Adaptive Gossip routing protocols. We use TCP and CBR based traffic\nmodels to analyze the performance of above mentioned protocols based on the\nparameters of Packet Delivery Ratio, Average End-to-End Delay and Throughput.\nWe will investigate the effect of change in the simulation time and Number of\nnodes for the MANET routing protocols. For Simulation, we have used ns-2\nsimulator."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1845v3", 
    "title": "Byzantine Broadcast in Point-to-Point Networks using Local Linear Coding", 
    "arxiv-id": "1106.1845v3", 
    "author": "Nitin Vaidya", 
    "publish": "2011-06-09T16:07:57Z", 
    "summary": "The goal of Byzantine Broadcast (BB) is to allow a set of fault-free nodes to\nagree on information that a source node wants to broadcast to them, in the\npresence of Byzantine faulty nodes. We consider design of efficient algorithms\nfor BB in {\\em synchronous} point-to-point networks, where the rate of\ntransmission over each communication link is limited by its \"link capacity\".\nThe throughput of a particular BB algorithm is defined as the average number of\nbits that can be reliably broadcast to all fault-free nodes per unit time using\nthe algorithm without violating the link capacity constraints. The {\\em\ncapacity} of BB in a given network is then defined as the supremum of all\nachievable BB throughputs in the given network, over all possible BB\nalgorithms.\n  We develop NAB -- a Network-Aware Byzantine broadcast algorithm -- for\narbitrary point-to-point networks consisting of $n$ nodes, wherein the number\nof faulty nodes is at most $f$, $f<n/3$, and the network connectivity is at\nleast $2f+1$. We also prove an upper bound on the capacity of Byzantine\nbroadcast, and conclude that NAB can achieve throughput at least 1/3 of the\ncapacity. When the network satisfies an additional condition, NAB can achieve\nthroughput at least 1/2 of the capacity.\n  To the best of our knowledge, NAB is the first algorithm that can achieve a\nconstant fraction of capacity of Byzantine Broadcast (BB) in arbitrary\npoint-to-point networks."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.1846v1", 
    "title": "New Efficient Error-Free Multi-Valued Consensus with Byzantine Failures", 
    "arxiv-id": "1106.1846v1", 
    "author": "Nitin Vaidya", 
    "publish": "2011-06-09T16:13:54Z", 
    "summary": "In this report, we investigate the multi-valued Byzantine consensus problem.\nWe introduce two algorithms: the first one achieves traditional validity\nrequirement for consensus, and the second one achieves a stronger \"q-validity\"\nrequirement. Both algorithms are more efficient than the ones introduces in our\nrecent PODC 2011 paper titled \"Error-Free Multi-Valued Consensus with Byzantine\nFailures\"."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.2065v1", 
    "title": "Oblivious Collaboration", 
    "arxiv-id": "1106.2065v1", 
    "author": "Benny Sudakov", 
    "publish": "2011-06-10T14:11:33Z", 
    "summary": "Communication is a crucial ingredient in every kind of collaborative work.\nBut what is the least possible amount of communication required for a given\ntask? We formalize this question by introducing a new framework for distributed\ncomputation, called {\\em oblivious protocols}.\n  We investigate the power of this model by considering two concrete examples,\nthe {\\em musical chairs} task $MC(n,m)$ and the well-known {\\em Renaming}\nproblem. The $MC(n,m)$ game is played by $n$ players (processors) with $m$\nchairs. Players can {\\em occupy} chairs, and the game terminates as soon as\neach player occupies a unique chair. Thus we say that player $P$ is {\\em in\nconflict} if some other player $Q$ is occupying the same chair, i.e.,\ntermination means there are no conflicts. By known results from distributed\ncomputing, if $m \\le 2n-2$, no strategy of the players can guarantee\ntermination. However, there is a protocol with $m = 2n-1$ chairs that always\nterminates. Here we consider an oblivious protocol where in every time step the\nonly communication is this: an adversarial {\\em scheduler} chooses an arbitrary\nnonempty set of players, and for each of them provides only one bit of\ninformation, specifying whether the player is currently in conflict or not. A\nplayer notified not to be in conflict halts and never changes its chair,\nwhereas a player notified to be in conflict changes its chair according to its\ndeterministic program. Remarkably, even with this minimal communication\ntermination can be guaranteed with only $m=2n-1$ chairs. Likewise, we obtain an\noblivious protocol for the Renaming problem whose name-space is small as that\nof the optimal nonoblivious distributed protocol.\n  Other aspects suggest themselves, such as the efficiency (program length) of\nour protocols. We make substantial progress here as well, though many\ninteresting questions remain open."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.2126v1", 
    "title": "MIS on the fly", 
    "arxiv-id": "1106.2126v1", 
    "author": "Ziv Bar-Joseph", 
    "publish": "2011-06-10T17:28:35Z", 
    "summary": "Humans are very good at optimizing solutions for specific problems.\nBiological processes, on the other hand, have evolved to handle multiple\nconstrained distributed environments and so they are robust and adaptable.\nInspired by observations made in a biological system we have recently presented\na simple new randomized distributed MIS algorithm \\cite{ZScience}. Here we\nextend these results by removing a number of strong assumptions that we made,\nmaking the algorithms more practical. Specifically we present an $O(\\log^2 n)$\nrounds synchronous randomized MIS algorithm which uses only 1 bit unary\nmessages (a beeping signal with collision detection), allows for asynchronous\nwake up, does not assume any knowledge of the network topology, and assumes\nonly a loose bound on the network size. We also present an extension with no\ncollision detection in which the round complexity increases to $(\\log^3 n)$.\nFinally, we show that our algorithm is optimal under some restriction, by\npresenting a tight lower bound of $\\Omega(\\log^2 n)$ on the number of rounds\nrequired to construct a MIS for a restricted model."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.3579v2", 
    "title": "Consensus vs Broadcast in Communication Networks with Arbitrary Mobile   Omission Faults", 
    "arxiv-id": "1106.3579v2", 
    "author": "Joseph Peters", 
    "publish": "2011-06-17T21:16:51Z", 
    "summary": "We compare the solvability of the Consensus and Broadcast problems in\nsynchronous communication networks in which the delivery of messages is not\nreliable. The failure model is the mobile omission faults model. During each\nround, some messages can be lost and the set of possible simultaneous losses is\nthe same for each round. We investigate these problems for the first time for\narbitrary sets of possible failures. Previously, these sets were defined by\nbounding the numbers of failures.\n  In this setting, we present a new necessary condition for the solvability of\nConsensus that unifies previous impossibility results in this area. This\ncondition is expressed using Broadcastability properties. As a very important\napplication, we show that when the sets of omissions that can occur are defined\nby bounding the numbers of failures, counted in any way (locally, globally,\netc.), then the Consensus problem is actually equivalent to the Broadcast\nproblem."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.4213v1", 
    "title": "A New and Efficient Algorithm-Based Fault Tolerance Scheme for A Million   Way Parallelism", 
    "arxiv-id": "1106.4213v1", 
    "author": "Guangming Tan", 
    "publish": "2011-06-21T14:24:43Z", 
    "summary": "Fault tolerance overhead of high performance computing (HPC) applications is\nbecoming critical to the efficient utilization of HPC systems at large scale.\nHPC applications typically tolerate fail-stop failures by checkpointing.\nAnother promising method is in the algorithm level, called algorithmic\nrecovery. These two methods can achieve high efficiency when the system scale\nis not very large, but will both lose their effectiveness when systems approach\nthe scale of Exaflops, where the number of processors including in system is\nexpected to achieve one million. This paper develops a new and efficient\nalgorithm-based fault tolerance scheme for HPC applications. When failure\noccurs during the execution, we do not stop to wait for the recovery of\ncorrupted data, but replace them with the corresponding redundant data and\ncontinue the execution. A background accelerated recovery method is also\nproposed to rebuild redundancy to tolerate multiple times of failures during\nthe execution. To demonstrate the feasibility of our new scheme, we have\nincorporated it to the High Performance Linpack. Theoretical analysis\ndemonstrates that our new fault tolerance scheme can still be effective even\nwhen the system scale achieves the Exaflops. Experiment using SiCortex SC5832\nverifies the feasibility of the scheme, and indicates that the advantage of our\nscheme can be observable even in a small scale."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.4985v1", 
    "title": "Dynamic Fractional Resource Scheduling vs. Batch Scheduling", 
    "arxiv-id": "1106.4985v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2011-06-24T14:54:51Z", 
    "summary": "We propose a novel job scheduling approach for homogeneous cluster computing\nplatforms. Its key feature is the use of virtual machine technology to share\nfractional node resources in a precise and controlled manner. Other VM-based\nscheduling approaches have focused primarily on technical issues or on\nextensions to existing batch scheduling systems, while we take a more\naggressive approach and seek to find heuristics that maximize an objective\nmetric correlated with job performance. We derive absolute performance bounds\nand develop algorithms for the online, non-clairvoyant version of our\nscheduling problem. We further evaluate these algorithms in simulation against\nboth synthetic and real-world HPC workloads and compare our algorithms to\nstandard batch scheduling approaches. We find that our approach improves over\nbatch scheduling by orders of magnitude in terms of job stretch, while leading\nto comparable or better resource utilization. Our results demonstrate that\nvirtualization technology coupled with lightweight online scheduling strategies\ncan afford dramatic improvements in performance for executing HPC workloads."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5158v1", 
    "title": "MONARC Simulation Framework", 
    "arxiv-id": "1106.5158v1", 
    "author": "Corina Stratan", 
    "publish": "2011-06-25T19:40:21Z", 
    "summary": "This paper discusses the latest generation of the MONARC (MOdels of Networked\nAnalysis at Regional Centers) simulation framework, as a design and modelling\ntool for large scale distributed systems applied to HEP experiments. A\nprocess-oriented approach for discrete event simulation is well-suited for\ndescribing concurrent running programs, as well as the stochastic arrival\npatterns that characterize how such systems are used. The simulation engine is\nbased on Threaded Objects (or Active Objects), which offer great flexibility in\nsimulating the complex behavior of distributed data processing programs. The\nengine provides an appropriate scheduling mechanism for the Active objects with\nsupport for interrupts. This approach offers a natural way of describing\ncomplex running programs that are data dependent and which concurrently compete\nfor shared resources as well as large numbers of concurrent data transfers on\nshared resources. The framework provides a complete set of basic components\n(processing nodes, data servers, network components) together with dynamically\nloadable decision units (scheduling or data replication modules) for easily\nbuilding complex Computing Model simulations. Examples of simulating complex\ndata processing systems are presented, and the way the framework is used to\ncompare different decision making algorithms or to optimize the overall Grid\narchitecture and/or the policies that govern the Grid's use."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5168v1", 
    "title": "LISA (Localhost Information Service Agent)", 
    "arxiv-id": "1106.5168v1", 
    "author": "Lucian Musat", 
    "publish": "2011-06-25T20:49:17Z", 
    "summary": "Grid computing has gained an increasing importance in the last years,\nespecially in the academic environments, offering the possibility to rapidly\nsolve complex scientific problems. The monitoring of the Grid jobs has a vital\nimportance for analyzing the system's performance, for providing the users an\nappropriate feed-back, and for obtaining historical data which may be used for\nperformance prediction. Several monitoring systems have been developed, with\ndifferent strategies to collect and store the information. We shall present\nhere a solution based on MonALISA, a distributed service for monitoring,\ncontrol and global optimization of complex systems, and LISA, a component\napplication of MonALISA which can help in optimizing other applications by\nmeans of monitoring services. The advantages of this system are, among others,\nflexibility, dynamic configuration, high communication performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5170v2", 
    "title": "The Contest Between Simplicity and Efficiency in Asynchronous Byzantine   Agreement", 
    "arxiv-id": "1106.5170v2", 
    "author": "Allison Lewko", 
    "publish": "2011-06-25T20:53:20Z", 
    "summary": "In the wake of the decisive impossibility result of Fischer, Lynch, and\nPaterson for deterministic consensus protocols in the aynchronous model with\njust one failure, Ben-Or and Bracha demonstrated that the problem could be\nsolved with randomness, even for Byzantine failures. Both protocols are natural\nand intuitive to verify, and Bracha's achieves optimal resilience. However, the\nexpected running time of these protocols is exponential in general. Recently,\nKapron, Kempe, King, Saia, and Sanwalani presented the first efficient\nByzantine agreement algorithm in the asynchronous, full information model,\nrunning in polylogarithmic time. Their algorithm is Monte Carlo and drastically\ndeparts from the simple structure of Ben-Or and Bracha's Las Vegas algorithms.\n  In this paper, we begin an investigation of the question: to what extent is\nthis departure necessary? Might there be a much simpler and intuitive Las Vegas\nprotocol that runs in expected polynomial time? We will show that the\nexponential running time of Ben-Or and Bracha's algorithms is no mere accident\nof their specific details, but rather an unavoidable consequence of their\ngeneral symmetry and round structure. We define a natural class of \"fully\nsymmetric round protocols\" for solving Byzantine agreement in an asynchronous\nsetting and show that any such protocol can be forced to run in expected\nexponential time by an adversary in the full information model. We assume the\nadversary controls $t$ Byzantine processors for $t = cn$, where $c$ is an\narbitrary positive constant $< 1/3$. We view our result as a step toward\nidentifying the level of complexity required for a polynomial-time algorithm in\nthis setting, and also as a guide in the search for new efficient algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5171v1", 
    "title": "A Distributed Agent Based System to Control and Coordinate Large Scale   Data Transfers", 
    "arxiv-id": "1106.5171v1", 
    "author": "Iosif C. Legrand", 
    "publish": "2011-06-25T20:54:12Z", 
    "summary": "We present a distributed agent based system used to monitor, configure and\ncontrol complex, large scale data transfers in the Wide Area Network. The\nLocalhost Information Service Agent (LISA) is a lightweight dynamic service\nthat provides complete system and applications monitoring, is capable to\ndynamically configure system parameters and can help in optimizing distributed\napplications.\n  As part of the MonALISA (Monitoring Agents in A Large Integrated Services\nArchitecture) system, LISA is an end host agent capable to collect any type of\nmonitoring information, to distribute them, and to take actions based on local\nor global decision units. The system has been used for the Bandwidth Challenge\nat Supercomputing 2006 to coordinate global large scale data transfers using\nFast Data Transfer (FDT) application between hundreds of servers distributed on\nmajor Grid sites involved in processing High Energy Physics data for the future\nLarge Hadron Collider experiments."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5299v1", 
    "title": "DistHash: A robust P2P DHT-based system for replicated objects", 
    "arxiv-id": "1106.5299v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T05:49:21Z", 
    "summary": "Over the Internet today, computing and communications environments are\nsignificantly more complex and chaotic than classical distributed systems,\nlacking any centralized organization or hierarchical control. There has been\nmuch interest in emerging Peer-to-Peer (P2P) network overlays because they\nprovide a good substrate for creating large-scale data sharing, content\ndistribution and application-level multicast applications. In this paper we\npresent DistHash, a P2P overlay network designed to share large sets of\nreplicated distributed objects in the context of large-scale highly dynamic\ninfrastructures. We present original solutions to achieve optimal message\nrouting in hop-count and throughput, provide an adequate consistency approach\namong replicas, as well as provide a fault-tolerant substrate."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5302v1", 
    "title": "Towards an IO intensive Grid application instrumentation in MedioGRID", 
    "arxiv-id": "1106.5302v1", 
    "author": "Vladimir Cretu", 
    "publish": "2011-06-27T06:03:28Z", 
    "summary": "Obtaining high performance in IO intensive applications requires systems that\nsupport reliable fast transfer, data replication, and caching. In this paper we\npresent an architecture designed for supporting IO intensive applications in\nMedioGRID, a system for real-time processing of satellite images, operating in\na Grid environment. The solution ensures that applications which are processing\ngeographical data have uniform access to data and is based on continuous\nmonitoring of the data transfers using MonALISA and its extensions. The\nMedioGRID architecture is also built on Globus, Condor and PBS and based on\nthis middleware we aim to extract information about the running systems. The\nresults obtained in testing MedioGRID system for large data transfers show that\nmonitoring system provides a very good view of system evolution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5303v1", 
    "title": "Intelligent strategies for DAG scheduling optimization in Grid   environments", 
    "arxiv-id": "1106.5303v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:03:42Z", 
    "summary": "The paper presents a solution to the dynamic DAG scheduling problem in Grid\nenvironments. It presents a distributed, scalable, efficient and fault-tolerant\nalgorithm for optimizing tasks assignment. The scheduler algorithm for tasks\nwith dependencies uses a heuristic model to optimize the total cost of tasks\nexecution. Also, a method based on genetic algorithms is proposed to optimize\nthe procedure of resources assignment. The experiments used the MonALISA\nmonitoring environment and its extensions. The results demonstrate very good\nbehavior in comparison with other scheduling approaches for this kind of DAG\nscheduling algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5309v1", 
    "title": "Resource CoAllocation for Scheduling Tasks with Dependencies, in Grid", 
    "arxiv-id": "1106.5309v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:24:40Z", 
    "summary": "Scheduling applications on wide-area distributed systems is useful for\nobtaining quick and reliable results in an efficient manner. Optimized\nscheduling algorithms are fundamentally important in order to achieve optimized\nresources utilization. The existing and potential applications include many\nfields of activity like satellite image processing and medicine. The paper\nproposes a scheduling algorithm for tasks with dependencies in Grid\nenvironments. CoAllocation represents a strategy that provides a schedule for\ntask with dependencies, having as main purpose the efficiency of the schedule,\nin terms of load balancing and minimum time for the execution of the tasks."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5310v1", 
    "title": "Advance Reservation of Resources for Task Execution in Grid Environments", 
    "arxiv-id": "1106.5310v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-27T06:24:50Z", 
    "summary": "The paper proposes a solution for the Grid scheduling problem, addressing in\nparticular the requirement of high performance an efficient algorithm must\nfulfill. Advance Reservation engages a distributed, dynamic, fault-tolerant and\nefficient strategy which reserves resources for future task execution. The\npaper presents the main features of the strategy, the functioning mechanism the\nstrategy is based on and the methods used for evaluating the algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5451v1", 
    "title": "Hybrid complex network topologies are preferred for   component-subscription in large-scale data-centres", 
    "arxiv-id": "1106.5451v1", 
    "author": "Dave Cliff", 
    "publish": "2011-06-27T17:16:41Z", 
    "summary": "We report on experiments exploring the interplay between the topology of the\ncomplex network of dependent components in a large-scale data-centre, and the\nrobustness and scaling properties of that data-centre. In a previous paper [1]\nwe used the SPECI large-scale data-centre simulator [2] to compare the\nrobustness and scaling characteristics of data-centres whose dependent\ncomponents are connected via Strogatz-Watts small-world (SW) networks [3],\nversus those organized as Barabasi-Albert scale-free (SF) networks [4], and\nfound significant differences. In this paper, we present results from using the\nKlemm-Eguiliz (KE) construction method [5] to generate complex network\ntopologies for data-centre component dependencies. The KE model has a control\nparameter {\\mu}\\in[0,1]\\inR that determines whether the networks generated are\nSW (0<{\\mu}<<1) or SF ({\\mu}=1) or a \"hybrid\" network topology part-way between\nSW and SF (0<{\\mu}<1). We find that the best scores for system-level\nperformance metrics of the simulated data-centres are given by \"hybrid\" values\nof {\\mu} significantly different from pure-SW or pure-SF."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5457v1", 
    "title": "Modelling Resilience in Cloud-Scale Data Centres", 
    "arxiv-id": "1106.5457v1", 
    "author": "Ilango Sriram", 
    "publish": "2011-06-27T17:31:49Z", 
    "summary": "The trend for cloud computing has initiated a race towards data centres (DC)\nof an ever-increasing size. The largest DCs now contain many hundreds of\nthousands of virtual machine (VM) services. Given the finite lifespan of\nhardware, such large DCs are subject to frequent hardware failure events that\ncan lead to disruption of service. To counter this, multiple redundant copies\nof task threads may be distributed around a DC to ensure that individual\nhardware failures do not cause entire jobs to fail. Here, we present results\ndemonstrating the resilience of different job scheduling algorithms in a\nsimulated DC with hardware failure. We use a simple model of jobs distributed\nacross a hardware network to demonstrate the relationship between resilience\nand additional communication costs of different scheduling methods."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5465v1", 
    "title": "SPECI-2: An open-source framework for predictive simulation of   cloud-scale data-centres", 
    "arxiv-id": "1106.5465v1", 
    "author": "Dave Cliff", 
    "publish": "2011-06-27T18:01:03Z", 
    "summary": "We introduce Version 2 of SPECI, a system for predictive simulation modeling\nof large-scale data-centres, i.e. warehouse-sized facilities containing\nhundreds of thousands of servers, as used to provide cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5570v1", 
    "title": "A distributed service for on demand end to end optical circuits", 
    "arxiv-id": "1106.5570v1", 
    "author": "Ciprian Dobre", 
    "publish": "2011-06-28T06:01:49Z", 
    "summary": "In this paper we present a system for monitoring and controlling dynamic\nnetwork circuits inside the USLHCNet network. This distributed service system\nprovides in near real-time complete topological information for all the\ncircuits, resource allocation and usage, accounting, detects automatically\nfailures in the links and network equipment, generate alarms and has the\nfunctionality to take automatic actions. The system is developed based on the\nMonALISA framework, which provides a robust monitoring and controlling service\noriented architecture, with no single points of failure."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5576v1", 
    "title": "Models and Techniques for Ensuring Reliability, Safety, Availability and   Security of Large Scale Distributed Systems", 
    "arxiv-id": "1106.5576v1", 
    "author": "Catalin Leordeanu", 
    "publish": "2011-06-28T06:53:59Z", 
    "summary": "17th International Conference on Control Systems and Computer Science (CSCS\n17), Bucharest, Romania, May 26-29, 2009. Vol. 1, pp. 401-406, ISSN: 2066-4451."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2306", 
    "link": "http://arxiv.org/pdf/1106.5846v1", 
    "title": "An Architectural Model for a Grid based Workflow Management Platform in   Scientific Applications", 
    "arxiv-id": "1106.5846v1", 
    "author": "Valentin Cristea", 
    "publish": "2011-06-29T06:03:11Z", 
    "summary": "With recent increasing computational and data requirements of scientific\napplications, the use of large clustered systems as well as distributed\nresources is inevitable. Although executing large applications in these\nenvironments brings increased performance, the automation of the process\nbecomes more and more challenging. While the use of complex workflow management\nsystems has been a viable solution for this automation process in business\noriented environments, the open source engines available for scientific\napplications lack some functionalities or are too difficult to use for\nnon-specialists. In this work we propose an architectural model for a grid\nbased workflow management platform providing features like an intuitive way to\ndescribe workflows, efficient data handling mechanisms and flexible fault\ntolerance support. Our integrated solution introduces a workflow engine\ncomponent based on ActiveBPEL extended with additional functionalities and a\nscheduling component providing efficient mapping between tasks and available\nresources."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.5908v1", 
    "title": "Hybrid-parallel sparse matrix-vector multiplication with explicit   communication overlap on current multicore-based systems", 
    "arxiv-id": "1106.5908v1", 
    "author": "Gerhard Wellein", 
    "publish": "2011-06-29T11:25:50Z", 
    "summary": "We evaluate optimized parallel sparse matrix-vector operations for several\nrepresentative application areas on widespread multicore-based cluster\nconfigurations. First the single-socket baseline performance is analyzed and\nmodeled with respect to basic architectural properties of standard multicore\nchips. Beyond the single node, the performance of parallel sparse matrix-vector\noperations is often limited by communication overhead. Starting from the\nobservation that nonblocking MPI is not able to hide communication cost using\nstandard MPI implementations, we demonstrate that explicit overlap of\ncommunication and computation can be achieved by using a dedicated\ncommunication thread, which may run on a virtual core. Moreover we identify\nperformance benefits of hybrid MPI/OpenMP programming due to improved load\nbalancing even without explicit communication overlap. We compare performance\nresults for pure MPI, the widely used \"vector-like\" hybrid programming\nstrategies, and explicit overlap on a modern multicore-based cluster and a Cray\nXE6 system."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.6122v1", 
    "title": "Simulation Framework for Modeling Large-Scale Distributed Systems", 
    "arxiv-id": "1106.6122v1", 
    "author": "Iosif C. Legrand", 
    "publish": "2011-06-30T06:37:34Z", 
    "summary": "Simulation has become the evaluation method of choice for many areas of\ndistributing computing research. However, most existing simulation packages\nhave several limitations on the size and complexity of the system being\nmodeled. Fine grained simulation of complex systems such as Grids requires high\ncomputational effort which can only be obtained by using an underlying\ndistributed architecture. We are proposing a new distributed simulation system\nthat has the advantage of being able to model very complex distributed systems\nwhile hiding the computational effort from the end-user."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1106.6304v1", 
    "title": "A Dynamic Elimination-Combining Stack Algorithm", 
    "arxiv-id": "1106.6304v1", 
    "author": "Adi Suissa", 
    "publish": "2011-06-30T17:12:14Z", 
    "summary": "Two key synchronization paradigms for the construction of scalable concurrent\ndata-structures are software combining and elimination. Elimination-based\nconcurrent data-structures allow operations with reverse semantics (such as\npush and pop stack operations) to \"collide\" and exchange values without having\nto access a central location. Software combining, on the other hand, is\neffective when colliding operations have identical semantics: when a pair of\nthreads performing operations with identical semantics collide, the task of\nperforming the combined set of operations is delegated to one of the threads\nand the other thread waits for its operation(s) to be performed. Applying this\nmechanism iteratively can reduce memory contention and increase throughput. The\nmost highly scalable prior concurrent stack algorithm is the\nelimination-backoff stack. The elimination-backoff stack provides high\nparallelism for symmetric workloads in which the numbers of push and pop\noperations are roughly equal, but its performance deteriorates when workloads\nare asymmetric. We present DECS, a novel Dynamic Elimination-Combining Stack\nalgorithm, that scales well for all workload types. While maintaining the\nsimplicity and low-overhead of the elimination-bakcoff stack, DECS manages to\nbenefit from collisions of both identical- and reverse-semantics operations.\nOur empirical evaluation shows that DECS scales significantly better than both\nblocking and non-blocking best prior stack algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626411000254", 
    "link": "http://arxiv.org/pdf/1107.0538v1", 
    "title": "Automatic Multi-GPU Code Generation applied to Simulation of Electrical   Machines", 
    "arxiv-id": "1107.0538v1", 
    "author": "Yvonnick Le Menach", 
    "publish": "2011-07-04T06:13:51Z", 
    "summary": "The electrical and electronic engineering has used parallel programming to\nsolve its large scale complex problems for performance reasons. However, as\nparallel programming requires a non-trivial distribution of tasks and data,\ndevelopers find it hard to implement their applications effectively. Thus, in\norder to reduce design complexity, we propose an approach to generate code for\nhybrid architectures (e.g. CPU + GPU) using OpenCL, an open standard for\nparallel programming of heterogeneous systems. This approach is based on Model\nDriven Engineering (MDE) and the MARTE profile, standard proposed by Object\nManagement Group (OMG). The aim is to provide resources to non-specialists in\nparallel programming to implement their applications. Moreover, thanks to model\nreuse capacity, we can add/change functionalities or the target architecture.\nConsequently, this approach helps industries to achieve their time-to-market\nconstraints and confirms by experimental tests, performance improvements using\nmulti-GPU environments."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S1793830914500220", 
    "link": "http://arxiv.org/pdf/1107.1866v4", 
    "title": "Priority-based task reassignments in hierarchical 2D mesh-connected   systems using tableaux", 
    "arxiv-id": "1107.1866v4", 
    "author": "Dohan Kim", 
    "publish": "2011-07-10T15:48:34Z", 
    "summary": "Task reassignments in 2D mesh-connected systems (2D-MSs) have been researched\nfor several decades. We propose a hierarchical 2D mesh-connected system\n(2D-HMS) in order to exploit the regular nature of a 2D-MS. In our approach\npriority-based task assignments and reassignments in a 2D-HMS are represented\nby tableaux and their algorithms. We show how task relocations for a\npriority-based task reassignment in a 2D-HMS are reduced to a jeu de taquin\nslide."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.2990v2", 
    "title": "Solving the At-Most-Once Problem with Nearly Optimal Effectiveness", 
    "arxiv-id": "1107.2990v2", 
    "author": "Aggelos Kiayias", 
    "publish": "2011-07-15T04:24:38Z", 
    "summary": "We present and analyze a wait-free deterministic algorithm for solving the\nat-most-once problem: how m shared-memory fail-prone processes perform\nasynchronously n jobs at most once. Our algorithmic strategy provides for the\nfirst time nearly optimal effectiveness, which is a measure that expresses the\ntotal number of jobs completed in the worst case. The effectiveness of our\nalgorithm equals n-2m+2. This is up to an additive factor of m close to the\nknown effectiveness upper bound n-m+1 over all possible algorithms and improves\non the previously best known deterministic solutions that have effectiveness\nonly n-log m o(n). We also present an iterative version of our algorithm that\nfor any $m = O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$ is both\neffectiveness-optimal and work-optimal, for any constant $\\epsilon > 0$. We\nthen employ this algorithm to provide a new algorithmic solution for the\nWrite-All problem which is work optimal for any\n$m=O\\left(\\sqrt[3+\\epsilon]{n/\\log n}\\right)$."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.3129v1", 
    "title": "Homomorphic Self-repairing Codes for Agile Maintenance of Distributed   Storage Systems", 
    "arxiv-id": "1107.3129v1", 
    "author": "Anwitaman Datta", 
    "publish": "2011-07-15T18:46:33Z", 
    "summary": "Distributed data storage systems are essential to deal with the need to store\nmassive volumes of data. In order to make such a system fault-tolerant, some\nform of redundancy becomes crucial, incurring various overheads - most\nprominently in terms of storage space and maintenance bandwidth requirements.\nErasure codes, originally designed for communication over lossy channels,\nprovide a storage efficient alternative to replication based redundancy,\nhowever entailing high communication overhead for maintenance, when some of the\nencoded fragments need to be replenished in news ones after failure of some\nstorage devices. We propose as an alternative a new family of erasure codes\ncalled self-repairing codes (SRC) taking into account the peculiarities of\ndistributed storage systems, specifically the maintenance process. SRC has the\nfollowing salient features: (a) encoded fragments can be repaired directly from\nother subsets of encoded fragments by downloading less data than the size of\nthe complete object, ensuring that (b) a fragment is repaired from a fixed\nnumber of encoded fragments, the number depending only on how many encoded\nblocks are missing and independent of which specific blocks are missing. This\npaper lays the foundations by defining the novel self-repairing codes,\nelaborating why the defined characteristics are desirable for distributed\nstorage systems. Then homomorphic self-repairing codes (HSRC) are proposed as a\nconcrete instance, whose various aspects and properties are studied and\ncompared - quantitatively or qualitatively with respect to other codes\nincluding traditional erasure codes as well as other recent codes designed\nspecifically for storage applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.3734v1", 
    "title": "Decentralized List Scheduling", 
    "arxiv-id": "1107.3734v1", 
    "author": "Denis Trystram", 
    "publish": "2011-07-19T15:13:23Z", 
    "summary": "Classical list scheduling is a very popular and efficient technique for\nscheduling jobs in parallel and distributed platforms. It is inherently\ncentralized. However, with the increasing number of processors, the cost for\nmanaging a single centralized list becomes too prohibitive. A suitable approach\nto reduce the contention is to distribute the list among the computational\nunits: each processor has only a local view of the work to execute. Thus, the\nscheduler is no longer greedy and standard performance guarantees are lost.\n  The objective of this work is to study the extra cost that must be paid when\nthe list is distributed among the computational units. We first present a\ngeneral methodology for computing the expected makespan based on the analysis\nof an adequate potential function which represents the load unbalance between\nthe local lists. We obtain an equation on the evolution of the potential by\ncomputing its expected decrease in one step of the schedule. Our main theorem\nshows how to solve such equations to bound the makespan. Then, we apply this\nmethod to several scheduling problems, namely, for unit independent tasks, for\nweighted independent tasks and for tasks with precendence constraints. More\nprecisely, we prove that the time for scheduling a global workload W composed\nof independent unit tasks on m processors is equal to W/m plus an additional\nterm proportional to log_2 W. We provide a lower bound which shows that this is\noptimal up to a constant. This result is extended to the case of weighted\nindependent tasks. In the last setting, precedence task graphs, our analysis\nleads to an improvement on the bound of Arora et al. We finally provide some\nexperiments using a simulator. The distribution of the makespan is shown to fit\nexisting probability laws. The additive term is shown by simulation to be\naround 3 \\log_2 W confirming the tightness of our analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.tcs.2013.04.017", 
    "link": "http://arxiv.org/pdf/1107.6014v1", 
    "title": "Snap-Stabilizing Message Forwarding Algorithm on Tree Topologies", 
    "arxiv-id": "1107.6014v1", 
    "author": "Vincent Villain", 
    "publish": "2011-07-29T16:37:44Z", 
    "summary": "In this paper, we consider the message forwarding problem that consists in\nmanaging the network resources that are used to forward messages. Previous\nworks on this problem provide solutions that either use a significant number of\nbuffers (that is n buffers per processor, where n is the number of processors\nin the network) making the solution not scalable or, they reserve all the\nbuffers from the sender to the receiver to forward only one message %while\nusing D buffers (where D refers to the diameter of the network) . The only\nsolution that uses a constant number of buffers per link was introduced in [1].\nHowever the solution works only on a chain networks. In this paper, we propose\na snap-stabilizing algorithm for the message forwarding problem that uses the\nsame complexity on the number of buffers as [1] and works on tree topologies."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0231v1", 
    "title": "Predicting global usages of resources endowed with local policies", 
    "arxiv-id": "1108.0231v1", 
    "author": "Gian Luigi Ferrari", 
    "publish": "2011-08-01T03:58:16Z", 
    "summary": "The effective usages of computational resources are a primary concern of\nup-to-date distributed applications. In this paper, we present a methodology to\nreason about resource usages (acquisition, release, revision, ...), and\ntherefore the proposed approach enables to predict bad usages of resources.\nKeeping in mind the interplay between local and global information occurring in\nthe application-resource interactions, we model resources as entities with\nlocal policies and global properties governing the overall interactions.\nFormally, our model takes the shape of an extension of pi-calculus with\nprimitives to manage resources. We develop a Control Flow Analysis computing a\nstatic approximation of process behaviour and therefore of the resource usages."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0599v1", 
    "title": "Proposal for improvement in the transfer and execution of multiple   instances of a virtual image", 
    "arxiv-id": "1108.0599v1", 
    "author": "Daniel Lombrana Gonzalez", 
    "publish": "2011-08-02T15:45:12Z", 
    "summary": "Virtualization technology allows currently any application run any\napplication complex and expensive computational (the scientific applications\nare a good example) on heterogeneous distributed systems, which make regular\nuse of Grid and Cloud technologies, enabling significant savings in computing\ntime. This model is particularly interesting for the mass execution of\nscientific simulations and calculations, allowing parallel execution of\napplications using the same execution environment (unchanged) used by the\nscientist as usual. However, the use and distribution of large virtual images\ncan be a problem (up to tens of GBytes), which is aggravated when attempting a\nmass mailing on a large number of distributed computers. This work has as main\nobjective to present an analysis of how implementation and a proposal for the\nimprovement (reduction in size) of the virtual images pretending reduce\ndistribution time in distributed systems. This analysis is done very specific\nrequirements that need an operating system (guest OS) on some aspects of its\nexecution."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.0972v1", 
    "title": "Network Localization on Unit Disk Graphs", 
    "arxiv-id": "1108.0972v1", 
    "author": "Nattakan Puttarak", 
    "publish": "2011-08-04T01:22:53Z", 
    "summary": "We study the problem of cooperative localization of a large network of nodes\nin integer-coordinated unit disk graphs, a simplified but useful version of\ngeneral random graph. Exploiting the property that the radius $r$ sets clear\ncut on the connectivity of two nodes, we propose an essential philosophy that\n\"no connectivity is also useful information just like the information being\nconnected\" in unit disk graphs. Exercising this philosophy, we show that the\nconventional network localization problem can be re-formulated to significantly\nreduce the search space, and that global rigidity, a necessary and sufficient\ncondition for the existence of unique solution in general graphs, is no longer\nnecessary. While the problem is still NP-hard, we show that a (depth-first)\ntree-search algorithm with memory O(N) ($N$ is the network size) can be\ndeveloped, and for practical setups, the search complexity and speed is very\nmanageable, and is magnitudes less than the conventional problem, especially\nwhen the graph is sparse or when only very limited anchor nodes are available."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.58.4", 
    "link": "http://arxiv.org/pdf/1108.1321v1", 
    "title": "Accurate location estimation of moving object with energy constraint &   adaptive update algorithms to save data", 
    "arxiv-id": "1108.1321v1", 
    "author": "Meenakshi Sati", 
    "publish": "2011-08-05T12:56:43Z", 
    "summary": "In research paper \"Accurate estimation of the target location of object with\nenergy constraint & Adaptive Update Algorithms to Save Data\" one of the central\nissues in sensor networks is track the location, of moving object which have\noverhead of saving data, an accurate estimation of the target location of\nobject with energy constraint .We do not have any mechanism which control and\nmaintain data .The wireless communication bandwidth is also very limited. Some\nfield which is using this technique are flood and typhoon detection, forest\nfire detection, temperature and humidity and ones we have these information use\nthese information back to a central air conditioning and ventilation system. In\nthis research paper, we propose protocol based on the prediction and adaptive\nbased algorithm which is using less sensor node reduced by an accurate\nestimation of the target location. we are using minimum three sensor node to\nget the accurate position .We can extend it upto four or five to find more\naccurate location but we have energy constraint so we are using three with\naccurate estimation of location help us to reduce sensor node..We show that our\ntracking method performs well in terms of energy saving regardless of mobility\npattern of the mobile target .We extends the life time of network with less\nsensor node. Once a new object is detected, a mobile agent will be initiated to\ntrack the roaming path of the object. The agent is mobile since it will choose\nthe sensor closest to the object to stay. The agent may invite some nearby\nslave sensors to cooperatively position the object and inhibit other irrelevant\n(i.e., farther) sensors from tracking the object. As a result, the\ncommunication and sensing overheads are greatly reduced."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1462v1", 
    "title": "On a New Multicomputer Interconnection Topology for Massively Parallel   Systems", 
    "arxiv-id": "1108.1462v1", 
    "author": "Nibedita Adhikari", 
    "publish": "2011-08-06T08:03:43Z", 
    "summary": "This paper introduces a new interconnection network topology called Balanced\nVarietal Hypercube (BVH), suitable for massively parallel systems. The proposed\ntopology being a hybrid structure retains almost all the attractive properties\nof Balanced Hypercube and Varietal Hypercube. The topology, various parameters,\nrouting and broadcasting of Balanced Varietal Hypercube are presented. The\nperformance of the Balanced Varietal Hypercube is compared with other networks.\nIn terms of diameter, cost and average distance and reliability the proposed\nnetwork is found to be better than the Hypercube, Balanced Hypercube and\nVarietal Hypercube. Also it is more reliable and cost-effective than Hypercube\nand Balanced Hypercube."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1631v1", 
    "title": "Load Balancing for MapReduce-based Entity Resolution", 
    "arxiv-id": "1108.1631v1", 
    "author": "Erhard Rahm", 
    "publish": "2011-08-08T08:48:57Z", 
    "summary": "The effectiveness and scalability of MapReduce-based implementations of\ncomplex data-intensive tasks depend on an even redistribution of data between\nmap and reduce tasks. In the presence of skewed data, sophisticated\nredistribution approaches thus become necessary to achieve load balancing among\nall reduce tasks to be executed in parallel. For the complex problem of entity\nresolution, we propose and evaluate two approaches for such skew handling and\nload balancing. The approaches support blocking techniques to reduce the search\nspace of entity resolution, utilize a preprocessing MapReduce job to analyze\nthe data distribution, and distribute the entities of large blocks among\nmultiple reduce tasks. The evaluation on a real cloud infrastructure shows the\nvalue and effectiveness of the proposed load balancing approaches."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.1928v1", 
    "title": "HybridNN: Supporting Network Location Service on Generalized Delay   Metrics", 
    "arxiv-id": "1108.1928v1", 
    "author": "Ernst Biersack", 
    "publish": "2011-08-09T13:57:18Z", 
    "summary": "Distributed Nearest Neighbor Search (DNNS) locates service nodes that have\nshortest interactive delay towards requesting hosts. DNNS provides an important\nservice for large-scale latency sensitive networked applications, such as VoIP,\nonline network games, or interactive network services on the cloud. Existing\nwork assumes the delay to be symmetric, which does not generalize to\napplications that are sensitive to one-way delays, such as the multimedia video\ndelivery from the servers to the hosts. We propose a relaxed inframetric model\nfor the network delay space that does not assume the triangle inequality and\ndelay symmetry to hold. We prove that the DNNS requests can be completed\nefficiently if the delay space exhibits modest inframetric dimensions, which we\ncan observe empirically. Finally, we propose a DNNS method named HybridNN\n(\\textit{Hybrid} \\textit{N}earest \\textit{N}eighbor search) based on the\ninframetric model for fast and accurate DNNS. For DNNS requests, HybridNN\nchooses closest neighbors accurately via the inframetric modelling, and\nscalably by combining delay predictions with direct probes to a pruned set of\nneighbors. Simulation results show that HybridNN locates nearly optimally the\nnearest neighbor. Experiments on PlanetLab show that HybridNN can provide\naccurate nearest neighbors that are close to optimal with modest query overhead\nand maintenance traffic."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2011.2414", 
    "link": "http://arxiv.org/pdf/1108.3268v1", 
    "title": "On the Performance of MPI-OpenMP on a 12 nodes Multi-core Cluster", 
    "arxiv-id": "1108.3268v1", 
    "author": "Mohiuddin Ahmed", 
    "publish": "2011-08-16T15:26:44Z", 
    "summary": "With the increasing number of Quad-Core-based clusters and the introduction\nof compute nodes designed with large memory capacity shared by multiple cores,\nnew problems related to scalability arise. In this paper, we analyze the\noverall performance of a cluster built with nodes having a dual Quad-Core\nProcessor on each node. Some benchmark results are presented and some\nobservations are mentioned when handling such processors on a benchmark test. A\nQuad-Core-based cluster's complexity arises from the fact that both local\ncommunication and network communications between the running processes need to\nbe addressed. The potentials of an MPI-OpenMP approach are pinpointed because\nof its reduced communication overhead. At the end, we come to a conclusion that\nan MPI-OpenMP solution should be considered in such clusters since optimizing\nnetwork communications between nodes is as important as optimizing local\ncommunications between processors in a multi-core cluster."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1108.4471v1", 
    "title": "Synchrony vs. Causality in Asynchronous Petri Nets", 
    "arxiv-id": "1108.4471v1", 
    "author": "Ursula Goltz", 
    "publish": "2011-08-23T01:24:08Z", 
    "summary": "Given a synchronous system, we study the question whether the behaviour of\nthat system can be exhibited by a (non-trivially) distributed and hence\nasynchronous implementation. In this paper we show, by counterexample, that\nsynchronous systems cannot in general be implemented in an asynchronous fashion\nwithout either introducing an infinite implementation or changing the causal\nstructure of the system behaviour."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1131v1", 
    "title": "Adaptive Mesh Refinement for Astrophysics Applications with ParalleX", 
    "arxiv-id": "1110.1131v1", 
    "author": "Thomas Sterling", 
    "publish": "2011-10-06T01:27:28Z", 
    "summary": "Several applications in astrophysics require adequately resolving many\nphysical and temporal scales which vary over several orders of magnitude.\nAdaptive mesh refinement techniques address this problem effectively but often\nresult in constrained strong scaling performance. The ParalleX execution model\nis an experimental execution model that aims to expose new forms of program\nparallelism and eliminate any global barriers present in a scaling-impaired\napplication such as adaptive mesh refinement. We present two astrophysics\napplications using the ParalleX execution model: a tabulated equation of state\ncomponent for neutron star evolutions and a cosmology model evolution.\nPerformance and strong scaling results from both simulations are presented. The\ntabulated equation of state data are distributed with transparent access over\nthe nodes of the cluster. This allows seamless overlapping of computation with\nthe latencies introduced by the remote access to the table. Because of the\nexpected size increases to the equation of state table, this type of table\npartitioning for neutron star simulations is essential while the implementation\nis greatly simplified by ParalleX semantics."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1553v1", 
    "title": "Hierarchical QR factorization algorithms for multi-core cluster systems", 
    "arxiv-id": "1110.1553v1", 
    "author": "and Yves Robert", 
    "publish": "2011-10-07T14:51:08Z", 
    "summary": "This paper describes a new QR factorization algorithm which is especially\ndesigned for massively parallel platforms combining parallel distributed\nmulti-core nodes. These platforms make the present and the foreseeable future\nof high-performance computing. Our new QR factorization algorithm falls in the\ncategory of the tile algorithms which naturally enables good data locality for\nthe sequential kernels executed by the cores (high sequential performance), low\nnumber of messages in a parallel distributed setting (small latency term), and\nfine granularity (high parallelism)."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.1991v1", 
    "title": "Cluster-Based Load Balancing Algorithms for Grids", 
    "arxiv-id": "1110.1991v1", 
    "author": "Orhan Dagdeviren", 
    "publish": "2011-10-10T10:32:29Z", 
    "summary": "E-science applications may require huge amounts of data and high processing\npower where grid infrastructures are very suitable for meeting these\nrequirements. The load distribution in a grid may vary leading to the\nbottlenecks and overloaded sites. We describe a hierarchical dynamic load\nbalancing protocol for Grids. The Grid consists of clusters and each cluster is\nrepresented by a coordinator. Each coordinator first attempts to balance the\nload in its cluster and if this fails, communicates with the other coordinators\nto perform transfer or reception of load. This process is repeated\nperiodically. We analyze the correctness, performance and scalability of the\nproposed protocol and show from the simulation results that our algorithm\nbalances the load by decreasing the number of high loaded nodes in a grid\nenvironment."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.2561v1", 
    "title": "Rigorous Calculation of the Partition Function for the Finite Number of   Ising Spins", 
    "arxiv-id": "1110.2561v1", 
    "author": "Valery I. Belokon", 
    "publish": "2011-10-12T03:34:51Z", 
    "summary": "The high-performance scalable parallel algorithm for rigorous calculation of\npartition function of lattice systems with finite number Ising spins was\ndeveloped. The parallel calculations run by C++ code with using of Message\nPassing Interface and massive parallel instructions. The algorithm can be used\nfor the research of the interacting spin systems in the Ising models of 2D and\n3D. The processing power and scalability is analyzed for different parallel and\ndistributed systems. Different methods of the speed up measuring allow obtain\nthe super-linear speeding up for the small number of processes. Program code\ncould be useful also for research by exact method of different Ising spin\nsystems, e.g. system with competition interactions."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.2677v1", 
    "title": "Hybrid static/dynamic scheduling for already optimized dense matrix   factorization", 
    "arxiv-id": "1110.2677v1", 
    "author": "Vivek Kale", 
    "publish": "2011-10-12T15:09:45Z", 
    "summary": "We present the use of a hybrid static/dynamic scheduling strategy of the task\ndependency graph for direct methods used in dense numerical linear algebra.\nThis strategy provides a balance of data locality, load balance, and low\ndequeue overhead. We show that the usage of this scheduling in communication\navoiding dense factorization leads to significant performance gains. On a 48\ncore AMD Opteron NUMA machine, our experiments show that we can achieve up to\n64% improvement over a version of CALU that uses fully dynamic scheduling, and\nup to 30% improvement over the version of CALU that uses fully static\nscheduling. On a 16-core Intel Xeon machine, our hybrid static/dynamic\nscheduling approach is up to 8% faster than the version of CALU that uses a\nfully static scheduling or fully dynamic scheduling. Our algorithm leads to\nspeedups over the corresponding routines for computing LU factorization in well\nknown libraries. On the 48 core AMD NUMA machine, our best implementation is up\nto 110% faster than MKL, while on the 16 core Intel Xeon machine, it is up to\n82% faster than MKL. Our approach also shows significant speedups compared with\nPLASMA on both of these systems."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.4854v1", 
    "title": "On the Practicality of `Practical' Byzantine Fault Tolerance", 
    "arxiv-id": "1110.4854v1", 
    "author": "Mema Roussopoulos", 
    "publish": "2011-10-21T18:02:22Z", 
    "summary": "Byzantine Fault Tolerant (BFT) systems are considered by the systems research\ncommunity to be state of the art with regards to providing reliability in\ndistributed systems. BFT systems provide safety and liveness guarantees with\nreasonable assumptions, amongst a set of nodes where at most f nodes display\narbitrarily incorrect behaviors, known as Byzantine faults. Despite this, BFT\nsystems are still rarely used in practice. In this paper we describe our\nexperience, from an application developer's perspective, trying to leverage the\npublicly available and highly-tuned PBFT middleware (by Castro and Liskov), to\nprovide provable reliability guarantees for an electronic voting application\nwith high security and robustness needs. We describe several obstacles we\nencountered and drawbacks we identified in the PBFT approach. These include\nsome that we tackled, such as lack of support for dynamic client management and\nleaving state management completely up to the application. Others still\nremaining include the lack of robust handling of non-determinism, lack of\nsupport for web-based applications, lack of support for stronger cryptographic\nprimitives, and others. We find that, while many of the obstacles could be\novercome with a revised BFT middleware implementation that is tuned\nspecifically for the needs of the particular application, they require\nsignificant engineering effort and time and their performance implications for\nthe end-application are unclear. An application developer is thus unlikely to\nbe willing to invest the time and effort to do so to leverage the BFT approach.\nWe conclude that the research community needs to focus on the usability of BFT\nalgorithms for real world applications, from the end-developer perspective, in\naddition to continuing to improve the BFT middleware performance, robustness\nand deployment layouts."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.64.9", 
    "link": "http://arxiv.org/pdf/1110.5969v1", 
    "title": "Reliable Provisioning of Spot Instances for Compute-intensive   Applications", 
    "arxiv-id": "1110.5969v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-10-27T03:12:07Z", 
    "summary": "Cloud computing providers are now offering their unused resources for leasing\nin the spot market, which has been considered the first step towards a\nfull-fledged market economy for computational resources. Spot instances are\nvirtual machines (VMs) available at lower prices than their standard on-demand\ncounterparts. These VMs will run for as long as the current price is lower than\nthe maximum bid price users are willing to pay per hour. Spot instances have\nbeen increasingly used for executing compute-intensive applications. In spite\nof an apparent economical advantage, due to an intermittent nature of biddable\nresources, application execution times may be prolonged or they may not finish\nat all. This paper proposes a resource allocation strategy that addresses the\nproblem of running compute-intensive jobs on a pool of intermittent virtual\nmachines, while also aiming to run applications in a fast and economical way.\nTo mitigate potential unavailability periods, a multifaceted fault-aware\nresource provisioning policy is proposed. Our solution employs price and\nruntime estimation mechanisms, as well as three fault tolerance techniques,\nnamely checkpointing, task duplication and migration. We evaluate our\nstrategies using trace-driven simulations, which take as input real price\nvariation traces, as well as an application trace from the Parallel Workload\nArchive. Our results demonstrate the effectiveness of executing applications on\nspot instances, respecting QoS constraints, despite occasional failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.5972v1", 
    "title": "Provisioning Spot Market Cloud Resources to Create Cost-effective   Virtual Clusters", 
    "arxiv-id": "1110.5972v1", 
    "author": "Rajkumar Buyya", 
    "publish": "2011-10-27T03:22:25Z", 
    "summary": "Infrastructure-as-a-Service providers are offering their unused resources in\nthe form of variable-priced virtual machines (VMs), known as \"spot instances\",\nat prices significantly lower than their standard fixed-priced resources. To\nlease spot instances, users specify a maximum price they are willing to pay per\nhour and VMs will run only when the current price is lower than the user's bid.\nThis paper proposes a resource allocation policy that addresses the problem of\nrunning deadline-constrained compute-intensive jobs on a pool of composed\nsolely of spot instances, while exploiting variations in price and performance\nto run applications in a fast and economical way. Our policy relies on job\nruntime estimations to decide what are the best types of VMs to run each job\nand when jobs should run. Several estimation methods are evaluated and\ncompared, using trace-based simulations, which take real price variation traces\nobtained from Amazon Web Services as input, as well as an application trace\nfrom the Parallel Workload Archive. Results demonstrate the effectiveness of\nrunning computational jobs on spot instances, at a fraction (up to 60% lower)\nof the price that would normally cost on fixed priced resources."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.6231v1", 
    "title": "Parallel implematation of flow and matching algorithms", 
    "arxiv-id": "1110.6231v1", 
    "author": "Agnieszka \u0141upi\u0144ska", 
    "publish": "2011-10-28T01:44:45Z", 
    "summary": "In our work we present two parallel algorithms and their lock-free\nimplementations using a popular GPU environment Nvidia CUDA. The first\nalgorithm is the push-relabel method for the flow problem in grid graphs. The\nsecond is the cost scaling algorithm for the assignment problem in complete\nbipartite graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1110.6288v1", 
    "title": "Reliability of Computational Experiments on Virtualised Hardware", 
    "arxiv-id": "1110.6288v1", 
    "author": "Lars Kotthoff", 
    "publish": "2011-10-28T10:21:23Z", 
    "summary": "We present preliminary results of an investigation into the suitability of\nvirtualised hardware -- in particular clouds -- for running computational\nexperiments. Our main concern was that the reported CPU time would not be\nreliable and reproducible. The results demonstrate that while this is true in\ncases where many virtual machines are running on the same physical hardware,\nthere is no inherent variation introduced by using virtualised hardware\ncompared to non-virtualised hardware."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.0875v2", 
    "title": "Game Theoretic Iterative Partitioning for Dynamic Load Balancing in   Distributed Network Simulation", 
    "arxiv-id": "1111.0875v2", 
    "author": "George Kesidis", 
    "publish": "2011-11-03T15:21:25Z", 
    "summary": "High fidelity simulation of large-sized complex networks can be realized on a\ndistributed computing platform that leverages the combined resources of\nmultiple processors or machines. In a discrete event driven simulation, the\nassignment of logical processes (LPs) to machines is a critical step that\naffects the computational and communication burden on the machines, which in\nturn affects the simulation execution time of the experiment. We study a\nnetwork partitioning game wherein each node (LP) acts as a selfish player. We\nderive two local node-level cost frameworks which are feasible in the sense\nthat the aggregate state information required to be exchanged between the\nmachines is independent of the size of the simulated network model. For both\ncost frameworks, we prove the existence of stable Nash equilibria in pure\nstrategies. Using iterative partition improvements, we propose game theoretic\npartitioning algorithms based on the two cost criteria and show that each\ndescends in a global cost. To exploit the distributed nature of the system, the\nalgorithm is distributed, with each node's decision based on its local\ninformation and on a few global quantities which can be communicated\nmachine-to-machine. We demonstrate the performance of our partitioning\nalgorithm on an optimistic discrete event driven simulation platform that\nmodels an actual parallel simulator."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.1129v1", 
    "title": "Domain decomposition and locality optimization for large-scale lattice   Boltzmann simulations", 
    "arxiv-id": "1111.1129v1", 
    "author": "Gerhard Wellein", 
    "publish": "2011-11-04T13:52:36Z", 
    "summary": "We present a simple, parallel and distributed algorithm for setting up and\npartitioning a sparse representation of a regular discretized simulation\ndomain. This method is scalable for a large number of processes even for\ncomplex geometries and ensures load balance between the domains, reasonable\ncommunication interfaces, and good data locality within the domain. Applying\nthis scheme to a list-based lattice Boltzmann flow solver can achieve similar\nor even higher flow solver performance than widely used standard graph\npartition based tools such as METIS and PT-SCOTCH."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.1170v3", 
    "title": "Record-replay debugging for the SCOOP concurrency model", 
    "arxiv-id": "1111.1170v3", 
    "author": "Bertrand Meyer", 
    "publish": "2011-11-04T16:15:14Z", 
    "summary": "To support developers in writing reliable and efficient concurrent programs,\nnovel concurrent programming abstractions have been proposed in recent years.\nProgramming with such abstractions requires new analysis tools because the\nexecution semantics often differs considerably from established models. We\npresent a record-replay technique for programs written in SCOOP, an\nobject-oriented programming model for concurrency. The resulting tool enables\ndevelopers to reproduce the nondeterministic execution of a concurrent program,\na necessary prerequisite for debugging and testing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.2237v1", 
    "title": "Choosing the best resource by method of mamdani", 
    "arxiv-id": "1111.2237v1", 
    "author": "Oleg Titov", 
    "publish": "2011-11-06T20:35:30Z", 
    "summary": "A method for selecting the best service for the storage of information by\nMamdani."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.2412v1", 
    "title": "Secured Data Consistency and Storage Way in Untrusted Cloud using Server   Management Algorithm", 
    "arxiv-id": "1111.2412v1", 
    "author": "C. Dinesh", 
    "publish": "2011-11-10T08:05:38Z", 
    "summary": "It is very challenging part to keep safely all required data that are needed\nin many applications for user in cloud. Storing our data in cloud may not be\nfully trustworthy. Since client doesn't have copy of all stored data, he has to\ndepend on Cloud Service Provider. But dynamic data operations, Read-Solomon and\nverification token construction methods don't tell us about total storage\ncapacity of server allocated space before and after the data addition in cloud.\nSo we have to introduce a new proposed system of efficient storage measurement\nand space comparison algorithm with time management for measuring the total\nallocated storage area before and after the data insertion in cloud. So by\nusing our proposed scheme, the value or weight of stored data before and after\nis measured by client with specified time in cloud storage area with accuracy.\nAnd here we also have proposed the multi-server restore point in server failure\ncondition. If there occurs any server failure, by using this scheme the data\ncan be recovered automatically in cloud server. Our proposed scheme efficiently\nchecks space for the in-outsourced data to maintain integrity. Here the TPA\nnecessarily doesn't have the delegation to audit user's data."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.2418v1", 
    "title": "Data Integrity and Dynamic Storage Way in Cloud Computing", 
    "arxiv-id": "1111.2418v1", 
    "author": "C. Dinesh", 
    "publish": "2011-11-10T08:23:47Z", 
    "summary": "It is not an easy task to securely maintain all essential data where it has\nthe need in many applications for clients in cloud. To maintain our data in\ncloud, it may not be fully trustworthy because client doesn't have copy of all\nstored data. But any authors don't tell us data integrity through its user and\nCSP level by comparison before and after the data update in cloud. So we have\nto establish new proposed system for this using our data reading protocol\nalgorithm to check the integrity of data before and after the data insertion in\ncloud. Here the security of data before and after is checked by client with the\nhelp of CSP using our \"effective automatic data reading protocol from user as\nwell as cloud level into the cloud\" with truthfulness. Also we have proposed\nthe multi-server data comparison algorithm with the calculation of overall data\nin each update before its outsourced level for server restore access point for\nfuture data recovery from cloud data server. Our proposed scheme efficiently\nchecks integrity in efficient manner so that data integrity as well as security\ncan be maintained in all cases by considering drawbacks of existing methods."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.3022v1", 
    "title": "Design of a Sliding Window over Asynchronous Event Streams", 
    "arxiv-id": "1111.3022v1", 
    "author": "Jian Lu", 
    "publish": "2011-11-13T15:20:49Z", 
    "summary": "The proliferation of sensing and monitoring applications motivates adoption\nof the event stream model of computation. Though sliding windows are widely\nused to facilitate effective event stream processing, it is greatly challenged\nwhen the event sources are distributed and asynchronous. To address this\nchallenge, we first show that the snapshots of the asynchronous event streams\nwithin the sliding window form a convex distributive lattice (denoted by\nLat-Win). Then we propose an algorithm to maintain Lat-Win at runtime. The\nLat-Win maintenance algorithm is implemented and evaluated on the open-source\ncontext-aware middleware we developed. The evaluation results first show the\nnecessity of adopting sliding windows over asynchronous event streams. Then\nthey show the performance of detecting specified predicates within Lat-Win,\neven when faced with dynamic changes in the computing environment."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-24650-0_34", 
    "link": "http://arxiv.org/pdf/1111.3334v1", 
    "title": "Fixing Data Anomalies with Prediction Based Algorithm in Wireless Sensor   Networks", 
    "arxiv-id": "1111.3334v1", 
    "author": "Partha Sarathi Mandal", 
    "publish": "2011-11-14T19:43:04Z", 
    "summary": "Data inconsistencies are present in the data collected over a large wireless\nsensor network (WSN), usually deployed for any kind of monitoring applications.\nBefore passing this data to some WSN applications for decision making, it is\nnecessary to ensure that the data received are clean and accurate. In this\npaper, we have used a statistical tool to examine the past data to fit in a\nhighly sophisticated prediction model i.e., ARIMA for a given sensor node and\nwith this, the model corrects the data using forecast value if any data anomaly\nexists there. Another scheme is also proposed for detecting data anomaly at\nsink among the aggregated data in the data are received from a particular\nsensor node. The effectiveness of our methods are validated by data collected\nover a real WSN application consisting of Crossbow IRIS Motes\n\\cite{Crossbow:2009}."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.4499v1", 
    "title": "A Low-Energy Fast Cyber Foraging Mechanism for Mobile Devices", 
    "arxiv-id": "1111.4499v1", 
    "author": "Mohsen Sharifi", 
    "publish": "2011-11-18T21:37:36Z", 
    "summary": "The ever increasing demands for using resource-constrained mobile devices for\nrunning more resource intensive applications nowadays has initiated the\ndevelopment of cyber foraging solutions that offload parts or whole\ncomputational intensive tasks to more powerful surrogate stationary computers\nand run them on behalf of mobile devices as required. The choice of proper mix\nof mobile devices and surrogates has remained an unresolved challenge though.\nIn this paper, we propose a new decision-making mechanism for cyber foraging\nsystems to select the best locations to run an application, based on context\nmetrics such as the specifications of surrogates, the specifications of mobile\ndevices, application specification, and communication network specification.\nExperimental results show faster response time and lower energy consumption of\nbenched applications compared to when applications run wholly on mobile devices\nand when applications are offloaded to surrogates blindly for execution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5239v2", 
    "title": "Distributed Signal Processing via Chebyshev Polynomial Approximation", 
    "arxiv-id": "1111.5239v2", 
    "author": "Pascal Frossard", 
    "publish": "2011-11-22T16:15:32Z", 
    "summary": "Unions of graph multiplier operators are an important class of linear\noperators for processing signals defined on graphs. We present a novel method\nto efficiently distribute the application of these operators. The proposed\nmethod features approximations of the graph multipliers by shifted Chebyshev\npolynomials, whose recurrence relations make them readily amenable to\ndistributed computation. We demonstrate how the proposed method can be applied\nto distributed processing tasks such as smoothing, denoising, inverse\nfiltering, and semi-supervised classification, and show that the communication\nrequirements of the method scale gracefully with the size of the network."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5391v1", 
    "title": "Hamiltonian Connectivity of Twisted Hypercube-Like Networks under the   Large Fault Model", 
    "arxiv-id": "1111.5391v1", 
    "author": "Xiaofan Yang", 
    "publish": "2011-11-23T03:05:45Z", 
    "summary": "Twisted hypercube-like networks (THLNs) are an important class of\ninterconnection networks for parallel computing systems, which include most\npopular variants of the hypercubes, such as crossed cubes, M\\\"obius cubes,\ntwisted cubes and locally twisted cubes. This paper deals with the\nfault-tolerant hamiltonian connectivity of THLNs under the large fault model.\nLet $G$ be an $n$-dimensional THLN and $F \\subseteq V(G)\\bigcup E(G)$, where $n\n\\geq 7$ and $|F| \\leq 2n - 10$. We prove that for any two nodes $u,v \\in V(G -\nF)$ satisfying a simple necessary condition on neighbors of $u$ and $v$, there\nexists a hamiltonian or near-hamiltonian path between $u$ and $v$ in $G-F$. The\nresult extends further the fault-tolerant graph embedding capability of THLNs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.5775v2", 
    "title": "Partial mutual exclusion for infinitely many processes", 
    "arxiv-id": "1111.5775v2", 
    "author": "Wim H. Hesselink", 
    "publish": "2011-11-24T14:17:05Z", 
    "summary": "Partial mutual exclusion is the drinking philosophers problem for complete\ngraphs. It is the problem that a process may enter a critical section CS of its\ncode only when some finite set nbh of other processes are not in their critical\nsections. For each execution of CS, the set nbh can be given by the\nenvironment. We present a starvation free solution of this problem in a setting\nwith infinitely many processes, each with finite memory, that communicate by\nasynchronous messages. The solution has the property of first-come\nfirst-served, in so far as this can be guaranteed by asynchronous messages. For\nevery execution of CS and every process in nbh, between three and six messages\nare needed. The correctness of the solution is argued with invariants and\ntemporal logic. It has been verified with the proof assistant PVS."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1111.6324v1", 
    "title": "Improving the Load Balancing Performance of Vlasiator", 
    "arxiv-id": "1111.6324v1", 
    "author": "Ilja Honkonen", 
    "publish": "2011-11-28T00:50:25Z", 
    "summary": "This whitepaper describes the load-balancing performance issues that are\nobserved and tackled during the petascaling of the Vlasiator codes. Vlasiator\nis a Vlasov-hybrid simulation code developed in Finnish Meteorological\nInstitute (FMI). Vlasiator models the communications associated with the\nspatial grid operated on as a hypergraph and partitions the grid using the\nparallel hypergraph partitioning scheme (PHG) of the Zoltan partitioning\nframework. The result of partitioning determines the distribution of grid cells\nto processors. It is observed that the partitioning phase takes a substantial\npercentage of the overall computation time. Alternative\n(graph-partitioning-based) schemes that perform almost as well as the\nhypergraph partitioning scheme and that require less preprocessing overhead and\nbetter balance are proposed and investigated. A comparison in terms of effect\non running time, preprocessing overhead and load-balancing quality of Zoltan's\nPHG, ParMeTiS, and PT-SCOTCH are presented. Test results on J\\\"uelich\nBlueGene/P cluster are presented."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijwmn.2011.3516", 
    "link": "http://arxiv.org/pdf/1112.0384v1", 
    "title": "Information Spreading in Dynamic Networks", 
    "arxiv-id": "1112.0384v1", 
    "author": "Zhifeng Sun", 
    "publish": "2011-12-02T04:56:12Z", 
    "summary": "We study the fundamental problem of information spreading (also known as\ngossip) in dynamic networks. In gossip, or more generally, $k$-gossip, there\nare $k$ pieces of information (or tokens) that are initially present in some\nnodes and the problem is to disseminate the $k$ tokens to all nodes. The goal\nis to accomplish the task in as few rounds of distributed computation as\npossible. The problem is especially challenging in dynamic networks where the\nnetwork topology can change from round to round and can be controlled by an\non-line adversary.\n  The focus of this paper is on the power of token-forwarding algorithms, which\ndo not manipulate tokens in any way other than storing and forwarding them. We\nfirst consider a worst-case adversarial model first studied by Kuhn, Lynch, and\nOshman~\\cite{kuhn+lo:dynamic} in which the communication links for each round\nare chosen by an adversary, and nodes do not know who their neighbors for the\ncurrent round are before they broadcast their messages. Our main result is an\n$\\Omega(nk/\\log n)$ lower bound on the number of rounds needed for any\ndeterministic token-forwarding algorithm to solve $k$-gossip. This resolves an\nopen problem raised in~\\cite{kuhn+lo:dynamic}, improving their lower bound of\n$\\Omega(n \\log k)$, and matching their upper bound of $O(nk)$ to within a\nlogarithmic factor.\n  We next show that token-forwarding algorithms can achieve subquadratic time\nin the offline version of the problem where the adversary has to commit all the\ntopology changes in advance at the beginning of the computation, and present\ntwo polynomial-time offline token-forwarding algorithms. Our results are a step\ntowards understanding the power and limitation of token-forwarding algorithms\nin dynamic networks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.1336v5", 
    "title": "Consensus over Random Graph Processes: Network Borel-Cantelli Lemmas for   Almost Sure Convergence", 
    "arxiv-id": "1112.1336v5", 
    "author": "Karl Henrik Johansson", 
    "publish": "2011-12-06T16:33:04Z", 
    "summary": "Distributed consensus computation over random graph processes is considered.\nThe random graph process is defined as a sequence of random variables which\ntake values from the set of all possible digraphs over the node set. At each\ntime step, every node updates its state based on a Bernoulli trial, independent\nin time and among different nodes: either averaging among the neighbor set\ngenerated by the random graph, or sticking with its current state.\nConnectivity-independence and arc-independence are introduced to capture the\nfundamental influence of the random graphs on the consensus convergence.\nNecessary and/or sufficient conditions are presented on the success\nprobabilities of the Bernoulli trials for the network to reach a global almost\nsure consensus, with some sharp threshold established revealing a consensus\nzero-one law. Convergence rates are established by lower and upper bounds of\nthe $\\epsilon$-computation time. We also generalize the concepts of\nconnectivity/arc independence to their analogues from the $*$-mixing point of\nview, so that our results apply to a very wide class of graphical models,\nincluding the majority of random graph models in the literature, e.g.,\nErd\\H{o}s-R\\'{e}nyi, gossiping, and Markovian random graphs. We show that under\n$*$-mixing, our convergence analysis continues to hold and the corresponding\nalmost sure consensus conditions are established. Finally, we further\ninvestigate almost sure finite-time convergence of random gossiping algorithms,\nand prove that the Bernoulli trials play a key role in ensuring finite-time\nconvergence. These results add to the understanding of the interplay between\nrandom graphs, random computations, and convergence probability for distributed\ninformation processing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.1851v2", 
    "title": "(MC2)2: A Generic Decision-Making Framework and its Application to Cloud   Computing", 
    "arxiv-id": "1112.1851v2", 
    "author": "Stefan Tai", 
    "publish": "2011-12-08T14:46:08Z", 
    "summary": "Cloud computing is a disruptive technology, representing a new model for\ninformation technology (IT) solution engineering and management that promises\nto introduce significant cost savings and other benefits. The adoption of Cloud\ncomputing requires a detailed comparison of infrastructure alternatives, taking\na number of aspects into careful consideration. Existing methods of evaluation,\nhowever, limit decision making to the relative costs of cloud computing, but do\nnot take a broader range of criteria into account. In this paper, we introduce\na generic, multi-criteria-based decision framework and an application for Cloud\nComputing, the Multi-Criteria Comparison Method for Cloud Computing ((MC2)2).\nThe framework and method allow organizations to determine what infrastructure\nbest suits their needs by evaluating and ranking infrastructure alternatives\nusing multiple criteria. Therefore, (MC2)2 offers a way to differentiate\ninfrastructures not only by costs, but also in terms of benefits, opportunities\nand risks. (MC2)2 can be adapted to facilitate a wide array of decision-making\nscenarios within the domain of information technology infrastructures,\ndepending on the criteria selected to support the framework."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.2025v1", 
    "title": "PC-Cluster based Storage System Architecture for Cloud Storage", 
    "arxiv-id": "1112.2025v1", 
    "author": "Thinn Thu Naing", 
    "publish": "2011-12-09T06:58:13Z", 
    "summary": "Design and architecture of cloud storage system plays a vital role in cloud\ncomputing infrastructure in order to improve the storage capacity as well as\ncost effectiveness. Usually cloud storage system provides users to efficient\nstorage space with elasticity feature. One of the challenges of cloud storage\nsystem is difficult to balance the providing huge elastic capacity of storage\nand investment of expensive cost for it. In order to solve this issue in the\ncloud storage infrastructure, low cost PC cluster based storage server is\nconfigured to be activated for large amount of data to provide cloud users.\nMoreover, one of the contributions of this system is proposed an analytical\nmodel using M/M/1 queuing network model, which is modeled on intended\narchitecture to provide better response time, utilization of storage as well as\npending time when the system is running. According to the analytical result on\nexperimental testing, the storage can be utilized more than 90% of storage\nspace. In this paper, two parts have been described such as (i) design and\narchitecture of PC cluster based cloud storage system. On this system, related\nto cloud applications, services configurations are explained in detailed. (ii)\nAnalytical model has been enhanced to be increased the storage utilization on\nthe target architecture."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.2431v2", 
    "title": "Distributed Particle Filter Implementation with Intermittent/Irregular   Consensus Convergence", 
    "arxiv-id": "1112.2431v2", 
    "author": "Amir Asif", 
    "publish": "2011-12-12T03:23:47Z", 
    "summary": "Motivated by non-linear, non-Gaussian, distributed multi-sensor/agent\nnavigation and tracking applications, we propose a multi-rate consensus/fusion\nbased framework for distributed implementation of the particle filter (CF/DPF).\nThe CF/DPF framework is based on running localized particle filters to estimate\nthe overall state vector at each observation node. Separate fusion filters are\ndesigned to consistently assimilate the local filtering distributions into the\nglobal posterior by compensating for the common past information between\nneighbouring nodes. The CF/DPF offers two distinct advantages over its\ncounterparts. First, the CF/DPF framework is suitable for scenarios where\nnetwork connectivity is intermittent and consensus can not be reached between\ntwo consecutive observations. Second, the CF/DPF is not limited to the Gaussian\napproximation for the global posterior density. A third contribution of the\npaper is the derivation of the exact expression for computing the posterior\nCramer-Rao lower bound (PCRLB) for the distributed architecture based on a\nrecursive procedure involving the local Fisher information matrices (FIM) of\nthe distributed estimators. The performance of the CF/DPF algorithm closely\nfollows the centralized particle filter approaching the PCRLB at the signal to\nnoise ratios that we tested."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.3725v1", 
    "title": "Web Services Non-Functional Classification to Enhance Discovery Speed", 
    "arxiv-id": "1112.3725v1", 
    "author": "Safaai Bin Deris", 
    "publish": "2011-12-16T07:39:02Z", 
    "summary": "Recently, the use and deployment of web services has dramatically increased.\nThis is due to the easiness, interoperability, and flexibility that web\nservices offer to the software systems, which other software structures don't\nsupport or support poorly. Web services discovery became more important and\nresearch conducted in this area became more critical. With the increasing\nnumber of published and publicly available web services, speed in web service\ndiscovery process is becoming an issue which cannot be neglected. This paper\nproposes a generic non-functional based web services classification algorithm.\nClassification algorithm depends on information supplied by web service\nprovider at the registration time. Authors have proved mathematically and\nexperimentally the usefulness and efficiency of proposed algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.3880v2", 
    "title": "CloudGenius: Automated Decision Support for Migrating Multi-Component   Enterprise Applications to Clouds", 
    "arxiv-id": "1112.3880v2", 
    "author": "Rajiv Ranjan", 
    "publish": "2011-12-16T16:24:34Z", 
    "summary": "One of the key problems in migrating multi-component enterprise applications\nto Clouds is selecting the best mix of VM images and Cloud infrastructure\nservices. A migration process has to ensure that Quality of Service (QoS)\nrequirements are met, while satisfying conflicting selection criteria, e.g.\nthroughput and cost. When selecting Cloud services, application engineers must\nconsider heterogeneous sets of criteria and complex dependencies across\nmultiple layers impossible to resolve manually. To overcome this challenge, we\npresent the generic recommender framework CloudGenius and an implementation\nthat leverage well known multi-criteria decision making technique Analytic\nHierarchy Process to automate the selection process based on a model, factors,\nand QoS requirements related to enterprise applications. In particular, we\nintroduce a structured migration process for multi-component enterprise\napplications, clearly identify the most important criteria relevant to the\nselection problem and present a multi-criteria-based selection algorithm.\nExperiments with the software prototype CumulusGenius show time complexities."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.4980v1", 
    "title": "Analysis of Bitcoin Pooled Mining Reward Systems", 
    "arxiv-id": "1112.4980v1", 
    "author": "Meni Rosenfeld", 
    "publish": "2011-12-21T10:40:38Z", 
    "summary": "In this paper we describe the various scoring systems used to calculate\nrewards of participants in Bitcoin pooled mining, explain the problems each\nwere designed to solve and analyze their respective advantages and\ndisadvantages."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1112.6254v1", 
    "title": "Scheduling Light-trails in WDM Rings", 
    "arxiv-id": "1112.6254v1", 
    "author": "Abhiram Ranade", 
    "publish": "2011-12-29T08:56:42Z", 
    "summary": "We consider the problem of scheduling communication on optical WDM\n(wavelength division multiplexing) networks using the light-trails technology.\nWe seek to design scheduling algorithms such that the given transmission\nrequests can be scheduled using minimum number of wavelengths (optical\nchannels). We provide algorithms and close lower bounds for two versions of the\nproblem on an $n$ processor linear array/ring network. In the {\\em stationary}\nversion, the pattern of transmissions (given) is assumed to not change over\ntime. For this, a simple lower bound is $c$, the congestion or the maximum\ntotal traffic required to pass through any link. We give an algorithm that\nschedules the transmissions using $O(c+\\log{n})$ wavelengths. We also show a\npattern for which $\\Omega(c+\\log{n}/\\log\\log{n})$ wavelengths are needed. In\nthe {\\em on-line} version, the transmissions arrive and depart dynamically, and\nmust be scheduled without upsetting the previously scheduled transmissions. For\nthis case we give an on-line algorithm which has competitive ratio\n$\\Theta(\\log{n})$. We show that this is optimal in the sense that every on-line\nalgorithm must have competitive ratio $\\Omega(\\log{n})$. We also give an\nalgorithm that appears to do well in simulation (for the classes of traffic we\nconsider), but which has competitive ratio between $\\Omega(\\log^2n/\\log\n\\log{n})$ and $O(\\log^2n)$. We present detailed simulations of both our\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1201.0360v1", 
    "title": "On the Performance of Exhaustive Search with Cooperating agents", 
    "arxiv-id": "1201.0360v1", 
    "author": "Ljupco Krstevski", 
    "publish": "2012-01-01T16:03:49Z", 
    "summary": "Despite the occurrence of elegant algorithms for solving complex problem,\nexhaustive search has retained its significance since many real-life problems\nexhibit no regular structure and exhaustive search is the only possible\nsolution. The advent of high-performance computing either via multicore\nprocessors or distributed processors emphasizes the possibility for exhaustive\nsearch by multiple search agents. Here we analyse the performance of exhaustive\nsearch when it is conducted by multiple search agents. Several strategies for\ncooperation between the search agents are evaluated. We discover that the\nperformance of the search improves with the increase in the level of\ncooperation. Same search performance can be achieved with homogeneous and\nheterogeneous search agents provided that the length of subregions allocated to\nindividual search regions follow the differences in the speeds of heterogeneous\nsearch agents."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TIT.2015.2468584", 
    "link": "http://arxiv.org/pdf/1201.2118v1", 
    "title": "A Massive Data Parallel Computational Framework for Petascale/Exascale   Hybrid Computer Systems", 
    "arxiv-id": "1201.2118v1", 
    "author": "Jian Tao", 
    "publish": "2012-01-10T17:20:17Z", 
    "summary": "Heterogeneous systems are becoming more common on High Performance Computing\n(HPC) systems. Even using tools like CUDA and OpenCL it is a non-trivial task\nto obtain optimal performance on the GPU. Approaches to simplifying this task\ninclude Merge (a library based framework for heterogeneous multi-core systems),\nZippy (a framework for parallel execution of codes on multiple GPUs), BSGP (a\nnew programming language for general purpose computation on the GPU) and\nCUDA-lite (an enhancement to CUDA that transforms code based on annotations).\nIn addition, efforts are underway to improve compiler tools for automatic\nparallelization and optimization of affine loop nests for GPUs and for\nautomatic translation of OpenMP parallelized codes to CUDA.\n  In this paper we present an alternative approach: a new computational\nframework for the development of massively data parallel scientific codes\napplications suitable for use on such petascale/exascale hybrid systems built\nupon the highly scalable Cactus framework. As the first non-trivial\ndemonstration of its usefulness, we successfully developed a new 3D CFD code\nthat achieves improved performance."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijngn.2011.3403", 
    "link": "http://arxiv.org/pdf/1201.2125v1", 
    "title": "Purging of untrustworthy recommendations from a grid", 
    "arxiv-id": "1201.2125v1", 
    "author": "S. Ramachandram", 
    "publish": "2012-01-10T17:46:16Z", 
    "summary": "In grid computing, trust has massive significance. There is lot of research\nto propose various models in providing trusted resource sharing mechanisms. The\ntrust is a belief or perception that various researchers have tried to\ncorrelate with some computational model. Trust on any entity can be direct or\nindirect. Direct trust is the impact of either first impression over the entity\nor acquired during some direct interaction. Indirect trust is the trust may be\ndue to either reputation gained or recommendations received from various\nrecommenders of a particular domain in a grid or any other domain outside that\ngrid or outside that grid itself. Unfortunately, malicious indirect trust leads\nto the misuse of valuable resources of the grid. This paper proposes the\nmechanism of identifying and purging the untrustworthy recommendations in the\ngrid environment. Through the obtained results, we show the way of purging of\nuntrustworthy entities."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijngn.2011.3403", 
    "link": "http://arxiv.org/pdf/1201.2360v2", 
    "title": "Adaptive Redundancy Management for Durable P2P Backup", 
    "arxiv-id": "1201.2360v2", 
    "author": "Pasquale Cataldi", 
    "publish": "2012-01-11T17:56:56Z", 
    "summary": "We design and analyze the performance of a redundancy management mechanism\nfor Peer-to-Peer backup applications. Armed with the realization that a backup\nsystem has peculiar requirements -- namely, data is read over the network only\nduring restore processes caused by data loss -- redundancy management targets\ndata durability rather than attempting to make each piece of information\navailabile at any time.\n  In our approach each peer determines, in an on-line manner, an amount of\nredundancy sufficient to counter the effects of peer deaths, while preserving\nacceptable data restore times. Our experiments, based on trace-driven\nsimulations, indicate that our mechanism can reduce the redundancy by a factor\nbetween two and three with respect to redundancy policies aiming for data\navailability. These results imply an according increase in storage capacity and\ndecrease in time to complete backups, at the expense of longer times required\nto restore data. We believe this is a very reasonable price to pay, given the\nnature of the application.\n  We complete our work with a discussion on practical issues, and their\nsolutions, related to which encoding technique is more suitable to support our\nscheme."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.3804v1", 
    "title": "Managing Communication Latency-Hiding at Runtime for Parallel   Programming Languages and Libraries", 
    "arxiv-id": "1201.3804v1", 
    "author": "Brian Vinter", 
    "publish": "2012-01-18T14:43:43Z", 
    "summary": "This work introduces a runtime model for managing communication with support\nfor latency-hiding. The model enables non-computer science researchers to\nexploit communication latency-hiding techniques seamlessly. For compiled\nlanguages, it is often possible to create efficient schedules for\ncommunication, but this is not the case for interpreted languages. By\nmaintaining data dependencies between scheduled operations, it is possible to\naggressively initiate communication and lazily evaluate tasks to allow maximal\ntime for the communication to finish before entering a wait state. We implement\na heuristic of this model in DistNumPy, an auto-parallelizing version of\nnumerical Python that allows sequential NumPy programs to run on distributed\nmemory architectures. Furthermore, we present performance comparisons for eight\nbenchmarks with and without automatic latency-hiding. The results shows that\nour model reduces the time spent on waiting for communication as much as 27\ntimes, from a maximum of 54% to only 2% of the total execution time, in a\nstencil application."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.4183v2", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs", 
    "arxiv-id": "1201.4183v2", 
    "author": "Guanfeng Liang", 
    "publish": "2012-01-19T22:05:08Z", 
    "summary": "In this paper, we explore the problem of iterative approximate Byzantine\nconsensus in arbitrary directed graphs. In particular, we prove a necessary and\nsufficient condition for the existence of iterative byzantine consensus\nalgorithms. Additionally, we use our sufficient condition to examine whether\nsuch algorithms exist for some specific graphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1201.6652v3", 
    "title": "\"Tri, Tri again\": Finding Triangles and Small Subgraphs in a Distributed   Setting", 
    "arxiv-id": "1201.6652v3", 
    "author": "Shir Peled", 
    "publish": "2012-01-31T19:02:49Z", 
    "summary": "Let G = (V,E) be an n-vertex graph and M_d a d-vertex graph, for some\nconstant d. Is M_d a subgraph of G? We consider this problem in a model where\nall n processes are connected to all other processes, and each message contains\nup to O(log n) bits. A simple deterministic algorithm that requires\nO(n^((d-2)/d) / log n) communication rounds is presented. For the special case\nthat M_d is a triangle, we present a probabilistic algorithm that requires an\nexpected O(ceil(n^(1/3) / (t^(2/3) + 1))) rounds of communication, where t is\nthe number of triangles in the graph, and O(min{n^(1/3) log^(2/3) n / (t^(2/3)\n+ 1), n^(1/3)}) with high probability.\n  We also present deterministic algorithms specially suited for sparse graphs.\nIn any graph of maximum degree Delta, we can test for arbitrary subgraphs of\ndiameter D in O(ceil(Delta^(D+1) / n)) rounds. For triangles, we devise an\nalgorithm featuring a round complexity of O(A^2 / n + log_(2+n/A^2) n), where A\ndenotes the arboricity of G."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.0616v1", 
    "title": "Case Tool: Fast Interconnections with New 3-Disjoint Paths MIN   Simulation Module", 
    "arxiv-id": "1202.0616v1", 
    "author": "Durg Singh Chauhan", 
    "publish": "2012-02-03T07:03:49Z", 
    "summary": "Multi-stage interconnection networks (MIN) can be designed to achieve fault\ntolerance and collision solving by providing a set of disjoint paths. In this\npaper, we are discussing the new simulator added to the tool designed for\ndeveloping fault tolerant MINs. The designed tool is one of its own kind and\nwill help the user in developing 2 and 3-disjoint path networks. The java\ntechnology has been used to design the tool and have been tested on different\nsoftware platform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.0970v1", 
    "title": "\u03c0-Control: A Personal Cloud Control Centre", 
    "arxiv-id": "1202.0970v1", 
    "author": "Alexander Schill", 
    "publish": "2012-02-05T15:25:02Z", 
    "summary": "Consumption of online services and cloud computing offerings is on the rise,\nlargely due to compelling advantages over traditional local applications. From\na user perspective, these include zero-maintenance of software, the always-on\nnature of such services, mashups of different applications and the networking\neffect with other users. Associated disadvantages are known, but effective\nmeans and tools to limit their effect are not yet well-established and not yet\ngenerally available to service users. We propose (1) a user-centric model of\ncloud elements beyond the conventional <SPI>aaS layers, including activities\nacross trust zones, and (2) a personal control console for all individual and\ncollaborative user activities in the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.1062v1", 
    "title": "On Stability Problems of Omega and 3-Disjoint Paths Omega Multi-stage   Interconnection Networks", 
    "arxiv-id": "1202.1062v1", 
    "author": "Mahesh Chandra Govil", 
    "publish": "2012-02-06T07:39:06Z", 
    "summary": "The research paper emphasizes that the Stable Matching problems are the same\nas the problems of stable configurations of Multi-stage Interconnection\nNetworks (MIN). We have discusses the Stability Problems of Existing Regular\nOmega Multi-stage Interconnection Network (OMIN) and Proposed 3-Disjoint Paths\nOmega Multi-stage Interconnection Network (3DON) using the approaches and\nsolutions provided by the Stable Matching Problem. Specifically, Stable\nMarriage Problem is used as an example of Stable Matching. On application of\nthe concept of the Stable Marriage over the MINs states that OMIN is highly\nstable in comparison to 3DON."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.1186v2", 
    "title": "Stone Age Distributed Computing", 
    "arxiv-id": "1202.1186v2", 
    "author": "Roger Wattenhofer", 
    "publish": "2012-02-06T16:20:06Z", 
    "summary": "The traditional models of distributed computing focus mainly on networks of\ncomputer-like devices that can exchange large messages with their neighbors and\nperform arbitrary local computations. Recently, there is a trend to apply\ndistributed computing methods to networks of sub-microprocessor devices, e.g.,\nbiological cellular networks or networks of nano-devices. However, the\nsuitability of the traditional distributed computing models to these types of\nnetworks is questionable: do tiny bio/nano nodes \"compute\" and/or \"communicate\"\nessentially the same as a computer? In this paper, we introduce a new model\nthat depicts a network of randomized finite state machines operating in an\nasynchronous environment. Although the computation and communication\ncapabilities of each individual device in the new model are, by design, much\nweaker than those of a computer, we show that some of the most important and\nextensively studied distributed computing problems can still be solved\nefficiently."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.1983v3", 
    "title": "The Locality of Distributed Symmetry Breaking", 
    "arxiv-id": "1202.1983v3", 
    "author": "Johannes Schneider", 
    "publish": "2012-02-09T13:46:11Z", 
    "summary": "Symmetry breaking problems are among the most well studied in the field of\ndistributed computing and yet the most fundamental questions about their\ncomplexity remain open. In this paper we work in the LOCAL model (where the\ninput graph and underlying distributed network are identical) and study the\nrandomized complexity of four fundamental symmetry breaking problems on graphs:\ncomputing MISs (maximal independent sets), maximal matchings, vertex colorings,\nand ruling sets. A small sample of our results includes\n  - An MIS algorithm running in $O(\\log^2\\Delta + 2^{O(\\sqrt{\\log\\log n})})$\ntime, where $\\Delta$ is the maximum degree. This is the first MIS algorithm to\nimprove on the 1986 algorithms of Luby and Alon, Babai, and Itai, when $\\log n\n\\ll \\Delta \\ll 2^{\\sqrt{\\log n}}$, and comes close to the $\\Omega(\\log \\Delta)$\nlower bound of Kuhn, Moscibroda, and Wattenhofer.\n  - A maximal matching algorithm running in $O(\\log\\Delta + \\log^4\\log n)$\ntime. This is the first significant improvement to the 1986 algorithm of\nIsraeli and Itai. Moreover, its dependence on $\\Delta$ is provably optimal.\n  - A method for reducing symmetry breaking problems in low\narboricity/degeneracy graphs to low degree graphs. (Roughly speaking, the\narboricity or degeneracy of a graph bounds the density of any subgraph.)\nCorollaries of this reduction include an $O(\\sqrt{\\log n})$-time maximal\nmatching algorithm for graphs with arboricity up to $2^{\\sqrt{\\log n}}$ and an\n$O(\\log^{2/3} n)$-time MIS algorithm for graphs with arboricity up to $2^{(\\log\nn)^{1/3}}$.\n  Each of our algorithms is based on a simple, but powerful technique for\nreducing a randomized symmetry breaking task to a corresponding deterministic\none on a poly$(\\log n)$-size graph."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.2509v1", 
    "title": "DEPAS: A Decentralized Probabilistic Algorithm for Auto-Scaling", 
    "arxiv-id": "1202.2509v1", 
    "author": "Dana Petcu", 
    "publish": "2012-02-12T09:26:40Z", 
    "summary": "The dynamic provisioning of virtualized resources offered by cloud computing\ninfrastructures allows applications deployed in a cloud environment to\nautomatically increase and decrease the amount of used resources. This\ncapability is called auto-scaling and its main purpose is to automatically\nadjust the scale of the system that is running the application to satisfy the\nvarying workload with minimum resource utilization. The need for auto-scaling\nis particularly important during workload peaks, in which applications may need\nto scale up to extremely large-scale systems.\n  Both the research community and the main cloud providers have already\ndeveloped auto-scaling solutions. However, most research solutions are\ncentralized and not suitable for managing large-scale systems, moreover cloud\nproviders' solutions are bound to the limitations of a specific provider in\nterms of resource prices, availability, reliability, and connectivity.\n  In this paper we propose DEPAS, a decentralized probabilistic auto-scaling\nalgorithm integrated into a P2P architecture that is cloud provider\nindependent, thus allowing the auto-scaling of services over multiple cloud\ninfrastructures at the same time. Our simulations, which are based on real\nservice traces, show that our approach is capable of: (i) keeping the overall\nutilization of all the instantiated cloud resources in a target range, (ii)\nmaintaining service response times close to the ones obtained using optimal\ncentralized auto-scaling approaches."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.2551v1", 
    "title": "A Simulation Model for Evaluating Distributed Systems Dependability", 
    "arxiv-id": "1202.2551v1", 
    "author": "Valentin Cristea", 
    "publish": "2012-02-12T18:17:10Z", 
    "summary": "In this paper we present a new simulation model designed to evaluate the\ndependability in distributed systems. This model extends the MONARC simulation\nmodel with new capabilities for capturing reliability, safety, availability,\nsecurity, and maintainability requirements. The model has been implemented as\nan extension of the multithreaded, process oriented simulator MONARC, which\nallows the realistic simulation of a wide-range of distributed system\ntechnologies, with respect to their specific components and characteristics.\nThe extended simulation model includes the necessary components to inject\nvarious failure events, and provides the mechanisms to evaluate different\nstrategies for replication, redundancy procedures, and security enforcement\nmechanisms, as well. The results obtained in simulation experiments presented\nin this paper probe that the use of discrete-event simulators, such as MONARC,\nin the design and development of distributed systems is appealing due to their\nefficiency and scalability."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.2981v1", 
    "title": "Theoretical Analysis and Tuning of Decentralized Probabilistic   Auto-Scaling", 
    "arxiv-id": "1202.2981v1", 
    "author": "Dana Petcu", 
    "publish": "2012-02-14T10:21:51Z", 
    "summary": "A major impediment towards the industrial adoption of decentralized\ndistributed systems comes from the difficulty to theoretically prove that these\nsystems exhibit the required behavior. In this paper, we use probability theory\nto analyze a decentralized auto-scaling algorithm in which each node\nprobabilistically decides to scale in or out. We prove that, in the context of\ndynamic workloads, the average load of the system is maintained within a\nvariation interval with a given probability, provided that the number of nodes\nand the variation interval length are higher than certain bounds. The paper\nalso proposes numerical algorithms for approximating these minimum bounds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPCC.2012.80", 
    "link": "http://arxiv.org/pdf/1202.3084v2", 
    "title": "On Dynamic Distributed Computing", 
    "arxiv-id": "1202.3084v2", 
    "author": "Anne-Marie Kermarrec", 
    "publish": "2012-02-14T16:50:12Z", 
    "summary": "This paper shows for the first time that distributed computing can be both\nreliable and efficient in an environment that is both highly dynamic and\nhostile. More specifically, we show how to maintain clusters of size $O(\\log\nN)$, each containing more than two thirds of honest nodes with high\nprobability, within a system whose size can vary \\textit{polynomially} with\nrespect to its initial size. Furthermore, the communication cost induced by\neach node arrival or departure is polylogarithmic with respect to $N$, the\nmaximal size of the system. Our clustering can be achieved despite the presence\nof a Byzantine adversary controlling a fraction $\\bad \\leq \\{1}{3}-\\epsilon$ of\nthe nodes, for some fixed constant $\\epsilon > 0$, independent of $N$. So far,\nsuch a clustering could only be performed for systems who size can vary\nconstantly and it was not clear whether that was at all possible for polynomial\nvariances."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TPDS.2012.239", 
    "link": "http://arxiv.org/pdf/1202.3669v2", 
    "title": "GPUs as Storage System Accelerators", 
    "arxiv-id": "1202.3669v2", 
    "author": "Matei Ripeanu", 
    "publish": "2012-02-16T19:08:29Z", 
    "summary": "Massively multicore processors, such as Graphics Processing Units (GPUs),\nprovide, at a comparable price, a one order of magnitude higher peak\nperformance than traditional CPUs. This drop in the cost of computation, as any\norder-of-magnitude drop in the cost per unit of performance for a class of\nsystem components, triggers the opportunity to redesign systems and to explore\nnew ways to engineer them to recalibrate the cost-to-performance relation. This\nproject explores the feasibility of harnessing GPUs' computational power to\nimprove the performance, reliability, or security of distributed storage\nsystems. In this context, we present the design of a storage system prototype\nthat uses GPU offloading to accelerate a number of computationally intensive\nprimitives based on hashing, and introduce techniques to efficiently leverage\nthe processing power of GPUs. We evaluate the performance of this prototype\nunder two configurations: as a content addressable storage system that\nfacilitates online similarity detection between successive versions of the same\nfile and as a traditional system that uses hashing to preserve data integrity.\nFurther, we evaluate the impact of offloading to the GPU on competing\napplications' performance. Our results show that this technique can bring\ntangible performance gains without negatively impacting the performance of\nconcurrently running applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TPDS.2012.239", 
    "link": "http://arxiv.org/pdf/1202.3943v1", 
    "title": "Many-Task Computing and Blue Waters", 
    "arxiv-id": "1202.3943v1", 
    "author": "Justin M. Wozniak", 
    "publish": "2012-02-17T16:01:53Z", 
    "summary": "This report discusses many-task computing (MTC) generically and in the\ncontext of the proposed Blue Waters systems, which is planned to be the largest\nNSF-funded supercomputer when it begins production use in 2012. The aim of this\nreport is to inform the BW project about MTC, including understanding aspects\nof MTC applications that can be used to characterize the domain and\nunderstanding the implications of these aspects to middleware and policies.\nMany MTC applications do not neatly fit the stereotypes of high-performance\ncomputing (HPC) or high-throughput computing (HTC) applications. Like HTC\napplications, by definition MTC applications are structured as graphs of\ndiscrete tasks, with explicit input and output dependencies forming the graph\nedges. However, MTC applications have significant features that distinguish\nthem from typical HTC applications. In particular, different engineering\nconstraints for hardware and software must be met in order to support these\napplications. HTC applications have traditionally run on platforms such as\ngrids and clusters, through either workflow systems or parallel programming\nsystems. MTC applications, in contrast, will often demand a short time to\nsolution, may be communication intensive or data intensive, and may comprise\nvery short tasks. Therefore, hardware and software for MTC must be engineered\nto support the additional communication and I/O and must minimize task dispatch\noverheads. The hardware of large-scale HPC systems, with its high degree of\nparallelism and support for intensive communication, is well suited for MTC\napplications. However, HPC systems often lack a dynamic resource-provisioning\nfeature, are not ideal for task communication via the file system, and have an\nI/O system that is not optimized for MTC-style applications. Hence, additional\nsoftware support is likely to be required to gain full benefit from the HPC\nhardware."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3109", 
    "link": "http://arxiv.org/pdf/1202.4347v1", 
    "title": "GPGPU Processing in CUDA Architecture", 
    "arxiv-id": "1202.4347v1", 
    "author": "Amit Bawaskar", 
    "publish": "2012-02-20T15:16:40Z", 
    "summary": "The future of computation is the Graphical Processing Unit, i.e. the GPU. The\npromise that the graphics cards have shown in the field of image processing and\naccelerated rendering of 3D scenes, and the computational capability that these\nGPUs possess, they are developing into great parallel computing units. It is\nquite simple to program a graphics processor to perform general parallel tasks.\nBut after understanding the various architectural aspects of the graphics\nprocessor, it can be used to perform other taxing tasks as well. In this paper,\nwe will show how CUDA can fully utilize the tremendous power of these GPUs.\nCUDA is NVIDIA's parallel computing architecture. It enables dramatic increases\nin computing performance, by harnessing the power of the GPU. This paper talks\nabout CUDA and its architecture. It takes us through a comparison of CUDA C/C++\nwith other parallel programming languages like OpenCL and DirectCompute. The\npaper also lists out the common myths about CUDA and how the future seems to be\npromising for CUDA."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.4508v1", 
    "title": "Processes, Roles and Their Interactions", 
    "arxiv-id": "1202.4508v1", 
    "author": "Johannes Reich", 
    "publish": "2012-02-21T01:41:49Z", 
    "summary": "Taking an interaction network oriented perspective in informatics raises the\nchallenge to describe deterministic finite systems which take part in networks\nof nondeterministic interactions. The traditional approach to describe\nprocesses as stepwise executable activities which are not based on the\nordinarily nondeterministic interaction shows strong centralization tendencies.\nAs suggested in this article, viewing processes and their interactions as\ncomplementary can circumvent these centralization tendencies.\n  The description of both, processes and their interactions is based on the\nsame building blocks, namely finite input output automata (or transducers).\nProcesses are viewed as finite systems that take part in multiple, ordinarily\nnondeterministic interactions. The interactions between processes are described\nas protocols.\n  The effects of communication between processes as well as the necessary\ncoordination of different interactions within a processes are both based on the\nrestriction of the transition relation of product automata. The channel based\nouter coupling represents the causal relation between the output and the input\nof different systems. The coordination condition based inner coupling\nrepresents the causal relation between the input and output of a single system.\n  All steps are illustrated with the example of a network of resource\nadministration processes which is supposed to provide requesting user processes\nexclusive access to a single resource."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5261v4", 
    "title": "Belief Consensus Algorithms for Fast Distributed Target Tracking in   Wireless Sensor Networks", 
    "arxiv-id": "1202.5261v4", 
    "author": "Santiago Zazo", 
    "publish": "2012-02-23T18:32:59Z", 
    "summary": "In distributed target tracking for wireless sensor networks, agreement on the\ntarget state can be achieved by the construction and maintenance of a\ncommunication path, in order to exchange information regarding local likelihood\nfunctions. Such an approach lacks robustness to failures and is not easily\napplicable to ad-hoc networks. To address this, several methods have been\nproposed that allow agreement on the global likelihood through fully\ndistributed belief consensus (BC) algorithms, operating on local likelihoods in\ndistributed particle filtering (DPF). However, a unified comparison of the\nconvergence speed and communication cost has not been performed. In this paper,\nwe provide such a comparison and propose a novel BC algorithm based on belief\npropagation (BP). According to our study, DPF based on metropolis belief\nconsensus (MBC) is the fastest in loopy graphs, while DPF based on BP consensus\nis the fastest in tree graphs. Moreover, we found that BC-based DPF methods\nhave lower communication overhead than data flooding when the network is\nsufficiently sparse."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5482v2", 
    "title": "Risk-Driven Compliant Access Controls for Clouds", 
    "arxiv-id": "1202.5482v2", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T15:49:39Z", 
    "summary": "There is widespread agreement that cloud computing have proven cost cutting\nand agility benefits. However, security and regulatory compliance issues are\ncontinuing to challenge the wide acceptance of such technology both from social\nand commercial stakeholders. An important facture behind this is the fact that\nclouds and in particular public clouds are usually deployed and used within\nbroad geographical or even international domains. This implies that the\nexchange of private and other protected data within the cloud environment would\nbe governed by multiple jurisdictions. These jurisdictions have a great degree\nof harmonisation; however, they present possible conflicts that are hard to\nnegotiate at run time. So far, important efforts were played in order to deal\nwith regulatory compliance management for large distributed systems. However,\nmeasurable solutions are required for the context of cloud. In this position\npaper, we are suggesting an approach that starts with a conceptual model of\nexplicit regulatory requirements for exchanging private data on a\nmultijurisdictional environment and build on it in order to define metrics for\nnon-compliance or, in other terms, risks to compliance. These metrics will be\nintegrated within usual data access-control policies and will be checked at\npolicy analysis time before a decision to allow/deny the data access is made."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5483v1", 
    "title": "An Architecture for Integrated Intelligence in Urban Management using   Cloud Computing", 
    "arxiv-id": "1202.5483v1", 
    "author": "Ashiq Anjum", 
    "publish": "2012-02-24T15:52:56Z", 
    "summary": "With the emergence of new methodologies and technologies it has now become\npossible to manage large amounts of environmental sensing data and apply new\nintegrated computing models to acquire information intelligence. This paper\nadvocates the application of cloud capacity to support the information,\ncommunication and decision making needs of a wide variety of stakeholders in\nthe complex business of the management of urban and regional development. The\ncomplexity lies in the interactions and impacts embodied in the concept of the\nurban-ecosystem at various governance levels. This highlights the need for more\neffective integrated environmental management systems. This paper offers a\nuser-orientated approach based on requirements for an effective management of\nthe urban-ecosystem and the potential contributions that can be supported by\nthe cloud computing community. Furthermore, the commonality of the influence of\nthe drivers of change at the urban level offers the opportunity for the cloud\ncomputing community to develop generic solutions that can serve the needs of\nhundreds of cities from Europe and indeed globally."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5512v1", 
    "title": "A Fault Tolerant, Dynamic and Low Latency BDII Architecture for Grids", 
    "arxiv-id": "1202.5512v1", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T17:51:35Z", 
    "summary": "The current BDII model relies on information gathering from agents that run\non each core node of a Grid. This information is then published into a Grid\nwide information resource known as Top BDII. The Top level BDIIs are updated\ntypically in cycles of a few minutes each. A new BDDI architecture is proposed\nand described in this paper based on the hypothesis that only a few attribute\nvalues change in each BDDI information cycle and consequently it may not be\nnecessary to update each parameter in a cycle. It has been demonstrated that\nsignificant performance gains can be achieved by exchanging only the\ninformation about records that changed during a cycle. Our investigations have\nled us to implement a low latency and fault tolerant BDII system that involves\nonly minimal data transfer and facilitates secure transactions in a Grid\nenvironment."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.5519v1", 
    "title": "Context-Aware Service Utilisation in the Clouds and Energy Conservation", 
    "arxiv-id": "1202.5519v1", 
    "author": "Richard McClatchey", 
    "publish": "2012-02-24T18:21:23Z", 
    "summary": "Ubiquitous computing environments are characterised by smart, interconnected\nartefacts embedded in our physical world that are projected to provide useful\nservices to human inhabitants unobtrusively. Mobile devices are becoming the\nprimary tools of human interaction with these embedded artefacts and\nutilisation of services available in smart computing environments such as\nclouds. Advancements in capabilities of mobile devices allow a number of user\nand environment related context consumers to be hosted on these devices.\nWithout a coordinating component, these context consumers and providers are a\npotential burden on device resources; specifically the effect of uncoordinated\ncomputation and communication with cloud-enabled services can negatively impact\nthe battery life. Therefore energy conservation is a major concern in realising\nthe collaboration and utilisation of mobile device based context-aware\napplications and cloud based services. This paper presents the concept of a\ncontext-brokering component to aid in coordination and communication of context\ninformation between mobile devices and services deployed in a cloud\ninfrastructure. A prototype context broker is experimentally analysed for\neffects on energy conservation when accessing and coordinating with cloud\nservices on a smart device, with results signifying reduction in energy\nconsumption."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.78.3", 
    "link": "http://arxiv.org/pdf/1202.6094v2", 
    "title": "Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -   Part II: Synchronous and Asynchronous Systems", 
    "arxiv-id": "1202.6094v2", 
    "author": "Guanfeng Liang", 
    "publish": "2012-02-28T00:01:48Z", 
    "summary": "This report contains two related sets of results with different assumptions\non synchrony. The first part is about iterative algorithms in synchronous\nsystems. Following our previous work on synchronous iterative approximate\nByzantine consensus (IABC) algorithms, we provide a more intuitive tight\nnecessary and sufficient condition for the existence of such algorithms in\nsynchronous networks1. We believe this condition and the previous results also\nhold in partially asynchronous algorithmic model.\n  In the second part of the report, we explore the problem in asynchronous\nnetworks. While the traditional Byzantine consensus is not solvable in\nasynchronous systems, approximate Byzantine consensus can be solved using\niterative algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1202.6134v2", 
    "title": "High Volume Computing: Identifying and Characterizing Throughput   Oriented Workloads in Data Centers", 
    "arxiv-id": "1202.6134v2", 
    "author": "Chunjie Luo", 
    "publish": "2012-02-28T06:37:31Z", 
    "summary": "For the first time, this paper systematically identifies three categories of\nthroughput oriented workloads in data centers: services, data processing\napplications, and interactive real-time applications, whose targets are to\nincrease the volume of throughput in terms of processed requests or data, or\nsupported maximum number of simultaneous subscribers, respectively, and we coin\na new term high volume computing (in short HVC) to describe those workloads and\ndata center computer systems designed for them. We characterize and compare HVC\nwith other computing paradigms, e.g., high throughput computing,\nwarehouse-scale computing, and cloud computing, in terms of levels, workloads,\nmetrics, coupling degree, data scales, and number of jobs or service instances.\nWe also preliminarily report our ongoing work on the metrics and benchmarks for\nHVC systems, which is the foundation of designing innovative data center\ncomputer systems for HVC workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1202.6456v1", 
    "title": "Resource-Competitive Communication", 
    "arxiv-id": "1202.6456v1", 
    "author": "Maxwell Young", 
    "publish": "2012-02-29T06:28:53Z", 
    "summary": "Consider the general scenario where Alice wishes to transmit a message m to\nBob. These two players share a communication channel; however, there exists an\nadversary, Carol, who aims to prevent the transmission of m by blocking this\nchannel. There are costs to send, receive or block m on the channel, and we\nask: How much do Alice and Bob need to spend relative to the adversary Carol in\norder to guarantee transmission of m?\n  We show that in a time-slotted network with constant costs to send, receive\nand block m in a slot, if Carol spends a total of B slots trying to block m,\nthen both Alice and Bob must be active for only O(B^{\\varphi - 1} +\n1)=O(B^{.62}+1) slots in expectation to transmit m, where \\varphi = (1 +\n\\sqrt{5})/2 is the golden ratio. Surprisingly, this result holds even if (1) B\nis unknown to either player; (2) Carol knows the algorithms of both players,\nbut not their random bits; and (3) Carol can attack using total knowledge of\npast actions of both players.\n  In the spirit of competitive analysis, approximation guarantees, and\ngame-theoretic treatments, our approach represents another notion of relative\nperformance that we call resource competitiveness. This new metric measures the\nworst-case performance of an algorithm relative to any adversarial strategy and\npertains to scenarios where all network devices are resource-constrained. Here,\nwe apply the resource-competitive results above to two concrete problems.\nFirst, we consider jamming attacks in WSNs and address the fundamental task of\npropagating m from a single device to all others in the presence of faults.\nSecond, we examine how to mitigate application-level DDoS attacks in a wired\nclient-server scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1203.0429v1", 
    "title": "Securing business operations in an SOA", 
    "arxiv-id": "1203.0429v1", 
    "author": "Pierre de Leusse", 
    "publish": "2012-03-02T11:51:20Z", 
    "summary": "Service-oriented infrastructures pose new challenges in a number of areas,\nnotably with regard to security and dependability. BT has developed a\ncombination of innovative security solutions and governance frameworks that can\naddress these challenges. They include advances in identity federation;\ndistributed usage and access management; context-aware secure messaging,\nrouting and transformation; and (security) policy governance for\nservice-oriented architectures. This paper discusses these developments and the\nsteps being taken to validate their functionality and performance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1203.0432v1", 
    "title": "Toward Governance of Cross-Cloud Application Deployment", 
    "arxiv-id": "1203.0432v1", 
    "author": "Krzysztof Zielinski", 
    "publish": "2012-03-02T12:03:02Z", 
    "summary": "In this article, the authors introduce the main ideas around the governance\nof cross-Cloud application deployment and their related concepts. It is argued\nthat, due to the increasing complexity and nature of the Cloud market, an\nintermediary specialized in brokering the deployment of different components of\na same application onto different Cloud products could both facilitate said\ndeployment and in some cases improve its quality in terms of cost, security &\nreliability and QoS. In order to fulfill these objectives, the authors propose\na high level architecture that relies on their previous work on governance of\npolicy & rule driven distributed systems. This architecture aims at supplying\nfive main functions of 1) translation of Service Level Agreements (SLAs) and\npricing into a common shared DSL, 2) correlation of analytical data (e.g.\nmonitoring, metering), 3) combination of Cloud products, 4) information from\nthird parties regarding different aspects of Quality of Service (QoS) and 5)\ncross-Cloud application deployment specification and governance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.213", 
    "link": "http://arxiv.org/pdf/1203.0435v1", 
    "title": "A common interface for multi-rule-engine distributed systems", 
    "arxiv-id": "1203.0435v1", 
    "author": "Krzysztof Zielinski", 
    "publish": "2012-03-02T12:07:57Z", 
    "summary": "The rule technological landscape is becoming ever more complex, with an\nextended number of specifications and products. It is therefore becoming\nincreasingly difficult to integrate rule-driven components and manage\ninteroperability in multi-rule engine environments. The described work presents\nthe possibility to provide a common interface for rule-driven components in a\ndistributed system. The authors' approach leverages on a set of discovery\nprotocol, rule interchange and user interface to alleviate the environment's\ncomplexity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0443v1", 
    "title": "Secure & Rapid Composition of Infrastructure Services in the Cloud", 
    "arxiv-id": "1203.0443v1", 
    "author": "Andreas Maierhofer", 
    "publish": "2012-03-02T12:25:09Z", 
    "summary": "A fundamental ambition of grid and distributed systems is to be capable of\nsustaining evolution and allowing for adaptability ((F. Losavio et al., 2002),\n(S. Radhakrishnan, 2005)). Furthermore, as the complexity and sophistication of\ntheses structures increases, so does the need for adaptability of each\ncomponent. One of the primary benefits of service oriented architecture (SOA)\nis the ability to compose applications, processes or more complex services from\nother services which increases the capacity for adaptation. This document\nproposes a novel infrastructure composition model that aims at increasing the\nadaptability of the capabilities exposed through it by dynamically managing\ntheir non functional requirements."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0651v1", 
    "title": "On Modeling Dependency between MapReduce Configuration Parameters and   Total Execution Time", 
    "arxiv-id": "1203.0651v1", 
    "author": "Javid Taheri", 
    "publish": "2012-03-03T13:18:51Z", 
    "summary": "In this paper, we propose an analytical method to model the dependency\nbetween configuration parameters and total execution time of Map-Reduce\napplications. Our approach has three key phases: profiling, modeling, and\nprediction. In profiling, an application is run several times with different\nsets of MapReduce configuration parameters to profile the execution time of the\napplication on a given platform. Then in modeling, the relation between these\nparameters and total execution time is modeled by multivariate linear\nregression. Among the possible configuration parameters, two main parameters\nhave been used in this study: the number of Mappers, and the number of\nReducers. For evaluation, two standard applications (WordCount, and Exim\nMainlog parsing) are utilized to evaluate our technique on a 4-node MapReduce\nplatform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.0740v1", 
    "title": "Resource Availability-Aware Advance Reservation for Parallel Jobs with   Deadlines", 
    "arxiv-id": "1203.0740v1", 
    "author": "Jundong Yang", 
    "publish": "2012-03-04T14:11:56Z", 
    "summary": "Advance reservation is important to guarantee the quality of services of jobs\nby allowing exclusive access to resources over a defined time interval on\nresources. It is a challenge for the scheduler to organize available resources\nefficiently and to allocate them for parallel AR jobs with deadline constraint\nappropriately. This paper provides a slot-based data structure to organize\navailable resources of multiprocessor systems in a way that enables efficient\nsearch and update operations, and formulates a suite of scheduling policies to\nallocate resources for dynamically arriving AR requests. The performance of the\nscheduling algorithms were investigated by simulations with different job sizes\nand durations, system loads and scheduling flexibilities. Simulation results\nshow that job sizes and durations, system load and the flexibility of\nscheduling will impact the performance metrics of all the scheduling\nalgorithms, and the PE-Worst-Fit algorithm becomes the best algorithm for the\nscheduler with the highest acceptance rate of AR requests, and the jobs with\nthe First-Fit algorithm experience the lowest average slowdown. The data\nstructure and scheduling policies can be used to organize and allocate\nresources for parallel AR jobs with deadline constraint in large-scale\ncomputing systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.1466v1", 
    "title": "Batch-oriented software appliances", 
    "arxiv-id": "1203.1466v1", 
    "author": "Sergio Maffioletti", 
    "publish": "2012-03-07T13:45:24Z", 
    "summary": "This paper presents AppPot, a system for creating Linux software appliances.\nAppPot can be run as a regular batch or grid job and executed in user space,\nand requires no special virtualization support in the infrastructure.\n  The main design goal of AppPot is to bring the benefits of a\nvirtualization-based IaaS cloud to existing batch-oriented computing\ninfrastructures.\n  In particular, AppPot addresses the application deployment and configuration\non large heterogeneous computing infrastructures: users are enabled to prepare\ntheir own customized virtual appliance for providing a safe execution\nenvironment for their applications. These appliances can then be executed on\nvirtually any computing infrastructure being in a private or public cloud as\nwell as any batch-controlled computing clusters the user may have access to.\n  We give an overview of AppPot and its features, the technology that makes it\npossible, and report on experiences running it in production use within the\nSwiss National Grid infrastructure SMSCG."
},{
    "category": "cs.DC", 
    "doi": "10.1109/SENSORCOMM.2008.130", 
    "link": "http://arxiv.org/pdf/1203.1888v1", 
    "title": "Matrix Representation of Iterative Approximate Byzantine Consensus in   Directed Graphs", 
    "arxiv-id": "1203.1888v1", 
    "author": "Nitin Vaidya", 
    "publish": "2012-03-08T19:00:19Z", 
    "summary": "This paper presents a proof of correctness of an iterative approximate\nByzantine consensus (IABC) algorithm for directed graphs. The iterative\nalgorithm allows fault- free nodes to reach approximate conensus despite the\npresence of up to f Byzantine faults. Necessary conditions on the underlying\nnetwork graph for the existence of a correct IABC algorithm were shown in our\nrecent work [15, 16]. [15] also analyzed a specific IABC algorithm and showed\nthat it performs correctly in any network graph that satisfies the necessary\ncondition, proving that the necessary condition is also sufficient. In this\npaper, we present an alternate proof of correctness of the IABC algorithm,\nusing a familiar technique based on transition matrices [9, 3, 17, 19].\n  The key contribution of this paper is to exploit the following observation:\nfor a given evolution of the state vector corresponding to the state of the\nfault-free nodes, many alternate state transition matrices may be chosen to\nmodel that evolution cor- rectly. For a given state evolution, we identify one\napproach to suitably \"design\" the transition matrices so that the standard\ntools for proving convergence can be applied to the Byzantine fault-tolerant\nalgorithm as well. In particular, the transition matrix for each iteration is\ndesigned such that each row of the matrix contains a large enough number of\nelements that are bounded away from 0."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.2081v2", 
    "title": "BSP vs MapReduce", 
    "arxiv-id": "1203.2081v2", 
    "author": "Matthew Felice Pace", 
    "publish": "2012-03-09T13:42:03Z", 
    "summary": "The MapReduce framework has been generating a lot of interest in a wide range\nof areas. It has been widely adopted in industry and has been used to solve a\nnumber of non-trivial problems in academia. Putting MapReduce on strong\ntheoretical foundations is crucial in understanding its capabilities. This work\nlinks MapReduce to the BSP model of computation, underlining the relevance of\nBSP to modern parallel algorithm design and defining a subclass of BSP\nalgorithms that can be efficiently implemented in MapReduce."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.2366v1", 
    "title": "Technical support for Life Sciences communities on a production grid   infrastructure", 
    "arxiv-id": "1203.2366v1", 
    "author": "Tristan Glatard", 
    "publish": "2012-03-11T19:22:51Z", 
    "summary": "Production operation of large distributed computing infrastructures (DCI)\nstill requires a lot of human intervention to reach acceptable quality of\nservice. This may be achievable for scientific communities with solid IT\nsupport, but it remains a show-stopper for others. Some application execution\nenvironments are used to hide runtime technical issues from end users. But they\nmostly aim at fault-tolerance rather than incident resolution, and their\noperation still requires substantial manpower. A longer-term support activity\nis thus needed to ensure sustained quality of service for Virtual Organisations\n(VO). This paper describes how the biomed VO has addressed this challenge by\nsetting up a technical support team. Its organisation, tooling, daily tasks,\nand procedures are described. Results are shown in terms of resource usage by\nend users, amount of reported incidents, and developed software tools. Based on\nour experience, we suggest ways to measure the impact of the technical support,\nperspectives to decrease its human cost and make it more community-specific."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3013v1", 
    "title": "A Protocol for the Atomic Capture of Multiple Molecules at Large Scale", 
    "arxiv-id": "1203.3013v1", 
    "author": "C\u00e9dric Tedeschi", 
    "publish": "2012-03-14T07:30:11Z", 
    "summary": "With the rise of service-oriented computing, applications are more and more\nbased on coordination of autonomous services. Envisioned over largely\ndistributed and highly dynamic platforms, expressing this coordination calls\nfor alternative programming models. The chemical programming paradigm, which\nmodels applications as chemical solutions where molecules representing digital\nentities involved in the computation, react together to produce a result, has\nbeen recently shown to provide the needed abstractions for autonomic\ncoordination of services. However, the execution of such programs over large\nscale platforms raises several problems hindering this paradigm to be actually\nleveraged. Among them, the atomic capture of molecules participating in concur-\nrent reactions is one of the most significant. In this paper, we propose a\nprotocol for the atomic capture of these molecules distributed and evolving\nover a large scale platform. As the density of possible reactions is crucial\nfor the liveness and efficiency of such a capture, the protocol proposed is\nmade up of two sub-protocols, each of them aimed at addressing different levels\nof densities of potential reactions in the solution. While the decision to\nchoose one or the other is local to each node participating in a program's\nexecution, a global coherent behaviour is obtained. Proof of liveness, as well\nas intensive simulation results showing the efficiency and limited overhead of\nthe protocol are given."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3098v1", 
    "title": "Generalized Asynchronous Systems", 
    "arxiv-id": "1203.3098v1", 
    "author": "Ekaterina S. Kudryashova", 
    "publish": "2012-03-14T14:37:25Z", 
    "summary": "The paper is devoted to a mathematical model of concurrency the special case\nof which is asynchronous system. Distributed asynchronous automata are\nintroduced here. It is proved that the Petri nets and transition systems with\nindependence can be considered like the distributed asynchronous automata. Time\ndistributed asynchronous automata are defined in standard way by the map which\nassigns time intervals to events. It is proved that the time distributed\nasynchronous automata are generalized the time Petri nets and asynchronous\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3575v1", 
    "title": "The Byzantine Brides Problem", 
    "arxiv-id": "1203.3575v1", 
    "author": "Nini Zhu", 
    "publish": "2012-03-15T21:26:34Z", 
    "summary": "We investigate the hardness of establishing as many stable marriages (that\nis, marriages that last forever) in a population whose memory is placed in some\narbitrary state with respect to the considered problem, and where traitors try\nto jeopardize the whole process by behaving in a harmful manner. On the\nnegative side, we demonstrate that no solution that is completely insensitive\nto traitors can exist, and we propose a protocol for the problem that is\noptimal with respect to the traitor containment radius."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.3885v1", 
    "title": "Decentralized Probabilistic Auto-Scaling for Heterogeneous Systems", 
    "arxiv-id": "1203.3885v1", 
    "author": "Dana Petcu", 
    "publish": "2012-03-17T18:48:23Z", 
    "summary": "The DEPAS (Decentralized Probabilistic Auto-Scaling) algorithm assumes an\noverlay network of computing nodes where each node probabilistically decides to\nshut down, allocate one or more other nodes or do nothing. DEPAS was\nformulated, tested, and theoretically analyzed for the simplified case of\nhomogenous systems. In this paper, we extend DEPAS to heterogeneous systems."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4257v1", 
    "title": "DiscopFlow: A new Tool for Discovering Organizational Structures and   Interaction Protocols in WorkFlow", 
    "arxiv-id": "1203.4257v1", 
    "author": "Faiez Gargouri", 
    "publish": "2012-03-19T20:53:03Z", 
    "summary": "This work deals with Workflow Mining (WM) a very active and promising\nresearch area. First, in this paper we give a critical and comparative study of\nthree representative WM systems of this area: the ProM, InWolve and\nWorkflowMiner systems. The comparison is made according to quality criteria\nthat we have defined such as the capacity to filter and convert a Workflow log,\nthe capacity to discover workflow perspectives and the capacity to support\nMulti-Analysis of processes. The major drawback of these systems is the non\npossibility to deal with organizational perspective discovering issue. We mean\nby organizational perspective, the organizational structures (federation,\ncoalition, market or hierarchy) and interaction protocols (contract net,\nauction or vote). This paper defends the idea that organizational dimension in\nMulti-Agent System is an appropriate approach to support the discovering of\nthis organizational perspective. Second, the paper proposes a Workflow log\nmeta-model which extends the classical one by considering the interactions\namong actors thanks to the FIPA-ACL Performatives. Third, it describes in\ndetails our DiscopFlow tool which validates our contribution."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4367v1", 
    "title": "Thesis Report: Resource Utilization Provisioning in MapReduce", 
    "arxiv-id": "1203.4367v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-03-20T10:06:24Z", 
    "summary": "In this thesis report, we have a survey on state-of-the-art methods for\nmodelling resource utilization of MapReduce applications regard to its\nconfiguration parameters. After implementation of one of the algorithms in\nliterature, we tried to find that if CPU usage modelling of a MapReduce\napplication can be used to predict CPU usage of another MapReduce application."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4751v8", 
    "title": "Optimism for Boosting Concurrency", 
    "arxiv-id": "1203.4751v8", 
    "author": "Srivatsan Ravi", 
    "publish": "2012-03-21T14:44:40Z", 
    "summary": "Modern concurrent programming benefits from a large variety of\nsynchronization techniques. These include conventional pessimistic locking, as\nwell as optimistic techniques based on conditional synchronization primitives\nor transactional memory. Yet, it is unclear which of these approaches better\nleverage the concurrency inherent to multi-cores.\n  In this paper, we compare the level of concurrency one can obtain by\nconverting a sequential program into a concurrent one using optimistic or\npessimistic techniques. To establish fair comparison of such implementations,\nwe introduce a new correctness criterion for concurrent programs, defined\nindependently of the synchronization techniques they use.\n  We treat a program's concurrency as its ability to accept a concurrent\nschedule, a metric inspired by the theories of both databases and transactional\nmemory. We show that pessimistic locking can provide strictly higher\nconcurrency than transactions for some applications whereas transactions can\nprovide strictly higher concurrency than pessimistic locks for others. Finally,\nwe show that combining the benefits of the two synchronization techniques can\nprovide strictly more concurrency than any of them individually. We propose a\nlist-based set algorithm that is optimal in the sense that it accepts all\ncorrect concurrent schedules. As we show via experimentation, the optimality in\nterms of concurrency is reflected by scalability gains."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.4938v2", 
    "title": "Advanced Programming Platform for efficient use of Data Parallel   Hardware", 
    "arxiv-id": "1203.4938v2", 
    "author": "Luis Cabellos", 
    "publish": "2012-03-22T09:54:58Z", 
    "summary": "Graphics processing units (GPU) had evolved from a specialized hardware\ncapable to render high quality graphics in games to a commodity hardware for\neffective processing blocks of data in a parallel schema. This evolution is\nparticularly interesting for scientific groups, which traditionally use mainly\nCPU as a work horse, and now can profit of the arrival of GPU hardware to HPC\nclusters. This new GPU hardware promises a boost in peak performance, but it is\nnot trivial to use. In this article a programming platform designed to promote\na direct use of this specialized hardware is presented. This platform includes\na visual editor of parallel data flows and it is oriented to the execution in\ndistributed clusters with GPUs. Examples of application in two characteristic\nproblems, Fast Fourier Transform and Image Compression, are also shown."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5004v2", 
    "title": "CUDA implementation of Wagener's 2D convex hull PRAM algorithm", 
    "arxiv-id": "1203.5004v2", 
    "author": "Colm O. Dunlaing", 
    "publish": "2012-03-22T14:30:25Z", 
    "summary": "This paper describes a CUDA implementation of Wagener's PRAM convex hull\nalgorithm in two dimensions. It is presented in Knuth's literate programming\nstyle."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5160v2", 
    "title": "Multiple Frequency Selection in DVFS-Enabled Processors to Minimize   Energy Consumption", 
    "arxiv-id": "1203.5160v2", 
    "author": "Javid Taheri", 
    "publish": "2012-03-23T02:42:38Z", 
    "summary": "In this chapter we focus on slack reclamation and propose a new slack\nreclamation technique, Multiple Frequency Selection DVFS (MFS-DVFS). The key\nidea is to execute each task with a linear combination of more than one\nfrequency such that this combination results in using the lowest energy by\ncovering the whole slack time of the task. We have tested our algorithm with\nboth random and real-world application task graphs and compared with the\nresults in previous researches in [9] and [12-13]. The experimental results\nshow that our approach can achieve energy almost identical to the optimum\nenergy saving."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.5196v1", 
    "title": "Market-Oriented Cloud Computing and the Cloudbus Toolkit", 
    "arxiv-id": "1203.5196v1", 
    "author": "Christian Vecchiola", 
    "publish": "2012-03-23T08:50:57Z", 
    "summary": "Cloud computing has penetrated the Information Technology industry deep\nenough to influence major companies to adopt it into their mainstream business.\nA strong thrust on the use of virtualization technology to realize\nInfrastructure-as-a-Service (IaaS) has led enterprises to leverage\nsubscription-oriented computing capabilities of public Clouds for hosting their\napplication services. In parallel, research in academia has been investigating\ntransversal aspects such as security, software frameworks, quality of service,\nand standardization. We believe that the complete realization of the Cloud\ncomputing vision will lead to the introduction of a virtual market where Cloud\nbrokers, on behalf of end users, are in charge of selecting and composing the\nservices advertised by different Cloud vendors. In order to make this happen,\nexisting solutions and technologies have to be redesigned and extended from a\nmarket-oriented perspective and integrated together, giving rise to what we\nterm Market-Oriented Cloud Computing.\n  In this paper, we will assess the current status of Cloud computing by\nproviding a reference model, discuss the challenges that researchers and IT\npractitioners are facing and will encounter in the near future, and present the\napproach for solving them from the perspective of the Cloudbus toolkit, which\ncomprises of a set of technologies geared towards the realization of Market\nOriented Cloud Computing vision. We provide experimental results demonstrating\nmarket-oriented resource provisioning and brokering within a Cloud and across\nmultiple distributed resources. We also include an application illustrating the\nhosting of ECG analysis as SaaS on Amazon IaaS (EC2 and S3) services."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1203.6096v1", 
    "title": "Asynchrony from Synchrony", 
    "arxiv-id": "1203.6096v1", 
    "author": "Eli Gafni", 
    "publish": "2012-03-27T22:26:02Z", 
    "summary": "We consider synchronous dynamic networks which like radio networks may have\nasymmetric communication links, and are affected by communication rather than\nprocessor failures. In this paper we investigate the minimal message\nsurvivability in a per round basis that allows for the minimal global\ncooperation, i.e., allows to solve any task that is wait-free read-write\nsolvable. The paper completely characterizes this survivability requirement.\nMessage survivability is formalized by considering adversaries that have a\nlimited power to remove messages in a round. Removal of a message on a link in\none direction does not necessarily imply the removal of the message on that\nlink in the other direction. Surprisingly there exist a single strongest\nadversary which solves any wait-free read/write task. Any different adversary\nthat solves any wait-free read/write task is weaker, and any stronger adversary\nwill not solve any wait-free read/write task. ABD \\cite{ABD} who considered\nprocessor failure, arrived at an adversary that is $n/2$ resilient,\nconsequently can solve tasks, such as $n/2$-set-consensus, which are not\nread/write wait-free solvable. With message adversaries, we arrive at an\nadversary which has exactly the read-write wait-free power. Furthermore, this\nadversary allows for a considerably simpler (simplest that we know of) proof\nthat the protocol complex of any read/write wait-free task is a subdivided\nsimplex, finally making this proof accessible for students with no\nalgebraic-topology prerequisites, and alternatively dispensing with the\nassumption that the Immediate Snapshot complex is a subdivided simplex."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1205.0451v2", 
    "title": "Mobile Cloud Computing: A Review on Smartphone Augmentation Approaches", 
    "arxiv-id": "1205.0451v2", 
    "author": "Abdullah Gani", 
    "publish": "2012-05-02T15:04:04Z", 
    "summary": "Smartphones have recently gained significant popularity in heavy mobile\nprocessing while users are increasing their expectations toward rich computing\nexperience. However, resource limitations and current mobile computing\nadvancements hinder this vision. Therefore, resource-intensive application\nexecution remains a challenging task in mobile computing that necessitates\ndevice augmentation. In this article, smartphone augmentation approaches are\nreviewed and classified in two main groups, namely hardware and software.\nGenerating high-end hardware is a subset of hardware augmentation approaches,\nwhereas conserving local resource and reducing resource requirements approaches\nare grouped under software augmentation methods. Our study advocates that\nconsreving smartphones' native resources, which is mainly done via task\noffloading, is more appropriate for already-developed applications than new\nones, due to costly re-development process. Cloud computing has recently\nobtained momentous ground as one of the major cornerstone technologies in\naugmenting smartphones. We present sample execution model for intensive mobile\napplications and devised taxonomy of augmentation approaches. For better\ncomprehension, the results of this study are summarized in a table."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.procs.2012.04.026", 
    "link": "http://arxiv.org/pdf/1205.0652v2", 
    "title": "On Exploiting Hotspot and Entropy for Data Forwarding in Delay Tolerant   Networks", 
    "arxiv-id": "1205.0652v2", 
    "author": "Shaojie Tang", 
    "publish": "2012-05-03T09:02:44Z", 
    "summary": "Performance of data forwarding in Delay Tolerant Networks (DTNs) benefits\nconsiderably if one can make use of human mobility in terms of social\nstructures. However, it is difficult and time-consuming to calculate the\ncentrality and similarity of nodes by using solutions for traditional social\nnetworks, this is mainly because of the transient node contact and the\nintermittently connected environment. In this work, we are interested in the\nfollowing question: Can we explore some other stable social attributes to\nquantify the centrality and similarity of nodes? Taking GPS traces of human\nwalks from the real world, we find that there exist two known phenomena. One is\npublic hotspot, the other is personal hotspot. Motivated by this observation,\nwe present Hoten (hotspot and entropy), a novel routing metric to improve\nrouting performance in DTNs. First, we use the relative entropy between the\npublic hotspots and the personal hotspots to compute the centrality of nodes.\nThen we utilize the inverse symmetrized entropy of the personal hotspots\nbetween two nodes to compute the similarity between them. Third, we exploit the\nentropy of personal hotspots of a node to estimate its personality. Besides, we\npropose a method to ascertain the optimized size of hotspot. Finally, we\ncompare our routing strategy with other state-of-the-art routing schemes\nthrough extensive trace-driven simulations, the results show that Hoten largely\noutperforms other solutions, especially in terms of combined overhead/packet\ndelivery ratio and the average number of hops per message."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.2546v1", 
    "title": "CloudMonitor: Profiling Power Usage", 
    "arxiv-id": "1205.2546v1", 
    "author": "Ian Sommerville", 
    "publish": "2012-05-11T15:00:53Z", 
    "summary": "In Cloud Computing platforms the addition of hardware monitoring devices to\ngather power usage data can be impractical or uneconomical due to the large\nnumber of machines to be metered. CloudMonitor, a monitoring tool that can\ngenerate power models for software-based power estimation, can provide insights\nto the energy costs of deployments without additional hardware. Accurate power\nusage data leads to the possibility of Cloud providers creating a separate\ntariff for power and therefore incentivizing software developers to create\nenergy-efficient applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3247v1", 
    "title": "Tripod of Requirements in Horizontal Heterogeneous Mobile Cloud   Computing", 
    "arxiv-id": "1205.3247v1", 
    "author": "Rashid Hafeez Khokhar", 
    "publish": "2012-05-15T03:29:23Z", 
    "summary": "Recent trend of mobile computing is emerging toward executing\nresource-intensive applications in mobile devices regardless of underlying\nresource restrictions (e.g. limited processor and energy) that necessitate\nimminent technologies. Prosperity of cloud computing in stationary computers\nbreeds Mobile Cloud Computing (MCC) technology that aims to augment computing\nand storage capabilities of mobile devices besides conserving energy. However,\nMCC is more heterogeneous and unreliable (due to wireless connectivity) compare\nto cloud computing. Problems like variations in OS, data fragmentation, and\nsecurity and privacy discourage and decelerate implementation and pervasiveness\nof MCC. In this paper, we describe MCC as a horizontal heterogeneous ecosystem\nand identify thirteen critical metrics and approaches that influence on\nmobile-cloud solutions and success of MCC. We divide them into three major\nclasses, namely ubiquity, trust, and energy efficiency and devise a tripod of\nrequirements in MCC. Our proposed tripod shows that success of MCC is\nachievable by reducing mobility challenges (e.g. seamless connectivity,\nfragmentation), increasing trust, and enhancing energy efficiency."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3797v1", 
    "title": "Toward Sustainable Networking: Storage Area Networks with Network Coding", 
    "arxiv-id": "1205.3797v1", 
    "author": "Emina Soljanin", 
    "publish": "2012-05-16T20:15:49Z", 
    "summary": "This manuscript provides a model to characterize the energy savings of\nnetwork coded storage (NCS) in storage area networks (SANs). We consider\nblocking probability of drives as our measure of performance. A mapping\ntechnique to analyze SANs as independent M/G/K/K queues is presented, and\nblocking probabilities for uncoded storage schemes and NCS are derived and\ncompared. We show that coding operates differently than the amalgamation of\nfile chunks and energy savings are shown to scale well with striping number. We\nillustrate that for enterprise-level SANs energy savings of 20-50% can be\nrealized."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CLOUD.2012.112", 
    "link": "http://arxiv.org/pdf/1205.3809v1", 
    "title": "Graph Coloring Algorithms for Muti-core and Massively Multithreaded   Architectures", 
    "arxiv-id": "1205.3809v1", 
    "author": "Alex Pothen", 
    "publish": "2012-05-16T20:59:48Z", 
    "summary": "We explore the interplay between architectures and algorithm design in the\ncontext of shared-memory platforms and a specific graph problem of central\nimportance in scientific and high-performance computing, distance-1 graph\ncoloring. We introduce two different kinds of multithreaded heuristic\nalgorithms for the stated, NP-hard, problem. The first algorithm relies on\nspeculation and iteration, and is suitable for any shared-memory system. The\nsecond algorithm uses dataflow principles, and is targeted at the\nnon-conventional, massively multithreaded Cray XMT system. We study the\nperformance of the algorithms on the Cray XMT and two multi-core systems, Sun\nNiagara 2 and Intel Nehalem. Together, the three systems represent a spectrum\nof multithreading capabilities and memory structure. As testbed, we use\nsynthetically generated large-scale graphs carefully chosen to cover a wide\nrange of input types. The results show that the algorithms have scalable\nruntime performance and use nearly the same number of colors as the underlying\nserial algorithm, which in turn is effective in practice. The study provides\ninsight into the design of high performance algorithms for irregular problems\non many-core architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.3830v2", 
    "title": "Parallel implementation of fast randomized algorithms for the   decomposition of low rank matrices", 
    "arxiv-id": "1205.3830v2", 
    "author": "John Feo", 
    "publish": "2012-05-16T23:41:33Z", 
    "summary": "We analyze the parallel performance of randomized interpolative decomposition\nby decomposing low rank complex-valued Gaussian random matrices up to 64 GB. We\nchose a Cray XMT supercomputer as it provides an almost ideal PRAM model\npermitting quick investigation of parallel algorithms without obfuscation from\nhardware idiosyncrasies. We obtain that on non-square matrices performance\nbecomes very good, with overall runtime over 70 times faster on 128 processors.\nWe also verify that numerically discovered error bounds still hold on matrices\nnearly two orders of magnitude larger than those previously tested."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4545v1", 
    "title": "Memory Lower Bounds for Randomized Collaborative Search and Applications   to Biology", 
    "arxiv-id": "1205.4545v1", 
    "author": "Amos Korman", 
    "publish": "2012-05-21T09:52:57Z", 
    "summary": "Initial knowledge regarding group size can be crucial for collective\nperformance. We study this relation in the context of the {\\em Ants Nearby\nTreasure Search (ANTS)} problem \\cite{FKLS}, which models natural cooperative\nforaging behavior such as that performed by ants around their nest. In this\nproblem, $k$ (probabilistic) agents, initially placed at some central location,\ncollectively search for a treasure on the two-dimensional grid. The treasure is\nplaced at a target location by an adversary and the goal is to find it as fast\nas possible as a function of both $k$ and $D$, where $D$ is the (unknown)\ndistance between the central location and the target. It is easy to see that\n$T=\\Omega(D+D^2/k)$ time units are necessary for finding the treasure.\nRecently, it has been established that $O(T)$ time is sufficient if the agents\nknow their total number $k$ (or a constant approximation of it), and enough\nmemory bits are available at their disposal \\cite{FKLS}. In this paper, we\nestablish lower bounds on the agent memory size required for achieving certain\nrunning time performances. To the best our knowledge, these bounds are the\nfirst non-trivial lower bounds for the memory size of probabilistic searchers.\nFor example, for every given positive constant $\\epsilon$, terminating the\nsearch by time $O(\\log^{1-\\epsilon} k \\cdot T)$ requires agents to use\n$\\Omega(\\log\\log k)$ memory bits. Such distributed computing bounds may provide\na novel, strong tool for the investigation of complex biological systems."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4809v1", 
    "title": "Iterative Approximate Byzantine Consensus under a Generalized Fault   Model", 
    "arxiv-id": "1205.4809v1", 
    "author": "Nitin Vaidya", 
    "publish": "2012-05-22T05:43:53Z", 
    "summary": "In this work, we consider a generalized fault model that can be used to\nrepresent a wide range of failure scenarios, including correlated failures and\nnon-uniform node reliabilities. This fault model is general in the sense that\nfault models studied in prior related work, such as f -total and f -local\nmodels, are special cases of the generalized fault model. Under the generalized\nfault model, we explore iterative approximate Byzantine consensus (IABC)\nalgorithms in arbitrary directed networks. We prove a necessary and sufficient\ncondition for the existence of IABC algorithms. The use of the generalized\nfault model helps to gain a better understanding of IABC algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.4883v1", 
    "title": "Hybrid Parallel Bidirectional Sieve based on SMP Cluster", 
    "arxiv-id": "1205.4883v1", 
    "author": "Lei Liu", 
    "publish": "2012-05-22T11:27:10Z", 
    "summary": "In this article, hybrid parallel bidirectional sieve method is implemented by\nSMP Cluster, the individual computational units joined together by the\ncommunication network, are usually shared-memory systems with one or more\nmulticore processor. To high-efficiency optimization, we propose average divide\ndata into nodes, generating double-ended queues (deque) for sieve method that\nare able to exploit dual-cores simultaneously start sifting out primes from the\nhead and tail.And each node create a FIFO queue as dynamic data buffer to ache\ntemporary data from another nodes send to. The approach obtains huge speedup\nand efficiency on SMP Cluster."
},{
    "category": "cs.DC", 
    "doi": "10.1142/S0129626414500042", 
    "link": "http://arxiv.org/pdf/1205.5055v1", 
    "title": "Neutron Star Evolutions using Tabulated Equations of State with a New   Execution Model", 
    "arxiv-id": "1205.5055v1", 
    "author": "Thomas Sterling", 
    "publish": "2012-05-22T20:46:11Z", 
    "summary": "The addition of nuclear and neutrino physics to general relativistic fluid\ncodes allows for a more realistic description of hot nuclear matter in neutron\nstar and black hole systems. This additional microphysics requires that each\nprocessor have access to large tables of data, such as equations of state, and\nin large simulations the memory required to store these tables locally can\nbecome excessive unless an alternative execution model is used. In this work we\npresent relativistic fluid evolutions of a neutron star obtained using a\nmessage driven multi-threaded execution model known as ParalleX. These neutron\nstar simulations would require substantial memory overhead dedicated entirely\nto the equation of state table if using a more traditional execution model. We\nintroduce a ParalleX component based on Futures for accessing large tables of\ndata, including out-of-core sized tables, which does not require substantial\nmemory overhead and effectively hides any increased network latency."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1205.6249v1", 
    "title": "Leader Election for Anonymous Asynchronous Agents in Arbitrary Networks", 
    "arxiv-id": "1205.6249v1", 
    "author": "Andrzej Pelc", 
    "publish": "2012-05-29T02:27:35Z", 
    "summary": "We study the problem of leader election among mobile agents operating in an\narbitrary network modeled as an undirected graph. Nodes of the network are\nunlabeled and all agents are identical. Hence the only way to elect a leader\namong agents is by exploiting asymmetries in their initial positions in the\ngraph. Agents do not know the graph or their positions in it, hence they must\ngain this knowledge by navigating in the graph and share it with other agents\nto accomplish leader election. This can be done using meetings of agents, which\nis difficult because of their asynchronous nature: an adversary has total\ncontrol over the speed of agents. When can a leader be elected in this\nadversarial scenario and how to do it? We give a complete answer to this\nquestion by characterizing all initial configurations for which leader election\nis possible and by constructing an algorithm that accomplishes leader election\nfor all configurations for which this can be done."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1206.0089v1", 
    "title": "Reaching Approximate Byzantine Consensus in Partially-Connected Mobile   Networks", 
    "arxiv-id": "1206.0089v1", 
    "author": "Yun Wang", 
    "publish": "2012-06-01T06:14:29Z", 
    "summary": "We consider the problem of approximate consensus in mobile networks\ncontaining Byzantine nodes. We assume that each correct node can communicate\nonly with its neighbors and has no knowledge of the global topology. As all\nnodes have moving ability, the topology is dynamic. The number of Byzantine\nnodes is bounded by f and known by all correct nodes. We first introduce an\napproximate Byzantine consensus protocol which is based on the linear iteration\nmethod. As nodes are allowed to collect information during several consecutive\nrounds, moving gives them the opportunity to gather more values. We propose a\nnovel sufficient and necessary condition to guarantee the final convergence of\nthe consensus protocol. The requirement expressed by our condition is not\n\"universal\": in each phase it affects only a single correct node. More\nprecisely, at least one correct node among those that propose either the\nminimum or the maximum value which is present in the network, has to receive\nenough messages (quantity constraint) with either higher or lower values\n(quality constraint). Of course, nodes' motion should not prevent this\nrequirement to be fulfilled. Our conclusion shows that the proposed condition\ncan be satisfied if the total number of nodes is greater than 3f+1."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s00446-013-0196-x", 
    "link": "http://arxiv.org/pdf/1206.0115v1", 
    "title": "Pipelining the Fast Multipole Method over a Runtime System", 
    "arxiv-id": "1206.0115v1", 
    "author": "Takahashi Toru", 
    "publish": "2012-06-01T08:05:39Z", 
    "summary": "Fast Multipole Methods (FMM) are a fundamental operation for the simulation\nof many physical problems. The high performance design of such methods usually\nrequires to carefully tune the algorithm for both the targeted physics and the\nhardware. In this paper, we propose a new approach that achieves high\nperformance across architectures. Our method consists of expressing the FMM\nalgorithm as a task flow and employing a state-of-the-art runtime system,\nStarPU, in order to process the tasks on the different processing units. We\ncarefully design the task flow, the mathematical operators, their Central\nProcessing Unit (CPU) and Graphics Processing Unit (GPU) implementations, as\nwell as scheduling schemes. We compute potentials and forces of 200 million\nparticles in 48.7 seconds on a homogeneous 160 cores SGI Altix UV 100 and of 38\nmillion particles in 13.34 seconds on a heterogeneous 12 cores Intel Nehalem\nprocessor enhanced with 3 Nvidia M2090 Fermi GPUs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2011.1203", 
    "link": "http://arxiv.org/pdf/1206.0419v1", 
    "title": "Pre-allocation Strategies of Computational Resources in Cloud Computing   using Adaptive Resonance Theory-2", 
    "arxiv-id": "1206.0419v1", 
    "author": "P Jayarekha", 
    "publish": "2012-06-03T04:42:23Z", 
    "summary": "One of the major challenges of cloud computing is the management of\nrequest-response coupling and optimal allocation strategies of computational\nresources for the various types of service requests. In the normal situations\nthe intelligence required to classify the nature and order of the request using\nstandard methods is insufficient because the arrival of request is at a random\nfashion and it is meant for multiple resources with different priority order\nand variety. Hence, it becomes absolutely essential that we identify the trends\nof different request streams in every category by auto classifications and\norganize preallocation strategies in a predictive way. It calls for designs of\nintelligent modes of interaction between the client request and cloud computing\nresource manager. This paper discusses about the corresponding scheme using\nAdaptive Resonance Theory-2."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijccsa.2011.1203", 
    "link": "http://arxiv.org/pdf/1206.0988v1", 
    "title": "Virtualization Implementation Model for Cost Effective & Efficient Data   Centers", 
    "arxiv-id": "1206.0988v1", 
    "author": "Azizah Abdul Rahman", 
    "publish": "2012-04-07T05:19:32Z", 
    "summary": "Data centers form a key part of the infrastructure upon which a variety of\ninformation technology services are built. They provide the capabilities of\ncentralized repository for storage, management, networking and dissemination of\ndata. With the rapid increase in the capacity and size of data centers, there\nis a continuous increase in the demand for energy consumption. These data\ncenters not only consume a tremendous amount of energy but are riddled with IT\ninefficiencies. Data center are plagued with thousands of servers as major\ncomponents. These servers consume huge energy without performing useful work.\nIn an average server environment, 30% of the servers are \"dead\" only consuming\nenergy, without being properly utilized. This paper proposes a five step model\nusing an emerging technology called virtualization to achieve energy efficient\ndata centers. The proposed model helps Data Center managers to properly\nimplement virtualization technology in their data centers to make them green\nand energy efficient so as to ensure that IT infrastructure contributes as\nlittle as possible to the emission of greenhouse gases, and helps to regain\npower and cooling capacity, recapture resilience and dramatically reducing\nenergy costs and total cost of ownership."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.0992v6", 
    "title": "Finite-time Convergent Gossiping", 
    "arxiv-id": "1206.0992v6", 
    "author": "Karl Henrik Johansson", 
    "publish": "2012-06-05T17:07:34Z", 
    "summary": "Gossip algorithms are widely used in modern distributed systems, with\napplications ranging from sensor networks and peer-to-peer networks to mobile\nvehicle networks and social networks. A tremendous research effort has been\ndevoted to analyzing and improving the asymptotic rate of convergence for\ngossip algorithms. In this work we study finite-time convergence of\ndeterministic gossiping. We show that there exists a symmetric gossip algorithm\nthat converges in finite time if and only if the number of network nodes is a\npower of two, while there always exists an asymmetric gossip algorithm with\nfinite-time convergence, independent of the number of nodes. For $n=2^m$ nodes,\nwe prove that a fastest convergence can be reached in $nm=n\\log_2 n$ node\nupdates via symmetric gossiping. On the other hand, under asymmetric gossip\namong $n=2^m+r$ nodes with $0\\leq r<2^m$, it takes at least $mn+2r$ node\nupdates for achieving finite-time convergence. It is also shown that the\nexistence of finite-time convergent gossiping often imposes strong structural\nrequirements on the underlying interaction graph. Finally, we apply our results\nto gossip algorithms in quantum networks, where the goal is to control the\nstate of a quantum system via pairwise interactions. We show that finite-time\nconvergence is never possible for such systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1118v1", 
    "title": "Research On Mobile Cloud Computing: Review, Trend, And Perspectives", 
    "arxiv-id": "1206.1118v1", 
    "author": "Abdullah Gani", 
    "publish": "2012-06-06T03:53:39Z", 
    "summary": "Mobile Cloud Computing (MCC) which combines mobile computing and cloud\ncomputing, has become one of the industry buzz words and a major discussion\nthread in the IT world since 2009. As MCC is still at the early stage of\ndevelopment, it is necessary to grasp a thorough understanding of the\ntechnology in order to point out the direction of future research. With the\nlatter aim, this paper presents a review on the background and principle of\nMCC, characteristics, recent research work, and future research trends. A brief\naccount on the background of MCC: from mobile computing to cloud computing is\npresented and then followed with a discussion on characteristics and recent\nresearch work. It then analyses the features and infrastructure of mobile cloud\ncomputing. The rest of the paper analyses the challenges of mobile cloud\ncomputing, summary of some research projects related to this area, and points\nout promising future research directions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1290v1", 
    "title": "Causality, Influence, and Computation in Possibly Disconnected Dynamic   Networks", 
    "arxiv-id": "1206.1290v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2012-06-06T18:10:34Z", 
    "summary": "In this work, we study the propagation of influence and computation in\ndynamic distributed systems. We focus on broadcasting models under a worst-case\ndynamicity assumption which have received much attention recently. We drop for\nthe first time in worst-case dynamic networks the common instantaneous\nconnectivity assumption and require a minimal temporal connectivity. Our\ntemporal connectivity constraint only requires that another causal influence\noccurs within every time-window of some given length. We establish that there\nare dynamic graphs with always disconnected instances with equivalent temporal\nconnectivity to those with always connected instances. We present a termination\ncriterion and also establish the computational equivalence with instantaneous\nconnectivity networks. We then consider another model of dynamic networks in\nwhich each node has an underlying communication neighborhood and the\nrequirement is that each node covers its local neighborhood within any\ntime-window of some given length. We discuss several properties and provide a\nprotocol for counting, that is for determining the number of nodes in the\nnetwork."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1653v2", 
    "title": "PriSM: A Private Social Mesh for Leveraging Social Networking at   Workplace", 
    "arxiv-id": "1206.1653v2", 
    "author": "Anwitaman Datta", 
    "publish": "2012-06-08T02:44:05Z", 
    "summary": "In this work we describe the PriSM framework for decentralized deployment of\na federation of autonomous social networks (ASN). The individual ASNs are\ncentrally managed by organizations according to their institutional needs,\nwhile cross-ASN interactions are facilitated subject to security and\nconfidentiality requirements specified by administrators and users of the ASNs.\nSuch decentralized deployment, possibly either on private or public clouds,\nprovides control and ownership of information/flow to individual organizations.\nLack of such complete control (if third party online social networking services\nwere to be used) has so far been a great barrier in taking full advantage of\nthe novel communication mechanisms at workplace that have however become\ncommonplace for personal usage with the advent of Web 2.0 platforms and online\nsocial networks. PriSM provides a practical solution for organizations to\nharness the advantages of online social networking both in\nintra/inter-organizational settings without sacrificing autonomy, security and\nconfidentiality needs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1899v3", 
    "title": "TTMA: Traffic-adaptive Time-division Multiple Access Protocol Wireless   Sensor Networks", 
    "arxiv-id": "1206.1899v3", 
    "author": "Rajeev K. Shakya", 
    "publish": "2012-06-09T02:06:59Z", 
    "summary": "This paper has been withdrawn by arXiv. arXiv admin note: author list\ntruncated due to disputed authorship and content. This submission repeats large\nportions of text from this http URL by other authors. Duty cycle mode in WSN\nimproves energy-efficiency, but also introduces packet delivery latency.\nSeveral duty-cycle based MAC schemes have been proposed to reduce latency, but\nthroughput is limited by duty-cycled scheduling performance. In this paper, a\nTraffic-adaptive Time-division Multiple Access (TTMA), a distributed TDMA-based\nMAC protocol is introduced to improves the throughput by traffic-adaptive\ntime-slot scheduling that increases the channel utilisation efficiency. The\nproposed time-slot scheduling method first avoids time-slots assigned to nodes\nwith no traffic through fast traffic notification. It then achieves better\nchannel utilisation among nodes having traffic through an ordered schedule\nnegotiation scheme. By decomposing traffic notification and data transmission\nscheduling into two phases leads each phase to be simple and efficient. The\nperformance evaluation shows that the two-phase design significantly improves\nthe throughput and outperforms the time division multiple access (TDMA) control\nwith slot stealing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.1984v1", 
    "title": "Energy-Aware Scheduling using Dynamic Voltage-Frequency Scaling", 
    "arxiv-id": "1206.1984v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-06-10T00:13:47Z", 
    "summary": "The energy consumption issue in distributed computing systems has become\nquite critical due to environmental concerns. In response to this, many\nenergy-aware scheduling algorithms have been developed primarily by using the\ndynamic voltage-frequency scaling (DVFS) capability incorporated in recent\ncommodity processors. The majority of these algorithms involve two passes:\nschedule generation and slack reclamation. The latter is typically achieved by\nlowering processor frequency for tasks with slacks. In this article, we study\nthe latest papers in this area and develop them. This study has been evaluated\nbased on results obtained from experiments with 1,500 randomly generated task\ngraphs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2015.2484345", 
    "link": "http://arxiv.org/pdf/1206.2187v1", 
    "title": "An Empirical Study of the Repair Performance of Novel Coding Schemes for   Networked Distributed Storage Systems", 
    "arxiv-id": "1206.2187v1", 
    "author": "Anwitaman Datta", 
    "publish": "2012-06-11T12:55:53Z", 
    "summary": "Erasure coding techniques are getting integrated in networked distributed\nstorage systems as a way to provide fault-tolerance at the cost of less storage\noverhead than traditional replication. Redundancy is maintained over time\nthrough repair mechanisms, which may entail large network resource overheads.\nIn recent years, several novel codes tailor-made for distributed storage have\nbeen proposed to optimize storage overhead and repair, such as Regenerating\nCodes that minimize the per repair traffic, or Self-Repairing Codes which\nminimize the number of nodes contacted per repair. Existing studies of these\ncoding techniques are however predominantly theoretical, under the simplifying\nassumption that only one object is stored. They ignore many practical issues\nthat real systems must address, such as data placement, de/correlation of\nmultiple stored objects, or the competition for limited network resources when\nmultiple objects are repaired simultaneously. This paper empirically studies\nthe repair performance of these novel storage centric codes with respect to\nclassical erasure codes by simulating realistic scenarios and exploring the\ninterplay of code parameters, failure characteristics and data placement with\nrespect to the trade-offs of bandwidth usage and speed of repairs."
},{
    "category": "cs.DC", 
    "doi": "10.4108/icst.simutools.2012.247736", 
    "link": "http://arxiv.org/pdf/1206.2772v3", 
    "title": "Time Warp on the Go (Updated Version)", 
    "arxiv-id": "1206.2772v3", 
    "author": "Moreno Marzolla", 
    "publish": "2012-06-13T11:48:48Z", 
    "summary": "In this paper we deal with the impact of multi and many-core processor\narchitectures on simulation. Despite the fact that modern CPUs have an\nincreasingly large number of cores, most softwares are still unable to take\nadvantage of them. In the last years, many tools, programming languages and\ngeneral methodologies have been proposed to help building scalable applications\nfor multi-core architectures, but those solutions are somewhat limited.\nParallel and distributed simulation is an interesting application area in which\nefficient and scalable multi-core implementations would be desirable. In this\npaper we investigate the use of the Go Programming Language to implement\noptimistic parallel simulations based on the Time Warp mechanism. Specifically,\nwe describe the design, implementation and evaluation of a new parallel\nsimulator. The scalability of the simulator is studied when in presence of a\nmodern multi-core CPU and the effects of the Hyper-Threading technology on\noptimistic simulation are analyzed."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.2775v3", 
    "title": "Parallel Discrete Event Simulation with Erlang", 
    "arxiv-id": "1206.2775v3", 
    "author": "Moreno Marzolla", 
    "publish": "2012-06-13T12:12:21Z", 
    "summary": "Discrete Event Simulation (DES) is a widely used technique in which the state\nof the simulator is updated by events happening at discrete points in time\n(hence the name). DES is used to model and analyze many kinds of systems,\nincluding computer architectures, communication networks, street traffic, and\nothers. Parallel and Distributed Simulation (PADS) aims at improving the\nefficiency of DES by partitioning the simulation model across multiple\nprocessing elements, in order to enabling larger and/or more detailed studies\nto be carried out. The interest on PADS is increasing since the widespread\navailability of multicore processors and affordable high performance computing\nclusters. However, designing parallel simulation models requires considerable\nexpertise, the result being that PADS techniques are not as widespread as they\ncould be. In this paper we describe ErlangTW, a parallel simulation middleware\nbased on the Time Warp synchronization protocol. ErlangTW is entirely written\nin Erlang, a concurrent, functional programming language specifically targeted\nat building distributed systems. We argue that writing parallel simulation\nmodels in Erlang is considerably easier than using conventional programming\nlanguages. Moreover, ErlangTW allows simulation models to be executed either on\nsingle-core, multicore and distributed computing architectures. We describe the\ndesign and prototype implementation of ErlangTW, and report some preliminary\nperformance results on multicore and distributed architectures using the well\nknown PHOLD benchmark."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.4175v1", 
    "title": "Clustered Network Coding for Maintenance in Practical Storage Systems", 
    "arxiv-id": "1206.4175v1", 
    "author": "Alexandre van Kempen", 
    "publish": "2012-06-19T11:01:42Z", 
    "summary": "Classical erasure codes, e.g. Reed-Solomon codes, have been acknowledged as\nan efficient alternative to plain replication to reduce the storage overhead in\nreliable distributed storage systems. Yet, such codes experience high overhead\nduring the maintenance process. In this paper we propose a novel erasure-coded\nframework especially tailored for networked storage systems. Our approach\nrelies on the use of random codes coupled with a clustered placement strategy,\nenabling the maintenance of a failed machine at the granularity of multiple\nfiles. Our repair protocol leverages network coding techniques to reduce by\nhalf the amount of data transferred during maintenance, as several files can be\nrepaired simultaneously. This approach, as formally proven and demonstrated by\nour evaluation on a public experimental testbed, enables to dramatically\ndecrease the bandwidth overhead during the maintenance process, as well as the\ntime to repair a failure. In addition, the implementation is made as simple as\npossible, aiming at a deployment into practical systems."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.5938v1", 
    "title": "Performance Evaluation of Ant-Based Routing Protocols for Wireless   Sensor Networks", 
    "arxiv-id": "1206.5938v1", 
    "author": "Kah Phooi Seng", 
    "publish": "2012-06-26T09:27:57Z", 
    "summary": "High efficient routing is an important issue in the design of limited energy\nresource Wireless Sensor Networks (WSNs). Due to the characteristic of the\nenvironment at which the sensor node is to operate, coupled with severe\nresources; on-board energy, transmission power, processing capability, and\nstorage limitations, prompt for careful resource management and new routing\nprotocol so as to counteract the differences and challenges. To this end, we\npresent an Improved Energy-Efficient Ant-Based Routing (IEEABR) Algorithm in\nwireless sensor networks. Compared to the state-of-the-art Ant-Based routing\nprotocols; Basic Ant-Based Routing (BABR) Algorithm, Sensor-driven and\nCost-aware ant routing (SC), Flooded Forward ant routing (FF), Flooded\nPiggybacked ant routing (FP), and Energy-Efficient Ant-Based Routing (EEABR),\nthe proposed IEEABR approach has advantages in terms of reduced energy usage\nwhich can effectively balance the WSN node's power consumption, and high energy\nefficiency. The performance evaluations for the algorithms on a real\napplication are conducted in a well known WSN MATLAB-based simulator (RMASE)\nusing both static and dynamic scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6016v1", 
    "title": "Cloud Infrastructure Service Management - A Review", 
    "arxiv-id": "1206.6016v1", 
    "author": "A. Anasuya Threse Innocent", 
    "publish": "2012-05-30T09:45:54Z", 
    "summary": "The new era of computing called Cloud Computing allows the user to access the\ncloud services dynamically over the Internet wherever and whenever needed.\nCloud consists of data and resources; and the cloud services include the\ndelivery of software, infrastructure, applications, and storage over the\nInternet based on user demand through Internet. In short, cloud computing is a\nbusiness and economic model allowing the users to utilize high-end computing\nand storage virtually with minimal infrastructure on their end. Cloud has three\nservice models namely, Cloud Software-as-a-Service (SaaS), Cloud\nPlatform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS).\nThis paper talks in depth of cloud infrastructure service management."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6207v1", 
    "title": "An Optimal Fully Distributed Algorithm to Minimize the Resource   Consumption of Cloud Applications", 
    "arxiv-id": "1206.6207v1", 
    "author": "Jue Hong", 
    "publish": "2012-06-27T09:02:05Z", 
    "summary": "According to the pay-per-use model adopted in clouds, the more the resources\nconsumed by an application running in a cloud computing environment, the\ngreater the amount of money the owner of the corresponding application will be\ncharged. Therefore, applying intelligent solutions to minimize the resource\nconsumption is of great importance. Because centralized solutions are deemed\nunsuitable for large-distributed systems or large-scale applications, we\npropose a fully distributed algorithm (called DRA) to overcome the scalability\nissues. Specifically, DRA migrates the inter-communicating components of an\napplication, such as processes or virtual machines, close to each other to\nminimize the total resource consumption. The migration decisions are made in a\ndynamic way and based only on local information. We prove that DRA achieves\nconvergence and results always in the optimal solution."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1206.6225v3", 
    "title": "Virtual Machine Migration: A Resource Intensive Outsourcing Mechanism   for Mobile Cloud Computing", 
    "arxiv-id": "1206.6225v3", 
    "author": "Abdullah Gani", 
    "publish": "2012-06-27T10:23:14Z", 
    "summary": "In Mobile Cloud Computing (MCC), Virtual Machine (VM) migration based process\noffloading is a dominant approach to enhance Smart Mobile Devices (SMDs). A\nchallenging aspect of VM deployment is the additional computing resources usage\nin the deployment and management of VM which obliges computing resources for VM\ncreation and configuration. The management of VM comprises computing resources\nexploitation in the monitoring of VM in entire lifecycle and physical resources\nmanagement for VM on SMDs. Therefore, VM migration based application offloading\nrequires additional computing resource. Consequently computing resources demand\nand execution time of the application increases respectively. In this paper, we\nempirically review the impact of VM deployment and management on the execution\ntime of application in diverse scenarios. We investigate VM deployment and\nmanagement for application processing in simulation environment by employing\nCloudSim: a simulation toolkit that provides an extensible simulation framework\nto model VM deployment and management for application processing in cloud\ninfrastructure. The significance of this work is to ensure that VM deployment\nand management necessitates additional computing resources on SMD for\napplication offloading. We evaluate VM deployment and management in application\nprocessing by analyzing Key Performance Parameters (KPPs) in different\nscenarios; such as VM deployment, the execution time of applications, and total\nexecution time of the simulation. We use KPPs to assess deviations in the\nresults of diverse experimental scenarios. The empirical analysis concludes\nthat VM deployment and management oblige additional resources on computing host\nwhich make it a heavyweight approach for process offloading on smart mobile\ndevice."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0180v1", 
    "title": "Naming and Counting in Anonymous Unknown Dynamic Networks", 
    "arxiv-id": "1208.0180v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2012-08-01T12:00:17Z", 
    "summary": "In this work, we study the fundamental naming and counting problems (and some\nvariations) in networks that are anonymous, unknown, and possibly dynamic. In\ncounting, nodes must determine the size of the network n and in naming they\nmust end up with unique identities. By anonymous we mean that all nodes begin\nfrom identical states apart possibly from a unique leader node and by unknown\nthat nodes have no a priori knowledge of the network (apart from some minimal\nknowledge when necessary) including ignorance of n. Network dynamicity is\nmodeled by the 1-interval connectivity model, in which communication is\nsynchronous and a worst-case adversary chooses the edges of every round subject\nto the condition that each instance is connected. We first focus on static\nnetworks with broadcast where we prove that, without a leader, counting is\nimpossible to solve and that naming is impossible to solve even with a leader\nand even if nodes know n. These impossibilities carry over to dynamic networks\nas well. We also show that a unique leader suffices in order to solve counting\nin linear time. Then we focus on dynamic networks with broadcast. We conjecture\nthat dynamicity renders nontrivial computation impossible. In view of this, we\nlet the nodes know an upper bound on the maximum degree that will ever appear\nand show that in this case the nodes can obtain an upper bound on n. Finally,\nwe replace broadcast with one-to-each, in which a node may send a different\nmessage to each of its neighbors. Interestingly, this natural variation is\nproved to be computationally equivalent to a full-knowledge model, in which\nunique names exist and the size of the network is known."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0615v2", 
    "title": "Enumerating Subgraph Instances Using Map-Reduce", 
    "arxiv-id": "1208.0615v2", 
    "author": "Jeffrey D. Ullman", 
    "publish": "2012-08-02T20:56:39Z", 
    "summary": "The theme of this paper is how to find all instances of a given \"sample\"\ngraph in a larger \"data graph,\" using a single round of map-reduce. For the\nsimplest sample graph, the triangle, we improve upon the best known such\nalgorithm. We then examine the general case, considering both the communication\ncost between mappers and reducers and the total computation cost at the\nreducers. To minimize communication cost, we exploit the techniques of (Afrati\nand Ullman, TKDE 2011)for computing multiway joins (evaluating conjunctive\nqueries) in a single map-reduce round. Several methods are shown for\ntranslating sample graphs into a union of conjunctive queries with as few\nqueries as possible. We also address the matter of optimizing computation cost.\nMany serial algorithms are shown to be \"convertible,\" in the sense that it is\npossible to partition the data graph, explore each partition in a separate\nreducer, and have the total computation cost at the reducers be of the same\norder as the computation cost of the serial algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.0712v2", 
    "title": "Description of the Chord Protocol using ASMs Formalism", 
    "arxiv-id": "1208.0712v2", 
    "author": "Zoran Ognjanovi\u0107", 
    "publish": "2012-08-03T11:17:57Z", 
    "summary": "This paper describes the overlay protocol Chord using the formalism of\nAbstract State Machines. The formalization concerns Chord actions that maintain\nring topology and manipulate distributed keys. We define a class of runs and\nprove the correctness of our formalization with respect to it."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2364474.2364487", 
    "link": "http://arxiv.org/pdf/1208.1793v2", 
    "title": "Real-time Data Collection Scheduling in Multi-hop Wireless Sensor   Networks", 
    "arxiv-id": "1208.1793v2", 
    "author": "Min Song", 
    "publish": "2012-08-08T22:47:10Z", 
    "summary": "We study real time periodic query scheduling for data collection in multihop\nWireless Sensor Networks (WSNs). Given a set of heterogenous data collection\nqueries in WSNs, each query requires the data from the source sensor nodes to\nbe collected to the control center within a certain end-to-end delay. We first\npropose almost-tight necessary conditions for a set of different queries to be\nschedulable by a WSN. We then develop a family of efficient and effective data\ncollection algorithms that can meet the real-time requirement under resource\nconstraints by addressing three tightly coupled tasks: (1) routing tree\nconstruction for data collection, (2) link activity scheduling, and (3)\npacket-level scheduling. Our theoretical analysis for the schedulability of\nthese algorithms show that they can achieve a constant fraction of the maximum\nschedulable load. For the case of overloaded networks where not all queries can\nbe possibly satisfied, we propose an efficient approximation algorithm to\nselect queries to maximize the total weight of selected schedulable queries.\nThe simulations corroborate our theoretical analysis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3412", 
    "link": "http://arxiv.org/pdf/1208.1922v1", 
    "title": "Heuristic based task scheduling in multiprocessor systems with genetic   algorithm by choosing the eligible processor", 
    "arxiv-id": "1208.1922v1", 
    "author": "Nishita Das", 
    "publish": "2012-08-09T14:34:20Z", 
    "summary": "In multiprocessor systems, one of the main factors of systems' performance is\ntask scheduling. The well the task be distributed among the processors the well\nbe the performance. Again finding the optimal solution of scheduling the tasks\ninto the processors is NP-complete, that is, it will take a lot of time to find\nthe optimal solution. Many evolutionary algorithms (e.g. Genetic Algorithm,\nSimulated annealing) are used to reach the near optimal solution in linear\ntime. In this paper we propose a heuristic for genetic algorithm based task\nscheduling in multiprocessor systems by choosing the eligible processor on\neducated guess. From comparison it is found that this new heuristic based GA\ntakes less computation time to reach the suboptimal solution."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.1942v1", 
    "title": "Scheduling Data Intensive Workloads through Virtualization on MapReduce   based Clouds", 
    "arxiv-id": "1208.1942v1", 
    "author": "L. S. S. Reddy", 
    "publish": "2012-08-09T15:07:43Z", 
    "summary": "MapReduce has become a popular programming model for running data intensive\napplications on the cloud. Completion time goals or deadlines of MapReduce jobs\nset by users are becoming crucial in existing cloud-based data processing\nenvironments like Hadoop. There is a conflict between the scheduling MR jobs to\nmeet deadlines and \"data locality\" (assigning tasks to nodes that contain their\ninput data). To meet the deadline a task may be scheduled on a node without\nlocal input data for that task causing expensive data transfer from a remote\nnode. In this paper, a novel scheduler is proposed to address the above problem\nwhich is primarily based on the dynamic resource reconfiguration approach. It\nhas two components: 1) Resource Predictor: which dynamically determines the\nrequired number of Map/Reduce slots for every job to meet completion time\nguarantee; 2) Resource Reconfigurator: that adjusts the CPU resources while not\nviolating completion time goals of the users by dynamically increasing or\ndecreasing individual VMs to maximize data locality and also to maximize the\nuse of resources within the system among the active jobs. The proposed\nscheduler has been evaluated against Fair Scheduler on virtual cluster built on\na physical cluster of 20 machines. The results demonstrate a gain of about 12%\nincrease in throughput of Jobs"
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.1975v2", 
    "title": "Block-Relaxation Methods for 3D Constant-Coefficient Stencils on GPUs   and Multicore CPUs", 
    "arxiv-id": "1208.1975v2", 
    "author": "Mark Berrill", 
    "publish": "2012-08-09T17:29:16Z", 
    "summary": "Block iterative methods are extremely important as smoothers for multigrid\nmethods, as preconditioners for Krylov methods, and as solvers for diagonally\ndominant linear systems. Developing robust and efficient algorithms suitable\nfor current and evolving GPU and multicore CPU systems is a significant\nchallenge. We address this issue in the case of constant-coefficient stencils\narising in the solution of elliptic partial differential equations on\nstructured 3D uniform and adaptively refined grids. Robust, highly parallel\nimplementations of block Jacobi and chaotic block Gauss-Seidel algorithms with\nexact inversion of the blocks are developed using different parallelization\ntechniques. Experimental results for NVIDIA Fermi GPUs and AMD multicore\nsystems are presented."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2649v1", 
    "title": "Survey and Analysis of Production Distributed Computing Infrastructures", 
    "arxiv-id": "1208.2649v1", 
    "author": "Jon Weissman", 
    "publish": "2012-08-13T18:00:09Z", 
    "summary": "This report has two objectives. First, we describe a set of the production\ndistributed infrastructures currently available, so that the reader has a basic\nunderstanding of them. This includes explaining why each infrastructure was\ncreated and made available and how it has succeeded and failed. The set is not\ncomplete, but we believe it is representative.\n  Second, we describe the infrastructures in terms of their use, which is a\ncombination of how they were designed to be used and how users have found ways\nto use them. Applications are often designed and created with specific\ninfrastructures in mind, with both an appreciation of the existing capabilities\nprovided by those infrastructures and an anticipation of their future\ncapabilities. Here, the infrastructures we discuss were often designed and\ncreated with specific applications in mind, or at least specific types of\napplications. The reader should understand how the interplay between the\ninfrastructure providers and the users leads to such usages, which we call\nusage modalities. These usage modalities are really abstractions that exist\nbetween the infrastructures and the applications; they influence the\ninfrastructures by representing the applications, and they influence the ap-\nplications by representing the infrastructures."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2675v1", 
    "title": "A GPU implementation of the Simulated Annealing Heuristic for the   Quadratic Assignment Problem", 
    "arxiv-id": "1208.2675v1", 
    "author": "Gerald Paul", 
    "publish": "2012-08-13T19:30:44Z", 
    "summary": "The quadratic assignment problem (QAP) is one of the most difficult\ncombinatorial optimization problems. An effective heuristic for obtaining\napproximate solutions to the QAP is simulated annealing (SA). Here we describe\nan SA implementation for the QAP which runs on a graphics processing unit\n(GPU). GPUs are composed of low cost commodity graphics chips which in\ncombination provide a powerful platform for general purpose parallel computing.\nFor SA runs with large numbers of iterations, we find performance 50-100 times\nbetter than that of a recent non-parallel but very efficient implementation of\nSA for the QAP"
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.2849v1", 
    "title": "Mapping Strategies for the PERCS Architecture", 
    "arxiv-id": "1208.2849v1", 
    "author": "Yogish Sabharwal", 
    "publish": "2012-08-14T12:53:24Z", 
    "summary": "The PERCS system was designed by IBM in response to a DARPA challenge that\ncalled for a high-productivity high-performance computing system. The IBM PERCS\narchitecture is a two level direct network having low diameter and high\nbisection bandwidth. Mapping and routing strategies play an important role in\nthe performance of applications on such a topology. In this paper, we study\nmapping strategies for PERCS architecture, that examine how to map tasks of a\ngiven job on to the physical processing nodes. We develop and present\nfundamental principles for designing good mapping strategies that minimize\ncongestion. This is achieved via a theoretical study of some common\ncommunication patterns under both direct and indirect routing mechanisms\nsupported by the architecture."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.3933v1", 
    "title": "A GPU-accelerated Branch-and-Bound Algorithm for the Flow-Shop   Scheduling Problem", 
    "arxiv-id": "1208.3933v1", 
    "author": "Daniel Tuyttens", 
    "publish": "2012-08-20T08:06:58Z", 
    "summary": "Branch-and-Bound (B&B) algorithms are time intensive tree-based exploration\nmethods for solving to optimality combinatorial optimization problems. In this\npaper, we investigate the use of GPU computing as a major complementary way to\nspeed up those methods. The focus is put on the bounding mechanism of B&B\nalgorithms, which is the most time consuming part of their exploration process.\nWe propose a parallel B&B algorithm based on a GPU-accelerated bounding model.\nThe proposed approach concentrate on optimizing data access management to\nfurther improve the performance of the bounding mechanism which uses large and\nintermediate data sets that do not completely fit in GPU memory. Extensive\nexperiments of the contribution have been carried out on well known FSP\nbenchmarks using an Nvidia Tesla C2050 GPU card. We compared the obtained\nperformances to a single and a multithreaded CPU-based execution. Accelerations\nup to x100 are achieved for large problem instances."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.4484v1", 
    "title": "Leader Election and Gathering for Asynchronous Transparent Fat Robots   without Chirality", 
    "arxiv-id": "1208.4484v1", 
    "author": "Krishnendu Mukhopadhyaya", 
    "publish": "2012-08-22T12:10:15Z", 
    "summary": "This paper proposes a distributed algorithm which deterministically gathers n\n(n > 4) asynchronous, fat robots. The robots are assumed to be transparent and\nthey have full visibility. The robots are initially considered to be\nstationary. A robot is visible in its motion. The robots do not store past\nactions. They are anonymous and can not be distinguished by their appearances\nand do not have common coordinate system or chirality. The robots do not\ncommunicate through message passing. In the proposed gathering algorithm one\nrobot moves at a time towards its destination. The robot which moves, is\nselected in such a way that, it will be the only robot eligible to move, until\nit reaches its destination. In case of a tie, this paper proposes a leader\nelection algorithm which produces an ordering of the robots and the first robot\nin the ordering becomes the leader. The ordering is unique in the sense that,\neach robot, characterized by its location, agrees on the same ordering. We show\nthat if a set of robots can be ordered then they can gather deterministically.\nThe paper also characterizes the cases, where ordering is not possible. This\npaper also presents an important fact that, if leader election is possible then\ngathering pattern formation is possible even with no chirality."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.4867v1", 
    "title": "Parameter-independent Iterative Approximate Byzantine Consensus", 
    "arxiv-id": "1208.4867v1", 
    "author": "Nitin H. Vaidya", 
    "publish": "2012-08-23T21:44:08Z", 
    "summary": "In this work, we explore iterative approximate Byzantine consensus algorithms\nthat do not make explicit use of the global parameter of the graph, i.e., the\nupper-bound on the number of faults, f."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.5075v4", 
    "title": "Exact Byzantine Consensus in Directed Graphs", 
    "arxiv-id": "1208.5075v4", 
    "author": "Nitin Vaidya", 
    "publish": "2012-08-24T22:17:23Z", 
    "summary": "Consider a synchronous point-to-point network of n nodes connected by\ndirected links, wherein each node has a binary input. This paper proves a tight\nnecessary and sufficient condition on the underlying communication topology for\nachieving Byzantine consensus among these nodes in the presence of up to f\nByzantine faults. We derive a necessary condition, and then we provide a\nconstructive proof of sufficiency by presenting a Byzantine consensus algorithm\nfor directed graphs that satisfy the necessary condition.\n  Prior work has developed analogous necessary and sufficient conditions for\nundirected graphs. It is known that, for undirected graphs, the following two\nconditions are together necessary and sufficient [8, 2, 6]: (i) n ? 3f + 1, and\n(ii) network connectivity greater than 2f. However, these conditions are not\nadequate to completely characterize Byzantine consensus in directed graphs."
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3411", 
    "link": "http://arxiv.org/pdf/1208.5620v3", 
    "title": "Self-Stabilizing Byzantine Resilient Topology Discovery and Message   Delivery", 
    "arxiv-id": "1208.5620v3", 
    "author": "Elad M. Schiller", 
    "publish": "2012-08-28T11:03:07Z", 
    "summary": "Traditional Byzantine resilient algorithms use 2f+1 vertex disjoint paths to\nensure message delivery in the presence of up to f Byzantine nodes. The\nquestion of how these paths are identified is related to the fundamental\nproblem of topology discovery. Distributed algorithms for topology discovery\ncope with a never ending task, dealing with frequent changes in the network\ntopology and unpredictable transient faults. Therefore, algorithms for topology\ndiscovery should be self-stabilizing to ensure convergence of the topology\ninformation following any such unpredictable sequence of events. We present the\nfirst such algorithm that can cope with Byzantine nodes. Starting in an\narbitrary global state, and in the presence of f Byzantine nodes, each node is\neventually aware of all the other non-Byzantine nodes and their connecting\ncommunication links. Using the topology information, nodes can, for example,\nroute messages across the network and deliver messages from one end user to\nanother. We present the first deterministic, cryptographicassumptions- free,\nself-stabilizing, Byzantine-resilient algorithms for network topology discovery\nand end-to-end message delivery. We also consider the task of r-neighborhood\ndiscovery for the case in which r and the degree of nodes are bounded by\nconstants. The use of r-neighborhood discovery facilitates polynomial time,\ncommunication and space solutions for the above tasks. The obtained algorithms\ncan be used to authenticate parties, in particular during the establishment of\nprivate secrets, thus forming public key schemes that are resistant to\nman-in-the-middle attacks of the compromised Byzantine nodes. A polynomial and\nefficient end-to-end algorithm that is based on the established private secrets\ncan be employed in between periodical re-establishments of the secrets."
},{
    "category": "cs.DC", 
    "doi": "10.1109/INDUSIS.2010.5565821", 
    "link": "http://arxiv.org/pdf/1209.0851v1", 
    "title": "A new scheduling algorithm for server farms load balancing", 
    "arxiv-id": "1209.0851v1", 
    "author": "Shafigh Parsazad", 
    "publish": "2012-09-05T02:53:30Z", 
    "summary": "This paper describes a new scheduling algorithm to distribute jobs in server\nfarm systems. The proposed algorithm overcomes the starvation caused by SRPT\n(Shortest Remaining Processing Time). This algorithm is used in process\nscheduling in operating system approach. The algorithm was developed to be used\nin dispatcher scheduling. This algorithm is non-preemptive discipline, similar\nto SRPT, in which the priority of each job depends on its estimated run time,\nand also the amount of time it has spent on waiting. Tasks in the servers are\nserved in order of priority to optimize the system response time. The\nexperiments show that the mean round around time is reduced in the server farm\nsystem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/INDUSIS.2010.5565821", 
    "link": "http://arxiv.org/pdf/1209.1076v1", 
    "title": "Communication/Computation Tradeoffs in Consensus-Based Distributed   Optimization", 
    "arxiv-id": "1209.1076v1", 
    "author": "Michael G. Rabbat", 
    "publish": "2012-09-05T18:45:21Z", 
    "summary": "We study the scalability of consensus-based distributed optimization\nalgorithms by considering two questions: How many processors should we use for\na given problem, and how often should they communicate when communication is\nnot free? Central to our analysis is a problem-specific value $r$ which\nquantifies the communication/computation tradeoff. We show that organizing the\ncommunication among nodes as a $k$-regular expander graph (Reingold, Vadhan,\nand Wigderson, 2002) yields speedups, while when all pairs of nodes communicate\n(as in a complete graph), there is an optimal number of processors that depends\non $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach\na fixed level of accuracy, by communicating less and less frequently as the\ncomputation progresses. Experiments on a real cluster solving metric learning\nand non-smooth convex minimization tasks demonstrate strong agreement between\ntheory and practice."
},{
    "category": "cs.DC", 
    "doi": "10.7815/ijorcs.25.2012.044", 
    "link": "http://arxiv.org/pdf/1209.2614v1", 
    "title": "A threshold secure data sharing scheme for federated clouds", 
    "arxiv-id": "1209.2614v1", 
    "author": "M. Padmavathamma", 
    "publish": "2012-09-12T13:44:48Z", 
    "summary": "Cloud computing allows users to view computing in a new direction, as it uses\nthe existing technologies to provide better IT services at low-cost. To offer\nhigh QOS to customers according SLA, cloud services broker or cloud service\nprovider uses individual cloud providers that work collaboratively to form a\nfederation of clouds. It is required in applications like Real-time online\ninteractive applications, weather research and forecasting etc., in which the\ndata and applications are complex and distributed. In these applications secret\ndata should be shared, so secure data sharing mechanism is required in\nFederated clouds to reduce the risk of data intrusion, the loss of service\navailability and to ensure data integrity. So In this paper we have proposed\nzero knowledge data sharing scheme where Trusted Cloud Authority (TCA) will\ncontrol federated clouds for data sharing where the secret to be exchanged for\ncomputation is encrypted and retrieved by individual cloud at the end. Our\nscheme is based on the difficulty of solving the Discrete Logarithm problem\n(DLOG) in a finite abelian group of large prime order which is NP-Hard. So our\nproposed scheme provides data integrity in transit, data availability when one\nof host providers are not available during the computation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.3356v1", 
    "title": "Autonomic Cloud Computing: Open Challenges and Architectural Elements", 
    "arxiv-id": "1209.3356v1", 
    "author": "Xiaorong Li", 
    "publish": "2012-09-15T04:40:46Z", 
    "summary": "As Clouds are complex, large-scale, and heterogeneous distributed systems,\nmanagement of their resources is a challenging task. They need automated and\nintegrated intelligent strategies for provisioning of resources to offer\nservices that are secure, reliable, and cost-efficient. Hence, effective\nmanagement of services becomes fundamental in software platforms that\nconstitute the fabric of computing Clouds. In this direction, this paper\nidentifies open issues in autonomic resource provisioning and presents\ninnovative management techniques for supporting SaaS applications hosted on\nClouds. We present a conceptual architecture and early results evidencing the\nbenefits of autonomic management of Clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.3904v1", 
    "title": "A Distributed Algorithm for Gathering Many Fat Mobile Robots in the   Plane", 
    "arxiv-id": "1209.3904v1", 
    "author": "Marios Mavronicolas", 
    "publish": "2012-09-18T10:36:45Z", 
    "summary": "In this work we consider the problem of gathering autonomous robots in the\nplane. In particular, we consider non-transparent unit-disc robots (i.e., fat)\nin an asynchronous setting. Vision is the only mean of coordination. Using a\nstate-machine representation we formulate the gathering problem and develop a\ndistributed algorithm that solves the problem for any number of robots.\n  The main idea behind our algorithm is for the robots to reach a configuration\nin which all the following hold: (a) The robots' centers form a convex hull in\nwhich all robots are on the convex, (b) Each robot can see all other robots,\nand (c) The configuration is connected, that is, every robot touches another\nrobot and all robots together form a connected formation. We show that starting\nfrom any initial configuration, the robots, making only local decisions and\ncoordinate by vision, eventually reach such a configuration and terminate,\nyielding a solution to the gathering problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4408v1", 
    "title": "A performance Analysis of the Game of Life based on parallel algorithm", 
    "arxiv-id": "1209.4408v1", 
    "author": "Zhouxiang Meng", 
    "publish": "2012-09-20T02:12:03Z", 
    "summary": "In this article, Conway's Game of Life using OpenMP parallel processing to\nsimulate several different parallel methods, experimental performance results\nand compare to find the optimal solution of the parallelization of the Game of\nLife. Finally pointed out the importance of the design of parallel algorithms\nin solving the parallel problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4485v1", 
    "title": "Performance Evaluation of Hierarchical Publish-Subscribe Monitoring   Architecture for Service-Oriented Applications", 
    "arxiv-id": "1209.4485v1", 
    "author": "Ivan Benc", 
    "publish": "2012-09-20T10:15:52Z", 
    "summary": "Contemporary high-performance service-oriented applications demand a\nperformance efficient run-time monitoring. In this paper, we analyze a\nhierarchical publish-subscribe architecture for monitoring service-oriented\napplications. The analyzed architecture is based on a tree topology and\npublish-subscribe communication model for aggregation of distributed monitoring\ndata. In order to satisfy interoperability and platform independence of\nservice-orientation, monitoring reports are represented as XML documents. Since\nXML formatting introduces a significant processing and network load, we analyze\nthe performance of monitoring architecture with respect to the number of\nmonitored nodes, the load of system machines, and the overall latency of the\nmonitoring system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.4620v2", 
    "title": "Broadcast Using Certified Propagation Algorithm in Presence of Byzantine   Faults", 
    "arxiv-id": "1209.4620v2", 
    "author": "Vartika Bhandari", 
    "publish": "2012-09-20T19:15:50Z", 
    "summary": "We explore the correctness of the Certified Propagation Algorithm (CPA) [6,\n1, 8, 5] in solving broadcast with locally bounded Byzantine faults. CPA allows\nthe nodes to use only local information regarding the network topology. We\nprovide a tight necessary and sufficient condition on the network topology for\nthe correctness of CPA. To the best of our knowledge, this work is the first to\nsolve the open problem in [8]. We also present some simple extensions of this\nresult"
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1209.5319v1", 
    "title": "Multiprocessor Scheduling Using Parallel Genetic Algorithm", 
    "arxiv-id": "1209.5319v1", 
    "author": "Abdullatif ALAbdullatif", 
    "publish": "2012-09-24T16:12:20Z", 
    "summary": "Tasks scheduling is the most challenging problem in the parallel computing.\nHence, the inappropriate scheduling will reduce or even abort the utilization\nof the true potential of the parallelization. Genetic algorithm (GA) has been\nsuccessfully applied to solve the scheduling problem. The fitness evaluation is\nthe most time consuming GA operation for the CPU time, which affect the GA\nperformance. The proposed synchronous master-slave algorithm outperforms the\nsequential algorithm in case of complex and high number of generations problem."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.1026v2", 
    "title": "Performance Constraint and Power-Aware Allocation For User Requests In   Virtual Computing Lab", 
    "arxiv-id": "1210.1026v2", 
    "author": "Nguyen Thanh Son", 
    "publish": "2012-10-03T08:38:53Z", 
    "summary": "Cloud computing is driven by economies of scale. A cloud system uses\nvirtualization technology to provide cloud resources (e.g. CPU, memory) to\nusers in form of virtual machines. Virtual machine (VM), which is a sandbox for\nuser application, fits well in the education environment to provide\ncomputational resources for teaching and research needs. In resource\nmanagement, they want to reduce costs in operations by reducing expensive cost\nof electronic bill of large-scale data center system. A lease-based model is\nsuitable for our Virtual Computing Lab, in which users ask resources on a lease\nof virtual machines. This paper proposes two host selection policies, named MAP\n(minimum of active physical hosts) and MAP-H2L, and four algorithms solving the\nlease scheduling problem. FF-MAP, FF-MAP-H2L algorithms meet a trade-off\nbetween the energy consumption and Quality of Service (e.g. performance). The\nsimulation on 7-day workload, which converted from LLNL Atlas log, showed the\nFF-MAP and FF-MAP-H2L algorithms reducing 7.24% and 7.42% energy consumption\nthan existing greedy mapping algorithm in the leasing scheduler Haizea. In\naddition, we introduce a ratio \\theta of consolidation in HalfPI-FF-MAP and\nPI-FF-MAP algorithms, in which \\theta is \\pi/2 and \\pi, and results on their\nsimulations show that energy consumption decreased by 34.87% and 63.12%\nrespectively."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.1804v2", 
    "title": "Distributed Deterministic Broadcasting in Wireless Networks of Weak   Devices under the SINR Model", 
    "arxiv-id": "1210.1804v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2012-10-05T16:21:03Z", 
    "summary": "In this paper we initiate a study of distributed deterministic broadcasting\nin ad-hoc wireless networks with uniform transmission powers under the SINR\nmodel. We design algorithms in two settings: with and without local knowledge\nabout immediate neighborhood. In the former setting, our solution has almost\noptimal O(Dlog2 n) time cost, where n is the size of a network, D is the\neccentricity of the network and {1,...,N} is the set of possible node IDs. In\nthe latter case, we prove an Omega(n log N) lower bound and develop an\nalgorithm matching this formula, where n is the number of network nodes. As one\nof the conclusions, we derive that the inherited cost of broadcasting\ntechniques in wireless networks is much smaller, by factor around\nmin{n/D,Delta}, than the cost of learning the immediate neighborhood. Finally,\nwe develop a O(D Delta log2 N) algorithm for the setting without local\nknowledge, where Delta is the upper bound on the degree of the communication\ngraph of a network. This algorithm is close to a lower bound Omega(D Delta)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/EAIT.2012.6407847", 
    "link": "http://arxiv.org/pdf/1210.2047v2", 
    "title": "A Declarative Recommender System for Cloud Infrastructure Services   Selection", 
    "arxiv-id": "1210.2047v2", 
    "author": "Armin Haller", 
    "publish": "2012-10-07T12:16:09Z", 
    "summary": "The cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice..."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2401v1", 
    "title": "Distributed Formal Concept Analysis Algorithms Based on an Iterative   MapReduce Framework", 
    "arxiv-id": "1210.2401v1", 
    "author": "M\u00edche\u00e1l \u00d3 Foghl\u00fa", 
    "publish": "2012-10-05T10:28:24Z", 
    "summary": "While many existing formal concept analysis algorithms are efficient, they\nare typically unsuitable for distributed implementation. Taking the MapReduce\n(MR) framework as our inspiration we introduce a distributed approach for\nperforming formal concept mining. Our method has its novelty in that we use a\nlight-weight MapReduce runtime called Twister which is better suited to\niterative algorithms than recent distributed approaches. First, we describe the\ntheoretical foundations underpinning our distributed formal concept analysis\napproach. Second, we provide a representative exemplar of how a classic\ncentralized algorithm can be implemented in a distributed fashion using our\nmethodology: we modify Ganter's classic algorithm by introducing a family of\nMR* algorithms, namely MRGanter and MRGanter+ where the prefix denotes the\nalgorithm's lineage. To evaluate the factors that impact distributed algorithm\nperformance, we compare our MR* algorithms with the state-of-the-art.\nExperiments conducted on real datasets demonstrate that MRGanter+ is efficient,\nscalable and an appealing algorithm for distributed problems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2580v1", 
    "title": "Scheduling tree-shaped task graphs to minimize memory and makespan", 
    "arxiv-id": "1210.2580v1", 
    "author": "Fr\u00e9d\u00e9ric Vivien", 
    "publish": "2012-10-09T12:16:36Z", 
    "summary": "This paper investigates the execution of tree-shaped task graphs using\nmultiple processors. Each edge of such a tree represents a large IO file. A\ntask can only be executed if all input and output files fit into memory, and a\nfile can only be removed from memory after it has been consumed. Such trees\narise, for instance, in the multifrontal method of sparse matrix factorization.\nThe maximum amount of memory needed depends on the execution order of the\ntasks. With one processor the objective of the tree traversal is to minimize\nthe required memory. This problem was well studied and optimal polynomial\nalgorithms were proposed.\n  Here, we extend the problem by considering multiple processors, which is of\nobvious interest in the application area of matrix factorization. With the\nmultiple processors comes the additional objective to minimize the time needed\nto traverse the tree, i.e., to minimize the makespan. Not surprisingly, this\nproblem proves to be much harder than the sequential one. We study the\ncomputational complexity of this problem and provide an inapproximability\nresult even for unit weight trees. Several heuristics are proposed, each with a\ndifferent optimization focus, and they are analyzed in an extensive\nexperimental evaluation using realistic trees."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-29892-9_26", 
    "link": "http://arxiv.org/pdf/1210.2857v1", 
    "title": "An Introduction on Dependency Between Hardware Life Time Components and   Dynamic Voltage Scaling", 
    "arxiv-id": "1210.2857v1", 
    "author": "Nasrin Jaberi", 
    "publish": "2012-10-10T10:07:59Z", 
    "summary": "The main open question is how to calculate the effect of switching between\nfrequencies in DVFS technique on the lifetime of the cluster components. As\nmoving from one frequency to another in DVFS technique always gives a shock to\nthe component and consequently decreases the component lifetime, therefore, it\nbecomes interesting to answer the question of how fast a component can change\nits speed in order to decrease power without changing its lifetime."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3077v1", 
    "title": "Investigating Decision Support Techniques for Automating Cloud Service   Selection", 
    "arxiv-id": "1210.3077v1", 
    "author": "Peter Strazdins", 
    "publish": "2012-10-10T22:12:26Z", 
    "summary": "The compass of Cloud infrastructure services advances steadily leaving users\nin the agony of choice. To be able to select the best mix of service offering\nfrom an abundance of possibilities, users must consider complex dependencies\nand heterogeneous sets of criteria. Therefore, we present a PhD thesis proposal\non investigating an intelligent decision support system for selecting Cloud\nbased infrastructure services (e.g. storage, network, CPU)."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3292v3", 
    "title": "Energy Efficient Decentralized Detection Based on Bit-optimal Multi-hop   Transmission in One-dimensional Wireless Sensor Networks", 
    "arxiv-id": "1210.3292v3", 
    "author": "Ashfaq Khokhar", 
    "publish": "2012-10-11T16:40:36Z", 
    "summary": "Existing information theoretic work in decentralized detection is largely\nfocused on parallel configuration of Wireless Sensor Networks (WSNs), where an\nindividual hard or soft decision is computed at each sensor node and then\ntransmitted directly to the fusion node. Such an approach is not efficient for\nlarge networks, where communication structure is likely to comprise of multiple\nhops. On the other hand, decentralized detection problem investigated for\nmulti-hop networks is mainly concerned with reducing number and/or size of\nmessages by using compression and fusion of information at intermediate nodes.\nIn this paper an energy efficient multi-hop configuration of WSNs is proposed\nto solve the detection problem in large networks with two objectives:\nmaximizing network lifetime and minimizing probability of error in the fusion\nnode. This optimization problem is considered under the constraint of total\nconsumed energy. The two objectives mentioned are achieved simultaneously in\nthe multi-hop configuration by exploring tradeoffs between different path\nlengths and number of bits allocated to each node for quantization. Simulation\nresults show significant improvement in the proposed multi-hop configuration\ncompared with the parallel configuration in terms of energy efficiency and\ndetection accuracy for different size networks, especially in larger networks."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.3876v1", 
    "title": "Power-efficient Hierarchical Data Aggregation using Compressive Sensing   in WSN", 
    "arxiv-id": "1210.3876v1", 
    "author": "Ashfaq Khokhar", 
    "publish": "2012-10-15T02:39:12Z", 
    "summary": "Compressive Sensing (CS) method is a burgeoning technique being applied to\ndiverse areas including wireless sensor networks (WSNs). In WSNs, it has been\nstudied in the context of data gathering and aggregation, particularly aimed at\nreducing data transmission cost and improving power efficiency. Existing CS\nbased data gathering work in WSNs assume fixed and uniform compression\nthreshold across the network, regard- less of the data field characteristics.\nIn this paper, we present a novel data aggregation architecture model that\ncombines a multi- resolution structure with compressed sensing. The compression\nthresholds vary over the aggregation hierarchy, reflecting the underlying data\nfield. Compared with previous relevant work, the proposed model shows its\nsignificant energy saving from theoretical analysis. We have also implemented\nthe proposed CS- based data aggregation framework on a SIDnet SWANS platform,\ndiscrete event simulator commonly used for WSN simulations. Our experiments\nshow substantial energy savings, ranging from 37% to 77% for different nodes in\nthe networking depending on the position of hierarchy."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.4690v2", 
    "title": "A Primarily Survey on Energy Efficiency in Cloud and Distributed   Computing Systems", 
    "arxiv-id": "1210.4690v2", 
    "author": "Albert Y. Zomaya", 
    "publish": "2012-10-17T10:55:56Z", 
    "summary": "A survey of available techniques in hardware to reduce energy consumption"
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.5774v2", 
    "title": "Fast Routing Table Construction Using Small Messages", 
    "arxiv-id": "1210.5774v2", 
    "author": "Boaz Patt-Shamir", 
    "publish": "2012-10-21T23:01:59Z", 
    "summary": "We describe a distributed randomized algorithm computing approximate\ndistances and routes that approximate shortest paths. Let n denote the number\nof nodes in the graph, and let HD denote the hop diameter of the graph, i.e.,\nthe diameter of the graph when all edges are considered to have unit weight.\nGiven 0 < eps <= 1/2, our algorithm runs in weak-O(n^(1/2 + eps) + HD)\ncommunication rounds using messages of O(log n) bits and guarantees a stretch\nof O(eps^(-1) log eps^(-1)) with high probability. This is the first\ndistributed algorithm approximating weighted shortest paths that uses small\nmessages and runs in weak-o(n) time (in graphs where HD in weak-o(n)). The time\ncomplexity nearly matches the lower bounds of weak-Omega(sqrt(n) + HD) in the\nsmall-messages model that hold for stateless routing (where routing decisions\ndo not depend on the traversed path) as well as approximation of the weigthed\ndiameter. Our scheme replaces the original identifiers of the nodes by labels\nof size O(log eps^(-1) log n). We show that no algorithm that keeps the\noriginal identifiers and runs for weak-o(n) rounds can achieve a\npolylogarithmic approximation ratio.\n  Variations of our techniques yield a number of fast distributed approximation\nalgorithms solving related problems using small messages. Specifically, we\npresent algorithms that run in weak-O(n^(1/2 + eps) + HD) rounds for a given 0\n< eps <= 1/2, and solve, with high probability, the following problems:\n  - O(eps^(-1))-approximation for the Generalized Steiner Forest (the running\ntime in this case has an additive weak-O(t^(1 + 2eps)) term, where t is the\nnumber of terminals);\n  - O(eps^(-2))-approximation of weighted distances, using node labels of size\nO(eps^(-1) log n) and weak-O(n^(eps)) bits of memory per node;\n  - O(eps^(-1))-approximation of the weighted diameter;\n  - O(eps^(-3))-approximate shortest paths using the labels 1,...,n."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.6384v1", 
    "title": "A Distributed Transportation Simplex Applied to a Content Distribution   Network Problem", 
    "arxiv-id": "1210.6384v1", 
    "author": "Yuri Frota", 
    "publish": "2012-10-23T21:21:06Z", 
    "summary": "A Content Distribution Network (CDN) can be defined as an overlay system that\nreplicates copies of contents at multiple points of a network, close to the\nfinal users, with the objective of improving data access. CDN technology is\nwidely used for the distribution of large-sized contents, like in video\nstreaming. In this paper we address the problem of finding the best server for\neach customer request in CDNs, in order to minimize the overall cost. We\nconsider the problem as a transportation problem and a distributed algorithm is\nproposed to solve it. The algorithm is composed of two independent phases: a\ndistributed heuristic finds an initial solution that may be later improved by a\ndistributed transportation simplex algorithm. It is compared with the\nsequential version of the transportation simplex and with an auction-based\ndistributed algorithm. Computational experiments carried out on a set of\ninstances adapted from the literature revealed that our distributed approach\nhas a performance similar to its sequential counterpart, in spite of not\nrequiring global information about the contents requests. Moreover, the results\nalso showed that the new method outperforms the based-auction distributed\nalgorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7057v1", 
    "title": "Efficient Distributed Locality Sensitive Hashing", 
    "arxiv-id": "1210.7057v1", 
    "author": "Rajendra Shinde", 
    "publish": "2012-10-26T05:59:55Z", 
    "summary": "Distributed frameworks are gaining increasingly widespread use in\napplications that process large amounts of data. One important example\napplication is large scale similarity search, for which Locality Sensitive\nHashing (LSH) has emerged as the method of choice, specially when the data is\nhigh-dimensional. At its core, LSH is based on hashing the data points to a\nnumber of buckets such that similar points are more likely to map to the same\nbuckets. To guarantee high search quality, the LSH scheme needs a rather large\nnumber of hash tables. This entails a large space requirement, and in the\ndistributed setting, with each query requiring a network call per hash bucket\nlook up, this also entails a big network load. The Entropy LSH scheme proposed\nby Panigrahy significantly reduces the number of required hash tables by\nlooking up a number of query offsets in addition to the query itself. While\nthis improves the LSH space requirement, it does not help with (and in fact\nworsens) the search network efficiency, as now each query offset requires a\nnetwork call. In this paper, focusing on the Euclidian space under $l_2$ norm\nand building up on Entropy LSH, we propose the distributed Layered LSH scheme,\nand prove that it exponentially decreases the network cost, while maintaining a\ngood load balance between different machines. Our experiments also verify that\nour scheme results in a significant network traffic reduction that brings about\nlarge runtime improvement in real world applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7624v1", 
    "title": "HEP Analysis Facility An Approach to Grid Computing", 
    "arxiv-id": "1210.7624v1", 
    "author": "Sanjeev Singh Sambyal", 
    "publish": "2012-10-29T11:29:52Z", 
    "summary": "HEP Analysis Facility is a cluster designed and implemented in Scientific\nLinux Cern 5.5 to grant High Energy Physics researchers one place where they\ncan go to undertake a particular task or to provide a parallel processing\narchitecture in which CPU resources are shared across a network and all\nmachines function as one large supercomputer."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7626v1", 
    "title": "Hep Cluster First Step Towards Grid Computing", 
    "arxiv-id": "1210.7626v1", 
    "author": "Sanjeev Singh Sambyal", 
    "publish": "2012-10-29T11:36:32Z", 
    "summary": "HEP Cluster is designed and implemented in Scientific Linux Cern 5.5 to grant\nHigh Energy Physics researchers one place where they can go to undertake a\nparticular task or to provide a parallel processing architecture in which CPU\nresources are shared across a network and all machines function as one large\nsupercomputer. It gives physicists a facility to access computers and data,\ntransparently, without having to consider location, operating system, account\nadministration, and other details. By using this facility researchers can\nprocess their jobs much faster than the stand alone desktop systems. Keywords:\nCluster, Network, Storage, Parallel Computing & Gris."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1210.7935v1", 
    "title": "Energy Efficient Algorithms and Power Consumption Techniques in High   Performance Computing", 
    "arxiv-id": "1210.7935v1", 
    "author": "Sanjay Mahajan", 
    "publish": "2012-10-30T09:14:40Z", 
    "summary": "High Performance Computing is an internet based computing which makes\ncomputer infrastructure and services available to the user for research\npurpose. However, an important issue which needs to be resolved before High\nPerformance Computing Cluster with large pool of servers gain widespread\nacceptance is the design of data centers with less energy consumption. It is\nonly possible when servers produce less heat and consume less power. Systems\nreliability decreases with increase in temperature due to heat generation\ncaused by large power consumption as computing in high temperature is more\nerror-prone. Here in this paper our approach is to design and implement a high\nperformance cluster for high-end research in the High Energy Physics stream.\nThis involves the usage of fine grained power gating technique in\nmicroprocessors and energy efficient algorithms that reduce the overall running\ncost of the data center."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1211.0235v1", 
    "title": "Feedback from nature: an optimal distributed algorithm for maximal   independent set selection", 
    "arxiv-id": "1211.0235v1", 
    "author": "Lei Xu", 
    "publish": "2012-11-01T17:41:34Z", 
    "summary": "Maximal Independent Set selection is a fundamental problem in distributed\ncomputing. A novel probabilistic algorithm for this problem has recently been\nproposed by Afek et al, inspired by the study of the way that developing cells\nin the fly become specialised. The algorithm they propose is simple and robust,\nbut not as efficient as previous approaches: the expected time complexity is\nO(log^2 n). Here we first show that the approach of Afek et al cannot achieve\nbetter efficiency than this across all networks, no matter how the probability\nvalues are chosen. However, we then propose a new algorithm that incorporates\nanother important feature of the biological system: adapting the probabilities\nused at each node based on local feedback from neighbouring nodes. Our new\nalgorithm retains all the advantages of simplicity and robustness, but also\nachieves the optimal efficiency of O(log n) expected time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427501", 
    "link": "http://arxiv.org/pdf/1211.1279v1", 
    "title": "Application-centric Resource Provisioning for Amazon EC2 Spot Instances", 
    "arxiv-id": "1211.1279v1", 
    "author": "Nandini Mukherjee", 
    "publish": "2012-11-06T15:58:55Z", 
    "summary": "In late 2009, Amazon introduced spot instances to offer their unused\nresources at lower cost with reduced reliability. Amazon's spot instances allow\ncustomers to bid on unused Amazon EC2 capacity and run those instances for as\nlong as their bid exceeds the current spot price. The spot price changes\nperiodically based on supply and demand, and customers whose bids exceed it\ngain access to the available spot instances. Customers may expect their\nservices at lower cost with spot instances compared to on-demand or reserved.\nHowever the reliability is compromised since the instances(IaaS) providing the\nservice(SaaS) may become unavailable at any time without any notice to the\ncustomer. Checkpointing and migration schemes are of great use to cope with\nsuch situation. In this paper we study various checkpointing schemes that can\nbe used with spot instances. Also we device some algorithms for checkpointing\nscheme on top of application-centric resource provisioning framework that\nincrease the reliability while reducing the cost significantly."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.1447v1", 
    "title": "Advance Reservation based DAG Application Scheduling Simulator for Grid   Environment", 
    "arxiv-id": "1211.1447v1", 
    "author": "Vipul A. Shah", 
    "publish": "2012-11-07T04:21:58Z", 
    "summary": "In the last decade, scheduling of Directed Acyclic Graph (DAG) application in\nthe context of Grid environment has attracted attention of many researchers.\nHowever, deployment of Grid environment requires skills, efforts, budget, and\ntime. Although various simulation toolkits or frameworks are available for\nsimulating Grid environment, either they support different possible studies in\nGrid computing area or takes lot of efforts in molding them to make them\nsuitable for scheduling of DAG application. In this paper, we describe design\nand implementation of GridSim based ready to use application scheduler for\nscheduling of DAG application in Grid environment. The proposed application\nscheduler supports supplying DAG application and configuration of Grid\nresources through GUI. We also describe implementation of Min-Min static\nscheduling algorithm for scheduling of DAG application to validate the proposed\nscheduler. Our proposed DAG application scheduling simulator is useful, easy,\nand time-saver."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.1658v1", 
    "title": "Extending Task Parallelism for Frequent Pattern Mining", 
    "arxiv-id": "1211.1658v1", 
    "author": "Andrew Lumsdaine", 
    "publish": "2012-11-07T20:18:30Z", 
    "summary": "Algorithms for frequent pattern mining, a popular informatics application,\nhave unique requirements that are not met by any of the existing parallel\ntools. In particular, such applications operate on extremely large data sets\nand have irregular memory access patterns. For efficient parallelization of\nsuch applications, it is necessary to support dynamic load balancing along with\nscheduling mechanisms that allow users to exploit data locality. Given these\nrequirements, task parallelism is the most promising of the available parallel\nprogramming models. However, existing solutions for task parallelism schedule\ntasks implicitly and hence, custom scheduling policies that can exploit data\nlocality cannot be easily employed. In this paper we demonstrate and\ncharacterize the speedup obtained in a frequent pattern mining application\nusing a custom clustered scheduling policy in place of the popular Cilk-style\npolicy. We present PFunc, a novel task parallel library whose customizable task\nscheduling and task priorities facilitated the implementation of our clustered\nscheduling policy."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.2032v1", 
    "title": "Implementation of Security in Distributed Systems - A Comparative Study", 
    "arxiv-id": "1211.2032v1", 
    "author": "Mohamed Firdhous", 
    "publish": "2012-11-09T02:52:32Z", 
    "summary": "This paper presents a comparative study of distributed systems and the\nsecurity issues associated with those systems. Four commonly used distributed\nsystems were considered for detailed analysis in terms of technologies\ninvolved, security issues faced by them and solution proposed to circumvent\nthose issues. Finally the security issues and the solutions were summarized and\ncompared with each other."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.2038v1", 
    "title": "Comparison of OpenMP & OpenCL Parallel Processing Technologies", 
    "arxiv-id": "1211.2038v1", 
    "author": "S. R. Sathe", 
    "publish": "2012-11-09T04:27:32Z", 
    "summary": "This paper presents a comparison of OpenMP and OpenCL based on the parallel\nimplementation of algorithms from various fields of computer applications. The\nfocus of our study is on the performance of benchmark comparing OpenMP and\nOpenCL. We observed that OpenCL programming model is a good option for mapping\nthreads on different processing cores. Balancing all available cores and\nallocating sufficient amount of work among all computing units, can lead to\nimproved performance. In our simulation, we used Fedora operating system; a\nsystem with Intel Xeon Dual core processor having thread count 24 coupled with\nNVIDIA Quadro FX 3800 as graphical processing unit."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.4290v2", 
    "title": "Toward a Principled Framework for Benchmarking Consistency", 
    "arxiv-id": "1211.4290v2", 
    "author": "Jay J. Wylie", 
    "publish": "2012-11-19T02:59:53Z", 
    "summary": "Large-scale key-value storage systems sacrifice consistency in the interest\nof dependability (i.e., partition tolerance and availability), as well as\nperformance (i.e., latency). Such systems provide eventual\nconsistency,which---to this point---has been difficult to quantify in real\nsystems. Given the many implementations and deployments of\neventually-consistent systems (e.g., NoSQL systems), attempts have been made to\nmeasure this consistency empirically, but they suffer from important drawbacks.\nFor example, state-of-the art consistency benchmarks exercise the system only\nin restricted ways and disrupt the workload, which limits their accuracy.\n  In this paper, we take the position that a consistency benchmark should paint\na comprehensive picture of the relationship between the storage system under\nconsideration, the workload, the pattern of failures, and the consistency\nobserved by clients. To illustrate our point, we first survey prior efforts to\nquantify eventual consistency. We then present a benchmarking technique that\novercomes the shortcomings of existing techniques to measure the consistency\nobserved by clients as they execute the workload under consideration. This\nmethod is versatile and minimally disruptive to the system under test. As a\nproof of concept, we demonstrate this tool on Cassandra."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.6473v1", 
    "title": "Int\u00e9gration des intergiciels de grilles de PC dans le nuage SlapOS :   le cas de BOINC", 
    "arxiv-id": "1211.6473v1", 
    "author": "Nicolas Gren\u00e8che", 
    "publish": "2012-11-27T23:05:10Z", 
    "summary": "In this article we describe the problems and solutions related to the\nintegration of desktop grid middleware in a cloud, in this case the open source\nSlapOS cloud. We focus on the issues about recipes that describe the\nintegration and the problem of the confinement of execution. They constitute\ntwo aspects of service-oriented architecture and Cloud Computing. These two\nissues solved with SlapOS are not in relation to what is traditionally done in\nthe clouds because we do not rely on virtual machines and, there is no data\ncenter (as defined in cloud). Moreover, we show that from the initial\ndeployment model we take into account not only Web applications, B2B\napplications... but also applications from the field of grids; here desktop\ngrid middleware which is a case study."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1211.6526v1", 
    "title": "Complexity Measures for Map-Reduce, and Comparison to Parallel Computing", 
    "arxiv-id": "1211.6526v1", 
    "author": "Kamesh Munagala", 
    "publish": "2012-11-28T06:03:24Z", 
    "summary": "The programming paradigm Map-Reduce and its main open-source implementation,\nHadoop, have had an enormous impact on large scale data processing. Our goal in\nthis expository writeup is two-fold: first, we want to present some complexity\nmeasures that allow us to talk about Map-Reduce algorithms formally, and\nsecond, we want to point out why this model is actually different from other\nmodels of parallel programming, most notably the PRAM (Parallel Random Access\nMemory) model. We are looking for complexity measures that are detailed enough\nto make fine-grained distinction between different algorithms, but which also\nabstract away many of the implementation details."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0085v1", 
    "title": "VUPIC: Virtual Machine Usage Based Placement in IaaS Cloud", 
    "arxiv-id": "1212.0085v1", 
    "author": "Kapil Phatnani", 
    "publish": "2012-12-01T08:51:48Z", 
    "summary": "Efficient resource allocation is one of the critical performance challenges\nin an Infrastructure as a Service (IaaS) cloud. Virtual machine (VM) placement\nand migration decision making methods are integral parts of these resource\nallocation mechanisms. We present a novel virtual machine placement algorithm\nwhich takes performance isolation amongst VMs and their continuous resource\nusage into account while taking placement decisions. Performance isolation is a\nform of resource contention between virtual machines interested in basic low\nlevel hardware resources (CPU, memory, storage, and networks bandwidth).\nResource contention amongst multiple co-hosted neighbouring VMs form the basis\nof the presented novel approach. Experiments are conducted to show the various\ncategories of applications and effect of performance isolation and resource\ncontention amongst them. A per-VM 3-dimensional Resource Utilization Vector\n(RUV) has been continuously calculated and used for placement decisions while\ntaking conflicting resource interests of VMs into account. Experiments using\nthe novel placement algorithm: VUPIC, show effective improvements in VM\nperformance as well as overall resource utilization of the cloud."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0156v1", 
    "title": "An Ontology based System for Cloud Infrastructure Services Discovery", 
    "arxiv-id": "1212.0156v1", 
    "author": "Surya Nepal", 
    "publish": "2012-12-01T20:39:22Z", 
    "summary": "The Cloud infrastructure services landscape advances steadily leaving users\nin the agony of choice. As a result, Cloud service identification and discovery\nremains a hard problem due to different service descriptions, non standardised\nnaming conventions and heterogeneous types and features of Cloud services. In\nthis paper, we present an OWL based ontology, the Cloud Computing Ontology\n(CoCoOn) that defines functional and non functional concepts, attributes and\nrelations of infrastructure services. We also present a system..."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0421v1", 
    "title": "Network delay-aware load balancing in selfish and cooperative   distributed systems", 
    "arxiv-id": "1212.0421v1", 
    "author": "Krzysztof Rzadca", 
    "publish": "2012-12-03T15:52:18Z", 
    "summary": "We consider a request processing system composed of organizations and their\nservers connected by the Internet.\n  The latency a user observes is a sum of communication delays and the time\nneeded to handle the request on a server. The handling time depends on the\nserver congestion, i.e. the total number of requests a server must handle. We\nanalyze the problem of balancing the load in a network of servers in order to\nminimize the total observed latency. We consider both cooperative and selfish\norganizations (each organization aiming to minimize the latency of the\nlocally-produced requests). The problem can be generalized to the task\nscheduling in a distributed cloud; or to content delivery in an\norganizationally-distributed CDNs.\n  In a cooperative network, we show that the problem is polynomially solvable.\nWe also present a distributed algorithm iteratively balancing the load. We show\nhow to estimate the distance between the current solution and the optimum based\non the amount of load exchanged by the algorithm. During the experimental\nevaluation, we show that the distributed algorithm is efficient, therefore it\ncan be used in networks with dynamically changing loads.\n  In a network of selfish organizations, we prove that the price of anarchy\n(the worst-case loss of performance due to selfishness) is low when the network\nis homogeneous and the servers are loaded (the request handling time is high\ncompared to the communication delay). After relaxing these assumptions, we\nassess the loss of performance caused by the selfishness experimentally,\nshowing that it remains low.\n  Our results indicate that a network of servers handling requests can be\nefficiently managed by a distributed algorithm. Additionally, even if the\nnetwork is organizationally distributed, with individual organizations\noptimizing performance of their requests, the network remains efficient."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.0427v2", 
    "title": "Exploring heterogeneity of unreliable machines for p2p backup", 
    "arxiv-id": "1212.0427v2", 
    "author": "Krzysztof Rzadca", 
    "publish": "2012-12-03T16:09:02Z", 
    "summary": "P2P architecture is a viable option for enterprise backup. In contrast to\ndedicated backup servers, nowadays a standard solution, making backups directly\non organization's workstations should be cheaper (as existing hardware is\nused), more efficient (as there is no single bottleneck server) and more\nreliable (as the machines are geographically dispersed).\n  We present the architecture of a p2p backup system that uses pairwise\nreplication contracts between a data owner and a replicator. In contrast to\nstandard p2p storage systems using directly a DHT, the contracts allow our\nsystem to optimize replicas' placement depending on a specific optimization\nstrategy, and so to take advantage of the heterogeneity of the machines and the\nnetwork. Such optimization is particularly appealing in the context of backup:\nreplicas can be geographically dispersed, the load sent over the network can be\nminimized, or the optimization goal can be to minimize the backup/restore time.\nHowever, managing the contracts, keeping them consistent and adjusting them in\nresponse to dynamically changing environment is challenging.\n  We built a scientific prototype and ran the experiments on 150 workstations\nin the university's computer laboratories and, separately, on 50 PlanetLab\nnodes. We found out that the main factor affecting the quality of the system is\nthe availability of the machines. Yet, our main conclusion is that it is\npossible to build an efficient and reliable backup system on highly unreliable\nmachines (our computers had just 13% average availability)."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.1284v1", 
    "title": "Integrated Green Cloud Computing Architecture", 
    "arxiv-id": "1212.1284v1", 
    "author": "M. R. Doomun", 
    "publish": "2012-12-06T10:57:59Z", 
    "summary": "Arbitrary usage of cloud computing, either private or public, can lead to\nuneconomical energy consumption in data processing, storage and communication.\nHence, green cloud computing solutions aim not only to save energy but also\nreduce operational costs and carbon footprints on the environment. In this\npaper, an Integrated Green Cloud Architecture (IGCA) is proposed that comprises\nof a client-oriented Green Cloud Middleware to assist managers in better\noverseeing and configuring their overall access to cloud services in the\ngreenest or most energy-efficient way. Decision making, whether to use local\nmachine processing, private or public clouds, is smartly handled by the\nmiddleware using predefined system specifications such as service level\nagreement (SLA), Quality of service (QoS), equipment specifications and job\ndescription provided by IT department. Analytical model is used to show the\nfeasibility to achieve efficient energy consumption while choosing between\nlocal, private and public Cloud service provider (CSP)."
},{
    "category": "cs.DC", 
    "doi": "10.5120/9944-4585", 
    "link": "http://arxiv.org/pdf/1212.3295v2", 
    "title": "Measures of Fault Tolerance in Distributed Simulated Annealing", 
    "arxiv-id": "1212.3295v2", 
    "author": "Aaditya Prakash", 
    "publish": "2012-12-13T20:00:40Z", 
    "summary": "In this paper, we examine the different measures of Fault Tolerance in a\nDistributed Simulated Annealing process. Optimization by Simulated Annealing on\na distributed system is prone to various sources of failure. We analyse\nsimulated annealing algorithm, its architecture in distributed platform and\npotential sources of failures. We examine the behaviour of tolerant distributed\nsystem for optimization task. We present possible methods to overcome the\nfailures and achieve fault tolerance for the distributed simulated annealing\nprocess. We also examine the implementation of Simulated Annealing in MapReduce\nsystem and possible ways to prevent failures in reaching the global optima.\nThis paper will be beneficial to those who are interested in implementing a\nlarge scale distributed simulated annealing optimization problem of industrial\nor academic interest. We recommend hybrid tolerance technique to optimize the\ntrade-off between efficiency and availability."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2513228.2513286", 
    "link": "http://arxiv.org/pdf/1212.4123v4", 
    "title": "An Interactive Graph-Based Automation Assistant: A Case Study to Manage   the GIPSY's Distributed Multi-tier Run-Time System", 
    "arxiv-id": "1212.4123v4", 
    "author": "Joey Paquet", 
    "publish": "2012-12-17T20:09:55Z", 
    "summary": "The GIPSY system provides a framework for a distributed multi-tier\ndemand-driven evaluation of heterogeneous programs, in which certain tiers can\ngenerate demands, while others can respond to demands to work on them. They are\nconnected through a virtual network that can be flexibly reconfigured at\nrun-time. Although the demand generator components were originally designed\nspecifically for the eductive (demand-driven) evaluation of Lucid intensional\nprograms, the GIPSY's run-time's flexible framework design enables it to\nperform the execution of various kinds of programs that can be evaluated using\nthe demand-driven computational model. Management of the GISPY networks has\nbecome a tedious (although scripted) task that took manual command-line console\nto do, which does not scale for large experiments. Therefore a new component\nhas been designed and developed to allow users to represent, visualize, and\ninteractively create, configure and seamlessly manage such a network as a\ngraph. Consequently, this work presents a Graphical GMT Manager, an interactive\ngraph-based assistant component for the GIPSY network creation and\nconfiguration management. Besides allowing the management of the nodes and\ntiers (mapped to hosts where store, workers, and generators reside), it lets\nthe user to visually control the network parameters and the interconnection\nbetween computational nodes at run-time. In this paper we motivate and present\nthe key features of this newly implemented graph-based component. We give the\ngraph representation details, mapping of the graph nodes to tiers, tier groups,\nand specific commands. We provide the requirements and design specification of\nthe tool and its implementation. Then we detail and discuss some experimental\nresults."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2513228.2513286", 
    "link": "http://arxiv.org/pdf/1212.4658v1", 
    "title": "Resource management on a VM based computer cluster for scientific   computing", 
    "arxiv-id": "1212.4658v1", 
    "author": "Piero Spinnato", 
    "publish": "2012-12-19T13:31:04Z", 
    "summary": "In the last ten years host virtualization has brought a revolution in the way\nalmost every activity related to information technology is thought of and\nperformed. The use of virtualization for HPC and HTC computing, while eagerly\ndesired, has probably been one of the last steps of this revolution, the\nperformance loss due to the hardware abstraction layer being the cause that\nslowed down a process that has been much faster in other fields. Nowadays the\nwidespread diffusion of virtualization and of new virtualization techniques\nseem to have helped breaking this last barrier and virtual host computing\ninfrastructures for HPC and HTC are found in many data centers. In this\ndocument the approach adopted at the INFN \"Laboratori Nazionali del Gran Sasso\"\nfor providing computational resources via a virtual host based computing\nfacility is described. Particular evidence is given to the storage layout, to\nthe middleware architecture and to resource allocation strategies, as these are\nissues for which a personalized solution was adopted. Other aspects may be\ncovered in the future within other documents."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1212.4692v1", 
    "title": "Map / Reduce Deisgn and Implementation of Apriori Alogirthm for handling   voluminous data-sets", 
    "arxiv-id": "1212.4692v1", 
    "author": "Kiran U. Shanbag", 
    "publish": "2012-12-19T15:04:12Z", 
    "summary": "Apriori is one of the key algorithms to generate frequent itemsets. Analyzing\nfrequent itemset is a crucial step in analysing structured data and in finding\nassociation relationship between items. This stands as an elementary foundation\nto supervised learning, which encompasses classifier and feature extraction\nmethods. Applying this algorithm is crucial to understand the behaviour of\nstructured data. Most of the structured data in scientific domain are\nvoluminous. Processing such kind of data requires state of the art computing\nmachines. Setting up such an infrastructure is expensive. Hence a distributed\nenvironment such as a clustered setup is employed for tackling such scenarios.\nApache Hadoop distribution is one of the cluster frameworks in distributed\nenvironment that helps by distributing voluminous data across a number of nodes\nin the framework. This paper focuses on map/reduce design and implementation of\nApriori algorithm for structured data analysis."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1212.5956v1", 
    "title": "Interoperability and Standardization of Intercloud Cloud Computing", 
    "arxiv-id": "1212.5956v1", 
    "author": "Tian Niu", 
    "publish": "2012-12-24T19:24:35Z", 
    "summary": "Cloud computing is getting mature, and the interoperability and\nstandardization of the clouds is still waiting to be solved. This paper\ndiscussed the interoperability among clouds about message transmission, data\ntransmission and virtual machine transfer. Starting from IEEE Pioneering Cloud\nComputing Initiative, this paper discussed about standardization of the cloud\ncomputing, especially intercloud cloud computing. This paper also discussed the\nstandardization from the market-oriented view."
},{
    "category": "cs.DC", 
    "doi": "10.5121/acij.2012.3604", 
    "link": "http://arxiv.org/pdf/1302.1939v1", 
    "title": "HTC Scientific Computing in a Distributed Cloud Environment", 
    "arxiv-id": "1302.1939v1", 
    "author": "W. Podiama", 
    "publish": "2013-02-08T04:29:16Z", 
    "summary": "This paper describes the use of a distributed cloud computing system for\nhigh-throughput computing (HTC) scientific applications. The distributed cloud\ncomputing system is composed of a number of separate\nInfrastructure-as-a-Service (IaaS) clouds that are utilized in a unified\ninfrastructure. The distributed cloud has been in production-quality operation\nfor two years with approximately 500,000 completed jobs where a typical\nworkload has 500 simultaneous embarrassingly-parallel jobs that run for\napproximately 12 hours. We review the design and implementation of the system\nwhich is based on pre-existing components and a number of custom components. We\ndiscuss the operation of the system, and describe our plans for the expansion\nto more sites and increased computing capacity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2203v1", 
    "title": "A Factor Framework for Experimental Design for Performance Evaluation of   Commercial Cloud Services", 
    "arxiv-id": "1302.2203v1", 
    "author": "Rainbow Cai", 
    "publish": "2013-02-09T06:18:04Z", 
    "summary": "Given the diversity of commercial Cloud services, performance evaluations of\ncandidate services would be crucial and beneficial for both service customers\n(e.g. cost-benefit analysis) and providers (e.g. direction of service\nimprovement). Before an evaluation implementation, the selection of suitable\nfactors (also called parameters or variables) plays a prerequisite role in\ndesigning evaluation experiments. However, there seems a lack of systematic\napproaches to factor selection for Cloud services performance evaluation. In\nother words, evaluators randomly and intuitively concerned experimental factors\nin most of the existing evaluation studies. Based on our previous taxonomy and\nmodeling work, this paper proposes a factor framework for experimental design\nfor performance evaluation of commercial Cloud services. This framework\ncapsules the state-of-the-practice of performance evaluation factors that\npeople currently take into account in the Cloud Computing domain, and in turn\ncan help facilitate designing new experiments for evaluating Cloud services."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2217v1", 
    "title": "Introducing Speculation in Self-Stabilization - An Application to Mutual   Exclusion", 
    "arxiv-id": "1302.2217v1", 
    "author": "Rachid Guerraoui", 
    "publish": "2013-02-09T11:09:22Z", 
    "summary": "Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits. Speculation consists in\nguaranteeing that the system satisfies its requirements for any execution but\nexhibits significantly better performances for a subset of executions that are\nmore probable. A speculative protocol is in this sense supposed to be both\nrobust and efficient in practice. We introduce the notion of speculative\nstabilization which we illustrate through the mutual exclusion problem. We then\npresent a novel speculatively stabilizing mutual exclusion protocol. Our\nprotocol is self-stabilizing for any asynchronous execution. We prove that its\nstabilization time for synchronous executions is diam(g)/2 steps (where diam(g)\ndenotes the diameter of the system). This complexity result is of independent\ninterest. The celebrated mutual exclusion protocol of Dijkstra stabilizes in n\nsteps (where n is the number of processes) in synchronous executions and the\nquestion whether the stabilization time could be strictly smaller than the\ndiameter has been open since then (almost 40 years). We show that this is\nindeed possible for any underlying topology. We also provide a lower bound\nproof that shows that our new stabilization time of diam(g)/2 steps is optimal\nfor synchronous executions, even if asynchronous stabilization is not required."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2012.6427525", 
    "link": "http://arxiv.org/pdf/1302.2227v1", 
    "title": "Virtual Machine Consolidation for Datacenter Energy Improvement", 
    "arxiv-id": "1302.2227v1", 
    "author": "Maziar Goudarzi", 
    "publish": "2013-02-09T12:23:21Z", 
    "summary": "Rapid growth and proliferation of cloud computing services around the world\nhas increased the necessity and significance of improving the energy efficiency\nof could implementations. Virtual machines (VM) comprise the backend of most,\nif not all, cloud computing services. Several VMs are often consolidated on a\nphysical machine to better utilize its resources. We take into account the\ncooling and network structure of the datacenter hosting the physical machines\nwhen consolidating the VMs so that fewer racks and routers are employed,\nwithout compromising the service-level agreements, so that unused routing and\ncooling equipment can be turned off to reduce energy consumption. Our\nexperimental results on four benchmarks shows that our technique improves\nenergy consumption of servers, network equipment, and cooling systems by 2.5%,\n18.8%, and 28.2% respectively, resulting in a total of 14.7% improvement on\naverage in the entire datacenter."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2529v2", 
    "title": "VM-MAD: a cloud/cluster software for service-oriented academic   environments", 
    "arxiv-id": "1302.2529v2", 
    "author": "Christian Panse", 
    "publish": "2013-02-11T16:49:49Z", 
    "summary": "The availability of powerful computing hardware in IaaS clouds makes cloud\ncomputing attractive also for computational workloads that were up to now\nalmost exclusively run on HPC clusters.\n  In this paper we present the VM-MAD Orchestrator software: an open source\nframework for cloudbursting Linux-based HPC clusters into IaaS clouds but also\ncomputational grids. The Orchestrator is completely modular, allowing flexible\nconfigurations of cloudbursting policies. It can be used with any batch system\nor cloud infrastructure, dynamically extending the cluster when needed. A\ndistinctive feature of our framework is that the policies can be tested and\ntuned in a simulation mode based on historical or synthetic cluster accounting\ndata.\n  In the paper we also describe how the VM-MAD Orchestrator was used in a\nproduction environment at the FGCZ to speed up the analysis of mass\nspectrometry-based protein data by cloudbursting to the Amazon EC2. The\nadvantages of this hybrid system are shown with a large evaluation run using\nabout hundred large EC2 nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2543v1", 
    "title": "Byzantine Vector Consensus in Complete Graphs", 
    "arxiv-id": "1302.2543v1", 
    "author": "Vijay K. Garg", 
    "publish": "2013-02-11T17:25:24Z", 
    "summary": "Consider a network of n processes each of which has a d-dimensional vector of\nreals as its input. Each process can communicate directly with all the\nprocesses in the system; thus the communication network is a complete graph.\nAll the communication channels are reliable and FIFO (first-in-first-out). The\nproblem of Byzantine vector consensus (BVC) requires agreement on a\nd-dimensional vector that is in the convex hull of the d-dimensional input\nvectors at the non-faulty processes. We obtain the following results for\nByzantine vector consensus in complete graphs while tolerating up to f\nByzantine failures:\n  * We prove that in a synchronous system, n >= max(3f+1, (d+1)f+1) is\nnecessary and sufficient for achieving Byzantine vector consensus.\n  * In an asynchronous system, it is known that exact consensus is impossible\nin presence of faulty processes. For an asynchronous system, we prove that n >=\n(d+2)f+1 is necessary and sufficient to achieve approximate Byzantine vector\nconsensus.\n  Our sufficiency proofs are constructive. We show sufficiency by providing\nexplicit algorithms that solve exact BVC in synchronous systems, and\napproximate BVC in asynchronous systems.\n  We also obtain tight bounds on the number of processes for achieving BVC\nusing algorithms that are restricted to a simpler communication pattern."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.2749v2", 
    "title": "Practical Size-based Scheduling for MapReduce Workloads", 
    "arxiv-id": "1302.2749v2", 
    "author": "Pietro Michiardi", 
    "publish": "2013-02-12T10:11:29Z", 
    "summary": "We present the Hadoop Fair Sojourn Protocol (HFSP) scheduler, which\nimplements a size-based scheduling discipline for Hadoop. The benefits of\nsize-based scheduling disciplines are well recognized in a variety of contexts\n(computer networks, operating systems, etc...), yet, their practical\nimplementation for a system such as Hadoop raises a number of important\nchallenges. With HFSP, which is available as an open-source project, we address\nissues related to job size estimation, resource management and study the\neffects of a variety of preemption strategies. Although the architecture\nunderlying HFSP is suitable for any size-based scheduling discipline, in this\nwork we revisit and extend the Fair Sojourn Protocol, which solves problems\nrelated to job starvation that affect FIFO, Processor Sharing and a range of\nsize-based disciplines. Our experiments, in which we compare HFSP to standard\nHadoop schedulers, pinpoint at a significant decrease in average job sojourn\ntimes - a metric that accounts for the total time a job spends in the system,\nincluding waiting and serving times - for realistic workloads that we generate\naccording to production traces available in literature."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_34", 
    "link": "http://arxiv.org/pdf/1302.3344v2", 
    "title": "CORE: Augmenting Regenerating-Coding-Based Recovery for Single and   Concurrent Failures in Distributed Storage Systems", 
    "arxiv-id": "1302.3344v2", 
    "author": "Patrick P. C. Lee", 
    "publish": "2013-02-14T09:08:38Z", 
    "summary": "Data availability is critical in distributed storage systems, especially when\nnode failures are prevalent in real life. A key requirement is to minimize the\namount of data transferred among nodes when recovering the lost or unavailable\ndata of failed nodes. This paper explores recovery solutions based on\nregenerating codes, which are shown to provide fault-tolerant storage and\nminimum recovery bandwidth. Existing optimal regenerating codes are designed\nfor single node failures. We build a system called CORE, which augments\nexisting optimal regenerating codes to support a general number of failures\nincluding single and concurrent failures. We theoretically show that CORE\nachieves the minimum possible recovery bandwidth for most cases. We implement\nCORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to\n20 storage nodes. We demonstrate that our CORE prototype conforms to our\ntheoretical findings and achieves recovery bandwidth saving when compared to\nthe conventional recovery approach based on erasure codes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.3752v2", 
    "title": "Checkpointing algorithms and fault prediction", 
    "arxiv-id": "1302.3752v2", 
    "author": "Dounia Zaidouni", 
    "publish": "2013-02-15T13:52:50Z", 
    "summary": "This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We extend the classical first-order analysis of Young\nand Daly in the presence of a fault prediction system, characterized by its\nrecall and its precision. In this framework, we provide an optimal algorithm to\ndecide when to take predictions into account, and we derive the optimal value\nof the checkpointing period. These results allow to analytically assess the key\nparameters that impact the performance of fault predictors at very large scale."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4059v1", 
    "title": "Distributed Deterministic Broadcasting in Uniform-Power Ad Hoc Wireless   Networks", 
    "arxiv-id": "1302.4059v1", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2013-02-17T11:36:51Z", 
    "summary": "Development of many futuristic technologies, such as MANET, VANET, iThings,\nnano-devices, depend on efficient distributed communication protocols in\nmulti-hop ad hoc networks. A vast majority of research in this area focus on\ndesign heuristic protocols, and analyze their performance by simulations on\nnetworks generated randomly or obtained in practical measurements of some\n(usually small-size) wireless networks. %some library. Moreover, they often\nassume access to truly random sources, which is often not reasonable in case of\nwireless devices. In this work we use a formal framework to study the problem\nof broadcasting and its time complexity in any two dimensional Euclidean\nwireless network with uniform transmission powers. For the analysis, we\nconsider two popular models of ad hoc networks based on the\nSignal-to-Interference-and-Noise Ratio (SINR): one with opportunistic links,\nand the other with randomly disturbed SINR. In the former model, we show that\none of our algorithms accomplishes broadcasting in $O(D\\log^2 n)$ rounds, where\n$n$ is the number of nodes and $D$ is the diameter of the network. If nodes\nknow a priori the granularity $g$ of the network, i.e., the inverse of the\nmaximum transmission range over the minimum distance between any two stations,\na modification of this algorithm accomplishes broadcasting in $O(D\\log g)$\nrounds.\n  Finally, we modify both algorithms to make them efficient in the latter model\nwith randomly disturbed SINR, with only logarithmic growth of performance.\n  Ours are the first provably efficient and well-scalable, under the two\nmodels, distributed deterministic solutions for the broadcast task."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4414v2", 
    "title": "Approximation Algorithms for Energy Minimization in Cloud Service   Allocation under Reliability Constraints", 
    "arxiv-id": "1302.4414v2", 
    "author": "Paul Renaud-Goud", 
    "publish": "2013-02-18T20:33:15Z", 
    "summary": "We consider allocation problems that arise in the context of service\nallocation in Clouds. More specifically, we assume on the one part that each\ncomputing resource is associated to a capacity constraint, that can be chosen\nusing Dynamic Voltage and Frequency Scaling (DVFS) method, and to a probability\nof failure. On the other hand, we assume that the service runs as a set of\nindependent instances of identical Virtual Machines. Moreover, there exists a\nService Level Agreement (SLA) between the Cloud provider and the client that\ncan be expressed as follows: the client comes with a minimal number of service\ninstances which must be alive at the end of the day, and the Cloud provider\noffers a list of pairs (price,compensation), this compensation being paid by\nthe Cloud provider if it fails to keep alive the required number of services.\nOn the Cloud provider side, each pair corresponds actually to a guaranteed\nsuccess probability of fulfilling the constraint on the minimal number of\ninstances. In this context, given a minimal number of instances and a\nprobability of success, the question for the Cloud provider is to find the\nnumber of necessary resources, their clock frequency and an allocation of the\ninstances (possibly using replication) onto machines. This solution should\nsatisfy all types of constraints during a given time period while minimizing\nthe energy consumption of used resources. We consider two energy consumption\nmodels based on DVFS techniques, where the clock frequency of physical\nresources can be changed. For each allocation problem and each energy model, we\nprove deterministic approximation ratios on the consumed energy for algorithms\nthat provide guaranteed probability failures, as well as an efficient\nheuristic, whose energy ratio is not guaranteed."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4558v1", 
    "title": "Checkpointing strategies with prediction windows", 
    "arxiv-id": "1302.4558v1", 
    "author": "Dounia Zaidouni", 
    "publish": "2013-02-19T09:50:38Z", 
    "summary": "This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We suppose that the fault-prediction system provides\nprediction windows instead of exact predictions, which dramatically complicates\nthe analysis of the checkpointing strategies. We propose a new approach based\nupon two periodic modes, a regular mode outside prediction windows, and a\nproactive mode inside prediction windows, whenever the size of these windows is\nlarge enough. We are able to compute the best period for any size of the\nprediction windows, thereby deriving the scheduling strategy that minimizes\nplatform waste. In addition, the results of this analytical evaluation are\nnicely corroborated by a comprehensive set of simulations, which demonstrate\nthe validity of the model and the accuracy of the approach."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2013.10.010", 
    "link": "http://arxiv.org/pdf/1302.4779v1", 
    "title": "Failure Data Analysis of HPC Systems", 
    "arxiv-id": "1302.4779v1", 
    "author": "Charng-Da Lu", 
    "publish": "2013-02-20T00:21:44Z", 
    "summary": "Continuous availability of HPC systems built from commodity components have\nbecome a primary concern as system size grows to thousands of processors. In\nthis paper, we present the analysis of 8-24 months of real failure data\ncollected from three HPC systems at the National Center for Supercomputing\nApplications (NCSA) during 2001-2004. The results show that the availability is\n98.7-99.8% and most outages are due to software halts. On the other hand, the\ndowntime are mostly contributed by hardware halts or scheduled maintenance. We\nalso used failure clustering analysis to identify several correlated failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.4808v3", 
    "title": "Verifying the Consistency of Remote Untrusted Services with Commutative   Operations", 
    "arxiv-id": "1302.4808v3", 
    "author": "Olga Ohrimenko", 
    "publish": "2013-02-20T05:22:35Z", 
    "summary": "A group of mutually trusting clients outsources a computation service to a\nremote server, which they do not fully trust and that may be subject to\nattacks. The clients do not communicate with each other and would like to\nverify the correctness of the remote computation and the consistency of the\nserver's responses. This paper first presents the Commutative-Operation\nverification Protocol (COP) that ensures linearizability when the server is\ncorrect and preserves fork-linearizability in any other case. All clients that\nobserve each other's operations are consistent, in the sense that their own\noperations and those operations of other clients that they see are\nlinearizable. Second, this work extends COP through authenticated data\nstructures to Authenticated COP, which allows consistency verification of\noutsourced services whose state is kept only remotely, by the server. This\nyields the first fork-linearizable consistency verification protocol for\ngeneric outsourced services that (1) relieves clients from storing the state,\n(2) supports wait-free client operations, and (3) handles sequences of\narbitrary commutative operations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5161v3", 
    "title": "Verifying PRAM Consistency over Read/Write Traces of Data Replicas", 
    "arxiv-id": "1302.5161v3", 
    "author": "Jian Lu", 
    "publish": "2013-02-21T02:45:25Z", 
    "summary": "Data replication technologies enable efficient and highly-available data\naccess, thus gaining more and more interests in both the academia and the\nindustry. However, data replication introduces the problem of data consistency.\nModern commercial data replication systems often provide weak consistency for\nhigh availability under certain failure scenarios. An important weak\nconsistency is Pipelined-RAM (PRAM) consistency. It allows different processes\nto hold different views of data. To determine whether a data replication system\nindeed provides PRAM consistency, we study the problem of Verifying PRAM\nConsistency over read/write traces (or VPC, for short).\n  We first identify four variants of VPC according to a) whether there are\nMultiple shared variables (or one Single variable), and b) whether write\noperations can assign Duplicate values (or only Unique values) for each shared\nvariable; the four variants are labeled VPC-SU, VPC-MU, VPC-SD, and VPC-MD.\nSecond, we present a simple VPC-MU algorithm, called RW-CLOSURE. It constructs\nan operation graph $\\mathcal{G}$ by iteratively adding edges according to three\nrules. Its time complexity is $O(n^5)$, where n is the number of operations in\nthe trace. Third, we present an improved VPC-MU algorithm, called READ-CENTRIC,\nwith time complexity $O(n^4)$. Basically it attempts to construct the operation\ngraph $\\mathcal{G}$ in an incremental and efficient way. Its correctness is\nbased on that of RW-CLOSURE. Finally, we prove that VPC-SD (so is VPC-MD) is\n$\\sf{NP}$-complete by reducing the strongly $\\sf{NP}$-complete problem\n3-PARTITION to it."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5192v4", 
    "title": "The CORE Storage Primitive: Cross-Object Redundancy for Efficient Data   Repair & Access in Erasure Coded Storage", 
    "arxiv-id": "1302.5192v4", 
    "author": "Anwitaman Datta", 
    "publish": "2013-02-21T06:14:04Z", 
    "summary": "Erasure codes are an integral part of many distributed storage systems aimed\nat Big Data, since they provide high fault-tolerance for low overheads.\nHowever, traditional erasure codes are inefficient on reading stored data in\ndegraded environments (when nodes might be unavailable), and on replenishing\nlost data (vital for long term resilience). Consequently, novel codes optimized\nto cope with distributed storage system nuances are vigorously being\nresearched. In this paper, we take an engineering alternative, exploring the\nuse of simple and mature techniques -juxtaposing a standard erasure code with\nRAID-4 like parity. We carry out an analytical study to determine the efficacy\nof this approach over traditional as well as some novel codes. We build upon\nthis study to design CORE, a general storage primitive that we integrate into\nHDFS. We benchmark this implementation in a proprietary cluster and in EC2. Our\nexperiments show that compared to traditional erasure codes, CORE uses 50% less\nbandwidth and is up to 75% faster while recovering a single failed node, while\nthe gains are respectively 15% and 60% for double node failures."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5481v1", 
    "title": "Classification and Characterization of Core Grid Protocols for Global   Grid Computing", 
    "arxiv-id": "1302.5481v1", 
    "author": "Vipul K. Dabhi", 
    "publish": "2013-02-22T04:20:36Z", 
    "summary": "Grid computing has attracted many researchers over a few years, and as a\nresult many new protocols have emerged and also evolved since its inception a\ndecade ago. Grid protocols play major role in implementing services that\nfacilitate coordinated resource sharing across diverse organizations. In this\npaper, we provide comprehensive coverage of different core Grid protocols that\ncan be used in Global Grid Computing. We establish the classification of core\nGrid protocols into i) Grid network communication and Grid data transfer\nprotocols, ii) Grid information security protocols, iii) Grid resource\ninformation protocols, iv) Grid management protocols, and v) Grid interface\nprotocols, depending upon the kind of activities handled by these protocols.\nAll the classified protocols are also organized into layers of the Hourglass\nmodel of Grid architecture to understand dependency among these protocols. We\nalso present the characteristics of each protocol. For better understanding of\nthese protocols, we also discuss applied protocols as examples from either\nGlobus toolkit or other popular Grid middleware projects. We believe that our\nclassification and characterization of Grid protocols will enable better\nunderstanding of core Grid protocols and will motivate further research in the\narea of Global Grid Computing."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5646v1", 
    "title": "Comments on \"Resource placement in Cartesian product of networks\"   [Imani, Sarbazi-Azad and Zomaya, J. Parallel Distrib. Comput., 70 (2010)   481-495]", 
    "arxiv-id": "1302.5646v1", 
    "author": "Pranava K. Jha", 
    "publish": "2013-02-16T17:51:31Z", 
    "summary": "The present note points out a number of errors, omissions, redundancies and\narbitrary deviations from the standard terminology in the paper \"Resource\nplacement in Cartesian product of networks,\" by N. Imani, H. Sarbazi-Azad and\nA.Y. Zomaya [J. Parallel Distrib. Comput. 70 (2010) 481-495]."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5679v1", 
    "title": "Memory Aware Load Balance Strategy on a Parallel Branch-and-Bound   Application", 
    "arxiv-id": "1302.5679v1", 
    "author": "Artur A. Pessoa", 
    "publish": "2013-02-22T19:17:13Z", 
    "summary": "The latest trends in high-performance computing systems show an increasing\ndemand on the use of a large scale multicore systems in a efficient way, so\nthat high compute-intensive applications can be executed reasonably well.\nHowever, the exploitation of the degree of parallelism available at each\nmulticore component can be limited by the poor utilization of the memory\nhierarchy available. Actually, the multicore architecture introduces some\ndistinct features that are already observed in shared memory and distributed\nenvironments. One example is that subsets of cores can share different subsets\nof memory. In order to achieve high performance it is imperative that a careful\nallocation scheme of an application is carried out on the available cores,\nbased on a scheduling model that considers the main performance bottlenecks, as\nfor example, memory contention. In this paper, the {\\em Multicore Cluster\nModel} (MCM) is proposed, which captures the most relevant performance\ncharacteristics in multicores systems such as the influence of memory hierarchy\nand contention. Better performance was achieved when a load balance strategy\nfor a Branch-and-Bound application applied to the Partitioning Sets Problem is\nbased on MCM, showing its efficiency and applicability to modern systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.5999v1", 
    "title": "Distributed Wear levelling of Flash Memories", 
    "arxiv-id": "1302.5999v1", 
    "author": "K. Gopinath", 
    "publish": "2013-02-25T05:55:57Z", 
    "summary": "For large scale distributed storage systems, flash memories are an excellent\nchoice because flash memories consume less power, take lesser floor space for a\ntarget throughput and provide faster access to data. In a traditional\ndistributed filesystem, even distribution is required to ensure load-balancing,\nbalanced space utilisation and failure tolerance. In the presence of flash\nmemories, in addition, we should also ensure that the number of writes to these\ndifferent flash storage nodes are evenly distributed, to ensure even wear of\nflash storage nodes, so that unpredictable failures of storage nodes are\navoided. This requires that we distribute updates and do garbage collection,\nacross the flash storage nodes. We have motivated the distributed wearlevelling\nproblem considering the replica placement algorithm for HDFS. Viewing the\nwearlevelling across flash storage nodes as a distributed co-ordination\nproblem, we present an alternate design, to reduce the message communication\ncost across participating nodes. We demonstrate the effectiveness of our design\nthrough simulation"
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1302.6224v5", 
    "title": "Distributed Computability in Byzantine Asynchronous Systems", 
    "arxiv-id": "1302.6224v5", 
    "author": "Maurice Herlihy", 
    "publish": "2013-02-25T20:50:25Z", 
    "summary": "In this work, we extend the topology-based approach for characterizing\ncomputability in asynchronous crash-failure distributed systems to asynchronous\nByzantine systems. We give the first theorem with necessary and sufficient\nconditions to solve arbitrary tasks in asynchronous Byzantine systems where an\nadversary chooses faulty processes. In our adversarial formulation, outputs of\nnon-faulty processes are constrained in terms of inputs of non-faulty processes\nonly. For colorless tasks, an important subclass of distributed problems, the\ngeneral result reduces to an elegant model that effectively captures the\nrelation between the number of processes, the number of failures, as well as\nthe topological structure of the task's simplicial complexes."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.1379v1", 
    "title": "GPU accelerated maximum cardinality matching algorithms for bipartite   graphs", 
    "arxiv-id": "1303.1379v1", 
    "author": "Umit V. Catalyurek", 
    "publish": "2013-03-06T16:38:37Z", 
    "summary": "We design, implement, and evaluate GPU-based algorithms for the maximum\ncardinality matching problem in bipartite graphs. Such algorithms have a\nvariety of applications in computer science, scientific computing,\nbioinformatics, and other areas. To the best of our knowledge, ours is the\nfirst study which focuses on GPU implementation of the maximum cardinality\nmatching algorithms. We compare the proposed algorithms with serial and\nmulticore implementations from the literature on a large set of real-life\nproblems where in majority of the cases one of our GPU-accelerated algorithms\nis demonstrated to be faster than both the sequential and multicore\nimplementations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2043v1", 
    "title": "Orientation and Connectivity Based Criteria for Asymptotic Consensus", 
    "arxiv-id": "1303.2043v1", 
    "author": "Bernadette Charron-Bost", 
    "publish": "2013-03-08T16:22:46Z", 
    "summary": "In this article, we establish orientation and connectivity based criteria for\nthe agreement algorithm to achieve asymptotic consensus in the context of\ntime-varying topology and communication delays. These criteria unify and extend\nmany earlier convergence results on the agreement algorithm for deterministic\nand discrete-time multiagent systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2171v1", 
    "title": "CPU and/or GPU: Revisiting the GPU Vs. CPU Myth", 
    "arxiv-id": "1303.2171v1", 
    "author": "Shubham Gupta", 
    "publish": "2013-03-09T05:35:31Z", 
    "summary": "Parallel computing using accelerators has gained widespread research\nattention in the past few years. In particular, using GPUs for general purpose\ncomputing has brought forth several success stories with respect to time taken,\ncost, power, and other metrics. However, accelerator based computing has\nsignifi- cantly relegated the role of CPUs in computation. As CPUs evolve and\nalso offer matching computational resources, it is important to also include\nCPUs in the computation. We call this the hybrid computing model. Indeed, most\ncomputer systems of the present age offer a degree of heterogeneity and\ntherefore such a model is quite natural.\n  We reevaluate the claim of a recent paper by Lee et al.(ISCA 2010). We argue\nthat the right question arising out of Lee et al. (ISCA 2010) should be how to\nuse a CPU+GPU platform efficiently, instead of whether one should use a CPU or\na GPU exclusively. To this end, we experiment with a set of 13 diverse\nworkloads ranging from databases, image processing, sparse matrix kernels, and\ngraphs. We experiment with two different hybrid platforms: one consisting of a\n6-core Intel i7-980X CPU and an NVidia Tesla T10 GPU, and another consisting of\nan Intel E7400 dual core CPU with an NVidia GT520 GPU. On both these platforms,\nwe show that hybrid solutions offer good advantage over CPU or GPU alone\nsolutions. On both these platforms, we also show that our solutions are 90%\nresource efficient on average.\n  Our work therefore suggests that hybrid computing can offer tremendous\nadvantages at not only research-scale platforms but also the more realistic\nscale systems with significant performance gains and resource efficiency to the\nlarge scale user community."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14472-6_1", 
    "link": "http://arxiv.org/pdf/1303.2619v1", 
    "title": "Making Systems More Robust with Flexible RPC Lookup", 
    "arxiv-id": "1303.2619v1", 
    "author": "Russell Power", 
    "publish": "2013-03-11T19:05:37Z", 
    "summary": "Modern distributed systems use names everywhere. Lockservices such as Chubby\nand ZooKeeper provide an effective mechanism for mapping from application names\nto server instances, but proper usage of them requires a large amount of\nerror-prone boiler-plate code.\n  Application programmers often try to write wrappers to abstract away this\nlogic, but it turns out there is a more general and easier way of handling the\nissue. We show that by extending the existing name resolution capabilities of\nRPC libraries, we can remove the need for such annoying boiler-plate code while\nat the same time making our services more robust."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.3626v1", 
    "title": "Non-blocking Patricia Tries with Replace Operations", 
    "arxiv-id": "1303.3626v1", 
    "author": "Niloufar Shafiei", 
    "publish": "2013-03-14T22:14:36Z", 
    "summary": "This paper presents a non-blocking Patricia trie implementation for an\nasynchronous shared-memory system using Compare&Swap. The trie implements a\nlinearizable set and supports three update operations: insert adds an element,\ndelete removes an element and replace replaces one element by another. The\nreplace operation is interesting because it changes two different locations of\ntree atomically. If all update operations modify different parts of the trie,\nthey run completely concurrently. The implementation also supports a wait-free\nfind operation, which only reads shared memory and never changes the data\nstructure. Empirically, we compare our algorithms to some existing set\nimplementations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.4191v4", 
    "title": "Parallel Search with Extended Fibonacci Primitive", 
    "arxiv-id": "1303.4191v4", 
    "author": "N. Sadagopan", 
    "publish": "2013-03-18T09:03:56Z", 
    "summary": "Search pattern experienced by the processor to search an element in secondary\nstorage devices follows a random sequence. Formally, it is a random walk and\nits modeling is crucial in studying performance metrics like memory access\ntime. In this paper, we first model the random walk using extended Fibonacci\nseries. Our simulation is done on a parallel computing model (PRAM) with EREW\nstrategy. Three search primitives are proposed under parallel computing model\nand each primitive is thoroughly tested on an array of size $10^7$ with the\nsize of random walk being $10^4$. Our findings reveal that search primitive\nwith pointer jumping is better than the other two primitives. Our key\ncontribution lies in modeling random walk as an extended Fibonacci series\ngenerator and simulating the same with various search primitives."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.4312v2", 
    "title": "Perfectly load-balanced, optimal, stable, parallel merge", 
    "arxiv-id": "1303.4312v2", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2013-03-18T16:40:58Z", 
    "summary": "We present a simple, work-optimal and synchronization-free solution to the\nproblem of stably merging in parallel two given, ordered arrays of m and n\nelements into an ordered array of m+n elements. The main contribution is a new,\nsimple, fast and direct algorithm that determines, for any prefix of the stably\nmerged output sequence, the exact prefixes of each of the two input sequences\nneeded to produce this output prefix. More precisely, for any given index\n(rank) in the resulting, but not yet constructed output array representing an\noutput prefix, the algorithm computes the indices (co-ranks) in each of the two\ninput arrays representing the required input prefixes without having to merge\nthe input arrays. The co-ranking algorithm takes O(log min(m,n)) time steps.\nThe algorithm is used to devise a perfectly load-balanced, stable, parallel\nmerge algorithm where each of p processing elements has exactly the same number\nof input elements to merge. Compared to other approaches to the parallel merge\nproblem, our algorithm is considerably simpler and can be faster up to a factor\nof two. Compared to previous algorithms for solving the co-ranking problem, the\nalgorithm given here is direct and maintains stability in the presence of\nrepeated elements at no extra space or time cost. When the number of processing\nelements p does not exceed (m+n)/log min(m,n), the parallel merge algorithm has\noptimal speedup. It is easy to implement on both shared and distributed memory\nparallel systems."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2013.43", 
    "link": "http://arxiv.org/pdf/1303.5164v1", 
    "title": "Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and   Scheduling", 
    "arxiv-id": "1303.5164v1", 
    "author": "Bingsheng He", 
    "publish": "2013-03-21T04:50:48Z", 
    "summary": "Graphics processors, or GPUs, have recently been widely used as accelerators\nin the shared environments such as clusters and clouds. In such shared\nenvironments, many kernels are submitted to GPUs from different users, and\nthroughput is an important metric for performance and total ownership cost.\nDespite the recently improved runtime support for concurrent GPU kernel\nexecutions, the GPU can be severely underutilized, resulting in suboptimal\nthroughput. In this paper, we propose Kernelet, a runtime system with dynamic\nslicing and scheduling techniques to improve the throughput of concurrent\nkernel executions on the GPU. With slicing, Kernelet divides a GPU kernel into\nmultiple sub-kernels (namely slices). Each slice has tunable occupancy to allow\nco-scheduling with other slices and to fully utilize the GPU resources. We\ndevelop a novel and effective Markov chain based performance model to guide the\nscheduling decision. Our experimental results demonstrate up to 31.1% and 23.4%\nperformance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5275v1", 
    "title": "Achieving Efficient Strong Scaling with PETSc using Hybrid MPI/OpenMP   Optimisation", 
    "arxiv-id": "1303.5275v1", 
    "author": "James Southern", 
    "publish": "2013-03-21T14:56:02Z", 
    "summary": "The increasing number of processing elements and decreas- ing memory to core\nratio in modern high-performance platforms makes efficient strong scaling a key\nrequirement for numerical algorithms. In order to achieve efficient scalability\non massively parallel systems scientific software must evolve across the entire\nstack to exploit the multiple levels of parallelism exposed in modern\narchitectures. In this paper we demonstrate the use of hybrid MPI/OpenMP\nparallelisation to optimise parallel sparse matrix-vector multiplication in\nPETSc, a widely used scientific library for the scalable solution of partial\ndifferential equations. Using large matrices generated by Fluidity, an open\nsource CFD application code which uses PETSc as its linear solver engine, we\nevaluate the effect of explicit communication overlap using task-based\nparallelism and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. We demonstrate a significant speedup\nover the pure-MPI mode and efficient strong scaling of sparse matrix-vector\nmultiplication on Fujitsu PRIMEHPC FX10 and Cray XE6 systems."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5837v1", 
    "title": "Multilevel communication optimal LU and QR factorizations for   hierarchical platforms", 
    "arxiv-id": "1303.5837v1", 
    "author": "Amal Khabou", 
    "publish": "2013-03-23T11:40:50Z", 
    "summary": "This study focuses on the performance of two classical dense linear algebra\nalgorithms, the LU and the QR factorizations, on multilevel hierarchical\nplatforms. We first introduce a new model called Hierarchical Cluster Platform\n(HCP), encapsulating the characteristics of such platforms. The focus is set on\nreducing the communication requirements of studied algorithms at each level of\nthe hierarchy. Lower bounds on communications are therefore extended with\nrespect to the HCP model. We then introduce multilevel LU and QR algorithms\ntailored for those platforms, and provide a detailed performance analysis. We\nalso provide a set of numerical experiments and performance predictions\ndemonstrating the need for such algorithms on large platforms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.5891v1", 
    "title": "Fault Tolerance in Distributed Systems using Fused State Machines", 
    "arxiv-id": "1303.5891v1", 
    "author": "Vijay K. Garg", 
    "publish": "2013-03-23T23:03:34Z", 
    "summary": "Replication is a standard technique for fault tolerance in distributed\nsystems modeled as deterministic finite state machines (DFSMs or machines). To\ncorrect f crash or f/2 Byzantine faults among n different machines, replication\nrequires nf additional backup machines. We present a solution called fusion\nthat requires just f additional backup machines. First, we build a framework\nfor fault tolerance in DFSMs based on the notion of Hamming distances. We\nintroduce the concept of an (f,m)-fusion, which is a set of m backup machines\nthat can correct f crash faults or f/2 Byzantine faults among a given set of\nmachines. Second, we present an algorithm to generate an (f,f)-fusion for a\ngiven set of machines. We ensure that our backups are efficient in terms of the\nsize of their state and event sets. Our evaluation of fusion on the widely used\nMCNC'91 benchmarks for DFSMs show that the average state space savings in\nfusion (over replication) is 38% (range 0-99%). To demonstrate the practical\nuse of fusion, we describe its potential application to the MapReduce\nframework. Using a simple case study, we compare replication and fusion as\napplied to this framework. While a pure replication-based solution requires 1.8\nmillion map tasks, our fusion-based solution requires only 1.4 million map\ntasks with minimal overhead during normal operation or recovery. Hence, fusion\nresults in considerable savings in state space and other resources such as the\npower needed to run the backup tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_8", 
    "link": "http://arxiv.org/pdf/1303.7195v1", 
    "title": "Parallelization in Scientific Workflow Management Systems", 
    "arxiv-id": "1303.7195v1", 
    "author": "Ulf Leser", 
    "publish": "2013-03-28T18:20:17Z", 
    "summary": "Over the last two decades, scientific workflow management systems (SWfMS)\nhave emerged as a means to facilitate the design, execution, and monitoring of\nreusable scientific data processing pipelines. At the same time, the amounts of\ndata generated in various areas of science outpaced enhancements in\ncomputational power and storage capabilities. This is especially true for the\nlife sciences, where new technologies increased the sequencing throughput from\nkilobytes to terabytes per day. This trend requires current SWfMS to adapt:\nNative support for parallel workflow execution must be provided to increase\nperformance; dynamically scalable \"pay-per-use\" compute infrastructures have to\nbe integrated to diminish hardware costs; adaptive scheduling of workflows in\ndistributed compute environments is required to optimize resource utilization.\nIn this survey we give an overview of parallelization techniques for SWfMS,\nboth in theory and in their realization in concrete systems. We find that\ncurrent systems leave considerable room for improvement and we propose key\nadvancements to the landscape of SWfMS."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1303.7270v1", 
    "title": "Data-Intensive Workload Consolidation on Hadoop Distributed File System", 
    "arxiv-id": "1303.7270v1", 
    "author": "Albert Y. Zomaya", 
    "publish": "2013-03-28T23:15:36Z", 
    "summary": "Workload consolidation, sharing physical resources among multiple workloads,\nis a promising technique to save cost and energy in cluster computing systems.\nThis paper highlights a few challenges of workload consolidation for Hadoop as\none of the current state-of-the-art data-intensive cluster computing system.\nThrough a systematic step-by-step procedure, we investigate challenges for\nefficient server consolidation in Hadoop environments. To this end, we first\ninvestigate the inter-relationship between last level cache (LLC) contention\nand throughput degradation for consolidated workloads on a single physical\nserver employing Hadoop distributed file system (HDFS). We then investigate the\ngeneral case of consolidation on multiple physical servers so that their\nthroughput never falls below a desired/predefined utilization level. We use our\nempirical results to model consolidation as a classic two-dimensional bin\npacking problem and then design a computationally efficient greedy algorithm to\nachieve minimum throughput degradation on multiple servers. Results are very\npromising and show that our greedy approach is able to achieve near optimal\nsolution in all experimented cases."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1303.7300v1", 
    "title": "Queuing Methodology Based Power Efficient Routing Protocol for Reliable   Data Communications in Manets", 
    "arxiv-id": "1303.7300v1", 
    "author": "M. K. Kaushik", 
    "publish": "2013-03-29T06:01:08Z", 
    "summary": "A mobile ad hoc network (MANET) is a wireless network that uses multi-hop\npeer-to- peer routing instead of static network infrastructure to provide\nnetwork connectivity. MANETs have applications in rapidly deployed and dynamic\nmilitary and civilian systems. The network topology in a MANET usually changes\nwith time. Therefore, there are new challenges for routing protocols in MANETs\nsince traditional routing protocols may not be suitable for MANETs. In recent\nyears, a variety of new routing protocols targeted specifically at this\nenvironment have been developed, but little performance information on each\nprotocol and no realistic performance comparison between them is available.\nThis paper presents the results of a detailed packet-level simulation comparing\nthree multi-hop wireless ad hoc network routing protocols that cover a range of\ndesign choices: DSR, NFPQR, and clustered NFPQR. By applying queuing\nmethodology to the introduced routing protocol the reliability and throughput\nof the network is increased."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1308.0083v1", 
    "title": "Dominant Resource Fairness in Cloud Computing Systems with Heterogeneous   Servers", 
    "arxiv-id": "1308.0083v1", 
    "author": "Ben Liang", 
    "publish": "2013-08-01T03:08:22Z", 
    "summary": "We study the multi-resource allocation problem in cloud computing systems\nwhere the resource pool is constructed from a large number of heterogeneous\nservers, representing different points in the configuration space of resources\nsuch as processing, memory, and storage. We design a multi-resource allocation\nmechanism, called DRFH, that generalizes the notion of Dominant Resource\nFairness (DRF) from a single server to multiple heterogeneous servers. DRFH\nprovides a number of highly desirable properties. With DRFH, no user prefers\nthe allocation of another user; no one can improve its allocation without\ndecreasing that of the others; and more importantly, no user has an incentive\nto lie about its resource demand. As a direct application, we design a simple\nheuristic that implements DRFH in real-world systems. Large-scale simulations\ndriven by Google cluster traces show that DRFH significantly outperforms the\ntraditional slot-based scheduler, leading to much higher resource utilization\nwith substantially shorter job completion times."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Grid.2012.25", 
    "link": "http://arxiv.org/pdf/1308.0148v1", 
    "title": "Balancing indivisible real-valued loads in arbitrary networks", 
    "arxiv-id": "1308.0148v1", 
    "author": "Ivo F. Sbalzarini", 
    "publish": "2013-08-01T10:32:33Z", 
    "summary": "In parallel computing, a problem is divided into a set of smaller tasks that\nare distributed across multiple processing elements. Balancing the load of the\nprocessing elements is key to achieving good performance and scalability. If\nthe computational costs of the individual tasks vary over time in an\nunpredictable way, dynamic load balancing aims at migrating them between\nprocessing elements so as to maintain load balance. During dynamic load\nbalancing, the tasks amount to indivisible work packets with a real-valued\ncost. For this case of indivisible, real- valued loads, we analyze the\nbalancing circuit model, a local dynamic load-balancing scheme that does not\nrequire global communication. We extend previous analyses to the present case\nand provide a probabilistic bound for the achievable load balance. Based on an\nanalogy with the offline balls-into-bins problem, we further propose a novel\nalgorithm for dynamic balancing of indivisible, real-valued loads. We benchmark\nthe proposed algorithm in numerical experiments and compare it with the\nclassical greedy algorithm, both in terms of solution quality and communication\ncost. We find that the increased communication cost of the proposed algorithm\nis compensated by a higher solution quality, leading on average to about an\norder of magnitude gain in overall performance."
},{
    "category": "cs.DC", 
    "doi": "10.5120/12908-0036", 
    "link": "http://arxiv.org/pdf/1308.0568v1", 
    "title": "Visualization of Job Scheduling in Grid Computers", 
    "arxiv-id": "1308.0568v1", 
    "author": "M A El-dosuky", 
    "publish": "2013-08-02T18:09:23Z", 
    "summary": "One of the hot problems in grid computing is job scheduling. It is known that\nthe job scheduling is NP-complete, and thus the use of heuristics is the de\nfacto approach to deal with this practice in its difficulty. The proposed is an\nimagination to fish swarm, job dispatcher and Visualization gridsim to execute\nsome jobs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1031v1", 
    "title": "Nephele Streaming: Stream Processing Under QoS Constraints At Scale", 
    "arxiv-id": "1308.1031v1", 
    "author": "Odej Kao", 
    "publish": "2013-08-05T16:15:58Z", 
    "summary": "The ability to process large numbers of continuous data streams in a\nnear-real-time fashion has become a crucial prerequisite for many scientific\nand industrial use cases in recent years. While the individual data streams are\nusually trivial to process, their aggregated data volumes easily exceed the\nscalability of traditional stream processing systems. At the same time,\nmassively-parallel data processing systems like MapReduce or Dryad currently\nenjoy a tremendous popularity for data-intensive applications and have proven\nto scale to large numbers of nodes. Many of these systems also provide\nstreaming capabilities. However, unlike traditional stream processors, these\nsystems have disregarded QoS requirements of prospective stream processing\napplications so far. In this paper we address this gap. First, we analyze\ncommon design principles of today's parallel data processing frameworks and\nidentify those principles that provide degrees of freedom in trading off the\nQoS goals latency and throughput. Second, we propose a highly distributed\nscheme which allows these frameworks to detect violations of user-defined QoS\nconstraints and optimize the job execution without manual interaction. As a\nproof of concept, we implemented our approach for our massively-parallel data\nprocessing framework Nephele and evaluated its effectiveness through a\ncomparison with Hadoop Online. For an example streaming application from the\nmultimedia domain running on a cluster of 200 nodes, our approach improves the\nprocessing latency by a factor of at least 13 while preserving high data\nthroughput when needed."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1303v1", 
    "title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service", 
    "arxiv-id": "1308.1303v1", 
    "author": "Shanmugapriyaa", 
    "publish": "2013-08-05T06:11:12Z", 
    "summary": "Enterprises are driving towards less cost, more availability, agility,\nmanaged risk - all of which is accelerated towards Cloud Computing. Cloud is\nnot a particular product, but a way of delivering IT services that are\nconsumable on demand, elastic to scale up and down as needed, and follow a\npay-for-usage model. Out of the three common types of cloud computing service\nmodels, Infrastructure as a Service (IaaS) is a service model that provides\nservers, computing power, network bandwidth and Storage capacity, as a service\nto their subscribers. Cloud can relate to many things but without the\nfundamental storage pieces, which is provided as a service namely Cloud\nStorage, none of the other applications is possible. This paper introduces\nCloud Storage, which covers the key technologies in cloud computing and Cloud\nStorage, management insights about cloud computing, different types of cloud\nservices, driving forces of cloud computing and cloud storage, advantages and\nchallenges of cloud storage and concludes by pinpointing few challenges to be\naddressed by the cloud storage providers."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1343v1", 
    "title": "Performance and Optimization Abstractions for Large Scale Heterogeneous   Systems in the Cactus/Chemora Framework", 
    "arxiv-id": "1308.1343v1", 
    "author": "Erik Schnetter", 
    "publish": "2013-08-06T16:40:47Z", 
    "summary": "We describe a set of lower-level abstractions to improve performance on\nmodern large scale heterogeneous systems. These provide portable access to\nsystem- and hardware-dependent features, automatically apply dynamic\noptimizations at run time, and target stencil-based codes used in finite\ndifferencing, finite volume, or block-structured adaptive mesh refinement\ncodes.\n  These abstractions include a novel data structure to manage refinement\ninformation for block-structured adaptive mesh refinement, an iterator\nmechanism to efficiently traverse multi-dimensional arrays in stencil-based\ncodes, and a portable API and implementation for explicit SIMD vectorization.\n  These abstractions can either be employed manually, or be targeted by\nautomated code generation, or be used via support libraries by compilers during\ncode generation. The implementations described below are available in the\nCactus framework, and are used e.g. in the Einstein Toolkit for relativistic\nastrophysics simulations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1358v1", 
    "title": "The Performance of Paxos and Fast Paxos", 
    "arxiv-id": "1308.1358v1", 
    "author": "Luiz E. Buzato", 
    "publish": "2013-08-06T17:44:13Z", 
    "summary": "Paxos and Fast Paxos are optimal consensus algorithms that are simple and\nelegant, while suitable for efficient implementation. In this paper, we compare\nthe performance of both algorithms in failure-free and failure-prone runs using\nTreplica, a general replication toolkit that implements these algorithms in a\nmodular and efficient manner. We have found that Paxos outperforms Fast Paxos\nfor small number of replicas and that collisions are not the cause of this\nperformance difference."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1763v1", 
    "title": "Linear Network Coding on Multi-Mesh of Trees (MMT) using All to All   Broadcast (AAB)", 
    "arxiv-id": "1308.1763v1", 
    "author": "Vipin Tyagi", 
    "publish": "2013-08-08T06:21:21Z", 
    "summary": "We introduce linear network coding on parallel architecture for multi-source\nfinite acyclic network. In this problem, different messages in diverse time\nperiods are broadcast and every nonsource node in the network decodes and\nencodes the message based on further communication.We wish to minimize the\ncommunication steps and time complexity involved in transfer of data from\nnode-to-node during parallel communication.We have used Multi-Mesh of Trees\n(MMT) topology for implementing network coding. To envisage our result, we use\nall-to-all broadcast as communication algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.1806v1", 
    "title": "A Survey of Current Trends in Distributed, Grid and Cloud Computing", 
    "arxiv-id": "1308.1806v1", 
    "author": "Kuldeep Goswami", 
    "publish": "2013-08-08T10:23:09Z", 
    "summary": "Through the 1990s to 2012 the internet changed the world of computing\ndrastically. It started its journey with parallel computing after it advanced\nto distributed computing and further to grid computing. And in present scenario\nit creates a new world which is pronounced as a Cloud Computing [1]. These all\nthree terms have different meanings. Cloud computing is based on backward\ncomputing schemes like cluster computing, distributed computing, grid computing\nand utility computing. The basic concept of cloud computing is virtualization.\nIt provides virtual hardware and software resources to various requesting\nprograms. This paper gives a detailed description about cluster computing, grid\ncomputing and cloud computing and gives an insight of some implementations of\nthe same. We try to list the inspirations for the advent of all these\ntechnologies. We also account for some present scenario faults of grid\ncomputing and also discuss new cloud computing projects which are being managed\nby the Government of India for learning. The paper also reviews the existing\nwork and covers (analytically), to some extent, some innovative ideas that can\nbe implemented."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2480v1", 
    "title": "A thread-parallel algorithm for anisotropic mesh adaptation", 
    "arxiv-id": "1308.2480v1", 
    "author": "Paul H. J. Kelly", 
    "publish": "2013-08-12T07:45:15Z", 
    "summary": "Anisotropic mesh adaptation is a powerful way to directly minimise the\ncomputational cost of mesh based simulation. It is particularly important for\nmulti-scale problems where the required number of floating-point operations can\nbe reduced by orders of magnitude relative to more traditional static mesh\napproaches.\n  Increasingly, finite element and finite volume codes are being optimised for\nmodern multi-core architectures. Typically, decomposition methods implemented\nthrough the Message Passing Interface (MPI) are applied for inter-node\nparallelisation, while a threaded programming model, such as OpenMP, is used\nfor intra-node parallelisation. Inter-node parallelism for mesh adaptivity has\nbeen successfully implemented by a number of groups. However, thread-level\nparallelism is significantly more challenging because the underlying data\nstructures are extensively modified during mesh adaptation and a greater degree\nof parallelism must be realised.\n  In this paper we describe a new thread-parallel algorithm for anisotropic\nmesh adaptation algorithms. For each of the mesh optimisation phases\n(refinement, coarsening, swapping and smoothing) we describe how independent\nsets of tasks are defined. We show how a deferred updates strategy can be used\nto update the mesh data structures in parallel and without data contention. We\nshow that despite the complex nature of mesh adaptation and inherent load\nimbalances in the mesh adaptivity, a parallel efficiency of 60% is achieved on\nan 8 core Intel Xeon Sandybridge, and a 40% parallel efficiency is achieved\nusing 16 cores in a 2 socket Intel Xeon Sandybridge ccNUMA system."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2694v1", 
    "title": "A Super-Fast Distributed Algorithm for Bipartite Metric Facility   Location", 
    "arxiv-id": "1308.2694v1", 
    "author": "Sriram V. Pemmaraju", 
    "publish": "2013-08-12T20:45:17Z", 
    "summary": "The \\textit{facility location} problem consists of a set of\n\\textit{facilities} $\\mathcal{F}$, a set of \\textit{clients} $\\mathcal{C}$, an\n\\textit{opening cost} $f_i$ associated with each facility $x_i$, and a\n\\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client\n$y_j$. The goal is to find a subset of facilities to \\textit{open}, and to\nconnect each client to an open facility, so as to minimize the total facility\nopening costs plus connection costs. This paper presents the first\nexpected-sub-logarithmic-round distributed O(1)-approximation algorithm in the\n$\\mathcal{CONGEST}$ model for the \\textit{metric} facility location problem on\nthe complete bipartite network with parts $\\mathcal{F}$ and $\\mathcal{C}$. Our\nalgorithm has an expected running time of $O((\\log \\log n)^3)$ rounds, where $n\n= |\\mathcal{F}| + |\\mathcal{C}|$. This result can be viewed as a continuation\nof our recent work (ICALP 2012) in which we presented the first\nsub-logarithmic-round distributed O(1)-approximation algorithm for metric\nfacility location on a \\textit{clique} network. The bipartite setting presents\nseveral new challenges not present in the problem on a clique network. We\npresent two new techniques to overcome these challenges. (i) In order to deal\nwith the problem of not being able to choose appropriate probabilities (due to\nlack of adequate knowledge), we design an algorithm that performs a random walk\nover a probability space and analyze the progress our algorithm makes as the\nrandom walk proceeds. (ii) In order to deal with a problem of quickly\ndisseminating a collection of messages, possibly containing many duplicates,\nover the bipartite network, we design a probabilistic hashing scheme that\ndelivers all of the messages in expected-$O(\\log \\log n)$ rounds."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.2979v5", 
    "title": "On Barriers and the Gap between Active and Passive Replication (Full   Version)", 
    "arxiv-id": "1308.2979v5", 
    "author": "Marco Serafini", 
    "publish": "2013-08-13T21:09:27Z", 
    "summary": "Active replication is commonly built on top of the atomic broadcast\nprimitive. Passive replication, which has been recently used in the popular\nZooKeeper coordination system, can be naturally built on top of the\nprimary-order atomic broadcast primitive. Passive replication differs from\nactive replication in that it requires processes to cross a barrier before they\nbecome primaries and start broadcasting messages. In this paper, we propose a\nbarrier function tau that explains and encapsulates the differences between\nexisting primary-order atomic broadcast algorithms, namely semi-passive\nreplication and Zookeeper atomic broadcast (Zab), as well as the differences\nbetween Paxos and Zab. We also show that implementing primary-order atomic\nbroadcast on top of a generic consensus primitive and tau inherently results in\nhigher time complexity than atomic broadcast, as witnessed by existing\nalgorithms. We overcome this problem by presenting an alternative,\nprimary-order atomic broadcast implementation that builds on top of a generic\nconsensus primitive and uses consensus itself to form a barrier. This algorithm\nis modular and matches the time complexity of existing tau-based algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.3648v1", 
    "title": "On the State and Importance of Reproducible Experimental Research in   Parallel Computing", 
    "arxiv-id": "1308.3648v1", 
    "author": "Jesper Larsson Tr\u00e4ff", 
    "publish": "2013-08-16T15:11:35Z", 
    "summary": "Computer science is also an experimental science. This is particularly the\ncase for parallel computing, which is in a total state of flux, and where\nexperiments are necessary to substantiate, complement, and challenge\ntheoretical modeling and analysis. Here, experimental work is as important as\nare advances in theory, that are indeed often driven by the experimental\nfindings. In parallel computing, scientific contributions presented in research\narticles are therefore often based on experimental data, with a substantial\npart devoted to presenting and discussing the experimental findings. As in all\nof experimental science, experiments must be presented in a way that makes\nreproduction by other researchers possible, in principle. Despite appearance to\nthe contrary, we contend that reproducibility plays a small role, and is\ntypically not achieved. As can be found, articles often do not have a\nsufficiently detailed description of their experiments, and do not make\navailable the software used to obtain the claimed results. As a consequence,\nparallel computational results are most often impossible to reproduce, often\nquestionable, and therefore of little or no scientific value. We believe that\nthe description of how to reproduce findings should play an important part in\nevery serious, experiment-based parallel computing research article. We aim to\ninitiate a discussion of the reproducibility issue in parallel computing, and\nelaborate on the importance of reproducible research for (1) better and sounder\ntechnical/scientific papers, (2) a sounder and more efficient review process\nand (3) more effective collective work. This paper expresses our current view\non the subject and should be read as a position statement for discussion and\nfuture work. We do not consider the related (but no less important) issue of\nthe quality of the experimental design."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.4166v1", 
    "title": "Patience-aware Scheduling for Cloud Services: Freeing Users from the   Chains of Boredom", 
    "arxiv-id": "1308.4166v1", 
    "author": "Carlos Queiroz", 
    "publish": "2013-08-19T20:30:38Z", 
    "summary": "Scheduling of service requests in Cloud computing has traditionally focused\non the reduction of pre-service wait, generally termed as waiting time. Under\ncertain conditions such as peak load, however, it is not always possible to\ngive reasonable response times to all users. This work explores the fact that\ndifferent users may have their own levels of tolerance or patience with\nresponse delays. We introduce scheduling strategies that produce better\nassignment plans by prioritising requests from users who expect to receive the\nresults earlier and by postponing servicing jobs from those who are more\ntolerant to response delays. Our analytical results show that the behaviour of\nusers' patience plays a key role in the evaluation of scheduling techniques,\nand our computational evaluation demonstrates that, under peak load, the new\nalgorithms typically provide better user experience than the traditional FIFO\nstrategy."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.4208v1", 
    "title": "A Systematic Mapping Study on Cloud Computing", 
    "arxiv-id": "1308.4208v1", 
    "author": "Frederico Durao", 
    "publish": "2013-08-20T02:17:27Z", 
    "summary": "Cloud Computing emerges from the global economic crisis as an option to use\ncomputing resources from a more rational point of view. In other words, a\ncheaper way to have IT resources. However, issues as security and privacy, SLA\n(Service Layer Agreement), resource sharing, and billing has left open\nquestions about the real gains of that model. This study aims to investigate\nstate-of-the-art in Cloud Computing, identify gaps, challenges, synthesize\navailable evidences both its use and development, and provides relevant\ninformation, clarifying open questions and common discussed issues about that\nmodel through literature. The good practices of systematic map- ping study\nmethodology were adopted in order to reach those objectives. Al- though Cloud\nComputing is based on a business model with over 50 years of existence,\nevidences found in this study indicate that Cloud Computing still presents\nlimitations that prevent the full use of the proposal on-demand."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.6058v1", 
    "title": "Data Grid Concepts for Data Security in Distributed Computing", 
    "arxiv-id": "1308.6058v1", 
    "author": "V. Sangeetha", 
    "publish": "2013-08-28T04:51:08Z", 
    "summary": "Data grid is a distributed computing architecture that integrates a large\nnumber of data and computing resources into a single virtual data management\nsystem. It enables the sharing and coordinated use of data from various\nresources and provides various services to fit the needs of high performance\ndistributed and data-intensive computing. Here data partitioning and dynamic\nreplication in data grid are considered. In which security and access\nperformance of a system are efficient. There are several important requirements\nfor data grids, including information survivability, security, and access\nperformance. More specifically, the investigation is the problem of optimal\nallocation of sensitive data objects that are partitioned by using secret\nsharing scheme or erasure coding scheme and replicated. DATA PARTITIONING is\nknown as the single data can be divided into multiple objects. REPLICATION is\nknown as process of sharing information. storing same data in multiple systems.\nReplication techniques are frequently used to improve data availability. Single\npoint failure does not affect this system. Where the data will be secured."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1308.6464v1", 
    "title": "Localizability of Wireless Sensor Networks: Beyond Wheel Extension", 
    "arxiv-id": "1308.6464v1", 
    "author": "Krishnendu Mukhopadhyaya", 
    "publish": "2013-08-29T13:37:44Z", 
    "summary": "A network is called localizable if the positions of all the nodes of the\nnetwork can be computed uniquely. If a network is localizable and embedded in\nplane with generic configuration, the positions of the nodes may be computed\nuniquely in finite time. Therefore, identifying localizable networks is an\nimportant function. If the complete information about the network is available\nat a single place, localizability can be tested in polynomial time. In a\ndistributed environment, networks with trilateration orderings (popular in real\napplications) and wheel extensions (a specific class of localizable networks)\nembedded in plane can be identified by existing techniques. We propose a\ndistributed technique which efficiently identifies a larger class of\nlocalizable networks. This class covers both trilateration and wheel\nextensions. In reality, exact distance is almost impossible or costly. The\nproposed algorithm based only on connectivity information. It requires no\ndistance information."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.0215v1", 
    "title": "Optimizing the MapReduce Framework on Intel Xeon Phi Coprocessor", 
    "arxiv-id": "1309.0215v1", 
    "author": "Richard Huynh", 
    "publish": "2013-09-01T12:39:30Z", 
    "summary": "With the ease-of-programming, flexibility and yet efficiency, MapReduce has\nbecome one of the most popular frameworks for building big-data applications.\nMapReduce was originally designed for distributed-computing, and has been\nextended to various architectures, e,g, multi-core CPUs, GPUs and FPGAs. In\nthis work, we focus on optimizing the MapReduce framework on Xeon Phi, which is\nthe latest product released by Intel based on the Many Integrated Core\nArchitecture. To the best of our knowledge, this is the first work to optimize\nthe MapReduce framework on the Xeon Phi.\n  In our work, we utilize advanced features of the Xeon Phi to achieve high\nperformance. In order to take advantage of the SIMD vector processing units, we\npropose a vectorization friendly technique for the map phase to assist the\nauto-vectorization as well as develop SIMD hash computation algorithms.\nFurthermore, we utilize MIMD hyper-threading to pipeline the map and reduce to\nimprove the resource utilization. We also eliminate multiple local arrays but\nuse low cost atomic operations on the global array for some applications, which\ncan improve the thread scalability and data locality due to the coherent L2\ncaches. Finally, for a given application, our framework can either\nautomatically detect suitable techniques to apply or provide guideline for\nusers at compilation time. We conduct comprehensive experiments to benchmark\nthe Xeon Phi and compare our optimized MapReduce framework with a\nstate-of-the-art multi-core based MapReduce framework (Phoenix++). By\nevaluating six real-world applications, the experimental results show that our\noptimized framework is 1.2X to 38X faster than Phoenix++ for various\napplications on the Xeon Phi."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.1049v3", 
    "title": "xDGP: A Dynamic Graph Processing System with Adaptive Partitioning", 
    "arxiv-id": "1309.1049v3", 
    "author": "Claudio Martella", 
    "publish": "2013-09-04T14:36:17Z", 
    "summary": "Many real-world systems, such as social networks, rely on mining efficiently\nlarge graphs, with hundreds of millions of vertices and edges. This volume of\ninformation requires partitioning the graph across multiple nodes in a\ndistributed system. This has a deep effect on performance, as traversing edges\ncut between partitions incurs a significant performance penalty due to the cost\nof communication. Thus, several systems in the literature have attempted to\nimprove computational performance by enhancing graph partitioning, but they do\nnot support another characteristic of real-world graphs: graphs are inherently\ndynamic, their topology evolves continuously, and subsequently the optimum\npartitioning also changes over time.\n  In this work, we present the first system that dynamically repartitions\nmassive graphs to adapt to structural changes. The system optimises graph\npartitioning to prevent performance degradation without using data replication.\nThe system adopts an iterative vertex migration algorithm that relies on local\ninformation only, making complex coordination unnecessary. We show how the\nimprovement in graph partitioning reduces execution time by over 50%, while\nadapting the partitioning to a large number of changes to the graph in three\nreal-world scenarios."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.1230v1", 
    "title": "A GPU Implementation for Two-Dimensional Shallow Water Modeling", 
    "arxiv-id": "1309.1230v1", 
    "author": "John D. Owens", 
    "publish": "2013-09-05T04:20:02Z", 
    "summary": "In this paper, we present a GPU implementation of a two-dimensional shallow\nwater model. Water simulations are useful for modeling floods, river/reservoir\nbehavior, and dam break scenarios. Our GPU implementation shows vast\nperformance improvements over the original Fortran implementation. By taking\nadvantage of the GPU, researchers and engineers will be able to study water\nsystems more efficiently and in greater detail."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.1630v1", 
    "title": "SimGrid: a Sustained Effort for the Versatile Simulation of Large Scale   Distributed Systems", 
    "arxiv-id": "1309.1630v1", 
    "author": "Fr\u00e9d\u00e9ric Suter", 
    "publish": "2013-09-06T13:16:20Z", 
    "summary": "In this paper we present Simgrid, a toolkit for the versatile simulation of\nlarge scale distributed systems, whose development effort has been sustained\nfor the last fifteen years. Over this time period SimGrid has evolved from a\none-laboratory project in the U.S. into a scientific instrument developed by an\ninternational collaboration. The keys to making this evolution possible have\nbeen securing of funding, improving the quality of the software, and increasing\nthe user base. In this paper we describe how we have been able to make advances\non all three fronts, on which we plan to intensify our efforts over the\nupcoming years."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.2328v1", 
    "title": "Hardware Support for Address Mapping in PGAS Languages; a UPC Case Study", 
    "arxiv-id": "1309.2328v1", 
    "author": "Tarek El-Ghazawi", 
    "publish": "2013-09-09T21:20:13Z", 
    "summary": "The Partitioned Global Address Space (PGAS) programming model strikes a\nbalance between the locality-aware, but explicit, message-passing model and the\neasy-to-use, but locality-agnostic, shared memory model. However, the PGAS rich\nmemory model comes at a performance cost which can hinder its potential for\nscalability and performance. To contain this overhead and achieve full\nperformance, compiler optimizations may not be sufficient and manual\noptimizations are typically added. This, however, can severely limit the\nproductivity advantage. Such optimizations are usually targeted at reducing\naddress translation overheads for shared data structures. This paper proposes a\nhardware architectural support for PGAS, which allows the processor to\nefficiently handle shared addresses. This eliminates the need for such\nhand-tuning, while maintaining the performance and productivity of PGAS\nlanguages. We propose to avail this hardware support to compilers by\nintroducing new instructions to efficiently access and traverse the PGAS memory\nspace. A prototype compiler is realized by extending the Berkeley Unified\nParallel C (UPC) compiler. It allows unmodified code to use the new\ninstructions without the user intervention, thereby creating a real productive\nprogramming environment. Two implementations are realized: the first is\nimplemented using the full system simulator Gem5, which allows the evaluation\nof the performance gain. The second is implemented using a softcore processor\nLeon3 on an FPGA to verify the implementability and to parameterize the cost of\nthe new hardware and its instructions. The new instructions show promising\nresults for the NAS Parallel Benchmarks implemented in UPC. A speedup of up to\n5.5x is demonstrated for unmodified and unoptimized codes. Unoptimized code\nperformance using this hardware was shown to also surpass the performance of\nmanually optimized code by up to 10%."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10586-013-0281-8", 
    "link": "http://arxiv.org/pdf/1309.2772v3", 
    "title": "A Practical Distributed Universal Construction with Unknown Participants", 
    "arxiv-id": "1309.2772v3", 
    "author": "Pascal Felber", 
    "publish": "2013-09-11T09:35:39Z", 
    "summary": "Modern distributed systems employ atomic read-modify-write primitives to\ncoordinate concurrent operations. Such primitives are typically built on top of\na central server, or rely on an agreement protocol. Both approaches provide a\nuniversal construction, that is, a general mechanism to construct atomic and\nresponsive objects. These two techniques are however known to be inherently\ncostly. As a consequence, they may result in bottlenecks in applications using\nthem for coordination. In this paper, we investigate another direction to\nimplement a universal construction. Our idea is to delegate the implementation\nof the universal construction to the clients, and solely implement a\ndistributed shared atomic memory at the servers side. The construction we\npropose is obstruction-free. It can be implemented in a purely asynchronous\nmanner, and it does not assume the knowledge of the participants. It is built\non top of grafarius and racing objects, two novel shared abstractions that we\nintroduce in detail. To assess the benefits of our approach, we present a\nprototype implementation on top of the Cassandra data store, and compare it\nempirically to the Zookeeper coordination service."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2014.2355778", 
    "link": "http://arxiv.org/pdf/1309.3200v4", 
    "title": "Distributed Estimation and Control of Algebraic Connectivity over Random   Graphs", 
    "arxiv-id": "1309.3200v4", 
    "author": "Sergio Barbarossa", 
    "publish": "2013-09-12T15:43:32Z", 
    "summary": "In this paper we propose a distributed algorithm for the estimation and\ncontrol of the connectivity of ad-hoc networks in the presence of a random\ntopology. First, given a generic random graph, we introduce a novel stochastic\npower iteration method that allows each node to estimate and track the\nalgebraic connectivity of the underlying expected graph. Using results from\nstochastic approximation theory, we prove that the proposed method converges\nalmost surely (a.s.) to the desired value of connectivity even in the presence\nof imperfect communication scenarios. The estimation strategy is then used as a\nbasic tool to adapt the power transmitted by each node of a wireless network,\nin order to maximize the network connectivity in the presence of realistic\nMedium Access Control (MAC) protocols or simply to drive the connectivity\ntoward a desired target value. Numerical results corroborate our theoretical\nfindings, thus illustrating the main features of the algorithm and its\nrobustness to fluctuations of the network graph due to the presence of random\nlink failures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2014.2355778", 
    "link": "http://arxiv.org/pdf/1309.3324v2", 
    "title": "Blazes: Coordination Analysis for Distributed Programs", 
    "arxiv-id": "1309.3324v2", 
    "author": "David Maier", 
    "publish": "2013-09-12T22:37:10Z", 
    "summary": "Distributed consistency is perhaps the most discussed topic in distributed\nsystems today. Coordination protocols can ensure consistency, but in practice\nthey cause undesirable performance unless used judiciously. Scalable\ndistributed architectures avoid coordination whenever possible, but\nunder-coordinated systems can exhibit behavioral anomalies under fault, which\nare often extremely difficult to debug. This raises significant challenges for\ndistributed system architects and developers. In this paper we present Blazes,\na cross-platform program analysis framework that (a) identifies program\nlocations that require coordination to ensure consistent executions, and (b)\nautomatically synthesizes application-specific coordination code that can\nsignificantly outperform general-purpose techniques. We present two case\nstudies, one using annotated programs in the Twitter Storm system, and another\nusing the Bloom declarative language."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2014.2355778", 
    "link": "http://arxiv.org/pdf/1309.3830v1", 
    "title": "Energy-Aware Aggregation of Dynamic Temporal Workload in Data Centers", 
    "arxiv-id": "1309.3830v1", 
    "author": "Deep Medhi", 
    "publish": "2013-09-16T05:13:32Z", 
    "summary": "Data center providers seek to minimize their total cost of ownership (TCO),\nwhile power consumption has become a social concern. We present formulations to\nminimize server energy consumption and server cost under three different data\ncenter server setups (homogeneous, heterogeneous, and hybrid hetero-homogeneous\nclusters) with dynamic temporal workload. Our studies show that the homogeneous\nmodel significantly differs from the heterogeneous model in computational time\n(by an order of magnitude). To be able to compute optimal configurations in\nnear real-time for large scale data centers, we propose two modes, aggregation\nby maximum and aggregation by mean. In addition, we propose two aggregation\nmethods, static (periodic) aggregation and dynamic (aperiodic) aggregation. We\nfound that in the aggregation by maximum mode, the dynamic aggregation resulted\nin cost savings of up to approximately 18% over the static aggregation. In the\naggregation by mean mode, the dynamic aggregation by mean could save up to\napproximately 50% workload rearrangement compared to the static aggregation by\nmean mode. Overall, our methodology helps to understand the trade-off in\nenergy-aware aggregation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2014.2355778", 
    "link": "http://arxiv.org/pdf/1309.4349v1", 
    "title": "Report: GPU Based Massive Parallel Kawasaki Kinetics In Monte Carlo   Modelling of Lipid Microdomains", 
    "arxiv-id": "1309.4349v1", 
    "author": "L. Pintal", 
    "publish": "2013-09-17T15:29:17Z", 
    "summary": "This paper introduces novel method of simulation of lipid biomembranes based\non Metropolis Hastings algorithm and Graphic Processing Unit computational\npower. Method gives up to 55 times computational boost in comparison to\nclassical computations. Extensive study of algorithm correctness is provided.\nAnalysis of simulation results and results obtained with classical simulation\nmethodologies are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TSP.2014.2355778", 
    "link": "http://arxiv.org/pdf/1309.4507v1", 
    "title": "Faster Fair Solution for the Reader-Writer Problem", 
    "arxiv-id": "1309.4507v1", 
    "author": "Oleg Mazonka", 
    "publish": "2013-09-18T00:08:18Z", 
    "summary": "A fast fair solution for Reader-Writer Problem is presented."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_29", 
    "link": "http://arxiv.org/pdf/1309.4887v1", 
    "title": "iDataCool: HPC with Hot-Water Cooling and Energy Reuse", 
    "arxiv-id": "1309.4887v1", 
    "author": "Tilo Wettig", 
    "publish": "2013-09-19T07:53:10Z", 
    "summary": "iDataCool is an HPC architecture jointly developed by the University of\nRegensburg and the IBM Research and Development Lab B\\\"oblingen. It is based on\nIBM's iDataPlex platform, whose air-cooling solution was replaced by a custom\nwater-cooling solution that allows for cooling water temperatures of 70C/158F.\nThe system is coupled to an adsorption chiller by InvenSor that operates\nefficiently at these temperatures. Thus a significant portion of the energy\nspent on HPC can be recovered in the form of chilled water, which can then be\nused to cool other parts of the computing center. We describe the architecture\nof iDataCool and present benchmarks of the cooling performance and the energy\n(reuse) efficiency."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_29", 
    "link": "http://arxiv.org/pdf/1309.5301v2", 
    "title": "Well-Structured Futures and Cache Locality", 
    "arxiv-id": "1309.5301v2", 
    "author": "Zhiyu Liu", 
    "publish": "2013-09-20T15:30:14Z", 
    "summary": "In \\emph{fork-join parallelism}, a sequential program is split into a\ndirected acyclic graph of tasks linked by directed dependency edges, and the\ntasks are executed, possibly in parallel, in an order consistent with their\ndependencies. A popular and effective way to extend fork-join parallelism is to\nallow threads to create \\emph{futures}. A thread creates a future to hold the\nresults of a computation, which may or may not be executed in parallel. That\nresult is returned when some thread \\emph{touches} that future, blocking if\nnecessary until the result is ready.\n  Recent research has shown that while futures can, of course, enhance\nparallelism in a structured way, they can have a deleterious effect on cache\nlocality. In the worst case, futures can incur $\\Omega(P T_\\infty + t\nT_\\infty)$ deviations, which implies $\\Omega(C P T_\\infty + C t T_\\infty)$\nadditional cache misses, where $C$ is the number of cache lines, $P$ is the\nnumber of processors, $t$ is the number of touches, and $T_\\infty$ is the\n\\emph{computation span}. Since cache locality has a large impact on software\nperformance on modern multicores, this result is troubling.\n  In this paper, however, we show that if futures are used in a simple,\ndisciplined way, then the situation is much better: if each future is touched\nonly once, either by the thread that created it, or by a later descendant of\nthe thread that created it, then parallel executions with work stealing can\nincur at most $O(C P T^2_\\infty)$ additional cache misses, a substantial\nimprovement. This structured use of futures is characteristic of many (but not\nall) parallel applications."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_29", 
    "link": "http://arxiv.org/pdf/1309.5442v1", 
    "title": "Operating the Cloud from Inside Out", 
    "arxiv-id": "1309.5442v1", 
    "author": "Alexander Schill", 
    "publish": "2013-09-21T06:43:06Z", 
    "summary": "Virtual machine images and instances (VMs) in cloud computing centres are\ntypically designed as isolation containers for applications, databases and\nnetworking functions. In order to build complex distributed applications,\nmultiple virtual machines must be connected, orchestrated and combined with\nplatform and infrastructure services from the hosting environment. There are\nseveral reasons why sometimes it is beneficial to introduce a new layer,\nCloud-in-a-VM, which acts as a portable management interface to a cluster of\nVMs. We reason about the benefits and present our Cloud-in-a-VM implementation\ncalled Nested Cloud which allows consumers to become light-weight cloud\noperators on demand and reap multiple advantages, including fully utilised\nresource allocations. The practical usefulness and the performance of the\nintermediate cloud stack VM are evaluated in a marketplace scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-38750-0_29", 
    "link": "http://arxiv.org/pdf/1309.5457v1", 
    "title": "Cable Fault Monitoring and Indication: A Review", 
    "arxiv-id": "1309.5457v1", 
    "author": "S. P. Karmore", 
    "publish": "2013-09-21T10:01:15Z", 
    "summary": "Underground cable power transmission and distribution system are susceptible\nto faults. Accurate fault location for transmission lines is of vital\nimportance. A quick detection and analysis of faults is necessity of power\nretailers and distributors. This paper reviews various fault locating methods\nand highly computational methods proposed by research community that are\ncurrently in use. The paper also presents some guidelines for design of fault\nlocation and remote indication, for reducing power outages and reducing heavy\nloss of revenue."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.5478v1", 
    "title": "Fast $k$-NNG construction with GPU-based quick multi-select", 
    "arxiv-id": "1309.5478v1", 
    "author": "Roshan D'Souza", 
    "publish": "2013-09-21T14:00:32Z", 
    "summary": "In this paper we describe a new brute force algorithm for building the\n$k$-Nearest Neighbor Graph ($k$-NNG). The $k$-NNG algorithm has many\napplications in areas such as machine learning, bio-informatics, and clustering\nanalysis. While there are very efficient algorithms for data of low dimensions,\nfor high dimensional data the brute force search is the best algorithm. There\nare two main parts to the algorithm: the first part is finding the distances\nbetween the input vectors which may be formulated as a matrix multiplication\nproblem. The second is the selection of the $k$-NNs for each of the query\nvectors. For the second part, we describe a novel graphics processing unit\n(GPU) -based multi-select algorithm based on quick sort. Our optimization makes\nclever use of warp voting functions available on the latest GPUs along with\nuse-controlled cache. Benchmarks show significant improvement over\nstate-of-the-art implementations of the $k$-NN search on GPUs."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.5522v1", 
    "title": "On the k-Atomicity-Verification Problem", 
    "arxiv-id": "1309.5522v1", 
    "author": "Li", 
    "publish": "2013-09-21T19:32:53Z", 
    "summary": "Modern Internet-scale storage systems often provide weak consistency in\nexchange for better performance and resilience. An important weak consistency\nproperty is k-atomicity, which bounds the staleness of values returned by read\noperations. The k-atomicity-verification problem (or k-AV for short) is the\nproblem of deciding whether a given history of operations is k-atomic. The 1-AV\nproblem is equivalent to verifying atomicity/linearizability, a well-known and\nsolved problem. However, for k > 2, no polynomial-time k-AV algorithm is known.\n  This paper makes the following contributions towards solving the k-AV\nproblem. First, we present a simple 2- AV algorithm called LBT, which is likely\nto be efficient (quasilinear) for histories that arise in practice, although it\nis less efficient (quadratic) in the worst case. Second, we present a more\ninvolved 2-AV algorithm called FZF, which runs efficiently (quasilinear) even\nin the worst case. To our knowledge, these are the first algorithms that solve\nthe 2-AV problem fully. Third, we show that the weighted k-AV problem, a\nnatural extension of the k-AV problem, is NP-complete."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.5671v3", 
    "title": "Vive la Diff\u00e9rence: Paxos vs. Viewstamped Replication vs. Zab", 
    "arxiv-id": "1309.5671v3", 
    "author": "Fred B. Schneider", 
    "publish": "2013-09-23T00:31:36Z", 
    "summary": "Paxos, Viewstamped Replication, and Zab are replication protocols that ensure\nhigh-availability in asynchronous environments with crash failures. Various\nclaims have been made about similarities and differences between these\nprotocols. But how does one determine whether two protocols are the same, and\nif not, how significant the differences are?\n  We propose to address these questions using refinement mappings, where\nprotocols are expressed as succinct specifications that are progressively\nrefined to executable implementations. Doing so enables a principled\nunderstanding of the correctness of the different design decisions that went\ninto implementing the various protocols. Additionally, it allowed us to\nidentify key differences that have a significant impact on performance."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.6452v2", 
    "title": "Location, Location, Location: Data-Intensive Distributed Computing in   the Cloud", 
    "arxiv-id": "1309.6452v2", 
    "author": "Adam Barker", 
    "publish": "2013-09-25T10:01:27Z", 
    "summary": "When orchestrating highly distributed and data-intensive Web service\nworkflows the geographical placement of the orchestration engine can greatly\naffect the overall performance of a workflow. Orchestration engines are\ntypically run from within an organisations' network, and may have to transfer\ndata across long geographical distances, which in turn increases execution time\nand degrades the overall performance of a workflow. In this paper we present\nCloudForecast: a Web service framework and analysis tool which given a workflow\nspecification, computes the optimal Amazon EC2 Cloud region to automatically\ndeploy the orchestration engine and execute the workflow. We use geographical\ndistance of the workflow, network latency and HTTP round-trip time between\nAmazon Cloud regions and the workflow nodes to find a ranking of Cloud regions.\nThis combined set of simple metrics effectively predicts where the workflow\norchestration engine should be deployed in order to reduce overall execution\ntime.\n  We evaluate our approach by executing randomly generated data-intensive\nworkflows deployed on the PlanetLab platform in order to rank Amazon EC2 Cloud\nregions. Our experimental results show that our proposed optimisation strategy,\ndepending on the particular workflow, can speed up execution time on average by\n82.25% compared to local execution. We also show that the standard deviation of\nexecution time is reduced by an average of almost 65% using the optimisation\nstrategy."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.6723v1", 
    "title": "A Comparative Study of Replication Techniques in Grid Computing Systems", 
    "arxiv-id": "1309.6723v1", 
    "author": "Mohammad Reza Khayyambashi", 
    "publish": "2013-09-26T05:31:20Z", 
    "summary": "Grid Computing is a type of parallel and distributed systems that is designed\nto provide reliable access to data and computational resources in wide area\nnetworks. These resources are distributed in different geographical locations,\nhowever are organized to provide an integrated service. Effective data\nmanagement in today`s enterprise environment is an important issue. Also,\nPerformance is one of the challenges of using these environments. For improving\nthe performance of file access and easing the sharing amongst distributed\nsystems, replication techniques are used. Data replication is a common method\nused in distributed environments, where essential data is stored in multiple\nlocations, so that a user can access the data from a site in his area. In this\npaper, we present a survey on basic and new replication techniques that have\nbeen proposed by other researchers. After that, we have a full comparative\nstudy on these replication strategies. Also, at the end of the paper, we\nsummarize the results and points of these replication techniques."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.6978v2", 
    "title": "Simple and Efficient Local Codes for Distributed Stable Network   Construction", 
    "arxiv-id": "1309.6978v2", 
    "author": "Paul G. Spirakis", 
    "publish": "2013-09-26T17:16:16Z", 
    "summary": "In this work, we study protocols so that populations of distributed processes\ncan construct networks. In order to highlight the basic principles of\ndistributed network construction we keep the model minimal in all respects. In\nparticular, we assume finite-state processes that all begin from the same\ninitial state and all execute the same protocol (i.e. the system is\nhomogeneous). Moreover, we assume pairwise interactions between the processes\nthat are scheduled by an adversary. The only constraint on the adversary\nscheduler is that it must be fair. In order to allow processes to construct\nnetworks, we let them activate and deactivate their pairwise connections. When\ntwo processes interact, the protocol takes as input the states of the processes\nand the state of the their connection and updates all of them. Initially all\nconnections are inactive and the goal is for the processes, after interacting\nand activating/deactivating connections for a while, to end up with a desired\nstable network. We give protocols (optimal in some cases) and lower bounds for\nseveral basic network construction problems such as spanning line, spanning\nring, spanning star, and regular network. We provide proofs of correctness for\nall of our protocols and analyze the expected time to convergence of most of\nthem under a uniform random scheduler that selects the next pair of interacting\nprocesses uniformly at random from all such pairs. Finally, we prove several\nuniversality results by presenting generic protocols that are capable of\nsimulating a Turing Machine (TM) and exploiting it in order to construct a\nlarge class of networks."
},{
    "category": "cs.DC", 
    "doi": "10.1371/journal.pone.0092409", 
    "link": "http://arxiv.org/pdf/1309.7720v1", 
    "title": "ASURA: Scalable and Uniform Data Distribution Algorithm for Storage   Clusters", 
    "arxiv-id": "1309.7720v1", 
    "author": "Ken-ichiro Ishikawa", 
    "publish": "2013-09-30T04:48:11Z", 
    "summary": "Large-scale storage cluster systems need to manage a vast amount of\ninformation denoting combinations of data identifiers (IDs) and corresponding\nnodes that store the data. Management using data distribution algorithms,\nrather than management using tables, has been proposed in recent research. In\nalgorithm management, data are distributed in accordance with a data\ndistribution algorithm that is capable of determining, on the basis of the\ndatum ID, the node in which the required data is being stored. Among the\nrequirements for a data distribution algorithm are short calculation times, low\nmemory consumption, uniform data distribution in accordance with the capacity\nof each node and the ability to handle the addition or removal of nodes. This\npaper presents a data distribution algorithm called ASURA (Advanced Scalable\nand Uniform storage by Random number Algorithm), which satisfies these\nrequirements. It offers roughly 0.6-{\\mu}s calculation time, kilobyte-order\nmemory consumption, less than 1% maximum variability between nodes in data\ndistribution, data distribution in accordance with the capacity of each node\nand optimal data movement to maintain data distribution in accordance with node\ncapacity when nodes are added or removed. ASURA is contrasted in this paper\nqualitatively and quantitatively with representative data distribution\nalgorithms: Consistent Hashing and Straw Buckets in CRUSH. The comparison\nresults show that ASURA can achieve the same storage cluster capacity as\nConsistent Hashing with dozens fewer nodes by virtue of the uniformity of its\ndistribution with the same level calculation time. They also show that the\nexecution time of ASURA is shorter than that of Straw Buckets in CRUSH. The\nresults reveal that ASURA is the best algorithm for large-scale storage cluster\nsystems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032035", 
    "link": "http://arxiv.org/pdf/1311.0058v1", 
    "title": "Dynamic web cache publishing for IaaS clouds using Shoal", 
    "arxiv-id": "1311.0058v1", 
    "author": "Ryan Taylor", 
    "publish": "2013-10-31T22:43:01Z", 
    "summary": "We have developed a highly scalable application, called Shoal, for tracking\nand utilizing a distributed set of HTTP web caches. Squid servers advertise\ntheir existence to the Shoal server via AMQP messaging by running Shoal Agent.\nThe Shoal server provides a simple REST interface that allows clients to\ndetermine their closest Squid cache. Our goal is to dynamically instantiate\nSquid caches on IaaS clouds in response to client demand. Shoal provides the\nVMs on IaaS clouds with the location of the nearest dynamically instantiated\nSquid Cache. In this paper, we describe the design and performance of Shoal."
},{
    "category": "cs.DC", 
    "doi": "10.1137/130943595", 
    "link": "http://arxiv.org/pdf/1311.1006v3", 
    "title": "Dynamic autotuning of adaptive fast multipole methods on hybrid   multicore CPU & GPU systems", 
    "arxiv-id": "1311.1006v3", 
    "author": "Sverker Holmgren", 
    "publish": "2013-11-05T10:31:57Z", 
    "summary": "We discuss an implementation of adaptive fast multipole methods targeting\nhybrid multicore CPU- and GPU-systems. From previous experiences with the\ncomputational profile of our version of the fast multipole algorithm, suitable\nparts are off-loaded to the GPU, while the remaining parts are threaded and\nexecuted concurrently by the CPU. The parameters defining the algorithm affects\nthe performance and by measuring this effect we are able to dynamically balance\nthe algorithm towards optimal performance. Our setup uses the dynamic nature of\nthe computations and is therefore of general character."
},{
    "category": "cs.DC", 
    "doi": "10.1109/JSEN.2013.2281208", 
    "link": "http://arxiv.org/pdf/1311.1084v1", 
    "title": "A Chemistry-Inspired Framework for Achieving Consensus in Wireless   Sensor Networks", 
    "arxiv-id": "1311.1084v1", 
    "author": "Marco Luise", 
    "publish": "2013-08-27T08:51:45Z", 
    "summary": "The aim of this paper is to show how simple interaction mechanisms, inspired\nby chemical systems, can provide the basic tools to design and analyze a\nmathematical model for achieving consensus in wireless sensor networks,\ncharacterized by balanced directed graphs. The convergence and stability of the\nmodel are first proven by using new mathematical tools, which are borrowed\ndirectly from chemical theory, and then validated by means of simulation\nresults, for different network topologies and number of sensors. The underlying\nchemical theory is also used to derive simple interaction rules that may\naccount for practical issues, such as the estimation of the number of neighbors\nand the robustness against perturbations. Finally, the proposed chemical\nsolution is validated under real-world conditions by means of a four-node\nhardware implementation where the exchange of information among nodes takes\nplace in a distributed manner (with no need for any admission control and\nsynchronism procedure), simply relying on the transmission of a pulse whose\nrate is proportional to the state of each sensor."
},{
    "category": "cs.DC", 
    "doi": "10.1109/JSEN.2013.2281208", 
    "link": "http://arxiv.org/pdf/1311.1884v1", 
    "title": "A Parallel Simulated Annealing Approach for the Mirrored Traveling   Tournament Problem", 
    "arxiv-id": "1311.1884v1", 
    "author": "Saurabh Jha", 
    "publish": "2013-11-08T06:16:43Z", 
    "summary": "The Traveling Tournament Problem (TTP) is a benchmark problem in sports\nscheduling and has been extensively studied in recent years. The Mirrored\nTraveling Tournament Problem (mTTP) is variation of the TTP that represents\ncertain types of sports scheduling problems where the main objective is to\nminimize the total distance traveled by all the participating teams. In this\npaper we test a parallel simulated annealing approach for solving the mTTP\nusing OpenMP on shared memory systems and we found that this approach is\nsuperior especially with respect to the number of solution instances that are\nprobed per second. We also see that there is significant speed up of 1.5x -\n2.2x in terms of number of solutions explored per unit time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/JSEN.2013.2281208", 
    "link": "http://arxiv.org/pdf/1311.2019v1", 
    "title": "Symmetric Interconnection Networks from Cubic Crystal Lattices", 
    "arxiv-id": "1311.2019v1", 
    "author": "Ram\u00f3n Beivide", 
    "publish": "2013-11-08T16:57:34Z", 
    "summary": "Torus networks of moderate degree have been widely used in the supercomputer\nindustry. Tori are superb when used for executing applications that require\nnear-neighbor communications. Nevertheless, they are not so good when dealing\nwith global communications. Hence, typical 3D implementations have evolved to\n5D networks, among other reasons, to reduce network distances. Most of these\nbig systems are mixed-radix tori which are not the best option for minimizing\ndistances and efficiently using network resources. This paper is focused on\nimproving the topological properties of these networks.\n  By using integral matrices to deal with Cayley graphs over Abelian groups, we\nhave been able to propose and analyze a family of high-dimensional grid-based\ninterconnection networks. As they are built over $n$-dimensional grids that\ninduce a regular tiling of the space, these topologies have been denoted\n\\textsl{lattice graphs}. We will focus on cubic crystal lattices for modeling\nsymmetric 3D networks. Other higher dimensional networks can be composed over\nthese graphs, as illustrated in this research. Easy network partitioning can\nalso take advantage of this network composition operation. Minimal routing\nalgorithms are also provided for these new topologies. Finally, some practical\nissues such as implementability and preliminary performance evaluations have\nbeen addressed."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032009", 
    "link": "http://arxiv.org/pdf/1311.2426v1", 
    "title": "Micro-CernVM: Slashing the Cost of Building and Deploying Virtual   Machines", 
    "arxiv-id": "1311.2426v1", 
    "author": "V. Nicolaou", 
    "publish": "2013-11-11T12:22:07Z", 
    "summary": "The traditional virtual machine building and and deployment process is\ncentered around the virtual machine hard disk image. The packages comprising\nthe VM operating system are carefully selected, hard disk images are built for\na variety of different hypervisors, and images have to be distributed and\ndecompressed in order to instantiate a virtual machine. Within the HEP\ncommunity, the CernVM File System has been established in order to decouple the\ndistribution from the experiment software from the building and distribution of\nthe VM hard disk images.\n  We show how to get rid of such pre-built hard disk images altogether. Due to\nthe high requirements on POSIX compliance imposed by HEP application software,\nCernVM-FS can also be used to host and boot a Linux operating system. This\nallows the use of a tiny bootable CD image that comprises only a Linux kernel\nwhile the rest of the operating system is provided on demand by CernVM-FS. This\napproach speeds up the initial instantiation time and reduces virtual machine\nimage sizes by an order of magnitude. Furthermore, security updates can be\ndistributed instantaneously through CernVM-FS. By leveraging the fact that\nCernVM-FS is a versioning file system, a historic analysis environment can be\neasily re-spawned by selecting the corresponding CernVM-FS file system\nsnapshot."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032009", 
    "link": "http://arxiv.org/pdf/1311.3070v1", 
    "title": "The Distributed Computing Paradigms: P2P, Grid, Cluster, Cloud, and   Jungle", 
    "arxiv-id": "1311.3070v1", 
    "author": "Dr. T. P. Singh", 
    "publish": "2013-11-13T10:26:58Z", 
    "summary": "The distributed computing is done on many systems to solve a large scale\nproblem. The growing of high-speed broadband networks in developed and\ndeveloping countries, the continual increase in computing power, and the rapid\ngrowth of the Internet have changed the way. In it the society manages\ninformation and information services. Historically, the state of computing has\ngone through a series of platform and environmental changes. Distributed\ncomputing holds great assurance for using computer systems effectively. As a\nresult, supercomputer sites and data centers have changed from providing high\nperformance floating point computing capabilities to concurrently servicing\nhuge number of requests from billions of users. The distributed computing\nsystem uses multiple computers to solve large-scale problems over the Internet.\nIt becomes data-intensive and network-centric. The applications of distributed\ncomputing have become increasingly wide-spread. In distributed computing, the\nmain stress is on the large scale resource sharing and always goes for the best\nperformance. In this article, we have reviewed the work done in the area of\ndistributed computing paradigms. The main stress is on the evolving area of\ncloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032009", 
    "link": "http://arxiv.org/pdf/1311.3200v2", 
    "title": "Are Lock-Free Concurrent Algorithms Practically Wait-Free?", 
    "arxiv-id": "1311.3200v2", 
    "author": "Nir Shavit", 
    "publish": "2013-11-13T16:39:40Z", 
    "summary": "Lock-free concurrent algorithms guarantee that some concurrent operation will\nalways make progress in a finite number of steps. Yet programmers prefer to\ntreat concurrent code as if it were wait-free, guaranteeing that all operations\nalways make progress. Unfortunately, designing wait-free algorithms is\ngenerally a very complex task, and the resulting algorithms are not always\nefficient. While obtaining efficient wait-free algorithms has been a long-time\ngoal for the theory community, most non-blocking commercial code is only\nlock-free.\n  This paper suggests a simple solution to this problem. We show that, for a\nlarge class of lock- free algorithms, under scheduling conditions which\napproximate those found in commercial hardware architectures, lock-free\nalgorithms behave as if they are wait-free. In other words, programmers can\nkeep on designing simple lock-free algorithms instead of complex wait-free\nones, and in practice, they will get wait-free progress.\n  Our main contribution is a new way of analyzing a general class of lock-free\nalgorithms under a stochastic scheduler. Our analysis relates the individual\nperformance of processes with the global performance of the system using Markov\nchain lifting between a complex per-process chain and a simpler system progress\nchain. We show that lock-free algorithms are not only wait-free with\nprobability 1, but that in fact a general subset of lock-free algorithms can be\nclosely bounded in terms of the average number of steps required until an\noperation completes.\n  To the best of our knowledge, this is the first attempt to analyze progress\nconditions, typically stated in relation to a worst case adversary, in a\nstochastic model capturing their expected asymptotic behavior."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2611462.2611469", 
    "link": "http://arxiv.org/pdf/1311.3425v3", 
    "title": "Breathe before Speaking: Efficient Information Dissemination Despite   Noisy, Limited and Anonymous Communication", 
    "arxiv-id": "1311.3425v3", 
    "author": "Amos Korman", 
    "publish": "2013-11-14T09:10:15Z", 
    "summary": "Distributed computing models typically assume reliable communication between\nprocessors. While such assumptions often hold for engineered networks, e.g.,\ndue to underlying error correction protocols, their relevance to biological\nsystems, wherein messages are often distorted before reaching their\ndestination, is quite limited. In this study we take a first step towards\nreducing this gap by rigorously analyzing a model of communication in large\nanonymous populations composed of simple agents which interact through short\nand highly unreliable messages.\n  We focus on the broadcast problem and the majority-consensus problem. Both\nare fundamental information dissemination problems in distributed computing, in\nwhich the goal of agents is to converge to some prescribed desired opinion. We\ninitiate the study of these problems in the presence of communication noise.\nOur model for communication is extremely weak and follows the push gossip\ncommunication paradigm: In each round each agent that wishes to send\ninformation delivers a message to a random anonymous agent. This communication\nis further restricted to contain only one bit (essentially representing an\nopinion). Lastly, the system is assumed to be so noisy that the bit in each\nmessage sent is flipped independently with probability $1/2-\\epsilon$, for some\nsmall $\\epsilon >0$.\n  Even in this severely restricted, stochastic and noisy setting we give\nnatural protocols that solve the noisy broadcast and the noisy\nmajority-consensus problems efficiently. Our protocols run in $O(\\log n /\n\\epsilon^2)$ rounds and use $O(n \\log n / \\epsilon^2)$ messages/bits in total,\nwhere $n$ is the number of agents. These bounds are asymptotically optimal and,\nin fact, are as fast and message efficient as if each agent would have been\nsimultaneously informed directly by an agent that knows the prescribed desired\nopinion."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2611462.2611469", 
    "link": "http://arxiv.org/pdf/1311.4112v1", 
    "title": "Big Data Analytics in Future Internet of Things", 
    "arxiv-id": "1311.4112v1", 
    "author": "Qihui Wu", 
    "publish": "2013-11-17T03:03:36Z", 
    "summary": "Current research on Internet of Things (IoT) mainly focuses on how to enable\ngeneral objects to see, hear, and smell the physical world for themselves, and\nmake them connected to share the observations. In this paper, we argue that\nonly connected is not enough, beyond that, general objects should have the\ncapability to learn, think, and understand both the physical world by\nthemselves. On the other hand, the future IoT will be highly populated by large\nnumbers of heterogeneous networked embedded devices, which are generating\nmassive or big data in an explosive fashion. Although there is a consensus\namong almost everyone on the great importance of big data analytics in IoT, to\ndate, limited results, especially the mathematical foundations, are obtained.\nThese practical needs impels us to propose a systematic tutorial on the\ndevelopment of effective algorithms for big data analytics in future IoT, which\nare grouped into four classes: 1) heterogeneous data processing, 2) nonlinear\ndata processing, 3) high-dimensional data processing, and 4) distributed and\nparallel data processing. We envision that the presented research is offered as\na mere baby step in a potentially fruitful research direction. We hope that\nthis article, with interdisciplinary perspectives, will stimulate more\ninterests in research and development of practical and effective algorithms for\nspecific IoT applications, to enable smart resource allocation, automatic\nnetwork operation, and intelligent service provisioning."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2611462.2611469", 
    "link": "http://arxiv.org/pdf/1311.5304v3", 
    "title": "Dynamic Partitioning-based JPEG Decompression on Heterogeneous Multicore   Architectures", 
    "arxiv-id": "1311.5304v3", 
    "author": "Bernd Burgstaller", 
    "publish": "2013-11-21T03:49:00Z", 
    "summary": "With the emergence of social networks and improvements in computational\nphotography, billions of JPEG images are shared and viewed on a daily basis.\nDesktops, tablets and smartphones constitute the vast majority of hardware\nplatforms used for displaying JPEG images. Despite the fact that these\nplatforms are heterogeneous multicores, no approach exists yet that is capable\nof joining forces of a system's CPU and GPU for JPEG decoding. In this paper we\nintroduce a novel JPEG decoding scheme for heterogeneous architectures\nconsisting of a CPU and an OpenCL-programmable GPU. We employ an offline\nprofiling step to determine the performance of a system's CPU and GPU with\nrespect to JPEG decoding. For a given JPEG image, our performance model uses\n(1) the CPU and GPU performance characteristics, (2) the image entropy and (3)\nthe width and height of the image to balance the JPEG decoding workload on the\nunderlying hardware. Our run-time partitioning and scheduling scheme exploits\ntask, data and pipeline parallelism by scheduling the non-parallelizable\nentropy decoding task on the CPU, whereas inverse cosine transformations\n(IDCTs), color conversions and upsampling are conducted on both the CPU and the\nGPU. Our kernels have been optimized for GPU memory hierarchies. We have\nimplemented the proposed method in the context of the libjpeg-turbo library,\nwhich is an industrial-strength JPEG encoding and decoding engine.\nLibjpeg-turbo's hand-optimized SIMD routines for ARM and x86 constitute a\ncompetitive yardstick for the comparison to the proposed approach.\nRetro-fitting our method with libjpeg-turbo provided insights on the\nsoftware-engineering aspects of re-engineering legacy code for heterogeneous\nmulticores."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jpdc.2014.08.001", 
    "link": "http://arxiv.org/pdf/1311.5904v3", 
    "title": "The IceProd Framework: Distributed Data Processing for the IceCube   Neutrino Observatory", 
    "arxiv-id": "1311.5904v3", 
    "author": "M. Zoll", 
    "publish": "2013-11-22T21:16:58Z", 
    "summary": "IceCube is a one-gigaton instrument located at the geographic South Pole,\ndesigned to detect cosmic neutrinos, iden- tify the particle nature of dark\nmatter, and study high-energy neutrinos themselves. Simulation of the IceCube\ndetector and processing of data require a significant amount of computational\nresources. IceProd is a distributed management system based on Python, XML-RPC\nand GridFTP. It is driven by a central database in order to coordinate and\nadmin- ister production of simulations and processing of data produced by the\nIceCube detector. IceProd runs as a separate layer on top of other middleware\nand can take advantage of a variety of computing resources, including grids and\nbatch systems such as CREAM, Condor, and PBS. This is accomplished by a set of\ndedicated daemons that process job submission in a coordinated fashion through\nthe use of middleware plugins that serve to abstract the details of job\nsubmission and job management from the framework."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_38", 
    "link": "http://arxiv.org/pdf/1311.5949v1", 
    "title": "GoFFish: A Sub-Graph Centric Framework for Large-Scale Graph Analytics", 
    "arxiv-id": "1311.5949v1", 
    "author": "Viktor Prasanna", 
    "publish": "2013-11-23T02:53:58Z", 
    "summary": "Large scale graph processing is a major research area for Big Data\nexploration. Vertex centric programming models like Pregel are gaining traction\ndue to their simple abstraction that allows for scalable execution on\ndistributed systems naturally. However, there are limitations to this approach\nwhich cause vertex centric algorithms to under-perform due to poor compute to\ncommunication overhead ratio and slow convergence of iterative superstep. In\nthis paper we introduce GoFFish a scalable sub-graph centric framework\nco-designed with a distributed persistent graph storage for large scale graph\nanalytics on commodity clusters. We introduce a sub-graph centric programming\nabstraction that combines the scalability of a vertex centric approach with the\nflexibility of shared memory sub-graph computation. We map Connected\nComponents, SSSP and PageRank algorithms to this model to illustrate its\nflexibility. Further, we empirically analyze GoFFish using several real world\ngraphs and demonstrate its significant performance improvement, orders of\nmagnitude in some cases, compared to Apache Giraph, the leading open source\nvertex centric implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_38", 
    "link": "http://arxiv.org/pdf/1311.6183v1", 
    "title": "Rethinking State-Machine Replication for Parallelism", 
    "arxiv-id": "1311.6183v1", 
    "author": "Fernando Pedone", 
    "publish": "2013-11-24T22:45:59Z", 
    "summary": "State-machine replication, a fundamental approach to designing fault-tolerant\nservices, requires commands to be executed in the same order by all replicas.\nMoreover, command execution must be deterministic: each replica must produce\nthe same output upon executing the same sequence of commands. These\nrequirements usually result in single-threaded replicas, which hinders service\nperformance. This paper introduces Parallel State-Machine Replication (P-SMR),\na new approach to parallelism in state-machine replication. P-SMR scales better\nthan previous proposals since no component plays a centralizing role in the\nexecution of independent commands---those that can be executed concurrently, as\ndefined by the service. The paper introduces P-SMR, describes a \"commodified\narchitecture\" to implement it, and compares its performance to other proposals\nusing a key-value store and a networked file system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.6505v1", 
    "title": "Evaluating the Impact of SDC on the GMRES Iterative Solver", 
    "arxiv-id": "1311.6505v1", 
    "author": "Frank Mueller", 
    "publish": "2013-11-25T22:19:39Z", 
    "summary": "Increasing parallelism and transistor density, along with increasingly\ntighter energy and peak power constraints, may force exposure of occasionally\nincorrect computation or storage to application codes. Silent data corruption\n(SDC) will likely be infrequent, yet one SDC suffices to make numerical\nalgorithms like iterative linear solvers cease progress towards the correct\nanswer. Thus, we focus on resilience of the iterative linear solver GMRES to a\nsingle transient SDC. We derive inexpensive checks to detect the effects of an\nSDC in GMRES that work for a more general SDC model than presuming a bit flip.\nOur experiments show that when GMRES is used as the inner solver of an\ninner-outer iteration, it can \"run through\" SDC of almost any magnitude in the\ncomputationally intensive orthogonalization phase. That is, it gets the right\nanswer using faulty data without any required roll back. Those SDCs which it\ncannot run through, get caught by our detection scheme."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.6902v1", 
    "title": "Good, Better, Best! - Unbeatable Protocols for Consensus and Set   Consensus", 
    "arxiv-id": "1311.6902v1", 
    "author": "Yoram Moses", 
    "publish": "2013-11-27T09:13:43Z", 
    "summary": "While the very first consensus protocols for the synchronous model were\ndesigned to match the worst-case lower bound, deciding in exactly t+1 rounds in\nall runs, it was soon realized that they could be strictly improved upon by\nearly stopping protocols. These dominate the first ones, by always deciding in\nat most t+1 rounds, but often much faster. A protocol is unbeatable if it can't\nbe strictly dominated. Unbeatability is often a much more suitable notion of\noptimality for distributed protocols than worst-case performance. Using a\nknowledge-based analysis, this paper studies unbeatability for both consensus\nand k-set consensus. We present unbeatable solutions to non-uniform consensus\nand k-set consensus, and uniform consensus in synchronous message-passing\ncontexts with crash failures.\n  The k-set consensus problem is much more technically challenging than\nconsensus, and its analysis has triggered the development of the topological\napproach to distributed computing. Worst-case lower bounds for this problem\nhave required either techniques based on algebraic topology, or reduction-based\nproofs. Our proof of unbeatability is purely combinatorial, and is a direct,\nalbeit nontrivial, generalization of the one for consensus. We also present an\nalternative topological unbeatability proof that allows to understand the\nconnection between the connectivity of protocol complexes and the decision time\nof processes.\n  For the synchronous model, only solutions to the uniform variant of k-set\nconsensus have been offered. Based on our unbeatable protocols for uniform\nconsensus and for non-uniform k-set consensus, we present a uniform k-set\nconsensus protocol that strictly dominates all known solutions to this problem\nin the synchronous model."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.7210v1", 
    "title": "QoS Based Framework for Effective Web Services in Cloud Computing", 
    "arxiv-id": "1311.7210v1", 
    "author": "Nagesh N. Jadhav", 
    "publish": "2013-11-28T05:16:09Z", 
    "summary": "Enhancements in technology always follow Consumer requirements. Consumer\nrequires best of service with least possible mismatch and on time. Numerous\napplications available today are based on Web Services and Cloud Computing.\nRecently, there exist many Web Services with similar functional\ncharacteristics. Choosing a right Service from group of similar Web Service is\na complicated task for Service Consumer. In that case, Service Consumer can\ndiscover the required Web Service using non functional attributes of the Web\nServices such as QoS. Proposed layered architecture and Web Service Cloud\ni.e.WS Cloud computing Framework synthesizes the Non functional attributes that\nincludes reliability, availability, response time, latency etc. The Service\nConsumer is projected to provide the QoS requirements as part of Service\ndiscovery query. This framework will discover and filter the Web Services form\nthe cloud and rank them according to Service Consumer preferences to facilitate\nService on time."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.7422v1", 
    "title": "LiteLab: Efficient Large-scale Network Experiments", 
    "arxiv-id": "1311.7422v1", 
    "author": "Jussi Kangasharju", 
    "publish": "2013-11-28T20:56:09Z", 
    "summary": "Large-scale network experiments is a challenging problem. Simulations,\nemulations, and real-world testbeds all have their advantages and\ndisadvantages. In this paper we present LiteLab, a light-weight platform\nspecialized for large-scale networking experiments. We cover in detail its\ndesign, key features, and architecture. We also perform an extensive evaluation\nof LiteLab's performance and accuracy and show that it is able to both simulate\nnetwork parameters with high accuracy, and also able to scale up to very large\nnetworks. LiteLab is flexible, easy to deploy, and allows researchers to\nperform large-scale network experiments with a short development cycle. We have\nused LiteLab for many different kinds of network experiments and are planning\nto make it available for others to use as well."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.7435v1", 
    "title": "Experimenting with BitTorrent on a Cluster: A Good or a Bad Idea?", 
    "arxiv-id": "1311.7435v1", 
    "author": "Jussi Kangasharju", 
    "publish": "2013-11-28T22:34:11Z", 
    "summary": "Evaluation of large-scale network systems and applications is usually done in\none of three ways: simulations, real deployment on Internet, or on an emulated\nnetwork testbed such as a cluster. Simulations can study very large systems but\noften abstract out many practical details, whereas real world tests are often\nquite small, on the order of a few hundred nodes at most, but have very\nrealistic conditions. Clusters and other dedicated testbeds offer a middle\nground between the two: large systems with real application code. They also\ntypically allow configuring the testbed to enable repeatable experiments. In\nthis paper we explore how to run large BitTorrent experiments in a cluster\nsetup. We have chosen BitTorrent because the source code is available and it\nhas been a popular target for research. Our contribution is twofold. First, we\nshow how to tweak and configure the BitTorrent client to allow for a maximum\nnumber of clients to be run on a single machine, without running into any\nphysical limits of the machine. Second, our results show that the behavior of\nBitTorrent can be very sensitive to the configuration and we revisit some\nexisting BitTorrent research and consider the implications of our findings on\npreviously published results. As we show in this paper, BitTorrent can change\nits behavior in subtle ways which are sometimes ignored in published works."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1311.7676v2", 
    "title": "Constructing Gazetteers from Volunteered Big Geo-Data Based on Hadoop", 
    "arxiv-id": "1311.7676v2", 
    "author": "Yue Zhang", 
    "publish": "2013-11-29T19:52:42Z", 
    "summary": "Traditional gazetteers are built and maintained by authoritative mapping\nagencies. In the age of Big Data, it is possible to construct gazetteers in a\ndata-driven approach by mining rich volunteered geographic information (VGI)\nfrom the Web. In this research, we build a scalable distributed platform and a\nhigh-performance geoprocessing workflow based on the Hadoop ecosystem to\nharvest crowd-sourced gazetteer entries. Using experiments based on geotagged\ndatasets in Flickr, we find that the MapReduce-based workflow running on the\nspatially enabled Hadoop cluster can reduce the processing time compared with\ntraditional desktop-based operations by an order of magnitude. We demonstrate\nhow to use such a novel spatial-computing infrastructure to facilitate\ngazetteer research. In addition, we introduce a provenance-based trust model\nfor quality assurance. This work offers new insights on enriching future\ngazetteers with the use of Hadoop clusters, and makes contributions in\nconnecting GIS to the cloud computing environment for the next frontier of Big\nGeo-Data analytics."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1402.0264v1", 
    "title": "A Many-core Machine Model for Designing Algorithms with Minimum   Parallelism Overheads", 
    "arxiv-id": "1402.0264v1", 
    "author": "Ning Xie", 
    "publish": "2014-02-03T00:22:59Z", 
    "summary": "We present a model of multithreaded computation, combining fork-join and\nsingle-instruction-multiple-data parallelisms, with an emphasis on estimating\nparallelism overheads of programs written for modern many-core architectures.\nWe establish a Graham-Brent theorem for this model so as to estimate execution\ntime of programs running on a given number of streaming multiprocessors. We\nevaluate the benefits of our model with four fundamental algorithms from\nscientific computing. In each case, our model is used to minimize parallelism\noverheads by determining an appropriate value range for a given program\nparameter; moreover experimentation confirms the model's prediction."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1402.0696v1", 
    "title": "On-Demand Grid Provisioning Using Cloud Infrastructures and Related   Virtualization Tools: A Survey and Taxonomy", 
    "arxiv-id": "1402.0696v1", 
    "author": "Mohammed Bakri Bashir", 
    "publish": "2014-02-04T11:27:56Z", 
    "summary": "Recent researches have shown that grid resources can be accessed by client\non-demand, with the help of virtualization technology in the Cloud. The virtual\nmachines hosted by the hypervisors are being utilized to build the grid network\nwithin the cloud environment. The aim of this study is to survey some concepts\nused for the on-demand grid provisioning using Infrastructure as a Service\nCloud and the taxonomy of its related components. This paper, discusses the\ndifferent approaches for on-demand grid using infrastructural Cloud, the issues\nit tries to address and the implementation tools. The paper also, proposed an\nextended classification for the virtualization technology used and a new\nclassification for the Grid-Cloud integration which was based on the\narchitecture, communication flow and the user demand for the Grid resources.\nThis survey, tools and taxonomies presented here will contribute as a guide in\nthe design of future architectures for further researches."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.123", 
    "link": "http://arxiv.org/pdf/1402.1309v1", 
    "title": "Backtracking algorithms for service selection", 
    "arxiv-id": "1402.1309v1", 
    "author": "Dejan Milojicic", 
    "publish": "2014-02-06T10:38:26Z", 
    "summary": "In this paper, we explore the automation of services' compositions. We focus\non the service selection problem. In the formulation that we consider, the\nproblem's inputs are constituted by a behavioral composition whose abstract\nservices must be bound to concrete ones. The objective is to find the binding\nthat optimizes the {\\it utility} of the composition under some services level\nagreements. We propose a complete solution. Firstly, we show that the service\nselection problem can be mapped onto a Constraint Satisfaction Problem (CSP).\nThe benefit of this mapping is that the large know-how in the resolution of the\nCSP can be used for the service selection problem. Among the existing\ntechniques for solving CSP, we consider the backtracking. Our second\ncontribution is to propose various backtracking-based algorithms for the\nservice selection problem. The proposed variants are inspired by existing\nheuristics for the CSP. We analyze the runtime gain of our framework over an\nintuitive resolution based on exhaustive search. Our last contribution is an\nexperimental evaluation in which we demonstrate that there is an effective gain\nin using backtracking instead of some comparable approaches. The experiments\nalso show that our proposal can be used for finding in real time, optimal\nsolutions on small and medium services' compositions."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.1932v1", 
    "title": "An EMUSIM Technique and its Components in Cloud Computing- A Review", 
    "arxiv-id": "1402.1932v1", 
    "author": "Prince Jain", 
    "publish": "2014-02-09T10:10:48Z", 
    "summary": "Recent efforts to design and develop Cloud technologies focus on defining\nnovel methods, policies and mechanisms for efficiently managing Cloud\ninfrastructures. One key challenge potential Cloud customers have before\nrenting resources is to know how their services will behave in a set of\nresources and the costs involved when growing and shrinking their resource\npool. Most of the studies in this area rely on simulation-based experiments,\nwhich consider simplified modeling of applications and computing environment.\nIn order to better predict service's behavior on Cloud platforms, an integrated\narchitecture that is based on both simulation and emulation. The proposed\narchitecture, named EMUSIM, automatically extracts information from application\nbehavior via emulation and then uses this information to generate the\ncorresponding simulation model. This paper presents brief overview of the\nEMUSIM technique and its components. The work in this paper focuses on\narchitecture and operation details of Automated Emulation Framework (AEF),\nQAppDeployer and proposes Cloud Sim Application for Simulation techniques."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2090v1", 
    "title": "We Are Impatient: Algorithms for Geographically Distributed Load   Balancing with (Almost) Arbitrary Load Functions", 
    "arxiv-id": "1402.2090v1", 
    "author": "Krzysztof Rzadca", 
    "publish": "2014-02-10T10:33:52Z", 
    "summary": "In geographically-distributed systems, communication latencies are\nnon-negligible. The perceived processing time of a request is thus composed of\nthe time needed to route the request to the server and the true processing\ntime. Once a request reaches a target server, the processing time depends on\nthe total load of that server; this dependency is described by a load function.\nWe consider a broad class of load functions; we just require that they are\nconvex and two times differentiable. In particular our model can be applied to\nheterogeneous systems in which every server has a different load function. This\napproach allows us not only to generalize results for queuing theory and for\nbatches of requests, but also to use empirically-derived load functions,\nmeasured in a system under stress-testing. The optimal assignment of requests\nto servers is communication-balanced, i.e. for any pair of non\nperfectly-balanced servers, the reduction of processing time resulting from\nmoving a single request from the overloaded to underloaded server is smaller\nthan the additional communication latency. We present a centralized and a\ndecentralized algorithm for optimal load balancing. We prove bounds on the\nalgorithms' convergence. To the best of our knowledge these bounds were not\nknown even for the special cases studied previously (queuing theory and batches\nof requests). Both algorithms are any-time algorithms. In the decentralized\nalgorithm, each server balances the load with a randomly chosen peer. Such\nalgorithm is very robust to failures. We prove that the decentralized algorithm\nperforms locally-optimal steps. Our work extends the currently known results by\nconsidering a broad class of load functions and by establishing theoretical\nbounds on the algorithms' convergence. These results are applicable for servers\nwhose characteristics under load cannot be described by a standard mathematical\nmodels."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2107v1", 
    "title": "OS-Assisted Task Preemption for Hadoop", 
    "arxiv-id": "1402.2107v1", 
    "author": "Pietro Michiardi", 
    "publish": "2014-02-10T11:14:19Z", 
    "summary": "This work introduces a new task preemption primitive for Hadoop, that allows\ntasks to be suspended and resumed exploiting existing memory management\nmechanisms readily available in modern operating systems. Our technique fills\nthe gap that exists between the two extremes cases of killing tasks (which\nwaste work) or waiting for their completion (which introduces latency):\nexperimental results indicate superior performance and very small overheads\nwhen compared to existing alternatives."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2491v1", 
    "title": "Optimizing the Cost for Resource Subscription Policy in IaaS Cloud", 
    "arxiv-id": "1402.2491v1", 
    "author": "K. Saravanan", 
    "publish": "2014-02-11T14:11:18Z", 
    "summary": "Cloud computing allow the users to efficiently and dynamically provision\ncomputing resource to meet their IT needs. Cloud Provider offers two\nsubscription plan to the customer namely reservation and on-demand. The\nreservation plan is typically cheaper than on-demand plan. If the actual\ncomputing demand is known in advance reserving the resource would be\nstraightforward. The challenge is how to make properly resource provisioning\nand how the customers efficiently purchase the provisioning options under\nreservation and on-demand. To address this issue, two-phase algorithm are\nproposed to minimize service provision cost in both reservation and on-demand\nplan. To reserve the correct and optimal amount of resources during\nreservation, proposed a mathematical formulae in the first phase. To predict\nresource demand, use kalman filter in the second phase. The evaluation result\nshows that the two-phase algorithm can significantly reduce the provision cost\nand the prediction is of reasonable accuracy."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2552v1", 
    "title": "Linial's Lower Bound Made Easy", 
    "arxiv-id": "1402.2552v1", 
    "author": "Jukka Suomela", 
    "publish": "2014-02-11T16:33:07Z", 
    "summary": "Linial's seminal result shows that any deterministic distributed algorithm\nthat finds a $3$-colouring of an $n$-cycle requires at least $\\log^*(n)/2 - 1$\ncommunication rounds. We give a new simpler proof of this theorem."
},{
    "category": "cs.DC", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2810v1", 
    "title": "Energy Efficient Scheduling of MapReduce Jobs", 
    "arxiv-id": "1402.2810v1", 
    "author": "Georgios Zois", 
    "publish": "2014-02-12T13:16:16Z", 
    "summary": "MapReduce is emerged as a prominent programming model for data-intensive\ncomputation. In this work, we study power-aware MapReduce scheduling in the\nspeed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on\nthe minimization of the total weighted completion time of a set of MapReduce\njobs under a given budget of energy. Using a linear programming relaxation of\nour problem, we derive a polynomial time constant-factor approximation\nalgorithm. We also propose a convex programming formulation that we combine\nwith standard list scheduling policies, and we evaluate their performance using\nsimulations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3034v1", 
    "title": "Formal Specification Language Based IaaS Cloud Workload Regression   Analysis", 
    "arxiv-id": "1402.3034v1", 
    "author": "Inderveer Chana", 
    "publish": "2014-02-13T05:24:37Z", 
    "summary": "Cloud Computing is an emerging area for accessing computing resources. In\ngeneral, Cloud service providers offer services that can be clustered into\nthree categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload\nanalysis. The efficient Cloud workload resource mapping technique is proposed.\nThis paper aims to provide a means of understanding and investigating IaaS\nCloud workloads and the resources. In this paper, regression analysis is used\nto analyze the Cloud workloads and identifies the relationship between Cloud\nworkloads and available resources. The effective organization of dynamic nature\nresources can be done with the help of Cloud workloads. Till Cloud workload is\nconsidered a vital talent, the Cloud resources cannot be consumed in an\neffective style. The proposed technique has been validated by Z Formal\nspecification language. This approach is effective in minimizing the cost and\nsubmission burst time of Cloud workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3305v1", 
    "title": "Real-Time Notification for Resource Synchronization", 
    "arxiv-id": "1402.3305v1", 
    "author": "Michael L. Nelson", 
    "publish": "2014-02-11T21:04:54Z", 
    "summary": "Web applications frequently leverage resources made available by remote web\nservers. As resources are created, updated, deleted, or moved, these\napplications face challenges to remain in lockstep with the server's change\ndynamics. Several approaches exist to help meet this challenge for use cases\nwhere \"good enough\" synchronization is acceptable. But when strict resource\ncoverage or low synchronization latency is required, commonly accepted\nWeb-based solutions remain elusive. This paper details characteristics of an\napproach that aims at decreasing synchronization latency while maintaining\ndesired levels of accuracy. The approach builds on pushing change notifications\nand pulling changed resources and it is explored with an experiment based on a\nDBpedia Live instance."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3319v2", 
    "title": "Flow-based reputation with uncertainty: Evidence-Based Subjective Logic", 
    "arxiv-id": "1402.3319v2", 
    "author": "Nicola Zannone", 
    "publish": "2014-02-13T21:49:52Z", 
    "summary": "The concept of reputation is widely used as a measure of trustworthiness\nbased on ratings from members in a community. The adoption of reputation\nsystems, however, relies on their ability to capture the actual trustworthiness\nof a target. Several reputation models for aggregating trust information have\nbeen proposed in the literature. The choice of model has an impact on the\nreliability of the aggregated trust information as well as on the procedure\nused to compute reputations. Two prominent models are flow-based reputation\n(e.g., EigenTrust, PageRank) and Subjective Logic based reputation. Flow-based\nmodels provide an automated method to aggregate trust information, but they are\nnot able to express the level of uncertainty in the information. In contrast,\nSubjective Logic extends probabilistic models with an explicit notion of\nuncertainty, but the calculation of reputation depends on the structure of the\ntrust network and often requires information to be discarded. These are severe\ndrawbacks.\n  In this work, we observe that the `opinion discounting' operation in\nSubjective Logic has a number of basic problems. We resolve these problems by\nproviding a new discounting operator that describes the flow of evidence from\none party to another. The adoption of our discounting rule results in a\nconsistent Subjective Logic algebra that is entirely based on the handling of\nevidence. We show that the new algebra enables the construction of an automated\nreputation assessment procedure for arbitrary trust networks, where the\ncalculation no longer depends on the structure of the network, and does not\nneed to throw away any information. Thus, we obtain the best of both worlds:\nflow-based reputation and consistent handling of uncertainties."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3781v1", 
    "title": "A Framework for Developing Real-Time OLAP algorithm using Multi-core   processing and GPU: Heterogeneous Computing", 
    "arxiv-id": "1402.3781v1", 
    "author": "M H Habaebi", 
    "publish": "2014-02-16T10:11:57Z", 
    "summary": "The overwhelmingly increasing amount of stored data has spurred researchers\nseeking different methods in order to optimally take advantage of it which\nmostly have faced a response time problem as a result of this enormous size of\ndata. Most of solutions have suggested materialization as a favourite solution.\nHowever, such a solution cannot attain Real- Time answers anyhow. In this paper\nwe propose a framework illustrating the barriers and suggested solutions in the\nway of achieving Real-Time OLAP answers that are significantly used in decision\nsupport systems and data warehouses."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3788v1", 
    "title": "Using of GPUs for cluster analysis of large data by K-means method", 
    "arxiv-id": "1402.3788v1", 
    "author": "Natalya Litvinenko", 
    "publish": "2014-02-16T11:36:39Z", 
    "summary": "This problem was solved within the framework of the grant project \"Solving of\nproblems of cluster analysis with application of parallel algorithms and cloud\ntechnologies\" in the Institute of Mathematics and Mathematical Modelling in\nAlmaty. The problem of cluster analysis for the large amount of data is very\nimportant in different areas of science - genetics, biology, sociology etc. At\nthe same time, such statistical known packages as STATISTICA, STADIA, SYSTAT\nand others do not allow to solve large problems. The new algorithm that uses\nthe high processing power of GPUs for solving clustering problems by the\nK-means method was developed. This algorithm is implemented as a C++\napplication in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce\n660. The developed software package for solving clustering problems by the\nmethod of K - means with using GPUs allows us to handle up to 2 million records\nwith number of features up to 25. The gain in the computing time is in factor\n5. We plan to increase this factor up to 20-30 after improving the algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICCCCM.2013.6648901", 
    "link": "http://arxiv.org/pdf/1402.3789v1", 
    "title": "Parallel algorithms for problems of cluster analysis with very large   amount of data", 
    "arxiv-id": "1402.3789v1", 
    "author": "Natalya Litvinenko", 
    "publish": "2014-02-16T11:53:30Z", 
    "summary": "In this paper we solve on GPUs massive problems with large amount of data,\nwhich are not appropriate for solution with the SIMD technology. For the given\nproblem we consider a three-level parallelization. The multithreading of CPU is\nused at the top level and graphic processors for massive computing. For solving\nproblems of cluster analysis on GPUs the nearest neighbor method (NNM) is\ndeveloped. This algorithm allows us to handle up to 2 millions records with\nnumber of features up to 25. Since sequential and parallel algorithms are\nfundamentally different, it is difficult to compare the computation times.\nHowever, some comparisons are made. The gain in the computing time is about 10\ntimes. We plan to increase this factor up to 50-100 after fine tuning of\nalgorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1080/13658816.2013.879151", 
    "link": "http://arxiv.org/pdf/1402.4010v1", 
    "title": "A GRASS GIS parallel module for radio-propagation predictions", 
    "arxiv-id": "1402.4010v1", 
    "author": "Peter Koro\u0161ec", 
    "publish": "2014-02-17T14:10:12Z", 
    "summary": "Geographical information systems are ideal candidates for the application of\nparallel programming techniques, mainly because they usually handle large data\nsets. To help us deal with complex calculations over such data sets, we\ninvestigated the performance constraints of a classic master-worker parallel\nparadigm over a message-passing communication model. To this end, we present a\nnew approach that employs an external database in order to improve the\ncalculation/communication overlap, thus reducing the idle times for the worker\nprocesses. The presented approach is implemented as part of a parallel\nradio-coverage prediction tool for the GRASS environment. The prediction\ncalculation employs digital elevation models and land-usage data in order to\nanalyze the radio coverage of a geographical area. We provide an extended\nanalysis of the experimental results, which are based on real data from an LTE\nnetwork currently deployed in Slovenia. Based on the results of the\nexperiments, which were performed on a computer cluster, the new approach\nexhibits better scalability than the traditional master-worker approach. We\nsuccessfully tackled real-world data sets, while greatly reducing the\nprocessing time and saturating the hardware utilization."
},{
    "category": "cs.DC", 
    "doi": "10.1080/13658816.2013.879151", 
    "link": "http://arxiv.org/pdf/1402.4247v1", 
    "title": "Effects of Easy Hybrid Parallelization with CUDA for   Numerical-Atomic-Orbital Density Functional Theory Calculation", 
    "arxiv-id": "1402.4247v1", 
    "author": "Sang-Mook Lee", 
    "publish": "2014-02-18T08:25:47Z", 
    "summary": "We modified a MPI-friendly density functional theory (DFT) source code within\nhybrid parallelization including CUDA. Our objective is to find out how simple\nconversions within the hybrid parallelization with mid-range GPUs affect DFT\ncode not originally suitable to CUDA. We settled several rules of hybrid\nparallelization for numerical-atomic-orbital (NAO) DFT codes. The test was\nperformed on a magnetite material system with OpenMX code by utilizing a\nhardware system containing 2 Xeon E5606 CPUs and 2 Quadro 4000 GPUs. 3-way\nhybrid routines obtained a speedup of 7.55 while 2-way hybrid speedup by 10.94.\nGPUs with CUDA complement the efficiency of OpenMP and compensate CPUs'\nexcessive competition within MPI."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22315381/IJETT-V8P273", 
    "link": "http://arxiv.org/pdf/1402.4758v2", 
    "title": "On Cloud-based Oversubscription", 
    "arxiv-id": "1402.4758v2", 
    "author": "Robert Green", 
    "publish": "2014-02-19T18:25:54Z", 
    "summary": "Rising trends in the number of customers turning to the cloud for their\ncomputing needs has made effective resource allocation imperative for cloud\nservice providers. In order to maximize profits and reduce waste, providers\nhave started to explore the role of oversubscribing cloud resources. However,\nthe benefits of cloud-based oversubscription are not without inherent risks.\nThis paper attempts to unveil the incentives, risks, and techniques behind\noversubscription in a cloud infrastructure. Additionally, an overview of the\ncurrent research that has been completed on this highly relevant topic is\nreviewed, and suggestions are made regarding potential avenues for future work."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22315381/IJETT-V8P273", 
    "link": "http://arxiv.org/pdf/1402.4958v1", 
    "title": "Erasure-Coded Byzantine Storage with Separate Metadata", 
    "arxiv-id": "1402.4958v1", 
    "author": "Marko Vukolic", 
    "publish": "2014-02-20T10:53:07Z", 
    "summary": "Although many distributed storage protocols have been introduced, a solution\nthat combines the strongest properties in terms of availability, consistency,\nfault-tolerance, storage complexity and the supported level of concurrency, has\nbeen elusive for a long time. Combining these properties is difficult,\nespecially if the resulting solution is required to be efficient and incur low\ncost. We present AWE, the first erasure-coded distributed implementation of a\nmulti-writer multi-reader read/write storage object that is, at the same time:\n(1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (i.e., with data\nnodes storing a bounded number of values) and (5) Byzantine fault-tolerant\n(BFT) using the optimal number of nodes. Furthermore, AWE is efficient since it\ndoes not use public-key cryptography and requires data nodes that support only\nreads and writes, further reducing the cost of deployment and ownership of a\ndistributed storage solution. Notably, AWE stores metadata separately from\n$k$-out-of-$n$ erasure-coded fragments. This enables AWE to be the first BFT\nprotocol that uses as few as $2t+k$ data nodes to tolerate $t$ Byzantine nodes,\nfor any $k \\ge 1$."
},{
    "category": "cs.DC", 
    "doi": "10.1186/s40064-016-1731-6", 
    "link": "http://arxiv.org/pdf/1402.4986v1", 
    "title": "Performance Impact of Data Layout on the GPU-accelerated IDW   Interpolation", 
    "arxiv-id": "1402.4986v1", 
    "author": "Hong Tian", 
    "publish": "2014-02-20T12:44:09Z", 
    "summary": "This paper focuses on evaluating the performance impact of different data\nlayouts on the GPU-accelerated IDW interpolation. First, we redesign and\nimprove our previous GPU implementation that was performed by exploiting the\nfeature CUDA Dynamic Parallel (CDP). And then, we implement three versions of\nGPU implementations, i.e., the naive version, the tiled version, and the\nimproved CDP version, based on five layouts including the Structure of Arrays\n(SoA), the Array of Sturcutes (AoS), the Array of aligned Sturcutes (AoaS), the\nStructure of Arrays of aligned Structures (SoAoS), and the Hybrid layout.\nExperimental results show that: the layouts AoS and AoaS achieve better\nperformance than the layout SoA for both the naive version and tiled version,\nwhile the layout SoA is the best choice for the improved CDP version. We also\nobserve that: for the two combined data layouts (the SoAoS and the Hybrid),\nthere are no notable performance gains when compared to other three basic\nlayouts. We recommend that: in practical applications, the layout AoaS is the\nbest choice since the tiled version is the fastest one among the three versions\nof GPU implementations, especially on single precision."
},{
    "category": "cs.DC", 
    "doi": "10.1186/s40064-016-1731-6", 
    "link": "http://arxiv.org/pdf/1402.5642v1", 
    "title": "VM Power Prediction in Distributed Systems for Maximizing Renewable   Energy Usage", 
    "arxiv-id": "1402.5642v1", 
    "author": "Ankur Sahai", 
    "publish": "2014-02-23T17:56:00Z", 
    "summary": "In the context of GreenPAD project it is important to predict the energy\nconsumption of individual (and mixture of) VMs / workload for optimal\nscheduling (running those VMs which require higher energy when there is more\ngreen energy available and vice-versa) in order to maximize green energy\nutilization.\n  For this we execute the following experiments on an Openstack cloud testbed\nconsisting of Fujitsu servers: VM energy measurement for different\nconfigurations (flavor + workload) and VM energy prediction for a new\nconfiguration. The automation framework for running these experiments uses bash\nscripts which call tools like 'stress' (simulating workloads), 'collected'\n(resource usage) and 'IPMI' (power measurement).\n  We propose a linear model for predicting the power usage of the VMs based on\nregression. We first collect the resource usage (using collected) and the\nassociated power usage (using IPMI) for different VM configurations and use\nthis to build a (multi-) regression model (between resource usage and VM energy\nconsumption). Then we use the information about the resource usage patterns of\nthe new workload to predict the power usage. For predicting power for mix of\nworkloads we execute (build a regression model based on) experiments with\nrandom workloads. We observe the highest energy usage for CPU-intensive\nworkloads followed by memory-intensive workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1402.6601v2", 
    "title": "Scheduling data flow program in xkaapi: A new affinity based Algorithm   for Heterogeneous Architectures", 
    "arxiv-id": "1402.6601v2", 
    "author": "Denis Trystram", 
    "publish": "2014-02-26T16:37:01Z", 
    "summary": "Efficient implementations of parallel applications on heterogeneous hybrid\narchitectures require a careful balance between computations and communications\nwith accelerator devices. Even if most of the communication time can be\noverlapped by computations, it is essential to reduce the total volume of\ncommunicated data. The literature therefore abounds with ad-hoc methods to\nreach that balance, but that are architecture and application dependent. We\npropose here a generic mechanism to automatically optimize the scheduling\nbetween CPUs and GPUs, and compare two strategies within this mechanism: the\nclassical Heterogeneous Earliest Finish Time (HEFT) algorithm and our new,\nparametrized, Distributed Affinity Dual Approximation algorithm (DADA), which\nconsists in grouping the tasks by affinity before running a fast dual\napproximation. We ran experiments on a heterogeneous parallel machine with six\nCPU cores and eight NVIDIA Fermi GPUs. Three standard dense linear algebra\nkernels from the PLASMA library have been ported on top of the Xkaapi runtime.\nWe report their performances. It results that HEFT and DADA perform well for\nvarious experimental conditions, but that DADA performs better for larger\nsystems and number of GPUs, and, in most cases, generates much lower data\ntransfers than HEFT to achieve the same performance."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1404.0743v2", 
    "title": "Pentago is a First Player Win: Strongly Solving a Game Using Parallel   In-Core Retrograde Analysis", 
    "arxiv-id": "1404.0743v2", 
    "author": "Geoffrey Irving", 
    "publish": "2014-04-03T01:01:21Z", 
    "summary": "We present a strong solution of the board game pentago, computed using\nexhaustive parallel retrograde analysis in 4 hours on 98304 ($3 \\times 2^{15}$)\nthreads of NERSC's Cray Edison. At $3.0 \\times 10^{15}$ states, pentago is the\nlargest divergent game solved to date by two orders of magnitude, and the only\nexample of a nontrivial divergent game solved using retrograde analysis. Unlike\nprevious retrograde analyses, our computation was performed entirely in-core,\nwriting only a small portion of the results to disk; an out-of-core\nimplementation would have been much slower. Symmetry was used to reduce\nbranching factor and exploit instruction level parallelism. Despite a\ntheoretically embarrassingly parallel structure, asynchronous message passing\nwas required to fit the computation into available RAM, causing latency\nproblems on an older Cray machine. All code and data for the project are open\nsource, together with a website which combines database lookup and on-the-fly\ncomputation to interactively explore the strong solution."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1404.1124v2", 
    "title": "A Task Allocation Schema Based on Response Time Optimization in Cloud   Computing", 
    "arxiv-id": "1404.1124v2", 
    "author": "Meilin Liu", 
    "publish": "2014-04-04T01:24:25Z", 
    "summary": "Cloud computing is a newly emerging distributed computing which is evolved\nfrom Grid computing. Task scheduling is the core research of cloud computing\nwhich studies how to allocate the tasks among the physical nodes so that the\ntasks can get a balanced allocation or each task's execution cost decreases to\nthe minimum or the overall system performance is optimal. Unlike the previous\ntask slices' sequential execution of an independent task in the model of which\nthe target is processing time, we build a model that targets at the response\ntime, in which the task slices are executed in parallel. Then we give its\nsolution with a method based on an improved adjusting entropy function. At\nlast, we design a new task scheduling algorithm. Experimental results show that\nthe response time of our proposed algorithm is much lower than the\ngame-theoretic algorithm and balanced scheduling algorithm and compared with\nthe balanced scheduling algorithm, game-theoretic algorithm is not necessarily\nsuperior in parallel although its objective function value is better."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-09873-9_47", 
    "link": "http://arxiv.org/pdf/1404.1328v1", 
    "title": "Efficient Task Replication for Fast Response Times in Parallel   Computation", 
    "arxiv-id": "1404.1328v1", 
    "author": "Gregory Wornell", 
    "publish": "2014-04-04T18:05:45Z", 
    "summary": "One typical use case of large-scale distributed computing in data centers is\nto decompose a computation job into many independent tasks and run them in\nparallel on different machines, sometimes known as the \"embarrassingly\nparallel\" computation. For this type of computation, one challenge is that the\ntime to execute a task for each machine is inherently variable, and the overall\nresponse time is constrained by the execution time of the slowest machine. To\naddress this issue, system designers introduce task replication, which sends\nthe same task to multiple machines, and obtains result from the machine that\nfinishes first. While task replication reduces response time, it usually\nincreases resource usage. In this work, we propose a theoretical framework to\nanalyze the trade-off between response time and resource usage. We show that,\nwhile in general, there is a tension between response time and resource usage,\nthere exist scenarios where replicating tasks judiciously reduces completion\ntime and resource usage simultaneously. Given the execution time distribution\nfor machines, we investigate the conditions for a scheduling policy to achieve\noptimal performance trade-off, and propose efficient algorithms to search for\noptimal or near-optimal scheduling policies. Our analysis gives insights on\nwhen and why replication helps, which can be used to guide scheduler design in\nlarge-scale distributed computing systems."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.1814v1", 
    "title": "CernVM Online and Cloud Gateway: a uniform interface for CernVM   contextualization and deployment", 
    "arxiv-id": "1404.1814v1", 
    "author": "R. Meusel", 
    "publish": "2014-04-07T15:11:38Z", 
    "summary": "In a virtualized environment, contextualization is the process of configuring\na VM instance for the needs of various deployment use cases. Contextualization\nin CernVM can be done by passing a handwritten context to the user data field\nof cloud APIs, when running CernVM on the cloud, or by using CernVM web\ninterface when running the VM locally. CernVM Online is a publicly accessible\nweb interface that unifies these two procedures. A user is able to define,\nstore and share CernVM contexts using CernVM Online and then apply them either\nin a cloud by using CernVM Cloud Gateway or on a local VM with the single-step\npairing mechanism. CernVM Cloud Gateway is a distributed system that provides a\nsingle interface to use multiple and different clouds (by location or type,\nprivate or public). Cloud gateway has been so far integrated with OpenNebula,\nCloudStack and EC2 tools interfaces. A user, with access to a number of clouds,\ncan run CernVM cloud agents that will communicate with these clouds using their\ninterfaces, and then use one single interface to deploy and scale CernVM\nclusters. CernVM clusters are defined in CernVM Online and consist of a set of\nCernVM instances that are contextualized and can communicate with each other."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.2076v1", 
    "title": "Efficient Optimal Algorithm of Task Scheduling in Cloud Computing   Environment", 
    "arxiv-id": "1404.2076v1", 
    "author": "Saloni Jain", 
    "publish": "2014-04-08T10:21:03Z", 
    "summary": "Cloud computing is an emerging technology in distributed computing which\nfacilitates pay per model as per user demand and requirement.Cloud consist of a\ncollection of virtual machine which includes both computational and storage\nfacility. The primary aim of cloud computing is to provide efficient access to\nremote and geographically distributed resources. Cloud is developing day by day\nand faces many challenges, one of them is scheduling. Scheduling refers to a\nset of policies to control the order of work to be performed by a computer\nsystem. A good scheduler adapts its scheduling strategy according to the\nchanging environment and the type of task. In this research paper we presented\na Generalized Priority algorithm for efficient execution of task and comparison\nwith FCFS and Round Robin Scheduling. Algorithm should be tested in cloud Sim\ntoolkit and result shows that it gives better performance compared to other\ntraditional scheduling algorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.2157v1", 
    "title": "Leveraging VMware vCloud Director Virtual Applications (vApps) for   Operational Expense (OpEx) Efficiency", 
    "arxiv-id": "1404.2157v1", 
    "author": "Ramon Alvarez", 
    "publish": "2014-04-08T14:41:51Z", 
    "summary": "Virtualization technology has provided many benefits to organizations, but it\ncannot provide automation. This causes operational expenditure (OpEx)\ninefficiencies, which are solved by cloud computing (vCloud Director vApps).\nOrganizations have adopted virtualization technology to reduce IT costs and\nmeet business needs. In addition to improved CapEx efficiency, virtualization\nhas enabled organizations to respond to business needs faster. While\nvirtualization has dramatically optimized core IT infrastructures,\norganizations struggle to reduce OpEx costs. Because virtualization only\naddresses server consolidation, administrators are faced with the manual and\nresource-intensive day-to-day tasks of managing the rest of the data center:\nnetworking, storage, user management. This manuscript presents details on how\nleverage vApps based on a virtualized platform to improve CapEx efficiency in\ntoday s data center. The combination of virtualization and cloud computing can\ntransform the data center into a dynamic, scalable, and agile resource capable\nof achieving significant CapEx and OpEx cost savings."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.2187v1", 
    "title": "Coherent Causal Memory", 
    "arxiv-id": "1404.2187v1", 
    "author": "Ernie Cohen", 
    "publish": "2014-03-12T22:58:50Z", 
    "summary": "Coherent causal memory (CCM) is causal memory in which prefixes of an\nexecution can be mapped to global memory states in a consistent way. While CCM\nrequires conflicting pairs of writes to be globally ordered, it allows writes\nto remain unordered with respect to both reads and nonconflicting writes.\nNevertheless, it supports assertional, state-based program reasoning using\ngeneralized Owicki-Gries proof outlines (where assertions can be attached to\nany causal program edge). Indeed, we show that from a reasoning standpoint, CCM\ndiffers from sequentially consistent (SC) memory only in that ghost code added\nby the user is not allowed to introduce new write-write races.\n  While CCM provides most of the formal reasoning leverage of SC memory, it is\nmuch more efficiently implemented. As an illustration, we describe a simple\nprogramming discipline that provides CCM on top of x86-TSO. The discipline is\nconsiderably more relaxed than the one needed to ensure SC; for example, it\nintroduces no burden whatsoever for programs in which at most one thread writes\nto any variable."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.2340v1", 
    "title": "Disaster Recovery Using Virtual Machines", 
    "arxiv-id": "1404.2340v1", 
    "author": "Dr. Timur Mirzoev", 
    "publish": "2014-04-09T01:05:38Z", 
    "summary": "Today, the importance of having 100% uptime for businesses and industries is\nclear: financial reasons and often strict government regulations for certain\nindustries require 100% business continuity. The concept of business continuity\n(BC), as Microsoft defines it: the ability of an organization to continue to\nfunction even after a disastrous event, accomplished through the deployment of\nredundant hardware and software, the use of fault tolerant systems, as well as\na solid backup and recovery strategy, directly relates to an organization s\nability to quickly restore and deploy IT backups and business operations in a\nshort period of time."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.2739v1", 
    "title": "Multiprocessor Scheduling of Dependent Tasks to Minimize Makespan and   Reliability Cost Using NSGA-II", 
    "arxiv-id": "1404.2739v1", 
    "author": "A. Anju", 
    "publish": "2014-04-10T08:53:10Z", 
    "summary": "Algorithms developed for scheduling applications on heterogeneous\nmultiprocessor system focus on asingle objective such as execution time, cost\nor total data transmission time. However, if more than oneobjective (e.g.\nexecution cost and time, which may be in conflict) are considered, then the\nproblem becomes more challenging. This project is proposed to develop a\nmultiobjective scheduling algorithm using Evolutionary techniques for\nscheduling a set of dependent tasks on available resources in a multiprocessor\nenvironment which will minimize the makespan and reliability cost. A\nNon-dominated sorting Genetic Algorithm-II procedure has been developed to get\nthe pareto- optimal solutions. NSGA-II is a Elitist Evolutionary algorithm, and\nit takes the initial parental solution without any changes, in all iteration to\neliminate the problem of loss of some pareto-optimal solutions.NSGA-II uses\ncrowding distance concept to create a diversity of the solutions."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.3272v3", 
    "title": "Efficient Lock-free Binary Search Trees", 
    "arxiv-id": "1404.3272v3", 
    "author": "Philippas Tsigas", 
    "publish": "2014-04-12T09:59:02Z", 
    "summary": "In this paper we present a novel algorithm for concurrent lock-free internal\nbinary search trees (BST) and implement a Set abstract data type (ADT) based on\nthat. We show that in the presented lock-free BST algorithm the amortized step\ncomplexity of each set operation - {\\sc Add}, {\\sc Remove} and {\\sc Contains} -\nis $O(H(n) + c)$, where, $H(n)$ is the height of BST with $n$ number of nodes\nand $c$ is the contention during the execution. Our algorithm adapts to\ncontention measures according to read-write load. If the situation is\nread-heavy, the operations avoid helping pending concurrent {\\sc Remove}\noperations during traversal, and, adapt to interval contention. However, for\nwrite-heavy situations we let an operation help pending {\\sc Remove}, even\nthough it is not obstructed, and so adapt to tighter point contention. It uses\nsingle-word compare-and-swap (\\texttt{CAS}) operations. We show that our\nalgorithm has improved disjoint-access-parallelism compared to similar existing\nalgorithms. We prove that the presented algorithm is linearizable. To the best\nof our knowledge this is the first algorithm for any concurrent tree data\nstructure in which the modify operations are performed with an additive term of\ncontention measure."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.3321v2", 
    "title": "Performance Analysis of Irregular Collective Communication with the   Crystal Router Algorithm", 
    "arxiv-id": "1404.3321v2", 
    "author": "Erwin Laure", 
    "publish": "2014-04-12T21:43:39Z", 
    "summary": "In order to achieve exascale performance it is important to detect potential\nbottlenecks and identify strategies to overcome them. For this, both\napplications and system software must be analysed and potentially improved. The\nEU FP7 project Collaborative Research into Exascale Systemware, Tools &\nApplications (CRESTA) chose the approach to co-design advanced simulation\napplications and system software as well as development tools. In this paper,\nwe present the results of a co-design activity focused on the simulation code\nNEK5000 that aims at performance improvements of collective communication\noperations. We have analysed the algorithms that form the core of NEK5000's\ncommunication module in order to assess its viability on recent computer\narchitectures before starting to improve its performance. Our results show that\nthe crystal router algorithm performs well in sparse, irregular collective\noperations for medium and large processor number but improvements for even\nlarger system sizes of the future will be needed. We sketch the needed\nimprovements, which will make the communication algorithms also beneficial for\nother applications that need to implement latency-dominated communication\nschemes with short messages. The latency-optimised communication operations\nwill also become used in a runtime-system providing dynamic load balancing,\nunder development within CRESTA."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/3/032055", 
    "link": "http://arxiv.org/pdf/1404.3861v2", 
    "title": "Spinner: Scalable Graph Partitioning in the Cloud", 
    "arxiv-id": "1404.3861v2", 
    "author": "Georgos Siganos", 
    "publish": "2014-04-15T10:26:24Z", 
    "summary": "Several organizations, like social networks, store and routinely analyze\nlarge graphs as part of their daily operation. Such graphs are typically\ndistributed across multiple servers, and graph partitioning is critical for\nefficient graph management. Existing partitioning algorithms focus on finding\ngraph partitions with good locality, but disregard the pragmatic challenges of\nintegrating partitioning into large-scale graph management systems deployed on\nthe cloud, such as dealing with the scale and dynamicity of the graph and the\ncompute environment.\n  In this paper, we propose Spinner, a scalable and adaptive graph partitioning\nalgorithm based on label propagation designed on top of the Pregel model.\nSpinner scales to massive graphs, produces partitions with locality and balance\ncomparable to the state-of-the-art and efficiently adapts the partitioning upon\nchanges. We describe our algorithm and its implementation in the Pregel\nprogramming model that makes it possible to partition billion-vertex graphs. We\nevaluate Spinner with a variety of synthetic and real graphs and show that it\ncan compute partitions with quality comparable to the state-of-the art. In\nfact, by using Spinner in conjunction with the Giraph graph processing engine,\nwe speed up different applications by a factor of 2 relative to standard hash\npartitioning."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2600212.2600223", 
    "link": "http://arxiv.org/pdf/1404.3913v1", 
    "title": "Analysis of Dynamic Scheduling Strategies for Matrix Multiplication on   Heterogeneous Platforms", 
    "arxiv-id": "1404.3913v1", 
    "author": "Loris Marchal", 
    "publish": "2014-04-15T13:56:02Z", 
    "summary": "The tremendous increase in the size and heterogeneity of supercomputers makes\nit very difficult to predict the performance of a scheduling algorithm.\nTherefore, dynamic solutions, where scheduling decisions are made at runtime\nhave overpassed static allocation strategies. The simplicity and efficiency of\ndynamic schedulers such as Hadoop are a key of the success of the MapReduce\nframework. Dynamic schedulers such as StarPU, PaRSEC or StarSs are also\ndeveloped for more constrained computations, e.g. task graphs coming from\nlinear algebra. To make their decisions, these runtime systems make use of some\nstatic information, such as the distance of tasks to the critical path or the\naffinity between tasks and computing resources (CPU, GPU,...) and of dynamic\ninformation, such as where input data are actually located. In this paper, we\nconcentrate on two elementary linear algebra kernels, namely the outer product\nand the matrix multiplication. For each problem, we propose several dynamic\nstrategies that can be used at runtime and we provide an analytic study of\ntheir theoretical performance. We prove that the theoretical analysis provides\nvery good estimate of the amount of communications induced by a dynamic\nstrategy and can be used in order to efficiently determine thresholds used in\ndynamic scheduler, thus enabling to choose among them for a given problem and\narchitecture."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.4152v2", 
    "title": "SWAPHI: Smith-Waterman Protein Database Search on Xeon Phi Coprocessors", 
    "arxiv-id": "1404.4152v2", 
    "author": "Bertil Schmidt", 
    "publish": "2014-04-16T07:02:36Z", 
    "summary": "The maximal sensitivity of the Smith-Waterman (SW) algorithm has enabled its\nwide use in biological sequence database search. Unfortunately, the high\nsensitivity comes at the expense of quadratic time complexity, which makes the\nalgorithm computationally demanding for big databases. In this paper, we\npresent SWAPHI, the first parallelized algorithm employing Xeon Phi\ncoprocessors to accelerate SW protein database search. SWAPHI is designed based\non the scale-and-vectorize approach, i.e. it boosts alignment speed by\neffectively utilizing both the coarse-grained parallelism from the many\nco-processing cores (scale) and the fine-grained parallelism from the 512-bit\nwide single instruction, multiple data (SIMD) vectors within each core\n(vectorize). By searching against the large UniProtKB/TrEMBL protein database,\nSWAPHI achieves a performance of up to 58.8 billion cell updates per second\n(GCUPS) on one coprocessor and up to 228.4 GCUPS on four coprocessors.\nFurthermore, it demonstrates good parallel scalability on varying number of\ncoprocessors, and is also superior to both SWIPE on 16 high-end CPU cores and\nBLAST+ on 8 cores when using four coprocessors, with the maximum speedup of\n1.52 and 1.86, respectively. SWAPHI is written in C++ language (with a set of\nSIMD intrinsics), and is freely available at http://swaphi.sourceforge.net."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.4250v3", 
    "title": "Witness structures and immediate snapshot complexes", 
    "arxiv-id": "1404.4250v3", 
    "author": "Dmitry N. Kozlov", 
    "publish": "2014-04-16T14:06:12Z", 
    "summary": "In this paper we introduce and study a new family of combinatorial simplicial\ncomplexes, which we call immediate snapshot complexes. Our construction and\nterminology is strongly motivated by theoretical distributed computing, as\nthese complexes are combinatorial models of the standard protocol complexes\nassociated to immediate snapshot read/write shared memory communication model.\nIn order to define the immediate snapshot complexes we need a new combinatorial\nobject, which we call a witness structure. These objects are indexing the\nsimplices in the immediate snapshot complexes, while a special operation on\nthem, called ghosting, describes the combinatorics of taking simplicial\nboundary. In general, we develop the theory of witness structures and use it to\nprove several combinatorial as well as topological properties of the immediate\nsnapshot complexes."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.4653v1", 
    "title": "An Efficient and Balanced Platform for Data-Parallel Subsampling   Workloads", 
    "arxiv-id": "1404.4653v1", 
    "author": "Christopher Stewart", 
    "publish": "2014-04-17T21:13:43Z", 
    "summary": "With the advent of internet services, data started growing faster than it can\nbe processed. To personalize user experience, this enormous data has to be\nprocessed in real time, in interactive fashion. In order to achieve faster data\nprocessing often a statistical method called subsampling. Subsampling workloads\ncompute statistics from a random subset of sample data (i.e., a subsample).\nData-parallel platforms group these samples into tasks; each task subsamples\nits data in parallel.\n  Current, state-of-the-art platforms such as Hadoop are built for large tasks\nthat run for long periods of time, but applications with smaller average task\nsizes suffer large overheads on these platforms. Tasks in subsampling workloads\nare sized to minimize the number of overall cache misses, and these tasks can\ncomplete in seconds. This technique can reduce the overall length of a\nmap-reduce job, but only when the savings from the cache miss rate reduction\nare not eclipsed by the platform overhead of task creation and data\ndistribution.\n  In this thesis, we propose a data-parallel platform with an efficient data\ndistribution component that breaks data-parallel subsampling workloads into\ncompute clusters with tiny tasks. Each tiny task completes in few hundreds of\nmilliseconds to seconds. Tiny tasks reduce processor cache misses caused by\nrandom subsampling, which speeds up per-task running time. However, they cause\nsignificant scheduling overheads and data distribution challenges. We propose a\ntask knee-pointing algorithm and a dynamic scheduler that schedules the tasks\nto worker nodes based on the availability and response times of the data nodes.\nWe compare our framework against various configurations of BashReduce and\nHadoop. A detailed discussion of tiny task approach on two workloads, EAGLET\nand Netflix movie rating is presented."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.4910v1", 
    "title": "Enumerating Maximal Bicliques from a Large Graph using MapReduce", 
    "arxiv-id": "1404.4910v1", 
    "author": "Srikanta Tirthapura", 
    "publish": "2014-04-19T01:43:52Z", 
    "summary": "We consider the enumeration of maximal bipartite cliques (bicliques) from a\nlarge graph, a task central to many practical data mining problems in social\nnetwork analysis and bioinformatics. We present novel parallel algorithms for\nthe MapReduce platform, and an experimental evaluation using Hadoop MapReduce.\nOur algorithm is based on clustering the input graph into smaller sized\nsubgraphs, followed by processing different subgraphs in parallel. Our\nalgorithm uses two ideas that enable it to scale to large graphs: (1) the\nredundancy in work between different subgraph explorations is minimized through\na careful pruning of the search space, and (2) the load on different reducers\nis balanced through the use of an appropriate total order among the vertices.\nOur evaluation shows that the algorithm scales to large graphs with millions of\nedges and tens of mil- lions of maximal bicliques. To our knowledge, this is\nthe first work on maximal biclique enumeration for graphs of this scale."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.5552v1", 
    "title": "Tolerating Silent Data Corruption in Opaque Preconditioners", 
    "arxiv-id": "1404.5552v1", 
    "author": "Frank Mueller", 
    "publish": "2014-04-22T16:48:30Z", 
    "summary": "We demonstrate algorithm-based fault tolerance for silent, transient data\ncorruption in \"black-box\" preconditioners. We consider both additive Schwarz\ndomain decomposition with an ILU(k) subdomain solver, and algebraic multigrid,\nboth implemented in the Trilinos library. We evaluate faults that corrupt\npreconditioner results in both single and multiple MPI ranks. We then analyze\nhow our approach behaves when then application is scaled. Our technique is\nbased on a Selective Reliability approach that performs most operations in an\nunreliable mode, with only a few operations performed reliably. We also\ninvestigate two responses to faults and discuss the performance overheads\nimposed by each. For a non-symmetric problem solved using GMRES and ILU, we\nshow that at scale our fault tolerance approach incurs only 22% overhead for\nthe worst case. With detection techniques, we are able to reduce this overhead\nto 1.8% in the worst case."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.5813v4", 
    "title": "Topology of the immediate snapshot complexes", 
    "arxiv-id": "1404.5813v4", 
    "author": "Dmitry N. Kozlov", 
    "publish": "2014-04-23T13:05:14Z", 
    "summary": "The immediate snapshot complexes were introduced as combinatorial models for\nthe protocol complexes in the context of theoretical distributed computing. In\nthe previous work we have developed a formal language of witness structures in\norder to define and to analyze these complexes.\n  In this paper, we study topology of immediate snapshot complexes. It was\nknown that these complexes are always pure and that they are pseudomanifolds.\nHere we prove two further independent topological properties. First, we show\nthat immediate snapshot complexes are collapsible. Second, we show that these\ncomplexes are homeomorphic to closed balls. Specifically, given any immediate\nsnapshot complex $P(\\tr)$, we show that there exists a homeomorphism\n$\\varphi:\\da^{|\\supp\\tr|-1}\\ra P(\\tr)$, such that $\\varphi(\\sigma)$ is a\nsubcomplex of $P(\\tr)$, whenever $\\sigma$ is a simplex in the simplicial\ncomplex $\\da^{|\\supp\\tr|-1}$."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6415v1", 
    "title": "The Impact Failure Detector", 
    "arxiv-id": "1404.6415v1", 
    "author": "Pierre Sens", 
    "publish": "2014-04-25T13:35:47Z", 
    "summary": "This work proposes a new and flexible unreliable failure detector whose\noutput is related to the trust level of a set of processes. By expressing the\nrelevance of each process of the set by an impact factor value, our approach\nallows the tuning of the detector output, making possible a softer or stricter\nmonitoring. The idea behind our proposal is that, according to an acceptable\nmargin of failures and the impact factor assigned to processes, in some\nscenarios, the failure of some low impact processes may not change the user\nconfidence in the set of processes, while the crash of a high impact factor\nprocess may seriously affect it. We outline the application scenarios and the\nproposed unreliable failure detector, giving a detailed account of the concept\non which it is based."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6561v2", 
    "title": "Distributed Computing on Core-Periphery Networks: Axiom-based Design", 
    "arxiv-id": "1404.6561v2", 
    "author": "David Peleg", 
    "publish": "2014-04-25T21:28:14Z", 
    "summary": "Inspired by social networks and complex systems, we propose a core-periphery\nnetwork architecture that supports fast computation for many distributed\nalgorithms and is robust and efficient in number of links. Rather than\nproviding a concrete network model, we take an axiom-based design approach. We\nprovide three intuitive (and independent) algorithmic axioms and prove that any\nnetwork that satisfies all axioms enjoys an efficient algorithm for a range of\ntasks (e.g., MST, sparse matrix multiplication, etc.). We also show the\nminimality of our axiom set: for networks that satisfy any subset of the\naxioms, the same efficiency cannot be guaranteed for any deterministic\nalgorithm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6681v2", 
    "title": "Power Management Techniques for Data Centers: A Survey", 
    "arxiv-id": "1404.6681v2", 
    "author": "Sparsh Mittal", 
    "publish": "2014-04-26T20:39:09Z", 
    "summary": "With growing use of internet and exponential growth in amount of data to be\nstored and processed (known as 'big data'), the size of data centers has\ngreatly increased. This, however, has resulted in significant increase in the\npower consumption of the data centers. For this reason, managing power\nconsumption of data centers has become essential. In this paper, we highlight\nthe need of achieving energy efficiency in data centers and survey several\nrecent architectural techniques designed for power management of data centers.\nWe also present a classification of these techniques based on their\ncharacteristics. This paper aims to provide insights into the techniques for\nimproving energy efficiency of data centers and encourage the designers to\ninvent novel solutions for managing the large power dissipation of data\ncenters."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6719v1", 
    "title": "Practical Experience Report: The Performance of Paxos in the Cloud", 
    "arxiv-id": "1404.6719v1", 
    "author": "Ken Birman", 
    "publish": "2014-04-27T07:14:47Z", 
    "summary": "This experience report presents the results of an extensive performance\nevaluation conducted using four open-source implementations of Paxos deployed\nin Amazon's EC2. Paxos is a fundamental algorithm for building fault-tolerant\nservices, at the core of state-machine replication. Implementations of Paxos\nare currently used in many prototypes and production systems in both academia\nand industry. Although all protocols surveyed in the paper implement Paxos,\nthey are optimized in a number of different ways, resulting in very different\nbehavior, as we show in the paper. We have considered a variety of\nconfigurations and failure-free and faulty executions. In addition to reporting\nour findings, we propose and assess additional optimizations to existing\nimplementations."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6721v1", 
    "title": "Optimistic Parallel State-Machine Replication", 
    "arxiv-id": "1404.6721v1", 
    "author": "Fernando Pedone", 
    "publish": "2014-04-27T07:34:36Z", 
    "summary": "State-machine replication, a fundamental approach to fault tolerance,\nrequires replicas to execute commands deterministically, which usually results\nin sequential execution of commands. Sequential execution limits performance\nand underuses servers, which are increasingly parallel (i.e., multicore). To\nnarrow the gap between state-machine replication requirements and the\ncharacteristics of modern servers, researchers have recently come up with\nalternative execution models. This paper surveys existing approaches to\nparallel state-machine replication and proposes a novel optimistic protocol\nthat inherits the scalable features of previous techniques. Using a replicated\nB+-tree service, we demonstrate in the paper that our protocol outperforms the\nmost efficient techniques by a factor of 2.4 times."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.6772v1", 
    "title": "Why a Global Time is Needed in a Dependable SoS", 
    "arxiv-id": "1404.6772v1", 
    "author": "Hermann Kopetz", 
    "publish": "2014-04-27T15:33:12Z", 
    "summary": "A system-of-systems (SoS) is a large information processing system formed by\nthe integration of autonomous computer systems (called constituent systems,\nCS), physical machines and humans for the purpose of providing new synergistic\nservices and/or more efficient economic processes. In a number of applications,\ne.g robotics, the autonomous CSs must coordinate their actions in the temporal\ndomain to realize the desired objectives. In this paper we argue that the\nintroduction of a proper global physical time establishes a shared view about\nthe progress of physical time and helps to realize the temporal coordination of\nthe autonomous CSs. The available global time can also be used to simplify the\nsolution of many challenging problems within the SoS, such as distributed\nresource allocation, and helps to improve the dependability and fault-tolerance\nof the SoS."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.7494v1", 
    "title": "Intelligent Resource Allocation Technique For Desktop-as-a-Service in   Cloud Environment", 
    "arxiv-id": "1404.7494v1", 
    "author": "Dr. Ajay Shanker Singh", 
    "publish": "2014-04-29T19:17:32Z", 
    "summary": "The specialty of desktop-as-a-service cloud computing is that user can access\ntheir desktop and can execute applications in virtual desktops on remote\nservers. Resource management and resource utilization are most significant in\nthe area of desktop-as-a-service, cloud computing; however, handling a large\namount of clients in the most efficient manner poses important challenges.\nEspecially deciding how many clients to handle on one server, and where to\nexecute the user applications at each time is important. This is because we\nhave to ensure maximum resource utilization along with user data\nconfidentiality, customer satisfaction, scalability, minimum Service level\nagreement (SLA) violation etc. Assigning too many users to one server leads to\ncustomer dissatisfaction, while assigning too little leads to higher\ninvestments costs. So we have taken into consideration these two situations\nalso. We study different aspects to optimize the resource usage and customer\nsatisfaction. Here in this paper We proposed Intelligent Resource Allocation\n(IRA) Technique which assures the above mentioned parameters like minimum SLA\nviolation. For this, priorities are assigned to user requests based on their\nSLA Factors in order to maintain their confidentiality. The results of the\npaper indicate that by applying IRA Technique to the already existing\noverbooking mechanism will improve the performance of the system with\nsignificant reduction in SLA violation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.7548v2", 
    "title": "Faster Transaction Commit even when Nodes Crash", 
    "arxiv-id": "1404.7548v2", 
    "author": "Paul Ezhilchelvan", 
    "publish": "2014-04-29T22:16:48Z", 
    "summary": "Atomic broadcasts play a central role in serialisable in-memory transactions.\nBest performing ones block, when a node crashes, until a new view is installed.\nWe augment a new protocol for uninterrupted progress in the interim period."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ASAP.2014.6868657", 
    "link": "http://arxiv.org/pdf/1404.7671v1", 
    "title": "Determining Majority in Networks with Local Interactions and very Small   Local Memory", 
    "arxiv-id": "1404.7671v1", 
    "author": "Paul G. Spirakis", 
    "publish": "2014-04-30T10:29:46Z", 
    "summary": "We study here the problem of determining the majority type in an arbitrary\nconnected network, each vertex of which has initially two possible types. The\nvertices may have a few additional possible states and can interact in pairs\nonly if they share an edge. Any (population) protocol is required to stabilize\nin the initial majority. We first present and analyze a protocol with 4 states\nper vertex that always computes the initial majority value, under any fair\nscheduler. As we prove, this protocol is optimal, in the sense that there is no\npopulation protocol that always computes majority with fewer than 4 states per\nvertex. However this does not rule out the existence of a protocol with 3\nstates per vertex that is correct with high probability. To this end, we\nexamine a very natural majority protocol with 3 states per vertex, introduced\nin [Angluin et al. 2008] where its performance has been analyzed for the clique\ngraph. We study the performance of this protocol in arbitrary networks. We\nprove that, when the two initial states are put uniformly at random on the\nvertices, this protocol converges to the initial majority with probability\nhigher than the probability of converging to the initial minority. In contrast,\nwe present an infinite family of graphs, on which the protocol can fail whp,\neven when the difference between the initial majority and the initial minority\nis $n - \\Theta(\\ln{n})$. We also present another infinite family of graphs in\nwhich the protocol of Angluin et al. takes an expected exponential time to\nconverge. These two negative results build upon a very positive result\nconcerning the robustness of the protocol on the clique. Surprisingly, the\nresistance of the clique to failure causes the failure in general graphs. Our\ntechniques use new domination and coupling arguments for suitably defined\nprocesses whose dynamics capture the antagonism between the states involved."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1404.7688v1", 
    "title": "On User Availability Prediction and Network Applications", 
    "arxiv-id": "1404.7688v1", 
    "author": "Yves Roudier", 
    "publish": "2014-04-30T11:24:26Z", 
    "summary": "User connectivity patterns in network applications are known to be\nheterogeneous, and to follow periodic (daily and weekly) patterns. In many\ncases, the regularity and the correlation of those patterns is problematic: for\nnetwork applications, many connected users create peaks of demand; in contrast,\nin peer-to-peer scenarios, having few users online results in a scarcity of\navailable resources. On the other hand, since connectivity patterns exhibit a\nperiodic behavior, they are to some extent predictable. This work shows how\nthis can be exploited to anticipate future user connectivity and to have\napplications proactively responding to it. We evaluate the probability that any\ngiven user will be online at any given time, and assess the prediction on\nsix-month availability traces from three different Internet applications.\nBuilding upon this, we show how our probabilistic approach makes it easy to\nevaluate and optimize the performance in a number of diverse network\napplication models, and to use them to optimize systems. In particular, we show\nhow this approach can be used in distributed hash tables, friend-to-friend\nstorage, and cache pre-loading for social networks, resulting in substantial\ngains in data availability and system efficiency at negligible costs."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.0609v3", 
    "title": "Optimization for Speculative Execution of Multiple Jobs in a   MapReduce-like Cluster", 
    "arxiv-id": "1406.0609v3", 
    "author": "Wing Cheong Lau", 
    "publish": "2014-06-03T07:44:33Z", 
    "summary": "Nowadays, a computing cluster in a typical data center can easily consist of\nhundreds of thousands of commodity servers, making component/ machine failures\nthe norm rather than exception. A parallel processing job can be delayed\nsubstantially as long as one of its many tasks is being assigned to a failing\nmachine. To tackle this so-called straggler problem, most parallel processing\nframeworks such as MapReduce have adopted various strategies under which the\nsystem may speculatively launch additional copies of the same task if its\nprogress is abnormally slow or simply because extra idling resource is\navailable. In this paper, we focus on the design of speculative execution\nschemes for a parallel processing cluster under different loading conditions.\nFor the lightly loaded case, we analyze and propose two optimization-based\nschemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing\nthe job utility and the Straggler Detection Algorithm (SDA) which minimizes the\noverall resource consumption of a job. We also derive the workload threshold\nunder which SCA or SDA should be used for speculative execution. Our simulation\nresults show both SCA and SDA can reduce the job flowtime by nearly 60%\ncomparing to the speculative execution strategy of Microsoft Mantri. For the\nheavily loaded case, we propose the Enhanced Speculative Execution (ESE)\nalgorithm which is an extension of the Microsoft Mantri scheme. We show that\nthe ESE algorithm can beat the Mantri baseline scheme by 18% in terms of job\nflowtime while consuming the same amount of resource."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.1215v3", 
    "title": "Parallel Algorithms for Generating Random Networks with Given Degree   Sequences", 
    "arxiv-id": "1406.1215v3", 
    "author": "Maleq Khan", 
    "publish": "2014-06-04T21:29:38Z", 
    "summary": "Random networks are widely used for modeling and analyzing complex processes.\nMany mathematical models have been proposed to capture diverse real-world\nnetworks. One of the most important aspects of these models is degree\ndistribution. Chung--Lu (CL) model is a random network model, which can produce\nnetworks with any given arbitrary degree distribution. The complex systems we\ndeal with nowadays are growing larger and more diverse than ever. Generating\nrandom networks with any given degree distribution consisting of billions of\nnodes and edges or more has become a necessity, which requires efficient and\nparallel algorithms. We present an MPI-based distributed memory parallel\nalgorithm for generating massive random networks using CL model, which takes\n$O(\\frac{m+n}{P}+P)$ time with high probability and $O(n)$ space per processor,\nwhere $n$, $m$, and $P$ are the number of nodes, edges and processors,\nrespectively. The time efficiency is achieved by using a novel load-balancing\nalgorithm. Our algorithms scale very well to a large number of processors and\ncan generate massive power--law networks with one billion nodes and $250$\nbillion edges in one minute using $1024$ processors."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.2628v2", 
    "title": "Merge Path - A Visually Intuitive Approach to Parallel Merging", 
    "arxiv-id": "1406.2628v2", 
    "author": "Yitzhak Birk", 
    "publish": "2014-06-10T17:04:23Z", 
    "summary": "Merging two sorted arrays is a prominent building block for sorting and other\nfunctions. Its efficient parallelization requires balancing the load among\ncompute cores, minimizing the extra work brought about by parallelization, and\nminimizing inter-thread synchronization requirements. Efficient use of memory\nis also important.\n  We present a novel, visually intuitive approach to partitioning two input\nsorted arrays into pairs of contiguous sequences of elements, one from each\narray, such that 1) each pair comprises any desired total number of elements,\nand 2) the elements of each pair form a contiguous sequence in the output\nmerged sorted array. While the resulting partition and the computational\ncomplexity are similar to those of certain previous algorithms, our approach is\ndifferent, extremely intuitive, and offers interesting insights. Based on this,\nwe present a synchronization-free, cache-efficient merging (and sorting)\nalgorithm.\n  While we use a shared memory architecture as the basis, our algorithm is\neasily adaptable to additional architectures. In fact, our approach is even\nrelevant to cache-efficient sequential sorting. The algorithms are presented,\nalong with important cache-related insights."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.2844v1", 
    "title": "Partitionnement D\u00e9terministe pour R\u00e9soudre les Probl\u00e8mes de   Programmation Par Contraintes en utilisant le Framework Parall\u00e8le Bobpp", 
    "arxiv-id": "1406.2844v1", 
    "author": "Bertrand Le Cun", 
    "publish": "2014-06-11T09:50:52Z", 
    "summary": "This paper presents a deterministic parallelization to explore a Constraint\nProgramming search space. This work is an answer to an industrial project named\nPAJERO, which is in need of a parallel constraint solver which always responds\nwith the same solution whether using sequential or parallel machines. It is\nwell known that parallel tree search changes the order in which the exploration\nof solution space is done. In the context where the first solution found is\nreturned, using a different number of cores may change the returned solution.\nIn the literature, several non deterministic strategies have been proposed to\nparallelize the exploration of Constraint Programming search space. Most of\nthem are based on the Work Stealing technique used to partition the Constraint\nProgramming search space on demand and during the execution of the search\nalgorithm. Our study focuses on the determinism of the parallel search versus\nthe sequential one. We consider that the sequential search algorithm is\ndeterministic, then propose an elegant solution introducing a total order on\nthe nodes in which the parallel algorithm always gives the same solution as the\nsequential one regardless of the number of cores used. To evaluate this\ndeterministic strategy, we ran tests using the Google OR-Tools Constraint\nProgramming solver on top of our parallel Bobpp framework. The performances are\nillustrated by solving Constraint Programming problems modeled in FlatZinc\nformat."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.2852v2", 
    "title": "On the Impact of Geometry on Ad Hoc Communication in Wireless Networks", 
    "arxiv-id": "1406.2852v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2014-06-11T10:27:14Z", 
    "summary": "In this work we address the question how important is the knowledge of\ngeometric location and network density to the efficiency of (distributed)\nwireless communication in ad hoc networks. We study fundamental communication\ntask of broadcast and develop well-scalable, randomized algorithms that do not\nrely on GPS information, and which efficiency formulas do not depend on how\ndense the geometric network is. We consider two settings: with and without\nspontaneous wake-up of nodes. In the former setting, in which all nodes start\nthe protocol at the same time, our algorithm accomplishes broadcast in $O(D\\log\nn + \\log^2 n)$ rounds under the SINR model, with high probability (whp), where\n$D$ is the diameter of the communication graph and $n$ is the number of\nstations. In the latter setting, in which only the source node containing the\noriginal message is active in the beginning, we develop a slightly slower\nalgorithm working in $O(D\\log^2 n)$ rounds whp. Both algorithms are based on a\nnovel distributed coloring method, which is of independent interest and\npotential applicability to other communication tasks under the SINR wireless\nmodel."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.3699v1", 
    "title": "Distributed Versioned Object Storage -- Alternatives at the OSD layer   (Poster Extended Abstract)", 
    "arxiv-id": "1406.3699v1", 
    "author": "Jay Lofstead", 
    "publish": "2014-06-14T08:02:02Z", 
    "summary": "The ability to store multiple versions of a data item is a powerful primitive\nthat has had a wide variety of uses: relational databases, transactional\nmemory, version control systems, to name a few. However, each implementation\nuses a very particular form of versioning that is customized to the domain in\nquestion and hidden away from the user. In our going project, we are reviewing\nand analyzing multiple uses of versioning in distinct domains, with the goal of\nidentifying the basic components required to provide a generic distributed\nmultiversioning object storage service, and define how these can be customized\nin order to serve distinct needs. With this primitive, new services can\nleverage multiversioning to ease development and provide specific consistency\nguarantees that address particular use cases. This work presents early results\nthat quantify the trade-offs in implementing versioning at the local storage\nlayer."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.3901v1", 
    "title": "OS4M: Achieving Global Load Balance of MapReduce Workload by Scheduling   at the Operation Level", 
    "arxiv-id": "1406.3901v1", 
    "author": "Zhiyong Liu", 
    "publish": "2014-06-16T04:16:41Z", 
    "summary": "The efficiency of MapReduce is closely related to its load balance. Existing\nworks on MapReduce load balance focus on coarse-grained scheduling. This study\nconcerns fine-grained scheduling on MapReduce operations, with each operation\nrepresenting one invocation of the Map or Reduce function. By default,\nMapReduce adopts the hash-based method to schedule Reduce operations, which\noften leads to poor load balance. In addition, the copy phase of Reduce tasks\noverlaps with Map tasks, which significantly hinders the progress of Map tasks\ndue to I/O contention. Moreover, the three phases of Reduce tasks run in\nsequence, while consuming different resources, thereby under-utilizing\nresources. To overcome these problems, we introduce a set of mechanisms named\nOS4M (Operation Scheduling for MapReduce) to improve MapReduce's performance.\nOS4M achieves load balance by collecting statistics of all Map operations, and\ncalculates a globally optimal schedule to distribute Reduce operations. With\nOS4M, the copy phase of Reduce tasks no longer overlaps with Map tasks, and the\nthree phases of Reduce tasks are pipelined based on their operation loads. OS4M\nhas been transparently incorporated into MapReduce. Evaluations on standard\nbenchmarks show that OS4M's job duration can be shortened by up to 42%,\ncompared with a baseline of Hadoop."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.4840v1", 
    "title": "Fast Trace Generation of Many-Core Embedded Systems with Native   Simulation", 
    "arxiv-id": "1406.4840v1", 
    "author": "Pablo S\u00e1nchez Espeso", 
    "publish": "2014-06-18T18:59:45Z", 
    "summary": "Embedded Software development and optimization are complex tasks. Late\navailably of hardware platforms, their usual low visibility and\ncontrollability, and their limiting resource constraints makes early\nperformance estimation an attractive option instead of using the final\nexecution platform. With early performance estimation, software development can\nprogress although the real hardware is not yet available or it is too complex\nto interact with. In this paper, we present how the native simulation framework\nSCoPE is extended to generate OTF trace files. Those trace files can be later\nvisualized with trace visualization tools, which recently were only used to\noptimize HPC workloads in order to iterate in the development process."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.4974v1", 
    "title": "Academic Cloud Computing Research: Five Pitfalls and Five Opportunities", 
    "arxiv-id": "1406.4974v1", 
    "author": "Ian Sommerville", 
    "publish": "2014-06-19T08:08:14Z", 
    "summary": "This discussion paper argues that there are five fundamental pitfalls, which\ncan restrict academics from conducting cloud computing research at the\ninfrastructure level, which is currently where the vast majority of academic\nresearch lies. Instead academics should be conducting higher risk research, in\norder to gain understanding and open up entirely new areas.\n  We call for a renewed mindset and argue that academic research should focus\nless upon physical infrastructure and embrace the abstractions provided by\nclouds through five opportunities: user driven research, new programming\nmodels, PaaS environments, and improved tools to support elasticity and\nlarge-scale debugging. The objective of this paper is to foster discussion, and\nto define a roadmap forward, which will allow academia to make longer-term\nimpacts to the cloud computing community."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.5759v1", 
    "title": "An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud   Computing Platform", 
    "arxiv-id": "1406.5759v1", 
    "author": "Syed Akther Hossain", 
    "publish": "2014-06-22T20:40:07Z", 
    "summary": "Cloud Computing is becoming a viable computing solution for services oriented\ncomputing. Several open-source cloud solutions are available to these supports.\nOpen-source software stacks offer a huge amount of customizability without huge\nlicensing fees. As a result, open source software are widely used for designing\ncloud, and private clouds are being built increasingly in the open source way.\nNumerous contributions have been made by the open-source community related to\nprivate-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private\ncloud management software. However, little has been done to systematically\ninvestigate the performance evaluation of this open-source cloud solution in\nthe existing literature. The performance evaluation aids new and existing\nresearch, industry and international projects when selecting OpenNebula\nsoftware to their work. The objective of this paper is to evaluate the\nload-balancing performance of the OpenNebula cloud management software. For the\nperformance evaluation, the OpenNebula cloud management software is installed\nand configured as a prototype implementation and tested on the DIU Cloud Lab.\nIn this paper, two set of experiments are conducted to identify the load\nbalancing performance of the OpenNebula cloud management platform- (1) Delete\nand Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping\nPhysical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform."
},{
    "category": "cs.DC", 
    "doi": "10.1109/TNET.2014.2321430", 
    "link": "http://arxiv.org/pdf/1406.5760v1", 
    "title": "Virtual Memory Streaming Technique for Virtual Machines (VMs) for Rapid   Scaling and High Performance in Cloud Environment", 
    "arxiv-id": "1406.5760v1", 
    "author": "Syed Akther Hossain", 
    "publish": "2014-06-22T20:44:10Z", 
    "summary": "This paper addresses the impact of Virtual Memory Streaming (VMS) technique\nin provisioning virtual machines (VMs) in cloud environment. VMS is a scaling\nvirtualization technology that allows different virtual machines rapid scale,\nhigh performance, and increase hardware utilization. Traditional hypervisors do\nnot support true no-downtime live migration, and its lack of memory\noversubscription can hurt the economics of a private cloud deployment by\nlimiting the number of VMs on each host. VMS brings together several advanced\nhypervisor memory management techniques including granular page sharing,\ndynamic memory footprint management, live migration, read caching, and a unique\nvirtual machine cloning capability. An architecture model is described,\ntogether with a proof-of-concept implementation, that VMS dynamically scaling\nof virtualized infrastructure with true live migration and cloning of VMs. This\npaper argues that VMS for Cloud allows requiring significantly reduced server\nmemory and reducing the time for virtualized resource scaling by instantly\nadding more virtual machines."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2015.66", 
    "link": "http://arxiv.org/pdf/1406.5975v1", 
    "title": "Scalable Analytics over Distributed Time-series Graphs using GoFFish", 
    "arxiv-id": "1406.5975v1", 
    "author": "Viktor Prasanna", 
    "publish": "2014-06-23T16:48:03Z", 
    "summary": "Graphs are a key form of Big Data, and performing scalable analytics over\nthem is invaluable to many domains. As our ability to collect data grows, there\nis an emerging class of inter-connected data which accumulates or varies over\ntime, and on which novel analytics - both over the network structure and across\nthe time-variant attribute values - is necessary. We introduce the notion of\ntime-series graph analytics and propose Gopher, a scalable programming\nabstraction to develop algorithms and analytics on such datasets. Our\nabstraction leverages a sub-graph centric programming model and extends it to\nthe temporal dimension using an iterative BSP (Bulk Synchronous Parallel)\napproach. Gopher is co-designed with GoFS, a distributed storage specialized\nfor time-series graphs, as part of the GoFFish distributed analytics platform.\nWe examine storage optimizations for GoFS, design patterns in Gopher to\nleverage the distributed data layout, and evaluate the GoFFish platform using\ntime-series graph data and applications on a commodity cluster."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2015.66", 
    "link": "http://arxiv.org/pdf/1406.5977v1", 
    "title": "Floe: A Continuous Dataflow Framework for Dynamic Cloud Applications", 
    "arxiv-id": "1406.5977v1", 
    "author": "Alok Kumbhare", 
    "publish": "2014-06-23T16:56:48Z", 
    "summary": "Applications in cyber-physical systems are increasingly coupled with online\ninstruments to perform long running, continuous data processing. Such \"always\non\" dataflow applications are dynamic, where they need to change the\napplications logic and performance at runtime, in response to external\noperational needs. Floe is a continuous dataflow framework that is designed to\nbe adaptive for dynamic applications on Cloud infrastructure. It offers\nadvanced dataflow patterns like BSP and MapReduce for flexible and holistic\ncomposition of streams and files, and supports dynamic recomposition at runtime\nwith minimal impact on the execution. Adaptive resource allocation strategies\nallow our framework to effectively use elastic Cloud resources to meet varying\ndata rates. We illustrate the design patterns of Floe by running an integration\npipeline and a tweet clustering application from the Smart Power Grids domain\non a private Eucalyptus Cloud. The responsiveness of our resource adaptation is\nvalidated through simulations for periodic, bursty and random workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2015.66", 
    "link": "http://arxiv.org/pdf/1406.7423v1", 
    "title": "An Efficient Read Dominant Data Replication Protocol under Serial   Isolation using Quorum Consensus Approach", 
    "arxiv-id": "1406.7423v1", 
    "author": "Ajay Agarwal", 
    "publish": "2014-06-28T17:40:19Z", 
    "summary": "In distributed systems, data replication provides better availability, higher\nread capacity, improved access efficiency and lower bandwidth requirements in\nthe system. In this paper, we propose a significantly efficient approach of the\ndata replication for serial isolation by using newly proposed Circular quorum\nsystems. This paper has three major contributions. First, we have proposed the\nCircular quorum systems that generalize the various existing quorum systems,\nsuch as Read-one-write-all (ROWA) quorum systems, Majority quorum systems, Grid\nquorum systems, Diamond quorum systems, D-Space quorum systems,\nMulti-dimensional-grid quorum systems and Generalized-grid quorum systems.\nSecond, Circular quorum systems not only generalizes but also improves the\nperformance over existing quorum systems of their category. Third, we proposed\na highly available Circular quorum consensus protocol for data replication\nunder serial isolation level that uses a suitable Circular quorum system for\nread dominant scenario."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1406.7524v1", 
    "title": "Towards a Generic Application Partitioning and Retraction Framework for   Pervasive Environments", 
    "arxiv-id": "1406.7524v1", 
    "author": "Nimal Nissanke", 
    "publish": "2014-06-29T17:08:17Z", 
    "summary": "Current mobile context-aware applications for pervasive environments have\nbeen designed to consume information from computational nodes or devices in\ntheir surroundings or environments. As the hardware industry continues making\nmuch smaller, compact and cheap hardware, the vision of having plenty of very\nsmall powerful digital networking nodes in, for e.g., the living room or\nbedroom, is not so far. Designing software that can make optimal use of all\nthese computational nodes when needed is still challenging; since software will\nnot only consume information from these nodes but parts of the software can be\nhosted on these different nodes. In this paper we propose the BubbleCodes\nFramework which is a generic application partitioning and retraction framework\nfor next generation context-aware applications that will have the capabilities\nto partition and retract themselves on multiple computational nodes in a\npervasive environment."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1406.7540v1", 
    "title": "Building global and scalable systems with Atomic Multicast", 
    "arxiv-id": "1406.7540v1", 
    "author": "Beno\u00eet Garbinato", 
    "publish": "2014-06-29T19:27:44Z", 
    "summary": "The rise of worldwide Internet-scale services demands large distributed\nsystems. Indeed, when handling several millions of users, it is common to\noperate thousands of servers spread across the globe. Here, replication plays a\ncentral role, as it contributes to improve the user experience by hiding\nfailures and by providing acceptable latency. In this paper, we claim that\natomic multicast, with strong and well-defined properties, is the appropriate\nabstraction to efficiently design and implement globally scalable distributed\nsystems. We substantiate our claim with the design of two modern online\nservices atop atomic multicast, a strongly consistent key-value store and a\ndistributed log. In addition to presenting the design of these services, we\nexperimentally assess their performance in a geographically distributed\ndeployment."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0384v2", 
    "title": "Fast and Compact Distributed Verification and Self-Stabilization of a   DFS Tree", 
    "arxiv-id": "1408.0384v2", 
    "author": "Chhaya Trehan", 
    "publish": "2014-08-02T15:19:46Z", 
    "summary": "We present algorithms for distributed verification and silent-stabilization\nof a DFS(Depth First Search) spanning tree of a connected network. Computing\nand maintaining such a DFS tree is an important task, e.g., for constructing\nefficient routing schemes. Our algorithm improves upon previous work in various\nways. Comparable previous work has space and time complexities of $O(n\\log\n\\Delta)$ bits per node and $O(nD)$ respectively, where $\\Delta$ is the highest\ndegree of a node, $n$ is the number of nodes and $D$ is the diameter of the\nnetwork. In contrast, our algorithm has a space complexity of $O(\\log n)$ bits\nper node, which is optimal for silent-stabilizing spanning trees and runs in\n$O(n)$ time. In addition, our solution is modular since it utilizes the\ndistributed verification algorithm as an independent subtask of the overall\nsolution. It is possible to use the verification algorithm as a stand alone\ntask or as a subtask in another algorithm. To demonstrate the simplicity of\nconstructing efficient DFS algorithms using the modular approach, We also\npresent a (non-sielnt) self-stabilizing DFS token circulation algorithm for\ngeneral networks based on our silent-stabilizing DFS tree. The complexities of\nthis token circulation algorithm are comparable to the known ones."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0500v3", 
    "title": "FlashGraph: Processing Billion-Node Graphs on an Array of Commodity SSDs", 
    "arxiv-id": "1408.0500v3", 
    "author": "Alexander S. Szalay", 
    "publish": "2014-08-03T13:44:09Z", 
    "summary": "Graph analysis performs many random reads and writes, thus, these workloads\nare typically performed in memory. Traditionally, analyzing large graphs\nrequires a cluster of machines so the aggregate memory exceeds the graph size.\nWe demonstrate that a multicore server can process graphs with billions of\nvertices and hundreds of billions of edges, utilizing commodity SSDs with\nminimal performance loss. We do so by implementing a graph-processing engine on\ntop of a user-space SSD file system designed for high IOPS and extreme\nparallelism. Our semi-external memory graph engine called FlashGraph stores\nvertex state in memory and edge lists on SSDs. It hides latency by overlapping\ncomputation with I/O. To save I/O bandwidth, FlashGraph only accesses edge\nlists requested by applications from SSDs; to increase I/O throughput and\nreduce CPU overhead for I/O, it conservatively merges I/O requests. These\ndesigns maximize performance for applications with different I/O\ncharacteristics. FlashGraph exposes a general and flexible vertex-centric\nprogramming interface that can express a wide variety of graph algorithms and\ntheir optimizations. We demonstrate that FlashGraph in semi-external memory\nperforms many algorithms with performance up to 80% of its in-memory\nimplementation and significantly outperforms PowerGraph, a popular distributed\nin-memory graph engine."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0510v1", 
    "title": "A note on \"The Need for End-to-End Evaluation of Cloud Availability\"", 
    "arxiv-id": "1408.0510v1", 
    "author": "Maurizio Naldi", 
    "publish": "2014-08-03T16:55:44Z", 
    "summary": "Cloud availability is a major performance parameter for cloud platforms, but\nthere are very few measurements on commercial platforms, and most of them rely\non outage reports as appeared on specialized sites, providers' dashboards, or\nthe general press. A paper recently presented at the PAM 2014 conference by Hu\net alii reports the results of a measurement campaign. In this note, the\nresults of that paper are summarized, highlighting sources of inaccuracy and\nsome possible improvements. In particular, the use of a low probing frequency\ncould lead to non detection of short outages, as well as to an inaccurate\nestimation of the outage duration statistics. Overcoming this lack of accuracy\nis relevant to properly assess SLA violations and establish the basis for\ninsurance claims."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0574v1", 
    "title": "Agreement in Partitioned Dynamic Networks", 
    "arxiv-id": "1408.0574v1", 
    "author": "Aikaterini Sotiraki", 
    "publish": "2014-08-04T03:25:20Z", 
    "summary": "In the dynamic network model, the communication graph is assumed to be\nconnected in every round but is otherwise arbitrary. We consider the related\nsetting of $p$-partitioned dynamic networks, in which the communication graph\nin each round consists of at most $p$ connected components. We explore the\nproblem of $k$-agreement in this model for $k\\geq p$. We show that if the\nnumber of processes is unknown then it is impossible to achieve $k$-agreement\nfor any $k$ and any $p\\geq 2$. Given an upper bound $n$ on the number of\nprocesses, we provide algorithms achieving $k$-agreement in $p(n-p)$ rounds for\n$k=p$ and in $O(n/\\epsilon)$ rounds for $k=\\lceil (1+\\epsilon)p \\rceil$."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0620v2", 
    "title": "Approximate Consensus in Highly Dynamic Networks: The Role of Averaging   Algorithms", 
    "arxiv-id": "1408.0620v2", 
    "author": "Thomas Nowak", 
    "publish": "2014-08-04T09:27:17Z", 
    "summary": "In this paper, we investigate the approximate consensus problem in highly\ndynamic networks in which topology may change continually and unpredictably. We\nprove that in both synchronous and partially synchronous systems, approximate\nconsensus is solvable if and only if the communication graph in each round has\na rooted spanning tree, i.e., there is a coordinator at each time. The striking\npoint in this result is that the coordinator is not required to be unique and\ncan change arbitrarily from round to round. Interestingly, the class of\naveraging algorithms, which are memoryless and require no process identifiers,\nentirely captures the solvability issue of approximate consensus in that the\nproblem is solvable if and only if it can be solved using any averaging\nalgorithm. Concerning the time complexity of averaging algorithms, we show that\napproximate consensus can be achieved with precision of $\\varepsilon$ in a\ncoordinated network model in $O(n^{n+1} \\log\\frac{1}{\\varepsilon})$ synchronous\nrounds, and in $O(\\Delta n^{n\\Delta+1} \\log\\frac{1}{\\varepsilon})$ rounds when\nthe maximum round delay for a message to be delivered is $\\Delta$. While in\ngeneral, an upper bound on the time complexity of averaging algorithms has to\nbe exponential, we investigate various network models in which this exponential\nbound in the number of nodes reduces to a polynomial bound. We apply our\nresults to networked systems with a fixed topology and classical benign fault\nmodels, and deduce both known and new results for approximate consensus in\nthese systems. In particular, we show that for solving approximate consensus, a\ncomplete network can tolerate up to 2n-3 arbitrarily located link faults at\nevery round, in contrast with the impossibility result established by Santoro\nand Widmayer (STACS '89) showing that exact consensus is not solvable with n-1\nlink faults per round originating from the same node."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.0812v1", 
    "title": "Lower Bounds for Structuring Unreliable Radio Networks", 
    "arxiv-id": "1408.0812v1", 
    "author": "Calvin Newport", 
    "publish": "2014-08-04T20:39:55Z", 
    "summary": "In this paper, we study lower bounds for randomized solutions to the maximal\nindependent set (MIS) and connected dominating set (CDS) problems in the dual\ngraph model of radio networks---a generalization of the standard graph-based\nmodel that now includes unreliable links controlled by an adversary. We begin\nby proving that a natural geographic constraint on the network topology is\nrequired to solve these problems efficiently (i.e., in time polylogarthmic in\nthe network size). We then prove the importance of the assumption that nodes\nare provided advance knowledge of their reliable neighbors (i.e, neighbors\nconnected by reliable links). Combined, these results answer an open question\nby proving that the efficient MIS and CDS algorithms from [Censor-Hillel, PODC\n2011] are optimal with respect to their dual graph model assumptions. They also\nprovide insight into what properties of an unreliable network enable efficient\nlocal computation."
},{
    "category": "cs.DC", 
    "doi": "10.1115/1.859711", 
    "link": "http://arxiv.org/pdf/1408.1021v1", 
    "title": "The Adaptive Priority Queue with Elimination and Combining", 
    "arxiv-id": "1408.1021v1", 
    "author": "Maurice Herlihy", 
    "publish": "2014-08-05T16:16:56Z", 
    "summary": "Priority queues are fundamental abstract data structures, often used to\nmanage limited resources in parallel programming. Several proposed parallel\npriority queue implementations are based on skiplists, harnessing the potential\nfor parallelism of the add() operations. In addition, methods such as Flat\nCombining have been proposed to reduce contention by batching together multiple\noperations to be executed by a single thread. While this technique can decrease\nlock-switching overhead and the number of pointer changes required by the\nremoveMin() operations in the priority queue, it can also create a sequential\nbottleneck and limit parallelism, especially for non-conflicting add()\noperations.\n  In this paper, we describe a novel priority queue design, harnessing the\nscalability of parallel insertions in conjunction with the efficiency of\nbatched removals. Moreover, we present a new elimination algorithm suitable for\na priority queue, which further increases concurrency on balanced workloads\nwith similar numbers of add() and removeMin() operations. We implement and\nevaluate our design using a variety of techniques including locking, atomic\noperations, hardware transactional memory, as well as employing adaptive\nheuristics given the workload."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.1127v1", 
    "title": "SADDLE: A Modular Design Automation Framework for Cluster Supercomputers   and Data Centres", 
    "arxiv-id": "1408.1127v1", 
    "author": "Konstantin S. Solnushkin", 
    "publish": "2014-08-05T21:59:26Z", 
    "summary": "In this paper we present SADDLE, a modular framework for automated design of\ncluster supercomputers and data centres. In contrast with commonly used\napproaches that operate on logic gate level (Verilog, VHDL) or board level\n(such as EDA tools), SADDLE works at a much higher level of abstraction: its\nbuilding blocks are ready-made servers, network switches, power supply systems\nand so on. Modular approach provides the potential to include low-level tools\nas elements of SADDLE's design workflow, moving towards the goal of electronic\nsystem level (ESL) design automation. Designs produced by SADDLE include\nproject documentation items such as bills of materials and wiring diagrams,\nproviding a formal specification of a computer system and streamlining assembly\noperations."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.1605v2", 
    "title": "Parallel Distributed Breadth First Search on the Kepler Architecture", 
    "arxiv-id": "1408.1605v2", 
    "author": "Enrico Mastrostefano", 
    "publish": "2014-08-07T14:34:15Z", 
    "summary": "We present the results obtained by using an evolution of our CUDA-based\nsolution for the exploration, via a Breadth First Search, of large graphs. This\nlatest version exploits at its best the features of the Kepler architecture and\nrelies on a combination of techniques to reduce both the number of\ncommunications among the GPUs and the amount of exchanged data. The final\nresult is a code that can visit more than 800 billion edges in a second by\nusing a cluster equipped with 4096 Tesla K20X GPUs."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.1935v1", 
    "title": "Non-Blocking Doubly-Linked Lists with Good Amortized Complexity", 
    "arxiv-id": "1408.1935v1", 
    "author": "Niloufar Shafiei", 
    "publish": "2014-08-08T18:42:50Z", 
    "summary": "We present a new non-blocking doubly-linked list implementation for an\nasynchronous shared-memory system. It is the first such implementation for\nwhich an upper bound on amortized time complexity has been proved. In our\nimplementation, operations access the list via cursors. Each cursor is\nassociated with an item in the list and is local to a process. The\nimplementation supports two update operations, insertBefore and delete, and two\nmove operations, moveRight and moveLeft. An insertBefore(c, x) operation\ninserts an item x into the list immediately before the cursor c's location. A\ndelete(c) operation removes the item at the cursor c's location and sets the\ncursor to the next item in the list. The move operations move the cursor one\nposition to the right or left. The update operations use single-word\nCompare&Swap instructions. The move operations only read shared memory and\nnever change the state of the data structure. If all update operations modify\ndifferent parts of the list, they run completely concurrently. Let cp(op) be\nthe maximum number of active cursors at any one time during the operation op.\nThe amortized complexity of each update operation op is O(cp(op)) and each move\noperation is O(1). We have written a detailed correctness proof and amortized\nanalysis of our implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.2071v1", 
    "title": "Near-Constant-Time Distributed Algorithms on a Congested Clique", 
    "arxiv-id": "1408.2071v1", 
    "author": "Vivek B. Sardeshmukh", 
    "publish": "2014-08-09T07:41:12Z", 
    "summary": "This paper presents constant-time and near-constant-time distributed\nalgorithms for a variety of problems in the congested clique model. We show how\nto compute a 3-ruling set in expected $O(\\log \\log \\log n)$ rounds and using\nthis, we obtain a constant-approximation to metric facility location, also in\nexpected $O(\\log \\log \\log n)$ rounds. In addition, assuming an input metric\nspace of constant doubling dimension, we obtain constant-round algorithms to\ncompute constant-factor approximations to the minimum spanning tree and the\nmetric facility location problems. These results significantly improve on the\nrunning time of the fastest known algorithms for these problems in the\ncongested clique setting."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.2116v1", 
    "title": "Self-stabilizing algorithms for Connected Vertex Cover and Clique   decomposition problems", 
    "arxiv-id": "1408.2116v1", 
    "author": "Stephane Rovedakis", 
    "publish": "2014-08-09T14:43:04Z", 
    "summary": "In many wireless networks, there is no fixed physical backbone nor\ncentralized network management. The nodes of such a network have to\nself-organize in order to maintain a virtual backbone used to route messages.\nMoreover, any node of the network can be a priori at the origin of a malicious\nattack. Thus, in one hand the backbone must be fault-tolerant and in other hand\nit can be useful to monitor all network communications to identify an attack as\nsoon as possible. We are interested in the minimum \\emph{Connected Vertex\nCover} problem, a generalization of the classical minimum Vertex Cover problem,\nwhich allows to obtain a connected backbone. Recently, Delbot et\nal.~\\cite{DelbotLP13} proposed a new centralized algorithm with a constant\napproximation ratio of $2$ for this problem. In this paper, we propose a\ndistributed and self-stabilizing version of their algorithm with the same\napproximation guarantee. To the best knowledge of the authors, it is the first\ndistributed and fault-tolerant algorithm for this problem. The approach\nfollowed to solve the considered problem is based on the construction of a\nconnected minimal clique partition. Therefore, we also design the first\ndistributed self-stabilizing algorithm for this problem, which is of\nindependent interest."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.2284v1", 
    "title": "Hadoop in Low-Power Processors", 
    "arxiv-id": "1408.2284v1", 
    "author": "Andreas Terzis", 
    "publish": "2014-08-10T23:55:23Z", 
    "summary": "In our previous work we introduced a so-called Amdahl blade microserver that\ncombines a low-power Atom processor, with a GPU and an SSD to provide a\nbalanced and energy-efficient system. Our preliminary results suggested that\nthe sequential I/O of Amdahl blades can be ten times higher than that a cluster\nof conventional servers with comparable power consumption. In this paper we\ninvestigate the performance and energy efficiency of Amdahl blades running\nHadoop. Our results show that Amdahl blades are 7.7 times and 3.4 times as\nenergy-efficient as the Open Cloud Consortium cluster for a data-intensive and\na compute-intensive application, respectively. The Hadoop Distributed\nFilesystem has relatively poor performance on Amdahl blades because both disk\nand network I/O are CPU-heavy operations on Atom processors. We demonstrate\nthree effective techniques to reduce CPU consumption and improve performance.\nHowever, even with these improvements, the Atom processor is still the system's\nbottleneck. We revisit Amdahl's law, and estimate that Amdahl blades need four\nAtom cores to be well balanced for Hadoop tasks."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.2657v1", 
    "title": "First Experiences With Validating and Using the Cray Power Management   Database Tool", 
    "arxiv-id": "1408.2657v1", 
    "author": "Thomas C. Schulthess", 
    "publish": "2014-08-12T08:33:03Z", 
    "summary": "In October 2013 CSCS installed the first hybrid Cray XC-30 system, dubbed Piz\nDaint. This system features the power management database (PMDB), that was\nrecently introduced by Cray to collect detailed power consumption information\nin a non-intrusive manner. Power measurements are taken on each node, with\nadditional measurements for the Aries network and blowers, and recorded in a\ndatabase. This enables fine-grained reporting of power consumption that is not\npossible with external power meters, and is useful to both application\ndevelopers and facility operators. This paper will show how benchmarks of\nrepresentative applications at CSCS were used to validate the PMDB on Piz\nDaint. Furthermore we will elaborate, with the well-known HPL benchmark serving\nas prototypical application, on how the PMDB streamlines the tuning for optimal\npower efficiency in production, which lead to Piz Daint being recognised as the\nmost energy efficient petascale supercomputer presently in operation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.3432v1", 
    "title": "Snapshot for Time: The One-Shot Case", 
    "arxiv-id": "1408.3432v1", 
    "author": "Eli Gafni", 
    "publish": "2014-08-14T21:14:08Z", 
    "summary": "We show that for one-shot problems - problems where a processor executes a\nsingle operation-execution - timing constraints can be captured by conditions\non the relation between original outputs and supplementary snapshots. In\naddition to the dictionary definition of the word snapshot, in distributed\ncomputing snapshots also stand for a task that imposes relation among sets\nwhich are output of processors. Hence, constrains relating the timing between\noperation-executions of processors can be captured by the sets relation\nrepresenting a task.\n  This allows to bring to bear techniques developed for tasks, to one-shot\nobjects. In particular, for the one-shot case the question of linearizability\nis moot. Nevertheless, current proof techniques of object implementation\nrequire the prover to provide linearization-points even in the one shot case.\nTransforming the object into a task relieves the prover of an implementation\nfrom the burden of finding the \"linearization-points,\" since if the task is\nsolvable, linearization points are guaranteed to exist. We exhibit this\nadvantage with a new algorithm to implement MWMR register in a SWMR system."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.4939v1", 
    "title": "Offloading MPI Parallel Prefix Scan (MPI_Scan) with the NetFPGA", 
    "arxiv-id": "1408.4939v1", 
    "author": "Martin Swany", 
    "publish": "2014-08-21T10:24:50Z", 
    "summary": "Parallel programs written using the standard Message Passing Interface (MPI)\nfrequently depend upon the ability to efficiently execute collective\noperations. MPI_Scan is a collective operation defined in MPI that implements\nparallel prefix scan which is very useful primitive operation in several\nparallel applications. This operation can be very time consuming. In this\npaper, we explore the use of hardware programmable network interface cards\nutilizing standard media access protocols for offloading the MPI_Scan operation\nto the underlying network. Our work is based upon the NetFPGA - a programmable\nnetwork interface with an on-board Virtex FPGA and four Ethernet interfaces. We\nhave implemented a network-level MPI_Scan operation using the NetFPGA for use\nin MPI environments. This paper compares the performance of this implementation\nwith MPI over Ethernet for a small configuration."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.4959v1", 
    "title": "A Software Parallel Programming Approach to FPGA-Accelerated Computing", 
    "arxiv-id": "1408.4959v1", 
    "author": "Paul Chow", 
    "publish": "2014-08-21T11:08:17Z", 
    "summary": "This paper introduces an effort to incorporate reconfigurable logic (FPGA)\ncomponents into a software programming model. For this purpose, we have\nimplemented a hardware engine for remote memory communication between hardware\ncomputation nodes and CPUs. The hardware engine is compatible with the API of\nGASNet, a popular communication library used for parallel computing\napplications. We have further implemented our own x86 and ARMv7 software\nversions of the GASNet Core API, enabling us to write distributed applications\nwith software and hardware GASNet components transparently communicating with\neach other."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.6328v1", 
    "title": "A Generic and Extensible Framework for Monitoring Energy Consumption of   OpenStack Clouds", 
    "arxiv-id": "1408.6328v1", 
    "author": "Marcos Dias de Assuncao", 
    "publish": "2014-08-27T06:44:31Z", 
    "summary": "Although cloud computing has been transformational to the IT industry, it is\nbuilt on large data centres that often consume massive amounts of electrical\npower. Efforts have been made to reduce the energy clouds consume, with certain\ndata centres now approaching a Power Usage Effectiveness (PUE) factor of 1.08.\nWhile this is an incredible mark, it also means that the IT infrastructure\naccounts for a large part of the power consumed by a data centre. Hence, means\nto monitor and analyse how energy is spent have never been so crucial. Such\nmonitoring is required not only for understanding how power is consumed, but\nalso for assessing the impact of energy management policies. In this article,\nwe draw lessons from experience on monitoring large-scale systems and introduce\nan energy monitoring software framework called KiloWatt API (KWAPI), able to\nhandle OpenStack clouds. The framework --- whose architecture is scalable,\nextensible, and completely integrated into OpenStack --- supports several\nwattmeter devices, multiple measurement formats, and minimises communication\noverhead."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.6729v1", 
    "title": "BSP Sorting: An experimental Study", 
    "arxiv-id": "1408.6729v1", 
    "author": "Constantinos J. Siniolakis", 
    "publish": "2014-08-28T14:04:52Z", 
    "summary": "The Bulk-Synchronous Parallel model of computation has been used for the\narchitecture independent design and analysis of parallel algorithms whose\nperformance is expressed not only in terms of problem size n but also in terms\nof parallel machine properties. In this paper the performance of\nimplementations of deterministic and randomized BSP sorting algorithms is\nexamined. The deterministic algorithm uses deterministic regular oversampling\nand parallel sample sorting and is augmented to handle duplicate keys\ntransparently with optimal asymptotic efficiency. The randomized algorithm is\nsample-sort based and uses oversampling and the ideas introduced with the\ndeterministic algorithm. The resulting randomized design, however, works\ndifferently from traditional parallel sample-sort based algorithms and is also\naugmented to transparently handle duplicate keys with optimal asymptotic\nefficiency thus eliminating the need to tag all input keys and to double\ncommunication/computation time. Both algorithms are shown to balance the\nwork-load evenly among the processors and the use and precise tuning of\noversampling that the BSP analysis allows combined with the transparent\nduplicate-key handling insures regular and balanced communication."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.6891v2", 
    "title": "Software-Defined Cloud Computing: Architectural Elements and Open   Challenges", 
    "arxiv-id": "1408.6891v2", 
    "author": "Young Yoon", 
    "publish": "2014-08-29T00:13:41Z", 
    "summary": "The variety of existing cloud services creates a challenge for service\nproviders to enforce reasonable Software Level Agreements (SLA) stating the\nQuality of Service (QoS) and penalties in case QoS is not achieved. To avoid\nsuch penalties at the same time that the infrastructure operates with minimum\nenergy and resource wastage, constant monitoring and adaptation of the\ninfrastructure is needed. We refer to Software-Defined Cloud Computing, or\nsimply Software-Defined Clouds (SDC), as an approach for automating the process\nof optimal cloud configuration by extending virtualization concept to all\nresources in a data center. An SDC enables easy reconfiguration and adaptation\nof physical resources in a cloud infrastructure, to better accommodate the\ndemand on QoS through a software that can describe and manage various aspects\ncomprising the cloud environment. In this paper, we present an architecture for\nSDCs on data centers with emphasis on mobile cloud applications. We present an\nevaluation, showcasing the potential of SDC in two use cases-QoS-aware\nbandwidth allocation and bandwidth-aware, energy-efficient VM placement-and\ndiscuss the research challenges and opportunities in this emerging area."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.6923v1", 
    "title": "GPGPU Computing", 
    "arxiv-id": "1408.6923v1", 
    "author": "Raluca Mariana Dragoescu", 
    "publish": "2014-08-29T05:24:20Z", 
    "summary": "Since the first idea of using GPU to general purpose computing, things have\nevolved over the years and now there are several approaches to GPU programming.\nGPU computing practically began with the introduction of CUDA (Compute Unified\nDevice Architecture) by NVIDIA and Stream by AMD. These are APIs designed by\nthe GPU vendors to be used together with the hardware that they provide. A new\nemerging standard, OpenCL (Open Computing Language) tries to unify different\nGPU general computing API implementations and provides a framework for writing\nprograms executed across heterogeneous platforms consisting of both CPUs and\nGPUs. OpenCL provides parallel computing using task-based and data-based\nparallelism. In this paper we will focus on the CUDA parallel computing\narchitecture and programming model introduced by NVIDIA. We will present the\nbenefits of the CUDA programming model. We will also compare the two main\napproaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries\nto unify the GPGPU computing models."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.7035v2", 
    "title": "Cooperation with Disagreement Correction in the Presence of   Communication Failures", 
    "arxiv-id": "1408.7035v2", 
    "author": "Paolo Falcone", 
    "publish": "2014-08-29T14:34:00Z", 
    "summary": "Vehicle-to-vehicle communication is a fundamental requirement in cooperative\nvehicular systems to achieve high performance while keeping high safety\nstandards. Vehicles periodically exchange critical information with nearby\nvehicles to determine their maneuvers according to the information quality and\nestablished strategies. However, wireless communication is prone to failures.\nThus, participants can be unaware that other participants have not received the\ninformation on time resulting in conflicting trajectories that may not be safe.\nWe present a deterministic solution that allows all participants to use a\ndefault strategy when other participants have not received on time the complete\ninformation. We base our solution on a timed distributed protocol that adapts\nits output according to the effect of message omission failures so that the\ndisagreement period occurs for no longer than a constant time (of the order of\nmilliseconds) that only depends on the message delay. We formally show the\ncorrectness and perform experiments to corroborate its efficiency. We explain\nhow the proposed solution can be used on vehicular platooning to attain high\nperformance and still guarantee high safety standards despite communication\nfailures. We believe that this work can facilitate the implementation of\ncooperative driving systems that have to deal with inherent (communication)\nuncertainties."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-07518-1_15", 
    "link": "http://arxiv.org/pdf/1408.7070v1", 
    "title": "An effective single-hop distributed hash table with high lookup   performance and low traffic overhead", 
    "arxiv-id": "1408.7070v1", 
    "author": "Claudio L. Amorim", 
    "publish": "2014-08-29T16:59:25Z", 
    "summary": "Distributed Hash Tables (DHTs) have been used in several applications, but\nmost DHTs have opted to solve lookups with multiple hops, to minimize bandwidth\ncosts while sacrificing lookup latency. This paper presents D1HT, an original\nDHT which has a peer-to-peer and self-organizing architecture and maximizes\nlookup performance with reasonable maintenance traffic, and a Quarantine\nmechanism to reduce overheads caused by volatile peers. We implemented both\nD1HT and a prominent single-hop DHT, and we performed an extensive and highly\nrepresentative DHT experimental comparison, followed by complementary\nanalytical studies. In comparison with current single-hop DHTs, our results\nshowed that D1HT consistently had the lowest bandwidth requirements, with\ntypical reductions of up to one order of magnitude, and that D1HT could be used\neven in popular Internet applications with millions of users. In addition, we\nran the first latency experiments comparing DHTs to directory servers, which\nrevealed that D1HT can achieve latencies equivalent to or better than a\ndirectory server, and confirmed its greater scalability properties. Overall,\nour extensive set of results allowed us to conclude that D1HT can provide a\nvery effective solution for a broad range of environments, from large-scale\ncorporate datacenters to widely deployed Internet applications."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14609-6", 
    "link": "http://arxiv.org/pdf/1409.0325v1", 
    "title": "Energy-Aware Cloud Management through Progressive SLA Specification", 
    "arxiv-id": "1409.0325v1", 
    "author": "Achim Streit", 
    "publish": "2014-09-01T08:35:08Z", 
    "summary": "Novel energy-aware cloud management methods dynamically reallocate\ncomputation across geographically distributed data centers to leverage regional\nelectricity price and temperature differences. As a result, a managed VM may\nsuffer occasional downtimes. Current cloud providers only offer high\navailability VMs, without enough flexibility to apply such energy-aware\nmanagement. In this paper we show how to analyse past traces of dynamic cloud\nmanagement actions based on electricity prices and temperatures to estimate VM\navailability and price values. We propose a novel SLA specification approach\nfor offering VMs with different availability and price values guaranteed over\nmultiple SLAs to enable flexible energy-aware cloud management. We determine\nthe optimal number of such SLAs as well as their availability and price\nguaranteed values. We evaluate our approach in a user SLA selection simulation\nusing Wikipedia and Grid'5000 workloads. The results show higher customer\nconversion and 39% average energy savings per VM."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14609-6", 
    "link": "http://arxiv.org/pdf/1409.3367v1", 
    "title": "HTML5 WebSocket protocol and its application to distributed computing", 
    "arxiv-id": "1409.3367v1", 
    "author": "Gabriel L. Muller", 
    "publish": "2014-09-11T09:36:46Z", 
    "summary": "HTML5 WebSocket protocol brings real time communication in web browsers to a\nnew level. Daily, new products are designed to stay permanently connected to\nthe web. WebSocket is the technology enabling this revolution. WebSockets are\nsupported by all current browsers, but it is still a new technology in constant\nevolution.\n  WebSockets are slowly replacing older client-server communication\ntechnologies. As opposed to comet-like technologies WebSockets' remarkable\nperformances is a result of the protocol's fully duplex nature and because it\ndoesn't rely on HTTP communications.\n  To begin with this paper studies the WebSocket protocol and different\nWebSocket servers implementations. This first theoretic part focuses more\ndeeply on heterogeneous implementations and OpenCL. The second part is a\nbenchmark of a new promising library.\n  The real-time engine used for testing purposes is SocketCluster.\nSocketCluster provides a highly scalable WebSocket server that makes use of all\navailable cpu cores on an instance. The scope of this work is reduced to\nvertical scaling of SocketCluster."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-14609-6", 
    "link": "http://arxiv.org/pdf/1409.3651v1", 
    "title": "An Advanced Approach On Load Balancing in Grid Computing", 
    "arxiv-id": "1409.3651v1", 
    "author": "Dushyant Vaghela", 
    "publish": "2014-09-12T05:40:34Z", 
    "summary": "With the rapid development in wide area networks and low cost, powerful\ncomputational resources, grid computing has gained its popularity. With the\nadvent of grid computing, space limitations of conventional distributed systems\ncan be overcome and underutilized computing resources at different locations\naround the world can be put to distributed jobs. Workload and resource\nmanagement is the main key grid services at the service level of grid\ninfrastructures, out of which load balancing in the main concern for grid\ndevelopers. It has been found that load is the major problem which server\nfaces, especially when the number of users increases. A lot of research is\nbeing done in the area of load management. This paper presents the various\nmechanisms of load balancing in grid computing so that the readers will get an\nidea of which algorithm would be suitable in different situations. Keywords:\nwide area network, distributed computing, load balancing."
},{
    "category": "cs.DC", 
    "doi": "10.1109/Emission.2014.6893974", 
    "link": "http://arxiv.org/pdf/1409.4082v1", 
    "title": "Virtual Laboratories in Cloud Infrastructure of Educational Institutions", 
    "arxiv-id": "1409.4082v1", 
    "author": "Evgeny Nikulchev", 
    "publish": "2014-09-14T17:53:49Z", 
    "summary": "Modern educational institutions widely used virtual laboratories and cloud\ntechnologies. In practice must deal with security, processing speed and other\ntasks. The paper describes the experience of the construction of an\nexperimental stand cloud computing and network management. Models and control\nprinciples set forth herein."
},{
    "category": "cs.DC", 
    "doi": "10.4236/ijcns.2014.77026", 
    "link": "http://arxiv.org/pdf/1409.4626v1", 
    "title": "Laboratory Test Bench for Research Network and Cloud Computing", 
    "arxiv-id": "1409.4626v1", 
    "author": "Simon Payain", 
    "publish": "2014-09-14T18:00:13Z", 
    "summary": "At present moment, there is a great interest in development of information\nsystems operating in cloud infrastructures. Generally, many of tasks remain\nunresolved such as tasks of optimization of large databases in a hybrid cloud\ninfrastructure, quality of service (QoS) at different levels of cloud services,\ndynamic control of distribution of cloud resources in application systems and\nmany others. Research and development of new solutions can be limited in case\nof using emulators or international commercial cloud services, due to the\nclosed architecture and limited opportunities for experimentation. Article\nprovides answers to questions on the establishment of a pilot cloud practically\n\"at home\" with the ability to adjust the width of the emulation channel and\ndelays in data transmission. It also describes architecture and configuration\nof the experimental setup. The proposed modular structure can be expanded by\navailable computing power."
},{
    "category": "cs.DC", 
    "doi": "10.4236/ijcns.2014.77026", 
    "link": "http://arxiv.org/pdf/1409.4711v2", 
    "title": "Doing-it-All with Bounded Work and Communication", 
    "arxiv-id": "1409.4711v2", 
    "author": "Alexander A. Schwarzmann", 
    "publish": "2014-09-16T17:39:27Z", 
    "summary": "We consider the Do-All problem, where $p$ cooperating processors need to\ncomplete $t$ similar and independent tasks in an adversarial setting. Here we\ndeal with a synchronous message passing system with processors that are subject\nto crash failures. Efficiency of algorithms in this setting is measured in\nterms of work complexity (also known as total available processor steps) and\ncommunication complexity (total number of point-to-point messages). When work\nand communication are considered to be comparable resources, then the overall\nefficiency is meaningfully expressed in terms of effort defined as work +\ncommunication. We develop and analyze a constructive algorithm that has work\n${\\cal O}( t + p \\log p\\, (\\sqrt{p\\log p}+\\sqrt{t\\log t}\\, ) )$ and a\nnonconstructive algorithm that has work ${\\cal O}(t +p \\log^2 p)$. The latter\nresult is close to the lower bound $\\Omega(t + p \\log p/ \\log \\log p)$ on work.\nThe effort of each of these algorithms is proportional to its work when the\nnumber of crashes is bounded above by $c\\,p$, for some positive constant $c <\n1$. We also present a nonconstructive algorithm that has effort ${\\cal O}(t + p\n^{1.77})$."
},{
    "category": "cs.DC", 
    "doi": "10.4236/ijcns.2014.77026", 
    "link": "http://arxiv.org/pdf/1409.5313v2", 
    "title": "Sandboxing for Software Transactional Memory with Deferred Updates", 
    "arxiv-id": "1409.5313v2", 
    "author": "Holger Machens", 
    "publish": "2014-09-18T14:26:22Z", 
    "summary": "Software transactional memory implementations which allow transactions to\nwork on inconsistent states of shared data, risk to cause application visible\nerrors such as memory access violations or endless loops. Hence, many\nimplementations rely on repeated incremental validation of every read of the\ntransaction to always guarantee for a consistent view of shared data. Because\nthis eager validation technique generates significant processing costs several\nproposals have been published to establish a sandbox for transactions, which\ntransparently prevents or suppresses those errors and thereby allows to reduce\nthe frequency of in-flight validations.\n  The most comprehensive sandboxing concept of transactions in software\ntransactional memory based on deferred updates and considering unmanaged\nlanguages, integrates multiple techniques such as signal interposition,\nout-of-band validation and static and dynamic instrumentation. The latter\ncomprises the insertion of a validation barrier in front of every direct write\nwhich addresses the execution stack of the thread and potentially results from\nunvalidated reads.\n  This paper basically results from a review of this sandboxing approach, which\nrevealed some improvements for sandboxing on C/C++. Based on knowledge about\nthe runtime environment and the compiler an error model has been developed to\nidentify critical paths to application visible errors. This analysis lead to a\nconcept for stack protection with less frequent validation, an alternative\nout-of-band validation technique and revealed additional risks of so-called\nwaivered regions without instrumentation inside transactions."
},{
    "category": "cs.DC", 
    "doi": "10.4236/ijcns.2014.77026", 
    "link": "http://arxiv.org/pdf/1409.5546v1", 
    "title": "Watchword-Oriented and Time-Stamped Algorithms for Tamper-Proof Cloud   Provenance Cognition", 
    "arxiv-id": "1409.5546v1", 
    "author": "Kazi Sakib", 
    "publish": "2014-09-19T08:33:15Z", 
    "summary": "Provenance is derivative journal information about the origin and activities\nof system data and processes. For a highly dynamic system like the cloud,\nprovenance can be accurately detected and securely used in cloud digital\nforensic investigation activities. This paper proposes watchword oriented\nprovenance cognition algorithm for the cloud environment. Additionally\ntime-stamp based buffer verifying algorithm is proposed for securing the access\nto the detected cloud provenance. Performance analysis of the novel algorithms\nproposed here yields a desirable detection rate of 89.33% and miss rate of\n8.66%. The securing algorithm successfully rejects 64% of malicious requests,\nyielding a cumulative frequency of 21.43 for MR."
},{
    "category": "cs.DC", 
    "doi": "10.4236/ijcns.2014.77026", 
    "link": "http://arxiv.org/pdf/1409.5552v1", 
    "title": "Active-Threaded Algorithms for Provenance Cognition in the Cloud   preserving Low Overhead and Fault Tolerance", 
    "arxiv-id": "1409.5552v1", 
    "author": "Kazi Sakib", 
    "publish": "2014-09-19T08:47:51Z", 
    "summary": "Provenance is the derivation history of information about the origin of data\nand processes. For a highly dynamic system such as the cloud, provenance must\nbe effectively detected to be used as proves to ensure accountability during\ndigital forensic investigations. This paper proposes active-threaded provenance\ncognition algorithms that ensure effective and high speed detection of\nprovenance information in the activity layer of the cloud. The algorithms also\nsupport encapsulation of the provenance information on specific targets.\nPerformance evaluation of the proposed algorithms reveal mean delay of 8.198\nseconds that is below the pre-defined benchmark of 10 seconds. Standard\ndeviation and cumulative frequencies for delays are found to be 1.434 and 45.1%\nrespectively."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.future.2014.09.005", 
    "link": "http://arxiv.org/pdf/1409.5715v2", 
    "title": "Elastic Business Process Management: State of the Art and Open   Challenges for BPM in the Cloud", 
    "arxiv-id": "1409.5715v2", 
    "author": "Philipp Hoenisch", 
    "publish": "2014-09-19T16:36:49Z", 
    "summary": "With the advent of cloud computing, organizations are nowadays able to react\nrapidly to changing demands for computational resources. Not only individual\napplications can be hosted on virtual cloud infrastructures, but also complete\nbusiness processes. This allows the realization of so-called elastic processes,\ni.e., processes which are carried out using elastic cloud resources. Despite\nthe manifold benefits of elastic processes, there is still a lack of solutions\nsupporting them.\n  In this paper, we identify the state of the art of elastic Business Process\nManagement with a focus on infrastructural challenges. We conceptualize an\narchitecture for an elastic Business Process Management System and discuss\nexisting work on scheduling, resource allocation, monitoring, decentralized\ncoordination, and state management for elastic processes. Furthermore, we\npresent two representative elastic Business Process Management Systems which\nare intended to counter these challenges. Based on our findings, we identify\nopen issues and outline possible research directions for the realization of\nelastic processes and elastic Business Process Management."
},{
    "category": "cs.DC", 
    "doi": "10.14445/22312803/IJCTT-V15P103", 
    "link": "http://arxiv.org/pdf/1409.6679v1", 
    "title": "Heterogeneous Multi core processors for improving the efficiency of   Market basket analysis algorithm in data mining", 
    "arxiv-id": "1409.6679v1", 
    "author": "Aashiha Priyadarshni. L", 
    "publish": "2014-09-23T17:44:59Z", 
    "summary": "Heterogeneous multi core processors can offer diverse computing capabilities.\nThe efficiency of Market Basket Analysis Algorithm can be improved with\nheterogeneous multi core processors. Market basket analysis algorithm utilises\napriori algorithm and is one of the popular data mining algorithms which can\nutilise Map/Reduce framework to perform analysis. The algorithm generates\nassociation rules based on transactional data and Map/Reduce motivates to\nredesign and convert the existing sequential algorithms for efficiency. Hadoop\nis the parallel programming platform built on Hadoop Distributed File\nSystems(HDFS) for Map/Reduce computation that process data as (key, value)\npairs. In Hadoop map/reduce, the sequential jobs are parallelised and the Job\nTracker assigns parallel tasks to the Task Tracker. Based on single threaded or\nmultithreaded parallel tasks in the task tracker, execution is carried out in\nthe appropriate cores. For this, a new scheduler called MB Scheduler can be\ndeveloped. Switching between the cores can be made static or dynamic. The use\nof heterogeneous multi core processors optimizes processing capabilities and\npower requirements for a processor and improves the performance of the system."
},{
    "category": "cs.DC", 
    "doi": "10.1109/UCC.2014.34", 
    "link": "http://arxiv.org/pdf/1409.8098v1", 
    "title": "Workflow Partitioning and Deployment on the Cloud using Orchestra", 
    "arxiv-id": "1409.8098v1", 
    "author": "Adam Barker", 
    "publish": "2014-09-29T12:39:13Z", 
    "summary": "Orchestrating service-oriented workflows is typically based on a design model\nthat routes both data and control through a single point - the centralised\nworkflow engine. This causes scalability problems that include the unnecessary\nconsumption of the network bandwidth, high latency in transmitting data between\nthe services, and performance bottlenecks. These problems are highly prominent\nwhen orchestrating workflows that are composed from services dispersed across\ndistant geographical locations. This paper presents a novel workflow\npartitioning approach, which attempts to improve the scalability of\norchestrating large-scale workflows. It permits the workflow computation to be\nmoved towards the services providing the data in order to garner optimal\nperformance results. This is achieved by decomposing the workflow into smaller\nsub workflows for parallel execution, and determining the most appropriate\nnetwork locations to which these sub workflows are transmitted and subsequently\nexecuted. This paper demonstrates the efficiency of our approach using a set of\nexperimental workflows that are orchestrated over Amazon EC2 and across several\ngeographic network regions."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2015.75", 
    "link": "http://arxiv.org/pdf/1409.8324v3", 
    "title": "Cache Serializability: Reducing Inconsistency in Edge Transactions", 
    "arxiv-id": "1409.8324v3", 
    "author": "Robbert van Renesse", 
    "publish": "2014-09-29T20:53:34Z", 
    "summary": "Read-only caches are widely used in cloud infrastructures to reduce access\nlatency and load on backend databases. Operators view coherent caches as\nimpractical at genuinely large scale and many client-facing caches are updated\nin an asynchronous manner with best-effort pipelines. Existing solutions that\nsupport cache consistency are inapplicable to this scenario since they require\na round trip to the database on every cache transaction.\n  Existing incoherent cache technologies are oblivious to transactional data\naccess, even if the backend database supports transactions. We propose T-Cache,\na novel caching policy for read-only transactions in which inconsistency is\ntolerable (won't cause safety violations) but undesirable (has a cost). T-Cache\nimproves cache consistency despite asynchronous and unreliable communication\nbetween the cache and the database. We define cache-serializability, a variant\nof serializability that is suitable for incoherent caches, and prove that with\nunbounded resources T-Cache implements this new specification. With limited\nresources, T-Cache allows the system manager to choose a trade-off between\nperformance and consistency.\n  Our evaluation shows that T-Cache detects many inconsistencies with only\nnominal overhead. We use synthetic workloads to demonstrate the efficacy of\nT-Cache when data accesses are clustered and its adaptive reaction to workload\nchanges. With workloads based on the real-world topologies, T-Cache detects\n43-70% of the inconsistencies and increases the rate of consistent transactions\nby 33-58%."
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2015.75", 
    "link": "http://arxiv.org/pdf/1501.00039v2", 
    "title": "Design, Construction, and Use of a Single Board Computer Beowulf   Cluster: Application of the Small-Footprint, Low-Cost, InSignal 5420 Octa   Board", 
    "arxiv-id": "1501.00039v2", 
    "author": "Tasha Pitt", 
    "publish": "2014-12-30T22:47:17Z", 
    "summary": "In recent years development in the area of Single Board Computing has been\nadvancing rapidly. At Wolters Kluwer's Corporate Legal Services Division a\nprototyping effort was undertaken to establish the utility of such devices for\npractical and general computing needs. This paper presents the background of\nthis work, the design and construction of a 64 core 96 GHz cluster, and their\npossibility of yielding approximately 400 GFLOPs from a set of small footprint\nInSignal boards created for just over $2,300. Additionally this paper discusses\nthe software environment on the cluster, the use of a standard Beowulf library\nand its operation, as well as other software application uses including Elastic\nSearch and ownCloud. Finally, consideration will be given to the future use of\nsuch technologies in a business setting in order to introduce new Open Source\ntechnologies, reduce computing costs, and improve Time to Market.\n  Index Terms: Single Board Computing, Raspberry Pi, InSignal Exynos 5420,\nLinaro Ubuntu Linux, High Performance Computing, Beowulf clustering, Open\nSource, MySQL, MongoDB, ownCloud, Computing Architectures, Parallel Computing,\nCluster Computing"
},{
    "category": "cs.DC", 
    "doi": "10.1109/ICDCS.2015.75", 
    "link": "http://arxiv.org/pdf/1501.00048v1", 
    "title": "Methods and Metrics for Fair Server Assessment under Real-Time Financial   Workloads", 
    "arxiv-id": "1501.00048v1", 
    "author": "Dimitrios S. Nikolopoulos", 
    "publish": "2014-12-30T23:56:53Z", 
    "summary": "Energy efficiency has been a daunting challenge for datacenters. The\nfinancial industry operates some of the largest datacenters in the world. With\nincreasing energy costs and the financial services sector growth, emerging\nfinancial analytics workloads may incur extremely high operational costs, to\nmeet their latency targets. Microservers have recently emerged as an\nalternative to high-end servers, promising scalable performance and low energy\nconsumption in datacenters via scale-out. Unfortunately, stark differences in\narchitectural features, form factor and design considerations make a fair\ncomparison between servers and microservers exceptionally challenging. In this\npaper we present a rigorous methodology and new metrics for fair comparison of\nserver and microserver platforms. We deploy our methodology and metrics to\ncompare a microserver with ARM cores against two servers with x86 cores,\nrunning the same real-time financial analytics workload. We define\nworkload-specific but platform-independent performance metrics for platform\ncomparison, targeting both datacenter operators and end users. Our methodology\nestablishes that a server based the Xeon Phi processor delivers the highest\nperformance and energy-efficiency. However, by scaling out energy-efficient\nmicroservers, we achieve competitive or better energy-efficiency than a\npower-equivalent server with two Sandy Bridge sockets despite the microserver's\nslower cores. Using a new iso-QoS (iso-Quality of Service) metric, we find that\nthe ARM microserver scales enough to meet market throughput demand, i.e. a 100%\nQoS in terms of timely option pricing, with as little as 55% of the energy\nconsumed by the Sandy Bridge server."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.00182v2", 
    "title": "An information services algorithm to heuristically summarize IP   addresses for a distributed, hierarchical directory service", 
    "arxiv-id": "1501.00182v2", 
    "author": "Martin Swany", 
    "publish": "2014-12-31T17:39:15Z", 
    "summary": "A distributed, hierarchical information service for computer networks might\nrely in several instances, located in different layers. A distributed directory\nservice, for example, might be comprised of upper level listings, and local\ndirectories. The upper level listings contain a compact version of the local\ndirectories. Clients desiring to access the information contained in local\ndirectories might first access the high-level listings, in order to locate the\nappropriate local instance. One of the keys for the competent operation of such\nservice is the ability of properly summarizing the information, which will be\nmaintained in the upper level directories. We analyze the case of the Lookup\nService in the Information Services plane of perfSONAR performance monitoring\ndistributed architecture, which implements IPv4 summarization in its functions.\nWe propose an empirical method, or heuristic, to achieve the summarizations,\nbased on the PATRICIA tree. We further apply the heuristic on a simulated\ndistributed test bed and contemplate the results."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.00513v1", 
    "title": "Self-Repairing Disk Arrays", 
    "arxiv-id": "1501.00513v1", 
    "author": "Thomas J. E. Schwarz", 
    "publish": "2015-01-02T23:35:06Z", 
    "summary": "As the prices of magnetic storage continue to decrease, the cost of replacing\nfailed disks becomes increasingly dominated by the cost of the service call\nitself. We propose to eliminate these calls by building disk arrays that\ncontain enough spare disks to operate without any human intervention during\ntheir whole lifetime. To evaluate the feasibility of this approach, we have\nsimulated the behavior of two-dimensional disk arrays with n parity disks and\nn(n-1)/2 data disks under realistic failure and repair assumptions. Our\nconclusion is that having n(n+1)/2 spare disks is more than enough to achieve a\n99.999 percent probability of not losing data over four years. We observe that\nthe same objectives cannot be reached with RAID level 6 organizations and would\nrequire RAID stripes that could tolerate triple disk failures."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.00567v1", 
    "title": "Adaptive Dispatching of Tasks in the Cloud", 
    "arxiv-id": "1501.00567v1", 
    "author": "Erol Gelenbe", 
    "publish": "2015-01-03T14:50:30Z", 
    "summary": "The increasingly wide application of Cloud Computing enables the\nconsolidation of tens of thousands of applications in shared infrastructures.\nThus, meeting the quality of service requirements of so many diverse\napplications in such shared resource environments has become a real challenge,\nespecially since the characteristics and workload of applications differ widely\nand may change over time. This paper presents an experimental system that can\nexploit a variety of online quality of service aware adaptive task allocation\nschemes, and three such schemes are designed and compared. These are a\nmeasurement driven algorithm that uses reinforcement learning, secondly a\n\"sensible\" allocation algorithm that assigns jobs to sub-systems that are\nobserved to provide a lower response time, and then an algorithm that splits\nthe job arrival stream into sub-streams at rates computed from the hosts'\nprocessing capabilities. All of these schemes are compared via measurements\namong themselves and with a simple round-robin scheduler, on two experimental\ntest-beds with homogeneous and heterogeneous hosts having different processing\ncapacities."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.01323v1", 
    "title": "Recent Developments in Cloud Based Systems: State of Art", 
    "arxiv-id": "1501.01323v1", 
    "author": "Kashish Ara Shakil", 
    "publish": "2015-01-05T11:07:13Z", 
    "summary": "Cloud computing is the new buzzword in the head of the techies round the\nclock these days. The importance and the different applications of cloud\ncomputing are overwhelming and thus, it is a topic of huge significance. It\nprovides several astounding features like Multitenancy, on demand service, pay\nper use etc. This manuscript presents an exhaustive survey on cloud computing\ntechnology and potential research issues in cloud computing that needs to be\naddressed."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.01405v1", 
    "title": "Warp-Level Parallelism: Enabling Multiple Replications In Parallel on   GPU", 
    "arxiv-id": "1501.01405v1", 
    "author": "David Hill", 
    "publish": "2015-01-07T09:15:48Z", 
    "summary": "Stochastic simulations need multiple replications in order to build\nconfidence intervals for their results. Even if we do not need a large amount\nof replications, it is a good practice to speed-up the whole simulation time\nusing the Multiple Replications In Parallel (MRIP) approach. This approach\nusually supposes to have access to a parallel computer such as a symmetric\nmul-tiprocessing machine (with many cores), a computing cluster or a computing\ngrid. In this paper, we propose Warp-Level Parallelism (WLP), a GP-GPU-enabled\nsolution to compute MRIP on GP-GPUs (General-Purpose Graphics Processing\nUnits). These devices display a great amount of parallel computational power at\nlow cost, but are tuned to process efficiently the same operation on several\ndata, through different threads. Indeed, this paradigm is called Single\nInstruction, Multiple Threads (SIMT). Our approach proposes to rely on small\nthreads groups, called warps, to perform independent computations such as\nreplications. We have benchmarked WLP with three different models: it allows\nMRIP to be computed up to six times faster than with the SIMT computing\nparadigm."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.01426v1", 
    "title": "Analysis and Clustering of Workload in Google Cluster Trace based on   Resource Usage", 
    "arxiv-id": "1501.01426v1", 
    "author": "Shuchi Sethi", 
    "publish": "2015-01-07T10:15:05Z", 
    "summary": "Cloud computing has gained interest amongst commercial organizations,\nresearch communities, developers and other individuals during the past few\nyears.In order to move ahead with research in field of data management and\nprocessing of such data, we need benchmark datasets and freely available data\nwhich are publicly accessible. Google in May 2011 released a trace of a cluster\nof 11k machines referred as Google Cluster Trace.This trace contains cell\ninformation of about 29 days.This paper provides analysis of resource usage and\nrequirements in this trace and is an attempt to give an insight into such kind\nof production trace similar to the ones in cloud environment.The major\ncontributions of this paper include Statistical Profile of Jobs based on\nresource usage, clustering of Workload Patterns and Classification of jobs into\ndifferent types based on k-means clustering.Though there have been earlier\nworks for analysis of this trace, but our analysis provides several new\nfindings such as jobs in a production trace are trimodal and there occurs\nsymmetry in the tasks within a long job type"
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02165v1", 
    "title": "Update Consistency for Wait-free Concurrent Objects", 
    "arxiv-id": "1501.02165v1", 
    "author": "Claude Jard", 
    "publish": "2015-01-09T15:03:33Z", 
    "summary": "In large scale systems such as the Internet, replicating data is an essential\nfeature in order to provide availability and fault-tolerance. Attiya and Welch\nproved that using strong consistency criteria such as atomicity is costly as\neach operation may need an execution time linear with the latency of the\ncommunication network. Weaker consistency criteria like causal consistency and\nPRAM consistency do not ensure convergence. The different replicas are not\nguaranteed to converge towards a unique state. Eventual consistency guarantees\nthat all replicas eventually converge when the participants stop updating.\nHowever, it fails to fully specify the semantics of the operations on shared\nobjects and requires additional non-intuitive and error-prone distributed\nspecification techniques. This paper introduces and formalizes a new\nconsistency criterion, called update consistency, that requires the state of a\nreplicated object to be consistent with a linearization of all the updates. In\nother words, whereas atomicity imposes a linearization of all of the\noperations, this criterion imposes this only on updates. Consequently some read\noperations may return out-dated values. Update consistency is stronger than\neventual consistency, so we can replace eventually consistent objects with\nupdate consistent ones in any program. Finally, we prove that update\nconsistency is universal, in the sense that any object can be implemented under\nthis criterion in a distributed system where any number of nodes may crash."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02175v1", 
    "title": "Brief Announcement: Update Consistency in Partitionable Systems", 
    "arxiv-id": "1501.02175v1", 
    "author": "Claude Jard", 
    "publish": "2015-01-09T15:31:40Z", 
    "summary": "Data replication is essential to ensure reliability, availability and\nfault-tolerance of massive distributed applications over large scale systems\nsuch as the Internet. However, these systems are prone to partitioning, which\nby Brewer's CAP theorem [1] makes it impossible to use a strong consistency\ncriterion like atomicity. Eventual consistency [2] guaranties that all replicas\neventually converge to a common state when the participants stop updating.\nHowever, it fails to fully specify shared objects and requires additional\nnon-intuitive and error-prone distributed specification techniques, that must\ntake into account all possible concurrent histories of updates to specify this\ncommon state [3]. This approach, that can lead to specifications as complicated\nas the implementations themselves, is limited by a more serious issue. The\nconcurrent specification of objects uses the notion of concurrent events. In\nmessage-passing systems, two events are concurrent if they are enforced by\ndifferent processes and each process enforced its event before it received the\nnotification message from the other process. In other words, the notion of\nconcurrency depends on the implementation of the object, not on its\nspecification. Consequently, the final user may not know if two events are\nconcurrent without explicitly tracking the messages exchanged by the processes.\nA specification should be independent of the system on which it is implemented.\nWe believe that an object should be totally specified by two facets: its\nabstract data type, that characterizes its sequential executions, and a\nconsistency criterion, that defines how it is supposed to behave in a\ndistributed environment. Not only sequential specification helps repeal the\nproblem of intention, it also allows to use the well studied and understood\nnotions of languages and automata. This makes possible to apply all the tools\ndeveloped for sequential systems, from their simple definition using structures\nand classes to the most advanced techniques like model checking and formal\nverification. Eventual consistency (EC) imposes no constraint on the convergent\nstate, that very few depends on the sequential specification. For example, an\nimplementation that ignores all the updates is eventually consistent, as all\nreplicas converge to the initial state. We propose a new consistency criterion,\nupdate consistency (UC), in which the convergent state must be obtained by a\ntotal ordering of the updates, that contains the sequential order of each"
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02282v1", 
    "title": "Adaptive and application dependent runtime guided hardware prefetcher   reconfiguration on the IBM POWER7", 
    "arxiv-id": "1501.02282v1", 
    "author": "Mateo Valero", 
    "publish": "2015-01-09T22:05:22Z", 
    "summary": "Hardware data prefetcher engines have been extensively used to reduce the\nimpact of memory latency. However, microprocessors' hardware prefetcher engines\ndo not include any automatic hardware control able to dynamically tune their\noperation. This lacking architectural feature causes systems to operate with\nprefetchers in a fixed configuration, which in many cases harms performance and\nenergy consumption.\n  In this paper, a piece of software that solves the discussed problem in the\ncontext of the IBM POWER7 microprocessor is presented. The proposed solution\ninvolves using the runtime software as a bridge that is able to characterize\nuser applications' workload and dynamically reconfigure the prefetcher engine.\nThe proposed mechanisms has been deployed over OmpSs, a state-of-the-art\ntask-based programming model. The paper shows significant performance\nimprovements over a representative set of microbenchmarks and High Performance\nComputing (HPC) applications."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02330v1", 
    "title": "Task-Cloning Algorithms in a MapReduce Cluster with Competitive   Performance Bounds", 
    "arxiv-id": "1501.02330v1", 
    "author": "Wing Cheong Lau", 
    "publish": "2015-01-10T10:55:41Z", 
    "summary": "Job scheduling for a MapReduce cluster has been an active research topic in\nrecent years. However, measurement traces from real-world production\nenvironment show that the duration of tasks within a job vary widely. The\noverall elapsed time of a job, i.e. the so-called flowtime, is often dictated\nby one or few slowly-running tasks within a job, generally referred as the\n\"stragglers\". The cause of stragglers include tasks running on\npartially/intermittently failing machines or the existence of some localized\nresource bottleneck(s) within a MapReduce cluster. To tackle this online job\nscheduling challenge, we adopt the task cloning approach and design the\ncorresponding scheduling algorithms which aim at minimizing the weighted sum of\njob flowtimes in a MapReduce cluster based on the Shortest Remaining Processing\nTime scheduler (SRPT). To be more specific, we first design a 2-competitive\noffline algorithm when the variance of task-duration is negligible. We then\nextend this offline algorithm to yield the so-called SRPTMS+C algorithm for the\nonline case and show that SRPTMS+C is $(1+\\epsilon)-speed$\n$o(\\frac{1}{\\epsilon^2})-competitive$ in reducing the weighted sum of job\nflowtimes within a cluster. Both of the algorithms explicitly consider the\nprecedence constraints between the two phases within the MapReduce framework.\nWe also demonstrate via trace-driven simulations that SRPTMS+C can\nsignificantly reduce the weighted/unweighted sum of job flowtimes by cutting\ndown the elapsed time of small jobs substantially. In particular, SRPTMS+C\nbeats the Microsoft Mantri scheme by nearly 25% according to this metric."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02724v1", 
    "title": "Towards Energy-Proportional Computing Using Subsystem-Level Power   Management", 
    "arxiv-id": "1501.02724v1", 
    "author": "Wu-chun Feng", 
    "publish": "2015-01-12T17:18:17Z", 
    "summary": "Massive data centers housing thousands of computing nodes have become\ncommonplace in enterprise computing, and the power consumption of such data\ncenters is growing at an unprecedented rate. Adding to the problem is the\ninability of the servers to exhibit energy proportionality, i.e., provide\nenergy-efficient execution under all levels of utilization, which diminishes\nthe overall energy efficiency of the data center. It is imperative that we\nrealize effective strategies to control the power consumption of the server and\nimprove the energy efficiency of data centers. With the advent of Intel Sandy\nBridge processors, we have the ability to specify a limit on power consumption\nduring runtime, which creates opportunities to design new power-management\ntechniques for enterprise workloads and make the systems that they run on more\nenergy-proportional.\n  In this paper, we investigate whether it is possible to achieve energy\nproportionality for enterprise-class server workloads, namely SPECpower_ssj2008\nand SPECweb2009 benchmarks, by using Intel's Running Average Power Limit (RAPL)\ninterfaces. First, we analyze the average power consumption of the full system\nas well as the subsystems and describe the energy proportionality of these\ncomponents. We then characterize the instantaneous power profile of these\nbenchmarks within different subsystems using the on-chip energy meters exposed\nvia the RAPL interfaces. Finally, we present the effects of power limiting on\nthe energy proportionality, performance, power and energy efficiency of\nenterprise-class server workloads. Our observations and results shed light on\nthe efficacy of the RAPL interfaces and provide guidance for designing\npower-management techniques for enterprise-class workloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.02729v1", 
    "title": "On the Energy Proportionality of Scale-Out Workloads", 
    "arxiv-id": "1501.02729v1", 
    "author": "Wu-chun Feng", 
    "publish": "2015-01-12T17:32:17Z", 
    "summary": "Our increasing reliance on the cloud has led to the emergence of scale-out\nworkloads. These scale-out workloads are latency-sensitive as they are user\ndriven. In order to meet strict latency constraints, they require massive\ncomputing infrastructure, which consume significant amount of energy and\ncontribute to operational costs. This cost is further aggravated by the lack of\nenergy proportionality in servers. As Internet services become even more\nubiquitous, scale-out workloads will need increasingly larger cluster\ninstallations. As such, we desire an investigation into the energy\nproportionality and the mechanisms to improve the power consumption of\nscale-out workloads.\n  Therefore, in this paper, we study the energy proportionality and power\nconsumption of clusters in the context of scale-out workloads. Towards this\nend, we evaluate the potential of power and resource provisioning to improve\nthe energy proportionality for this class of workloads. Using data serving, web\nsearching and data caching as our representative workloads, we first analyze\nthe component-level power distribution on a cluster. Second, we characterize\nhow these workloads utilize the cluster. Third, we analyze the potential of\npower provisioning techniques (i.e., active low-power, turbo and idle low-power\nmodes) to improve the energy proportionality of scale-out workloads. We then\ndescribe the ability of active low-power modes to provide trade-offs in power\nand latency. Finally, we compare and contrast power provisioning and resource\nprovisioning techniques. Our study reveals various insights which will help\nimprove the energy proportionality and power consumption of scale-out\nworkloads."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.03064v1", 
    "title": "Proceedings of the Workshop on High Performance Energy Efficient   Embedded Systems (HIP3ES) 2015", 
    "arxiv-id": "1501.03064v1", 
    "author": "Jordi Carrabina", 
    "publish": "2015-01-13T16:29:18Z", 
    "summary": "Proceedings of the Workshop on High Performance Energy Efficient Embedded\nSystems (HIP3ES) 2015. Amsterdam, January 21st. Collocated with HIPEAC 2015\nConference."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.03336v1", 
    "title": "Reducing overheads of dynamic scheduling on heterogeneous chips", 
    "arxiv-id": "1501.03336v1", 
    "author": "Mar\u00eda J. Garzar\u00e1n", 
    "publish": "2015-01-14T13:04:51Z", 
    "summary": "In recent processor development, we have witnessed the integration of GPU and\nCPUs into a single chip. The result of this integration is a reduction of the\ndata communication overheads. This enables an efficient collaboration of both\ndevices in the execution of parallel workloads.\n  In this work, we focus on the problem of efficiently scheduling chunks of\niterations of parallel loops among the computing devices on the chip (the GPU\nand the CPU cores) in the context of irregular applications. In particular, we\nanalyze the sources of overhead that the host thread experiments when a chunk\nof iterations is offloaded to the GPU while other threads are executing\nconcurrently other chunks on the CPU cores. We carefully study these overheads\non different processor architectures and operating systems using Barnes Hut as\na study case representative of irregular applications. We also propose a set of\noptimizations to mitigate the overheads that arise in presence of\noversubscription and take advantage of the different features of the\nheterogeneous architectures. Thanks to these optimizations we reduce\nEnergy-Delay Product (EDP) by 18% and 84% on Intel Ivy Bridge and Haswell\narchitectures, respectively, and by 57% on the Exynos big.LITTLE."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.03481v1", 
    "title": "Iso-Quality of Service: Fairly Ranking Servers for Real-Time Data   Analytics", 
    "arxiv-id": "1501.03481v1", 
    "author": "Dimitrios S. Nikolopoulos", 
    "publish": "2015-01-14T20:42:07Z", 
    "summary": "We present a mathematically rigorous Quality-of-Service (QoS) metric which\nrelates the achievable quality of service metric (QoS) for a real-time\nanalytics service to the server energy cost of offering the service. Using a\nnew iso-QoS evaluation methodology, we scale server resources to meet QoS\ntargets and directly rank the servers in terms of their energy-efficiency and\nby extension cost of ownership. Our metric and method are platform-independent\nand enable fair comparison of datacenter compute servers with significant\narchitectural diversity, including micro-servers. We deploy our metric and\nmethodology to compare three servers running financial option pricing workloads\non real-life market data. We find that server ranking is sensitive to data\ninputs and desired QoS level and that although scale-out micro-servers can be\nup to two times more energy-efficient than conventional heavyweight servers for\nthe same target QoS, they are still six times less energy efficient than\nhigh-performance computational accelerators."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.03610v3", 
    "title": "DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast   Streams", 
    "arxiv-id": "1501.03610v3", 
    "author": "Zhenjie Zhang", 
    "publish": "2015-01-15T09:37:32Z", 
    "summary": "In a data stream management system (DSMS), users register continuous queries,\nand receive result updates as data arrive and expire. We focus on applications\nwith real-time constraints, in which the user must receive each result update\nwithin a given period after the update occurs. To handle fast data, the DSMS is\ncommonly placed on top of a cloud infrastructure. Because stream properties\nsuch as arrival rates can fluctuate unpredictably, cloud resources must be\ndynamically provisioned and scheduled accordingly to ensure real-time response.\nIt is quite essential, for the existing systems or future developments, to\npossess the ability of scheduling resources dynamically according to the\ncurrent workload, in order to avoid wasting resources, or failing in delivering\ncorrect results on time. Motivated by this, we propose DRS, a novel dynamic\nresource scheduler for cloud-based DSMSs. DRS overcomes three fundamental\nchallenges: (a) how to model the relationship between the provisioned resources\nand query response time (b) where to best place resources; and (c) how to\nmeasure system load with minimal overhead. In particular, DRS includes an\naccurate performance model based on the theory of \\emph{Jackson open queueing\nnetworks} and is capable of handling \\emph{arbitrary} operator topologies,\npossibly with loops, splits and joins. Extensive experiments with real data\nconfirm that DRS achieves real-time response with close to optimal resource\nconsumption."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.03619v5", 
    "title": "Optimal Operator State Migration for Elastic Data Stream Processing", 
    "arxiv-id": "1501.03619v5", 
    "author": "Hongyang Chao", 
    "publish": "2015-01-15T10:11:17Z", 
    "summary": "A cloud-based data stream management system (DSMS) handles fast data by\nutilizing the massively parallel processing capabilities of the underlying\nplatform. An important property of such a DSMS is elasticity, meaning that\nnodes can be dynamically added to or removed from an application to match the\nlatter's workload, which may fluctuate in an unpredictable manner. For an\napplication involving stateful operations such as aggregates, the addition /\nremoval of nodes necessitates the migration of operator states. Although the\nimportance of migration has been recognized in existing systems, two key\nproblems remain largely neglected, namely how to migrate and what to migrate,\ni.e., the migration mechanism that reduces synchronization overhead and result\ndelay during migration, and the selection of the optimal task assignment that\nminimizes migration costs. Consequently, migration in current systems typically\nincurs a high spike in result delay caused by expensive synchronization\nbarriers and suboptimal task assignments. Motivated by this, we present the\nfirst comprehensive study on efficient operator states migration, and propose\ndesigns and algorithms that enable live, progressive, and optimized migrations.\nExtensive experiments using real data justify our performance claims."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.04504v2", 
    "title": "TARDIS: Timestamp based Coherence Algorithm for Distributed Shared   Memory", 
    "arxiv-id": "1501.04504v2", 
    "author": "Srinivas Devadas", 
    "publish": "2015-01-19T14:38:00Z", 
    "summary": "A new memory coherence protocol, Tardis, is proposed. Tardis uses timestamp\ncounters representing logical time as well as physical time to order memory\noperations and enforce sequential consistency in any type of shared memory\nsystem. Tardis is unique in that as compared to the widely-adopted directory\ncoherence protocol, and its variants, it completely avoids multicasting and\nonly requires O(log N) storage per cache block for an N-core system rather than\nO(N) sharer information. Tardis is simpler and easier to reason about, yet\nachieves similar performance to directory protocols on a wide range of\nbenchmarks run on 16, 64 and 256 cores."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.04557v2", 
    "title": "Precision-Aware application execution for Energy-optimization in HPC   node system", 
    "arxiv-id": "1501.04557v2", 
    "author": "V\u00edt Vondr\u00e1k", 
    "publish": "2015-01-19T16:56:10Z", 
    "summary": "Power consumption is a critical consideration in high performance computing\nsystems and it is becoming the limiting factor to build and operate Petascale\nand Exascale systems. When studying the power consumption of existing systems\nrunning HPC workloads, we find that power, energy and performance are closely\nrelated which leads to the possibility to optimize energy consumption without\nsacrificing (much or at all) the performance. In this paper, we propose a HPC\nsystem running with a GNU/Linux OS and a Real Time Resource Manager (RTRM) that\nis aware and monitors the healthy of the platform. On the system, an\napplication for disaster management runs. The application can run with\ndifferent QoS depending on the situation. We defined two main situations.\nNormal execution, when there is no risk of a disaster, even though we still\nhave to run the system to look ahead in the near future if the situation\nchanges suddenly. In the second scenario, the possibilities for a disaster are\nvery high. Then the allocation of more resources for improving the precision\nand the human decision has to be taken into account. The paper shows that at\ndesign time, it is possible to describe different optimal points that are going\nto be used at runtime by the RTOS with the application. This environment helps\nto the system that must run 24/7 in saving energy with the trade-off of losing\nprecision. The paper shows a model execution which can improve the precision of\nresults by 65% in average by increasing the number of iterations from 1e3 to\n1e4. This also produces one order of magnitude longer execution time which\nleads to the need to use a multi-node solution. The optimal trade-off between\nprecision vs. execution time is computed by the RTOS with the time overhead\nless than 10% against a native execution."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.04822v3", 
    "title": "Self-Stabilizing Repeated Balls-into-Bins", 
    "arxiv-id": "1501.04822v3", 
    "author": "Gustavo Posta", 
    "publish": "2015-01-20T14:33:59Z", 
    "summary": "We study the following synchronous process that we call \"repeated\nballs-into-bins\". The process is started by assigning $n$ balls to $n$ bins in\nan arbitrary way. In every subsequent round, from each non-empty bin one ball\nis chosen according to some fixed strategy (random, FIFO, etc), and re-assigned\nto one of the $n$ bins uniformly at random.\n  We define a configuration \"legitimate\" if its maximum load is\n$\\mathcal{O}(\\log n)$. We prove that, starting from any configuration, the\nprocess will converge to a legitimate configuration in linear time and then it\nwill only take on legitimate configurations over a period of length bounded by\nany polynomial in $n$, with high probability (w.h.p.). This implies that the\nprocess is self-stabilizing and that every ball traverses all bins in\n$\\mathcal{O}(n \\log^2 n)$ rounds, w.h.p."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.04854v1", 
    "title": "i2MapReduce: Incremental MapReduce for Mining Evolving Big Data", 
    "arxiv-id": "1501.04854v1", 
    "author": "Ge Yu", 
    "publish": "2015-01-20T15:47:46Z", 
    "summary": "As new data and updates are constantly arriving, the results of data mining\napplications become stale and obsolete over time. Incremental processing is a\npromising approach to refreshing mining results. It utilizes previously saved\nstates to avoid the expense of re-computation from scratch.\n  In this paper, we propose i2MapReduce, a novel incremental processing\nextension to MapReduce, the most widely used framework for mining big data.\nCompared with the state-of-the-art work on Incoop, i2MapReduce (i) performs\nkey-value pair level incremental processing rather than task level\nre-computation, (ii) supports not only one-step computation but also more\nsophisticated iterative computation, which is widely used in data mining\napplications, and (iii) incorporates a set of novel techniques to reduce I/O\noverhead for accessing preserved fine-grain computation states. We evaluate\ni2MapReduce using a one-step algorithm and three iterative algorithms with\ndiverse computation characteristics. Experimental results on Amazon EC2 show\nsignificant performance improvements of i2MapReduce compared to both plain and\niterative MapReduce performing re-computation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.05041v1", 
    "title": "Pilot-Abstraction: A Valid Abstraction for Data-Intensive Applications   on HPC, Hadoop and Cloud Infrastructures?", 
    "arxiv-id": "1501.05041v1", 
    "author": "Shantenu Jha", 
    "publish": "2015-01-21T02:55:02Z", 
    "summary": "HPC environments have traditionally been designed to meet the compute demand\nof scientific applications and data has only been a second order concern. With\nscience moving toward data-driven discoveries relying more on correlations in\ndata to form scientific hypotheses, the limitations of HPC approaches become\napparent: Architectural paradigms such as the separation of storage and compute\nare not optimal for I/O intensive workloads (e.g. for data preparation,\ntransformation and SQL). While there are many powerful computational and\nanalytical libraries available on HPC (e.g. for scalable linear algebra), they\ngenerally lack the usability and variety of analytical libraries found in other\nenvironments (e.g. the Apache Hadoop ecosystem). Further, there is a lack of\nabstractions that unify access to increasingly heterogeneous infrastructure\n(HPC, Hadoop, clouds) and allow reasoning about performance trade-offs in this\ncomplex environment. At the same time, the Hadoop ecosystem is evolving rapidly\nand has established itself as de-facto standard for data-intensive workloads in\nindustry and is increasingly used to tackle scientific problems. In this paper,\nwe explore paths to interoperability between Hadoop and HPC, examine the\ndifferences and challenges, such as the different architectural paradigms and\nabstractions, and investigate ways to address them. We propose the extension of\nthe Pilot-Abstraction to Hadoop to serve as interoperability layer for\nallocating and managing resources across different infrastructures. Further,\nin-memory capabilities have been deployed to enhance the performance of\nlarge-scale data analytics (e.g. iterative algorithms) for which the ability to\nre-use data across iterations is critical. As memory naturally fits in with the\nPilot concept of retaining resources for a set of tasks, we propose the\nextension of the Pilot-Abstraction to in-memory resources."
},{
    "category": "cs.DC", 
    "doi": "10.1109/GRID.2010.5697950", 
    "link": "http://arxiv.org/pdf/1501.05286v1", 
    "title": "Distributed mining of large scale remote sensing image archives on   public computing infrastructures", 
    "arxiv-id": "1501.05286v1", 
    "author": "Igor G. Olaizola", 
    "publish": "2015-01-17T08:37:44Z", 
    "summary": "Earth Observation (EO) mining aims at supporting efficient access and\nexploration of petabyte-scale space- and airborne remote sensing archives that\nare currently expanding at rates of terabytes per day. A significant challenge\nis performing the analysis required by envisaged applications --- like for\ninstance process mapping for environmental risk management --- in reasonable\ntime. In this work, we address the problem of content-based image retrieval via\nexample-based queries from EO data archives. In particular, we focus on the\nanalysis of polarimetric SAR data, for which target decomposition theorems have\nproved fundamental in discovering patterns in data and characterize the ground\nscattering properties. To this end, we propose an interactive region-oriented\ncontent-based image mining system in which 1) unsupervised ingestion processes\nare distributed onto virtual machines in elastic, on-demand computing\ninfrastructures 2) archive-scale content hierarchical indexing is implemented\nin terms of a \"big data\" analytics cluster-computing framework 3) query\nprocessing amounts to traversing the generated binary tree index, computing\ndistances that correspond to descriptor-based similarity measures between image\ngroups and a query image tile. We describe in depth both the strategies and the\nactual implementations for the ingestion and indexing components, and verify\nthe approach by experiments carried out on the NASA/JPL UAVSAR full\npolarimetric data archive. We report the results of the tests performed on\ncomputer clusters by using a public Infrastructure-as-a-Service and evaluating\nthe impact of cluster configuration on system performance. Results are\npromising for data mapping and information retrieval applications."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2851141.2851145", 
    "link": "http://arxiv.org/pdf/1501.05387v6", 
    "title": "Gunrock: A High-Performance Graph Processing Library on the GPU", 
    "arxiv-id": "1501.05387v6", 
    "author": "John D. Owens", 
    "publish": "2015-01-22T04:21:53Z", 
    "summary": "For large-scale graph analytics on the GPU, the irregularity of data access\nand control flow, and the complexity of programming GPUs have been two\nsignificant challenges for developing a programmable high-performance graph\nlibrary. \"Gunrock\", our graph-processing system designed specifically for the\nGPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on\noperations on a vertex or edge frontier. Gunrock achieves a balance between\nperformance and expressiveness by coupling high performance GPU computing\nprimitives and optimization strategies with a high-level programming model that\nallows programmers to quickly develop new graph primitives with small code size\nand minimal GPU programming knowledge. We evaluate Gunrock on five key graph\nprimitives and show that Gunrock has on average at least an order of magnitude\nspeedup over Boost and PowerGraph, comparable performance to the fastest GPU\nhardwired primitives, and better performance than any other GPU high-level\ngraph library."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2851141.2851145", 
    "link": "http://arxiv.org/pdf/1501.05789v1", 
    "title": "FlexCloud: A Flexible and Extendible Simulator for Performance   Evaluation of Virtual Machine Allocation", 
    "arxiv-id": "1501.05789v1", 
    "author": "Qin Xiong", 
    "publish": "2015-01-23T13:05:35Z", 
    "summary": "Cloud Data centers aim to provide reliable, sustainable and scalable services\nfor all kinds of applications. Resource scheduling is one of keys to cloud\nservices. To model and evaluate different scheduling policies and algorithms,\nwe propose FlexCloud, a flexible and scalable simulator that enables users to\nsimulate the process of initializing cloud data centers, allocating virtual\nmachine requests and providing performance evaluation for various scheduling\nalgorithms. FlexCloud can be run on a single computer with JVM to simulate\nlarge scale cloud environments with focus on infrastructure as a service;\nadopts agile design patterns to assure the flexibility and extensibility;\nmodels virtual machine migrations which is lack in the existing tools; provides\nuser-friendly interfaces for customized configurations and replaying. Comparing\nto existing simulators, FlexCloud has combining features for supporting public\ncloud providers, load-balance and energy-efficiency scheduling. FlexCloud has\nadvantage in computing time and memory consumption to support large-scale\nsimulations. The detailed design of FlexCloud is introduced and performance\nevaluation is provided."
},{
    "category": "cs.DC", 
    "doi": "10.1145/2851141.2851145", 
    "link": "http://arxiv.org/pdf/1501.06238v9", 
    "title": "Sky: Opinion Dynamics Based Consensus for P2P Network with Trust   Relationships", 
    "arxiv-id": "1501.06238v9", 
    "author": "Jiwu Shu", 
    "publish": "2015-01-26T02:35:07Z", 
    "summary": "Traditional Byzantine consensus does not work in P2P network due to Sybil\nattack while the most prevalent Sybil-proof consensus at present still can't\nresist adversary with dominant compute power. This paper proposed opinion\ndynamics based consensus for P2P network with trust relationships, consisting\nof the sky framework and the sky model. With the sky framework, opinion\ndynamics can be applied in P2P network for consensus which is Sybil-proof\nthrough trust relationships and emerges from local interactions of each node\nwith its direct contacts without topology, global information or even sample of\nthe network involved. The sky model has better performance of convergence than\nexisting models including MR, voter and Sznajd, and its lower bound of fault\ntolerance performance is also analyzed and proved. Simulations show that our\napproach can tolerant failures by at least 13% random nodes or 2% top\ninfluential nodes while over 96% correct nodes still make correct decision\nwithin 70 seconds on the SNAP Wikipedia who-votes-on-whom network for initial\nconfiguration of convergence>0.5 with reasonable latencies. Comparing to\ncompute power based consensus, our approach can resist any faulty or malicious\nnodes by unfollowing them. To the best of our knowledge, it's the first work to\nbring opinion dynamics to P2P network for consensus."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.06479v2", 
    "title": "CheepSync: A Time Synchronization Service for Resource Constrained   Bluetooth Low Energy Advertisers", 
    "arxiv-id": "1501.06479v2", 
    "author": "Jay Warrior", 
    "publish": "2015-01-26T16:52:40Z", 
    "summary": "Clock synchronization is highly desirable in distributed systems, including\nmany applications in the Internet of Things and Humans (IoTH). It improves the\nefficiency, modularity and scalability of the system, and optimizes use of\nevent triggers. For IoTH, Bluetooth Low Energy (BLE) - a subset of the recent\nBluetooth v4.0 stack - provides a low-power and loosely coupled mechanism for\nsensor data collection with ubiquitous units (e.g., smartphones and tablets)\ncarried by humans. This fundamental design paradigm of BLE is enabled by a\nrange of broadcast advertising modes. While its operational benefits are\nnumerous, the lack of a common time reference in the broadcast mode of BLE has\nbeen a fundamental limitation. This paper presents and describes CheepSync: a\ntime synchronization service for BLE advertisers, especially tailored for\napplications requiring high time precision on resource constrained BLE\nplatforms. Designed on top of the existing Bluetooth v4.0 standard, the\nCheepSync framework utilizes low-level timestamping and comprehensive error\ncompensation mechanisms for overcoming uncertainties in message transmission,\nclock drift and other system specific constraints. CheepSync was implemented on\ncustom designed nRF24Cheep beacon platforms (as broadcasters) and commercial\noff-the-shelf Android ported smartphones (as passive listeners). We demonstrate\nthe efficacy of CheepSync by numerous empirical evaluations in a variety of\nexperimental setups, and show that its average (single-hop) time\nsynchronization accuracy is in the 10us range."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.06663v1", 
    "title": "On Longest Repeat Queries Using GPU", 
    "arxiv-id": "1501.06663v1", 
    "author": "Bojian Xu", 
    "publish": "2015-01-27T06:04:35Z", 
    "summary": "Repeat finding in strings has important applications in subfields such as\ncomputational biology. The challenge of finding the longest repeats covering\nparticular string positions was recently proposed and solved by \\.{I}leri et\nal., using a total of the optimal $O(n)$ time and space, where $n$ is the\nstring size. However, their solution can only find the \\emph{leftmost} longest\nrepeat for each of the $n$ string position. It is also not known how to\nparallelize their solution. In this paper, we propose a new solution for\nlongest repeat finding, which although is theoretically suboptimal in time but\nis conceptually simpler and works faster and uses less memory space in practice\nthan the optimal solution. Further, our solution can find \\emph{all} longest\nrepeats of every string position, while still maintaining a faster processing\nspeed and less memory space usage. Moreover, our solution is\n\\emph{parallelizable} in the shared memory architecture (SMA), enabling it to\ntake advantage of the modern multi-processor computing platforms such as the\ngeneral-purpose graphics processing units (GPU). We have implemented both the\nsequential and parallel versions of our solution. Experiments with both\nbiological and non-biological data show that our sequential and parallel\nsolutions are faster than the optimal solution by a factor of 2--3.5 and 6--14,\nrespectively, and use less memory space."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.07056v1", 
    "title": "An Effective Framework for Managing University Data using a Cloud based   Environment", 
    "arxiv-id": "1501.07056v1", 
    "author": "Mansaf Alam", 
    "publish": "2015-01-28T10:37:33Z", 
    "summary": "Management of data in education sector particularly management of data for\nbig universities with several employees, departments and students is a very\nchallenging task. There are also problems such as lack of proper funds and\nmanpower for management of such data in universities. Education sector can\neasily and effectively take advantage of cloud computing skills for management\nof data. It can enhance the learning experience as a whole and can add entirely\nnew dimensions to the way in which education is imbibed. Several benefits of\nCloud computing such as monetary benefits, environmental benefits and remote\ndata access for management of data such as university database can be used in\neducation sector. Therefore, in this paper we have proposed an effective\nframework for managing university data using a cloud based environment. We have\nalso proposed cloud data management simulator: a new simulation framework which\ndemonstrates the applicability of cloud in the current education sector. The\nframework consists of a cloud developed for processing a universities database\nwhich consists of staff and students. It has the following features (i) support\nfor modeling cloud computing infrastructure, which includes data centers\ncontaining university database; (ii) a user friendly interface; (iii)\nflexibility to switch between the different types of users; and (iv)\nvirtualized access to cloud data."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.07079v1", 
    "title": "The Affinity Effects of Parallelized Libraries in Concurrent   Environments", 
    "arxiv-id": "1501.07079v1", 
    "author": "Antonio R. Mury", 
    "publish": "2015-01-28T12:10:46Z", 
    "summary": "The use of cloud computing grows as it appears to be an additional resource\nfor High-Performance Parallel and Distributed Computing (HPDC), especially with\nrespect to its use in support of scientific applications. Many studies have\nbeen devoted to determining the effect of the virtualization layer on the\nperformance, but most of the studies conducted so far lack insight into the\njoint effects between application type, virtualization layer and parallelized\nlibraries in applications. This work introduces the concept of affinity with\nregard to the combined effects of the virtualization layer, class of\napplication and parallelized libraries used in these applications. Affinity is\nhere defined as the degree of influence that one application has on other\napplications when running concurrently in virtual environments hosted on the\nsame real server. The results presented here show how parallel libraries used\nin application implementation have a significant influence and how the\ncombinations between these types of libraries and classes of applications could\nsignificantly influence the performance of the environment. In this context,\nthe concept of affinity is then used to evaluate these impacts to contribute to\nbetter stability and performance in the computational environment."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.07379v1", 
    "title": "Hardness of Virtual Network Embedding with Replica Selection", 
    "arxiv-id": "1501.07379v1", 
    "author": "Stefan Schmid", 
    "publish": "2015-01-29T08:59:38Z", 
    "summary": "Efficient embedding virtual clusters in physical network is a challenging\nproblem. In this paper we consider a scenario where physical network has a\nstructure of a balanced tree. This assumption is justified by many real- world\nimplementations of datacenters. We consider an extension to virtual cluster\nembedding by introducing replication among data chunks. In many real-world\napplications, data is stored in distributed and redundant way. This assumption\nintroduces additional hardness in deciding what replica to process. By\nreduction from classical NP-complete problem of Boolean Satisfia- bility, we\nshow limits of optimality of embedding. Our result holds even in trees of edge\nheight bounded by three. Also, we show that limiting repli- cation factor to\ntwo replicas per chunk type does not make the problem simpler."
},{
    "category": "cs.DC", 
    "doi": "10.1109/MCOM.2016.7378439", 
    "link": "http://arxiv.org/pdf/1501.07701v1", 
    "title": "Reliable Initialization of GPU-enabled Parallel Stochastic Simulations   Using Mersenne Twister for Graphics Processors", 
    "arxiv-id": "1501.07701v1", 
    "author": "David Hill", 
    "publish": "2015-01-30T08:49:18Z", 
    "summary": "Parallel stochastic simulations tend to exploit more and more computing power\nand they are now also developed for General Purpose Graphics Process Units\n(GP-GPUs). Conse-quently, they need reliable random sources to feed their\napplications. We propose a survey of the current Pseudo Random Numbers\nGenerators (PRNG) available on GPU. We give a particular focus to the recent\nMersenne Twister for Graphics Processors (MTGP) that has just been released.\nOur work provides empirically checked statuses designed to initialize a\nparticular configuration of this generator, in order to prevent any potential\nbias introduced by the parallelization of the PRNG."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1501.07800v4", 
    "title": "Locality-aware parallel block-sparse matrix-matrix multiplication using   the Chunks and Tasks programming model", 
    "arxiv-id": "1501.07800v4", 
    "author": "Elias Rudberg", 
    "publish": "2015-01-30T15:15:22Z", 
    "summary": "We present a method for parallel block-sparse matrix-matrix multiplication on\ndistributed memory clusters. By using a quadtree matrix representation, data\nlocality is exploited without prior information about the matrix sparsity\npattern. A distributed quadtree matrix representation is straightforward to\nimplement due to our recent development of the Chunks and Tasks programming\nmodel [Parallel Comput. 40, 328 (2014)]. The quadtree representation combined\nwith the Chunks and Tasks model leads to favorable weak and strong scaling of\nthe communication cost with the number of processes, as shown both\ntheoretically and in numerical experiments.\n  Matrices are represented by sparse quadtrees of chunk objects. The leaves in\nthe hierarchy are block-sparse submatrices. Sparsity is dynamically detected by\nthe matrix library and may occur at any level in the hierarchy and/or within\nthe submatrix leaves. In case graphics processing units (GPUs) are available,\nboth CPUs and GPUs are used for leaf-level multiplication work, thus making use\nof the full computing capacity of each node.\n  The performance is evaluated for matrices with different sparsity structures,\nincluding examples from electronic structure calculations. Compared to methods\nthat do not exploit data locality, our locality-aware approach reduces\ncommunication significantly, achieving essentially constant communication per\nnode in weak scaling tests."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.0050v1", 
    "title": "Cloud Scheduler: a resource manager for distributed compute clouds", 
    "arxiv-id": "1007.0050v1", 
    "author": "R. J. Sobie", 
    "publish": "2010-06-30T23:54:01Z", 
    "summary": "The availability of Infrastructure-as-a-Service (IaaS) computing clouds gives\nresearchers access to a large set of new resources for running complex\nscientific applications. However, exploiting cloud resources for large numbers\nof jobs requires significant effort and expertise. In order to make it simple\nand transparent for researchers to deploy their applications, we have developed\na virtual machine resource manager (Cloud Scheduler) for distributed compute\nclouds. Cloud Scheduler boots and manages the user-customized virtual machines\nin response to a user's job submission. We describe the motivation and design\nof the Cloud Scheduler and present results on its use on both science and\ncommercial clouds."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.0066v2", 
    "title": "A Taxonomy and Survey of Energy-Efficient Data Centers and Cloud   Computing Systems", 
    "arxiv-id": "1007.0066v2", 
    "author": "Albert Zomaya", 
    "publish": "2010-07-01T04:44:27Z", 
    "summary": "Traditionally, the development of computing systems has been focused on\nperformance improvements driven by the demand of applications from consumer,\nscientific and business domains. However, the ever increasing energy\nconsumption of computing systems has started to limit further performance\ngrowth due to overwhelming electricity bills and carbon dioxide footprints.\nTherefore, the goal of the computer system design has been shifted to power and\nenergy efficiency. To identify open challenges in the area and facilitate\nfuture advancements it is essential to synthesize and classify the research on\npower and energy-efficient design conducted to date. In this work we discuss\ncauses and problems of high power / energy consumption, and present a taxonomy\nof energy-efficient design of computing systems covering the hardware,\noperating system, virtualization and data center levels. We survey various key\nworks in the area and map them to our taxonomy to guide future design and\ndevelopment efforts. This chapter is concluded with a discussion of\nadvancements identified in energy-efficient computing and our vision on future\nresearch directions."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.0107v1", 
    "title": "First Smart Spaces", 
    "arxiv-id": "1007.0107v1", 
    "author": "Andy Wilson", 
    "publish": "2010-07-01T09:11:42Z", 
    "summary": "This document describes the Gloss software currently implemented. The\ndescription of the Gloss demonstrator for multi-surface interaction can be\nfound in D17. The ongoing integration activity for the work described in D17\nand D8 constitutes our development of infrastructure for a first smart space.\nIn this report, the focus is on infrastructure to support the implementation of\nlocation aware services. A local architecture provides a framework for\nconstructing Gloss applications, termed assemblies, that run on individual\nphysical nodes. A global architecture defines an overlay network for linking\nindividual assemblies. Both local and global architectures are under active\ndevelopment."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.0328v1", 
    "title": "Autonomic Management in a Distributed Storage System", 
    "arxiv-id": "1007.0328v1", 
    "author": "Markus Tauber", 
    "publish": "2010-07-02T10:08:57Z", 
    "summary": "This thesis investigates the application of autonomic management to a\ndistributed storage system. Effects on performance and resource consumption\nwere measured in experiments, which were carried out in a local area test-bed.\nThe experiments were conducted with components of one specific distributed\nstorage system, but seek to be applicable to a wide range of such systems, in\nparticular those exposed to varying conditions. The perceived characteristics\nof distributed storage systems depend on their configuration parameters and on\nvarious dynamic conditions. For a given set of conditions, one specific\nconfiguration may be better than another with respect to measures such as\nresource consumption and performance. Here, configuration parameter values were\nset dynamically and the results compared with a static configuration. It was\nhypothesised that under non-changing conditions this would allow the system to\nconverge on a configuration that was more suitable than any that could be set a\npriori. Furthermore, the system could react to a change in conditions by\nadopting a more appropriate configuration. Autonomic management was applied to\nthe peer-to-peer (P2P) and data retrieval components of ASA, a distributed\nstorage system. The effects were measured experimentally for various workload\nand churn patterns. The management policies and mechanisms were implemented\nusing a generic autonomic management framework developed during this work. The\nexperimental evaluations of autonomic management show promising results, and\nsuggest several future research topics. The findings of this thesis could be\nexploited in building other distributed storage systems that focus on\nharnessing storage on user workstations, since these are particularly likely to\nbe exposed to varying, unpredictable conditions."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.1049v3", 
    "title": "Simple Gradecast Based Algorithms", 
    "arxiv-id": "1007.1049v3", 
    "author": "Ezra N. Hoch", 
    "publish": "2010-07-07T04:50:43Z", 
    "summary": "Gradecast is a simple three-round algorithm presented by Feldman and Micali.\nThe current work presents a very simple algorithm that utilized Gradecast to\nachieve Byzantine agreement. Two small variations of the presented algorithm\nlead to improved algorithms for solving the Approximate agreement problem and\nthe Multi-consensus problem.\n  An optimal approximate agreement algorithm was presented by Fekete, which\nsupports up to 1/4 n Byzantine nodes and has message complexity of O(n^k),\nwhere n is the number of nodes and k is the number of rounds.\n  Our solution to the approximate agreement problem is optimal, simple and\nreduces the message complexity to O(k * n^3), while supporting up to 1/3 n\nByzantine nodes.\n  Multi consensus was first presented by Bar-Noy et al. It consists of\nconsecutive executions of l Byzantine consensuses. Bar-Noy et al., show an\noptimal amortized solution to this problem, assuming that all nodes start each\nconsensus instance at the same time, a property that cannot be guaranteed with\nearly stopping. Our solution is simpler, preserves round complexity optimality,\nallows early stopping and does not require synchronized starts of the consensus\ninstances."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.1086v1", 
    "title": "On the Power of Impersonation Attacks", 
    "arxiv-id": "1007.1086v1", 
    "author": "Michael Okun", 
    "publish": "2010-07-07T09:33:58Z", 
    "summary": "In this paper we consider a synchronous message passing system in which in\nevery round an external adversary is able to send each processor up to k\nmessages with falsified sender identities and arbitrary content. It is formally\nshown that this impersonation model is slightly stronger than the asynchronous\nmessage passing model with crash failures. In particular, we prove that\n(k+1)-set agreement can be solved in this model, while k-set agreement is\nimpossible, for any k>=1. The different strength of the asynchronous and\nimpersonation models is exhibited by the order preserving renaming problem, for\nwhich an algorithm with n+k target namespace exists in the impersonation model,\nwhile an exponentially larger namespace is required in case of asynchrony."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.1189v3", 
    "title": "A Jamming-Resistant MAC Protocol for Multi-Hop Wireless Networks", 
    "arxiv-id": "1007.1189v3", 
    "author": "Jin Zhang", 
    "publish": "2010-07-07T17:25:53Z", 
    "summary": "This paper presents a simple local medium access control protocol, called\n\\textsc{Jade}, for multi-hop wireless networks with a single channel that is\nprovably robust against adaptive adversarial jamming. The wireless network is\nmodeled as a unit disk graph on a set of nodes distributed arbitrarily in the\nplane. In addition to these nodes, there are adversarial jammers that know the\nprotocol and its entire history and that are allowed to jam the wireless\nchannel at any node for an arbitrary $(1-\\epsilon)$-fraction of the time steps,\nwhere $0<\\epsilon<1$ is an arbitrary constant. We assume that the nodes cannot\ndistinguish between jammed transmissions and collisions of regular messages.\nNevertheless, we show that \\textsc{Jade} achieves an asymptotically optimal\nthroughput if there is a sufficiently dense distribution of nodes."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.1261v1", 
    "title": "MalStone: Towards A Benchmark for Analytics on Large Data Clouds", 
    "arxiv-id": "1007.1261v1", 
    "author": "Steve Vejcik", 
    "publish": "2010-07-07T23:26:22Z", 
    "summary": "Developing data mining algorithms that are suitable for cloud computing\nplatforms is currently an active area of research, as is developing cloud\ncomputing platforms appropriate for data mining. Currently, the most common\nbenchmark for cloud computing is the Terasort (and related) benchmarks.\nAlthough the Terasort Benchmark is quite useful, it was not designed for data\nmining per se. In this paper, we introduce a benchmark called MalStone that is\nspecifically designed to measure the performance of cloud computing middleware\nthat supports the type of data intensive computing common when building data\nmining models. We also introduce MalGen, which is a utility for generating data\non clouds that can be used with MalStone."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.1709v2", 
    "title": "A Fault-Resistant Asynchronous Clock Function", 
    "arxiv-id": "1007.1709v2", 
    "author": "Danny Dolev", 
    "publish": "2010-07-10T09:45:11Z", 
    "summary": "Consider an asynchronous network in a shared-memory environment consisting of\nn nodes. Assume that up to f of the nodes might be Byzantine (n > 12f), where\nthe adversary is full-information and dynamic (sometimes called adaptive). In\naddition, the non-Byzantine nodes may undergo transient failures. Nodes advance\nin atomic steps, which consist of reading all registers, performing some\ncalculation and writing to all registers.\n  This paper contains three main contributions. First, the clock-function\nproblem is defined, which is a generalization of the clock synchronization\nproblem. This generalization encapsulates previous clock synchronization\nproblem definitions while extending them to the current paper's model. Second,\na randomized asynchronous self-stabilizing Byzantine tolerant clock\nsynchronization algorithm is presented.\n  In the construction of the clock synchronization algorithm, a building block\nthat ensures different nodes advance at similar rates is developed. This\nfeature is the third contribution of the paper. It is self-stabilizing and\nByzantine tolerant and can be used as a building block for different algorithms\nthat operate in an asynchronous self-stabilizing Byzantine model.\n  The convergence time of the presented algorithm is exponential. Observe that\nin the asynchronous setting the best known full-information dynamic Byzantine\nagreement also has expected exponential convergence time, even though currently\nthere is no known reduction between the two."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.2902v1", 
    "title": "LANC: locality-aware network coding for better P2P traffic localization", 
    "arxiv-id": "1007.2902v1", 
    "author": "Guoqing Zhang", 
    "publish": "2010-07-17T06:25:34Z", 
    "summary": "As ISPs begin to cooperate to expose their network locality information as\nservices, e.g., P4P, solutions based on locality information provision for P2P\ntraffic localization will soon approach their capability limits. A natural\nquestion is: can we do any better provided that no further locality information\nimprovement can be made? This paper shows how the utility of locality\ninformation could be limited by conventional P2P data scheduling algorithms,\neven as sophisticated as the local rarest first policy.\n  Network coding's simplified data scheduling makes it competent for improving\nP2P application's throughput. Instead of only using locality information in the\ntopology construction, this paper proposes the locality-aware network coding\n(LANC) that uses locality information in both the topology construction and\ndownloading decision, and demonstrates its exceptional ability for P2P traffic\nlocalization. The randomization introduced by network coding enhances the\nchance for a peer to find innovative blocks in its neighborhood. Aided by\nproper locality-awareness, the probability for a peer to get innovative blocks\nfrom its proximity will increase as well, resulting in more efficient use of\nnetwork resources. Extensive simulation results show that LANC can\nsignificantly reduce P2P traffic redundancy without sacrificing\napplication-level performance. Aided by the same locality knowledge, the\ntraffic redundancies of LANC in most cases are less than 50\\% of the current\nbest approach that does not use network coding."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.06.005", 
    "link": "http://arxiv.org/pdf/1007.4057v1", 
    "title": "Precise, Scalable and Online Request Tracing for Multi-tier Services of   Black Boxes", 
    "arxiv-id": "1007.4057v1", 
    "author": "Dan Meng", 
    "publish": "2010-07-23T07:37:32Z", 
    "summary": "As more and more multi-tier services are developed from commercial\noff-the-shelf components or heterogeneous middleware without source code\navailable, both developers and administrators need a request tracing tool to\n(1) exactly know how a user request of interest travels through services of\nblack boxes; (2) obtain macro-level user request behavior information of\nservices without the necessity of inundating within massive logs. Previous\nresearch efforts either accept imprecision of probabilistic correlation methods\nor present precise but unscalable tracing approaches that have to collect and\nanalyze large amount of logs; Besides, previous precise request tracing\napproaches of black boxes fail to propose macro-level abstractions that enables\ndebugging performance-in-the-large, and hence users have to manually interpret\nmassive logs. This paper introduces a precise, scalable and online request\ntracing tool, named PreciseTracer, for multi-tier services of black boxes. Our\ncontributions are four-fold: first, we propose a precise request tracing\nalgorithm for multi-tier services of black boxes, which only uses\napplication-independent knowledge; second, we respectively present micro-level\nand macro-level abstractions: component activity graphs and dominated causal\npath patterns to represent causal paths of each individual request and\nrepeatedly executed causal paths that account for significant fractions; third,\nwe present two mechanisms: tracing on demand and sampling to significantly\nincrease system scalability; fourth, we design and implement an online request\ntracing tool. PreciseTracer's fast response, low overhead and scalability make\nit a promising tracing tool for large-scale production systems."
},{
    "category": "cs.DC", 
    "doi": "10.1017/S1471068410000190", 
    "link": "http://arxiv.org/pdf/1007.4438v1", 
    "title": "Threads and Or-Parallelism Unified", 
    "arxiv-id": "1007.4438v1", 
    "author": "Ricardo Rocha", 
    "publish": "2010-07-26T12:36:31Z", 
    "summary": "One of the main advantages of Logic Programming (LP) is that it provides an\nexcellent framework for the parallel execution of programs. In this work we\ninvestigate novel techniques to efficiently exploit parallelism from real-world\napplications in low cost multi-core architectures. To achieve these goals, we\nrevive and redesign the YapOr system to exploit or-parallelism based on a\nmulti-threaded implementation. Our new approach takes full advantage of the\nstate-of-the-art fast and optimized YAP Prolog engine and shares the underlying\nexecution environment, scheduler and most of the data structures used to\nsupport YapOr's model. Initial experiments with our new approach consistently\nachieve almost linear speedups for most of the applications, proving itself as\na good alternative for exploiting implicit parallelism in the currently\navailable low cost multi-core architectures."
},{
    "category": "cs.DC", 
    "doi": "10.1063/1.3462759", 
    "link": "http://arxiv.org/pdf/1007.4723v1", 
    "title": "Public Infrastructure for Monte Carlo Simulation: publicMC@BATAN", 
    "arxiv-id": "1007.4723v1", 
    "author": "L. T. Handoko", 
    "publish": "2010-07-27T13:58:10Z", 
    "summary": "The first cluster-based public computing for Monte Carlo simulation in\nIndonesia is introduced. The system has been developed to enable public to\nperform Monte Carlo simulation on a parallel computer through an integrated and\nuser friendly dynamic web interface. The beta version, so called\npublicMC@BATAN, has been released and implemented for internal users at the\nNational Nuclear Energy Agency (BATAN). In this paper the concept and\narchitecture of publicMC@BATAN are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1063/1.3462759", 
    "link": "http://arxiv.org/pdf/1007.4890v2", 
    "title": "PowerTracer: Tracing requests in multi-tier services to save cluster   power consumption", 
    "arxiv-id": "1007.4890v2", 
    "author": "Haining Wang", 
    "publish": "2010-07-28T08:13:24Z", 
    "summary": "As energy proportional computing gradually extends the success of DVFS\n(Dynamic voltage and frequency scaling) to the entire system, DVFS control\nalgorithms will play a key role in reducing server clusters' power consumption.\nThe focus of this paper is to provide accurate cluster-level DVFS control for\npower saving in a server cluster. To achieve this goal, we propose a request\ntracing approach that online classifies the major causal path patterns of a\nmulti-tier service and monitors their performance data as a guide for accurate\nDVFS control. The request tracing approach significantly decreases the time\ncost of performance profiling experiments that aim to establish the empirical\nperformance model. Moreover, it decreases the controller complexity so that we\ncan introduce a much simpler feedback controller, which only relies on the\nsingle-node DVFS modulation at a time as opposed to varying multiple CPU\nfrequencies simultaneously. Based on the request tracing approach, we present a\nhybrid DVFS control system that combines an empirical performance model for\nfast modulation at different load levels and a simpler feedback controller for\nadaption. We implement a prototype of the proposed system, called PowerTracer,\nand conduct extensive experiments on a 3-tier platform. Our experimental\nresults show that PowerTracer outperforms its peer in terms of power saving and\nsystem performance."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1007.5088v1", 
    "title": "Simplified Distributed Programming with Micro Objects", 
    "arxiv-id": "1007.5088v1", 
    "author": "Maarten van Steen", 
    "publish": "2010-07-29T00:12:33Z", 
    "summary": "Developing large-scale distributed applications can be a daunting task.\nobject-based environments have attempted to alleviate problems by providing\ndistributed objects that look like local objects. We advocate that this\napproach has actually only made matters worse, as the developer needs to be\naware of many intricate internal details in order to adequately handle partial\nfailures. The result is an increase of application complexity. We present an\nalternative in which distribution transparency is lessened in favor of clearer\nsemantics. In particular, we argue that a developer should always be offered\nthe unambiguous semantics of local objects, and that distribution comes from\ncopying those objects to where they are needed. We claim that it is often\nsufficient to provide only small, immutable objects, along with facilities to\ngroup objects into clusters."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.0198v1", 
    "title": "Why The Results of Parallel and Serial Monte Carlo Simulations May   Differ", 
    "arxiv-id": "1104.0198v1", 
    "author": "Boris D. Lubachevsky", 
    "publish": "2011-04-01T15:26:43Z", 
    "summary": "Parallel Monte Carlo simulations often expose faults in random number\ngenerators"
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.2566v1", 
    "title": "Load-Balancing Spatially Located Computations using Rectangular   Partitions", 
    "arxiv-id": "1104.2566v1", 
    "author": "\u00dcmit V. \u00c7ataly\u00fcrek", 
    "publish": "2011-04-13T18:08:57Z", 
    "summary": "Distributing spatially located heterogeneous workloads is an important\nproblem in parallel scientific computing. We investigate the problem of\npartitioning such workloads (represented as a matrix of non-negative integers)\ninto rectangles, such that the load of the most loaded rectangle (processor) is\nminimized. Since finding the optimal arbitrary rectangle-based partition is an\nNP-hard problem, we investigate particular classes of solutions: rectilinear,\njagged and hierarchical. We present a new class of solutions called m-way\njagged partitions, propose new optimal algorithms for m-way jagged partitions\nand hierarchical partitions, propose new heuristic algorithms, and provide\nworst case performance analyses for some existing and new heuristics. Moreover,\nthe algorithms are tested in simulation on a wide set of instances. Results\nshow that two of the algorithms we introduce lead to a much better load balance\nthan the state-of-the-art algorithms. We also show how to design a two-phase\nalgorithm that reaches different time/quality tradeoff."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.3876v1", 
    "title": "Extending and Implementing the Self-adaptive Virtual Processor for   Distributed Memory Architectures", 
    "arxiv-id": "1104.3876v1", 
    "author": "Juha Koivisto", 
    "publish": "2011-04-19T20:44:19Z", 
    "summary": "Many-core architectures of the future are likely to have distributed memory\norganizations and need fine grained concurrency management to be used\neffectively. The Self-adaptive Virtual Processor (SVP) is an abstract\nconcurrent programming model which can provide this, but the model and its\ncurrent implementations assume a single address space shared memory. We\ninvestigate and extend SVP to handle distributed environments, and discuss a\nprototype SVP implementation which transparently supports execution on\nheterogeneous distributed memory clusters over TCP/IP connections, while\nretaining the original SVP programming model."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.3947v1", 
    "title": "Communication Optimalement Stabilisante sur Canaux non Fiables et non   FIFO", 
    "arxiv-id": "1104.3947v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-20T06:08:34Z", 
    "summary": "A self-stabilizing protocol has the capacity to recover a legitimate behavior\nwhatever is its initial state. The majority of works in self-stabilization\nassume a shared memory model or a communication using reliable and FIFO\nchannels. In this article, we interest in self-stabilizing systems using\nbounded but non reliable and non FIFO channels. We propose a stabilizing\ncommunication protocol with optimal fault resilience. In more details, this\nprotocol simulates a reliable and FIFO channel and ensures a minimal number of\nlooses, duplications, creations, and re-ordering of messages."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4022v1", 
    "title": "Auto-Stabilisation et Confinement de Fautes Malicieuses : Optimalit\u00e9   du Protocole min+1", 
    "arxiv-id": "1104.4022v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2011-04-20T13:45:26Z", 
    "summary": "A self-stabilizing is naturally resilient to transients faults (that is,\nfaults of finite duration). Recently, a new class of protocol appears. These\nprotocols are self-stabilizing and are moreover resilient to a limited number\nof permanent faults. In this article, we interest in self-stabilizing protocols\nthat tolerate very hard permanent faults: Byzantine faults. We introduce two\nnew scheme of Byzantine containment in self-stabilizing systems. We show that,\nfor the problem of BFS spanning tree construction, the well known\nself-stabilizing protocol min+1 provides without significant modification the\nbest Byzantine containment with respect to these new schemes."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4379v1", 
    "title": "Platforms for Building and Deploying Applications for Cloud Computing", 
    "arxiv-id": "1104.4379v1", 
    "author": "Karthik Sukumar", 
    "publish": "2011-04-22T02:51:54Z", 
    "summary": "Cloud computing is rapidly emerging as a new paradigm for delivering IT\nservices as utlity-oriented services on subscription-basis. The rapid\ndevelopment of applications and their deployment in Cloud computing\nenvironments in efficient manner is a complex task. In this article, we give a\nbrief introduction to Cloud computing technology and Platform as a Service, we\nexamine the offerings in this category, and provide the basis for helping\nreaders to understand basic application platform opportunities in Cloud by\ntechnologies such as Microsoft Azure, Sales Force, Google App, and Aneka for\nCloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application\nPlatform (CAP) leveraging these concepts and allowing an easy development of\nCloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers\nfacilities for quickly developing Cloud applications and a modular platform\nwhere additional services can be easily integrated to extend the system\ncapabilities, thus being at pace with the rapidly evolution of Cloud computing."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.30.1", 
    "link": "http://arxiv.org/pdf/1104.4475v1", 
    "title": "Tiled QR factorization algorithms", 
    "arxiv-id": "1104.4475v1", 
    "author": "Yves Robert", 
    "publish": "2011-04-22T16:45:02Z", 
    "summary": "This work revisits existing algorithms for the QR factorization of\nrectangular matrices composed of p-by-q tiles, where p >= q. Within this\nframework, we study the critical paths and performance of algorithms such as\nSameh and Kuck, Modi and Clarke, Greedy, and those found within PLASMA.\nAlthough neither Modi and Clarke nor Greedy is optimal, both are shown to be\nasymptotically optimal for all matrices of size p = q^2 f(q), where f is any\nfunction such that \\lim_{+\\infty} f= 0. This novel and important complexity\nresult applies to all matrices where p and q are proportional, p = \\lambda q,\nwith \\lambda >= 1, thereby encompassing many important situations in practice\n(least squares). We provide an extensive set of experiments that show the\nsuperiority of the new algorithms for tall matrices."
},lol]